Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 803?812,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
A Relational Model of Semantic Similarity between Words using
Automatically Extracted Lexical Pattern Clusters from the Web
Danushka Bollegala
?
danushka@mi.ci.i.
u-tokyo.ac.jp
Yutaka Matsuo
matsuo@biz-model.
t.u-tokyo.ac.jp
The University of Tokyo
7-3-1, Hongo, Tokyo, 113-8656, Japan
Mitsuru Ishizuka
ishizuka@i.
u-tokyo.ac.jp
Abstract
Semantic similarity is a central concept
that extends across numerous fields such
as artificial intelligence, natural language
processing, cognitive science and psychol-
ogy. Accurate measurement of semantic
similarity between words is essential for
various tasks such as, document cluster-
ing, information retrieval, and synonym
extraction. We propose a novel model
of semantic similarity using the semantic
relations that exist among words. Given
two words, first, we represent the seman-
tic relations that hold between those words
using automatically extracted lexical pat-
tern clusters. Next, the semantic similar-
ity between the two words is computed
using a Mahalanobis distance measure.
We compare the proposed similarity mea-
sure against previously proposed seman-
tic similarity measures on Miller-Charles
benchmark dataset and WordSimilarity-
353 collection. The proposed method out-
performs all existing web-based seman-
tic similarity measures, achieving a Pear-
son correlation coefficient of 0.867 on the
Millet-Charles dataset.
1 Introduction
Similarity is a fundamental concept in theories
of knowledge and behavior. Psychological ex-
periments have shown that similarity acts as an
organizing principle by which individuals clas-
sify objects, and make generalizations (Goldstone,
1994). For example, a biologist would classify
a newly found animal specimen based upon the
properties that it shares with existing categories
of animals. We can then make additional infer-
ences on the new specimen using the properties
?
Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
known for the existing category. As the simi-
larity between two objects X and Y increases,
so does the probability of correctly inferring that
Y has the property T upon knowing that X has
T (Tenenbaum, 1999). Accurate measurement of
semantic similarity between lexical units such as
words or phrases is important for numerous tasks
in natural language processing such as word sense
disambiguation (Resnik, 1995), synonym extrac-
tion (Lin, 1998a), and automatic thesauri gener-
ation (Curran, 2002). In information retrieval,
similar or related words are used to expand user
queries to improve recall (Sahami and Heilman,
2006).
Semantic similarity is a context dependent and
dynamic phenomenon. New words are constantly
being created and existing words are assigned with
new senses on the Web. To decide whether two
words are semantically similar, it is important to
know the semantic relations that hold between the
words. For example, the words horse and cow can
be considered semantically similar because both
horses and cows are useful animals in agriculture.
Similarly, a horse and a car can be considered se-
mantically similar because cars, and historically
horses, are used for transportation. Semantic re-
lations such as X and Y are used in agriculture,
or X and Y are used for transportation, exist be-
tween two words X and Y in these examples. We
use bold-italics, X, to denote the slot of a word X
in a lexical pattern.
We propose a relational model to compute the
semantic similarity between two words. First, us-
ing snippets retrieved from a web search engine,
we present an automatic lexical pattern extraction
algorithm to represent the semantic relations that
exist between two words. For example, given two
words ostrich and bird, we extract X is a Y, X is
a large Y, and X is a flightless Y from the Web.
Using a set of semantically related words as train-
ing data, we evaluate the confidence of a lexical
803
pattern as an indicator of semantic similarity. For
example, the pattern X is a Y is a better indica-
tor of semantic similarity between X and Y than
the pattern X and Y. Consequently, we would like
to emphasize the former pattern by assigning it a
higher confidence score. It is noteworthy that all
lexical patterns are not independent ? multiple lex-
ical patterns can express the same semantic rela-
tion. For example, the pattern X is a large Y sub-
sumes the more general pattern X is a Y and they
both indicate a hypernymic relationship between
X and Y. By clustering the semantically related
patterns into groups, we can both overcome the
data sparseness problem, and reduce the number
of parameters during training. To identify seman-
tically related patterns, we use a sequential pattern
clustering algorithm that is based on the distribu-
tional hypothesis (Harris, 1954). We represent two
words by a feature vector defined over the clus-
ters of patterns. Finally, the semantic similarity
is computed as the Mahalanobis distance between
points corresponding to the feature vectors. By
using Mahalanobis distance instead of Euclidean
distance, we can account for the inter-dependence
between semantic relations.
2 Related Work
Geometric models, such as multi-dimensional
scaling has been used in psychological ex-
periments analyzing the properties of similar-
ity (Krumhansl, 1978). These models represent
objects as points in some coordinate space such
that the observed dissimilarities between objects
correspond to the metric distances between the re-
spective points. Geometric models assume that
objects can be adequately represented as points in
some coordinate space and that dissimilarity be-
haves like a metric distance function satisfying
minimality, symmetry, and triangle inequality as-
sumptions. However, both dimensional and metric
assumptions are open to question.
Tversky (1977) proposed the contrast model of
similarity to overcome the problems in geometric
models. The contrast model relies on featural rep-
resentation of objects, and it is used to compute the
similarity between the representations of two ob-
jects. Similarity is defined as an increasing func-
tion of common features (i.e. features in common
to the two objects), and as a decreasing function of
distinctive features (i.e. features that apply to one
object but not the other). The attributes of objects
are primal to contrast model and it does not ex-
plicitly incorporate the relations between objects
when measuring similarity.
Hahn et al (2003) define similarity between
two representations as the complexity required to
transform one representation into the other. Their
model of similarity is based on the Representa-
tional Distortion theory, which aims to provide
a theoretical framework of similarity judgments.
Their experiments using pattern sequences and ge-
ometric shapes show an inverse correlation be-
tween the number of transformations required to
convert one pattern (or shape) to another, and the
perceived similarity ratings by human subjects.
How to represent an object, which transformations
are allowed on a representation, and how to mea-
sure the complexity of a transformation, are all
important decisions in the transformational model
of similarity. Although distance measures such as
edit distance have been used to find approximate
matches in a dictionary, it is not obvious how to
compute semantic similarity between words using
representational distortion theory.
Given a taxonomy of concepts, a straightfor-
ward method to calculate similarity between two
words (or concepts) is to find the length of the
shortest path connecting the two words in the tax-
onomy (Rada et al, 1989). If a word is polyse-
mous (i.e. has more than one sense) then multi-
ple paths might exist between the two words. In
such cases, only the shortest path between any two
senses of the words is considered for calculating
similarity. A problem that is frequently acknowl-
edged with this approach is that it relies on the
notion that all links in the taxonomy represent a
uniform distance. As a solution to this problem,
Schickel-Zuber and Faltings (2007) propose ontol-
ogy structure based similarity (OSS) between two
concepts in an ontology, which is an asymmetric
distance function.
Resnik (1995) proposed a similarity measure
using information content. He defined the similar-
ity between two concepts C
1
and C
2
in the taxon-
omy as the maximum of the information content of
all concepts C that subsume both C
1
and C
2
. Then
the similarity between two words is defined as the
maximum of the similarity between any concepts
that the words belong to. He used WordNet as the
taxonomy; information content is calculated using
the Brown corpus.
Li et al, (2003) combined structural seman-
804
tic information from a lexical taxonomy, and in-
formation content from a corpus, in a nonlinear
model. They proposed a similarity measure that
uses shortest path length, depth and local density
in a taxonomy. Their experiments reported a Pear-
son correlation coefficient of 0.8914 on the Miller-
Charles benchmark dataset (Miller and Charles,
1998). Lin (1998b) defined the similarity between
two concepts as the information that is in common
to both concepts and the information contained in
each individual concept.
Cilibrasi and Vitanyi (2007) proposed a distance
metric between words using page-counts retrieved
from a web search engine. The proposed metric is
named Normalized Google Distance (NGD) and is
defined as the normalized information distance (Li
et al, 2004) between two strings. They evaluate
NGD in a word classification task. Unfortunately
NGD only uses page-counts of words and ignores
the context in which the words appear. Therefore,
it produces inaccurate similarity scores when one
or both words between which similarity is com-
puted are polysemous.
Sahami and Heilman (2006) measured semantic
similarity between two queries using snippets re-
turned for those queries by a search engine. For
each query, they collect snippets from a search
engine and represent each snippet as a TF-IDF-
weighted term vector. Each vector is L
2
normal-
ized and the centroid of the set of vectors is com-
puted. Semantic similarity between two queries
is then defined as the inner product between the
corresponding centroid vectors. They did not
compare their similarity measure with taxonomy-
based similarity measures.
Chen et al, (2006) propose a web-based double-
checking model to compute the semantic similar-
ity between words. For two words X and Y , they
collect snippets for each word from a web search
engine. Then they count the number of occur-
rences of X in the snippets for Y , and Y in the
snippets for X . The two values are combined non-
linearly to compute the similarity between X and
Y . This method heavily depends on the search en-
gine?s ranking algorithm. Although two words X
and Y may be very similar, there is no reason to
believe that one can find Y in the snippets for X ,
or vice versa. This observation is confirmed by the
experimental results in their paper which reports 0
similarity scores for many pairs of words in the
Miller-Charles dataset.
In our previous work (Bollegala et al, 2007),
we proposed a semantic similarity measure using
page counts and snippets retrieved from a Web
search engine. To compute the similarity between
two words X and Y , we queried a web search en-
gine using the query X AND Y and extract lex-
ical patterns that combine X and Y from snip-
pets. A feature vector is formed using frequen-
cies of 200 lexical patterns in snippets and four
co-occurrence measures: Dice coefficient, overlap
coefficient, Jaccard coefficient and pointwise mu-
tual information. We trained a two-class support
vector machine using automatically selected syn-
onymous and non-synonymous word pairs from
WordNet. This method reports a Pearson corre-
lation coefficient of 0.837 with Miller-Charles rat-
ings. However, it does not consider the relatedness
between patterns.
Gabrilovich and Markovitch (2007) represent
words using weighted vectors of Wikipedia-based
concepts, and define the similarity between words
as the cosine of the angle between the correspond-
ing vectors. Their method can be used to com-
pute similarity between words as well as between
texts. Although Wikipedia is growing in popular-
ity, not all concepts found on the Web have arti-
cles in Wikipedia. Specially, novel or not very
popular concepts are not adequately covered by
Wikipedia. Moreover, their method requires the
concepts to be independent. For non-independent,
hierarchical taxonomies such as open directory
project (ODP)
1
, their method produces suboptimal
results.
3 Relational Model of Similarity
We propose a model to compute the semantic sim-
ilarity between two words a and b using the set
of semantic relations R(a, b) that hold between a
and b. We call the proposed model the relational
model of semantic similarity and it is defined by
the following equation,
sim(a, b) = ?(R(a, b)). (1)
Here, sim(a, b) is the semantic similarity between
the two words a and b, and ? is a weighting
function defined over the set of semantic relations
R(a, b). Given that a particular set of semantic
relations are known to hold between two words,
the function ? expresses our confidence on those
words being semantically similar.
1
http://www.dmoz.org
805
A semantic relation can be expressed in a num-
ber of ways. For example, given a taxonomy of
words such as the WordNet, semantic relations
(i.e. hypernymy, meronymy, synonymy etc.) be-
tween words can be directly looked up in the tax-
onomy. Alternatively, the labels of the edges in
the path connecting two words can be used as
semantic relations. However, in this paper we
do not assume the availability of manually cre-
ated resources such as dictionaries or taxonomies.
We represent semantic relations using automati-
cally extracted lexical patterns. Lexical patterns
have been successfully used to represent various
semantic relations between words such as hyper-
nymy (Hearst, 1992), and meronymy (Berland and
Charniak, 1999). Following these previous ap-
proaches, we represent R(a, b) as a set of lexical
patterns. Moreover, we denote the frequency of a
lexical pattern r for a word pair (a, b) by f(r, a, b).
So far we have not defined the functional form
of ?. A straightforward approach is to use a lin-
early weighted combination of relations as shown
below,
?(R(a, b)) =
?
r
i
?R(a,b)
w
i
? f(r
i
, a, b). (2)
Here, w
i
is the weight associated with the lexical
pattern r
i
and can be determined using training
data. However, this formulation has two funda-
mental drawbacks. First, the number of weight
parameters w
i
is equal to the number of lexical
patterns. Typically two words can co-occur in nu-
merous patterns. Consequently, we end up with a
large number of parameters in the model. Com-
plex models with a large number of parameters
are difficult to train because they tend to overfit to
the training data. Second, the linear combination
given in Equation 2 assumes the lexical patterns
to be mutually independent. However, in practice
this is not true. For example, both patterns X is a
Y and Y such as X indicate a hypernymic relation
between X and Y.
To overcome the above mentioned limitations,
we first cluster the lexical patterns to identify the
semantically related patterns. Our clustering algo-
rithm is detailed in section 3.2. Next, we define ?
using the formed clusters as follows,
?(R(a, b)) = x
T
ab
??. (3)
Here, x
ab
is a feature vector representing the
words a and b. Each formed cluster contributes
a feature in vector x
ab
as described later in Sec-
tion 5. The vector ? is a prototypical vector rep-
resenting synonymous word pairs. We compute
? as the centroid of feature vectors representing
synonymous word pairs. ? is the inter-cluster cor-
relation matrix. The (i, j)-th element of matrix ?
denotes the correlation between the two clusters c
i
and c
j
. Matrix ? is expected to capture the de-
pendence between semantic relations. Intuitively,
if two clusters i and j are highly correlated, then
the (i, j)-th element of ? will be closer to 1. Equa-
tion 3 computes the similarity between a word pair
(a, b) and a set of synonymous word pairs. In-
tuitively, if the relations that exist between a and
b are typical relations that hold between synony-
mous word pairs, then Equation 3 returns a high
similarity score for a and b.
The proposed relational model of semantic sim-
ilarity differs from feature models of similarity,
such as the contrast model (Tversky, 1977), in
that it is defined over the set of semantic relations
that exist between two words instead of the set of
features for each word. Specifically, in contrast
model, the similarity S(a, b) between two objects
a and b is defined in terms of the features common
to a and b, A ? B, the features that are distinctive
to a, A?B, and the features that are distinctive to
b, B ?A. The contrast model is formalized in the
following equation,
S(a, b) = ?f(A ?B)? ?f(A?B)? ?f(B ?A). (4)
Here, the function f measures the salience of a
particular set of features, and non-negative param-
eters ?, ?, and ? determine the relative weights
assigned to the different components. However, in
the relational model of similarity we do not focus
on features of individual words but on relations be-
tween two words.
Modeling similarity as a phenomenon of rela-
tions between objects rather than features of indi-
vidual objects is central to computational models
of analogy-making such as the structure mapping
theory (SMT) (Falkenhainer et al, 1989). SMT
claims that an analogy is a mapping of knowl-
edge from one domain (base) into another (target)
which conveys that a system of relations known
to hold in the base also holds in the target. The
target objects do not have to resemble their corre-
sponding base objects. During the mapping pro-
cess, features of individual objects are dropped
and only relations are mapped. The proposed rela-
tional model of similarity uses this relational view
806
Ostrich, a large, flightless bird that lives in the dry grass-
lands of Africa.
Figure 1: A snippet returned for the query ?ostrich
* * * * * bird?.
of similarity to compute semantic similarity be-
tween words.
3.1 Extracting Lexical Patterns
To compute semantic similarity between two
words using the relational model (Equation 3),
we must first extract the numerous lexical pat-
terns from contexts in which those two words ap-
pear. For this purpose, we propose a pattern ex-
traction algorithm using snippets retrieved from
a web search engine. The proposed method re-
quires no language-dependent preprocessing such
as part-of-speech tagging or dependency parsing,
which can be both time consuming at Web scale,
and likely to produce incorrect results because of
the fragmented and ill-formed snippets.
Given two words a and b, we query a web search
engine using the wildcard query ?a * * * * * b?
and download snippets. The ?*? operator matches
one word or none in a web page. Therefore, our
wildcard query retrieves snippets in which a and
b appear within a window of seven words. We
attempt to approximate the local context of two
words using wildcard queries. For example, Fig-
ure 1 shows a snippet retrieved for the query ?os-
trich * * * * * bird?.
For a snippet S, retrieved for a word pair (a, b),
first, we replace the two words a and b, respec-
tively, with two variables X and Y. We replace all
numeric values by D, a marker for digits. Next,
we generate all subsequences of words from S that
satisfy all of the following conditions.
(i). A subsequence must contain exactly one oc-
currence of each X and Y
(ii). The maximum length of a subsequence is L
words.
(iii). A subsequence is allowed to have gaps. How-
ever, we do not allow gaps of more than g
number of words. Moreover, the total length
of all gaps in a subsequence should not ex-
ceed G words.
(iv). We expand all negation contractions in a con-
text. For example, didn?t is expanded to did
not. We do not skip the word not when gen-
erating subsequences. For example, this con-
dition ensures that from the snippet X is not a
Y, we do not produce the subsequence X is a
Y.
Finally, we count the frequency of all generated
subsequences and only use subsequences that oc-
cur more than N times as lexical patterns.
The parameters L, g, G and N are set exper-
imentally, as explained later in Section 6. It is
noteworthy that the proposed pattern extraction al-
gorithm considers all the words in a snippet, and
is not limited to extracting patterns only from the
mid-fix (i.e., the portion of text in a snippet that
appears between the queried words). Moreover,
the consideration of gaps enables us to capture re-
lations between distant words in a snippet. We use
a modified version of the prefixspan algorithm (Pei
et al, 2004) to generate subsequences from a text
snippet. Specifically, we use the constraints (ii)-
(iv) to prune the search space of candidate sub-
sequences. For example, if a subsequence has
reached the maximum length L, or contains the
maximum number of gaps G, then we will not ex-
tend it further. By pruning the search space, we
can speed up the pattern generation process. How-
ever, none of these modifications affect the accu-
racy of the proposed semantic similarity measure
because the modified version of the prefixspan al-
gorithm still generates the exact set of patterns that
we would obtain if we used the original prefixspan
algorithm (i.e. without pruning) and subsequently
remove patterns that violate the above mentioned
constraints. For example, some patterns extracted
form the snippet shown in Figure 1 are: X, a large
Y, X a flightless Y, and X, large Y lives.
3.2 Clustering Lexical Patterns
A semantic relation can be expressed using more
than one pattern. By grouping the semantically
related patterns, we can both reduce the model
complexity in Equation 2, and consider the depen-
dence among semantic relations in Equation 3. We
use the distributional hypothesis (Harris, 1954) to
find semantically related lexical patterns. The dis-
tributional hypothesis states that words that occur
in the same context have similar meanings. If two
lexical patterns are similarly distributed over a set
of word pairs, then from the distributional hypoth-
esis it follows that the two patterns must be similar.
We represent a pattern p by a vector p in which
807
the i-th element is the frequency f(a
i
, b
i
, p) of p in
a word pair (a
i
, b
i
). Given a set P of patterns and
a similarity threshold ?, Algorithm 1 returns clus-
ters of similar patterns. First, the function SORT
sorts the patterns in the descending order of their
total occurrences in all word pairs. The total oc-
currences of a pattern p is defined as ?(p), and is
given by,
?(p) =
?
(a,b)?W
f(a, b, p). (5)
Here, W is the set of word pairs. Then the outer
for-loop (starting at line 3), repeatedly takes a pat-
tern p
i
from the ordered set P , and in the inner for-
loop (starting at line 6), finds the cluster, c
?
(? C)
that is most similar to p
i
. Similarity between p
i
and the cluster centroid c
j
is computed using co-
sine similarity. The centroid vector c
j
of cluster c
j
is defined as the vector sum of all pattern vectors
for patterns in that cluster (i.e. c
j
=
?
p?c
j
p).
If the maximum similarity exceeds the threshold
?, we append p
i
to c
?
(line 14). Here, the op-
erator ? denotes vector addition. Otherwise, we
form a new cluster {p
i
} and append it to C, the
set of clusters. After all patterns are clustered,
we compute the (i, j) element of the inter-cluster
correlation matrix ? (Equation 3) as the inner-
product between the centroid vectors c
i
and c
j
of
the corresponding clusters i and j. The parame-
ter ? (? [0, 1]) determines the purity of the formed
clusters and is set experimentally in Section 5. Al-
gorithm 1 scales linearly with the number of pat-
terns. Moreover, sorting the patterns by their to-
tal word pair frequency prior to clustering ensures
that the final set of clusters contains the most com-
mon relations in the dataset.
4 Evaluation Procedure
Evaluating a semantic similarity measure is diffi-
cult because the notion of semantic similarity is
subjective. Miller-Charles (1998) dataset has been
frequently used to benchmark semantic similar-
ity measures. Miller-Charles dataset contains 30
word pairs rated by a group of 38 human subjects.
The word pairs are rated on a scale from 0 (no sim-
ilarity) to 4 (perfect synonymy). Because of the
omission of two word pairs in earlier versions of
WordNet, most researchers had used only 28 pairs
for evaluations. The degree of correlation between
the human ratings in the benchmark dataset and
the similarity scores produced by an automatic se-
mantic similarity measure, can be considered as a
Algorithm 1 Sequential pattern clustering algo-
rithm.
Input: patterns P = {p
1
, . . . ,p
n
}, threshold ?
Output: clusters C
1: SORT(P )
2: C ? {}
3: for pattern p
i
? P do
4: max ? ??
5: c
?
? null
6: for cluster c
j
? C do
7: sim ? cosine(p
i
, c
j
)
8: if sim > max then
9: max ? sim
10: c
?
? c
j
11: end if
12: end for
13: if max ? ? then
14: c
?
? c
?
? p
i
15: else
16: C ? C ? {p
i
}
17: end if
18: end for
19: return C
measurement of how well the semantic similarity
measure captures the notion of semantic similar-
ity held by humans. In addition to Miller-Charles
dataset we also evaluate on the WordSimilarity-
353 (Finkelstein et al, 2002) dataset. In con-
trast to Miller-Charles dataset which has only 30
word pairs, WordSimilarity-353 dataset contains
353 word pairs. Each pair has 13-16 human judg-
ments, which were averaged for each pair to pro-
duce a single relatedness score. Following the pre-
vious work, we use both Miller-Charles dataset
and WordSimilarity-353 dataset to evaluate the
proposed semantic similarity measure.
5 Computing Semantic Similarity
To extract lexical patterns that express numer-
ous semantic relations, we first select synonymous
words from WordNet synsets. A synset is a set
of synonymous words assigned for a particular
sense of a word in WordNet. We randomly select
2000 synsets of nouns from WordNet. From each
synset, a pair of synonymous words is selected.
For polysemous nouns, we selected synonyms
from the dominant sense. To perform a fair evalu-
ation, we do not select any words that appear in the
Miller-Charles dataset or the WordSimilarity-353
808
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 1.2
 1.3
 1.4
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Av
era
ge 
Sim
ilar
ity
Clustering Threshold
Figure 2: Average similarity vs. clustering thresh-
old ?
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Clu
ste
r S
par
sity
Clustering Threshold
Figure 3: Sparsity vs. clustering threshold ?
dataset, which are used later for evaluation pur-
poses. As we describe later, the clustering thresh-
old ? is tuned using this set of 2000 word pairs
selected from the WordNet.
We use the YahooBOSS API
2
and download
1000 snippets for each of those word pairs. Ex-
perimentally, we set the values for the parameters
in the pattern extraction algorithm (Section 3.1):
L = 5, g = 2, G = 4, and extract 5, 238, 637
unique patterns. However, only 1, 680, 914 of
those patterns occur more than twice. Low fre-
quency patterns often contain misspellings and are
not suitable for training. Therefore, we selected
patterns that occur at least 10 times in the snip-
pet collection. Moreover, we remove very long
patterns (ca. over 20 characters). The final set
contains 140, 691 unique lexical patterns. The re-
mainder of the experiments described in the paper
use those patterns.
2
http://developer.yahoo.com/search/boss/
We use the clustering Algorithm 1 to cluster the
extracted patterns. The only parameter in Algo-
rithm 1, the clustering threshold ?, is set as fol-
lows. We vary the value of theta ? from 0 to 1,
and use Algorithm 1 to cluster the extracted set
of patterns. We use the resultant set of clusters to
represent a word pair by a feature vector. We com-
pute a feature from each cluster as follows. First,
we assign a weight w
ij
to a pattern p
i
that is in a
cluster c
j
as follows,
w
ij
=
?(p
i
)
?
q?c
j
?(q)
. (6)
Here, ?(q) is the total frequency of a pattern, and it
is given by Equation 5. Because we perform a hard
clustering on patterns, a pattern can belong to only
one cluster (i.e. w
ij
= 0 for p
i
/? c
j
). Finally, we
compute the value of the j-th feature in the feature
vector for word pair (a, b) as follows,
?
p
i
?c
j
w
ij
f(a, b, p
i
). (7)
For each set of clusters, we compute the element
?
ij
of the corresponding inter-cluster correlation
matrix ? by the cosine similarity between the cen-
troid vectors for clusters c
i
and c
j
. The prototype
vector ? in Equation 3 is computed as the vector
sum of individual feature vectors for the synony-
mous word pairs selected from the WordNet as de-
scribed above. We then use Equation 3 to compute
the average of similarity scores for synonymous
word pairs we selected from WordNet.
We select the ? that maximizes the average
similarity score between those synonymous word
pairs. Formally, the optimal value of ?,
?
? is given
by the following Equation,
?
? = argmax
??[0,1]
(
1
|W |
?
(a,b)?W
sim(a, b)
)
. (8)
Here, W is the set of synonymous word pairs
(a, b), |W | is the total number of synonymous
word pairs (i.e. 2000 in our experiments), and
sim(a, b) is given by Equation 3. Because the av-
erages are taken over 2000 word pairs this proce-
dure gives a reliable estimate for ?. Moreover,
this method does not require negative training
instances such as, non-synonymous word pairs,
which are difficult to create manually. Average
similarity scores for various ? values are shown
in Figure 2. From Figure 2, we see that initially
average similarity increases when ? is increased.
809
This is because clustering of semantically related
patterns reduces the sparseness in feature vectors.
Average similarity is stable within a range of ? val-
ues between 0.5 and 0.7. However, increasing ?
beyond 0.7 results in a rapid drop of average sim-
ilarity. To explain this behavior consider Figure
3 where we plot the sparsity of the set of clusters
(i.e. the ratio between singletons to total clusters)
against threshold ?. As seen from Figure 3, high ?
values result in a high percentage of singletons be-
cause only highly similar patterns will form clus-
ters. Consequently, feature vectors for different
word pairs do not have many features in common.
The maximum average similarity score of 1.303 is
obtained with ? = 0.7, corresponding to 17, 015
total clusters out of which 12, 476 are singletons
with exactly one pattern (sparsity = 0.733). For
the remainder of the experiments in this paper we
set ? to this optimal value and use the correspond-
ing set of clusters to compute semantic similarity
by Equation 3. Similarity scores computed us-
ing Equation 3 can be greater than 1 (see Figure
2) because of the terms corresponding to the non-
diagonal elements in ?. We do not normalize the
similarity scores to [0, 1] range in our experiments
because the evaluation metrics we use are insensi-
tive to linear transformations of similarity scores.
6 Experiments
Table 1 compares the proposed method against
Miller-Charles ratings (MC), and previously pro-
posed web-based semantic similarity measures:
Jaccard, Dice, Overlap, PMI (Bollegala et al,
2007), Normalized Google Distance (NGD) (Cili-
brasi and Vitanyi, 2007), Sahami and Heil-
man (SH) (2006), co-occurrence double checking
model (CODC) (Chen et al, 2006), and support
vector machine-based (SVM) approach (Bollegala
et al, 2007). The bottom row of Table 1 shows the
Pearson correlation coefficient of similarity scores
produced by each algorithm with MC. All similar-
ity scores, except for the human-ratings in Miller-
Charles dataset, are normalized to [0, 1] range for
the ease of comparison. It is noteworthy that the
Pearson correlation coefficient is invariant under a
linear transformation. All similarity scores shown
in Table 1 except for the proposed method are
taken from the original published papers.
The highest correlation is reported by the pro-
posed semantic similarity measure. The improve-
ment of the proposed method is statistically sig-
nificant (confidence interval [0.73, 0.93]) against
all the similarity measures compared in Table 1
except against the SVM approach. From Table 1
we see that measures that use contextual informa-
tion from snippets (e.g. SH, CODC, SVM, and
proposed) outperform the ones that use only co-
occurrence statistics (e.g. Jaccard, overlap, Dice,
PMI, and NGD) such as page-counts. This is be-
cause similarity measures that use contextual in-
formation are better equipped to compute the sim-
ilarity between polysemous words. Although both
SVM and proposed methods use lexical patterns,
unlike the proposed method, the SVM method
does not consider the relatedness between pat-
terns. The superior performance of the proposed
method is attributable to its consideration of relat-
edness of patterns.
Table 2 summarizes the previously proposed
WordNet-based semantic similarity measures. De-
spite the fact that the proposed method does not
use manually compiled resources such as Word-
Net for computing similarity, its performance is
comparable to similarity measures that use Word-
Net. We believe that the proposed method will
be useful to compute the semantic similarity be-
tween named-entities for which manually created
resources are either incomplete or do not exist.
We evaluate the proposed method using the
WordSimilarity-353 dataset. Experimental re-
sults are presented in Table 3. Following pre-
vious work, we use Spearman rank correlation
coefficient, which does not require ratings to be
linearly dependent, for the evaluations on this
dataset. Likewise with the Miller-Charles ratings,
we measure the correlation between the similar-
ity scores produced by the proposed method for
word pairs in the WordSimilarity-353 dataset and
the human ratings. A higher Spearman correla-
tion coefficient (value=0.504, confidence interval
[0.422, 0.578]) indicates a better agreement with
the human notion of semantic similarity. From Ta-
ble 3 we can see that the proposed method outper-
forms a wide variety of semantic similarity mea-
sures developed using numerous resources includ-
ing lexical resources such as WordNet and knowl-
edge sources such as Wikipedia (i.e. WikiRe-
late!). In contrast to the Miller-Charles dataset
which only contains common English words se-
lected from the WordNet, the WordSimilarity-353
dataset contains word pairs where one or both
words are named entities (e.g. (Maradona, foot-
810
Table 1: Semantic similarity scores on Miller-Charles dataset
Word Pair MC Jaccrad Dice Overlap PMI NGD SH CODC SVM Proposed
automobile-car 3.920 0.650 0.664 0.831 0.427 0.466 0.225 0.008 0.980 0.918
journey-voyage 3.840 0.408 0.424 0.164 0.468 0.556 0.121 0.005 0.996 1.000
gem-jewel 3.840 0.287 0.300 0.075 0.688 0.566 0.052 0.012 0.686 0.817
boy-lad 3.760 0.177 0.186 0.593 0.632 0.456 0.109 0.000 0.974 0.958
coast-shore 3.700 0.783 0.794 0.510 0.561 0.603 0.089 0.006 0.945 0.975
asylum-madhouse 3.610 0.013 0.014 0.082 0.813 0.782 0.052 0.000 0.773 0.794
magician-wizard 3.500 0.287 0.301 0.370 0.863 0.572 0.057 0.008 1.000 0.997
midday-noon 3.420 0.096 0.101 0.116 0.586 0.687 0.069 0.010 0.819 0.987
furnace-stove 3.110 0.395 0.410 0.099 1.000 0.638 0.074 0.011 0.889 0.878
food-fruit 3.080 0.751 0.763 1.000 0.449 0.616 0.045 0.004 0.998 0.940
bird-cock 3.050 0.143 0.151 0.144 0.428 0.562 0.018 0.006 0.593 0.867
bird-crane 2.970 0.227 0.238 0.209 0.516 0.563 0.055 0.000 0.879 0.846
implement-tool 2.950 1.000 1.000 0.507 0.297 0.750 0.098 0.005 0.684 0.496
brother-monk 2.820 0.253 0.265 0.326 0.623 0.495 0.064 0.007 0.377 0.265
crane-implement 1.680 0.061 0.065 0.100 0.194 0.559 0.039 0.000 0.133 0.056
brother-lad 1.660 0.179 0.189 0.356 0.645 0.505 0.058 0.005 0.344 0.132
car-journey 1.160 0.438 0.454 0.365 0.205 0.410 0.047 0.004 0.286 0.165
monk-oracle 1.100 0.004 0.005 0.002 0.000 0.579 0.015 0.000 0.328 0.798
food-rooster 0.890 0.001 0.001 0.412 0.207 0.568 0.022 0.000 0.060 0.018
coast-hill 0.870 0.963 0.965 0.263 0.350 0.669 0.070 0.000 0.874 0.356
forest-graveyard 0.840 0.057 0.061 0.230 0.495 0.612 0.006 0.000 0.547 0.442
monk-slave 0.550 0.172 0.181 0.047 0.611 0.698 0.026 0.000 0.375 0.243
coast-forest 0.420 0.861 0.869 0.295 0.417 0.545 0.060 0.000 0.405 0.150
lad-wizard 0.420 0.062 0.065 0.050 0.426 0.657 0.038 0.000 0.220 0.231
cord-smile 0.130 0.092 0.097 0.015 0.208 0.460 0.025 0.000 0 0.006
glass-magician 0.110 0.107 0.113 0.396 0.598 0.488 0.037 0.000 0.180 0.050
rooster-voyage 0.080 0.000 0.000 0.000 0.228 0.487 0.049 0.000 0.017 0.052
noon-string 0.080 0.116 0.123 0.040 0.102 0.488 0.024 0.000 0.018 0.000
Correlation - 0.260 0.267 0.382 0.549 0.205 0.580 0.694 0.834 0.867
Table 2: Comparison with WordNet-based simi-
larity measures.
Method Correlation
Edge-counting 0.664
Jiang & Conrath (1998) 0.848
Lin (1998a) 0.822
Resnik (1995) 0.745
Li et al (2003) 0.891
ball) and (Jerusalem, Israel)). Because the pro-
posed method use snippets retrieved from a web
search engine, it is capable of extracting expres-
sive lexical patterns that can explicitly state the re-
lationship between two entities.
If we must compare n objects using a feature
model of similarity, then we only need to define
features for each of those n objects. However, in
the proposed relational model we must define re-
lations between all pairs of objects. In the case
where all n objects are different, this requires us to
define relations for n(n?1)/2 object pairs. Defin-
ing relations for all pairs can be computationally
costly for large n values. Efficiently comparing n
objects using a relational model is an interesting
future research direction of the current work.
Table 3: Results on WordSimilarity-353 dataset.
Method Correlation
WordNet Edges (Jarmasz, 1993) 0.27
Hirst & St-Onge (1997) 0.34
Jiang & Conrath (1998) 0.34
WikiRelate! (Strube and Ponzetto, 2006) 0.19-0.48
Leacock & Chodrow (1998) 0.36
Lin (1998b) 0.36
Resnik (1995) 0.37
Proposed 0.504
7 Conclusion
We proposed a relational model to measure the
semantic similarity between two words. First, to
represent the numerous semantic relations that ex-
ist between two words, we extract lexical patterns
from snippets retrieved from a web search engine.
Second, we cluster the extracted patterns to iden-
tify the semantically related patterns. Third, us-
ing the pattern clusters we define a feature vector
to represent two words and compute the semantic
similarity by taking into account the inter-cluster
correlation. The proposed method outperformed
all existing web-based semantic similarity mea-
sures on two benchmark datasets.
811
References
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proc. of ACL?99, pages 57?
64.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proc. of WWW?07, pages 757?
766.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proc. of the COLING/ACL ?06, pages 1009?1016.
R.L. Cilibrasi and P.M.B. Vitanyi. 2007. The google
similarity distance. IEEE Transactions on Knowl-
edge and Data Engineering, 19(3):370?383.
J. Curran. 2002. Ensemble menthods for automatic
thesaurus extraction. In Proc. of EMNLP.
B. Falkenhainer, K.D. Forbus, and D. Gentner. 1989.
Structure mapping engine: Algorithm and examples.
Artificial Intelligence, 41:1?63.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
TOIS, 20:116?131.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In Proc. of IJCAI?07, pages
1606?1611.
R. L. Goldstone. 1994. The role of similarity in cat-
egorization: providing a groundwork. Cognition,
52:125?157.
U. Hahn, N. Chater, and L. B. Richardson. 2003. Sim-
ilarity as transformation. Cognition, 87:1?32.
Z. Harris. 1954. Distributional structure. Word,
10:146?162.
M.A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of 14th
COLING, pages 539?545.
G. Hirst and D. St-Onge. 1997. Lexical chains as rep-
resentations of context for the detection and correc-
tion of malapropisms.
M. Jarmasz. 1993. Roget?s thesaurus as a lexical re-
source for natural language processing. Master?s
thesis, University of Ottawa.
J.J. Jiang and D.W. Conrath. 1998. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of ROCLING?98.
C. L. Krumhansl. 1978. Concerning the applicability
of geometric models to similarity data: The inter-
relationship between similarity and spatial density.
Psychological Review, 85:445?463.
C. Leacock and M. Chodorow. 1998. Combining Lo-
cal Context and WordNet Similarity for Word Sense
Identification. MIT.
M. Li, X. Chen, X. Li, B. Ma, and P.M.B. Vitanyi.
2004. The similarity metric. IEEE Transactions on
Information Theory, 50(12):3250?3264.
D. Lin. 1998a. Automatic retreival and clustering of
similar words. In Proc. of the 17th COLING, pages
768?774.
D. Lin. 1998b. An information-theoretic definition of
similarity. In Proc. of the 15th ICML, pages 296?
304.
G. Miller and W. Charles. 1998. Contextual corre-
lates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
J. Pei, J. Han, B. Mortazavi-Asi, J. Wang, H. Pinto,
Q. Chen, U. Dayal, and M. Hsu. 2004. Mining se-
quential patterns by pattern-growth: the prefixspan
approach. IEEE Transactions on Knowledge and
Data Engineering, 16(11):1424?1440.
R. Rada, H. Mili, E. Bichnell, and M. Blettner. 1989.
Development and application of a metric on seman-
tic nets. IEEE Transactions on Systems, Man and
Cybernetics, 9(1):17?30.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of
IJCAI?95.
M. Sahami and T. Heilman. 2006. A web-based kernel
function for measuring the similarity of short text
snippets. In Proc. of WWW?06.
V. Schickel-Zuber and B. Faltings. 2007. Oss: A se-
mantic similarity function based on hierarchical on-
tologies. In Proc. of IJCAI?07, pages 551?556.
M. Strube and S. P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedness using wikipedia. In
Proc. of AAAI? 06.
J. B. Tenenbaum. 1999. Bayesian modeling of human
concept learning. In NIPS?99.
A. Tversky. 1977. Features of similarity. Psychologi-
cal Review, 84:327?652.
D. McLean Y. Li, Zuhair A. Bandar. 2003. An ap-
proch for measuring semantic similarity between
words using multiple information sources. IEEE
Transactions on Knowledge and Data Engineering,
15(4):871?882.
812
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1021?1029,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Relation Extraction by Mining Wikipedia Texts Using
Information from the Web
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu Yang and Mitsuru Ishizuka
The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
yulan@mi.ci.i.u-tokyo.ac.jp
okazaki@is.s.u-tokyo.ac.jp
matsuo@biz-model.t.utokyo.ac.jp
yangzl@tkl.iis.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp
Abstract
This paper presents an unsupervised rela-
tion extraction method for discovering and
enhancing relations in which a specified
concept in Wikipedia participates. Using
respective characteristics of Wikipedia ar-
ticles and Web corpus, we develop a clus-
tering approach based on combinations of
patterns: dependency patterns from depen-
dency analysis of texts in Wikipedia, and
surface patterns generated from highly re-
dundant information related to the Web.
Evaluations of the proposed approach on
two different domains demonstrate the su-
periority of the pattern combination over
existing approaches. Fundamentally, our
method demonstrates how deep linguistic
patterns contribute complementarily with
Web surface patterns to the generation of
various relations.
1 Introduction
Machine learning approaches for relation extrac-
tion tasks require substantial human effort, partic-
ularly when applied to the broad range of docu-
ments, entities, and relations existing on the Web.
Even with semi-supervised approaches, which use
a large unlabeled corpus, manual construction of a
small set of seeds known as true instances of the
target entity or relation is susceptible to arbitrary
human decisions. Consequently, a need exists for
development of semantic information-retrieval al-
gorithms that can operate in a manner that is as
unsupervised as possible.
Currently, the leading methods in unsupervised
information extraction collect redundancy infor-
mation from a local corpus or use the Web as a
corpus (Pantel and Pennacchiotti, 2006); (Banko
et al, 2007); (Bollegala et al, 2007): (Fan et
al., 2008); (Davidov and Rappoport, 2008). The
standard process is to scan or search the cor-
pus to collect co-occurrences of word pairs with
strings between them, and then to calculate term
co-occurrence or generate surface patterns. The
method is used widely. However, even when pat-
terns are generated from well-written texts, fre-
quent pattern mining is non-trivial because the
number of unique patterns is loose, but many pat-
terns are non-discriminative and correlated. A
salient challenge and research interest for frequent
pattern mining is abstraction away from different
surface realizations of semantic relations to dis-
cover discriminative patterns efficiently.
Linguistic analysis is another effective tech-
nology for semantic relation extraction, as de-
scribed in many reports such as (Kambhatla,
2004); (Bunescu and Mooney, 2005); (Harabagiu
et al, 2005); (Nguyen et al, 2007). Currently, lin-
guistic approaches for semantic relation extraction
are mostly supervised, relying on pre-specification
of the desired relation or initial seed words or pat-
terns from hand-coding. The common process is
to generate linguistic features based on analyses of
the syntactic features, dependency, or shallow se-
mantic structure of text. Then the system is trained
to identify entity pairs that assume a relation and
to classify them into pre-defined relations. The ad-
vantage of these methods is that they use linguistic
technologies to learn semantic information from
different surface expressions.
As described herein, we consider integrating
linguistic analysis with Web frequency informa-
tion to improve the performance of unsupervised
relation extraction. As (Banko et al, 2007)
reported, ?deep? linguistic technology presents
problems when applied to heterogeneous text on
the Web. Therefore, we do not parse informa-
tion from the Web corpus, but from well written
texts. Particularly, we specifically examine unsu-
pervised relation extraction from existing texts of
Wikipedia articles. Wikipedia resources of a fun-
1021
damental type are of concepts (e.g., represented
by Wikipedia articles as a special case) and their
mutual relations. We propose our method, which
groups concept pairs into several clusters based on
the similarity of their contexts. Contexts are col-
lected as patterns of two kinds: dependency pat-
terns from dependency analysis of sentences in
Wikipedia, and surface patterns generated from
highly redundant information from the Web.
The main contributions of this paper are as fol-
lows:
? Using characteristics of Wikipedia articles
and the Web corpus respectively, our study
yields an example of bridging the gap sep-
arating ?deep? linguistic technology and re-
dundant Web information for Information
Extraction tasks.
? Our experimental results reveal that relations
are extractable with good precision using
linguistic patterns, whereas surface patterns
from Web frequency information contribute
greatly to the coverage of relation extraction.
? The combination of these patterns produces
a clustering method to achieve high pre-
cision for different Information Extraction
applications, especially for bootstrapping a
high-recall semi-supervised relation extrac-
tion system.
2 Related Work
(Hasegawa et al, 2004) introduced a method for
discovering a relation by clustering pairs of co-
occurring entities represented as vectors of con-
text features. They used a simple representation
of contexts; the features were words in sentences
between the entities of the candidate pairs.
(Turney, 2006) presented an unsupervised algo-
rithm for mining the Web for patterns expressing
implicit semantic relations. Given a word pair, the
output list of lexicon-syntactic patterns was ranked
by pertinence, which showed how well each pat-
tern expresses the relations between word pairs.
(Davidov et al, 2007) proposed a method for
unsupervised discovery of concept specific rela-
tions, requiring initial word seeds. That method
used pattern clusters to define general relations,
specific to a given concept. (Davidov and Rap-
poport, 2008) presented an approach to discover
and represent general relations present in an arbi-
trary corpus. That approach incorporated a fully
unsupervised algorithm for pattern cluster discov-
ery, which searches, clusters, and merges high-
frequency patterns around randomly selected con-
cepts.
The field of Unsupervised Relation Identifica-
tion (URI)?the task of automatically discover-
ing interesting relations between entities in large
text corpora?was introduced by (Hasegawa et
al., 2004). Relations are discovered by cluster-
ing pairs of co-occurring entities represented as
vectors of context features. (Rosenfeld and Feld-
man, 2006) showed that the clusters discovered by
URI are useful for seeding a semi-supervised rela-
tion extraction system. To compare different clus-
tering algorithms, feature extraction and selection
method, (Rosenfeld and Feldman, 2007) presented
a URI system that used surface patterns of two
kinds: patterns that test two entities together and
patterns that test either of two entities.
In this paper, we propose an unsupervised rela-
tion extraction method that combines patterns of
two types: surface patterns and dependency pat-
terns. Surface patterns are generated from the Web
corpus to provide redundancy information for re-
lation extraction. In addition, to obtain seman-
tic information for concept pairs, we generate de-
pendency patterns to abstract away from different
surface realizations of semantic relations. Depen-
dency patterns are expected to be more accurate
and less spam-prone than surface patterns from the
Web corpus. Surface patterns from redundancy
Web information are expected to address the data
sparseness problem. Wikipedia is currently widely
used information extraction as a local corpus; the
Web is used as a global corpus.
3 Characteristics of Wikipedia articles
Wikipedia, unlike the whole Web corpus, has
several characteristics that markedly facilitate in-
formation extraction. First, as an earlier report
(Giles, 2005) explained, Wikipedia articles are
much cleaner than typical Web pages. Because
the quality is not so different from standard writ-
ten English, we can use ?deep? linguistic tech-
nologies, such as syntactic or dependency parsing.
Secondly, Wikipedia articles are heavily cross-
linked, in a manner resembling cross-linking of
the Web pages. (Gabrilovich and Markovitch,
2006) assumed that these links encode numerous
interesting relations among concepts, and that they
provide an important source of information in ad-
1022
dition to the article texts.
To establish the background for this paper, we
start by defining the problem under consideration:
relation extraction from Wikipedia. We use the en-
cyclopedic nature of the corpus by specifically ex-
amining the relation extraction between the enti-
tled concept (ec) and a related concept (rc), which
are described in anchor text in this article. A com-
mon assumption is that, when investigating the se-
mantics in articles such as those in Wikipedia (e.g.
semantic Wikipedia (Volkel et al, 2006)), key in-
formation related to a concept described on a page
p lies within the set of links l(p) on that page; par-
ticularly, it is likely that a salient semantic relation
r exists between p and a related page p? ? l(p).
Given the scenario we described along with
earlier related works, the challenges we face are
these: 1) enumerating all potential relation types
of interest for extraction is highly problematic for
corpora as large and varied as Wikipedia; 2) train-
ing data or seed data are difficult to label. Consid-
ering (Davidov and Rappoport, 2008), which de-
scribes work to get the target word and relation
cluster given a single (?hook?) word, their method
depends mainly on frequency information from
the Web to obtain a target and clusters. Attempt-
ing to improve the performance, our solution for
these challenges is to combine frequency informa-
tion from the Web and the ?high quality? charac-
teristic of Wikipedia text.
4 Pattern Combination Method for
Relation Extraction
With the scene and challenges stated, we propose a
solution in the following way. The intuitive idea is
that we integrate linguistic technologies on high-
quality text in Wikipedia and Web mining tech-
nologies on a large-scale Web corpus. In this sec-
tion, we first provide an overview of our method
along with the function of the main modules. Sub-
sequently, we explain each module in the method
in detail.
4.1 Overview of the Method
Given a set of Wikipedia articles as input, our
method outputs a list of concept pairs for each ar-
ticle with a relation label assigned to each concept
pair. Briefly, the proposed approach has four main
modules, as depicted in Fig. 1.
? Text Preprocessor and Concept Pair Col-
lector preprocesses Wikipedia articles to
Wikipedia articles
Preprocessor
Concept pair collection
Sentence filtering
Web context collector
Web Context
T
i
= t1, t2?tn
P
i
=  p1,p2?pn
Dependency 
pattern Extractor
n1i,?n1j
?
ni2i, ..n2j
ni,?nj
?
surface clustering
depend clustering
Relation list
Output: 
relations for each article
input:
Eric Emerson Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
Eric Emers  Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
Eric Emers  Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
...
...
?
?
?
?
...
...
?
?
?
?
...
...
?
?
?
?
Tyco becoming
joined
comp:
CEO
obj: cc:
joined
obj:subj:
joined
obj: cc:
Clustering approach
Figure 1: Framework of the proposed approach
split text and filter sentences. It outputs con-
cept pairs, each of which has an accompany-
ing sentence.
? Web Context Collector collects context in-
formation from the Web and generates ranked
relational terms and surface patterns for each
concept pair.
? Dependency Pattern Extractor generates
dependency patterns for each concept pair
from corresponding sentences in Wikipedia
articles.
? Clustering Algorithm clusters concept pairs
based on their context. It consists of the two
sub-modules described below.
? Depend Clustering, which merges con-
cept pairs using dependency patterns
alone, aiming at obtaining clusters of
concept pairs with good precision;
? Surface Clustering, which clusters
concept pairs using surface patterns
based on the resultant clusters of depend
clustering. The aim is to merge more
concept pairs into existing clusters with
surface patterns to improve the coverage
of clusters.
1023
4.2 Text Preprocessor and Concept Pair
Collector
This module pre-processes Wikipedia article texts
to collect concept pairs and corresponding sen-
tences. Given a concept described in a Wikipedia
article, our idea of preprocessing executes initial
consideration of all anchor-text concepts linking
to other Wikipedia articles in the article as related
concepts that might share a semantic relation with
the entitled concept. The link structure, more par-
ticularly, the structure of outgoing links, provides
a simple mechanism for identifying relevant arti-
cles. We split text into sentences and select sen-
tences containing one reference of an entitled con-
cept and one of the linked texts for the dependency
pattern extractor module.
4.3 Web Context Collector
Querying a concept pair using a search engine
(Google), we characterize the semantic relation
between the pair by leveraging the vast size of the
Web. Our hypothesis is that there exist some key
terms and patterns that provide clues to the rela-
tions between pairs. From the snippets retrieved
by the search engine, we extract relational infor-
mation of two kinds: ranked relational terms as
keywords and surface patterns. Here surface pat-
terns are generated with support of ranked rela-
tional terms.
4.3.1 Relational Term Ranking
To collect relational terms as indicators for each
concept pair, we look for verbs and nouns from
qualified sentences in the snippets instead of sim-
ply finding verbs. Using only verbs as relational
terms might engender the loss of various important
relations, e.g. noun relations ?CEO?, ?founder?
between a person and a company. Therefore, for
each concept pair, a list of relational terms is col-
lected. Then all the collected terms of all concept
pairs are combined and ranked using an entropy-
based algorithm which is described in (Chen et al,
2005). With their algorithm, the importance of
terms can be assessed using the entropy criterion,
which is based on the assumption that a term is ir-
relevant if its presence obscures the separability of
the dataset. After the ranking, we obtain a global
ranked list of relational terms Tall for the whole
dataset (all the concept pairs). For each concept
pair, a local list of relational terms Tcp is sorted ac-
cording to the terms? order in Tall. Then from the
relational term list Tcp, a keyword tcp is selected
Table 1: Surface patterns for a concept pair
Pattern Pattern
ec ceo rc rc found ec
ceo rc found ec rc succeed as ceo of ec
rc be ceo of ec ec ceo of rc
ec assign rc as ceo ec found by ceo rc
ceo of ec rc ec found in by rc
for each concept pair cp as the first term appearing
in the term list Tcp. Keyword tcp will be used to
initialize the clustering algorithm in Section 4.5.1.
4.3.2 Surface Pattern Generation
Because simply taking the entire string between
two concept words captures an excess of extra-
neous and incoherent information, we use Tcp of
each concept pair as a key for surface pattern gen-
eration. We classified words into Content Words
(CWs) and Functional Words (FWs). From each
snippet sentence, the entitled concept, related con-
cept, or the keyword kcp is considered to be a Con-
tent Word (CW). Our idea of obtaining FWs is to
look for verbs, nouns, prepositions, and coordinat-
ing conjunctions that can help make explicit the
hidden relations between the target nouns.
Surface patterns have the following general
form.
[CW1] Infix1 [CW2] Infix2 [CW3] (1)
Therein, Infix1 and Infix2 respectively con-
tain only and any number of FWs. A pattern ex-
ample is ?ec assign rc as ceo (keyword)?. All gen-
erated patterns are sorted by their frequency, and
all occurrences of the entitled concept and related
concept are replaced with ?ec? and ?rc?, respec-
tively for pattern matching of different concept
pairs.
Table 1 presents examples of surface patterns
for a sample concept pair. Pattern windows are
bounded by CWs to obtain patterns more precisely
because 1) if we use only the string between two
concepts, it may not contain some important re-
lational information, such as ?ceo ec resign rc?
in Table 1; 2) if we generate patterns by setting
a windows surrounding two concepts, the number
of unique patterns is often exponential.
4.4 Dependency Pattern Extractor
In this section, we describe how to obtain depen-
dency patterns for relation clustering. After pre-
processing, selected sentences that contain at least
1024
one mention of an entitled concept or related con-
cept are parsed into dependency structures. We de-
fine dependency patterns as sub-paths of the short-
est dependency path between a concept pair for
two reasons. One is that the shortest path de-
pendency kernels outperform dependency tree ker-
nels by offering a highly condensed representation
of the information needed to assess their relation
(Bunescu and Mooney, 2005). The other reason
is that embedded structures of the linguistic repre-
sentation are important for obtaining good cover-
age of the pattern acquisition, as explained in (Cu-
lotta and Sorensen, 2005); (Zhang et al, 2006).
The process of inducing dependency patterns has
two steps.
1. Shortest dependency path inducement. From
the original dependency tree structure by parsing
the selected sentence for each concept pair, we
first induce the shortest dependency path with the
entitled concept and related concept.
2. Dependency pattern generation. We use
a frequent tree-mining algorithm (Zaki, 2002) to
generate sub-paths as dependency patterns from
the shortest dependency path for relation cluster-
ing.
4.5 Clustering Algorithm for Relation
Extraction
In this subsection, we present a clustering algo-
rithm that merges concept pairs based on depen-
dency patterns and surface patterns. The algorithm
is based on k-means clustering for relation cluster-
ing.
The dependency pattern has the properties of
being more accurate, but the Web context has the
advantage of containing much more redundant in-
formation than Wikipedia. Our idea of concept
pair clustering is a two-step clustering process:
first it clusters concept pairs into clusters with
good precision using dependency patterns; then it
improves the coverage of the clusters using surface
patterns.
4.5.1 Initial Centroid Selection and Distance
Function Definition
The standard k-means algorithm is affected by
the choice of seeds and the number of clusters
k. However, as we claimed in the Introduc-
tion section, because we aim to extract relations
from Wikipedia articles in an unsupervised man-
ner, cluster number k is unknown and no good
centroids can be predicted. As described in this
paper, we select centroids based on the keyword
tcp of each concept pair.
First of all, all concept pairs are grouped by
their keywords tcp. Let G = {G1, G2, ...Gn}
be the resultant groups, where each Gi =
{cpi1, cpi2, ...} identify a group of concept pairs
sharing the same keyword tcp (such as ?CEO?).
We rank all the groups by their number of concept
pairs and then choose the top k groups. Then a
centroid ci is selected for each group Gi by Eq. 2.
ci = argmaxcp?Gi |{cpij |(dis1(cpij , cp)+
? ? dis2(cpij , cp)) <= Dz, 1 ? j ? |Gi|}| (2)
We assume a centroid for each group to be the
concept pair which has the most other concept
pairs in the same group that have distance less
than Dz with it. Also, Dz is a threshold to avoid
noisy concept pairs: we assign it 1/3. To balance
the contribution between dependency patterns and
surface patterns, ? is used. The distance function
to calculate the distance between dependency pat-
tern sets DPi, DPj of two concept pairs cpi and
cpj is dis1. The distance is decided by the number
of overlapped dependency patterns with Eq. 3.
dis1(cpi, cpj) = 1? |DPi ?DPj |?(|DPi| ? |DPj |)
(3)
Actually, dis2 is the distance function to calcu-
late distance between two surface pattern sets of
two concept pairs. To compute the distance over
surface patterns, we implement the distance func-
tion dis2(cpi, cpj) in Fig. 2.
Algorithm 1: distance function dis2(cpi, cpj)
Input: SP1 = {sp11, ..., sp1m}(surface patterns of
cpi)
SP2 = {sp21, ..., sp2n} (surface patterns of cpj)
Output: dis (distance between SP1 and SP2)
define a m? n distance matrix A:
{Aij = LD(sp1i,sp2j)Max(|sp1i|,|sp2j |) , 1?i?m; 1?j?n};
dis ? 0
for min(m,n) times do
(x, y) ? argmin0<i<m;0<j<nAij ;
dis ? dis + Axy/min(m,n);
Ax? ? 1; A?y ? 1;
return dis
Figure 2: Distance function over surface patterns
As shown in Fig. 2, the distance algorithm per-
forms as: firstly it defines a m?n distance matrix
A, then repeatedly selects two nearest sequences
and sums up their distances. While computing
1025
dis2, we use the Levenshtein distance LD to mea-
sure the difference of two surface patterns. The
Levenshtein distance is a metric for measuring the
amount of difference between two sequences (i.e.,
the so-called edit distance). Each generated sur-
face pattern is a sequence of words. The distance
of two surface patterns is defined as the fraction of
the LD value to the length of the longer sequence.
For estimating the number of clusters k, we ap-
ply the stability-based criteria from (Chen et al,
2005) to decide the number of optimal clusters k
automatically.
4.5.2 Concept Pair Clustering with
Dependency Patterns
Given the initial seed concept pairs and cluster
number k, this stage merges concept pairs over de-
pendency patterns into k clusters. Each concept
pair cpi has a set of dependency patterns DPi. We
calculate distances between two pairs cpi and cpj
using above the function dis1(cpi, cpj). The clus-
tering algorithm is portrayed in Fig. 3. The pro-
cess of depend clustering is to assign each concept
pair to the cluster with the closest centroid and
then recomputing each centroid based on the cur-
rent members of its cluster. As shown in Figure 3,
this is done iteratively by repeating both two steps
until a stopping criterion is met. We apply the ter-
mination condition as: centroids do not change be-
tween iterations.
Algorithm 2: Depend Clustering
Input: I = {cp1, ..., cpn}(all concept pairs)
C = {c1, ..., ck} (k initial centroids)
Output: Md : I ? C (cluster membership)
Ir (rest of concept pairs not clustered)
Cd = {c1, ..., ck} (recomputed centroids)
while stopping criterion has not been met do
for each cpi ? I do
if mins?1..k dis1(cpi, cs) <= Dl then
Md(cpi) ? argmins?1..k dis1(cpi, cs)else
Md(cpi) ? 0
for each j ? {1..k} do
recompute cj as the centroid of
{cpi|mloc(cpi) = j}
Ir ? C0
return C and Cd
Figure 3: Clustering with dependency patterns
Because many concept pairs are scattered and
do not belong to any of the top k clusters, we
filter concept pairs with distance larger than Dl
with the seed concept pairs. Such concept pairs
ST1
ST3 ST4
ST2
Text3: RC was hired as EC?s CEO Text4: EC assign RC as CEO
Text1: the CEO of EC is RC Text2: RC is the CEO of EC
Figure 4: Example showing why surface cluster-
ing is needed
are stored in C0. We named the cluster of concept
pairs Ir which are left to be clustered in the next
step of clustering. After this step, concept pairs
with similar dependency patterns are merged into
same clusters, see Fig. 4 (ST1, ST2).
4.5.3 Concept Pair Clustering with Surface
Patterns
A salient difficulty posed by dependency pattern
clustering is that concept pairs of the same se-
mantic relation cannot be merged if they are ex-
pressed in different dependency structures. Fig-
ure 4 presents an example demonstrating why we
perform surface pattern clustering. As depicted
in Fig. 4, ST1, ST2, ST3, and ST4 are depen-
dency structures for four concept pairs that should
be classified as the same relation ?CEO?. However
ST3 and ST4 can not be merged with ST1 and
ST2 using the dependency patterns because their
dependency structures are too diverse to share suf-
ficient dependency patterns.
In this step, we use surface patterns to merge
more concept pairs for each cluster to improve the
coverage. Figure 5 portrays the algorithm. We
assume that each concept pair has a set of sur-
face patterns from the Web context collector mod-
ule. As shown in Figure 5, surface clustering is
done iteratively by repeating two steps until a stop-
ping criterion is met: using the distance function
dis2 explained in the preceding section, assign
each concept pair to the cluster with the closest
centroid and recomputing each centroid based on
the current members of its cluster. We apply the
same termination condition as depend clustering.
1026
Additionally, we filter concept pairs with distance
greater than Dg with the centroid concept pairs.
Algorithm 3: Surface Clustering
Input: Ir (rest of concept pairs)
Cd = {c1, ..., ck} (initial centroids)
Output: Ms : Ir ? C (cluster membership)
Cs = {c1, ..., ck} (final centroids)
while stopping criterion has not been met do
for each cpi ? Ir do
if mins?1..k dis2(cpi, cs) <= Dg then
Ms(cpi) ? argmins?1..k dis2(cpi, cs)else
Ms(cpi) ? 0
for each j ? 1..k do
recompute cj as the centroid of cluster
{cpi|Md(cpi) = j ?Ms(cpi) = j}
return clusters C
Figure 5: Clustering with surface patterns
Finally we have k clusters of concept pairs, each
of which has a centroid concept pair. To attach
a single relation label to each cluster, we use the
centroid concept pair.
5 Experiments
We apply our algorithm to two categories in
Wikipedia: ?American chief executives? and
?Companies?. Both categories are well defined
and closed. We conduct experiments for extract-
ing various relations and for measuring the quality
of these relations in terms of precision and cover-
age. We use coverage as an evaluation instead of
using recall as a measure. The coverage is used to
evaluate all correctly extracted concept pairs. It is
defined as the fraction of all the correctly extracted
concept pairs to the whole set of concept pairs. To
balance between precision and coverage of clus-
tering, we integrate two parameters: Dl, Dg.
We downloaded the Wikipedia dump as of De-
cember 3, 2008. The performance of the pro-
posed method is evaluated using different pattern
types: dependency patterns, surface patterns, and
their combination. We compare our method with
(Rosenfeld and Feldman, 2007)?s URI method.
Their algorithm outperformed that presented in the
earlier work using surface features of two kinds for
unsupervised relation extraction: features that test
two entities together and features that test only one
entity each. For comparison, we use a k-means
clustering algorithm using the same cluster num-
ber k.
Table 2: Results for the category: ?American chief
executives?
method Existing method Proposed method
(Rosenfeld et al) (Our method)
Relation # Ins. pre # Ins. pre
(sample)
chairman 434 63.52 547 68.37
(x be chairman of y)
ceo 396 73.74 423 77.54
(x be ceo of y)
bear 138 83.33 276 86.96
(x be bear in y)
attend 225 67.11 313 70.28
(x attend y)
member 14 85.71 175 91.43
(x be member of y)
receive 97 67.97 117 73.53
(x receive y)
graduate 18 83.33 92 88.04
(x graduate from y)
degree 5 80.00 78 82.05
(x obtain y degree)
marry 55 41.67 74 61.25
(x marry y)
earn 23 86.96 51 88.24
(x earn y)
award 23 43.47 46 84.78
(x won y award)
hold 5 80.00 37 72.97
(x hold y degree)
become 35 74.29 37 81.08
(x become y)
director 24 67.35 29 79.31
(x be director of y)
die 18 77.78 19 84.21
(x die in y)
all 1510 68.27 2314 75.63
5.1 Wikipedia Category: ?American chief
executives?
We choose appropriate Dl(concept pair filter in
depend clustering) and Dg(concept pair filter in
surface clustering) in a development set. To bal-
ance precision and coverage, we set 1/3 for both
Dl and Dg.
The 526 articles in this category are used for
evaluation. We obtain 7310 concept pairs from
the articles as our dataset. The top 18 groups are
chosen to obtain the centroid concept pairs. Of
these, 15 binary relations are the clearly identifi-
able relations shown in Table 2, where # Ins. rep-
resents the number of concept pairs clustered us-
ing each method, and pre denotes the precision of
each cluster.
The proposed approach shows higher precision
and better coverage than URI in Table 2. This
result demonstrates that adding dependency pat-
terns from linguistic analysis contributes more to
the precision and coverage of the clustering task
than the sole use of surface patterns.
1027
Table 3: Performance of different pattern types
Pattern type #Instance Precision Coverage
dependency 1127 84.29 13.00%
surface 1510 68.27 14.10%
Combined 2314 75.63 23.94%
Table 4: Results for the category: ?Companies?
Method Existing method Proposed method
(Rosenfeld et al) (Our method)
Relation # Ins. pre # Ins. pre
(sample)
found 82 75.61 163 84.05
(found x in y)
base 82 76.83 122 82.79
(x be base in y)
headquarter 23 86.97 120 89.34
(x be headquarter in y)
service 37 51.35 108 69.44
(x offer y service)
store 113 77.88 88 72.72
(x open store in y)
acquire 59 62.71 70 64.28
(x acquire y)
list 51 64.71 67 70.15
(x list on y)
product 25 76.00 57 77.19
(x produce y)
CEO 37 64.86 39 66.67
(ceo x found y)
buy 53 62.26 37 56.76
(x buy y)
establish 35 82.86 26 80.77
(x be establish in y)
locate 14 50.00 24 75.00
(x be locate in y)
all 685 71.03 1039 76.87
To examine the contribution of dependency pat-
terns, we compare results obtained with patterns
of different kinds. Table 3 shows the precision and
coverage scores. The best precision is achieved by
dependency patterns. The precision is markedly
better than that of surface patterns. However, the
coverage is worse than that by surface patterns. As
we reported, many concept pairs are scattered and
do not belong to any of the top k clusters, the cov-
erage is low.
5.2 Wikipedia Category: ?Companies?
We also evaluate the performance for the ?Com-
panies? category. Instead of using all the arti-
cles, we randomly select 434 articles for evalua-
tion and 4073 concept pairs from the articles form
our dataset for this category. We also set Dl and
Dg to 1/3. Then 28 groups are chosen. For each
group, a centroid concept pair is obtained. Finally,
of 28 clusters, 25 binary relations are clearly iden-
tifiable relations. Table 4 presents some relations.
Table 5: Performance of different pattern types
Pattern type #Instance Precision Coverage
dependency 551 82.58 11.17%
surface 685 71.03 11.95%
Combined 1039 76.87 19.61%
Our clustering algorithms use two filters Dl and
Dg to filter scattering concept pairs. In Table 4, we
present that concept pairs are clustered with good
precision. As in the first experiments, the combi-
nation of dependency patterns and surface patterns
contribute greatly to the precision and coverage.
Table 5 shows that, using dependency patterns,
the precision is the highest (82.58%), although the
coverage is the lowest.
All experimental results support our idea
mainly in two aspects: 1) Dependency analysis
can abstract away from different surface realiza-
tions of text. In addition, embedded structures of
the dependency representation are important for
obtaining a good coverage of the pattern acqui-
sition. Furthermore, the precision is better than
that of the string surface patterns from Web pages
of various kinds. 2) Surface patterns are used to
merge concept pairs with relations represented in
different dependency structures with redundancy
information from the vast size of Web pages. Us-
ing surface patterns, more concept pairs are clus-
tered, and the coverage is improved.
6 Conclusions
To discover a range of semantic relations from
a large corpus, we present an unsupervised rela-
tion extraction method using deep linguistic in-
formation to alleviate surface and noisy surface
patterns generated from a large corpus, and use
Web frequency information to ease the sparse-
ness of linguistic information. We specifically ex-
amine texts from Wikipedia articles. Relations
are gathered in an unsupervised way over pat-
terns of two types: dependency patterns by parsing
sentences in Wikipedia articles using a linguistic
parser, and surface patterns from redundancy in-
formation from the Web corpus using a search en-
gine. We report our experimental results in com-
parison to those of previous works. The results
show that the best performance arises from a com-
bination of dependency patterns and surface pat-
terns.
1028
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead and Oren Etzioni. 2007.
Open information extraction from the Web. In Pro-
ceedings of IJCAI-2007.
Danushka Bollegala, Yutaka Matsuo and Mitsuru
Ishizuka. 2007. Measuring Semantic Similarity be-
tween Words Using Web Search Engines. In Pro-
ceedings of WWW-2007.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of HLT/EMLNP-2005.
Jinxiu Chen, Donghong Ji, Chew Lim Tan and
Zhengyu Niu. 2005. Unsupervised Feature Se-
lection for Relation Extraction. In Proceedings of
IJCNLP-2005.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the ACL-2004.
Dmitry Davidov, Ari Rappoport and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by Web mining. In Proceed-
ings of ACL-2007.
Dmitry Davidov and Ari Rappoport. 2008. Classifi-
cation of Semantic Relationships between Nominals
Using Pattern Clusters. In Proceedings of ACL-
2008.
Wei Fan, Kun Zhang, Hong Cheng, Jing Gao, Xifeng
Yan, Jiawei Han, Philip S. Yu and Olivier Ver-
scheure. 2008. Direct Mining of Discriminative and
Essential Frequent Patterns via Model-based Search
Tree. In Proceedings of KDD-2008.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proceedings of
AAAI-2006.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature 438:900C901.
Sanda Harabagiu, Cosmin Adrian Bejan and Paul
Morarescu. 2005. Shallow semantics for relation
extraction. In Proceedings of IJCAI-2005.
Takaaki Hasegawa, Satoshi Sekine and Ralph Grish-
man. 2004. Discovering Relations among Named
Entities from Large Corpora. In Proceedings of
ACL-2004.
Nanda Kambhatla. 2004. Combining lexical, syntactic
and semantic features with maximum entropy mod-
els. In Proceedings of ACL-2004.
Dat P.T. Nguyen, Yutaka Matsuo and Mitsuru Ishizuka.
2007. Relation extraction from Wikipedia using sub-
tree mining. In Proceedings of AAAI-2007.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceedings
of ACL-2006.
Benjamin Rosenfeld and Ronen Feldman. 2006.
URES: an Unsupervised Web Relation Extraction
System. In Proceedings of COLING/ACL-2006.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for Unsupervised Relation Identification. In
Proceedings of CIKM-2007.
Peter D. Turney. 2006. Expressing implicit seman-
tic relations without supervision. In Proceedings of
ACL-2006.
Max Volkel, Markus Krotzsch, Denny Vrandecic,
Heiko Haller and Rudi Studer. 2006. Semantic
wikipedia. In Proceedings of WWW-2006.
Mohammed J. Zaki. 2002. Efficiently mining frequent
trees in a forest. In Proceedings of SIGKDD-2002.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of ACL-2006.
1029
Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extracting Key Phrases to Disambiguate
Personal Name Queries in Web Search
Danushka Bollegala Yutaka Matsuo ?
Graduate School of Information Science and Technology
The University of Tokyo
7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan
danushka@mi.ci.i.u-tokyo.ac.jp
y.matsuo@aist.go.jp
ishizuka@i.u-tokyo.ac.jp
Mitsuru Ishizuka
Abstract
Assume that you are looking for informa-
tion about a particular person. A search
engine returns many pages for that per-
son?s name. Some of these pages may
be on other people with the same name.
One method to reduce the ambiguity in the
query and filter out the irrelevant pages, is
by adding a phrase that uniquely identi-
fies the person we are interested in from
his/her namesakes. We propose an un-
supervised algorithm that extracts such
phrases from the Web. We represent each
document by a term-entity model and clus-
ter the documents using a contextual sim-
ilarity metric. We evaluate the algorithm
on a dataset of ambiguous names. Our
method outperforms baselines, achieving
over 80% accuracy and significantly re-
duces the ambiguity in a web search task.
1 Introduction
The Internet has grown into a collection of bil-
lions of web pages. Web search engines are im-
portant interfaces to this vast information. We
send simple text queries to search engines and re-
trieve web pages. However, due to the ambigu-
ities in the queries, a search engine may return
a lot of irrelevant pages. In the case of personal
name queries, we may receive web pages for other
people with the same name (namesakes). For ex-
ample, if we search Google 1 for Jim Clark, even
among the top 100 results we find at least eight
different Jim Clarks. The two popular namesakes;
?National Institute of Advanced Industrial Science and
Technology
1www.google.com
Jim Clark the Formula one world champion (46
pages), and Jim Clark the founder of Netscape (26
pages), cover the majority of the pages. What if
we are interested only in the Formula one world
champion and want to filter out the pages for the
other Jim Clarks? One solution is to modify our
query by including a phrase such as Formula one
or racing driver with the name, Jim Clark.
This paper presents an automatic method to ex-
tract such phrases from the Web. We follow a
three-stage approach. In the first stage we rep-
resent each document containing the ambiguous
name by a term-entity model, as described in sec-
tion 5.2. We define a contextual similarity metric
based on snippets returned by a search engine, to
calculate the similarity between term-entity mod-
els. In the second stage, we cluster the documents
using the similarity metric. In the final stage, we
select key phrases from the clusters that uniquely
identify each namesake.
2 Applications
Two tasks that can readily benefit from automat-
ically extracted key phrases to disambiguate per-
sonal names are query suggestion and social net-
work extraction. In query suggestion (Gauch and
Smith, 1991), the search engine returns a set of
phrases to the user alongside with the search re-
sults. The user can then modify the original query
using these phrases to narrow down the search.
Query suggestion helps the users to easily navigate
through the result set. For personal name queries,
the key phrases extracted by our algorithm can be
used as suggestions to reduce the ambiguity and
narrow down the search on a particular namesake.
Social networking services (SNSs) have been
given much attention on the Web recently. As
a kind of online applications, SNSs can be used
17
to register and share personal information among
friends and communities. There have been recent
attempts to extract social networks using the infor-
mation available on the Web 2(Mika, 2004; Mat-
suo et al, 2006). In both Matsuo?s (2006) and
Mika?s (2004) algorithms, each person is repre-
sented by a node in the social network and the
strength of the relationship between two people
is represented by the length of the edge between
the corresponding two nodes. As a measure of the
strength of the relationship between two people A
and B, these algorithms use the number of hits ob-
tained for the query A AND B. However, this ap-
proach fails when A or B has namesakes because
the number of hits in these cases includes the hits
for the namesakes. To overcome this problem, we
could include phrases in the query that uniquely
identify A and B from their namesakes.
3 Related Work
Person name disambiguation can be seen as
a special case of word sense disambiguation
(WSD) (Schutze, 1998; McCarthy et al, 2004)
problem which has been studied extensively in
Natural Language Understanding. However, there
are several fundamental differences between WSD
and person name disambiguation. WSD typically
concentrates on disambiguating between 2-4 pos-
sible meanings of the word, all of which are a
priori known. However, in person name disam-
biguation in Web, the number of different name-
sakes can be much larger and unknown. From a
resource point of view, WSD utilizes sense tagged
dictionaries such as WordNet, whereas no dictio-
nary can provide information regarding different
namesakes for a particular name.
The problem of person name disambiguation
has been addressed in the domain of research pa-
per citations (Han et al, 2005), with various super-
vised methods proposed for its solution. However,
citations have a fixed format compared to free text
on the Web. Fields such as co-authors, title, jour-
nal name, conference name, year of publication
can be easily extracted from a citation and provide
vital information to the disambiguation process.
Research on multi-document person name res-
olution (Bagga and Baldwin, 1998; Mann and
Yarowsky, 2003; Fleischman and Hovy, 2004) fo-
cuses on the related problem of determining if
2http://flink.sematicweb.org/. The system won the 1st
place at the Semantic Web Challenge in ISWC2004.
two instances with the same name and from dif-
ferent documents refer to the same individual.
Bagga and Baldwin (1998) first perform within-
document coreference resolution to form coref-
erence chains for each entity in each document.
They then use the text surrounding each reference
chain to create summaries about each entity in
each document. These summaries are then con-
verted to a bag of words feature vector and are
clustered using standard vector space model of-
ten employed in IR. The use of simplistic bag of
words clustering is an inherently limiting aspect of
their methodology. On the other hand, Mann and
Yarowsky (2003) proposes a richer document rep-
resentation involving automatically extracted fea-
tures. However, their clustering technique can be
basically used only for separating two people with
the same name. Fleischman and Hovy (2004) con-
structs a maximum entropy classifier to learn dis-
tances between documents that are then clustered.
Their method requires a large training set.
Pedersen et al (2005) propose an unsupervised
approach to resolve name ambiguity by represent-
ing the context of an ambiguous name using sec-
ond order context vectors derived using singular
value decomposition (SVD) on a co-occurrence
matrix. They agglomeratively cluster the vec-
tors using cosine similarity. They evaluate their
method only on a conflated dataset of pseudo-
names, which begs the question of how well such
a technique would fair on a more real-world chal-
lenge. Li et al (2005) propose two approaches to
disambiguate entities in a set of documents: a su-
pervisedly trained pairwise classifier and an unsu-
pervised generative model. However, they do not
evaluate the effectiveness of their method in Web
search.
Bekkerman and McCallum (2005) present two
unsupervised methods for finding web pages re-
ferring to a particular person: one based on
link structure and another using Agglomera-
tive/Conglomerative Double Clustering (A/CDC).
Their scenario focuses on simultaneously disam-
biguating an existing social network of people,
who are closely related. Therefore, their method
cannot be applied to disambiguate an individual
whose social network (for example, friends, col-
leagues) is not known. Guha and Grag (2004)
present a re-ranking algorithm to disambiguate
people. The algorithm requires a user to select one
of the returned pages as a starting point. Then,
18
Table 1: Data set for experiments
Collection No of namesakes
person-X 4
Michael Jackson 3
Jim Clark 8
William Cohen 10
through comparing the person descriptions, the al-
gorithm re-ranks the entire search results in such
a way that pages referring to the same person de-
scribed in the user-selected page are ranked higher.
A user needs to browse the documents in order to
find which matches the user?s intended referent,
which puts an extra burden on the user.
None of the above mentioned works attempt to
extract key phrases to disambiguate person name
queries, a contrasting feature in our work.
4 Data Set
We select three ambiguous names (Micheal Jack-
son, William Cohen and Jim Clark) that appear in
previous work in name resolution. For each name
we query Google with the name and download top
100 pages. We manually classify each page ac-
cording to the namesakes discussed in the page.
We ignore pages which we could not decide the
namesake from the content. We also remove pages
with images that do not contain any text. No pages
were found where more than one namesakes of a
name appear. For automated pseudo-name evalua-
tion purposes, we select four names (Bill Clinton,
Bill Gates, Tom Cruise and Tiger Woods) for con-
flation, who we presumed had one vastly predom-
inant sense. We download 100 pages from Google
for each person. We replace the name of the per-
son by ?person-X? in the collection, thereby intro-
ducing ambiguity. The structure of our dataset is
shown in Table 1.
5 Method
5.1 Problem Statement
Given a collection of documents relevant to an am-
biguous name, we assume that each document in
the collection contains exactly one namesake of
the ambiguous name. This is a fair assumption
considering the fact that although namesakes share
a common name, they specializes in different
fields and have different Web appearances. More-
over, the one-to-one association between docu-
ments and people formed by this assumption, let
us model the person name disambiguation prob-
lem as a one of hard-clustering of documents.
The outline of our method is as following;
Given a set of documents representing a group of
people with the same name, we represent each
document in the collection using a Term-Entity
model (section 5.2). We define a contextual sim-
ilarity metric (section 5.4) and then cluster (sec-
tion 5.5) the term-entity models using the contex-
tual similarity between them. Each cluster is con-
sidered to be representing a different namesake.
Finally, key phrases that uniquely identify each
namesake are selected from the clusters. We per-
form experiments at each step of our method to
evaluate its performance.
5.2 Term-Entity Model
The first step toward disambiguating a personal
name is to identify the discriminating features of
one person from another. In this paper we propose
Term-Entity models to represent a person in a doc-
ument.
Definition. A term-entity model T (A), represent-
ing a person A in a document D, is a boolean
expression of n literals a1, a2, . . . , an. Here, a
boolean literal ai is a multi-word term or a named
entity extracted from the document D.
For simplicity, we only consider boolean ex-
pressions that combine the literals through AND
operator.
The reasons for using terms as well as named
entities in our model are two fold. Firstly, there are
multi-word phrases such as secretary of state, rac-
ing car driver which enable us to describe a person
uniquely but not recognized by named entity tag-
gers. Secondly, automatic term extraction (Frantzi
and Ananiadou, 1999) can be done using statistical
methods and does not require extensive linguistic
resources such as named entity dictionaries, which
may not be available for some domains.
5.3 Creating Term-Entity Models
We extract terms and named entities from each
document to build the term-entity model for that
document. For automatic multi-word term ex-
traction, we use the C-value metric proposed by
Frantzi et al (1999). Firstly, the text from which
we need to extract terms is tagged using a part
of speech tagger. Then a linguistic filter and a
stop words list constrain the word sequences that
19
020
40
60
80
100
120 President of the United States
George Bush
pr
es
id
en
tia
l
ge
or
ge
pr
es
id
en
t
ne
ws
bi
og
ra
ph
y
ga
m
es
bu
sh
bu
sh
s
lib
ra
ry
fa
th
er
vic
e
go
ve
rn
m
en
t
pr
es
id
en
ts
sh
al
l
un
ite
d
st
at
es
ex
ec
ut
ive
Figure 1: Distribution of words in snippets for
?George Bush? and ?President of the United
States?
are allowed as genuine multi-word terms. The
linguistic filter contains a predefined set of pat-
terns of nouns, adjectives and prepositions that are
likely to be terms. The sequences of words that re-
main after this initial filtering process (candidate
terms) are evaluated for their termhood (likeliness
of a candidate to be a term) using C-value. C-
value is built using statistical characteristics of the
candidate string, such as, total frequency of oc-
currence of the candidate string in the document,
the frequency of the candidate string as part of
other longer candidate strings, the number of these
longer candidate terms and the length of candidate
string (in number of words). We select the candi-
dates with higher C-values as terms (see (Frantzi
and Ananiadou, 1999) for more details on C-value
based term extraction).
To extract entities for the term-entity model, the
documents were annotated by a named entity tag-
ger 3. We select personal names, organization
names and location names to be included in the
term-entity model.
5.4 Contextual Similarity
We need to calculate the similarity between term-
entity models derived from different documents,
in order to decide whether they belong to the
same namesake or not. WordNet 4 based similar-
ity metrics have been widely used to compute the
semantic similarity between words in sense dis-
3The named entity tagger was developed by the Cognitive
Computation Group at UIUC. http://L2R.cs.uiuc.edu/ cog-
comp/eoh/ne.html
4http://wordnet.princeton.edu/perl/webwn
0
30
60
90
120
150
President of the United States
Tiger Woods
st
at
es
ge
or
ge
go
lfe
r
bu
sh
w
oo
ds
to
ur
vi
ce
tig
er
ch
ea
ts
sh
al
l
go
ve
rn
m
en
t
pr
es
id
en
ts
pg
a
go
lf
un
ite
d
pr
es
id
en
t
re
vi
ew
s
ex
ec
ut
iv
e
Figure 2: Distribution of words in snippets for
?Tiger Woods? and ?President of the United
States?
ambiguation tasks (Banerjee and Pedersen, 2002;
McCarthy et al, 2004). However, most of the
terms and entities in our term-entity models are
proper names or multi-word expressions which are
not listed in WordNet.
Sahami et al (2005) proposed the use of snip-
pets returned by a Web search engine to calculate
the semantic similarity between words. A snippet
is a brief text extracted from a document around
the query term. Many search engines provide snip-
pets alongside with the link to the original docu-
ment. Since snippets capture the immediate sur-
rounding of the query term in the document, we
can consider a snippet as the context of a query
term. Using snippets is also efficient because we
do not need to download the source documents.
To calculate the contextual similarity between two
terms (or entities), we first collect snippets for
each term (or entity) and pool the snippets into
a combined ?bag of words?. Each collection of
snippets is represented by a word vector, weighted
by the normalized frequency (i.e., frequency of a
word in the collection is divided by the total num-
ber of words in the collection). Then, the contex-
tual similarity between two phrases is defined as
the inner product of their snippet-word vectors.
Figures 1 and 2 show the distribution of most
frequent words in snippets for the queries ?George
Bush?, ?Tiger Woods? and ?President of the
United States?. In Figure 1 we observe the words
?george? and ?bush? appear in snippets for the
query ?President of the United States?, whereas
in Figure 2 none of the high frequent words ap-
pears in snippets for both queries. Contextual
20
similarity calculated as the inner product between
word vectors is 0.2014 for ?George Bush? and
?President of the United States?, whereas the
same is 0.0691 for ?Tiger Woods? and ?Presi-
dent of the United States?. We define the simi-
larity sim(T (A), T (B)), between two term-entity
models T (A) = {a1, . . . , an} and T (B) =
{b1, . . . , bm} of documents A and B as follows,
sim(T (A), T (B)) = 1n
n?
i=1
max
1?j?m
|ai| ? |bj |. (1)
Here, |ai| represents the vector that contains the
frequency of words that appear in the snippets
for term/entity ai. Contextual similarity between
terms/entities ai and bj , is defined as the inner
product |ai| ? |bj |. Without a loss of generality we
assume n ? m in formula 1.
5.5 Clustering
We use Group-average agglomerative clustering
(GAAC) (Cutting et al, 1992), a hybrid of single-
link and complete-link clustering, to group the
documents that belong to a particular namesake.
Initially, we assign a separate cluster for each of
the documents in the collection. Then, GAAC in
each iteration executes the merger that gives rise
to the cluster ? with the largest average correla-
tion C(?) where,
C(?) = 12
1
|?|(|?| ? 1)
X
u??
X
v??
sim(T (u), T (v)) (2)
Here, |?| denotes the number of documents in
the merged cluster ?; u and v are two documents
in ? and sim(T (u), T (v)) is given by equation 1.
Determining the total number of clusters is an im-
portant issue that directly affects the accuracy of
disambiguation. We will discuss an automatic
method to determine the number of clusters in sec-
tion 6.3.
5.6 Key phrases Selection
GAAC process yields a set of clusters representing
each of the different namesakes of the ambiguous
name. To select key phrases that uniquely iden-
tify each namesake, we first pool all the terms and
entities in all term-entity models in each cluster.
For each cluster we select the most discrimina-
tive terms/entities as the key phrases that uniquely
identify the namesake represented by that cluster
from the other namesakes. We achieve this in
two steps. In the first step, we reduce the num-
ber of terms/entities in each cluster by removing
terms/entities that also appear in other clusters.
In the second step, we select the terms/entities
in each cluster according to their relevance to
the ambiguous name. We compute the con-
textual similarity between the ambiguous name
and each term/entity and select the top ranking
terms/entities from each cluster.
6 Experiments and Results
6.1 Evaluating Contextual Similarity
In section 5.4, we defined the similarity between
documents (i.e., term-entity models created from
the documents) using a web snippets based con-
textual similarity (Formula 1). However, how well
such a metric represents the similarity between
documents, remains unknown. Therefore, to eval-
uate the contextual similarity among documents,
we group the documents in ?person-X? dataset
into four classes (each class representing a differ-
ent person) and use Formula 1 to compute within-
class and cross-class similarity histograms, as il-
lustrated in Figure 3.
Ideally, within-class similarity distribution
should have a peak around 1 and cross-class sim-
ilarity distribution around 0, whereas both his-
tograms in Figure 3(a) and 3(b) have their peaks
around 0.2. However, within-class similarity dis-
tribution is heavily biased toward to the right of
this peak and cross-class similarity distribution to
the left. Moreover, there are no document pairs
with more than 0.5 cross-class similarity. The ex-
perimental results guarantees the validity of the
contextual similarity metric.
6.2 Evaluation Metric
We evaluate experimental results based on the
confusion matrix, where A[i.j] represents the
number of documents of ?person i? predicted as
?person j? in matrix A. A[i, i] represents the num-
ber of correctly predicted documents for ?person
i?. We define the disambiguation accuracy as the
sum of diagonal elements divided by the sum of
all elements in the matrix.
6.3 Cluster Quality
Each cluster formed by the GAAC process is sup-
posed to be representing a different namesake.
Ideally, the number of clusters formed should be
equal to the number of different namesakes for
21
0300
600
900
1200
1500
1.11.00.90.80.70.60.50.40.30.20.1
(a) Within-class similarity distribution in
?person-X? dataset
0
1000
2000
3000
4000
5000
1.11.00.90.80.70.60.50.40.30.20.1
(b) Cross-class similarity distribution in
?person-X? dataset
Figure 3: The histogram of within-class and cross-class similarity distributions in ?person-X? dataset. X
axis represents the similarity value. Y axis represents the number of document pairs from the same class
(within-class) or from different classes (cross-class) that have the corresponding similarity value.
the ambiguous name. However, in reality it is
impossible to exactly know the number of name-
sakes that appear on the Web for a particular name.
Moreover, the distribution of pages among name-
sakes is not even. For example, in the ?Jim Clark?
dataset 78% of documents belong to the two fa-
mous namesakes (CEO Nestscape and Formula
one world champion). The rest of the documents
are distributed among the other six namesakes. If
these outliers get attached to the otherwise pure
clusters, both disambiguation accuracy and key
phrase selection deteriorate. Therefore, we moni-
tor the quality of clustering and terminate further
agglomeration when the cluster quality drops be-
low a pre-set threshold. Numerous metrics have
been proposed for evaluating quality of cluster-
ing (Kannan et al, 2000). We use normalized
cuts (Shi and Malik, 2000) as a measure of cluster-
quality.
Let, V denote the set of documents for a name.
Consider, A ? V to be a cluster of documents
taken from V . For two documents x,y in V ,
sim(x, y) represents the contextual similarity be-
tween the documents (Formula 1). Then, the nor-
malized cut Ncut(A) of cluster A is defined as,
Ncut(A) =
?
x?A y?(V?A) sim(x, y)?
x?A y?V sim(x, y)
. (3)
For a set, {A1, . . . , An} of non-overlapping n
clusters Ai, we define the quality of clustering,
Ac
cu
ra
cy
Quality
0
0.2
0.4
0.6
0.8
1
1.2
0.8 0.85 0.9 0.95 1 1.05
Figure 4: Accuracy Vs Cluster Quality for person-
X data set.
Quality({A1, . . . , An}), as follows,
Quality({A1, . . . , An}) = 1n
n?
i=1
Ncut(Ai). (4)
To explore the faithfulness of cluster quality
in approximating accuracy, we compare accuracy
(calculated using human-annotated data) and clus-
ter quality (automatically calculated using For-
mula 4) for person-X data set. Figure 4 shows
cluster quality in x-axis and accuracy in y-axis.
We observe a high correlation (Pearson coefficient
of 0.865) between these two measures, which en-
ables us to guide the clustering process through
cluster quality.
When cluster quality drops below a pre-defined
22
Threshold
Ac
cu
ra
cy
0.69
0.7
0.71
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.6 0.7 0.8 0.9 1
Figure 5: Accuracy Vs Threshold value for
person-X data set.
threshold, we terminate further clustering. We
assign the remaining documents to the already
formed clusters based on the correlation (For-
mula 2) between the document and the cluster. To
determine the threshold of cluster quality, we use
person-X collection as training data. Figure 5 il-
lustrates the variation of accuracy with threshold.
We select threshold at 0.935 where accuracy max-
imizes in Figure 5. Threshold was fixed at 0.935
for the rest of the experiments.
6.4 Disambiguation Accuracy
Table 2 summarizes the experimental results. The
baseline, majority sense , assigns all the doc-
uments in a collection to the person that have
most documents in the collection. Proposed
method outperforms the baseline in all data sets.
Moreover, the accuracy values for the proposed
method in Table 2 are statistically significant (t-
test: P(T?t)=0.0087, ? = 0.05) compared to the
baseline. To identify each cluster with a name-
sake, we chose the person that has most num-
ber of documents in the cluster. ?Found? column
shows the number of correctly identified name-
sakes as a fraction of total namesakes. Although
the proposed method correctly identifies the pop-
ular namesakes, it fails to identify the namesakes
who have just one or two documents in the collec-
tion.
6.5 Web Search Task
Key phrases extracted by the proposed method are
listed in Figure 6 (Due to space limitations, we
show only the top ranking key phrases for two col-
lections). To evaluate key phrases in disambiguat-
Table 2: Disambiguation accuracy for each collec-
tion.
Collection Majority Proposed Found
Sense Method Correct
person-X 0.3676 0.7794 4/4
Michael Jackson 0.6470 0.9706 2/3
Jim Clark 0.4407 0.7627 3/8
William Cohen 0.7614 0.8068 3/10
Michael Jackson
Jim Clark
fan club
trial
world network
superstar 
new charity song
neverland ranch
beer hunter
ultimate beer FAQ
christmas beer
great beer
pilsener beer
barvaria
CLUSTER #1 CLUSTER #2
CLUSTER #1 CLUSTER #2
racing driver
rally
scotsman
driving genius
scottish automobile racer
british rally news
entrepreneur
story
silicon valley
CEO
silicon graphics 
SGI/ Netscape
Figure 6: Top ranking key phrases in clusters for
Michael Jackson and Jim Clark datasets.
ing namesakes, we set up a web search experiment
as follows. We search for the ambiguous name and
the key phrase (for example, ?Jim Clark? AND
?racing driver?) and classify the top 100 results
according to their relevance to each namesake. Re-
sults of our experiment on Jim Clark dataset for
the top ranking key phrases are shown in Table 3.
In Table 3 we classified Google search results
into three categories. ?person-1? is the formula
one racing world champion, ?person -2? is the
founder of Netscape and ?other? category contains
rest of the pages that we could not classify to pre-
vious two groups 5. We first searched Google
without adding any key phrases to the name. In-
cluding terms racing diver, rally and scotsman,
Table 3: Effectiveness of key phrases in disam-
biguating namesakes.
Phrase person-1 person-2 others Hits
NONE 41 26 33 1,080,000
racing driver 81 1 18 22,500
rally 42 0 58 82,200
scotsman 67 0 33 16,500
entrepreneur 1 74 25 28,000
story 17 53 30 186,000
silicon valley 0 81 19 46,800
5some of these pages were on other namesakes and some
were not sufficiently detailed to properly classify
23
which were the top ranking terms for Jim Clark
the formula one champion, yields no results for the
other popular namesake. Likewise, the key words
entrepreneur and silicon valley yield results fort
he founder of Netscape. However, the key word
story appears for both namesakes. A close investi-
gation revealed that, the keyword story is extracted
from the title of the book ?The New New Thing:
A Silicon Valley Story?, a book on the founder of
Netscape.
7 Conclusion
We proposed and evaluated a key phrase extraction
algorithm to disambiguate people with the same
name on the Web. We represented each document
with a term-entity model and used a contextual
similarity metric to cluster the documents. We also
proposed a novel approach to determine the num-
ber of namesakes. Our experiments with pseudo
and naturally ambiguous names show a statisti-
cally significant improvement over the baseline
method. We evaluated the key phrases extracted
by the algorithm in a web search task. The web
search task reveals that including the key phrases
in the query considerably reduces ambiguity. In
future, we plan to extend the proposed method
to disambiguate other types of entities such as
location names, product names and organization
names.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. In Proceedings of COLING, pages 79?85.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using word net. In Proceedings of the third in-
ternational conference on computational linguistics
and intelligent text processing, pages 136?145.
Ron Bekkerman and Andrew McCallum. 2005. Dis-
ambiguating web appearances of people in a social
network. In Proceedings of the 14th international
conference on World Wide Web, pages 463?470.
Douglass R. Cutting, Jan O. Pedersen, David Karger,
and John W. Tukey. 1992. Scatter/gather: A cluster-
based approach to browsing large document collec-
tions. In Proceedings SIGIR ?92, pages 318?329.
M.B. Fleischman and E. Hovy. 2004. Multi-document
person name resolution. In Proceedings of 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Reference Resolution Workshop.
K.T. Frantzi and S. Ananiadou. 1999. The c-value/nc-
value domain independent method for multi-word
term extraction. Journal of Natural Language Pro-
cessing, 6(3):145?179.
S. Gauch and J. B. Smith. 1991. Search improvement
via automatic query reformulation. ACM Trans. on
Information Systems, 9(3):249?280.
R. Guha and A. Garg. 2004. Disambiguating people in
search. In Stanford University.
Hui Han, Hongyuan Zha, and C. Lee Giles. 2005.
Name disambiguation in author citations using a k-
way spectral clustering method. In Proceedings of
the International Conference on Digital Libraries.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2000. On clusterings: Good, bad, and spectral. In
Proceedings of the 41st Annual Symposium on the
Foundation of Computer Science, pages 367?380.
Xin Li, Paul Morie, and Dan Roth. 2005. Semantic
integration in text, from ambiguous names to identi-
fiable entities. AI Magazine, American Association
for Artificial Intelligence, Spring:45?58.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceed-
ings of CoNLL-2003, pages 33?40.
Y. Matsuo, J. Mori, and M. Hamasaki. 2006. Poly-
phonet: An advanced social network extraction sys-
tem. In to appear in World Wide Web Conference
(WWW).
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant word senses in untagged
text. In Proceedings of the 42nd Meeting of the As-
sociation for Computational Linguistics (ACL?04),
pages 279?286.
P. Mika. 2004. Bootstrapping the foaf-web: and ex-
periment in social networking network minning. In
Proceedings of 1st Workshop on Friend of a Friend,
Social Networking and the Semantic Web.
Ted Pedersen, Amruta Purandare, and Anagha Kulka-
rni. 2005. Name discrimination by clustering sim-
ilar contexts. In Proceedings of the Sixth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Mehran Sahami and Tim Heilman. 2005. A web-based
kernel function for matching short text snippets. In
International Workshop located at the 22nd Inter-
national Conference on Machine Learning (ICML
2005).
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8):888?
905.
24
Improving Chronological Sentence Ordering
by Precedence Relation
Naoaki OKAZAKI
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656,
Japan
okazaki@miv.t.u-tokyo.ac.jp
Yutaka MATSUO
AIST
2-41-6 Aomi, Koto-ku,
Tokyo 135-0064,
Japan
y.matsuo@carc.aist.go.jp
Mitsuru ISHIZUKA
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656,
Japan
ishizuka@miv.t.u-tokyo.ac.jp
Abstract
It is necessary to find a proper arrangement of sen-
tences in order to generate a well-organized sum-
mary from multiple documents. In this paper we de-
scribe an approach to coherent sentence ordering for
summarizing newspaper articles. Since there is no
guarantee that chronological ordering of extracted
sentences, which is widely used by conventional sum-
marization system, arranges each sentence behind
presupposed information of the sentence, we improve
chronological ordering by resolving antecedent sen-
tences of arranged sentences. Combining the re-
finement algorithm with topical segmentation and
chronological ordering, we address our experiment to
test the effectiveness of the proposed method. The
results reveal that the proposed method improves
chronological sentence ordering.
1 Introduction
The growth of computerized documents enables
us to find relevant information easily owing to
technological advances in Information Retrieval.
Although it is convenient that we can obtain a
great number of documents with a search en-
gine, this situation also presents the information
pollution problem: ?Who is willing to take the
tedious burden of reading all those text docu-
ments?? Automatic text summarization (Mani,
2001), is one solution to the problem, providing
users with a condensed version of the original
text.
Most existing summarization systems make
use of sentence or paragraph extraction, which
finds significant textual segments in source doc-
uments, and compile them in a summary. After
we select significant sentences as a material for
a summary, we must find a proper arrangement
of the sentences and edit each sentence by delet-
ing unnecessary parts or inserting necessary ex-
pressions. Although there has been a great deal
of research on extraction since the early stage
of natural language processing (Luhn, 1958),
research on post-processing of automatic sum-
marization is relatively small in number. It is
essential to pay attention to sentence ordering
in case of multi-document summarization. Sen-
tence position in the original document, which
yields a good clue to sentence arrangement for
single-document summarization, is not enough
for multi-document summarization because we
must consider inter-document order at the same
time.
In this paper we propose an approach to co-
herent text structuring for summarizing news-
paper articles. We improve chronological order-
ing, which is widely used by conventional sum-
marization system, complementing presupposed
information of each sentence. The rest of this
paper is organized as follows. We first review
the sentence ordering problem and present our
approach to generate an acceptable ordering in
the light of coherence relation. The subsequent
section (Section 3) addresses evaluation metrics
and experiment results. In Section 4 we discuss
future work and conclude this paper.
2 Sentence Ordering
2.1 Sentence ordering problem
Our goal is to determine the most probable per-
mutation of given sentences and to generate a
well-structured text. When a human is asked
to make an arrangement of sentences, he or she
may perform this task without difficulty just
as we write out thoughts in a text. However,
we must consider what accomplishes this task
since computers are unaware of order of things
by nature. Discourse coherence as typified by
rhetorical relation (Mann and Thompson, 1988)
and coherence relation (Hobbs, 1990) is of help
to this question. Hume (Hume, 1748) claimed
that qualities from which association arises and
by which the mind is conveyed from one idea
to another are three: resemblance; contiguity
in time or place; and cause and effect. That
is to say we should organize a text from frag-
c) Dolly gave birth to two children in her life.
b) The father is of a different kind and Dolly 
    had been pregnant for about five months.
a) Dolly the clone sheep was born in 1996.
Sentences Preferred ordering
[a-c-b]
Refinement
Chronological order
Figure 1: A chronological ordering is not enough.
mented information on the basis of topical rele-
vancy, chronological sequence, and cause-effect
relation. It is especially true in sentence order-
ing of newspaper articles because we must ar-
range a large number of time-series events con-
cerning several topics.
Barzilay et al (Barzilay et al, 2002) address
the problem of sentence ordering in the context
of multi-document summarization and the im-
pact of sentence ordering on readability of a
summary. They proposed two naive sentence-
ordering techniques such as majority ordering
(examines most frequent orders in the original
documents) and chronological ordering (orders
sentence by the publication date). Showing that
using naive ordering algorithms does not pro-
duce satisfactory orderings, Barzilay et al also
investigates through experiments with humans,
how to identify patterns of orderings that can
improve the algorithm. Based on the exper-
iments, they propose another algorithm that
utilizes chronological ordering with topical seg-
mentation to separate sentences referring to a
topic from ones referring to another.
Lapata (Lapata, 2003) proposes another ap-
proach to information ordering based on a prob-
abilistic model that assumes the probability of
any given sentence is determined by its adja-
cent sentence and learns constraints on sen-
tence order from a corpus of domain specific
texts. Lapata estimates transitional probabil-
ity between sentences by some attributes such
as verbs (precedence relationships of verbs in
the corpus), nouns (entity-based coherence by
keeping track of the nouns) and dependencies
(structure of sentences).
2.2 Improving chronological ordering
Against the background of these studies, we
propose the use of antecedence sentences to ar-
range sentences. Let us consider an example
shown in Figure 1. There are three sentences a,
b, and c from which we get an order [a-b-c]
by chronological ordering. When we read these
sentences in this order, we find sentence b to
be incorrectly positioned. This is because sen-
tence b is written on the presupposition that
the reader may know that Dolly had a child. In
other words, it is more fitting to assume sen-
tence b to be an elaboration of sentence c. As
one may easily imagine, there are some prece-
dent sentences prior to sentence b in the origi-
nal document. Lack of presupposition obscures
what a sentence is saying and confuses the read-
ers. Hence, we should refine the chronological
order and revise the order to [a-c-b], putting
sentence c before sentence b.
We show a block diagram of our ordering al-
gorithm shown in Figure 2. Given nine sen-
tences denoted by [a b ... i], for example,
the algorithm eventually produces an order-
ing, [a-b-f-c-i-g-d-h-e]. We consider top-
ical segmentation and chronological ordering to
be fundamental to sentence ordering as well as
conventional ordering techniques (Barzilay et
al., 2002) and make an attempt to refine the
ordering. We firstly recognize topics in source
documents to separate sentences referring to a
topic from ones referring to another. In Fig-
ure 2 example we obtain two topical segments
(clusters) as an output from the topical cluster-
ing. In the second phase we order sentences of
each segment by the chronological order. If two
sentences have the same chronological order, we
elaborate the order on the basis of sentence po-
sition and resemblance relation. Finally, we re-
fine each ordering by resolving antecedent sen-
tences and output the final ordering. In the rest
of this section we give a detailed description of
each phase.
2.3 Topical clustering
The first task is to categorize sentences by their
topics. We assume a newspaper article to be
written about one topic. Hence, to classify top-
ics in sentences, we have only to classify articles
a ab bc c
d
d
e
e
f
f
g
g
h hi
i
abc
he
ig
d
f
abf
dh
ci
e
g
Topical clustering by documents
Chronological ordering 
with resemblance relation
Ordering refinement
by precedence relation
Cluster #1
Unorderedsentences Orderedsentences
Cluster #2
Figure 2: The outline of the ordering algorithm.
by their topics. Given l articles and we found
m kinds of terms in the articles. Let D be a
document-term matrix (l ?m), whose element
Dij represents frequency of a term #j in doc-
ument #i, We use Di to denote a term vector
(i-component row vector) of document #i. Af-
ter measuring distance or dissimilarity between
two articles #x and #y:
distance(Dx, Dy) = 1? Dx ?Dy|Dx||Dy| , (1)
we apply the nearest neighbor method (Cover
and Hart, 1967) to merge a pair of clusters
when their minimum distance is lower than a
given parameter ? = 0.3 (determined empiri-
cally). At last we classify sentences according
to topical clusters, assuming that a sentence in
a document belonging to a cluster also belongs
to the same cluster.
2.4 Chronological ordering
It is difficult for computers to find a resemblance
or cause-effect relation between two phenom-
ena while we do not have conclusive evidence
whether a pair of sentences gathered arbitrarily
from multiple documents has some relation. A
newspaper usually deals with novel events that
have occurred since the last publication. Hence,
publication date (time) of each article turns out
to be a good estimator of resemblance relation
(i.e., we observe a trend or series of relevant
events in a time period), contiguity in time, and
cause-effect relation (i.e., an event occurs as a
result of previous events). Although resolving
temporal expressions in sentences (e.g., yester-
day, the next year, etc.) (Mani and Wilson,
2000; Mani et al, 2003) may give a more pre-
cise estimation of these relations, it is not an
easy task. For this reason we order sentences
of each segment (cluster) by the chronological
a
.
.
c'
.
b
c
.
Article #1 Article #2 Article #3
chronological order
Figure 3: Background idea of ordering refine-
ment by precedence relation.
order, assigning a time stamp for each sentence
by its publication date (i.e., the date when the
article was written).
When there are sentences having the same
time stamp, we elaborate the order on the ba-
sis of sentence position and sentence connectiv-
ity. We restore an original ordering if two sen-
tences have the same time stamp and belong
to the same article. If sentences have the same
time stamp and are not from the same article,
we arrange a sentence which is more similar to
previously ordered sentences to assure sentence
connectivity.
2.5 Ordering refinement by precedence
relation
After we obtain an ordering of a topical seg-
ment by chronological ordering, we improve it
as shown in Figure 1 based on antecedence sen-
tences. Figure 3 shows the background idea
of ordering refinement by precedence relation.
Just as in the example in Figure 1, we have
three sentences a, b, and c in chronological or-
der. At first we get sentence a out of the sen-
tences and check its antecedent sentences. See-
ing that there are no sentences prior to sentence
a in article #1, we accept to put sentence a
here. Then we get sentence b out of remaining
sentences and check its antecedent sentences.
We find several sentences before sentence b in
article #2 this time. Grasping what the an-
tecedent sentences are saying, we confirm first
of all whether what they are saying is mentioned
by previously arranged sentences (i.e., sentence
a). If it is mentioned, we put sentence b here
and extend the ordering to [a-b]. Otherwise,
we search a substitution for what the precedence
sentences are saying from the remaining sen-
tences (i.e., sentence c in this example). In the
Figure 3 example, we find out that sentence a is
not referring to what sentence c? is saying but
sentence c is approximately referring to that.
Start
End
a
b
ef
e f
c
d
f
f
d
df
d f
e
de
d e
f
No precedent sentences before 
sentence a. Choose a.
Choose the rest, sentence f.
The refined ordering is
a-b-e-c-d-f.
No precedent sentences before 
sentence b. Choose b.
There are precedent sentences 
before sentence c.
Search a shortest path from c 
to b and a. We found sentence 
e to be the closest to the 
precedent sentences of c.
Search a shortest path from e 
to b and a. No precedent 
sentences before e. Choose e.
We find a path from c to b and 
a via e is the shortest.
There are precedent sentences 
before sentence d.
Search a shortest path from d 
to c, e, b and a. We find the 
direct path from d to c is the 
shortest.
0
0
.2
.7
.6 1
0
0
.4.8
(1)(1)
(2)
(3)
(3)
(4)
(5)
(6)
(7)
(8)(8)
(6)(7)
(5)
(4)
(2)
Figure 4: Ordering refinement by precedence relation as a shortest path problem.
Putting sentence c before b, we finally get the
refined ordering [a-c-b].
Supposing that sentence c mentions similar
information as c? but expresses more than c?,
it is nothing unusual that an extraction method
does not choose sentence c? but sentence c.
Because a method for multi-document summa-
rization (e.g., MMR (Carbonell and Goldstein,
1998)) makes effort to acquire information cov-
erage and refuse redundant information at the
same time, it is quite natural that the method
does not choose both sentence c? and c in terms
of redundancy and prefers sentence c as c? in
terms of information coverage.
Figure 4 illustrates how the algorithm refines
a given chronological ordering [a-b-c-d-e-f].
We define distance as a dissimilarity value of
precedent information of a sentence. When
a sentence has antecedent sentences and their
content is not mentioned by previously arranged
sentences, this distance will be high. When a
sentence has no precedent sentences, we define
the distance to be 0. In the example shown
in Figure 4 example we do not change posi-
tion of sentences a and b because they do not
have precedent sentences (i.e., they are lead sen-
tences). On the other hand, sentence c has
some precedent sentences in its original docu-
ment. Preparing a term vector of the precedent
sentences, we calculate how much the precedent
content is covered by other sentences using dis-
tance defined above. In Figure 4 example the
distance from sentence a and b to c is high
(distance = 0.7). We search a shortest path
from sentence c to sentences a and b by best-
first search in order to find suitable sentences
before sentence c. Given that sentence e in Fig-
ure 4 describes similar content as the precedent
sentences of sentence c and is a lead sentence,
we trace the shortest path from sentence c to
sentences a and b via sentence e. We extend
the resultant ordering to [a-b-e-c], inserting
sentence e before sentence c. Then we consider
sentence d, which is not a lead sentence again
(distance = 0.4). Preparing a term vector of the
precedent sentences of sentence d, we search a
shortest path from sentence d to sentences a,
b, c, and e. The search result shows that we
should leave sentence d this time because the
precedent content seems to be described in sen-
tences a, b, c, and e better than f. In this way
we get the final ordering, [a-b-e-c-d-f].
3 Evaluation
In this section we describe our experiment to
test the effectiveness of the proposed method.
3.1 Experiment and evaluation metrics
We conducted an experiment of sentence order-
ing through multi-document summarization to
test the effectiveness of the proposed method.
We utilized the TSC-3 (Hirao et al, to appear in
2004) test collection, which consists of 30 sets of
multi-document summarization tasks. For more
information about TSC-3 task, see the work-
shop proceedings. Performing an important
sentence extraction (Okazaki et al, to appear
in 2004) up to the specified number of sentences
(approximately 10% of summarization rate), we
made a material for a summary (i.e., extracted
sentences) for each task. We order the sentences
by six methods: human-made ordering (HO) as
the highest anchor; random ordering (RO) as
the lowest anchor; chronological ordering (CO)
(i.e., phase 2 only); chronological ordering with
topical segmentation (COT) (i.e., phases 1 and
2); proposed method without topical segmenta-
tion (PO) (i.e., phases 2 and 3); and proposed
method with topical segmentation (POT)). We
asked human judges to evaluate sentence order-
ing of these summaries.
The first evaluation task is a subjective grad-
ing where a human judge marks an ordering of
summary sentences on a scale of 4: 4 (perfect), 3
(acceptable), 2 (poor), and 1 (unacceptable). We
give a clear criterion of scoring to the judges as
follows. A perfect summary is a text that we
cannot improve any further by re-ordering. An
acceptable summary is a one that makes sense
and is unnecessary to be revised even though
there may be some room for improvement in
terms of readability. A poor summary is a one
that loses a thread of the story at some places
and requires minor amendment to bring it up to
the acceptable level. An unacceptable summary
is a one that leaves much to be improved and
requires overall restructuring rather than par-
tial revision. Additionally, we inform the judges
that summaries were made of the same set of
extracted sentences and only sentence ordering
made differences between the summaries in or-
der to avoid any disturbance in rating.
In addition to the rating, it is useful that we
examine how close an ordering is to an accept-
able one when the ordering is regarded as poor.
Considering that several sentence-ordering pat-
terns are acceptable for a given summary, we
An ordering to evaluate: 
The corrected ordering:
s5, s6, s7, s8, s1, s2, s9, s3, s4
s5, s6, s7, s9, s2, s8, s1, s3, s4
( )
)(
Correction by move operation
A judge is supposed to show how to improve an ordering.
The judge's reading is interupted before the points marked with black circles.
Figure 5: Correction of an ordering.
think that it is valuable to measure the degree of
correction because this metric virtually requires
a human corrector to prepare a correct answer
for each ordering in his or her mind. Therefore,
a human judge is supposed to illustrate how to
improve an ordering of a summary when he or
she marks the summary with poor in the rat-
ing task. We restrict applicable operations of
correction to move operation so as to keep the
minimum correction of the ordering. We define
a move operation here as removing a sentence
and inserting the sentence into an appropriate
place (see Figure 5).
Supposing a sentence ordering to be a rank,
we can calculate rank correlation coefficient of a
permutation of an ordering pi and a permutation
of the reference ordering ?. Let {s1, ..., sn} be a
set of summary sentences identified with index
numbers from 1 to n. We define a permutation
pi ? Sn to denote an ordering of sentences where
pi(i) represents an order of sentence si. Simi-
larly, we define a permutation ? ? Sn to denote
the corrected ordering. For example, the pi and
? in Figure 5 will be:
pi =
( 1 2 3 4 5 6 7 8 9
5 6 8 9 1 2 3 4 7
)
, (2)
? =
( 1 2 3 4 5 6 7 8 9
7 5 8 9 1 2 3 6 4
)
. (3)
Spearman?s rank correlation ?s(pi, ?) and
Kendall?s rank correlation ?k(pi, ?) are known
as famous rank correlation metrics.
?s(pi, ?) = 1? 6n(n+ 1)(n? 1)
n?
i=1
(pi(i)? ?(i))2
(4)
?k(pi, ?) = 1n(n? 1)/2 ?
n?1?
i=1
n?
j=i+1
sgn(pi(j)? pi(i)) ? sgn(?(j)? ?(i)), (5)
4 3 2 1
RO 0.0 0.0 6.0 94.0
CO 13.1 22.6 63.1 1.2
COT 10.7 22.6 61.9 4.8
PO 16.7 38.1 45.2 0.0
POT 15.5 36.9 44.0 3.6
HO 52.4 21.4 26.2 0.0
Table 1: Distribution of rating score of order-
ings in percent figures.
where sgn(x) = 1 for x > 0 and ?1 otherwise.
These metrics range from ?1 (an inverse rank)
to 1 (an identical rank) via 0 (a non-correlated
rank). In the example shown in Equations 2 and
3 we obtain ?s(pi, ?) = 0.85 and ?k(pi, ?) = 0.72.
We propose another metric to assess the de-
gree of sentence continuity in reading, ?c(pi, ?):
?c(pi, ?) = 1n
n?
i=1
eq
(
pi??1(i), pi??1(i? 1) + 1
)
,
(6)
where: pi(0) = ?(0) = 0; eq(x, y) = 1 when x
equals y and 0 otherwise. This metric ranges
from 0 (no continuity) to 1 (identical). The
summary in Figure 5 may interrupt judge?s
reading after sentence S7, S1, S2 and S9 as he
or she searches a next sentence to read. Hence,
we observe four discontinuities in the order-
ing and calculate sentence continuity ?c(pi, ?) =
(9? 4)/9 = 0.56.
3.2 Results
Table 1 shows distribution of rating score of
each method in percent figures. Judges marked
about 75% of human-made ordering (HO) as ei-
ther perfect or acceptable while they rejected as
many as 95% of random ordering (RO). Chrono-
logical ordering (CO) did not yield satisfactory
result losing a thread of 63% summaries al-
though CO performed much better than RO.
Topical segmentation could not contribute to
ordering improvement of CO as well: COT is
slightly worse than CO. After taking an in-
depth look at the failure orderings, we found
the topical clustering did not perform well dur-
ing this test. We suppose the topical clustering
could not prove the merits with this test collec-
tion because the collection consists of relevant
articles retrieved by some query and polished
well by a human so as not to include unrelated
articles to a topic.
On the other hand, the proposed method
(PO) improved chronological ordering much
better than topical segmentation. Note that the
sum of perfect and acceptable ratio jumped up
from 36% (CO) to 55% (PO). This shows the
ordering refinement by precedence relation im-
proves chronological ordering by pushing poor
ordering to an acceptable level.
Table 2 reports closeness of orderings to the
corrected ones with average scores (AVG) and
the standard deviations (SD) of the three met-
rics ?s, ?k and ?c. It appears that average figures
shows similar tendency to the rating task with
three measures: HO is the best; PO is better
than CO; and RO is definitely the worst. We
applied one-way analysis of variance (ANOVA)
to test the effect of four different methods (RO,
CO, PO and HO). ANOVA proved the effect of
the different methods (p < 0.01) for three met-
rics. We also applied Tukey test to compare the
difference between these methods. Tukey test
revealed that RO was definitely the worst with
all metrics. However, Spearman?s rank correla-
tion ?S and Kendall?s rank correlation ?k failed
to prove the significant difference between CO,
PO and HO. Only sentence continuity ?c proved
PO is better than CO; and HO is better than
CO (? = 0.05). The Tukey test proved that
sentence continuity has better conformity to the
rating results and higher discrimination to make
a comparison.
Table 3 shows closeness of orderings to ones
made by human (all results of HO should be 1
by necessity). Although we found RO is clearly
the worst as well as other results, we cannot find
the significant difference between CO, PO, and
HO with all metrics. This result presents to the
difficulty of automatic evaluation by preparing
one correct ordering.
4 Conclusions
In this paper we described our approach to co-
herent sentence ordering for summarizing news-
paper articles. We conducted an experiment
of sentence ordering through multi-document
summarization. The proposed method which
utilizes precedence relation of sentence archived
good results, raising poor chronological order-
ings to an acceptable level by 20%. We also pro-
posed an evaluation metric that measures sen-
tence continuity and a amendment-based eval-
uation task. The amendment-based evalua-
tion outperformed the evaluation that compares
an ordering with an answer made by a hu-
man. The sentence continuity metric applied to
the amendment-based task showed more agree-
Spearman Kendall Continuity
Method AVG SD AVG SD AVG SD
RO 0.041 0.170 0.035 0.152 0.018 0.091
CO 0.838 0.185 0.870 0.270 0.775 0.210
COT 0.847 0.164 0.791 0.440 0.741 0.252
PO 0.843 0.180 0.921 0.144 0.856 0.180
POT 0.851 0.158 0.842 0.387 0.820 0.240
HO 0.949 0.157 0.947 0.138 0.922 0.138
Table 2: Comparison with corrected ordering.
Spearman Kendall Continuity
Method AVG SD AVG SD AVG SD
RO -0.117 0.265 -0.073 0.202 0.054 0.064
CO 0.838 0.185 0.778 0.198 0.578 0.218
COT 0.847 0.164 0.782 0.186 0.571 0.229
PO 0.843 0.180 0.792 0.184 0.606 0.225
POT 0.851 0.158 0.797 0.171 0.599 0.237
HO 1.000 0.000 1.000 0.000 1.000 0.000
Table 3: Comparison with human-made ordering.
ments with the rating result.
We plan to do further study on the sentence
ordering problem in future work, exploring how
to apply our algorithm to documents other than
newspaper or integrate ordering problem with
extraction problem to improve each other. We
also recognize the necessity to establish an auto-
matic evaluation method of sentence ordering.
Acknowledgments
We made use of Mainichi Newspaper and Yomi-
uri Newspaper articles and summarization test
collection of TSC-3.
References
R. Barzilay, E. Elhadad, and K. McKeown.
2002. Inferring strategies for sentence order-
ing in multidocument summarization. Jour-
nal of Artifical Intelligence Research (JAIR),
17:35?55.
J. Carbonell and J. Goldstein. 1998. The use of
MMR, diversity-based reranking for reorder-
ing documents and producing summaries.
In Proceedings of the 21st Annual Interna-
tional ACM-SIGIR Conference on Research
and Development in Information Retrieval,
pages 335?336.
T. M. Cover and P. E. Hart. 1967. Nearest
neighbor pattern classification. IEEE Trans-
actions on Information Theory, IT-13:21?27.
T. Hirao, T. Fukusima, M. Okumura, and
H. Nanba. to appear in 2004. Text summa-
rization challenge 3: text summarization eval-
uation at ntcir workshop4. In Working note
of the 4th NTCIR Workshop Meeting.
J. Hobbs. 1990. Literature and Cognition, CSLI
Lecture Notes 21. CSLI.
D. Hume. 1748. Philosophical Essays concern-
ing Human Understanding.
M. Lapata. 2003. Probabilistic text structur-
ing: experiments with sentence ordering. In
Proceedings of the 41st Meeting of the Asso-
ciation of Computational Linguistics, pages
545?552.
H. P. Luhn. 1958. The automatic creation of
literature abstracts. IBM Journal of Research
and Development, 2(2):159?165.
I. Mani and G. Wilson. 2000. Robust temporal
processing of news. In Proceedings of the 38th
Annual Meeting of ACL?2000, pages 69?76.
I. Mani, B. Schiffman, and J. Zhang. 2003. In-
ferring temporal ordering of events in news.
Proceedings of the Human Language Technol-
ogy Conference (HLT-NAACL) ?03.
I. Mani. 2001. Audomatic Summarization.
John Benjamins.
W. Mann and S. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory
of text organization. Text, 8:243?281.
N. Okazaki, Y. Matsuo, and M. Ishizuka. to
appear in 2004. TISS: An integrated summa-
rization system for TSC-3. In Working note
of the 4th NTCIR Workshop Meeting.
A Co-occurrence Graph-based Approach for
Personal Name Alias Extraction from Anchor Texts
Danushka Bollegala ?
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
danushka@mi.ci.i.u-
tokyo.ac.jp
Yutaka Matsuo
National Institute of Advanced
Industrial Science and
Technology
1-18-13, Sotokanda, Tokyo,
101-0021, Japan
y.matsuo@aist.go.jp
Mitsuru Ishizuka
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
ishizuka@i.u-
tokyo.ac.jp
Abstract
A person may have multiple name aliases
on the Web. Identifying aliases of a name
is important for various tasks such as in-
formation retrieval, sentiment analysis and
name disambiguation. We introduce the no-
tion of a word co-occurrence graph to rep-
resent the mutual relations between words
that appear in anchor texts. Words in an-
chor texts are represented as nodes in the
co-occurrence graph and an edge is formed
between nodes which link to the same url.
For a given personal name, its neighboring
nodes in the graph are considered as can-
didates of its aliases. We formalize alias
identification as a problem of ranking nodes
in this graph with respect to a given name.
We integrate various ranking scores through
support vector machines to leverage a robust
ranking function and use it to extract aliases
for a given name. Experimental results on a
dataset of Japanese celebrities show that the
proposed method outperforms all baselines,
displaying a MRR score of 0.562.
1 Introduction
Searching for information about people in the Web is
one of the most common activities of Internet users.
Around 30% of search engine queries include person
names (Guha and Garg, 2004). However, an indi-
vidual might have multiple nicknames or aliases on
?Research Fellow of the Japan Society for the Promotion of
Science (JSPS)
the Web. For example, the famous Japanese major
league baseball player Hideki Matsui is often called
as Godzilla in web contents. Identifying aliases of
a name is important in various tasks such as infor-
mation retrieval (Salton and McGill, 1986), senti-
ment analysis (Turney, 2002) and name disambigua-
tion (Bekkerman and McCallum, 2005).
In information retrieval, to improve recall of a
web search on a person name, a search engine can
automatically expand the query using aliases of the
name. In our previous example, a user who searches
for Hideki Matsui might also be interested in re-
trieving documents in which Matsui is referred to
as Godzilla. People use different aliases when ex-
pressing their opinions about an entity. By aggre-
gating texts written on an individual that use various
aliases, a sentiment analysis system can make an in-
formed judgment on the sentiment. Name disam-
biguation focuses on identifying different individu-
als with the same name. For example, for the name
Jim Clark, aside from the two most popular name-
sakes - the formula-one racing champion and the
founder of Netscape - at least ten different people are
listed among the top 100 results returned by Google
for the name. Although namesakes have identical
names, their nicknames usually differ. Therefore, a
name disambiguation algorithm can benefit from the
knowledge related to name aliases.
We propose an alias extraction method that ex-
ploits anchor texts and the links indicated by the
anchor texts. Link structure has been studied
extensively in information retrieval and has been
found to be useful in various tasks such as rank-
ing of web pages, identification of hub-authority
865
sites, text categorization and social network extrac-
tion (Chakrabarti, 2003). Anchor texts pointing to
an url provide useful semantic clues regarding the
resource represented by the url.
If the majority of inbound anchor texts of an
url contain a person name, then it is likely that
the remainder of the anchor texts contain informa-
tion about aliases of the name. For example, an
image of Hideki Matsui on a web page might be
linked using the real name, Hideki Matsui, as well
as aliases Godzilla and Matsu Hide. However, ex-
tracting aliases from anchor texts is a challenging
problem due to the noise in both link structure and
anchor texts. For example, web pages of extremely
diverse topics link to yahoo.com using various an-
chor texts. Moreover, given the scale of the Web,
broken links and incorrectly linked anchor texts are
abundant. Naive heuristics are insufficient to extract
aliases from anchor texts.
Our main contributions are summarized as fol-
lows:
? We introduce word co-occurrence graphs to
represents words that appear in anchor texts
and formalize the problem of alias extraction
as a one of ranking nodes in the graph with re-
spect to a given name.
? We define various ranking scores to evaluate
the appropriateness of a word as an alias of a
name. Moreover, the ranking scores are inte-
grated using support vector machines to lever-
age a robust alias detection method.
2 Related Work
Hokama and Kitagawa (2006) propose an alias ex-
traction method that is specific to Japanese lan-
guage. For a given name p, they search for the query
* koto p 1 and extract the words that match the aster-
isk. However, koto is highly ambiguous and extracts
lots of incorrect aliases. Moreover, the method can-
not extract aliases when a name and its aliases ap-
pear in separate documents.
Anchor texts and link structure have been em-
ployed in synonym extraction (Chen et al, 2003)
and translations extraction (Lu et al, 2004). Chen
et al (2003) propose the use of hyperlink structure
1koto is written in hiragana and and means also known as
Hideki MatsuiGodzilla Matsu Hide
????
Yankees baseball
sportsNew York
Figure 1: Co-occurrence graph for Hideki Matsui
within a particular domain to generate a domain-
specific thesaurus. First, a set of high quality web-
sites from a given domain is selected. Second, sev-
eral link analysis techniques are used to remove
noisy links and the navigational structure of the web-
site is converted into a content structure. Third,
pointwise mutual information is applied to identify
phrases within content structures to create a domain
specific thesaurus. They evaluate the thesaurus in a
query expansion task. Anchor texts written in differ-
ent languages that point the same object have been
used in cross-language information retrieval (CLIR)
to translate user queries. Lu et al (2004) extend this
idea by associating anchor texts written using a piv-
otal third language to find translations of queries.
3 Method
3.1 Outline
We introduce word co-occurrence graph, an undi-
rected graph, to represent words that appear in an-
chor texts. For each word that appears in the vocabu-
lary of words in anchor texts, we create a node in the
graph. Two words are considered as co-occurring if
two anchor texts containing these words link to the
same url. An edge is formed between two nodes if
the words represented by those nodes co-occur. Fig-
ure 1 illustrates a portion of the co-occurrence graph
in the proximity of Hideki Matsui as extracted by
this method from anchor texts.
Representing words that appear in anchor texts
as a graph enables us to capture the complex inter-
relations between the words. Words in inbound an-
chor texts of an url contain important semantic clues
866
regarding the resource represented by the url. Such
words form a clique in the co-occurrence graph,
indicating their close connectivity. Moreover, co-
occurrence graphs represent indirect relationships
between words. For example, in Figure 1 Hideki
Matsui is connected to New York via Yankees.
We model the problem of extracting aliases as
a one of ranking nodes in the co-occurrence graph
with respect to a real name. Usually, an individual
has just one or two aliases. A name alias extraction
algorithm must identify the correct aliases among a
vast number of related terms for an individual.
3.2 Word Co-occurrence Graph
Let V be the vocabulary of words wi that appear
in anchor texts. The boolean function A(ai, wi) re-
turns true if the anchor text ai contains the word wi.
Moreover, let the boolean function L(ai, ui) to be
true if the anchor text ai points to url ui. Then two
words wi, wj are defined to be co-occurring in a url
u, if A(ai, wi) ? A(aj , wj) ? L(ai, u) ? L(aj , u) is
true for at least one pair of anchor texts (ai, aj). In
other words, two words are said to co-occur in an url
if at least one inbound pair of anchor texts contains
the two words. Moreover, we define the number of
co-occurrences of wi and wj to be the number of
different urls they co-occur.
We define word co-occurrence graph, G(V,E)
(V is the set of nodes and E is the set of edges) as an
undirected graph where each word wi in vocabulary
V is represented by a node in the graph. Because
one-to-one mapping pertains between a word and a
node, for simplicity we use wi to represent both the
word and the corresponding node in the graph. An
edge eij ? E is created between two nodes wi, wj if
they co-occur. Given a personal name p, represented
by a node p in the co-occurrence graph, our objec-
tive is to identify the nodes that represent aliases of
p. We rank the nodes in the graph with respect to
p such that more likely a node is an alias of p, the
higher the rank it is assigned. According to our def-
inition, a node that lies n hops away from p has an
n-order co-occurrence with p. Considering the fact
that a single web page might link to many pages with
diverse topics, higher order co-occurrences with p
(i.e. nodes that appear further from p) are unreliable
as aliases of p. Consequently, we limit C(p), the set
of candidate aliases of p, to nodes which are directly
Table 1: Contingency table for a candidate alias x
x C(p)? {x}
p k n? k n
V ? {p} K ? k N ? n?K + k N ? n
V K N ?K N
connected to p in the graph. In Figure 1 candidates
of Hideki Matsui fall inside the dotted ellipse.
3.3 Ranking of Candidates
To evaluate the strength of co-occurrence between
a candidate alias and the real name, for each candi-
date alias x in C(p) we create a contingency table
as shown in Table 1. In Table 1, the first row repre-
sents candidates of p and the first column represents
nodes in the graph. Therein, k is the number of urls
in which p and x co-occur, K is the number of urls
in which at least one inbound anchor text contains
the candidate x, n is the number of urls in which
at least one inbound anchor text contains p and N is
the total number of urls in the crawl. Next, we define
various ranking scores based on Table 1.
Simplest of all ranking scores is the link frequency
(lf ). We define link frequency of an candidate x as
the number of different urls in which x and p co-
occur. This is exactly the value of k in Table 1.
Link frequency is biased towards highly frequent
words. A word that has a high frequency in anchor
texts can also report a high co-occurrence with p.
tfidf measure which is popularly used in information
retrieval can be used to normalize this bias. tfidf is
computed from Table 1 as follows,
tfidf(nj) = k log NK + 1 .
From Table 1 we compute co-occurrence mea-
sures; log likelihood ratio LLR (Dunning, 1993),
chi-squared measure CS, point-wise mutual infor-
mation PMI (Church and Hanks, 1991) and hyper
geometric distribution HG (Hisamitsu and Niwa,
2001). Each of these measures is used to rank candi-
date aliases of a given name. Because of the limited
availability of space, we omit the definitions of these
measures.
Furthermore, we define popular set overlap mea-
sures; cosine measure, overlap coefficient and Dice
coefficient from Table 1 as follows,
867
cosine(p, x) = k?n+?K ,
overlap(p, x) = kmin(n,K) ,
Dice(p, x) = 2kn+K .
3.4 Hub weighting
A frequently observed phenomenon on the Web is
that many web pages with diverse topics link to so
called hubs such as Google, Yahoo or Amazon. Be-
cause two anchor texts might link to a hub for en-
tirely different reasons, co-occurrences coming from
hubs are prone to noise. To overcome the adverse ef-
fects of a hub h when computing the ranking scores
described in section 3.3, we multiply the number
of co-occurrences of words linked to h by a factor
?(h, p) where,
?(h, p) = td? 1 . (1)
Here, t is the number of inbound anchor texts of
h that contain the real name p, d is the total num-
ber of inbound anchor texts of h. If many anchor
texts that link to h contain p (i.e., larger t value)
then the reliability of h as a source of information
about p increases. On the other hand, if h has many
inbound links (i.e., larger d value) then it is likely
to be a noisy hub and gets discounted when mul-
tiplied by ?(<< 1). Intuitively, Formula 1 boosts
hubs that are likely to be containing information re-
garding p, while penalizing those that contain vari-
ous other topics.
3.5 Training
In section 3.3 we introduced 9 ranking scores to
evaluate the appropriateness of a candidate alias for
a given name. Each of the scores is computed
with and without weighting for hubs, resulting in
2? 9 = 18 ranking scores. The ranking scores cap-
ture different statistical properties of candidates; it is
not readily apparent which ranking scores best con-
vey aliases of a name. We use real world name-alias
data to learn the proper combination of the ranking
scores.
We represent each candidate alias as a vector of
the ranking scores. Because we use the 18 rank-
ing scores described above, each candidate is repre-
sented by an 18-dimensional vector. Given a set of
personal names and their aliases, we model the train-
ing process as a preference learning task. For each
name, we impose a binary preference constraint be-
tween the correct alias and each candidate.
For example, let us assume for a name wp we
selected the four candidates a1, a2, a3, a4. With-
out loss of generality, let us further assume that a1
and a2 are the correct aliases of p. Therefore, we
form four partial preferences: a1 ? a3, a1 ? a4,
a2 ? a3 and a2 ? a4. Here, x ? y denotes
the fact that x is preferred to y. We use ranking
SVMs (Joachims, 2002) to learn a ranking function
from preference constraints. Ranking SVMs attempt
to minimize the number of discordant pairs during
training, thereby improving average precision. The
trained SVM model is used to rank a set of candi-
dates extracted for a name. Then the highest ranking
candidate is selected as the alias of the name.
4 Experiments
We crawled Japanese web sites and extracted anchor
texts and urls linked by the anchor texts. A web
site might use links for purely navigational purposes,
which convey no semantic clue. To remove naviga-
tional links in our dataset, we prepare a list of words
that are commonly used in navigational menus, such
as top, last, next, previous, links, etc and remove
anchor texts that contain those words. In addition
we remove any links that point to pages within the
same site. All urls with only one inbound anchor text
are removed from the dataset. After the above men-
tioned processing, the dataset contains 24, 456, 871
anchor texts pointing to 8, 023, 364 urls. The aver-
age number of inbound anchor texts per url is 3.05
and its standard deviation is 54.02. We tokenize
anchor texts using the Japanese morphological an-
alyzer MeCab (Kudo et al, 2004) and select nouns
as nodes in the co-occurrence graph.
For training and evaluation purposes we manually
assigned aliases for 441 Japanese celebrities. The
name-alias dataset covers people from various fields
868
Table 2: Mean Reciprocal Rank
Method MRR Method MRR
SVM (RBF) 0.5625 lf 0.0839
SVM (Linear) 0.5186 cosine 0.0761
SVM (Quad) 0.4898 tfidf 0.0757
SVM (Cubic) 0.4087 Dice 0.0751
tfidf(h) 0.3957 overlap(h) 0.0750
LLR(h) 0.3879 PMI(h) 0.0624
cosine(h) 0.3701 LLR 0.0604
lf(h) 0.3677 HG 0.0399
HG(h) 0.3297 CS 0.0079
Dice(h) 0.2905 PMI 0.0072
CS(h) 0.1186 overlap 0.0056
of cinema, sports, politics and mass-media. The ma-
jority of people in the dataset have only one alias
assigned. For each real name in the dataset we ex-
tract a set of candidates using the proposed method.
We then sort the real names in the dataset accord-
ing to the number of candidates extracted for them.
We select the top 50 real names with the greatest
number of candidate aliases for evaluation purposes
because recognizing the correct alias from numerous
candidates is a more challenging task that enables us
to perform a strict evaluation. On average a name in
our evaluation dataset has 6500 candidates, of which
only one is correct. The rest of the 391 (441 ? 50)
names are used for training.
We compare the proposed method (SVM) against
various baseline ranking scores using mean recip-
rocal rank (MRR) (Baeza-Yates and Ribeiro-Neto,
1999). The MRR is defined as follows;
MRR = 1n
n?
i=1
1
Ri . (2)
Therein, Ri is the rank assigned to a correct alias and
n is the total number of aliases. The MRR is widely
used in information retrieval to evaluate the rank-
ing of search results. Formula 2 gives high MRR to
ranking scores which assign higher ranks to correct
aliases.
Our experimental results are summarized in Ta-
ble 2. The hub weighted versions of ranking scores
are denoted by (h). We trained rank SVMs with
linear SVM (Linear), quadratic SVM (Quad), cubic
SVM (Cubic) and radial basis functions (RBF) SVM
(RBF) kernels. As shown in Table 2, the proposed
SVM-based method has the highest MRR values
among all methods compared. The best results are
obtained with the RBF kernel (SVM RBF). In fact
for 21 out of 50 names in our dataset, SVM (RBF)
correctly ranks their aliases at the first rank. Con-
sidering the fact that each name has more than 6000
candidate aliases, this is a marked improvement over
the baselines. It is noteworthy in Table 2 that the
hub-weighted versions of ranking scores outperform
the corresponding non-weighted version. This jus-
tifies the hub weighting method proposed in sec-
tion 3.4. The hub-weighted tfidf score (tfidf(h)) has
the best MRR among the baseline ranking scores.
For polynomial kernels, we observe a drop of preci-
sion concomitant with the complexity of the kernel,
which occurs as a result of over-fitting.
Table 3 shows the top-three ranked aliases ex-
tracted for Hideki Matsui by various methods. En-
glish translation of words are given within brackets.
The correct alias, Godzilla, is ranked first by SVM
(RBF). Moreover, the correct alias is followed by
the last name Matsui and his team, New York Yan-
kees. In fact, tfidf(h), LLR(h) and lf(h) all have the
exact ranking for the top three candidates. Hide,
which is an abbreviated form of Hideki, is ranked
second by these measures. However, none con-
tains the alias Godzilla among the top three candi-
dates. The non-hub weighted measures tend to in-
clude general terms such as Tokyo, Yomiuri (a pop-
ular Japanese newspaper), Nikkei (a Japanese busi-
ness newspaper), and Tokyo stock exchange. A close
analysis revealed that such general terms frequently
co-occur with a name in hubs. Without adjusting
the co-occurrences coming from hubs, such terms
invariably receive high ranking scores, as shown in
Table 3.
Incorrect tokenization of Japanese names is a
main source of error. Many aliases are out-of-
dictionary (unknown) words, which are known to
produce incorrect tokenizations in Japanese mor-
phological analyzers. Moreover, a name and its
aliases can be written in various scripts: Hiragana,
Katanaka, Kanji, Roman and even combinations of
multiple scripts. Some foreign names such as David
even have orthographic variants in Japanese: da-
bid-do or de-bid-do. Failing to recognize the differ-
ent ways in which a name can be written engenders
wrong preference constraints during training.
869
Table 3: Top ranking candidate aliases for Hideki Matsui
Method First Second Third
SVM (RBF) (Godzilla) (Matsui) (Yankees)
tfidf(h) (Matsui) (Hide) (Yankees)
LLR(h) (Matsui) (Hide) (Yankees)
cosine(h) (Matsui) (Yankees) (Hide)
lf(h) (Matsui) (Hide) (Yankees)
HG(h) (Matsui) (Yankees) (Hide)
Dice(h) (Matsui) (Yankees) (Hide)
CS(h) (Matsui) (Major league) (player)
lf (Tokyo) (Yomiuri) (Nikkei)
cosine (Yomiuri) (Tokyo stock exchange) (Matsui)
tfidf (Yomiuri) (Tokyo) (Tokyo stock exchange)
Dice (Yomiuri) (Tokyo stock exchange) (Matsui)
overlap(h) (play) (Godzilla) (Steinbrenner)
PMI(h) (play) (Godzilla) (Steinbrenner)
LLR (Yomiuri) (Tokyo stock exchange) (jiji.com)
HG (Yomiuri) (Tokyo stock exchange) (Matsui)
CS (jiji.com) (Tokyo stock exchange) (Yomiuri)
PMI (Komdatzien) (picture) (contents)
overlap (Komdatzien) (picture) (contents)
5 Conclusion
We proposed a method to extract aliases of a given
name using anchor texts and link structure. We cre-
ated a co-occurrence graph to represent words in an-
chor texts and modeled the problem of alias extrac-
tion as a one of ranking nodes in this graph with re-
spect to a given name. In future, we intend to apply
the proposed method to extract aliases for other en-
tity types such as products, organizations and loca-
tions.
References
R.A. Baeza-Yates and B.A. Ribeiro-Neto. 1999. Modern
Information Retrieval. ACM Press / Addison-Wesley.
R. Bekkerman and A. McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In
Proc. of the World Wide Web Conference (WWW? 05),
pages 463?470.
S. Chakrabarti. 2003. Mining the Web: Discovering
Knowledge from Hypertext Data. Morgan Kaufmann.
Z. Chen, S. Liu, L. Wenyin, Ge. Pu, and W. Ma. 2003.
Building a web thesaurus from web link structure.
In Proc. of the 26th annual international ACM SI-
GIR conference on Research and development in in-
formaion retrieval, pages 48?55.
K. Church and P. Hanks. 1991. Word association norms,
mutual information and lexicography. Computational
Linguistics, 16:22?29.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19:61?74.
R. Guha and A. Garg. 2004. Disambiguating people in
search. In Stanford University.
T. Hisamitsu and Y. Niwa. 2001. Topic-word selection
based on combinatorial probability. In Proc. of NL-
PRS?01, pages 289?296.
T. Hokama and H. Kitagawa. 2006. Extracting
mnemonic names of people from the web. In Proc.
of 9th International Conference on Asian Digital Li-
braries (ICADL?06), pages 121?130.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. of the ACM conference on
Knowledge Discovery and Data Minning (KDD).
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Ap-
plying conditional random fields to japanese morpho-
logical analysis. In Proc. of EMNLP?04.
W. Lu, L. Chien, and H. Lee. 2004. Anchor text mining
for translation of web queries: A transitive translation
approach. ACM Transactions on Information Systems,
22(2):242?269.
G. Salton and M.J. McGill. 1986. Introduction to Mod-
ern Information Retreival. McGraw-Hill Inc., New
York, NY.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification of
reviews. In Proc. of the ACL, pages 417?424.
870
Proceedings of NAACL HLT 2007, pages 340?347,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An Integrated Approach to Measuring Semantic Similarity between Words
Using Information available on the Web
Danushka Bollegala
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
danushka@mi.ci.i.u-
tokyo.ac.jp
Yutaka Matsuo
National Institute of Advanced
Industrial Science and
Technology
1-18-13, Sotokanda, Tokyo,
101-0021, Japan
y.matsuo@aist.go.jp
Mitsuru Ishizuka
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
ishizuka@i.u-
tokyo.ac.jp
Abstract
Measuring semantic similarity between
words is vital for various applications
in natural language processing, such as
language modeling, information retrieval,
and document clustering. We propose a
method that utilizes the information avail-
able on the Web to measure semantic sim-
ilarity between a pair of words or entities.
We integrate page counts for each word in
the pair and lexico-syntactic patterns that
occur among the top ranking snippets for
the AND query using support vector ma-
chines. Experimental results on Miller-
Charles? benchmark data set show that the
proposed measure outperforms all the ex-
isting web based semantic similarity mea-
sures by a wide margin, achieving a cor-
relation coefficient of 0.834. Moreover,
the proposed semantic similarity measure
significantly improves the accuracy (F -
measure of 0.78) in a named entity cluster-
ing task, proving the capability of the pro-
posed measure to capture semantic simi-
larity using web content.
1 Introduction
The study of semantic similarity between words has
been an integral part of natural language processing
and information retrieval for many years. Semantic
similarity measures are vital for various applications
in natural language processing such as word sense
disambiguation (Resnik, 1999), language model-
ing (Rosenfield, 1996), synonym extraction (Lin,
1998a) and automatic thesaurus extraction (Curran,
2002).
Pre-compiled taxonomies such as WordNet 1 and
text corpora have been used in previous work on se-
mantic similarity (Lin, 1998a; Resnik, 1995; Jiang
and Conrath, 1998; Lin, 1998b). However, seman-
tic similarity between words change over time as
new senses and associations of words are constantly
created. One major issue behind taxonomies and
corpora oriented approaches is that they might not
necessarily capture similarity between proper names
such as named entities (e.g., personal names, loca-
tion names, product names) and the new uses of ex-
isting words. For example, apple is frequently asso-
ciated with computers on the Web but this sense of
apple is not listed in the WordNet. Maintaining an
up-to-date taxonomy of all the new words and new
usages of existing words is costly if not impossible.
The Web can be regarded as a large-scale, dy-
namic corpus of text. Regarding the Web as a live
corpus has become an active research topic recently.
Simple, unsupervised models have shown to per-
form better when n-gram counts are obtained from
the Web rather than from a large corpus (Keller and
Lapata, 2003; Lapata and Keller, 2005). Resnik and
Smith (2003) extract bilingual sentences from the
Web to create parallel corpora for machine trans-
lation. Turney (2001) defines a point wise mutual
information (PMI-IR) measure using the number of
hits returned by a Web search engine to recognize
synonyms. Matsuo et. al, (2006b) follows a similar
1http://wordnet.princeton.edu/
340
approach to measure the similarity between words
and apply their method in a graph-based word clus-
tering algorithm.
Due to the huge number of documents and the
high growth rate of the Web, it is difficult to di-
rectly analyze each individual document separately.
Search engines provide an efficient interface to this
vast information. Page counts and snippets are two
useful information sources provided by most Web
search engines. Page count of a query is the number
of pages that contain the query words 2. A snippet is
a brief window of text extracted by a search engine
around the query term in a document. Snippets pro-
vide useful information about the immediate context
of the query term.
This paper proposes a Web-based semantic simi-
larity metric which combines page counts and snip-
pets using support vector machines. We extract
lexico-syntactic patterns from snippets. For exam-
ple, X is a Y indicates there is a high semantic sim-
ilarity between X and Y. Automatically extracted
lexico-syntactic patterns have been successfully em-
ployed in various term extraction tasks (Hearst,
1992).
Our contributions are summarized as follows:
? We propose a lexico-syntactic patterns-based
approach to compute semantic similarity using
snippets obtained from a Web search engine.
? We integrate different Web-based similarity
scores using WordNet synsets and support vec-
tor machines to create a robust semantic sim-
ilarity measure. The integrated measure out-
performs all existing Web-based semantic sim-
ilarity measures in a benchmark dataset and a
named entity clustering task. To the best of
our knowledge, this is the first attempt to com-
bine both WordNet synsets and Web content to
leverage a robust semantic similarity measure.
2 Previous Work
Given a taxonomy of concepts, a straightforward
method for calculating similarity between two words
(concepts) is to find the length of the shortest path
2page count may not necessarily be equal to the word fre-
quency because the queried word may appear many times in a
page
connecting the two words in the taxonomy (Rada
et al, 1989). If a word is polysemous (i.e., having
more than one sense) then multiple paths may ex-
ist between the two words. In such cases only the
shortest path between any two senses of the words is
considered for the calculation of similarity. A prob-
lem frequently acknowledged with this approach is
that it relies on the notion that all links in the taxon-
omy represent uniform distances.
Resnik (1995) proposes a similarity measure
based on information content. He defines the sim-
ilarity between two concepts C1 and C2 in the tax-
onomy as the maximum of the information content
of all concepts C that subsume both C1 and C2.
Then the similarity between two words are defined
as the maximum of the similarity between any con-
cepts that the words belong to. He uses WordNet as
the taxonomy and information content is calculated
using the Brown corpus.
Li et al, (2003) combines structural semantic in-
formation from a lexical taxonomy and informa-
tion content from a corpus in a non-linear model.
They propose a similarity measure that uses shortest
path length, depth and local density in a taxonomy.
Their experiments using WordNet and the Brown
corpus reports a Pearson correlation coefficient of
0.8914 on the Miller and Charles? (1998) bench-
mark dataset. They do not evaluate their method on
similarities between named entities. Recently, some
work has been carried out on measuring semantic
similarity using web content. Matsuo et al, (2006a)
propose the use of Web hits for the extraction of
communities on the Web. They measure the associ-
ation between two personal names using the overlap
coefficient, calculated based on the number of Web
hits for each individual name and their conjunction.
Sahami et al, (2006) measure semantic similarity
between two queries using the snippets returned for
those queries by a search engine. For each query,
they collect snippets from a search engine and rep-
resent each snippet as a TF-IDF weighted term vec-
tor. Each vector is L2 normalized and the centroid
of the set of vectors is computed. Semantic similar-
ity between two queries is then defined as the inner
product between the corresponding centroid vectors.
They do not compare their similarity measure with
taxonomy based similarity measures.
Chen et al, (2006) propose a web-based double-
341
checking model to compute semantic similarity be-
tween words. For two words P and Q, they col-
lect snippets for each word from a web search en-
gine. Then they count the number of occurrences of
word P in the snippets for word Q and the number
of occurrences of word Q in the snippets for word
P . These values are combined non-linearly to com-
pute the similarity between P and Q. This method
heavily depends on the search engine?s ranking al-
gorithm. Although two words P and Q may be very
similar, there is no reason to believe that one can find
Q in the snippets for P , or vice versa. This observa-
tion is confirmed by the experimental results in their
paper which reports 0 similarity scores for many
pairs of words in the Miller and Charles (1998) data
set.
3 Method
In this section we will describe the various similarity
features we use in our model. We utilize page counts
and snippets returned by the Google 3 search engine
for simple text queries to define various similarity
scores.
3.1 Page Counts-based Similarity Scores
For the rest of this paper we use the notation H(P )
to denote the page count for the query P in a search
engine. Terra and Clarke (2003) compare various
similarity scores for measuring similarity between
words in a corpus. We modify the traditional Jac-
card, overlap (Simpson), Dice and PMI measures
for the purpose of measuring similarity using page
counts. WebJaccard coefficient between words (or
phrases) P and Q, WebJaccard(P,Q), is defined
by,
WebJaccard(P,Q)
=
{ 0 if H(P ?Q) ? c
H(P?Q)
H(P )+H(Q)?H(P?Q) otherwise
.(1)
Here, P ? Q denotes the conjunction query P AND
Q. Given the scale and noise in the Web, some words
might occur arbitrarily, i.e. by random chance, on
some pages. Given the scale and noise in web data, it
is a possible that two words man order to reduce the
adverse effect due to random co-occurrences, we set
3http://www.google.com
the WebJaccard coefficient to zero if the page counts
for the query P ?Q is less than a threshold c. 4
Likewise, we define WebOverlap coefficient,
WebOverlap(P,Q), as,
WebOverlap(P,Q)
=
{ 0 if H(P ?Q) ? c
H(P?Q)
min(H(P ),H(Q)) otherwise
.(2)
We define WebDice as a variant of Dice coeffi-
cient. WebDice(P,Q) is defined as,
WebDice(P,Q)
=
{ 0 if H(P ?Q) ? c
2H(P?Q)
H(P )+H(Q) otherwise
. (3)
We define WebPMI as a variant form of PMI using
page counts by,
WebPMI(P,Q)
=
?
?
?
0 if H(P ?Q) ? c
log2(
H(P?Q)
N
H(P )
N
H(Q)
N
) otherwise .(4)
Here, N is the number of documents indexed by the
search engine. Probabilities in Formula 4 are esti-
mated according to the maximum likelihood princi-
ple. In order to accurately calculate PMI using For-
mula 4, we must know N , the number of documents
indexed by the search engine. Although estimating
the number of documents indexed by a search en-
gine (Bar-Yossef and Gurevich, 2006) is an interest-
ing task itself, it is beyond the scope of this work. In
this work, we set N = 1010 according to the number
of indexed pages reported by Google.
3.2 Snippets-based Synonymous Word
Patterns
Page counts-based similarity measures do not con-
sider the relative distance between P and Q in a page
or the length of the page. Although P and Q occur
in a page they might not be related at all. Therefore,
page counts-based similarity measures are prone to
noise and are not reliable when H(P ?Q) is low. On
the other hand snippets capture the local context of
query words. We propose lexico-syntactic patterns
extracted from snippets as a solution to the problems
with page counts-based similarity measures.
4we set c = 5 in our experiments
342
To illustrate our pattern extraction algorithm con-
sider the following snippet from Google for the
query jaguar AND cat.
?The Jaguar is the largest cat in Western Hemi-
sphere and can subdue a larger prey than can the
puma?
Here, the phrase is the largest indicates a hy-
pernymic relationship between Jaguar and the cat.
Phrases such as also known as, is a, part of, is an ex-
ample of all indicate various of semantic relations.
Such indicative phrases have been successfully ap-
plied in various tasks such as synonym extraction,
hyponym extraction (Hearst, 1992) and fact extrac-
tion (Pasca et al, 2006).
We describe our pattern extraction algorithm in
three steps.
Step 1
We replace the two query terms in a snippet by two
wildcards X and Y. We extract all word n-grams that
contain both X and Y. In our experiments we ex-
tracted n-grams for n = 2 to 5. For example, from
the previous snippet we extract the pattern, X is the
largest X. In order to leverage the pattern extraction
process, we randomly select 5000 pairs of synony-
mous nouns from WordNet synsets. We ignore the
nouns which do not have synonyms in the WordNet.
For nouns with more than one sense, we select syn-
onyms from its dominant sense. For each pair of
synonyms (P,Q), we query Google for ?P? AND
?Q? and download the snippets. Let us call this col-
lection of snippets as the positive corpus. We apply
the above mentioned n-gram based pattern extrac-
tion procedure and count the frequency of each valid
pattern in the positive corpus.
Step 2
Pattern extraction algorithm described in step 1
yields 4, 562, 471 unique patterns. 80%of these pat-
terns occur less than 10 times in the positive corpus.
It is impossible to learn with such a large number of
sparse patterns. Moreover, some patterns might oc-
cur purely randomly in a snippet and are not good
indicators of semantic similarity. To measure the
reliability of a pattern as an indicator of semantic
similarity we employ the following procedure. We
create a set of non-synonymous word-pairs by ran-
domly shuffling the words in our data set of synony-
Table 1: Contingency table
v other than v All
Freq. in positive corpus pv P ? pv P
Freq. in negative corpus nv N ? nv N
mous word-pairs. We check each pair of words in
this newly created data set against WordNet and con-
firm that they do not belong to any of the synsets
in the WordNet. From this procedure we created
5000 non-synonymous pairs of words. For each
non-synonymous word-pair, we query Google for
the conjunction of its words and download snippets.
Let us call this collection of snippets as the nega-
tive corpus. For each pattern generated in step 1, we
count its frequency in the negative corpus.
Step 3
We create a contingency table as shown in Table 1
for each pattern v extracted in step 1 using its fre-
quency pv in positive corpus and nv in negative cor-
pus. In Table 1, P denotes the total frequency of all
patterns in the positive corpus and N denotes that in
the negative corpus.
Using the information in Table 1, we calculate
?2 (Manning and Schu?tze, 2002) value for each pat-
tern as,
?2 = (P +N)(pv(N ? nv)? nv(P ? pv))
2
PN(pv + nv)(P +N ? pv ? nv) .
(5)
We selected the top ranking 200 patterns experimen-
tally as described in section 4.2 according to their ?2
values. Some of the selected patterns are shown in
Table 2.
3.3 Training
For each pair of synonymous and non-synonymous
words in our datasets, we count the frequency of
occurrence of the patterns selected in Step 3. We
normalize the frequency count of each pattern by
dividing from the total frequency of all patterns.
Moreover, we compute the page counts-based fea-
tures as given by formulae (1-4). Using the 200
pattern features and the 4 page counts-based fea-
tures we create 204 dimensional feature vectors for
each training instance in our synonymous and non-
synonymous datasets. We train a two class support
vector machine (SVM) (Vapnik, 1998), where class
343
+1 represents synonymous word-pairs and class
?1 represents non-synonymous word-pairs. Finally,
SVM outputs are converted to posterior probabilities
(Platt, 2000). We consider the posterior probability
of a given pair of words belonging to class +1 as the
semantic similarity between the two words.
4 Experiments
To evaluate the performance of the proposed se-
mantic similarity measure, we conduct two sets of
experiments. Firstly, we compare the similarity
scores produced by the proposed measure against
the Miller-Charles? benchmark dataset. We analyze
the performance of the proposed measure with the
number of snippets and the size of the training data
set. Secondly, we apply the proposed measure in a
real-world named entity clustering task and measure
its performance.
4.1 The Benchmark Dataset
We evaluated the proposed method against Miller-
Charles (1998) dataset, a dataset of 30 5 word-pairs
rated by a group of 38 human subjects. Word-
pairs are rated on a scale from 0 (no similarity) to
4 (perfect synonymy). Miller-Charles? dataset is
a subset of Rubenstein-Goodenough?s (1965) orig-
inal dataset of 65 word-pairs. Although Miller-
Charles? experiment was carried out 25 years
later than Rubenstein-Goodenough?s, two sets of
ratings are highly correlated (Pearson correlation
coefficient=0.97). Therefore, Miller-Charles ratings
can be considered as a reliable benchmark for eval-
uating semantic similarity measures.
4.2 Pattern Selection
We trained a linear kernel SVM with top N pattern
features (ranked according to their ?2 values) and
calculated the Pearson correlation coefficient against
the Miller-Charles? benchmark dataset. Experimen-
tal results are shown in Figure 1. From Figure 1
we select N = 200, where correlation maximizes.
Features with the highest linear kernel weights are
shown in Table 2 alongside with their ?2 values. The
weight of a feature in the linear kernel can be consid-
ered as a rough estimate of the influence it has on the
5Due to the omission of two word-pairs in earlier versions
of WordNet most researchers had used only 28 pairs for evalu-
ations
0 200 400 600 800 1000120014001600180020000.780
0.782
0.784
0.786
0.788
0.790
0.792
0.794
0.796
0.798
0.800
C
or
re
la
tio
n 
C
oe
ffi
ci
en
t (
r)
Number of pattern features (N)
Figure 1: Correlation vs No of pattern features
Table 2: Features with the highest SVM linear ker-
nel weights
feature ?2 SVM weight
WebDice N/A 8.19
X/Y 33459 7.53
X, Y : 4089 6.00
X or Y 3574 5.83
X Y for 1089 4.49
X . the Y 1784 2.99
with X ( Y 1819 2.85
X=Y 2215 2.74
X and Y are 1343 2.67
X of Y 2472 2.56
final SVM output. WebDice has the highest linear
kernel weight followed by a series of patterns-based
features. WebOverlap (rank=18, weight=2.45), We-
bJaccard (rank=66, weight=0.618) and WebPMI
(rank=138, weight=0.0001) are not shown in Table 2
due to space limitations. It is noteworthy that the
pattern features in Table 2 agree with the intuition.
Lexical patterns (e.g., X or Y, X and Y are, X of Y) as
well as syntactic patterns (e.g., bracketing, comma
usage) are extracted by our method.
4.3 Semantic Similarity
We score the word-pairs in Miller-Charles dataset
using the page counts-based similarity measures,
previous work on web-based semantic similarity
measures (Sahami (2006), Chen (2006)) and the
proposed method (SVM). Results are shown in Ta-
ble 4.3. All figures except for the Miller-Charles
ratings are normalized into [0, 1] range for the ease
of comparison 6. Proposed method (SVM) re-
6Pearson correlation coefficient is invariant against a linear
transformation
344
Table 3: Semantic Similarity of Human Ratings and baselines on Miller-Charles dataset
Word Pair Miller- Web Web Web Web Sahami Chen (CODC) Proposed
Charles Jaccard Dice Overlap PMI (2006) (2006) (SVM)
cord-smile 0.13 0.102 0.108 0.036 0.207 0.090 0 0
rooster-voyage 0.08 0.011 0.012 0.021 0.228 0.197 0 0.017
noon-string 0.08 0.126 0.133 0.060 0.101 0.082 0 0.018
glass-magician 0.11 0.117 0.124 0.408 0.598 0.143 0 0.180
monk-slave 0.55 0.181 0.191 0.067 0.610 0.095 0 0.375
coast-forest 0.42 0.862 0.870 0.310 0.417 0.248 0 0.405
monk-oracle 1.1 0.016 0.017 0.023 0 0.045 0 0.328
lad-wizard 0.42 0.072 0.077 0.070 0.426 0.149 0 0.220
forest-graveyard 0.84 0.068 0.072 0.246 0.494 0 0 0.547
food-rooster 0.89 0.012 0.013 0.425 0.207 0.075 0 0.060
coast-hill 0.87 0.963 0.965 0.279 0.350 0.293 0 0.874
car-journey 1.16 0.444 0.460 0.378 0.204 0.189 0.290 0.286
crane-implement 1.68 0.071 0.076 0.119 0.193 0.152 0 0.133
brother-lad 1.66 0.189 0.199 0.369 0.644 0.236 0.379 0.344
bird-crane 2.97 0.235 0.247 0.226 0.515 0.223 0 0.879
bird-cock 3.05 0.153 0.162 0.162 0.428 0.058 0.502 0.593
food-fruit 3.08 0.753 0.765 1 0.448 0.181 0.338 0.998
brother-monk 2.82 0.261 0.274 0.340 0.622 0.267 0.547 0.377
asylum-madhouse 3.61 0.024 0.025 0.102 0.813 0.212 0 0.773
furnace-stove 3.11 0.401 0.417 0.118 1 0.310 0.928 0.889
magician-wizard 3.5 0.295 0.309 0.383 0.863 0.233 0.671 1
journey-voyage 3.84 0.415 0.431 0.182 0.467 0.524 0.417 0.996
coast-shore 3.7 0.786 0.796 0.521 0.561 0.381 0.518 0.945
implement-tool 2.95 1 1 0.517 0.296 0.419 0.419 0.684
boy-lad 3.76 0.186 0.196 0.601 0.631 0.471 0 0.974
automobile-car 3.92 0.654 0.668 0.834 0.427 1 0.686 0.980
midday-noon 3.42 0.106 0.112 0.135 0.586 0.289 0.856 0.819
gem-jewel 3.84 0.295 0.309 0.094 0.687 0.211 1 0.686
Correlation 1 0.259 0.267 0.382 0.548 0.579 0.693 0.834
ports the highest correlation of 0.8129 in our ex-
periments. Our implementation of Co-occurrence
Double Checking (CODC) measure (Chen et al,
2006) reports the second best correlation of 0.6936.
However, CODC measure reports zero similarity for
many word-pairs. This is because for a word-pair
(P,Q), we might not necessarily find Q among the
top snippets for P (and vice versa). CODC mea-
sure returns zero under these conditions. Sahami
et al (2006) is ranked third with a correlation of
0.5797. Among the four page counts based mea-
sures WebPMI reports the highest correlation (r =
0.5489). Overall, the results in Table 4.3 suggest
that snippet-based measures are more accurate than
page counts-based measures in capturing semantic
similarity. This is evident for word-pairs where at
least one of the words is a polysemous word (e.g.,
pairs that include cock, brother). Page counts-based
measures do not consider the context in which the
words appear in a page, thus cannot disambiguate
Table 4: Comparison with taxonomy based methods
Method correlation
Human replication 0.901
Resnik (1995) 0.745
Lin (1998) 0.822
Li et al(2003) 0.891
Edge-counting 0.664
Information content 0.745
Jiang & Conrath (1998) 0.848
proposed (SVM) 0.834
the multiple senses.
As summarized in Table 4.3, proposed method
is comparable with the WordNet based methods.
In fact, the proposed method outperforms simple
WordNet based approaches such as Edge-Counting
and Information Content measures. However, con-
sidering the high correlation between human sub-
jects (0.9), there is still room for improvement.
Figure 2 illustrates the effect of the number
of snippets on the performance of the proposed
345
0 100 200 300 400 500 600 700 800 900 10000.70
0.71
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.80
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Number of snippets
Figure 2: Correlation vs No of snippets
 500  1000  1500  2000  2500  3000  3500  4000negative examples  500
 1000 1500
 2000 2500
 3000 3500
 4000
positive examples
 0.45 0.5
 0.55 0.6
 0.65 0.7
 0.75 0.8
 0.85
correlation
Figure 3: Correlation vs No of positive and negative
training instances
method. Correlation coefficient steadily improves
with the number of snippets used for extracting pat-
terns. When few snippets are processed only a few
patterns are found, thus the feature vector becomes
sparse, resulting in poor performance. Figure 3 de-
picts the correlation with human ratings for various
combinations of positive and negative training in-
stances. Maximum correlation coefficient of 0.834
is achieved with 1900 positive training examples and
2400 negative training examples. Moreover, Fig-
ure 3 reveals that correlation does not improve be-
yond 2500 positive and negative training examples.
Therefore, we can conclude that 2500 examples are
sufficient to leverage the proposed semantic similar-
ity measure.
4.4 Named Entity Clustering
Measuring semantic similarity between named en-
tities is vital in many applications such as query
expansion (Sahami and Heilman, 2006) and com-
munity mining (Matsuo et al, 2006a). Since most
named entities are not covered by WordNet, simi-
larity measures based on WordNet alne cannot be
Table 5: Performance of named entity clustering
Method Precision Recall F Measure
WebJaccard 0.5926 0.712 0.6147
WebOverlap 0.5976 0.68 0.5965
WebDice 0.5895 0.716 0.6179
WebPMI 0.2649 0.428 0.2916
Sahami (2006) 0.6384 0.668 0.6426
Chen (2006) 0.4763 0.624 0.4984
Proposed 0.7958 0.804 0.7897
used in such tasks. Unlike common English words,
named entities are constantly being created. Manu-
ally maintaining an up-to-date taxonomy of named
entities is costly, if not impossible. The proposed
semantic similarity measure is appealing as it does
not require pre-compiled taxonomies. In order to
evaluate the performance of the proposed measure
in capturing the semantic similarity between named
entities, we set up a named entity clustering task.
We selected 50 person names from 5 categories :
tennis players, golfers, actors, politicians and scien-
tists, (10 names from each category) from the dmoz
directory 7. For each pair of names in our dataset,
we measure the association between the two names
using the proposed method and baselines. We use
group-average agglomerative hierarchical clustering
to cluster the names in our dataset into five clusters.
We employed the B-CUBED metric (Bagga and
Baldwin, 1998) to evaluate the clustering results. As
summarized in Table 5 the proposed method outper-
forms all the baselines with a statistically significant
(p ? 0.01 Tukey HSD) F score of 0.7897.
5 Conclusion
We propose an SVM-based approach to combine
page counts and lexico-syntactic patterns extracted
from snippets to leverage a robust web-based seman-
tic similarity measure. The proposed similarity mea-
sure outperforms existing web-based similarity mea-
sures and competes with models trained on Word-
Net. It requires just 2500 synonymous word-pairs,
automatically extracted from WordNet synsets, for
training. Moreover, the proposed method proves
useful in a named entity clustering task. In future,
we intend to apply the proposed method to automat-
ically extract synonyms from the web.
7http://dmoz.org
346
References
A. Bagga and B. Baldwin. 1998. Entity-based cross doc-
ument coreferencing using the vector space model. In
Proc. of 36th COLING-ACL, pages 79?85.
Z. Bar-Yossef and M. Gurevich. 2006. Random sam-
pling from a search engine?s index. In Proceedings of
15th International World Wide Web Conference.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proc. of the COLING/ACL 2006, pages 1009?1016.
J. Curran. 2002. Ensemble menthods for automatic the-
saurus extraction. In Proc. of EMNLP.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of 14th COLING,
pages 539?545.
J.J. Jiang and D.W. Conrath. 1998. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of the International Conference on Research in
Computational Linguistics ROCLING X.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
M. Lapata and F. Keller. 2005. Web-based models ofr
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1):1?31.
D. Lin. 1998a. Automatic retreival and clustering of sim-
ilar words. In Proc. of the 17th COLING, pages 768?
774.
D. Lin. 1998b. An information-theoretic definition of
similarity. In Proc. of the 15th ICML, pages 296?304.
C. D. Manning and H. Schu?tze. 2002. Foundations of
Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Y. Matsuo, J. Mori, M. Hamasaki, K. Ishida,
T. Nishimura, H. Takeda, K. Hasida, and M. Ishizuka.
2006a. Polyphonet: An advanced social network ex-
traction system. In Proc. of 15th International World
Wide Web Conference.
Y. Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka.
2006b. Graph-based word clustering using web search
engine. In Proc. of EMNLP 2006.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1?28.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the world wide web
of facts - step one: the one-million fact extraction chal-
lenge. In Proc. of AAAI-2006.
J. Platt. 2000. Probabilistic outputs for support vec-
tor machines and comparison to regularized likelihood
methods. Advances in Large Margin Classifiers, pages
61?74.
R. Rada, H. Mili, E. Bichnell, and M. Blettner. 1989.
Development and application of a metric on semantic
nets. IEEE Transactions on Systems, Man and Cyber-
netics, 9(1):17?30.
P. Resnik and N. A. Smith. 2003. The web as a parallel
corpus. Computational Linguistics, 29(3):349?380.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of
14th International Joint Conference on Aritificial In-
telligence.
P. Resnik. 1999. Semantic similarity in a taxonomy: An
information based measure and its application to prob-
lems of ambiguity in natural language. Journal of Ar-
itificial Intelligence Research, 11:95?130.
R. Rosenfield. 1996. A maximum entropy approach to
adaptive statistical modelling. Computer Speech and
Language, 10:187?228.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8:627?633.
M. Sahami and T. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proc. of 15th International World Wide
Web Conference.
E. Terra and C.L.A. Clarke. 2003. Frequency estimates
for statistical word similarity measures. In Proc. of the
NAACL/HLT, pages 165?172.
P. D. Turney. 2001. Minning the web for synonyms:
Pmi-ir versus lsa on toefl. In Proc. of ECML-2001,
pages 491?502.
V. Vapnik. 1998. Statistical Learning Theory. Wiley,
Chichester, GB.
D. McLean Y. Li, Zuhair A. Bandar. 2003. An approch
for measuring semantic similarity between words us-
ing multiple information sources. IEEE Transactions
on Knowledge and Data Engineering, 15(4):871?882.
347
Proceedings of NAACL HLT 2007, Companion Volume, pages 125?128,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Subtree Mining for Relation Extraction from Wikipedia
Dat P.T. Nguyen
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
nptdat@mi.ci.i.u-tokyo.ac.jp
Yutaka Matsuo
National Institute of Advanced
Industrial Science and Technology
Sotokanda 1-18-13
Tokyo 101-0021, Japan
y.matsuo@aist.go.jp
Mitsuru Ishizuka
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
ishizuka@i.u-tokyo.ac.jp
Abstract
In this study, we address the problem of extract-
ing relations between entities from Wikipedia?s
English articles. Our proposed method first an-
chors the appearance of entities in Wikipedia?s
articles using neither Named Entity Recognizer
(NER) nor coreference resolution tool. It then
classifies the relationships between entity pairs
using SVM with features extracted from the
web structure and subtrees mined from the
syntactic structure of text. We evaluate our
method on manually annotated data from ac-
tual Wikipedia articles.
1 Introduction
Wikipedia (www.wikipedia.org) has emerged as the
world?s largest online encyclopedia. Because the ency-
clopedia is managed by the Wikipedia Foundation, and
because numerous collaborators in the world continu-
ously develop and edit its articles, its contents are be-
lieved to be quite reliable despite its openness.
This study is intended to deal with the problem of
extracting binary relations between entity pairs from
Wikipedia?s English version. A binary relation is defined
as a triple (ep, rel, es) in which ep and es are entities and
rel indicates a directed relationship of ep and es. Current
experiment limits entities and relations to a reasonable
size in that an entity is classifiable as person, organiza-
tion, location, artifact, year, month or date; and a rela-
tion can be founder, chairman, CEO, COO, president,
director, vice chairman, spouse, birth date, birth place,
foundation, product and location.
To our knowledge, only one recent work has at-
tempted relation extraction on Wikipedia: (Culotta et al,
2006) presents a probabilistic model to integrate extrac-
tion and mining tasks performed on biographical text of
Wikipedia. Some other works (Brin, 1998; Agichtein and
Gravano, 2000; Ravichandran and Hovy, 2002) rely on
the abundance of web data to obtain easy patterns and
learn such patterns based mostly on lexical information.
Rather than analyzing dependency path between entity
pair proposed in (Bunescu and Mooney, 2006; Cui et al,
2005), our method analyzes a subtree derived from the
dependency structure. Such subtree contains more evi-
dence of the entities? inter-relation than the path in some
cases. We propose a new feature obtained from the sub-
tree by using a subtree-mining technique.
In addition, we also make use of the characteristics of
Wikipedia to allocate the mentions of entities and further
identify their types to help the relation extraction process.
2 Wikipedia?s Article Characteristics
Due to the encyclopedic style, each Wikipedia article
mainly provides information for a specific entity and fur-
ther mentions other entities related to it. Culotta et al
(2006) defines the entities as principal entity and sec-
ondary entity respectively. We predict only relationships
between the principal entity and each mentioned sec-
ondary entity that contains a link to its descriptive article.
We put some assumptions in this study: a relation-
ship can be expressed completely in one sentence. Fur-
thermore, a relationship between an entity pair might be
expressed with the implication of the principal entity in
some cases. Thus, for an article, only sentences contain-
ing at least a secondary entity are necessarily analyzed.
An interesting characteristic of Wikipedia is the cate-
gory hierarchy that is used to classify articles according to
their content. Additionally, those articles for famous en-
tities provide summary sections on their right side, which
are created by human editors. Finally, the first sentence
of an article often defines the principal entity.
3 Proposed Method
Figure 1 delineates our framework for relation extrac-
tion. First, Wikipedia articles are processed to remove
HTML tags and to extract hyperlinks that point to other
Wikipedia articles. Raw text is submitted to a pipeline
including a Sentence Splitter, a Tokenizer and a Phrase
Chunker supplied by the OpenNLP 1 tool set. The in-
stances of the principal entity and secondary entities are
then anchored in the articles. The Secondary Entity De-
tector simply labels the appropriate surface texts of the
hyperlinks to other Wikipedia articles, which are proper
1http://opennlp.sourceforge.net/
125
Figure 1: System framework
nouns as secondary entities. The Principal Entity Detec-
tor will be explained in the following subsection.
After the entities are anchored, sentences that include
at least one mention of secondary entities will be selected
by a Sentence Detector. Each mention of the secondary
entities is considered as a relation candidate between the
underlying entity and the principal entity. Secondary en-
tities are always explicit, although the principal entity is
sometimes implicit in sentences containing no mention.
Keywords that provide clues for each relation label will
be identified by a Keyword Extractor. Parallely, an Entity
Classifier module classifies the entities into types. The
Relation Extractor extracts subtree feature from a pair of
the principal entity and a mention of secondary entity. It
then incorporates subtree feature together with entity type
feature into a feature vector and classifies relations of the
entity pair using SVM-based classifiers.
3.1 Principal Entity Detector
This module detects all referring expressions of the prin-
cipal entity in an article. All occurrences of identified
expressions are labeled as mentions of the principal en-
tity. We adopt (Morton, 2000) to classify the expressions
into three types: (1) personal pronoun (2) proper noun
(3) common nouns. Based on chunking information, we
propose a simple technique to identify a set of referring
expressions of the principal entity, denoted as F:
(i) Start with F = {}.
(ii) Select the first two chunks for F: the proper chunk
(nounphase with at least one proper noun) of the article
title and the first proper chunk in the first sentence of the
article, if any. If F is still empty, stop.
(iii) For each remaining proper chunk p in the article, if
p is derived from any expressions selected in (ii), then
F ? p. Proper chunk p1 is derived from proper chunk p2
if all its proper nouns appear in p2.
(iv) In the article, select c as the most frequent subjective
pronouns, find c? as its equivalent objective pronoun and
add them to F.
(v) For each chunk p with the pattern [DT N1 . . . Nk]
where DT is a determiner and Nk?s are a common nouns,
if p appears more frequently than all the selected pro-
nouns in (iv), then F ? p.
Table 1: Sample extracted referring expressions
Article Referring expressions Step
[NP Bill/NNP Gates/NNP ] (ii)
[NP William/NNP H./NNP Gates/NNP ] (ii)
Bill Gates [NP Gates/NNP ] (iii)
[NP The/DT Gates/NNP ] (iii)
[NP he/PRP ] (iv)
[NP him/PRP ] (iv)
[NP Microsoft/NNP ] (ii)
[NP The/DT Microsoft/NNP Corporation/NNP ] (ii)
Microsoft [NP that/DT Microsoft/NNP ] (iii)
[NP It/PRP ] (iv)
[NP the/DT company/NN ] (v)
[NP Microsoft/NNP Windows/NNP ] (ii)
Microsoft [NP Microsoft/NNP ] (iii)
Windows [NP Windows/NNP ] (iii)
[NP the/DT Windows/NNP ] (iii)
[NP it/PRP ] (iv)
Table 2: List of relations and their keywords
Relation Keywords
CEO CEO, chief, executive, officer
Chairmans chairman
COO coo, chief, operating, officer
Director director
Founder found, founder, founded, establish, form, foundation, open
President president
Vice
chairman
vice, chairman
Birth date born, bear, birth, birthday
Birth
place
born, bear
Foundation found, establish, form, founded, open, create, formed, estab-
lished, foundation, founding, cofounder, founder
Location headquartered, based, locate, headquarter, base, location, situate,
located
Product product, include, release, produce, service, operate, provide,
market, manage, development, focus, manufacture, provider,
launch, make, sell, introduce, producer, supplier, possess, re-
tailer, design, involve, production, offering, serve, sale, supply
Spouse marry, wife, married, husband, marriage
Table 1 shows some extracted referring expressions.
The third column indicates in which step the expressions
are selected. Supported by the nature of Wikipedia, our
technique provides better results than those of the coref-
erence tool in LingPipe library 2 and OpenNLP tool set.
3.2 Entity Classifier
Entity type is very useful for relation extraction. For in-
stance, the relation label between a person and an orga-
nization should be founder, chairman, etc., but cannot
be spouse, product, etc. We first identify year, month
and date entities by directly examining their surface text.
Types of other entities are identified by classifying their
corresponding articles. We develop one SVM-based clas-
sifier for each remaining type using the following fea-
tures: category feature (categories collected when trac-
ing from the article upto k level of its category structure),
pronoun feature (the most frequent subjective pronoun
in the article) and singular noun feature (singular nouns
of the first sentence of the article).
3.3 Keyword Extractor
Our hypothesis in this research is that there exist some
keywords that provide clues for the relationship between
2http://www.alias-i.com/lingpipe/index.html
126
Figure 2: Dependency trees in (a) & (b); core trees with respect to CEO relationship in (c) & (d); new representation
of the core trees in (e) & (f); common subtree in (g). The red phrase EP denotes the principal entity; the blue phrase
ES denotes the secondary entity.
a pair. For example, to express the founder relation, a
sentence should contain one keyword such as: found,
founder, founded, co-founders, or establish, etc. We iden-
tify such keywords by using a semi-automatic method.
First, we automatically extract some true relations from
summary sections of Wikipedia articles. Then, we map
entities in such relations to those in sentences to build
sample sentences for each relationship . Tf-idf model is
exploited to measure the relevance of words to each re-
lationship for those on the dependency path between the
entity pair. Finally, we choose the keywords manually
from lists of candidates ranked by relevance score with
respect to each relation. Table 2 shows our result selected
from ranked lists of total 35,820 keyword candidates us-
ing only one hour of human labor.
3.4 Subtree Feature from Dependency Path
In this subsection, we will describe how to obtain effi-
cient features for extracting relation using subtree min-
ing. We extend the idea of Bunescu et al (Bunescu and
Mooney, 2006) suggesting the analysis of dependency
path between the entities for extracting relation, in that
paths between the secondary entity and the keywords of r
will be added to the dependency path between the entities
to create a tree. The expanded tree is defined as core tree
of r because it attempts to capture the clues for r. Steps to
extract the core treeC of a relationship r from a sentence
s are described as follows:
(i)] Initialize the core tree C as blank.
(ii) Derive the dependency tree D from s.
(iii) Label the group of nodes corresponding to words of
secondary entity by an ES node in D.
(iv) If the principal entity appears in s, apply (iii) to re-
place principal entity with EP. Then extract P0 as shortest
path from ES to EP in D and add P0 ?C.
(v) For each keyword w of r, extract Pw as the shortest
path from ES to node of w and add Pw ?C.
Figures 2c & 2d present exemplary core trees of CEO
relationship derived from the dependency trees in Figures
2a & 2b. To analyze both words and relations of a core
tree uniformly, we transform it into a uniform graph for-
mat (Figures 2e & 2f) in which core tree words and rela-
tions are also represented as graph nodes.
We define a basic element of a relationship r as a key
pattern that commonly appears in various core trees of r.
As an example, the core trees in Figures 2e & 2f share
a common pattern in Figure 2g. Intuitively, this subtree
shares the core trees of sentences that express the idea of
?joined the company as CEO? or ?joined the company
and do something as CEO?.
We denote T = (V , E) as a directed tree, in which
V is a set of nodes and E is a set of directed edges.
Node y is an ancestor of node x, denoted by x ? y,
if (x,y) ? E or ?i1, ..., ik (k ? N and k ? 1) such that
(x, i1),(i1, i2), ...,(ik?1, ik),(ik,y) ? E. We define that a
tree S = (VS, ES) is a subtree of T if and only if: (i)VS ?V ,
and (ii) ?(x,y) ? ES, we have x? y in T .
We use a subtree as a feature for relation extraction.
From a set of training sentences with respect to a relation-
ship r, we derive the core trees. A frequent tree-mining
algorithm (Zaki, 2002) is used to generate subtrees from
that set of core trees to form the feature space. Each
mined subtree corresponds to a value of the feature.
4 Experiments and Evaluations
In this experiment, 5,975 articles are selected, in which
45 articles are for testing and 5,930 articles for train-
ing. We apply the framework in Figure 1 on the train-
ing articles to extract keywords and select relation candi-
dates. Subsequently, 3,833 positive instances (each con-
tains at least one relation) and 805 negative instances (the
ones containing no relation) from the candidates are an-
notated to train the Relation Extractor. Among 39,467
127
Table 3: Compare our proposed system and baselines
Precision(%) Recall(%) F1(%)
B0 8.70 22.26 12.51
B1 9.88 25.31 14.21
DepTree 29.07 53.86 37.76
Table 4: Result of Entity Classifier with various levels (k
value) of exploited category structure
Depth k(%) Accuracy(%)
1 64.0
2 69.5
3 81.0
4 81.5
5 79.5
6 77.5
7 77.0
8 78.0
9 75.0
10 74.5
entities collected from all principal and secondary enti-
ties, we randomly select 3,300 entities and manually an-
notate their types for the Entity Classifier. Finally, we use
3,100 entities for training and 200 entities for testing.
We develop two baseline systems to evaluate our
method, which use bag-of-words model. The second sys-
tem (B1 in Table 3) works like the Keyword Extractor
on training instances in that it calculates tf-idf scores for
words on the dependency path between the entities with
respect to each relation. During testing, it accumulates
tf-idf scores of words on the path and chooses the relation
label that gives the highest score for the entity pair. The
only difference between the two baseline systems is that
the first one (B0 in Table 3) focuses on all the words be-
tween the entities in sentence text, not dependency path.
In our experiments, dependency graphs are obtained
by Minipar parser (Lin, 1998), classifiers are trained by
SVM Light (Joachims, 1999) with 2nd- order polynomial
kernel, subtrees are mined by FREQT 3 tree miner.
On the basis of preliminary experiments, we report the
performance of our system compared with those of base-
line systems in Table 3. The result shows that our pro-
posed method gives a substantial improvement over the
baselines. Although the recall is quite adequate, preci-
sion is low. Data analysis reveals that although the mined
subtrees capture key features for relationships, they also
generate many irrelevant features which degrade the per-
formance. It is necessary to carry out feature selection
step for subtree feature. One more reason of the poor
precision is that our system suffers from the error accu-
mulation in a long pipeline of entity detection, entity clas-
sification, dependency parsing and relation classification.
Table 4 shows the effectiveness of different values of k
parameter in Entity Classifier. The classifier works best
when we trace four levels on category system. An inter-
esting fact is that Wikipedia can be used as an external
3http://chasen.org/t?aku/software/freqt/
knowledge source for Named Entity Recognition.
5 Conclusions and Future Works
We have presented a method to extract relations between
entities from Wikipedia articles by incorporating infor-
mation from the Wikipedia structure and by the analysis
of Wikipedia text. The key features of our method in-
clude: (1) an algorithm to build the core syntactic tree
that reflects the relation between a given entity pair more
accurately; (2) the use of a tree-mining algorithm to iden-
tify the basic elements of syntactic structure of sentences
for relationships; (3) method to make use of the nature of
Wikipedia for entity allocation and entity classification.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In
the 5th ACM International Conference on Digital Li-
braries.
S. Brin. 1998. Extracting patterns and relations from
the world wide web. In Proceedings of the 1998 Inter-
national Workshop on the Web and Databases, pages
172?183.
R.C. Bunescu and R.J. Mooney. 2006. Extracting rela-
tions from text: From word sequences to dependency
paths. In Anne Kao and Steve Poteet, editors, Text
Mining and Natural Language Processing.
H. Cui, R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua.
2005. Question answering passage retrieval using de-
pendency relations. In Proceedings of SIGIR.
A. Culotta, A. McCallum, and J. Betz. 2006. Integrating
probabilistic extraction models and data mining to dis-
cover relations and patterns in text. In Proceedings of
the HLT-NAACL-2006.
T. Joachims. 1999. Making large-scale svm learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT-Press.
D. Lin. 1998. Dependency-based evaluation of minipar.
In Proceedings of the Workshop on the Evaluation of
Parsing Systems, 1st International Conference on Lan-
guage Resources and Evaluation.
T. Morton. 2000. Coreference for nlp applications. In
Proceedings of the ACL-2000.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of the ACL-2002, pages 41?47.
M.J. Zaki. 2002. Efficiently mining frequent trees in a
forest. In Proceedings of 8th ACM SIGKDD.
128
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 542?550,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Graph-based Word Clustering using a Web Search Engine
Yutaka Matsuo
National Institute of Advanced
Industrial Science and Technology
1-18-13 Sotokanda, Tokyo 101-0021
y.matsuo@aist.go.jp
Takeshi Sakaki
University of Tokyo
7-3-1 Hongo
Tokyo 113-8656
Ko?ki Uchiyama
Hottolink Inc.
2-11-17 Nishi-gotanda
Tokyo 141-0031
uchi@hottolink.co.jp
Mitsuru Ishizuka
University of Tokyo
7-3-1 Hongo
Tokyo 113-8656
ishizuka@i.u-tokyo.ac.jp
Abstract
Word clustering is important for automatic
thesaurus construction, text classification,
and word sense disambiguation. Recently,
several studies have reported using the
web as a corpus. This paper proposes
an unsupervised algorithm for word clus-
tering based on a word similarity mea-
sure by web counts. Each pair of words
is queried to a search engine, which pro-
duces a co-occurrence matrix. By calcu-
lating the similarity of words, a word co-
occurrence graph is obtained. A new kind
of graph clustering algorithm called New-
man clustering is applied for efficiently
identifying word clusters. Evaluations are
made on two sets of word groups derived
from a web directory and WordNet.
1 Introduction
The web is a good source of linguistic informa-
tion for several natural language techniques such
as question answering, language modeling, and
multilingual lexicon acquisition. Numerous stud-
ies have examined the use of the web as a corpus
(Kilgarriff, 2003).
Web-based models perform especially well
against the sparse data problem: Statistical tech-
niques perform poorly when the words are rarely
used. For example, F. Keller et al (2002) use the
web to obtain frequencies for unseen bigrams in
a given corpus. They count for adjective-noun,
noun-noun, and verb-object bigrams by querying
a search engine, and demonstrate that web fre-
quencies (web counts) correlate with frequencies
from a carefully edited corpus such as the British
National Corpus (BNC). Aside from counting bi-
grams, various tasks are attainable using web-
based models: spelling correction, adjective order-
ing, compound noun bracketing, countability de-
tection, and so on (Lapata and Keller, 2004). For
some tasks, simple unsupervised models perform
better when n-gram frequencies are obtained from
the web rather than from a standard large corpus;
the web yields better counts than the BNC.
The web is an excellent source of information
on new words. Therefore, automatic thesaurus
construction (Curran, 2002) offers great potential
for various useful NLP applications. Several stud-
ies have addressed the extraction of hypernyms
and hyponyms from the web (Miura et al, 2004;
Cimiano et al, 2004). P. Turney (2001) presents a
method to recognize synonyms by obtaining word
counts and calculating pointwise mutual informa-
tion (PMI). For further development of automatic
thesaurus construction, word clustering is benefi-
cial, e.g. for obtaining synsets. It also contributes
to word sense disambiguation (Li and Abe, 1998)
and text classification (Dhillon et al, 2002) be-
cause the dimensionality is reduced efficiently.
This paper presents an unsupervised algorithm
for word clustering based on a word similarity
measure by web counts. Given a set of words, the
algorithm clusters the words into groups so that
the similar words are in the same cluster. Each pair
of words is queried to a search engine, which re-
sults in a co-occurrence matrix. By calculating the
similarity of words, a word co-occurrence graph
is created. Then, a new kind of graph clustering
algorithm, called Newman clustering, is applied.
Newman clustering emphasizes betweenness of an
edge and identifies densely connected subgraphs.
To the best of our knowledge, this is the first
attempt to obtain word groups using web counts.
Our contributions are summarized as follows:
542
? A new algorithm for word clustering is de-
scribed. It has few parameters and thus is
easy to implement as a baseline method.
? We evaluate the algorithm on two sets of
word groups derived from a web directory
and WordNet. The chi-square measure and
Newman clustering are both used in our al-
gorithm, they are revealed to outperform PMI
and hierarchical clustering.
We target Japanese words in this paper. The re-
mainder of this paper is organized as follows: We
overview the related studies in the next section.
Our proposed algorithm is described in Section 3.
Sections 4 and 5 explain evaluations and advance
discussion. Finally, we conclude the paper.
2 Related Works
A number of studies have explained the use of
the web for NLP tasks e.g., creating multilingual
translation lexicons (Cheng et al, 2004), text clas-
sification (Huang et al, 2004), and word sense dis-
ambiguation (Turney, 2004). M. Baroni and M.
Ueyama summarize three approaches to use the
web as a corpus (Baroni and Ueyama, 2005): us-
ing web counts as frequency estimates, building
corpora through search engine queries, and crawl-
ing the web for linguistic purposes. Commercial
search engines are optimized for ordinary users.
Therefore, it is desirable to crawl the web and to
develop specific search engines for NLP applica-
tions (Cafarella and Etzioni, 2005). However, con-
sidering that great efforts are taken in commercial
search engines to maintain quality of crawling and
indexing, especially against spammers, it is still
important to pursue the possibility of using the
current search engines for NLP applications.
P. Turney (Turney, 2001) presents an unsu-
pervised learning algorithm for recognizing syn-
onyms by querying a web search engine. The
task of recognizing synonyms is, given a target
word and a set of alternative words, to choose the
word that is most similar in meaning to the tar-
get word. The algorithm uses pointwise mutual
information (PMI-IR) to measure the similarity of
pairs of words. It is evaluated using 80 synonym
test questions from the Test of English as a Foreign
Language (TOEFL) and 50 from the English as a
Second Language test (ESL). The algorithm ob-
tains a score of 74%, contrasted to that of 64% by
Latent Semantic Analysis (LSA). Terra and Clarke
(Terra and Clarke, 2003) provide a comparative in-
vestigation of co-occurrence frequency estimation
on the performance of synonym tests. They report
that PMI (with a certain window size) performs
best on average. Also, PMI-IR is useful for cal-
culating semantic orientation and rating reviews
(Turney, 2002).
As described, PMI is one of many measures to
calculate the strength of word similarity or word
association (Manning and Schu?tze, 2002). An
important assumption is that similarity between
words is a consequence of word co-occurrence, or
that the proximity of words in text is indicative of
relationship between them, such as synonymy or
antonymy. A commonly used technique to obtain
word groups is distributional clustering (Baker and
McCallum, 1998). Distributional clustering of
words was first proposed by Pereira Tishby & Lee
in (Pereira et al, 1993): They cluster nouns ac-
cording to their conditional verb distributions.
Graphic representations for word similarity
have also been advanced by several researchers.
Kageura et al (2000) propose automatic thesaurus
generation based on a graphic representation. By
applying a minimum edge cut, the corresponding
English terms and Japanese terms are identified
as a cluster. Widdows and Dorow (2002) use a
graph model for unsupervised lexical acquisition.
A graph is produced by linking pairs of words
which participate in particular syntactic relation-
ships. An incremental cluster-building algorithm
achieves 82% accuracy at a lexical acquisition
task, evaluated against WordNet classes. Another
study builds a co-occurrence graph of terms and
decomposes it to identify relevant terms by dupli-
cating nodes and edges (Tanaka-Ishii and Iwasaki,
1996). It focuses on transitivity: if transitivity
does not hold between three nodes (e.g., if edge
a-b and b-c exist but edge a-c does not), the nodes
should be in separate clusters.
A network of words (or named entities) on the
web is investigated also in the context of the Se-
mantic Web (Cimiano et al, 2004; Bekkerman and
McCallum, 2005). Especially, a social network of
persons is mined from the web using a search en-
gine (Kautz et al, 1997; Mika, 2005; Matsuo et
al., 2006). In these studies, the Jaccard coefficient
is often used to measure the co-occurrence of enti-
ties. We compare Jaccard coefficients in our eval-
uations.
In the research field on complex networks,
543
Table 1: Web counts for each word.
printer print InterLaser ink TV Aquos Sharp
17000000 103000000 215 18900000 69100000 1760000000 2410000 186000000
Table 2: Co-occurrence matrix by web counts.
printer print InterLaser ink TV Aquos Sharp
printer ? 4780000 179 4720000 4530000 201000 990000
print 4780000 ? 183 4800000 8390000 86400 1390000
InterLaser 179 183 ? 116 65 0 0
ink 4720000 4800000 116 ? 10600000 144000 656000
TV 4530000 8390000 65 10600000 ? 1660000 42300000
Aquos 201000 86400 0 144000 1660000 ? 1790000
Sharp 990000 1390000 0 656000 42300000 1790000 ?
structures of various networks are investigated in
detail. For example, Motter (2002) targeted a
conceptual network from a thesaurus and demon-
strated its small-world structure. Recently, nu-
merous works have identified communities (or
densely-connected subgraphs) from large net-
works (Newman, 2004; Girvan and Newman,
2002; Palla et al, 2005) as explained in the next
section.
3 Word Clustering using Web Counts
3.1 Co-occurrence by a Search Engine
A typical word clustering task is described as fol-
lows: given a set of words (nouns), cluster words
into groups so that the similar words are in the
same cluster 1. Let us take an example. As-
sume a set of words is given: ???? (printer),
?? (print), ???????? (InterLaser), ?
?? (ink), TV (TV), Aquos (Aquos), and Sharp
(Sharp). Apparently, the first four words are re-
lated to a printer, and the last three words are re-
lated to a TV 2. In this case, we would like to have
two word groups: the first four and the last three.
We query a search engine3 to obtain word
counts. Table 1 shows web counts for each word.
Table 2 shows the web counts for pairs of words.
For example, we submit a query printer AND In-
terLaser to a search engine, and are directed to 179
documents. Thereby, nC2 queries are necessary to
obtain the matrix if we have n words. We call Ta-
ble 2 a co-occurrence matrix.
We can calculate the pointwise mutual informa-
1In this paper, we limit our scope to clustering nouns. We
discuss the extension in Section 4.
2InterLaser is a laser printer made by Epson Corp. Aquos
is a liquid crystal TV made by Sharp Corp.
3Google (www.google.co.jp) is used in our study.
tion between word w1 and w2 as
PMI(w1, w2) = log2
p(w1, w2)
p(w1)p(w2)
.
Probability p(w1) is estimated by fw1/N , where
fw1 represents the web count of w1 and N repre-
sents the number of documents on the web. Prob-
ability of co-occurrence p(w1, w2) is estimated by
fw1,w2/N where fw1,w2 represents the web count
of w1 AND w2.
The PMI values are shown in Table 3. We set
N = 1010 according to the number of indexed
pages on Google. Some values are inconsistent
with our intuition: Aquos is inferred to have high
PMI to TV and Sharp, but also to printer. None
of the words has high PMI with TV. These are be-
cause the range of the word count is broad. Gen-
erally, mutual information tends to provide a large
value if either word is much rarer than the other.
Various statistical measures based on co-
occurrence analysis have been proposed for es-
timating term association: the DICE coefficient,
Jaccard coefficient, chi-square test, and the log-
likelihood ratio (Manning and Schu?tze, 2002). In
our algorithm, we use the chi-square (?2) value in-
stead of PMI. The chi-square value is calculated as
follows: We denote the number of pages contain-
ing both w1 and w2 as a. We also denote b, c, d as
follows4.
w2 ?w2
w1 a b
?w1 c d
Thereby, the expected frequency of (w1, w2) is
(a+ c)(a+ b)/N . Eventually, chi-square is calcu-
lated as follows (Manning and Schu?tze, 2002).
4Note that N = a + b + c + d.
544
Table 3: A matrix of pointwise mutual information.
printer print InterLaser ink TV Aquos Sharp
printer ? 4.771 8.936 7.199 0.598 5.616 1.647
print 4.771 ? 6.369 4.624 -1.111 1.799 -0.463
InterLaser 8.936 6.369 ? 8.157 0.781 ??* ??*
ink 7.199 4.624 8.157 ? 1.672 4.983 0.900
TV 0.598 -1.111 0.781 1.672 ? 1.969 0.370
Aquos 5.616 1.799 ??*. 4.983 1.969 ? 5.319
Sharp 1.647 -0.463 ??* 0.900 0.370 5.319 ?
* represents that the PMI is not available because the co-occurrence web count is zero, in which case we set ??.
Table 4: A matrix of chi-square values.
printer print InterLaser ink TV Aquos Sharp
printer ? 6880482.6 399.2 5689710.7 0.0* 0.0* 0.0*
print 6880482.6 ? 277.8 3321184.6 176855.5 0.0* 0.0*
InterLaser 399.2 277.8 ? 44.8 0.0* 0.0 0.0
ink 5689710.7 3321184.6 44.8 ? 1419485.5 0.0* 0.0*
TV 0.0* 176855.5 0.0* 1419485.5 ? 26803.2 70790877.6
Aquos 0.0* 0.0* 0.0 0.0* 26803.2 ? 729357.7
Sharp 0.0* 0.0* 0.0 0.0* 70790877.6 729357.7 ?
* represents that the observed co-occurrence frequency is below the expected value, in which case we set 0.0.
Figure 1: Examples of Newman clustering.
?2(w1, w2)
= N ? (a? d? b? c)
2
(a + b)? (a + c)? (b + d)? (c + d)
However, N is a huge number on the web and
sometimes it is difficult to know exactly. There-
fore we regard the co-occurrence matrix as a con-
tingency table:
b? =
?
w?W ;w 6=w2
fw1,w , c
? =
?
w?W ;w 6=w1
fw2,w;
d? =
?
w,w??W ;w and w? 6=w1 nor w2
fw,w? , N ? =
?
w,w??W
fw,w? ,
where W represents a given set of words. Then
chi-square (within the word list W ) is defined as
?2W (w1, w2) =
N ? ? (a? d? ? b? ? c?)2
(a + b?)? (a + c?)? (b? + d?)? (c? + d?) .
We should note that ?2W depends on a word
set W . It calculates the relative strength of co-
occurrences. Table 4 shows the ?2W values. Aquos
has high values only with TV and Sharp as ex-
pected.
3.2 Clustering on Co-occurrence Graph
Recently, a series of effective graph clustering
methods has been advanced. Pioneering work that
specifically emphasizes edge betweenness was
done by Girvan and Newman (2002): we call the
method as GN algorithm. Betweenness of an edge
is the number of shortest paths between pairs of
nodes that run along it. Figure 1 (i) shows that
two ?communities? (in Girvan?s term), i.e. {a,b,c}
and {d,e,f,g}, which are connected by edge c-d.
Edge c-d has high betweenness because numerous
shortest paths (e.g., from a to d, from b to e, . . .)
traverse the edge. The graph is likely to be sepa-
rated into densely connected subgraphs if we cut
the high betweenness edge.
The GN algorithm is different from the mini-
mum edge cut. For (i), the results are identical: By
cutting edge c-d, which is a minimum edge cut, we
can obtain two clusters. However in case of (ii),
there are two candidates for the minimum edge
cut, whereas the highest betweenness edge is still
only edge c-d. Girvan et al (2002) shows that this
clustering works well to various networks from
biological to social networks. Numerous studies
have been inspired by that work. One prominent
effort is a faster variant of GN algorithm (New-
man, 2004), which we call Newman clustering in
545
Figure 2: An illustration of graph-based word
clustering.
this paper.
In Newman clustering, instead of explicitly cal-
culating high-betweenness edges (which is com-
putationally demanding), an objective function is
defined as follows:
Q =
?
i
(
eii ?
(
?
j
eij
)2)
(1)
We assume that we have separate clusters, and that
eij is the fraction5 of edges in the network that
connect nodes in cluster i to those in cluster j.
The term eii denotes the fraction of edges within
the clusters. The term
?
j eij represents the ex-
pected fraction of edges within the cluster. If a par-
5We can calculate eij using the number of edges between
cluster i and j divided by the number of all edges.
Figure 3: A word graph for 88 Japanese words.
ticular division gives no more within-community
edges than would be expected by random chance,
then we would obtain Q = 0. In practice, values
greater than about 0.3 appear to indicate signifi-
cant group structure (Newman, 2004).
Newman clustering is agglomerative (although
we can intuitively understand that a graph with-
out high betweenness edges is ultimately ob-
tained). We repeatedly join clusters together in
pairs, choosing at each step the joint that provides
the greatest increase in Q. Currently, Newman
clustering is one of the most efficient methods for
graph-based clustering.
The illustration of our algorithm is shown in
Fig. 2. First, we obtain web counts among a given
set of words using a search engine. Then PMI or
the chi-square values are calculated. If the value is
above a certain threshold6, we invent an edge be-
tween the two nodes. Then, we apply graph clus-
tering and finally identify groups of words. This il-
lustration shows that the chi-square measure yields
the correct clusters.
The algorithm is described in Fig. 4. The pa-
rameters are few: a threshold dthre for a graph and,
optionally, the number of clusters nc. This enables
easy implementation of the algorithm. Figure 3
is a small network of 88 Japanese words obtained
through 3828 search queries. We can see that some
parts in the graph are densely connected.
4 Experimental Results
This section addresses evaluation. Two sets of
word groups are used for the evaluation: one is
derived from documents on a web directory; an-
other is from WordNet. We first evaluate the co-
6In this example, 4.0 for PMI and 200 for ?2.
546
? ?
1. Input A set of words is given. The number of words
is denoted as n.
2. Obtain frequencies Put a query for each pair of
words to a search engine, and obtain a co-
occurrence matrix. Then calculate the chi-square
matrix (alternatively a PMI matrix, or a Jaccard
matrix.)
3. Make a graph Set a node for each word, and an
edge to a pair of nodes whose ?2 value is above a
threshold. The threshold is determined so that the
network density (the number of edges divided by
nC2) is dthre.
4. Apply Newman clustering Initially set each node
as a cluster. Then merge two clusters repeatedly
so that Q is maximized. Terminate if Q does
not increase anymore, or when a given number
of clusters nc is obtained. (Alternatively, apply
average-link hierarchical clustering.)
5. Output Output groups of words.
? ?
Figure 4: Our algorithm for word clustering.
occurrence measures, then we evaluate the cluster-
ing methods.
4.1 Word Groups from an Open Directory
We collected documents from the Japanese Open
Directory (dmoz.org/World/Japanese). The
dmoz japanese category contains about 130,000
documents and more than 10,000 classes. We
chose 9 categories out of the top 12 categories:
art, sports, computer, game, society, family, sci-
ence, and health. We crawled 1000 documents for
each category, i.e., 9000 documents in all.
For each category, a word group is obtained
through the procedure in Fig. 5. We consider
that the specific words to a category are relevant
to some extent, and that they can therefore be re-
garded as a word group. Examples are shown in
Table 5. In all, 90 word sets are obtained and
merged. We call the word set DMOZ-J data.
Our task is, given 90 words, to cluster the words
into the correct nine groups. Here we investigate
whether the correct nine words are selected for
each word using the co-occurrence measure. We
compare pointwise mutual information (PMI), the
Jaccard coefficient (Jaccard), and chi-square (?2).
We chose these methods for comparison because
PMI performs best in (Terra and Clarke, 2003).
The Jaccard coefficient is often used in social net-
work mining from the web. Table 7 shows the pre-
cision of each method. Experiments are repeated
five times. We keep each method that outputs the
? ?
1. For each category, crawl 1000 documents ran-
domlya
2. Apply the Japanese morphological analysis sys-
tem ChaSen (Matsumoto et al, 2000) to the doc-
uments. Calculate the score of each word w in
category c similarly to TF-IDF:
score(w, c) = fc(w)? log(Nall/fall(w))
where fc denotes the document frequency of
word w in category c, Nall denotes the number of
all documents, and fall(w) denotes the frequency
of word w in all documents.
3. For each category, the top 10 words are selected
as the word group.
aWe first get al urls, sort them, and select a sample
randomly.
? ?
Figure 5: Procedure for obtaining word groups for
a category.
Table 7: Precision for DMOZ-J set.
PMI Jaccard ?2
Mean 0.415 0.402 0.537
Min 0.396 0.376 0.493
Max 0.447 0.424 0.569
SD 0.020 0.020 0.032
highest nine words for each word, groups of ten
words. Therefore, recall is the same as the preci-
sion. From the table, the chi-square performs best.
PMI is slightly better than the Jaccard coefficient.
4.2 Word Groups from WordNet
Next, we make a comparison using WordNet 7. By
extracting 10 words that have the same hypernym
(i.e. coordinates), we produce a word group. Ex-
amples are shown in Table 6. Nine word groups
are merged into one, as with DMOZ-J. The exper-
iments are repeated 10 times. Table 8 shows the
result. Again, the chi-square performs best among
the methods that were compared.
Detailed analyses of the results revealed that
word groups such as bacteria and diseases are clus-
tered correctly. However, word groups such as
computers (in which homepage, server and client
are included) are not well clustered: these words
tend to be polysemic, which causes difficulty.
4.3 Evaluation of Clustering
We compare two clustering methods: Newman
clustering and average-link agglomerative cluster-
7We use a partly-translated version of WordNet.
547
Table 5: Examples of word groups from DMOZ-J.
category specific words to a category as a word group
??? (art) ?? (gallery),?? (artwork),?? (theater),???? (saxophone),?? (verse),??? (live con-
cert),??? (guitar),?? (performance),??? (ballet),?? (personal exhibition)
????????
(recreation)
?? (raising), ?? (poult), ????? (hamster), ??? (travel diary), ???? (national park),
?? (brewing),?? (boat race),?? (competition),??? (fishing pond)
?? (health) ?? (illness),?? (patient),?? (myositis),?? (surgery),?? (dialysis),????? (steroid),?
? (test),?? (medical ward),??? (collagen disease),?? (clinic)
Table 6: Examples of word groups from WordNet.
hypernym hyponyms as a word group
?? (gem) ????? (amethyst),?????? (aquamarine),?????? (diamond),????? (emer-
ald),??????? (moonstone),????? (peridot),??? (ruby),????? (sapphire),
???? (topaz),????? (tourmaline)
?? (academic field) ???? (natural science),?? (mathematics),?? (agronomics),??? (architectonics),??
? (geology),??? (psychology),???? (computer science),???? (cognitive science),?
?? (sociology),??? (linguistics)
??? (drink) ?? (milk),????? (alcohol),???? (cooling beverage),???? (carbonated beverage),
???? (soda),??? (cocoa),???????? (fruit juice),???? (coffee),?? (tea),?
???????? (mineral water)
Table 8: Precision of WordNet set.
PMI Jaccard ?2
Mean 0.549 0.484 0.584
Min 0.473 0.415 0.498
Max 0.593 0.503 0.656
SD 0.037 0.027 0.048
Table 9: Precision, recall and the F-measure for
each clustering.
PMI Jaccard ?2
Average precision 0.633 0.603 0.486
-link recall 0.102 0.101 0.100
F-measure 0.179 0.173 0.164
Newman precision 0.751 0.739 0.546
recall 0.103 0.103 0.431
F-measure 0.182 0.181 0.480
ing, which is often used in word clustering.
A word co-occurrence graph is created using
PMI, Jaccard, and chi-square measures. The
threshold is determined so that the network den-
sity dthre is 0.3. Then, we apply clustering to ob-
tain nine clusters; nc = 9. Finally, we compare
the resultant clusters with the correct categories.
Clustering results for DMOZ-J sets are shown
in Table 9. Newman clustering produces higher
precision and recall. Especially, the combination
of chi-square and Newman is the best in our ex-
periments.
5 Discussion
In this paper, the scope of co-occurrence is
document-wide. One reason is that major com-
mercial search engines do not support a type of
query w1 NEAR w2. Another reason is in (Terra
and Clarke, 2003) document-wide co-occurrences
perform comparable to other Windows-based co-
occurrences.
Many types of co-occurrence exist other than
noun-noun. We limit our scope to noun-noun
co-occurrences in this paper. Other types of co-
occurrence such as verb-noun can be investigated
in future studies. Also, co-occurrence for the
second-order similarity can be sought. Because
web documents are sometimes difficult to analyze,
we keep our algorithm as simple as possible. An-
alyzing semantic relations and applying distribu-
tional clustering is another goal for future work.
A salient weak point of our algorithm is the
number of necessary queries allowed to a search
engine. For obtaining a graph of n words, O(n2)
queries are required, which discourages us from
undertaking large experiments. However some de-
vices are possible: if we analyze the texts of the
top retrieved pages by query w, we can guess what
words are likely to co-occur with w. This prepro-
cessing seems promising at least in social network
extraction: we can eliminate 85% of queries in
the 500 nodes case while retaining more than 90%
precision (Asada et al, 2005).
In our evaluation, the chi-square measure per-
formed well. One reason is that the PMI performs
worse when a word group contains rare or frequent
words, as is generally known for mutual informa-
tion measure (Manning and Schu?tze, 2002). An-
other reason is that if we put one word and two
words to a search engine, the result might be in-
consistent. In an extreme case, the web count of
w1 is below the web count of w1ANDw2. This
548
phenomenon depends on how a search engine pro-
cesses AND operator, and results in unstable val-
ues for the PMI. On the other hand, our method
by the chi-square uses a co-occurrence matrix as a
contingency table. For that reason, it suffers less
from the problem. Other statistical measures such
as the likelihood ratio are also applicable.
6 Conclusion
This paper describes a new approach for word
clustering using a search engine. The chi-square
measure is used to overcome the broad range of
word counts for a given set of words. We also ap-
ply recently-developed Newman clustering, which
yields promising results through our evaluations.
Our algorithm has few parameters. Therefore,
it can be used easily as a baseline, as suggested by
(Lapata and Keller, 2004). New words are gener-
ated day by day on the web. We believe that to
automatically identify new words and obtain word
groups potentially enhances many NLP applica-
tions.
References
Yohei Asada, Yutaka Matsuo, and Mitsuru Ishizuka.
2005. Increasing scalability of researcher network
extraction from the web. Journal of Japanese Soci-
ety for Artificial Intelligence, 20(6).
D. Baker and A. McCallum. 1998. Distributional
clustering of words for text classification. In Proc.
SIGIR-98.
M. Baroni and M. Ueyama. 2005. Building general-
and special-purpose corpora by web crawling. In
Proc. NIJL International Workshop on Language
Corpora.
R. Bekkerman and A. McCallum. 2005. Disambiguat-
ing web appearances of people in a social network.
In Proc. WWW 2005.
M. Cafarella and O. Etzioni. 2005. A search engine for
natural language applications. In Proc. WWW2005.
P. Cheng, W. Lu, J. Teng, and L. Chien. 2004. Cre-
ating multilingual translation lexicons with regional
variations using web corpora. In Proc. ACL 2004,
pages 534?541.
P. Cimiano, S. Handschuh, and S. Staab. 2004. To-
wards the self-annotating web. In Proc. WWW2004,
pages 462?471.
J. Curran. 2002. Ensemble methods for automatic the-
saurus extraction. In Proc. EMNLP 2002.
I. Dhillon, S. Mallela, and R. Kumar. 2002. Enhanced
word clustering for hierarchical text classification.
In Proc. KDD-2002, pages 191?200.
Michelle Girvan and M. E. J. Newman. 2002. Com-
munity structure in social and biological networks.
Proceedings of National Academy of Sciences USA,
99:8271?8276.
C. Huang, S. Chuang, and L. Chien. 2004. Categoriz-
ing unknown text segments for information extrac-
tion using a search result mining approach. In Proc.
IJCNLP 2004, pages 576?586.
K. Kageura, K. Tsuji, and A. Aizawa. 2000. Auto-
matic thesaurus generation through multiple filter-
ing. In Proc. COLING 2000.
H. Kautz, B. Selman, and M. Shah. 1997. The hidden
Web. AI magazine, 18(2):27?35.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Using
the web to overcome data sparseness. In EMNLP-
02, pages 230?237.
A. Kilgarriff. 2003. Introduction to the special issue
on the web as corpus. Computer Linguistics, 29(3).
M. Lapata and F. Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised
web-based models for a range of nlp tasks. In Proc.
HLT-NAACL 2004, pages 121?128.
H. Li and N. Abe. 1998. Word clustering and dis-
ambiguation based on co-occurrence data. In Proc.
COLING-ACL98.
C. D. Manning and H. Schu?tze. 2002. Foundations
of statistical natural language processing. The MIT
Press, London.
Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hi-
rano, H. Matsuda, K. Takaoka, and M. Asahara.
2000. Morphological analysis system ChaSen ver-
sion 2.2.1 manual. Technical report, NIST.
Y. Matsuo, J. Mori, M. Hamasaki, H. Takeda,
T. Nishimura, K. Hasida, and M. Ishizuka. 2006.
POLYPHONET: An advanced social network ex-
traction system. In Proc. WWW 2006.
P. Mika. 2005. Flink: Semantic web technology for the
extraction and analysis of social networks. Journal
of Web Semantics, 3(2).
K. Miura, Y. Tsuruoka, and J. Tsujii. 2004. Auto-
matic acquisition of concept relations from web doc-
uments with sense clustering. In Proc. IJCNLP04.
A. Motter, A. de Moura, Y. Lai, and P. Dasgupta. 2002.
Topology of the conceptual network of language.
Physical Review E, 65.
M. Newman. 2004. Fast algorithm for detecting com-
munity structure in networks. Phys. Rev. E, 69.
549
G. Palla, I. Derenyi, I. Farkas, and T. Vicsek. 2005.
Uncovering the overlapping community structure of
complex networks in nature and society. Nature,
435:814.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In Proc. ACL93, pages
183?190.
K. Tanaka-Ishii and H. Iwasaki. 1996. Clustering co-
occurrence graph using transitivity. In Proc. 16th In-
ternational Conference on Computational Linguis-
tics, pages 680?585.
E. Terra and C. Clarke. 2003. Frequency estimates
for statistical word similarity measures. In Proc.
HLT/NAACL 2003.
P. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proc. ECML-2001,
pages 491?502.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. ACL?02, pages 417?424.
P. Turney. 2004. Word sense disambiguation by web
mining for word co-occurrence probabilities. In
Proc. SENSEVAL-3.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. COLING
2002.
550
