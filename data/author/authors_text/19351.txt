Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1764?1768,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Improved Decipherment of Homophonic Ciphers
Malte Nuhn and Julian Schamper and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper, we present two improve-
ments to the beam search approach for
solving homophonic substitution ciphers
presented in Nuhn et al. (2013): An im-
proved rest cost estimation together with
an optimized strategy for obtaining the or-
der in which the symbols of the cipher are
deciphered reduces the beam size needed
to successfully decipher the Zodiac-408
cipher from several million down to less
than one hundred: The search effort is re-
duced from several hours of computation
time to just a few seconds on a single CPU.
These improvements allow us to success-
fully decipher the second part of the fa-
mous Beale cipher (see (Ward et al., 1885)
and e.g. (King, 1993)): Having 182 differ-
ent cipher symbols while having a length
of just 762 symbols, the decipherment is
way more challenging than the decipher-
ment of the previously deciphered Zodiac-
408 cipher (length 408, 54 different sym-
bols). To the best of our knowledge, this
cipher has not been deciphered automati-
cally before.
1 Introduction
State-of-the-art statistical machine translation sys-
tems use large amounts of parallel data to estimate
translation models. However, parallel corpora are
expensive and not available for every domain.
Decipherment uses only monolingual data to
train a translation model: Improving the core deci-
pherment algorithms is an important step for mak-
ing decipherment techniques useful for training
practical machine translation systems.
In this paper we present improvements to the
beam search algorithm for deciphering homo-
phonic substitution ciphers as presented in Nuhn
et al. (2013). We show significant improvements
in computation time on the Zodiac-408 cipher and
show the first decipherment of part two of the
Beale ciphers.
2 Related Work
Regarding the decipherment of 1:1 substitution ci-
phers, various works have been published: Most
older papers do not use a statistical approach and
instead define some heuristic measures for scoring
candidate decipherments. Approaches like Hart
(1994) and Olson (2007) use a dictionary to check
if a decipherment is useful. Clark (1998) defines
other suitability measures based on n-gram counts
and presents a variety of optimization techniques
like simulated annealing, genetic algorithms and
tabu search. On the other hand, statistical ap-
proaches for 1:1 substitution ciphers are published
in the natural language processing community:
Ravi and Knight (2008) solve 1:1 substitution ci-
phers optimally by formulating the decipherment
problem as an integer linear program (ILP) while
Corlett and Penn (2010) solve the problem using
A
?
search. Ravi and Knight (2011) report the
first automatic decipherment of the Zodiac-408 ci-
pher. They use a combination of a 3-gram lan-
guage model and a word dictionary. As stated in
the previous section, this work can be seen as an
extension of Nuhn et al. (2013). We will there-
fore make heavy use of their definitions and ap-
proaches, which we will summarize in Section 3.
3 General Framework
In this Section we recap the beam search frame-
work introduced in Nuhn et al. (2013).
3.1 Notation
We denote the ciphertext with f
N
1
=
f
1
. . . f
j
. . . f
N
which consists of cipher
1764
tokens f
j
? V
f
. We denote the plain-
text with e
N
1
= e
1
. . . e
i
. . . e
N
(and its
vocabulary V
e
respectively). We define
e
0
= f
0
= e
N+1
= f
N+1
= $ with ?$?
being a special sentence boundary token. Homo-
phonic substitutions are formalized with a general
function ? : V
f
? V
e
. Following (Corlett and
Penn, 2010), cipher functions ?, for which not all
?(f)?s are fixed, are called partial cipher func-
tions. Further, ?
?
is said to extend ?, if for all
f ? V
f
that are fixed in ?, it holds that f is also
fixed in ?
?
with ?
?
(f) = ?(f). The cardinality
of ? counts the number of fixed f ?s in ?. When
talking about partial cipher functions we use the
notation for relations, in which ? ? V
f
? V
e
.
3.2 Beam Search
The main idea of (Nuhn et al., 2013) is to struc-
ture all partial ??s into a search tree: If a cipher
containsN unique symbols, then the search tree is
of height N . At each level a decision about the n-
th symbol is made. The leaves of the tree form full
hypotheses. Instead of traversing the whole search
tree, beam search descents the tree top to bottom
and only keeps the most promising candidates at
each level. Practically, this is done by keeping
track of all partial hypotheses in two arraysH
s
and
H
t
. During search all allowed extensions of the
partial hypotheses in H
s
are generated, scored and
put into H
t
. Here, the function EXT ORDER (see
Section 5) chooses which cipher symbol is used
next for extension, EXT LIMITS decides which ex-
tensions are allowed, and SCORE (see Section 4)
scores the new partial hypotheses. PRUNE then
selects a subset of these hypotheses. Afterwards
the array H
t
is copied to H
s
and the search pro-
cess continues with the updated arrayH
s
. Figure 1
shows the general algorithm.
4 Score Estimation
The score estimation function is crucial to the
search procedure: It predicts how good or bad a
partial cipher function ?might become, and there-
fore, whether it?s worth to keep it or not.
To illustrate how we can calculate these scores,
we will use the following example with vocabular-
ies V
f
= {A,B,C,D}, V
e
= {a, b, c, d}, exten-
sion order (B,C,A,D), and cipher text
1
?(f
N
1
) = $ ABDD CABC DADC ABDC $
1
We include blanks only for clarity reasons.
1: function BEAM SEARCH(EXT ORDER)
2: init sets H
s
, H
t
3: CARDINALITY = 0
4: H
s
.ADD((?, 0))
5: while CARDINALITY < |V
f
| do
6: f = EXT ORDER[CARDINALITY]
7: for all ? ? H
s
do
8: for all e ? V
e
do
9: ?
?
:= ? ? {(e, f)}
10: if EXT LIMITS(?
?
) then
11: H
t
.ADD(?
?
,SCORE (?
?
))
12: end if
13: end for
14: end for
15: PRUNE(H
t
)
16: CARDINALITY = CARDINALITY + 1
17: H
s
= H
t
18: H
t
.CLEAR()
19: end while
20: return best scoring cipher function in H
s
21: end function
Figure 1: The general structure of the beam search
algorithm for decipherment of substitution ciphers
as presented in Nuhn et al. (2013). This paper im-
proves the functions SCORE and EXT ORDER.
and partial hypothesis ? = {(A, a), (B, b)}. This
yields the following partial decipherment
?(f
N
1
) = $ ab.. .ab. .a.. ab.. $
The score estimation function can only use this
partial decipherment to calculate the hypothesis?
score, since there are not yet any decisions made
about the other positions.
4.1 Baseline
Nuhn et al. (2013) present a very simple rest
cost estimator, which calculates the hypothesis?
score based only on fully deciphered n-grams, i.e.
those parts of the partial decipherment that form a
contiguous chunk of n deciphered symbols. For
all other n-grams containing not yet deciphered
symbols, a trivial estimate of probability 1 is as-
sumed, making it an admissible heuristic. For the
above example, this baseline yields the probability
p(a|$) ? p(b|a) ? 1
4
? p(b|a) ? 1
6
? p(b|a) ? 1
2
. The
more symbols are fixed, the more contiguous n-
grams become available. While being easy and ef-
ficient to compute, it can be seen that for example
the single ?a? is not involved in the computation of
1765
the score at all. In practical decipherment, like e.g.
the Zodiac-408 cipher, this forms a real problem:
While making the first decisions?i.e. traversing
the first levels of the search tree?only very few
terms actually contribute to the score estimation,
and thus only give a very coarse score. This makes
the beam search ?blind? when not many symbols
are deciphered yet. This is the reason, why Nuhn
et al. (2013) need a large beam size of several mil-
lion hypotheses in order to not lose the right hy-
pothesis during the first steps of the search.
4.2 Improved Rest Cost Estimation
The rest cost estimator we present in this paper
solves the problem mentioned in the previous sec-
tion by also including lower order n-grams: In the
example mentioned before, we would also include
unigram scores into the rest cost estimate, yielding
a score of p(a|$)?p(b|a)?1
3
?p(a)?p(b|a)?1
2
?p(a)1
2
?
p(a) ? p(b|a) ? 1
2
. Note that this is not a simple lin-
ear interpolation of different n-gram trivial scores:
Each symbol is scored only using the maximum
amount of context available. This heuristic is non-
admissible, since an increased amount of context
can always lower the probabilty of some symbols.
However, experiments show that this score estima-
tion function works great.
5 Extension Order
Besides having a generally good scoring function,
also the order in which decisions about the cipher
symbols are made is important for obtaining reli-
able cost estimates. Generally speaking we want
an extension order that produces partial decipher-
ments that contain useful information to decide
whether a hypothesis is worth being kept or not
as early as possible.
It is also clear that the choice of a good ex-
tension order is dependent on the score estima-
tion function SCORE. After presenting the previ-
ous state of the art, we introduce a new extension
order optimized to work together with our previ-
ously introduced rest cost estimator.
5.1 Baseline
In (Nuhn et al., 2013), two strategies are pre-
sented: One which at each step chooses the most
frequent remaining cipher symbol, and another,
which greedily chooses the next symbol to max-
imize the number of contiguously fixed n-grams
in the ciphertext.
LM order
Perplexity
Zodiac-408 Beale Pt. 2
1 19.49 18.35
2 14.09 13.96
3 12.62 11.81
4 11.38 10.76
5 11.19 9.33
6 10.13 8.49
7 10.15 8.27
8 9.98 8.27
Table 1: Perplexities of the correct decipherment
of Zodiac-408 and part two of the Beale ciphers
using the character based language model used in
beam search. The language model was trained on
the English Gigaword corpus.
5.2 Improved Extension Order
Each partial mapping ? defines a partial decipher-
ment. We want to choose an extension order such
that all possible partial decipherments following
this extension order are as informative as possible:
Due to that, we can only use information about
which symbols will be deciphered, not their actual
decipherment. Since our heuristic is based on n-
grams of different orders, it seems natural to evalu-
ate an extension order by counting how many con-
tiguously deciphered n-grams are available: Our
new strategy tries to find an extension order op-
timizing the weighted sum of contiguously deci-
phered n-gram counts
2
N
?
n=1
w
n
?#
n
.
Here n is the n-gram order, w
n
the weight for or-
der n, and #
n
the number of positions whose max-
imum context is of size n.
We perform a beam search over all possible
enumerations of the cipher vocabulary: We start
with fixing only the first symbol to decipher. We
then continue with the second symbol and evalu-
ate all resulting extension orders of length 2. In
our experiments, we prune these candidates to the
100 best ones and continue with length 3, and so
on.
Suitable values for the weights w
n
have to be
chosen. We try different weights for the different
2
If two partial extension orders have the same score after
fixing n symbols, we fall back to comparing the scores of
the partial extension orders after fixing only the first n ? 1
symbols.
1766
i02
h
08
a
03
v
01
e
05
d
09
e
07
p
03
o
07
s
10
i
11
t
03
e
14
d
03
i
03
n
05
t
06
h
01
e
13
c
04
o
10
u
01
n
01
t
04
y
01
o
12
f
04
b
04
e
15
d
09
f
03
o
04
r
06
d
04
a
07
b
07
o
09
u
03
t
13
f
01
o
01
u
08
r
05
m
03
i
08
l
09
e
14
s
06
f
01
r
05
o
07
m
04
b
06
u
02
f
04
o
10
r
07
d
01
s
11
i
03
n
02
a
06
n
03
e
05
x
01
c
03
a
01
v
01
a
03
t
10
i
13
o
03
n
05
o
08
r
06
v
01
a
08
u
03
l
01
t
11
s
12
i
04
x
01
f
01
e
01
e
03
t
02
b
06
e
07
l
02
o
11
w
06
t
08
h
08
e
15
s
06
u
04
r
06
f
04
a
10
.
.
.
p
04
a
14
p
01
e
07
r
05
n
02
u
02
m
02
b
01
e
14
r
05
o
03
n
05
e
15
d
10
e
01
s
01
c
01
r
01
i
03
b
05
e
06
s
08
t
01
h
08
c
04
e
10
x
01
a
14
c
07
t
02
l
09
o
12
c
02
a
04
l
09
i
13
t
02
y
01
o
02
f
03
t
07
h
02
e
11
v
01
a
10
r
07
l
07
t
11
s
09
o
04
t
01
h
03
a
06
t
04
n
03
o
06
d
05
i
13
f
02
f
03
i
03
c
04
u
07
l
09
t
02
y
01
w
04
i
12
l
01
l
02
b
03
e
01
h
02
a
09
d
10
i
07
n
06
f
01
i
13
n
01
d
10
i
03
n
05
g
04
i
03
t
05
Table 2: Beginning and end of part two of the Beale cipher. Here we show a relabeled version of the ci-
pher, which encodes knowledge of the gold decipherment to assign reasonable names to all homophones.
The original cipher just consists of numbers.
orders on the Zodiac-408 cipher with just a beam
size of 26. With such a small beam size, the exten-
sion order plays a crucial role for a successful de-
cipherment: Depending on the choice of the differ-
ent weights w
n
we can observe decipherment runs
with 3 out of 54 correct mappings, up to 52 out
of 54 mappings correct. Even though the choice
of weights is somewhat arbitrary, we can see that
generally giving higher weights to higher n-gram
orders yields better results.
We use the weights w
8
1
=
(0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0) for the
following experiments. It is interesting to com-
pare these weights to the perplexities of the
correct decipherment measured using different
n-gram orders (Table 5). However, at this point
we do not see any obvious connection between
perplexities and weights w
n
, and leave this as a
further research direction.
6 Experimental Evaluation
6.1 Zodiac Cipher
Using our new algorithm we are able to decipher
the Zodiac-408 with just a beam size of 26 and a
language model order of size 8. By keeping track
of the gold hypothesis while performing the beam
search, we can see that the gold decipherment in-
deed always remains within the top 26 scoring hy-
potheses. Our new algorithm is able to decipher
the Zodiac-408 cipher in less than 10s on a sin-
gle CPU, as compared to 48h of CPU time using
the previously published heuristic, which required
a beam size of several million. Solving a cipher
with such a small beam size can be seen as ?read-
ing off the solution?.
6.2 Beale Cipher
We apply our algorithm to the second part of the
Beale ciphers with a 8-gram language model.
Compared to the Zodiac-408, which has length
408 while having 54 different symbols (7.55 ob-
servations per symbol), part two of the Beale ci-
phers has length 762 while having 182 different
symbols (4.18 observations per symbol). Com-
pared to the Zodiac-408, this is both, in terms of
redundancy, as well as in size of search space, a
way more difficult cipher to break.
Here we run our algorithm with a beam size of
10M and achieve a decipherment accuracy of 157
out of 185 symbols correct yielding a symbol error
rate of less than 5.4%. The gold decipherment is
pruned out of the beam after 35 symbols have been
fixed.
We also ran our algorithm on the other parts
of the Beale ciphers: The first part has a length
520 and contains 299 different cipher symbols
(1.74 observations per symbol), while part three
has length 618 and has 264 symbols which is
2.34 observations per mapping. However, our al-
gorithm does not yield any reasonable decipher-
ments. Since length and number of symbols indi-
cate that deciphering these ciphers is again more
difficult than for part two, it is not clear whether
the other parts are not a homophonic substitution
cipher at all, or whether our algorithm is still not
good enough to find the correct decipherment.
7 Conclusion
We presented two extensions to the beam search
method presented in (Nuhn et al., 2012), that re-
duce the search effort to decipher the Zodiac-408
enormously. These improvements allow us to au-
tomatically decipher part two of the Beale ciphers.
To the best of our knowledge, this has not been
1767
done before. This algorithm might prove useful
when applied to word substitution ciphers and to
learning translations from monolingual data.
Acknowledgements
The authors thank Mark Kozek from the Depart-
ment of Mathematics at Whittier College for chal-
lenging us with a homophonic cipher he created.
Working on his cipher led to developing the meth-
ods presented in this paper.
References
Andrew J. Clark. 1998. Optimisation heuristics for
cryptology. Ph.D. thesis, Faculty of Information
Technology, Queensland University of Technology.
Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040?1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.
George W. Hart. 1994. To decode short cryptograms.
Communications of the Association for Computing
Machinery (CACM), 37(9):102?108, September.
John C. King. 1993. A reconstruction of the key to
beale cipher number two. Cryptologia, 17(3):305?
317.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156?164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Annual Meeting of the Assoc. for Computational
Linguistics, pages 1569?1576, Sofia, Bulgaria, Au-
gust.
Edwin Olson. 2007. Robust dictionary attack of
short simple substitution ciphers. Cryptologia,
31(4):332?342, October.
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812?819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
239?247, Portland, Oregon, June. Association for
Computational Linguistics.
James B Ward, Thomas Jefferson Beale, and Robert
Morriss. 1885. The Beale Papers.
1768
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1769?1773,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Cipher Type Detection
Malte Nuhn
Human Language Technology
and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
nuhn@cs.rwth-aachen.de
Kevin Knight
Information Sciences Institute
University of Southern California
knight@isi.edu
Abstract
Manual analysis and decryption of enci-
phered documents is a tedious and error
prone work. Often?even after spend-
ing large amounts of time on a par-
ticular cipher?no decipherment can be
found. Automating the decryption of var-
ious types of ciphers makes it possible
to sift through the large number of en-
crypted messages found in libraries and
archives, and to focus human effort only
on a small but potentially interesting sub-
set of them. In this work, we train a clas-
sifier that is able to predict which enci-
pherment method has been used to gener-
ate a given ciphertext. We are able to dis-
tinguish 50 different cipher types (speci-
fied by the American Cryptogram Associ-
ation) with an accuracy of 58.5%. This is a
11.2% absolute improvement over the best
previously published classifier.
1 Introduction
Libraries and archives contain a large number of
encrypted messages created throughout the cen-
turies using various encryption methods. For the
great majority of the ciphers an analysis has not
yet been conducted, simply because it takes too
much time to analyze each cipher individually, or
because it is too hard to decipher them. Automatic
methods for analyzing and classifying given ci-
phers makes it possible to sift interesting messages
and by that focus the limited amount of human re-
sources to a promising subset of ciphers.
For specific types of ciphers, there exist au-
tomated tools to decipher encrypted messages.
However, the publicly available tools often de-
pend on a more or less educated guess which
type of encipherment has been used. Furthermore,
they often still need human interaction and are
only restricted to analyzing very few types of ci-
phers. In practice however, there are many differ-
ent types of ciphers which we would like to an-
alyze in a fully automatic fashion: Bauer (2010)
gives a good overview over historical methods that
have been used to encipher messages in the past.
Similarly, the American Cryptogram Association
(ACA) specifies a set of 56 different methods for
enciphering a given plaintext:
Each encipherment method M
i
can be seen as
a function that transforms a given plaintext into a
ciphertext using a given key, or short:
cipher = M
i
(plain, key)
When analyzing an unknown ciphertext, we are
interested in the original plaintext that was used to
generate the ciphertext, i.e. the opposite direction:
plain = M
?1
i
(cipher, key)
Obtaining the plaintext from an enciphered mes-
sage is a difficult problem. We assume that the
decipherment of a message can be separated into
solving three different subproblems:
1. Find the encipherment method M
i
that was
used to create the cipher
cipher ? M
i
.
2. Find the key that was used together with the
methodM
i
to encipher the plaintext to obtain
cipher = M
i
(plain, key).
3. Decode the message using M
i
and key
cipher ? M
?1
i
(cipher, key)
Thus, an intermediate step to deciphering an un-
known ciphertext is to find out which encryption
method was used. In this paper, we present a clas-
sifier that is able to predict just that: Given an un-
known ciphertext, it can predict what kind of en-
cryption method was most likely used to generate
1769
? Type: CMBIFID
? Plaintext:
WOMEN NSFOO TBALL ISGAI
NINGI NPOPU LARIT YANDT
HETOU RNAME
? Key:
LEFTKEY=?IACERATIONS?
RIGHTKEY=?KNORKOPPING?
PERIOD=3, LROUTE=1
RROUTE=1, USE6X6=0
? Ciphertext:
WTQNG GEEBQ BPNQP VANEN
KDAOD GAHQS PKNVI PTAAP
DGMGR PCSGN
Figure 1: Example ?CMBIFID? cipher: Text is
grouped in five character chunks for readability.
it. The results of our classifier are a valuable input
to human decipherers to make a first categoriza-
tion of an unknown ciphertext.
2 Related Work
Central to this work is the list of encryption meth-
ods provided by the American Cipher Associa-
tion
1
. This list contains detailed descriptions and
examples of each of the cipher types, allowing us
to implement them. Figure 3 lists these methods.
We compare our work to the only previously
published cipher type classifier for classical ci-
phers
2
. This classifier is trained on 16, 800 cipher-
texts and is implemented in javascript to run in the
web browser: The user can provide the ciphertext
as input to a web page that returns the classifier?s
predictions. The source code of the classifier is
available online. Our work includes a reimple-
mentation of the features used in that classifier.
As examples for work that deals with the auto-
mated decipherment of cipher texts, we point to
(Ravi and Knight, 2011), and (Nuhn et al., 2013).
These publications develop specialized algorithms
for solving simple and homophonic substitution
ciphers, which are just two out of the 56 cipher
types defined by the ACA. We also want to men-
tion (de Souza et al., 2013), which presents a ci-
pher type classifier for the finalist algorithms of
the Advanced Encryption Standard (AES) contest.
1
http://cryptogram.org/cipher_types.html
2
See http://bionsgadgets.appspot.com/gadget_forms/
refscore_extended.html and https://sites.google.com/site/
bionspot/cipher-id-tests
plaintext keyencipherciphertextclassifier training
type
Figure 2: Overview over the data generation and
training of the classifier presented in this work.
3 General Approach
Given a ciphertext, the task is to find the right en-
cryption method. Our test set covers 50 out of 56
cipher types specified by ACA, as listed in Fig-
ure 3. We are going to take a machine learning ap-
proach which is based on the observation that we
can generate an infinite amount of training data.
3.1 Data Flow
The training procedure is depicted in Figure 2:
Based upon a large English corpus, we first choose
possible plaintext messages. Then, for each enci-
pherment method, we choose a random key and
encipher each of the plaintext messages using the
encipherment method and key. By doing this, we
can obtain (a theoretically infinite) amount of la-
beled data of the form (type, ciphertext). We can
then train a classifier on this data and evaluate it
on some held out data.
Figure 1 shows that in general the key can con-
sist of more than just a codeword: In this case,
the method uses two codewords, a period length,
two different permutation parameters, and a gen-
eral decision whether to use a special ?6?6? vari-
ant of the cipher or not. If not defined otherwise,
we choose random settings for these parameters.
If the parameters are integers, we choose random
values from a uniform distribution (in a sensible
range). In case of codewords, we choose the 450k
most frequent words from an English dictionary.
We train on cipher texts of random length.
3.2 Classifiers
The previous state-of-the-art classifier by BION
uses a random forest classifier (Breiman, 2001).
The version that is available online, uses 50 ran-
1770
? 6x6bifid
? 6x6playfair
? amsco
? bazeries
? beaufort
? bifid6
? bifid7
? (cadenus)
? cmbifid
? columnar
? digrafid
? dbl chckrbrd
? four square
? fracmorse
? grandpre
? (grille)
? gromark
? gronsfeld
? homophonic
? mnmedinome
? morbit
? myszkowski
? nicodemus
? nihilistsub
? (nihilisttransp)
? patristocrat
? period 7 vig.
? periodic gro-
mark
? phillips
? plaintext
? playfair
? pollux
? porta
? portax
? progkey beau-
fort
? progressivekey
? quagmire2
? quagmire3
? quagmire4
? ragbaby
? randomdigit
? randomtext
? redefence
? (route transp)
? runningkey
? seriatedpfair
? swagman
? tridigital
? trifid
? trisquare
? trisquare hr
? two square
? two sq. spiral
? vigautokey
? (vigenere)
? (vigslidefair)
Figure 3: Cipher types specified by ACA. Our classifier is able to recognize 50 out of these 56 ciphers.
The braced cipher types are not covered in this work.
dom decision trees. The features used by this clas-
sifier are described in Section 4.
Further, we train a support vector machine using
the libSVM toolkit (Chang and Lin, 2011). This
is feasible for up to 100k training examples. Be-
yond this point, training times become too large.
We perform multi class classification using ?-SVC
and a polynomial kernel. Multi class classification
is performed using one-against-one binary classifi-
cation. We select the SVM?s free parameters using
a small development set of 1k training examples.
We also use Vowpal Wabbit (Langford et al.,
2007) to train a linear classifier using stochastic
gradient descent. Compared to training SVMs,
Vowpal Wabbit is extremely fast and allows using
a lot of training examples. We use a squared loss
function, adaptive learning rates and don?t employ
any regularization. We train our classifier with up
to 1M training examples. The best performing set-
tings use one-against-all classification, 20 passes
over the training data and the default learning rate.
Quadratic features resulted in much slower train-
ing, while not providing any gains in accuracy.
4 Features
We reimplemented all of the features used in the
BION classifier, and add three newly developed
sets of features, resulting in a total of 58 features.
In order to further structure these features, we
group these features as follows: We call the set
of features that relate to the length of the cipher
LEN. This set contains binary features firing when
the cipher length is a multiple of 2, 3, 5, 25, any
of 4-15, and any of 4-30. We call the set of fea-
tures that are based on the fact that the cipher-
text contains a specific symbol HAS. This set con-
tains binary features firing when the cipher con-
tains a digit, a letter (A-Z), the ?#? symbol, the
letter ?j?, the digit ?0?. We also introduce an-
other set of features called DGT that contains two
features, firing when the cipher is starting or end-
ing with a digit. The set VIG contains 5 features:
The feature score is based on the best possible bi-
gram LM perplexity of a decipherment compatible
with the decipherment process of the cipher types
Autokey, Beaufort, Porta, Slidefair and Vigenere.
Further, we also include the features IC, MIC,
MKA, DIC, EDI, LR, ROD and LDI, DBL, NOMOR,
RDI, PTX, NIC, PHIC, BDI, CDD, SSTD, MPIC,
SERP, which were introduced in the BION classi-
fier
3
. Thus, the first 22 data points in Figure 4 are
based on previously known features by BION. We
further present the following additional features.
4.1 Repetition Feature (REP)
This set of features is based on how often the ci-
phertext contains symbols that are repeated ex-
actly n times in a row: For example the cipher-
text shown in Figure 1 contains two positions with
repetitions of length n = 2, because the cipher-
text contains EE, as well as AA. Beyond length
2, there are no repeats. These numbers are then
normalized by dividing them by the total number
of repeats of length 2 ? n ? 5.
4.2 Amsco Feature (AMSC)
The idea of the AMSCO cipher is to fill consec-
utive chunks of one and two plaintext characters
into n columns of a grid (see Table 1). Then a
permutation of the columns is performed, and the
resulting permuted plaintext is read of line by line
and forms the final ciphertext. This feature reads
the ciphertext into a similar grid of up to 5 columns
3
See http://home.comcast.net/
?
acabion/acarefstats.html
1771
Plaintext w om e ns f
oo t ba l li
Permutation 3 5 1 4 2
Table 1: Example grid used for AMSCO ciphers.
and then tries all possible permutations to retain
the original plaintext. The result of this opera-
tion is then scored with a bigram language model.
Depending on whether the difference in perplexity
between ciphertext and deciphered text exceeds a
given threshold, this binary feature fires.
4.3 Variant Feature (VAR)
In the variant cipher, the plaintext is written into
a block under a key word. All letters in the first
column are enciphered by shifting them using the
first key letter of the key word, the second column
uses the second key letter, etc. For different pe-
riods (i.e. lengths of key words), the ciphertext
is structured into n columns and unigram statis-
tics for each column are calculated. The frequency
profile of each column is compared to the unigram
frequency profile using a perplexity measure. This
binary feature fires when the resulting perplexities
are lower than a specific threshold.
5 Results
Figure 4 shows the classification accuracy for the
BION baseline, as well as our SVM and VW based
classifiers for a test set of 305 ciphers that have
been published in the ACA. The classifiers shown
in this figure are trained on cipher texts of ran-
dom length. We show the contribution of all the
features we used in the classifier on the x-axis.
Furthermore we also vary the amount of training
data we use to train the classifiers from 10k to 1M
training examples. It can be seen that when using
the same features as BION, our prediction accu-
racy is compatible with the BION classifier. The
main improvement of our classifier stems from the
REP, AMSC and VAR features. Our best classi-
fier is more than 11% more accurate than previous
state-of-the-art BION classifier.
We identified the best classifier on a held-out
set of 1000 ciphers, i.e. 20 ciphers for each ci-
pher type. Here the three new features improve the
VW-1M classifier from 50.9% accuracy to 56.0%
accuracy, and the VW-100k classifier from 48.9%
to 54.6%. Note that this held-out set is based on
the exact same generator that we used to create the
training data with. However, we also report the
results of our method on the completely indepen-
dently created ACA test set in Figure 4.
6 Conclusion
We presented a state-of-the art classifier for cipher
type detection. The approach we present is easily
extensible to cover more cipher types and allows
incorporating new features.
Acknowledgements
We thank Taylor Berg-Kirkpatrick, Shu Cai, Bill
Mason, Be?ata Megyesi, Julian Schamper, and
Megha Srivastava for their support and ideas. This
work was supported by ARL/ARO (W911NF-10-
1-0533) and DARPA (HR0011-12-C-0014).
10
20
30
40
50
60
H
A
S
L
E
N
V
I
G
I
C
M
I
C
M
K
A
D
I
C
E
D
I
L
R
R
O
D
L
D
I
D
B
L
N
M
O
R
R
D
I
P
T
X
N
I
C
P
H
I
C
B
D
I
C
D
D
S
S
T
D
M
P
I
C
S
E
R
P
R
E
P
A
M
S
C
V
A
R
Features
A
c
c
u
r
a
c
y
(
%
)
BION
SVM 10k
SVM100k
VW 100k
VW 1M
Figure 4: Classifier accuracy vs. training data and set of features used. From left to right more and
more features are used, the x-axis shows which features are added. The feature names are described in
Section 4. The features right of the vertical line are presented in this paper. The horizontal line shows
the previous state-of-the art accuracy (BION) of 47.3%, we achieve 58.49%.
1772
References
F.L. Bauer. 2010. Decrypted Secrets: Methods and
Maxims of Cryptology. Springer.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32, October.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
William AR de Souza, Allan Tomlinson, and Luiz MS
de Figueiredo. 2013. Cipher identification with a
neural network.
John Langford, Lihong Li, and Alex Strehl. 2007.
Vowpal Wabbit. https://github.com/
JohnLangford/vowpal_wabbit/wiki.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In ACL (1), pages 1568?1576.
Sujith Ravi and Kevin Knight. 2011. Bayesian Infer-
ence for Zodiac and Other Homophonic Ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
239?247, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
1773
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156?164,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Deciphering Foreign Language by Combining Language Models and
Context Vectors
Malte Nuhn and Arne Mauser? and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper we show how to train statis-
tical machine translation systems on real-
life tasks using only non-parallel monolingual
data from two languages. We present a mod-
ification of the method shown in (Ravi and
Knight, 2011) that is scalable to vocabulary
sizes of several thousand words. On the task
shown in (Ravi and Knight, 2011) we obtain
better results with only 5% of the computa-
tional effort when running our method with
an n-gram language model. The efficiency
improvement of our method allows us to run
experiments with vocabulary sizes of around
5,000 words, such as a non-parallel version of
the VERBMOBIL corpus. We also report re-
sults using data from the monolingual French
and English GIGAWORD corpora.
1 Introduction
It has long been a vision of science fiction writers
and scientists to be able to universally communi-
cate in all languages. In these visions, even previ-
ously unknown languages can be learned automati-
cally from analyzing foreign language input.
In this work, we attempt to learn statistical trans-
lation models from only monolingual data in the
source and target language. The reasoning behind
this idea is that the elements of languages share sta-
tistical similarities that can be automatically identi-
fied and matched with other languages.
This work is a big step towards large-scale and
large-vocabulary unsupervised training of statistical
translation models. Previous approaches have faced
constraints in vocabulary or data size. We show how
?Author now at Google Inc., amauser@google.com.
to scale unsupervised training to real-life transla-
tion tasks and how large-scale experiments can be
done. Monolingual data is more readily available,
if not abundant compared to true parallel or even
just translated data. Learning from only monolin-
gual data in real-life translation tasks could improve
especially low resource language pairs where few or
no parallel texts are available.
In addition to that, this approach offers the op-
portunity to decipher new or unknown languages
and derive translations based solely on the available
monolingual data. While we do tackle the full unsu-
pervised learning task for MT, we make some very
basic assumptions about the languages we are deal-
ing with:
1. We have large amounts of data available in
source and target language. This is not a very
strong assumption as books and text on the in-
ternet are readily available for almost all lan-
guages.
2. We can divide the given text in tokens and
sentence-like units. This implies that we know
enough about the language to tokenize and
sentence-split a given text. Again, for the vast
majority of languages, this is not a strong re-
striction.
3. The writing system is one-dimensional left-to-
right. It has been shown (Lin and Knight, 2006)
that the writing direction can be determined
separately and therefore this assumption does
not pose a real restriction.
Previous approaches to unsupervised training for
SMT prove feasible only for vocabulary sizes up to
around 500 words (Ravi and Knight, 2011) and data
156
sets of roughly 15,000 sentences containing only
about 4 tokens per sentence on average. Real data
as it occurs in texts such as web pages or news texts
does not meet any of these characteristics.
In this work, we will develop, describe, and
evaluate methods for large vocabulary unsupervised
learning of machine translation models suitable for
real-world tasks. The remainder of this paper is
structured as follows: In Section 2, we will review
the related work and describe how our approach ex-
tends existing work. Section 3 describes the model
and training criterion used in this work. The im-
plementation and the training of this model is then
described in Section 5 and experimentally evaluated
in Section 6.
2 Related Work
Unsupervised training of statistical translations sys-
tems without parallel data and related problems have
been addressed before. In this section, we will re-
view previous approaches and highlight similarities
and differences to our work. Several steps have been
made in this area, such as (Knight and Yamada,
1999), (Ravi and Knight, 2008), or (Snyder et al,
2010), to name just a few. The main difference of
our work is, that it allows for much larger vocab-
ulary sizes and more data to be used than previous
work while at the same time not being dependent on
seed lexica and/or any other knowledge of the lan-
guages.
Close to the methods described in this work,
Ravi and Knight (2011) treat training and transla-
tion without parallel data as a deciphering prob-
lem. Their best performing approach uses an EM-
Algorithm to train a generative word based trans-
lation model. They perform experiments on a
Spanish/English task with vocabulary sizes of about
500 words and achieve a performance of around
20 BLEU compared to 70 BLEU obtained by a sys-
tem that was trained on parallel data. Our work uses
the same training criterion and is based on the same
generative story. However, we use a new training
procedure whose critical parts have constant time
and memory complexity with respect to the vocab-
ulary size so that our methods can scale to much
larger vocabulary sizes while also being faster.
In a different approach, Koehn and Knight (2002)
induce a bilingual lexicon from only non-parallel
data. To achieve this they use a seed lexicon which
they systematically extend by using orthographic as
well as distributional features such as context, and
frequency. They perform their experiments on non-
parallel German-English news texts, and test their
mappings against a bilingual lexicon. We use a
greedy method similar to (Koehn and Knight, 2002)
for extending a given lexicon, and we implicitly also
use the frequency as a feature. However, we perform
fully unsupervised training and do not start with a
seed lexicon or use linguistic features.
Similarly, Haghighi et al (2008) induce a one-
to-one translation lexicon only from non-parallel
monolingual data. Also starting with a seed lexi-
con, they use a generative model based on canoni-
cal correlation analysis to systematically extend the
lexicon using context as well as spelling features.
They evaluate their method on a variety of tasks,
ranging from inherently parallel data (EUROPARL)
to unrelated corpora (100k sentences of the GIGA-
WORD corpus). They report F-measure scores of the
induced entries between 30 to 70. As mentioned
above, our work neither uses a seed lexicon nor or-
thographic features.
3 Translation Model
In this section, we describe the statistical training
criterion and the translation model that is trained us-
ing monolingual data. In addition to the mathemat-
ical formulation of the model we describe approxi-
mations used.
Throughout this work, we denote the source lan-
guage words as f and target language words as e.
The source vocabulary is Vf and we write the size
of this vocabulary as |Vf |. The same notation holds
for the target vocabulary with Ve and |Ve|.
As training criterion for the translation model?s
parameters ?, Ravi and Knight (2011) suggest
arg max
?
?
?
?
?
f
?
e
P (e) ? p?(f |e)
?
?
?
(1)
We would like to obtain ? from Equation 1 using
the EM Algorithm (Dempster et al, 1977). This
becomes increasingly difficult with more complex
translation models. Therefore, we use a simplified
157
translation model that still contains all basic phe-
nomena of a generic translation process. We formu-
late the translation process with the same generative
story presented in (Ravi and Knight, 2011):
1. Stochastically generate the target sentence ac-
cording to an n-gram language model.
2. Insert NULL tokens between any two adjacent
positions of the target string with uniform prob-
ability.
3. For each target token ei (including NULL)
choose a foreign translation fi (including
NULL) with probability P?(fi|ei).
4. Locally reorder any two adjacent foreign words
fi?1, fi with probability P (SWAP) = 0.1.
5. Remove the remaining NULL tokens.
In practice, however, it is not feasible to deal with
the full parameter table P?(fi|ei) which models the
lexicon. Instead we only allow translation models
where for each source word f the number of words
e? with P (f |e?) 6= 0 is below some fixed value. We
will refer to this value as the maximum number of
candidates of the translation model and denote it
with NC . Note that for a given e this does not nec-
essarily restrict the number of entries P (f ?|e) 6= 0.
Also note that with a fixed value of NC , time and
memory complexity of the EM step isO(1) with re-
spect to |Ve| and |Vf |.
In the following we divide the problem of maxi-
mizing Equation 1 into two parts:
1. Determining a set of active lexicon entries.
2. Choosing the translation probabilities for the
given set of active lexicon entries.
The second task can be achieved by running the
EM algorithm on the restricted translation model.
We deal with the first task in the following section.
4 Monolingual Context Similarity
As described in Section 3 we need some mecha-
nism to iteratively choose an active set of translation
candidates. Based on the assumption that some of
the active candidates and their respective probabili-
ties are already correct, we induce new active candi-
dates. In the context of information retrieval, Salton
et al (1975) introduce a document space where each
document identified by one or more index terms is
represented by a high dimensional vector of term
weights. Given two vectors v1 and v2 of two doc-
uments it is then possible to calculate a similarity
coefficient between those given documents (which
is usually denoted as s(v1, v2)). Similar to this we
represent source and target words in a high dimen-
sional vector space of target word weights which we
call context vectors and use a similarity coefficient
to find possible translation pairs. We first initialize
these context vectors using the following procedure:
1. Using only the monolingual data for the target
language, prepare the context vectors vei with
entries vei,ej :
(a) Initialize all vei,ej = 0
(b) For each target sentence E:
For each word ei in E:
For each word ej 6= ei in E:
vei,ej = vei,ej + 1.
(c) Normalize each vector vei such that
?
ej
(vei,ej )
2 != 1 holds.
Using the notation ei =
(
ej : vei,ej , . . .
)
these
vectors might for example look like
work = (early : 0.2, late : 0.1, . . . )
time = (early : 0.2, late : 0.2, . . . ).
2. Prepare context vectors vfi,ej for the source
language using only the monolingual data for
the source language and the translation model?s
current parameter estimate ?:
(a) Initialize all vfi,ej = 0
(b) Let E??(F ) denote the most probable
translation of the foreign sentence F ob-
tained by using the current estimate ?.
(c) For each source sentence F :
For each word fi in F :
For each word ej 6= E?(fi)1 in
E?(F ):
vfi,ej = vfi,ej + 1
(d) Normalize each vector vfi such that
?
ej
(vfi,ej )
2 != 1 holds.
1denoting that ej is not the translation of fi in E?(F )
158
Adapting the notation described above, these
vectors might for example look like
Arbeit = (early : 0.25, late : 0.05, . . . )
Zeit = (early : 0.15, late : 0.25, . . . )
Once we have set up the context vectors ve and
vf , we can retrieve translation candidates for some
source word f by finding those words e? that maxi-
mize the similarity coefficient s(ve? , vf ), as well as
candidates for a given target word e by finding those
words f ? that maximize s(ve, vf ?). In our implemen-
tation we use the Euclidean distance
d(ve, vf ) = ||ve ? vf ||2. (2)
as distance measure.2 The normalization of context
vectors described above is motivated by the fact that
the context vectors should be invariant with respect
to the absolute number of occurrences of words.3
Instead of just finding the best candidates for a
given word, we are interested in an assignment that
involves all source and target words, minimizing the
sum of distances between the assigned words. In
case of a one-to-one mapping the problem of assign-
ing translation candidates such that the sum of dis-
tances is minimal can be solved optimally in poly-
nomial time using the hungarian algorithm (Kuhn,
1955). In our case we are dealing with a many-
to-many assignment that needs to satisfy the max-
imum number of candidates constraints. For this,
we solve the problem in a greedy fashion by simply
choosing the best pairs (e, f) first. As soon as a tar-
get word e or source word f has reached the limit
of maximum candidates, we skip all further candi-
dates for that word e (or f respectively). This step
involves calculating and sorting all |Ve| ? |Vf | dis-
tances which can be done in time O(V 2 ? log(V )),
with V = max(|Ve|, |Vf |). A simplified example of
this procedure is depicted in Figure 1. The example
already shows that the assignment obtained by this
algorithm is in general not optimal.
2We then obtain pairs (e, f) that minimize d.
3This gives the same similarity ordering as using un-
normalized vectors with the cosine similarity measure
ve?vf
||ve||2?||vf ||2
which can be interpreted as measuring the cosine
of the angle between the vectors, see (Manning et al, 2008).
Still it is noteworthy that this procedure is not equivalent to the
tf-IDF context vectors described in (Salton et al, 1975).
x
y
time (e)
Arbeit (f)
work (e)
Zeit (f)
Figure 1: Hypothetical example for a greedy one-to-one
assignment of translation candidates. The optimal assign-
ment would contain (time,Zeit) and (work,Arbeit).
5 Training Algorithm and Implementation
Given the model presented in Section 3 and the
methods illustrated in Section 4, we now describe
how to train this model.
As described in Section 4, the overall procedure
is divided into two alternating steps: After initializa-
tion we first perform EM training of the translation
model for 20-30 iterations using a 2-gram or 3-gram
language model in the target language. With the ob-
tained best translations we induce new translation
candidates using context similarity. This procedure
is depicted in Figure 2.
5.1 Initialization
Let NC be the maximum number of candidates per
source word we allow, Ve and Vf be the target/source
vocabulary and r(e) and r(f) the frequency rank of
a source/target word. Each word f ? Vf with fre-
quency rank r(f) is assigned to all words e ? Ve
with frequency rank
r(e) ? [ start(f) , end(f) ] (3)
where
start(f) = max(0 , min
(
|Ve| ?Nc ,
?
|Ve|
|Vf |
? r(f)?
Nc
2
?
)
)
(4)
end(f) = min (start(f) +Nc, |Ve|) . (5)
This defines a diagonal beam4 when visualizing
the lexicon entries in a matrix where both source
and target words are sorted by their frequency rank.
However, note that the result of sorting by frequency
4The diagonal has some artifacts for the highest and lowest
frequency ranks. See, for example, left side of Figure 2.
159
In
it
ia
li
za
ti
on
ta
rg
et
w
or
ds
source words
E
M
It
er
at
io
n
s
ta
rg
et
w
or
ds
source words C
on
te
xt
V
ec
to
rs
ta
rg
et
w
or
ds
source words
E
M
It
er
at
io
n
s
. . .
Figure 2: Visualization of the training procedure. The big rectangles represent word lexica in different stages of the
training procedure. The small rectangles represent word pairs (e, f) for which e is a translation candidate of f , while
dots represent word pairs (e, f) for which this is not the case. Source and target words are sorted by frequency so that
the most frequent source words appear on the very left, and the most frequent target words appear at the very bottom.
and thus the frequency ranks are not unique when
there are words with the same frequency. In this
case, we initially obtain some not further specified
frequency ordering, which is then kept throughout
the procedure.
This initialization proves useful as we show by
taking an IBM1 lexicon P (f |e) extracted on the
parallel VERBMOBIL corpus (Wahlster, 2000): For
each word e we calculate the weighted rank differ-
ence
?ravg(e) =
?
f
P (f |e) ? |(r(e)? r(f)| (6)
and count how many of those weighted rank dif-
ferences are smaller than a given value NC2 . Here
we see that for about 1% of the words the weighted
rank difference lies withinNC = 50, and even about
3% for NC = 150 respectively. This shows that the
initialization provides a first solid guess of possible
translations.
5.2 EM Algorithm
The generative story described in Section 3 is im-
plemented as a cascade of a permutation, insertion,
lexicon, deletion and language model finite state
transducers using OpenFST (Allauzen et al, 2007).
Our FST representation of the LM makes use of
failure transitions as described in (Allauzen et al,
2003). We use the forward-backward algorithm on
the composed transducers to efficiently train the lex-
icon model using the EM algorithm.
5.3 Context Vector Step
Given the trained parameters ? from the previous run
of the EM algorithm we set the context vectors ve
and vf up as described in Section 4. We then calcu-
late and sort all |Ve|?|Vf | distances which proves fea-
sible in a few CPU hours even for vocabulary sizes
of more than 50,000 words. This is achieved with
the GNU SORT tool, which uses external sorting for
sorting large amounts of data.
To set up the new lexicon we keep the bNC2 c
best translations for each source word with respect
to P (e|f), which we obtained in the previous EM
run. Experiments showed that it is helpful to also
limit the number of candidates per target words. We
therefore prune the resulting lexicon using P (f |e)
to a maximum of bN
?
C
2 c candidates per target word
afterwards. Then we fill the lexicon with new can-
didates using the previously sorted list of candidate
pairs such that the final lexicon has at most NC
candidates per source word and at most N ?C can-
didates per target word. We set N ?C to some value
N ?C > NC . All experiments in this work were run
with N ?C = 300. Values of N
?
C ? NC seem to pro-
duce poorer results. Not limiting the number of can-
didates per target word at all also typically results in
weaker performance. After the lexicon is filled with
candidates, we initialize the probabilities to be uni-
form. With this new lexicon the process is iterated
starting with the EM training.
6 Experimental Evaluation
We evaluate our method on three different corpora.
At first we apply our method to non-parallel Span-
ish/English data that is based on the OPUS corpus
(Tiedemann, 2009) and that was also used in (Ravi
and Knight, 2011). We show that our method per-
forms better by 1.6 BLEU than the best performing
method described in (Ravi and Knight, 2011) while
160
Name Lang. Sent. Words Voc.
OPUS
Spanish 13,181 39,185 562
English 19,770 61,835 411
VERBMOBIL
German 27,861 282,831 5,964
English 27,862 294,902 3,723
GIGAWORD
French 100,000 1,725,993 68,259
English 100,000 1,788,025 64,621
Table 1: Statistics of the corpora used in this paper.
being approximately 15 to 20 times faster than their
n-gram based approach.
After that we apply our method to a non-parallel
version of the German/English VERBMOBIL corpus,
which has a vocabulary size of 6,000 words on the
German side, and 3,500 words on the target side and
which thereby is approximately one order of magni-
tude larger than the previous OPUS experiment.
We finally run our system on a subset of the non-
parallel French/English GIGAWORD corpus, which
has a vocabulary size of 60,000 words for both
French and English. We show first interesting re-
sults on such a big task.
In case of the OPUS and VERBMOBIL corpus,
we evaluate the results using BLEU (Papineni et al,
2002) and TER (Snover et al, 2006) to reference
translations. We report all scores in percent. For
BLEU higher values are better, for TER lower val-
ues are better. We also compare the results on these
corpora to a system trained on parallel data.
In case of the GIGAWORD corpus we show lexi-
con entries obtained during training.
6.1 OPUS Subtitle Corpus
6.1.1 Experimental Setup
We apply our method to the corpus described in
Table 6. This exact corpus was also used in (Ravi
and Knight, 2011). The best performing methods
in (Ravi and Knight, 2011) use the full 411 ? 579
lexicon model and apply standard EM training. Us-
ing a 2-gram LM they obtain 15.3 BLEU and with
a whole segment LM, they achieve 19.3 BLEU. In
comparison to this baseline we run our algorithm
with NC = 50 candidates per source word for both,
a 2-gram and a 3-gram LM. We use 30 EM iterations
between each context vector step. For both cases we
run 7 EM+Context cycles.
6.1.2 Results
Figure 3 and Figure 4 show the evolution of BLEU
and TER scores for applying our method using a 2-
gram and a 3-gram LM.
In case of the 2-gram LM (Figure 3) the transla-
tion quality increases until it reaches a plateau after
5 EM+Context cycles. In case of the 3-gram LM
(Figure 4) the statement only holds with respect to
TER. It is notable that during the first iterations TER
only improves very little until a large chunk of the
language unravels after the third iteration. This be-
havior may be caused by the fact that the corpus only
provides a relatively small amount of context infor-
mation for each word, since sentence lengths are 3-4
words on average.
0 1 2 3 4 5 6 7 88
10
12
14
16 Full EM best (BLEU)
Iteration
BL
EU
66
68
70
72
74
76
78
80
TE
R
BLEU
TER
Figure 3: Results on the OPUS corpus with a 2-gram LM,
NC = 50, and 30 EM iterations between each context
vector step. The dashed line shows the best result using a
2-gram LM in (Ravi and Knight, 2011).
Table 2 summarizes these results and compares
them with (Ravi and Knight, 2011). Our 3-gram
based method performs by 1.6 BLEU better than
their best system which is a statistically significant
improvement at 95% confidence level. Furthermore,
Table 2 compares the CPU time needed for training.
Our 3-gram based method is 15-20 times faster than
running the EM based training procedure presented
in (Ravi and Knight, 2011) with a 3-gram LM5.
5(Ravi and Knight, 2011) only report results using a 2-gram
LM and a whole-segment LM.
161
0 1 2 3 4 5 6 7 88
10
12
14
16
18
20
22
24
Full EM best (BLEU)
Iteration
BL
EU
64
66
68
70
72
TE
R
BLEU
TER
Figure 4: Results on the OPUS corpus with a 3-gram LM,
NC = 50, and 30 EM iterations between each context
vector step. The dashed line shows the best result using a
whole-segment LM in (Ravi and Knight, 2011)
Method CPU BLEU TER
EM, 2-gram LM
411 cand. p. source word
(Ravi and Knight, 2011)
?850h6 15.3 ?
EM, Whole-segment LM
411 cand. p. source word
(Ravi and Knight, 2011)
?7 19.3 ?
EM+Context, 2-gram LM
50 cand. p. source word
(this work)
50h8 15.2 66.6
EM+Context, 3-gram LM
50 cand. p. source word
(this work)
200h8 20.9 64.5
Table 2: Results obtained on the OPUS corpus.
To summarize: Our method is significantly faster
than n-gram LM based approaches and obtains bet-
ter results than any previously published method.
6Estimated by running full EM using the 2-gram LM using
our implementation for 90 Iterations yielding 15.2 BLEU.
7?4,000h when running full EM using a 3-gram LM, using
our implementation. Estimated by running only the first itera-
tion and by assuming that the final result will be obtained after
90 iterations. However, (Ravi and Knight, 2011) report results
using a whole segment LM, assigning P (e) > 0 only to se-
quences seen in training. This seems to work for the given task
but we believe that it can not be a general replacement for higher
order n-gram LMs.
8Estimated by running our method for 5? 30 iterations.
6.2 VERBMOBIL Corpus
6.2.1 Experimental Setup
The VERBMOBIL corpus is a German/English
corpus dealing with short sentences for making ap-
pointments. We prepared a non-parallel subset of
the original VERBMOBIL (Wahlster, 2000) by split-
ting the corpus into two parts and then selecting only
the German side from the first half, and the English
side from the second half such that the target side
is not the translation of the source side. The source
and target vocabularies of the resulting non-parallel
corpus are both more than 9 times bigger compared
to the OPUS vocabularies. Also the total amount of
word tokens is more than 5 times larger compared
to the OPUS corpus. Table 6 shows the statistics of
this corpus. We run our method for 5 EM+Context
cycles (30 EM iterations each) using a 2-gram LM.
After that we run another five EM+Context cycles
using a 3-gram LM.
6.2.2 Results
Our results on the VERBMOBIL corpus are sum-
marized in Table 3. Even on this more complex
task our method achieves encouraging results: The
Method BLEU TER
5? 30 Iterations EM+Context
50 cand. p. source word, 2-gram LM
11.7 67.4
+ 5? 30 Iterations EM+Context
50 cand. p. source word, 3-gram LM
15.5 63.2
Table 3: Results obtained on the VERBMOBIL corpus.
translation quality increases from iteration to itera-
tion until the algorithm finally reaches 11.7 BLEU
using only the 2-gram LM. Running further five
cycles using a 3-gram LM achieves a final perfor-
mance of 15.5 BLEU. Och (2002) reports results of
48.2 BLEU for a single-word based translation sys-
tem and 56.1 BLEU using the alignment template
approach, both trained on parallel data. However, it
should be noted that our experiment only uses 50%
of the original VERBMOBIL training data to simulate
a truly non-parallel setup.
162
Iter. e p(f1|e) f1 p(f2|e) f2 p(f3|e) f3 p(f4|e) f4 p(f5|e) f5
1. the 0.43 la 0.31 l? 0.11 une 0.04 le 0.04 les
2. several 0.57 plusieurs 0.21 les 0.09 des 0.03 nombreuses 0.02 deux
3. where 0.63 ou` 0.17 mais 0.06 indique 0.04 pre?cise 0.02 appelle
4. see 0.49 e?viter 0.09 effet 0.09 voir 0.05 envisager 0.04 dire
5. January 0.25 octobre 0.22 mars 0.09 juillet 0.07 aou?t 0.07 janvier
? Germany 0.24 Italie 0.12 Espagne 0.06 Japon 0.05 retour 0.05 Suisse
Table 4: Lexicon entries obtained by running our method on the non-parallel GIGAWORD corpus. The first column
shows in which iteration the algorithm found the first correct translations f (compared to a parallely trained lexicon)
among the top 5 candidates
6.3 GIGAWORD
6.3.1 Experimental Setup
This setup is based on a subset of the monolingual
GIGAWORD corpus. We selected 100,000 French
sentences from the news agency AFP and 100,000
sentences from the news agency Xinhua. To have a
more reliable set of training instances, we selected
only sentences with more than 7 tokens. Note that
these corpora form true non-parallel data which, be-
sides the length filtering, were not specifically pre-
selected or pre-processed. More details on these
non-parallel corpora are summarized in Table 6. The
vocabularies have a size of approximately 60,000
words which is more than 100 times larger than the
vocabularies of the OPUS corpus. Also it incor-
porates more than 25 times as many tokens as the
OPUS corpus.
After initialization, we run our method with
NC = 150 candidates per source word for 20 EM
iterations using a 2-gram LM. After the first context
vector step with NC = 50 we run another 4 ? 20
iterations with NC = 50 with a 2-gram LM.
6.3.2 Results
Table 4 shows example lexicon entries we ob-
tained. Note that we obtained these results by us-
ing purely non-parallel data, and that we neither
used a seed lexicon, nor orthographic features to as-
sign e.g. numbers or proper names: All results are
obtained using 2-gram statistics and the context of
words only. We find the results encouraging and
think that they show the potential of large-scale un-
supervised techniques for MT in the future.
7 Conclusion
We presented a method for learning statistical ma-
chine translation models from non-parallel data. The
key to our method lies in limiting the translation
model to a limited set of translation candidates and
then using the EM algorithm to learn the probabil-
ities. Based on the translations obtained with this
model we obtain new translation candidates using
a context vector approach. This method increased
the training speed by a factor of 10-20 compared
to methods known in literature and also resulted
in a 1.6 BLEU point increase compared to previ-
ous approaches. Due to this efficiency improvement
we were able to tackle larger tasks, such as a non-
parallel version of the VERBMOBIL corpus having
a nearly 10 times larger vocabulary. We also had a
look at first results of our method on an even larger
Task, incorporating a vocabulary of 60,000 words.
We have shown that, using a limited set of trans-
lation candidates, we can significantly reduce the
computational complexity of the learning task. This
work serves as a big step towards large-scale unsu-
pervised training for statistical machine translation
systems.
Acknowledgements
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation. The authors would like to thank Su-
jith Ravi and Kevin Knight for providing us with the
OPUS subtitle corpus and David Rybach for kindly
sharing his knowledge about the OpenFST library.
163
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 40?47. Association for
Computational Linguistics.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In Jan Holub and Jan Zda?rek, editors, CIAA,
volume 4783 of Lecture Notes in Computer Science,
pages 11?23. Springer.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, B, 39.
Aria Haghighi, Percy Liang, T Berg-Kirkpatrick, and
Dan Klein. 2008. Learning Bilingual Lexicons from
Monolingual Corpora. In Proceedings of ACL08 HLT,
pages 771?779. Association for Computational Lin-
guistics.
Kevin Knight and Kenji Yamada. 1999. A computa-
tional approach to deciphering unknown scripts. In
ACL Workshop on Unsupervised Learning in Natural
Language Processing, number 1, pages 37?44. Cite-
seer.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL02 workshop on Unsupervised lex-
ical acquisition, number July, pages 9?16. Association
for Computational Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistic Quar-
terly, 2:83?97.
Shou-de Lin and Kevin Knight. 2006. Discovering
the linear writing order of a two-dimensional ancient
hieroglyphic script. Artificial Intelligence, 170:409?
421, April.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schuetze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Franz J. Och. 2002. Statistical Machine Translation:
From Single-Word Models to Alignment Templates.
Ph.D. thesis, RWTH Aachen University, Aachen, Ger-
many, October.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sujith Ravi and Kevin Knight. 2008. Attacking decipher-
ment problems optimally with low-order n-gram mod-
els. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 812?819, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 12?21,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Gerard M. Salton, Andrew K. C. Wong, and Chang S.
Yang. 1975. A vector space model for automatic in-
dexing. Commun. ACM, 18(11):613?620, November.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA, Au-
gust.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In 48th Annual Meeting of the Association for
Computational Linguistics, number July, pages 1048?
1057.
Jo?rg Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and in-
terfaces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia, Borovets, Bul-
garia.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of speech-to-speech translations. Springer-
Verlag, Berlin.
164
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 615?621,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Decipherment Complexity in 1:1 Substitution Ciphers
Malte Nuhn and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper we show that even for the
case of 1:1 substitution ciphers?which
encipher plaintext symbols by exchang-
ing them with a unique substitute?finding
the optimal decipherment with respect to a
bigram language model is NP-hard. We
show that in this case the decipherment
problem is equivalent to the quadratic as-
signment problem (QAP). To the best of
our knowledge, this connection between
the QAP and the decipherment problem
has not been known in the literature be-
fore.
1 Introduction
The decipherment approach for MT has recently
gained popularity for training and adapting trans-
lation models using only monolingual data. The
general idea is to find those translation model
parameters that maximize the probability of the
translations of a given source text in a given lan-
guage model of the target language.
In general, the process of translation has a wide
range of phenomena like substitution and reorder-
ing of words and phrases. In this paper we only
study models that substitute tokens?i.e. words
or letters?with a unique substitute. It therefore
serves as a very basic case for decipherment and
machine translation.
Multiple techniques like integer linear program-
ming (ILP), A? search, genetic algorithms, and
Bayesian inference have been used to tackle the
decipherment problem for 1:1 substitution ciphers.
The existence of such a variety of different ap-
proaches for solving the same problem already
shows that there is no obvious way to solve the
problem optimally.
In this paper we show that decipherment of 1:1
substitution ciphers is indeed NP-hard and thus ex-
plain why there is no single best approach to the
problem. The literature on decipherment provides
surprisingly little on the analysis of the complexity
of the decipherment problem. This might be re-
lated to the fact that a statistical formulation of the
decipherment problem has not been analyzed with
respect to n-gram language models: This paper
shows the close relationship of the decipherment
problem to the quadratic assignment problem. To
the best of our knowledge the connection between
the decipherment problem and the quadratic as-
signment problem was not known.
The remainder of this paper is structured as
follows: In Section 2 we review related work.
Section 3 introduces the decipherment problem
and describes the notation and definitions used
throughout this paper. In Section 4 we show that
decipherment using a unigram language model
corresponds to solving a linear sum assignment
problem (LSAP). Section 5 shows the connection
between the quadratic assignment problem and de-
cipherment using a bigram language model. Here
we also give a reduction of the traveling sales-
man problem (TSP) to the decipherment problem
to highlight the additional complexity in the deci-
pherment problem.
2 Related Work
In recent years a large number of publications
on the automatic decipherment of substitution ci-
phers has been published. These publications were
mostly dominated by rather heuristic methods and
did not provide a theoretical analysis of the com-
plexity of the decipherment problem: (Knight and
Yamada, 1999) and (Knight et al, 2006) use the
EM algorithm for various decipherment problems,
like e.g. word substitution ciphers. (Ravi and
Knight, 2008) and (Corlett and Penn, 2010) are
able to obtain optimal (i.e. without search errors)
decipherments of short cryptograms given an n-
615
gram language model. (Ravi and Knight, 2011),
(Nuhn et al, 2012), and (Dou and Knight, 2012)
treat natural language translation as a deciphering
problem including phenomena like reordering, in-
sertion, and deletion and are able to train transla-
tion models using only monolingual data.
In this paper we will show the connection be-
tween the decipherment problem and the linear
sum assignment problem as well as the quadratic
assignment problem: Regarding the linear sum as-
signment problem we will make use of definitions
presented in (Burkard and ela, 1999). Concern-
ing the quadratic assignment problem we will use
basic definitions from (Beckmann and Koopmans,
1957). Further (Burkard et al, 1998) gives a good
overview over the quadratic assignment problem,
including different formulations, solution meth-
ods, and an analysis of computational complexity.
The paper also references a vast amount of fur-
ther literature that might be interesting for future
research.
3 Definitions
In the following we will use the machine trans-
lation notation and denote the ciphertext with
fN1 = f1 . . . fj . . . fN which consists of cipher
tokens fj ? Vf . We denote the plaintext with
eN1 = e1 . . . ei . . . eN (and its vocabulary Ve re-
spectively). We define
e0 = f0 = eN+1 = fN+1 = $ (1)
with ?$? being a special sentence boundary token.
We use the abbreviations V e = Ve ? {$} and V f
respectively.
A general substitution cipher uses a table
s(e|f) which contains for each cipher token f a
probability that the token f is substituted with the
plaintext token e. Such a table for substituting
cipher tokens {A,B,C,D} with plaintext tokens
{a, b, c, d} could for example look like
a b c d
A 0.1 0.2 0.3 0.4
B 0.4 0.2 0.1 0.3
C 0.4 0.1 0.2 0.3
D 0.3 0.4 0.2 0.1
The 1:1 substitution cipher encrypts a given
plaintext into a ciphertext by replacing each plain-
text token with a unique substitute: This means
that the table s(e|f) contains all zeroes, except for
one ?1.0? per f ? Vf and one ?1.0? per e ? Ve.
For example the text
abadcab
would be enciphered to
BCBADBC
when using the substitution
a b c d
A 0 0 0 1
B 1 0 0 0
C 0 1 0 0
D 0 0 1 0
We formalize the 1:1 substitutions with a bijective
function ? : Vf ? Ve. The general decipher-
ment goal is to obtain a mapping ? such that the
probability of the deciphered text is maximal:
?? = argmax
?
p(?(f1)?(f2)?(f3)...?(fN )) (2)
Here p(. . . ) denotes the language model. De-
pending on the structure of the language model
Equation 2 can be further simplified.
Given a ciphertext fN1 , we define the unigram
count Nf of f ? V f as1
Nf =
N+1?
i=0
?(f, fi) (3)
This implies that Nf are integer counts > 0. We
similarly define the bigram count Nff ? of f, f ? ?
V f as
Nff ? =
N+1?
i=1
?(f, fi?1) ? ?(f ?, fi) (4)
This definition implies that
(a) Nff ? are integer counts > 0 of bigrams found
in the ciphertext fN1 .
(b) Given the first and last token of the cipher f1
and fN , the bigram counts involving the sen-
tence boundary token $ need to fulfill
N$f = ?(f, f1) (5)
Nf$ = ?(f, fN ) (6)
(c) For all f ? Vf
?
f ??Vf
Nff ? =
?
f ??Vf
Nf ?f (7)
must hold.
1Here ? denotes the Kronecker delta.
616
Similarly, we define language model matrices S
for the unigram and the bigram case. The uni-
gram language model Sf is defined as
Sf = log p(f) (8)
which implies that
(a) Sf are real numbers with
Sf ? [??, 0] (9)
(b) The following normalization constraint holds:
?
f?Vf
exp(Sf ) = 1 (10)
Similarly for the bigram language model matrix
Sff ? , we define
Sff ? = log p(f ?|f) (11)
This definition implies that
(a) Sff ? are real numbers with
Sff ? ? [??, 0] (12)
(b) For the sentence boundary symbol, it holds
that
S$$ = ?? (13)
(c) For all f ? Vf the following normalization
constraint holds:
?
f ??Vf
exp(Sff ?) = 1 (14)
4 Decipherment Using Unigram LMs
4.1 Problem Definition
When using a unigram language model, Equa-
tion 2 simplifies to finding
?? = argmax
?
N?
i=1
p(?(fi)) (15)
which can be rewritten as
?? = argmax
?
?
f?Vf
NfS?(f) (16)
When defining cff ? = Nf log p(f ?), for f, f ? ?
Vf , Equation 16 can be brought into the form of
?? = argmax
?
?
f?Vf
cf?(f) (17)
Figure 1 shows an illustration of this problem.
A
B
C
a
b
c
Ve Vf
cij A B Ca NA log p(a) NB log p(a) NC log p(a)b NA log p(b) NB log p(b) NC log p(b)c NA log p(c) NB log p(c) NC log p(c)
Figure 1: Linear sum assignment problem for a
cipher with Ve = {a, b, c}, Vf = {A,B,C}, uni-
gram counts Nf , and unigram probabilities p(e).
4.2 The Linear Sum Assignment Problem
The practical problem behind the linear sum
assignment problem can be described as fol-
lows: Given jobs {j1, . . . , jn} and workers
{w1, . . . , wn}, the task is to assign each job ji to a
worker wj . Each assignment incurs a cost cij and
the total cost for assigning all jobs and workers is
to be minimized.
This can be formalized as finding the assign-
ment
?? = argmin
?
n?
i=1
ci?(i) (18)
The general LSAP can be solved in polynomial
time using the Hungarian algorithm (Kuhn, 1955).
However, since the matrix cij occurring for the de-
cipherment using a unigram language model can
be represented as the product cij = ai ? bj the
decipherment problem can be solved more easily:
In the Section ?Optimal Matching?, (Bauer, 2010)
shows that in this case the optimal assignment is
found by sorting the jobs ji by ai and workers wj
by bj and then assigning the jobs ji to workers wj
that have the same rank in the respective sorted
lists. Sorting and then assigning the elements can
be done in O(n log n).
5 Decipherment Using Bigram LMs
5.1 Problem Definition
When using a 2-gram language model, Equation 2
simplifies to
?? = argmax
?
?
?
?
N+1?
j=1
p(?(fj)|?(fj?1))
?
?
? (19)
617
xy
l1l2
l3 l4
Assignments
l1 l2 l3 l4(a) f1 f2 f3 f4(b) f1 f4 f3 f2
Flows
f1 f2 f3 f4
f1 1
f2 1
f3 1
f4 1
Figure 2: Hypothetical quadratic assignment prob-
lem with locations l1 . . . l4 and facilities f1 . . . f4
with all flows being zero except f1 ? f2 and
f3 ? f4. The distance between locations l1 . . . l4
is implicitly given by the locations in the plane,
implying a euclidean metric. Two example assign-
ments (a) and (b) are shown, with (b) having the
lower overall costs.
Using the definitions from Section 3, Equation 19
can be rewritten as
?? = argmax
?
?
?
?
?
f?Vf
?
f ??Vf
Nff ?S?(f)?(f ?)
?
?
? (20)
(Bauer, 2010) arrives at a similar optimization
problem for the ?combined method of frequency
matching? using bigrams and mentions that it can
be seen as a combinatorial problem for which an
efficient way of solving is not known. However,
he does not mention the close connection to the
quadratic assignment problem.
5.2 The Quadratic Assignment Problem
The quadratic assignment problem was introduced
by (Beckmann and Koopmans, 1957) for the fol-
lowing real-life problem:
Given a set of facilities {f1, . . . , fn} and a set
of locations {l1, . . . , ln} with distances for each
pair of locations, and flows for each pair of facili-
ties (e.g. the amount of supplies to be transported
between a pair of facilities) the problem is to as-
sign the facilities to locations such that the sum
of the distances multiplied by the corresponding
flows (which can be interpreted as total transporta-
tion costs) is minimized. This is visualized in Fig-
ure 2.
Following (Beckmann and Koopmans, 1957)
we can express the quadratic assignment problem
as finding
?? = argmin
?
?
?
?
n?
i=1
n?
j=1
aijb?(i)?(j) +
n?
i=1
ci?(i)
?
?
?
(21)
where A = (aij), B = (bij), C = (cij) ? Nn?n
and ? a permutation
? : {1, . . . , n} ? {1, . . . , n}. (22)
This formulation is often referred to as Koopman-
Beckman QAP and often abbreviated as
QAP (A,B,C). The so-called pure or ho-
mogeneous QAP
?? = argmin
?
?
?
?
n?
i=1
n?
j=1
aijb?(i)?(j)
?
?
? (23)
is obtained by setting cij = 0, and is often denoted
as QAP (A,B).
In terms of the real-life problem presented in
(Beckmann and Koopmans, 1957) the matrix A
can be interpreted as distance matrix for loca-
tions {l1 . . . ln} and B as flow matrix for facilities
{f1 . . . fn}.
(Sahni and Gonzalez, 1976) show that the
quadratic assignment problem is strongly NP-
hard.
We will now show the relation between the
quadratic assignment problem and the decipher-
ment problem.
5.3 Decipherment Problem  Quadratic
Assignment Problem
Every decipherment problem is directly a
quadratic assignment problem, since the ma-
trices Nff ? and Sff ? are just special cases of
the general matrices A and B required for the
quadratic assignment problem. Thus a reduction
from the decipherment problem to the quadratic
assignment problem is trivial. This means that all
algorithms capable of solving QAPs can directly
be used to solve the decipherment problem.
5.4 Quadratic Assignment Problem 
Decipherment Problem
Given QAP (A,B) with integer matrices A =
(aij), B = (bij) i, j ? {1, . . . , n} we construct
the count matrix Nff ? and language model ma-
trix Sff ? in such a way that the solution for the
decipherment problem implies the solution to the
618
quadratic assignment problem, and vice versa. We
will use the vocabularies V e = V f = {1, . . . , n+
3}, with n + 3 being the special sentence bound-
ary token ?$?. The construction of Nff ? and Sff ?
is shown in Figure 3.
To show the validity of our construction, we will
1. Show that Nff ? is a valid count matrix.
2. Show that Sff ? is a valid bigram language
model matrix.
3. Show that the decipherment problem and
the newly constructed quadratic assignment
problem are equivalent.
We start by showing that Nff ? is a valid count
matrix:
(a) By construction, Nff ? has integer counts that
are greater or equal to 0.
(b) By construction, Nff ? at boundaries is:
? N$f = ?(f, 1)
? Nf$ = ?(f, n+ 2)
(c) Regarding the properties ?
f ?
Nff ? =
?
f ?
Nf ?f :
? For all f ? {1, . . . , n} the count proper-
ties are equivalent to
a?f? +
?
f ?
a?ff ? = a??f +
?
f ?
a?f ?f + ?(f, 1)
(24)
which holds by construction of a??f and
a?f?.
? For f = n+1 the count property is equiv-
alent to
1 +
?
f ?
a?f ?? = 2 +
?
f ?
a??f ? (25)
which follows from Equation (24) by
summing over all f ? {1, . . . , n}.
? For f = n+2 and f = n+3, the condi-
tion is fulfilled by construction.
We now show that Sff ? is a valid bigram lan-
guage model matrix:
(a) By construction, Sff ? ? [??, 0] holds.
(b) By construction, S$$ = ?? holds.
(c) By the construction of b?f?, the values Sff ? ful-
fill ?f ? exp(Sff ?) = 1 for all f . This works
since all entries b?ff ? are chosen to be smaller
than ?log(n+ 2).
We now show the equivalence of the quadratic
assignment problem and the newly constructed de-
cipherment problem. For this we will use the defi-
nitions
A? = {1, . . . , n} (26)
B? = {n+ 1, n+ 2, n+ 3} (27)
We first show that solutions of the constructed
decipherment problem with score > ?? fulfill
?(f) = f for f ? B?.
All mappings ?, with ?(f) = f ? for any f ?
A? and f ? ? B? will induce a score of ?? since
for f ? A? all Nff > 0 and Sf ?f ? = ?? for
f ? ? B?. Thus any ? with score > ?? will fulfill
?(f) ? B? for f ? B?. Further, by enumerating all
six possible permutations, it can be seen that only
the ? with ?(f) = f for f ? B? induces a score of
> ??. Thus we can rewrite
n+3?
f=1
n+3?
f ?=1
Nff ?S?(f)?(f ?) (28)
to
?
f?A?
?
f?A?
Nff ?S?(f)?(f ?)
? ?? ?
(AA)
+
?
f?A?
?
f ??B?
Nff ?S?(f)f ?
? ?? ?
(AB)
+
?
f?B?
?
f ??A?
Nff ?Sf?(f ?)
? ?? ?
(BA)
+
?
f?B?
?
f ??B?
Nff ?Sff ?
? ?? ?
(BB)
Here
? (AB) is independent of ? since
?f ? A?, f ? ? {n+ 1, n+ 3} : Sff ? = S1f ?
(29)
and
?f ? A? : Nf,n+2 = 0 (30)
? (BA) is independent of ? since
?f ? ? A?, f ? B? : Sff ? = Sf1 (31)
? (BB) is independent of ?.
619
Nff ? =
?
??????????
a?11 a?12 ? ? ? a?1n a?1? 0 0
a?21 a?22 ? ? ? a?2n a?2? 0 0... ... . . . ... ... ... ...
a?n1 a?n2 ? ? ? a?nn a?n? 0 0
a??1 a??2 ? ? ? a??n 0 2 0
0 0 ? ? ? 0 1 0 1
1 0 ? ? ? 0 0 0 0
?
??????????
Sff ? =
?
???????????
b?11 b?12 ? ? ? b?1n ?2 b?1? ?2
b?21 b?22 ? ? ? b?2n ?2 b?2? ?2... ... . . . ... ... ... ...
b?n1 b?n2 ? ? ? b?nn ?2 b?n? ?2
?1 ?1 ? ? ? ?1 ?? ?1 ??
?2 ?2 ? ? ? ?2 ?2 ?? ?2
?0 ?0 ? ? ? ?0 ?? ?? ??
?
???????????
a?ff ? = aff ? ?min
f? f? ?
{af? f? ?}+ 1 b?ff ? = bff ? ?maxf? f? ?
{bf? f? ?} ? log(n+ 2)
a?f? = max
?
?
?
n?
f ?=1
af ?f ? aff ? , 0
?
?
?+ ?(f, 1) b?f? = log
?
?1?
n?
f ?=1
exp(b?ff ?)?
2
n+ 2
?
?
a??f ? = max
?
?
?
n?
f=1
aff ? ? af ?f , 0
?
?
? ?i = ? log(n+ i)
Figure 3: Construction of matrices Nff ? and Sff ? of the decipherment problem from matrices A = (aij)
and B = (bij) of the quadratic assignment problem QAP (A,B).
Thus, with some constant c, we can finally rewrite
Equation 28 as
c+
n?
f=1
n?
f ?=1
Nff ?S?(f)?(f ?) (32)
Inserting the definition of Nff ? and Sff ? (simpli-
fied using constants c?, and c??) we obtain
c+
n?
f=1
n?
f ?=1
(aff ? + c?)(b?(f)?(f ?) + c??) (33)
which is equivalent to the original quadratic as-
signment problem
argmax
?
?
?
n?
f=1
n?
f ?=1
aff ?b?(f)?(f ?)
?
?
? (34)
Thus we have shown that a solution to the
quadratic assignment problem in Equation 34 is
a solution to the decipherment problem in Equa-
tion 20 and vice versa. Assuming that calculat-
ing elementary functions can be done inO(1), set-
ting up Nff ? and Sff ? can be done in polynomial
time.2 Thus we have given a polynomial time re-
duction from the quadratic assignment problem to
2This is the case if we only require a fixed number of dig-
its precision for the log and exp operations.
the decipherment problem: Since the quadratic as-
signment problem is NP-hard, it follows that the
decipherment problem is NP-hard, too.
5.5 Traveling Salesman Problem 
Decipherment Problem
Using the above construction we can immediately
construct a decipherment problem that is equiva-
lent to the traveling salesman problem by using
the quadratic assignment problem formulation of
the traveling salesman problem.
Without loss of generality3 we assume that the
TSP?s distance matrix fulfills the constraints of a
bigram language model matrix Sff ? . Then the
count matrix Nff ? needs to be chosen as
Nff ? =
?
??????????
0 1 0 ? ? ? 0 0 0
0 0 1 ? ? ? 0 0 0
0 0 0 ? ? ? 0 0 0
... ... ... . . . ... ... ...
0 0 0 ? ? ? 0 1 0
0 0 0 ? ? ? 0 0 1
1 0 0 ? ? ? 0 0 0
?
??????????
(35)
which fulfills the constraints of a bigram count
matrix.
3The general case can be covered using the reduction
shown in Section 5.
620
This matrix corresponds to a ciphertext of the
form
$abcd$ (36)
and represents the tour of the traveling salesman in
an intuitive way. The mapping ? then only decides
in which order the cities are visited, and only costs
between two successive cities are counted.
This shows that the TSP is only a special case
of the decipherment problem.
6 Conclusion
We have shown the correspondence between solv-
ing 1:1 substitution ciphers and the linear sum as-
signment problem and the quadratic assignment
problem: When using unigram language models,
the decipherment problem is equivalent to the lin-
ear sum assignment problem and solvable in poly-
nomial time. For a bigram language model, the de-
cipherment problem is equivalent to the quadratic
assignment problem and is NP-hard.
We also pointed out that all available algorithms
for the quadratic assignment problem can be di-
rectly used to solve the decipherment problem.
To the best of our knowledge, this correspon-
dence between the decipherment problem and the
quadratic assignment problem has not been known
previous to our work.
Acknowledgements
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation.
References
Friedrich L. Bauer. 2010. Decrypted Secrets: Methods
and Maxims of Cryptology. Springer, 4th edition.
Martin J. Beckmann and Tjalling C. Koopmans. 1957.
Assignment problems and the location of economic
activities. Econometrica, 25(4):53?76.
Rainer E. Burkard and Eranda ela. 1999. Linear as-
signment problems and extensions. In Handbook
of Combinatorial Optimization - Supplement Volume
A, pages 75?149. Kluwer Academic Publishers.
Rainer E. Burkard, Eranda ela, Panos M. Pardalos, and
Leonidas S. Pitsoulis. 1998. The quadratic assign-
ment problem. In Handbook of Combinatorial Op-
timization, pages 241?338. Kluwer Academic Pub-
lishers.
Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040?1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 266?275,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Kevin Knight and Kenji Yamada. 1999. A computa-
tional approach to deciphering unknown scripts. In
Proceedings of the ACL Workshop on Unsupervised
Learning in Natural Language Processing, num-
ber 1, pages 37?44. Association for Computational
Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for deci-
pherment problems. In Proceedings of the Confer-
ence on Computational Linguistics and Association
of Computation Linguistics (COLING/ACL) Main
Conference Poster Sessions, pages 499?506, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for
the assignment problem. Naval Research Logistic
Quarterly, 2(1-2):83?97.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156?164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812?819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 12?21, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Sartaj Sahni and Teofilo Gonzalez. 1976. P-complete
approximation problems. Journal of the Association
for Computing Machinery (JACM), 23(3):555?565,
July.
621
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1568?1576,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Beam Search for Solving Substitution Ciphers
Malte Nuhn and Julian Schamper and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper we address the problem of
solving substitution ciphers using a beam
search approach. We present a concep-
tually consistent and easy to implement
method that improves the current state of
the art for decipherment of substitution ci-
phers and is able to use high order n-gram
language models. We show experiments
with 1:1 substitution ciphers in which the
guaranteed optimal solution for 3-gram
language models has 38.6% decipherment
error, while our approach achieves 4.13%
decipherment error in a fraction of time
by using a 6-gram language model. We
also apply our approach to the famous
Zodiac-408 cipher and obtain slightly bet-
ter (and near to optimal) results than pre-
viously published. Unlike the previous
state-of-the-art approach that uses addi-
tional word lists to evaluate possible deci-
pherments, our approach only uses a letter-
based 6-gram language model. Further-
more we use our algorithm to solve large
vocabulary substitution ciphers and im-
prove the best published decipherment er-
ror rate based on the Gigaword corpus of
7.8% to 6.0% error rate.
1 Introduction
State-of-the-art statistical machine translation
(SMT) systems use large amounts of parallel data
to estimate translation models. However, parallel
corpora are expensive and not available for every
domain.
Recently different works have been published
that train translation models using only non-
parallel data. Although first practical applications
of these approaches have been shown, the overall
decipherment accuracy of the proposed algorithms
is still low. Improving the core decipherment algo-
rithms is an important step for making decipher-
ment techniques useful for practical applications.
In this paper we present an effective beam
search algorithm which provides high decipher-
ment accuracies while having low computational
requirements. The proposed approach allows us-
ing high order n-gram language models, is scal-
able to large vocabulary sizes and can be adjusted
to account for a given amount of computational
resources. We show significant improvements in
decipherment accuracy in a variety of experiments
while being computationally more effective than
previous published works.
2 Related Work
The experiments proposed in this paper touch
many of previously published works in the deci-
pherment field.
Regarding the decipherment of 1:1 substitution
ciphers various works have been published: Most
older papers do not use a statistical approach and
instead define some heuristic measures for scoring
candidate decipherments. Approaches like (Hart,
1994) and (Olson, 2007) use a dictionary to check
if a decipherment is useful. (Clark, 1998) defines
other suitability measures based on n-gram counts
and presents a variety of optimization techniques
like simulated annealing, genetic algorithms and
tabu search.
On the other hand, statistical approaches for
1:1 substitution ciphers were published in the nat-
ural language processing community: (Ravi and
Knight, 2008) solve 1:1 substitution ciphers opti-
mally by formulating the decipherment problem as
an integer linear program (ILP) while (Corlett and
Penn, 2010) solve the problem using A? search.
We use our own implementation of these methods
to report optimal solutions to 1:1 substitution ci-
1568
phers for language model orders n = 2 and n = 3.
(Ravi and Knight, 2011a) report the first au-
tomatic decipherment of the Zodiac-408 cipher.
They use a combination of a 3-gram language
model and a word dictionary. We run our beam
search approach on the same cipher and report
better results without using an additional word
dictionary?just by using a high order n-gram lan-
guage model.
(Ravi and Knight, 2011b) report experiments on
large vocabulary substitution ciphers based on the
Transtac corpus. (Dou and Knight, 2012) improve
upon these results and provide state-of-the-art re-
sults on a large vocabulary word substitution ci-
pher based on the Gigaword corpus. We run our
method on the same corpus and report improve-
ments over the state of the art.
(Ravi and Knight, 2011b) and (Nuhn et al,
2012) have shown that?even for larger vocabu-
lary sizes?it is possible to learn a full translation
model from non-parallel data. Even though this
work is currently only able to deal with substi-
tution ciphers, phenomena like reordering, inser-
tions and deletions can in principle be included in
our approach.
3 Definitions
In the following we will use the machine trans-
lation notation and denote the ciphertext with
fN1 = f1 . . . fj . . . fN which consists of cipher
tokens fj ? Vf . We denote the plaintext with
eN1 = e1 . . . ei . . . eN (and its vocabulary Ve re-
spectively). We define
e0 = f0 = eN+1 = fN+1 = $ (1)
with ?$? being a special sentence boundary token.
We use the abbreviations V e = Ve ? {$} and V f
respectively.
A general substitution cipher uses a table
s(e|f) which contains for each cipher token f a
probability that the token f is substituted with the
plaintext token e. Such a table for substituting
cipher tokens {A,B,C,D} with plaintext tokens
{a, b, c, d} could for example look like
a b c d
A 0.1 0.2 0.3 0.4
B 0.4 0.2 0.1 0.3
C 0.4 0.1 0.2 0.3
D 0.3 0.4 0.2 0.1
The 1:1 substitution cipher encrypts a given
plaintext into a ciphertext by replacing each plain-
text token with a unique substitute: This means
that the table s(e|f) contains all zeroes, except for
one ?1.0? per f ? Vf and one ?1.0? per e ? Ve.
For example the text
abadcab
would be enciphered to
BCBADBC
when using the substitution
a b c d
A 0 0 0 1
B 1 0 0 0
C 0 1 0 0
D 0 0 1 0
In contrast to the 1:1 substitution cipher, the ho-
mophonic substitution cipher allows multiple ci-
pher tokens per plaintext token, which means that
the table s(e|f) is all zero, except for one ?1.0? per
f ? Vf . For example the above plaintext could be
enciphered to
ABCDECF
when using the homophonic substitution
a b c d
A 1 0 0 0
B 0 1 0 0
C 1 0 0 0
D 0 0 0 1
E 0 0 1 0
F 0 1 0 0
We will use the definition
nmax = maxe
?
f
s(e|f) (2)
to characterize the maximum number of different
cipher symbols allowed per plaintext symbol.
We formalize the 1:1 substitutions with a bijec-
tive function ? : Vf ? Ve and homophonic sub-
stitutions with a general function ? : Vf ? Ve.
Following (Corlett and Penn, 2010), we call
cipher functions ?, for which not all ?(f)?s are
fixed, partial cipher functions . Further, ?? is
said to extend ?, if for all f that are fixed in ?, it
holds that f is also fixed in ?? with ??(f) = ?(f).
1569
The cardinality of ? counts the number of fixed
f ?s in ?.
When talking about partial cipher functions we
use the notation for relations, in which ? ? Vf ?
Ve. For example with
? = {(A, a)} ?? = {(A, a), (B, b)}
it follows that ? ?1?? and
|?| = 1 |??| = 2
?(A) = a ??(A) = a
?(B) = undefined ??(B) = b
The general decipherment goal is to obtain a
mapping ? such that the probability of the deci-
phered text is maximal:
?? = argmax
?
p(?(f1)?(f2)?(f3)...?(fN )) (3)
Here p(. . . ) denotes the language model. De-
pending on the structure of the language model
Equation 3 can be further simplified.
4 Beam Search
In this Section we present our beam search ap-
proach to solving Equation 3. We first present the
general algorithm, containing many higher level
functions. We then discuss possible instances of
these higher level functions.
4.1 General Algorithm
Figure 1 shows the general structure of the beam
search algorithm for the decipherment of substi-
tution ciphers. The general idea is to keep track
of all partial hypotheses in two arrays Hs and Ht.
During search all possible extensions of the partial
hypotheses in Hs are generated and scored. Here,
the function EXT ORDER chooses which cipher
symbol is used next for extension, EXT LIMITS
decides which extensions are allowed, and SCORE
scores the new partial hypotheses. PRUNE then se-
lects a subset of these hypotheses which are stored
to Ht. Afterwards the array Hs is copied to Ht
and the search process continues with the updated
array Hs.
Due to the structure of the algorithm the car-
dinality of all hypotheses in Hs increases in each
step. Thus only hypotheses of the same cardinality
1shorthand notation for ?? extends ?
1: function BEAM SEARCH(EXT ORDER,
EXT LIMITS, PRUNE)
2: init sets Hs, Ht
3: CARDINALITY = 0
4: Hs.ADD((?, 0))
5: while CARDINALITY < |Vf | do
6: f = EXT ORDER[CARDINALITY]
7: for all ? ? Hs do
8: for all e ? Ve do
9: ?? := ? ? {(e, f)}
10: if EXT LIMITS(??) then
11: Ht.ADD(??,SCORE (??))
12: end if
13: end for
14: end for
15: PRUNE(Ht)
16: CARDINALITY = CARDINALITY + 1
17: Hs = Ht
18: Ht.CLEAR()
19: end while
20: return best scoring cipher function in Hs
21: end function
Figure 1: The general structure of the beam
search algorithm for decipherment of substitu-
tion ciphers. The high level functions SCORE,
EXT ORDER, EXT LIMITS and PRUNE are de-
scribed in Section 4.
are compared in the pruning step. When Hs con-
tains full cipher relations, the cipher relation with
the maximal score is returned.2
Figure 2 illustrates how the algorithm explores
the search space for a homophonic substitution ci-
pher. In the following we show several instances
of EXT ORDER, EXT LIMITS, SCORE, and PRUNE.
4.2 Extension Limits (EXT LIMITS)
In addition to the implicit constraint of ? being
a function Vf ? Ve, one might be interested in
functions of a specific form:
For 1:1 substitution ciphers
(EXT LIMITS SIMPLE) ? must fulfill that the
number of cipher letters f ? Vf that map to any
e ? Ve is at most one. Since partial hypotheses
violating this condition can never ?recover? when
being extended, it becomes clear that these partial
hypotheses can be left out from search.
2n-best output can be implemented by returning the n best
scoring hypotheses in the final array Hs.
1570
?a
b
c
d
a
bc
d
a
bc
d
a
bc
d
a
bc
d
. . .
. . .
. . .
. . .
. . .
a
bc
d
a
bc
d
a
bc
d
a
bc
d
. . .
. . .
. . .
. . .
a
bc
d
a
bc
d
a
bc
d
a
bc
d
B C A D
Figure 2: Illustration of the search space explored by the beam search algorithm with cipher vocabulary
Vf = {A,B,C,D}, plaintext vocabulary Ve = {a, b, c, d}, EXT ORDER = (B,C,A,D), homophonic
extension limits (EXT LIMITS HOMOPHONIC) with nmax = 4, and histogram pruning with nkeep = 4.
Hypotheses are visualized as nodes in the tree. The x-axis represents the extension order. At each level
only those 4 hypotheses that survived the histogram pruning process are extended.
Homophonic substitution ciphers can be han-
dled by the beam search algorithm, too. Here
the condition that ? must fulfill is that the num-
ber of cipher letters f ? Vf that map to any
e ? Ve is at most nmax (which we will call
EXT LIMITS HOMOPHONIC). As soon as this con-
dition is violated, all further extensions will also
violate the condition. Thus, these partial hypothe-
ses can be left out.
4.3 Score Estimation (SCORE)
The score estimation function needs to predict
how good or bad a partial hypothesis (cipher func-
tion) might become. We propose simple heuristics
that use the n-gram counts rather than the original
ciphertext. The following formulas consider the
2-gram case. Equations for higher n-gram orders
can be obtained analogously.
With Equation 3 in mind, we want to estimate
the best possible score
N+1?
j=1
p(??(fj)|??(fj?1)) (4)
which can be obtained by extensions ?? ? ?. By
defining counts3
Nff ? =
N+1?
i=1
?(f, fi?1)?(f ?, fi) (5)
3? denotes the Kronecker delta.
we can equivalently use the scores
?
f,f ??V f
Nff ? log p(??(f ?)|??(f)) (6)
Using this formulation it is easy to propose
a whole class of heuristics: We only present
the simplest heuristic, which we call TRIV-
IAL HEURISTIC. Its name stems from the fact that
it only evaluates those parts of a given ?? that are
already fixed, and thus does not estimate any fu-
ture costs. Its score is calculated as
?
f,f ????
Nff ? log p(??(f ?)|??(f)). (7)
Here f, f ? ? ?? denotes that f and f ? need to
be covered in ??. This heuristic is optimistic since
we implicitly use ?0? as estimate for the non fixed
parts of the sum, for which Nff ? log p(?|?) ? 0
holds.
It should be noted that this heuristic can be im-
plemented very efficiently. Given a partial hypoth-
esis ? with given SCORE(?) the score of an exten-
sion ?? can be calculated as
SCORE(??) = SCORE(?) + NEWLY FIXED(?, ??)
(8)
where NEWLY FIXED only includes scores for
n-grams that have been newly fixed in ?? during
the extension step from ? to ??.
1571
4.4 Extension Order (EXT ORDER)
For the choice which ciphertext symbol should be
fixed next during search, several possibilities ex-
ist: The overall goal is to choose an extension or-
der that leads to an overall low error rate. Intu-
itively it seems a good idea to first try to decipher
higher frequent words rather than the lowest fre-
quent ones. It is also clear that the choice of a good
extension order is dependent on the score estima-
tion function SCORE: The extension order should
lead to informative scores early on so that mislead-
ing hypotheses can be pruned out early.
In most of our experiments we will
make use of a very simple extension order:
HIGHEST UNIGRAM FREQUENCY simply fixes
the most frequent symbols first.
In case of the Zodiac-408, we use another strat-
egy that we call HIGHEST NGRAM COUNT ex-
tension order. In each step it greedily chooses
the symbol that will maximize the number of
fixed ciphertext n-grams. This strategy is use-
ful because the SCORE function we use is TRIV-
IAL HEURISTIC, which is not able to provide in-
formative scores if only few full n-grams are fixed.
4.5 Pruning (PRUNE)
We propose two pruning methods:
HISTOGRAM PRUNING sorts all hypotheses
according to their score and then keeps only the
best nkeep hypotheses.
THRESHOLD PRUNING keeps only those hy-
potheses ?keep for which
SCORE(?keep) ? SCORE(?best)? ? (9)
holds for a given parameter ? ? 0. Even though
THRESHOLD PRUNING has the advantage of not
needing to sort all hypotheses, it has proven dif-
ficult to choose proper values for ?. Due to this,
all experiments presented in this paper only use
HISTOGRAM PRUNING.
5 Iterative Beam Search
(Ravi and Knight, 2011b) propose a so called ?it-
erative EM algorithm?. The basic idea is to run a
decipherment algorithm?in their case an EM al-
gorithm based approach?on a subset of the vo-
cabulary. After having obtained the results from
the restricted vocabulary run, these results are used
to initialize a decipherment run with a larger vo-
cabulary. The results from this run will then be
used for a further decipherment run with an even
larger vocabulary and so on. In our large vocabu-
lary word substitution cipher experiments we it-
eratively increase the vocabulary from the 1000
most frequent words, until we finally reach the
50000 most frequent words.
6 Experimental Evaluation
We conduct experiments on letter based 1:1 sub-
stitution ciphers, the homophonic substitution ci-
pher Zodiac-408, and word based 1:1 substitution
ciphers.
For a given reference mapping ?ref , we eval-
uate candidate mappings ? using two error mea-
sures: Mapping Error Rate MER(?, ?ref ) and
Symbol Error Rate SER(?, ?ref ). Roughly
speaking, SER reports the fraction of symbols
in the deciphered text that are not correct, while
MER reports the fraction of incorrect mappings
in ?.
Given a set of symbols Veval with unigram
countsN(v) for v ? Veval, and the total amount of
running symbols Neval = ?
v?Veval
N(v) we define
MER = 1?
?
v?Veval
1
|Veval|
? ?(?(v), ?ref (v))
(10)
SER = 1?
?
v?Veval
N(v)
Neval
? ?(?(v), ?ref (v))
(11)
Thus the SER can be seen as a weighted form of
the MER, emphasizing errors for frequent words.
In decipherment experiments, SER will often be
lower than MER, since it is often easier to deci-
pher frequent words.
6.1 Letter Substitution Ciphers
As ciphertext we use the text of the English
Wikipedia article about History4, remove all pic-
tures, tables, and captions, convert all letters to
lowercase, and then remove all non-letter and non-
space symbols. This corpus forms the basis for
shorter cryptograms of size 2, 4, 8, 16, 32, 64, 128,
and 256?of which we generate 50 each. We make
sure that these shorter cryptograms do not end or
start in the middle of a word. We create the ci-
phertext using a 1:1 substitution cipher in which
we fix the mapping of the space symbol ? ?. This
4http://en.wikipedia.org/wiki/History
1572
Order Beam MER [%] SER [%] RT [s]
3 10 33.15 25.27 0.01
3 100 12.00 6.95 0.06
3 1k 7.37 3.06 0.53
3 10k 5.10 1.42 5.33
3 100k 4.93 1.31 47.70
3 ?? 4.93 1.31 19 700.00
4 10 55.97 48.19 0.02
4 100 18.15 14.41 0.10
4 1k 5.13 3.42 0.89
4 10k 1.55 1.00 8.57
4 100k 0.39 0.06 81.34
5 10 69.19 60.13 0.02
5 100 35.57 29.02 0.14
5 1k 10.89 8.47 1.29
5 10k 0.38 0.06 11.91
5 100k 0.38 0.06 120.38
6 10 74.65 64.77 0.03
6 100 40.26 33.38 0.17
6 1k 13.53 10.08 1.58
6 10k 2.45 1.28 15.77
6 100k 0.09 0.05 151.85
Table 1: Symbol error rates (SER), Mapping er-
ror rates (MER) and runtimes (RT) in dependence
of language model order (ORDER) and histogram
pruning size (BEAM) for decipherment of letter
substitution ciphers of length 128. Runtimes are
reported on a single core machine. Results for
beam size ??? were obtained using A? search.
makes our experiments comparable to those con-
ducted in (Ravi and Knight, 2008). Note that fix-
ing the ? ? symbol makes the problem much eas-
ier: The exact methods show much higher com-
putational demands for lengths beyond 256 letters
when not fixing the space symbol.
The plaintext language model we use a letter
based (Ve = {a, . . . , z, }) language model trained
on a subset of the Gigaword corpus (Graff et al,
2007).
We use extension limits fitting the 1:1 substi-
tution cipher nmax = 1 and histogram pruning
with different beam sizes.
For comparison we reimplemented the ILP ap-
proach from (Ravi and Knight, 2008) as well as
the A? approach from (Corlett and Penn, 2010).
Figure 3 shows the results of our algorithm for
different cipher length. We use a beam size of
100k for the 4, 5 and 6-gram case. Most remark-
ably our 6-gram beam search results are signifi-
cantly better than all methods presented in the lit-
erature. For the cipher length of 32 we obtain a
symbol error rate of just 4.1% where the optimal
solution (i.e. without search errors) for a 3-gram
2 4 8 16 32 64 128 2560
10
20
30
40
50
60
70
80
90
100
Cipher Length
Sy
mb
ol
Er
ror
Ra
te
(%
)
Exact 2gram
Exact 3gram
Beam 3gram
Beam 4gram
Beam 5gram
Beam 6gram
Figure 3: Symbol error rates for decipherment of
letter substitution ciphers of different lengths. Er-
ror bars show the 95% confidence interval based
on decipherment on 50 different ciphers. Beam
search was performed with a beam size of ?100k?.
language model has a symbol error rate as high as
38.3%.
Table 1 shows error rates and runtimes of our
algorithm for different beam sizes and language
model orders given a fixed ciphertext length of 128
letters. It can be seen that achieving close to op-
timal results is possible in a fraction of the CPU
time needed for the optimal solution: In the 3-
gram case the optimal solution is found in 1400 thof the time needed using A? search. It can also
be seen that increasing the language model order
does not increase the runtime much while provid-
ing better results if the beam size is large enough:
If the beam size is not large enough, the decipher-
ment accuracy decreases when increasing the lan-
guage model order: This is because the higher or-
der heuristics do not give reliable scores if only
few n-grams are fixed.
To summarize: The beam search method is sig-
nificantly faster and obtains significantly better re-
sults than previously published methods. Further-
more it offers a good trade-off between CPU time
and decipherment accuracy.
1573
i l i k e k i l l i n g p e o p l
e b e c a u s e i t i s s o m u c
h f u n i t i n m o r e f u n t h
a n k i l l i n g w i l d g a m e
i n t h e f o r r e s t b e c a u
s e m a n i s t h e m o a t r a n
g e r o u e a n a m a l o f a l l
t o k i l l s o m e t h i n g g i
Figure 4: First 136 letters of the Zodiac-408 cipher
and its decipherment.
6.2 Zodiac-408 Cipher
As ciphertext we use a transcription of the
Zodiac-408 cipher. It consists of 54 different sym-
bols and has a length of 408 symbols.5 The ci-
pher has been deciphered by hand before. It con-
tains some mistakes and ambiguities: For exam-
ple, it contains misspelled words like forrest (vs.
forest), experence (vs. experience), or paradice
(vs. paradise). Furthermore, the last 17 letters
of the cipher do not form understandable English
when applying the same homophonic substitution
that deciphers the rest of the cipher. This makes
the Zodiac-408 a good candidate for testing the ro-
bustness of a decipherment algorithm.
We assume a homophonic substitution cipher,
even though the cipher is not strictly homophonic:
It contains three cipher symbols that correspond
to two or more plaintext symbols. We ignore this
fact for our experiments, and count?in case of the
MER only?the decipherment for these symbols
as correct when the obtained mapping is contained
in the set of reference symbols. We use extension
limits with nmax = 8 and histogram pruning
with beam sizes of 10k up to 10M .
The plaintext language model is based on the
same subset of Gigaword (Graff et al, 2007) data
as the experiments for the letter substitution ci-
phers. However, we first removed all space sym-
5hence its name
Order Beam MER [%] SER [%] RT [s]
4 10k 71.43 67.16 222
4 100k 66.07 61.52 1 460
4 1M 39.29 34.80 12 701
4 10M 19.64 16.18 125 056
5 10k 94.64 96.57 257
5 100k 10.71 5.39 1 706
5 1M 8.93 3.19 14 724
5 10M 8.93 3.19 152 764
6 10k 87.50 84.80 262
6 100k 94.64 94.61 1 992
6 1M 8.93 2.70 17 701
6 10M 7.14 1.96 167 181
Table 2: Symbol error rates (SER), Mapping er-
ror rates (MER) and runtimes (RT) in dependence
of language model order (ORDER) and histogram
pruning size (BEAM) for the decipherment of the
Zodiac-408 cipher. Runtimes are reported on a
128-core machine.
bols from the training corpus before training the
actual letter based 4-gram, 5-gram, and 6-gram
language model on it. Other than (Ravi and
Knight, 2011a) we do not use any word lists and
by that avoid any degrees of freedom in how to in-
tegrate it into the search process: Only an n-gram
language model is used.
Figure 4 shows the first parts of the cipher and
our best decipherment. Table 2 shows the results
of our algorithm on the Zodiac-408 cipher for dif-
ferent language model orders and pruning settings.
To summarize: Our final decipherment?for
which we only use a 6-gram language model?has
a symbol error rate of only 2.0%, which is slightly
better than the best decipherment reported in (Ravi
and Knight, 2011a). They used an n-gram lan-
guage model together with a word dictionary and
obtained a symbol error rate of 2.2%. We thus ob-
tain better results with less modeling.
6.3 Word Substitution Ciphers
As ciphertext, we use parts of the JRC corpus
(Steinberger et al, 2006) and the Gigaword cor-
pus (Graff et al, 2007). While the full JRC corpus
contains roughly 180k word types and consists of
approximately 70M running words, the full Giga-
word corpus contains around 2M word types and
roughly 1.5G running words.
We run experiments for three different setups:
The ?JRC? and ?Gigaword? setups use the first
half of the respective corpus as ciphertext, while
the plaintext language model of order n = 3 was
1574
Setup Top MER [%] SER [%] RT [hh:mm]
Gigaword 1k 81.91 27.38 03h 10m
Gigaword 10k 30.29 8.55 09h 21m
Gigaword 20k 21.78 6.51 16h 25m
Gigaword 50k 19.40 5.96 49h 02m
JRC 1k 73.28 15.42 00h 32m
JRC 10k 15.82 2.61 13h 03m
JRC-Shuf 1k 76.83 19.04 00h 31m
JRC-Shuf 10k 15.08 2.58 13h 03m
Table 3: Word error rates (WER), Mapping error
rates (MER) and runtimes (RT) for iterative deci-
pherment run on the (TOP) most frequent words.
Error rates are evaluated on the full vocabulary.
Runtimes are reported on a 128-core machine.
trained on the second half. The ?JRC-Shuf? setup
is created by randomly selecting half of the sen-
tences of the JRC corpus as ciphertext, while the
language model was trained on the complemen-
tary half of the corpus.
We encrypt the ciphertext using a 1:1 substi-
tution cipher on word level, imposing a much
larger vocabulary size. We use histogram prun-
ing with a beam size of 128 and use extension
limits of nmax = 1. Different to the previous
experiments, we use iterative beam search with
iterations as shown in Table 3.
The results for the Gigaword task are directly
comparable to the word substitution experiments
presented in (Dou and Knight, 2012). Their fi-
nal decipherment has a symbol error rate of 7.8%.
Our algorithm obtains 6.0% symbol error rate. It
should be noted that the improvements of 1.8%
symbol error rate correspond to a larger improve-
ment in terms of mapping error rate. This can also
be seen when looking at Table 3: An improvement
of the symbol error rate from 6.51% to 5.96% cor-
responds to an improvement of mapping error rate
from 21.78% to 19.40%.
To summarize: Using our beam search algo-
rithm in an iterative fashion, we are able to im-
prove the state-of-the-art decipherment accuracy
for word substitution ciphers.
7 Conclusion
We have presented a simple and effective beam
search approach to the decipherment problem. We
have shown in a variety of experiments?letter
substitution ciphers, the Zodiac-408, and word
substitution ciphers?that our approach outper-
forms the current state of the art while being con-
ceptually simpler and keeping computational de-
mands low.
We want to note that the presented algorithm is
not restricted to 1:1 and homophonic substitution
ciphers: It is possible to extend the algorithm to
solve n:m mappings. Along with more sophis-
ticated pruning strategies, score estimation func-
tions, and extension orders, this will be left for fu-
ture research.
Acknowledgements
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. Experiments were
performed with computing resources granted by
JARA-HPC from RWTH Aachen University un-
der project ?jara0040?.
References
Andrew J. Clark. 1998. Optimisation heuristics for
cryptology. Ph.D. thesis, Faculty of Information
Technology, Queensland University of Technology.
Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040?1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 266?275,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Linguistic Data Consortium, Philadelphia.
George W. Hart. 1994. To decode short cryptograms.
Communications of the Association for Computing
Machinery (CACM), 37(9):102?108, September.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156?164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.
Edwin Olson. 2007. Robust dictionary attack of
short simple substitution ciphers. Cryptologia,
31(4):332?342, October.
1575
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812?819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011a. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
239?247, Portland, Oregon, June. Association for
Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011b. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 12?21, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz? Erjavec, and Dan Tufis?. 2006.
The JRC-Acquis: A multilingual aligned parallel
corpus with 20+ languages. In In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation (LREC), pages 2142?2147.
European Language Resources Association.
1576
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 759?764,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
EM Decipherment for Large Vocabularies
Malte Nuhn and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
This paper addresses the problem of EM-
based decipherment for large vocabular-
ies. Here, decipherment is essentially
a tagging problem: Every cipher token
is tagged with some plaintext type. As
with other tagging problems, this one can
be treated as a Hidden Markov Model
(HMM), only here, the vocabularies are
large, so the usual O(NV
2
) exact EM ap-
proach is infeasible. When faced with
this situation, many people turn to sam-
pling. However, we propose to use a type
of approximate EM and show that it works
well. The basic idea is to collect fractional
counts only over a small subset of links
in the forward-backward lattice. The sub-
set is different for each iteration of EM.
One option is to use beam search to do the
subsetting. The second method restricts
the successor words that are looked at, for
each hypothesis. It does this by consulting
pre-computed tables of likely n-grams and
likely substitutions.
1 Introduction
The decipherment of probabilistic substitution ci-
phers (ciphers in which each plaintext token can
be substituted by any cipher token, following a
distribution p(f |e), cf. Table 2) can be seen as
an important step towards decipherment for MT.
This problem has not been studied explicitly be-
fore. Scaling to larger vocabularies for proba-
bilistic substitution ciphers decipherment is a dif-
ficult problem: The algorithms for 1:1 or homo-
phonic substitution ciphers are not applicable, and
standard algorithms like EM training become in-
tractable when vocabulary sizes go beyond a few
hundred words. In this paper we present an effi-
cient EM based training procedure for probabilis-
tic substitution ciphers which provides high deci-
pherment accuracies while having low computa-
tional requirements. The proposed approach al-
lows using high order n-gram language models,
and is scalable to large vocabulary sizes. We show
improvements in decipherment accuracy in a va-
riety of experiments (including MT) while being
computationally more efficient than previous pub-
lished work on EM-based decipherment.
2 Related Work
Several methods exist for deciphering 1:1 substi-
tution ciphers: Ravi and Knight (2008) solve 1:1
substitution ciphers by formulating the decipher-
ment problem as an integer linear program. Cor-
lett and Penn (2010) solve the same problem us-
ing A
?
search. Nuhn et al (2013) present a beam
search approach that scales to large vocabulary
and high order language models. Even though be-
ing successful, these algorithms are not applicable
to probabilistic substitution ciphers, or any of its
extensions as they occur in decipherment for ma-
chine translation.
EM training for probabilistic ciphers was first
covered in Ravi and Knight (2011). Nuhn et al
(2012) have given an approximation to exact EM
training using context vectors, allowing to train-
ing models even for larger vocabulary sizes. Ravi
(2013) report results on the OPUS subtitle corpus
using an elaborate hash sampling technique, based
on n-gram language models and context vectors,
that is computationally very efficient.
Conventional beam search is a well studied
topic: Huang et al (1992) present beam search for
automatic speech recognition, using fine-grained
pruning procedures. Similarly, Young and Young
(1994) present an HMM toolkit, including pruned
forward-backward EM training. Pal et al (2006)
use beam search for training of CRFs.
759
Method Publications Complexity
EM Full (Knight et al, 2006), (Ravi and Knight, 2011) O(NV
n
)
EM Fixed Candidates (Nuhn et al, 2012) O(N)
EM Beam This Work O(NV )
EM Lookahead This Work O(N)
Table 1: Different approximations to exact EM training for decipherment. N is the cipher sequence
length, V the size of the target vocabulary, and n the order of the language model.
The main contribution of this work is the pre-
selection beam search that?to the best of our
knowledge?was not known in literature before,
and serves as an important step to applying EM
training to the large vocabulary decipherment
problem. Table 1 gives an overview of the EM
based methods. More details are given in Sec-
tion 3.2.
3 Probabilistic Substitution Ciphers
We define probabilistic substitutions ciphers us-
ing the following generative story for ciphertext
sequences f
N
1
:
1. Stochastically generate a plaintext sequence
e
N
1
according to a bigram
1
language model.
2. For each plaintext token e
n
choose a substi-
tution f
n
with probability P (f
n
|e
n
, ?).
This generative story corresponds to the model
p(e
N
1
, f
N
1
, ?) = p(e
N
1
) ? p(f
N
1
|e
N
1
, ?) , (1)
with the zero-order membership model
p(f
N
1
|e
N
1
, ?) =
N
?
n=1
p
lex
(f
n
|e
n
, ?) (2)
with parameters p(f |e, ?) ? ?
f |e
and normaliza-
tion constraints ?e
?
f
?
f |e
= 1, and first-order
plaintext sequence model
P (e
N
1
) =
N
?
n=1
p
LM
(e
n
|e
n?1
) . (3)
Thus, the probabilistic substitution cipher can be
seen as a Hidden Markov Model. Table 2 gives an
overview over the model. We want to find those
parameters ? that maximize the marginal distribu-
tion p(f
N
1
|?):
? = argmax
?
?
?
?
?
?
[e
N
1
]
p(f
N
1
, e
N
1
|?
?
)
?
?
?
(4)
1
This can be generalized to n-gram language models.
After we obtained the parameters ? we
can obtain e
N
1
as the Viterbi decoding
argmax
e
N
1
{
p(e
N
1
|f
N
1
, ?)
}
.
3.1 Exact EM training
In the decipherment setting, we are given the ob-
served ciphertext f
N
1
and the model p(f
N
1
|e
N
1
, ?)
that explains how the observed ciphertext has been
generated given a latent plaintext e
N
1
. Marginaliz-
ing the unknown e
N
1
, we would like to obtain the
maximum likelihood estimate of ? as specified in
Equation 4. We iteratively compute the maximum
likelihood estimate by applying the EM algorithm
(Dempster et al, 1977):
?
?
f |e
=
?
n:f
n
=f
p
n
(e|f
N
1
, ?)
?
f
?
n:f
n
=f
p
n
(e|f
N
1
, ?)
(5)
with
p
n
(e|f
N
1
, ?) =
?
[e
N
1
:e
n
=e]
p(e
N
1
|f
N
1
, ?) (6)
being the posterior probability of observing the
plaintext symbol e at position n given the cipher-
text sequence f
N
1
and the current parameters ?.
p
n
(e|f
N
1
, ?) can be efficiently computed using the
forward-backward algorithm.
3.2 Approximations to EM-Training
The computational complexity of EM training
stems from the sum
?
[e
N
1
:e
n
=e]
contained in the
posterior p
n
(e|f
N
1
, ?). However, we can approx-
imate this sum (and hope that the EM training
procedure is still working) by only evaluating the
dominating terms, i.e. we only evaluate the sum
for sequences e
N
1
that have the largest contribu-
tions to
?
[e
N
1
:e
n
=e]
. Note that due to this approxi-
mation, the new parameter estimates in Equation 5
can become zero. This is a critical issue, since
pairs (e, f) with p(f |e) = 0 cannot recover from
760
Sequence of cipher tokens : f
N
1
= f
1
, . . . , f
N
Sequence of plaintext tokens : e
N
1
= e
1
, . . . , e
N
Joint probability : p(f
N
1
, e
N
1
|?) = p(e
N
1
) ? p(f
N
1
|e
N
1
, ?)
Language model : p(e
N
1
) =
N
?
n=1
p
LM
(e
n
|e
n?1
)
Membership probabilities : p(f
N
1
|e
N
1
, ?) =
N
?
n=1
p
lex
(f
n
|e
n
, ?)
Paramater Set : ? = {?
f |e
}, p(f |e, ?) = ?
f |e
Normalization : ?e :
?
f
?
f |e
= 1
Probability of cipher sequence : p(f
N
1
|?) =
?
[e
N
1
]
p(f
N
1
, e
N
1
|?)
Table 2: Definition of the probabilistic substitution cipher model. In contrast to simple or homophonic
substitution ciphers, each plaintext token can be substituted by multiple cipher text tokens. The parameter
?
f |e
represents the probability of substituting token e with token f .
acquiring zero probability in some early iteration.
In order to allow the lexicon to recover from these
zeros, we use a smoothed lexicon ?p
lex
(f |e) =
?p
lex
(f |e) + (1 ? ?)/|V
f
| with ? = 0.9 when
conducting the E-Step.
3.2.1 Beam Search
Instead of evaluating the sum for terms with the
exact largest contributions, we restrict ourselves to
terms that are likely to have a large contribution to
the sum, dropping any guarantees about the actual
contribution of these terms.
Beam search is a well known algorithm related
to this idea: We build up sequences e
c
1
with grow-
ing cardinality c. For each cardinality, only a set
of the B most promising hypotheses is kept. Then
for each active hypothesis of cardinality c, all pos-
sible extensions with substitutions f
c+1
? e
c+1
are explored. Then in turn only the best B out of
the resulting B ? V
e
many hypotheses are kept and
the algorithm continues with the next cardinality.
Reaching the full cardinality N , the algorithm ex-
plored B ?N ? V
e
many hypotheses, resulting in a
complexity of O(BNV
e
).
Even though EM training using beam search
works well, it still suffers from exploring all V
e
possible extensions for each active hypothesis, and
thus scaling linearly with the vocabulary size. Due
to that, standard beam search EM training is too
slow to be used in the decipherment setting.
3.2.2 Preselection Search
Instead of evaluating all substitutions f
c+1
?
e
c+1
? V
e
, this algorithm only expands a fixed
number of candidates: For a hypothesis ending in
a language model state ?, we only look at B
LM
many successor words e
c+1
with the highest LM
probability p
LM
(e
c+1
|?) and at B
lex
many suc-
cessor words e
c+1
with the highest lexical prob-
ability p
lex
(f
c+1
|e
c+1
). Altogether, for each hy-
pothesis we only look at (B
LM
+B
lex
) many suc-
cessor states. Then, just like in the standard beam
search approach, we prune all explored new hy-
potheses and continue with the pruned set of B
many hypotheses. Thus, for a cipher of length N
we only explore N ? B ? (B
LM
+ B
lex
) many hy-
potheses.
2
Intuitively speaking, our approach solves the
EM training problem for decipherment using large
vocabularies by focusing only on those substitu-
tions that either seem likely due to the language
model (?What word is likely to follow the cur-
rent partial decipherment??) or due to the lexicon
model (?Based on my knowledge about the cur-
rent cipher token, what is the most likely substitu-
tion??).
In order to efficiently find the maximizing e for
p
LM
(e|?) and p
lex
(f |e), we build a lookup ta-
ble that contains for each language model state ?
the B
LM
best successor words e, and a separate
lookup table that contains for each source word f
the B
lex
highest scoring tokens e. The language
model lookup table remains constant during all it-
erations, while the lexicon lookup table needs to
be updated between each iteration.
Note that the size of the LM lookup table scales
linearly with the number of language model states.
Thus the memory requirements for the lookup ta-
2
We always use B = 100, B
lex
= 5, and B
LM
= 50.
761
f1 f2 f3 f4 f5
e5e4e3e2e1
Beam Search Preselection SearchFull Search
f6
...start
Vocab
Sentence
Figure 1: Illustration of the search space explored by full search, beam search, and preselection search.
Full search keeps all possible hypotheses at cardinality c and explores all possible substitutions at (c+1).
Beam search only keeps the B most promising hypotheses and then selects the best new hypotheses for
cardinality (c+ 1) from all possible substitutions. Preselection search keeps only the B best hypotheses
for every cardinality c and only looks at the (B
lex
+ B
LM
) most promising substitutions for cardinality
(c+ 1) based on the current lexicon (B
lex
dashed lines) and language model (B
LM
solid lines).
Name Lang. Sent. Words Voc.
VERBMOBIL English 27,862 294,902 3,723
OPUS
Spanish 13,181 39,185 562
English 19,770 61,835 411
Table 3: Statistics of the copora used in this pa-
per: The VERBMOBIL corpus is used to conduct
experiments on simple substitution ciphers, while
the OPUS corpus is used in our Machine Transla-
tion experiments.
ble do not form a practical problem of our ap-
proach. Figure 1 illustrates full search, beam
search, and our proposed method.
4 Experimental Evaluation
We first show experiments for data in which the
underlying model is an actual 1:1 substitution ci-
pher. In this case, we report the word accuracy
of the final decipherment. We then show experi-
ments for a simple machine translation task. Here
we report translation quality in BLEU. The cor-
pora used in this paper are shown in Table 3.
4.1 Simple Substitution Ciphers
In this set of experiments, we compare the exact
EM training to the approximations presented in
this paper. We use the English side of the German-
English VERBMOBIL corpus (Wahlster, 2000) to
construct a word substitution cipher, by substitut-
ing every word type with a unique number. In or-
der to have a non-parallel setup, we train language
Vocab LM Method Acc.[%] Time[h]
200 2 exact 97.19 224.88
200 2 beam 98.87 9.04
200 2 presel. 98.50 4.14
500 2 beam 92.12 24.27
500 2 presel. 92.16 4.70
3 661 3 beam 91.16 302.81
3 661 3 presel. 90.92 19.68
3 661 4 presel. 92.14 23.72
Table 4: Results for simple substitution ciphers
based on the VERBMOBIL corpus using exact,
beam, and preselection EM. Exact EM is not
tractable for vocabulary sizes above 200.
models of order 2, 3 and 4 on the first half of the
corpus and use the second half as ciphertext. Ta-
ble 4 shows the results of our experiments.
Since exact EM is not tractable for vocabulary
sizes beyond 200 words, we train word classes on
the whole corpus and map the words to classes
(consistent along the first and second half of the
corpus). By doing this, we create new simple sub-
stitution ciphers with smaller vocabularies of size
200 and 500. For the smallest setup, we can di-
rectly compare all three EM variants. We also in-
clude experiments on the original corpus with vo-
cabulary size of 3661. When comparing exact EM
training with beam- and preselection EM training,
the first thing we notice is that it takes about 20
times longer to run the exact EM training than
training with beam EM, and about 50 times longer
than the preselection EM training. Interestingly,
762
Model Method BLEU [%] Runtime
2-gram Exact EM(Ravi and Knight, 2011) 15.3 850.0h
whole segment lm Exact EM(Ravi and Knight, 2011) 19.3 850.0h
2-gram Preselection EM (This work) 15.7 1.8h
3-gram Preselection EM (This work) 19.5 1.9h
Table 5: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) on
the Spanish/English OPUS corpus using only non-parallel corpora for training.
the accuracy of the approximations to exact EM
training is better than that of the exact EM train-
ing. Even though this needs further investigation,
it is clear that the pruned versions of EM training
find sparser distributions p
lex
(f |e): This is desir-
able in this set of experiments, and could be the
reason for improved performance.
For larger vocabularies, exact EM training is not
tractable anymore. We thus constrain ourselves to
running experiments with beam and preselection
EM training only. Here we can see that the runtime
of the preselection search is roughly the same as
when running on a smaller vocabulary, while the
beam search runtime scales almost linearly with
the vocabulary size. For the full vocabulary of
3661 words, preselection EM using a 4-gram LM
needs less than 7% of the time of beam EM with a
3-gram LM and performs by 1% better in symbol
accuracy.
To summarize: Beam search EM is an or-
der of magnitude faster than exact EM training
while even increasing decipherment accuracy. Our
new preselection search method is in turn or-
ders of magnitudes faster than beam search EM
while even being able to outperform exact EM and
beam EM by using higher order language mod-
els. We were thus able to scale the EM deci-
pherment to larger vocabularies of several thou-
sand words. The runtime behavior is also consis-
tent with the computational complexity discussed
in Section 3.2.
4.2 Machine Translation
We show that our algorithm is directly applicable
to the decipherment problem for machine transla-
tion. We use the same simplified translation model
as presented by Ravi and Knight (2011). Because
this translation model allows insertions and dele-
tions, hypotheses of different cardinalities coex-
ist during search. We extend our search approach
such that pruning is done for each cardinality sep-
arately. Other than that, we use the same pres-
election search procedure as used for the simple
substitution cipher task.
We run experiments on the opus corpus as pre-
sented in (Tiedemann, 2009). Table 5 shows pre-
viously published results using EM together with
the results of our new method:
(Ravi and Knight, 2011) is the only publication
that reports results using exact EM training and
only n-gram language models on the target side:
It has an estimated runtime of 850h. All other
published results (using EM training and Bayesian
inference) use context vectors as an additional
source of information: This might be an explana-
tion why Nuhn et al (2012) and Ravi (2013) are
able to outperform exact EM training as reported
by Ravi and Knight (2011). (Ravi, 2013) reports
the most efficient method so far: It only consumes
about 3h of computation time. However, as men-
tioned before, those results are not directly compa-
rable to our work, since they use additional context
information on the target side.
Our algorithm clearly outperforms the exact
EM training in run time, and even slighlty im-
proves performance in BLEU. Similar to the sim-
ple substitution case, the improved performance
might be caused by inferring a sparser distribution
p
lex
(f |e). However, this requires further investi-
gation.
5 Conclusion
We have shown a conceptually consistent and easy
to implement EM based training method for deci-
pherment that outperforms exact and beam search
EM training for simple substitution ciphers and
decipherment for machine translation, while re-
ducing training time to a fraction of exact and
beam EM. We also point out that the preselection
method presented in this paper is not restricted to
word based translation models and can also be ap-
plied to phrase based translation models.
763
References
Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040?1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, B, 39.
Xuedong Huang, Fileno Alleva, Hsiao wuen Hon, Mei
yuh Hwang, and Ronald Rosenfeld. 1992. The
sphinx-ii speech recognition system: An overview.
Computer, Speech and Language, 7:137?148.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised Analysis for De-
cipherment Problems. In Proceedings of the
COLING/ACL on Main conference poster sessions,
COLING-ACL ?06, pages 499?506, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156?164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Annual Meeting of the Assoc. for Computational
Linguistics, pages 1569?1576, Sofia, Bulgaria, Au-
gust.
Chris Pal, Charles Sutton, and Andrew McCallum.
2006. Sparse forward-backward using minimum di-
vergence beams for fast training of conditional ran-
dom fields. In International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP).
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812?819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 12?21, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Sujith Ravi. 2013. Scalable decipherment for ma-
chine translation via hash sampling. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 362?371,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
J?org Tiedemann. 2009. News from OPUS - A col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237?248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of speech-to-speech translations. Springer-
Verlag, Berlin.
S.J. Young and Sj Young. 1994. The htk hidden
markov model toolkit: Design and philosophy. En-
tropic Cambridge Research Laboratory, Ltd, 2:2?
44.
764
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 304?311,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2012
Matthias Huck, Stephan Peitz, Markus Freitag, Malte Nuhn and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
This paper describes the statistical ma-
chine translation (SMT) systems developed at
RWTH Aachen University for the translation
task of the NAACL 2012 Seventh Workshop on
Statistical Machine Translation (WMT 2012).
We participated in the evaluation campaign
for the French-English and German-English
language pairs in both translation directions.
Both hierarchical and phrase-based SMT sys-
tems are applied. A number of different tech-
niques are evaluated, including an insertion
model, different lexical smoothing methods,
a discriminative reordering extension for the
hierarchical system, reverse translation, and
system combination. By application of these
methods we achieve considerable improve-
ments over the respective baseline systems.
1 Introduction
For the WMT 2012 shared translation task1 RWTH
utilized state-of-the-art phrase-based and hierarchi-
cal translation systems as well as an in-house sys-
tem combination framework. We give a survey of
these systems and the basic methods they implement
in Section 2. For both the French-English (Sec-
tion 3) and the German-English (Section 4) language
pair, we investigate several different advanced tech-
niques. We concentrate on specific research direc-
tions for each of the translation tasks and present the
respective techniques along with the empirical re-
sults they yield: For the French?English task (Sec-
tion 3.1), we apply a standard phrase-based system.
1http://www.statmt.org/wmt12/
translation-task.html
For the English?French task (Section 3.2), we aug-
ment a hierarchical phrase-based setup with a num-
ber of enhancements like an insertion model, dif-
ferent lexical smoothing methods, and a discrimina-
tive reordering extension. For the German?English
(Section 4.3) and English?German (Section 4.4)
tasks, we utilize morpho-syntactic analysis to pre-
process the data (Section 4.1) and employ sys-
tem combination to produce a consensus hypothesis
from normal and reverse translations (Section 4.2) of
phrase-based and hierarchical phrase-based setups.
2 Translation Systems
2.1 Phrase-Based System
The phrase-based translation (PBT) system used
in this work is an in-house implementation of the
state-of-the-art decoder described in (Zens and Ney,
2008). We use the standard set of models with
phrase translation probabilities and lexical smooth-
ing in both directions, word and phrase penalty,
distance-based distortion model, an n-gram target
language model and three binary count features. The
parameter weights are optimized with minimum er-
ror rate training (MERT) (Och, 2003).
2.2 Hierarchical Phrase-Based System
For our hierarchical phrase-based translation
(HPBT) setups, we employ the open source trans-
lation toolkit Jane (Vilar et al, 2010; Stein et
al., 2011; Vilar et al, 2012), which has been
developed at RWTH and is freely available for
non-commercial use. In hierarchical phrase-based
translation (Chiang, 2007), a weighted synchronous
context-free grammar is induced from parallel text.
304
In addition to contiguous lexical phrases, hierar-
chical phrases with up to two gaps are extracted.
The search is carried out with a parsing-based
procedure. The standard models integrated into our
Jane systems are: phrase translation probabilities
and lexical smoothing probabilities in both trans-
lation directions, word and phrase penalty, binary
features marking hierarchical phrases, glue rule,
and rules with non-terminals at the boundaries,
four binary count features, and an n-gram language
model. Optional additional models comprise IBM
model 1 (Brown et al, 1993), discriminative word
lexicon (DWL) models and triplet lexicon models
(Mauser et al, 2009), discriminative reordering ex-
tensions (Huck et al, 2011a), insertion and deletion
models (Huck and Ney, 2012), and several syntactic
enhancements like preference grammars (Stein
et al, 2010) and string-to-dependency features
(Peter et al, 2011). We utilize the cube pruning
algorithm (Huang and Chiang, 2007) for decoding
and optimize the model weights with MERT.
2.3 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses generated
with different translation engines. The basic concept
of RWTH?s approach to machine translation system
combination is described in (Matusov et al, 2006;
Matusov et al, 2008). This approach includes an
enhanced alignment and reordering framework. A
lattice is built from the input hypotheses. The trans-
lation with the best score within the lattice according
to a couple of statistical models is selected as con-
sensus translation.
2.4 Other Tools and Techniques
We employ GIZA++ (Och and Ney, 2003) to train
word alignments. The two trained alignments are
heuristically merged to obtain a symmetrized word
alignment for phrase extraction. All language mod-
els (LMs) are created with the SRILM toolkit (Stol-
cke, 2002) and are standard 4-gram LMs with in-
terpolated modified Kneser-Ney smoothing (Kneser
and Ney, 1995; Chen and Goodman, 1998). We
evaluate in truecase, using the BLEU (Papineni et al,
2002) and TER (Snover et al, 2006) measures.
French English
EP + NC Sentences 2.1M
Running Words 63.3M 57.6M
Vocabulary 147.8K 128.5K
Singletons 5.4K 5.1K
+ 109 Sentences 22.9M
Running Words 728.6M 624.0M
Vocabulary 1.7M 1.7M
Singletons 0.8M 0.8M
+ UN Sentences 35.4M
Running Words 1 113.5M 956.4M
Vocabulary 1.9M 2.0M
Singletons 0.9M 1.0M
Table 1: Corpus statistics of the preprocessed French-
English parallel training data. EP denotes Europarl, NC
denotes News Commentary. In the data, numerical quan-
tities have been replaced by a single category symbol.
3 French-English Setups
We trained phrase-based translation systems for
French?English and hierarchical phrase-based
translation systems for English?French. Corpus
statistics for the French-English parallel data are
given in Table 1. The LMs are 4-grams trained on
the provided resources for the respective language
(Europarl, News Commentary, UN, 109, and mono-
lingual News Crawl language model training data).2
For French?English we also investigate a smaller
English LM on Europarl and News Commentary
data only. For English?French we experiment with
additional target-side data from the LDC French Gi-
gaword Second Edition (LDC2009T28), which is an
archive of newswire text data that has been acquired
over several years by the LDC.3 The LDC French
Gigaword v2 is permitted for constrained submis-
sions in the WMT shared translation task. As a de-
velopment set for MERT, we use newstest2009 in all
setups.
3.1 Experimental Results French?English
For the French?English task, the phrase-based
SMT system (PBT) is set up using the standard mod-
els listed in Section 2.1. We vary the training data
we use to train the system and compare the results.
2The parallel 109 corpus is often also referred to as WMT
Giga French-English release 2.
3http://www.ldc.upenn.edu
305
newstest2008 newstest2009 newstest2010 newstest2011
French?English BLEU TER BLEU TER BLEU TER BLEU TER
PBT baseline 20.3 63.8 23.0 60.0 23.2 59.1 24.7 57.3
+ LM: +109+UN 22.5 61.4 26.2 57.3 26.6 56.1 27.7 54.5
+ TM: +109 23.3 60.8 27.6 56.2 27.6 55.4 29.1 53.4
Table 2: Results for the French?English task (truecase). newstest2009 is used as development set. BLEU and TER
are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011
English?French BLEU TER BLEU TER BLEU TER BLEU TER
HPBT 20.9 66.0 23.6 62.5 25.1 60.2 27.4 57.6
+ 109 and UN 22.5 63.2 25.4 59.8 27.0 57.1 29.9 53.9
+ LDC Gigaword v2 23.0 63.0 25.9 59.4 27.3 56.9 29.6 54.1
+ insertion model 23.0 62.9 26.1 59.2 27.2 56.8 30.0 53.7
+ noisy-or lexical scores 23.2 62.5 26.1 59.0 27.6 56.4 30.2 53.4
+ DWL 23.3 62.5 26.2 58.9 27.9 55.9 30.4 53.2
+ IBM-1 23.4 62.3 26.2 58.8 28.0 55.7 30.4 53.1
+ discrim. RO 23.5 62.2 26.7 58.5 28.1 55.9 30.8 52.8
Table 3: Results for the English?French task (truecase). newstest2009 is used as development set. BLEU and TER
are given in percentage.
It should be noted that these setups do not use any
English LDC Gigaword data for LM training at all.
Our baseline system uses the Europarl and News
Commentary data for training LM and phrase table.
Corpus statistics are shown in the ?EP+NC? section
of Table 1. This results in a performance of 24.7
points BLEU on newstest2011. Then we add the 109
as well as UN data and more monolingual English
data from the News Crawl corpus to the data used
for training the language model. This system ob-
tains a score of 27.7 points BLEU on newstest2011.
Our final system uses Europarl, News Commentary,
109 and UN data and News Crawl monolingual data
for LM training and the Europarl, News Commen-
tary and 109 data (Table 1) for phrase table training.
Using these data sets the system reaches 29.1 points
BLEU.
The experimental results are summarized in Ta-
ble 2.
3.2 Experimental Results English?French
For the English?French task, the baseline system is
a hierarchical phrase-based setup including the stan-
dard models as listed in Section 2.2, apart from the
binary count features. We limit the recursion depth
for hierarchical rules with a shallow-1 grammar (de
Gispert et al, 2010).
In a shallow-1 grammar, the generic non-terminal
X of the standard hierarchical approach is replaced
by two distinct non-terminals XH and XP . By
changing the left-hand sides of the rules, lexical
phrases are allowed to be derived from XP only, hi-
erarchical phrases from XH only. On all right-hand
sides of hierarchical rules, the X is replaced by XP .
Gaps within hierarchical phrases can thus solely be
filled with purely lexicalized phrases, but not a sec-
ond time with hierarchical phrases. The initial rule
is substituted with
S ? ?XP?0,XP?0?
S ? ?XH?0,XH?0? ,
(1)
and the glue rule is substituted with
S ? ?S?0XP?1, S?0XP?1?
S ? ?S?0XH?1, S?0XH?1? .
(2)
The main benefit of a restriction of the recursion
depth is a gain in decoding efficiency, thus allow-
ing us to set up systems more rapidly and to explore
more model combinations and more system config-
urations.
306
The experimental results for English?French are
given in Table 3. Starting from the shallow hi-
erarchical baseline setup on Europarl and News
Commentary parallel data only (but Europarl, News
Commentary, 109, UN, and News Crawl data for LM
training), we are able to improve translation qual-
ity considerably by first adopting more parallel (109
and UN) and monolingual (French LDC Gigaword
v2) training resources and then employing several
different models that are not included in the baseline
already. We proceed with individual descriptions of
the methods we use and report their respective effect
in BLEU on the test sets.
109 and UN (up to +2.5 points BLEU) While the
amount of provided parallel data from Europarl
and News Commentary sources is rather lim-
ited (around 2M sentence pairs in total), the
UN and the 109 corpus each provide a substan-
tial collection of further training material. By
appending both corpora, we end up at roughly
35M parallel sentences (cf. Table 1). We utilize
this full amount of data in our system, but ex-
tract a phrase table with only lexical (i.e. non-
hierarchical) phrases from the full parallel data.
We add it as a second phrase table to the base-
line system, with a binary feature that enables
the system to reward or penalize the application
of phrases from this table.
LDC Gigaword v2 (up to +0.5 points BLEU)
The LDC French Gigaword Second Edition
(LDC2009T28) provides some more monolin-
gual French resources. We include a total of
28.2M sentences from both the AFP and APW
collections in our LM training data.
insertion model (up to +0.4 points BLEU) We add
an insertion model to the log-linear model com-
bination. This model is designed as a means to
avoid the omission of content words in the hy-
potheses. It is implemented as a phrase-level
feature function which counts the number of in-
serted words. We apply the model in source-to-
target and target-to-source direction. A target-
side word is considered inserted based on lexi-
cal probabilities with the words on the foreign
language side of the phrase, and vice versa for
a source-side word. As thresholds, we compute
individual arithmetic averages for each word
from the vocabulary (Huck and Ney, 2012).
noisy-or lexical scores (up to +0.4 points BLEU) In
our baseline system, the tNorm(?) lexical scor-
ing variant as described in (Huck et al, 2011a)
is employed with a relative frequency (RF) lex-
icon model for phrase table smoothing. The
single-word based translation probabilities of
the RF lexicon model are extracted from word-
aligned parallel training data, in the fashion
of (Koehn et al, 2003). We exchange the base-
line lexical scoring with a noisy-or (Zens and
Ney, 2004) lexical scoring variant tNoisyOr(?).
DWL (up to +0.3 points BLEU) We augment
our system with phrase-level lexical scores
from discriminative word lexicon (DWL) mod-
els (Mauser et al, 2009; Huck et al, 2011a)
in both source-to-target and target-to-source di-
rection. The DWLs are trained on News Com-
mentary data only.
IBM-1 (up to +0.1 points BLEU) On News Com-
mentary and Europarl data, we train IBM
model-1 (Brown et al, 1993) lexicons in both
translation directions and also use them to com-
pute phrase-level scores.
discrim. RO (up to +0.4 points BLEU) The modi-
fication of the grammar to a shallow-1 version
restricts the search space of the decoder and is
convenient to prevent overgeneration. In order
not to be too restrictive, we reintroduce more
flexibility into the search process by extending
the grammar with specific reordering rules
XP ? ?XP?0XP?1,XP?1XP?0?
XP ? ?XP?0XP?1,XP?0XP?1? .
(3)
The upper rule in Equation (3) is a swap rule
that allows adjacent lexical phrases to be trans-
posed, the lower rule is added for symmetry
reasons, in particular because sequences as-
sembled with these rules are allowed to fill gaps
within hierarchical phrases. Note that we apply
a length constraint of 10 to the number of ter-
minals spanned by an XP . We introduce two
binary indicator features, one for each of the
two rules in Equation (3). In addition to adding
307
German English
Sentences 2.0M
Running Words 55.3M 55.7M
Vocabulary 191.6K 129.0K
Singletons 75.5K 51.8K
Table 4: Corpus statistics of the preprocessed German-
English parallel training data (Europarl and News Com-
mentary). In the data, numerical quantities have been re-
placed by a single category symbol.
these rules, a discriminatively trained lexical-
ized reordering model is applied (Huck et al,
2012).
4 German-English Setups
We trained phrase-based and hierarchical transla-
tion systems for both translation directions of the
German-English language pair. Corpus statistics for
German-English can be found in Table 4. The lan-
guage models are 4-grams trained on the respective
target side of the bilingual data as well as on the pro-
vided News Crawl corpus. For the English language
model the 109 French-English, UN and LDC Giga-
word Fourth Edition corpora are used additionally.
For the 109 French-English, UN and LDC Gigaword
corpora we apply the data selection technique de-
scribed in (Moore and Lewis, 2010). We examine
two different language models, one with LDC data
and one without. All German?English systems are
optimized on newstest2010. For English?German,
we use newstest2009 as development set. The news-
test2011 set is used as test set and the scores for new-
stest2008 are included for completeness.
4.1 Morpho-Syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the German text
is preprocessed by splitting German compound
words with the frequency-based method described in
(Koehn and Knight, 2003). To further reduce trans-
lation complexity of PBT, we employ the long-range
part-of-speech based reordering rules proposed by
Popovic? and Ney (2006).
4.2 Reverse Translation
For reverse translations we need to change the word
order of the bilingual corpus. For example, if we re-
verse both source and target language, the original
training example ?der Hund mag die Katze . ? the
dog likes the cat .? is converted into a new training
example ?. Katze die mag Hund der? . cat the likes
dog the?. We call this type of modification of source
or target language reversion. A system trained of
this data is called reverse. This modification changes
the corpora and hence the language model and align-
ment training produce different results.
4.3 Experimental Results German?English
Our results for the German?English task are shown
in Table 5. For this task, we apply the idea of reverse
translation for both the phrase-based and the hierar-
chical approach. It seems that the reversed systems
perform slightly worse. However, when we em-
ploy system combination using both reverse trans-
lation setups (PBT reverse and HPBT reverse) and
both baseline setups (PBT baseline and HPBT base-
line), the translation quality is improved by up to 0.4
points in BLEU and 1.0 points TER compared to the
best single system.
The addition of LDC Gigaword corpora (+GW)
to the language model training data of the baseline
setups shows improvements in both BLEU and TER.
Furthermore, with the system combination including
these setups, we are able to report an improvement
of up to 0.7 points BLEU and 1.0 points TER over the
best single setup. Compared to the system combina-
tion based on systems which are not using the LDC
Gigaword corpora, we gain 0.3 points in BLEU and
0.4 points in TER.
4.4 Experimental Results English?German
Our results for the English?German task are shown
in Table 6. For this task, we first compare sys-
tems using one, two or three language models of
different parts of the data. The language model
for systems with only one language model is cre-
ated with all monolingual and parallel data. A lan-
guage model with all monolingual data and a lan-
guage model with all parallel data is created for the
systems with two language models. For the systems
with three language models, we also split the parallel
data in two parts consisting of either only Europarl
data or only News Commentary data. For PBT the
system with two language models performs best for
all test sets. Further, we apply the idea of reverse
308
newstest2008 newstest2009 newstest2010 newstest2011
German?English BLEU TER BLEU TER BLEU TER BLEU TER
PBT baseline 21.1 62.3 20.8 61.4 23.7 59.3 21.3 61.3
PBT reverse 20.8 62.4 20.6 61.5 23.6 59.2 21.2 61.2
HPBT baseline 21.3 62.5 20.9 61.7 23.9 59.4 21.3 61.6
HPBT reverse 21.2 63.5 20.9 62.0 23.6 59.2 21.4 61.9
system combination (secondary) 21.5 61.6 21.2 60.6 24.3 58.3 21.7 60.3
PBT baseline +GW 21.5 61.9 21.2 61.1 24.0 59.0 21.3 61.4
PBT reverse 20.8 62.4 20.6 61.5 23.6 59.2 21.2 61.2
HPBT baseline +GW 21.6 62.3 21.3 61.3 24.0 59.4 21.6 61.5
HPBT reverse 21.2 63.5 20.9 62.0 23.6 59.2 21.4 61.9
system combination (primary) 21.9 61.2 21.4 60.5 24.7 58.0 21.9 60.2
Table 5: Results for the German?English task (truecase). +GW denotes the usage of LDC Gigaword data for the
language model, newstest2010 serves as development set. BLEU and TER are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011
English?German BLEU TER BLEU TER BLEU TER BLEU TER
PBT baseline 1 LM 14.6 71.7 14.8 70.8 15.8 66.9 15.3 70.0
PBT baseline 2 LM (*) 14.9 70.9 14.9 70.4 16.0 66.3 15.4 69.5
PBT baseline 3 LM 14.8 71.5 14.9 70.5 16.0 66.7 15.1 70.1
PBT reverse 2 LM (*) 14.9 71.4 15.1 70.2 15.9 66.5 15.0 69.7
HPBT baseline 2 LM (*) 15.1 71.8 15.3 71.1 16.2 67.4 15.4 70.3
HPBT baseline 2 LM opt on 4bleu-ter 15.2 68.4 15.0 67.7 15.9 64.6 15.1 67.1
HPBT reverse 2 LM (*) 15.4 71.3 15.3 70.7 16.7 66.9 15.5 70.1
syscombi of (*) 15.6 69.2 15.4 68.9 16.5 65.0 15.6 68.0
Table 6: Results for the English?German task (truecase). newstest2009 is used as development set. BLEU and TER
are given in percentage.
translation for both the phrase-based and the hier-
archical approach. The PBT reverse 2 LM systems
perform slightly worse compared to PBT baseline 2
LM. The HPBT reverse 2 LM performs better com-
pared to HPBT baseline 2 LM. When we employ
system combination using both reverse translation
setups (PBT reverse 2 LM and HPBT reverse 2 LM)
and both baseline setups (PBT baseline 2 LM and
HPBT baseline 2 LM), the translation quality is im-
proved by up to 0.2 points in BLEU and 2.1 points in
TER compared to the best single system.
5 Conclusion
For the participation in the WMT 2012 shared trans-
lation task, RWTH experimented with both phrase-
based and hierarchical translation systems. Several
different techniques were evaluated and yielded con-
siderable improvements over the respective base-
line systems as well as over our last year?s setups
(Huck et al, 2011b). Among these techniques are
an insertion model, the noisy-or lexical scoring vari-
ant, additional phrase-level lexical scores from IBM
model 1 and discriminative word lexicon models, a
discriminative reordering extension for hierarchical
translation, reverse translation, and system combi-
nation.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The Mathemat-
309
ics of Statistical Machine Translation: Parameter Es-
timation. Computational Linguistics, 19(2):263?311,
June.
Stanley F. Chen and Joshua Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report TR-10-98, Computer
Science Group, Harvard University, Cambridge, Mas-
sachusetts, USA, August.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hierar-
chical Phrase-Based Translation with Weighted Finite-
State Transducers and Shallow-n Grammars. Compu-
tational Linguistics, 36(3):505?533.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 144?151,
Prague, Czech Republic, June.
Matthias Huck and Hermann Ney. 2012. Insertion
and Deletion Models for Statistical Machine Trans-
lation. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics -
Human Language Technologies conference, Montreal,
Canada, June.
Matthias Huck, Saab Mansour, Simon Wiesler, and Her-
mann Ney. 2011a. Lexicon Models for Hierarchical
Phrase-Based Machine Translation. In International
Workshop on Spoken Language Translation, pages
191?198, San Francisco, California, USA, December.
Matthias Huck, Joern Wuebker, Christoph Schmidt,
Markus Freitag, Stephan Peitz, Daniel Stein, Arnaud
Dagnelies, Saab Mansour, Gregor Leusch, and Her-
mann Ney. 2011b. The RWTH Aachen Machine
Translation System for WMT 2011. In EMNLP 2011
Sixth Workshop on Statistical Machine Translation,
pages 405?412, Edinburgh, UK, July.
Matthias Huck, Stephan Peitz, Markus Freitag, and Her-
mann Ney. 2012. Discriminative Reordering Exten-
sions for Hierarchical Phrase-Based Machine Transla-
tion. In 16th Annual Conference of the European As-
sociation for Machine Translation, Trento, Italy, May.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, volume 1, pages 181?
184, May.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In Proceedings of Euro-
pean Chapter of the ACL (EACL 2009), pages 187?
194.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of the Human Language Technology Conf.
(HLT-NAACL), pages 127?133, Edmonton, Canada,
May/June.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Multi-
ple Machine Translation Systems Using Enhanced Hy-
potheses Alignment. In Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 33?40, Trento, Italy, April.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Proc. of
the Conf. on Empirical Methods for Natural Language
Processing (EMNLP), pages 210?218, Singapore, Au-
gust.
Robert C. Moore and William Lewis. 2010. Intelli-
gent Selection of Language Model Training Data. In
ACL (Short Papers), pages 220?224, Uppsala, Swe-
den, July.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, Penn-
sylvania, USA, July.
Jan-Thorsten Peter, Matthias Huck, Hermann Ney, and
Daniel Stein. 2011. Soft String-to-Dependency Hier-
archical Machine Translation. In International Work-
shop on Spoken Language Translation, pages 246?
253, San Francisco, California, USA, December.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Translation.
In International Conference on Language Resources
and Evaluation, pages 1278?1283, Genoa, Italy, May.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
310
Association for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA, Au-
gust.
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Features
for Hierarchical Machine Translation. In Conf. of the
Association for Machine Translation in the Americas
(AMTA), Denver, Colorado, USA, October/November.
Daniel Stein, David Vilar, Stephan Peitz, Markus Fre-
itag, Matthias Huck, and Hermann Ney. 2011. A
Guide to Jane, an Open Source Hierarchical Trans-
lation Toolkit. The Prague Bulletin of Mathematical
Linguistics, (95):5?18, April.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf. on
Speech and Language Processing (ICSLP), volume 2,
pages 901?904, Denver, Colorado, USA, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open source hierarchical transla-
tion, extended with reordering and lexicon models. In
ACL 2010 Joint Fifth Workshop on Statistical Machine
Translation and Metrics MATR, pages 262?270, Upp-
sala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2012. Jane: an advanced freely available hier-
archical machine translation toolkit. Machine Trans-
lation, pages 1?20. http://dx.doi.org/10.1007/s10590-
011-9120-y.
Richard Zens and Hermann Ney. 2004. Improve-
ments in Phrase-Based Statistical Machine Transla-
tion. In Proc. Human Language Technology Conf. /
North American Chapter of the Association for Com-
putational Linguistics Annual Meeting (HLT-NAACL),
pages 257?264, Boston, Massachusetts, USA, May.
Richard Zens and Hermann Ney. 2008. Improvements
in Dynamic Programming Beam Search for Phrase-
based Statistical Machine Translation. In Interna-
tional Workshop on Spoken Language Translation,
pages 195?205, Honolulu, Hawaii, USA, October.
311
