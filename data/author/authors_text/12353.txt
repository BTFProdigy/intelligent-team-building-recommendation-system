Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 25?30,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Joint memory-based learning of syntactic and semantic dependencies
in multiple languages
Roser Morante, Vincent Van Asch
CNTS - Language Technology Group
University of Antwerp
Prinsstraat 13
B-2000 Antwerpen, Belgium
{Roser.Morante,Vincent.VanAsch}@ua.ac.be
Antal van den Bosch
Tilburg University
Tilburg centre for Creative Computing
P.O. Box 90153
NL-5000 LE Tilburg, The Netherlands
Antal.vdnBosch@uvt.nl
Abstract
In this paper we present a system submitted to the
CoNLL Shared Task 2009 performing the identifi-
cation and labeling of syntactic and semantic depen-
dencies in multiple languages. Dependencies are
truly jointly learned, i.e. as if they were a single
task. The system works in two phases: a classifica-
tion phase in which three classifiers predict different
types of information, and a ranking phase in which
the output of the classifiers is combined.
1 Introduction
In this paper we present the machine learning system
submitted to the CoNLL Shared Task 2009 (Hajic?
et al, 2009). The task is an extension to multi-
ple languages (Burchardt et al, 2006; Hajic? et al,
2006; Kawahara et al, 2002; Palmer and Xue, 2009;
Surdeanu et al, 2008; Taule? et al, 2008) of the
CoNLL Shared Task 2008, combining the identifica-
tion and labeling of syntactic dependencies and se-
mantic roles. Our system is a joint-learning system
tested in the ?closed? challenge, i.e. without making
use of external resources.
Our system operates in two phases: a classifica-
tion phase in which three memory-based classifiers
predict different types of information, and a rank-
ing phase in which the output of the classifiers is
combined by ranking the predictions. Semantic and
syntactic dependencies are jointly learned and pro-
cessed. In the task description no precise defini-
tion is given of joint learning. We consider that a
joint-learning system is one in which semantic and
syntactic dependencies are learned and processed
jointly as a single task. In our system this is achieved
by fully merging semantic and syntactic dependen-
cies at the word level as the first step.
One direct consequence of merging the two tasks,
is that the class space becomes more complex;
the number of classes increases. Many machine-
learning approaches do not scale well to larger class
spaces in terms of efficiency and computer resource
requirements. Memory-based learning is a noted ex-
ception, as it is largely insensitive to the number of
classes in terms of efficiency. This is the primary
reason for using memory-based learning. Memory-
based language processing (Daelemans and van den
Bosch, 2005) is based on the idea that NLP prob-
lems can be solved by storing solved examples of the
problem in their literal form in memory, and apply-
ing similarity-based reasoning on these examples in
order to solve new ones. Memory-based algorithms
have been previously applied to semantic role la-
beling and parsing separately (Morante et al, 2008;
Canisius and Tjong Kim Sang, 2007).
We briefly discuss the issue of true joint learning
of two tasks in Section 2. The system is described
in Section 3, Section 4 presents and discusses the
results, and in Section 5 we put forward some con-
clusions and future research.
2 Joint learning
When two tasks share the same feature space, there
is the natural option to merge them and consider the
merge as a single task. The merging of two tasks
will typically lead to an increase in the number of
classes, and generally a more complex class space.
In practice, if two combined tasks are to some ex-
25
tent related, the increase will tend to be less than the
product of the number of classes in the two original
tasks, as classes from both tasks will tend to cor-
relate. Yet, even a mild increase of the number of
classes leads to a further fragmentation of the class
space, and thus to less training examples per class
label. Joint learning can therefore only lead to posi-
tive results if the data sparsity effect of the fragmen-
tation of the class space is counter-balanced by an
improved learnability.
Here, we treat the syntactic and semantic tasks as
one and the same task. At the word level, we merge
the class labels of the two tasks into single labels,
and present the classifiers with these labels. Further
on in our system, as we describe in the next sec-
tion, we do make use of the compositionality of the
labels, as the semantic and syntactic output spaces
represented two different types of structure.
3 System description
The joint system that we submitted works in
two phases: a classification phase in which three
memory-based classifiers predict different aspects of
joint syntactic and semantic labeling, and a ranking
phase in which the output of the classifiers is com-
bined. Additionally, a memory-based classifier is
used for predicate sense disambiguation. As a first
step, before generating the instances of the classi-
fiers we merge the semantic and syntactic dependen-
cies into single labels. The merged version of the
dependencies from an example sentence is shown
in Table 1, where column MERGED DEPs contains
all the dependencies of a token separated by a blank
space expressed in labels with the following format:
PHEAD::PDEPREL:APRED.
3.1 Phase 1: Classification
In the classification phase, three classifiers predict
different local aspects of the global output structure.
The classifiers have been optimized for English, by
training on the full training set and testing on the
development set; these optimized settings were then
used for the other six languages. We experimented
with manually selected parameters and with param-
eters selected by a genetic algorithm, but the param-
eters found by the genetic algorithm did not yield
better results than the manually selected parameters.
N Token Merged Dependencies
1 Housing 2::NMOD:A1
2 starts 2:: :A2 3::SBJ: 4:: :A1 6:: :A1 13:: :A0
3 are 0::ROOT:
4 expected 3::VC:
5 to 4::OPRD:C-A1
6 quicken 5::IM:
7 a 8::NMOD:
8 bit 6::OBJ:A2
9 from 6::ADV:A3
10 August 13::NMOD:AM-TMP
11 ?s 10::SUFFIX:
12 annual 13::NMOD:AM-TMP
13 pace 9::PMOD:
14 of 13::NMOD:A2
15 1,350,000 16::NMOD:
16 units 14::PMOD:
17 . 3::P:
Table 1: Example sentence with merged depen-
dency labels.
3.1.1 Classifier 1: Pairwise semantic and
syntact dependencies
Classifier 1 predicts the merged semantic and syn-
tactic dependencies that hold between two tokens.
Instances represent combinations of pairs of tokens
within a sentence. Each token is combined with all
other tokens in the sentence. The class predicted is
the PDEPREL:APRED label. The amount of classes
per language is shown in Table 2 (?Classifier 1?).
Number of classes
Lang. Classifier 1 Classifier 2
Cat 111 111
Chi 309 1209
Cze 395 1221
Eng 351 1957
Ger 152 300
Jap 103 505
Spa 124 124
Table 2: Number of classes per language predicted
by Classifiers 1 and 2.
We use an IB1 memory?based algorithm as im-
plemented in TiMBL (version 6.1.2) 1, a memory-
based classifier based on the k-nearest neighbor
1TiMBL: http://ilk.uvt.nl/timbl
26
rule. The IB1 algorithm was parameterised by us-
ing modified value difference as the similarity met-
ric, gain ratio for feature weighting, using 11 k-
nearest neighbors, and weighting the class vote of
neighbors as a function of their inverse linear dis-
tance. Because of time limitations we used TRIBL
for Czech and Chinese to produce the official results,
although we also provide postevaluation results pro-
duced with IB1. TRIBL is a hybrid combination
of IB1 and IGTREE, a fast decision-tree approxi-
mation of k-NN (Daelemans and van den Bosch,
2005), trading off fast decision-tree lookup on the
most important features (in our experiments, five)
with slower k-NN classification on the remaining
features.
The features2 used by this classifier are:
? The word, lemma, POS and FILLPRED3 of the to-
ken, the combined token and of two tokens before
and after token and combined token.
? POS and FILLPRED of the third token before and
after token and combined token.
? Distance between token and combined token, loca-
tion of token in relation to combined token.
Because data are skewed towards the NONE
class, we downsampled the training instances so that
there would be a negative instance for every positive
instance. Instances with the NONE class to be kept
were randomly selected.
3.1.2 Classifier 2: Per-token relations
Classifier 2 predicts the labels of the dependency
relations of a token with its syntactic and/or seman-
tic head(s). Instances represent a token. As an ex-
ample, the instance that represents token 2 in Table 1
would have as class: :A2-SBJ: - :A1- :A1- :A0.
The amount of classes per language is shown in Ta-
ble 2 under ?Classifier 2?. The number of classes
exceeds 1,000 for Chinese, Czech, and English.
The features used by the classifier are the word,
lemma, POS and FILLPRED of the token and two
tokens before and after the token. We use the IB1
memory?based algorithm parameterised in the same
way as Classifier 1.
2POS refers to predicted part-of-speech and lemma to pre-
dicted lemma in the description of features for all classifiers.
3The FILLPRED column has value Y if a token is a predi-
cate.
3.1.3 Classifier 3: Pairwise detection of a
relation
Classifier 3 is a binary classifier that predicts
whether two tokens have a dependency relation. In-
stance representation follows the same scheme as
with Classifier 1. We use the IGTREE algorithm as
implemented in TiMBL. The data are also skewed
towards the NONE class, so we downsampled the
training instances so that there would be a negative
instance for every four positive instances.
The features used by this classifier are:
? The word, lemma, POS and FILLPRED of the to-
ken, of the combined token, and of two tokens be-
fore and after the token.
? Word and lemma of two tokens before and after
combined token.
? Distance between token and combined token.
3.1.4 Results
The results of the Classifiers are presented in Ta-
ble 3. The performance of Classifiers 1 and 3 is sim-
ilar across languages, whereas the scores for Clas-
sifier 2 are lower for Chinese, Czech and English.
This can be explained by the fact that the number of
classes that Classifier 2 predicts for these languages
is significantly higher.
Lang. C1 C2 C3
Cat 94.77 86.30 97.96
Chi 92.10 70.11 95.47
Cze 87.33 67.87 93.88
Eng 94.17 76.16 95.37
Ger 92.76 83.23 93.77
Jap 91.55 81.22 96.75
Spa 94.76 84.40 96.39
Table 3: Micro F1 scores per classifier (C) and per
language.
Training times for the three classifiers were rea-
sonably short, as is to be expected with memory-
based classification. With English, C2 takes just
over two minutes to train, and C3 half a minute. C1
takes 8 hours and 18 minutes, due to the much larger
amount of examples and features.
3.2 Phase 2: Ranking
The classifier that is at the root of generating the
desired output (dependency graphs and semantic
27
role assignments) is Classifier 1, which predicts the
merged semantic and syntactic dependencies that
hold between two tokens (PDEPREL:APRED la-
bels). If this classifier would be able to predict the
dependencies with 100% accuracy, no further pro-
cessing would be necessary. Naturally, however, the
classifier predicts incorrect dependencies to a certain
degree, and does not provide a graph in wich all to-
kens have at least a syntactic head. It achieves 51.3%
labeled macro F1. The ranking phase improves this
performance. This is done in three steps: (i) ranking
the predictions of Classifier 1; (ii) constructing an
intermediate dependency tree, and (iii) adding extra
semantic dependencies to the tree.
3.2.1 Ranking predictions of Classifier 1
In order to disambiguate between all possible de-
pendencies predicted by this classifier, the system
applies ranking rules. It analyses the dependency
relations that have been predicted for a token with
its potential parents in the sentence and ranks them.
For example, for a sentence with 10 tokens, the sys-
tem would make 10 predictions per token. The pre-
dictions are first ranked by entropy of the class dis-
tribution for that prediction, then using the output of
Classifier 2, and next using the output of Classifier 3.
Ranking by entropy In order to compute entropy
we use the (inverse-linear) distance-weighted class
label distributions among the nearest neighbors that
Classifier 1 was able to find. For example, the pre-
diction for an instance can be: { NONE (2.74),
NMOD: (0.48) }. We can compute the entropy for
this instance using the formula in (1):
?
n?
i=1
P (labeli)log2(P (labeli)) (1)
with
- n: the total number of different labels in the distri-
bution, and
- P (labeli): the weight of label ithe total sum of the weights in the distribution
The system ranks the prediction with the lowest
entropy in position 1, while the prediction with the
highest entropy is ranked in the last position. The
rationale behind this is that the lower the entropy,
the more certain the classifier is about the predicted
dependency. Table 4 lists the first six heads for the
predicate word ?starts? ranked by entropy (cf. Ta-
ble 1).
Head Predicted label Distribution Entropy
Housing NONE { NONE (8.51) } 0.0
expected :A1 { :A1 (5.64) } 0.0
to NONE { NONE (4.74) } 0.0
quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56
are NONE { NONE (2.56), SBJ: (0.52) } 0.65
starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93
Table 4: Output of Classifier 1 for the first six heads
of ?starts?, ranked by entropy.
On the development data for English, applying
this rule causes a marked error reduction of 26.5%
on labeled macro F1: from 51.3% to 64.2%.
Ranking by Classifier 2 The next ranking step is
performed by using the predictions of Classifier 2,
i.e. the estimated labels of the dependency rela-
tions of a token with its syntactic and/or semantic
head(s). The system ranks the predictions that are
not in the set of possible dependencies predicted by
Classifier 2 at the bottom of the ranked list.
Head Predicted label Distribution Entropy
expected :A1 { :A1 (5.64) } 0.0
Housing NONE { NONE (8.51) } 0.0
to NONE { NONE (4.74) } 0.0
quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56
are NONE { NONE (2.56), SBJ: (0.52) } 0.65
starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93
Table 5: Output of Classifier 1 for the first six heads
of ?starts?. Ranked by entropy and Classifier 2.
Because this is done after ranking by entropy, the
instances with the lowest entropy are still at the top
of the list. Table 5 displays the re-ranked six heads
of ?starts?, given that Classifier 2 has predicted that
possible relations to heads are SBJ:A1 and :A1, and
given that only ?expected? is associated with one of
these two relations.
On the development data for English, applying
this rule induces a 9.0% error reduction on labeled
macro F1: from 64.2% to 67.4%.
Ranking by Classifier 3 The final ranking step
makes use of Classifier 3, which predicts the rela-
tion that holds between two tokens. The dependency
relations predicted by Classifier 1 that are not con-
firmed by Classifier 3 predicting that a relation exists
are moved to the end of the ranked list. Table 6 lists
the resulting ranked list. On the development data
for English, applying this rule yields another 5.2%
28
error reduction on labeled macro F1: from 67.4% to
69.1%.
Head Predicted label Distribution Entropy
expected :A1 { :A1 (5.64) } 0.0
quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56
starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93
Housing NONE { NONE (8.51) } 0.0
to NONE { NONE (4.74) } 0.0
are NONE { NONE (2.56), SBJ: (0.52) } 0.65
Table 6: Output of Classifier 1 for the first six heads
of ?starts?. Ranked by entropy, Classifier 2, and
Classifier 3.
3.2.2 Construction of the intermediate
dependency tree
After ranking the predictions of Classifier 1, the
system selects a syntactic head for every token. This
is motivated by the fact that every token has one and
only one syntactic head. The system selects the pre-
diction with the best ranking that has in the PDE-
PREL part a value different than ? ?.
The intermediate tree can have more than one root
or no root at all. To make sure that every sentence
has one and only one root we apply some extra rules.
If the sentence does not have a token with a root la-
bel, the system checks the distributions of Classi-
fier 1. The token with the rootlabel in its distribution
that is the head of the biggest number of tokens is
taken as root. If the intermediate tree has more than
one root, the last root is taken as root. The other root
tokens get the label with a syntax part (PDEPREL)
that has the highest score in the distribution of Clas-
sifier 1.
The product of this step is a tree in which ev-
ery token is uniquely linked to a syntactic head.
Because syntactic and semantic dependencies have
been linked, the tree contains also semantic depen-
dencies. However, the tree is missing the purely se-
mantic dependencies. The next step adds these rela-
tions to the dependency tree.
3.2.3 Adding extra semantic dependencies
In order to find the tokens that have only a seman-
tic relation with a predicate, the system analyses for
each predicate (i.e. tokens marked with Y in FILL-
PRED) the list of predictions made by Classifier 1
and selects the predictions in which the PDEPREL
part of the label is ? ? and the APRED part of the
label is different than ? ?. On the development data
for English, applying this rule produces a 6.7% er-
ror reduction on labeled macro F1: from 69.1% to
71.1%.
3.3 Predicate sense disambiguation
Predicate sense disambiguation is performed by a
classifier per language that predicts the sense of the
predicate, except for Japanese, as with that language
the lemma is taken as the sense. We use the IGTREE
algorithm. Instances represent predicates and the
features used are the word, lemma and POS of the
predicate, and the lemma and POS of two tokens be-
fore and after the predicate. The results per language
are presented in Table 7.
Lang. Cat Chi Cze Eng Ger Spa
F1 82.40 94.85 87.84 93.64 73.57 81.13
Table 7: Micro F1 for the predicate sense disam-
biguation.
4 Overall results
The system was developed by training on the train-
ing set provided by the task organisers and testing
on the development set. The final results were ob-
tained by testing on the testing set. Table 8 shows
the global results of the system for syntactic and se-
mantic dependencies.
Lang. F1 Precision Recall
Cat 73.75 74.91 72.63
Chi 67.16 68.09 66.26
Chi* 67.79 68.70 66.89
Cze 60.50 62.55 58.58
Cze* 68.68 70.38 67.07
Eng 78.19 79.69 76.74
Ger 67.51 69.52 65.62
Jap 77.75 81.91 73.98
Spa 70.78 71.34 70.22
Av. 70.81 72.57 69.15
Table 8: Macro F1, precision and recall for all de-
pendencies per language. Postevaluation results are
marked with *.
Table 9 shows the scores of syntactic and seman-
tic dependencies in isolation.
29
Syntax Semantics
Lang. LA F1 Precision Recall
Cat 77.33 70.14 72.49 67.94
Chi 67.58 66.71 68.59 64.93
Chi* 67.92 67.63 69.48 65.86
Cze 49.41 71.49 75.68 67.75
Cze* 60.03 77.28 80.73 74.11
Eng 80.35 75.97 79.04 73.13
Ger 73.88 61.01 65.15 57.36
Jap 86.17 68.82 77.66 61.80
Spa 73.07 68.48 69.62 67.38
Av. 72.54 68.95 72.60 65.76
Table 9: Labeled attachment (LA) score for syntac-
tic dependencies and Macro F1, precision and recall
of semantic dependencies per language. Postevalua-
tion results are marked with *.
5 Conclusions
In this paper we presented the system that we sub-
mitted to the ?closed? challenge of the CoNLL
Shared Task 2009. We observe fairly low scores,
which can be possibly improved for all languages by
making use of the available morpho-syntactic fea-
tures, which we did not use in the present system,
by optimising the classifiers per language, and by
improving the reranking algorithm. We also ob-
serve a relatively low recall on the semantic task as
compared to overall recall, indicating that syntactic
dependencies are identified with a better precision-
recall balance. A logical continuation of this study
is to compare joint learning to learning syntactic and
semantic dependencies in isolation, using the same
architecture. Only then will we be able to put for-
ward conclusions about the performance of a joint
learning system versus the performance of a system
that learns syntax and semantics independently.
Acknowledgments
This study was made possible through financial sup-
port from the University of Antwerp (GOA project
BIOGRAPH), and from the Netherlands Organisa-
tion for Scientific Research.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
S. Canisius and E. Tjong Kim Sang. 2007. A con-
straint satisfaction approach to dependency parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 1124?1128.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
R. Morante, W. Daelemans, and V. Van Asch. 2008. A
combined memory-based semantic role labeler of en-
glish. In Proc. of the CoNLL 2008, pages 208?212,
Manchester, UK.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
30
Proceedings of the Workshop on BioNLP: Shared Task, pages 59?67,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A memory?based learning approach to event extraction in biomedical texts
Roser Morante, Vincent Van Asch, Walter Daelemans
CNTS - Language Technology Group
University of Antwerp
Prinsstraat 13
B-2000 Antwerpen, Belgium
{Roser.Morante,Walter.Daelemans,Vincent.VanAsch}@ua.ac.be
Abstract
In this paper we describe the memory-based ma-
chine learning system that we submitted to the
BioNLP Shared Task on Event Extraction. We mod-
eled the event extraction task using an approach that
has been previously applied to other natural lan-
guage processing tasks like semantic role labeling
or negation scope finding. The results obtained by
our system (30.58 F-score in Task 1 and 29.27 in
Task 2) suggest that the approach and the system
need further adaptation to the complexity involved
in extracting biomedical events.
1 Introduction
In this paper we describe the memory-based ma-
chine learning system that we submitted to the
BioNLP shared task on event extraction1. The sys-
tem operates in three phases. In the first phase, event
triggers and entities other than proteins are detected.
In the second phase, event participants and argu-
ments are identified. In the third phase, postprocess-
ing heuristics select the best frame for each event.
Memory-based language processing (Daelemans
and van den Bosch, 2005) is based on the idea that
NLP problems can be solved by reuse of solved ex-
amples of the problem stored in memory. Given
a new problem, the most similar examples are re-
trieved, and a solution is extrapolated from them.
As language processing tasks typically involve many
1Web page: http://www-tsujii.is.s.u-tokyo.
ac.jp/GENIA/SharedTask/index.html
subregularities and (pockets of) exceptions, it has
been argued that memory-based learning is at an
advantage in solving these highly disjunctive learn-
ing problems compared to more eager learning that
abstract from the examples, as the latter eliminates
not only noise but also potentially useful exceptions
(Daelemans et al, 1999).
The BioNLP Shared Task 2009 takes a
linguistically-motivated approach, which is re-
flected in the properties of the shared task definition:
rich semantics, a text-bound approach, and decom-
position of linguistic phenomena. Memory-based
algorithms have been successfully applied in lan-
guage processing to a wide range of linguistic tasks,
from phonology to semantic analysis. Our goal was
to investigate the performance of a memory?based
approach to the event extraction task, using only
the information available in the training corpus and
modelling the task applying an approach similar to
the one that has been applied to tasks like semantic
role labeling (Morante et al, 2008) or negation
scope detection (Morante and Daelemans, 2009).
In Section 2 we briefly describe the task. Section
3 reviews some related work. Section 4 presents the
system, and Section 5 the results. Finally, some con-
clusions are put forward in Section 6.
2 Task description
The BioNLP Shared Task 2009 on event extrac-
tion consists of recognising bio-molecular events in
biomedical texts, focusing on molecular events in-
volving proteins and genes. An event is defined as a
relation that holds between multiple entities that ful-
fil different roles. Events can participate in one type
59
of events: regulation events.
The task is divided into the three subtasks listed
below. We participated in subtasks 1 and 2.
? Task 1: event detection and characterization. This
task involves event trigger detection, event typing,
and event participant recognition.
? Task 2: event argument recognition. Recognition
of entities other than proteins and the assignment of
these entities as event arguments.
? Task 3: recognition of negations and speculations.
The task did not include a named entity recogni-
tion subtask. A gold standard set of named entity
annotations for proteins was provided by the organ-
isation. A dataset based on the publicly available
portion of the GENIA (Collier et al, 1999) corpus
annotated with events (Kim et al, 2008) and of the
BioInfer (Pyysalo et al, 2007) corpus was provided
for training, and held-out parts of the same corpora
were provided for development and testing.
The inter-annotator agreement reported for the
Genia Event corpus is 56% strict match2, which
means that the event type is the same, the clue ex-
pressions are overlapping and the themes are the
same. This low inter-annotator agreement is an in-
dicator of the complexity of the task. Similar low
inter-annotator agreement rates (49.00 %) in identi-
fication of events have been reported by Sasaki et al
(2008).
3 Related work
In recent years, research on text mining in the
biomedical domain has experienced substantial
progress, as shown in reviews of work done in this
field (Krallinger and Valencia, 2005; Ananiadou and
McNaught, 2006; Krallinger et al, 2008b). Some
corpora have been annotated with event level infor-
mation of different types: PropBank-style frames
(Wattarujeekrit et al, 2004; Chou et al, 2006),
frame independent roles (Kim et al, 2008), and
specific roles for certain event types (Sasaki et al,
2008). The focus on extraction of event frames us-
ing machine learning techniques is relatively new
because there were no corpora available.
2We did not find inter-annotator agreement measures in
the paper that describes the corpus (Kim et al, 2008), but in
www-tsujii.is.s.u-tokyo.ac.jp/T-FaNT/T-FaNT
.files/Slides/Kim.pdf.
Most work focuses on extracting biological rela-
tions from corpora, which consists of finding asso-
ciations between entities within a text phrase. For
example, Bundschus et al (2008) develop a Condi-
tional Random Fields (CRF) system to identify re-
lations between genes and diseases from a set of
GeneRIF (Gene Reference Into Function) phrases.
A shared task was organised in the framework of
the Language Learning in Logic Workshop 2005 de-
voted to the extraction of relations from biomedical
texts (Ne?dellec, 2005). Extracting protein-protein
interactions has also produced a lot of research, and
has been the focus of the BioCreative II competi-
tion (Krallinger et al, 2008a).
As for event extraction, Yakushiji et al (2001)
present work on event extraction based on full-
parsing and a large-scale, general-purpose grammar.
They implement an Argument Structure Extractor.
The parser is used to convert sentences that describe
the same event into an argument structure for this
event. The argument structure contains arguments
such as semantic subject and object. Information
extraction itself is performed using pattern matching
on the argument structure. The system extracts 23 %
of the argument structures uniquely, and 24% with
ambiguity. Sasaki et al (2008) present a supervised
machine learning system that extracts event frames
from a corpus in which the biological process E. coli
gene regulation was linguistically annotated by do-
main experts. The frames being extracted specify
all potential arguments of gene regulation events.
Arguments are assigned domain-independent roles
(Agent, Theme, Location) and domain-dependent
roles (Condition, Manner). Their system works in
three steps: (i) CRF-based named entity recogni-
tion to assign named entities to word sequences; (ii)
CRF-based semantic role labeling to assign seman-
tic roles to word sequences with named entity labels;
(iii) Comparison of word sequences with event pat-
terns derived from the corpus. The system achieves
50% recall and 20% precision.
We are not aware of work that has been carried
out on the data set of the BioNLP Shared Task 2009
before the task took place.
60
4 System description
We developed a supervised machine learning sys-
tem. The system operates in three phases. In the first
phase, event triggers and entities other than proteins
are detected. In the second phase, event participants
and arguments are identified. In the third phase,
postprocessing heuristics select the best frame for
each event. Parameterisation of the classifiers used
in Phases 1 and 2 was performed by experiment-
ing with sets of parameters on the development set.
We experimented with manually selected parame-
ters and with parameters selected by a genetic algo-
rithm, but the parameters found by the genetic algo-
rithm did not yield better results than the manually
selected parameters
As a first step, we preprocess the corpora with the
GDep dependency parser (Sagae and Tsujii, 2007)
so that we can use part-of-speech tags and syntac-
tic information as features for the machine learner.
GDep is a a dependency parser for biomedical text
trained on the Tsujii Lab?s GENIA treebank. The
dependency parser predicts for every word the part-
of-speech tag, the lemma, the syntactic head, and
the dependency relation. In addition to these regular
dependency tags it also provides information about
the IOB-style chunks and named entities. The clas-
sifiers use the output of GDep in addition to some
frequency measures as features.
We represent the data into a columns format, fol-
lowing the standard format of the CoNLL Shared
Task 2006 (Buchholz and Marsi, 2006), in which
sentences are separated by a blank line and fields
are separated by a single tab character. A sentence
consists of tokens, each one starting on a new line.
4.1 Phase 1: Entity Detection
In the first phase, a memory based classifier pre-
dicts for every word in the corpus whether it is an
entity or not and the type of entity. In this set-
ting, entity refers to what in the shared task def-
inition are events and entities other than proteins.
Classes are defined in the IOB-style3 in order to
find entities that span over multiple words. Figure
1 shows a simplified version of a sentence in which
high level is a Positive Regulation event that spans
over multiple tokens and proenkephalin is a Pro-
3I stands for ?inside?, B for ?beginning?, and O for ?outside?.
tein. The Protein class does not need to be predicted
by the classifier because this information is pro-
vided by the Task organisers. The classes predicted
are: O, {B,I}-Entity, {B,I}-Binding, {B,I}-Gene Ex-
pression, {B,I}-Localization, {B,I}-Negative Regula-
tion, {B,I}-Positive Regulation, {B,I}-Phosphorylation,
{B,I}-Protein Catabolism, {B,I}-Transcription.
Token Class Token Class
Upon O which O
activation O correlate O
, O with O
T O high B-Positive regulation
lymphocyte O level I-Positive regulation
accumulate O of O
high O proenkephalin B-Protein
level O mRNA O
of O in O
the O the O
neuropeptide O cell O
enkephalin O . O
Figure 1: Instance representation for the entity de-
tection classifier.
We use the IB1 memory?based classifier as im-
plemented in TiMBL (version 6.1.2) (Daelemans
et al, 2007), a supervised inductive algorithm for
learning classification tasks based on the k-nearest
neighbor classification rule (Cover and Hart, 1967).
The memory-based learning algorithm was param-
eterised in this case by using modified value differ-
ence as the similarity metric, gain ratio for feature
weighting, using 7 k-nearest neighbors, and weight-
ing the class vote of neighbors as a function of their
inverse linear distance. For training we did not use
the entire set of instances from the training data. We
downsampled the instances keeping 5 negative in-
stances (class label O) for every positive instance.
Instances to be kept were randomly selected. The
features used by this classifier are the following:
? About the token in focus: word, chunk tag, named
entity tag as provided by the dependency parser,
and, for every entity type, a number indicating how
many times the focus word triggered this type of en-
tity in the training corpus.
? About the context of the token in focus: lemmas
ranging from the lemma at position -4 until the
lemma at position +3 (relative to the focus word);
part-of-speech ranging from position -1 until posi-
tion +1; chunk ranging from position -1 until posi-
tion +1 relative to the focus word; the chunk be-
61
fore the chunk to which the focus word belongs;
a boolean indicating if a word is a protein or not
for the words ranging from position -2 until posi-
tion +3.
Class label Precision Recall F-score
B-Gene expression 59.32 60.23 59.77
B-Regulation 30.41 33.58 31.91
B-Entity 40.21 41.49 40.84
B-Positive regulation 41.16 46.25 43.56
B-Binding 57.76 53.14 55.36
B-Negative regulation 42.94 48.67 45.63
I-Negative regulation 7.69 3.33 4.65
I-Positive regulation 14.29 13.24 13.74
B-Phosphorylation 75.68 71.80 73.68
I-Regulation 14.29 10.00 11.77
B-Transcription 48.78 59.70 53.69
I-Entity 20.00 16.13 17.86
B-Localization 75.00 60.00 66.67
B-Protein catabolism 73.08 100.00 84.44
O 97.66 97.62 97.64
Table 1: Results of the entity detection classifier.
Entities that are not in the table have a precision and
recall of 0.
Table 1 shows the results4 of this first step. All
class labels with a precision and recall of 0 are left
out. The overall accuracy is 95.4%. This high ac-
curacy is caused by the skewness of the data in the
training corpus, which contains a higher proportion
of instances with class label O. Instances with this
class are correctly classified in the development test.
B-Protein catabolism and B-Phosphorylation get the
highest scores. The reason why these classes get
higher scores can be that the words that trigger these
events are less diverse.
4.2 Phase 2: predicting the arguments and
participants of events
In the second phase, another memory-based clas-
sifier predicts the participants and arguments of an
event. Participants have the main role in the event
and arguments are entities that further specify the
event. In (1), for the event phosphorylation the sys-
tem has to find that STAT1, STAT3, STAT4, STAT5a,
and STAT5b are participants with the role Theme and
that tyrosine is an argument with the role Site.
4In this section we provide results on development data be-
cause the gold test data have not been made available.
(1) IFN-alpha enhanced tyrosine phosphorylation
of STAT1, STAT3, STAT4, STAT5a, and
STAT5b.
We use the IB1 algorithm as implemented in
TiMBL (version 6.1.2) (Daelemans et al, 2007).
The classifier was parameterised by using gain ratio
for feature weighting, overlap as distance metrics,
11 nearest neighbors for extrapolation, and normal
majority voting for class voting weights.
For this classifier, instances represent combina-
tions of an event with all the entities in a sentence,
for as many events as there are in a sentence. Entities
include entities and events. We use as input the out-
put of the classifier in Phase 1, so only events and
entities classified as such in Phase 1, and the gold
proteins will be combined. Events can have partici-
pants and arguments in a sentence different that their
sentence. We calculated that in the training corpus
these cases account for 5.54% of the relations, and
decided to restrict the combinations at the sentence
level. For the sentence in (1) above, where tyrosine,
phosphorylation, STAT1, STAT3, STAT4, STAT5a,
and STAT5b are entities and of those only phospho-
rylation is an event, the instances would be produced
by combining phosphorylation with the seven enti-
ties.
The features used by this classifier are the follow-
ing:
? Of the event and of the combined entity: first word,
last word, type, named entity provided by GDep,
chain of lemmas, chain of part-of-speech (POS)
tags, chain of chunk tags, dependency label of the
first word, dependency label of the last word.
? Of the event context and of the combined entity con-
text: word, lemma, POS, chunk, and GDep named
entity of the five previous and next words.
? Of the context between event and combined entity:
the chain of chunks in between, number of tokens in
between, a binary feature indicating whether event
is located before or after entity.
? Others: four features indicating the parental rela-
tion between the first and last words of the event
and the first and last words of the entity. The values
for this feature are: event father, event ancestor, en-
tity father, entity ancestor, none. Five binary fea-
tures indicating if the event accepts certain roles
(Theme, Site, ToLoc, AtLoc, Cause).
62
Table 2 shows the results of this classifier per type
of participant (Cause, Site, Theme) and type of ar-
gument (AtLoc, ToLoc). Arguments are very infre-
quent, and the participants are skewed towards the
class Theme. Classes Site and Theme score high F1,
and in both cases recall is higher than precision. The
fact that the classifier overpredicts Sites and Themes
will have a negative influence in the final scores of
the full system. Further research will focus on im-
proving precision.
Part/Arg Total Precision Recall F1
Cause 61 28.88 21.31 24.52
Site 20 54.83 85.00 66.66
Theme 683 55.50 72.32 62.80
AtLoc 1 25.00 100.00 40.00
ToLoc 4 75.00 75.00 75.00
Table 2: Results of finding the event participants and
arguments.
Table 3 shows the results of finding the event par-
ticipants and arguments per event type, expressed in
terms of accuracy on the development corpus. Cause
is easier to predict for Positive Regulation events,
Site is the easiest class to predict, taking into ac-
count that AtLoc and ToLoc occur only 5 times in
total, and Theme can be predicted successfully for
Transcription and Gene Expression events, whereas
it gets lower scores for Regulation, Binding, and
Positive Regulation events.
Event Arguments/Participants
Type Cause Site Theme AtLoc ToLoc
Binding - 100.00 56.00 - -
Gene Expr. - - 89.95 - -
Localization - - 73.07 100.00 75.00
- Regulation 11.11 0.00 75.00 - -
Phosphorylation 0.00 100.00 70.83 - -
+ Regulation 27.77 90.90 56.77 - -
Protein Catab. - - 60.00 - -
Regulation 13.33 0.00 46.87 - -
Transcription - - 94.44 - -
Table 3: Results of finding the event participants and
arguments per event type (accuracy).
Table 4 shows the results of finding the event par-
ticipants that are Entity and Protein per type of event
for events that are not regulations. Entity scores high
in all cases, whereas Protein scores high for Tran-
scription and Gene Expression events and low for
Binding events.
Event Arg./Part. Type
Type Entity Protein
Binding 100.00 56.00
Gene Expr. - 89.90
Localization 80.00 73.07
Phosphorylation 100.00 68.00
Protein Catab. - 60.00
Transcription - 94.44
Table 4: Results of finding the event participants and
arguments that are Entity and Protein per event type
(accuracy).
Table 5 shows the results of finding the partic-
ipants and arguments of regulation events. In the
case of regulation events, Entity is easier to classify
with Positive Regulation events, and Protein with
Negative Regulation events. In the cases in which
events are participants of regulation events, Bind-
ing, Gene Expression and Phosphorylation are easier
to classify with Positive Regulation events, Local-
ization with Regulation events, Protein Catabolism
with Negative Regulation events, and Transcription
is easy to classify in all cases.
Arg./Part. Event Type
Type Regulation + Regulation -Regulation
Entity 0.00 90.90 0.00
Protein 17.85 38.88 45.45
Binding - 75.00 66.66
Gene Expr. 66.66 90.47 75.00
Localization 100.00 80.00 75.00
Phosphorylation 0.00 44.44 0.00
Protein Catab. 0.00 40.00 100.00
Transcription 100.00 92.85 100.00
Table 5: Results of finding event arguments and par-
ticipants for regulation events (accuracy).
From the results of the system in this phase we can
extract some conclusions: data are skewed towards
the Theme class; Themes are not equally predictable
for the different types of events, they are better
predictable for Gene Expression and Transcription;
Proteins are more difficult to classify when they are
Themes of regulation events; and Transcription and
Localization events are easier to predict as Themes
of regulation events, compared to the other types of
events that are Themes of regulation events. This
63
suggests that it could be worth experimenting with
a classifier per entity type and with a classifier per
role, instead of using the same classifier for all types
of entities.
4.3 Phase 3: heuristics to select the best frame
per event
Phases 1 and 2 aimed at identifying events and can-
didates to event participants. However, the purpose
of the task is to extract full frames of events. For a
sentence like the one in (1) above, the system has to
extract the event frames in (2).
(2) 1. Phosphorylation (phosphorylation): Theme
(STAT1) Site (tyrosine)
2. Phosphorylation (phosphorylation): Theme
(STAT3) Site (tyrosine)
3. Phosphorylation (phosphorylation): Theme
(STAT5a) Site (tyrosine)
4. Phosphorylation (phosphorylation): Theme
(STAT4) Site (tyrosine)
5. Phosphorylation (phosphorylation): Theme
(STAT5b) Site (tyrosine)
It is necessary to apply heuristics in order to build
the event frames from the output of the second clas-
sifier, which for the sentence in (1) above should
contain the predictions in (3).
(3) 1. phosphorylation STAT1 : Theme
2. phosphorylation STAT3 : Theme
3. phosphorylation STAT5a : Theme
4. phosphorylation STAT4 : Theme
5. phosphorylation STAT5b : Theme
6. phosphorylation tyrosine : Site
Thus, in the third phase, postprocessing heuristics
determine which is the frame of each event.
4.3.1 Specific heuristics for each type of event
The system contains different rules for each of the
5 types of participants (Cause, Site, Theme, AtLoc,
ToLoc). The text entities are the entities defined dur-
ing Phase 2. An event is created for every text entity
for which the system predicted at least one partic-
ipant or argument. To illustrate this we can take a
look at the predictions for the Gene Expression event
in (4) where the identifiers starting by T refer to en-
tities in the text. The prediction would results in the
events listed in (5).
(4) Gene expression=
Theme:T11=Theme:T12=Theme:T13
(5) E1 Gene expression:T23 Theme:T11
E2 Gene expression:T23 Theme:T12
E3 Gene expression:T23 Theme:T13
Gene expression, Transcription, and Protein
catabolism. These type of events have only a
Theme. Therefore, an event frame is created for ev-
ery Theme predicted for events that belong to these
types.
Localization. A Localization event can have one
Theme and 2 arguments: AtLoc and ToLoc. A
Localization event with more than one predicted
Theme will result in as many frames as predicted
Themes. The arguments are passed on to every
frame.
Binding. A Binding event can have multiple
Themes and multiple Site arguments. If the system
predicts more than one Theme for a Binding event,
the heuristics first check if these Themes are in a co-
ordination structure. Coordination checking consists
of checking whether the word ?and? can be found
between the Themes. Coordinated Themes will give
rise to separate frames. Every participant and loose
Theme is added to all created event lines. This case
applies to the sentence in (6)
(6) When we analyzed the nature of STAT
proteins capable of binding to IL-2Ralpha,
pim-1, and IRF-1 GAS elements after cytokine
stimulation, we observed IFN-alpha-induced
binding of STAT1, STAT3, and STAT4, but not
STAT5 to all of these elements.
The frames that should be created for this sen-
tence listed in (7).
(7) 1. Binding (binding): Theme(STAT4)
Theme2(IRF-1) Site2(GAS elements)
2. Binding (binding): Theme(STAT3)
Theme2:(IL-2Ralpha) Site2(GAS elements)
3. Binding (binding): Theme(STAT3)
Theme2(IRF-1) Site2(GAS elements)
4. Binding (binding): Theme(STAT4)
Theme2(pim-1) Site2(GAS elements)
5. Binding (binding): Theme(STAT1)
Theme2(IL-2Ralpha) Site2(GAS elements)
64
6. Binding (binding): Theme(STAT4)
Theme2(IL-2Ralpha) Site2(GAS elements)
7. Binding (binding): Theme(IL-2Ralpha)
Site(GAS elements)
8. Binding (binding): Theme(pim-1) Site(GAS
elements)
9. Binding (binding): Theme(STAT1)
Theme2(IRF-1) Site2(GAS elements)
10. Binding (binding): Theme(STAT3)
Theme2(pim-1) Site2(GAS elements)
11. Binding (binding): Theme(IRF-1) Site(GAS
elements)
12. Binding (binding): Theme(STAT1)
Theme2(pim-1) Site2(GAS elements)
Phosphorylation. A Phosphorylation event can
have one Theme and one Site. Multiple Themes for
the same event will result in multiple frames. The
Site argument will be added to every frame.
Regulation, Positive regulation, and Negative
regulation. A Regulation event can have a Theme,
a Cause, a Site, and a CSite. For Regulation events
the system uses a different approach when creating
new frames. It first checks which of the participants
and arguments occurs the most frequent in a predic-
tion and it creates as many separate frames as are
needed to give every participant/argument its own
frame. The remaining participants/arguments are
added to the nearest frame. For this type of event
a new frame can be created not only for multiple
Themes but also for e.g. multiple Sites. The purpose
of this strategy is to increase the recall of Regulation
events.
4.3.2 Postprocessing
After translating predictions into frames some
corrections are made.
1. Every Theme and Cause that is not a Protein is
thrown away.
2. Every frame that has no Theme is provided
with a default Theme. If no Protein is found
before the focus word, the closest Protein after
the word is taken as the default Theme.
3. Duplicates are removed.
5 Results
The official results of our system for Task 1 are pre-
sented in Table 6. The best F1 score are for Gene Ex-
pression and Protein Catabolism events. The lowest
results are for all the types of regulation events and
for Binding events. Binding events are more diffi-
cult to predict correctly because they can have more
than one Theme.
Total Precision Recall F1
Binding 347 12.97 31.03 18.29
Gene Expr. 722 51.39 68.96 58.89
Localization 174 20.69 78.26 32.73
Phosphorylation 135 28.15 67.86 39.79
Protein Catab. 14 64.29 42.86 51.43
Transcription 137 24.82 41.46 31.05
Regulation 291 8.93 23.64 12.97
+Regulation 983 11.70 31.68 17.09
-Regulation 379 11.08 29.85 16.15
TOTAL 3182 22.50 47.70 30.58
Table 6: Official results of Task 1. Approximate
Span Matching/Approximate Recursive Matching.
The official results of our system for Task 2 are
presented in Table 7. Results are similar to the re-
sults of Task 1 because there are not many more ar-
guments than participants. Recognising arguments
was the additional goal of Task 2 in relation to
Task 1.
Total Precision Recall F1
Binding 349 11.75 28.28 16.60
Gene Expr. 722 51.39 68.96 58.89
Localization 174 17.82 67.39 28.18
Phosphorylation 139 15.83 39.29 22.56
Protein Catab. 14 64.29 42.86 51.43
Transcription 137 24.82 41.46 31.05
Regulation 292 8.56 22.73 12.44
+Regulation 987 11.35 30.85 16.59
-Regulation 379 11.08 29.20 15.76
TOTAL 3193 21.52 45.77 29.27
Table 7: Official results of Task 2. Approximate
Span Matching/Approximate Recursive Matching.
Results obtained on the development set are a lit-
tle bit higher. For Task1 an overall F1 of 34.78 and
for Task 2 33.54.
For most event types precision and recall are un-
balanced, the system scores higher in recall. Fur-
ther research should focus on increasing precision
because the system is predicting false positives. It
would be possible to add a step in order to fil-
ter out the false positives by comparing word se-
quences with event patterns derived from the cor-
pus, which is an approach taken in the system by
Sasaki et al (2008) .
65
In the case of Binding events, both precision and
recall are low. There are two explanations for this.
In the first place, the first classifier misses almost
half of the binding events. As an example, for
the sentence in (8.1), the gold standard identifies as
binding event the multiwords binds as a homodimer
and form heterodimers, whereas the system identi-
fies two binding events for the same sentence, binds
and homodimer, none of which is correct because
the correct one is the multiword unit. For the sen-
tence in (8.2), the gold standard identifies as binding
events bind, form homo-, and heterodimers, whereas
the system identifies only binds.
(8) 1. The KBF1/p50 factor binds as a homodimer but can
also form heterodimers with the products of other
members of the same family, like the c-rel and v-rel
(proto)oncogenes.
2. A mutant of KBF1/p50 (delta SP), unable to bind to
DNA but able to form homo- or heterodimers, has been
constructed.
From the sentence in (8.1) above the eight frames
in (9) should be extracted, whereas the system ex-
tracts only the frames in (10), which are incorrect
because the events have not been correctly identi-
fied.
(9) 1. Binding(binds as a homodimer) : Theme(KBF1)
2. Binding(binds as a homodimer) : Theme(p50)
3. Binding(form heterodimers) : Theme(KBF1)
Theme2(c-rel)
4. Binding(form heterodimers) : Theme(p50)
Theme2(v-rel)
5. Binding(form heterodimers) : Theme(p50)
Theme2(c-rel)
6. Binding(form heterodimers) : Theme(KBF1)
Theme2(v-rel)
7. Binding(bind) : Theme(p50)
8. Binding(bind) : Theme(KBF1)
(10) 1. Binding(binds) : Theme(v-rel)
2. Binding(homodimer) : Theme(c-rel)
The complexity of frame extraction of Binding
events contrasts with the less complex extraction of
frames for Gene Expression events, like the one in
sentence (11), where expression has been identified
correctly by the system as an event and the frame in
(12) has been correctly extracted.
(11) Thus, c-Fos/c-Jun heterodimers might contribute to the
repression of DRA gene expression.
(12) Gene Expression(expression) : Theme(DRA)
6 Conclusions
In this paper we presented a supervised machine
learning system that extracts event frames from
biomedical texts in three phases. The system partic-
ipated in the BioNLP Shared Task 2009, achieving
an F-score of 30.58 in Task 1, and 29.27 in Task 2.
The frame extraction task was modeled applying the
same approach that has been applied to tasks like se-
mantic role labeling or negation scope detection, in
order to check whether such an approach would be
suitable for a frame extraction task. The results ob-
tained for the present task do not compare to results
obtained in the mentioned tasks, where state of the
art F-scores are above 80.
Extracting biomedical event frames is more com-
plex than labeling semantic roles because of several
reasons. Semantic roles are mostly assigned to syn-
tactic constituents, predicates have only one frame
and all the arguments belong to the same frame. In
contrast, in the biomedical domain one event can
have several frames, each frame having different
participants, the boundaries of which do not coin-
cide with syntactic constituents.
The system presented here can be improved in
several directions. Future research will concentrate
on increasing precision in general, and precision and
recall of binding events in particular. Analysing in
depth the errors made by the system at each phase
will allow us to find the weaker aspects of the sys-
tem. From the results of the system in the second
phase we could draw some conclusions: data are
skewed towards the Theme class; Themes are not
equally predictable for the different types of events;
Proteins are more difficult to classify when they are
Themes of regulation events; and Transcription and
Localization events are easier to predict as Themes
of regulation events, compared to the other types of
events that are Themes of regulation events. We plan
to experiment with a classifier per entity type and
with a classifier per role, instead of using the same
classifier for all types of entities. Additionally, the
effects of the postprocessing rules in Phase 3 will be
evaluated.
66
Acknowledgments
Our work was made possible through financial sup-
port from the University of Antwerp (GOA project
BIOGRAPH). We are grateful to two anonymous re-
viewers for their valuable comments.
References
S. Ananiadou and J. McNaught. 2006. Text Mining for
Biology and Biomedicine. Artech House Books, Lon-
don.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. of the X
CoNLL Shared Task, New York. SIGNLL.
M. Bundschus, M. Dejori, M. Stetter, V. Tresp, and H-
P Kriegel. 2008. Extraction of semantic biomedi-
cal relations from text using conditional random fields.
BMC Bioinformatics, 9.
W.C. Chou, R.T.H. Tsai, Y-S. Su, W. Ku, T-Y Sung, and
W-L Hsu. 2006. A semi-automatic method for an-
notating a biomedical proposition bank. In Proc. of
ACL Workshop on Frontiers in Linguistically Anno-
tated Corpora 2006, pages 5?12.
N. Collier, H.S. Park, N. Ogata, Y. Tateisi, C. Nobata,
T. Sekimizu, H. Imai, and J. Tsujii. 1999. The GE-
NIA project: corpus-based knowledge acquisition and
information extraction from genome research papers.
In Proc. of EACL 1999.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor
pattern classification. Institute of Electrical and Elec-
tronics Engineers Transactions on Information The-
ory, 13:21?27.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learning.
Machine Learning, Special issue on Natural Language
Learning, 34:11?41.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2007. TiMBL: Tilburg memory based
learner, version 6.1, reference guide. Technical Report
Series 07-07, ILK, Tilburg, The Netherlands.
J.D. Kim, T. Ohta, and J. Tsujii. 2008. Corpus annotation
for mining biomedical events from literature. BMC
Bioinformatics, 9:10.
M. Krallinger and A. Valencia. 2005. Text-mining and
information-retrieval services for molecular biology.
Genome Biology, 6:224.
M. Krallinger, F. Leitner, C. Rodriguez-Penagos, and
A. Valencia. 2008a. Overview of the protein?protein
interaction annotation extraction task of BioCreative
II. Genome Biology, 9(Suppl 2):S4.
M. Krallinger, A. Valencia, and L. Hirschman. 2008b.
Linking genes to literature: text mining, informa-
tion extraction, and retrieval applications for biology.
Genome Biology, 9(Suppl 2):S8.
R. Morante and W. Daelemans. 2009. A metalearning
approach to processing the scope of negation. In Pro-
ceedings of CoNLL 2009, Boulder, Colorado.
R. Morante, W. Daelemans, and V. Van Asch. 2008. A
combined memory-based semantic role labeler of En-
glish. In Proc. of the CoNLL 2008, pages 208?212,
Manchester, UK.
C. Ne?dellec. 2005. Learning language in logic ? genic
interaction extraction challenge. In Proc. of Learn-
ing Language in Logic Workshop 2005, pages 31?37,
Bonn.
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007. BioInfer: a corpus
for information extraction in the biomedical domain.
BMC Bioinformatics, 8(50).
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with lr models and parser ensem-
bles. In Proc. of CoNLL 2007 Shared Task, EMNLP-
CoNLL, pages 82?94, Prague. ACL.
Y. Sasaki, P. Thompson, P. Cotter, J. McNaught, and
S. Ananiadou. 2008. Event frame extraction based
on a gene regulation corpus. In Proc. of Coling 2008,
pages 761?768.
T. Wattarujeekrit, P.K. Shah, and N. Collier. 2004.
PASBio: predicate-argument structures for event ex-
traction in molecular biology. BMC Bioinformatics,
5:155.
A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001.
Event extraction from biomedical papers using a full
parser. In Pac Symp Biocomput.
67
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 579?589, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Statistical Relational Learning Approach to Identifying
Evidence Based Medicine Categories
Mathias Verbeke? Vincent Van Asch? Roser Morante?
Paolo Frasconi? Walter Daelemans? Luc De Raedt?
? Department of Computer Science, Katholieke Universiteit Leuven, Belgium
{mathias.verbeke, luc.deraedt}@cs.kuleuven.be
? Department of Linguistics, Universiteit Antwerpen, Belgium
{roser.morante, vincent.vanasch, walter.daelemans}@ua.ac.be
? Dipartimento di Sistemi e Informatica, Universita` degli Studi di Firenze, Italy
p-f@dsi.unifi.it
Abstract
Evidence-based medicine is an approach
whereby clinical decisions are supported by
the best available findings gained from scien-
tific research. This requires efficient access
to such evidence. To this end, abstracts in
evidence-based medicine can be labeled using
a set of predefined medical categories, the so-
called PICO criteria. This paper presents an
approach to automatically annotate sentences
in medical abstracts with these labels. Since
both structural and sequential information are
important for this classification task, we use
kLog, a new language for statistical relational
learning with kernels. Our results show a clear
improvement with respect to state-of-the-art
systems.
1 Introduction
Evidence-based medicine (EBM) or evidence-based
practice (EBP) combines clinical expertise, the pref-
erences and values of the patient and the best
available evidence to make good patient care deci-
sions. Clinical research findings are systematically
reviewed, appraised and used to improve the patient
care, for which efficient access to such evidence is
required. In order to facilitate the search process,
medical documents are labeled using a set of prede-
fined medical categories, the PICO criteria. PICO is
an acronym for the mnemonic concepts that are used
to construct queries when searching for scientific ev-
idence in the EBM process. The need to automatize
the annotation process has initiated research into au-
tomatic approaches to annotate sentences in medical
documents with the PICO labels.
As indicated by Kim et al2011), both the struc-
tural information of the words in the sentence, and
that of the sentences in the document are important
features for this task. Furthermore, sequential infor-
mation can leverage the dependencies between dif-
ferent sentences in the text. Therefore we propose an
approach using kLog (Frasconi et al2012) to tackle
this problem. kLog is a new language for statistical
relational learning with kernels, that is embedded in
Prolog, and builds upon and links together concepts
from database theory, logic programming and learn-
ing from interpretations. Learning from interpreta-
tions is a logical and relational learning setting (De
Raedt et al2008) in which the examples are inter-
pretations, that is, sets of tuples that are true in the
examples. In a sense, each example can be viewed
as a small relational database. kLog is able to trans-
form relational into graph-based representations and
apply kernel methods to extract an extended high-
dimensional feature space.
The choice for kLog was motivated by previous
results (Verbeke et al2012), where we showed that
a statistical relational learning approach using kLog
is able to process the contextual aspects of language
improving on state-of-the-art results for hedge cue
detection. However, the current task adds two levels
of complexity. First, next to the relations between
the words in the sentence, now also the relations be-
tween the sentences in the document become impor-
tant. In the proposed approach, we first generate a
feature space with kLog that captures the intrasen-
tential properties and relations. Hereafter, these fea-
tures serve as input for a structured output support
vector machine that can handle sequence tagging
579
(Tsochantaridis et al2004), in order to take the
intersentential features into account. Second, since
there are more than two categories, and each sen-
tence can have multiple labels, the problem is now a
multiclass multilabel classification task.
The main contribution of this paper is that we
show that kLog?s relational nature and its ability
to declaratively specify and use background knowl-
edge is beneficial for natural language learning prob-
lems. This is shown on the NICTA-PIBOSO corpus,
for which we present results that indicate a clear im-
provement on the state-of-the-art.
The remainder of this paper is organized as fol-
lows. In Section 2, we outline earlier work that is
related to the research presented here. Section 3 de-
scribes the methodology of our method. We present
a thorough evaluation of our method in Section
4. The last section draws conclusions and presents
some ideas for future work.
2 Related Work
EBM is an approach to clinical problem-solving
based on ?systematically finding, appraising, and us-
ing contemporaneous research findings as the ba-
sis for clinical decisions? (Rosenberg and Donald,
1995). The evidence-based process consists of four
steps: (1) Formulating a question from a patient?s
problem; (2) Searching the literature for relevant
clinical articles; (3) Evaluating the evidence; And
(4) implementing useful findings in clinical prac-
tice. Given the amounts of medical publications
available in databases such as PubMed, automating
step 2 is crucial to help doctors in their practice.
Efforts in this direction from the NLP community
have so far focused on corpus annotation (Demner-
Fushman and Lin, 2007; Kim et al2011), text cate-
gorization (Davis-Desmond and Molla?, 2012), sum-
marization (Molla? and Santiago-Mart??nez, 2011),
and question-anwering (Niuet al2003; Demner-
Fushman and Lin, 2007).
The existing corpora are usually annotated with
the PICO mnemonic (Armstrong, 1999) concepts,
that are used to build queries when searching for
literature for EBM purposes. The PICO concepts
are: primary Problem (P) or population, main Inter-
vention (I), main intervention Comparison (C), and
Outcome of intervention (O). PICO helps determin-
ing what terms are important in a query and there-
fore it helps building the query, which is sent to the
search repositories. Once the documents are found,
they need to be read by a person who eliminates ir-
relevant documents.
The first attempt to classify PICO concepts is pre-
sented in Demner-Fushman and Lin (2007), who
apply a rule-based approach to identify sentences
where PICO concepts occur and a supervised ap-
proach to classify sentences that contain an Out-
come. The features used by this classifier are n-
grams, position, and semantic information from the
parser used to process the data. The system is trained
on 275 abstracts manually annotated. The accura-
cies reported range from 80% for Population, 86%
for Problem, 80% for Intervention, and, from 64%
to 95% for Outcome depending on the test set of ab-
stracts.
Kim et al2011) perform a similar classification
task in two steps. First a classifier identifies the sen-
tences that contain PICO concepts, and then another
classifier assigns PICO tags to the sentences found
to be relevant by the previous classifier. The sys-
tem is based on a CRF algorithm and is trained on
the NICTA-PIBOSO corpus. This dataset contains
1,000 medical abstracts manually annotated with an
extension of the PICO tagset, for which the defini-
tions are listed in Table 1. The annotation is per-
formed at sentence level and one sentence may have
more than one tag. An example of an annotated
abstract from the corpus can be found in the sup-
plementary material. The features used by the al-
gorithm include features derived from the context,
semantic relations, structure and sequencing of the
text. The system is evaluated for 5-way and 6-way
classification and results are provided apart from
structured and unstructured abstracts. The F-scores
for structured abstracts is 89.32% for 5-way classifi-
cation and 80.88% for 6-way classification, whereas
for unstructured abstracts it is 71.54% for 5-way
classification and 64.66% for 6-way classification.
Chung (2009) uses CRF to classify PICO con-
cepts by combining them with general categories as-
sociated with rhetorical roles: Aim, Method, Results
and Conclusion. Her system is tested on corpora of
abstracts of randomized control trials. First struc-
tured abstracts with headings labeled with PICO
580
Background Material that informs and may place the current study in perspective, e.g. work that preceded the
current; information about disease prevalence; etc.
Population The group of individual persons, objects or items comprising the study?s sample, or from which
the sample was taken for statistical measurement
Intervention The act of interfering with a condition to modify it or with a process to change its course (includes
prevention)
Outcome The sentence(s) that best summarizes the consequences of an intervention
Study Design The type of study that is described in the abstract
Other Any sentence not falling into one of the other categories and presumed to provide little help with
clinical decision making, i.e. non-key or irrelevant sentences
Table 1: Definitions of the semantic tags used as annotation categories (taken from Kim et al2011)).
concepts are used. A sentence level classification
task is performed, assigning only one rhetorical role
per sentence. The F-scores obtained range from 0.93
to 0.98. Then another sentence level classification
task is performed to automatically assign the labels
Intervention, Participant and Outcome Measures to
sentences in unstructured and structured abstracts
without headings. F-scores of up to 0.83 and 0.84
are obtained for Intervention and Outcome Measure
sentences.
Other work aimed at identifying rhetorical zones
in biomedical articles. In this case areas of text are
classified in terms of the rhetorical categories In-
troduction, Methods, Results and Discussion (IM-
RAD) (Agarwal and Yu, 2009) or richer categories,
such as problem-setting or insight (Mizuta et al
2006).
There exists a wide range of statistical relational
learning systems (Getoor and Taskar, 2007; De
Raedt et al2008), and many of these systems
are in principle useful for natural language process-
ing. The most popular formalism today is Markov
Logic, which has already been used for natural lan-
guage processing tasks such as semantic role label-
ing (Riedel and Meza-Ruiz, 2008) and coreference
resolution (Poon and Domingos, 2008). With re-
spect to Markov Logic, two distinguishing features
of kLog are that 1) it employs kernel based meth-
ods grounded in statistical learning theory, and 2) it
employs a Prolog like language for defining and us-
ing background knowledge. As Prolog is a program-
ming language, this is more flexible that the formal-
ism used by Markov Logic.
3 Methodology
In learning from examples, or interpretations (De
Raedt et al2008), the instances are sampled iden-
tically and independently from some unknown but
fixed distribution. They can be represented as pairs
z = (x, y), in which x represents the inputs and y
the outputs. An example interpretation can be found
in Figure 3, where the hasCategory relation repre-
sents y in this case, since it is the target relation we
want to predict. The inputs x are formed by all other
facts. The task is now to learn a function h : X ? Y
that maps the inputs to the outputs. Sentences may
have multiple labels. Hence this is a structured out-
put task where the output is a sequence of sets of
labels attached to the sentences in a given document.
kLog is the new statistical relational language for
learning with kernels that we use to tackle the PICO
categories classification task. The novelty of kLog
is that, based on the regular, linguistic features, it
allows to define an extended high-dimensional fea-
ture space that is also able to take relational features
into account in a principled manner. Furthermore,
its declarative approach offers a flexible and inter-
pretable way to construct features.
The choice of kLog is motivated by our previous
results (Verbeke et al2012), where we showed that
the relational representation of the domain as used
by kLog is able to take the contextual aspects of lan-
guage into account. Whereas there we only used
the relations at the sentence level, the current task
adds a new level of complexity, since the identifica-
tion of PICO categories in abstracts also requires to
take into account various relations between the sen-
tences of an abstract. The general workflow of our
approach is depicted in Figure 1, which will be de-
581
Database
(Fig. 3)
Extensionalized 
database
Graph
(Fig. 4)
Kernel matrix/
feature vectors
Statistical 
learner
Raw data
(sentence)
Feature extraction
(lemma, POS,?)
Declarative feature 
construction
Graphicalization
Feature 
generation
Graph kernel 
(NSPDK)
kLog
Figure 1: General kLog workflow.
scribed step by step in the following paragraphs.
Preprocessing The sentences have been prepro-
cessed with a named entity tagger and a dependency
parser.
Named entity tagging has been performed with
the BiogaphTA named entity module, which
matches token sequences with entries in the UMLS
database1. UMLS integrates over 2 million names
for some 900,000 concepts from more than 60 fami-
lies of biomedical vocabularies (Bodenreider, 2004).
The tagger matches sequences with a length of max-
imum 4 tokens. This covers 66.2% of the UMLS
entries. By using UMLS, different token sequences
referring to the same concept can be mapped to
the same concept identifier (CID). The BiographTA
named entity tagger has been evaluated on the
BioInfer corpus (Pyysalo et al2007) obtaining a
72.02 F1 score.
Dependency parsing has been performed with the
GENIA dependency parser GDep (Sagae and Tsu-
jii, 2007), which uses a best-first probabilistic shift-
reduce algorithm based on the LR algorithm (Knuth,
1965) and extended by the pseudo-projective pars-
ing technique. This parser is a version of the KSDep
dependency parser trained on the GENIA Treebank
for parsing biomedical text. KSDep was evaluated
in the CoNLL Shared Task 2007 obtaining a La-
beled Attachment Score of 89.01% for the English
dataset. GDEP outputs the lemmas, chunks, Genia
named entities and dependency relations of the to-
kens in a sentence.
This information can be represented as an
Entity/Relationship (E/R) diagram, a modeling
paradigm that is frequently used in database theory
(Garcia-Molina et al2008). The E/R-model for the
1From UMLS only the MRCONSO.RRF and MRSTY.RRF
files are used.
problem under consideration is shown in Figure 2,
which provides an abstract representation of the ex-
amples, i.e. medical abstracts in this case. We will
show later how this abstract representation can be
unrolled for each example, resulting in a graph; cf.
also Figure 4 for our example sentence. This rela-
tional database representation will serve as the input
for kLog.
w
depHead
next
wordID
depRel
lemma
POS-tag
chunktag
wordString
NEGenia
NEUMLS
sentence
hasWord
class
sentID
hasCategory
nextS
Figure 2: E/R-diagram modeling the sentence identifica-
tion task.
The entities are the words and sentences in the
abstract. They are represented by the rectangles in
the E/R-model. Each entity can have a number of
properties attached to it, depicted by the ovals and
has a unique identifier (underlined properties). As
in database theory, each entity corresponds with a
tuple, or fact, in the database.
Figure 3 shows a part of an example interpretation
z. For example, w(w4 1,?Surgical?,?Surgical?,b-
np,jj,?O?,?O?) specifies a word entity, with w4 1 as
identifier and the other arguments as properties. As
indicated before, as lexical information we take the
token string itself, its lemma, the part-of-speech tag
and the chunk tag into account. We also include
some semantic information, namely two binary val-
ues indicating if the word is a (biological) named
entity. sentence(s4,4) represents a sentence entity,
with its index in the abstract as a property.
Furthermore, the E/R-diagram also contains a
number of relationships, which are represented by
582
sentence(s4,4)
hasCategory(s4,?background?)
w(w4_1,?Surgical?,?Surgical?,b-np,
jj,?O?,?O?) hasWord(s4,w4_1)
dh(w4_1,w4_2,nmod)
nextW(w4_2,w4_1)
w(w4_2,?excision?,?excision?,i-np,
nn,?O?,?O?) hasWord(s4,w4_2)
dh(w4_2,w4_5,sub)
nextW(w4_3,w4_2)
w(w4_3,?of?,?of?,b-pp,in,?O?,?O?)
hasWord(s4,w4_3)
dh(w4_3,w4_2,nmod)
nextW(w4_4,w4_3)
w(w4_4,?CNV?,?CNV?,b-np,nn,
?B-protein?,?O?) hasWord(s4,w4_4)
dh(w4_4,w4_3,pmod)
nextW(w4_5,w4_4)
...
Figure 3: Part of an example interpretation z, represent-
ing the example sentence in Figure 4.
the diamonds. They are linked to the entities that
participate in the relationship, or stand alone if they
characterize general properties of the interpretation.
An example relation is nextW(w4 2,w4 1), which
indicates the sequence of the words in the sentence.
dh(w4 1,w4 2,nmod) specifies that word w4 1 is
a noun modifier of word w4 2, and thus serves to
incorporate the dependency relationships between
the words. hasCategory(s4,?background?) signi-
fies that sentence s4 is a sentence describing back-
ground information. This relation is the target re-
lation that we want to predict for this task and will
not be taken into account as a feature, but is listed in
the database and only used during the training of the
model.
Since the previously described entities and rela-
tionships are listed explicitly in the database, these
are called extensional relations, in contrast to the in-
tensional relations, as we will describe next.
Declarative feature construction A strength of
kLog is that it is also capable of constructing fea-
tures declaratively, by using intensional relations.
This enables one to encode additional background
knowledge based on a small set of preprocessed fea-
tures, which renders experimentation very flexible
and makes the results more interpretable. It further-
more allows one to limit the required features to the
core discriminative ones. These intensional features
are defined through definite clauses, and is done us-
ing an extension of the declarative programming lan-
guage Prolog. The following features were used.
We make a distinction between the features used for
structured and unstructured abstracts.
For structured abstracts, four intensional relations
were defined. The relation lemmaRoot(S,L) is
specified as:
lemmaRoot(S,L) ?
hasWord(S, I),
w(I,_,L,_,_,_,_),
dh(I,_,root).
For each sentence, it only selects the lemmas
of the root word in the dependency tree, which
markedly limits the number of word features used.
The following relations are related to, and try to
capture the document structure imposed by the sec-
tion headers present in the structured abstracts.
hasHeaderWord(S,X) identifies whether a sen-
tence is a header of a section. In order to realize this,
it selects the words of a sentence that count more
than four characters (to discard short names of bio-
logical entities), which all need to be uppercase.
hasHeaderWord(S,X) ?
w(W,X,_,_,_,_,_),
hasWord(S,W),
(atom(X) -> name(X,C) ; C = X),
length(C,Len),
Len > 4,
all_upper(C).
Also the sentences below a certain section header
need to be marked as belonging to this sec-
tion, which is done by the relation hasSection-
Header(S,X).
hasSectionHeader(S,X) ?
nextS(S1,S),
hasHeaderWord(S1,X).
hasSectionHeader(S,X) ?
nextS(S1,S),
not isHeaderSentence(S),
once(hasSectionHeader(S1,X)).
583
For the unstructured abstracts, also the lemma-
Root relation is used, but next to the lemma, now
also the part-of-speech tag of the root word is taken
into account. Since the unstructured abstracts lack
section headers, other features were needed to dis-
tinguish between the different sections, for which
the relation prevLemmaRoot proved to be very in-
formative. It adds the lemma of the root word in the
previous sentence as a property to the current sen-
tence under consideration.
prevLemmaRoot(S,L) ?
nextS(S1,S),
lemmaRoot(S1,L,_).
The intensional predicates are grounded. This is
a proces similar to materialization in databases, that
is, the atoms implied by the background knowledge
and the facts in the example are all computed using
Prolog?s deduction mechanism. This leads to the
extensionalized database, in which both the exten-
sional as well as the grounded intensional predicates
are listed.
Graphicalization and feature generation In the
third step, the interpretations are graphicalized, i.e.
transformed into graphs. Since the facts that form
the interpretation still conform to the E/R-diagram,
this can be interpreted as unfolding the E/R-diagram
over the data. An example illustrating this process
is given in Figure 4. Each interpretation is converted
into a bipartite graph, for which there is a vertex for
every ground atom of every E-relation, one for every
ground atom of every R-relation, and an undirected
edge {e, r} if an entity e participates in relationship
r.
The obtained graphs can then be used in the next
step for feature generation. This is done by means
of a graph kernel ?, which calculates the similar-
ity between two graphicalized interpretations. Any
graph kernel that allows fast computations on large
graphs and has a flexible bias to enable heteroge-
neous features can in theory be applied. In the cur-
rent implementation, an extension of the Neighbor-
hood Subgraph Pairwise Distance Kernel (NSPDK)
(Costa and De Grave, 2010) is used.
NSPDK is a decomposition kernel (Haussler,
1999), in which pairs of subgraphs are compared
                                         
      
       
s0
s1
s2
s3
nextS
nextS
next
s4
s5
s6
 
 
 
s7
 
s8
 
s9
 
title
title
Surgical  excision  of  CNV  may  allow  stabilisation  or  improvement  of  vision.
background
next next                         
dh(nmod)
dh(sub)
dh(pmod)
  
hasWord
  
  
  
  
  
Figure 4: Graphicalization Gz of interpretation z.
to each other in order to calculate the similarity be-
tween two graphs. These subgraphs can be seen as
circles in the graph, and are defined by three hyper-
parameters. First of all, there is the center of the
subgraph, the kernel point, which can be any entity
or relation in the graph. The entities and relations
to be taken into account as kernel points are marked
beforehand as a subset of the intensional and exten-
sional domain relations. The radius r determines
the size of the subgraphs and defines which entities
or relations around the kernel point are taken into ac-
count. Each entity or relation that is within a number
of r edges away from the kernel point is considered
to be part of the subgraph. The third hyperparam-
eter, the distance d, determines how far apart from
each other the kernel points can be. Each subgraph
around a kernel point that is within a distance d or
less from the current kernel point will be considered.
This is captured by the relation Rr,d(Av, Bu, G) be-
tween two rooted subgraphs Av, Bu and a graph G,
which selects all pairs of neighborhood graphs of ra-
dius r whose roots are at distance d in a given graph
G.
The kernel ?r,d(G,G?) between graphs G and G?
on the relation Rr,d is then defined as:
?r,d(G,G
?) =
?
Av , Bu ? R
?1
r,d(G)
A?v? , B
?
u? ? R
?1
r,d(G
?)
?(Av, A
?
v?)?(Bu, B
?
u?)
(1)
584
For efficiency reasons, an upper bound is imposed
on the radius and distance parameters, which leads
to the following kernel definition:
Kr?,d?(G,G
?) =
r??
r=0
d??
d=0
?r,d(G,G
?) (2)
We hereby limit the sum of the ?r,d kernels for all
increasing values of the radius and distance parame-
ter up to a maximum given value of r?, respectively
d?.
The result of this graphicalization and feature
generation process is an extended, high-dimensional
feature space, which serves as input for the statisti-
cal learner in the next step.
Learning The constructed feature space contains
one feature vector per sentence. This implies that
the sequence information of the sentences at the doc-
ument level is not taken into account yet. Since the
order of the sentences in the abstract is a valuable
feature for this prediction problem, a learner that
reflects this in the learning process is needed, al-
though in principle any statistical learner can be used
on the feature space constructed by kLog. There-
fore we opted for SVM-HMM2 (Tsochantaridis et
al., 2004), which is an implementation of structural
support vector machines for sequence tagging. In
contrast to a conventional Hidden Markov Model,
SVM-HMM is able to take these entire feature vec-
tors as observations, and not just atomic tokens.
In our case, the instances to be tagged are formed
by the sentences for which feature vectors were cre-
ated in the previous step. The qid is a special fea-
ture that is used in the structured SVM to restrict
the generation of constraints. Since every document
needs to be represented as a sequence of sentences,
in SVM-HMM, the qid?s are used to obtain the doc-
ument structure. The order of the HMM was set
to 2, which means that the two previous sentences
were considered for collective classification. The
cost value was set to 500, and was determined via
cross-validation. For epsilon, the default value, 0.5,
was kept, since this mainly only influences the run-
ning time and memory consumption during training.
2http://www.cs.cornell.edu/people/tj/
svm_light/svm_hmm.html
All S U
Nb. Abstracts 1000 376 624
Nb. Sentences 10379 4774 5605
- Background 2557 669 1888
- Intervention 690 313 377
- Outcome 4523 2240 2283
- Population 812 369 443
- Study Design 233 149 84
- Other 1564 1034 530
Table 2: Number of abstracts and sentences for Struc-
tured (S) and Unstructured (U) abstract sets, including
number of sentences per class (taken from (Kim et al
2011)).
4 Evaluation
We evaluate the performance of kLog against a base-
line system and a memory-based tagger (Daelemans
and van den Bosch, 2005). The results are also com-
pared against those from Kim et al2011), which is
the state-of-the-art system for this task.
4.1 Datasets
We perform our experiments on the NICTA-
PIBOSO dataset from Kim et al2011) (kindly pro-
vided by the authors). It contains 1,000 abstracts of
which 500 were retrieved from MEDLINE by query-
ing for diverse aspects in the traumatic brain injury
and spinal cord injury domain. The dataset consists
of two types of abstracts. If the abstract contains
section headings (e.g. Background, Methodology,
Results, etc.), it is considered to be structured. This
information can be used as a feature in the model.
The other abstracts are regarded unstructured.
The definitions of the semantic tags used as an-
notations categories are a variation on the PICO tag
set, with the addition of two additional categories
(see Table 1 in Section 2). Each sentence can be an-
notated with multiple classes. This renders the task
a multiclass multilabel classification problem. The
statistics on this dataset can be found in Table 2.
In order to apply the same evaluation setting as
Kim et al2011), we used the dataset from Demner-
Fushman et al2005) as external dataset. It con-
sists of 100 sentences of which 51 are structured.
Because the semantic tag set used for annotation
slightly differs from the one presented in Table 1,
and to make our results comparable, we will use the
same mapping as used in Kim et al2011).
585
4.2 Baseline and benchmarks
We compare the kLog system to three other systems:
a baseline system, a memory-based system, and the
scores reported by Kim et al2011).
The memory-based system that we use is based
on the memory-based tagger MBT3 (Daelemans and
van den Bosch, 2005). This machine learner is orig-
inally designed for part-of-speech tagging. It pro-
cesses data on a sentence basis by carrying out se-
quential tagging, viz. the class label or other features
from previously tagged tokens can be used when
classifying a new token. In our setup, the sentences
of an abstract are taken as the processing unit and
the collection of all sentences in an abstract is taken
as one sequence.
The features that are used to label a sentence are
the class labels of four previous sentences, the am-
bitags of the following two sentences, the lemma of
the dependency root of the sentence, the position of
the sentence in the abstract, the lemma of the root
of the previous sentence, and section information.
For each root lemma, all possible class labels, as ob-
served in the training data, are concatenated into one
ambitag. These tags are stored in a list. An am-
bitag for a sentence is retrieved by looking up the
root lemma in this list. The position of the sentence
is expressed by a number. Section information is ob-
tained by looking for a previous sentence that con-
sists of only one token in uppercase. Finally, basic
lemmatization is carried out by removing a final S.
All other settings of MBT are the default settings
and no feature optimization nor feature selection has
been carried out to prevent overfitting.
When a class label contains multiple labels, like
e.g. population and study design, these labels are
concatenated in an alphabetically sorted manner.
This method of working reduces the multilabel prob-
lem to a problem with many different labels, i.e. the
label powerset method of Tsoumakas et al2010).
The baseline system is exactly the same as
the memory-based system except that no machine
learner is included. The most frequent class label in
the training data, i.e. Outcome, is assigned to each
instance. The memory-based system enables us to
compare kLog against a basic machine learning ap-
proach, using few features. The majority baseline
3http://ilk.uvt.nl/mbt [16 March 2012]
system enables us to compare the memory-based
system and kLog against a baseline in which no in-
formation about the observations is used.
4.3 Parametrization
From the kernel definition it might be clear that
the kLog hyperparameters, namely the distance d
and radius r, can have a strong influence on the
results. This requires a deliberate choice during
parametrization. From a linguistic perspective, the
use of unigrams and bigrams is justifiable, since
most phrases that reveal clues on the structure of the
abstract (e.g. evaluation measures, methodolody, fu-
ture work) can be expressed with single or pairs of
words. This is reflected by a distance and radius both
set to 1, which enables to take all possible combina-
tions of consecutive words into account and captures
the relational information attached to the word in fo-
cus, i.e. the current kernel point. This is confirmed
by cross-validation on other settings for the hyper-
parameters.
Since kLog generates a feature vector, only the
sequence information at word level is taken into ac-
count by kLog. Since we use a sequence labeling
approach as statistical learner, i.e. SVM-HMM, at
the level of the abstract this information is however
implicitly taken into account during learning. For
SVM-HMM, only the cost parameter C, which reg-
ulates the trade-off between the slack and the mag-
nitude of the weight-vector, and , that specifies the
precision to which constraints are required to be sat-
isfied by the solution, were optimized by means of
cross-validation. For the other parameters, the de-
fault values were used.
4.4 Results
Experiments are run on structured and unstructured
abstracts separately. On the NICTA-PIBOSO cor-
pus, we performed 10-fold cross-validation. Over
all folds, all labels, i.e. the parts of the multilabels,
are compared in a binary way between gold standard
and prediction. Summing all true positives, false
positives, and false negatives over all folds leads to
micro-averaged F-scores. This was done for two dif-
ferent settings. In one setting, CV/6-way, we com-
bined the labeling of the sentences with the identifi-
cation of irrelevant information, by adding the Other
586
label as an extra class in the classification. The re-
sults are listed in Table 3.
CV/6-way MBT Kim et alLog
Label S U S U S U
Background 71.0 61.3 81.84 68.46 86.19 76.90
Intervention 24.3 6.4 20.25 12.68 26.05 16.14
Outcome 87.9 70.4 92.32 72.94 92.99 77.69
Population 50.6 15.9 56.25 39.80 35.62 21.58
Study Design 45.9 13.10 43.95 4.40 45.5 6.67
Other 86.1 20.9 69.98 24.28 87.98 24.42
Table 3: F-scores per class for structured (S) and unstruc-
tured (U) abstracts.
For this setting, kLog is able to outperform both
MBT and the system of Kim et al2011), for both
structured and unstructured abstracts on all classes
except Population. From Table 4, where the micro-
average F-scores over all classes and for all settings
are listed, it can be observed that kLog performs up
to 3.73% better than MBT over structured abstracts,
and 9.67% better over unstructured ones.
Although to a lesser extent for the structured ab-
stracts, the same pattern can be observed for the
CV/5-way setting, where we tried to classify the sen-
tences only, without considering the irrelevant ones.
The per-class results for this setting are shown in Ta-
ble 5. Now the scores for Population are comparable
to the other systems, due to which we assume these
sentences are similar in structure to the ones labeled
with Other.
For the external corpus, the results are listed in Ta-
ble 6. Although kLog performs comparably for the
individual classes Background and Intervention, its
overall performance is worse on the structured ab-
stracts. In case of the unstructured abstracts, kLog
performs better on the majority of the individual
classes and in overall performance for the 5-way set-
ting, and comparable for the 4-way setting.
Baseline MBT kLog
Method S U S U S U
CV/6-way 43.90 41.87 80.56 57.47 84.29 67.14
CV/5-way 61.79 46.66 86.96 64.37 87.67 72.95
Ext/5-way 66.18 6.76 36.34 11.56 20.50 14.00
Ext/4-way 30.11 27.23 67.29 55.96 50.40 50.50
Table 4: Micro-averaged F1-score obtained for structured
(S) and unstructured (U) abstracts, both for 10-fold cross-
validation (CV) and on the external corpus (Ext).
CV/5-way MBT Kim et alLog
Label S U S U S U
Background 87.1 64.9 87.92 70.67 91.45 80.06
Intervention 48.0 6.9 48.08 21.39 45.58 22.65
Outcome 95.8 75.9 96.03 80.51 96.21 83.04
Population 70.9 21.4 63.88 43.15 63.96 23.32
Study Design 50.0 7.4 47.44 8.6 48.08 4.50
Table 5: F-scores per class for 5-way classification over
structured (S) and unstructured (U) abstracts.
MBT Kim et alLog
Label S U S U S U
Ext/5-way
Background 58.9 15.7 56.18 15.67 58.30 29.10
Intervention 21.5 13.8 15.38 28.57 40.00 34.30
Outcome 29.3 17.8 81.34 60.45 27.80 24.10
Population 10.7 17.8 35.62 28.07 5.60 28.60
Other 40.7 3.5 46.32 15.77 11.40 8.50
Ext/4-way
Background 90.4 67.5 77.27 37.5 65 68.6
Intervention 29 23.1 28.17 8.33 28.1 32.3
Outcome 74.1 74.6 90.5 78.77 72.4 72.7
Population 48.7 23.8 42.86 28.57 11.8 15.4
Table 6: F-scores per class for 5-way and 4-way classifi-
cation over structured (S) and unstructured (U) abstracts
on the external corpus.
As a general observation, it is important to note
that there is a high variability between the different
labels. Due to kLog?s ability to take the structured
input into account, we assume a correlation between
the sentence structure of the label and the predic-
tion quality. We intend to perform an extensive error
analysis, in order to detect patterns which may allow
us to incorporate additional declarative background
knowledge into our model.
5 Conclusions
We presented a statistical relational learning ap-
proach for the automatic identification of PICO cat-
egories in medical abstracts. To this extent, we used
kLog, a new framework for logical and relational
learning with kernels. Due to its graphical approach,
it is able to exploit the full relational representation,
that is often inherent in language structure. Since
contextual features are often essential and relations
are prevalent, the aim of this paper was to show
that statistical relational learning in general, and the
graph kernel-based approach of kLog in particular,
is specifically suited for problems in natural lan-
587
guage learning.
In future work, we intend to explore additional
ways to incorporate background knowledge in a
declarative way, since it renders the language learn-
ing problem more intuitive and gives a better under-
standing of feature contribution. Furthermore, we
also want to investigate the use of SRL approaches
for high-relational domains, and make a clear com-
parison with related techniques.
6 Acknowledgements
This research is funded by the Research Foundation
Flanders (FWO project G.0478.10 - Statistical Re-
lational Learning of Natural Language), and made
possible through financial support from the KU Leu-
ven Research Fund (GOA project 2008/08 Proba-
bilistic Logic Learning), the University of Antwerp
(GOA project BIOGRAPH) and the Italian Min-
istry of Education, University, and Research (PRIN
project 2009LNP494 - Statistical Relational Learn-
ing: Algorithms and Applications). The authors
would like to thank Fabrizio Costa, Kurt De Grave
and the anonymous reviewers for their valuable
feedback.
References
Shashank Agarwal and Hong Yu. 2009. Automatically
Classifying Sentences in Full-text Biomedical Articles
into Introduction, Methods, Results and Discussion.
Bioinformatics, 25(23):3174?3180.
E. C. Armstrong. 1999. The Well-built Clinical Ques-
tion: the Key to Finding the Best Evidence Efficiently.
WMJ, 98(2):25?28.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): Integrating Biomed-
ical Terminology. Nucleic Acids Research,
32(Suppl.1):D267?D270.
Grace Y Chung. 2009. Sentence Retrieval for Abstracts
of Randomized Controlled Trials. BMC Medical In-
formatics and Decision Making, 9(10).
Fabrizio Costa and Kurt De Grave. 2010. Fast Neighbor-
hood Subgraph Pairwise Distance Kernel. Proceed-
ings of the 26th International Conference on Machine
Learning, 255?262, Haifa, Israel. Omnipress.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing.. Studies in
Natural Language Processing. Cambridge University
Press, Cambridge, UK.
P. Davis-Desmond and Diego Molla?. 2012. Detection of
Evidence in Clinical Research Papers. Proceedings of
the Australasian Workshop On Health Informatics and
Knowledge Management (HIKM 2012), Melbourne,
Australia, 129:13?20. Australian Computer Society,
Inc.
Dina Demner-Fushman, Barbara Few, Susan E. Hauser,
and George Thoma. 2005. Automatically Identifying
Health Outcome Information in MEDLINE Records.
Journal of the American Medical Informatics Associa-
tion (JAMIA), 13:52?60.
Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering Clinical Questions with Knowledge Based
and Statistical Techniques. Computational Linguis-
tics, 33(1):63?103.
Luc De Raedt, Paolo Frasconi, Kristian Kersting, and
Stephen Muggleton, editors. 2008. Probabilistic In-
ductive Logic Programming. In: Lecture Notes in
Computer Science (LNCS), 4911. Springer-Verlag,
Heidelberg, Germany.
Paolo Frasconi, Fabrizio Costa, Luc De Raedt, and
Kurt De Grave. 2012. kLog - a Language
for Logical and Relational Learning with Kernels.
arXiv:1205.3981v2.
Hector Garcia-Molina, Jeff Ullman, and Jennifer Widom.
2008. Database Systems: The Complete Book. Pren-
tice Hall Press, Englewood Cliffs, NJ, USA.
Lise Getoor and Ben Taskar. 2007. Introduction to Sta-
tistical Relational Learning (Adaptive Computation
and Machine Learning). The MIT Press, Cambridge,
MA, USA.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report (UCSC-CRL-99-10), Uni-
versity of California at Santa Cruz.
Su Nam Kim, David Martinez, Lawrence Cavedon, and
Lars Yencken. 2011. Automatic Classification of Sen-
tences to Support Evidence Based Medicine. BMC
Bioinformatics, 12(2):S5.
Donald E. Knuth. 1965. On the Translation of Lan-
guages from Left to Right. Information and Control,
8: 607?639.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier.
2006. Zone Analysis in Biology Articles as a Basis
for Information Extraction. International Journal of
Medical Informatics, 75(6):468?487.
Diego Molla? and Mara Elena Santiago-Mart??nez. 2011.
Development of a Corpus for Evidence Medicine
Summarisation. Proceedings of the 2011 Australasian
Language Technology Workshop (ALTA 2011), Can-
berra, Australia, 86?94. Association for Computa-
tional Linguistics.
Yun Niu, Graeme Hirst, Gregory McArthur, and Patri-
cia Rodriguez-Gianolli. 2003. Answering Clinical
588
Questions with Role Identification. Proceedings of the
ACL, Workshop on Natural Language Processing in
Biomedicine. Sapporo, Japan, 73?80. Association for
Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov logic.
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2008. Hon-
olulu, Hawaii, 650?659. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: a Corpus for Information
Extraction in the Biomedical Domain. BMC Bioinfor-
matics, 8:50.
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective
semantic role labelling with Markov logic. Proceed-
ings of the Twelfth Conference on Computational Nat-
ural Language Learning (CoNLL 2008. Manchester,
United Kingdom, 193?197. Association for Computa-
tional Linguistics.
William Rosenberg and Anna Donald. 1995. Evidence
Based Medicine: an Approach to Clinical Problem
Solving. British Medical Journal, 310(6987):1122?
1126.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
Parsing and Domain Adaptation with LR Models and
Parser Ensembles. Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, Prague, Czech
Republic, 1044?1050. Association for Computational
Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support Vector
Machine Learning for Interdependent and Structured
Output Spaces. Proceedings of the twenty-first inter-
national conference on Machine learning (ICML), Al-
berta, Canada, 104?111. ACM.
Grigorios Tsoumakas and Ioannis Katakis and Ioannis P.
Vlahavas. Oded Maimon and Lior Rokach, editors.
2010. Mining Multi-label Data. In: Data Mining and
Knowledge Discovery Handbook, 2nd ed., 667?685.
Springer-Verlag, Heidelberg, Germany.
Mathias Verbeke, Paolo Frasconi, Vincent Van Asch,
Roser Morante, Walter Daelemans, and Luc De Raedt.
2012. Kernel-based Logical and Relational Learning
with kLog for Hedge Cue Detection. Proceedings of
the 21th International Conference on Inductive Logic
Programming, in press.
589
CNTS: Memory-Based Learning of Generating Repeated References
Iris Hendrickx, Walter Daelemans, Kim Luyckx, Roser Morante, Vincent Van Asch
CNTS, Department of Linguistics
University of Antwerp
Prinsstraat 13, 2000, Antwerp, Belgium
firstname.lastname@ua.ac.be
Abstract
In this paper we describe our machine learning
approach to the generation of referring expres-
sions. As our algorithm we use memory-based
learning. Our results show that in case of pre-
dicting the TYPE of the expression, having one
general classifier gives the best results. On the
contrary, when predicting the full set of prop-
erties of an expression, a combined set of spe-
cialized classifiers for each subdomain gives
the best performance.
1 Introduction
In this paper we describe the systems with which
we participated in the GREC task of the REG 2008
challenge (Belz and Varges, 2007). The GREC task
concerns predicting which expression is appropriate
to refer to a particular discourse referent in a certain
position in a text, given a set of alternative referring
expressions for selection. The organizers provided
the GREC corpus that consists of 2000 texts col-
lected from Wikipedia, from 5 different subdomains
(people, cities, countries, mountains and rivers) .
One of the main goals of the task is to discover
what kind of information is useful in the input to
make the decision between candidate referring ex-
pressions. We experimented with a pool of features
and several machine learning algorithms in order to
achieve this goal.
2 Method
We apply a standard machine learning approach
to the task. We train a classifier to predict the
correct label for each mention. As our machine
learning algorithm we use memory-based learn-
ing as implemented in the Timbl package (Daele-
mans et al, 2007). To select the optimal algorith-
mic parameter setting for each classifier we used
a heuristic optimization method called paramsearch
(Van den Bosch, 2004). We also tried several other
machine learning algorithms implemented in the
Weka package (Witten and Frank, 2005), but these
experiments did not lead to better results and are not
further discussed here.
We developed four systems: a system that only
predicts the TYPE of each expression (Type), so it
predicts four class labels; and a system that pre-
dicts the four properties (TYPE, EMPATHIC, HEAD,
CASE) of each expression simultaneously (Prop).
The class labels predicted by this system are con-
catenated strings: ?common no nominal plain?, and
these concatenations lead to 14 classes, which
means that not all combinations appear in the train-
ing set. For both Type an Prop we created two vari-
ants: one general classifer (g) that is trained on all
subdomains, and a set of combined specialized clas-
sifiers (s) that are optimized for each domain sepa-
rately.
3 System description
To build the feature representations, we first prepro-
cessed the texts performing the following actions:
rule-based tokenization, memory-based part-of-
speech tagging, NP-chunking, Named entity recog-
nition, and grammatical relation finding (Daelemans
and van den Bosch, 2005). We create an instance for
each mention, using the following features to repre-
194
sent each instance:
? Positional features: the sentence number, the
NP number, a boolean feature that indicates if
the mention appears in the first sentence.
? Syntactic and semantic category given of the
entity (SEMCAT, SYNCAT).
? Local context of 3 words and POS tags left and
right of the entity.
? Distance to the previous mention measured in
sentences and in NPs.
? Trigram pattern of the given syntactic cate-
gories of 3 previous mentions.
? Boolean feature indicating if the previous sen-
tence contains another named entity than the
entity in focus.
? the main verb of the sentence.
We do not use any information about the given set
of alternative expressions except for post process-
ing. In a few cases our classifier predicts a label that
is not present in the set of alternatives. For those
cases we choose the most frequent class label (as es-
timated on the training set).
We experimented with predicting all subdomains
with the same classifier and with creating separate
classifiers for each subdomains. We expected that
semantically different domains would have different
preferences for expressions.
4 Results
We provide results for the four systems Type-g,
Type-s, Prop-g and Prop-s in Table 1. The evalua-
tion script was provided by the organisers. The vari-
ant Type-g performs best with a score of 76.52% on
the development set.
5 Conclusions
In this paper we described our machine learning ap-
proach to the generation of referring expressions.
We reported results of four memory-based systems.
Predicting all subdomains with the same classifier
is more efficient when predicting the coarse-grained
TYPE class. On the contrary, training specialized
classifiers for each subdomain works better for the
Data Type-g Type-s
Cities 64.65 60.61
Countries 75.00 71.74
Mountains 75.42 77.07
People 85.37 72.50
Rivers 65.00 80.00
All 76.52 72.26
Data Prop-g Prop-s
Cities 63.64 65.66
Countries 72.83 69.57
Mountains 72.08 74.58
People 79.51 79.51
Rivers 65.00 70.00
All 73.02 73.93
Table 1: Accuracy on GREC development set.
more fine-grained prediction of all properties simu-
laneously. For the test set we will present results the
two best systems: CNTS-Type-g and CNTS-Prop-s.
Acknowledgments
This research is funded by FWO, IWT, GOA BOF UA,
and the STEVIN programme funded by the Dutch and
Flemish Governments.
References
A. Belz and S. Varges. 2007. Generation of repeated ref-
erences to discourse entities. In In Proceedings of the
11th European Workshop on Natural Language Gen-
eration (ENLG?07), pages 9?16.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2007. TiMBL: Tilburg Memory
Based Learner, version 6.1, reference manual. Techni-
cal Report 07-07, ILK, Tilburg University.
A. Van den Bosch. 2004. Wrapped progressive sampling
search for optimizing learning algorithm parameters.
In Proceedings of the 16th Belgian-Dutch Conference
on Artificial Intelligence, pages 219?226.
I. H. Witten and E. Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques, second
edition. Morgan Kaufmann, San Francisco.
195
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 208?212
Manchester, August 2008
A Combined Memory-Based Semantic Role Labeler of English
Roser Morante, Walter Daelemans, Vincent Van Asch
CNTS - Language Technology Group
University of Antwerp
Prinsstraat 13, B-2000 Antwerpen, Belgium
{Roser.Morante,Walter.Daelemans,Vincent.VanAsch}@ua.ac.be
Abstract
We describe the system submitted to
the closed challenge of the CoNLL-2008
shared task on joint parsing of syntactic
and semantic dependencies. Syntactic de-
pendencies are processed with the Malt-
Parser 0.4. Semantic dependencies are
processed with a combination of memory-
based classifiers. The system achieves
78.43 labeled macro F1 for the complete
problem, 86.07 labeled attachment score
for syntactic dependencies, and 70.51 la-
beled F1 for semantic dependencies.
1 Introduction
In this paper we describe the system submitted to
the closed challenge of the CoNLL-2008 shared
task on joint parsing of syntactic and semantic de-
pendencies (Surdeanu et al, 2008). Compared to
the previous shared tasks on semantic role label-
ing, the innovative feature of this one is that it
consists of extracting both syntactic and seman-
tic dependencies. The semantic dependencies task
comprises labeling the semantic roles of nouns and
verbs and disambiguating the frame of predicates.
The system that we present extracts syntactic
and semantic dependencies independently. Syn-
tactic dependencies are processed with the Malt-
Parser 0.4 (Nivre, 2006; Nivre et al, 2007). Se-
mantic dependencies are processed with a combi-
nation of memory-based classifiers.
Memory-based language processing (Daele-
mans and van den Bosch, 2005) is based on the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
idea that NLP problems can be solved by stor-
ing solved examples of the problem in their literal
form in memory, and applying similarity-based
reasoning on these examples in order to solve new
ones. Keeping literal forms in memory has been
argued to provide a key advantage over abstracting
methods in NLP that ignore exceptions and sub-
regularities (Daelemans et al, 1999).
Memory-based algorithms have been previously
applied to semantic role labeling. Van den
Bosch et al (2004) participated in the CoNLL-
2004 shared task with a system that extended
the basic memory-based learning method with
class n-grams, iterative classifier stacking, and
automatic output post-processing. Tjong Kim
Sang et al (2005) participated in the CoNLL-
2005 shared task with a system that incorporates
spelling error correction techniques. Morante and
Busser (2007) participated in the SemEval-2007
competition with a semantic role labeler for Span-
ish based on gold standard constituent syntax.
These systems use different types of constituent
syntax (shallow parsing, full parsing). We are
aware of two systems that perform semantic role
labeling based on dependency syntax previous to
the CoNLL-2008 shared task. Hacioglu (2004)
converts the data from the CoNLL-2004 shared
task into dependency trees and uses support vector
machines. Morante (2008) describes a memory-
based semantic role labeling system for Spanish
based on gold standard dependency syntax.
We developed a memory-based system for the
CoNLL-2008 shared task in order to evaluate the
performance of this methodology in a completely
new semantic role labeling setting.
The paper is organised as follows. In Section 2
the system is described, Section 3 contains an anal-
ysis of the results, and Section 4 puts forward some
208
conclusions.
2 System description
The system processes syntactic and semantic de-
pendencies independently. The syntactic depen-
dencies are processed with the MaltParser 0.4. The
semantic dependencies are processed with a cas-
cade of memory-based classifiers. We use the
IB1 classifier as implemented in TiMBL (version
6.1.2) (Daelemans et al, 2007), a supervised in-
ductive algorithm for learning classification tasks
based on the k-nearest neighbor classification rule
(Cover and Hart, 1967). In IB1, similarity is de-
fined by computing (weighted) overlap of the fea-
ture values of a test instance and a memorized ex-
ample. The metric combines a per-feature value
distance metric with global feature weights that
account for relative differences in discriminative
power of the features.
2.1 Syntactic dependencies
The MaltParser 0.4
1
(Nivre, 2006; Nivre et al,
2007) is an inductive dependency parser that uses
four essential components: a deterministic algo-
rithm for building labeled projective dependency
graphs; history-based feature models for predict-
ing the next parser action; support vector ma-
chines for mapping histories to parser actions;
and graph transformations for recovering non-
projective structures.
The learner type used was support vector ma-
chines, with the same parameter options re-
ported by (Nivre et al, 2006). The parser
algorithm used was Nivre, with the options
and model (eng.par) for English as specified
on http://w3.msi.vxu.se/users/jha/conll07/. The
tagset.pos, tagset.cpos and tagset.dep were ex-
tracted from the training corpus.
2.2 Semantic dependencies
The semantics task consists of finding the predi-
cates, assigning a PropBank or a NomBank frame
to them and extracting their semantic role depen-
dencies. Because of lack of resources, we did not
have time to develop a word sense disambiguation
system. So, predicates were assigned the frame
?.01? by default.
The system handles the semantic role labeling
task in three steps: predicate identification, seman-
1
Web page of MaltParser 0.4:
http://w3.msi.vxu.se/?nivre/research/MaltParser.html.
tic dependency classification, and combination of
classifiers.
2.2.1 Predicate identification
In this phase, a classifier predicts if a word is a
predicate or not. The IB1 algorithm was param-
eterised by using overlap as the similarity metric,
information gain for feature weighting, using 7 k-
nearest neighbors, and weighting the class vote of
neighbors as a function of their inverse linear dis-
tance. The instances represent all nouns and verbs
in the corpus and they have the following features:
? Word form, lemma, part of speech (POS), the three last
letters of the word, and the lemma and POS of the five
previous and five next words. To obtain the previous
word we perform a linear left-to-right search. This is
how previous has to be interpreted further on when fea-
tures are described.
The accuracy of the classifier on the develop-
ment test is 0.9599 (4240/4417) for verbs and
0.8981 (9226/10272) for nouns.
2.2.2 Semantic dependency classification
In this phase, three groups of multi-class clas-
sifiers predict in one step if there is a dependency
between a word and a predicate, and the type of
dependency, i.e. semantic role.
Group 1 (G1) consists of two classifiers: one
for predicates that are nouns and another for pred-
icates that are verbs. The instances represent a
predicate-word combination. The predicates are
those that have been classified as such in the previ-
ous phase. As for the combining words, determin-
ers and certain combinations are excluded based
on the fact that they never have a role in the train-
ing corpus.
The IB1 algorithm was parameterised by using
overlap as the similarity metric, information gain
for feature weighting, using 11 k-nearest neigh-
bors, and weighting the class vote of neighbors as
a function of their inverse linear distance. The fea-
tures of the noun classifier are:
? About the predicate: word form. About the combining
word: word form, POS, dependency type, word form
of the two previous and two next words. Chain of POS
types between the word and the predicate. Distance be-
tween the word and the predicate. Binary feature indi-
cating if the word depends on the predicate. Six chains
of POS tags between the word and its three previous and
three next predicates in relation to the current predicate.
209
The features of the verb classifier are:
? The same as for the noun classifier and additionally:
POS of the word next to the current combining word,
binary feature indicating if the combining word de-
pends on the predicate previous to the current predicate,
binary feature indicating if the predicate previous to the
combining word is located before or after the current
predicate.
The verb classifier achieves an overall accuracy
of 0.9244 (80805/87412), and the noun classifier,
0.9173 (69836/76132) in the development set.
Group 2 (G2) consists also of two classifiers:
one for predicates that are nouns and another for
predicates that are verbs. The instances represent
combinations of word-predicate, but the test cor-
pus contains only those instances that G1 has clas-
sified as having a role.
The IB1 algorithm was parameterised in the
same way as for G1, except that it computes 7 k-
nearest neighbors instead of 11. The two classifiers
use the same features:
? About the predicate: word form, chain of lemmas of the
syntactic siblings, chain of lemmas of the syntactic chil-
dren. About the combining word: word form, POS, de-
pendency type, word form of the two previous and the
two next words, POS+type of dependency and lemma
of the syntactic father, chain of dependency types and
chain of lemmas of the syntactic children. Chain of
POS types between word and predicate, distance and
syntactic dependency type between word and predicate.
The verb classifier achieves an overall accuracy
of 0.5656 (4160/7355), and the noun classifier,
0.5017 (2234/4452) in the development set.
Group 3 (G3) consists of one classifier. Like
G2, instances represent combinations of word-
predicate, but the test corpus contains only those
instances that G1 has classified as having a role..
The IB1 algorithm was parameterised in the same
way as for G2. It uses the following features:
About the predicate: lemma, POS, POS of the 3 previous
and 3 next predicates. About the combining word: lemma,
POS, and dependency type, POS of the 3 previous and 3 next
words. Distance between the predicate and the word. A bi-
nary feature indicating if the combining word is located be-
fore or after the predicate.
The classifier achieves an overall accuracy of
0.5527 (6526/11807).
2.2.3 Combination of classifiers
In this phase the three groups of classifiers are
combined in a simple way: if G2 and G3 agree
in classifying a semantic dependency, their solu-
tion is chosen, else the solution of G1 is chosen.
This system combination choice is explained by
the fact that G1 has a higher accuracy than G2 and
G3 when the three classifiers are applied to the de-
velopment set. G2 and G3 are used to eliminate
overgeneration of roles by G1.
The performance of the system in the develop-
ment corpus with only the G1 classifiers is 66.07
labeled F1. The combined system achieves a
10.8% error reduction, with 69.75 labeled F1.
3 Results
The results of the system are shown in Table 1.
We will focus on commenting on the semantic
scores. The system scores 71.88 labeled F1 in the
in-domain corpus (WSJ) and 59.23 in the out-of-
domain corpus (Brown). Unlabeled F1 in the WSJ
corpus is almost 10% higher than labeled F1. La-
beled precision is 12.40% higher than labeled re-
call.
WSJ BROWN
SYNTACTIC SCORES
Labeled attachment score 86.88 79.58
Unlabeled attachment score 89.37 84.85
Label accuracy score 91.48 86.00
SEMANTIC SCORES
Labeled precision 78.61 65.25
Labeled recall 66.21 54.23
Labeled F1 71.88 59.23
Unlabeled precision 89.13 83.61
Unlabeled recall 75.08 69.48
Unlabeled F1 81.50 75.89
OVERALL MACRO SCORES
Labeled macro precision 82.74 72.41
Labeled macro recall 76.54 66.90
Labeled macro F1 79.52 69.55
Unlabeled macro precision 89.25 84.23
Unlabeled macro recall 82.22 77.16
Unlabeled macro F1 85.59 80.54
Table 1: Results of the system in the WSJ and
BROWN corpora expressed in %.
3.1 Discussion
The performance of the semantic role labeler is af-
fected considerably by the performance of the first
classifier for predicate detection. The system can-
not recover from the predicates that are missed in
this phase. Experiments without the first classifier
and with gold standard predicates (detection and
classification) result in 80.89 labeled F1, 9.01 %
210
higher than the results of the system with predi-
cate detection. We opted for identifying predicates
as a first step in order to reduce the number of
training instances for the second phase, classifica-
tion of semantic dependencies. For the same rea-
son, we opted for selecting only nouns and verbs
as instances, aware of the fact that we would miss
a very low number of predicates with other cate-
gories. The results of predicate identification can
be improved by setting up a combined system, in-
stead of a single classifier, and by incorporating a
system for frame disambiguation.
Equally important would be to find better fea-
tures for the identification of noun predicates,
since the features used generalise better for verbs
than for nouns. Table 2 shows that the system is
better at identifying verbs than it is at identifying
nouns.
Total F1 Pred. F1 Pred.
Id.&Cl. Id.
CC 3 - -
CD 1 - -
IN 3 - -
JJ 16 - -
NN 3635 77.57 85.59
NNP 10 30.77 38.46
NNS 1648 75.47 83.65
PDT 2 - -
RP 4 - -
VB 1278 79.28 98.87
VBD 1320 85.44 99.24
VBG 742 77.05 94.41
VBN 985 76.43 92.08
VBP 343 78.60 97.81
VBZ 504 80.94 97.36
WP 2 - -
WRB 2 - -
Table 2: Predicate (Pred.) identification (Id.) and
classification (Cl.) in the WSJ corpus expressed in
%.
A characteristic of the semantic role labeler is
that recall is considerably lower than precision
(12.40 %). This can be further analysed with the
data shown in Table 3.
Except for the dependency VB*+AM-NEG,
precision is higher than recall for all semantic de-
pendencies. We run the semantic role labeler with
gold standard predicates and with gold standard
syntax and predicates. The difference between pre-
cision and recall is around 10 % in both cases,
which confirms that low recall is a characteristic
of the semantic role labeler, probably caused by
the fact that the features do not generalise good
enough. The semantic role labeler with gold stan-
Dependency Total Recall Prec. F1
NN*+A0 2339 42.41 77.80 54.90
NN*+A1 3757 61.17 78.32 68.69
NN*+A2 1537 45.48 82.24 58.57
NN*+A3 349 50.14 88.38 63.98
NN*+AM-ADV 32 9.38 37.50 15.01
NN*+AM-EXT 33 18.18 85.71 30.00
NN*+AM-LOC 232 30.60 63.96 41.40
NN*+AM-MNR 344 34.59 79.87 48.27
NN*+AM-NEG 35 2.86 100.00 5.56
NN*+AM-TMP 492 54.88 83.33 66.18
VB*+A0 3509 68.99 82.63 75.20
VB*+A1 4844 74.24 83.28 78.50
VB*+A2 1085 55.94 69.21 61.87
VB*+A3 169 41.42 79.55 54.48
VB*+A4 99 74.75 88.10 80.88
VB*+AM-ADV 488 38.93 59.19 46.97
VB*+AM-CAU 70 50.00 70.00 58.33
VB*+AM-DIR 81 29.63 57.14 39.02
VB*+AM-DIS 315 52.70 74.11 61.60
VB*+AM-EXT 32 50.00 59.26 54.24
VB*+AM-LOC 355 52.11 57.10 54.49
VB*+AM-MNR 335 46.57 61.18 52.88
VB*+AM-MOD 539 92.21 95.95 94.04
VB*+AM-NEG 227 94.71 90.34 92.47
VB*+AM-PNC 113 33.63 54.29 41.53
VB*+AM-TMP 1068 64.51 80.40 71.58
VB*+C-A1 192 65.10 74.85 69.64
VB*+R-A0 222 65.77 87.43 75.07
VB*+R-A1 155 49.68 73.33 59.23
VB*+R-AM-LOC 21 23.81 71.43 35.71
VB*+R-AM-TMP 52 46.15 66.67 54.54
Table 3: Semantic dependencies identification and
classification in the WSJ corpus for dependencies
with more than 20 occurences expressed in %.
dard predicates scores 86.06 % labeled precision
and 76.32 % labeled recall. The semantic role
labeler with gold standard predicates and syntax
scores 89.20 % precision and 79.47 % recall.
Table 3 also shows that the unbalance between
precision and recall is higher for dependencies of
nouns than for dependencies of verbs, and that
both recall and precision are higher for dependen-
cies from verbs. Thus, the system performs better
for verbs than for nouns. This is in part caused
by the fact that more noun predicates than verb
predicates are missed in the predicate identifica-
tion phase. The scores of the the semantic role
labeler with gold standard predicates show lower
differences in F1 between verbs and nouns.
The fact that the semantic role labeler performs
3.16 % labeled F1 better with gold standard syntax
(compared to the system with gold standard syntax
and predicates) confirms that gold standard syntax
provides useful information to the system.
Additionally, the difference in performance be-
tween the semantic role labeler presented to the
211
competition and the semantic role labeler with
gold standard predicates (9.01 % labeled F1) sug-
gests that, although the results of the system are
encouraging, there is room for improvement, and
improvement should focus on increasing the recall
scores.
4 Conclusions
In this paper we have presented a system submitted
to the closed challenge of the CoNLL-2008 shared
task on joint parsing of syntactic and semantic de-
pendencies. We have focused on describing the
part of the system that extracts semantic dependen-
cies, a combination of memory-based classifiers.
The system achieves a semantic score of 71,88 la-
beled F1. Results show that the system is con-
siderably affected by the first phase of predicate
identification, that the system is better at extract-
ing the semantic dependencies of verbs than those
of nouns, and that recall is substantially lower than
precision. These facts suggest that, although the
results are encouraging, there is room for improve-
ment.
5 Acknowledgements
This work was made possible through financial
support from the University of Antwerp (GOA
project BIOGRAPH), and from the Flemish Insti-
tute for the Promotion of Innovation by Science
and Technology Flanders (IWT) (TETRA project
GRAVITAL). The experiments were carried out in
the CalcUA computing facilities. We are grateful
to Stefan Becuwe for his support.
References
Cover, T. M. and P. E. Hart. 1967. Nearest neigh-
bor pattern classification. Institute of Electrical and
Electronics Engineers Transactions on Information
Theory, 13:21?27.
Daelemans, W. and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
Daelemans, W., A. Van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learn-
ing. Machine Learning, Special issue on Natural
Language Learning, 34:11?41.
Daelemans, W., J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2007. TiMBL: Tilburg memory based
learner, version 6.1, reference guide. Technical Re-
port Series 07-07, ILK, Tilburg, The Netherlands.
Hacioglu, K. 2004. Semantic role labeling using de-
pendency trees. In COLING ?04: Proceedings of
the 20th international conference on Computational
Linguistics, Morristown, NJ, USA. ACL.
Morante, R. and B. Busser. 2007. ILK2: Semantic
role labelling for Catalan and Spanish using TiMBL.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 183?
186.
Morante, R. 2008. Semantic role labeling tools trained
on the Cast3LB-CoNLL-SemRol corpus. In Pro-
ceedings of the LREC 2008, Marrakech, Morocco.
Nivre, J., J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo?projective dependency pars-
ing with support vector machines. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, CoNLL-X, New York City, NY,
June.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K?ubler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: a language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95?135.
Nivre, J. 2006. Inductive Dependency Parsing.
Springer.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
Tjong Kim Sang, E., S. Canisius, A. van den Bosch,
and T. Bogers. 2005. Applying spelling error cor-
rection techniques for improving semantic role la-
belling. In Proceedings of the Ninth Conference
on Natural Language Learning (CoNLL-2005), Ann
Arbor, MI.
van den Bosch, A., S. Canisius, W. Daelemans, I. Hen-
drickx, and E. Tjong Kim Sang. 2004. Memory-
based semantic role labeling: Optimizing features,
algorithm, and output. In Ng, H.T. and E. Riloff, ed-
itors, Proceedings of the Eighth Conference on Com-
putational Natural Language Learning (CoNLL-
2004), Boston, MA, USA.
212
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 31?36,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Using Domain Similarity for Performance Estimation
Vincent Van Asch
CLiPS - University of Antwerp
Antwerp, Belgium
Vincent.VanAsch@ua.ac.be
Walter Daelemans
CLiPS - University of Antwerp
Antwerp, Belgium
Walter.Daelemans@ua.ac.be
Abstract
Many natural language processing (NLP)
tools exhibit a decrease in performance
when they are applied to data that is lin-
guistically different from the corpus used
during development. This makes it hard to
develop NLP tools for domains for which
annotated corpora are not available. This
paper explores a number of metrics that
attempt to predict the cross-domain per-
formance of an NLP tool through statis-
tical inference. We apply different sim-
ilarity metrics to compare different do-
mains and investigate the correlation be-
tween similarity and accuracy loss of NLP
tool. We find that the correlation between
the performance of the tool and the sim-
ilarity metric is linear and that the latter
can therefore be used to predict the perfor-
mance of an NLP tool on out-of-domain
data. The approach also provides a way to
quantify the difference between domains.
1 Introduction
Domain adaptation has recently turned into a
broad field of study (Bellegarda, 2004). Many re-
searchers note that the linguistic variation between
training and testing corpora is an important fac-
tor in assessing the performance of an NLP tool
across domains. For example, a tool that has been
developed to extract predicate-argument structures
from abstracts of biomedical research papers, will
exhibit a lower performance when applied to legal
texts.
However, the notion of domain is mostly arbi-
trarily used to refer to some kind of semantic area.
There is unfortunately no unambiguous measure
to assert a domain shift, except by observing the
performance loss of an NLP tool when applied
across different domains. This means that we typ-
ically need annotated data to reveal a domain shift.
In this paper we will show how unannotated data
can be used to get a clearer view on how datasets
differ. This unsupervised way of looking at data
will give us a method to measure the difference be-
tween data sets and allows us to predict the perfor-
mance of an NLP tool on unseen, out-of-domain
data.
In Section 2 we will explain our approach in
detail. In Section 3 we deal with a case study
involving basic part-of-speech taggers, applied to
different domains. An overview of related work
can be found in Section 4. Finally, Section 5 con-
cludes this paper and discusses options for further
research.
2 Approach
When developing an NLP tool using supervised
learning, annotated data with the same linguistic
properties as the data for which the tool is devel-
oped is needed, but not always available. In many
cases, this means that the developer needs to col-
lect and annotate data suited for the task. When
this is not possible, it would be useful to have a
method that can estimate the performance on cor-
pus B of an NLP tool trained on corpus A in an
unsupervised way, i.e., without the necessity to an-
notate a part of B.
In order to be able to predict in an unsupervised
way the performance of an NLP tool on different
corpora, we need a way to measure the differences
between the corpora. The metric at hand should
be independent from the annotation labels, so that
it can be easily applied on any given corpus. The
aim is to find a metric such that the correlation be-
tween the metric and the performance is statisti-
cally significant. In the scope of this article the
concept metric stands for any way of assigning a
sufficiently fine-grained label to a corpus, using
only unannotated data. This means that, in our
view, a metric can be an elaborate mixture of fre-
quency counts, rules, syntactic pattern matching or
31
even machine learner driven tools. However, in the
remainder of this paper we will only look at fre-
quency based similarity metrics since these met-
rics are easily applicable and the experiments con-
ducted using these metrics were already encourag-
ing.
3 Experimental design
3.1 Corpus
We used data extracted from the British National
Corpus (BNC) (2001) and consisting of written
books and periodicals1. The BNC annotators pro-
vided 9 domain codes (i.e. wridom), making it
possible to divide the text from books and peri-
odicals into 9 subcorpora. These annotated se-
mantic domains are: imaginative (wridom1), nat-
ural & pure science (wridom2), applied science
(wridom3), social science (wridom4), world af-
fairs (wridom5), commerce & finance (wridom6),
arts (wridom7), belief & thought (wridom8), and
leisure (wridom9).
The extracted corpus contains sentences in
which every token is tagged with a part-of-speech
tag as defined by the BNC. Since the BNC has
been tagged automatically, using the CLAWS4 au-
tomatic tagger (Leech et al, 1994) and the Tem-
plate Tagger (Pacey et al, 1997), the experiments
in this article are artificial in the sense that they do
not learn real part-of-speech tags but rather part-
of-speech tags as they are assigned by the auto-
matic taggers.
3.2 Similarity metrics
To measure the difference between two corpora
we implemented six similarity metrics: Re?nyi2
(Re?nyi, 1961), Variational (L1) (Lee, 2001),
Euclidean (Lee, 2001), Cosine (Lee, 2001),
Kullback-Leibler (Kullback and Leibler, 1951)
and Bhattacharyya coefficient (Comaniciu et al,
2003; Bhattacharyya, 1943). We selected these
measures because they are well-described and pro-
duce results for this task in an acceptable time
span.
The metrics are computed using the relative fre-
quencies of words. For example, to calculate the
1This is done by selecting texts with BNC category codes
for text type (i.e. alltyp3 (written books and periodicals)) and
for medium (i.e. wrimed1 (book), wrimed2 (periodical), and
wrimed3 (miscellaneous: published)).
2The Re?nyi divergence has a parameter ? and Kullback-
Leibler is a special case of the Re?nyi divergence, viz. with
? = 1.
Re?nyi divergence between corpus P and corpus Q
the following formula is applied:
Re?nyi(P ;Q;?) = 1(??1) log2
(
?k p1??k q
?
k
)
pk is the relative frequency of a token k in the
first corpus P , and qk is the relative frequency of
token k in the second corpus Q. ? is a free param-
eter and with ? = 1 the Re?nyi divergence becomes
equivalent to the Kullback-Leibler divergence.
R?nyi 0.99
Euclidean
LESS SIMILAR                                                                                        
MORE SIMILAR                                                                                        
social-art                                                                                          
social-belief                                                                                       
social-world                                                                                        
social-imaginative                                                                                  
art-social                                                                                          
MORE SIMILAR                                                                       
LESS SIMILAR                                                                                        
social-art                                                                                          
social-belief                                                                      
social-world                                                                                        
social-imaginative                                                                                  
Figure 1: A visual comparison of two similarity
metrics: Re?nyi with ? = 0.99 and Euclidean.
Figure 1 gives an impression of the difference
between two similarity metrics: Re?nyi (? = 0.99)
and Euclidean. Only four domain combinations
are shown for the sake of clarity. From the graph
it can be observed that the social and imaginative
domains are the least similar in both cases. Be-
sides the different ordering, there is also a differ-
ence in symmetry. Contrary to the symmetric Eu-
clidean metric, the Re?nyi scores differ, depending
on whether social constitutes the test set and art
the training set, or vice versa. The dashed line on
Figure 1 (left) is a reverse score, namely for art-
social. A divergence score may diverge a lot from
its reverse score.
In practice, the best metric to choose is the met-
ric that gives the best linear correlation between
the metric and the accuracy of an NLP tool applied
across domains. We tested 6 metrics: Re?nyi, Vari-
ational (L1), Euclidean, Cosine, Kullback-Leibler,
and the Bhattacharyya coefficient. For Re?nyi, we
tested four different ?-values: 0.95, 0.99, 1.05,
and 1.1. Most metrics gave a linear correlation
but for our experiments with data-driven POS tag-
ging, the Re?nyi metric with ? = 0.99 was the best
32
according to the Pearson product-moment corre-
lation. For majority this correlation was 0.91, for
Mbt 0.93, and for SVMTool 0.93.
3.3 Part-of-speech tagging
The experiments carried out in the scope of this
article are all part-of-speech (POS) tagging tasks.
There are 91 different POS labels in the BNC cor-
pus which are combinations of 57 basic labels. We
used three algorithms to assign part-of-speech la-
bels to the words from the test corpus:
Majority This algorithm assigns the POS label
that occurs most frequently in the training set for a
given word, to the word in the test set. If the word
did not occur in train, the overall most frequent tag
was used.
Memory based POS tagger (Daelemans and
van den Bosch, 2005) A machine learner that
stores examples in memory (Mbt) and uses the
kNN algorithm to assign POS labels. The default
settings were used.
SVMTool POS tagger (Gime?nez and Ma?rquez,
2004) Support vectors machines in a sequential
setup are used to assign the POS labels. The de-
fault settings were used.
3.4 Results and analysis
Figure 2 shows the outcome of 72 cross-validation
experiments on the data from the British National
Corpus. The graph for the majority baseline is
shown in Figure 2a. The results for the memory
based tagger are shown in Figure 2b and the graph
for SVMTool is displayed in Figure 2c.
For every domain, the data is divided into five
parts. For all pairs of domains, each part from
the training domain is paired with each part from
the testing domain. This results in a 25 cross-
validation cross-domain experiment. A data point
in Figure 2 is the average outcome of such a 25
fold experiment. The abscissa of a data point
is the Re?nyi similarity score between the train-
ing and testing component of an experiment. The
? parameter was set to 0.99. We propose that
the higher (less negative) the similarity score, the
more similar training and testing data are.
The ordinate is the accuracy of the POS tagging
experiment. The dotted lines are the 95% predic-
tion intervals for every data point. These bound-
aries are obtained by linear regression using all
other data points. The interpretation of the inter-
vals is that any point, given all other data points
25 20 15 10 5R?nyi divergence score with alpha=0.99
74
76
78
80
82
84
86
88
90
92
Maj
ority
 acc
urac
y (%
)
(72 data points)
Majority accuracy prediction
95% prediction interval
(a) Majority POS tagger.
25 20 15 10 5R?nyi divergence score with alpha=0.99
86
87
88
89
90
91
92
93
94
95
Mbt
 acc
urac
y (%
)
(72 data points)
Mbt accuracy prediction
95% prediction interval
(b) Memory based POS tagger.
25 20 15 10 5R?nyi divergence score with alpha=0.99
88
89
90
91
92
93
94
95
96
SVM
Too
l ac
cura
cy (
%)
(72 data points)
SVMTool accuracy prediction
95% prediction interval
(c) SVMTool POS tagger.
Figure 2: The varying accuracy of three POS tag-
gers with varying distance between train and test
corpus of different domains.
from the graph, can be predicted with 95% cer-
tainty, to lie between the upper and lower interval
boundary at the similarity score of that point. The
average difference between the lower and the up-
per interval boundary is 4.36% for majority, 1.92%
for Mbt and 1.59% for SVMTool. This means that,
33
Majority Mbt SVMTool
average accuracy 84.94 91.84 93.48
standard deviation 2.50 1.30 1.07
Table 1: Average accuracy and standard deviation on 72 cross-validation experiments.
when taking the middle of the interval as the ex-
pected accuracy, the maximum error is 0.8% for
SVMTool. Since the difference between the best
and worst accuracy score is 4.93%, using linear re-
gression means that one can predict the accuracy
three times better. For Mbt with a range of 5.84%
between best and worst accuracy and for majority
with 12.7%, a similar figure is obtained.
Table 1 shows the average accuracies of the al-
gorithms for all 72 experiments. For this article,
the absolute accuracy of the algorithms is not un-
der consideration. Therefore, no effort has been
made to improve on these accuracy scores. One
can see that the standard deviation for SVMTool
and Mbt is lower than for majority, suggesting that
these algorithms are less susceptible to domain
variation.
The good linear fit for the graphs of Figure 2
cannot be reproduced with every algorithm. For
algorithms that do not have a sufficiently strong re-
lation between training corpus and assigned class
label, the linear relation is lost. Clearly, it remains
feasible to compute an interval for the data points,
but as a consequence of the non-linearity, the pre-
dicted intervals would be similar or even bigger
than the difference between the lowest and highest
accuracy score.
In Figure 3 the experiments of Figure 2 are
reproduced using test and training sets from the
same domain. Since we used the same data sets
as for the out-of-domain experiments, we had to
carry out 20 fold cross-validation for these exper-
iments. Because of this different setup the results
are shown in a different figure. There is a data
point for every domain.
Although the average distance between test and
training set are smaller for in-domain experiments,
we still observe a linear relation for Mbt and SVM,
for majority there is still a visual hint of linearity.
For in-domain the biggest difference between test
and train set is for the leisure domain (Re?nyi score:
-6.0) which is very close to the smallest out-of-
domain difference (-6.3 for social sciences?world
affairs). This could mean that the random varia-
tion between test and train can approach the varia-
6.5 6.0 5.5 5.0 4.5 4.0R?nyi divergence score with alpha=0.99
85
86
87
88
89
90
91
acc
urac
y (%
)
(9 data points)
Majority accuracy prediction
95% prediction interval
(a) Majority POS tagger.
6.5 6.0 5.5 5.0 4.5 4.0R?nyi divergence score with alpha=0.99
92.5
93.0
93.5
94.0
94.5
95.0
95.5
acc
urac
y (%
)
(9 data points)
Mbt accuracy prediction
95% prediction interval
(b) Memory based POS tagger.
6.5 6.0 5.5 5.0 4.5 4.0R?nyi divergence score with alpha=0.99
93.5
94.0
94.5
95.0
95.5
96.0
96.5
acc
urac
y (%
)
(9 data points)
SVMTool accuracy prediction
95% prediction interval
(c) SVMTool POS tagger.
Figure 3: The varying accuracy of three POS tag-
gers with varying distance between train and test
corpus of the same domain.
34
tion between domains but this observation is made
in abstraction from the different data set sizes for
in and out of domain experiments. For majority
the average accuracy over all domains is 88.25%
(stdev: 0.87), for Mbt 94.07% (0.63), and for
SVMTool 95.06% (0.59). Which are, as expected,
higher scores than the figures in Table 1.
4 Related Work
In articles dealing with the influence of domain
shifts on the performance of an NLP tool, the
in-domain data and out-of-domain data are taken
from different corpora, e.g., sentences from movie
snippets, newspaper texts and personal weblogs
(Andreevskaia and Bergler, 2008). It can be ex-
pected that these corpora are indeed dissimilar
enough to consider them as separate domains, but
no objective measure has been used to define them
as such. The fact that the NLP tool produces
lower results for cross-domain experiments can be
taken as an indication of the presence of sepa-
rate domains. A nice overview paper on statisti-
cal domain adaptation can be found in Bellegarda
(2004).
A way to express the degree of relatedness,
apart from this well-known accuracy drop, can be
found in Daume? and Marcu (2006). They propose
a domain adaptation framework containing a pa-
rameter pi. Low values of pi mean that in-domain
and out-of-domain data differ significantly. They
also used Kullback-Leibler divergence to compute
the similarity between unigram language models.
Blitzer et al (2007) propose a supervised way
of measuring the similarity between the two do-
mains. They compute the Huber loss, as a proxy
of the A-distance (Kifer et al, 2004), for every
instance that they labeled with their tool. The re-
sulting measure correlates with the adaptation loss
they observe when applying a sentiment classifi-
cation tool on different domains.
5 Conclusions and future work
This paper showed that it is possible to narrow
down the prediction of the accuracy of an NLP
tool on an unannotated corpus by measuring the
similarity between this unannotated corpus and the
corpus the tagger was trained on in an unsuper-
vised way. A prerequisite to be able to make a reli-
able prediction, is to have sufficient annotated data
to measure the correlation between the accuracy
and a metric. We observed that, in order to make a
prediction interval that is narrower than the differ-
ence between the lowest and highest accuracy on
the annotated corpora, the algorithm used, should
capture sufficient information from training.
The observation that it is feasible to make re-
liable predictions using unannotated data, can be
of help when training a system for a task in a do-
main for which no annotated data is available. As
a first step, the metric resulting in the best linear
fit between the metric and the accuracy should be
searched. If a linear relation can be established,
one can take annotated training data from the do-
main that is closest to the unannotated corpus and
assume that this will give the best accuracy score.
In this article we implemented a way to mea-
sure the similarity between two corpora. One may
decide to use such a metric to categorize the avail-
able corpora for a given task into groups, depend-
ing on their similarity. It should be noted that in
order to do this, a symmetric metric should be
used. Indeed, an asymmetric metric like the Re?nyi
divergence will give a different value depending
on whether the similarity between corpus P and
corpus Q is measured as Re?nyi(P ;Q;?) or as
Re?nyi(Q;P ;?).
Further research should explore the usability of
linear regression for other NLP tasks. Although
no specific adaptation to the POS tagging task was
made, it may not be straightforward to find a lin-
ear relation for more complicated tasks. For such
tasks, it may be useful to insert n-grams into the
metric. Or, if a parser was first applied to the data,
it is possible to insert syntactic features in the met-
ric. Of course, these adaptations may influence
the efficiency of the metric, but if a good linear
relation between the metric and the accuracy can
be found, the metric is useful. Another option to
make the use of the metric less task dependent is
by not using the distribution of the tokens but by
using distributions of the features used by the ma-
chine learner. Applying this more generic setup of
our experiments to other NLP tools may lead to the
discovery of a metric that is generally applicable.
Acknowledgments
This research was made possible through finan-
cial support from the University of Antwerp
(BIOGRAPH GOA-project).
35
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Over-
coming Domain Dependence in Sentiment Tagging.
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), 290?298. As-
sociation for Computational Linguistics. Columbus,
Ohio, USA.
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42:93?108.
Anil Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bulletin of the Calcutta
Mathematical Society, 35:99?109.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes and
Blenders: Domain Adaptation for Sentiment Clas-
sification. Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
440?447. Association for Computational Linguis-
tics. Prague, Czech Republic.
British National Corpus Consortium. 2001. The
British National Corpus, version 2 (BNC World).
Distributed by Oxford University Computing
Services on behalf of the BNC Consortium.
http://www.natcorp.ox.ac.uk (Last accessed: April
2, 2010).
Dorin Comaniciu, Visvanathan Ramesh, and Peter
Meer. 2003. Kernel-Based Object Tracking. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 25(5):564?575.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press, Cambridge, UK.
Hal Daume? III and Daniel Marcu. 2006. Domain
Adaptation for Statistical Classifiers. Journal of Ar-
tificial Intelligence Research, 26:101?126.
T. Mark Ellison and Simon Kirby. 2006. Measuring
Language Divergence by Intra-Lexical Comparison.
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, 273?280. Association for Computa-
tional Linguistics. Sidney, Australia.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC?04), 43?46. European Language Re-
sources Association. Lisbon, Portugal.
Daniel Kifer, Shai Ben-David, and Johannes Gehrke.
2004. Detecting change in data streams. Proceed-
ings of the 30th Very Large Data Bases Conference
(VLDB?04), 180?191. VLDB Endowment. Toronto,
Canada.
Solomon Kullback and Richard. A. Leibler. 1951. On
Information and Sufficiency. The Annals of Mathe-
matical Statistics, 22(1):79?86.
Lillian Lee. 2001. On the Effectiveness
of the Skew Divergence for Statistical Lan-
guage Analysis. 8th International Workshop
on Artificial Intelligence and Statistics (AISTATS
2001), 65?72. Florida, USA. Online reposi-
tory http://www.gatsby.ucl.ac.uk/aistats/aistats2001
(Last accessed: April 2, 2010).
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. CLAWS4: The tagging of the British Na-
tional Corpus. Proceedings of the 15th International
Conference on Computational Linguistics (COLING
94), 622?628. Kyoto, Japan.
Michael Pacey, Steven Fligelstone, and Paul Rayson.
1997. How to generalize the task of annotation.
Corpus Annotation: Linguistic Information from
Computer Text Corpora, 122?136. London: Long-
man.
Alfre?d Re?nyi. 1961. On measures of information
and entropy. Proceedings of the 4th Berkeley Sym-
posium on Mathematics, Statistics and Probability,
1:547?561. University of California Press. Berke-
ley, California, USA.
36
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 40?47,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Memory-Based Resolution of In-Sentence Scopes of Hedge Cues
Roser Morante, Vincent Van Asch, Walter Daelemans
CLiPS - University of Antwerp
Prinsstraat 13
B-2000 Antwerpen, Belgium
{Roser.Morante,Walter.Daelemans,Vincent.VanAsch}@ua.ac.be
Abstract
In this paper we describe the machine
learning systems that we submitted to the
CoNLL-2010 Shared Task on Learning to
Detect Hedges and Their Scope in Nat-
ural Language Text. Task 1 on detect-
ing uncertain information was performed
by an SVM-based system to process the
Wikipedia data and by a memory-based
system to process the biological data.
Task 2 on resolving in-sentence scopes of
hedge cues, was performed by a memory-
based system that relies on information
from syntactic dependencies. This system
scored the highest F1 (57.32) of Task 2.
1 Introduction
In this paper we describe the machine learning
systems that CLiPS1 submitted to the closed track
of the CoNLL-2010 Shared Task on Learning to
Detect Hedges and Their Scope in Natural Lan-
guage Text (Farkas et al, 2010).2 The task con-
sists of two subtasks: detecting whether a sentence
contains uncertain information (Task 1), and re-
solving in-sentence scopes of hedge cues (Task 2).
To solve Task 1, systems are required to classify
sentences into two classes, ?Certain? or ?Uncer-
tain?, depending on whether the sentence contains
factual or uncertain information. Three annotated
training sets are provided: Wikipedia paragraphs
(WIKI), biological abstracts (BIO-ABS) and bio-
logical full articles (BIO-ART). The two test sets
consist of WIKI and BIO-ART data.
Task 2 requires identifying hedge cues and find-
ing their scope in biomedical texts. Finding the
scope of a hedge cue means determining at sen-
tence level which words in the sentence are af-
fected by the hedge cue. For a sentence like the
1Web page: http://www.clips.ua.ac.be
2Web page: http://www.inf.u-szeged.hu/rgai
/conll2010st
one in (1) extracted from the BIO-ART training
corpus, systems have to identify likely and sug-
gested as hedge cues, and they have to find that
likely scopes over the full sentence, and that sug-
gested scopes over by the role of murine MIB in
TNF? signaling. A scope will be correctly re-
solved only if both the cue and the scope are cor-
rectly identified.
(1) <xcope id=2> The conservation from Drosophila to
mammals of these two structurally distinct but
functionally similar E3 ubiquitin ligases is <cue
ref=2>likely</cue> to reflect a combination of
evolutionary advantages associated with: (i)
specialized expression pattern, as evidenced by the
cell-specific expression of the neur gene in sensory
organ precursor cells [52]; (ii) specialized function, as
<xcope id=1> <cue ref=1>suggested</cue> by the
role of murine MIB in TNF? signaling</xcope> [32];
(iii) regulation of protein stability, localization, and/or
activity</xcope>.
Systems are to be trained on BIO-ABS and
BIO-ART and tested on BIO-ART. Example (1)
shows that sentences in the BIO-ART dataset can
be quite complex because of their length, because
of their structure - very often they contain enu-
merations, and because they contain bibliographic
references and references to tables and figures.
Handling these phenomena is necessary to detect
scopes correctly in the setting of this task. Note
that the scope of suggested above does not include
the bibliographic reference [32], whereas the scope
of likely includes all the bibliographic references,
and that the scope of likely does not include the
final punctuation mark.
In the case of the BIO data, we approach Task
1 as a prerequisite for Task 2. Therefore we treat
them as two consecutive classification tasks: a first
one that consists of classifying the tokens of a sen-
tence as being at the beginning of a hedge sig-
nal, inside or outside. This allows the system to
find multiword hedge cues. We tag a sentence as
uncertain if at least a hedge cue is found in the
sentence. The second classification task consists
40
of classifying the tokens of a sentence as being
the first element of the scope, the last, or nei-
ther. This happens as many times as there are
hedge cues in the sentence. The two classification
tasks are implemented using memory-based learn-
ers. Memory-based language processing (Daele-
mans and van den Bosch, 2005) is based on the
idea that NLP problems can be solved by reuse of
solved examples of the problem stored in memory.
Given a new problem, the most similar examples
are retrieved, and a solution is extrapolated from
them.
Section 2 is devoted to related work. In Sec-
tion 3 we describe how the data have been prepro-
cessed. In Section 4 and Section 5 we present the
systems that perform Task 1 and Task 2. Finally,
Section 6 puts forward some conclusions.
2 Related work
Hedging has been broadly treated from a theoret-
ical perspective. The term hedging is originally
due to Lakoff (1972). Palmer (1986) defines a
term related to hedging, epistemic modality, which
expresses the speaker?s degree of commitment to
the truth of a proposition. Hyland (1998) focuses
specifically on scientific texts. He proposes a prag-
matic classification of hedge expressions based on
an exhaustive analysis of a corpus. The catalogue
of hedging cues includes modal auxiliaries, epis-
temic lexical verbs, epistemic adjectives, adverbs,
nouns, and a variety of non?lexical cues. Light
et al (2004) analyse the use of speculative lan-
guage in MEDLINE abstracts. Some NLP appli-
cations incorporate modality information (Fried-
man et al, 1994; Di Marco and Mercer, 2005).
As for annotated corpora, Thompson et al (2008)
report on a list of words and phrases that express
modality in biomedical texts and put forward a cat-
egorisation scheme. Additionally, the BioScope
corpus (Vincze et al, 2008) consists of a collec-
tion of clinical free-texts, biological full papers,
and biological abstracts annotated with negation
and speculation cues and their scope.
Although only a few pieces of research have fo-
cused on processing negation, the two tasks of the
CoNLL-2010 Shared Task have been addressed
previously. As for Task 1, Medlock and Briscoe
(2007) provide a definition of what they consider
to be hedge instances and define hedge classifi-
cation as a weakly supervised machine learning
task. The method they use to derive a learning
model from a seed corpus is based on iteratively
predicting labels for unlabeled training samples.
They report experiments with SVMs on a dataset
that they make publicly available3. The experi-
ments achieve a recall/precision break even point
(BEP) of 0.76. They apply a bag-of-words ap-
proach to sample representation. Medlock (2008)
presents an extension of this work by experiment-
ing with more features (part-of-speech, lemmas,
and bigrams). With a lemma representation the
system achieves a peak performance of 0.80 BEP,
and with bigrams of 0.82 BEP. Szarvas (2008) fol-
lows Medlock and Briscoe (2007) in classifying
sentences as being speculative or non-speculative.
Szarvas develops a MaxEnt system that incor-
porates bigrams and trigrams in the feature rep-
resentation and performs a complex feature se-
lection procedure in order to reduce the number
of keyword candidates. It achieves up to 0.85
BEP and 85.08 F1 by using an external dictio-
nary. Kilicoglu and Bergler (2008) apply a lin-
guistically motivated approach to the same clas-
sification task by using knowledge from existing
lexical resources and incorporating syntactic pat-
terns. Additionally, hedge cues are weighted by
automatically assigning an information gain mea-
sure and by assigning weights semi?automatically
depending on their types and centrality to hedging.
The system achieves results of 0.85 BEP.
As for Task 2, previous work (Morante and
Daelemans, 2009; O?zgu?r and Radev, 2009) has
focused on finding the scope of hedge cues in
the BioScope corpus (Vincze et al, 2008). Both
systems approach the task in two steps, identify-
ing the hedge cues and finding their scope. The
main difference between the two systems is that
Morante and Daelemans (2009) perform the sec-
ond phase with a machine learner, whereas O?zgur
and Radev (2009) perform the second phase with
a rule-based system that exploits syntactic infor-
mation.
The approach to resolving the scopes of hedge
cues that we present in this paper is similar to
the approach followed in Morante and Daelemans
(2009) in that the task is modelled in the same
way. A difference between the two systems is that
this system uses only one classifier to solve Task
2, whereas the system described in Morante and
Daelemans (2009) used three classifiers and a met-
3Available at
http://www.benmedlock.co.uk/hedgeclassif.html.
41
alearner. Another difference is that the system in
Morante and Daelemans (2009) used shallow syn-
tactic features, whereas this system uses features
from both shallow and dependency syntax. A third
difference is that that system did not use a lexicon
of cues, whereas this system uses a lexicon gener-
ated from the training data.
3 Preprocessing
As a first step, we preprocess the data in order
to extract features for the machine learners. We
convert the xml files into a token-per-token rep-
resentation, following the standard CoNLL for-
mat (Buchholz and Marsi, 2006), where sentences
are separated by a blank line and fields are sepa-
rated by a single tab character. A sentence consists
of a sequence of tokens, each one starting on a new
line.
The WIKI data are processed with the Memory
Based Shallow Parser (MBSP) (Daelemans and
van den Bosch, 2005) in order to obtain lemmas,
part-of-speech (PoS) tags, and syntactic chunks,
and with the MaltParser (Nivre, 2006) in order to
obtain dependency trees. The BIO data are pro-
cessed with the GDep parser (Sagae and Tsujii,
2007) in order to get the same information.
# WORD LEMMA PoS CHUNK NE D LABEL C S
1 The The DT B-NP O 3 NMOD O O O
2 structural structural JJ I-NP O 3 NMOD O O O
3 evidence evidence NN I-NP O 4 SUB O O O
4 lends lend VBZ B-VP O 0 ROOT B F O
5 strong strong JJ B-NP O 6 NMOD I O O
6 support support NN I-NP O 4 OBJ I O O
7 to to TO B-PP O 6 NMOD O O O
8 the the DT B-NP O 11 NMOD O O O
9 inferred inferred JJ I-NP O 11 NMOD B O F
10 domain domain NN I-NP O 11 NMOD O O O
11 pair pair NN I-NP O 7 PMOD O L L
12 , , , O O 4 P O O O
13 resulting result VBG B-VP O 4 VMOD O O O
14 in in IN B-PP O 13 VMOD O O O
15 a a DT B-NP O 18 NMOD O O O
16 high high JJ I-NP O 18 NMOD O O O
17 confidence confidence NN I-NP O 18 NMOD O O O
18 set set NN I-NP O 14 PMOD O O O
19 of of IN B-PP O 18 NMOD O O O
20 domain domain NN B-NP O 21 NMOD O O O
21 pairs pair NNS I-NP O 19 PMOD O O O
22 . . . O O 4 P O O O
Table 1: Preprocessed sentence.
Table 1 shows a preprocessed sentence with the
following information per token: the token num-
ber in the sentence, word, lemma, PoS tag, chunk
tag, named entity tag, head of token in the depen-
dency tree, dependency label, cue tag, and scope
tags separated by a space, for as many cues as
there are in the sentence.
In order to check whether the conversion from
the xml format to the CoNLL format is a source
of error propagation, we convert the gold CoNLL
files into xml format and we run the scorer pro-
vided by the task organisers. The results obtained
are listed in Table 2.
Task 1 Task 2
WIKI BIO-ART BIO-ABS BIO-ART BIO-ABS
F1 100.00 100.00 100.00 99.10 99.66
Table 2: Evaluation of the conversion from xml to
CoNLL format.
4 Task 1: Detecting uncertain
information
In Task 1 sentences have to be classified as con-
taining uncertain or unreliable information or not.
The task is performed differently for theWIKI and
for the BIO data, since we are interested in finding
the hedge cues in the BIO data, as a first step to-
wards Task 2.
4.1 Wikipedia system (WIKI)
In the WIKI data a sentence is marked as uncertain
if it contains at least one weasel, or cue for uncer-
tainty. The list of weasels is quite extensive and
contains a high number of unique occurrences. For
example, the training data contain 3133 weasels
and 1984 weasel types, of which 63% are unique.
This means that a machine learner will have diffi-
culties in performing the classification task. Even
so, some generic structures can be discovered
in the list of weasels. For example, the differ-
ent weasels A few people and A few sprawling
grounds follow a pattern. We manually select the
42 most frequent informative tokens4 from the list
of weasels in the training partition. In the remain-
der of this section we will refer to these tokens as
weasel cues.
Because of the wide range of weasels, we opt
for predicting the (un)certainty of a sentence, in-
stead of identifying the weasels. The sentence
classification is done in three steps: instance cre-
ation, SVM classification and sentence labeling.
4Weasel cues: few, number, variety, bit, great, majority,
range, variety, all, almost, arguably, certain, commonly, gen-
erally, largely, little, many, may, most, much, numerous, of-
ten, one, other, others, perhaps, plenty of, popular, possibly,
probably, quite, relatively, reportedly, several, some, suggest,
there be, the well-known, various, very, wide, widely.
42
4.1.1 Instance creation
Although we only want to predict the (un)certainty
of a sentence as a whole, we classify every token
in the sentence separately. After parsing the data
we create one instance per token, with the excep-
tion of tokens that have a part-of-speech from the
list: #, $, :, LS, RP, UH, WP$, or WRB. The ex-
clusion of these tokens is meant to simplify the
classification task.
The features used by the system during classifi-
cation are the following:
? About the token: word, lemma, PoS tag, chunk tag,
dependency head, and dependency label.
? About the token context: lemma, PoS tag, chunk tag
and dependency label of the two tokens to the left and
right of the token in focus in the string of words of the
sentence.
? About the weasel cues: a binary marker that indicates
whether the token in focus is a weasel cue or not, and a
number defining the number of weasel cues that there
are in the entire sentence.
These instances with 24 non-binary features
carry the positive class label if the sentence is un-
certain. We use a binarization script that rewrites
the instance to a format that can be used with a
support vector machine and during this process,
feature values that occur less than 2 times are
omitted.
4.1.2 SVM classification
To label the instances of the unseen data we use
SVMlight (Joachims, 2002). We performed some
experiments with different settings and decided
to only change the type of kernel from the de-
fault linear kernel to a polynomial kernel. For
the Wikipedia training data, the training of the
246,876 instances with 68417 features took ap-
proximately 22.5 hours on a 32 bit, 2.2GHz, 2GB
RAM Mac OS X machine.
4.1.3 Sentence labeling
In this last step, we collect all instances from the
same sentence and inspect the predicted labels for
every token. If more than 5% of the instances are
marked as uncertain, the whole sentence is marked
as uncertain. The idea behind the setup is that
many tokens are very ambiguous in respect to un-
certainty because they do not carry any informa-
tion. Fewer tokens are still ambiguous, but contain
some information, and a small set of tokens are al-
most unambiguous. This small set of informative
tokens does not have to coincide with weasels nor
weasels cues. The result is that we cannot predict
the actual weasels in a sentence, but we get an in-
dication of the presence of tokens that are common
in uncertain sentences.
4.2 Biological system (BIO)
The system that processes the BIO data is different
from the system that processes theWIKI data. The
BIO system uses a classifier that predicts whether
a token is at the beginning of a hedge signal, inside
or outside. So, instances represent tokens. The in-
stance features encode the following information:
? About the token: word, lemma, PoS tag, chunk tag, and
dependency label.
? About the context to the left and right in the string of
words of the sentence: word of the two previous and
three next tokens, lemma and dependency label of pre-
vious and next tokens, deplabel, and chunk tag and PoS
of next token. A binary feature indicating whether the
next token has an SBAR chunk tag.
? About the context in the syntactic dependency tree:
chain of PoS tags, chunk tags and dependency label
of children of token; word, lemma, PoS tag, chunk tag,
and dependency label of father; combined tag with the
lemma of the token and the lemma of its father; chain
of dependency labels from token to ROOT. Lemma of
next token, if next token is syntactic child of token. If
token is a verb, lemma of the head of the token that is
its subject.
? Dictionary features. We extract a list of hedge cues
from the training corpus. Based on this list, two binary
features indicate whether token and next token are po-
tential cues.
? Lemmas of the first noun, first verb and first adjective
in the sentence.
The classifier is the decision tree IGTree as im-
plemented in TiMBL (version 6.2) 5(Daelemans
et al, 2009), a fast heuristic approximation of k-
nn, that makes a heuristic approximation of near-
est neighbor search by a top down traversal of the
tree. It was parameterised by using overlap as the
similarity metric and information gain for feature
weighting. Running the system on the test data
takes 10.44 seconds in a 64 bit 2.8GHz 8GB RAM
Intel Xeon machine with 4 cores.
4.3 Results
All the results published in the paper are calcu-
lated with the official scorer provided by the task
organisers. We provide precision (P), recall (R)
and F1. The official results of Task 1 are pre-
sented in Table 3. We produce in-domain and
5TiMBL: http://ilk.uvt.nl/timbl
43
cross-domain results. The BIO in-domain re-
sults have been produced with the BIO system,
by training on the training data BIO-ABS+BIO-
ART, and testing on the test data BIO-ART. The
WIKI in-domain results have been produced by
the WIKI system by training on WIKI and test-
ing on WIKI. The BIO cross-domain results have
been produced with the BIO system, by train-
ing on BIO-ABS+BIO-ART+WIKI and testing on
BIO-ART. The WIKI cross-domain results have
been produced with the WIKI system by train-
ing on BIO-ABS+BIO-ART+WIKI and testing on
WIKI. Training the SVM with BIO-ABS+BIO-
ART+WIKI augmented the training time exponen-
tially and the system did not finish on time for sub-
mission. We report post-evaluation results.
In-domain Cross-domain
P R F1 P R F1
WIKI 80.55 44.49 57.32 80.64* 44.94* 57.71*
BIO 81.15 82.28 81.71 80.54 83.29 81.89
Table 3: Uncertainty detection results (Task 1 -
closed track). Post-evaluation results are marked
with *.
In-domain results confirm that uncertain sen-
tences inWikipedia text are more difficult to detect
than uncertain sentences in biological text. This
is caused by a loss in recall of the WIKI system.
Compared to results obtained by other systems
participating in the CoNLL-2010 Shared Task, the
BIO system performs 4.47 F1 lower than the best
system, and the WIKI system performs 2.85 F1
lower. This indicates that there is room for im-
provement. As for cross-domain results, we can-
not conclude that the cross-domain data harm the
performance of the system, but we cannot state
either that the cross-domain data improve the re-
sults. Since we performed Task 1 as a step towards
Task 2, it is interesting to know what is the per-
formance of the system in identifying hedge cues.
Results are shown in Table 4. One of the main
sources of errors in detecting the cues are due to
the cue or. Of the 52 occurrences in the test corpus
BIO-ART, the system produces 3 true positives, 8
false positives and 49 false negatives.
In-domain Cross-domain
P R F1 P R F1
Bio 78.75 74.69 76.67 78.14 75.45 76.77
Table 4: Cue matching results (Task 1 - closed
track).
5 Task 2: Resolution of in-sentence
scopes of hedge cues
Task 2 consists of resolving in-sentence scopes of
hedge cues in biological texts. The system per-
forms this task in two steps, classification and
postprocessing, taking as input the output of the
system that finds cues.
5.1 Classification
In the classification step a memory-based classi-
fier classifies tokens as being the first token in the
scope sequence, the last, or neither, for as many
cues as there are in the sentence. An instance rep-
resents a pair of a predicted hedge cue and a token.
All tokens in a sentence are paired with all hedge
cues that occur in the sentence.
The classifier used is an IB1 memory?based al-
gorithm as implemented in TiMBL (version 6.2)6
(Daelemans et al, 2009), a memory-based classi-
fier based on the k-nearest neighbor rule (Cover
and Hart, 1967). The IB1 algorithm is parame-
terised by using overlap as the similarity metric,
gain ratio for feature weighting, using 7 k-nearest
neighbors, and weighting the class vote of neigh-
bors as a function of their inverse linear distance.
Running the system on the test data takes 53 min-
utes in a 64 bit 2.8GHz 8GB RAM Intel Xeon ma-
chine with 4 cores.
The features extracted to perform the classifi-
cation task are listed below. Because, as noted
by O?zgu?r and Radev (2009) and stated in the an-
notation guidelines of the BioScope corpus7, the
scope of a cue can be determined from its lemma,
PoS tag, and from the syntactic construction of the
clause (passive voice vs. active, coordination, sub-
ordination), we use, among others, features that
encode information from the dependency tree.
? About the cue: chain of words, PoS label, dependency
label, chunk label, chunk type; word, PoS tag, chunk
tag, and chunk type of the three previous and next to-
kens in the string of words in the sentence; first and
last word, chain of PoS tags, and chain of words of the
chunk where cue is embedded, and the same features
for the two previous and two next chunks; binary fea-
ture indicating whether cue is the first, last or other to-
ken in sentence; binary feature indicating whether cue
is in a clause with a copulative construction; PoS tag
and dependency label of the head of cue in the depen-
dency tree; binary feature indicating whether cue is lo-
cated before or after its syntactic head in the string of
6TiMBL: http://ilk.uvt.nl/timbl.
7Available at: http://www.inf.u-szeged.hu/
rgai/project/nlp/bioscope/Annotation%20
guidelines2.1.pdf.
44
words of the sentence; feature indicating whether cue
is followed by an S-BAR or a coordinate construction.
? About the token: word, PoS tag, dependency label,
chunk tag, chunk type; word, PoS tag, chunk tag, and
chunk type of the three previous and three next tokens
in the string of words of the sentence; chain of PoS
tag and lemmas of two and three tokens to the right of
token in the string of words of the sentence; first and
last word, chain of PoS tags, and chain of words of the
chunk where token is embedded, and the same features
for the two previous and two next chunks; PoS tag and
deplabel of head of token in the dependency tree; bi-
nary feature indicating whether token is part of a cue.
? About the token in relation to cue: binary features indi-
cating whether token is located before or after cue and
before or after the syntactic head of cue in the string
of words of the sentence; chain of PoS tags between
cue and token in the string of words of the sentence;
normalised distance between cue and token (number of
tokens in between divided by total number of tokens);
chain of chunks between cue and token; feature indi-
cating whether token is located before cue, after cue or
wihtin cue.
? About the dependency tree: feature indicating who is
ancestor (cue, token, other); chain of dependency la-
bels and chain of PoS tags from cue to common an-
cestor, and from token to common ancestor, if there is
a common ancestor; chain of dependency labels and
chain of PoS from token to cue, if cue is ancestor of to-
ken; chain of dependency labels and chain of PoS from
cue to token, if token is ancestor of cue; chain of de-
pendency labels and PoS from cue to ROOT and from
token to ROOT.
Features indicating whether token is a candidate to be
the first token of scope (FEAT-FIRST), and whether
token is a candidate to be the last token of the scope
(FEAT-LAST). These features are calculated by a
heuristics that takes into account detailed information
of the dependency tree. The value of FEAT-FIRST de-
pends on whether the clause is in active or in passive
voice, on the PoS of the cue, and on the lemma in some
cases (for example, verbs appear, seem). The value of
FEAT-LAST depends on the PoS of the cue.
5.2 Postprocessing
In the corpora provided for this task, scopes are
annotated as continuous sequences of tokens that
include the cue. However, the classifiers only pre-
dict the first and last element of the scope. In or-
der to guarantee that all scopes are continuous se-
quences of tokens we apply a first postprocessing
step (P-SCOPE) that builds the sequence of scope
based on the following rules:
1. If one token has been predicted as FIRST and one as
LAST, the sequence is formed by the tokens between
FIRST and LAST.
2. If one token has been predicted as FIRST and none has
been predicted as LAST, the sequence is formed by the
tokens between FIRST and the first token that has value
1 for FEAT-LAST.
3. If one token has been predicted as FIRST and more
than one as LAST, the sequence is formed by the tokens
between FIRST and the first token predicted as LAST
that is located after cue.
4. If one token has been predicted as LAST and none as
FIRST, the sequence will start at the hedge cue and it
will finish at the token predicted as LAST.
5. If no token has been predicted as FIRST and more than
one as LAST, the sequence will start at the hedge cue
and will end at the first token predicted as LAST after
the hedge signal.
6. If one token has been predicted as LAST and more than
one as FIRST, the sequence will start at the cue.
7. If no tokens have been predicted as FIRST and no to-
kens have been predicted as LAST, the sequence will
start at the hedge cue and will end at the first token that
has value 1 for FEAT-LAST.
The system predicts 987 scopes in total. Of
these, 1 FIRST and 1 LAST are predicted in 762
cases; a different number of predictions is made
for FIRST and for LAST in 217 cases; no FIRST
and no LAST are predicted in 5 cases, and 2
FIRST and 2 LAST are predicted in 3 cases. In 52
cases no FIRST is predicted, in 93 cases no LAST
is predicted.
Additionally, as exemplified in Example 1 in
Section 1, bibliographic references and references
to tables and figures do not always fall under the
scope of cues, when the references appear at the
end of the scope sequence. If references that ap-
pear at the end of the sentence have been predicted
by the classifier within the scope of the cue, these
references are set out of the scope in a second post-
processing step (P-REF).
5.3 Results
The official results of Task 2 are presented in Ta-
ble 5. The system scores 57.32 F1, which is the
highest score of the systems that participated in
this task.
In-domain
P R F1
BIO 59.62 55.18 57.32
Table 5: Scope resolution official results (Task 2 -
closed track).
In order to know what is the effect of the post-
processing steps, we evaluate the output of the
system before performing step P-REF and before
performing step P-SCOPE. Table 6 shows the re-
sults of the evaluation. Without P-REF, the perfor-
mance decreases in 7.30 F1. This is caused by the
45
fact that a considerable proportion of scopes end
in a reference to bibliography, tables, or figures.
Without P-SCOPE it decreases 4.50 F1 more. This
is caused, mostly, by the cases in which the classi-
fier does not predict the LAST class.
In-domain
P R F1
BIO before P-REF 51.98 48.20 50.02
BIO before P-SCOPE 48.82 44.43 46.52
Table 6: Scope resolution results before postpro-
cessing steps.
It is not really possible to compare the scores
obtained in this task to existing research previous
to the CoNLL-2010 Shared Task, namely the re-
sults obtained by O?zgu?r and Radev (2009) on the
BioScope corpus with a rule-based system and by
Morante and Daelemans (2009) on the same cor-
pus with a combination of classifiers. O?zgu?r and
Radev (2009) report accuracy scores (61.13 on full
text), but no F measures are reported. Morante and
Daelemans (2009) report percentage of correct
scopes for the full text data set (42.37), obtained
by training on the abstracts data set, whereas the
results presented in Table 5 are reported in F mea-
sures and obtained in by training and testing on
other corpora. Additionally, the system has been
trained on a corpus that contains abstracts and full
text articles, instead of only abstracts. However,
it is possible to confirm that, even with informa-
tion on dependency syntax, resolving the scopes of
hedge cues in biological texts is not a trivial task.
The scores obtained in this task are much lower
than the scores obtained in other tasks that involve
semantic processing, like semantic role labeling.
The errors of the system in Task 2 are caused
by different factors. First, there is error propaga-
tion from the system that finds cues. Second, the
system heavily relies on information from the syn-
tactic dependency tree. The parser used to prepro-
cess the data (GDep) has been trained on abstracts,
instead of full articles, which means that the per-
formance on full articles will be lower, since sen-
tence are longer and more complex. Third, en-
coding the information of the dependency tree
in features for the learner is not a straightfor-
ward process. In particular, some errors in resolv-
ing the scope are caused by keeping subordinate
clauses within the scope, as in sentence (2), where,
apart from not identifying speculated as a cue, the
system wrongly includes resulting in fewer high-
confidence sequence assignments within the scope
of may. This error is caused in the instance con-
struction phase, because token assignments gets
value 1 for feature FEAT-LAST and token algo-
rithm gets value 0, whereas it should have been
otherwise.
(2) We speculated that the presence of multiple isotope
peaks per fragment ion in the high resolution Orbitrap
MS/MS scans <xcope id=1><cue ref=1>may
</cue> degrade the sensitivity of the search
algorithm, resulting in fewer high-confidence
sequence assignments</xcope>.
Additionally, the test corpus contains an article
about the annotation of a corpus of hedge cues,
thus, an article that contains metalanguage. Our
system can not deal with sentences like the one in
(3), in which all cues with their scopes are false
positives.
(3) For example, the word <xcope id=1><cue ref=1>
may</cue> in sentence 1</xcope>) <xcope id=2>
<cue ref=2>indicates that</cue> there is some
uncertainty about the truth of the event, whilst the
phrase Our results show that in 2) <xcope id=3>
<cue ref=3>indicates that</cue> there is
experimental evidence to back up the event described
by encodes</xcope></xcope>.
6 Conclusions and future research
In this paper we presented the machine learning
systems that we submitted to the CoNLL-2010
Shared Task on Learning to Detect Hedges and
Their Scope in Natural Language Text. The BIO
data were processed by memory-based systems in
Task 1 and Task 2. The system that performs Task
2 relies on information from syntactic dependen-
cies. This system scored the highest F1 (57.32) of
Task 2.
As for Task 1, in-domain results confirm that
uncertain sentences inWikipedia text are more dif-
ficult to detect than uncertain sentences in biolog-
ical text. One of the reasons is that the number of
weasels is much higher and diverse than the num-
ber of hedge cues. BIO cross-domain results show
that adding WIKI data to the training set causes a
slight decrease in precision and a slight increase
in recall. The errors of the BIO system show that
some cues, like or are difficult to identify, because
they are ambiguous. As for Task 2, results indi-
cate that resolving the scopes of hedge cues in bi-
ological texts is not a trivial task. The scores ob-
tained in this task are much lower than the scores
obtained in other tasks that involve semantic pro-
cessing, like semantic role labeling. The results
46
are influenced by propagation of errors from iden-
tifying cues, errors in the dependency tree, the ex-
traction process of syntactic information from the
dependency tree to encode it in the features, and
the presence of metalanguage on hedge cues in the
test corpus. Future research will focus on improv-
ing the identification of hedge cues and on using
different machine learning techniques to resolve
the scope of cues.
Acknowledgements
The research reported in this paper was made pos-
sible through financial support from the University
of Antwerp (GOA project BIOGRAPH).
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the CoNLL-X Shared Task, New
York. SIGNLL.
Thomas M. Cover and Peter E. Hart. 1967. Nearest
neighbor pattern classification. Institute of Electri-
cal and Electronics Engineers Transactions on In-
formation Theory, 13:21?27.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-based language processing. Cambridge
University Press, Cambridge, UK.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, and
Antal Van den Bosch. 2009. TiMBL: Tilburg Mem-
ory Based Learner, version 6.2, Reference Guide.
Number 09-01 in Technical Report Series. Tilburg,
The Netherlands.
Chrysanne Di Marco and Robert E. Mercer, 2005.
Computing attitude and affect in text: Theory and
applications, chapter Hedging in scientific articles
as a means of classifying citations. Springer-Verlag,
Dordrecht.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Carol Friedman, Philip Alderson, John Austin, James J.
Cimino, and Stephen B. Johnson. 1994. A general
natural?language text processor for clinical radiol-
ogy. Journal of the American Medical Informatics
Association, 1(2):161?174.
Ken Hyland. 1998. Hedging in scientific research ar-
ticles. John Benjamins B.V, Amsterdam.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines, volume 668 of The
Springer International Series in Engineering and
Computer Science. Springer.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9(Suppl 11):S10.
George Lakoff. 1972. Hedges: a study in meaning
criteria and the logic of fuzzy concepts. Chicago
Linguistics Society Papers, 8:183?228.
Marc Light, Xin Y.Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: facts, speculations, and
statements in between. In Proceedings of the Bi-
oLINK 2004, pages 17?24.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of ACL 2007, pages 992?
999.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41:636?654.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of BioNLP 2009, pages 28?36, Boul-
der, Colorado.
Joakim Nivre. 2006. Inductive Dependency Parsing,
volume 34 of Text, Speech and Language Technol-
ogy. Springer.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detect-
ing speculations and their scopes in scientific text.
In Proceedings of EMNLP 2009, pages 1398?1407,
Singapore.
Frank R. Palmer. 1986. Mood and modality. CUP,
Cambridge, UK.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proceedings of CoNLL 2007:
Shared Task, pages 82?94, Prague, Czech Republic.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of ACL 2008, pages
281?289, Columbus, Ohio, USA. ACL.
Paul Thompson, Giulia Venturi, John McNaught,
Simonetta Montemagni, and Sophia Ananiadou.
2008. Categorising modality in biomedical texts. In
Proceedings of the LREC 2008 Workshop on Build-
ing and Evaluating Resources for Biomedical Text
Mining 2008, pages 27?34, Marrakech. LREC.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
47
