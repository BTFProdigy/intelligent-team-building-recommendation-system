Proceedings of NAACL HLT 2009: Short Papers, pages 189?192,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Lexical and Syntactic Priming and Their Impact in Deployed Spoken Dialog
Systems
Svetlana Stoyanchev and Amanda Stent
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400, USA
svetastenchikova@gmail.com, amanda.stent@stonybrook.edu
Abstract
In this paper, we examine user adaptation to
the system?s lexical and syntactic choices in
the context of the deployed Let?s Go! dialog
system. We show that in deployed dialog sys-
tems with real users, as in laboratory experi-
ments, users adapt to the system?s lexical and
syntactic choices. We also show that the sys-
tem?s lexical and syntactic choices, and con-
sequent user adaptation, can have an impact
on recognition of task-related concepts. This
means that system prompt formulation, even
in flexible input dialog systems, can be used
to guide users into producing utterances con-
ducive to task success.
1 Introduction
Numerous studies have shown that people adapt
their syntactic and lexical choices in conversation to
those of their conversational partners, both human
(Brennan, 1996; Pickering et al, 2000; Lockridge
and Brennan, 2002; Reitter et al, 2006) and com-
puter (Branigan et al, 2003; Brennan, 1991; Bren-
nan, 1996; Gustafson et al, 1997; Ward and Litman,
2007). User adaptation to the system?s lexical and
syntactic choices can be particularly useful in flexi-
ble input dialog systems. Limited input dialog sys-
tems, including most commercial systems, require
the user to respond to each system prompt using
only the concept and words currently requested by
the system. Flexible input dialog systems allow the
user to respond to system prompts with concepts
and words in addition to or other than the ones cur-
rently requested, and may even allow the user to
take task initiative. Speech recognition (ASR) accu-
racy in limited input systems is better than in flexi-
ble input systems (Danieli and Gerbino, 1995; Smith
and Gordon, 1997). However, task completion rates
and times are better in flexible input systems (Chu-
Carroll and Nickerson, 2000; Smith and Gordon,
1997). With user adaptation, in flexible input dia-
log systems prompts can be formulated to maximize
ASR accuracy and reduce the number of ASR time-
outs (Sheeder and Balogh, 2003).
Previous research on user adaptation to dialog
systems was conducted in laboratory settings. How-
ever, the behavior of recruited subjects in a quiet
laboratory may differ from that of real users in the
noisy world (Ai et al, 2007). Here we present the
first study, to the best of our knowledge, that in-
vestigates the adaptive behavior of real users of a
live dialog system. We analyze dialogs from CMU?s
Let?s Go! dialog system (Raux et al, 2005). We
look at the effects of the system?s lexical and syn-
tactic choices on: 1) lexical and syntactic choices
in user responses; and 2) concept identification rates
for user responses. We confirm prior results showing
that users adapt to the system?s lexical and syntactic
choices. We also show that particular choices for
system prompts can lead to higher concept identifi-
cation rates.
2 Experimental Method
We conducted our experiment using the Let?s Go!
telephone-based spoken dialog system that provides
information about bus routes in Pittsburgh (Raux
et al, 2005). The users are naive callers from the
general population seeking information about bus
189
condition request departure confirm departure request arrival confirm arrival
location location location location
(1) Where are you leav-
ing from?
Leaving from X, is this
correct?
Where are you going
to?
Going to X, is this
correct
(2) Where are you leav-
ing from?
From X, is this cor-
rect?
Where are you going
to?
To X, is this correct
(3) What is the place of
your departure
X, is this correct? What is the place of
your arrival?
X, is this correct
(4) Where do you want to
leave from?
You want to leave from
X, is this correct?
Where do you want to
go to?
You want to go to X,
is this correct
Table 1: Experimental conditions
Spkr Task type Utterance
Sys Open Welcome to the CMU Let?s
Go bus information system.
What can I do for you?
Usr 61A schedule
Sys Request
Departure
Where do you wanna leave
from?
Usr Location From downtown
Sys Confirm
Departure
Leaving from downtown. Is
this correct?
Usr Location Yes
Sys Request
Arrival
Where are you going to?
Usr Location Oakland
Sys Confirm
Arrival
Going to Waterfront. Is this
correct?
Usr Location No, to Oakland
Figure 1: Dialog extract from Let?s Go! data
schedules. In order to provide the user with route
information, Let?s Go! elicits a departure location,
a destination, a departure time, and optionally a bus
route number. Each concept value provided by the
user is explicitly confirmed by the system. Figure 1
shows an example dialog with the system.
Let?s Go! is a flexible input dialog system. The
user can respond to a system prompt using a single
word or short phrase, e.g. Downtown, or a complete
sentence, e.g. I am leaving from downtown1.
We ran four experimental conditions for two
months. The conditions varied in the lexical choice
and syntax of system prompts for two system re-
quest location tasks and two system confirm loca-
tion tasks (see Table 1). System prompts differed
1The user response can also contain concepts not requested
in the prompt, e.g. specifying departure location and bus num-
ber in one response.
by presence of a verb (to leave, to go) or a preposi-
tion (to, from), and by the syntactic form of the verb.
The request location prompt contained both a verb
and a preposition in the experimental conditions (1,
3, and 4). The confirm location prompt contained
both a verb and a preposition in conditions 1 and 4,
only a preposition in condition 2, and neither verb
nor preposition in condition 3. In conditions 1 and
4, both request and confirmation prompts differed in
the verb form (leaving/leave, going/go).
2184 dialogs were used for this analysis. For each
experimental condition, we counted the percentages
of verbs, verb forms, prepositions, and locations in
the ASR output for user responses to system request
location and confirm location prompts. Although
the data contains recognition errors, the only differ-
ence in system functionality between the conditions
is the formulation of the system prompt, so any sta-
tistically significant difference in user responses be-
tween different conditions can be attributed to the
formulation of the prompt.
3 Syntactic Adaptation
We analyze whether users are more likely to use ac-
tion verbs (leave, leaving, go, or going) and prepo-
sitions (to, from) in response to system prompts that
use a verb or a preposition. This analysis is interest-
ing because ASR partially relies on context words,
words related to a particular concept type such as
place, time or bus route. For example, the likelihood
of correctly recognizing the location Oakland in the
utterance ?going to Oakland? is different from the
likelihood of correctly recognizing the single word
utterance ?Oakland?.
Table 2 shows the percentages of user responses
190
Cond. Sys uses Sys uses % with % with
verb prep verb prep
Responses to request location prompt
(1) yes yes 2.3% ? 5.6%
(2) yes yes 1.9% 4.3%
(3) no no 0.7% 4.5%
(4) yes yes 2.4%? 6.0%
Responses to confirm location prompt
(1) yes yes 15.7% ? ? 23.4%
(2) no yes 3.9% 16.9%
(3) no no 6.4% 12.7%
(4) yes yes 10.8% 22.0%
Table 2: Percentages of user utterances containing verbs
and prepositions. ? indicates a statistically significant dif-
ference (p<0.01) from the no action verb condition (3).
? indicates a statistically significant difference from the
no action verb in confirmation condition (2).
in each experimental condition that contain a verb
and/or a preposition. We observe adaptation to the
presence of a verb in user responses to request lo-
cation prompts. The prompts in conditions 1, 2 and
4 contain a verb, while those in condition 3 do not.
The differences between conditions 1 and 3, and be-
tween conditions 4 and 3, are statistically significant
(p<0.01)2. The difference between conditions 2 and
3 is not statistically significant, perhaps due to the
absence of a verb in a prior confirm location prompt.
A similar adaptation to the presence of a verb in
the system prompt is seen in user responses to con-
firm location prompts. The prompts in conditions
1 and 4 contain a verb while those in conditions 2
and 3 do not. The differences between conditions
1 and 2, and between conditions 1 and 3, are statis-
tically significant (p<.01), while the difference be-
tween conditions 4 and 2 exhibits a trend. We hy-
pothesize that the lack of the statistically significant
differences between conditions 4 and 2, and condi-
tions 4 and 3, is caused by the low relative frequency
in our data of dialogs in condition 4.
We do not find statistically significant differences
in the use of prepositions. However, we observe a
trend showing higher likelihood of a preposition in
user responses to confirm location in the conditions
where the system uses a preposition. Prepositions
are short closed-class context words that are more
likely to be misrecognized (Goldwater et al, 2008).
2All analyses in this section are t-tests with Bonferroni ad-
justment.
Condition/ LEAVING LEAVE total
User?s verb (progressive) (simple)
(1) Progressive 74.5% 25.5% 55
(3) Neutral 61.3% 38.7% 31
(4) Simple 43% 57% 42
Condition/ GOING GO total
User?s verb (progressive) (simple)
(1) Progressive 84.4% 15.6% 45
(3) Neutral 66.6% 33.4% 21
(4) Simple 46.5% 53.5% 43
Table 3: Usage of verb forms in user utterances
Hence, more data (or human transcription) may be
required to see a statistically significant effect.
4 Lexical Adaptation
We analyze whether system choice of a particular
verb form affects user choice of verb form. For
this analysis we only consider user utterances in
response to a request location or confirm location
prompt that contain a concept and at least one of the
verb forms leaving, going, leave, or go3.
Table 3 shows the total counts and percentages
of each verb form in the progressive form condition
(condition 1), and the neutral condition (condition
3), and the simple form condition (condition 4)4.
We find that the system?s choice of verb form has
a statistically significant impact on the user?s choice
(?2 test, p<0.01). In the neutral condition, users
are more likely to choose the progressive verb form.
In the progressive form condition, this preference in-
creases by 13.2% for the verb to leave, and by 17.8%
for the verb to go. By contrast, in the simple form
condition, this preference decreases by 18.3% for
the verb to leave and by 20.1% for the verb to go,
making users slightly more likely to choose the sim-
ple verb form than the progressive verb form.
5 Effect of Adaptation on Speech
Recognition Performance
The correct identification and recognition of task-
related concepts in user utterances is an essential
functionality of a dialog system. Table 4 shows
3Such utterances constitute 3% of all user responses to all
request and confirm place prompts in our data.
4We ignore condition 2 where the verb is used only in the
request prompt.
191
System
prompt
Arrival
request
Departure
request
(1) 72.2% ? 63.8%
(2) 77.4% 61.0%
(3) 74.5% ? 61.5%
(4) 82.0% 66.0%
Table 4: Concept identification rates following request
location prompts. ? indicates a statistically significant
difference (p<0.01 with Bonferroni adjustment) from
condition 4.
the percentage of user utterances following a re-
quest location prompt that contain an automatically-
recognized location concept. Condition 4, where the
system prompt uses the verb form to leave, achieves
the highest concept identification rates. The differ-
ences in concept identification rates between condi-
tions 1 and 4, and between conditions 3 and 4, are
statistically significant for request arrival location
(t-test, p<.01). Other differences are not statistically
significant, perhaps due to lack of data.
6 Conclusions and Future Work
In this paper, we showed that in deployed dialog sys-
tems with real users, as in laboratory experiments,
users adapt to the lexical and syntactic choices of the
system. We also showed that user adaptation to sys-
tem prompts can have an impact on recognition of
task-related concepts. This means that the formula-
tion of system prompts, even in flexible input dialog
systems, can be used to guide users into producing
utterances conducive to task success.
In future work, we plan to confirm these results
using transcribed data. We also plan additional ex-
periments on adaptation in Let?s Go!, including an
analysis of the time course of adaptation and further
analyses of the impact of adaptation on ASR perfor-
mance.
7 Acknowledgements
We would like to thank the Let?s Go! researchers at
CMU for making Let?s Go! available. This research
was supported by the NSF under grant no. 0325188.
References
H. Ai, A. Raux, D. Bohus, M. Eskenazi, and D. Lit-
man. 2007. Comparing spoken dialog corpora col-
lected with recruited subjects versus real users. In Pro-
ceedings of SIGDial.
H. Branigan, M. Pickering, J. Pearson, J. McLean, and
C. Nass. 2003. Syntactic alignment between comput-
ers and people: the role of belief about mental states.
In Proceedings of CogSci.
S. Brennan. 1991. Conversation with and through com-
puters. User Modeling and User-Adapted Interaction,
1(1):67?86.
S. Brennan. 1996. Lexical entrainment in spontaneous
dialog. In Proceedings of ISSD.
J. Chu-Carroll and J. Nickerson. 2000. Evaluating au-
tomatic dialogue strategy adaptation for a spoken dia-
logue system. In Proceedings of NAACL.
M. Danieli and E. Gerbino. 1995. Metrics for evaluat-
ing dialogue strategies in a spoken language system.
In Proceedings of the AAAI Spring Symposium on Em-
pirical Methods in Discourse Interpretation and Gen-
eration.
S. Goldwater, D. Jurafsky, and C. Manning. 2008.
Which words are hard to recognize? Lexical, prosodic,
and disfluency factors that increase asr error rates. In
Proceedings of ACL/HLT.
J. Gustafson, A. Larsson, R. Carlson, and K. Hellman.
1997. How do system questions influence lexical
choices in user answers? In Proceedings of Eu-
rospeech.
C. Lockridge and S. Brennan. 2002. Addressees? needs
influence speakers? early syntactic choices. Psycho-
nomics Bulletin and Review.
M. Pickering, H. Branigan, A. Cleland, and A. Stew-
art. 2000. Activation of syntactic priming during
language production. Journal of Psycholinguistic Re-
search, 29(2):205?216.
A. Raux, B. Langner, A. Black, and M Eskenazi. 2005.
Let?s Go public! taking a spoken dialog system to the
real world. In Proceedings of Eurospeech.
E. Reitter, J. Moore, and F. Keller. 2006. Priming of syn-
tactic rules in task-oriented dialogue and spontaneous
conversation. In Proceedings of CogSci.
T. Sheeder and J. Balogh. 2003. Say it like you mean
it: priming for structure in caller responses to a spoken
dialog system. International Journal of Speech Tech-
nology, 6(2):103?111.
R. Smith and S. Gordon. 1997. Effects of variable initia-
tive on linguistic behavior in human-computer spoken
natural language dialogue. Computational Linguistics,
23(1):141?168.
A. Ward and D. Litman. 2007. Automatically measuring
lexical and acoustic/prosodic convergence in tutorial
dialog corpora. In Proceedings of the SLaTE Work-
shop on Speech and Language Technology in Educa-
tion.
192
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 42?49,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Predicting Concept Types in User Corrections in Dialog
Svetlana Stoyanchev and Amanda Stent
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400, USA
svetlana.stoyanchev@gmail.com, amanda.stent@stonybrook.edu
Abstract
Most dialog systems explicitly confirm
user-provided task-relevant concepts.
User responses to these system confirma-
tions (e.g. corrections, topic changes) may
be misrecognized because they contain
unrequested task-related concepts. In this
paper, we propose a concept-specific lan-
guage model adaptation strategy where
the language model (LM) is adapted to
the concept type(s) actually present in
the user?s post-confirmation utterance.
We evaluate concept type classification
and LM adaptation for post-confirmation
utterances in the Let?s Go! dialog system.
We achieve 93% accuracy on concept type
classification using acoustic, lexical and
dialog history features. We also show that
the use of concept type classification for
LM adaptation can lead to improvements
in speech recognition performance.
1 Introduction
In most dialog systems, the system explicitly con-
firms user-provided task-relevant concepts. The
user?s response to a confirmation prompt such as
?leaving from Waterfront?? may consist of a sim-
ple confirmation (e.g. ?yes?), a simple rejection
(e.g. ?no?), a correction (e.g. ?no, Oakland?) or a
topic change (e.g. ?no, leave at 7? or ?yes, and go
to Oakland?). Each type of utterance has implica-
tions for further processing. In particular, correc-
tions and topic changes are likely to contain un-
requested task-relevant concepts that are not well
represented in the recognizer?s post-confirmation
language model (LM)1. This means that they are
1The word error rate on post-confirmation Let?s Go! utter-
ances containing a concept is 10% higher than on utterances
likely to be misrecognized, frustrating the user and
leading to cascading errors. Correct determina-
tion of the content of post-confirmation utterances
can lead to improved speech recognition, fewer
and shorter sequences of speech recognition er-
rors, and improved dialog system performance.
In this paper, we look at user responses to sys-
tem confirmation prompts CMU?s deployed Let?s
Go! dialog system. We adopt a two-pass recogni-
tion architecture (Young, 1994). In the first pass,
the input utterance is processed using a general-
purpose LM (e.g. specific to the domain, or spe-
cific to the dialog state). Recognition may fail
on concept words such as ?Oakland? or ?61C? ,
but is likely to succeed on closed-class words (e.g.
?yes?, ?no?, ?and?, ?but?, ?leaving?). If the ut-
terance follows a system confirmation prompt, we
then use acoustic, lexical and dialog history fea-
tures to determine the task-related concept type(s)
likely to be present in the utterance. In the second
recognition pass, any utterance containing a con-
cept type is re-processed using a concept-specific
LM. We show that: (1) it is possible to achieve
high accuracy in determining presence or absence
of particular concept types in a post-confirmation
utterance; and (2) 2-pass speech recognition with
concept type classification and language model
adaptation can lead to improved speech recogni-
tion performance for post-confirmation utterances.
The rest of this paper is structured as follows: In
Section 2 we discuss related work. In Section 3 we
describe our data. In Section 4 we present our con-
cept type classification experiment. In Section 5
we present our LM adaptation experiment. In Sec-
tion 6 we conclude and discuss future work.
without a concept.
42
2 Related Work
When a dialog system requests a confirmation,
the user?s subsequent corrections and topic change
utterances are particularly likely to be misrecog-
nized. Considerable research has now been done
on the automatic detection of spoken corrections.
Linguistic cues to corrections include the num-
ber of words in the post-confirmation utterance
and the use of marked word order (Krahmer et
al., 2001). Prosodic cues include F0 max, RMS
max, RMS mean, duration, speech tempo, and
percentage of silent frames(Litman et al, 2006;
Hirschberg et al, 2004; Levow, 1998). Discourse
cues include the removal, repetition, addition or
modification of a concept, the system?s dialog act
type, and information about error rates in the dia-
log so far (Krahmer et al, 2001; et al, 2002; Lit-
man et al, 2006; Walker et al, 2000). In our ex-
periments, we use most of these features as well as
additional lexical features.
We can use knowledge of the type or content
of a user utterance to modify system behavior.
For example, in this paper we use the concept
type(s) in the user?s utterance to adapt the recog-
nizer?s LM. It is now common practice to adapt
the recognizer to the type, context or style of in-
put speech (Bellegarda, 2004). LM adaptation
has been used to improve automatic speech recog-
nition performance in automated meeting tran-
scription (Tur and Stolcke, 2007), speech-driven
question answering (Stenchikova et al, 2008),
broadcast news recognition (Gildea and Hofmann,
1999), and spoken dialog systems (Tur, 2005).
LMs in dialog systems can be adapted to the di-
alog state (e.g. (Riccardi and Gorin, 2000; Esteve
et al, 2001)), the topic (Iyer and Ostendorf, 1999;
Gildea and Hofmann, 1999), or the speaker (Tur,
2007).
3 Data
In this experiment we use annotated dialog tran-
scripts and speech from the Let?s Go! sys-
tem, a telephone-based spoken dialog system that
provides information about bus routes in Pitts-
burgh (Raux et al, 2005). The data we used comes
from the first two months of Let?s Go! system
operation in 2005 (2411 dialogs), and one month
in 2006 (1430 dialogs). This data has been tran-
scribed, and annotated by hand for concept types.
In order to provide the user with route in-
formation, Let?s Go! must elicit a departure
1 Sys Welcome to the CMU Let?s Go bus informa-
tion system. What can I do for you?
2 User I need to go from Oakland:p
ASR I need to go .from. can?t
3 Sys Where do you wanna leave from?
4 User to Waterfront:p
ASR told. .me. Waterfront
5 Sys Leaving from WATERFRONT . Did I get
that right?
6 User OAKLAND:p
ASR OAKLAND
7 Sys Leaving from OAKLAND . Did I get that
right?
Figure 1: Dialog extract from Let?s Go! data. User
utterances are annotated with concept types (e.g.
:p for place)
location, a destination, a departure time, and
optionally a bus route number. Each concept
value provided by the user is explicitly con-
firmed by the system (see Figure 1). In the
annotated transcripts, the following concepts are
labeled: neighborhood, place, time,
hour, minute, time-of-day, and bus.
For our experiments we collapsed these concepts
into three concept types: time , place and bus.
Let?s Go! has five dialog states corresponding
to the type of user utterance it expects: first-query,
next-query, yes-no, place and time. Its speech
recognizer uses dialog state-specific n-gram LMs
trained on user utterances from the 2005 data.
We focus on user utterances in response to sys-
tem confirmation prompts (the yes-no state). Ta-
ble 1 shows statistics about yes-no state utterances
in Let?s Go!. Table 2 shows a confusion matrix
for confirmation prompt concept type and post-
confirmation utterance concept type. This table
indicates the potential for misrecognition of post-
confirmation utterances. For example, in the 2006
dataset after a system confirmation prompt for a
bus, a bus concept is used in only 64% of concept-
containing user utterances.
In our experiments, we used the 2006 data to
train concept type classifiers and for testing. We
used the 2005 data to build LMs for our speech
recognition experiment.
4 Concept Classification
4.1 Method
Our goal is to classify each post-confirmation user
utterance by the concept type(s) it contains (place,
time, bus or none) for later language-model adap-
tation (see Section 5). From the post-confirmation
user utterances in the 2006 dataset described in
43
Event 2005 2006
num % num %
Total dialogs 2411 1430
Total yes-no confirms 9098 100 9028 100
Yes-no confirms with
a concept
2194 24 1635 18.1
Dialog State
Total confirm place
utts
5548 61 5347 59.2
Total confirm bus utts 1763 19.4 1589 17.6
Total confirm time
utts
1787 19.6 2011 22.3
Concept Type Features
Yes-no utts with place 1416 15.6 1007 11.2
Yes-no utts with time 296 3.2 305 3.4
Yes-no utts with bus 584 6.4 323 3.6
Lexical Features
Yes-no utts with ?yes? 4395 48.3 3693 40.9
Yes-no utts with ?no? 2076 22.8 1564 17.3
Yes-no utts with ?I? 203 2.2 129 1.4
Yes-no utts with
?from?
114 1.3 185 2.1
Yes-no utts with ?to? 204 2.2 237 2.6
Acoustic Features
feature mean stdev mean stdev
Duration (seconds) 1.341 1.097 1.365 1.242
RMS mean .037 .033 .055 .049
F0 mean 183.0 60.86 185.7 58.63
F0 max 289.8 148.5 296.9 146.5
Table 1: Statistics on post-confirmation utterances
place bus time
2005 dataset
confirm place 0.86 0.13 0.01
confirm bus 0.18 0.81 0.01
confirm time 0.07 0.01 0.92
2006 dataset
confirm place 0.87 0.10 0.03
confirm bus 0.34 0.64 0.02
confirm time 0.15 0.13 0.71
Table 2: Confirmation state vs. user concept type
Section 3, we extracted the features described in
Section 4.2 below. To identify the correct concept
type(s) for each utterance, we used the human an-
notations provided with the data.
We performed a series of 10-fold cross-
validation experiments to examine the impact of
different types of feature on concept type classifi-
cation. We trained three binary classifiers for each
experiment, one for each concept type, i.e. we sep-
arately classified each post-confirmation utterance
as place + or place -, time + or time -, and bus + or
bus -. We used Weka?s implementation of the J48
decision tree classifier (Witten and Frank, 2005)2.
For each experiment, we report precision (pre+)
and recall (rec+) for determining presence of each
concept type, and overall classification accuracy
2J48 gave the highest classification accuracy compared to
other machine learning algorithms we tried on this data.
for each concept type (place, bus and time)3. We
also report overall pre+, rec+, f-measure (f+), and
classification accuracy across the three concept
types. Finally, we report the percentage of switch+
errors and switch errors. Switch+ errors are utter-
ances containing bus classified as time/place, time
as bus/place, and place as bus/time; these are the
errors most likely to cause decreases in speech
recognition accuracy after language model adap-
tation. Switch errors include utterances with no
concept classified as place, bus or time.
Only utterances classified as containing one of
the three concept types are subject to second-
pass recognition using a concept-specific language
model. Therefore, these are the only utterances on
which speech recognition performance may im-
prove. This means that we want to maximize rec+
(proportion of utterances containing a concept that
are classified correctly). On the other hand, utter-
ances that are incorrectly classified as containing a
particular concept type will be subject to second-
pass recognition using a poorly-chosen language
model. This may cause speech recognition per-
formance to suffer. This means that we want to
minimize switch+ errors.
4.2 Features
We used the features summarized in Table 3. All
of these features are available at run-time and so
may be used in a live system. Below we give ad-
ditional information about the RAW and LEX fea-
tures; the other feature sets are self-explanatory.
4.2.1 Acoustic and Dialog History Features
The acoustic/prosodic and dialog history features
are adapted from those identified in previous work
on detecting speech recognition errors (particu-
larly (Litman et al, 2006)). We anticipated that
these features would help us distinguish correc-
tions and rejections from confirmations.
4.2.2 Lexical Features
We used lexical features from the user?s current ut-
terance. Words in the output of first-pass ASR are
highly indicative both of concept presence or ab-
sence, and of the presence of particular concept
types; for example, going to suggests the pres-
ence of a place. We selected the most salient lexi-
3We do not report precision or recall for determining ab-
sence of each concept type. In our data set 82.2% of the ut-
terances do not contain any concepts (see Table 1). Conse-
quently, precision and recall for determining absence of each
concept type are above .9 in each of the experiments.
44
Feature type Feature source Features
System confirmation type
(DIA)
system log System?s confirmation prompt concept type (confirm time,
confirm place, or confirm bus)
Acoustic (RAW) raw speech F0 max; RMS max; RMS mean; Duration; Difference be-
tween F0 max in first half and in second half
Lexical (LEX) transcripts/ASR output Presence of specific lexical items; Number of tokens in utter-
ance; [transcribed speech only] String edit distance between
current and previous user utterances
Dialog history (DH1, DH3) 1-3 previous utterances System?s dialog states of previous utterances(place, bus,
time, confirm time, confirm place, or confirm bus); [tran-
scribed speech only] Concept(s) that occurred in user?s ut-
terances (YES/NO for each of the concepts place, bus, time)
ASR confidence score (ASR) ASR output Speech recognizer confidence score
Concept type match (CTM) transcripts/ASR output Presence of concept-specific lexical items
Table 3: Features for concept type classifiers
cal features (unigrams and bigrams) for each con-
cept type by computing the mutual information be-
tween potential features and concept types (Man-
ning et al, 2008). For each lexical feature t and
each concept type class c ? { place +, place -,
time +, time -, bus +, bus -}, we computed I:
I = NtcN ? log2
N ? Ntc
Nt. ? N.c
+
N0c
N ? log2
N ? N0c
N0. ? N.c
+
Nt0
N ? log2
N ? Nt0
Nt. ? N.0
+
N00
N ? log2
N ? N00
N0. ? N.0
where Ntc= number of utterances where t co-
occurs with c, N0c= number of utterances with c
but without t, Nt0= number of utterances where t
occurs without c, N00= number of utterances with
neither t nor c, Nt.= total number of utterances
containing t, N.c= total number of utterances con-
taining c, and N = total number of utterances.
To identify the most relevant lexical features,
we extracted from the data all the transcribed user
utterances. We removed all words that realize con-
cepts (e.g. ?61C?, ?Squirrel Hill?), as these are
likely to be misrecognized in a post-confirmation
utterance. We then extracted all word unigrams
and bigrams. We computed the mutual informa-
tion between each potential lexical feature and
concept type. We then selected the 30 features
with the highest mutual information which oc-
curred at least 20 times in the training data4.
For transcribed speech only, we also compute
the string edit distance between the current and
previous user utterances. This gives some indica-
tion of whether the current utterance is a correc-
tion or topic change (vs. a confirmation). How-
4We aimed to select equal number of features for each
class with information measure in the top 25%. 30 was an
empirically derived threshold for the number of lexical fea-
tures to satisfy the desired condition.
ever, for recognized speech recognition errors re-
duce the effectiveness of this feature (and of the
concept features in the dialog history feature set).
4.3 Baseline
A simple baseline for this task, No-Concept, al-
ways predicts none in post-confirmation utter-
ances. This baseline achieves overall classifica-
tion accuracy of 82% but rec+ of 0. At the other
extreme, the Confirmation State baseline assigns
to each utterance the dialog system?s confirmation
prompt type (using the DIA feature). This base-
line achieves rec+ of .79, but overall classification
accuracy of only 14%. In all of the models used in
our experiments, we include the current confirma-
tion prompt type (DIA) feature.
4.4 Experiment Results
In this section we report the results of experiments
on concept type classification in which we exam-
ine the impact of the feature sets presented in Ta-
ble 3. We report performance separately for recog-
nized speech, which is available at runtime (Table
5); and for transcribed speech, which gives us an
idea of best possible performance (Table 4).
4.4.1 Features from the Current Utterance
We first look at lexical (LEX) and prosodic (RAW)
features from the current utterance. For both rec-
ognized and transcribed speech, the LEX model
achieves significantly higher rec+ and overall ac-
curacy than the RAW model (p < .001). For
recognized speech, however, the LEX model has
significantly more switch+ errors than the RAW
model (p < .001). This is not surprising since the
majority of errors made by the RAW model are
labeling an utterance with a concept as none. Ut-
terances misclassified in this way are not subject to
second-pass recognition and do not increase WER.
45
Features Place Time Bus Overall
pre+ rec+ acc pre+ rec+ acc pre+ rec+ acc pre+ rec+ f+ acc switch+ switch
No Concept 0 0 .86 0 0 0.81 0 0 .92 0 0 0 0.82 0 0
Confirmation State 0.87 0.85 0.86 0.64 0.54 0.58 0.71 0.87 0.78 0.14 0.79 0.24 0.14 17 72.3
RAW 0.65 0.53 0.92 0.25 0.01 0.96 0.38 0.07 0.96 0.67 0.34 0.45 0.85 6.43 4.03
LEX 0.81 0.88 0.96 0.77 0.48 0.98 0.83 0.59 0.98 0.87 0.72 0.79 0.93 7.32 3.22
LEX RAW 0.83 0.84 0.96 0.75 0.54 0.98 0.76 0.59 0.98 0.88 0.70 0.78 0.93 7.39 3.00
DH1 LEX 0.85 0.91 0.97 0.72 0.63 0.98 0.89 0.83 0.99 0.88 0.81 0.84 0.95 5.48 2.85
DH3 LEX 0.85 0.87 0.97 0.72 0.59 0.98 0.92 0.82 0.99 0.89 0.78 0.83 0.94 5.22 2.62
Table 4: Concept type classification results: transcribed speech (all models include feature DIA). Best
overall values in each group are highlighted in bold.
Features Place Time Bus Overall
pre+ rec+ acc pre+ rec+ acc pre+ rec+ acc pre+ rec+ f+ acc switch+ switch
No Concept 0 0 .86 0 0 0.81 0 0 .92 0 0 0 0.82 0 0
Confirmation State 0.87 0.85 0.86 0.64 0.54 0.58 0.71 0.87 0.78 0.14 0.79 0.24 0.14 17 72.3
RAW 0.65 0.53 0.92 0.25 0.01 0.96 0.38 0.07 0.96 0.67 0.34 0.45 0.85 6.43 4.03
LEX 0.70 0.70 0.93 0.67 0.15 0.97 0.65 0.62 0.98 0.75 0.56 0.64 0.89 9.94 4.93
LEX RAW 0.70 0.72 0.93 0.66 0.38 0.97 0.68 0.57 0.98 0.76 0.60 0.67 0.90 10.32 5.10
DH1 LEX RAW 0.71 0.68 0.93 0.68 0.38 0.97 0.78 0.63 0.98 0.77 0.60 0.67 0.90 8.15 4.55
DH3 LEX RAW 0.71 0.70 0.93 0.67 0.42 0.97 0.79 0.63 0.98 0.77 0.62 0.68 0.90 7.20 4.57
ASR DH3 LEX
RAW
0.71 0.70 0.93 0.69 0.42 0.97 0.79 0.63 0.98 0.77 0.62 0.68 0.90 7.20 4.54
CTM DH3 LEX
RAW
0.82 0.82 0.96 0.86 0.71 0.99 0.76 0.68 0.98 0.85 0.74 0.79 0.93 3.89 2.94
CTM ASR DH3
LEX RAW
0.82 0.81 0.96 0.86 0.69 0.99 0.76 0.68 0.98 0.85 0.74 0.79 0.93 4.27 3.01
Table 5: Concept type classification results: recognized speech (all models include feature DIA). Best
overall values in each group are highlighted in bold.
For transcribed speech, the LEX RAW model
does not perform significantly differently from the
LEX model in terms of overall accuracy, rec+, or
switch+ errors. However, for recognized speech,
LEX RAW achieves significantly higher rec+ and
overall accuracy than LEX (p < .001). Lexical
content from transcribed speech is a very good in-
dicator of concept type. However, lexical content
from recognized speech is noisy, so concept type
classification from ASR output can be improved
by using acoustic/prosodic features.
We note that models containing only features
from the current utterance perform significantly
worse than the confirmation state baseline in terms
of rec+ (p < .001). However, they have signif-
icantly better overall accuracy and fewer switch+
errors (p < .001) .
4.4.2 Features from the Dialog History
Next, we add features from the dialog history
to our best-performing models so far. For tran-
scribed speech, DH1 LEX performs significantly
better than LEX in terms of overall accuracy, rec+,
and switch+ errors (p < .001). DH3 LEX per-
forms significantly worse than DH1 LEX in terms
of rec+ (p < 0.05). For recognized speech,
neither DH1 LEX RAW nor DH3 LEX RAW is
significantly different from LEX RAW in terms
of rec+ or overall accuracy. However, both
DH1 LEX RAW and DH3 LEX RAW do per-
form significantly better than LEX RAW in terms
of switch+ errors (p < .05). There are
no significant performance differences between
DH1 LEX RAW and DH3 LEX RAW.
4.4.3 Features Specific to Recognized Speech
Finally, we add the ASR and CTM features to
models trained on recognized speech.
We hypothesized that the classifier can use the
recognizer?s confidence score to decide whether
an utterance is likely to have been misrecognized.
However, ASR DH3 LEX RAW is not signifi-
cantly different from DH3 LEX RAW in terms of
rec+, overall accuracy or switch+ errors.
We hypothesized that the CTM feature will im-
prove cases where a part of (but not the whole)
concept instance is recognized in first-pass recog-
nition5. The generic language model used in first-
pass recognition recognizes some concept-related
words. So, if in the utterance Madison avenue,
avenue (but not Madison), is recognized in the
first-pass recognition, the CTM feature can flag
the utterance with a partial match for place, help-
ing the classifier to correctly assign the place
5We do not try the CTM feature on transcribed speech be-
cause there is a one-to-one correspondence between presence
of the concept and the CTM feature, so it perfectly indicates
presence of a concept.
46
type to the utterance. Then, in the second-pass
recognition the utterance will be decoded with
a place concept-specific language model, poten-
tially improving speech recognition performance.
Adding the CTM feature to DH3 LEX RAW and
ASR DH3 LEX RAW leads to a large statistically
significant improvement in all measures: a 12%
absolute increase in rec+, a 3% absolute increase
in overall accuracy, and decreases in switch+ er-
rors (p < .001). There are no statistically signifi-
cant differences between these two models.
4.4.4 Summary and Discussion
In this section we evaluated different models for
concept type classification. The best perform-
ing transcribed speech model, DH1 LEX, signif-
icantly outperforms the Confirmation State base-
line on overall accuracy and on switch+ and switch
errors (p < .001), and is not significantly different
on rec+. The best performing recognized speech
model, CTM DH3 LEX RAW, significantly out-
performs the Confirmation State baseline on
overall accuracy and on switch+ and switch er-
rors, but is significantly worse on rec+ (p < .001).
The best transcribed speech model achieves signif-
icantly higher rec+ and overall accuracy than the
best recognized speech model (p < .01).
5 Speech Recognition Experiment
In this section we report the impact of concept type
prediction on recognition of post-confirmation ut-
terances in Let?s Go! system data. We hypothe-
sized that speech recognition performance for ut-
terances containing a concept can be improved
with the use of concept-specific LMs. We (1) com-
pare the existing dialog state-specific LM adap-
tation approach used in Let?s Go! with our pro-
posed concept-specific adaptation; (2) compare
two approaches to concept-specific adaptation (us-
ing the system?s confirmation prompt type and us-
ing our concept type classifiers); and (3) evaluate
the impact of different concept type classifiers on
concept-specific LM adaptation.
5.1 Method
We used the PocketSphinx speech recognition en-
gine (et al, 2006) with gender-specific telephone-
quality acoustic models built for Communica-
tor (et al, 2000). We trained trigram LMs us-
ing 0.5 ratio discounting with the CMU language
modeling toolkit (Xu and Rudnicky, 2000)6. We
built state- and concept-specific hierarchical LMs
from the Let?s Go! 2005 data. The LMs are built
with [place], [time] and [bus] submodels.
We evaluate speech recognition performance
on the post-confirmation user utterances from the
2006 testing dataset. Each experiment varies in 1)
the LM used for the final recognition pass and 2)
the method of selecting a LM for use in decoding.
5.1.1 Language models
We built seven LMs for these experiments. The
state-specific LM contains all utterances in the
training data that were produced in the yes-no di-
alog state. The confirm-place, confirm-bus and
confirm-time LMs contain all utterances produced
in the yes-no dialog state following confirm place,
confirm bus and confirm time system confirma-
tion prompts respectively. Finally, the concept-
place, concept-bus and concept-time LMs contain
all utterances produced in the yes-no dialog state
that contain a mention of a place, bus or time.
5.1.2 Decoders
In the baseline, 1-pass general condition, we
use the state-specific LM to recognize all post-
confirmation utterances. In the 1-pass state ex-
perimental condition we use the confirm-place,
confirm-bus and confirm-time LMs to recog-
nize testing utterances produced following a con-
firm place, confirm bus and confirm time prompt
respectively7 . In the 1-pass concept experimen-
tal condition we use the concept-place, concept-
bus and concept-time LMs to recognize testing ut-
terances produced following a confirm place, con-
firm bus and confirm time prompt respectively.
In the 2-pass conditions we perform first-pass
recognition using the general LM. Then, we clas-
sify the output of the first pass using a concept
type classifier. Finally, we perform second-pass
recognition using the concept-place, concept-bus
or concept-time LMs if the utterance was classi-
fied as place, bus or time respectively8 . We used
the three classification models with highest overall
rec+: DH3 LEX RAW, ASR DH3 LEX RAW,
6We chose the same speech recognizer, acoustic models,
language modeling toolkit, and LM building parameters that
are used in the live Let?s Go! system (Raux et al, 2005).
7As we showed in Table 2, most, but not all, utterances in
a confirmation state contain the corresponding concept.
8We treat utterances classified as containing more than
concept type as none. In the 2006 data, only 5.6% of ut-
terances with a concept contain more than one concept type.
47
Recognizer Concept type Language Overall Concept utterances
classifier model WER WER Concept recall
1-pass general state-specific 38.49% 49.12% 50.75%
1-pass confirm state confirm-{place,bus,time} 38.83% 48.96% 51.36%
1-pass confirm state concept-{place,bus,time},
state-specific
46.47% ? 50.73% ? 52.9% ?
2-pass DH3 LEX RAW concept-{place,bus,time},
state-specific
38.48% 47.56% ? 53.2% ?
2-pass ASR DH3 LEX
RAW
concept-{place,bus,time},
state-specific
38.51% 47.99% ? 52.7%
2-pass CTM ASR DH3
LEX RAW
concept-{place,bus,time},
state-specific
38.42% 47.86% ? 52.6%
2-pass oracle concept-{place,bus,time},
state-specific
37.85% ? 45.94% ? 54.91% ?
Table 6: Speech recognition results. ? indicates significant difference (p<.01). ? indicates significant
difference (p<.05). * indicates near-significant trend in difference (p<.07). Significance for WER is
computed as a paired t-test. Significance for concept recall is an inference on proportion.
and CTM ASR DH3 LEX RAW. To get an idea
of ?best possible? performance, we also report 2-
pass oracle recognition results, assuming an oracle
classifier that always outputs the correct concept
type for an utterance.
5.2 Results
In Table 6 we report average per-utterance word
error rate (WER) on post-confirmation utterances,
average per-utterance WER on post-confirmation
utterances containing a concept, and average con-
cept recall rate (percentage of correctly recog-
nized concepts) on post-confirmation utterances
containing a concept. In slot-filling dialog sys-
tems like Let?s Go!, the concept recall rate largely
determines the potential of the system to under-
stand user-provided information and continue the
dialog successfully. Our goal is to maximize con-
cept recall and minimize concept utterance WER,
without causing overall WER to decline.
As Table 6 shows, the 1-pass state and 1-pass
concept recognizers perform better than the 1-
pass general recognizer in terms of concept recall,
but worse in terms of overall WER. Most of these
differences are not statistically significant. How-
ever, the 1-pass concept recognizer has signifi-
cantly worse overall and concept utterance WER
than the 1-pass general recognizer (p < .01).
All of the 2-pass recognizers that use au-
tomatic concept prediction achieve significantly
lower concept utterance WER than the 1-pass
general recognizer (p < .05). Differences be-
tween these recognizers in overall WER and con-
cept recall are not significant.
The 2-pass oracle recognizer achieves signif-
icantly higher concept recall and significantly
lower overall and concept utterance WER than
the 1-pass general recognizer (p < .01). It
also achieves significantly lower concept utterance
WER than any of the 2-pass recognizers that use
automatic concept prediction (p < .01).
Our 2-pass concept results show that it is possi-
ble to use knowledge of the concepts in a user?s ut-
terance to improve speech recognition. Our 1-pass
concept results show that this cannot be effec-
tively done by assuming that the user will always
address the system?s question; instead, one must
consider the user?s actual utterance and the dis-
course history (as in our DH3 LEX RAW model).
6 Conclusions and Future Work
In this paper, we examined user responses to sys-
tem confirmation prompts in task-oriented spoken
dialog. We showed that these post-confirmation
utterances may contain unrequested task-relevant
concepts that are likely to be misrecognized. Us-
ing acoustic, lexical, dialog state and dialog his-
tory features, we were able to classify task-
relevant concepts in the ASR output for post-
confirmation utterances with 90% accuracy. We
showed that use of a concept type classifier can
lead to improvements in speech recognition per-
formance in terms of WER and concept recall.
Of course, any possible improvements in speech
recognition performance are dependent on (1) the
performance of concept type classification; (2)
the accuracy of the first-pass speech recognition;
and (3) the accuracy of the second-pass speech
recognition. For example, with our general lan-
guage model, we get a fairly high overall WER
of 38.49%. In future work, we will systematically
vary the WER of both the first- and second-pass
48
speech recognizers to further explore the interac-
tion between speech recognition performance and
concept type classification.
The improvements our two-pass recognizers
achieve have quite small local effects (up to 3.18%
absolute improvement in WER on utterances con-
taining a concept, and less than 1% on post-
confirmation utterances overall) but may have
larger impact on dialog completion times and task
completion rates, as they reduce the number of
cascading recognition errors in the dialog (et al,
2002). Furthermore, we could also use knowledge
of the concept type(s) contained in a user utterance
to improve dialog management and response plan-
ning (Bohus, 2007). In future work, we will look
at (1) extending the use of our concept-type clas-
sifiers to utterances following any system prompt;
and (2) the impact of these interventions on overall
metrics of dialog success.
7 Acknowledgements
We would like to thank the researchers at CMU
for providing the Let?s Go! data and additional
resources.
References
J. R. Bellegarda. 2004. Statistical language model
adaptation: Review and perspectives. Speech Com-
munication Special Issue on Adaptation Methods for
Speech Recognition, 42:93?108.
D. Bohus. 2007. Error awareness and recovery in
task-oriented spoken dialog systems. Ph.D. thesis,
Carnegie Mellon University.
Y. Esteve, F. Bechet, A. Nasr, and R. Mori. 2001.
Stochastic finite state automata language model trig-
gered by dialogue states. In Proceedings of Eu-
rospeech.
A. Rudnicky et al 2000. Task and domain specific
modelling in the Carnegie Mellon Communicator
system. In Proceedings of ICSLP.
J. Shin et al 2002. Analysis of user behavior under
error conditions in spoken dialogs. In Proceedings
of ICSLP.
D. Huggins-Daines et al 2006. Sphinx: A free, real-
time continuous speech recognition system for hand-
held devices. In Proceedings of ICASSP.
D. Gildea and T. Hofmann. 1999. Topic-based lan-
guage models using EM. In Proceedings of Eu-
rospeech.
J. Hirschberg, D. Litman, and M. Swerts. 2004.
Prosodic and other cues to speech recognition fail-
ures. Speech Communication, 43:155?175.
R. Iyer and M. Ostendorf. 1999. Modeling long dis-
tance dependencies in language: Topic mixtures ver-
sus dynamic cache model. IEEE Transactions on
Speech and Audio Processing, 7(1):30?39.
E. Krahmer, M. Swerts, M. Theune, and M. Weegels.
2001. Error detection in spoken human-machine in-
teraction. International Journal of Speech Technol-
ogy, 4(1).
G.-A. Levow. 1998. Characterizing and recognizing
spoken corrections in human-computer dialogue. In
Proceedings of COLING-ACL.
D. Litman, J.Hirschberg, and M. Swerts. 2006. Char-
acterizing and predicting corrections in spoken dia-
logue systems. Computational Linguistics, 32:417?
438.
C. D. Manning, P. Raghavan, and H. Schu?tze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
A. Raux, B. Langner, A. Black, and M Eskenazi. 2005.
Let?s Go Public! Taking a spoken dialog system to
the real world. In Proceedings of Eurospeech.
G. Riccardi and A. L. Gorin. 2000. Stochastic lan-
guage adaptation over time and state in a natural spo-
ken dialog system. IEEE Transactions on Speech
and Audio Processing, 8(1):3?9.
S. Stenchikova, D. Hakkani-Tu?r, and G. Tur. 2008.
Name-aware speech recognition for interactive
question answering. In Proceedings of ICASSP.
G. Tur and A. Stolcke. 2007. Unsupervised language
model adaptation for meeting recognition. In Pro-
ceedings of ICASSP.
G. Tur. 2005. Model adaptation for spoken language
understanding. In Proceedings of ICASSP.
G. Tur. 2007. Extending boosting for large scale
spoken language understanding. Machine Learning,
69(1):55?74.
M. Walker, J. Wright, and I. Langkilde. 2000. Using
natural language processing and discourse features
to identify understanding errors in a spoken dialogue
system. In Proceedings of ICML.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
W. Xu and A. Rudnicky. 2000. Language modeling
for dialog system. In Proceedings of ICSLP.
S. Young. 1994. Detecting misrecognitions and out-
of-vocabulary words. In Proceedings of ICASSP.
49
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 61?69,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Automating Model Building in c-rater 
 
 
Jana Z. Sukkarieh 
Educational Testing Service  
Rosedale Road, Princeton, NJ 08541 
jsukkarieh@ets.org 
Svetlana Stoyanchev 
Stony Brook University  
Stony Brook, NY, 11794 
svetastenchikova@gmail.com 
 
  
 
 
Abstract 
c-rater is Educational Testing Service?s 
technology for the content scoring of short 
student responses.  A major step in the scor-
ing process is Model Building where vari-
ants of model answers are generated that 
correspond to the rubric for each item or test 
question. Until recently, Model Building 
was knowledge-engineered (KE) and hence 
labor and time intensive. In this paper, we 
describe our approach to automating Model 
Building in c-rater. We show that c-rater 
achieves comparable accuracy on automati-
cally built and KE models. 
1 Introduction 
c-rater (Leacock and Chodorow, 2003) is Edu-
cational Testing Service?s (ETS) technology 
for the automatic content scoring of short free-
text student answers, ranging in length from a 
few words to approximately 100 words. While 
other content scoring systems [e.g., Intelligent. 
Essay Assessor (Foltz, Laham and Landauer, 
2003), SEAR (Christie, 1999), IntelliMetric 
(Vantage Learning Tech, 2000)] take a holis-
tic 1  approach, c-rater takes an analytical ap-
proach to scoring content. The item rubrics 
specify content in terms of main points or con-
cepts required to appear in a student?s correct 
answer. An example of a test question or item 
follows: 
                                                 
1 Holistic means an overall score is given for a student?s 
answer as opposed to scores for individual components of 
a student?s answer. 
 
Item 1 (Full credit: 2 points) 
Stimulus: A Reading passage 
 
Prompt:  
In the space below, write the 
question that Alice was most 
likely trying to answer when 
she performed Step B. 
Concepts or main/key points: 
C :1  How does rain forma-
tion occur in winter? 
C : 2 How is rain formed? 
C : 3 How do temperature 
and altitude contribute 
to the formation of 
rain? 
 
Scoring rules:  
2 points for C1  
1 for C2 (only if C1 is not present) 
1 for C3 (only if C1 and C2 are not present)  
Otherwise 0 
 
We view c-rater's task as a textual entailment 
(TE) problem. We use TE here to mean either 
a paraphrase or an inference (up to the context 
of the item or test question). c-rater's task is 
reduced to a TE problem in the following way:  
 
Given a concept, C, (e.g., ?body increases 
its temperature?) and a student answer, A, 
(e.g., either ?the body raises temperature,? 
?the body responded. His temperature was 
37? and now it is 38?,? or ?Max has a fe-
ver?) and the context of the item, the goal 
is to check whether C is an inference or 
paraphrase of A (in other words, A implies 
C and A is true). 
 
There are four main steps in c-rater. The first 
one is Model Building (MB), where a set of 
model answers are generated (either manually 
or automatically). Second, c-rater automati-
cally processes model answers and students? 
answers using a set of natural language proc-
essing (NLP) tools and extracts the linguistic 
features. Third, the matching algorithm  
Goldmap uses the linguistic features culmi-
nated from both MB and NLP to automatically 
determine whether a student?s response entails 
the expected concepts. Finally, c-rater applies 
61
the scoring rules to produce a score and feed-
back that justifies the score to the student.  
 
Until recently, MB was knowledge-engineered 
(KE). The KE approach for one item required, 
on average, 12 hours of time and labor. This 
paper describes our approach to automatic MB. 
We show that c-rater achieves comparable ac-
curacy on automatically- and manually-built 
models. Section 2 outlines others? work in this 
domain and emphasizes the contribution of this 
paper. Section 3 outlines c-rater. In Section 4, 
we describe how MB works. Section 5 ex-
plains how we automate the process. Prior to 
the conclusion, we report the evaluation of this 
work.    
 
2 Automatic Content Scoring:  
Others? Work  
A few systems that deal with both short an-
swers and analytic-based content exist. The 
task, in general, is reduced to comparing a stu-
dent?s answer to a model answer. Recent work 
by Mohler and Mihalcea (2009) at the Univer-
sity of North Texas uses unsupervised methods 
in text-to-text semantic similarity comparing 
unseen students? answers to one correct an-
swer. Previous work, including c-rater, used 
supervised techniques to compare unseen stu-
dents? answers to the space of potentially ?all 
possible correct answers? specified in the ru-
bric of the item at hand. The techniques varied 
from information extraction with knowledge-
engineered patterns representing the model 
answers [Automark at Intelligent Assessment 
Technologies (Mitchell, 2002), the Oxford-
UCLES system (Sukkarieh, et. al., 2003) at the 
University of Oxford] to data mining tech-
niques using very shallow linguistic features 
[e.g., Sukkarieh and Pulman (2005) and Car-
melTC at Carnegie Mellon University (Rose, 
et al 2003)]. Data mining techniques proved 
not to be very transparent when digging up 
justifications for scores. 
 
c-rater?s model building process is similar to 
generating patterns but the patterns in c-rater 
are written in English instead of a formal lan-
guage. The aim of the process is to produce a 
non-trivial space of possible correct answers 
guided by a subset of the students? answers. 
The motivation is that the best place to look for 
variations and refinements for the rubric is the 
students? answers. This is what test developers 
do before piloting a large-scale exam. From an 
NLP point of view, the idea is that generating 
this space will make scoring an unseen answer 
easier than just having one correct answer. 
However, similar to what other systems re-
ported, generating manually-engineered pat-
terns is very costly. In Sukkarieh et al (2004) 
there was an attempt to generate patterns 
automatically but the results reported were not 
comparable to those using manually-generated 
patterns. This paper presents improvements on 
previous supervised approaches by automating 
the process of model-answer building using 
well-known NLP methods and resources while 
yielding comparable results to knowledge-
engineered methods.  
3 c-rater, in Brief 
In c-rater, manual MB has its own graphical 
interface, Alchemist. MB uses the NLP tools 
and Goldmap (which reside in the c-rater 
Engine). On the other hand, Goldmap depends 
on the model generated. The c-rater Engine 
performs NLP on input text and concept rec-
ognition or TE between the input text and each 
concept (see Figure 1). First, a student answer 
is processed for spelling corrections in an at-
tempt to decrease the noise for subsequent 
NLP tools. In the next stage, parts-of-speech 
tagging and parsing are performed (the 
OpenNLP parser is used 
http://opennlp.sourceforge.net). In the third 
stage, a parse tree is passed through a feature 
extractor. Manually-generated rules extract 
features from the parse tree. The result is a flat 
structure representing phrases, predicates, and 
relationships between predicates and entities. 
Each phrase is annotated with a label indicat-
ing whether it is independent or dependent. 
Each entity is annotated with a syntactic and 
semantic role. In the pronoun resolution 
stage, pronouns are resolved to either an entity 
in the student?s answer or the question. Finally, 
a morphology analyzer reduces words to their 
lemmas.2 The culmination of the above tools 
results in a set of linguistic features used by the 
matching algorithm, Goldmap. In addition to 
the item-independent linguistic features col-
lected by the NLP tools, Goldmap uses item-
dependent features specified in MB to decide 
whether a student?s answer, A, and a model 
                                                 
2 We do not go into detail, assuming that the reader is 
familiar with the described NLP techniques. 
62
answer match, i.e. that concept C represented 
in the model answer, is entailed by A.   
 
 
 
Figure 1. c-rater Engine   
 
4 KE Model Building 
A dataset of student answers for an item is split 
into development (DEV), cross-validation 
(XVAL), and blind (BLIND) datasets. DEV is 
used to build the model, XVAL is used to vali-
date it and BLIND is used to evaluate it. All 
datasets are double-scored holistically by hu-
man raters and the scoring process takes an 
average 3 hours per item for a dataset of 
roughly 200 answers. 
 
For each concept Ci in item X, a model builder 
uses DEV to create a set of Model Sentences 
(MSij) that s/he believes entails concept Ci in 
the context of the item. S/he is required to 
write MSij in complete sentences. For each 
model sentence MSij,, the model builder selects 
the Required Lexicon (RLijk), a set of the most 
essential lexical entities required to appear in a 
student?s answer. Then, for each RLijk, the 
model builder selects a set of Similar Lexicon 
(SLijkt), guided by the list of words automati-
cally extracted from a dependency-based the-
saurus (cs.ualberta.ca/~lindek/downloads.htm).  
 
The process is exemplified in Figure 2. Pre-
sented with the concept, ?What causes rain to 
form in winter time?,? a model builder writes 
model sentences like ?Why does rain fall in 
the winter?,? highlights or selects lexical items 
that s/he believes are the required tokens  
(e.g., ?why,? ?rain,? ?fall,? ?in,? ?winter?) 
and writes a list of similar lexical entities for 
each required token if needed (e.g., {descend, 
go~down, ?} are similar to words like?fall?).3
 
 
 
Figure 2. KE Model Building 
 
The model for each item X is comprised of the 
scoring rules, the collections of model sen-
tences MSij, associated lexical entities RLijk, 
and corresponding similar lexicon SLijkt. Each 
model answer is written in terms of MSij 
where:  
 
MSij entails Ci for i=1,?, N, and N is the 
number of concepts specified for item X. 
For each concept Ci, Goldmap checks 
whether answer A entails Ci, by check-
ing whether A entails one of the model 
sentences MSij, given the additional fea-
tures RLijk and corresponding SLijkt. 
 
In practice, model building works as follows. 
The model builder, guided by the DEV dataset 
and holistic scores, starts with writing a few 
model sentences and selects corresponding 
required (RLijk) and similar (SLijkt) lexicon. 
S/he then uses the c-rater engine to automati-
cally evaluate the model using the DEV data-
set, i.e., using the model produced up to that 
point. Goldmap is used to detect if any answers 
in the DEV dataset contain any of the model 
sentences and scores are assigned for each an-
swer. If the scoring agreement between c-rater 
and each of the two human raters (in terms of a 
kappa statistic) is much lower than that be-
tween the two human raters, then the model is 
judged unsuitable and the process continues 
iteratively until kappa statistics on the DEV 
dataset are satisfactory, i.e., c-rater?s agree-
ment with human raters is as high as the kappa 
between human raters. Once kappa statistics on 
DEV are satisfactory, the model builder uses  
                                                 
3 We use lexicon, lexical entities, words, terms and to-
kens interchangeably meaning either uni- or bi-grams. 
63
c-rater to evaluate the model on the XVAL 
dataset automatically. Again, until the scoring 
agreement between c-rater and human raters 
on XVAL dataset is satisfactory, the model 
builder iteratively changes the model. Unlike 
the DEV dataset, the XVAL dataset is never 
seen by a model builder. The logic here is that 
over-fitting DEV is a concern, making it hard 
or impossible to generalize beyond this set. 
Hence, the results on XVAL can help prevent 
over-fitting and ideally would predict results 
over unseen data. 
    
Note that a model builder can introduce what 
we call a negative concept Ci-1 for a concept Ci 
and adjust the scoring rules accordingly. When 
this happens, a model builder writes model 
sentences MSi-1j  entailing Ci-1 , and selects re-
quired words RLi-1jk and corresponding similar 
words SLi-1jkt  in the same way for any other 
(positive) concept. 
 
On average, MB takes 12 hours of manual 
work per item (plus 2 hours, on average, for an 
optional model review by someone other than 
the model builder). This process is time con-
suming and error-prone despite utilizing a 
user-friendly interface like Alchemist. In addi-
tion, the satisfaction criterion while building a 
model is subjective to the model builder.  
5 Automated Model Building 
The process of writing model sentences de-
scribed above involves: 1) finding the parts of 
students? answers containing the concept for 
each expected concept, 2) abstracting over 
?similar? parts, and 3) representing the abstrac-
tion in one (or more) model sentence(s). The 
process, as mentioned earlier, is similar to 
writing rules for information extraction, but 
here one writes them in English sentences and 
not in a formal language. In practice, there is 
no mechanism in Alchemist to cluster ?simi-
lar? parts and MB, in this aspect, is not per-
formed in any systematic manner. Hence, we 
introduce what we call concept-based scoring 
? used instead of the holistic human scoring. In 
concept-based scoring, human raters annotate 
students? responses for each concept C, and 
highlight the part of the answer that entails C.  
In Sukkarieh and Blackmore (2009), we de-
scribe concept-based scoring in detail and how 
this helps in the KE-MB approach. In this pa-
per, we extend the approach by showing how 
concept-based scores used in the automated 
approach reduce the time needed for MB sub-
stantially while yielding comparable results. 
Concept-based scoring is done manually. On 
average, it takes around 3.5 hours per item for 
a dataset of roughly 200 answers.  
 
The MB process is reduced to: 
  
1. Concept-based scoring 
2. Automatically selecting required lexicon 
3. Automatically selecting similar lexicon 
 
While holistic scoring takes on average 3 hours 
for a dataset of 200 answers, concept-based 
scoring takes 3.5 hours for the same set. How-
ever, automated MB takes 0 hours of human 
intervention?a substantial reduction over the 
12 hours required for manual MB.    
5.1   Concept-based Scoring 
We have developed a concept-based scoring 
interface (CBS) that can be customized for 
each item [due to lack of space we do not in-
clude an illustration].  The CBS interface dis-
plays a student?s answer to an item and all of 
the concepts corresponding to that item. The 
terms {Absent, Present, Negated} are what we 
call analytic or concept-based scores. Using 
CBS, the human scorer clicks Present when a 
concept is present and Negated when a concept 
is negated or refuted (the default is Absent). 
This is done for each concept. The human 
scorer also highlights the part of a student?s 
answer that entails the concept in the context 
of the item. We call a quote corresponding to 
concept C ?Positive Evidence? or ?Negative 
Evidence? for Present and Negated, respec-
tively. For example, assume a student answer 
for Item 1 is ?Her research tells us a lot about 
rain and hail; in particular, the impact that 
temperature variations have on altitude con-
tribute to the formation of rain.? For  
Concept C3, the human rater highlights the 
Positive Evidence, ?the impact that tempera-
ture variations have on altitude contribute to 
the formation of rain.? Parts of answers corre-
sponding to one piece of Evidence (positive or 
negative) do not need to be in the same sen-
tence and could be scattered over a few lines.  
 
Similar to the KE approach, we split the  
double-concept-based scored dataset into DEV 
and XVAL sets. However, the splitting is done 
64
according to the presence (or absence) of a 
concept. We use stratified sampling (Tucker, 
1998) trying to uniformly split data such that 
each concept is represented in the DEV as well 
as the XVAL datasets. As mentioned earlier, 
the KE approach can include negative con-
cepts; currently we do not use Negative Evi-
dence automatically. In the remainder of this 
paper, Evidence is taken to mean the collection 
of Positive Evidence.    
5.2 Automatically Selecting Model  
Sentences 
Motivation
During manual MB with Alchemist, a model 
builder is guided by the complete set of stu-
dents? answers in the DEV dataset, including 
holistic scores. Concept-based scoring allows a 
model builder, if we were to continue the man-
ual MB, to be guided by concept-based scores 
and students? answers highlighted with the 
Evidence that corresponds to each concept 
when writing model sentences as shown, 
where MSij entails Ci and Eir entails Ci. 
 
Concept Ci Evidence Eir MSij
C1 E11 MS11
 E1s1 MS1t1
C2 E21 MS21
 E2s2 MS2t2
Cn ? ? 
 
Further, students may misspell, write ungram-
matically, or use incomplete sentences. Hence, 
Evidence may contain spelling and grammati-
cal errors. Evidence may also be in the form of 
incomplete sentences. Although human model 
builders generating sentences with Alchemist 
are asked to write complete MSij,, there is no 
reason why MSij, needs to be in the form of 
complete sentences. The NLP tools in the  
c-rater engine can cope with a reasonable 
amount of misspelled words as well as un-
grammatical and/or incomplete sentences.  
 
We observe the following: 
 
1. Concepts are seen as a set of model sen-
tences that are subsumed by the list of 
model sentences built by humans 
2. Evidence is seen as a list of model 
?sentences? that nearly subsume the set gener-
ated by humans (i.e., the intersection is not 
empty)   
Approach 
In the automatic approach, we select the Evi-
dence highlighted in the DEV dataset as MSijs. 
We either choose the intersection of Evidence 
(i.e., where both human raters agree) or the 
union (i.e., highlighted by either human) as 
entailing a concept.  
5.3 Automatically Selecting Required 
Lexicon 
Motivation 
Required lexicon for an item includes the most 
essential lexicon for this item. In the KE ap-
proach, the required lexicon is selected by the 
model builder, who makes a judgment about it. 
In Alchemist, a model builder is presented 
with a tokenized model sentence and s/he 
clicks on a token to select it as a required lexi-
cal entity. 
  
We have observed that selecting required lexi-
con RLijk involves ignoring or removing noise, 
such as stop-words (e.g., ?a,? ?the,? ?to,? etc.), 
from the presented model sentence. For exam-
ple, a model builder may select the words, 
?how,? ?rain,? ?formation,? and ?winter? in 
the model sentence ?How does rain formation 
occur in the winter?? and ignore the rest. In 
addition, there might be words other than stop-
words that can be ignored. For example, if a 
model builder writes, ?It may help Alice and 
scientists to know how rain formation occurs 
in the winter? ? the tokens ?scientists? and 
?Alice? are not stop-words and can be ignored.  
Approach 
We evaluate five methods of automatically 
selecting the required lexicon: 
 
1. Consider all tokens in MSij  
2. Consider all tokens in MSij without stop-
words 
3. Consider all heads of NPs and VPs (nouns 
and verbs) 
4. Consider all heads of all various syntactic 
roles including adjectives and adverbs 
5. Consider the lexicon with the highest mu-
tual information measures, with all lexical 
tokens in model sentences corresponding 
to the same concept   
 
65
The first method does not need any elabora-
tion. In the following, we briefly elaborate on 
each of the other methods. 
 
5.3.1 All Words Without Stop Lexicon 
In addition to the list of stop-words provided in 
Van Rijsbergen?s book (Rijsbergen, 2004) and 
the ones we extracted from WordNet 2.0 
http://wordnet.princeton.edu/
(except for ?zero,? ?minus,? ?plus,? and ?op-
posite?), we have developed a list of approxi-
mately 2,000 stop-words based on students? 
data. This includes various interjections and 
common short message service (SMS) abbre-
viations that are found in students? data (see 
Table 1 for examples).  
 
1. Umm 2. Aka 3. Coz 
4. Viz. 5. e.g. 6. Hmm 
7. Phew 8. Aha 9. Wow 
10. Ta 11.Yippee 12. NTHING 
13. Dont know 14. Nada 15. Guess 
16. Yoink 17. RUOK 18. SPK 
Table 1. Student-driven stop-words 
 
5.3.2 Head Words of Noun and Verb 
Phrases  
The feature extractor in c-rater, mentioned in 
Section 2, labels the various noun and verb 
phrases with a corresponding syntactic or se-
mantic role using in-house developed rules. 
We extract the heads of these by simply con-
sidering the rightmost lexical entity with an 
expected POS tag, i.e., for noun phrases we 
look for the rightmost nominal lexical entity, 
for verb phrases we look for the rightmost 
verbs.   
 
5.3.3 Head Words of all Phrases 
We consider all phrases or syntactic roles, i.e., 
not only noun and verb phrases but also adjec-
tive and adverb phrases. 
 
5.3.4 Words with Highest Mutual  
Information  
The mutual information (MI) method measures 
the mutual dependence of two variables. MI in 
natural language tasks has been used for in-
formation retrieval (Manning et. al., 2008) and 
for feature selection in classification tasks 
(Stoyanchev and Stent, 2009).  
 
Here, MI selects words that are indicative of 
the correct answer while filtering out the words 
that are also frequent in incorrect answers. Our 
algorithm selects a lexical term if it has high 
mutual dependence with a correct concept or 
Evidence in students? answers. For each term 
mentioned in a students? answer we compute 
mutual information measure (I): 
 
where N11 is the number of student answers 
with the term co-occurring with a correct con-
cept or Evidence, N01 is the number of student 
answers with a correct concept but without the 
term, N10 is the number of student answers 
with the term but without a correct concept, 
N00 is the number of student answers with nei-
ther the term nor a correct concept, N1. is the 
total number of student answers with the term, 
N.1 is the total number of utterances with a cor-
rect concept, and N is the total number of ut-
terances. The MI method selects the terms or 
words predictive of both presence and absence 
of a concept.  In this task we are interested in 
finding the terms that indicate presence of a 
correct concept. We ignore the words that are 
more likely to occur without the concept (the 
words for which N11< N10). In this study, after 
looking at the list of words produced, we sim-
ply selected the top 40 words with the highest 
mutual information measure.  
5.4 Automatically Selecting Similar  
Lexicon 
Motivation 
In the KE approach, once a model builder se-
lects a required word, a screen on Alchemist 
lists similar words extracted automatically 
from Dekang Lin?s dependency-based thesau-
rus. The model builder can also use other re-
sources like Roget?s thesaurus 
(http://gutenberg.org/etext/22) and WordNet 
3.0 (http://wordnet.princeton.edu/). The model 
builder can also write her/his own words that 
s/he believes are similar to the required word.  
 
Approach 
Other than choosing no similar lexicon to a 
required word W, automatically selecting simi-
66
lar lexicon consists of the following experi-
ments: 
 
1. All words similar to W in Dekang Lin?s 
generated list 
2. Direct synonyms for W or its lemma from 
WordNet 3.0 (excluding compounds). 
Compounds are excluded because we no-
ticed many irrelevant compounds that 
could not replace uni-grams in our data. 
3. All similar words for W or its lemma from 
WordNet 3.0, i.e., direct synonyms, related 
words and hypernyms (excluding com-
pounds). Hypernyms of W are restricted to 
a maximum of 2 levels up from W 
 
To summarize, for each concept in the KE ap-
proach, a model builder writes a set of Model 
Sentences, manually selects Required Lexicon 
and Similar Lexicon for each required word. In 
the automated approach, all of the above is 
selected automatically. Table 2 summarizes the 
methods or experiments. We refer to a method 
or experiment in the order of selection of RLijk 
and SLijkt; e.g., we denote the method where all 
words were required and similar lexicon cho-
sen from WordNet Direct synonyms by AWD. 
HSVocWA denotes the method where heads of 
NPs and VPs with similar words from Word-
Net All, i.e., direct, related, and hypernyms are 
selected.  A method name preceded by I or U 
refers to Evidence Intersection or Union, re-
spectively. For each item, there are 40 experi-
ments/methods performed with Evidence as 
model sentences. 
 
Model 
Sentences Required Lexicon Similar Lexicon 
Concepts  
(C) 
 
All words (A) None chosen (N) 
Evidence 
Intersection 
(I) 
 
All words with no stop-
words (S) 
Lin all (L) 
Evidence 
Union (U) 
Heads of NPs and VPs 
(HSvoc) 
WordNet direct 
synonyms (WD) 
 Heads of all phrases (HA) WordNet al 
similar words 
(WA) 
 Highest Mutual informa-
tion measure (M) 
 
Table 2. Parameters and ?Values? of Model  
Building 
Before presenting the evaluation results, we 
make a note about spelling correction. c-rater 
has its own automatic spelling corrector. Here, 
we only outline how spelling correction relates 
to a model. In the KE approach, model sen-
tences are assumed to not having spelling er-
rors. We use the model sentences, the stimulus 
(if it exists), and the prompt of the item for 
additional guidance to select the correctly-
spelled word from a list of potential correctly-
spelled words designated by the spelling cor-
rector. On the other hand, the Evidence can be 
misspelled. Consequently, when the Evidence 
is considered for model sentences, the spelling 
corrector first performs spelling correction on 
the Evidence, using stimulus, concepts, and 
prompts as guides. The students? answers are 
then corrected, as in the KE approach. 
6 Evaluation 
The study involves 12 test items developed at 
ETS for grades 7 and 8. There are seven Read-
ing Comprehension items, denoted R1-R7 and 
five Mathematics items, denoted M1-M5. 
Score points for the items range from 0 to 3 
and the number of concepts ranges from 2 to 7. 
The answers for these items were collected in 
schools in Maine, USA. The number of an-
swers collected for each item ranges from 190-
264. Answers were concept-based scored by 
two human raters (H1, H2). We split the dou-
ble-scored students? answers available into 
DEV (90-100 answers), XVAL (40-50) and 
BLIND (60-114). Training data refer to DEV 
together with XVAL datasets.  Results are re-
ported in terms of un-weighted kappa, repre-
senting scoring agreement with humans on the 
BLIND dataset.  H1/2 refers to the agreement 
between the two humans, c-H1/2 denotes the 
average of kappa values between c-rater and 
each human (c-H1 and c-H2). Table 3 reports 
the best kappa over the 40 experiments on 
BLIND (Auto I or U). The baseline (Auto C) 
uses concepts as model sentences.  
 
Item 
#Training 
(Blind) H1/2 Manual 
Auto 
C 
Auto 
I or U 
   c-H1/2 c-H1/2 c-H1/2 
R1 150  (114) 1.0    0.94   0.51 0.97 
R2 150  (113) 0.76    0.69   0.28 0.76 
R3 150  (107) 0.96    0.87   0.18 0.88 
R4 150    (66) 0.77    0.71   0.46 0.75 
R5 130    (60) 0.71    0.58   0.22 0.61 
R6 130    (61) 0.71    0.73   0.23 0.77 
R7 130    (61) 0.87    0.55   0.42 0.42 
M1 130    (67) 0.71      0.6   0.0 0.66 
M2 130    (67) 0.8     0.71   0.54 0.67 
M3 130    (67) 0.86    0.76   0.0 0.79 
M4 130    (67) 0.87    0.82   0.13 0.82 
M5 130    (67) 0.77    0.63   0.29 0.65 
Table 3. Best on BLIND over all experiments 
67
The accuracy using the automated approach 
with Evidence as model sentences is compara-
ble to that of the KE approach (noted in the 
column labeled, ?Manual?) with a 0.1 maxi-
mum difference in un-weighted kappa statis-
tics. The first methods (in terms of running 
order) yielding the best results for the items (in 
order of appearance in Table 3) are ISWD, 
ISW, ISN, IMN, IHSVocN, UHALA, ISN, 
UHSVocN, SLA, ISN, IHAN and IHS-
VocWA. The methods yielding the best results 
(regardless of running order) for all items us-
ing the Evidence were: 
IHAN U/IHAWD IHAWA 
U/IHALA U/IHSvocN IHSvocWA 
UHSvocLA UHSvocWA UHSvocWD 
U/ISLA U/ISN U/ISWA 
U/ISWD U/IAWA IMN 
IMWD   
This approach was only evaluated on a small 
number of items. We expect that some meth-
ods will outperform others through additional 
evaluation.  
In an operational setting (i.e., not a research 
environment), we must choose a model before 
we score the BLIND data. Hence, a voting 
strategy over all the experiments has to be de-
vised based on the results on DEV and XVAL. 
Following our original logic, i.e., using XVAL 
to avoid over-fitting and predicting the results 
of BLIND, we implemented a simple voting 
strategy. We considered c-H1/2 on XVAL for 
each experiment. We found the maximum over 
all the c-H1/2 for all experiments. The model 
corresponding to the maximum was considered 
the model for the item and used to score the 
BLIND data.  When there was a tie, the first 
method to yield the maximum W chosen.  
Table 4 shows the results on BLIND using the 
voting strategy. The results are comparable to 
those of the manual approach except for R7 
which has 7 concepts, the highest number of 
concepts among all items. The results also 
show that the voting strategy did not select the 
?best? model or experiment. We notice that 
some methods were better in detecting whether 
an answer entailed a concept C than detecting 
whether it entailed another  
concept D, specified for the same item. This 
implies that the voting strategy will have to be 
a function that not only considers the overall 
kappa agreement (i.e., holistic scores), but 
concept-based agreement (i.e., using concept-
based scores).  Next, we noticed that for R7, 
XVAL did not predict the results on BLIND. 
This was mainly due to the inability to apply 
stratified sampling with such a small sample 
size when there are 7 concepts involved. Fur-
ther, we may need to take advantage of the 
training data differently, e.g. an n-fold cross-
validation approach. Finally, when there is a 
tie, factors other than running order should be 
considered. 
 
Item 
#Training 
(Blind) H1/2 Manual 
Auto 
(C) 
Auto 
(I or U) 
   c-H1/2 c-H1/2 c-H1/2 
R1 150  (114) 1.0    0.94   0.51 0.88 
R2 150  (113) 0.76   0.69   0.18 0.61 
R3 150  (107) 0.96   0.87   0.18 0.86 
R4 150    (66) 0.77   0.71   0.38 0.67 
R5 130    (60) 0.71   0.58   0.17 0.51 
R6 130    (61) 0.71   0.73   0.13 0.73 
R7 130    (61) 0.87   0.55   0.39 0.16 
M1 130     67) 0.71    0.6    0.0 0.65 
M2 130     67) 0.8    0.71   0.54 0.58 
M3 130     67) 0.86   0.76   0.0 0.79 
M4 130     67) 0.87   0.82   0.13 0.68 
M5 130     67) 0.77   0.63   0.26 0.49 
Table 4. Voting Strategy results on BLIND 
In all of the above experiments, the Evidence 
was corrected using the c-rater?s automatic 
spelling corrector using the stimulus (in case of 
Reading), the concepts, and the prompts to 
guide the selection of the correctly-spelled 
words. 
7 Conclusion 
Analytic-based content scoring is an applica-
tion of textual entailment. The complexity of 
the problem increases due to the noise in stu-
dent data, the context of an item, and different 
subject areas. In this paper, we have shown 
that building a c-rater scoring model for an 
item can be reduced from 12 to 0 hours of hu-
man intervention with comparable scoring per-
formance. This is a significant improvement on 
research to date using supervised techniques.  
In addition, as far as we know, no one other 
than Calvo et al (2005) made any comparisons 
between a manually-built ?thesaurus? (e.g. 
WordNet) and an automatically-generated 
?thesaurus? (e.g. Dekang Lin?s database) in an 
NLP task or application prior to our work. Our 
next step is to evaluate (and refine) the ap-
proach on a larger set of items. Further im-
provements will include using Negative Evi-
dence, automating concept-based scoring, in-
vestigating a context-sensitive selection of 
similar words using the students? answers and 
experimenting with various voting strategies. 
Finally, we need to compare the results re-
ported using unsupervised techniques on the 
same items and datasets if possible.   
68
Acknowledgments 
Special thanks to Michael Flor, Rene Lawless, 
Sarah Ohls and Waverely VanWinkle. 
References 
Calvo H., Gelbukh A., and Kilgarriff A. (2005). 
Distributional thesaurus vs. WordNet: A com-
parison of backoff techniques for unsupervised 
PP attachment. In CICLing.  
Christie, J.R. (1999). Automated essay marking for 
both content and style. In Proceedings of the 3rd 
International Computer Assisted Assessment 
Conference. Loughborough University. 
Loughborough, Uk. 
Foltz, P.W. and Laham, D. and Landauer, T.K. 
(2003) Automated essay scoring. Applications to 
Educational technology. http://www-
psych.nmsu.edu/%7Epfoltz/reprints/Edmedia99.
html 
Leacock, C. and Chodorow, M. (2003) C-rater: 
Automated Scoring of Short-Answer Questions. 
Computers and Humanities. pp.  389-405 
Manning C. D., Raghavan P., and Sch?utze H. 
(2008). Introduction to Information Retrieval. 
Cambridge University Press. 
Mitchell, T. and Russel, T. and Broomhead, P. and 
Aldrige, N. (2002) Towards robust computerised 
marking of free-text responses. Proceedings of 
the 6th International Computer Assisted As-
sessment Conference. 
Mohler M. and Mihalcea R (2009). Text-to-text 
Semantic Similarity for Automatic Short Answer 
Grading. Proceedings of the European Chapter 
of the Association for Computational Linguis-
tics, Athens, Greece, March 2009. 
Ros?, C. P. and Roque, A. and Bhembe, D. and 
VanLehn, K.. (2003) A hybrid text classification 
approach for analysis of student essays. Proceed-
ings of the HLT-NAACL 03 Workshop on Edu-
cational Applications of NLP.  
Stoyanchev S. and Stent A. (2009). Predicting Con-
cept Types in User Corrections in Dialog. Pro-
ceedings of EACL Workshop on the Semantic 
Representation of Spoken Language. Athens, 
Greece. 
Sukkarieh, J. Z., and Blackmore, J. (2009). c-rater: 
Automatic Content Scoring for Short Con-
structed Responses. Proceedings of the 22nd In-
ternational Conference for the Florida Artificial 
Intelligence Research Society, Florida, USA. 
Sukkarieh, J.Z. and Stephen G. Pulman (2005). 
Information Extraction and Machine Learning: 
Auto-marking short free-text responses for Sci-
ence questions. Proceedings of the 12th Interna-
tional conference on Artificial Intelligence in 
Education, Amsterdam, The Netherlands. 
Sukkarieh, J.Z. Pulman S. G. and Raikes, N. 
(2004). Auto-marking 2: An update on the 
UCLES-Oxford University research into using 
computational linguistics to score short, free text 
responses. Proceedings of the AIEA, Philadel-
phia, USA. 
Sukkarieh, J. Z. and Pulman, S. G. and Raikes, N. 
(2003) Auto-marking: using computational lin-
guistics to score short, free text responses.  
Proceedings of international association of 
educational assessment. Manchester, UK. 
Tucker H. G. (1998) Mathematical Methods in 
Sample Surveys. Series on multivariate analysis 
Vol. 3. University of California, Irvine.  
Van Rijsbergen C. J. ( 2004) The Geometry of In-
formation Retrieval. Cambridge University 
Press.  The Edinburgh Building, Cambridge, 
CB2 2RU, UK. 
Vantage. (2000) A study of expert scoring and In-
telliMetric scoring accuracy for dimensional 
scoring of grade 11 student writing responses. 
Technical report RB-397, Vantage Learning 
Tech. 
   
 
 
 
69
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 144?147,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Concept Form Adaptation in Human-Computer Dialog
Svetlana Stoyanchev and Amanda Stent
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400, USA
svetastenchikova@gmail.com, amanda.stent@gmail.com
Abstract
In this work we examine user adaptation
to a dialog system?s choice of realiza-
tion of task-related concepts. We ana-
lyze forms of the time concept in the Let?s
Go! spoken dialog system. We find that
users adapt to the system?s choice of time
form. We also find that user adaptation
is affected by perceived system adapta-
tion. This means that dialog systems can
guide users? word choice and can adapt
their own recognition models to gain im-
proved ASR accuracy.
1 Introduction
Considerable research has now demonstrated that
human dialog partners exhibit lexical and syntac-
tic convergence; that is, that in a human-human
conversation the participants become more simi-
lar in their use of language over time (Brennan
and Clark, 1996; Lockridge and Brennan, 2002;
Pickering and others, 2000; Reitter et al, 2006).
Several Wizard-of-Oz studies have also shown ev-
idence of convergence in human-computer dia-
log (Branigan and others, 2003; Brennan, 1996;
Gustafson and others, 1997).
In recent work, we examined user adaptation1
to the system?s choice of verb and preposition us-
ing the deployed Let?s Go! spoken dialog system
(Stoyanchev and Stent, 2009a). This was the first
study to look at convergence with real users of a
real dialog system and examined user adaptation
to verbs and prepositions. The study described
in this paper is a follow-on to our previous study.
1In this paper, we use the term adaptation to indicate di-
rectional convergence, e.g. user adaptation to a system. We
make no claims about the psycholinguistic models underly-
ing this adaptation.
Here we look at user adaptation to the system?s
choice of realization of task-related concepts. In
this paper, we: (1) Confirm our previous results
showing that users adapt to the system?s choice of
words, using transcribed data rather than ASR out-
put; (2) Show that adaptation can persist over time;
and (3) Show that user adaptation is strengthened
by perceived system adaptation.
In addition to providing further evidence of con-
vergence in human-computer dialog, our results
have implications for dialog system design. Cur-
rently, much dialog systems research is devoted to
improving ASR accuracy, because this is a signifi-
cant contributor to task success rates and to dialog
length. One way to improve ASR accuracy is to
use targeted language models. Since users adapt to
the system?s choices of realization for task-related
concepts, we can predict the user?s choice of real-
ization and use this to adjust the ASR?s language
model, improving ASR accuracy specifically on
concept words. Another way to improve ASR ac-
curacy is to guide the user into using words that are
likely to be recognized correctly (Hockey and oth-
ers, 2003; Sheeder and Balogh, 2003; Tomko and
Rosenfeld, 2006). Our results imply that if the de-
signer of a dialog system wants to improve ASR
accuracy, system prompts should be designed to
use word choices that are more recognizable; and,
when, possible, to be adaptive to the user?s choice
of form for task-related concepts.
2 System
We conducted our experiment using the Let?s Go!
deployed telephone-based spoken dialog system
which provides information about bus routes in
Pittsburgh (Raux and others, 2005). Let?s Go! an-
swers the phones at the transit authority?s website
outside of normal business hours. Its users are
144
SYS: Welcome to the CMU Let?s Go! Bus Information
System. How may I help you?
USR: I?d like to go to Forbes and Murray.
SYS: To Forbes and Murray. Where are you leaving
from?
USR: Carnegie Mellon.
SYS: From Carnegie Mellon. What time would you like
to leave?
USR: Ten thirty p.m.
SYS: Leaving at ten thirty p. m.?
USR: Yes
SYS: There are five buses running between Carnegie
Mellon and Forbes and Murray. . .
Figure 1: Sample dialog with Let?s Go!
Time form Example realizations Frequency
TIME four, five, six thirty. . . 31.1%
APM four a.m., ten p.m., one
fifteen p. m.
43.5%
POD four in the morn-
ing/evening/afternoon/
4.6%
OCLOCK five o?clock 16%
OTHER four o?clock p. m., sixteen
hundred hours
4.8%
Table 1: Time forms in users? utterances and their
relative frequencies in one month of Let?s Go!
2006 data.
naive callers from the general population with a
real task to perform. In order to provide bus route
information, Let?s Go! elicits values for several
task-related concepts: an optional bus route num-
ber, a departure place, a destination and a desired
travel time. Each concept is explicitly confirmed.
Figure 1 shows a sample dialog with the system.
In this work we investigate adaptation to the
time concept because it has multiple different re-
alizations, as shown in Table 1. This variability
is not unique to time; however, it is the only task-
related concept in Let?s Go! that is not usually
realized using named entities (which exhibit less
variability).
3 Method
In order to study adaptation, we need to identify a
prime, a point in the conversation where one part-
ner introduces a realization. In Let?s Go! the sys-
tem always asks the user to specify a departure
time. The user then typically says a time, which
the system confirms (see Figure 1). We simulate
an ASR error on the user?s response to the sys-
tem?s time request, so that when the system con-
firms the departure time it confirms a time other
than that recognized in the user?s response. To
make the system?s error more realistic, the time
in the simulated error is a time that is phonetically
close to the time (hour and minute) recognized in
the user?s response. The system?s confirmation
prompt is our prime.
The system runs in one of the three condi-
tions: SYS TIME, SYS APM, or SYS POD. In
each condition it uses the corresponding time for-
mat (TIME, APM, or POD as shown in Table 1).
TIME is the most frequent form in the 2006 Let?s
Go! corpus, but it is potentially ambiguous as it
can mean either night or day. APM is the shortest
unambiguous form. POD is longer and has a very
low frequency in the 2006 Let?s Go! corpus.2
We collected approximately 2000 dialogs with
Let?s Go! using this setup. We used the ASR
output to identify dialogs where a time appears
in the ASR output at least twice3. We manually
transcribed 50 dialogs for each experimental con-
dition. Some of these turned out not to contain
mentions of time either before or after the system?s
time confirmation prompt, so we excluded them.
We examine whether the user adapts to the
system?s choice of form for realizing the time
concept, both in the first time-containing post-
confirmation utterance, and in the rest of the dialog
(until the user hangs up or says ?New query?).
4 Results
In this section we first examine user adaptation to
system?s choice of time expression, and then look
at how perceived system adaptation affects user
adaptation.
4.1 User adaptation to system time form
If the user adapts to the system?s time form, then
we would expect to see a greater proportion of the
system?s time form in user utterances following
the prime. We compare the proportion of three
time forms (APM, TIME, and POD) in each sys-
tem condition for 1) Unprimed, 2) First After, and
3) All After user?s utterances, as shown in Table 2.
Unprimed utterances are the user?s time specifica-
tion immediately prior to the prime (the system?s
confirmation prompt). First After utterances are
user utterances immediately following the prime.
All After utterances are all user utterances from the
prime until the user either hangs up or says ?New
2We would have liked to also include OCLOCK in the
experiment. However, due to resource limitations we had to
choose only three conditions.
3The most frequent user response to the system?s request
to specify a departure time is ?Now?; we exclude these from
our experiment.
145
Unprimed
system/user Usr:APM Usr:TIME Usr:POD
SYS APM 25% 42% 8%
SYS TIME 30% 52% 2%
SYS POD 24% 49% 4%
First After
system/user Usr:APM Usr:TIME Usr:POD
SYS APM 49% 29% ? 2%
SYS TIME 21% ? 58% 0%
SYS POD 29% 45% 5%
All After
system/user Usr:APM Usr:TIME Usr:POD
SYS APM 63% 19% ? 3%
SYS TIME 21% ? 50% 2%
SYS POD 37% ? 38% 4%
Table 2: Proportions of time forms in different
system prompt conditions. The highest propor-
tion among system conditions for each time form
is highlighted. Occurrences of time forms other
than the three examined time forms are excluded
from this table. ? indicates a statistically signif-
icant difference from the highlighted value in the
column (p < .05 with Bonferroni adjustment). ?
indicates a statistically significant difference from
the highlighted value in the column (p < .01 with
Bonferroni adjustment).
query?. To test the statistical significance of our
results we perform inference on proportions for a
large sample.
APM There are no statistically significant differ-
ences in the proportions of Usr:APM4 forms in
Unprimed utterances for the different system con-
ditions. The proportion of Usr:APM forms in
First After utterances is significantly higher in the
SYS APM condition than in the SYS TIME con-
dition (p < .01), although not significantly dif-
ferent than in the SYS POD condition. The pro-
portion of Usr:APM forms in the All After ut-
terances is significantly higher in the SYS APM
condition than in both the SYS TIME and the
SYS POD conditions (p < .01). We conclude that
there is user adaptation to system time form in the
SYS APM condition.
TIME There are no statistically significant dif-
ferences in the proportions of Usr:TIME forms in
Unprimed utterances for the different system con-
ditions. The proportions of Usr:TIME forms in the
First After utterances in the SYS TIME condition
is significantly higher than that in the SYS APM
condition (p < .01), but not significantly higher
than that in the SYS POD condition. The same
is true of Usr:TIME forms in the All After utter-
4Usr:time-form refers to the occurrence of the time-form
in a user?s utterance.
condition keep adapt switch total
adaptive 81.8% - 18.2% 33
non-adaptive 37.5% 29.1% 35.4% 48
Table 3: Proportions of user actions in First After
confirmation utterances
ances. We conclude that there is user adaptation to
system time form in the SYS TIME condition.
POD We did not find statistically significant dif-
ferences in Usr:POD forms for the different sys-
tem conditions in either the Unprimed, First After
or All After data. Because this is the long unam-
biguous form, users may have felt that it would
not be recognized or that it would be inefficient to
produce it.
Figures 2 illustrates the effect of user adaptation
on time form for the SYS APM and SYS TIME
conditions.
4.2 The effect of system adaptation on user
adaptation
Sometimes the user happens to use the same form
in their initial specification of time that the system
uses in its confirmation prompt. This gives the il-
lusion that the system is adapting its choice of time
form to the user. We examined whether users? per-
ception of system adaptation affected user adapta-
tion in First After confirmation utterances.
For this analysis we used only the dialogs in
the SYS APM and SYS TIME conditions since
the POD form is rare in the Unprimed utterances.
We distinguish between three possible user actions
following the system?s confirmation prompt: 1)
keep - use the same form as in the unprimed ut-
terance; 2) adapt ? switch to the same form as in
the system?s confirmation prompt; and 3) switch -
switch to a different form than the one used in the
system?s confirmation prompt or in the unprimed
utterance.
Table 3 shows the proportions for each possible
user action. In the adaptive condition users are
twice as likely to keep the time form than in the
non-adaptive condition (81.8% vs. 37.5%). This
difference is statistically significant (p < .001).
In the non-adaptive system condition users who
change time form are slightly more likely to switch
(35.4%) than to adapt (29.1%).
These results suggest that when the system does
not adapt to the user, the user?s choice is unpre-
dictable. However, if the system adapts to the
user, the user is likely to keep the same form. This
146
Figure 2: User Utterances with TIME APM and TIME ONLY.
means that if the system can adapt to the user when
the user chooses a form that is more likely to be
recognized correctly, that provides positive rein-
forcement, making the user more likely to use that
felicitous form in the future. Furthermore, if the
system does adapt to the user then it may be pos-
sible with high accuracy to predict the user?s form
for subsequent utterances, and to use this infor-
mation to improve ASR accuracy for subsequent
utterances (Stoyanchev and Stent, 2009b).
5 Conclusions and Future Work
In this paper, we analyzed user adaptation to a dia-
log system?s choice of task-related concept forms.
We showed that users do adapt to the system?s
word choices, and that users are more likely to
adapt when the system appears to adapt to them.
This information may help us guide users into
more felicitous word choices, and/or modify the
system to better recognize anticipated user word
choices. In future work we plan to analyze the
effect of ASR adaptation to user word choice on
speech recognition performance in spoken dialog.
References
H. Branigan et al 2003. Syntactic alignment between
computers and people: The role of belief about men-
tal states. In Proceedings of the 25th Annual Confer-
ence of the Cognitive Science Society.
S. Brennan and H. Clark. 1996. Conceptual pacts and
lexical choice in conversation. Journal of Experi-
mental Psychology, 22(6):1482?1493.
S. Brennan. 1996. Lexical entrainment in spontaneous
dialog. In Proceedings of ISSD, pages 41?44.
J. Gustafson et al 1997. How do system questions
influence lexical choices in user answers? In Pro-
ceedings of Eurospeech.
B. Hockey et al 2003. Targeted help for spoken dia-
logue systems: intelligent feedback improves naive
users performance. In Proceedings of EACL.
C. Lockridge and S. Brennan. 2002. Addressees?
needs influence speakers? early syntactic choices.
Psychonomics Bulletin and Review, 9:550?557.
M. Pickering et al 2000. Activation of syntactic prim-
ing during language production. Journal of Psy-
cholinguistic Research, 29(2):205?216.
A. Raux et al 2005. Let?s go public! taking a spoken
dialog system to the real world. In Proceedings of
Eurospeech.
E. Reitter, J. Moore, and F. Keller. 2006. Priming of
syntactic rules in task-oriented dialogue and sponta-
neous conversation. In Proceedings of CogSci.
T. Sheeder and J. Balogh. 2003. Say it like you mean
it: Priming for structure in caller responses to a spo-
ken dialog system. International Journal of Speech
and Technology, 6:103?111.
S. Stoyanchev and A. Stent. 2009a. Lexical and syn-
tactic priming and their impact in deployed spoken
dialog systems. In Proceedings of NAACL.
S. Stoyanchev and A. Stent. 2009b. Predicting concept
types in user corrections in dialog. In Proceedings of
the EACL Workshop on the Semantic Representation
of Spoken Language.
S. Tomko and R. Rosenfeld. 2006. Shaping user input
in speech graffiti: a first pass. In Proceedings of
CHI.
147
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 333?336,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Generating Expository Dialogue from Monologue:
Motivation, Corpus and Preliminary Rules
Paul Piwek
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
p.piwek@open.ac.uk
Svetlana Stoyanchev
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
s.stoyanchev@open.ac.uk
Abstract
Generating expository dialogue from mono-
logue is a task that poses an interesting and re-
warding challenge for Natural Language Pro-
cessing. This short paper has three aims:
firstly, to motivate the importance of this
task, both in terms of the benefits of ex-
pository dialogue as a way to present in-
formation and in terms of potential applica-
tions; secondly, to introduce a parallel cor-
pus of monologues and dialogues which en-
ables a data-driven approach to this challenge;
and, finally, to describe work-in-progress on
semi-automatic construction of Monologue-
to-Dialogue (M2D) generation rules.
1 Introduction
The tasks of text generation ? e.g., Reiter et al
(2005) and Demir et al (2008) ? and generation
in dialogue ? e.g., Stent (2002) and DeVault et al
(2008) ? are central topics in Natural Language Gen-
eration (NLG). What sets the two tasks apart is the
interactive nature of dialogue, where participants
need to adapt their contributions to each other.
This paper introduces an NLG task, the genera-
tion of expository dialogue, to the Computational
Linguistics community which occupies the middle
ground between these two tasks. An expository di-
alogue is an authored conversation between two fic-
tive characters. It can be presented as text, audio or
film. Although there is no real-time interactivity, in
expository dialogue the contributions of the charac-
ters do need to mesh with each other. The main pur-
pose of expository dialogue is to present informa-
tion (a description, explanation or definition) to the
reader, hearer or viewer, in contrast with dramatic
dialogue, which tells a story.
The use of expository dialogue goes back as far as
Plato (c. 470-399 BC), who expressed his ideas as
dialogues between Socrates and his contemporaries.
Recently, a number of empirical studies show that
for some purposes expository dialogue has advan-
tages over monologue: for learners, dialogue can be
more memorable, stimulate them to formulate their
own questions (Craig et al, 2000), and get them to
talk with each other (Lee et al, 1998). Expository
dialogue has also been found to be more effective
for persuasion (Suzuki and Yamada, 2004).
Additionally, dialogue lends itself very well
for multimedia presentations by computer-animated
agents (Andre? et al, 2000; van Deemter et al,
2008). Potential application domains include ed-
ucation, (serious) games and E-Health. In educa-
tion, information from textbooks could be presented
in dialogue form, possibly using virtual reality plat-
forms such as Second Life. Automatically gener-
ating dialogue from text for non-player characters
could have a tremendous impact on the gaming in-
dustry; e.g., (IGDA Game Writers SIG, 2003) state
that the amount of dialogue script for a character-
driven computer game is usually many times that
for the average film. In connection with E-health,
consider patient information leaflets, which are of-
ten left unread; presenting them as movies between
a virtual pharmacist and client may help address this.
Thus instead of being presented with
(1) a. You can take aspirin,
b. if you have a headache.
333
c. Though aspirin does have side effects:
d. it can harm circulation.
the patient could watch a movie on their mobile de-
vice of an exchange between a virtual client (lay-
man, L) and pharmacist (expert, E):
(2) L: What if I have a headache?
E: You can take aspirin
L: But does it have side effects?
E: Yes, it can harm circulation.
So far, research on generating expository dialogue
has been firmly rooted in classical AI approaches.
Work in this area starts from knowledge represen-
tations or databases (Andre? et al, 2000), and even
research that does take text as input ? e.g., Piwek
et al (2007) describe a system for generating di-
alogues such as Example 2 ? relies on handcrafted
rules. Two challenges present themselves for NLP
research: 1) generation of expository dialogue from
text, and 2) use of data-driven, rather than manually
authored, generation rules.
Apart from the cost of manually authoring gener-
ation rules, previous research has found that human-
authored rules can result in ?too much information
[being] given too quickly? (Williams et al, 2007),
which can be addressed by conversational padding.
We argue that rather than trying to invent padding
rules, the best strategy is to learn rules automatically
from professionally authored dialogues.
2 The CODA Corpus
To make inroads into data-driven dialogue genera-
tion, we first need to have the necessary resources.
We propose to view Monologue-to-Dialogue (M2D)
generation as analogous to machine translation; con-
sequently we need a parallel corpus for learning
mappings from the source (monologue) to the tar-
get (dialogue) texts. In the ongoing CODA1 project
we have created such a corpus. It consists of profes-
sionally authored dialogues2 that have been aligned
with monologues (written by ourselves) expressing
the same information. Since our ultimate aim is to
generate dialogues that resemble those written by
1COherent Dialogue Automatically generated from text
2Most dialogues are from the Gutenberg library to facilitate
our planned release of the corpus to the research community.
Sp Dialog act Dialogue Turn Monologue
E: Complex
Question
When you have
a pain in your
foot, how do
you know it?
When you
have a pain in
your foot (i)
you know it
because you
L: Explain I feel it. can feel it. (ii)
E: Explain-
Contradict
But you do not
feel it until a
nerve reports
the hurt to the
brain.
But you do not
feel it until a
nerve reports
the hurt to the
brain. (iii)
E: YN-
Question
Yet the brain is
the seat of the
mind , is it not?
Yet the brain is
the seat of the
mind. (iv)
Table 1: Parallel Monologue and Dialogue Example from
Mark Twain?s ?What is Man??
acclaimed authors, we started with professionally
authored dialogues and created the corresponding
monologues. From a practical point of view, it was
more feasible to use existing dialogue by acclaimed
authors than to hire professional authors to write di-
alogue based on monologues.
We have annotated both dialogues and mono-
logues: dialogue with dialogue acts and monologue
with discourse relations.3 We achieved 91% agree-
ment on segmentation and kappa=.82 for dialogue
act annotation on 11 dialogue act tags. We devel-
oped a D2MTranslation tool for monologue author-
ing, segmentation and dialogue annotation.
In January 2010, the corpus included 500 turns
from ?What is man??, a dialogue by Mark Twain,
and 88 turns from ?Evolving Algebras?, an aca-
demic paper in the form of dialogue by Yuri Gure-
vich.4 Both of these expository dialogues present
conversation between an expert (Old Man in Twain
and Author in Gurevich) and a layman (Young Man
in Twain and Quisani in Gurevich). Table 1 shows
an example of a dialogue fragment, aligned mono-
logue and dialogue act annotations. The discourse
structure of the monologue is depicted in Figure 1.
Table 2 shows the distribution of the dialogue acts
between expert and layman. In both dialogues, the
3See (Stoyanchev and Piwek, 2010) for details.
4In addition to these dialogues we are working on a dialogue
by Berkeley (Three Dialogues between Hylas and Philonous)
and a selection of shorter fragments (for copyrights reasons) by
authors such as Douglas Hofstadter and Paul Feyerabend.
334
Figure 1: Discourse structure of the monologue in Table 1
most frequent dialogue act is Explain, where a char-
acter presents information (as a new idea or as a re-
sponse to another utterance). Also, in both dialogues
the layman asks more often for clarification than
the expert. The distribution over information re-
quests (yes/no, factoid, and complex questions) and
responses (yes, no, factoid) differs between the two
dialogues: in Twain?s dialogue, the expert mostly
requests information and the layman responds to re-
quests, whereas in Gurevich?s dialogue it is the other
way around.
The differences in style suggests that the M2D
mapping rules will be author or style-specific. By
applying M2D rules obtained from two different au-
thors (e.g., Twain and Gurevich) to the same text
(e.g., the aspirin example) we can generate two dif-
ferent dialogues. This will enable us to vary the pre-
sentation style of automatically generated dialogues.
Twain Gurevich
Tag Expert Layman Expert Layman
Explain 69 55 49 24
Clarify 1 15 0 6
Request 60 26 2 29
Response 14 43 9 0
Table 2: Dialogue act tag frequencies for expert and lay-
man in a sample of 250 turns from Twain and 88 turns
from Gurevich dialogues.
3 Rules
We automatically derive M2D rules from the aligned
discourse relations and dialogue acts in our parallel
corpus of monologues and dialogues. Table 3 shows
three rules generated from the parallel dialogue?
monologue fragment in Table 1. The first rule, R1,
is based on the complete discourse structure of the
monologue (i?iv), whereas R2 and R3 are based on
only a part of it: R2 is based on i?iii, whereas R3 is
based on i and ii. By generating rules from subtrees
of a discourse structure, we obtain several rules from
a single dialogue fragment in the corpus.
Condition Elaboration
b a dc
Contrast
Condition Elaboration
b a dc
Condition
b a
Contrast
c ? d
(1)
(2)
Elaboration
dc
Contrast
a ? b
(4)
(3)
Figure 2: Discourse structures of the monologue in Ex-
ample 1. a-b and c-d indicate a concatenation of two
clauses.
Let us illustrate the use of such rules by applying
them to Example 1 about aspirin. The relations be-
tween the clauses of the example are depicted in Fig-
ure 2 (1). To generate a dialogue, we apply a match-
ing M2D rule. Alternatively, we can first simplify
the discourse structure of the monologue by remov-
ing relation nodes as illustrated in Figure 2 (2?4).
The simplified structure in Figure 2 (2) matches
rule R2 from Table 3. By applying R2 we gener-
ate the dialogue in Table 4: the expert asks a com-
plex question composed of clauses a and b, which
the layman answers with an explanation generated
from the same set of clauses. Then the expert offers
a contradicting explanation generated from c and d.
To generate dialogue sentences for a corresponding
discourse structure we are adapting the approach to
paraphrasing of Barzilay and McKeown (2001).
4 Conclusion
This short paper presented three angles on the
Monologue-to-Dialogue (M2D) task. First, as an
opinion piece, it motivates the task of generating ex-
pository dialogue from monologue. We described
empirical research that provides evidence for the
effectiveness of expository dialogue and discussed
applications from education, gaming and E-health.
Second, we introduced the CODA corpus for ad-
dressing the task. Finally, we reported on work-
in-progress on semi-automatic construction of M2D
rules. Our implemented algorithm extracts several
M2D rules from the corpus that are applicable even
to a relatively simple input. Additionally, frequency
analysis of dialogue tags suggests that there is scope
for generating different dialogue styles.
The timeliness of this research is evidenced by the
emergence of a Question Generation (QG) commu-
335
ID Dialogue Structure Monologue Structure
R1 E: Complex Question (i-ii) Contrast (Contrast (Condition(i,ii), iii, iv))
L: Explain (i-ii)
E: Explain-Contradict (iii)
E: YNQuestion (iv)
R2 E: Complex Question (i-ii) Contrast (Condition(i,ii), iii)
L: Explain(i-ii)
E: Explain-Contradict (iii)
R3 E: Complex Question (i-ii) Condition (i,ii)
L: Explain (i-ii)
Table 3: Monologue-to-Dialogue rules extracted from the parallel example in Table 1
Sp Dialogue act Dialogue Turn
E: Complex Ques-
tion a-b
If you have a headache, what
do you do?
L: Explain a-b Take aspirin.
E: Explain-
Contradict
c-d
But aspirin does have side
effects: it can harm circula-
tion
Table 4: A dialogue generated from the monologue about
aspirin by applying the rule R2 (see Table 3)
nity. QG is a subtask of M2D. The first QG work-
shop was held at the end of 2008, resulting in pro-
posals for a Shared Task and Evaluation Campaign
(Rus and Graesser, 2009) for 2010. The CODA cor-
pus should prove to be a useful resource not only for
M2D researchers, but also for the QG community.
Acknowledgments
The research reported in this paper was funded by
the UK Engineering and Physical Sciences Research
Council under grant EP/G/020981/1.
References
E. Andre?, T. Rist, S. van Mulken, M. Klesen, and
S. Baldes. 2000. The automated design of believable
dialogues for animated presentation teams. In Em-
bodied Conversational Agents, pages 220?255. MIT
Press, Cambridge, Mass.
R. Barzilay and K. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In Proc. of ACL/EACL,
Toulouse.
S. Craig, B. Gholson, M. Ventura, A. Graesser, and the
Tutoring Research Group. 2000. Overhearing dia-
logues and monologues in virtual tutoring sessions.
International Journal of Artificial Intelligence in Ed-
ucation, 11:242?253.
S. Demir, S. Carberry, and K. McCoy. 2008. Generating
Textual Summaries of Bar Charts . In Procs of INLG
2008, Ohio, June.
D. DeVault, D. Traum, and R. Artstein. 2008. Making
Grammar-Based Generation Easier to Deploy in Dia-
logue Systems. In Procs SIGdial 2008, Ohio, June.
J. Lee, F. Dinneen, and J. McKendree. 1998. Supporting
student discussions: it isn?t just talk. Education and
Information Technologies, 3:217?229.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. T2D: Generating Dialogues between Virtual
Agents Automatically from Text. In Intelligent Virtual
Agents, LNAI 4722, pages 161?174. Springer Verlag.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005.
Choosing Words in Computer-Generated Weather
Forecasts. Artificial Intelligence, 167:137?169.
V. Rus and A. Graesser, editors. 2009. The Ques-
tion Generation Shared Task and Evaluation Chal-
lenge. The University of Memphis. Available at:
http://www.questiongeneration.org/.
IGDA Game Writers SIG. 2003. International game
developers association?s (IGDA) guide to writing for
games. IGDA White Paper.
A. Stent. 2002. A conversation acts model for generating
spoken dialogue contributions. Computer Speech and
Language, 16(3-4):313?352.
S. Stoyanchev and P. Piwek. 2010. Constructing the
CODA corpus. In Procs of LREC 2010, Malta, May.
S. V. Suzuki and S. Yamada. 2004. Persuasion through
overheard communication by life-like agents. In Procs
of the 2004 IEEE/WIC/ACM International Conference
on Intelligent Agent Technology, Beijing, September.
K. van Deemter, B. Krenn, P. Piwek, M. Klesen,
M. Schro?der, and S. Baumann. 2008. Fully gener-
ated scripted dialogue for embodied agents. Artificial
Intelligence Journal, 172(10):1219?1244.
S. Williams, P. Piwek, and R. Power. 2007. Generat-
ing Monologue and Dialogue to Present Personalised
Medical Information to Patients. In Procs ENLG
2007, pages 167?170, Schloss Dagstuhl, Germany.
336
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 242?247,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Data-oriented Monologue-to-Dialogue Generation
Paul Piwek
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
p.piwek@open.ac.uk
Svetlana Stoyanchev
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
s.stoyanchev@open.ac.uk
Abstract
This short paper introduces an implemented
and evaluated monolingual Text-to-Text gen-
eration system. The system takes mono-
logue and transforms it to two-participant di-
alogue. After briefly motivating the task
of monologue-to-dialogue generation, we de-
scribe the system and present an evaluation in
terms of fluency and accuracy.
1 Introduction
Several empirical studies show that delivering in-
formation in the form of a dialogue, as opposed to
monologue, can be particularly effective for educa-
tion (Craig et al, 2000; Lee et al, 1998) and per-
suasion (Suzuki and Yamada, 2004). Information-
delivering or expository dialogue was already em-
ployed by Plato to communicate his philosophy. It
is used primarily to convey information and possibly
also make an argument; this in contrast with dra-
matic dialogue which focuses on character develop-
ment and narrative.
Expository dialogue lends itself well for presenta-
tion through computer-animated agents (Prendinger
and Ishizuka, 2004). Most information is however
locked up as text in leaflets, books, newspapers,
etc. Automatic generation of dialogue from text in
monologue makes it possible to convert information
into dialogue as and when needed.
This paper describes the first data-oriented
monologue-to-dialogue generation system which re-
lies on the automatic mapping of the discourse
relations underlying monologue to appropriate se-
quences of dialogue acts. The approach is data-
oriented in that the mapping rules have been auto-
matically derived from an annotated parallel mono-
logue/dialogue corpus, rather than being hand-
crafted.
The paper proceeds as follows. Section 2 reviews
existing approaches to dialogue generation. Section
3 describes the current approach. We provide an
evaluation in Section 4. Finally, Section 5 describes
our conclusions and plans for further research.
2 Related Work
For the past decade, generation of information-
delivering dialogues has been approached primarily
as an AI planning task. Andre? et al (2000) describe
a system, based on a centralised dialogue planner,
that creates dialogues between a virtual car buyer
and seller from a database; this approach has been
extended by van Deemter et al (2008). Others have
used (semi-) autonomous agents for dialogue gener-
ation (Cavazza and Charles, 2005; Mateas and Stern,
2005).
More recently, first steps have been taken towards
treating dialogue generation as an instance of Text-
to-Text generation (Rus et al, 2007). In particu-
lar, the T2D system (Piwek et al, 2007) employs
rules that map text annotated with discourse struc-
tures, along the lines of Rhetorical Structure Theory
(Mann and Thompson, 1988), to specific dialogue
sequences. Common to all the approaches discussed
so far has been the manual creation of generation
resources, whether it be mappings from knowledge
representations or discourse to dialogue structure.
242
With the creation of the publicly available1 CODA
parallel corpus of monologue and dialogue (Stoy-
anchev and Piwek, 2010a), it has, however, become
possible to adopt a data-oriented approach. This cor-
pus consists of approximately 700 turns of dialogue,
by acclaimed authors such as Mark Twain, that are
aligned with monologue that was written on the ba-
sis of the dialogue, with the specific aim to express
the same information as the dialogue.2 The mono-
logue side has been annotated with discourse rela-
tions, using an adaptation of the annotation guide-
lines of Carlson and Marcu (2001), whereas the di-
alogue side has been marked up with dialogue acts,
using tags inspired by the schemes of Bunt (2000),
Carletta et al (1997) and Core and Allen (1997).
As we will describe in the next section, our ap-
proach uses the CODA corpus to extract mappings
from monologue to dialogue.
3 Monologue-to-Dialogue Generation
Approach
Our approach is based on five principal steps:
I Discourse parsing: analysis of the input mono-
logue in terms of the underlying discourse rela-
tions.
II Relation conversion: mapping of text annotated
with discourse relations to a sequence of dia-
logue acts, with segments of the input text as-
signed to corresponding dialogue acts.
III Verbalisation: verbal realisation of dialogue
acts based on the dialogue act type and text of
the corresponding monologue segment.
IV Combination Putting the verbalised dialogues
acts together to create a complete dialogue, and
V Presentation: Rendering of the dialogue (this
can range for simple textual dialogue scripts to
computer-animated spoken dialogue).
1computing.open.ac.uk/coda/data.html
2Consequently, the corpus was not constructed entirely of
pre-existing text; some of the text was authored as part of the
corpus construction. One could therefore argue, as one of the re-
viewers for this paper did, that the approach is not entirely data-
driven, if data-driven is interpreted as ?generated from unadul-
terated, free text, without any human intervention needed?.
For step I we rely on human annotation or existing
discourse parsers such as DAS (Le and Abeysinghe,
2003) and HILDA (duVerle and Prendinger, 2009).
For the current study, the final step, V, consists sim-
ply of verbatim presentation of the dialogue text.
The focus of the current paper is with steps II and
III (with combination, step IV, beyond the scope of
the current paper). Step II is data-oriented in that
we have extracted mappings from discourse relation
occurrences in the corpus to corresponding dialogue
act sequences, following the approach described in
Piwek and Stoyanchev (2010). Stoyanchev and Pi-
wek (2010b) observed in the CODA corpus a great
variety of Dialogue Act (DA) sequences that could
be used in step II, however in the current version
of the system we selected a representative set of the
most frequent DA sequences for the five most com-
mon discourse relations in the corpus. Table 1 shows
the mapping from text with a discourse relations
to dialogue act sequences (i indicates implemented
mappings).
DA sequence A C C E M TR
D T R M T
YNQ; Expl i i d
YNQ; Yes; Expl i i i d
Expl; CmplQ; Expl i d
ComplQ; Expl i/t i/t i i c
Expl; YNQ;Yes i d
Expl; Contrad. i d
FactQ; FactA; Expl i c
Expl; Agr; Expl i d
Expl; Fact; Expl t c
Table 1: Mappings from discourse relations (A = Attribu-
tion, CD = Condition, CT = Contrast, ER = Explanation-
Reason, MM = Manner-Means) to dialogue act sequences
(explained below) together with the type of verbalisation
transformation TR being d(irect) or c(omplex).
For comparison, the table also shows the much
less varied mappings implemented by the T2D sys-
tem (indicated with t). Note that the actual mappings
of the T2D system are directly from discourse rela-
tion to dialogue text. The dialogue acts are not ex-
plicitly represented by the system, in contrast with
the current two stage approach which distinguishes
between relation conversion and verbalisation.
243
Verbalisation, step III, takes a dialogue act type
and the specification of its semantic content as given
by the input monologue text. Mapping this to the
appropriate dialogue act requires mappings that vary
in complexity.
For example, Expl(ain) can be generated by sim-
ply copying a monologue segment to dialogue utter-
ance. The dialogue acts Yes and Agreement can be
generated using canned text, such as ?That is true?
and ?I agree with you?.
In contrast, ComplQ (Complex Question), FactQ
(Factoid Question), FactA (Factiod Answer) and
YNQ (Yes/No Question) all require syntactic ma-
nipulation. To generate YNQ and FactQ, we use
the CMU Question Generation tool (Heilman and
Smith, 2010) which is based on a combination
of syntactic transformation rules implemented with
tregex (Levy and Andrew, 2006) and statistical
methods. To generate the Compl(ex) Q(uestion) in
the ComplQ;Expl Dialogue Act (DA) sequence, we
use a combination of the CMU tool and lexical trans-
formation rules.3 The GEN example in Table 2 il-
lustrates this: The input monologue has a Manner-
Means relations between the nucleus ?In September,
Ashland settled the long-simmering dispute? and the
satellite ?by agreeing to pay Iran 325 million USD?.
The satellite is copied without alteration to the Ex-
plain dialogue act. The nucleus is processed by ap-
plying the following template-based rule:
Decl? How Yes/No Question(Decl)
In words, the input consisting of a declarative sen-
tence is mapped to a sequence consisting of the word
?How? followed by a Yes/No-question (in this case
?Did Ashland settle the long-simmering dispute in
December??) that is obtained with the CMU QG tool
from the declarative input sentence. A similar ap-
proach is applied for the other relations (Attribution,
Condition and Explanation-Reason) that can lead to
a ComplQ; Expl dialogue act sequence (see Table 1).
Generally, sequences requiring only copying or
canned text are labelled d(irect) in Table 1, whereas
those requiring syntactic transformation are labelled
c(omplex).
3In contrast, the ComplQ in the DA sequence
Expl;ComplQ;Expl is generated using canned text such as
?Why?? or ?Why is that??.
4 Evaluation
We evaluate the output generated with both complex
and direct rules for the relations of Table 1.
4.1 Materials, Judges and Procedure
The input monologues were text excerpts from the
Wall Street Journal as annotated in the RST Dis-
course Treebank4. They consisted of a single sen-
tence with one internal relation, or two sentences
(with no internal relations) connected by a single
relation. To factor out the quality of the discourse
annotations, we used the gold standard annotations
of the Discourse Treebank and checked these for
correctness, discarding a small number of incorrect
annotations.5 We included text fragments with a
variety of clause length, ordering of nucleus and
satellite, and syntactic structure of clauses. Table 2
shows examples of monologue/dialogue pairs: one
with a generated dialogue and the other from the cor-
pus.
Our study involved a panel of four judges, each
fluent speakers of English (three native) and ex-
perts in Natural Language Generation. We collected
judgements on 53 pairs of monologue and corre-
sponding dialogue. 19 pairs were judged by all four
judges to obtain inter-annotator agreement statistics,
the remainder was parcelled out. 38 pairs consisted
of WSJ monologue and generated dialogue, hence-
forth GEN, and 15 pairs of CODA corpus monologue
and human-authored dialogue, henceforth CORPUS
(instances of generated and corpus dialogue were
randomly interleaved) ? see Table 2 for examples.
The two standard evaluation measures for lan-
guage generation, accuracy and fluency (Mellish and
Dale, 1998), were used: a) accuracy: whether a
dialogue (from GEN or CORPUS) preserves the in-
formation of the corresponding monologue (judge-
ment: ?Yes? or ?No?) and b) monologue and dialogue
fluency: how well written a piece of monologue or
dialogue from GEN or CORPUS is. Fluency judge-
ments were on a scale from 1 ?incomprehensible? to
5 ?Comprehensible, grammatically correct and nat-
urally sounding?.
4www.isi.edu/?marcu/discourse/Corpora.html
5For instance, in our view ?without wondering? is incorrectly
connected with the attribution relation to ?whether she is mov-
ing as gracefully as the scenery.?
244
GEN Monologue
In September, Ashland settled the
long-simmering dispute by agreeing to
pay Iran 325 million USD.
Dialogue (ComplQ; Expl)
A: How did Ashland settle the
long-simmering dispute in December?
B: By agreeing to pay Iran 325
million USD.
CORPUS Monologue
If you say ?I believe the world is
round?, the ?I? is the mind.
Dialogue (FactQ; FactA)
A: If you say ?I believe the world is round?,
who is the ?I? that is speaking?
B: The mind.
Table 2: Monologue-Dialogue Instances
4.2 Results
Accuracy Three of the four judges marked 90%
of monologue-dialogue pairs as presenting the same
information (with pairwise ? of .64, .45 and .31).
One judge interpreted the question differently and
marked only 39% of pairs as containing the same
information. We treated this as an outlier, and ex-
cluded the accuracy data of this judge. For the in-
stances marked by more than one judge, we took the
majority vote. We found that 12 out of 13 instances
(or 92%) of dialogue and monologue pairs from the
CORPUS benchmark sample were judged to contain
the same information. For the GEN monologue-
dialogue pairs, 28 out of 31 (90%) were judged to
contain the same information.
Fluency Although absolute agreement between
judges was low,6 pairwise agreement in terms of
Spearman rank correlation (?) is reasonable (aver-
age: .69, best: .91, worst: .56). For the subset of in-
stances with multiple annotations, we used the data
from the judge with the highest average pair-wise
agreement (? = .86)
The fluency ratings are summarised in Figure 1.
Judges ranked both monologues and dialogues for
6For the four judges, we had an average pairwise ? of .34
with the maximum and minimum values of .52 and .23, respec-
tively.
Figure 1: Mean Fluency Rating for Monologues and Dia-
logues (for 15 CORPUS and 38 GEN instances) with 95%
confidence intervals
the GEN sample higher than for the CORPUS sam-
ple (possibly as a result of slightly greater length of
the CORPUS fragments and some use of archaic lan-
guage). However, the drop in fluency, see Figure 2,
from monologue to dialogue is greater for GEN sam-
ple (average: .89 points on the rating scale) than the
CORPUS sample (average: .33) (T-test p<.05), sug-
gesting that there is scope for improving the genera-
tion algorithm.
Figure 2: Fluency drop from monologue to correspond-
ing dialogue (for 15 CORPUS and 38 GEN instances). On
the x-axis the fluency drop is marked, starting from no
fluency drop (0) to a fluency drop of 3 (i.e., the dialogue
is rated 3 points less than the monologue on the rating
scale).
245
Direct versus Complex rules We examined the
difference in fluency drop between direct and com-
plex rules. Figure 3 shows that the drop in fluency
for dialogues generated with complex rules is higher
than for the dialogues generated using direct rules
(T-test p<.05). This suggests that use of direct rules
is more likely to result in high quality dialogue. This
is encouraging, given that Stoyanchev and Piwek
(2010a) report higher frequencies in professionally
authored dialogues of dialogue acts (YNQ, Expl) that
can be dealt with using direct verbalisation (in con-
trast with low frequency of, e.g., FactQ).
Figure 3: Decrease in Fluency Score from Monologue
to Dialogue comparing Direct (24 samples) and Complex
(14 samples) dialogue generation rules
5 Conclusions and Further Work
With information presentation in dialogue form be-
ing particularly suited for education and persua-
sion, the presented system is a step towards mak-
ing information from text automatically available
as dialogue. The system relies on discourse-to-
dialogue structure rules that were automatically ex-
tracted from a parallel monologue/dialogue corpus.
An evaluation against a benchmark sample from the
human-written corpus shows that both accuracy and
fluency of generated dialogues are not worse than
that of human-written dialogues. However, drop in
fluency between input monologue and output dia-
logue is slightly worse for generated dialogues than
for the benchmark sample. We also established a dif-
ference in quality of output generated with complex
versus direct discourse-to-dialogue rules, which can
be exploited to improve overall output quality.
In future research, we aim to evaluate the accu-
racy and fluency of longer stretches of generated di-
alogue. Additionally, we are currently carrying out
a task-related evaluation of monologue versus dia-
logue to determine the utility of each.
Acknowledgements
We would like to thank the three anonymous
reviewers for their helpful comments and sug-
gestions. We are also grateful to our col-
leagues in the Open University?s Natural Lan-
guage Generation group for stimulating discussions
and feedback. The research reported in this pa-
per was carried out as part of the CODA re-
search project (http://computing.open.ac.uk/coda/)
which was funded by the UK?s Engineering and
Physical Sciences Research Council under Grant
EP/G020981/1.
References
E. Andre?, T. Rist, S. van Mulken, M. Klesen, and
S. Baldes. 2000. The automated design of believable
dialogues for animated presentation teams. In Jus-
tine Cassell, Joseph Sullivan, Scott Prevost, and Eliz-
abeth Churchill, editors, Embodied Conversational
Agents, pages 220?255. MIT Press, Cambridge, Mas-
sachusetts.
H. Bunt. 2000. Dialogue pragmatics and context spec-
ification. In H. Bunt and W. Black, editors, Abduc-
tion, Belief and Context in Dialogue: Studies in Com-
putational Pragmatics, volume 1 of Natural Language
Processing, pages 81?150. John Benjamins.
J. Carletta, A. Isard, S. Isard, J. Kowtko, G. Doherty-
Sneddon, and A. Anderson. 1997. The reliability of
a dialogue structure coding scheme. Computational
Linguistics, 23:13?31.
L. Carlson and D. Marcu. 2001. Discourse tagging
reference manual. Technical Report ISI-TR-545, ISI,
September.
M. Cavazza and F. Charles. 2005. Dialogue Gener-
ation in Character-based Interactive Storytelling. In
Proceedings of the AAAI First Annual Artificial Intel-
ligence and Interactive Digital Entertainment Confer-
ence, Marina Del Rey, California, USA.
M. Core and J. Allen. 1997. Coding Dialogs with
the DAMSL Annotation Scheme. In Working Notes:
AAAI Fall Symposium on Communicative Action in
Humans and Machine.
246
S. Craig, B. Gholson, M. Ventura, A. Graesser, and the
Tutoring Research Group. 2000. Overhearing dia-
logues and monologues in virtual tutoring sessions.
International Journal of Artificial Intelligence in Ed-
ucation, 11:242?253.
D. duVerle and H. Prendinger. 2009. A novel discourse
parser based on support vector machines. In Proc 47th
Annual Meeting of the Association for Computational
Linguistics and the 4th Int?l Joint Conf on Natural
Language Processing of the Asian Federation of Nat-
ural Language Processing (ACL-IJCNLP?09), pages
665?673, Singapore, August.
M. Heilman and N. A. Smith. 2010. Good question!
statistical ranking for question generation. In Proc. of
NAACL/HLT, Los Angeles.
Huong T. Le and Geehta Abeysinghe. 2003. A study to
improve the efficiency of a discourse parsing system.
In Proceedings 4th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing-03), Springer LNCS 2588, pages 101?114.
J. Lee, F. Dinneen, and J. McKendree. 1998. Supporting
student discussions: it isn?t just talk. Education and
Information Technologies, 3:217?229.
R. Levy and G. Andrew. 2006. Tregex and tsurgeon:
tools for querying and manipulating tree data struc-
tures. In 5th International Conference on Language
Resources and Evaluation (LREC 2006)., Genoa, Italy.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
M. Mateas and A. Stern. 2005. Structuring content in the
faade interactive drama architecture. In Proc. of Artifi-
cial Intelligence and Interactive Digital Entertainment
(AIIDE), Marina del Rey, Los Angeles, June.
C. Mellish and R. Dale. 1998. Evaluation in the context
of natural language generation. Computer Speech and
Language, 12:349?373.
P. Piwek and S. Stoyanchev. 2010. Generating Exposi-
tory Dialogue from Monologue: Motivation, Corpus
and Preliminary Rules. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 333?336, Los Angeles, Cali-
fornia, June.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. T2D: Generating Dialogues between Virtual
Agents Automatically from Text. In Intelligent Vir-
tual Agents: Proceedings of IVA07, LNAI 4722, pages
161?174. Springer Verlag.
H. Prendinger and M. Ishizuka, editors. 2004. Life-Like
Characters: Tools, Affective Functions, and Applica-
tions. Cognitive Technologies Series. Springer, Berlin.
V. Rus, A. Graesser, A. Stent, M. Walker, and M. White.
2007. Text-to-Text Generation. In R. Dale and
M. White, editors, Shared Tasks and Comparative
Evaluation in Natural Language Generation: Work-
shop Report, Arlington, Virginia.
S. Stoyanchev and P. Piwek. 2010a. Constructing the
CODA corpus. In Procs of LREC 2010, Malta, May.
S. Stoyanchev and P. Piwek. 2010b. Harvesting re-usable
high-level rules for expository dialogue generation. In
6th International Natural Language Generation Con-
ference (INLG 2010), Dublin, Ireland, 7-8, July.
S. V. Suzuki and S. Yamada. 2004. Persuasion through
overheard communication by life-like agents. In Procs
of the 2004 IEEE/WIC/ACM International Conference
on Intelligent Agent Technology, Beijing, September.
K. van Deemter, B. Krenn, P. Piwek, M. Klesen,
M. Schroeder, and S. Baumann. 2008. Fully Gen-
erated Scripted Dialogue for Embodied Agents. Arti-
ficial Intelligence Journal, 172(10):1219?1244.
247
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 109?113,
Dublin, Ireland, August 23-24, 2014.
AT&T: The Tag&Parse Approach to Semantic Parsing of Robot Spatial
Commands
Svetlana Stoyanchev, Hyuckchul Jung, John Chen, Srinivas Bangalore
AT&T Labs Research
1 AT&T Way Bedminster NJ 07921
{sveta,hjung,jchen,srini}@research.att.com
Abstract
The Tag&Parse approach to semantic
parsing first assigns semantic tags to each
word in a sentence and then parses the
tag sequence into a semantic tree. We
use statistical approach for tagging, pars-
ing, and reference resolution stages. Each
stage produces multiple hypotheses which
are re-ranked using spatial validation. We
evaluate the Tag&Parse approach on a cor-
pus of Robotic Spatial Commands as part
of the SemEval Task6 exercise. Our sys-
tem accuracy is 87.35% and 60.84% with
and without spatial validation.
1 Introduction
In this paper we describe a system participating
in the SemEval2014 Task-6 on Supervised Seman-
tic Parsing of Robotic Spatial Commands. It pro-
duces a semantic parse of natural language com-
mands addressed to a robot arm designed to move
objects on a grid surface. Each command directs
a robot to change position of an object given a
current configuration. A command uniquely iden-
tifies an object and its destination, for example
?Move the turquoise pyramid above the yellow
cube?. System output is a Robot Control Lan-
guage (RCL) parse (see Figure 1) which is pro-
cessed by the robot arm simulator. The Robot Spa-
tial Commands dataset (Dukes, 2013) is used for
training and testing.
Our system uses a Tag&Parse approach which
separates semantic tagging and semantic parsing
stages. It has four components: 1) semantic tag-
ging, 2) parsing, 3) reference resolution, and 4)
spatial validation. The first three are trained using
LLAMA (Haffner, 2006), a supervised machine
learning toolkit, on the RCL-parsed sentences.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
For semantic tagging, we train a maximum en-
tropy sequence tagger for assigning a semantic la-
bel and value to each word in a sentence, such as
type cube or color blue. For parsing, we train a
constituency parser on non-lexical RCL semantic
trees. For reference resolution, we train a maxi-
mum entropy model that identifies entities for ref-
erence tags found by previous components. All of
these components can generate multiple hypothe-
ses. Spatial validation re-ranks these hypotheses
by validating them against the input spatial con-
figuration. The top hypothesis after re-ranking is
returned by the system.
Separating tagging and parsing stages has sev-
eral advantages. A tagging stage allows the system
flexibility to abstract from possible grammatical or
spelling errors in a command. It assigns a seman-
tic category to each word in a sentence. Words not
contributing to the semantic meaning are assigned
?O? label by the tagger and are ignored in the fur-
ther processing. Words that are misspelled can po-
tentially receive a correct tag when a word simi-
larity feature is used in building a tagging model.
This will be especially important when process-
ing output of spoken commands that may contain
recognition errors.
The remainder of the paper is organized thusly.
In Section 2 we describe each of the components
used in our system. In Section 3 we describe the
results reported for SemEval2014 and evaluation
of each system component. We summarize our
findings and present future work in Section 4.
2 System
2.1 Sequence Tagging
A sequence tagging approach is used for condi-
tional inference of tags given a word sequence.
It is used for many natural language tasks, such
as part of speech (POS) and named entity tag-
ging (Toutanova and others, 2003; Carreras et al.,
2003). We train a sequence tagger for assign-
109
Figure 1: RCL tree for a sentence Move the turquoise pyramid above the yellow cube.
Word index tag label
Move 1 action move
the 2 O -
turquoise 3 color cyan
pyramid 4 type prism
above 5 relation above
the 6 O -
yellow 7 color yellow
cube 8 type cube
Table 1: Tagging labels for a sentence Move the
turquoise pyramid above the yellow cube.
ing a combined semantic tag and label (such as
type cube) to each word in a command. The tags
used for training are extracted from the leaf-level
nodes of the RCL trees. Table 2 shows tags and
labels for a sample sentence ?Move the turquoise
pyramid above the yellow cube? extracted from
the RCL parse tree (see Figure 1). In some cases,
a label is the same as a word (yellow, cube) while
in other cases, it differs (turquoise - cyan, pyramid
- prism).
We train a sequence tagger using LLAMA max-
imum entropy (maxent) classification (Haffner,
2006) to predict the combined semantic tag and
label of each word. Neighboring words, immedi-
ately neighboring semantic tags, and POS tags are
used as features, where the POS tagger is another
sequence tagging model trained on the Penn Tree-
bank (Marcus et al., 1993). We also experimented
with a tagger that assigns tags and labels in sep-
arate sequence tagging models, but it performed
poorly.
2.2 Parsing
We use a constituency parser for building RCL
trees. The input to the parser is a sequence of
tags assigned by a sequence tagger, such as ?ac-
tion color type relation color type? for the exam-
ple in Figure 1.
The parser generates multiple RCL parse tree
hypotheses sorted in the order of their likelihood.
The likelihood of a tree T given a sequence of tags
T is determined using a probabilistic context free
grammar (PCFG) G:
P (T |S) =
?
r?T
P
G
(r) (1)
The n-best parses are obtained using the CKY
algorithm, recording the n-best hyperedge back-
pointers per constituent along the lines of (Huang
and Chiang, 2005). G was obtained and P
G
was
estimated from a corpus of non-lexical RCL trees
generated by removing all nodes descendant from
the tag nodes (action, color, etc.). Parses may con-
tain empty nodes not corresponding to any tag in
the input sequence. These are hypothesized by the
parser at positions in between input tags and in-
serted as edges according to the PCFG, which has
probabilistic rules for generating empty nodes.
2.3 Reference Resolution
Reference resolution identifies the most prob-
able antecedent for each anaphor within a
text (Hirschman and Chinchor, 1997). It applies
when multiple candidates antecedents are present.
For example, in a sentence ?Pick up the red cube
standing on a grey cube and place it on top of
the yellow one?, the anaphor it has two candidate
antecedents corresponding to entity segments the
red cube and a grey cube. In our system, anaphor
and antecedents are represented by reference tags
occurring in one sentence. A reference tag is ei-
ther assigned by a sequence tagger to one of the
words (e.g. to a pronoun) or is inserted into a
tree by the parser (e.g. ellipsis). We train a bi-
nary maxent model for this task using LLAMA.
The input is a pair consisting of an anaphor and
a candidate antecedent, along with their features.
110
Features that are used include the preceding and
following words as well as the tags/labels of both
the anaphor and candidate antecedent. The refer-
ence resolution component selects the antecedent
for which the model returns the highest score.
2.4 Spatial Validation
SemEval2014 Task6 provided a spatial planner
which takes an RCL command as an input and
determines if that command is executable in the
given spatial context. At each step described in
2.1?2.3, due to the statistical nature of our ap-
proach, multiple hypotheses can be easily com-
puted with different confidence values. We used
the spatial planner to validate the final output RCL
commands from the three steps by checking if the
RCLs are executable or not. We generate multi-
ple tagger output hypotheses. For each tagger out-
put hypothesis, we generate multiple parser out-
put hypotheses. For each parser output hypothe-
sis, we generate multiple reference resolution out-
put hypotheses. The resulting output hypotheses
are ranked in the order of confidence scores with
the highest tagging output scores ranked first, fol-
lowed by the parsing output scores, and, finally,
reference resolution output scores. The system re-
turns the result of the top scored command that is
valid according to the spatial validator.
In many applications, there can be a tool or
method to validate tag/parse/reference outputs
fully or partially. Note that in our system the val-
idation is performed after all output is generated.
Tightly coupled validation, such as checking va-
lidity of a tagged entity or a parse constituent,
could help in computing hypotheses at each step
(e.g., feature values based on possible entities or
actions) and it remains as future work.
3 Results
In this section, we present evaluation results on the
three subsets of the data summarized in Table 3. In
the TEST2500 data set, the models are trained on
the initial 2500 sentences of the Robot Commands
Treebank and evaluated on the last 909 sentences
(this corresponds to the data split of the SemEval
task). In TEST500 data set, the models are trained
on the initial 500 sentences of the training set and
evaluated on the last 909 test sentences. We re-
port these results to analyze the models? perfor-
mance on a reduced training size. In DEV2500
data set, models are trained on 90% of the initial
2500 sentences and evaluated on 10% of the 2500
# Dataset Avg # hyp Accuracy
1 TEST2500 1-best 1 86.0%
2 TEST2500 max-5 3.34 95.2%
3 TEST500 1-best 1 67.9%
4 TEST500 max-5 4.25 83.8%
5 DEV2500 1-best 1 90.8%
6 DEV2500 max-5 2.9 98.0%
Table 3: Tagger accuracy for 1-best and maximum
of 5-best hypotheses (max-5).
sentences using a random data split. We observe
that sentence length and standard deviation of test
sentences in the TEST2500 data set is higher than
on the training sentences while in the DEV2500
data set training and test sentence length and stan-
dard deviation are comparable.
3.1 Semantic Tagging
Table 3 presents sentence accuracy of the seman-
tic tagging stage. Tagging accuracy is evaluated
on 1-best and on max-5 best tagger outputs. In
the max-5 setting the number of hypotheses gen-
erated by the tagger varies for each input with the
average numbers reported in Table 3. Tagging ac-
curacy on TEST2500 using 1-best is 86.0%. Con-
sidering max-5 best tagging sequences, the accu-
racy is 95.2%. On the TEST500 data set tagging
accuracy is 67.9% and 83.8% on 1-best and max-
5 best sequences respectively, approximately 8%
points lower than on TEST2500 data set. On the
DEV2500 data set tagging accuracy is 90.8% and
98.0% on 1-best and max-5 best sequences, 4.8%
and 2.8% points higher than on the TEST2500
data set. The higher performance on DEV2500 in
comparison to the TEST2500 can be explained by
the higher complexity of the test sentences in com-
parison to the training sentences in the TEST2500
data set.
3.2 RCL Parsing
Parsing was evaluated using the EVALB scoring
metric (Collins, 1997). Its 1-best F-measure accu-
racy on gold standard TEST2500 and DEV2500
semantic tag sequences was 96.17% and 95.20%,
respectively. On TEST500, its accuracy remained
95.20%. On TEST2500 with system provided in-
put sequences, its accuracy was 94.79% for 869
out of 909 sentences that were tagged correctly.
3.3 System Accuracy
Table 4 presents string accuracy of automatically
generated RCL parse trees on each data set. The
111
Name Train #sent Train Sent. len. (stdev) Test #sent Test Sent. Len. (stdev)
TEST2500 2500 13.44 (5.50) 909 13.96 (5.59)
TEST500 500 14.62(5.66) 909 13.96 (5.59)
DEV2500 2250 13.43 ( 5.53) 250 13.57 (5.27)
Table 2: Number of sentences, average length and standard deviation of the data sets.
results are obtained by comparing system output
RCL parse string with the reference RCL parse
string. For each data set, we ran the system
with and without spatial validation. We ran RCL
parser and reference resolution on automatically
assigned semantic tags (Auto) and oracle tagging
(Orcl). We observed that some tag labels can be
verified systematically and corrected them with
simple rules: e.g., change ?front? to ?forward?
because relation specification in (Dukes, 2013)
doesn?t have ?front? even though annotations in-
cluded cases with ?front? as relation.
The system performance on TEST2500 data
set using automatically assigned tags and no spa-
tial validation is 60.84%. In this mode, the sys-
tem uses 1-best parser and 1-best tagger output.
With spatial validation, which allows the system to
re-rank parser and tagger hypotheses, the perfor-
mance increases by 27% points to 87.35%. This
indicates that the parser and the tagger component
often produce a correct output which is not ranked
first. Using oracle tags without / with spatial vali-
dation on TEST2500 data set the system accuracy
is 67.55% / 94.83%, 7% points above the accuracy
using predicted tags.
The system performance on TEST500 data set
using automatically assigned tags with / with-
out spatial validation is 48.95% / 74.92%, ap-
proximately 12% points below the performance
on TEST2500 (Row 1). Using oracle tags with-
out / with spatial validation the performance on
TEST500 data set is 63.89% / 94.94%. The per-
formance without spatial validation is only 4% be-
low TEST2500, while with spatial validation the
performance on TEST2500 and TEST500 is the
same. These results indicate that most perfor-
mance degradation on a smaller data set is due to
the semantic tagger.
The system performance on DEV2500 data set
using automatically assigned tags without / with
spatial validation is 68.0% / 96.80% (Row 5), 8%
points above the performance on TEST2500 (Row
1). With oracle tags, the performance is 69.60%
/ 98.0%, which is 2-3% points above TEST2500
(Row 2). These results indicate that most perfor-
mance improvement on a better balanced data set
# Dataset Tag Accuracy without / with
spatial validation
1 TEST2500 Auto 60.84 / 87.35
2 TEST2500 Orcl 67.55 / 94.83
3 TEST500 Auto 48.95 / 74.92
4 TEST500 Orcl 63.89 / 94.94
5 DEV2500 Auto 68.00 / 96.80
6 DEV2500 Orcl 69.60 / 98.00
Table 4: System accuracy with and without spatial
validation using automatically assigned tags and
oracle tags (OT).
DEV2500 is due to better semantic tagging.
4 Summary and Future Work
In this paper, we present the results of semantic
processing for natural language robot commands
using Tag&Parse approach. The system first tags
the input sentence and then applies non-lexical
parsing to the tag sequence. Reference resolution
is applied to the resulting parse trees. We com-
pare the results of the models trained on the data
sets of size 500 (TEST500) and 2500 (TEST2500)
sentences. We observe that sequence tagging
model degrades significantly on a smaller data set.
Parsing and reference resolution models, on the
other hand, perform nearly as well on both train-
ing sizes. We compare the results of the models
trained on more (DEV2500) and less (TEST2500)
homogeneous training/testing data sets. We ob-
serve that a semantic tagging model is more sen-
sitive to the difference between training and test
set than parsing model degrading significantly a
less homogeneous data set. Our results show that
1) both tagging and parsing models will benefit
from an improved re-ranking, and 2) our parsing
model is robust to a data size reduction while tag-
ging model requires a larger training data set.
In future work we plan to explore how
Tag&Parse approach will generalize in other do-
mains. In particular, we are interested in using
a combination of domain-specific tagging models
and generic semantic parsing (Das et al., 2010) for
processing spoken commands in a dialogue sys-
tem.
112
References
Xavier Carreras, Llu??s M`arquez, and Llu??s Padr?o.
2003. A Simple Named Entity Extractor Using Ad-
aBoost. In Proceedings of the CoNLL, pages 152?
157, Edmonton, Canada.
Michael Collins. 1997. Three Generative Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL, pages 16?23.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-
Semantic Parsing. In HLT-NAACL, pages 948?956.
Kais Dukes. 2013. Semantic Annotation of Robotic
Spatial Commands. In Language and Technology
Conference (LTC).
Patrick Haffner. 2006. Scaling large margin classifiers
for spoken language understanding. Speech Com-
munication, 48(3-4):239?261.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-
7 Coreference Task Definition. In Proceedings of
the Message Understanding Conference (MUC-7).
Science Applications International Corporation.
Liang Huang and David Chiang. 2005. Better K-
best Parsing. In Proceedings of the Ninth Inter-
national Workshop on Parsing Technology, Parsing
?05, pages 53?64, Stroudsburg, PA, USA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of the 2003 Conference of the
NAACL on Human Language Technology - Volume
1, pages 173?180.
113
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 9?16
Manchester, UK. August 2008
Exact Phrases in Information Retrieval for Question Answering
Svetlana Stoyanchev, and Young Chol Song, and William Lahti
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
svetastenchikova, nskystars, william.lahti @gmail.com
Abstract
Question answering (QA) is the task of
finding a concise answer to a natural lan-
guage question. The first stage of QA in-
volves information retrieval. Therefore,
performance of an information retrieval
subsystem serves as an upper bound for the
performance of a QA system. In this work
we use phrases automatically identified
from questions as exact match constituents
to search queries. Our results show an im-
provement over baseline on several docu-
ment and sentence retrieval measures on
the WEB dataset. We get a 20% relative
improvement in MRR for sentence extrac-
tion on the WEB dataset when using au-
tomatically generated phrases and a fur-
ther 9.5% relative improvement when us-
ing manually annotated phrases. Surpris-
ingly, a separate experiment on the indexed
AQUAINT dataset showed no effect on IR
performance of using exact phrases.
1 Introduction
Question answering can be viewed as a sophisti-
cated information retrieval (IR) task where a sys-
tem automatically generates a search query from
a natural language question and finds a concise
answer from a set of documents. In the open-
domain factoid question answering task systems
answer general questions like Who is the creator of
The Daily Show?, or When was Mozart born?. A
variety of approaches to question answering have
been investigated in TREC competitions in the last
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
decade from (Vorhees and Harman, 1999) to (Dang
et al, 2006). Most existing question answering
systems add question analysis, sentence retrieval
and answer extraction components to an IR sys-
tem.
Since information retrieval is the first stage of
question answering, its performance is an up-
per bound on the overall question answering sys-
tem?s performance. IR performance depends on
the quality of document indexing and query con-
struction. Question answering systems create a
search query automatically from a user?s question,
through various levels of sophistication. The sim-
plest way of creating a query is to treat the words
in the question as the terms in the query. Some
question answering systems (Srihari and Li, 1999)
apply linguistic processing to the question, iden-
tifying named entities and other query-relevant
phrases. Others (Hovy et al, 2001b) use ontolo-
gies to expand query terms with synonyms and hy-
pernyms.
IR system recall is very important for question
answering. If no correct answers are present in a
document, no further processing will be able to
find an answer. IR system precision and rank-
ing of candidate passages can also affect question
answering performance. If a sentence without a
correct answer is ranked highly, answer extrac-
tion may extract incorrect answers from these erro-
neous candidates. Collins-Thompson et al (2004)
show that there is a consistent relationship between
the quality of document retrieval and the overall
performance of question answering systems.
In this work we evaluate the use of exact phrases
from a question in document and passage retrieval.
First, we analyze how different parts of a ques-
tion contribute to the performance of the sentence
extraction stage of question answering. We ana-
9
lyze the match between linguistic constituents of
different types in questions and sentences contain-
ing candidate answers. For this analysis, we use a
set of questions and answers from the TREC 2006
competition as a gold standard.
Second, we evaluate the performance of doc-
ument retrieval in our StoQA question answering
system. We compare the performance of docu-
ment retrieval from the Web and from an indexed
collection of documents using different methods of
query construction, and identify the optimal algo-
rithm for query construction in our system as well
as its limitations.
Third, we evaluate passage extraction from a set
of documents. We analyze how the specificity of a
query affects sentence extraction.
The rest of the paper is organized as follows:
In Section 2, we summarize recent approaches to
question answering. In Section 3, we describe the
dataset used in this experiment. In Section 5, we
describe our method and data analysis. In Sec-
tion 4, we outline the architecture of our question
answering system. In Section 6, we describe our
experiments and present our results. We summa-
rize in Section 7.
2 Related Work
Information retrieval (IR) for question answering
consists of 2 steps: document retrieval and passage
retrieval.
Approaches to passage retrieval include sim-
ple word overlap (Light et al, 2001), density-
based passage retrieval (Clarke et al, 2000), re-
trieval based on the inverse document frequency
(IDF) of matched and mismatched words (Itty-
cheriah et al, 2001), cosine similarity between a
question and a passage (Llopis and Vicedo, 2001),
passage/sentence ranking by weighting different
features (Lee and others, 2001), stemming and
morphological query expansion (2004), and vot-
ing between different retrieval methods (Tellex
et al, 2003). As in previous approaches, we
use words and phrases from a question for pas-
sage extraction and experiment with using exactly
matched phrases in addition to words. Similarly
to Lee (2001), we assign weights to sentences in
retrieved documents according to the number of
matched constituents.
Systems vary in the size of retrieved passages.
Some systems identify multi-sentence and variable
size passages (Ittycheriah et al, 2001; Clarke et
al., 2000). An optimal passage size may depend
on the method of answer extraction. We use single
sentence extraction because our system?s semantic
role labeling-based answer extraction functions on
individual sentences.
White and Sutcliffe (2004) performed a man-
ual analysis of questions and answers for 50 of the
TREC questions. The authors computed frequency
of terms matching exactly, with morphological, or
semantic variation between a question and a an-
swer passage. In this work we perform a similar
analysis automatically. We compare frequencies
of phrases and words matching between a question
and candidate sentences.
Query expansion has been investigated in sys-
tems described in (Hovy et al, 2001a; Harabagiu
et al, 2006). They use WordNet (Miller, 1995) for
query expansion, and incorporate semantic roles in
the answer extraction process. In this experiment
we do not expand query terms.
Corpus pre-processing and encoding informa-
tion useful for retrieval was shown to improve doc-
ument retrieval (Katz and Lin, 2003; Harabagiu
et al, 2006; Chu-Carroll et al, 2006). In our
approach we evaluate linguistic question process-
ing technique which does not require corpus pre-
processing.
Statistical machine translation model is used
for information retrieval by (Murdock and Croft,
2005). The model estimates probability of a ques-
tion given an answer and is trained on <question,
candidate sentence> pairs. It capturing synonymy
and grammar transformations using a statistical
model.
3 Data
In this work we evaluate our question answering
system on two datasets: the AQUAINT corpus, a 3
gigabyte collection of news documents used in the
TREC 2006 competition; and the Web.
We use questions from TREC, a yearly ques-
tion answering competition. We use a subset
of questions with non-empty answers 1 from the
TREC 2006 dataset 2. The dataset provides a list
of matching documents from the AQUAINT cor-
pus and correct answers for each question. The
dataset contains 387 questions; the AQUAINT cor-
pus contains an average of 3.5 documents per ques-
1The questions where an answer was not in the dataset
were not used in this analysis
2http://trec.nist.gov/data/qa/t2006 qadata.html
10
tion that contain the correct answer to that ques-
tion. Using correct answers we find the correct
sentences from the matching documents. We use
this information as a gold standard for the IR task.
We index the documents in the AQUAINT cor-
pus using the Lucene (Apache, 2004 2008) engine
on the document level. We evaluate document re-
trieval using gold standard documents from the
AQUAINT corpus. We evaluate sentence extrac-
tion on both AQUAINT and the Web automatically
using regular expressions for correct answers pro-
vided by TREC.
In our experiments we use manually and auto-
matically created phrases. Our automatically cre-
ated phrases were obtained by extracting noun,
verb and prepositional phrases and named entities
from the question dataset using then NLTK (Bird
et al, 2008) and Lingpipe (Carpenter and Bald-
win, 2008) tools. Our manually created phrases
were obtained by hand-correcting these automatic
annotations (e.g. to remove extraneous words and
phrases and add missed words and phrases from
the questions).
4 System
For the experiments in this paper we use the StoQA
system. This system employs a pipeline architec-
ture with three main stages as illustrated in Fig-
ure 1: question analysis, document and sentence
extraction (IR), and answer extraction. After the
user poses a question, it is analyzed. Target named
entities and semantic roles are determined. A
query is constructed, tailored to the search tools in
use. Sentences containing target terms are then ex-
tracted from the documents retrieved by the query.
The candidate sentences are processed to identify
and extract candidate answers, which are presented
to the user.
We use the NLTK toolkit (Bird et al, 2008)
for question analysis and can add terms to search
queries using WordNet (Miller, 1995). Our system
can currently retrieve documents from either the
Web (using the Yahoo search API (Yahoo!, 2008)),
or the AQUAINT corpus (Graff, 2002) (through
the Lucene indexer and search engine (Apache,
2004 2008)). When using Lucene, we can assign
different weights to different types of search term
(e.g. less weight to terms than to named entities
added to a query) (cf. (Lee and others, 2001)).
We currently have two modules for answer ex-
traction, which can be used separately or together.
Candidate sentences can be tagged with named en-
tity information using the Lydia system (Lloyd et
al., 2005). The tagged word/phrase matching the
target named entity type most frequently found is
chosen as the answer. Our system can also extract
answers through semantic role labeling, using the
SRL toolkit from (Punyakanok et al, 2008). In
this case, the tagged word/phrase matching the tar-
get semantic role most frequently found is chosen
as the answer.
Figure 1: Architecutre of our question answering
system
5 Method
5.1 Motivation
Question answering is an engineering-intensive
task. System performance improves as more so-
phisticated techniques are applied to data process-
ing. For example, the IR stage in question an-
swering is shown to improve with the help of tech-
niques like predictive annotations and relation ex-
traction; matching of semantic and syntactic re-
11
Target United Nations
Question What was the number of member nations of the U.N. in 2000?
Named Entity U.N., United Nations
Phrases ?member nations of the U.N.?
Converted Q-phrase ?member nations of the U.N. in 2000?
Baseline Query was the number of member nations of the U.N. in 2000
United Nations
Lucene Query with phrases was the number of member nations of the U.N. in 2000
and NE ?United Nations?, ?member nations of the u.n.?
Cascaded web query
query1 ?member nations of the U.N. in 2000? AND ( United Nations )
query2 ?member nations of the u.n.? AND ( United Nations )
query3 (number of member nations of the U.N. in 2000) AND ( United
Nations )
query4 ( United Nations )
Table 1: Question processing example: terms of a query
lations in a question and a candidate sentence
are known to improve overall QA system perfor-
mance (Prager et al, 2000; Stenchikova et al,
2006; Katz and Lin, 2003; Harabagiu et al, 2006;
Chu-Carroll et al, 2006).
In this work we analyze less resource expensive
techniques, such as chunking and named entity de-
tection, for IR in question answering. Linguistic
analysis in our system is applied to questions and
to candidate sentences only. There is no need for
annotation of all documents to be indexed, so our
techniques can be applied to IR on large datasets
such as the Web.
Intuitively, using phrases in query construction
may improve retrieval precision. For example,
if we search for In what year did the movie win
academy awards? using a disjunction of words
as our query we may match irrelevant documents
about the military academy or Nobel prize awards.
However, if we use the phrase ?academy awards?
as one of the query terms, documents with this
term will receive a higher ranking. A counterargu-
ment for using phrases is that academy and awards
are highly correlated and therefore the documents
that contain both will be more highly ranked. We
hypothesize that for phrases where constituents are
not highly correlated, exact phrase extraction will
give more benefit.
5.2 Search Query
We process each TREC question and target 3 to
identify named entities. Often, the target is a com-
plete named entity (NE), however, in some of the
TREC questions the target contains a named entity,
e.g. tourists massacred at Luxor in 1997, or 1991
eruption of Mount Pinatubo with named entities
Luxor and Mount Pinatubo. For the TREC ques-
tion What was the number of member nations of
the U.N. in 2000?, the identified constituents and
automatically constructed query are shown in Ta-
ble 1. Named entities are identified using Ling-
pipe (Carpenter and Baldwin, 2008), which iden-
tifies named entities of type organization, location
and person. Phrases are identified automatically
using the NLTK toolkit (Bird et al, 2008). We
extract noun phrases, verb phrases and preposi-
tional phrases. The rules for identifying phrases
are mined from a dataset of manually annotated
parse trees (Judge et al, 2006) 4. Converted Q-
phrases are heuristically created phrases that para-
phrase the question in declarative form using a
small set of rules. The rules match a question to a
pattern and transform the question using linguistic
information. For example, one rule matches Who
is|was NOUN|PRONOUN VBD and converts it to
NOUN|PRONOUN is|was VBD. 5
3The TREC dataset alo provides a target topic for each
questions, and we include it in the query.
4The test questions are not in this dataset.
5Q-phrase is extracted only for who/when/where ques-
tions. We used a set of 6 transformation patterns in this ex-
periment.
12
Named Entities Phrases
great pyramids; frank sinatra; mt.
pinatubo; miss america; manchester
united; clinton administration
capacity of the ballpark; groath rate; se-
curity council; tufts university endow-
ment; family members; terrorist organi-
zation
Table 2: Automatically identified named entities and phrases
A q-phrase represents how a simple answer is
expected to appear, e. g. a q-phrase for the ques-
tion When was Mozart born? is Mozart was born.
We expect a low probability of encountering a q-
phrase in retrieved documents, but a high prob-
ability of co-occurrence of q-phrases phrase with
correct answers.
In our basic system (baseline), words (trivial
query constituents) from question and target form
the query. In the experimental system, the query is
created from a combination of words, quoted exact
phrases, and quoted named entities. Table 2 shows
some examples of phrases and named entities used
in queries. The goal of our analysis is to evaluate
whether non-trivial query constituents can improve
document and sentence extraction.
We use a back-off mechanism with both of
our IR subsystems to improve document extrac-
tion. The Lucene API allows the user to cre-
ate arbitrarily long queries and assign a weight to
each query constituent. We experiment with as-
signing different weights based on the type of a
query constituent. Assigning a higher weight to
phrase constituents increases the scores for docu-
ments matching a phrase, but if no phrase matches
are found documents matching lower-scored con-
stituents will be returned.
The query construction system for the Web first
produces a query containing only converted q-
phrases which have low recall and high precision
(query 1 in table 1). If this query returns less than
20 results, it then constructs a query using phrases
(query 2 in table 1), if this returns less than 20 re-
sults, queries without exact phrases (queries 3 and
4) are used. Every query contains a conjunction
with the question target to increase precision for
the cases where the target is excluded from con-
verted q-phrase or an exact phrase.
For both our IR subsystems we return a maxi-
mum of 20 documents. We chose this relatively
low number of documents because our answer ex-
traction algorithm relies on semantic tagging of
candidate sentences, which is a relatively time-
consuming operation.
The text from each retrieved documents is split
into sentences using Lingpipe. The same sen-
tence extraction algorithm is used for the output
from both IR subsystems (AQUAINT/Lucene and
Web/Yahoo). The sentence extraction algorithm
assigns a score to each sentence according to the
number of matched terms it contains.
5.3 Analysis of Constituents
For our analysis of the impact of different linguis-
tic constituent types on document retrieval we use
the TREC 2006 dataset which consists of ques-
tions, documents containing answers to each ques-
tion, and supporting sentences, sentences from
these documents that contain the answer to each
question.
Table 3 shows the number of times each con-
stituent type appears in a supporting sentence and
the proportion of supporting sentences containing
each constituent type (sent w/answer column). The
?All Sentences? column shows the number of con-
stituents in all sentences of candidate documents.
The precision column displays the chance that a
given sentence is a supporting sentence if a con-
stituent of a particular type is present in it. Con-
verted q-phrase has the highest precision, followed
by phrases, verbs, and named entities. Words have
the highest chance of occurrence in a supporting
sentence (.907), but they also have a high chance
of occurrence in a document (.745).
This analysis supports our hypothesis that using
exact phrases may improve the performance of in-
formation retrieval for question answering.
6 Experiment
In these experiments we look at the impact of using
exact phrases on the performance of the document
retrieval and sentence extraction stages of question
answering. We use our StoQA question answering
system. Questions are analyzed as described in the
previous section. For document retrieval we use
the back-off method described in the previous sec-
13
sent w/ answer all sentences precision
num proportion num proportion
Named Entity 907 0.320 4873 0.122 .18
Phrases 350 0.123 1072 0.027 .34
Verbs 396 0.140 1399 0.035 .28
Q-Phrases 11 0.004 15 0.00038 .73
Words 2573 0.907 29576 0.745 .086
Total Sentences 2836 39688
Table 3: Query constituents in sentences of correct documents
avg doc avg doc overall avg overall avg corr avg corr avg corr
sent sent sent sent sent
recall MRR doc recall MRR recall in top 1 in top 10 in top 50
IR with Lucene on AQUAINT dataset
baseline (words disjunction 0.530 0.631 0.756 0.314 0.627 0.223 1.202 3.464
from target and question)
baseline 0.514 0.617 0.741 0.332 0.653 0.236 1.269 3.759
+ auto phrases
words 0.501 0.604 0.736 0.316 0.653 0.220 1.228 3.705
+ auto NEs & phrases
baseline 0.506 0.621 0.738 0.291 0.609 0.199 1.231 3.378
+ manual phrases
words 0.510 0.625 0.738 0.294 0.609 0.202 1.244 3.368
+ manual NEs & phrases
IR with Yahoo API on WEB
baseline - - - 0.183 0.570 0.101 0.821 2.316
words disjunction
cascaded - - - 0.220 0.604 0.140 0.956 2.725
using auto phrases
cascaded - - - 0.241 0.614 0.155 1.065 3.016
using manual phrases
Table 4: Document retrieval evaluation.
tion. We performed the experiments using first au-
tomatically generated phrases, and then manually
corrected phrases.
For document retrieval we report: 1) average re-
call, 2) average mean reciprocal ranking (MRR),
and 3) overall document recall. Each question has
a document retrieval recall score which is the pro-
portion of documents identified from all correct
documents for this question. The average recall
is the individual recall averaged over all questions.
MRR is the inverse index of the first correct doc-
ument. For example, if the first correct document
appears second, the MRR score will be 1/2. MRR
is computed for each question and averaged over
all questions. Overall document recall is the per-
centage of questions for which at least one correct
document was retrieved. This measure indicates
the upper bound on the QA system.
For sentence retrieval we report 1) average sen-
tence MRR, 2) overall sentence recall, 3) average
precision of the first sentence, 4) number of cor-
rect candidate sentences in the top 10 results, and
5) number of correct candidate sentences in the top
50 results 6.
Table 4 shows our experimental results. First,
we evaluate the performance of document retrieval
on the indexed AQUAINT dataset. Average doc-
ument recall for our baseline system is 0.53, in-
dicating that on average half of the correct doc-
uments are retrieved. Average document MRR
is .631, meaning that on average the first correct
document appears first or second. Overall docu-
ment recall indicates that 75.6% of queries con-
tain a correct document among the retrieved docu-
ments. Average sentence recall is lower than docu-
ment recall indicating that some proportion of cor-
rect answers is not retrieved using our heuristic
sentence extraction algorithm. The average sen-
tence MRR is .314 indicating that the first correct
sentence is approximately third on the list. With
6Although the number of documents is 20, multiple sen-
tences may be extracted from each document.
14
the AQUAINT dataset, we notice no improvement
with exact phrases.
Next, we evaluate sentence retrieval from the
WEB. There is no gold standard for the WEB
dataset so we do not report document retrieval
scores. Sentence scores on the WEB dataset are
lower than on the AQUAINT dataset 7.
Using back-off retrieval with automatically cre-
ated phrases and named entities, we see an im-
provement over the baseline system performance
for each of the sentence measures on the WEB
dataset. Average sentence MRR increases 20%
from .183 in the baseline to .220 in the experimen-
tal system. With manually created phrases MRR
improves a further 9.5% to .241. This indicates
that information retrieval on the WEB dataset can
benefit from a better quality of chunker and from a
properly converted question phrase. It also shows
that the improvement is not due to simply match-
ing random substrings from a question, but that
linguistic information is useful in constructing the
exact match phrases. Precision of automatically
detected phrases is affected by errors during auto-
matic part-of-speech tagging of questions. An ex-
ample of an error due to POS tagging is the iden-
tification of a phrase was Rowling born due to a
failure to identify that born is a verb.
Our results emphasize the difference between
the two datasets. AQUAINT dataset is a collec-
tion of a large set of news documents, while WEB
is a much larger resource of information from a
variety of sources. It is reasonable to assume
that on average there are much fewer documents
with query words in AQUAINT corpus than on the
WEB. Proportion of correct documents from all re-
trieved WEB documents on average is likely to be
lower than this proportion in documents retrieved
from AQUAINT. When using words on a query
to AQUAINT dataset, most of the correct docu-
ments are returned in the top matches. Our results
indicate that over 50% of correct documents are
retrieved in the top 20 results. Results in table 3
indicate that exactly matched phrases from a ques-
tion are more precise predictors of presence of an
answer. Using exact matched phrases in a WEB
query allows a search engine to give higher rank to
more relevant documents and increases likelihood
of these documents in the top 20 matches.
Although overall performance on the WEB
dataset is lower than on AQUAINT, there is a po-
7Our decision to use only 20 documents may be a factor.
tential for improvement by using a larger set of
documents and improving our sentence extraction
heuristics.
7 Conclusion and Future Work
In this paper we present a document retrieval ex-
periment on a question answering system. We
evaluate the use of named entities and of noun,
verb, and prepositional phrases as exact match
phrases in a document retrieval query. Our re-
sults indicate that using phrases extracted from
questions improves IR performance on WEB data.
Surprisingly, we find no positive effect of using
phrases on a smaller closed set of data.
Our data analysis shows that linguistic phrases
are more accurate indicators for candidate sen-
tences than words. In future work we plan to evalu-
ate how phrase type (noun vs. verb vs. preposition)
affects IR performance.
Acknowledgment
We would like to thank professor Amanda Stent
for suggestions about experiments and proofread-
ing the paper. We would like to thank the reviewers
for useful comments.
References
Apache. 2004-2008. Lucene.
http://lucene.apache.org/java/docs/index.html.
Bilotti, M., B. Katz, and J. Lin. 2004. What works
better for question answering: Stemming or morpho-
logical query expansion? In Proc. SIGIR.
Bird, S., E. Loper, and E. Klein. 2008.
Natural Language ToolKit (NLTK).
http://nltk.org/index.php/Main Page.
Carpenter, B. and B. Baldwin. 2008. Lingpipe.
http://alias-i.com/lingpipe/index.html.
Chu-Carroll, J., J. Prager, K. Czuba, D. Ferrucci, and
P. Duboue. 2006. Semantic search via XML frag-
ments: a high-precision approach to IR. In Proc.
SIGIR.
Clarke, C., G. Cormack, D. Kisman, and T. Lynam.
2000. Question answering by passage selection
(multitext experiments for TREC-9). In Proc. TREC.
Collins-Thompson, K., J. Callan, E. Terra, and C. L.A.
Clarke. 2004. The effect of document retrieval qual-
ity on factoid question answering performance. In
Proc. SIGIR.
Dang, H., J. Lin, and D. Kelly. 2006. Overview of
the TREC 2006 question answering track. In Proc.
TREC.
15
Graff, D. 2002. The AQUAINT corpus of English
news text. Technical report, Linguistic Data Con-
sortium, Philadelphia, PA, USA.
Harabagiu, S., A. Hickl, J. Williams, J. Bensley,
K. Roberts, Y. Shi, and B. Rink. 2006. Question
answering with LCC?s CHAUCER at TREC 2006.
In Proc. TREC.
Hovy, E., L. Gerber, U. Hermjakob, M. Junk, and C.-Y.
Lin. 2001a. Question answering in Webclopedia. In
Proc. TREC.
Hovy, E., U. Hermjakob, and C.-Y. Lin. 2001b. The
use of external knowledge in factoid QA. In Proc.
TREC.
Ittycheriah, A., M. Franz, and S. Roukos. 2001. IBM?s
statistical question answering system ? TREC-10. In
Proc. TREC.
Judge, J., A. Cahill, and J. van Genabith. 2006.
QuestionBank: Creating a corpus of parse-annotated
questions. In Proc. ACL.
Katz, B. and J. Lin. 2003. Selectively using relations to
improve precision in question answering. In Proc. of
the EACL Workshop on Natural Language Process-
ing for Question Answering.
Lee, G. G. et al 2001. SiteQ: Engineering high per-
formance QA system using lexico-semantic pattern
matching and shallow NLP. In Proc. TREC.
Light, M., G. S. Mann, E. Riloff, and E. Breck. 2001.
Analyses for elucidating current question answering
technology. Journal of Natural Language Engineer-
ing, 7(4).
Llopis, F. and J. L. Vicedo. 2001. IR-n: A passage re-
trieval system at CLEF-2001. In Proc. of the Second
Workshop of the Cross-Language Evaluation Forum
(CLEF 2001).
Lloyd, L., D. Kechagias, and S. Skiena. 2005. Ly-
dia: A system for large-scale news analysis. In Proc.
SPIRE, pages 161?166.
Miller, George A. 1995. WordNet: a lexical database
for english. Communications of the ACM, 38(11).
Murdock, V. and W. B. Croft. 2005. Simple transla-
tion models for sentence retrieval in factoid question
answering. In Proc. SIGIR.
Prager, J., E. Brown, and A. Coden. 2000. Question-
answering by predictive annotation. In ACM SIGIR.
QA -to site.
Punyakanok, V., D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in seman-
tic role labeling. Computational Linguistics, 34(2).
Srihari, R. and W. Li. 1999. Information extraction
supported question answering. In Proc. TREC.
Stenchikova, S., D. Hakkani-Tur, and G. Tur. 2006.
QASR: Question answering using semantic roles for
speech interface. In Proc. ICSLP-Interspeech 2006.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative evaluation of passage retrieval al-
gorithms for question answering. In Proc. SIGIR.
Vorhees, V. and D. Harman. 1999. Overview of the
eighth Text REtrieval Conference (TREC-8). In
?Proc. TREC?.
White, K. and R. Sutcliffe. 2004. Seeking an upper
bound to sentence level retrieval in question answer-
ing. In Proc. SIGIR.
Yahoo!, Inc. 2008. Yahoo! search API.
http://developer.yahoo.com/search/.
16
Harvesting Re-usable High-level Rules
for Expository Dialogue Generation
Svetlana Stoyanchev
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
s.stoyanchev@open.ac.uk
Paul Piwek
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
p.piwek@open.ac.uk
Abstract
This paper proposes a method for extract-
ing high-level rules for expository dialogue
generation. The rules are extracted from di-
alogues that have been authored by expert
dialogue writers. We examine the rules that
can be extracted by this method, focusing on
whether different dialogues and authors ex-
hibit different dialogue styles.
1 Introduction
In the past decade, a new area of Natural Language
Generation (NLG) has emerged: the automated gen-
eration of expository dialogue, also often referred to
as scripted, authored or fictive dialogue. Research in
this area began with the seminal study by Andre? et
al. (2000), which explored generation of dialogues
between a virtual car buyer and seller from technical
data on a car. This strand of work was developed fur-
ther in the NECA project (van Deemter et al, 2008)
and has since been extended to other domains, in-
cluding explanation of medical histories (Williams
et al, 2007), patient information leaflets (Piwek et
al., 2007) and Wall Street Journal articles (Hernault
et al, 2008).
Systems for generating expository dialogue have
explored different inputs (databases, knowledge rep-
resentations and text), generation methods (e.g.,
rule versus constraint-based approaches) and out-
puts (from dialogue scripts in text form to audio and
computer-animated dialogue). A common trait of all
these systems is, however, that at some point in the
generation process, they produce a dialogue script, a
text file which specifies what the interlocutors say,
possibly enriched with mark-up for dialogue acts,
speech and gestures ? see, e.g., Piwek et al (2002).
These systems are different from conventional dia-
logue systems in that the system does not engage in
a dialogue with the user; rather, the system generates
a dialogue between two or more fictitious charac-
ters for the user/audience to view and learn from. In
other words, the dialogue is used to deliver informa-
tion to the user or audience, rather than between the
interlocutors. Piwek (2008) discusses several empir-
ical studies that identify benefits of the use of expos-
itory dialogue for education and persuasion.
In this paper, we take a step towards addressing
two shortcomings of the work so far. Firstly, all
the work cited has relied on hand-crafted resources
(typically rules) for creating the dialogue. With the
resources being created by non-expert dialogue au-
thors (e.g., academic researchers), generated dia-
logues based on these resources may not be optimal;
for instance, Williams et al (2007) found that gener-
ated dialogues can be too information-dense, requir-
ing conversational padding. Secondly, the resources
for creating dialogue are tied to a specific domain,
making it hard to redeploy a system in new domains.
We propose to address the first issue by automat-
ically creating dialogue generation resources from a
corpus of dialogues written by known effective dia-
logue authors. This fits in with a trend in dialogue
modelling and generation to create resources from
empirical data (Oh and Rudnicky, 2002; DeVault et
al., 2008; Henderson et al, 2008; Belz and Kow,
2009).
The second issue is addressed by specifying di-
alogue generation rules at a level of detail that ab-
stracts over the particulars of the domain and fits in
with existing NLG architectures. The reference ar-
chitecture of Reiter and Dale (2000) identifies three
principal NLG tasks: Document Planning (DP),
Microplanning and Realisation. DP is primarily
non-linguistic: it concerns selection of information
and organization of this information into a coherent
whole. The latter is achieved by making sure that
the information is tied together by Rhetorical Rela-
tions such as Contrast, Elaboration and Explanation,
in other words, it is part of a Rhetorical Structure.
We propose that dialogue generation rules interface
with Rhetorical Structure and map to a Sequence of
Dialogue Acts.
Interestingly, the interface between DP and Mi-
croplanning has also been identified as a place where
decisions and preferences regarding style take an ef-
fect (McDonald and Pustejovsky, 1985). A ques-
tion that we explore in this paper is whether dialogue
styles exist at the highly abstract level we focus on
in this paper. We concentrate on style in the sense of
?[t]he manner of expression characteristic of a par-
ticular writer?1.
The remainder of this paper is set up as follows.
In Section 2, we introduce the corpus that we use to
extract dialogue generation resources. Section 3 ex-
amines the dialogues in the corpus for prima facie
evidence for stylistic differences between authors at
the dialogue level. In Section 4, we describe our ap-
proach to extracting high-level dialogue generation
rules from the corpus. Next, in Section 5 we anal-
yse the resulting rules, looking for further evidence
of different dialogue styles. We also compare the
rules that were harvested from our corpus with hand-
crafted rules in terms of content and variety. Finally,
Section 6 contains our conclusions and a discussion
of avenues for further research.
2 A Parallel Monologue-Dialogue Corpus
The current work makes use of a corpus of human-
authored dialogues, the CODA corpus.2 In total, this
corpus consist of about 800 dialogue turns. This
1From definition 13.a. of the Oxford English Dictionary at
http://dictionary.oed.com
2Further information on the construction of this cor-
pus can be found in the annotation manual at comput-
ing.open.ac.uk/coda/AnnotationManual.pdf.
paper is based on three dialogues from the cor-
pus: George Berkeley?s ?Dialogues between Hylas
and Philonous? (extract of 172 turns), Mark Twain?s
?What is man?? (extract of 445 turns) and Yuri Gure-
vich?s ?Evolving Algebras? (extract of 89 turns).
Berkeley?s dialogue is one of the classics of philoso-
phy, arguing for the, at first sight, extravagant claim
that ?there is no such thing as material substance in
the world?. Twain, according to the Encyclopaedia
Britannica ?one of America?s best and most beloved
writers?, takes on the concept of free will. Gure-
vich?s dialogue deals with the mathematical concept
of evolving algebras. Of these dialogues, Twain is
by a large margin the longest (over 800 turns in total)
and the only one which is aimed specifically at the
general public, rather than an academic/specialist
audience.
For each of the dialogues, the corpus also con-
tains human-authored monologue which expresses
the same content as the dialogue. Monologue and
dialogue are aligned through mappings from mono-
logue snippets to dialogue spans. As a result, the
CODA corpus is a parallel monologue-dialogue cor-
pus. Both the monologue and dialogue come with
annotations: the monologue with Rhetorical Struc-
ture Theory (RST) relations (Mann and Thompson,
1988; Carlson and Marcu, 2001) and the dialogue
side with an adaptation of existing Dialogue Act an-
notation schemes (Carletta et al, 1997; Core and
Allen, 1997). Table 2 contains an overview of these
RST relations and Dialogue Act labels.
3 Dialogue Analysis
In this section we examine whether there is prima
facie evidence for differences in style between the
three dialogues. Whereas existing work in NLG on
style has focused on lexical and syntactic choice,
see Reiter and Williams (2008), here we focus on
higher-level characteristics of the dialogues, in par-
ticular, proportion of turns with multiple dialogue
acts, frequencies of dialogue act bigrams, and rela-
tion between dialogue acts and speaker roles.
An important reason for determining whether
there are different styles involved, is that this has
implications for how we use the corpus to create
expository dialogue generation resources. If differ-
ent dialogues employ different styles, we need to be
RST relations Dialogue Acts
Enablement, Cause, Evaluation (Subjective, Inferred),
Comment, Attribution, Condition-Hypothetical, Contrast,
Comparison, Summary, Manner-means, Topic-Comment
(Problem-Solution, Statement-Response, Question-
Answer, Rhetorical Question) Background, Temporal,
Elaboration/Explanation, (Additional, General-Specific,
Example, Object-attribute, Definition, Evidence, Reason),
Same-unit
Explain, Info-Request (Init-Factoid-
InfoReq, Init-YN-InfoReq, Init-
Complex-InfReq), Init-Request-
Clarify, Response-Answer (Resp-
Answer-Yes/No, and Resp-Answer-
Factoid), Resp-Agree, Resp-
Contradict
Table 1: RST relations and Dialogue Acts used in the CODA corpus. Annotators used the fine-grained
categories in italics that are listed in brackets. For the current study, we rely, however, on the higher-level
categories that preceed the fine-grained categories and which combine several of them.
careful with creating resources which combine data
from different dialogues. Merging such data, if any-
thing, may lead to the generation of dialogues which
exhibit features from several possibly incompatible
styles. Since our aim is specifically to generate dia-
logues that emulate the masters of dialogue author-
ing, it is then probably better to create resources
based on data from a single master or dialogue.
3.1 Multi-act Turns
One of the characteristics of dialogue is the pace
and the amount of information presented in each
of the speaker?s turns. In a fast-paced dialogue
turns are concise containing a single dialogue act.
Such dialogues of the form A:Init B:Response A:Init
B:Response ... are known as ?pingpong? dialogue.
Twain?s ?What is man?? dialogue starts in this fash-
ion (O.M. = Old Man; Y.M = Young Man):
O.M. What are the materials of
which a steam-engine is made?
Y.M. Iron, steel, brass, white-metal,
and so on.
O.M. Where are these found?
Y.M In the rocks.
O.M. In a pure state?
Y.M. No?in ores.
. . .
One character serves as the initiator and the other
replies with a response. With turns that contain more
than one dialogue, henceforth multi-act turns, this
pattern can be broken:
O.M. . . .
And you not only did not make that
Author Twain Gurevich Berkeley
Multi-act 34% 43% 24%
Layman/Expert 45%/55% 36%/64% 51%/49%
Table 2: Proportion of multi-act utterances and their
distribution between Layman and Expert
machinery yourself, but you have NOT
EVEN ANY COMMAND OVER IT.
Y.M. This is too much.
You think I could have formed no
opinion but that one?
O.M. Spontaneously? No. And . . .
Multi-act turns are turns comprised of multiple dia-
logue acts, such as the Young Man?s in the exam-
ple above, where a Resp-Contradict (?This is too
much.?) is followed by an Init-YN-Request (?You
think I could have formed no opinion but that one??).
The dialogue pace may vary throughout a dia-
logue. We, however, find that overall proportions
of multi-act turns and their distribution between ex-
pert and layman vary between the authors (see Ta-
ble 2). Gurevich?s dialogue has the highest propor-
tion (43%) of multi-act turns and majority of them
are attributed to the expert. Only 24% of Berkeley?s
dialogue turns consist of multiple dialogue acts and
they are evenly split between the expert and the lay-
man. Gurevich?s dialogue is the type of dialogue
where an expert gives a lesson to a layman while
in Berkeley?s dialogue one character often comple-
ments ideas of the other character making it difficult
to determine which of the characters is an expert.
The amount of multi-act turns seems to be one of
the stylistic choices made by a dialogue author.
3.2 Dialogue Diversity
Figure 1: Bigram coverage for the 1-st to 4th most
frequent bigrams.
Dialogues are essentially a sequence of turns,
where each turn consists of one or more dialogue
acts. For our measure of dialogue diversity we focus
on two-turn sequences (i.e., turn bigrams), where a
turn is identified by the sequence of dialogue acts it
contains.
We define bigram coverage for i as the percent-
age that the top i most frequent bigrams contribute
to all bigrams in the corpus. Diversity of the dia-
logue is inversely related to the dialogue coverage.
In a dialogue with minimal diversity, the same turn,
consisting of one or more dialogue acts, is repeated
throughout the dialogue. The turn bigram consisting
of two such turns has 100% bigram coverage.
Figure 1 shows the coverage for 1 ? i ? 4 for
each author in the corpus.3 Out of the three authors,
Twain?s dialogues are the most diverse where the top
4 bigrams constitute only 15% of all bigrams. In
Gurevich?s dialogues the four most frequent bigrams
constitute 25% and in Berkeley 40%.
Note that for all three authors the dialogue cov-
erage for the 4 most frequent bigrams is quite low
indicating high variability in bigrams used. To
achieve such variability in automatically generated
dialogues we need a large number of distinct gener-
ation rules.
3This range was chosen for illustration purposes. Bigram
coverage can be compared for any i ?total number of distinct
bigrams.
3.3 Dialogue Acts and Speaker Roles
One of the most frequent bigrams for all three au-
thors was, not unexpectedly, the sequence:
A: InfoRequest
B: Response-Answer
There is, however, a difference in the roles of speak-
ers A and B. In all dialogues, one of the speakers
took on the expert role and the other the layman role.
For the aforementioned bigram, both in Berkeley?s
and Gurevich?s dialogues the layman typically ini-
tiates the request for information and the expert re-
sponds (and often goes on to explain the response in
Gurevich?s dialogue):
Q: Is it difficult to define basic
transition rules in full generality?
A: No. Here is the definition.
? Any local function update is a rule.
. . .
(From Gurevich?s dialogue)
In contrast, in Twain?s dialogues the roles are typ-
ically reversed: the expert asks and the layman re-
sponds:
O.M. Then the impulse which moves you
to submit to the tax is not ALL
compassion, charity, benevolence?
Y.M. Well?perhaps not.
Both techniques allow the author to convey a par-
ticular piece of information, but each giving rise its
very own dialogue style.
4 Approach to Rule Extraction
Comparing statistics for individual dialogues gives
us some idea about whether different styles are in-
volved. The true test for whether different styles are
involved is, however, whether for the same content
different realizations are generated. Unfortunately,
for our three dialogues the content is different to be-
gin with. The parallel corpus allows us, however, to
get around this problem. From the parallel corpus
we can extract rules which map RST structures to
dialogue act sequences. The Lefthand Side (LHS)
of a rule represents a particular rhetorical structure
found in the monologue side, whereas the Right-
hand Side (RHS) of the rule represents the dialogue
act sequence with which it is aligned in the corpus.
Such rules can be compared between the different
dialogues: in particular, we can examine whether the
same LHS gives rise to similar or different RHSs.
4.1 Comparison with previous work
Hernault et al (2008) manually construct surface-
level rules mapping monologue to dialogue.
Surface-level rules execute text-to-text conversion
operating directly on the input string. In our ap-
proach, we separate the conversion into two stages.
A first stage converts RST structures to Dialogue
Act sequences. A second stage, which is beyond
the scope of this paper, converts Dialogue Act se-
quences to text.
A further difference between the current approach
and Hernault et al?s is that the LHS of our rules
can match nested RST structures. This covers, what
we call, simple rules (involving a single RST re-
lation, e.g., Contrast(X,Y)) and complex rules (in-
volving 2 or more nested RST relations, e.g., Con-
trast(Condition(X,Y),Z)). Hernault et al only allow
for simple rules. A detailed comparison between our
approach and that of Hernault et al, using the attri-
bution rule as an example, can be found in Section
5.3.
id DA turns
0 Init-YN-
InfoReq
Is your mind a part of your PHYSI-
CAL equipment ?
0 Resp-
Answer-No
No.
1 Explain It is independent of it ; it is spiritual
2 Init-YN-
InfoReq
Being spiritual, it cannot be af-
fected by physical influences?
2 Resp-
Answer-No
No.
3 Init-YN-
InfoReq
Does the mind remain sober with
the body is drunk ?
- decorative Well?
3 Resp-
Answer-No
No.
Table 3: Example of annotated dialogue (from Mark
Twain?s ?What is man??).
4.2 Rule Extraction Algorithm
Table 3 and Figure 2 show annotated dialogue (au-
thored by Twain) and its annotated monologue trans-
lation. Each terminal node of the RST structure
corresponds to a part of a monologue snippet. All
nodes with the same id correspond to a complete
Condition
Attribution
id=2
Contrast
id=0
id=1
Being spiritual,
by phisical influences.
nuc
id=3
Let?s for a minute 
assume that
Explanation
it can not be affected
your mind is not partid=0
of your physical equipment, it is spiritual.that it is independent of it,
However, 
the mind    does not
remain sober
when the bodyis drunk.
nuc
nuc
Figure 2: RST structure for the translation of dia-
logue in Table 3
span rule
0-0 Attribution(0, 0)
0-1 Attribution( Explanation(0, 1))
2-3 Contrast(2, 3)
0-3 Condition (Attribution( Ex-
plain(0, 1)), Contrast(2, 3))
Table 4: RST sub-structures: LHS of monologue-to-
dialogue mapping rules
snippet and are linked to the dialogue act(s) with the
same ids. The relation between monologue snippets
and dialogue act segments is one-to-many. In other
words, one snippet (e.g. snippets with id=0, id=2)
can be expressed by multiple dialogue act segments.
Rules are extracted as follows: For each (auto-
matically extracted) sub-structure of the RST struc-
tures on the monologue side, a rule is created (see
Table 4). Two constraints restrict extraction of sub-
structures: 1) spans of the structure?s terminal nodes
must be consecutive and 2) none of the ids of the
terminal nodes are shared with a node outside the
sub-structure.
For example, Explanation(0, 1) is not extracted
because the node with id=0 appears also under the
Attribution relation which is not a part of this sub-
structure.
Additionally, rules are generated by removing a
relation and its satellite node and moving a nucleus
node one level up. Attribution(0, 0) was extracted
from a tree that had the Explanation relation and its
satellite child 1 pruned. This operation relies on the
validity of the following principle for RST (Marcu,
1997): ?If a relation holds between two textual spans
of the tree structure of a text, that relation also holds
between the most important units of the constituent
subspans.?
The RST sub-structure is the LHS of a rule and
dialogue act sequences are the RHS of a rule.
5 Results: Analysis of the Rules
In this section we describe the rules collected from
the corpus. We compare the rules collected from the
dialogues of different authors. We also compare the
rules constructed manually in previous work with
the rules collected from the corpus, specifically for
the attribution relation.
5.1 Rule Statistics
relation Twain Gurev Berk all
simple 31 (33) 29 (38) 25 (26) 81 (97)
complex 19 26 16 61 (61)
null 15 (22) 9 (18) 9 (27) 25 (67)
total 65 64 50 167
# turns 85 78 96 259
Table 5: Numbers of extracted distinct structural
rules (total occurrences are parenthesized)
relation Twain Gurevich Berkley
attribution 15% 2% 12%
contrast 18% 9% 17%
expl/elab 34% 47% 26%
eval 9% 6% 21%
other 24% 36% 24%
total 100% 100% 100%
Table 6: Proportions of relations expressed as rules
relation Twain Gurevich Berkley
overall 2.4 1.9 2.9
contrast 2.3 2 2.6
elab/expl 2.7 1.7 3.3
eval 2 2 2.5
Table 7: Average number of turns in simple rules
Simple rules are the rules with one RST relation in
the LHS. Complex rules are the rules with multiple
RST relations in the LHS. In Table 4, rules for the
LHS 0-0 and 2-3 are simple while the rules for 0-1
and 0-3 are complex. Null rules are the rules with no
RST relation in the LHS.
From our sample of 259 translated and annotated
dialogue turns from the corpus, we extracted 81 sim-
ple, 61 complex, and 25 null rules (null rules involve
no RST structure and are discussed below). Table 5
shows the number of distinct rules per author.4 In
parentheses we show the number of actual (not nec-
essarily distinct) rule occurrences in corpus. The
majority of simple rules in the corpus (65 out of 81)
occur only once.5 This shows that the dialogue au-
thors use a variety of dialogue act sequences when
presenting their arguments in dialogue.
To compare dialogue styles we compare the rules
across the dialogues of different authors. Table 6
shows the proportions of relation types in each au-
thor?s dialogues that are mapped to a dialogue struc-
ture and produce a mapping rule.6 Not all relations
in monologue are mapped to a dialogue structure.
For example, Explain moves may contain multiple
clauses that are presented by a single character in
the same turn. We find differences in distributions
of relation types mapped to dialogue between the
three authors (Fisher?s exact test p<.01). Berkeley?s
dialogues produce more mapping rules with Eval-
uation and less with Explanation/Elaboration rela-
tions than the other two authors. Gurevich?s di-
alogues produce less mapping rules with Attribu-
tion and Contrast relations than the other two au-
thors. This difference between distributions of re-
lation types mapped to dialogue has an important
implication for dialogue generation. Dialogue gen-
eration programs may vary the style of a dialogue
by choosing which discourse relations of the mono-
logue are mapped to dialogue turns.
Another relevant property of a rule is the number
of turns in the RHS of the rule. Number of turns in a
rule shows how many times the dialogue characters
switch to present information of the monologue cor-
responding to the LHS of the rule. The average num-
bers of turns in the RHS of all rules of the Twain,
Gurevich, and Berkeley dialogues are 2.4, 1.9, 2.9
respectively (see Table 7). They are all pairwise sig-
nificantly different (t-test p < .05) ranking the au-
4Two rules are distinct if either their LHS (relation in mono-
logue) or RHSs (sequence of dialogue acts) are different.
565=81-(97-81)
6This includes simple and complex rules
thors in the order Gurevich < Twain < Berkeley
according to the number of turns in the RHS of the
rule. Similar ranking also appears as a trend for in-
dividual relations suggesting that this is the effect of
the author?s style rather than the relations (the dis-
tribution of relation types is different across the au-
thors). This suggests that dialogue generation may
affect the style of automatically generated dialogue
by selectively choosing rules with longer (or shorter)
RHS.
5.2 Null Rule
A null rule is a rule where a sequence of dialogue
turns between two characters corresponds with a text
segment with no rhetorical relation. A text segment
without a rhetorical relation corresponds to a leaf
node in the RST structure. A null rule typically cre-
ates a dialogue fragment consisting of a yes/no ques-
tion (Init-YN-Info-Req) followed by yes/no answer,
or a complex information request (e.g. What is your
opinion on X?) followed by an Explain dialogue act,
or a presentation of an argument (Explain dialogue
act) followed by a response that signals agreement
(Resp-Agree). Null rules create more interactivity in
the dialogue.
The monologue segment corresponding to the
LHS of a null rule may be in a rhetorical relation
with another segment, such that the LHS of the null
rule is embedded into another rule. Table 8 shows an
example of a null rule embedded in a contrast rule.
Turns 1 - 3 correspond to the RHS of the Null rule
and 1 - 4 correspond to the RHS of the Contrast rule.
Null rules can be used to turn information into
dialogue, even when there is no RST relation. For
example, we may want to convey the piece of in-
formation A,B,C,D,E in that order, with rel1(A,B)
and rel2(D,E). Whereas a simple rule may apply to
relations and turn them into dialogue, C is left un-
touched. However, a null rule can be applied to C, to
also turn its presentation into a dialogue exchange.
5.3 Case Study: the Attribution Rule
In this section we present a comparison of manu-
ally created rules for the RST attribution relation and
rules extracted from the CODA corpus.
Hernault et al manually construct two surface-
level rules for the Attribution (S,N)7 relation (see
7N is a nucleus phrase that carries main information and S is
Table 9). In the Dialogue Act column we show
the dialogue act representation of the correspond-
ing surface-level rules. The first rule converts attri-
bution relation into a Complex-Info-Request by the
Layman followed with the Explain by the Expert.
The second rule converts the attribution relation into
Explain by the Expert, Factoid-Info-Request by the
Layman and Factoid-Response by Expert. In both
rules, the Expert is the one providing information
(N) to the Layman and information is presented in
Explain dialogue act
Table 10 shows six attribution rules we collected
from phrases with attribution relation in the corpus
(Twain1-4,Berkeley1,Gurevich)8. We notice several
differences with the manually constructed rules:
? The variety of dialogue act sequences: each
RHS of the rule (or dialogue act sequence) is
different.
? Main information (N) can be presented by
either the expert (Twain1, Twain2, Twain3,
Berkeley1) or by the layman (Twain4, Gure-
vich1).
? Main information (N) can be presented in
different dialogue acts: Explain dialogue act
(Twain1, Twain4, Berkeley), YN-Info-Request
(Twain2, Twain3), or Complex-Info-Request
(Gurevich).
? Contextual information is part of the rule and
may be used when choosing which rule to ap-
ply.
6 Conclusions and Further Work
In this paper, we have introduced a new approach to
creating resources for automatically generating ex-
pository dialogue. The approach is based on ex-
tracting high-level rules from RST relations to Di-
alogue Act sequences using a parallel Monologue-
Dialogue corpus. The approach results in rules that
are reusable across applications and based on known
expert dialogue authors.
After examining differences between the dia-
logues in the corpus in order to obtain prima facie
evidence for differences in style, we conducted a
detailed evaluation of the rules that were extracted
a satellite phrase that contains the entity to whom N is attributed
8These are all the rules for attribution RST relation from 50
annotated turns for each author
Turn Speaker Dialogue act Dialogue
Contrast rule. Segment with contrast relation:
[He never does anything for any one else?s comfort , spiritual or physical.] [EXCEPT ON THOSE DISTINCT TERMS
? that it shall FIRST secure HIS OWN spiritual comfort ].
Null rule. Segment without rhetorical relation:
He never does anything for any one else?s comfort , spiritual or physical
1 Layman decorative Come!
2 Expert Init-YN-Request He never does anything for any one else ? s comfort , spiritual or physical ?
3 Expert Resp-Answer-No No
4 Expert Explain EXCEPT ON THOSE DISTINCT TERMS ? that it shall FIRST secure HIS
OWN spiritual comfort .
Table 8: Contrast rule example containing null rule from Twain dialogue.
Rule 1
Speaker Surface-level Rule Dialogue act Example Dialogue
Layman What did + GetSubject(S+N) + Getmain-
VerbLemma(S+N)
Complex-Info-Request What did S say?
Expert AddifNotPresentIn(N, That) + N Explain N
Rule 2
Expert RemoveIfPresentIn(N, That) + N Explain N
Layman Who GetMainVerb(N) that? Factoid-Info-Req Who said that?
Expert GetSubjectFromSentence(S+N) Factoid-Response S did
Table 9: Manually created rules for Attribution(S,N) relation (Hernault et al, 2008)
from the corpus. We extracted 167 distinct rules and
discussed the three types of rules: null, simple and
complex (depending on the number of RST relation
in the LHS: 0, 1 or more).
We found differences between authors in several
respects, specifically:
? number of turns per simple rule
? number of dialogue acts per simple rule
? combination of speaker roles and dialogue acts
A detailed comparison between our automatically
extracted attribution rule and the hand-crafted rules
used by Hernault et al showed up a number of
differences. Apart from the fact that the corpus
yielded many more rules than the two manually cre-
ated ones, there were differences in which interlocu-
tor presented particular information and which dia-
logue acts were being used.
The current work has focussed on high-level map-
ping rules which can be used both for generation
from databases and knowledge representations and
also for generation from text. In future work, we
will focus on mapping text (in monologue form) to
dialogue. For this we need to combine the high-
level rules with rules for paraphrasing the text in the
monologue with text for the dialogue acts that ex-
press the same information in dialogue form. For
automatically extracting these surface level map-
pings we will draw on the approach to learning para-
phrases from a corpus that is described in Barzilay
and McKeown (2001). An important component of
our future effort will be to evaluate whether automat-
ically generating dialogues from naturally-occurring
monologues, following the approach described here,
results in dialogues that are fluent and coherent and
preserve the information from the input monologue.
Acknowledgements
We would like to thank the anonymous reviewers
of INLG2010 for their helpful comments and our
colleagues in the Open University?s NLG group
for stimulating discussions on the content of this
paper. The research reported in this paper was
carried out as part of the CODA project (CO-
herent Dialogue Automatically generated from
text; see http://computing.open.ac.uk/coda/)
which is funded by the UK Engineering and
Physical Sciences Research Council under grant
EP/G/020981/1.
References
E. Andre?, T. Rist, S. van Mulken, M. Klesen, and
S. Baldes. 2000. The automated design of believable
dialogues for animated presentation teams. In Em-
Speaker Dialogue act Dialogue
Twain1 I will put that law into words, keep it in your mind: FROM HIS CRADLE TO HIS GRAVE A MAN NEVER DOES...
Satellite of Summary
Layman Init-YN-InfoReq Will you put that law into words?
Expert Resp-Answer-Yes Yes.
Expert Resp-Explain This is the law, keep it in your mind. FROM HIS CRADLE TO HIS GRAVE A
MAN NEVER DOES...
Twain2 I can not imagine that there is some other way of looking at it. Satellite of Explanation
Expert Init-Complex-InfoReq /clarify What makes you think that?
Layman decorative Pray what else could I think?
Expert Init-YN-InfoReq Do you imagine that there is some other way of looking at it?
Twain3 One cannot doubt that he felt well.Satellite of Evaluation-Conclusion
Expert Init-YN-InfoReq He felt well?
Layman Resp-Answer-Yes One cannot doubt it.
Twain4 As I said a minute ago Hamilton fought that duel to get PUBLIC approval. Nucleus of Explanation
Layman Init-Explain/contradict A minute ago you said Hamilton fought that duel to get PUBLIC approval.
Resp-Agree Resp-Agree I did.
Berkeley1 You can not conceive a vehement sensation to be without pain or pleasure.
Expert Init-Explain Again, try in your thoughts, Hylas, if you can conceive a vehement sensation to
be without pain or pleasure.
Layman Resp-Contradict You can not.
Gurevich I will explain what static algebras are exactly. Nucleus of Statement-response
Layman Init-Complex-InfoReq Please explain to me what static algebras are exactly.
Expert Resp-Agree Gladly.
Table 10: Attribution Examples. Satellite is italicised.
bodied Conversational Agents, pages 220?255. MIT
Press, Cambridge, Mass.
R. Barzilay and K. McKeown. 2001. Extracting Para-
phrases from a Parallel Corpus. In Proceedings of the
ACL, Toulouse, France.
A. Belz and E. Kow. 2009. System Building Cost vs.
Output Quality in Data-to-Text Generation. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation (ENLG?09), Athens, Greece.
J. Carletta, A. Isard, and J. C. Kowtko. 1997. The relia-
bility of a dialogue structure coding scheme. Compu-
tational Linguistics, 23:13?31.
L. Carlson and D. Marcu. 2001. Discourse tagging
reference manual. Technical Report ISI-TR-545, ISI,
September.
M. Core and J. Allen. 1997. Coding dialogs with the
damsl annotation scheme. In Working Notes: AAAI
Fall Symposium on Communicative Action in Humans
and Machine.
D. DeVault, D. Traum, and R. Artstein. 2008. Making
Grammar-Based Generation Easier to Deploy in Dia-
logue Systems. In Procs SIGdial 2008, Ohio, June.
E.Reiter and S. Williams. 2008. Three approaches to
generating texts in different styles. In Proceedings of
the Symposium on Style in text: creative generation
and identification of authorship.
J. Henderson, O. Lemon, and K. Georgila. 2008. Hy-
brid Reinforcement / Supervised Learning of Dialogue
Policies from Fixed Datasets. Computational Linguis-
tics, 34(4):487?511.
H. Hernault, P. Piwek, H. Prendinger, and M. Ishizuka.
2008. Generating dialogues for virtual agents using
nested textual coherence relations. In IVA08: 8th In-
ternational Conference on Intelligent Virtual Agents.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
D. Marcu. 1997. From Discourse Structures to
Text Summaries. In The Proceedings of the
ACL?97/EACL?97 Workshop on Intelligent Scalable
Text Summarization, pages 82?88, Madrid, Spain.
D. McDonald and J. Pustejovsky. 1985. A computational
theory of prose style for natural language generation.
In Proceedings of the second conference on European
chapter of the Association for Computational Linguis-
tics, pages 187?193, Geneva, Switzerland.
A. Oh and A. Rudnicky. 2002. Stochastic natural lan-
guage generation for spoken dialog. Computer Speech
and Language, 16(3/4):387?407.
P. Piwek, B. Krenn, M. Schroeder, M. Grice, S. Bau-
mann, and H. Pirker. 2002. RRL: A Rich Repre-
sentation Language for the Description of Agent Be-
haviour in NECA. In Proceedings of the AAMAS work-
shop ?Embodied conversational agents - let?s specify
and evaluate them!?, Bologna, Italy, July.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. T2D: Generating Dialogues between Virtual
Agents Automatically from Text. In Intelligent Virtual
Agents, LNAI 4722, pages 161?174. Springer Verlag.
P. Piwek. 2008. Presenting Arguments as Fictive Dia-
logue. In Proceedings of 8th Workshop on Computa-
tional Models of Natural Argument (CMNA08), Patras,
Greece, July. ISBN 978-960-6843-12-9.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press, Cambridge.
K. van Deemter, B. Krenn, P. Piwek, M. Klesen,
M. Schroeder, and S. Baumann. 2008. Fully gener-
ated scripted dialogue for embodied agents. Artificial
Intelligence Journal, 172(10):1219?1244.
S. Williams, P. Piwek, and R. Power. 2007. Generat-
ing Monologue and Dialogue to Present Personalised
Medical Information to Patients. In Procs ENLG
2007, pages 167?170, Schloss Dagstuhl, Germany.
The First Question Generation Shared Task Evaluation 
Challenge 
Vasile Rus1, Brendan Wyse2, Paul Piwek2, Mihai Lintean1, Svetlana Stoyanchev2 
and Cristian Moldovan1
 
1 Department of Computer Science/Institute for Intelligent Systems, The University of 
Memphis, Memphis, TN, 38152, USA 
{vrus,mclinten,cmoldova}@memphis.edu 
2 Centre for Research in Computing, Open University, UK 
bjwyse@gmail.com and {p.piwek, s.stoyanchev}@open.ac.uk 
 
Abstract. The paper briefly describes the First Shared Task Evaluation 
Challenge on Question Generation that took place in Spring 2010. The 
campaign included two tasks: Task A ? Question Generation from Paragraphs 
and Task B ? Question Generation from Sentences. An overview of each of the 
tasks is provided.   
Keywords: question generation, shared task evaluation campaign. 
1   Introduction 
Question Generation is an essential component of learning environments, help 
systems, information seeking systems, multi-modal conversations between virtual 
agents, and a myriad of other applications (Lauer, Peacock, and Graesser, 1992; 
Piwek et al, 2007). 
Question Generation has been recently defined as the task (Rus & Graesser, 2009) 
of automatically generating questions from some form of input. The input could vary 
from information in a database to a deep semantic representation to raw text. 
The first Shared Task Evaluation Challenge on Question Generation (QG-STEC) 
follows a long tradition of STECs in Natural Language Processing (see the annual 
tasks run by the Conference on Natural Language Learning - CoNLL). In particular, 
the idea of a QG-STEC was inspired by the recent activity in the Natural Language 
Generation (NLG) community to offer shared task evaluation campaigns as a 
potential avenue to provide a focus for research in NLG and to increase the visibility 
of NLG in the wider Natural Language Processing (NLP) community (White and 
Dale, 2008). It should be noted that the QG is currently perceived as a discourse 
processing task rather than a traditional NLG task (Rus & Graesser, 2009). 
Two core aspects of a question are the goal of the question and its importance. It is 
difficult to determine whether a particular question is good without knowing the 
context in which it is posed; ideally one would like to have information about what 
counts as important and what the goals are in the current context. This suggests that a 
STEC on QG should be tied to a particular application, e.g. tutoring systems. 
However, an application-specific STEC would limit the pool of potential participants 
to those interested in the target application. Therefore, the challenge was to find a 
framework in which the goal and importance are intrinsic to the source of questions 
and less tied to a particular context/application. One possibility was to have the 
general goal of asking questions about salient items in a source of information, e.g. 
core ideas in a paragraph of text. Our tasks have been defined with this concept in 
mind. Adopting the basic principle of application-independence has the advantage of 
escaping the problem of a limited pool of participants (to those interested in a 
particular application had that application been chosen as the target for a QG STEC). 
Another decision aimed at attracting as many participants as possible and 
promoting a more fair comparison environment was the input for the QG tasks. 
Adopting a specific representation for the input would have favored some participants 
already familiar with such a representation. Therefore, we have adopted as a second 
guiding principle for the first QG-STEC tasks: no representational commitment. That 
is, we wanted to have as generic an input as possible. The input to both task A and B 
in the first QG STEC is raw text. 
The First Workshop on Question Generation (www.questiongeneration.org) has 
identified four categories of QG tasks (Rus & Graesser, 2009): Text-to-Question, 
Tutorial Dialogue,  Assessment, and Query-to-Question. The two tasks in the first QG 
STEC are part of the Text-to-Question category or part of the Text-to-text Natural 
Language Generation task categories (Dale & White, 2007). It is important to say that 
the two tasks offered in the first QG STEC were selected among 5 candidate tasks by 
the members of the QG community. A preference poll was conducted and the most 
preferred tasks, Question Generation from Paragraphs (Task A) and Question 
Generation from Sentences (Task B), were chosen to be offered in the first QG STEC. 
The other three candidate tasks were: Ranking Automatically Generated Questions 
(Michael Heilman and Noah Smith), Concept Identification and Ordering (Rodney 
Nielsen and Lee Becker), and Question Type Identification (Vasile Rus and Arthur 
Graesser). 
There is overlap between Task A and B. This was intentional with the aim of 
encouraging people preferring one task to participate in the other. The overlap 
consists of the specific questions in Task A which are more or less similar with the 
type of questions targeted by Task B. 
Overall, we had 1 submission for Task A and 4 submissions for Task B. We also 
had an additional submission on development data for Task A. 
2   TASK A: Question Generation from Paragraphs 
1.1   Task Definition 
The Question Generation from Paragraphs (QGP) task challenges participants to 
generate a list of 6 questions from a given input paragraph. The six questions should 
be at three scope levels: 1 x broad (entire input paragraph), 2 x medium (multiple 
sentences), and 3 x specific (sentence or less). The scope is defined by the portion of 
the paragraph that answers the question. 
The Question Generation from Paragraphs (QGP) task has been defined such that it 
is application-independent. Application-independent means questions will be judged 
based on content analysis of the input paragraph; questions whose answers span more 
input text are ranked higher. 
We show next an example paragraph together with six interesting, application-
independent questions that could be generated. We will use the paragraph and 
questions to describe the judging criteria. 
Table 1.  Example of input paragraph (from  http://en.wikipedia.org/wiki/Abraham_lincoln).  
Input Paragraph 
Abraham Lincoln (February 12, 1809 ? April 15, 1865), the 16th 
President of the United States, successfully led his country through 
its greatest internal crisis, the American Civil War, preserving the 
Union and ending slavery. As an outspoken opponent of the 
expansion of slavery in the United States, Lincoln won the 
Republican Party nomination in 1860 and was elected president 
later that year. His tenure in office was occupied primarily with the 
defeat of the secessionist Confederate States of America in the 
American Civil War. He introduced measures that resulted in the 
abolition of slavery, issuing his Emancipation Proclamation in 1863 
and promoting the passage of the Thirteenth Amendment to the 
Constitution. As the civil war was drawing to a close, Lincoln 
became the first American president to be assassinated. 
Table 2.  Examples of questions and scores for the paragraph in Table 1.  
Questions Scope 
Who is Abraham Lincoln? General 
What major measures did President Lincoln introduce? Medium 
How did President Lincoln die? Medium 
When was Abraham Lincoln elected president? Specific 
When was President Lincoln assassinated? Specific 
What party did Abraham Lincoln belong to? Specific 
 
 
A set of five scores, one for each criterion (specificity, syntax, semantics, question 
type correctness, diversity), and a composite score will be assigned to each question. 
Each question at each position will be assigned a composite score ranging from 1 
(first/top ranked, best) to 4 (worst rank), 1 meaning the question is at the right level of 
specificity given its rank (e.g. the broadest question that the whole paragraph answers 
will get a score of 1 if in the first position) and also it is syntactically and semantically 
correct as well as unique/diverse from other generated questions in the set. 
Ranking of questions based on scope assures a maximum score for the six 
questions of 1, 2, 2, 3, 3 and 3, respectively. A top-rank score of 1 is assigned to a 
broad scope question that is also syntactically and semantically correct or acceptable, 
i.e. if it is semantically ineligible then a decision about its scope cannot be made and 
thus a worst-rank score of 4 is assigned. A maximum score of 2 is assigned to 
medium-scope questions while a maximum score of 3 is assigned to specific 
questions. The best configuration of scores (1, 2, 2, 3, 3, 3) would only be possible for 
paragraphs that could trigger the required number of questions at each scope level, 
which may not always be the case. 
1.3   Data Sources and Annotation 
The primary source of input paragraphs were: Wikipedia, OpenLearn, 
Yahoo!Answers. We collected 20 paragraphs from each of these three sources. We 
collected both a development data set (65 paragraphs) and a test data set (60 
paragraphs). For the development data set we manually generated and scored 6 
questions per paragraph for a total of 6 x 65 = 390 questions.  
Paragraphs were selected such that they are self-contained (no need for previous 
context to be interpreted, e.g. will have no unresolved pronouns) and contain around 
5-7 sentences for a total of 100-200 tokens (excluding punctuation). In addition, we 
aimed for a diversity of topics of general interest. 
We also provided discourse relations based on HILDA, a freely available 
automatic discourse parser (duVerle & Prendinger, 2009). 
2   TASK B: Question Generation from Sentences 
2.1   Task Definition 
Participants were given a set of inputs, with each input consisting of:  
 
? a single sentence and  
? a specific target question type (e.g., WHO?, WHY?, HOW?, WHEN?; see 
below for the complete list of types used in the challenge).  
 
For each input, the task was to generate 2 questions of the specified target question 
type.  
Input sentences, 60 in total, were selected from OpenLearn, Wikipedia and Yahoo! 
Answers (20 inputs from each source). Extremely short or long sentences were not 
included. Prior to receiving the actual test data, participants were provided with a 
development data set consisting of sentences from the aforementioned sources and, 
for one or more target question types, examples of questions. These questions were 
manually authored and cross-checked by the team organizing Task B.  
The following example is taken from the development data set. Each instance has a 
unique identifier and information on the source it was extracted from. The <text> 
element contains the input sentence and the <question> elements contain possible 
questions. The <question> element has the type attribute for specification of the target 
question type. 
 
<instance id="3">  
 <id>OpenLearn</id>  
 <source>A103_5</source>  
 <text> 
  The poet Rudyard Kipling lost his only son  
  in the trenches in 1915. 
 </text>  
 <question type="who"> 
  Who lost his only son in the trenches in 1915? 
 </question> 
 <question type="when"> 
  When did Rudyard Kipling lose his son? 
 </question> 
 <question type="how many"> 
  How many sons did Rudyard Kipling have? 
 </question> 
</instance> 
 
Note that input sentences were provided as raw text. Annotations were not 
provided. There are a variety of NLP open-source tools available to potential 
participants and the choice of tools and how these tools are used was considered a 
fundamental part of the challenge.  
This task was restricted to the following question types: WHO, WHERE, WHEN, 
WHICH, WHAT, WHY, HOW MANY/LONG, YES/NO. Participants were provided 
with this list and definitions of each of the items in it. 
2.2   Evaluation criteria for System Outputs and Human Judges 
The evaluation criteria fulfilled two roles. Firstly, they were provided to the 
participants as a specification of the kind of questions that their systems should aim to 
generate. Secondly, they also played the role of guidelines for the judges of system 
outputs in the evaluation exercise.   
For this task, five criteria were identified: relevance, question type, syntactic 
correctness and fluency, ambiguity, and variety. All criteria are associated with a 
scale from 1 to N (where N is 2, 3 or 4), with 1 being the best score and N the worst 
score. 
The procedure for applying these criteria is as follows: 
 
? Each of the criteria is applied independently of the other criteria to each of 
the generated questions (except for the stipulation provided below).  
 
We need some specific stipulations for cases where no question is returned in 
response to an input. For each target question type, two questions are expected. 
Consequently, we have the following two possibilities regarding missing questions: 
 
? No question is returned for a particular target question type: for each of 
the missing questions, the worst score is recorded for all criteria. 
? Only one question is returned: For the missing question, the worst score is 
assigned on all criteria. The question that is present is scored following 
the criteria, with the exception of the VARIETY criterion for which the 
lowest possible score is assigned.  
 
We compute the overall score on a specific criterion. We can also compute a score 
which aggregates the overall scores for the criteria. 
Conclusions 
The submissions to the first QG STEC are now being evaluated using peer-review 
mechanism in which participants blindly evaluate their peers questions. At least two 
reviews per submissions are performed with the results to be made public at the 3rd 
Workshop on Question Generation that will take place in June 2010. 
 
Acknowledgments. We are grateful to a number of people who contributed to the 
success of the First Shared Task Evaluation Challenge on Question Generation: 
Rodney Nielsen, Amanda Stent, Arthur Graesser, Jose Otero, and James Lester. Also, 
we would like to thank the National Science Foundation who partially supported this 
work through grants RI-0836259 and RI-0938239 (awarded to Vasile Rus) and the 
Engineering and Physical Sciences Research Council who partially supported the 
effort on Task B through grant EP/G020981/1 (awarded to Paul Piwek). The views 
expressed in this paper are solely the authors?. 
References 
1. Lauer, T., Peacock, E., & Graesser, A. C. (1992) (Eds.). Questions and information systems. 
Hillsdale, NJ: Erlbaum. 
2. Rus, V. and Graesser, A.C. (2009). Workshop Report: The Question Generation Task and 
Evaluation Challenge, Institute for Intelligent Systems, Memphis, TN, ISBN: 978-0-615-
27428-7.  
3. Piwek, P., H. Hernault, H. Prendinger, M. Ishizuka (2007). T2D: Generating Dialogues 
between Virtual Agents Automatically from Text. In: Intelligent Virtual Agents: 
Proceedings of IVA07, LNAI 4722, September 17-19, 2007, Paris, France, (Springer-
Verlag, Berlin Heidelberg) pp.161-174 
4. Dale, R. & M. White (2007) (Eds.). Position Papers of the Workshop on Shared Tasks and 
Comparative Evaluation in Natural Language Generation. 
5. duVerle, D. and Prendinger, H. (2009). A novel discourse parser based on Support Vector 
Machines. Proc 47th Annual Meeting of the Association for Computational Linguistics and 
the 4th Int'l Joint Conf on Natural Language Processing of the Asian Federation of Natural 
Language Processing (ACL-IJCNLP'09), Singapore, Aug 2009 (ACL and AFNLP), pp 665-
673. 
 
 
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 335?337,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
The CODA System for Monologue-to-Dialogue Generation
Svetlana Stoyanchev
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
s.stoyanchev@open.ac.uk
Paul Piwek
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
p.piwek@open.ac.uk
Abstract
This paper describes an implemented mono-
lingual Text-to-Text generation system. The
system takes monologue and transforms it to
two-participant dialogue. The system uses
mappings between discourse relations in text
and dialogue acts in dialogue. These map-
pings are extracted from a parallel monologue
and dialogue corpus.
1 Introduction
This paper describes the CODA system,1 a Text-to-
Text generation system that converts text parsed with
discourse relations (Mann and Thompson, 1988)
into information-delivering dialogue between two
characters. By information-delivering dialogue, we
mean dialogue (akin to that used by Plato) that is
used primarily to convey information and possibly
also to make an argument; this in contrast with dra-
matic dialogue which focuses on character develop-
ment and narrative.
Several empirical studies show that delivering
information as dialogue, rather than monologue,
can be particularly effective for education (Craig
et al, 2000; Lee et al, 1998) and persuasion
(Suzuki and Yamada, 2004). Information-delivering
dialogue also lends itself well for presentation
through computer-animated agents (Prendinger and
Ishizuka, 2004).
1CODA stands for COherent Dialogue Automatically gen-
erated from text (see http://computing.open.ac.uk/coda/). The
CODA project is funded by the UK?s Engineering and Physical
Sciences Research Council under Grant EP/G020981/1.
With most information locked up in text (books,
newspapers, leaflets, etc.), automatic generation of
dialogue from text in monologue makes it possible
to convert information into dialogue on demand.
In contrast to previous Text-to-Dialogue sys-
tems (Piwek et al, 2007), the CODA system is data-
driven and modular. The system is composed of
three modules: Dialogue Modeller, Verbalizer, and
Dialogue Merger.
The Dialogue modeller determines appropriate
dialogue act sequences that can be used for con-
verting a segment of input text containing a sin-
gle discourse relation into dialogue. The mod-
ule is data-oriented in that the mappings it uses
between discourse structure and dialogue act se-
quences have been derived from the CODA paral-
lel monologue/dialogue corpus (Stoyanchev and Pi-
wek, 2010).
The Verbalizer converts text segments together
with a specification of the target dialogue act types
into dialogue utterances.
The Dialogue modeller and verbaliser compo-
nents overgenerate possible outputs for each dis-
course relation in monologue. The Dialogue Merger
component selects one of the proposed outputs for
each text segment of the input and merges them into
a single coherent dialogue.
2 System Design
In this section we describe the three components of
the system: dialogue modeller, verbalizer, and dia-
logue merger.
Before we look at each of the modules, we, how-
ever, first need to specify more precisely what the
335
Input MANNER-MEANS [In September,
Ashland settled the long-simmering
dispute] [by agreeing to pay Iran
$325 million.]
Dialogue 1. (ComplexQ; Explain)
Modeller 2. (Explain; ComplexQ; Explain)
3. (Explain; YesNoQ; Explain)
Verbalizer
DA Seq1
A: How did Ashland settle the long-
simmering dispute in September?
B: By agreeing to pay Iran $325
million.
Verbalizer
DA Seq2
A: In September, Ashland settled
the long-simmering dispute.
B: How?
A: By agreeing to pay Iran $325
million.
Verbalizer
DA Seq3
A: In September, Ashland settled
the long-simmering dispute.
B: By agreeing to pay Iran $325
million?
A: Correct.
Dialogue
Merger
Select one of the DA sequences
based on overall dialogue
Table 1: Example of the output from each component
input for our system is. The system expects text that
has already been annotated with a discourse struc-
ture. There have been recent encouraging advances
in the automatic parsing of discourse structure, e.g.,
see duVerle and Prendinger (2009), but the state-of-
the-art is not yet at a point where it provides suffi-
ciently reliable inputs for our purposes. To demon-
strate the functionality of our system without relying
on still imperfect discourse parsing, we use the RST-
parsed Wall Street Journal corpus as input (Carlson
et al, 2001).
Throughout the remainder of this section, we use
the outputs for each of the modules in Table 1 as a
running example.
2.1 Dialogue Modeller
The Dialogue Modeller component takes as input a
snippet of monologue text annotated with discourse
structure. For each input Discourse Relation struc-
ture (DR), the dialogue modeller outputs a set of dia-
logue act (DA) sequences appropriate for expressing
the same information, but now in dialogue form.
The Dialogue modeller uses a configuration XML
file to look up possible DA sequences for the input
DA sequence
YesNoQ; Explain
YesNoQ; Yes; Explain
Explain; ComplexQ; Explain
ComplexQ; Explain
Explain; YesNoQ; Resp-Answer-Yes
Explain; Contradict
Factoid-Info-Req;Factoid-Resp;Explain
Exlain; Resp-Agree;Explain
Table 2: Dialogue act sequences
discourse structure. In the current system configu-
ration we extract these mappings from the CODA
parallel corpus of professionally authored dialogues
and parallel monologues. We use the eight most fre-
quent DA sequences (see Table2) that occur on the
dialogue side of discourse relations in the parallel
dataset. Each discourse relation is mapped to one
or more DA sequences with a score indicating fre-
quency of this mapping in the CODA corpus.
The dialogue modeller can be customised with
mappings from other sources such as a different cor-
pus, manually authored mappings or a mapping ar-
rived at through experimental methods.
The current version of the dialogue modeller sup-
ports input with only one level of discourse structure
annotation. As a result, all input structures contain
parts made of two segments and one discourse rela-
tion between these segments. In the future work, we
plan to implement a dialogue modeller that accepts
more complex (nested) discourse structures.
2.2 Verbalizer
The verbalizer is rule-based and has three types of
rules: discourse relation (DR)-specific, generic, and
canned. All of the rules take as input a monologue
segment and a target dialogue act. DR-specific rules
also use the discourse relation and segment nuclear-
ity of the input segment.2 The verbalization rules are
ordered according to their priority with DR-specific
rules having a higher priority.
Generic and DR-specific rules use the CMU ques-
tion generation tool (Heilman and Smith, 2010) in
combination with syntactic and lexical manipulation
rules. Canned text rules are used to generate An-
swerYes, Agree and Clarify dialogue acts by proba-
2Nucleus is the more salient segment in a relation.
336
bilistic selection from a set of utterances extracted
from the CODA corpus. For example, the Agree
dialogue act is verbalized as one of the statements:
I agree with you; I agree; I couldn?t agree more;
I completely agree; Absolutely; Very true; Right;
True. Probabilistic selection from a list allows us
to generate non-repetitive dialogues. The system is
extendible, such that new rules can be easily added
to the implementation.
2.3 Dialogue Merger
The Dialogue Merger component takes as input ver-
balized dialogue act sequences. The tasks of the Di-
alogue Merger include: 1) selecting the best ver-
balized sequence and 2) assigning speaker roles
(TEACHER or STUDENT) to dialogue turns.
We aim to create diverse dialogues, in particular,
by avoiding repetitive use of the same dialogue act
sequences. This is achieved as follows. Selection of
DA sequence is incremental, considering one rela-
tion at a time. For each relation, the dialogue merger
selects a dialogue act sequence that has been suc-
cessfully verbalized by the verbalizer and which, so
far, has been used the smallest number of times (out
of all the sequences that have been used up to this
point).
Although in the original authored dialogues, both
TEACHER and STUDENT ask questions and give ex-
planations, in our preliminary experiments observers
made negative comments about mixing initiative be-
tween the STUDENT and the TEACHER in the gen-
erated dialogues. In the current version, the speaker
roles are assigned based on the dialogue act. All
questions and clarification requests are assigned to
the STUDENT and other dialogue acts are assigned
to the TEACHER.
As an additional post-processing step, to main-
tain perspective in the dialogue, we change pronouns
in the dialogue turns. The turns assigned to the
TEACHER character remain unchanged. The turns
assigned to the STUDENT character change the per-
spective: non-possessive pronouns are inverted, e.g.
you ? I, we ? us, my ? your.
3 Conclusions and Further Work
In this paper, we described a Text-to-Dialogue gen-
eration system that converts text annotated with dis-
course relations into dialogue. The system is modu-
lar, data-driven, and takes advantage of state-of-the-
art question generation tools. Our evaluation of the
dialogue modeller and verbalizer components de-
scribed in (Piwek and Stoyanchev, 2011) shows that
both accuracy and fluency of generated dialogues
are not worse than that of human-written dialogues.
We plan to release the CODA Text-to-Dialogue
system as open source code later this year. The sys-
tem can be used as a starting point for researchers
interested in evaluating NLP tools for question gen-
eration, dialogue modelling and paraphrasing in a
dialogue generation task.
References
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proceedings of the
Second SIGdial Workshop on Discourse and Dialogue,
SIGDIA.
S. Craig, B. Gholson, M. Ventura, A. Graesser, and the
Tutoring Research Group. 2000. Overhearing dia-
logues and monologues in virtual tutoring sessions.
International Journal of Artificial Intelligence in Ed-
ucation, 11:242?253.
D. duVerle and H. Prendinger. 2009. A novel discourse
parser based on support vector machines. In Procs of
ACL-IJCNLP), pages 665?673, Singapore, August.
M. Heilman and N. A. Smith. 2010. Good question!
statistical ranking for question generation. In Proc. of
NAACL/HLT, Los Angeles.
J. Lee, F. Dinneen, and J. McKendree. 1998. Supporting
student discussions: it isn?t just talk. Education and
Information Technologies, 3:217?229.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
P. Piwek and S. Stoyanchev. 2011. Data-oriented
Monologue-to-Dialogue Generation. In Procs of ACL.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. T2D: Generating Dialogues between Virtual
Agents Automatically from Text. In Procs of IVA07,
LNAI 4722, pages 161?174. Springer Verlag.
H. Prendinger and M. Ishizuka, editors. 2004. Life-Like
Characters: Tools, Affective Functions, and Applica-
tions. Cognitive Technologies Series. Springer, Berlin.
S. Stoyanchev and P. Piwek. 2010. Constructing the
CODA corpus. In Procs of LREC, Malta.
S. V. Suzuki and S. Yamada. 2004. Persuasion through
overheard communication by life-like agents. In Procs
of the 2004 IEEE/WIC/ACM International Conference
on Intelligent Agent Technology, Beijing.
337
Proceedings of the SIGDIAL 2013 Conference, pages 132?136,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Exploring Features For Localized Detection of Speech Recognition Errors
Eli Pincus and Svetlana Stoyanchev and Julia Hirschberg
Department of Computer Science, Columbia University, USA
elipincus@gmail.com & sstoyanchev@cs.columbia.edu
& julia@cs.columbia.edu
Abstract
We address the problem of localized error
detection in Automatic Speech Recognition
(ASR) output to support the generation of tar-
geted clarifications in spoken dialogue sys-
tems. Localized error detection finds specific
mis-recognized words in a user utterance. Tar-
geted clarifications, in contrast with generic
?please repeat/rephrase? clarifications, target
a specific mis-recognized word in an utter-
ance (Stoyanchev et al, 2012a) and require
accurate detection of such words. We extend
and modify work presented in (Stoyanchev et
al., 2012b) by experimenting with a new set
of features for predicting the likelihood of a
local error in an ASR hypothesis on an un-
sifted version of the original dataset. We im-
prove over baseline results, where only ASR-
generated features are used, by constructing
optimal feature sets for utterance and word
mis-recognition prediction. The f-measure for
identifying incorrect utterances improves by
2.2% and by 3.9% for identifiying incorrect
words.
1 Introduction
Spoken Dialogue Systems typically indicate their lack
of understanding of user input by simple requests for
repetition or rephrasing ? ?I?m sorry, I didn?t under-
stand you.?, or ?Can you please repeat??. However
human conversational partners generally provide more
targeted clarification requests. Corpus analysis of hu-
man conversations have shown that people are more
likely to indicate what they have understood and what
they have not understood by producing reprise clar-
ification questions (Purver, 2004; Stoyanchev et al,
2012a), as illustrated in the following exchange where
XXX indicates a word misunderstood by speaker B:
A: Do you have any XXX in your bag?
B: Do I have any what in my bag?
A reprise clarification question targets a specific mis-
recognized word and incorporates recognized context
into a clarification question.
We investigate replacing generic please repeat clari-
fications with more natural targeted clarifications in au-
tomatic spoken systems. Targeted clarifications allow
users to provide a concise response to a clarification
question which is beneficial for spoken systems accept-
ing broad vocabulary and flexible syntax. Examples of
such systems include tutoring systems, intelligent as-
sistants, and spoken translation systems (Litman and
Silliman, 2004; Dzikovska et al, 2009; Akbacak et al,
2009).
To enable Spoken Dialogue Systems (SDS) to gen-
erate targeted clarification questions, we must first be
able to identify mis-recognized words with high accu-
racy. We term such mis-recognition detection localized
error detection. Accurate distinction between correctly
and incorrectly recognized words is essential to the cre-
ation of appropriate targeted clarification questions.
In previous research on recognition error detection in
dialogue systems, researchers have addressed error de-
tection at the utterance level (Hirschberg et al, 2004;
Komatani and Okuno, 2010). In this paper we present
results of classification experiments designed to de-
tect localized errors within the utterance. Our base-
line results are obtained from a classifier trained only
on word posterior probabilities generated by an Auto-
matic Speech Recognition (ASR) engine. ASR confi-
dence score computation is an active research area, re-
lying upon acoustic and lexical collocation information
to compute confidence scores. We determine whether
improvement over baseline can be achieved by training
a classifier for utterance and word mis-recognition pre-
diction on an expanded feature set that includes lexical,
positional, prosodic, semantic, syntactic as well as ad-
ditional ASR score features. All of the features we ex-
periment with can be computed from an ASR hypothe-
sis without affecting the performance of a SDS materi-
ally. After determining optimal feature sets we experi-
ment with one- and two-stage approaches for localized
error detection. The first simply identifies whether a
word is correctly recognized or not. The second first
classifies an utterance as incorrect or correct and then
classifies errors only on utterances labeled incorrect.
132
This work extends earlier work in which we eval-
uated a smaller set of syntactic and prosodic fea-
tures (Stoyanchev et al, 2012b). In addition to im-
provements implemented in the ASR engine that we
use to produce ASR hypotheses, our current work re-
ports results on a larger dataset which includes com-
mands to the system and utterances containing disflu-
encies. Here, we propose a framework for localized
error detection that does not rely upon pre-filtering of
the dataset.
In Section 2 we describe our corpus. In Section 3
we discuss our classification experiments. In Section
4 we discuss our results. In Section 5 we present our
conclusions and discuss future research.
2 Data
We conduct our machine learning experiments on the
DARPA TRANSTAC corpus (Weiss et al, 2008). The
TRANSTAC corpus is comprised of staged conversa-
tions between American military personnel and Ara-
bic interviewees utilizing IraqComm speech-to-speech
translation system (Akbacak et al, 2009). This data
was collected by NIST between 2005 and 2008 in eval-
uation exercises. The dataset contains audio record-
ings and manual transcript of English and Arabic utter-
ances. We used SRI?s DynaSpeak (Franco et al, 2002)
speech recognition system to recognize the English ut-
terances and use posterior probabilities from DynaS-
peak as our baseline feature. We create a corpus from
this dataset that contains over 99% of the English ut-
terances. 38 utterances were removed from the dataset
either for lack of actual speech data or errors in refer-
ence transcription. 26.2% of our cleaned corpus con-
sist of mis-recognized instances and 6.4% of the total
words in it are incorrectly recognized by DynaSpeak
(see Table 1). We are using an unsifted version of the
corpus used in our previous work (Stoyanchev et al,
2012b) whose hypotheses were produced with a new
version of the DynaSpeak ASR system. In our previous
work utterances containing disfluencies and commands
to the system were excluded. We seek to avoid the cas-
cading errors that would follow from implementing a
2-step framework for localized error detection where
the first step is command and disfluency detection and
the second step is localized error detection. The 1-step
framework also has the advantage of working for all ut-
terances including ones that contain commands or dis-
fluencies. Due to these differences, our current results
are not directly comparable with our previous results.
Table 1: Corpus statistics
Overall Correct ASR Incorr ASR
All utts. 3,952 2,914 (73.7%) 1,038 (26.2%)
All wrds. 25,333 23,705 (93.6%) 1,628 (6.4%)
wrds in err utts 7,888 6,260 (79.4%) 1,628 (20.6%)
3 Method
We analyze how the performance of predicting mis-
recognized utterances and words is affected by the
use of lexical, positional, prosodic, semantic, and syn-
tactic features in addition to ASR confidence scores.
We perform machine learning experiments using the
Weka Machine Learning Library to construct a J48
decision tree classifier boosted with MultiBoostAB
method (Witten and Eibe, 2005).
Baseline confidence features We use ASR posterior
scores extracted from the log files output by Dynaspeak
as a baseline feature set in our experiments. In the utter-
ance mis-recognition prediction experiment, we calcu-
late the average of the logarithm of the ASR posterior
scores over all words in the hypothesis. In the word
mis-recognition prediction experiment we use the log-
arithm of the posterior score of a given word.
Feature selection We run a heuristic feature ex-
ploration experiment to identify optimal feature sets
for predicting mis-recognized utterances and mis-
recognized words. We first use a greedy approach
adding one feature at a time to the baseline ASR feature
set and only keep a feature in the set if it improves F-
measure predicting mis-recognition. We then use an al-
ternate greedy approach in which we begin with a fea-
ture set composed of all extracted features and proceed
to remove one feature at a time and only leave it out
of the set if incorrect F-measure improved or remained
the same with its absence. The second approach yields
the optimal feature sets for both utterance and word
mis-recognition prediction. Table 2 lists the features
that make up these optimal sets. For incorrect utter-
ance prediction, we run a 10-fold cross validation on
all utterances. For incorrect word prediction, we run a
10-fold cross validation on all words in mis-recognized
utterances.1 We next describe the features we found to
be useful in prediction and those that did not improve
performance.
3.1 Useful Features
ASR context features We use the logarithm of the pos-
terior score of a given word and the average of the log-
arithm of the posterior scores for both a given word and
its surrounding context. We use one word context be-
fore and after the given word. We also use the average
of the logarithm of the posterior scores for all words in
the utterance.
Lexical features We hypothesize that properties of
words such as length and frequency are predictive of
whether a word is correctly recognized. In particular,
noting that words of greater length are often better rec-
ognized by an ASR engine, we examine the length, fre-
quency, and posterior score of the maximum and min-
1Because of the size limitations of our dataset feature se-
lection and evaluation are performed on the same dataset.
133
imum words in an utterance. For mis-recognized ut-
terance prediction, we find that the average length of a
word in the utterance are useful features for predicting
both mis-recognized utterances and words. For mis-
recognized word prediction, we find the word length of
the surrounding words, the current word, and the fre-
quency of the longest word in an utterance are useful.
We also find that utterance length calculated in words is
a useful feature for predicting both utterance and word
mis-recognition.
Positional features Motivated by the use of dialogue
history features in Lopes et al(2011), we find that the
location of the hypothesis relative to the speaker?s first
utterance in the dialogue (utterance location) is a use-
ful feature. Similarly, we obtain improvement from the
word index feature, the distance of the word from the
first word in the utterance.
Syntactic POS tags were shown to be helpful in our
previous work and we find that these tags improve the
current results as well. We obtain these from the Stan-
ford POS tagger (et al, 2003). In mis-recognized ut-
terance prediction, we use unigram and bigram counts
of POS tags as a feature. For mis-recognized word pre-
diction, we use the word?s POS tag as well as the POS
tag for the surrounding one or two words.
We obtain a binary Func/Content feature using a
function word list to distinguish function from content
words. The list includes certain adverbs, conjunctions,
determiners, modal verbs, primary verbs such as be,
prepositions, pronouns, and WP-pronouns. These tags
also boost our ability to identify mis-recognized words.
The feature Func/Tot ratio is the fraction of function
words to total words in an ASR hypothesis. We hy-
pothesize that an extreme value of the Func/Tot ratio
may indicate a potential mis-recognition, and it does
improve both utterance and word mis-recognition pre-
diction.
3.2 Less Useful Features
Features we do not find helpful include information as-
sociated with the minimum length word in the utter-
ance, the fraction of words in an utterance that pos-
sess greater length than the average length word in the
corpus, as well as syntactic features such as a depen-
dency tag assigned to the word. Additional unhelp-
ful features include prosodic features, such as shimmer
and jitter identified by PRAAT (Boersma and Weenink,
2013) and pitch and phrase information extracted from
AuToBI(Rosenberg, 2010) software. Performing a se-
mantic role label of our hypotheses with the software
SENNA (Collobert et al, 2011) also did not provide
helpful semantic features.
System Performance To evaluate performance of
our mis-recognized word classifier, we use the selected
features in 1-stage and 2-stage approaches. First, we
train models for utterance and word classification sep-
Table 2: Features
Cat Specific In Optimal Utt
Feature Set
In Optimal
Wrd Feature
Set
ASR Log Post Score Yes (avg of all
wrds in utt)
Yes (curr wrd)
ASR-
CTX
Log Post Score No Yes (avg of curr
wrd, curr wrd
context, avg of
all wrds in utt)
Lex Wrd length Yes (avg wrd
length in utt)
Yes
(curr,prev,next)
Max Wrd freq No Yes
Utt length Yes Yes
POS Utt location Yes Yes
Word Index No Yes (curr)
Syn POS Tag Yes (unigram
and bigram
count)
Yes
(curr,prev,next)
Func/Cont tag No Yes (curr, prev,
next)
Func/Tot ratio Yes Yes
arately on 80% of the dataset with up-sampling (35%)2
of the incorrect instances as well as with the actual dis-
tribution of incorrect instances in the corpus (20.6% ut-
terances, 6.4% words). We then test these models on
the remaining 20% of the dataset using the 1-stage and
2-stage approach. In the 1-stage approach we test on
20% of the total words in the corpus. In the 2-stage ap-
proach we first test on 20% of the total utterances in the
corpus and then only test on the words in the utterances
labeled as mis-recognized.
4 Results
New Feature Experiments Using our newly con-
structed utterance feature set we are able to boost incor-
rect utterance classification F-measure by 2.2% from
.597 to .610 (see Table 3). The increase in F-measure
for incorrect utterance mis-recognition is due to an in-
crease in incorrect utterance recall from .531 to .555.
There is a slight decrease in incorrect utterance pre-
cision from .682 to .678. Overall classification accu-
racy improves by 2.1% points (absolute) from 81.2%
to 83.3%. Using our newly constructed word feature
set we are able to improve incorrect word classification
F-measure by 3.9% from .620 to .644 (see Table 4). For
incorrect word classification there is an increase in both
mis-recognized word precision and recall; the former
increasing from .678 to .719 and the latter increasing
from .571 to .584. The results for incorrect word clas-
sification represent a statistically significant improve-
2This percentage was derived empirically.
134
Table 3: Utterance new feature experiment results
Feature Correct Incorrect % F-Measure Incorr Imp Accuracy
P ? R ? F P ? R ? F over ASR Only
ASR .845 ? .912 ? .877 .682 ? .531 ? .597 - 81.2%
ASR+LEX+POS+SYN .851 ? .906 ? .878 .678 ? .555 ? .610 2.2% 83.3%
Table 4: Word new feature experiment results
Feature Correct Incorrect % F-Measure Incorr Imp Accuracy
P ? R ? F P ? R ? F over ASR only
ASR .893 ? .930 ? .911 .678 ? .571 ? .620 - 85.5%
ASR+LEX+POS+SYN .897 ? .941 ? .918 .719 ? .584 ? .644 3.9% 86.7%
Table 5: 1-stage and 2-stage approach results
Experiment Correct Incorrect Accuracy
P ? R ? F P ? R ? F
Maj. Baseline .94 ? 1.00 ? .97 - ? 0 ? - 94%
1-stage original .97 ? .94 ? .96 .39 ? .57 ? .46 92%
1-stage (35% upsample) .98 ? .90 ? .94 .31 ? .72 ? .44 89%
2-stage original .96 ? .98 ? .97 .51 ? .34 ? .41 94%
2-stage (35% upsample) .96 ? .96 ? .96 .41 ? .46 ? .43 93%
ment3. Overall classification accuracy improves by
1.2% points (absolute) from 85.5% to 86.7%.
1-stage and 2-stage experiments To estimate how
well a dialogue system could perform incorrect word
classification we run our 1-stage and 2-stage ap-
proaches. The 1-stage approaches (with and without
up-sampling) are able to achieve higher recall; while
the 2-stage approaches (with and without up-sampling)
are able to achieve higher precision. The 2-stage re-
sult?s higher precision is not surprising given that this
approach has two chances to filter out correct words
? first with utterance classification and then with
word classification. In our 1-stage approach with up-
sampling we are able to identify almost 3/4 (72%) of
the incorrect words in the corpus (see Table 5). In
our 2-stage approach without up-sampling we are able
to accurately label just over 1/2 (51%) of the total in-
stances we identify as incorrect. In future work we will
experiment with additional features in order to boost
precision for incorrect word classification to a level
suitable for use in the construction of reprise clarifi-
cation questions.
5 Conclusions
We have presented results of machine learning exper-
iments that utilize new features to improve localized
detection of ASR errors to assist spoken dialogue sys-
tem?s production of reprise clarification questions. We
conducted feature selection experiments to find optimal
feature sets to train classifiers for utterance and word
mis-recognition prediction. We find that certain lexi-
cal, positional, and syntactic features improve classi-
fication results over a baseline feature set containing
only ASR posterior score features. We improve incor-
rect F-measure for utterance mis-recognition prediction
by 2.2% by adding utterance length, location, fraction
3?2test(p < .01)
of function words to total words, average word length,
and unigram and bigram count to the baseline feature
set. By removing average word length as well as uni-
gram and bigram count from this optimal set for utter-
ances and adding the current word?s ASR-context fea-
tures, length, distance from first word, POS tag, Con-
tent/Function tag as well as the length of the current?s
words surrounding 1 or 2 word contexts, we improve
incorrect F-measure for word mis-recognition predic-
tion by 3.9% . We then employ these feature sets in
1-stage and 2-stage approached to obtain our final re-
sults. The 2-stage (no up-sampling) approach yields the
highest precision for detection of word mis-recognition
at 51% while the 1-stage (with 35% up-sampling) ap-
proach yields the highest recall for detection of word
mis-recognition at 72%.
In order to implement this approach in a working
dialog system we would need to increase our word
mis-recognition precision. The presence of false pos-
itives in mis-recognition prediction (correctly recog-
nized words classified as mis-recognized) could lead
to unnecessary clarification requests ? potentially de-
railing the dialogue.
In future work we will experiment with additional
corpora as well as with an even more fine-grained ap-
proach to local error detection, looking for deletions,
insertions, and substitutions. Potentially, optimal clas-
sifiers could be found for each of these types of mis-
recognition. If we are able to identify the type of ASR
error as well as its location, we should be able to im-
prove our construction of clarifications questions.
We will also continue our investigation of how to use
reprise clarification questions in SDS. Once we have
detected localized ASR errors we must still refine our
strategies for constructing clarification questions using
this information. We are also studying how appropri-
ate and inappropriate reprise clarification questions are
handled by SDS users.
135
References
M. Akbacak, H. Franco, M. Frandsen, S. Hasan,
H. Jameel, A. Kathol, S. Khadivi, X. Lei, A. Man-
dal, S. Mansour, K. Precoda, C. Richey, D. Ver-
gyri, W. Wang, M. Yang, and J. Zheng. 2009. Re-
cent advances in sri?s iraqcomm; iraqi arabic-english
speech-to-speech translation system. In Acoustics,
Speech and Signal Processing, 2009. ICASSP 2009.
IEEE International Conference on, pages 4809?
4812.
P. Boersma and D. Weenink. 2013. Praat: do-
ing phonetics by computer [computer program].
http://www.fon.hum.uva.nl/praat/.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch.
M. O. Dzikovska, C. B. Callaway, E. Farrow, J. D.
Moore, N. Steinhauser, and G. Campbell. 2009.
Dealing with interpretation errors in tutorial dia-
logue. In Proceedings of the SIGDIAL 2009 Con-
ference: The 10th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
?09, pages 38?45, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
K. Toutanova et al 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1.
H. Franco, J. Zheng, J. Butzberger, F. Cesari, M. Fr,
J. Arnold, V. Ramana, A. Stolcke R. Gadde, and
V. Abrash. 2002. Dynaspeak: Sri?s scalable speech
recognizer for embedded and mobile systems. In
Proceedings of the second international conference
on Human Language Technology Research, HLT
?02, pages 25?30, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
J. Hirschberg, D. J. Litman, and M. Swerts. 2004.
Prosodic and other cues to speech recognition fail-
ures. Speech Communication, 43(1-2):155?175.
K. Komatani and H. G. Okuno. 2010. Online error
detection of barge-in utterances by using individual
users utterance histories in spoken dialogue system.
D. J. Litman and S. Silliman. 2004. Itspoke: an
intelligent tutoring spoken dialogue system. In
Demonstration Papers at HLT-NAACL 2004, HLT-
NAACL?Demonstrations ?04, pages 5?8, Strouds-
burg, PA, USA.
J. Lopes, M. Eskenazi, and I. Trancoso. 2011. To-
wards choosing better primes for spoken dialog sys-
tems. In Proceedings of the IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU), Keystone, CO.
M. Purver. 2004. The Theory and Use of Clarification
Requests in Dialogue. Ph.D. thesis, King?s College,
University of London.
A. Rosenberg. 2010. Autobi - a tool for auto-
matic tobi annotation. In Takao Kobayashi, Kei-
kichi Hirose, and Satoshi Nakamura, editors, IN-
TERSPEECH, pages 146?149. ISCA.
S. Stoyanchev, A. Liu, and J. Hirschberg. 2012a. Clar-
ification questions with feedback 2012. In Interdis-
ciplinary Workshop on Feedback Behaviors in Dia-
log.
S. Stoyanchev, P. Salletmayr, J. Yang, and
J. Hirschberg. 2012b. Localized detection of
speech recognition errors. In Spoken Language
Technology Workshop (SLT), 2012 IEEE, pages
25?30.
B. Weiss, C. Schlenoff, G. Sanders, M. Steves, S. Con-
don, J. Phillips, and D. Parvaz. 2008. Perfor-
mance evaluation of speech translation systems. In
Nicoletta Calzolari (Conference Chair) et al, editor,
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC?08),
Marrakech, Morocco, may. European Language
Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
I. Witten and F. Eibe. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
136
Proceedings of the SIGDIAL 2013 Conference, pages 137?141,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Modelling Human Clarification Strategies
Svetlana Stoyanchev, Alex Liu, Julia Hirschberg
Columbia University, New York NY 10027
sstoyanchev, al3037, julia @cs.columbia.edu
Abstract
We model human responses to speech recog-
nition errors from a corpus of human clarifi-
cation strategies. We employ learning tech-
niques to study 1) the decision to either stop
and ask a clarification question or to continue
the dialogue without clarification, and 2) the
decision to ask a targeted clarification question
or a more generic question. Targeted clarifi-
cation questions focus specifically on the part
of an utterance that is misrecognized, in con-
trast with generic requests to ?please repeat?
or ?please rephrase?. Our goal is to generate
targeted clarification strategies for handling er-
rors in spoken dialogue systems, when appro-
priate. Our experiments show that linguis-
tic features, in particular the inferred part-of-
speech of a misrecognized word are predictive
of human clarification decisions. A combina-
tion of linguistic features predicts a user?s de-
cision to continue or stop a dialogue with ac-
curacy of 72.8% over a majority baseline accu-
racy of 59.1%. The same set of features predict
the decision to ask a targeted question with ac-
curacy of 74.6% compared with the majority
baseline of 71.8%.1
1 Introduction
Clarification questions are common in human-human
dialogue. They help dialogue participants main-
tain dialogue flow and resolve misunderstandings.
Purver (2004) finds that in human-human dialogue
speakers most frequently use reprise clarification ques-
tions to resolve recognition errors. Reprise clarification
questions use portions of the misunderstood utterance
which are thought to be correctly recognized to target
the part of an utterance that was misheard or misunder-
stood. In the following example from (Purver, 2004),
Speaker B has failed to hear the word toast and so con-
structs a clarification question using a portion of the
correctly understood utterance ? the word some ? to
query the portion of the utterance B has failed to under-
stand:
1This work was partially funded by DARPA HR0011-12-
C-0016 as a Columbia University subcontract to SRI Interna-
tional.
A: Can I have some toast please?
B: Some?
A: Toast.
Unlike human conversational partners, most di-
alogue systems today employ generic ?please re-
peat/rephrase? questions asking a speaker to repeat or
rephrase an entire utterance. Our goal is to introduce
reprise, or targeted, clarifications into an automatic
spoken system. Targeted clarifications can be espe-
cially useful for systems accepting unrestricted speech,
such as tutoring systems, intelligent agents, and speech
translation systems. Using a reprise question, a user
can correct an error by repeating only a portion of
an utterance. Targeted questions also provide natural
grounding and implicit confirmation by signalling to
the conversation partner which parts of an utterance
have been recognized.
In order to handle a misrecognition, the system must
first identify misrecognized words (Stoyanchev et al,
2012), determine the type of question to ask, and con-
struct the question. In this work, we address two points
necessary for determining the type of question to ask:
? Is it appropriate for a system to ask a clarification
question when a misrecognized word is detected?
? Is it possible to ask a targeted clarification ques-
tion for a given sentence and an error segment?
To answer these questions, we analyze a corpus of hu-
man responses to transcribed utterances with missing
information which we collected using Amazon Me-
chanical Turk (2012). Although the data collection
was text-based, we asked annotators to respond as they
would in a dialogue. In Section 2, we describe related
work on error recovery strategies in dialogue systems.
In Section 3, we describe the corpus used in this exper-
iment. In Section 4, we describe our experiments on
human clarification strategy modelling. We conclude
in Section 5 with our plan for applying our models in
spoken systems.
2 Related work
To handle errors in speech recognition, slot-filling di-
alogue systems typically use simple rejection (?I?m
sorry. I didn?t understand you.?) when they have
low confidence in a recognition hypothesis and ex-
plicit or implicit confirmation when confidence scores
137
are higher. Machine learning approaches have been
successfully employed to determine dialogue strate-
gies (Bohus and Rudnicky, 2005; Bohus et al, 2006;
Rieser and Lemon, 2006), such as when to provide
help, repeat a previous prompt, or move on to the next
prompt. Reiser and Lemon (2006) use machine learn-
ing to determine an optimal clarification strategy in
multimodal dialogue. Komatani et al (2006) propose a
method to generate a help message based on perceived
user expertise. Corpus studies on human clarifications
in dialogue indicate that users ask task-related ques-
tions and provide feedback confirming their hypothesis
instead of giving direct indication of their misunder-
standing (Skantze, 2005; Williams and Young, 2004;
Koulouri and Lauria, 2009). In our work, we model
human strategies with the goal of building a dialogue
system which can generate targeted clarification ques-
tions for recognition errors that require additional user
input but which can also recover from other errors au-
tomatically, as humans do.
3 Data
In our experiments, we use a dataset of human re-
sponses to missing information, which we collected
with Amazon Mechanical Turk (AMT). Each AMT an-
notator was given a set of Automatic Speech Recog-
nition (ASR) transcriptions of an English utterance
with a single misrecognized segment. 925 such utter-
ances were taken from acted dialogues between En-
glish and Arabic speakers conversing through SRI?s
IraqComm speech-to-speech translation system (Akba-
cak et al, 2009). Misrecognized segments were re-
placed by ?XXX? to indicate the missing information,
simulating a dialogue system?s automatic detection of
misrecognized words (Stoyanchev et al, 2012). For
each sentence, AMT workers were asked to 1) indi-
cate whether other information in the sentence made
its meaning clear despite the error, 2) guess the miss-
ing word if possible, 3) guess the missing word?s part-
of-speech (POS) if possible, and 4) create a targeted
clarification question if possible. Three annotators an-
notated each sentence. Table 1 summarizes the results.
In 668 (72%) of the sentences an error segment corre-
sponds to a single word while in 276 (28%) of them, an
error segment corresponds to multiple words. For mul-
tiple word error segments, subjects had the option of
guessing multiple words and POS tags. We scored their
guess correct if any of their guesses matched the syn-
tactic head word of an error segment determined from
an automatically assigned dependency parse structure.
We manually corrected annotators? POS tags if the
hypothesized word was itself correct. After this post-
processing, we see that AMT workers hypothesized
POS correctly in 57.7% of single-word and 60.2% of
multi-word error cases. They guessed words correctly
in 34.9% and 19.3% of single- and multi-word error
cases. They choose to ask a clarification question in
38.3% /47.9% of cases and 76.1%/62.3% of these ques-
tions were targeted clarification questions. These re-
Single-word Agree Multi-word
error error
Total sent 668 (72%) - 276 (28%)
Correct POS 57.7% 62% 60.2%
Correct word 34.9% 25% 19.3%
Ask a question 38.3% 39% 47.9%
Targeted question 76.1% 25% 62.3%
Table 1: Annotation summary for single-word and
multi-word error cases. Absolute annotator agreement
is shown for single-word error cases.
sults indicate that people are often able to guess a POS
tag and sometimes an actual word. We observe that 1)
in a single-word error segment, subjects are better at
guessing an actual word than they are in a multi-word
error segment; and 2) in a multi-word error segment,
subjects are more likely to ask a clarification question
and less likely to ask a targeted question. All three an-
notators agree on POS tags in 62% of cases and on hy-
pothesized words in 25%. Annotators? agreement on
response type is low ? not surprising since there is
more than one appropriate and natural way to respond
in dialogue. In 39% of cases, all three annotators agree
on the decision to stop/continue and in only 25% of
cases all three annotators agree on asking a targeted
clarification question. Figure 1 shows the annotator
Figure 1: Distribution of decisions to ask a question or
continue dialogue without a question.
distribution for asking a clarification question vs. con-
tinuing the dialogue based on hypothesized POS tag. It
indicates that annotators are more likely to ask a ques-
tion than continue without a question when they hy-
pothesize a missing word to be a content word (noun
or adjective) or when they are unsure of the POS of the
missing word. They are more likely to continue when
they believe a missing word is a function word. How-
ever, when they believe a missing word is a verb, they
are more likely to continue, and they are also likely to
identify the missing verb correctly.
Figure 2 shows a distribution of annotator decisions
as to the type of question they would ask. The pro-
portion of targeted question types varies with hypoth-
esized POS. It is more prevalent than confirm and
generic questions combined for all POS tags except
preposition and question word, indicating that annota-
tors are generally able to construct a targeted clarifica-
tion question based on their analysis of the error seg-
ment.
138
Figure 2: Distribution of decisions for targeted, confir-
mation, and generic question types.
4 Experiment
We use our AMT annotations to build classifiers for 1)
choice of action: stop and engage in clarification vs.
continue dialogue; and 2) type of clarification ques-
tion (targeted vs. non-targeted) to ask. For the con-
tinue/stop experiment, we aim to determine whether a
system should stop and ask a clarification question. For
the targeted vs. non-targeted experiment, we aim to de-
termine whether it is possible to ask a targeted clarifi-
cation question.2
Using the Weka (Witten and Eibe, 2005) machine
learning framework, we build classifiers to predict
AMT decisions. We automatically assign POS tags to
transcripts using the Stanford tagger (Toutanova and
others, 2003). We compare models built with an au-
tomatically tagged POS for an error word (POS-auto)
with one built with POS guessed by a user (POS-
guess). Although a dialogue manager may not have
access to a correct POS, it may simulate this by pre-
dicting POS of the error. We assign dependency tags
using the AMU dependency parser (Nasr et al, 2011)
which has been optimized on the Transtac dataset.
We hypothesize that a user?s dialogue move depends
on the syntactic structure of a sentence as well as
on syntactic and semantic information about the er-
ror word and its syntactic parent. To capture sentence
structure, we use features associated with the whole
sentence: POS ngram, all pairs of parent-child depen-
dency tags in a sentence (Dep-pair), and all semantic
roles (Sem-presence) in a sentence. To capture the syn-
tactic and semantic role of a misrecognized word, we
use features associated with this word: POS tag, depen-
dency tag (Dep-tag), POS of the parent word (Parent-
POS), and semantic role of an error word (Sem-role).
We first model individual annotators? decisions for
each of the three annotation instances. We measure
the value that each feature adds to a model, using an-
notators? POS guess (POS-guess). Next, we model a
joint annotators? decision using the automatically as-
signed POS-auto feature. This model simulates a sys-
tem behaviour in a dialogue with a user where a system
chooses a single dialogue move for each situation. We
run 10-fold cross validation using the Weka J48 Deci-
2If any annotators asked a targeted question, we assign a
positive label to this instance, and negative otherwise.
sion Tree algorithm.
Feature Description
Count
Word-position beginning if a misrecognized word is
the first word in the sentence, end if it
is the last word, middle otherwise.
Utterance-length number of words in the sentence
Part-of-speech (compare)
POS-auto POS tag of the misrecognized word au-
tomatically assigned on a transcript
POS-guess POS tag of the misrecognized word
guessed by a user
POS ngrams
POS ngrams all bigrams and trigrams of POS tags in
a sentence
Syntactic Dependency
Dep-tag dependency tag of the misrecognized
word automatically assigned on a tran-
script
Dep-pair dependency tags of all (parent, child)
pairs in the sentence
Parent-POS POS tag of the syntactic parent of the
misrecognized word
Semantic
Sem-role semantic role of the misrecognized
word
Sem-presence all semantic roles present in a sentence
Table 2: Features
4.1 Stop/Continue Experiment
In this experiment, we classify each instance in the
dataset into a binary continue or stop decision. Since
each instance is annotated by three annotators, we first
predict individual annotators? decisions. The absolute
agreement on continue/stop is 39% which means that
61% of sentences are classified into both classes. We
explore the role of each feature in predicting these de-
cisions. All features used in this experiment, except for
the POS-guess feature, are extracted from the sentences
automatically. Variation in the POS-guess feature may
explain some of the difference between annotator deci-
sions.
Features Acc F-measure %Diff
Majority baseline 59.1%
All features 72.8% ? 0.726 0.0%
less utt length 72.9% ? 0.727 +0.1%
less POS ngrams 72.8% ? 0.727 +0.1%
less Semantic 72.6% ? 0.724 -0.3%
less Syn. Depend. 71.5% ? 0.712 -1.9%
less Position 71.2% ? 0.711 -2.0%
less POS 67.9% ? 0.677 -6.7%
POS only 70.1% ? 0.690 -5.0%
Table 3: Stop/Continue experiment predicting individ-
ual annotator?s decision with POS-guess. Accuracy, F-
measure and Difference of f-measure from All feature.
?indicates statistically significant difference from the
majority baseline (p<.01)
Table 3 shows the results of continue/stop classifica-
tion. A majority baseline method predicts the most fre-
quent class continue and has 59.1% accuracy. In com-
parison, our classifier, built with all features, achieves
72.8% accuracy.
139
Next, we evaluate the utility of each feature by re-
moving it from the feature set and comparing the model
built without it with a model built on all features. POS
is the most useful feature, as we expected: when it is
removed from the feature set, the f-measure decreases
by 6.7%. A model trained on the POS-guess feature
alone outperforms a model trained on all other features.
Word position in the sentence is the next most salient
feature, contributing 2% to the f-measure. The syntac-
tic dependency features Syn-Dep, Dep-pair, and Parent
POS together contribute 1.9%.3
Next, we predict a majority decision for each sen-
tence. Table 4 shows the accuracy of this prediction.
A majority baseline has an accuracy of 59.9%. When
we use a model trained on the POS-auto feature alone,
accuracy rises to 66.1%, while a combination of all fea-
tures further increases it to 69.2%.
Features Acc F-measure
Majority baseline 59.9%
POS 66.1% ? 0.655
All features 69.2% ? 0.687
Table 4: Stop/Continue experiment predicting majority
decision, using POS-auto. ?indicates statistically sig-
nificant difference from the majority baseline (p<.01).
4.2 Targeted Clarification Experiment
In this experiment, we classify each instance into tar-
geted or not targeted categories. The targeted category
comprises the cases in which an annotator chooses to
stop and ask a targeted question. We are interested in
identifying these cases in order to determine whether a
system should try to ask a targeted clarification ques-
tion. Table 5 shows the results of this experiment. The
majority baseline predicts not targeted and has a 71.8%
accuracy because in most cases, no question is asked.
A model trained on all features increases accuracy to
74.6%. POS is the most salient feature, contributing
3.8% to the f-measure. All models that use POS fea-
ture are significantly different from the baseline. The
next most salient features are POS ngram and a com-
bination of syntactic dependency features contributing
1% and .5% to the f-measure respectively.
Table 6 shows system performance in predicting a
joint annotators? decision of whether a targeted ques-
tion can be asked. A joint decision in this experiment
is considered not targeted when none of the annotators
chooses to ask a targeted question. We aim at identi-
fying the cases where position of an error word makes
it difficult to ask a clarification question, such as for a
sentence XXX somebody steal these supplies. Using the
automatically assigned POS (POS-auto) feature alone
achieves an accuracy of 62.2%, which is almost 10%
above the baseline. A combination of all features, sur-
prisingly, lowers the accuracy to 59.4%. Interestingly, a
combination of all features less POS increases accuracy
3All trained models are significantly different from the
baseline. None of the trained models are significantly dif-
ferent from each other.
Features Acc F-measure %Diff
Majority baseline 71.8%
All features 74.6% ? 0.734 0.0%
All feature (POS guess)
less Utt length 74.8% ? 0.736 +0.3%
less Position 74.9% ? 0.731 -0.4%
less Semantic 74.8% ? 0.737 +0.4%
less Syn. Depend. 74.2% ? 0.730 -0.5%
less POS ngram 74.2% ? 0.727 -1.0%
less POS 74.0% 0.706 -3.8%
POS 74.1% ? 0.731 -0.4%
Table 5: Targeted/not experiment predicting individ-
ual annotator?s decision with POS-guess. Accuracy, F-
measure and Difference of f-measure from All feature.
?indicates statistically significant difference from the
majority baseline (p<.05)
above the baseline by 7.6% points to 60.1% accuracy.
Features Acc F-measure
Majority baseline 52.5%
POS only 62.2% ? 0.622
All features 59.4% ? 0.594
All features less POS 60.1% ? 0.600
Table 6: Targeted/not experiment predicting majority
decision, using POS tag feature POS-auto. ?indicates
statistically significant difference from the majority
baseline.
5 Conclusions and Future Work
In this paper we have described experiments modelling
human strategies in response to ASR errors. We have
used machine learning techniques on a corpus anno-
tated by AMT workers asked to respond to missing in-
formation in an utterance. Although annotation agree-
ment in this task is low, we aim to learn natural strate-
gies for a dialogue system by combining the judge-
ments of several annotators. In a dialogue, as in other
natural language tasks, there is more than one appro-
priate response in each situation. A user does not judge
the system (or another speaker) by a single response.
Over a dialogue session, appropriateness, or lack of it
in system actions, becomes evident. We have shown
that by using linguistic features we can predict the de-
cision to either ask a clarification question or continue
dialogue with an accuracy of 72.8% in comparison with
the 59.1% baseline. The same linguistic features pre-
dict a targeted clarification question with an accuracy
of 74.6% compared to the baseline of 71.8%.
In future work, we will apply modelling of a clari-
fication choice strategy in a speech-to-speech transla-
tion task. In our related work, we have addressed the
problem of automatic correction of some ASR errors
for cases when humans believe a dialogue can continue
without clarification In other work, we have addressed
the creation of targeted clarification questions for han-
dling the cases when such questions are appropriate.
Combining these research directions, we are develop-
ing a clarification component for a speech-to-speech
translation system that responds naturally to speech
recognition errors.
140
References
M. Akbacak, Franco, H., M. Frandsen, S. Hasan,
H. Jameel, A. Kathol, S. Khadivi, X. Lei, A. Man-
dal, S. Mansour, K. Precoda, C. Richey, D. Vergyri,
W. Wang, M. Yang, and J. Zheng. 2009. Recent ad-
vances in SRI?s IraqCommtm Iraqi Arabic-English
speech-to-speech translation system. In ICASSP,
pages 4809?4812.
Amazon Mechanical Turk. 2012.
http://aws.amazon.com/mturk/, accessed on 28
may, 2012.
D. Bohus and A. I. Rudnicky. 2005. A principled ap-
proach for rejection threshold optimization in spoken
dialog systems. In INTERSPEECH, pages 2781?
2784.
D. Bohus, B. Langner, A. Raux, A. Black, M. Eske-
nazi, and A. Rudnicky. 2006. Online supervised
learning of non-understanding recovery policies. In
Proceedings of SLT.
Y. Fukubayashi, K. Komatani, T. Ogata, and H. Okuno.
2006. Dynamic help generation by estimating user?s
mental model in spoken dialogue systems. In Pro-
ceedings of the International Conference on Spoken
Language Processing (ICSLP).
T. Koulouri and S. Lauria. 2009. Exploring miscom-
munication and collaborative behaviour in human-
robot interaction. In SIGDIAL Conference, pages
111?119.
A. Nasr, F. Be?chet, J.F. Rey, B. Favre, and J. Le Roux.
2011. Macaon: an nlp tool suite for processing
word lattices. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Systems
Demonstrations, pages 86?91. Association for Com-
putational Linguistics.
M. Purver. 2004. The Theory and Use of Clarification
Requests in Dialogue. Ph.D. thesis, King?s College,
University of London.
V. Rieser and O. Lemon. 2006. Using machine learn-
ing to explore human multimodal clarification strate-
gies. In ACL.
G. Skantze. 2005. Exploring human error recov-
ery strategies: Implications for spoken dialogue sys-
tems. Speech Communication, 45(2-3):325?341.
Svetlana Stoyanchev, Philipp Salletmayr, Jingbo Yang,
and Julia Hirschberg. 2012. Localized detection
of speech recognition errors. In SLT, pages 25?30.
IEEE.
K. Toutanova et al 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1. Association for Computational Linguistics.
J. D. Williams and S. Young. 2004. Characterizing
task-oriented dialog using a simulated ASR channel.
In Proceedings of the ICSLP, Jeju, South Korea.
I. Witten and F. Eibe. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
141
Proceedings of the SIGDIAL 2014 Conference, pages 123?132,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Dialogue Act Modeling for Non-Visual Web Access
Vikas Ashok
Dept of Computer Science
Stony Brook University
Stony Brook , New York
Yevgen Borodin
Charmtech Labs LLC
CEWIT SBU R & D Park
Stony Brook , New York
Svetlana Stoyanchev
AT&T Labs Research
New York City, New York
(While at Columbia University)
I V Ramakrishnan
Charmtech Labs LLC
CEWIT SBU R & D Park
Stony Brook , New York
vganjiguntea@cs.sunysb.edu, borodin@charmtechlabs.com,
sstoyanchev@cs.columbia.edu, ram@charmtechlabs.com
Abstract
Speech-enabled dialogue systems have the
potential to enhance the ease with which
blind individuals can interact with the Web
beyond what is possible with screen read-
ers - the currently available assistive tech-
nology which narrates the textual content
on the screen and provides shortcuts to
navigate the content. In this paper, we
present a dialogue act model towards de-
veloping a speech enabled browsing sys-
tem. The model is based on the corpus
data that was collected in a wizard-of-oz
study with 24 blind individuals who were
assigned a gamut of browsing tasks. The
development of the model included exten-
sive experiments with assorted feature sets
and classifiers; the outcomes of the exper-
iments and the analysis of the results are
presented.
1 Introduction
The Web is the ?go-to? computing infrastructure
for participating in our fast-paced digital society.
It has the potential to provide an even greater ben-
efit to blind people who once required human as-
sistance with many of their activities. According
to the American Federation for the Blind, there
are 21.5 million Americans who have vision loss,
of whom 1.5 million are computer users (AFB,
2013).
Blind users employ screen readers as the as-
sistive technology to interact with digital con-
tent (e.g.., JAWS (Freedom-Scientific, 2014) and
VoiceOver (Apple-Inc., 2013)). Screen readers se-
rially narrate the content of the screen using text-
to-speech engines and enable users to navigate in
the content using keyboard shortcuts and touch-
screen gestures.
Navigating content-rich web pages and con-
ducting online transactions spanning multiple
pages requires using shortcuts and this can get
quite cumbersome and tedious. Specifically, in
online shopping a user typically browses through
product categories, searches for products, adds
products to cart, logs into his/her account, and fi-
nally makes a payment. All these steps require
screen-reader users listen through a lot of content,
fill forms, and find links and buttons that have to be
selected to get through these steps. If users do not
want to go through all content on the page, they
have to remember and use a number of different
shortcuts. Beginner users often use the ?Down?
key to go through the page line by line, listening
to all content on the way (Borodin et al., 2010).
Now suppose that blind users were to tell the
web browser what they wanted to accomplish and
let the browsing application automatically deter-
mine what has to be clicked, fill out forms, help
find products, answer questions, breeze through
checkout, and wherever possible, relieve the user
from doing all the mundane and tedious low-level
operations such as clicking, typing, etc. The abil-
ity to carry out a dialogue with the web browser at
a higher level has the potential to overcome the
limitations of shortcut-based screen reading and
thus offers a richer and more productive user ex-
perience for blind people.
The first step toward building a dialogue-based
system is the understanding of what users could
say and dialogue act modeling. Although di-
alogue act modeling is a well-researched topic
(with details provided in related work - Section
2), it has remained unexplored in the context of
web accessibility for blind people. The commer-
cial speech-based applications have been around
for a while and new ones continue to emerge at a
rapid pace; however, these are mainly stand-alone
(e.g.., Apple?s Siri) domain specific systems that
are not connected to web browsers, which pre-
cludes dialogue-based interaction with the Web.
Current spoken input modules integrated with web
123
browsers are limited to certain specific functional-
ities such as search (e.g.., Google?s voice search)
or are used as a measure of last resort (e.g.., Siri
searching for terms online).
In this paper, we made a principal step towards
building a dialogue-based assistive web browsing
system for blind people; specifically, we built a
dialogue act model for non-visual access to the
Web. The contributions of this paper include:
1) a unique dialogue corpus for non-visual web ac-
cess, collected during the wizard-of-oz user study
conducted with 24 blind participants (Section 3);
2) the design of a suitable dialogue act scheme
(Section 3); 3) experimentation with classifiers ca-
pable of identifying the dialogue acts associated
with utterances based on combinations of lexi-
cal/syntactic, contextual, and task-related feature
sets (Section 4); 4) investigation of the impor-
tance of each feature set with respect to classifi-
cation performance to assess whether simple lex-
ical/syntactic features are sufficient for obtaining
an acceptable performance (Section 5).
2 Related Work
While previous research addressed spoken dia-
logue interfaces for a domain-specific websites,
such as news or movie search (Ferreras and
Carde?noso-Payo, 2005; Wang et al., 2014), dia-
logue interface to generic web sites is a novel task.
Spoken dialogue systems (SDS) can be classified
by the type of initiative: system, user, or mixed
initiative (Lee et al., 2010). In a system-initiative
SDS, a system guides a user through a series of
information gathering and information presenting
prompts. In a user-initiative system, a user can
initiate and steer the interaction. Mixed-initiative
systems allow both system and user-initiated ac-
tions.
Dialogue systems also differ in the types of di-
alogue manager: finite state based, form based,
or agent based (Lee et al., 2010), (Chotimongkol,
2008). Finite state and form filling systems are
usually system-initiative. These systems have a
fixed set of dialogue states and finite set of possi-
ble user commands that map to system actions. In
contrast, a speech-enabled browsing system pro-
posed in this work is an agent-based system. The
set of actions of this system correspond to user ac-
tions during web browsing. The domain of possi-
ble user commands at each point of the dialogue
depends on the current web page that is viewed by
a user. The dialogue state in a voice browsing sys-
tem is compiled at run-time as the user can visit
any web page.
While a users dialogue acts in a form-based
or finite state system depends primarily on a di-
alogue state, in an agent-based system with user-
initiative, the space of users dialogue acts at each
dialogue state is open. To determine dialogue
manager action, it is essential for the system to
identify users intent or dialogue act. In this
work, we address dialogue act modelling for open-
domain voice web browsing as a proof of concept
for the system.
Dialogue act (DA) annotation schemes for spo-
ken dialogue systems follow theories on speech
acts originally developed by Searle (1975). A
number of DA annotation schemes have been de-
veloped previously (Core and Allen, 1997), (Car-
letta et al., 1997). Several of dialogue tagging
schemes strive to provide domain-independence
(Core and Allen, 1997), (Bunt, 2011).
Bunt (2011) developed a NIST standardized
domain-independent annotation scheme which in-
corporates elements from the previously devel-
oped annotation schemes. It is a hierarchical
multi-dimensional annotation scheme. Each func-
tional segment (part of an utterance correspond-
ing to a DA) can have a general purpose function,
such as Inform, Propositional Question, Yes/No
Question, and a dimension-specific function in any
number of 10 defined dimensions, such as Task,
Feedback, or Time management.
In the analysis of human-computer dialogues, it
is common to adopt DA annotation schemes to suit
specific domains. Generic domain-independent
schemes are geared towards the analysis of nat-
ural human-human dialogue and provide rich an-
notation structure that can cover complexity of
natural dialogue. Domain-specific dialogues use
a subset of the generic dialogue structure. For
example, Ohtake et al. (2009) developed a DA
scheme for tourist-guide domain motivated by a
generic annotation scheme (Ohtake et al., 2010),
and Bangalore and Stent (2009) created a dialogue
scheme for a catalogue product ordering dialogue
system. In our work we design DA scheme for
Web-Browsing domain motivated by the DAMSL
(Core and Allen, 1997) schema for task-oriented
dialogue.
We used a Wizard-of-Oz (WOZ) approach to
collect an initial dataset of spoken voice com-
124
Task ?
u
?
d
Shopping 121 16
Email 92 16
Flight 180 16
Hotel 179 16
Job 76 16
Admission 144 16
Overall 792 96
Table 1: Corpus details. ?
u
- number of utterances,
?
d
- number of dialogs.
mands by both blind and sighted users. WOZ is
commonly used before building a dialogue system
(Chotimongkol, 2008), (Ohtake et al., 2009), (Es-
kenazi et al., 1999).
In previous work on dialogue modelling, Stol-
cke et al. (2000) used HMM approach to predict
dialogue acts in a switchboard human-human di-
alogue corpus achieving 65% accuracy. Rangara-
jan Sridhar et al. (2009) applied a maximum en-
tropy classifier on the Switchbord corpus. Using
a combination of lexical, syntactic, and prosodic
features, the authors achieve accuracy of 72%
on that corpus. Following the work of Rangara-
jan Sridhar et al. (2009), we use supervised classi-
fication approach to determine dialogue act on the
annotated corpus of human-wizard web-browsing
dialogues.
3 Corpus and Annotation
In this section, we describe the corpus and the
associated dialogue act scheme. The corpus was
collected using a WOZ user study with 24 blind
participants. Exactly 50% of the participants indi-
cated that they were very comfortable with screen
readers, while the remaining 50% said they were
not comfortable with computers. We will refer to
them as ?experts? and ?beginners? respectively.
The study required each participant to complete
a set of typical web browsing tasks (shopping,
sending an email, booking a flight, reserving a ho-
tel room, searching for a job and applying for uni-
versity admission) using unrestricted speech com-
mands ranging from simple commands such as
?click the search button?, to complex commands
such as ?buy this product?. Unknown to the partic-
ipants, these commands were executed by a wiz-
ard and appropriate responses were narrated using
a screen reader. The dialogs were effective; al-
most every participant was able to complete each
assigned task by engaging in a dialogue with the
wizarded interface.
As shown in Table 1, the corpus consists of a
total of 96 dialogs collected during the execution
of 6 tasks and captures approximately 22 hours of
speech with a total of 792 user utterances and 774
system utterances. There is exactly 1 dialogue per
task for any given participant. Each user turn con-
sists of a single command that is usually a sim-
ple sentence or phrase. Each system turn is either
narration of webpage content or information re-
quest for the purpose of either form filling or dis-
ambiguation. Therefore, each dialogue turn was
treated as a single utterance and every utterance
was identified with a single associated dialogue
act.
The corpus was manually annotated with dia-
logue act labels and the labeling scheme was ver-
ified by measuring the inter-annotator agreement.
The rest of this section describes the annotation
scheme.
3.1 Dialogue Act Annotation
The dialogue act annotation scheme was inspired
by the DAMSL scheme (Core and Allen, 1997)
for task oriented dialogue. The proposed scheme
was also influenced by extended DAMSL tagset
(Stolcke et al., 2000) and the DIT++ annotation
scheme (Bunt, 2011). We customized the annota-
tion scheme to suit the non-visual web access do-
main, thereby making it more relevant to our cor-
pus and tasks.
Table 2 lists the dialogue acts for both user
and system utterances. The user dialogue act
tagset consists of labels representing task related
requests (Command-Intention, Command-Task,
Command-Multiple, Command-Navigation), in-
quiries (Question-Task, Help-Task) and informa-
tion input (Information-Task), whereas the system
DA tagset contains labels representing informa-
tion requests (Prompt), answers to user inquiries
(Question-Answer, Help-Response) and other sys-
tem responses (Short-Response, Long-Response,
etc.) to user commands.
Inter-rater agreement values for different tasks
in the corpus are presented in Table 3. The ? val-
ues for all tasks are above 0.80, which according
to Fleiss? guidelines (Fleiss, 1973), indicates ex-
cellent inter-rater reliability on the DA annotation.
Therefore, the DA tagset is generic enough to be
applicable for a wide varity of tasks that can be
performed on the web. Note that the dialogue act
scheme was specially designed for non-visual web
125
User dialogue Acts
Dialogue Act Description Frequency
Command-Intention Indication of user?s intention or end goal, e.g. I wish to buy a Bluetooth speaker 0.117
Command-Task Basic action commands like click, select, enter, etc. 0.072
Command-Multiple Complex commands requiring an execution plan comprising a sequence of basic
commands, e.g. buy this product, book this room, etc.
0.162
Command-Navigation Commands directing the movement of cursor like go to, stop, next etc. 0.136
Information-Task Information required for completing a task, e.g. departure date/return date in-
formation for flight booking task, first name, phone number, etc.
0.442
Question-Task Task specific questions like What is the cheapest flight?, What is the basic
salary?, etc.
0.041
Self-Talk Utterances not directed towards the system, e.g. hmmm, what should I do next? 0.002
Help-Task Request for help when the user wishes to speak with the experimenter, e.g. Help,
what does that mean?
0.024
System dialogue Acts
dialogue Act Description Frequency
Prompt Request for information from user to complete a task, e.g. First Name, text box
blank
0.460
Short-Response A short response to a user command, e.g. description of product, brief details of
flight, acknowledgements, etc.
0.198
Long-Response A lengthy response to a user command, e.g. Narration of entire page, list of
search results, etc.
0.120
Keyboard-Response Response to user keyboard actions 0.072
Article-Response Narration of an article 0.034
Question-Answer Response to a user question regarding task (non-help) 0.044
No-Response No response for some navigation commands like Stop 0.041
Help-Response Response to a help request from the user 0.026
Table 2: dialogue acts for non-visual Web access
access. Insofar as sighted people are concerned,
a more elaborate scheme would be required since
their utterances are dominated by visual cues, a
fact that was confirmed by a parallel user study
with sighted participants on the same set of web
tasks that were used in the wizard-of-oz study.
4 Features
This section describes the different feature sets
that we experimented with for our classification
tasks. The vector representation for training the
DA classifiers integrates several types of features
(Table 4): unigrams (U ) and syntactic features
(S), context related features (C), task related fea-
tures (T ), presence of words anywhere in an
utterance(P) and presence of words at the begin-
ning of an utterance(B). The last two feature sets
are similar to the ones used in Boyer et al. (2010).
Task ?
Shopping 0.865
Email 0.829
Flight 0.894
Hotel 0.848
Job 0.824
Admission 0.800
Table 3: Inter-rater agreement measured in terms
of Cohen?s ? for all tasks in the corpus.
The feature sets C, P , B and S are specific to
the domain of non-visual web access and were
hand-crafted based on the following three factors:
knowledge of the browsing behavior of blind users
reported in previous studies, e.g. (Borodin et al.,
2010); manual analysis of the corpus; mitigate the
effect of noise that is usually present in standard
lexical/syntactic feature sets such as n-grams and
parse tree rules. Each of the features in C, P , B
and S were crafted to have a close correspondence
to some dialogue act. For example, p
nav
is closely
tied to the Command-Navigation dialogue act.
4.1 Unigrams
Unigrams (U in Table 4) are one of the commonly
used lexical features for training dialogue act clas-
sifiers (e.g. (Boyer et al., 2010), (Stolcke et al.,
2000), (Rangarajan Sridhar et al., 2009)). Encod-
ing unigrams as features is based on the obser-
vation that some words appear more frequently
in certain dialogue acts compared to other di-
alogue acts. For example, approximately 73%
of ?want? occur in the Command-Intention DA,
100% of ?skip? occur in the Command-Navigation
DA and approximately 92% of ?select? occur
in the Command-Task DA. Word-DA corrections
can also be automatically identified using SVM
classifers trained on unigram features. Table 5
126
Overall Feature Set
UNIGRAMS (U )
Feature Description Binary
u Unigrams N
PRESENCE OF WORDS IN COMMANDS (P )
p
iyou
The utterance contains either I or you Y
p
help
The utterance contains the word help Y
p
helpq
The utterance contains words usually associated with help requests. E.g., how, am I, etc. Y
p
prev
The immediately preceding system DA is Prompt and the utterance contains words also
present in this immediately preceding system utterance
Y
p
intent
The utterance contains words , need, desire, prefer, like and their synonyms Y
p
browser
The utterance contains words also present in the web browser tab title. E.g., email, job Y
p
html
The utterance contains references to HTML elements. E.g., form, box, link, page, etc. Y
p
basic
The utterance contains a verb representing basic operations on a web page. E.g., click, edit. Y
p
nbasic
The utterance contains a verb not related to basic web page operations; a verb usually
associated with task or domain related actions. E.g. send, open, compose, etc.
Y
p
nav
The utterance contains words related to cursor movement. E.g., go to, continue, next, etc. Y
p
question
The utterance contains words usually associated with questions. E.g., what, when, why Y
SYNTACTIC STRUCTURE OF COMMANDS (S)
s
np
The utterance is a noun phrase with atleast two words Y
s
noun
The utterance consists of a single noun Y
s
basic
The utterance consists of a single verb representing basic web page operations. E.g., click,
edit, erase, select, etc.
Y
s
nbasic
The utterance consists of a single verb representing task or domain related actions. e.g.
send, open, compose, order, etc.
Y
CONTEXT RELATED FEATURES (C)
c
first
The utterance is the first command to be issued when a new website is loaded in the browser Y
c
previous
dialogue act of the immediately preceding system utterance N
POSITION OF WORDS IN COMMANDS (B)
b
nav
The utterance begins with word(s) related to cursor movement. e.g. go to, continue, etc. Y
b
question
The utterance begins with a word that is usually associated with a question. E.g., what,
when, where, why, etc.
Y
b
i
The utterance begins with the personal pronoun I. Y
b
helpq
The utterance begins with word(s) usually associated with help requests. E.g., how, am I Y
TASK RELATED FEATURES (T )
t
name
Name of the task associated with the utterance N
Table 4: Feature set for user dialogue act classification. The complete list of words associated with each
feature in P and B is provided in Appendix A.
presents few such correlations. Note that some of
the words in Table 5 are task-specific (noise); a
consequence of using a small dataset.
4.2 Presence of Words in Commands
In constract to unigram features that take into
account all possible word-DA correlations, the
presence-of-word features (P in Table 4) are lim-
ited to certain specific words that have strong cor-
relations with the DA types. For each feature
p ? P , if the presence of certain specific words
associated with p occur in an utterance, then p is
set to true. The set of words for every p that cor-
responds to some dialogue act d was contructed
by determining the discriminatory words for d us-
ing simple statistical analysis of the corpus (e.g.
relative frequencies of words) as well as by an ex-
amination of the weights of different words learnt
by the SVM classifier trained on a development
dataset using unigram features alone. e.g.., the
words continue and skip occur much more fre-
quently in Command-Navigation than in other di-
alogue acts (see Table 5) and hence are included
in p
nav
. Note however that not all discrimina-
tory words in Table 5 were used. Only generic
words, independent of any specific task, were se-
lected (see Appendix A for details).
4.3 Syntactic Structure of Commands
The binary syntactic features (S in Table 4) were
automatically extracted using the Stanford parser
(Klein and Manning, 2003). As in word-DA
correlations, some of the syntactic structure-DA
correlations were also identified by a manual in-
127
Dialogue Act Discriminatory Words
Command-Intention want, compose, book, for, look, email, find, an, ac-
counting, Stanford, a, airplane, message, I, music,
get, ticket, positions, need, bluetooth, jobs, new
Command-Task repeat, choose, delete, select, link, edit, enter,
erase, clear, fill, in, click, third, at, body, box,
again, blue, that
Command-Multiple play, read, senior, send, reviews, Harlem, artists,
study, submit, details, law, description, Kitaro,
mornings, availability, apply, construction, pay,
reservations, proceed, it, this, available
Comand-Navigation skip, next, previous, go, page, finish, stop, item,
continue, back, line, before, box, first, second, to,
top, home, part, would
Information-Task JFK, customer, no, August, July, USA, October,
Kahalui, October30th, anytime, coach, today, non-
stop, movies, York
Question-Task price, time, fare, layover, times, is, what?s, any-
thing, cheaper, best, flight, airline, complete, one-
stop, departure, cards, price, much, cost, weekly.
Help-Task help, do, mean, does, say, can, supposed, some-
thing, how, use, voice, have, apply, reservation, by,
address, give, get
Table 5: Top discriminative unigrams based on
weights from SVM classifier.
vestigation of the corpus. For example, 82.1%
of single noun-only utterrances (s
noun
) have the
DA Information-Task, 76.2% of ?basic? verb-only
utterances (s
basic
) have the DA Command-Task
and 83.3% of ?non-basic? verb-only utterances
(s
nbasic
) have the DA Command-Multiple.
4.4 Context Related Features
The local context (C in Table 4) provides valuable
cues to identify the dialogue act associated with
a user utterance. It was observed during the study
that user utterance is influenced to a large extent by
the immediately preceding system utterance. For
example, 89.95% of all user utterances immedi-
ately following the system Prompt were observed
to be Information-Task. In addition, most of the
time (probability 87.5%), the first utterance issued
for a task was Command-Intention.
4.5 Position-of-Word in Commands
Design of feature set B in Table 4 was inspired by
an analysis of the corpus which revealed that cer-
tain dialogue acts are characterized by the pres-
ence of certain words at the beginning of the cor-
responding utterances. For example, 93.4% of
all Command-Navigation utterances begin with a
cursor-movement related word (e.g. next, previ-
ous, etc. see Appendix A for the complete list).
4.6 Task Related Features
Since it is possible for different tasks to exhibit dif-
ferent feature vector patterns for the same dialogue
act, incorporating task name (T in Table 4) as an
additional feature may therefore improve classifi-
Group Composition
G1 U
G2 P ? B ? S
G3 C ? B ? S
G4 C ? P ? S
G5 C ? P ? B
G6 C ? P ? B ? S
G7 C ? P ? B ? S ? T
G8 C ? P ? B ? S ? U
Table 6: Feature groups.
cation performance by exploiting these variations
(if any) between tasks.
5 Classification Results
All classification tasks were performed using the
WEKA toolkit (Hall et al., 2009). The classifica-
tion experiments were done using Support Vector
Machine (frequently used for benchmarking), J48
Decision Tree (appropriate for a small size mostly
binary feature set) and Random Forest classifiers.
The model parameters for all classifiers were opti-
mized for maximum performance.
In addition, experiments were also performed
to assess the utility of each feature set (Table 4).
Specifically, the performance of classifiers with
different combinations (Groups 1-8 in Table 6) of
feature sets was evaluated to assess the importance
of each individual feature set. We primarily fo-
cussed on domain-specific feature sets (P , B, C
and S). Observe that group G6 differs from any
of G2 ? G5 by exactly one feature set. This lets
us to assess the individual utility of P , B, C and
S . In addition, we also extended G6 by including
U (G7) and T (G8) to determine if there was any
noticeable improvement in performance. G1 with
only unigram features serves as a baseline. All re-
ported results (Table 7) are based on 5-fold cross
validation: 632 instances for training and 158 in-
stances for testing. Table 7 presents the classifica-
tion results for different feature groups. The DA
Self-Talk was excluded from classification due to
insufficient number (2) of data points.
5.1 Classification Performance
Overall Performance: As seen in Table 7, the
tree-based classifiers (J48 and RF) performed bet-
ter than SVM in a majority of the feature groups
(6 out of 8). The random forest classifier yielded
the best performance (91% Precision, 90% Recall)
for feature group G6, whereas the G3-SVM com-
bination had the lowest performance (69% Preci-
sion, 67% Recall). However, all groups includ-
128
Performance of Feature Groups
G1 G2 G3 G4 G5 G6 G7 G8
DA MODEL P R P R P R P R P R P R P R P R
CI
SVM .83 .80 .84 .95 .71 .95 .91 .96 .82 .90 .91 .95 .89 .96 .89 .94
J48 .74 .74 .83 .90 .80 .93 .84 .95 .81 .93 .83 .95 .85 .93 .91 .95
RF .76 .74 .81 .90 .85 .94 .88 .90 .80 .87 .84 .93 .88 .89 .87 .95
CT
SVM .87 .73 .86 .81 .93 .30 .89 .87 .84 .81 .89 .83 .89 .81 .92 .88
J48 .80 .64 .80 .70 1.0 .28 .88 .79 .80 .70 .85 .75 .83 .87 .86 .67
RF .72 .58 .84 .89 .81 .26 .88 .89 .85 .85 .79 .93 .77 .78 .88 .80
CM
SVM .73 .65 .77 .58 .36 .30 .78 .64 .78 .59 .78 .64 .80 .62 .79 .78
J48 .74 .36 .78 .79 .68 .87 .83 .59 .81 .78 .76 .83 .81 .80 .76 .87
RF .79 .56 .80 .81 .68 .83 .80 .59 .82 .79 .81 .83 .80 .82 .76 .89
CN
SVM .89 .84 .93 .87 .96 .82 .67 .96 .94 .87 .96 .89 .94 .87 .90 .92
J48 .89 .65 .95 .95 .96 .92 .65 .93 .95 .95 .95 .92 .92 .93 .87 .90
RF .82 .86 .94 .94 .95 .92 .66 .95 .95 .95 .95 .95 .94 .93 .91 .88
IT
SVM .70 .89 .82 .93 .70 .81 .81 .79 .82 .93 .82 .93 .82 .94 .85 .90
J48 .54 .93 .96 .97 .94 .97 .80 .82 .96 .97 .97 .96 .96 .97 .94 .94
RF .65 .93 .98 .98 .95 .97 .81 .82 .97 .98 .98 .97 .98 .98 .97 .92
QT
SVM .66 .46 .87 .27 .90 .30 .80 .30 .62 .31 .80 .31 .70 .33 .85 .49
J48 .44 .36 .62 .33 .80 .23 .90 .30 .53 .34 .62 .31 .56 .47 .93 .32
RF .63 .36 .65 .31 .61 .39 .78 .27 .54 .35 .83 .39 .68 .51 .87 .33
HT
SVM .77 .71 .73 .65 .80 .45 .79 .63 .63 .67 .78 .63 .72 .64 .92 .76
J48 .86 .79 .80 .57 .80 .33 .81 .60 .70 .50 .81 .55 .55 .52 .93 .91
RF .85 .70 .79 .65 .78 .33 .75 .60 .74 .67 .90 .48 .67 .67 .90 .80
Overall
SVM .77 .76 .83 .82 .69 .67 .80 .79 .82 .82 .84 .83 .84 .83 .85 .85
J48 .70 .66 .88 .88 .87 .85 .80 .78 .88 .88 .89 .88 .88 .89 .87 .86
RF .74 .73 .90 .90 .86 .85 .80 .79 .89 .89 .91 .90 .90 .89 .88 .87
Table 7: Classification Results. The overall performance is the weighted average over all dialogue acts.
Notation: J48-Decision Tree, RF-Random Forest, SVM-Support Vector Machine, P-Precision, R-Recall,
CI-Command-Intention, CT-Command-Task, CM-Command-Multiple, CN-Command-Navigation, IT-
Information-Task, QT-Question-Task, HT-Help-Task. The best performances for each DA are high-
lighted in bold.
ing G3 did better than G1 with tree-based clas-
sifiers. G1 was consistently outperformed by the
other groups.
Performance on dialogue acts: In 6/8 feature
groups, the performance of SVM with respect to
IT dialogue act was significantly worse than that
of tree-based classifiers. However, SVM produced
consistently good results (> 80% in most cases)
for the CI and CT dialogue acts. All classifiers
performed very well in case of CN dialogue act
(> 80% for 7/8 groups). However, none of the
classifiers performed well in case of QT.
5.2 Importance of feature sets
From Table 7, it can be inferred that contextual
features (C) do not contribute to improving overall
classification performance. In particular, for each
classifier, the difference in overall performance
between groups G2 (excluding C) and G6 (includ-
ing C) is very small (worst case: 1% difference
in both P and R). However, inclusion of C signifi-
cantly improved the classification performance of
RF for QT and CI dialogue acts (18% improve-
ment in P, 8% improvement in R for QT, 3% im-
provement in both P and R for CI). Even in case of
J48, where group G6 yields the best performance,
Dialogue Act Discriminatory Rules
Command-Intention
? c
first
? ?b
nav
? ?p
html
? ?s
noun
? c
first
? ?b
nav
? p
html
? p
iyou
? ?c
first
??b
nav
?p
intent
??p
nav
??p
question
Command-Task
? ?c
first
??b
nav
??p
intent
??p
helpq
?p
basic
?
?p
nbasic
? ?c
first
??b
nav
??p
intent
??p
helpq
?p
basic
?
p
nbasic
? p
html
Command-Multiple
? ?c
first
??b
nav
??p
intent
??p
helpq
??p
basic
?
?p
nbasic
? c
previous
= [h|k|l|n] ? ?p
html
?
?p
question
? ?c
first
??b
nav
??p
intent
??p
helpq
??p
basic
?
p
nbasic
? c
previous
= [
?
p]
Comand-Navigation
? c
first
? b
nav
? c
first
? ?b
nav
? p
html
? ?p
iyou
? ?c
first
? b
nav
? ?s
np
? ?c
first
? b
nav
? s
np
? c
previous
= [s|a]
Information-Task
? ?c
first
??b
nav
??p
intent
??p
helpq
??p
basic
?
?p
nbasic
? c
previous
= [p]
? ?c
first
??b
nav
??p
intent
??p
helpq
??p
basic
?
p
nbasic
? c
previous
= [p] ? ?p
iyou
Question-Task
? ?c
first
??b
nav
??p
intent
??p
helpq
??p
basic
?
?p
nbasic
? c
previous
= [h|k|l|n] ? ?p
html
?
p
question
? ?c
first
??b
nav
??p
intent
??p
helpq
??p
basic
?
?p
nbasic
?c
previous
= [q|s|a]??p
nav
??p
html
?
?s
noun
Help-Task ? ?c
first
??b
nav
??p
intent
?p
helpq
?p
iyou
??b
i
Table 8: A select sample of J48 rules (conf ?
0.75 and descending order of support) for group
G6. Notation: ?c
first
stands for c
first
= false
and c
first
stands for c
first
= true.
129
Utterance Actual DA Predicted DA Comments
?Continue to booking it? Command-Multiple Command-Navigation
This utterance was issued while performing the book a hotel room task. This
command essentially is the same as ?book it?. The presence of a navigation
related verb continue at the beginning caused the classifiers to incorrectly classify
it as Command-Navigation.
?I am looking to check in
on July 23rd?
Information-Task Command-Intention
This utterance was in response to a system prompt for check-in date while per-
forming the book a hotel room task. The presence of first person nominative
pronoun ?I? caused the classifiers to categorize it as Command-Intention.
?What does that mean?? Help-Task Question-Task
This utterance was directed towards the experimenter and therefore it was anno-
tated as Help-Task. However, the absence of the keyword help and the presence
of a Wh-word what at the beginning of the command caused the classifiers to
incorrectly classify this command as Question-Task.
?Best available price??
?Ok, return time??
?Price??
?Layover??
Question-Task
Command-Multiple
Information
The absence of Question related words like Wh-words, is, etc. at the beginning
coupled with the fact that these commands are noun phrases caused the classifiers
to incorrectly classify them as either Command-Multiple or Information.
Table 9: A few incorrectly classified utterances.
contextual features were found to be a component
of some of the high-confidence, high-support J48
rules (Table 8) for CI and QT. Similar claims can
also be made for syntactic features(S), where al-
though there is not much difference in overall per-
formance between groups G5 and G6 (Worst Case:
2% drop in P, 1% drop in R), improvements were
observed in case of RF for QT and CI dialogue
acts (29% improvement in P, 4% improvement in
R for QT, 4% improvement in P, 6% improvement
in R for CI).
Excluding either word-existential features (P)
or word-position related features (B), however,
caused a significant drop in overall performance
(Worst case: 15% drop in P, 16% drop in R with-
out P , 11% drop in both P and R without B). Ta-
ble 8 further highlights the importance of feature
set P , since over 50% of the high performing J48
rules (Table 8) have at least one feature of type P
with true as their truth values.
It can be seen in Table 7 that adding either un-
igrams or task-name to the existing feature set of
G6 does not affect the overall performance. How-
ever, the use of unigram features improved re-
sults of all the classifiers for the HT DA. No such
DA specific improvements were seen with task-
name as an added feature to G6. This suggests
that the feature values of G6 for all DAs are task-
independent.
5.3 Prediction Errors
It is clear from Table 7 that the prediction accu-
racies of CM, QT and HT are not nearly as good
as those of other dialogue acts. Table 9 provides
some insights into this issue via illustrative exam-
ples from the corpus.
Notice that the errors in case of CI, CM and HT
are mostly related to choice of words used in the
utterances, whereas mistakes in the prediction of
QT are mainly due to inadequate information or
the incompleteness of the utterances. Therefore, it
is recommended that the speech enabled web dia-
logue systems enforce a constraint requiring users
to express their complete thoughts in each of their
corresponding utterances.
6 Conclusion
Experiments with the dialogue act model de-
scribed in the paper indicate that with a small set
of simple lexical/syntactic features it is possible
to achieve a high overall dialogue act recogni-
tion accuracy (over 90% precision and recall) us-
ing simple and well-known tree-based classifiers
such as decision trees and random forests. It is
hence possible to build speech-enabled dialogue-
based assistive web browsing systems with low
computational overhead that, inturn, can result in
low latency response times - a critical requirement
from a usability perspective for blind users. Fi-
nally, a dialogue model for non-visual web access,
such as the one described in this paper, can be the
key driver of goal-oriented web browsing - a next
generation assistive technology that will empower
blind users to stay focused on high-level browsing
tasks, while the system does all of the low-level
operations such as clicking on links, filling forms,
etc., necessary to accomplish the tasks.
Acknowledgements
Research reported in this publication was sup-
ported by the National Eye Institute of the Na-
tional Institutes of Health under award number
1R43EY21962-1A1. We would like to thank
Lighthouse Guild International and Dr. William
Seiple in particular for helping conduct user stud-
ies.
130
References
AFB. 2013. Facts and figures on american adults
with vision loss. http://www.afb.org/
info/blindness-statistics/adults/
facts-and-figures/235, January.
Apple-Inc. 2013. Voiceover for os x. http:
//www.apple.com/accessibility/osx/
voiceover/.
Srinivas Bangalore and Amanda J Stent. 2009. In-
cremental parsing models for dialog task structure.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 94?102. Association for Compu-
tational Linguistics.
Yevgen Borodin, Jeffrey P Bigham, Glenn Dausch, and
IV Ramakrishnan. 2010. More than meets the eye:
a survey of screen-reader browsing strategies. In
Proceedings of the 2010 International Cross Dis-
ciplinary Conference on Web Accessibility (W4A),
page 13. ACM.
Kristy Elizabeth Boyer, Eun Young Ha, Robert
Phillips, Michael D Wallis, Mladen A Vouk, and
James C Lester. 2010. Dialogue act modeling in
a complex task-oriented domain. In Proceedings
of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 297?305.
Association for Computational Linguistics.
Harry Bunt. 2011. Multifunctionality in dialogue.
Computer Speech & Language, 25(2):222?245.
Jean Carletta, Stephen Isard, Gwyneth Doherty-
Sneddon, Amy Isard, Jacqueline C Kowtko, and
Anne H Anderson. 1997. The reliability of a dia-
logue structure coding scheme. Computational lin-
guistics, 23(1):13?31.
Ananlada Chotimongkol. 2008. Learning the structure
of task-oriented conversations from the corpus of in-
domain dialogs. Ph.D. thesis, SRI International.
Mark G Core and James Allen. 1997. Coding dialogs
with the damsl annotation scheme. In AAAI fall sym-
posium on communicative action in humans and ma-
chines, pages 28?35. Boston, MA.
Maxine Eskenazi, Alexander I Rudnicky, Karin Gre-
gory, Paul C Constantinides, Robert Brennan,
Christina L Bennett, and Jwan Allen. 1999. Data
collection and processing in the carnegie mellon
communicator. In EUROSPEECH.
C?esar Gonz?alez Ferreras and Valent??n Carde?noso-Payo.
2005. Development and evaluation of a spoken di-
alog system to access a newspaper web site. In IN-
TERSPEECH, pages 857?860.
J.L. Fleiss. 1973. Statistical methods for rates and
proportions Rates and proportions. Wiley.
Freedom-Scientific. 2014. Screen read-
ing software from freedom scientific.
http://www.freedomscientific.com/
products/fs/jaws-product-page.asp.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10?
18.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim,
Donghyeon Lee, and Gary Geunbae Lee. 2010. Re-
cent approaches to dialog management for spoken
dialog systems. JCSE, 4(1):1?22.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Kiyonori Ohtake, Teruhisa Misu, Chiori Hori, Hideki
Kashioka, and Satoshi Nakamura. 2009. Annotat-
ing dialogue acts to construct dialogue systems for
consulting. In Proceedings of the 7th Workshop on
Asian Language Resources, pages 32?39. Associa-
tion for Computational Linguistics.
Kiyonori Ohtake, Teruhisa Misu, Chiori Hori, Hideki
Kashioka, and Satoshi Nakamura. 2010. Dialogue
acts annotation for nict kyoto tour dialogue corpus
to construct statistical dialogue systems. In LREC.
Yury Puzis, Yevgen Borodin, Rami Puzis, and IV Ra-
makrishnan. 2013. Predictive web automation as-
sistant for people with vision impairments. In Pro-
ceedings of the 22nd international conference on
World Wide Web, pages 1031?1040. International
World Wide Web Conferences Steering Committee.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth Narayanan. 2009. Combining lexi-
cal, syntactic and prosodic cues for improved online
dialog act tagging. Computer Speech & Language,
23(4):407?422.
John R Searle. 1975. Indirect speech acts. Syntax and
semantics, 3:59?82.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339?373.
Lu Wang, Larry Heck, and Dilek Hakkani-Tur. 2014.
Leveraging semantic web search and browse ses-
sions for multi-turn spoken dialog systems.
131
A List of Words Predictive of Dialogue
Acts
Table 10 lists all the words associated with
presence-of-word (P) and position-of-word (B)
related features (Table 4) used in this work. No-
tice that all words specified in Table 10 are task-
independent. This ensures that the proposed fea-
ture set is generic enough to be applicable for a
wide variety of tasks on the web. The proposed
list of words can be easily extended by adding syn-
onyms, which can be obtained automatically from
publicly available sources like WordNet (Miller,
1995).
Features Predictive Words
p
iyou
I, you
p
help
help
p
helpq
, b
helpq
how, can, do, am I
p
prev
dynamically determined at runtime
p
intent
want, like, would, need, prefer
p
browser
dynamically determined at runtime
p
html
body, page, form, box, field, search, link, button,
list, dropdown
p
basic
clear, select, fill, delete, click, edit, erase, submit,
repeat, choose, enter, check
p
nbasic
any verb not in the p
basic
list above
p
nav
, b
nav
skip, go to, next, first, last, back, continue, previ-
ous, stop, go back, finish, home page
p
question
, b
question
what, where, why, when, how
Table 10: Complete list of predictive words for
features in P and B of Table 4.
As explained earlier, the words in Table 10 were
selected by performing simple statistical analysis
of corpus and also by examining the word-weights
produced by the SVM classifier trained on uni-
gram features alone. In other words, some of the
words in Table 10 were borrowed from Table 5
that lists discriminatory unigrams for different di-
alogue acts. Note that the task-dependent words
(e.g. ?Stanford?, ?airplane?, etc.) in Table 5 were
ignored while constructing Table 10.
B Sample Dialogue in the Corpus
Table 11 presents an example of a dialogue that
was collected during the execution of the Shop-
ping task by a participant in the Wizard-Of-Oz
study. For deeper understanding, the wizard ac-
tions for every user utterance are also listed.
Subject: ?I want to buy CD?
Wizard: [enter ?CDs? in the search box, clicks ?Go?, and
narrate search results one by one]
Screen reader: Verbatim 97458 700 MB 80 Minute . . . Disc CD-R
by Verbatim link . . .
Subject: ?Search for new age items?
Wizard: [searches for ?new age items?, clicks on results]
Screen reader: Age of Wushu - Free Amazon . . . Game Connect
link by Snail Games USA . . .
Subject: ?stop?
Wizard: [presses pause shortcut]
Subject: ?new age music CDs?
Wizard: [searches for ?new music CDs?]
Screen reader: The ultimate Most Relaxing New Age . . . .
Subject: ?new age music CDs by Kitaro?
Wizard: [Searches for ?new music cds by Kitaro?]
Screen reader: Most Relaxing New Age Music link by Kitaro,
List 8 items, $10.87 link, order in the next 3 hours
. . . Tenku link by Kitaro . . .
Subject: ?sample recordings?
Wizard: [follows the current link]
Screen reader: Page loading
Wizard: [plays the first sample track]
Screen reader: [audio sample]
Subject: ?next?
Wizard: [plays the next sample track]
Screen reader: [audio sample]
Subject: ?next?
Wizard: [plays the next sample track]
Screen reader: [audio sample]
Subject: ?go back?
Wizard: [presses back button]
Screen reader: page loading, [repeats the visited link]
Subject: ?next CD?
Wizard: [clicks the title of the next item in search result]
Screen Reader: Ancient link by Kitaro . . . $14.98 link . . .
Subject: ?listen to audio?
Wizard: [follows link]
Screen Reader: Page loading
Wizard: [plays the next sample track]
Screen reader: [audio sample]
Subject: ?next?
Wizard: [plays the next sample track]
Screen reader: [audio sample]
Subject: ?buy this cd?
Wizard: [clicks ?Add to cart? button, then clicks ?Proceed
to Checkout? button]
Screen reader: [reads out all captions]
Table 11: An example dialogue from corpus along
with associated wizard actions.
132
Proceedings of the SIGDIAL 2014 Conference, pages 238?242,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Detecting Inappropriate Clarification Requests in Spoken Dialogue
Systems
Alex Liu
1
, Rose Sloan
2
, Mei-Vern Then
1
, Svetlana Stoyanchev
3
,
Julia Hirschberg
1
, Elizabeth Shriberg
4
Columbia University
1
, Yale University
2
, AT&T Labs Research
3
, SRI International
4
{al3037@columbia.edu, rose.sloan@yale.edu,
mt2837@columbia.edu, sveta@research.att.com,
julia@cs.columbia.edu, elizabeth.shriberg@sri.com}
Abstract
Spoken Dialogue Systems ask for clarifi-
cation when they think they have misun-
derstood users. Such requests may dif-
fer depending on the information the sys-
tem believes it needs to clarify. However,
when the error type or location is misiden-
tified, clarification requests appear confus-
ing or inappropriate. We describe a clas-
sifier that identifies inappropriate requests,
trained on features extracted from user re-
sponses in laboratory studies. This classi-
fier achieves 88.5% accuracy and .885 F-
measure in detecting such requests.
1 Introduction
When Spoken Dialogue Systems (SDS) believe
they have not understood a user, they generate re-
quests for clarification. For example, in the fol-
lowing exchange, the System believes it has mis-
understood the word Washington in the user?s ut-
terance and asks a clarification question, prompt-
ing the user to repeat the misrecognized word.
User: I?d like a ticket to Washington.
System: A ticket to where?
User: Washington.
Clarification requests may be generic or specific
to the type and location of the information the sys-
tem believes it has not recognized. Targeted clar-
ifications focus on a specific part of an utterance,
as in the system?s question above. They use under-
stood portions of an utterance (?I?d like a ticket
to?) to query a misunderstood portion (?Wash-
ington?). Targeted clarification is a type of task-
related request, which has been shown to be more
effective and prevalent in human-human dialogues
than more general clarification requests (Skantze,
2005). Such generic clarifications signal mis-
understanding without identifying the type or lo-
cation of the misunderstanding. They often take
the form of a request to repeat or rephrase, e.g.
?please repeat?, ?please rephrase?, ?what did
you say??.
Questions that address a particular type of mis-
recognition come in several varieties. Systems
may ask reprise clarification questions, by repeat-
ing a recognized portion of an utterance (Ginzburg
and Cooper, 2004; Purver, 2004). Systems may
also request that users spell a word if they be-
lieve the misrecognized word is a proper name,
especially one that is not in its vocabulary (OOV).
They may ask the user to provide a synonym for
OOV terms that are not proper names. Systems
may also ask users to disambiguate homophones
(e.g. ?Did you mean ?right? as in correct or ?rite? as
in a ritual??). They may request confirmation ex-
plicitly (e.g. ?I heard you say Washington. Is that
correct??), or implicitly, by repeating the recog-
nized information while asking a follow-up query
(e.g. ?When do you want to go to Washington??).
Each request type may be appropriate in different
circumstances. However, when systems make in-
appropriate requests to users, such as to rephrase
a proper name or to confirm a statement that con-
tains a misrecognized word, dialogues often go
awry. Therefore, it is extremely important for sys-
tems to know when a request is inappropriate, so
that they can provide a different clarification re-
quest or fall back to a more generic strategy.
In this work, we develop a data-driven method
for detecting inappropriate clarification requests.
We have defined a list of inappropriate request
types and have collected a corpus of speaker re-
sponses to both appropriate and inappropriate re-
quests under laboratory conditions. We use this
corpus to train an inappropriate clarification clas-
sifier to be used by a system after a user responds
to a system request, in order to determine whether
the question was appropriate or not. In Section 2,
we describe previous research on error handling in
dialogue. We describe our data set in Section 3 and
238
our approach in Section 4. We present our evalua-
tion results in Section 5. We conclude in Section 6
and discuss future directions.
2 Related Work
Today?s SDS use generic approaches to clarifica-
tion, asking the user to repeat or rephrase an en-
tire utterance when the system believes it has not
been understood correctly. They use confidence
scores on the ASR hypothesis to decide whether
to accept, reject, or ask for clarification (Bohus
and Rudnicky, 2005). Hypotheses with low scores
may be confirmed and those with lower scores will
trigger a generic request for repetition or rephras-
ing. Researchers have found that the formulation
of system prompts has a significant effect on the
success of SDS interaction. Goldberg et al. (2003)
find that form of a clarification question affects
user frustration and the consequent success of clar-
ification subdialogue. In previous work, we ex-
plored the use of targeted reprise clarifications to
improve naturalness (Stoyanchev et al., 2014).
Lendvai et al. (2002) apply machine learning
methods to detect errors in human-machine di-
alogue, focusing on predicting when a user ut-
terance causes a misunderstanding. Litman et
al. (2006) identify user corrections of the system?s
recognition errors from speech prosody, ASR con-
fidence scores, and the dialogue history. In con-
trast, we focus here on detecting when a system
clarification request is the cause of dialogue prob-
lems. We employ only lexical features here, as
well as the type of system request, to investigate
user responses to a wide variety of system re-
quests, and to identify system errors in request for-
mulation from user reactions. In future work we
will include acoustic and prosodic features as well.
3 Data
Our data consists of spoken answers to clarifica-
tion requests collected at Columbia University us-
ing a simulated dialogue system in order to control
recognition results and type of system response.
The system displays a sentence and asks the user
to read it. The system then issues a pre-prepared
clarification request, which may be appropriate or
inappropriate, to which the user responds. For ex-
ample, in the following exchange, the system sim-
ulates a misunderstanding of the word furor by
asking a targeted reprise clarification question.
User: We hope this won?t create a furor.
System: Create a what?
User: A furor, an uproar.
The system issued six different types of clari-
fication requests: confirmation; rephrase, spell, or
disambiguate part of the utterance; targeted reprise
clarification; and a targeted-reprise-rephrase com-
bination. These request types were chosen based
on the types of requests made by the SRI Thunder-
BOLT speech-to-speech translation system (Ayan
and others, 2013). Confirmation questions sim-
ply ask the user to confirm an ASR hypothesis.
Rephrase-part requests ask users to rephrase a spe-
cific part of an utterance which is played back
to the user. Spell questions ask users to spell a
word or phrase using the NATO alphabet. Disam-
biguate questions clarify ambiguous terms. Tar-
geted reprise clarification questions make use of
the recognized portion of an utterance to query the
part that has been misrecognized based on the sys-
tem?s assessment. Targeted-reprise-rephrase re-
quests are similar, with the additional request for
the user to rephrase a portion of the utterance
believed to have been misrecognized, which is
played to the user.
Inappropriate requests in this study were de-
fined as those that resulted from the Thunder-
BOLT system?s incorrect identification of an er-
ror segment or an error type. For example, the
clarification request ?Please say a different word
for Afdhal? is inappropriate since it asks for a
rephrasal of a proper name. A request to spell
a very long phrase is also identified as inappro-
priate since users have found this difficult, espe-
cially when using the NATO alphabet. Requests
to disambiguate in the system provide two possi-
ble senses of the ambiguous word and are inap-
propriate when the correct sense is not one of the
two provided. Targeted reprise clarification ques-
tions are inappropriate when the error segment is
not correctly recognized and an errorful segment
is included in the question (e.g. ?The okay I zoo
would like what??). An appropriate question cor-
rectly identifies the error segment or ambiguous
term and the error type. For example, the ques-
tion ?I think ?Afdhal? is a name. Please spell it?,
would be appropriate when ?Afdhal? is OOV be-
cause it correctly targets the error and its type.
For each clarification request type, except for
confirmation questions, which are always appro-
priate, we created one or more types of inappro-
priate requests for each of the conditions we ob-
239
served in dialogues collected with the Thunder-
BOLT system. For example, when the system
asks the user to rephrase a part of their utter-
ance which the system believes to be a misrecog-
nized non-proper-name, the question is appropri-
ate when indeed that non-proper-name has been
misrecognized. However, the request will be in-
appropriate when the hypothesized error segment
played back to the user is a partial word, a proper
name, an extended segment including a name, or
a function word. We created instances of each
of these conditions for our users to respond to in
our experiment. A full list of the system question
types and their appropriate and inappropriate con-
ditions is provided in Table 3, in the Appendix.
We prepared 228 clarification requests (84 appro-
priate and 144 inappropriate), 12 for each of the 19
categories listed in Table 3 in the Appendix, based
on data in the TRANSTAC dataset (Akbacak and
others, 2009). Our subjects were 17 native Ameri-
can English speakers, each of whom answered 114
requests. We recorded speakers? answers to 714
appropriate and 1224 inappropriate requests. As
most request types have more than one inappro-
priate version, 63% of the requests in the data set
are inappropriate.
4 Experiment
We used the Weka machine learning library (Wit-
ten and Eibe, 2005) to train classifiers to predict
whether a clarification request was appropriate or
inappropriate. Our features were extracted from
transcripts of user utterances, and included lexical,
syntactic, numeric, and features from the output of
Linguistic Inquiry and Word Count (LIWC) (Pen-
nebaker et al., 2007) as described in Table 1.
We included unigram and bigram features, ex-
cluding unigrams that appeared fewer than 3 times
in the dataset (11% of the unigrams), and bi-
grams that appeared fewer than 2 times (25%),
with thresholds set empirically. LIWC features
were extracted using the LIWC 2007 software,
which includes lexical categories, such as articles
and negations, and psychological constructs, such
as affect and cognition. In one version of the
corpus, we replaced sequences of user spellings
with the tag ?SPELL? and disfluencies with the
symbol ?DISF?. We used the Stanford POS tag-
ger (Toutanova and others, 2003) to tag both
the original corpus as well as the modified ver-
sion. In the latter, we replaced the ?SPELL? and
Feature Description
word unigrams
(Lexical)
Count of unigrams
word bigrams
(Lexical)
Count of bigrams
pos bigrams
(Syntactic)
Bigrams of POS assigned by Stanford
tagger
liwc LIWC Output
func ratio Proportion of function words in re-
sponse
len spell Total length of spelling sequences in
response
request type Type of request preceding response
Table 1: Features used in Classification.
?DISF? tags with the symbols themselves. We
also mapped nine of the most frequent unigrams
to their own POS classes, such as ?no?, ?not?,
and ?neither? to ?NO? and ?word? to ?WORD?.
We then used counts of POS bigrams as a syn-
tactic feature. Additionally, as we observed that
responses to inappropriate requests contained a
higher proportion of function words, we added this
as a numeric feature. We also observed that aver-
age length of responses to inappropriate requests
was greater than responses to appropriate ones,
and we hypothesized this was in part due to in-
appropriate requests to spell long phrases. There-
fore, we also used the length of the total spelling
sequences, or the count of letters spelled out, as a
numeric feature. We also added type of clarifica-
tion request as a feature since some requests are
less likely to be inappropriate than others. For ex-
ample, we consider confirmation questions (?Did
you say . . . ??) to always be appropriate.
5 Results
We report classification results using Weka?s J48
decision tree classifier with 10-fold cross valida-
tion in Table 2, which outperformed JRip and
LibSVM in our experiments. Compared to the
majority baseline of 63.2% accuracy and .489 F-
measure, our classifier which uses all of the fea-
tures in Table 1 achieves a significant improve-
ment, with an accuracy of 88.5% and an F-
measure of .885. A baseline method that uses
only system request type feature (Req. type base-
line) achieves accuracy of 73.7% and F-measure
of .686, which is significantly below the perfor-
mance of the trained classifier. To identify the
most important features in predicting inappropri-
ate requests, we iteratively removed a single fea-
ture from the full feature set and re-evaluated pre-
diction accuracy. Table 2 shows absolute decrease
240
Features Acc (%) P/R/F-Measure
Majority baseline 63.2 * 0.399/0.632/0.489
Req. type baseline 73.7 * 0.814/0.737/0.686
All Features 88.5 0.885/0.885/0.885
less request type ?7.6 * ?0.076
less liwc ?2.3 ?0.023
less pos bigrams ?2.0 ?0.020
less word unigrams ?0.4 ?0.004
less func ratio ?0.1 ?0.001
less len spell ?0.05 ?0.0005
less word bigrams +0.05 +0.0007
Table 2: Classifying Inappropriate Requests: All
Features vs. Baseline vs. Leave-One-Out Classi-
fiers, where * indicates statistically significant dif-
ference from All Features (p < 0.01)
in percentage points and in F-measure when each
feature is removed in turn compared to the clas-
sifier trained on the full features set. We found
that system request type was the most important
feature, as performance decreased by 7.6 percent-
age points without it. This makes sense in light of
the fact that the ratio of inappropriate to appropri-
ate requests varied for the different request types
represented in our dataset. The next most useful
features were the output of LIWC and the POS
bigrams. We had hypothesized that, since LIWC
captures the presence of negations and assents, it
could capture negative user responses to the sys-
tem such as yes or no. As for the POS bigrams, we
modified the POS tags to mark common words and
included start and end markers in the bigrams be-
cause we hypothesized that the first words and last
words in the responses might be particularly infor-
mative. Looking at the decision tree created with
all our features, we find that the first five branches
involve decisions regarding the unigrams ?name?
and ?SPELL? (a collapsed spelling sequence), the
?START, ?neither?? bigram, the LIWC ingestion-
word feature, and the type of request, in that order.
Not only do these findings confirm our hypothe-
ses, they also confirm that the unigrams ?name?,
?SPELL?, and ?neither? which we had mapped to
special POS classes are particularly useful.
After training our model, we used it to classify
our entire dataset to see which responses it per-
formed well on and which it tended to misclassify.
Responses to targeted reprise and targeted-reprise-
rephrase questions together accounted for around
half of the misclassified instances. Many easily
identifiable responses to inappropriate requests in-
volved the user correcting the system, as in the fol-
lowing example:
User: You are going to need to dole out
punishment.
System: I think this is a name: ?dole out
punishment?. Please spell that name.
User: It is not a name, it is a phrase, dole
out punishment.
However, when the users did not correct the sys-
tem after an inappropriate request, their responses
appeared no different from answers to appropri-
ate requests. In the following example, the system
misrecognizes ?hyperbaric? and interprets it as the
word ?hyper? followed by an unknown phrase, but
the user simply ignores the request and repeats.
User: We are going to put you in a
hyperbaric chamber.
System: Put you in a high what? Please
give me another word or phrase
for ?perbaric?.
User: Hyperbaric chamber.
Many cases in which appropriate requests were
misclassified as inappropriate involved users re-
sponding correctly to targeted or targeted-rephrase
questions. We hypothesize that these are also due
primarily to users ignoring the inappropriate sys-
tem request and providing the information the sys-
tem should have asked for. As a result, those cases
make it difficult to distinguish between responses
to appropriate and inappropriate targeted ques-
tions. Of course, users may be giving prosodic
cues to indicate confusion or uncertainty or hyper-
articulating in their responses. We will address the
use of prosodic features in predicting inappropri-
ate requests in future work.
6 Conclusions
In this work, we have addressed a novel task of
identifying inappropriate clarification requests us-
ing features extracted from user responses. We
collected responses to inappropriate clarification
requests based on six request types in a simulated
SDS environment. The classifier trained on this
dataset detects inappropriate requests with accu-
racy of 88.5%, which is 25.3 percentage points
above the majority baseline, and an F-measure of
.885, which is .396 points above the majority F-
measure. In future work, we will include acoustic
and prosodic features as well as lexical features
and we will evaluate the use of an inappropriate
clarification request component in our speech-to-
speech translation system.
241
References
M. Akbacak et al. 2009. Recent advances in SRI?s
IraqComm
tm
Iraqi Arabic-English speech-to-speech
translation system. In ICASSP, pages 4809?4812.
N. F. Ayan et al. 2013. ?Can you give me another word
for hyperbaric??: Improving speech translation using
targeted clarification questions. In Acoustics, Speech
and Signal Processing (ICASSP), 2013 IEEE Interna-
tional Conference on, pages 8391?8395. IEEE.
D. Bohus and A. I. Rudnicky. 2005. A principled ap-
proach for rejection threshold optimization in spoken
dialog systems. In INTERSPEECH, pages 2781?2784.
J. Ginzburg and R. Cooper. 2004. Clarification, ellip-
sism and the nature of contextual updates. Linguistics
and Philosophy, 27(3).
J. Goldberg, M. Ostendorf, and K. Kirchhoff. 2003.
The impact of response wording in error correction
subdialogs. In ISCA Tutorial and Research Workshop
on Error Handling in Spoken Dialogue Systems.
P. Lendvai, A. van den Bosch, E. Krahmer, and
M. Swerts. 2002. Improving machine-learned de-
tection of miscommunications in human-machine di-
alogues through informed data splitting. In Proceed-
ings of the ESSLLI Workshop on Machine Learning Ap-
proaches in Computational Linguistics, pages 1?15.
D. Litman, J. Hirschberg, and M. Swerts. 2006. Char-
acterizing and predicting corrections in spoken dia-
logue systems. Computational linguistics, 32(3):417?
438.
J. W. Pennebaker, C. K. Chung, M. Ireland, A. Gon-
zales, and R. J. Booth, 2007. The development and
psychometric properties of LIWC2007. Austin, TX.
M. Purver. 2004. The Theory and Use of Clarification
Requests in Dialogue. Ph.D. thesis, King?s College,
University of London.
G. Skantze. 2005. Exploring human error recovery
strategies: Implications for spoken dialogue systems.
Speech Communication, 45(2-3):325?341.
S. Stoyanchev, A. Liu, and J. Hirschberg. 2014. To-
wards natural clarification questions in dialogue sys-
tems. In Proceedings of AISB2014.
K. Toutanova et al. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1.
Association for Computational Linguistics.
I. Witten and F. Eibe. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
mann, San Francisco, 2nd edition.
Appendix
ID Simulation Appro. Example
1. Confirmation
1 Correctly recognized utterance yes Did you say ?place this on the pane??
2 Misrecognized utterance yes Did you say ?these are in um searches will cause the insur-
gents to priest buyer??
2. Rephrase-part
1 Full non-name word or phrase yes Please say a different word for ?surmise?.
2 Partial word no Please say a different word for ?nouncing?.
3 Name no Please say a different word for ?Afdhal?.
4 Extended segment including name no Please say a different word for ?checkpoint at Betirma?.
5 Function word no Please say a different word for ?off over?.
3. Disambiguate
1 One choice is correct yes Did you mean fliers as in handouts or fliers as in pilots?
2 Neither choice is correct no Did you mean plane as in aircraft or plain as in simple?
3 Word being disambiguated was not said no Did you mean sight as in vision or site as in location?
4. Spell
1 Name yes Please spell ?Hadi Al Hemdani?.
2 Non-name no I think this is a name: ?eluding?. Please spell that name.
3 Extended segment no Please spell ?staff are stealing themselves?.
5. Reprise
1 Error segment correctly recognized and
no other errors
yes We will search some of the what?
2 Recognition error right before ?what?
word
no Supplies of I see them what?
3 Recognition error which is not the last
word before ?what?
no Ask if they are for eating for what?
6. Reprise rephrase
1 No errors outside of the error segment yes Use a what? Please say another word for ?bristled?.
2 Error segment is a partial word no Are there any my what? Please say another word for ?nors?.
3 Error outside the targeted segment no Be a right is what? Please say another word for ?rain?.
Table 3: Clarification Requests and Contexts in which they are Appropriate and Inappropriate.
242
Proceedings of the SIGDIAL 2014 Conference, pages 257?259,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
MVA: The Multimodal Virtual Assistant
Michael Johnston
1
, John Chen
1
, Patrick Ehlen
2
, Hyuckchul Jung
1
, Jay Lieske
2
, Aarthi Reddy
1
,
Ethan Selfridge
1
, Svetlana Stoyanchev
1
, Brant Vasilieff
2
, Jay Wilpon
1
AT&T Labs Research
1
, AT&T
2
{johnston,jchen,ehlen,hjung,jlieske,aarthi,
ethan,sveta,vasilieff,jgw}@research.att.com
Abstract
The Multimodal Virtual Assistant (MVA)
is an application that enables users to plan
an outing through an interactive multi-
modal dialog with a mobile device. MVA
demonstrates how a cloud-based multi-
modal language processing infrastructure
can support mobile multimodal interac-
tion. This demonstration will highlight in-
cremental recognition, multimodal speech
and gesture input, contextually-aware lan-
guage understanding, and the targeted
clarification of potentially incorrect seg-
ments within user input.
1 Introduction
With the recent launch of virtual assistant appli-
cations such as Siri, Google Now, S-Voice, and
Vlingo, spoken access to information and services
on mobile devices has become commonplace. The
Multimodal Virtual Assistant (MVA) project ex-
plores the application of multimodal dialog tech-
nology in the virtual assistant landscape. MVA de-
parts from the existing paradigm for dialog-based
mobile virtual assistants that display the unfold-
ing dialog as a chat display. Instead, the MVA
prototype situates the interaction directly within a
touch-based interface that combines a map with
visual information displays. Users can interact
using combinations of speech and gesture inputs,
and the interpretation of user commands depends
on both map and GUI display manipulation and
the physical location of the device.
MVA is a mobile application that allows users
to plan a day or evening out with friends using
natural language and gesture input. Users can
search and browse over multiple interconnected
domains, including music events, movie show-
ings, and places to eat. They can specify multi-
ple parameters in natural language, such as ?Jazz
concerts around San Francisco next Saturday?. As
users find interesting events and places, they can
be collected together into plans and shared with
others. The central components of the graph-
ical user interface are a dynamic map showing
business and event locations, and an information
display showing the current recognition, system
prompts, search result listing, or plans (Figure 1).
Figure 1: MVA User Interface
Spoken input begins when the user taps a micro-
phone button on the display. As the user speaks,
incremental speech recognition results appear. In
addition to enabling voice input, the microphone
button also activates the map as a drawing can-
vas, and enables the user to combine speech with
drawing in coordinated multimodal commands.
For example, a user might say, ?Movies playing
tonight in this area? while simultaneously outlin-
ing a relevant area on the map. Or a user may say,
?Restaurants? while drawing a line down a spe-
cific street. MVA determines the intent and dis-
ambiguates concepts in the input in order to re-
turn relevant results. MVA then responds to user
input multimodally, by updating the display and
using speech synthesis to summarize results, pro-
vide feedback, or make requests for clarification
and additional information.
257
2 Sample Interaction
In Figure 2 we present a sample of interaction
from MVA that illustrates some of its capabilities.
The user starts with a spoken natural language
query where they specify some constraints: the
type of music (jazz), location (San Francisco), and
time (tomorrow). The system gets low confidence
on the location, so it constructs a targeted clarifi-
cation for clarifying only that constraint. The user
repeats the location, and then the system searches
for events meeting the user?s constraints. The user
then reviews the results, and follows on with a
refinement: ?What about blues??. Even though
many parameters in this query are underspecified,
the system applies contextually-aware natural lan-
guage understanding and interprets this as ?Blues
concerts near San Francisco tomorrow?. After
selecting a concert, the user then searches for a
restaurant nearby. The location of the concert re-
mains salient. The user follows up with a mul-
timodal query combining speech and gesture to
search for similar restaurants in an adjoining area.
U: ?Jazz concerts near San Francisco tomorrow.?
S: ?Where did you want to see jazz tomorrow??
U: ?San Francisco.?
S: ?I found 20 jazz concerts in San
Francisco tomorrow.?
[Zooms map to San Francisco and displays
pins on map and list of results]
U: ?What about blues??
S: ?I found 20 blues concerts in
San Francisco tomorrow.?
U: [Clicks on a concert listing and adds it
to the plan]
U: ?Sushi restaurants near there.?
S: ?I found 10 sushi restaurants.?
U: ?What about here??
[Circles adjoining area on map]
S: ?I found 5 sushi restaurants in
the area you indicated?
Figure 2: Sample Interaction
3 System Architecture
Figure 3 shows the underlying multimodal assis-
tant architecture supporting the MVA app. The
user interacts with a native iOS client. When the
user taps the microphone icon, this initiates the
flow of audio interleaved with gesture and context
information streamed over a WebSocket connec-
tion to the platform.
This stream of interleaved data is handled at
the server by a multimodal natural language pro-
cessing pipeline. This fields incoming packets of
Figure 3: MVA Multimodal assistant Architecture
data from the client, demuxes the incoming data
stream, and sends audio, ink traces, and context
information to three modules that operate in par-
allel. The audio is processed using the AT&T
Watson
SM
speech recognition engine (Goffin et
al., 2005). Recognition is performed using a dy-
namic hierarchical language model (Gilbert et al.,
2011) that combines a statistical N-gram language
model with weighted sub-grammars. Ink traces
are classified into gestures using a linear classi-
fier. Speech recognition results serve as input to
two NLUmodules. A discriminative stochastic se-
quence tagger assigns tags to phrases within the
input, and then the overall string with tags is as-
signed by a statistical intent classifier to one of
a number of intents handled by the system e.g.
search(music event), refine(location).
The NLU results are passed along with gesture
recognition results and the GUI and device context
to a multimodal dialog manager. The contextual
resolution component determines if the input is a
query refinement or correction. In either case, it
retrieves the previous command from a user con-
text store and combines the new content with the
context through destructive unification (Ehlen and
Johnston, 2012). A location salience component
then applies to handle cases where a location is
not specified verbally. This component uses a su-
pervised classifier to select from among a series
of candidate locations, including the gesture (if
present), the current device location, or the current
map location (Ehlen and Johnston, 2010).
The resolved semantic interpretation of the ut-
terance is then passed to a Localized Error Detec-
tion (LED) module (Stoyanchev et al., 2012). The
LEDmodule contains two maximum entropy clas-
sifiers that independently predict whether a con-
258
cept is present in the input, and whether a con-
cept?s current interpretation is correct. These clas-
sifiers use word scores, segment length, confu-
sion networks and other recognition and context
features. The LED module uses these classifiers
to produce two probability distributions; one for
presence and one for correctness. These distri-
butions are then used by a Targeted Clarification
component (TC) to either accept the input as is,
reject all of the input, or ask a targeted clarifica-
tion question (Stoyanchev et al., 2013). These de-
cisions are currently made using thresholds tuned
manually based on an initial corpus of user inter-
action withMVA. In the targeted clarification case,
the input is passed to the natural language gen-
eration component for surface realization, and a
prompt is passed back to the client for playback
to the user. Critically, the TC component decides
what to attempt to add to the common ground
by explicit or implicit confirmation, and what to
explicitly query from the user; e.g. ?Where did
you want to see jazz concerts??. The TC com-
ponent also updates the context so that incoming
responses from the user can be interpreted with re-
spect to the context set up by the clarification.
Once a command is accepted by the multimodal
dialog manager, it is passed to the Semantic Ab-
straction Layer (SAL) for execution. The SAL in-
sulates natural language dialog capabilities from
the specifics of any underlying external APIs that
the system may use in order to respond to queries.
A general purpose time normalization component
projects relative time expressions like ?tomorrow
night? or ?next week? onto a reference timeframe
provided by the client context and estimates the
intended time interval. A general purpose location
resolution component maps from natural language
expressions of locations such as city names and
neighborhoods to specific geographic coordinates.
These functions are handled by SAL?rather than
relying on any time and location handling in the
underlying information APIs?to provide consis-
tency across application domains.
SAL also includes mechanisms for category
mapping; the NLU component tags a portion
of the utterance as a concept (e.g., a mu-
sic genre or a cuisine) and SAL leverages
this information to map a word sequence to
generic domain-independent ontological represen-
tations/categories that are reusable across different
backend APIs. Wrappers in SAL map from these
categories, time, and location values to the spe-
cific query language syntax and values for each
specific underlying API. In some cases, a single
natural language query to MVA may require mul-
tiple API calls to complete, and this is captured
in the wrapper. SAL also handles API format dif-
ferences by mapping all API responses into a uni-
fied format. This unified format is then passed to
our natural language generation component to be
augmented with prompts, display text, and instruc-
tions to the client for updating the GUI. This com-
bined specification of a multimodal presentation is
passed to the interaction manager and routed back
to the client to be presented to the user.
In addition to testing the capabilities of our mul-
timodal assistant platform, MVA is designed as a
testbed for running experiments with real users.
Among other topics, we are planning experiments
with MVA to evaluate methods of multimodal in-
formation presentation and natural language gen-
eration, error detection and error recovery.
Acknowledgements
Thanks to Mike Kai and to Deepak Talesra for
their work on the MVA project.
References
Patrick Ehlen and Michael Johnston. 2010. Location
grounding in multimodal local search. In Proceed-
ings of ICMI-MLMI, pages 32?39.
Patrick Ehlen and Michael Johnston. 2012. Multi-
modal dialogue in mobile local search. In Proceed-
ings of ICMI, pages 303?304.
Mazin Gilbert, Iker Arizmendi, Enrico Bocchieri, Dia-
mantino Caseiro, Vincent Goffin, Andrej Ljolje,
Mike Phillips, Chao Wang, and Jay G. Wilpon.
2011. Your mobile virtual assistant just got smarter!
In Proceedings of INTERSPEECH, pages 1101?
1104. ISCA.
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,
Dilek Hakkani-Tur, Andrej Ljolje, S. Parthasarathy,
Mazim Rahim, Giuseppe Riccardi, and Murat Sar-
aclar. 2005. The AT&T WATSON speech recog-
nizer. In Proceedings of ICASSP, pages 1033?1036,
Philadelphia, PA, USA.
Svetlana Stoyanchev, Philipp Salletmayer, Jingbo
Yang, and Julia Hirschberg. 2012. Localized de-
tection of speech recognition errors. In Proceedings
of SLT, pages 25?30.
Svetlana Stoyanchev, Alex Liu, and Julia Hirschberg.
2013. Modelling human clarification strategies. In
Proceedings of SIGDIAL 2013, pages 137?141.
259
