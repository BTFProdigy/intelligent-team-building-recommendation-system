Lex ica l i zed  H idden Markov  Mode ls  for  Par t -o f -Speech  Tagg ing  
Sang-Zoo  Lee  and Jun - i ch i  Tsu j i i  
Del)artInent of Infor lnation Science 
Graduate  School of Scien(:e 
University of Tokyo, Hongo 7-3-1 
Bunkyo-ku, Tokyo 113, Ja, l)3,iI 
{lee,tsujii} ((~is.s.u-tokyo.ac.jp 
Hae-Chang R im 
Det)artment of Computer  Science 
Korea Ulfiversity 
i 5-Ca Anam-Dong,  Seongbuk-C~u 
Seoul 136-701, Korea 
rim~)nll).korea.ac.kr 
Abst rac t  
Since most previous works tbr HMM-1)ased tag- 
ging consider only part-ofsl)eech intbrmation in 
contexts, their models (:minor utilize lexical in- 
forlnatiol~ which is crucial tbr resolving some 
morphological tmfl)iguity. In this paper we in- 
troduce mliformly lexicalized HMMs fin: i)art -
ofst)eech tagging in 1)oth English and \](ore, an. 
The lexicalized models use a simplified back-off 
smoothing technique to overcome data Sl)arse- 
hess. In experiment;s, lexi(:alized models a(:hieve 
higher accuracy than non-lexicifliz(~d models 
and the l)ack-off smoothing metho(l mitigates 
data sparseness 1)etter (;ban simple smoothing 
methods. 
1 I n t roduct ion  
1)arl;-Ofsl)e(:('h(POS) tagging is a l)ro(:ess ill 
which a l)rOl)('.r \])()S l;ag is assigned to ea(:h wor(l 
in raw tex(;s. Ev('n though morl)h()logi(:ally am- 
l)iguous words have more thnn one P()S tag, 
they l)elong to just one tag in a colll;ex(;. 'J~o 
resolve such ambiguity, taggers lmve to consult 
various som'ces of inibrmation such as lexica\] 
i)retbrences (e.g. without consulting context, 
table is more probably a n(mn than a. ver}) or 
an adje(:t;ive), tag n-gram context;s (e.g. after a 
non-1)ossessiv(: pronoun, table is more l)robal)ly 
a verb than a. nmm or an adjective., as in th, ey ta- 
ble an amendment), word n-grain conl;e.xl;s (e.g. 
betbre lamp, table is more probal)ly an adjective 
than ~ noun or ~ verb, as in I need a table lamp), 
and so on(Lee et al, 1.999). 
However, most previous HMM-1)ased tag- 
gers consider only POS intbrmation in con- 
texts, and so they C~I~IlII()t capture lexical infi)r- 
nmtion which is necessary for resolving some 
mort)hological alnbiguity. Some recent, works 
lmve rel)orted thai; tagging a('curacy could 
l)e iml)roved 1)y using lexicM intbrnml;ion in 
their models such as the transtbrmation-based 
patch rules(Brill, 1994), the ln~txinnun entropy 
model(lIatn~q)arkhi, 1996), the statistical ex- 
ical ruh:s(Lee et al, 1999), the IIMM consid- 
ering multi-words(Kim, 1996), the selectively 
lexicalized HMM(Kim et al, 1999), and so on. 
In the l)revious works(Kim, 1996)(Kim et al, 
1999), however, their ItMMs were lexicalized se- 
h:ctively and resl;rictively. 
\]n this l>al)er w('. prol)ose a method of uni- 
formly lcxicalizing the standard IIMM for part- 
o f  speech tagging in both English and Korean. 
Because the slmrse-da.ta problem is more seri- 
ous in lexicMized models ttl~ll ill the standard 
model, a simplified version of the well-known 
back-oil' smoothing nml;hod is used to overcome 
the. 1)rol)lem. For experiments, the Brown cor- 
pus(Francis, 1982) is used lbr English tagging 
and the KUNLP (:orlms(Lee ('t al., 1999) is 
used for Kore, an tagging. Tim eXl)criln(;nl;~t\] re- 
sults show that lexicalized models l)erform bet- 
ter than non-lexicalized models and the simpli- 
fied back-off smoothing technique can mitigate 
data sparseness betl;er than silnple smoothing 
techniques. 
2 Ti le "s tandard"  HMM 
We basically follow the not~ti(m of (Charniak 
et al, 1993) to describe Bayesian models. In 
this paper, we assume that {w I , 'w~,..., w ~0 } is 
a set of words, {t t , t '2 , . . . , t ;}  is a set of POS 
tags, a sequence of random variables l'lq,,~ = 
l~q lazy... I'E~ is a sentence of n words, and a 
sequence of random w~riables T1,,, = 7~T,2... TT~ 
is a sequence of n POS tags. Because each of 
random wtrbflfles W can take as its value any 
of the words in the vocabulary, we denote the 
value of l'l(i by wi mM a lmrticular sequence of 
wflues tbr H~,j (i < j) by wi, j. In a similar wl.ty, 
we denote the value of Ti by l,i and a particular 
481 
sequence of values for T/,j (i _< j) t)y ti,j. For 
generality, terms wi,j and ti,j (i > j) are defined 
as being empty. 
Tile purpose of Bayesian models for POS tag- 
ging is to find the most likely sequence of POS 
tags for a given sequence of' words, as follows: 
= arg lnaxPr (T , ,n  =- I W,,,, = w,, ,d 
tl,n 
Because l'efhrence to the random variables 
thelnselves can 1)e oulitted, the above equation 
b eco lnes :  
T('wl,n) = argmax Pr(tl,n \[ wl,,z) (1) 
~'l,~t 
Now, Eqn. 1 is transtbrnled into Eqn. 2 since 
Pr(wl,n) is constant for all tq,~, 
Pr (l.j ,n, wl,n) 
T(*/q,n) -- argmax 
t ,  .... Pr('wl,n) 
= arDnaxP,'(tj,,~,w,,,,) (2) 
tl ,n 
Then, tile prolmbility Pr(tL,z, wl,n ) is broken 
down into Eqn. 3 by using tile chain rule. 
fl(Pr(ti,t\],i-l,Wl,i-1) ) 
Pr(tl,n,~q,r,,) = x Pr(/~i \[tl,i,~Vl,i-l) (3) 
i= l  
Because it is difficult to compute Eqn. 3, the 
standard ItMM simplified it t)3; making a strict 
Markov assumption to get a more tract~d)le 
tbrm. 
Pr(tl,,,, Wl,n) ~ x Pr(wi I td (4) 
i= l  
I51 the standard HMM, the probability of the 
current tag ti depends oi5 only the previous K 
tags ti-K,i-1 and the t)robability of' the cur- 
rent word wi depends on only the current ag 1. 
Thereibre, this model cannot consider lexical in- 
formation in contexts. 
3 Lex ica l i zed  HMMs 
In English POS tagging, the tagging unit is a 
word. On the contrary, Korean POS tagging 
prefers a morpheme 2. 
1Usually, K is determined as1 (bigram as in (Char- 
niak et al, 1993)) or 2 (trigram as in (Merialdo, 1991)). 
2The main reason is that the mtmber of word-unit 
tags is not finite because I(orean words can be ti'eely 
and newly formed l)y agglutinating morphemes(Lee t 
al., 1999). 
, / ,  
Flies/NNS Flies/VBZ 
like/CS like/IN like/JJ like/VB 
a/A~ a/IN a/NN 
ttower/NN flower/VB 
. / .  
$/$ 
Figure 1: A word-unit lattice ot' "Flies like a 
\ [ l ower  ." 
Figure 1 shows a word-unit lattice of an Eil- 
glish sentence, "Flies like a flowc'r.", where each 
node has a word and its word-unit tag. Fig- 
ure 2 shows a morpheme-unit lattice of a Ko- 
rean sentence, "NcoNeun tIal Su issDa.", where 
each node has a morphenm and its morI)heme- 
unit tag. In case of Korean, transitions across 
a word boundary, which are depicted by a solid 
line, are distinguished fl'om transitions within a 
word, which are depicted by a dotted line. ill 
both cases, sequences connected by bold lines 
indicate the most likely sequences. 
3.1 Word-un i t  mode ls  
Lexicalized HMMs fbr word-unit agging are de- 
fined 1)y making a less strict Markov assmnp- 
tion, as tbllows: 
A(T(K,j), W( I ; j ) )~  Pr(tl,,~,wl,n) 
i=\] x Pr(wi I ti-L,i, wi-I , i -1) 
Ill models A(T(K,j), 14/(L j)) ,  the probability of 
the current tag ti depends on both tile previ- 
ous I f  tags t i -K, i - i  and the previous d words 
wi- j , i - i  and the probability of the current word 
'wi depends on the current ag and the previous 
L tags ti_L, i and the previous I words wi-l , i -~. 
So, they can consider lexieal inforination. In ex- 
periments, we set I f  as 1 or 2, J as 0 or K, L as 
1 or 2, and 1 as 0 or L. If J and I are zero, the 
above models are non-lexicalized models. Oth- 
erwise, they are lexicalized models. 
482 
$/, 
Neo/N NI" Ncol/VV 
?. 4 
No'an~ P X Ncun/EFD 
H~d/NNCC Hd/NNBU H~(VV \ ] Ia /VX 
S'a/NNCG Su/NNBG 
iss/\zJ iss/VX 
Da/EFF Da/EFC 
?"'OOoo,,,j~g_._.--"- 
./ss. 
$/$ 
Figure 2: A morl)heme-unit latti(:(; of "N,oN,'un 
llal S'u i.ssl)a." (= You (:an do it.) 
r l  f in a lexicalized model A(~/(9,2), lI ('J,2)), fin" ex- 
mnl)lc , the t)robal)ility of a node "a/AT" of tlm 
most likely sequen(:e in Figure 1 is calculate(t as 
tbllows: 
l'r(AT' I NM& vIL Fli(:,~, lit,:c) 
? tq  ? x Pr(a t :'1~, NNS,  VH, 1 l'~,c.s, lil,:c) 
3.2  Morphe lne-un i t  mode ls  
l);~yesian models for lnOrl)heme-unit tagging 
tin(t the most likely se(lueame of mor\])h(mms 
and corresponding tags fi)r ;~ given sequence of 
words, as follows: 
~'(11) ,1,,) = al'glll;XX Pr (c  l,v,, ?/~,,,u I '1,,,,~) (6) 
Cl~u flltl,,t 
, ra-ax Pr(c,,,,, m,,. ' ,,,,, ,,,) (7) 
Cl,~tllt~l,u 
In the above equations, u(_> 'n) denotes the 
llllIlll)cr of morph(mms in a Se(ltlell(;e ('orre- 
spending the given word sequ('ncc, c denotes 
a morl)heme-mfit tag, 'm. denotes a morl)heme , 
aim p denotes a type of transition froln the pre- 
v ious  tag to the current ag. p can have one of 
two values, "#" denoting a transition across a 
word bomldary and "+" denoting a transition 
within a word. Be(-ause it is difficult to calculate 
Eqn. 6, the word sequence term 'w~,,, is usually 
ignored as ill Eqn. 7. Instead, we introduce p in 
Eqn. 7 to consider word-spacing 3. 
Tile probability Pr(cj ,~L, P2,u, 'm,~ ,u) is also bro- 
ken down into Eqn. 8 t)3r using the chain rule. 
Pr(c~ ,,,, P2,,, , 'm, , ,,,,) 
f l  ( \])r(ci,Pi \[ cl,i-l,P2,i-l,'lnl,i-l) ) 
~- X P1"(1~'1,i \[('d,i,I,2,i,17tl,i_\]) (8) i=1 
\]3('caus(' Eqn. 8 is not easy to (;omlmte ~it is 
sinll)lified by making a Marker assmnt)tion to 
get; a more tractal)le forlll. 
In a similar way to the case of word-unit; tag- 
ging, lexicalize(t HMMs for morl)heme-mfit tag- 
ging are defined by making a less strict Markov 
assunq)tion, as tblh)ws: 
A(C\[,q(K,.\]), AJ\[sI(L,1)) 1= Pr(c\],,,,p2,,,, 'mq,~,) 
I'r(c \[,pd I ,,I,i-,Uc/--lC/-' (!)) 
~=~, x l ' r (mi l c i  l,,i\[,>-L+l,,i\],'mi-l,i--I) 
In models A(C\[.q(tc,,I),M\[q(L,Q), the 1)robal)il- 
ity of the (:urrent mori)heme tag ci depends 
on l)oth the 1)revious K |:ags Ci_K,i_ 1 (oi)tion- 
ally, th(' tyl)eS of their transition Pi-K~ 1,i-~) 
a.n(l the 1)revious ,\] morl)hemes H~,i_.l,i_ 1 all(1 
the probability of the current mort)heine 'm,i (t(> 
1)en(ls on the current, tag and I:he previous L 
tags % l,,i (optional\]y, the typ('~s of their tran- 
sition Pi -L-t-I,i) and the 1)revious I morl)hemes 
?lti--l,i-1. ~()~ t\]l(ly ('&ll &lSO (-onsid(,r h;xi(-al in- 
formation. 
In a lexicalized model A(C,.(~#), M(~,2)) whea:e 
word-spa(:ing is considered only in the tag prob- 
al)ilities, for example, the 1)rol)al)ility of a nod(; 
"S'u/NNBG" of the most likely sequence in Fig- 
urc 2 is calculated as follows: 
Pr(NNBG, # \[ Vl4 EFD, +, Ha, l) 
x Pr(gu \[ VV, EFD, NNBG,  Ha, l) 
3.3  Parameter  es t imat ion  
In supervised lcarning~ the simpliest parameter 
estimation is the maximum likelihood(ML) cs- 
t imation(Duda et al, 1973) which lnaximizes 
the i)robal)ility ot! a training set. The ML esti- 
mate of tag (K+l ) -gram i)robal)ility, PrML (f;i \[ 
t,i-K,i-i), is calculated as follows: 
P Pr(ti l ti_ir,i_j) __ \]: q ( t i - i ( , i )  (10) 
ML Fq(ti-lGi-l) 
aMost 1)rcvious HMM-bascd Korean taggcrs except 
(Kim et al, 1998) did not consider word-spacing. 
483 
where the flmction Fq(x) returns the fl:equency 
of x in the training set. When using the max- 
imum likelihood estimation, data sparseness i
more serious in lexicalized models than in non- 
lexicalized models because the former has even 
more parameters than the latter. 
In (Chen, 1996), where various smoothing 
techniques was tested for a language model 
by using the perplexity measure, a back-off 
smoothing(Katz, 1987) is said to perform bet- 
ter on a small traning set than other methods. 
In the back-off smoothing, the smoothed prob- 
ability of tag (K+l ) -gram PrsBo(ti \[ ti-l~,i-l) 
is calculated as tbllows: 
Pr (ti \[ ti-I(,i-~) = 
,5'1~20 
drPrML(ti \[ti-I(,i-1) " if r>0 (11) 
c~(ti-K,i-1) Prsso(ti \[ ti-K+l,i-l)if r = 0 
where r = Fq(ti_t(,i), r* = ( r+ 1)'nr+l 
7~, r
r* (r+l.) x~%.+l 
dr  ~ F l t l  
1-  (r+l)xm.+l 
n l  
n,. denotes the nmnber of (K+l ) -gram whose 
frequency is r, and the coefficient dr is called 
the discount ratio, which reflects the Good- 
~lhtring estimate(Good, 1953) 4. Eqn. 11 means 
that Prxgo(ti \[ ti-K,i-l) is under-etimated by 
dr than its maximum likelihood estimate, if 
r > 0, or is backed off by its smoothing term 
Prsuo(ti \[ ti-K+j,i-l) in proportion to the 
value of the flmction (~(ti-K,i-t) of its condi- 
tional term ti-K,i-1, if r = 0. 
However, because Eqn. 11 requires compli- 
cated computation in ~(ti-l(,i-1), we simI)lify 
it to get a flmction of the frequency of a condi- 
tional term, as tbllows: 
ct(Fq(ti-K,i-1) = f) = 
E\[Fq(ti-I(,i-1) = f\] Ax E7-o E\[Fq(ti-K,i-1) -= f\] 
where A = 1 - ~ Pr (tglti-/c,i-,), 
SBO ti--K,i~r>O 
E\[Fq(ti-g,i-1) = f\] = 
SP\]to ( ti \[ti-K + l,i-1) 
t i -  K + L i,r=O,F q( t i -  K, i -1)= f ' 
(12) 
In Eqn. 12, the range of .f is bucketed into 7 
4Katz  said that  d,. = i if r > 5. 
regions such as f = 0, 1, 2, 3, 4, 5 and f > 6 since 
it is also difficult to compute this equation tbr 
all possible values of f .  
Using the formalism of our simplified back-off 
smoothing, each of probabilities whose ML es- 
timate is zero is backed off by its corresponding 
smoothing term. In experiments, the smooth- 
ing terms of Prsl~o(ti \[ ti-K,i-l,~t)i-,l,i-l) are  
determined as follows: 
PI'sBo(ti\[ ti-Ii+l,i-h )if K> 1,d> 1 
wi_j+~,i_~ 
Prsuo(ti i fK  >_ 1, d = 1 
Prs13o(ti \[ ti-K+Li-l) if K > 1, J = 0 
PrAD(ti) if K = 0, J = 0 
Also, the snloothing terms of' Pl's\]~o(wi 
ti_L,i, Wi_l,i_ 1 ) are determined as follows: 
\[ Prst~o(wi 
Prsuo  (wi 
Prs,o (wi 
PrsBO(Wi) 
PrA.O i) 
ti-L+~,i, ) if L _> 1, I>  1 
i l ) i - I+ l  , i - I  
ti-L,i) if L _> 1, I = 1 
ti-L+Li) if L >_ 1, I = 0 
i f L  = 0, I --= 0 
i l L  = -1 ,  I = 0 
In Eqn. 13 and 14, the smoothing term of a 
unigram probability is calculated by using an 
additive smoothing with 5 = 10 .2 which is cho- 
sen through experiments. The equation for the 
additive smoothing(Chen, 1996) is as tbllows: 
Fq(ti-t(,i) + 5 
AD ~tl (Fq(ti-lf,i) + 5) 
In a similar way, the smoothing terms of param- 
eters in Eqn. 9 ~re determined. 
3.4 Model  decoding 
h'om the viewpoint of the lattice structure, the 
t)roblem of POS tagging can be regarded as the 
problem of finding the most likely path ti'om the 
start node ($/$) to the end node ($/$). The 
Viterbi search algorithm(Forney, 1973), which 
has been used for HMM decoding, can be effec- 
tively applied to this task just with slight mod- 
ification 5. 
4 Exper iments  
4.1 Environment 
In experiments, the Brown corpus is used tbr 
English POS tagging and the KUNLP corpus 
'%uch modification is explained in detail in (Lee, 
1999). 
(13) 
(14) 
484 
NW 1,113,189 
NS 
NT  
DA 
RUA 
Brown KUNLP 
167,115 
53,885 15,211 
82 65 
1.64 3.4:1 
61.54% 26.72% 
NW Number of words. NS Number of sen- 
tcnccs. NT Numl){'.r of tags (nlorpheme-unit 
tag for KUNLP). DA Degree of mnbiguity 
(i.e. the number of tags per word). RUA 
1\].atio f mlanlbiguous words. 
Table 1: Intbrmat ion al)out the Brown eortms 
and the KUNLP tort}us 
Inside-test ()utside-|;(;st 
ML 95.57 94 .97  
= 1) 
-AD(a - \](}- \]) 
AD(~ = 1{}2T)  - 
ADO; - -  =a) 
A\]) ( ( ;  = 
AD(5  = 
AD(5 = 
AD(5 = \]\]}-~7)- 
AD(5 = 
93.92 93.02 
95.02 94.79 
95.42 95.08 
95.55 95.05 
95.57 94.98 
95.57 94.94 :  
95.57 94.91 
95.57 94.89 
95.57 94.87 
SBO 95.55 95.25 
ML Maximum likelihood estimate (with sim- 
ple smoothing). A\]) Additiv(~ smoothing. 
SBO Sinll}liticd 1)ack-off smootlfing. 
lal)l(, 2: lagging accura(:y (}f A(C(\]:o), M0}:0 )) 
for Kore~m POS tagging. Table 1 shows some 
intbrmation M)out 1}oth (:ori)ora {~. Each of them 
was segmented into two parts, the training set 
of 90% and the test; set of 10%, ill. the way that  
each sentence in the test set was extra{'tc, d \]i'()ln 
every 1(} senl;ellce. A(:cording to Tabl(! 1, Ko- 
reml is said to 1)e lllOre (litli(:ult to disambiguat(; 
tl\]ml English. 
We assmne "closed" wmabulary for English 
and "open" vocabulary for Korean since we do 
not h~ve any Engl ish morphological  mmlyzer 
consistent with the Brown corlms. Therefore, 
for morphological mmlysis of English, we just 
aNote that some sentcnc('.s, which have coml}osite 
tags(such as "HV+TO" in "hafta") ,  "ILLEGAL" tag, 
or "NIL" tag~ were remov(M fronl the Brown corl)us and 
tags with "*" (not) such as "BEZ*" were r(',l)la(:(~(t 1)y (:of 
r{~st}o\]ttling ta s without "*" such as "BEZ". 
2M 
1.5M 
IM 
(}.5M 
I I I I I 
- ML  
AD .x .  - 
SBO 
1,02 ,01 ,02 ,01 ,02 ,0  
{},(} 0 ,01  ,(} 1 ,02 , (}  2 ,0  
\ ] '  - - I  I \[ I I 
.99 
.98 
.97 _ I~L~ ~_? 
1,02 ,01 ,023} 13} 2,{1 
(},0 {},{} 1 ,01 ,1}  2 ,02 ,0  
.98 
.97 
2)6 
.(,):, ?vii, -r J--  
AD '?- - 
.:)4 SBO -~--- 
1,02,01,02,(11,02,0 
o,00,01,01,02,02,0 
1. 
.99 
.98 
\[ 
( 
.97 
.96 
I I I I I i t ~  
I I I I I t t } I I I I I 
1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,01,01,12,0 1,1 1,1 0,01,0 1,12,0 2,2 2,2 2,2 2,2 
(a) # of paraln{;ters 
M\], -D- 
AD -?- - - 
SB( )  
I I I I I I I I I I I I I 
1 , 1 1, l 1,0 1,1 2,01,1 2,22,22,22,2 1 ,(} 2,01,12,2 
0,01 0 1,12,01,11,10,01,01 l 2,02,22,22,22,2 
(1)} Inside-test 
1,11,I 1,01,12,01,I 2,22,22,22,21,02,01,12,2 
0 01,01,12,01,11,10,01,01,12,02,22,22,22,2 
(c) Ouiside-test 
1,02,01,02,01,02,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,00,01,0 1,02,02,0 0,01,01,l  2,01,11,10,(11,{11,I 2,02,22,22,22,2 
(d) inside vs. outside-test in SBO 
Figure 3: Results of English tagging 
485 
looked up the dictionary tailored to the Brown 
corpus. In case of Korean, we have used a Ko- 
rean morphological analyzer(Lee, 1999) which 
is consistent with the KUNLP corpus. 
4.2  Resu l t s  and  eva luat ion  
Table 2 shows the tagging accuracy of the sim- 
plest HMM, A(C(l:0),M(0:0)), for Korean tag- 
ging, according to various smoothing meth- 
ods 7. Note that ML denotes a simple smooth- 
ing method where ML estimates with prob- 
ability less than 10 -9  a re  smoothed and re- 
placed by 10-9? Because, in the outside-test, 
AD(d = 10 -2) performs better than ML and 
kD(a ? 10-2), we use 5 = 10 -2 in our ad- 
ditive smoothing. According to Table 2, SBO 
I)ertbrms well even in the simplest HMM. 
Figure 3 illustrates 4 graphs'about the results 
of English tagging: (a) the number of param- 
eters in each model, (b) the accuracy of each 
model tbr the training set, (c) the accuracy of 
each model for the test set, and (d) the accuracy 
of each model with SBO tbr both training and 
test set. Here, labels in x-axis sI)ecify models 
K,  ,1 in the way that ~ denotes A(T(\];,j) , W(Lj)). 
Therefore, the first 6 models are non-lexicalized 
models and tile others are lexicalized models. 
Actually, SBO uses more parameters than 
others. The three smoothing methods, ML, 
AD, SBO, perform well for the training set; 
since the inside-tests usually have little data 
sparseness. On the other hand, tbr the un- 
seen test set, the simple methods, ML and 
AD, cannot mitigate the data sparseness prob- 
lem, especially in sophisticated models. How- 
ever, our method SBO can overcome the prob- 
lem, as shown in Figure 3(c). Also, we can 
see in Figure 3(d) that some lexicalized mod- 
els achieve higher accuracy than non-lexicalized 
models. We can say that the best lexicalized 
model, A(T(1,~),W(1,1)) using SBO, improved 
the simple bigram model, A(T(L0),W(0,0)) us- 
? ~ 0 mg SBO, from 97.19>/o to 97.87~ (the error re- 
duction ratio of 24.20%). Interestingly, some 
lexicalized models (such as A(T(1,1), W-(0,0)) and 
A(T(1,1), W(1,o))), which have a relatively small 
number of paranmters, perform better than 
non-lexicalized models in the case of outside- 
tests using SBO. Untbrtunately, we cannot ex- 
r Ins ide - tes t  means  an  exper iment  on  the  t ra in ing  set  
i t se l f  and  outs ide - tes t  an  exper iment  on  the  tes t  se t .  
.96 
.94 ~ ?  . .~  uu . X ? "" " ~ '  " .~1%~ ~ 
.92 
.90 
ML ~ k 
.88 AD .x. - 
SBO 
.86 I I I I I I I I I I f I I I I I I I 
1,02,01,02,01,02,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,00,01,01,02,02~0 0,01,01,12,01,11,10,01,01,12,02,22,22,22,2 
(a) Outside-test 
? 97 I I I I I I ~ I I I d~ I I I t I I -~  
C,M + ? 
.966 C~,/l~/ + 
.9(;2 ~.~, -~I~ X \[\] 
+ 
1,02,01,02,01102,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,00,01,01,02,02,0 0,01,01,12,01,11,10,01,01,12,02,22,22,22,2 
(b) Considering word-spacing 
+ 
x 
? 
x 
\ [ \ ]  ? 
? 
I l l l l l l l  
Figure 4: Results of Korean tagging 
pect the result of outside-tests from that of 
inside-tests because there is no direct relation 
t)etween them 
Figm:e 4 includes 2 graphs about the re- 
sults of Korean tagging: (a) the outside ac- 
curacy of each model A(C(K,j),MiL,I)) and 
(b) the outside accnracy of each model 
A(C\[s\](~-g),M\[s\](L,0) with/without considering 
word-spacing when using SBO. Here, labels in 
K,J de-  x-axis specify models in the way that ,7,, 
notes A(C\[s\](K,j),i~/I\[.~\](Lj)) and, tbr example, 
C , ,M in (b) denotes k(C~(,r,j), M(L,r)). 
As shown in Figure 4, the simple meth- 
ods, ML and AD, cannot mitigate that sparse- 
data problem, t)ut our method SBO can over- 
come it. Also, some lexicalized models per- 
tbrm better than non-lexicalized models. On 
the other hand, considering word-spacing ives 
good clues to the models sometimes, but yet 
we cannot sw what is the best ww. From 
the experimental results, we can say that the 
best model, A(C(9,2),M(2,2)) using SBO, im- 
proved the previous models, A(C(1,0), M(o,o)) us- 
486 
ing ML(Lee, 1995), and A(G(,,0), M(0,0))using 
ML(Kim et al, 1998), t'ronl 94.97% and 95.05% 
to 96.98% (the error reduction ratio of 39.95% 
mid 38.99%) respectively. 
5 Conc lus ion  
We have 1)resented unitbrmly lexicalized HMMs 
for POS tagging of English and Korean. In 
the models, data sparseness was etlix:tively mit- 
igated by using our simplified ba(-k-ofl" smooth- 
ing. From the ext)eriments, we have ol)served 
that lexical intbrmation is usefifl fi)r POS tag- 
ging in HMMs, as is in other models, and 
ore" lexicalized models improved non-lexicalized 
models by the error reduction ratio of 24.20% 
(in English tagging) and 39.95% (in Korean tag- 
ging). 
G('.nerally, the mfiform extension of models 
requires ral)id increase of parameters, and hence 
suffers fl'om large storage a.nd sparse data. l~.e- 
cently in many areas where HMMs are used, 
many eflbrts to extend models non-mfitbrmly 
have been made, sometimes resulting in notice- 
able improvement. For this reason~ we are try- 
ing to transfbnn our uniform models into non- 
mliform models, which may 1)e more effective 
in terms of both st)ace (:omt)h'~xity and relial)le 
estimation of I)areme|;ers, without loss of accu- 
racy. 
Re ferences  
12. Brill. 1994. Some Advances in 
~l?ansformation-B ased Part of St)eech 
~Dtgging. In P~ve. of the 12th, Nat'l Cm?. on 
Art'tficial hdelligencc(AAAI-.9~), 722-727. 
E. Charniak, C. Hendrickson, N. Jacobson, and 
M. Perkowitz. 1993. l~3quations for Part- 
o f  Speech %~gging. In Proc, of the 11th, 
Nat'l CoT~:f. on Artificial Intclligence(AAAL 
93), 784-789. 
S. F. Chen. 1996. Building Probabilistic Models 
for Natural Language. Doctoral Dissert~tion, 
Harvard University, USA. 
R. O. Duda and R. E. Hart. 1973. Pattern CIas- 
s'~fication and Scene Analysis. John Wiley. 
G. D. Forney. 1973. The Viterbi Algorithm. Ill 
Proc. of the IEEE, 61:268-278. 
W. N. Francis and H. Ku~era. 1982. Fre- 
quency Analysis of English Usage: Lczicon 
and GTnmmar. Houghton Mitltin Coral)any , 
Boston, Massachusetts. 
I. J. Good. 1953. "The Population Frequen- 
cies of Species and the Estimation of Pop- 
ulation Parameters," Ill Biometrika, 40(3- 
4):237-264. 
S. M. Katz. 1987. Estimation of Probabilities 
fronl Sparse Data for the Language Model 
Component of a Speech Recognizer. In IEEE 
Transactions on Acoustics, Speech, and Signal 
i'rocessing(ASSl'), 35(3):400-401. 
J.-\]). Kim, S.-Z. Lee, and H.-C. Rim. 1998. 
A Morpheme-Unit POS Tagging Model Con- 
sidering Word-Spacing. Ill Pwc. of th.e I0 th 
National CoT~:fercnce on Korean h~:formation 
PTveessing, 3-8. 
J.-D. Kim, S.-Z. Lee, and H.-C. Rim. 1999. 
HMM Specialization with Selective Lexi- 
calization. In Pwe. of the joint SIGDAT 
Co~l:h':rence on Empirical Methods in Nat- 
'aral Language Processing and Very La'qtc 
Co'rpora(EMNLP- VL C-99), ld4-148. 
J.-H. Kim. 1996. Lcxieal Disambig'aation with 
Error-Driven Learning. Doctoral Disserta- 
tion, Korea Advanced Institute of Science and 
Te.clmology(KAIST), Korea. 
S.-H. Lee. 1995. Korean POS Tagging System 
Considering Unknown Words. Master The- 
sis, Korea Advanced Institute of Science and 
Teclmology(KAIST), Korea. 
S.-Z. Lee, .I.-D. Kim, W.-H. Ryu, and H.- 
C. Rim. 1999. A Part-of Speech Tagging 
Model Using Lexical l/.ules Based on Corlms 
Statistics. In Pwc. of the International Con- 
ference on Computer \])'lvcessin 9 of Oriental 
Languages(lCCPOL-99), 385-390. 
S.-Z. Lee. 1999. New Statistical Models for Au- 
tomatic POS Tagging. Doctoral Dissertation, 
l(orea University, Korea. 
B. Merialdo. 1991. Tagging Text with a Prol)- 
abilisl;ic Model. In P~vc. of the International 
Conference on Acoustic, Spccch and Signal 
Processing(ICASSP-91), 809-812. 
A. Ratnap~rkhi. 1996. A Maximum Entrol)y 
Model tbr Part-of-Speech Tagging. In Proe. 
of the Empirical Methods in Natural Lan- 
guage P~vcessi'ng Co'a:fercnce(EMNLP-9b'), 
133-142. 
487 
 
	 
	 	 Automatic Word Spacing Using Hidden Markov Model
for Rening Korean Text Corpora
Do-Gil Lee and Sang-Zoo Lee and Hae-Chang Rim
NLP Lab., Dept. of Computer Science and Engineering, Korea University
1, 5-ka, Anam-dong, Seongbuk-ku, Seoul 136-701, Korea
Heui-Seok Lim
Dept. of Information and Communications, Chonan University
115 AnSeo-dong, CheonAn 330-704, Korea
Abstract
This paper proposes a word spacing model using
a hidden Markov model (HMM) for rening Ko-
rean raw text corpora. Previous statistical ap-
proaches for automatic word spacing have used
models that make use of inaccurate probabilities
because they do not consider the previous spac-
ing state. We consider word spacing problem as
a classication problem such as Part-of-Speech
(POS) tagging and have experimented with var-
ious models considering extended context. Ex-
perimental result shows that the performance
of the model becomes better as the more con-
text considered. In case of the same number
of parameters are used with other method, it
is proved that our model is more eective by
showing the better results.
1 Introduction
Automatic word spacing is a process to de-
cide correct boundaries between words in a sen-
tence containing spacing errors. In Korean,
word spacing is very important to increase the
readability and to communicate the accurate
meaning of a text. For example, if a sentence
\!Qt  ~??\? [?t#Q $4(Father entered the
room)" is written as \!Qt  ~??\? [?t#Q $4
(Father entered the bag)", then its meaning
is changed a lot.
There are many word spacing errors in doc-
uments on the Internet, which is the principal
source of information. To deal with these docu-
ments properly, an automatic word spacing sys-
tem is absolutely necessary. Besides, it plays
an important role as a preprocessor of a mor-
phological analyzer that is a fundamental tool
for natural language processing applications, a
postprocessor to restore line boundaries from
an OCR, a postprocessor for continuous-syllable
sentence from a speech recognition system, and
one module for an orthographic error revision
system.
In Korean, spacing unit is Eojeol. Each Eo-
jeol consists of one or more words and a word
consists of one or more morphemes. Figure
1 represents their relationships for a sentence
\o^=??  sl???`? {9%3". According to the
rules of Korean spelling, the main principle for
word spacing is to split every word in a sen-
tence. Because one morpheme may form a word
and several morphemes too, there are confusing
cases to distinguish among words. Even though
postpositions belong to words, they should be
concatenated with the preceding word. Besides,
there are many conicting (but can be permit-
ted) cases with the principles. For example,
spacing or concatenating individual nouns in-
cluding a compound noun are both considered
as right. As mentioned, word spacing is impor-
tant for some reasons, but it is di?cult for even
man to space words correctly by spelling rules
because of the characteristics of Korean and the
inconsistent rules. Especially, it is much more
confused in the case of having no inuence on
understanding the meaning of a sentence.
In this paper, we propose a word spacing
model
1
using an HMM. HMM is a widely used
statistical model to solve various NLP prob-
lems such as POS tagging(Charniak et al, 1993;
Merialdo, 1994; Kim et al, 1998a; Lee, 1999).
We regard the word spacing problem as a classi-
cation problem such as the POS tagging prob-
lem. When using an HMM for automatic word
spacing task, raw texts can be used as training
1
Strictly speaking, our model described here is an Eo-
jeol spacing model rather than a word spacing model
because spacing unit of Korean is Eojeol. But we in
this paper do not distinguish between Eojeol and word
for convenience. Therefore, we use the term \word" as
word, spacing unit in English.
  	

    	

     	 
 
Eojeol
word
morpheme
proper noun :
person name
postposition
noun : story
noun : book
postposition
verb : read
prefinal ending
ending
Figure 1: Constitution of the sentence \o^=??  sl???`? {9%3"
data. Therefore, we expect that HMM can be
applied to the task eectively without bothering
to construct training data.
2 Related Works
Previous approaches for automatic word spac-
ing can be classied into two groups: rule based
approach and statistical approach. The rule-
based approach uses lexical information and
heuristic rules(Choi, 1997; Kim et al, 1998b;
Kang, 1998; Kang, 2000). Lexical information
consists of postposition and Eomi
2
information,
a list of spaced word examples, etc. Heuristic
rules are composed of longest match or short-
est match rule, morphological rules, and error
patterns. This approach has disadvantage re-
quiring higher computational complexity than
the statistical approach. It also costs too much
in constructing and maintaining lexical informa-
tion. Most of rule-based systems use a morpho-
logical analyzer to recognize word boundaries.
Another disadvantages of rule-based approach
are resulted from using morphological analyzer.
First, if ambiguous analyses are possible, fre-
quent backtracking may be caused and many
errors are propagated by an erroneous analy-
sis. Second, results of automatic word spacing
are highly dependent on the morphological an-
alyzer; false word boundary recognition occurs
if morphological analysis fails due to unknown
words. In addition, if an erroneous word is suc-
cessfully analyzed through overgeneration, the
error cannot even be detected. Finally, if a word
2
Eomi is a grammatical morpheme of Korean which
is attached to verbal root
spacing system is used as a preprocessor of a
morphological analyzer, the same morphologi-
cal analyzing process should be repeated twice.
The statistical approach uses syllable statis-
tics extracted from large amount of corpora to
decide whether two adjacent syllables should be
spaced or not(Shim, 1996; Shin and Park, 1997;
Chung and Lee, 1999; Jeon and Park, 2000;
Kang and Woo, 2001). In contrast to the rule-
based approach, it does not require many costs
to construct and to maintain statistics because
they can be acquired automatically. It is more
robust against unknown words than rule-based
approach that uses a morphological analyzer.
A statistical method proposed in Kang and
Woo (2001) has shown the best performance so
far. In this method, word spacing probability
P (x
i
; x
i+1
), between two adjacent syllables x
i
and x
i+1
, is in Equation 1. If the probability is
greater than 0:375, a space is inserted between
x
i
and x
i+1
.
P (x
i
; x
i+1
) = 0:25  P
R
(x
i 1
; x
i
) +
0:5  P
M
(x
i
; x
i+1
) +
0:25  P
L
(x
i+1
; x
i+2
) (1)
In Equation 1, P
R
, P
M
, and P
L
denote the
probability of a space being inserted in the right,
middle, and left of the two syllables, respec-
tively. They are calculated as follows:
P
R
(x
i 1
; x
i
) =
freq(x
i 1
; x
i
; SPACE)
freq(x
i 1
; x
i
)
P
M
(x
i
; x
i+1
) =
freq(x
i
; SPACE; x
i+1
)
freq(x
i
; x
i+1
)
PL
(x
i+1
; x
i+2
) =
freq(SPACE; x
i+1
; x
i+2
)
freq(x
i+1
; x
i+2
)
In the above equations, freq(x) denotes a fre-
quency of a string x from training data, and
SPACE denotes a white space.
Similar to this method, other statistical sys-
tems usually use the word spacing probability
estimated from every syllable bigram
3
in the
corpora. They calculate the probability by com-
bining P
R
, P
M
, and P
L
and compare it with a
certain threshold. If the probability is higher
than the threshold, then a space is inserted be-
tween two syllables.
It is reported that the performance is so sensi-
tive to training data: it shows somewhat dier-
ent performance according to similarity between
input document and training data. And there is
a crucial problem in the statistical method re-
sulted from not considering the previous spacing
state. For example, consider a sentence \/BN??
?+???e?" of which correctly word spaced sen-
tence is \/BN???+? ?? e?". According to Equa-
tion 1, the word spacing probability of \??" and
\e?" will be calculated as follows:
P (??;e?) = 0:25  P
R
(?+?;??) + 0:5  P
M
(??;e?)
+ 0:25  P
L
(e?;)
The probability P
R
(?+?;??) as follows:
P
R
(?+?;??) =
freq(?+?;??; SPACE)
freq(?+?;??)
But a space should have been inserted be-
tween \?+?" and \??" in the correct sentence,
we should use freq(SPACE;??; SPACE) in-
stead of freq(?+?;??; SPACE) in order to get
the correct word spacing probability. This phe-
nomenon comes from not considering the previ-
ous spacing state. To alleviate this problem, we
can consider the previous spacing state that the
system has decided before. But errors can be
propagated from the previous false word spac-
ing result. Eventually, to avoid such propagated
errors, the system has to generate all possible in-
terpretations from a given sentence and choose
the best one. To choose the best state from all
possible states, we use an HMM in this paper.
3
syllable bigram is dened to be any combination of
two syllables with or without a space.
3 Word Spacing Model based on
Hidden Markov Model
POS tagging is the most representative area
for HMM. Before explaining our word spacing
model using HMM, let's consider the POS tag-
ging model using an HMM. POS tagging func-
tion  (W ) is to nd the most likely sequence
of POS tags T = (t
1
; t
2
; : : : ; t
n
) for a given sen-
tence of words W = (w
1
; w
2
; : : : ; w
n
) and is de-
ned in Equation 2:
 (W )
def
=
argmax
T
P (T j W ) (2)
= argmax
T
P (T )P (W j T )
P (W )
(3)
= argmax
T
P (T )P (W j T ) (4)
= argmax
T
P (T;W ) (5)
Using Bayes' rule, Equation 2 becomes Equa-
tion 3. Since P (W ) is a constant for T , Equa-
tion 3 is transformed into Equation 4.
The probability P (T;W ) is broken down into
the following equations by using the chain rule:
P (T;W ) = P (t
1;n
; w
1;n
) (6)
=
n
Y
i=1
 
P (t
i
j t
1;i 1
; w
1;i 1
)
P (w
i
j t
1;i
; w
1;i 1
)
!
(7)

n
Y
i=1
P (t
i
j t
i K;i 1
)P (w
i
j t
i
) (8)
Markov assumptions (conditional indepen-
dence) used in Equation 8 are that the prob-
ability of a current tag t
i
conditionally depends
on only the previous K tags and that the prob-
ability of a current word w
i
conditionally de-
pends on only the current tag. In Equation 8,
P (t
i
j t
i K;i 1
) is called transition probability
and P (w
i
j t
i
) is called lexical probability. Mod-
els are classied in terms of K. The larger K
is, the more context can be considered. Because
of the data sparseness problem, bigram model
(K is 1) and trigram model (K is 2) are used in
general.
The word spacing problem can be consid-
ered similar to POS tagging. We dene a
word spacing task as a task to nd the most
likely sequence of word spacing tags T =
(t
1
; t
2
; : : : ; t
n
) for a given sentence of syllables
S = (s
1
; s
2
; : : : ; s
n
). Our word spacing model is
dened as in Equation 9:
argmax
T
P (T j S) (9)
Word spacing tag is a tag to indicate whether
the current syllable and the next one should
be spaced or not. Tag, 1 means that a space
should be put after the current syllable. Tag,
0 means that the current and the next syllable
should not be spaced. For example, if we at-
tach the word spacing tags to a sentence \/BN??
?+? ?? e?. (I can study)", then it is tagged as
\/BN/0+??/0+?+?/1+??/1+e?/0+/0+./1".
Our proposed word spacing model is to nd
the tag sequence T for maximizing the proba-
bility P (T; S).
P (T; S )
= P (t
1;n
; s
1;n
) (10)
=

P (t
1
)  p(s
1
j t
1
)



P (t
2
j t
1
; s
1
)  P (s
2
j t
1;2
; s
1
)


 
P (t
3
j t
1;2
; s
1;2
)
P (s
3
j t
1;3
; s
1;2
)
!
   

 
P (t
n
j t
1;n 1
; s
1;n 1
)
P (s
n
j t
1;n
; s
1;n 1
)
!
(11)
=
n
Y
i=1
 
P (t
i
j t
1;i 1
; s
1;i 1
)
P (s
i
j t
1;i
; s
1;i 1
)
!
(12)

n
Y
i=1
 
P (t
i
j t
i K;i 1
; s
i J;i 1
)
P (s
i
j t
i L;i
; s
i I;i 1
)
!
(13)
There are two Markov assumptions in Equa-
tion 13. One is that the probability of a current
tag t
i
conditionally depends on only the previ-
ous K (word spacing) tags and the previous J
syllables. The other is that the probability of
a current syllable s
i
conditionally depends on
only the previous L tags, the current tag t
i
, and
the previous I tags. This model is denoted by
(T
(K:J)
; S
(L:I)
). Similar to the POS tagging
model, P (t
i
j t
i K;i 1
; s
i J;i 1
) is called tran-
sition probability, and P (s
i
j t
i L;i
; s
i I;i 1
) is
called syllable probability in Equation 13. On
the other hand, our word spacing model uses
less strict Markov assumptions to consider a
larger context. The larger the values of K, J ,
L, and I are, the more context can be consid-
ered. In order to avoid the data sparseness and
excessively increasing parameters of a model, it
is important to select proper values. In our cur-
rent work, they are restricted as follows:
0  K;J; L; I  2
Thus, 3333 = 81 models are possible. But
we do not use the case of (K;J) = (0; 0) in the
trasition probabilities. As a result, we actually
use 72 models. It has not yet been known that
which model is the best. We can verify this only
by means of experiments. Some possible models
and their equations are listed in Table 1.
Probabilities can be estimated simply by the
maximum likelihood estimator (MLE) from raw
texts. The syllable probabilities and the tran-
sition probabilities of the model (T
(1:2)
; S
(1:2)
)
are estimated as follows:
P
MLE
(t
i
j t
i 1
; s
i 2;i 1
)
=
freq(s
i 2
; t
i 1
; s
i 1
; t
i
)
freq(s
i 2
; t
i 1
; s
i 1
)
P
MLE
(s
i
j t
i 1;i
; s
i 2;i 1
)
=
freq(s
i 2
; t
i 1
; s
i 1
; t
i
; s
i
)
freq(s
i 2
; t
i 1
; s
i 1
; t
i
)
To avoid zero probability, we just set very low
value such as 0:00001 if an estimated probability
is 0.
The probability that the model
(T
(1:1)
; S
(0:1)
) outputs \/BN/0+??/0+?+?/1+
??/1+e?/0+/0+./1" from a sentence \/BN??
?+???e?." is calculated as follows:
P (T; S) = P (t
0
= 0 j s
 1
= $; t
 1
= 1)
 P (s
0
=/BN j s
 1
= $; t
0
= 0)
 P (t
1
= 0 j s
0
=/BN; t
0
= 0)
 P (s
1
=?? j s
0
=/BN; t
1
= 0)
 P (1 j??0)  P (?+? j??1)
 P (1 j?+?1)  P (?? j?+?1)
 P (0 j??1)  P (e? j??0)
 P (0 je?0)  P ( je?0)
 P (1 j0)  P (. j1)
\$" is a pseudo syllable which denotes the start
of a sentence, and its tag is always 1.
4
The
4
Because any two adjacent sentences should always
be spaced.
Table 1: Some models and their equations
Model Equation
(T
(1:0)
; S
(0:0)
)
Q
n
i=1
P (t
i
j t
i 1
)  P (s
i
j t
i
)
(T
(1:1)
; S
(0:1)
)
Q
n
i=1
P (t
i
j t
i 1
; s
i 1
)  P (s
i
j t
i
; s
i 1
)
(T
(1:1)
; S
(1:1)
)
Q
n
i=1
P (t
i
j t
i 1
; s
i 1
)  P (s
i
j t
i 1;i
; s
i 1
)
(T
(1:2)
; S
(1:2)
)
Q
n
i=1
P (t
i
j t
i 1
; s
i 2;i 1
)  P (s
i
j t
i 1;i
; s
i 2;i 1
)
(T
(2:2)
; S
(2:2)
)
Q
n
i=1
P (t
i
j t
i 2;i 1
; s
i 2;i 1
)  P (s
i
j t
i 2;i
; s
i 2;i 1
)
most probable sequence of word spacing tags is
e?ciently computed by using the Viterbi algo-
rithm.
4 Experimental Results
We used balanced 21st Century Sejong Project's
raw corpus of 26 million word size. As the bal-
anced corpus is used as training data, we ex-
pect that the performance would not be sensi-
tive too much to a certain document genre. The
ETRI POS tagged corpus of 288,269 word size
was used for evaluation. We modied the cor-
pus with no word boundary form for automatic
word spacing evaluation.
We used three kinds of evaluation measures:
syllable-unit accuracy (P
syl
), word-unit recall
(R
word
), and word-unit precision (P
word
). The
word-unit recall is the rate of the number of cor-
rectly spaced words compared to the number of
total words in a test document. The word-unit
precision measures how accurate the system's
results are. The reason why we do not divide the
syllable-unit accuracy as recall and precision is
that the number of syllables in a document and
that of the system created are the same. Each
measure is dened as follows:
P
syl
=
S
correct
S
total
 100(%)
R
word
=
W
correct
W
Dtotal
 100(%)
P
word
=
W
correct
W
Stotal
 100(%)
Where, S
correct
is the number of correctly
spaced syllables, S
total
is the total number of
syllables in a document, W
correct
is the number
of correctly spaced words, W
Dtotal
is the total
number of words in a document, and W
Stotal
is
the total number of words created by a system.
To investigate every model, we calculated the
two accuracies for dierent K, J , L, and I. Ac-
curacies for each model are listed in Table 2.
According to the experimental results, we
are sure that models considering more contexts
show better results. The model (T
(2:2)
; S
(1:2)
)
is the best for all measures.
Note that some models show the better ac-
curacies than the model (T
(2:2)
; S
(2:2)
), which
uses the largest context. It seems that this is
caused by sparseness of data. After evaluat-
ing the method of Kang and Woo (2001) for
our training and test data, it shows 93:06%
syllable-unit accuracy, 76:71% word-unit recall,
and 67:80% word-unit precision. Compared
with these results, our model shows much better
performance. If I is two in (S
(K:J)
; T
(L:I)
), syl-
lable trigrams are used. Although I is less than
two (such as the model (T
(2:1)
; S
(1:1)
, which
uses syllable bigrams), our model is better than
Kang and Woo (2001)'s. This fact tells us that
our model is also more eective even when used
the same number of parameters of the model.
There are two questions that we want to
know about the word spacing models: First,
how much training data is required to get the
best performance of a given model. Second,
which model best ts a given training cor-
pus. To answer these questions, we compare
the performance of various models according to
the size of training corpus in Figure 2. The
left plot shows the syllable-unit precision and
the right plot shows the word-unit precision.
In the gure, \HMM" denotes the proposed
model, and its number decides the model's
type. \Kang" denotes Kang and Woo (2001)'s
model. \HMM2110" uses syllable unigrams,
\HMM2111" and \Kang" use syllable bigrams,
and \HMM2212" uses syllable trigrams. The
models used here are the models that show the
best accuracies among the models that use same
Table 2: Experimental results according to (K, J , L, I)
Model P
syl
R
word
P
word
Model P
syl
R
word
P
word
Model P
syl
R
word
P
word
(0,1,0,0) 84.26 41.28 44.06 (0,1,0,1) 88.93 55.38 57.10 (0,1,0,2) 88.45 53.83 55.88
(0,1,1,0) 89.44 56.91 61.34 (0,1,1,1) 95.58 79.31 82.58 (0,1,1,2) 95.74 79.76 83.68
(0,1,2,0) 84.44 42.15 47.02 (0,1,2,1) 92.86 70.26 71.63 (0,1,2,2) 94.97 76.90 79.45
(0,2,0,0) 85.48 45.65 47.52 (0,2,0,1) 88.93 56.24 57.21 (0,2,0,2) 89.59 58.23 59.88
(0,2,1,0) 90.22 59.12 63.74 (0,2,1,1) 95.60 79.26 82.94 (0,2,1,2) 95.92 80.41 84.56
(0,2,2,0) 86.46 47.62 52.15 (0,2,2,1) 93.44 72.06 73.90 (0,2,2,2) 95.22 77.84 80.59
(1,0,0,0) 85.75 47.05 48.96 (1,0,0,1) 90.24 60.73 62.20 (1,0,0,2) 89.74 58.68 61.09
(1,0,1,0) 89.28 59.80 59.98 (1,0,1,1) 95.64 81.17 81.81 (1,0,1,2) 95.90 81.50 83.56
(1,0,2,0) 82.85 45.10 45.38 (1,0,2,1) 93.30 73.04 73.39 (1,0,2,2) 94.94 77.52 78.88
(1,1,0,0) 85.83 49.95 50.43 (1,1,0,1) 90.96 63.18 64.89 (1,1,0,2) 90.21 62.99 62.58
(1,1,1,0) 89.85 61.47 62.80 (1,1,1,1) 96.15 82.88 84.10 (1,1,1,2) 96.17 82.67 84.86
(1,1,2,0) 84.21 49.44 49.29 (1,1,2,1) 94.07 75.54 76.87 (1,1,2,2) 95.62 80.32 82.13
(1,2,0,0) 87.21 54.25 54.85 (1,2,0,1) 90.83 63.34 64.59 (1,2,0,2) 91.54 66.39 67.00
(1,2,1,0) 90.74 64.14 65.63 (1,2,1,1) 96.07 82.44 84.09 (1,2,1,2) 96.39 83.51 85.91
(1,2,2,0) 86.96 55.50 55.95 (1,2,2,1) 94.67 77.53 79.28 (1,2,2,2) 95.90 81.39 83.42
(2,0,0,0) 86.18 50.25 51.42 (2,0,0,1) 90.44 61.97 63.61 (2,0,0,2) 89.77 61.52 62.17
(2,0,1,0) 89.49 61.07 61.32 (2,0,1,1) 95.83 82.11 82.73 (2,0,1,2) 95.91 82.09 83.39
(2,0,2,0) 83.37 46.52 47.15 (2,0,2,1) 93.55 73.91 74.63 (2,0,2,2) 95.03 78.36 78.96
(2,1,0,0) 86.51 52.60 53.46 (2,1,0,1) 91.10 64.81 65.85 (2,1,0,2) 90.69 65.11 65.10
(2,1,1,0) 90.34 64.04 64.90 (2,1,1,1) 96.29 83.73 84.74 (2,1,1,2) 96.28 83.43 85.21
(2,1,2,0) 85.07 52.32 52.63 (2,1,2,1) 94.31 76.69 77.82 (2,1,2,2) 95.91 81.51 83.45
(2,2,0,0) 88.58 58.94 59.84 (2,2,0,1) 91.78 67.07 68.32 (2,2,0,2) 92.44 69.88 70.54
(2,2,1,0) 91.65 67.82 69.14 (2,2,1,1) 96.26 83.46 84.88 (2,2,1,2) 96.69 84.93 86.82
(2,2,2,0) 88.97 61.20 62.28 (2,2,2,1) 95.01 78.99 80.60 (2,2,2,2) 96.04 82.05 83.96
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
10000 100000 1e+06 1e+07
sy
lla
bl
e-
un
it 
pr
ec
isi
on
 (%
)
size of training corpus (# of words)
HMM2110
HMM2111
HMM2212
Kang
20
25
30
35
40
45
50
55
60
65
70
75
80
85
10000 100000 1e+06 1e+07
w
o
rd
-u
ni
t p
re
cis
io
n 
(%
)
size of training corpus (# of words)
HMM2110
HMM2111
HMM2212
Kang
Figure 2: Accuracies according to the size of training corpus
syllable ngrams.
We can observe the changes of the accura-
cies according to the size of the training data.
\HMM2110" using syllable unigrams converges
quickly on small training data. \HMM2111"
and \Kang" using syllable bigrams converge
on much more training data. Note that
\HMM2212" does not converge in these plots.
Therefore, there is a possibility of improve-
ment of this model's performance on more large
training data. \HMM2212" shows lower per-
formance than other models on small training
data. The reason is that the data sparseness
problem occurs.
5 Conclusion
Recently, text resources available from the In-
ternet have been rapidly increased. However,
there are many word spacing errors in those re-
sources, which cannot be used before correct-
ing errors. Therefore, the need for automatic
word spacing system to rene text corpora has
been raised. In this paper, we have proposed an
automatic word spacing model using an HMM.
Our method is a statistical approach and does
not require complex processes and costs in con-
structing and maintaining lexical information
as in the rule-based approach. The proposed
model can eectively solve the word spacing
problem by using only syllable statistics auto-
matically extracted from raw corpora. Accord-
ing to the experimental results, our model shows
higher performance than the previous method
even when using the same number of parame-
ters. We used just MLE to estimate probability,
but the more a model extends the context; the
more the data sparseness problem may arise.
In future work, we plan to adopt a smoothing
technique to increase the performance. Further
research on an eective evaluation method for
conicting cases is also necessary.
References
E. Charniak, C. Hendrickson, N. Jacobson, and
M. Perkowitz. 1993. Equations for part-of-
speech tagging. In National Conference on
Articial Intelligence, pages 784{789.
J.-H. Choi. 1997. Automatic Korean spacing
words correction system with bidirectional
longest match strategy. In Proceedings of the
9th Conference on Hangul and Korean Infor-
mation Processing, pages 145{151.
Y.-M. Chung and J.-Y. Lee. 1999. Automatic
word-segmentation at line-breaks for Korean
text processing. In Proceedings of the 6th
Conference of Korea Society for Information
Mangement, pages 21{24.
N.-Y. Jeon and H.-R. Park. 2000. Automatic
word-spacing of syllable bi-gram information
for Korean OCR postprocessing. In Proceed-
ings of the 12th Conference on Hangul and
Korean Information Processing, pages 95{
100.
S.-S. Kang and C.-W. Woo. 2001. Automatic
segmentation of words using syllable bigram
statistics. In Proceedings of the 6th Natural
Language Processing Pacic Rim Symposium,
pages 729{732.
S.-S. Kang. 1998. Automatic word-
segmentation for Hangul sentences. In
Proceedings of the 10th Conference on
Hangul and Korean Information Processing,
pages 137{142.
S.-S. Kang. 2000. Eojeol-block bidirectional
algorithm for automatic word spacing of
Hangul sentences. Journal of the Korea In-
formation Science Society, 27(4):441{447.
J.-D. Kim, H.-S. Lim, S.-Z. Lee, and H.-C. Rim.
1998a. Twoply hidden markov model: A Ko-
rean pos tagging model based on morpheme-
unit with word-unit context. Computer Pro-
cessing of Oriental Languages, 11(3):277{290.
K.-S. Kim, H.-J. Lee, and S.-J. Lee. 1998b.
Three-stage spacing system for Korean in
sentence with no word boundaries. Journal
of the Korea Information Science Society,
25(12):1838{1844.
S.-Z. Lee. 1999. New statistical models for au-
tomatic part-of-speech tagging. Ph.D. thesis,
Korea University.
B. Merialdo. 1994. Tagging english text with a
probabilistic model. Computational Linguis-
tics, 20(2):155{172.
Kwangseob Shim. 1996. Automated word-
segmentation for Korean using mutual infor-
mation of syllables. Journal of the Korea In-
formation Science Society, 23(9):991{1000.
J.-H. Shin and H.-R. Park. 1997. A statisti-
cal model for Korean text segmentation using
syllable-level bigrams. In Proceedings of the
9th Conference on Hangul and Korean Infor-
mation Processing, pages 255{260.
