Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 811?821,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Shallow Local Multi Bottom-up Tree Transducers
in Statistical Machine Translation
Fabienne Braune and Nina Seemann and Daniel Quernheim and Andreas Maletti
Institute for Natural Language Processing, University of Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{braunefe,seemanna,daniel,maletti}@ims.uni-stuttgart.de
Abstract
We present a new translation model in-
tegrating the shallow local multi bottom-
up tree transducer. We perform a large-
scale empirical evaluation of our obtained
system, which demonstrates that we sig-
nificantly beat a realistic tree-to-tree base-
line on the WMT 2009 English?German
translation task. As an additional contribu-
tion we make the developed software and
complete tool-chain publicly available for
further experimentation.
1 Introduction
Besides phrase-based machine translation sys-
tems (Koehn et al, 2003), syntax-based systems
have become widely used because of their abil-
ity to handle non-local reordering. Those systems
use synchronous context-free grammars (Chi-
ang, 2007), synchronous tree substitution gram-
mars (Eisner, 2003) or even more powerful for-
malisms like synchronous tree-sequence substitu-
tion grammars (Sun et al, 2009). However, those
systems use linguistic syntactic annotation at dif-
ferent levels. For example, the systems proposed
by Wu (1997) and Chiang (2007) use no linguis-
tic information and are syntactic in a structural
sense only. Huang et al (2006) and Liu et al
(2006) use syntactic annotations on the source lan-
guage side and show significant improvements in
translation quality. Using syntax exclusively on
the target language side has also been success-
fully tried by Galley et al (2004) and Galley et
al. (2006). Nowadays, open-source toolkits such
as Moses (Koehn et al, 2007) offer syntax-based
components (Hoang et al, 2009), which allow
experiments without expert knowledge. The im-
provements observed for systems using syntactic
annotation on either the source or the target lan-
guage side naturally led to experiments with mod-
els that use syntactic annotations on both sides.
However, as noted by Lavie et al (2008), Liu et
al. (2009), and Chiang (2010), the integration of
syntactic information on both sides tends to de-
crease translation quality because the systems be-
come too restrictive. Several strategies such as
(i) using parse forests instead of single parses (Liu
et al, 2009) or (ii) soft syntactic constraints (Chi-
ang, 2010) have been developed to alleviate this
problem. Another successful approach has been
to switch to more powerful formalisms, which al-
low the extraction of more general rules. A par-
ticularly powerful model is the non-contiguous
version of synchronous tree-sequence substitu-
tion grammars (STSSG) of Zhang et al (2008a),
Zhang et al (2008b), and Sun et al (2009),
which allows sequences of trees on both sides of
the rules [see also (Raoult, 1997)]. The multi
bottom-up tree transducer (MBOT) of Arnold and
Dauchet (1982) and Lilin (1978) offers a middle
ground between traditional syntax-based models
and STSSG. Roughly speaking, an MBOT is an
STSSG, in which all the discontinuities must oc-
cur on the target language side (Maletti, 2011).
This restriction yields many algorithmic advan-
tages over both the traditional models as well as
STSSG as demonstrated by Maletti (2010). For-
mally, they are expressive enough to express all
sensible translations (Maletti, 2012)1. Figure 2
displays sample rules of the MBOT variant, called
`MBOT, that we use (in a graphical representation
of the trees and the alignment).
In this contribution, we report on our novel sta-
tistical machine translation system that uses an
`MBOT-based translation model. The theoreti-
cal foundations of `MBOT and their integration
into our translation model are presented in Sec-
tions 2 and 3. In order to empirically evaluate the
`MBOT model, we implemented a machine trans-
1A translation is sensible if it is of linear size increase
and can be computed by some (potentially copying) top-down
tree transducer.
811
S?
NP1
JJ11
Official111
NNS12
forecasts121
VP2
VBD21
predicted211
NP22
QP221
RB2211
just22111
CD2212
322121
NN222
%2221
Figure 1: Example tree t with indicated positions.
We have t(21) = VBD and t|221 is the subtree
marked in red.
lation system that we are going to make available
to the public. We implemented `MBOT inside
the syntax-based component of the Moses open
source toolkit. Section 4 presents the most im-
portant algorithms of our `MBOT decoder. We
evaluate our new system on the WMT 2009 shared
translation task English ? German. The trans-
lation quality is automatically measured using
BLEU scores, and we confirm the findings by pro-
viding linguistic evidence (see Section 5). Note
that in contrast to several previous approaches, we
perform large scale experiments by training sys-
tems with approx. 1.5 million parallel sentences.
2 Theoretical Model
In this section, we present the theoretical genera-
tive model used in our approach to syntax-based
machine translation. Essentially, it is the local
multi bottom-up tree transducer of Maletti (2011)
with the restriction that all rules must be shallow,
which means that the left-hand side of each rule
has height at most 2 (see Figure 2 for shallow
rules and Figure 4 for rules including non-shallow
rules). The rules extracted from the training exam-
ple of Figure 3 are displayed in Figure 4. Those
extracted rules are forcibly made shallow by re-
moving internal nodes. The application of those
rules is illustrated in Figures 5 and 6.
For those that want to understand the inner
workings, we recall the principal model in full de-
tail in the rest of this section. Since we utilize syn-
tactic parse trees, let us introduce trees first. Given
an alphabet ? of labels, the set T? of all ?-trees is
the smallest set T such that ?(t1, . . . , tk) ? T for
all ? ? ?, integer k ? 0, and t1, . . . , tk ? T . In-
tuitively, a tree t consists of a labeled root node ?
followed by a sequence t1, . . . , tk of its children.
A tree t ? T? is shallow if t = ?(t1, . . . , tk) with
? ? ? and t1, . . . , tk ? ?.
NP
QP NN ?
( PP
von AP NN
)
S
NP VBD NP ?
( S
NP VAFIN PP VVPP
)
Figure 2: Sample `MBOT rules.
To address a node inside a tree, we use its po-
sition, which is a word consisting of positive in-
tegers. Roughly speaking, the root of a tree is
addressed with the position ? (the empty word).
The position iw with i ? N addresses the po-
sition w in the ith direct child of the root. In
this way, each node in the tree is assigned a
unique position. We illustrate this notion in Fig-
ure 1. Formally, the positions pos(t) ? N? of
a tree t = ?(t1, . . . , tk) are inductively defined
by pos(t) = {?} ? pos(k)(t1, . . . , tk), where
pos(k)(t1, . . . , tk) =
?
1?i?k
{iw | w ? pos(ti)} .
Let t ? T? and w ? pos(t). The label of t at
position w is t(w), and the subtree rooted at posi-
tion w is t|w. These notions are also illustrated in
Figure 1. A position w ? pos(t) is a leaf (in t) if
w1 /? pos(t). In other words, leaves do not have
any children. Given a subset N ? ?, we let
leafN (t) = {w ? pos(t) | t(w) ? N, w leaf in t}
be the set of all leaves labeled by elements of N .
When N is the set of nonterminals, we call them
leaf nonterminals. We extend this notion to se-
quences t1, . . . , tk ? T? by
leaf(k)N (t1, . . . , tk) =
?
1?i?k
{iw | w ? leafN (ti)}.
Let w1, . . . , wn ? pos(t) be (pairwise prefix-
incomparable) positions and t1, . . . , tn ? T?.
Then t[wi ? ti]1?i?n denotes the tree that is ob-
tained from t by replacing (in parallel) the subtrees
at wi by ti for every 1 ? i ? n.
Now we are ready to introduce our model,
which is a minor variation of the local multi
bottom-up tree transducer of Maletti (2011). Let
? and ? be the input and output symbols, respec-
tively, and let N ? ? ?? be the set of nontermi-
nal symbols. Essentially, the model works on pairs
?t, (u1, . . . , uk)? consisting of an input tree t ? T?
812
and a sequence u1, . . . , uk ? T? of output trees.
Such pairs are pre-translations of rank k. The pre-
translation ?t, (u1, . . . , uk)? is shallow if all trees
t, u1, . . . , uk in it are shallow.
Together with a pre-translation we typically
have to store an alignment. Given a pre-translation
?t, (u1, . . . , uk)? of rank k and 1 ? i ? k,
we call ui the ith translation of t. An align-
ment for this pre-translation is an injective map-
ping ? : leaf(k)N (u1, . . . , uk)? leafN (t)?N such
that if (w, j) ? ran(?), then also (w, i) ? ran(?)
for all 1 ? j ? i.2 In other words, if an alignment
requests the ith translation, then it should also re-
quest all previous translations.
Definition 1 A shallow local multi bottom-up tree
transducer (`MBOT) is a finite set R of rules to-
gether with a mapping c : R ? R such that every
rule, written t ?? (u1, . . . , uk), contains a shal-
low pre-translation ?t, (u1, . . . , uk)? and an align-
ment ? for it.
The components t, (u1, . . . , uk), ?, and c(?)
are called the left-hand side, the right-hand
side, the alignment, and the weight of the
rule ? = t ?? (u1, . . . , uk). Figure 2 shows two
example `MBOT rules (without weights). Overall,
the rules of an `MBOT are similar to the rules of
an SCFG (synchronous context-free grammar), but
our right-hand sides contain a sequence of trees
instead of just a single tree. In addition, the align-
ments in an SCFG rule are bijective between leaf
nonterminals, whereas our model permits multi-
ple alignments to a single leaf nonterminal in the
left-hand side (see Figure 2).
Our `MBOT rules are obtained automatically
from data like that in Figure 3. Thus, we (word)
align the bilingual text and parse it in both the
source and the target language. In this manner
we obtain sentence pairs like the one shown in
Figure 3. To these sentence pairs we apply the
rule extraction method of Maletti (2011). The
rules extracted from the sentence pair of Figure 3
are shown in Figure 4. Note that these rules
are not necessarily shallow (the last two rules are
not). Thus, we post-process the extracted rules
and make them shallow. The shallow rules corre-
sponding to the non-shallow rules of Figure 4 are
shown in Figure 2.
Next, we define how to combine rules to form
derivations. In contrast to most other models, we
2ran(f) for a mapping f : A? B denotes the range of f ,
which is {f(a) | a ? A}.
S
NP
JJ
Official
NNS
forecasts
VP
VBD
predicted
NP
QP
RB
just
CD
3
NN
%
S
NP
ADJA
Offizielle
NN
Prognosen
VAFIN
sind
VP
PP
APPR
von
AP
ADV
nur
CARD
3
NN
%
VVPP
ausgegangen
Figure 3: Aligned parsed sentences.
only introduce a derivation semantics that does
not collapse multiple derivations for the same
input-output pair.3 We need one final notion.
Let ? = t ?? (u1, . . . , uk) be a rule and
w ? leafN (t) be a leaf nonterminal (occurrence)
in the left-hand side. The w-rank rk(?, w) of the
rule ? is
rk(?, w) = max {i ? N | (w, i) ? ran(?)} .
For example, for the lower rule ? in Figure 2 we
have rk(?, 1) = 1, rk(?, 2) = 2, and rk(?, 3) = 1.
Definition 2 The set ?(R, c) of weighted pre-
translations of an `MBOT (R, c) is the smallest
set T subject to the following restriction: If there
exist
? a rule ? = t?? (u1, . . . , uk) ? R,
? a weighted pre-translation
?tw, cw, (uw1 , . . . , uwkw)? ? T
for every w ? leafN (t) with
? rk(?, w) = kw,4
? t(w) = tw(?),5 and
? for every iw? ? leaf(k)N (u1, . . . , uk),6
ui(w?) = uvj (?) with ?(iw?) = (v, j),
then ?t?, c?, (u?1, . . . , u?k)? ? T is a weighted pre-
translation, where
? t? = t[w ? tw | w ? leafN (t)],
3A standard semantics is presented, for example,
in (Maletti, 2011).
4If w has n alignments, then the pre-translation selected
for it has to have suitably many output trees.
5The labels have to coincide for the input tree.
6Also the labels for the output trees have to coincide.
813
JJ
Official ?
( ADJA
Offizielle
) NNS
forecasts ?
( NN
Prognosen
) VBD
predicted ?
( VAFIN
sind ,
VVPP
ausgegangen
) RB
just ?
( ADV
nur
) CD
3 ?
( CARD
3
) NN
% ?
( NN
%
)
NP
JJ NNS ?
( NP
ADJA NN
) QP
RB CD ?
( AP
ADV CARD
) NP
QP NN ?
( PPAPPR
von
AP NN )
S
NP VP
VBD NP
? (
S
NP VAFIN VP
PP VVPP
)
Figure 4: Extracted (even non-shallow) rules. We obtain our rules by making those rules shallow.
? c? = c(?) ??w?leafN (t) cw, and
? u?i = ui[iw? ? uvj | ?(iw?) = (v, j)] for
every 1 ? i ? k.
Rules that do not contain any nonterminal
leaves are automatically weighted pre-translations
with their associated rule weight. Otherwise, each
nonterminal leaf w in the left-hand side of a rule ?
must be replaced by the input tree tw of a pre-
translation ?tw, cw, (uw1 , . . . , uwkw)?, whose root islabeled by the same nonterminal. In addition, the
rank rk(?, w) of the replaced nonterminal should
match the number kw of components in the se-
lected weighted pre-translation. Finally, the non-
terminals in the right-hand side that are aligned
to w should be replaced by the translation that the
alignment requests, provided that the nontermi-
nal matches with the root symbol of the requested
translation. The weight of the new pre-translation
is obtained simply by multiplying the rule weight
and the weights of the selected weighted pre-
translations. The overall process is illustrated in
Figures 5 and 6.
3 Translation Model
Given a source language sentence e, our transla-
tion model aims to find the best corresponding tar-
get language translation g?;7 i.e.,
g? = arg maxg p(g|e) .
We estimate the probability p(g|e) through a log-
linear combination of component models with pa-
rameters ?m scored on the pre-translations ?t, (u)?
such that the leaves of t concatenated read e.8
p(g|e) ?
7?
m=1
hm
(
?t, (u)?
)?m
Our model uses the following features
hm(?t, (u1, . . . , uk)?) for a general pre-translation
? = ?t, (u1, . . . , uk)?:
7Our main translation direction is English to German.
8Actually, t must embed in the parse tree of e; see Sec-
tion 4.
(1) The forward translation weight using the rule
weights as described in Section 2
(2) The indirect translation weight using the rule
weights as described in Section 2
(3) Lexical translation weight source? target
(4) Lexical translation weight target? source
(5) Target side language model
(6) Number of words in the target sentences
(7) Number of rules used in the pre-translation
(8) Number of target side sequences; here k times
the number of sequences used in the pre-
translations that constructed ? (gap penalty)
The rule weights required for (1) are relative
frequencies normalized over all rules with the
same left-hand side. In the same fashion the rule
weights required for (2) are relative frequencies
normalized over all rules with the same right-
hand side. Additionally, rules that were extracted
at most 10 times are discounted by multiplying
the rule weight by 10?2. The lexical weights
for (2) and (3) are obtained by multiplying the
word translationsw(gi|ej) [respectively,w(ej |gi)]
of lexically aligned words (gi, ej) accross (possi-
bly discontiguous) target side sequences.9 When-
ever a source word ej is aligned to multiple target
words, we average over the word translations.10
h3(?t, (u1, . . . , uk)?)
=
?
lexical item
e occurs in t
average {w(g|e) | g aligned to e}
The computation of the language model esti-
mates for (6) is adapted to score partial transla-
tions consisting of discontiguous units. We ex-
plain the details in Section 4. Finally, the count c
of target sequences obtained in (7) is actually used
as a score 1001?c. This discourages rules with
many target sequences.
9The lexical alignments are different from the alignments
used with a pre-translation.
10If the word ej has no alignment to a target word, then
it is assumed to be aligned to a special NULL word and this
alignment is scored.
814
Combining a rule with pre-translations:
NP
JJ NNS ?
( NP
ADJA NN
)
JJ
Official ?
( ADJA
Offizielle
) NNS
forecasts ?
( NN
Prognosen
)
Obtained new pre-translation:
NP
JJ
Official
NNS
forecasts
?
( NPADJA
Offizielle
NN
Prognosen
)
Figure 5: Simple rule application.
Combining a rule with pre-translations:
S
NP VBD NP ?
( S
NP VAFIN PP VVPP
)
NP
JJ
Official
NNS
forecasts
? (
NP
ADJA
Offizielle
NN
Prognosen
) VBD
predicted ?
( VAFIN
sind ,
VVPP
ausgegangen
)
NP
QP
RB
just
CD
3
NN
% ?
(
PP
von AP
ADV
nur
CARD
3
NN
%
)
Obtained new pre-translation:
S
NP
JJ
Official
NNS
forecasts
VBD
predicted
NP
QP
RB
just
CD
3
NN
%
?
(
S
NP
ADJA
Offizielle
NN
Prognosen
VAFIN
sind
PP
von AP
ADV
nur
CARD
3
NN
%
VVPP
ausgegangen
)
Figure 6: Complex rule application.
S
NP VAFIN PP VVPP
Offizielle Prognosen ( sind , ausgegangen ) von nur 3 %
Figure 7: Illustration of LM scoring.
815
4 Decoding
We implemented our model in the syntax-based
component of the Moses open-source toolkit
by Koehn et al (2007) and Hoang et al (2009).
The standard Moses syntax-based decoder only
handles SCFG rules; i.e, rules with contiguous
components on the source and the target lan-
guage side. Roughly speaking, SCFG rules are
`MBOT rules with exactly one output tree. We
thus had to extend the system to support our
`MBOT rules, in which arbitrarily many output
trees are allowed.
The standard Moses syntax-based decoder uses
a CYK+ chart parsing algorithm, in which each
source sentence is parsed and contiguous spans are
processed in a bottom-up fashion. A rule is appli-
cable11 if the left-hand side of it matches the non-
terminal assigned to the full span by the parser and
the (non-)terminal assigned to each subspan.12 In
order to speed up the decoding, cube pruning (Chi-
ang, 2007) is applied to each chart cell in order
to select the most likely hypotheses for subspans.
The language model (LM) scoring is directly in-
tegrated into the cube pruning algorithm. Thus,
LM estimates are available for all considered hy-
potheses. To accommodate `MBOT rules, we had
to modify the Moses syntax-based decoder in sev-
eral ways. First, the rule representation itself is ad-
justed to allow sequences of shallow output trees
on the target side. Naturally, we also had to ad-
just hypothesis expansion and, most importantly,
language model scoring inside the cube pruning
algorithm. An overview of the modified pruning
procedure is given in Algorithm 1.
The most important modifications are hidden
in lines 5 and 8. The expansion in Line 5 in-
volves matching all nonterminal leaves in the rule
as defined in Definition 2, which includes match-
ing all leaf nonterminals in all (discontiguous) out-
put trees. Because the output trees can remain
discontiguous after hypothesis creation, LM scor-
ing has to be done individually over all output
trees. Algorithm 2 describes our LM scoring in
detail. In it we use k strings w1, . . . , wk to col-
lect the lexical information from the k output com-
11Note that our notion of applicable rules differs from the
default in Moses.
12Theoretically, this allows that the decoder ignores unary
parser nonterminals, which could also disappear when we
make our rules shallow; e.g., the parse tree left in the pre-
translation of Figure 5 can be matched by a rule with left-
hand side NP(Official, forecasts).
Algorithm 1 Cube pruning with `MBOT rules
Data structures:
- r[i, j]: list of rules matching span e[i . . . j]
- h[i, j]: hypotheses covering span e[i . . . j]
- c[i, j]: cube of hypotheses covering span e[i . . . j]
1: for all `MBOT rules ? covering span e[i . . . j] do
2: Insert ? into r[i, j]
3: Sort r[i, j]
4: for all (l?? r) ? r[i, j] do
5: Create h[i, j] by expanding all nonterminals in l with
best scoring hypotheses for subspans
6: Add h[i, j] to c[i, j]
7: for all hypotheses h ? c[i, j] do
8: Estimate LM score for h // see Algorithm 2
9: Estimate remaining feature scores
10: Sort c[i, j]
11: Retrieve first ? elements from c[i, j] // we use ? = 103
ponents (u1, . . . , uk) of a rule. These strings can
later be rearranged in any order, so we LM-score
all of them separately. Roughly speaking, we ob-
tain wi by traversing ui depth-first left-to-right.
If we meet a lexical element (terminal), then we
add it to the end of wi. On the other hand, if we
meet a nonterminal, then we have to consult the
best pre-translation ? ? = ?t?, (u?1, . . . , u?k?)?, which
will contribute the subtree at this position. Sup-
pose that u?j will be substituted into the nontermi-
nal in question. Then we first LM-score the pre-
translation ? ? to obtain the string w?j correspond-
ing to u?j . This string w?j is then appended to wi.
Once all the strings are built, we score them using
our 4-gram LM. The overall LM score for the pre-
translation is obtained by multiplying the scores
for w1, . . . , wk. Clearly, this treats w1, . . . , wk as
k separate strings, although they eventually will
be combined into a single string. Whenever such
a concatenation happens, our LM scoring will au-
tomatically compute n-gram LM scores based on
the concatenation, which in particular means that
the LM scores get more accurate for larger spans.
Finally, in the final rule only one component is al-
lowed, which yields that the LM indeed scores the
complete output sentence.
Figure 7 illustrates our LM scoring for a pre-
translation involving a rule with two (discontigu-
ous) target sequences (the construction of the pre-
translation is illustrated in Figure 6). When pro-
cessing the rule rooted at S, an LM estimate is
computed by expanding all nonterminal leaves. In
our case, these are NP, VAFIN, PP, and VVPP.
However, the nodes VAFIN and VVPP are assem-
bled from a (discontiguous) tree sequence. This
means that those units have been considered as in-
816
Algorithm 2 LM scoring
Data structures:
- (u1, . . . , uk): right-hand side of a rule
- (w1, . . . , wk): k strings all initially empty
1: score = 1
2: for all 1 ? i ? k do
3: for all leaves ` in ui (in lexicographic order) do
4: if ` is a terminal then
5: Append ` to wi
6: else
7: LM score the best hypothesis for the subspan
8: Expand wi by the corresponding w?j
9: score = score ? LM(wi)
dependent until now. So far, the LM scorer could
only score their associated unigrams. However,
we also have their associated strings w?1 and w?2,
which can now be used. Since VAFIN and VVPP
now become parts of a single tree, we can perform
LM scoring normally. Assembling the string we
obtain
Offizielle Prognosen sind von nur 3 %
ausgegangen
which is scored by the LM. Thus, we first score
the 4-grams ?Offizielle Prognosen sind von?, then
?Prognosen sind von nur?, etc.
5 Experiments
5.1 Setup
The baseline system for our experiments is the
syntax-based component of the Moses open-
source toolkit of Koehn et al (2007) and Hoang
et al (2009). We use linguistic syntactic anno-
tation on both the source and the target language
side (tree-to-tree). Our contrastive system is the
`MBOT-based translation system presented here.
We provide the system with a set of SCFG as well
as `MBOT rules. We do not impose any maximal
span restriction on either system.
The compared systems are evaluated on the
English-to-German13 news translation task of
WMT 2009 (Callison-Burch et al, 2009). For
both systems, the used training data is from the
4th version of the Europarl Corpus (Koehn, 2005)
and the News Commentary corpus. Both trans-
lation models were trained with approximately
1.5 million bilingual sentences after length-ratio
filtering. The word alignments were generated
by GIZA++ (Och and Ney, 2003) with the grow-
diag-final-and heuristic (Koehn et al, 2005). The
13Note that our `MBOT-based system can be applied to any
language pair as it involves no language-specific engineering.
System BLEU
Baseline 12.60
`MBOT ?13.06
Moses t-to-s 12.72
Table 1: Evaluation results. The starred results
are statistically significant improvements over the
Baseline (at confidence p < 0.05).
English side of the bilingual data was parsed us-
ing the Charniak parser of Charniak and John-
son (2005), and the German side was parsed us-
ing BitPar (Schmid, 2004) without the function
and morphological annotations. Our German 4-
gram language model was trained on the Ger-
man sentences in the training data augmented
by the Stuttgart SdeWaC corpus (Web-as-Corpus
Consortium, 2008), whose generation is detailed
in (Baroni et al, 2009). The weights ?m in the
log-linear model were trained using minimum er-
ror rate training (Och, 2003) with the News 2009
development set. Both systems use glue-rules,
which allow them to concatenate partial transla-
tions without performing any reordering.
5.2 Results
We measured the overall translation quality with
the help of 4-gram BLEU (Papineni et al, 2002),
which was computed on tokenized and lower-
cased data for both systems. The results of our
evaluation are reported in Table 1. For com-
parison, we also report the results obtained by
a system that utilizes parses only on the source
side (Moses tree-to-string) with its standard fea-
tures.
We can observe from Table 1 that our `MBOT-
based system outperforms the baseline. We ob-
tain a BLEU score of 13.06, which is a gain of
0.46 BLEU points over the baseline. This im-
provement is statistically significant at confidence
p < 0.05, which we computed using the pairwise
bootstrap resampling technique of Koehn (2004).
Our system is also better than the Moses tree-to-
string system. However this improvement (0.34)
is not statistically significant. In the next section,
we confirm the result of the automatic evaluation
through a manual examination of some transla-
tions generated by our system and the baseline.
In Table 2, we report the number of `MBOT
rules used by our system when decoding the test
set. By lex we denote rules containing only lexical
817
lex non-term total
contiguous 23,175 18,355 41,530
discontiguous 315 2,516 2,831
Table 2: Number of rules used in decoding test
(lex: only lexical items; non-term: at least one
nonterminal).
2-dis 3-dis 4-dis
2,480 323 28
Table 3: Number of k-discontiguous rules.
items. The label non-term stands for rules contain-
ing at least one leaf nonterminal. The results show
that approx. 6% of all rules used by our `MBOT-
system have discontiguous target sides. Further-
more, the reported numbers show that the system
also uses rules in which lexical items are com-
bined with nonterminals. Finally, Table 3 presents
the number of rules with k target side components
used during decoding.
5.3 Linguistic Analysis
In this section we present linguistic evidence sup-
porting the fact that the `MBOT-based system sig-
nificantly outperforms the baseline. All exam-
ples are taken from the translation of the test set
used for automatic evaluation. We show that when
our system generates better translations, this is di-
rectly related to the use of `MBOT rules.
Figures 8 and 9 show the ability of our system to
correctly reorder multiple segments in the source
sentence where the baseline translates those seg-
ments sequentially. An analysis of the generated
derivations shows that our system produces the
correct translation by taking advantage of rules
with discontiguous units on target language side.
The rules used in the presented derivations are dis-
played in Figures 10 and 11. In the first example
(Figure 8), we begin by translating ?((smuggle)VB
(eight projectiles)NP (into the kingdom)PP)VP? into
the discontiguous sequence composed of (i) ?(acht
geschosse)NP? ; (ii) ?(in das ko?nigreich)PP? and
(iii) ?(schmuggeln)VP?. In a second step we as-
semble all sequences in a rule with contiguous tar-
get language side and, at the same time, insert the
word ?(zu)PTKZU? between ?(in das ko?nigreich)PP?
and ?(schmuggeln)VP?.
The second example (Figure 9) illustrates a
more complex reordering. First, we trans-
VP
VB NP PP
?
( NP
NP
, PP
PP
, VVINF
VVINF
)
S
TO VP
?
( VP
NP PP PTKZU VVINF
)
Figure 10: Used `MBOT rules for verbal reorder-
ing
VP
ADV commented on NP
?
( NP
NP
, ADV
ADV
, VPP
kommentiert
)
VP
VBZ VP
?
( NP
NP
, VAFIN
VAFIN
, ADV
ADV
, VPP
VPP
)
TOP
NP VP
?
( TOP
NP VAFIN NP ADV VVPP
)
Figure 11: Used `MBOT rules for verbal reorder-
ing
late ?((again)ADV commented on (the problem
of global warming)NP)VP? into the discontigu-
ous sequence composed of (i) ?(das problem
der globalen erwa?rmung)NP?; (ii) ?(wieder)ADV?
and (iii) ?(kommentiert)VPP?. In a second step,
we translate the auxiliary ?(has)VBZ? by in-
serting ?(hat)VAFIN? into the sequence. We
thus obtain, for the input segment ?((has)VBZ
(again)ADV commented on (the problem of global
warming)NP)VP?, the sequence (i) ?(das problem
der globalen erwa?rmung)NP?; (ii) ?(hat)VAFIN?;
(iii) ?(wieder)ADV?; (iv) ?(kommentiert)VVPP?. In
a last step, the constituent ?(president va?clav
klaus)NP? is inserted between the discontiguous
units ?(hat)VAFIN? and ?(wieder)ADV? to form the
contiguous sequence ?((das problem der glob-
alen erwa?rmung)NP (hat)VAFIN (pra?sident va?clav
klaus)NP (wieder)ADV (kommentiert)VVPP)TOP?.
Figures 12 and 13 show examples where our
system generates complex words in the target
language out of a simple source language word.
Again, an analysis of the generated derivation
shows that `MBOT takes advantage of rules hav-
ing several target side components. Examples of
such rules are given in Figure 14. Through its
ability to use these discontiguous rules, our sys-
tem correctly translates into reflexive or particle
verbs such as ?konzentriert sich? (for the English
?focuses?) or ?besteht darauf ? (for the English
?insist?). Another phenomenon well handled by
our system are relative pronouns. Pronouns such
as ?that? or ?whose? are systematically translated
818
. . . geplant hatten 8 geschosse in das ko?nigreich zu schmuggeln
. . . had planned to smuggle 8 projectiles into the kingdom
. . . vorhatten zu schmuggeln 8 geschosse in das ko?nigreich
Figure 8: Verbal Reordering (top: our system, bottom: baseline)
das problem der globalen erwa?rmung hat pra?sident va?clav klaus wieder kommentiert
president va?clav klaus has again commented on the problem of global warming
pra?sident va?clav klaus hat wieder kommentiert das problem der globalen erwa?rmung
Figure 9: Verbal Reordering (top: our system, bottom: baseline)
. . . die serbische delegation bestand darauf , dass jede entscheidung . . .
. . . the serbian delegation insisted that every decision . . .
. . . die serbische delegation bestand , jede entscheidung . . .
Figure 12: Relative Clause (top: our system, bot-
tom: baseline)
. . . die roadmap von bali , konzentriert sich auf die bemu?hungen . . .
. . . the bali roadmap that focuses on efforts . . .
. . . die bali roadmap , konzentriert auf bemu?hungen . . .
Figure 13: Reflexive Pronoun (top: our system,
bottom: baseline)
into both both, ?,? and ?dass? or ?,? and ?deren?
(Figure 12).
6 Conclusion and Future Work
We demonstrated that our `MBOT-based machine
translation system beats a standard tree-to-tree
system (Moses tree-to-tree) on the WMT 2009
translation task English ? German. To achieve
this we implemented the formal model as de-
scribed in Section 2 inside the Moses machine
translation toolkit. Several modifications were
necessary to obtain a working system. We publicly
release all our developed software and our com-
plete tool-chain to allow independent experiments
and evaluation. This includes our `MBOT decoder
IN
that
?
( $,
,
, KOUS
dass
) VBZ
focuses
?
( VVFIN
konzentriert
, PRF
sich
)
Figure 14: `MBOT rules generating a relative
clause/reflexive pronoun
presented in Section 4 and a separate C++ module
that we use for rule extraction (see Section 3).
Besides the automatic evaluation, we also per-
formed a small manual analysis of obtained trans-
lations and show-cased some examples (see Sec-
tion 5.3). We argue that our `MBOT approach can
adequately handle discontiguous phrases, which
occur frequently in German. Other languages that
exhibit such phenomena include Czech, Dutch,
Russian, and Polish. Thus, we hope that our sys-
tem can also successfully be applied for other lan-
guage pairs, which we plan to pursue as well.
In other future work, we want to investigate
full backwards application of `MBOT rules, which
would be more suitable for the converse transla-
tion direction German? English. The current in-
dependent LM scoring of components has some
negative side-effects that we plan to circumvent
with the use of lazy LM scoring.
Acknowledgement
The authors thank Alexander Fraser for his ongo-
ing support and advice. All authors were finan-
cially supported by the German Research Founda-
tion (DFG) grant MA 4959 / 1-1.
819
References
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc. 4th Workshop on Statistical Machine Trans-
lation, pages 1?28.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. 43rd ACL, pages 173?180.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computat. Linguist., 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. 48th ACL, pages 1443?
1452.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. 41st
ACL, pages 205?208.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
44th ACL, pages 961?968.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proc. 6th Int. Workshop Spoken Language Transla-
tion, pages 152?159.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. 7th Conf. Association
for Machine Translation of the Americas, pages 66?
73.
Philip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 127?133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT Speech Translation Evaluation.
In Proc. 2nd Int. Workshop Spoken Language Trans-
lation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. 10th Ma-
chine Translation Summit, pages 79?86.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. 2nd ACL Workshop on Syntax and
Structure in Statistical Translation, pages 87?95.
Eric Lilin. 1978. Une ge?ne?ralisation des transducteurs
d?e?tats finis d?arbres: les S-transducteurs. The`se
3e`me cycle, Universite? de Lille.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. 44th ACL, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
47th ACL, pages 558?566.
Andreas Maletti. 2010. Why synchronous tree sub-
stitution grammars? In Proc. HLT-NAACL, pages
876?884.
Andreas Maletti. 2011. How to train your multi
bottom-up tree transducer. In Proc. 49th ACL, pages
825?834.
Andreas Maletti. 2012. Every sensible extended top-
down tree transducer is a multi bottom-up tree trans-
ducer. In Proc. HLT-NAACL, pages 263?273.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computat. Linguist., 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. 41st ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. 40th
ACL, pages 311?318.
Jean-Claude Raoult. 1997. Rational tree relations.
Bull. Belg. Math. Soc. Simon Stevin, 4(1):149?176.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proc. 20th COLING, pages 162?168.
820
Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-
contiguous tree sequence alignment-based model for
statistical machine translation. In Proc. 47th ACL,
pages 914?922.
Web-as-Corpus Consortium. 2008. SDeWaC ? a 0.88
billion word corpus for german. Website: http:
//wacky.sslmit.unibo.it/doku.php.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computat. Linguist., 23(3):377?403.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008a. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. 46th ACL, pages 559?567.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and
Sheng Li. 2008b. Grammar comparison study
for translational equivalence modeling and statis-
tical machine translation. In Proc. 22nd Inter-
national Conference on Computational Linguistics,
pages 1097?1104.
821
Proc. EACL 2012 Workshop on Applications of Tree Automata Techniques in Natural Language Processing, pages 1?10,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Preservation of Recognizability for
Weighted Linear Extended Top-Down Tree Transducers?
Nina Seemann and Daniel Quernheim and Fabienne Braune and Andreas Maletti
University of Stuttgart, Institute for Natural Language Processing
{seemanna,daniel,braunefe,maletti}@ims.uni-stuttgart.de
Abstract
An open question in [FU?LO?P, MALETTI,
VOGLER: Weighted extended tree trans-
ducers. Fundamenta Informaticae 111(2),
2011] asks whether weighted linear ex-
tended tree transducers preserve recogniz-
ability in countably complete commuta-
tive semirings. In this contribution, the
question is answered positively, which is
achieved with a construction that utilizes
inside weights. Due to the completeness
of the semiring, the inside weights always
exist, but the construction is only effective
if they can be effectively determined. It is
demonstrated how to achieve this in a num-
ber of important cases.
1 Introduction
Syntax-based statistical machine translation
(Knight, 2007) created renewed interest in tree
automata and tree transducer theory (Fu?lo?p
and Vogler, 2009). In particular, it sparked
research on extended top-down tree transduc-
ers (Graehl et al, 2009), which are top-down
tree transducers (Rounds, 1970; Thatcher, 1970)
in which the left-hand sides can contain several
(or no) input symbols. A recent contribution
by Fu?lo?p et al (2011) investigates the theoretical
properties of weighted extended tree transduc-
ers over countably complete and commutative
semirings (Hebisch and Weinert, 1998; Golan,
1999). Such semirings permit sums of countably
many summands, which still obey the usual
associativity, commutativity, and distributivity
laws. We will use the same class of semirings.
? All authors were financially supported by the EMMY
NOETHER project MA / 4959 / 1-1 of the German Research
Foundation (DFG).
Input? Parser ? TM ? LM ? Output
Figure 1: Syntax-based machine translation pipeline.
Extended top-down tree transducers are used as
translation models (TM) in syntax-based machine
translation. In the standard pipeline (see Figure 1;
LM is short for language model) the translation
model is applied to the parses of the input sen-
tence, which can be represented as a recogniz-
able weighted forest (Fu?lo?p and Vogler, 2009).
In practice, only the best or the n-best parses are
used, but in principle, we can use the recogniz-
able weighted forest of all parses. In either case,
the translation model transforms the input trees
into a weighted forest of translated output trees.
A class of transducers preserves recognizability
if for every transducer of the class and each rec-
ognizable weighted forest, this weighted forest
of translated output trees is again recognizable.
Fu?lo?p et al (2011) investigates which extended
top-down tree transducers preserve recognizabil-
ity under forward (i.e., the setting previously de-
scribed) and backward application (i.e., the set-
ting, in which we start with the output trees and
apply the inverse of the translation model), but the
question remained open for forward application
of weighted linear extended top-down tree trans-
ducers [see Table 1 for an overview of the exist-
ing results for forward application due to Engel-
friet (1975) in the unweighted case and Fu?lo?p et
al. (2010) and Fu?lo?p et al (2011) for the weighted
case]. In conclusion, Fu?lo?p et al (2011) ask: ?Are
there a commutative semiring S that is count-
ably complete wrt.
?
, a linear wxttM [weighted
extended top-down tree transducer with regular
look-ahead; see Section 4], and a recognizable
1
model preserves regularity
unweighted
ln-XTOP 3
l-XTOP 3
l-XTOPR 3
XTOP 7
weighted
ln-XTOP 3
l-XTOP 3
l-XTOPR 3
XTOP 7
Table 1: Overview of the known results due to Engel-
friet (1975) and Fu?lo?p et al (2011) and our results in
boxes.
weighted tree language ? such that M(?) [for-
ward application] is not recognizable? Or even
harder, are there S and M with the same prop-
erties such that M(1?) [1? is the weighted forest
in which each tree has weight 1] is not recogniz-
able??
In this contribution, we thus investigate preser-
vation of recognizability (under forward applica-
tion) for linear extended top-down tree transduc-
ers with regular look-ahead (Engelfriet, 1977),
which are equivalent to linear weighted extended
tree transducers by Fu?lo?p et al (2011). We show
that they always preserve recognizability, thus
confirming the implicit hypothesis of Fu?lo?p et al
(2011). The essential tool for our construction is
the inside weight (Lari and Young, 1990; Graehl
et al, 2008) of the states of the weighted tree
grammar (Alexandrakis and Bozapalidis, 1987)
representing the parses. The inside weight of a
state q is the sum of all weights of trees accepted
in this state. In our main construction (see Sec-
tion 5) we first compose the input weighted tree
grammar with the transducer (input restriction).
This is particularly simple since we just abuse
the look-ahead of the initial rules. In a second
step, we normalize the obtained transducer, which
yields the standard product construction typically
used for input restriction. Finally, we project to
the output by basically eliminating the left-hand
sides. In this step, the inside weights of states
belonging to deleted subtrees are multiplied to
the production weight. Due to the completeness
of the semiring, the inside weights always ex-
ist, but the infinite sums have to be computed ef-
fectively for the final step of the construction to
be effective. This problem is addressed in Sec-
tion 6, where we show several methods to effec-
tively compute or approximate the inside weights
for all states of a weighted tree grammar.
2 Notation
Our weights will be taken from a commuta-
tive semiring (A,+, ?, 0, 1), which is an algebraic
structure of two commutative monoids (A,+, 0)
and (A, ?, 1) such that ? distributes over + and
0 ? a = 0 for all a ? A. An infinitary sum opera-
tion
?
is a family (
?
I)I where I is a countable
index set and
?
I : A
I ? A. Given f : I ? A,
we write
?
i?I f(i) instead of
?
I f . The semi-
ring together with the infinitary sum operation
?
is countably complete (Eilenberg, 1974; Hebisch
and Weinert, 1998; Golan, 1999; Karner, 2004) if
for all countable sets I and ai ? A with i ? I
?
?
i?I ai = am + an if I = {m,n},
?
?
i?I ai =
?
j?J
(?
i?Ij
ai
)
if I =
?
j?J Ij
for countable sets J and Ij with j ? J such
that Ij ? Ij? = ? for all different j, j? ? J ,
and
? a ?
(?
i?I ai
)
=
?
i?I(a ? ai) for all a ? A.
For such a semiring, we let a? =
?
i?N a
i for
every a ? A. In the following, we assume that
(A,+, ?, 0, 1) is a commutative semiring that is
countably complete with respect to
?
.
Our trees have node labels taken from an al-
phabet ? and leaves might also be labeled by el-
ements of a set V . Given a set T , we write ?(T )
for the set
{?(t1, . . . , tk) | k ? N, ? ? ?, t1, . . . , tk ? T} .
The set T?(V ) of ?-trees with V -leaves is defined
as the smallest set T such that V ? ?(T ) ? T .
We write T? for T?(?). For each tree t ? T?(V )
we identify nodes by positions. The root of t has
position ? and the position iw with i ? N and
w ? N? addresses the position w in the i-th di-
rect subtree at the root. The set of all positions
in t is pos(t). We write t(w) for the label (taken
from ? ? V ) of t at position w ? pos(t). Sim-
ilarly, we use t|w to address the subtree of t that
is rooted in position w, and t[u]w to represent the
tree that is obtained from replacing the subtree t|w
at w by u ? T?(V ). For a given set L ? ? ? V
of labels, we let
posL(t) = {w ? pos(t) | t(w) ? L}
2
be the set of all positions whose label belongs
to L. We also write posl(t) instead of pos{l}(t).
We often use the set X = {x1, x2, . . . } of vari-
ables and its finite subsets Xk = {x1, . . . , xk}
for every k ? N to label leaves. Let V
be a set potentially containing some variables
of X . The tree t ? T?(V ) is linear if
|posx(t)| ? 1 for every x ? X . Moreover,
var(t) = {x ? X | posx(t) 6= ?} collects all
variables that occur in t. Given a finite set Q and
T ? T?(V ), we let
Q[T ] = {q(t) | q ? Q, t ? T} .
We will treat elements ofQ[T ] (in which elements
ofQ are always used as unary symbols) as special
trees of T??Q(V ). A substitution ? is a mapping
? : X ? T?(V ). When applied to t ? T?(V ),
it returns the tree t?, which is obtained from t
by replacing all occurrences of x ? X (in par-
allel) by ?(x). This can be defined recursively
by x? = ?(x) for all x ? X , v? = v for all
v ? V \X , and ?(t1, . . . , tk)? = ?(t1?, . . . , tk?)
for all ? ? ? and t1, . . . , tk ? T?(V ).
3 Weighted Tree Grammars
In this section, we will recall weighted tree
grammars (Alexandrakis and Bozapalidis, 1987)
[see (Fu?lo?p and Vogler, 2009) for a modern treat-
ment and a complete historical account]. In gen-
eral, weighted tree grammars (WTGs) offer an ef-
ficient representation of weighted forests, which
are sets of trees such that each individual tree
is equipped with a weight. The representation
is even more efficient than packed forests (Mi et
al., 2008) and moreover can represent an infinite
number of weighted trees. To avoid confusion
between the nonterminals of a parser, which pro-
duces the forests considered here, and our WTGs,
we will refer to the nonterminals of our WTG as
states.
Definition 1. A weighted tree grammar (WTG) is
a system (Q,?, q0, P ) where
? Q is a finite set of states (nonterminals),
? ? is the alphabet of symbols,
? q0 ? Q is the starting state, and
? P is a finite set of productions q
a
? t, where
q ? Q, a ? A, and t ? T?(Q).
Example 2. We illustrate our notation on the
WTG Gex = (Q,?, qs, P ) where
? Q = {qs, qnp, qprp, qn, qadj},
? ? contains ?S?, ?NP?, ?VP?, ?PP?, ?DT?,
?NN?, ?N?, ?VBD?, ?PRP?, ?ADJ?, ?man?,
?hill?, ?telescope?, ?laughs?, ?the?, ?on?,
?with?, ?old?, and ?young?, and
? P contains the productions
qs
1.0
? S(qnp,VP(VBD(laughs))) (?1)
qnp
0.4
? NP(qnp,PP(qprp, qnp))
qnp
0.6
? NP(DT(the), qn) (?2)
qprp
0.5
? PRP(on)
qprp
0.5
? PRP(with)
qn
0.3
? N(qadj , qn)
qn
0.3
? NN(man) (?3)
qn
0.2
? NN(hill)
qn
0.2
? NN(telescope)
qadj
0.5
? ADJ(old)
qadj
0.5
? ADJ(young)
It produces a weighted forest representing sen-
tences about young and old men with telescopes
on hills.
In the following, let G = (Q,?, q0, P ) be a
WTG. For every production ? = q
a
? t in P , we
let wtG(?) = a. The semantics of G is defined
with the help of derivations. Let ? ? T?(Q) be
a sentential form, and let w ? posQ(?) be such
that w is the lexicographically smallest Q-labeled
position in ?. Then ? ??G ?[t]w if ?(w) = q. For
a sequence ?1, . . . , ?n ? P of productions, we
let wtG(?1 ? ? ? ?n) =
?n
i=1 wtG(?i). For every
q ? Q and t ? T?(Q), we let
wtG(q, t) =
?
?1,...,?n?P
q?
?1
G ????
?n
G t
wtG(?1 ? ? ? ?n) .
The WTG G computes the weighted forest
LG : T? ? A such that LG(t) = wtG(q0, t) for
every t ? T?. Two WTGs are equivalent if they
compute the same weighted forest. Since produc-
tions of weight 0 are useless, we often omit them.
Example 3. For the WTG Gex of Example 2 we
display a derivation with weight 0.18 for the sen-
tence ?the man laughs? in Figure 2.
The notion of inside weights (Lari and Young,
1990) is well-established, and Maletti and Satta
3
qs ??1G
S
qnp VP
VBD
laughs
??2G
S
NP
DT
the
qn
VP
VBD
laughs
??3G
S
NP
DT
the
NN
man
VP
VBD
laughs
Figure 2: Derivation with weight 1.0 ? 0.6 ? 0.3.
(2009) consider them for WTGs. Let us recall the
definition.
Definition 4. The inside weight of state q ? Q is
inG(q) =
?
t?T?
wtG(q, t) .
In Section 6 we demonstrate how to compute
inside weights. Finally, let us introduce WTGs in
normal form. The WTG G is in normal form if
t ? ?(Q) for all its productions q
a
? t in P . The
following theorem was proven by Alexandrakis
and Bozapalidis (1987) as Proposition 1.2.
Theorem 5. For every WTG there exists an
equivalent WTG in normal form.
Example 6. The WTG Gex of Example 2 is not
normalized. To illustrate the normalization step,
we show the normalization of the production ?2,
which is replaced by the following three produc-
tions:
qnp
0.6
? NP(qdt, qn) qdt
1.0
? DT(qt)
qt
1.0
? the .
4 Weighted linear extended tree
transducers
The model discussed in this contribution is an ex-
tension of the classical top-down tree transducer,
which was introduced by Rounds (1970) and
Thatcher (1970). Here we consider a weighted
and extended variant that additionally has regular
look-ahead. The weighted top-down tree trans-
ducer is discussed in (Fu?lo?p and Vogler, 2009),
and extended top-down tree transducers were
studied in (Arnold and Dauchet, 1982; Knight and
Graehl, 2005; Knight, 2007; Graehl et al, 2008;
Graehl et al, 2009). The combination (weighted
extended top-down tree transducer) was recently
investigated by Fu?lo?p et al (2011), who also con-
sidered (weighted) regular look-ahead, which was
first introduced by Engelfriet (1977) in the un-
weighted setting.
Definition 7. A linear extended top-down
tree transducer with full regular look-ahead
(l-XTOPRf ) is a system (S,?,?, s0, G,R) where
? S is a finite set of states,
? ? and ? are alphabets of input and output
symbols, respectively,
? s0 ? S is an initial state,
? G = (Q,?, q0, P ) is a WTG, and
? R is a finite set of weighted rules of the form
`
a
?? r where
? a ? A is the rule weight,
? ` ? S[T?(X)] is the linear left-hand
side,
? ? : var(`)? Q is the look-ahead, and
? r ? T?(S[var(`)]) is the linear right-
hand side.
In the following, let M = (S,?,?, s0, G,R)
be an l-XTOPRf . We assume that the WTG G
contains a state > such that wtG(>, t) = 1 for
every t ? T?. In essence, this state represents
the trivial look-ahead. If ?(x) = > for every
rule `
a
?? r ? R and x ? var(r) (respectively,
x ? var(`)), then M is an l-XTOPR (respectively,
l-XTOP). l-XTOPR and l-XTOP coincide exactly
with the models of Fu?lo?p et al (2011), and in the
latter model we drop the look-ahead component ?
and the WTG G completely.
Example 8. The rules of our running example
l-XTOP Mex (over the input and output alpha-
bet ?, which is also used by the WTG Gex of Ex-
ample 2) are displayed in Figure 3.
Next, we present the semantics. Without loss
of generality, we assume that we can distin-
guish states from input and output symbols (i.e.,
S ? (? ? ?) = ?). A sentential form of M is a
tree of SF(M) = T?(Q[T?]). Let ? = `
a
?? r be
a rule of R. Moreover, let ?, ? ? SF(M) be sen-
tential forms and w ? N? be the lexicographically
smallest position in posQ(?). We write ?
b
?M,? ?
if there exists a substitution ? : X ? T? such that
? ? = ?[`?]w,
? ? = ?[r?]w, and
? b = a ?
?
x?var(`) wtG(?(x), ?(x)).
4
s0
S
NP
x1 x2
VP
x3
? 0.6
S
NP
s1
x1
s2
x2
VP
s3
x3
?
?
? 0.4
S
s1
x1
VP
s3
x3
s2
N
ADJ
x1
x2
? 0.7
N
ADJ
s5
x1
s2
x2
?
?
? 0.3
s2
x2
s1
NP
x1 x2
? 0.5
NP
s1
x1
s2
x2
?
?
? 0.5
s1
x1
s1
DT
the
? 1.0
DT
the
s3
VBD
laughs
? 1.0
VBD
laughs
s2
PP
x1 x2
? 1.0
PP
s4
x1
s1
x2
s2
NN
man /
hill /
telescope
? 1.0
NN
man /
hill /
telescope
s4
PRP
on /
with
? 1.0
PRP
on /
with
Figure 3: Example rules of an l-XTOP. We collapsed rules with the same left-hand side as well as several lexical
items to save space.
s0
S
NP
NP
DT
the
NN
man
PP
PRP
on
NP
DT
the
NN
hill
VP
VBD
laughs
0.4?M
S
s1
NP
DT
the
NN
man
VP
s3
VBD
laughs
0.5?M
S
NP
s1
DT
the
s2
NN
man
VP
s3
VBD
laughs
??M
S
NP
DT
the
NN
man
VP
VBD
laughs
Figure 4: Derivation with weight 0.4 ? 0.5 ? 1.0 (rules omitted).
The tree transformation ?M computed byM is de-
fined by
?M (t, u) =
?
?1,...,?n?R
s0(t)
a1?M,?1 ???
an?M,?nu
a1 ? . . . ? an
for every t ? T? and u ? T?.
Example 9. A sequence of derivation steps of the
l-XTOP Mex is illustrated in Figure 4. The trans-
formation it computes is capable of deleting the
PP child of every NP-node with probability 0.4 as
well as deleting the ADJ child of every N-node
with probability 0.3.
A detailed exposition to unweighted l-XTOPR
is presented by Arnold and Dauchet (1982) and
Graehl et al (2009).
5 The construction
In this section, we present the main construction
of this contribution, in which we will construct a
WTG for the forward application of another WTG
via an l-XTOPR. Let us first introduce the main
notions. Let L : T? ? A be a weighted forest
and ? : T??T? ? A be a weighted tree transfor-
mation. Then the forward application of L via ?
yields the weighted forest ?(L) : T? ? A such
that (?(L))(u) =
?
t?T?
L(t) ? ?(t, u) for ev-
ery u ? T?. In other words, to compute the
weight of u in ?(L), we consider all input trees t
and multiply their weight in L with their trans-
lation weight to u. The sum of all those prod-
ucts yields the weight for u in ?(L). In the par-
ticular setting considered in this contribution, the
weighted forest L is computed by a WTG and the
weighted tree transformation ? is computed by an
l-XTOPR. The question is whether the resulting
weighted forest ?(L) can be computed by a WTG.
Our approach to answer this question con-
sists of three steps: (i) composition, (ii) nor-
malization, and (iii) range projection, which
we address in separate sections. Our input is
5
qs ?
S
qnp qvp
?
S
NP
qnp qpp
qvp ?2
S
NP
qnp qpp
VP
VBD
qv
qs ?
S
qnp qvp
?
S
NP
qdt qn
qvp ?2
S
NP
qdt qn
VP
VBD
qv
Figure 5: Two derivations (without production and
grammar decoration) with weight 0.4 [top] and
0.6 [bottom] of the normalized version of the
WTG Gex (see Example 10).
the WTG G? = (Q?,?, q?0, P
?), which com-
putes the weighted forest L = LG? , and
the l-XTOPR M = (S,?,?, s0, G,R) with
G = (Q,?, q0, P ), which computes the weighted
tree transformation ? = ?M . Without loss of gen-
erality, we suppose thatG andG? contain a special
state > such that wtG(>, t) = wtG?(>, t) = 1
for all t ? T?. Moreover, we assume that the
WTG G? is in normal form. Finally, we assume
that s0 is separated, which means that the initial
state of M does not occur in any right-hand side.
Our example l-XTOP Mex has this property. All
these restrictions can be assumed without loss of
generality. Finally, for every state s ? S, we let
Rs = {`
a
?? r ? R | `(?) = s} .
5.1 Composition
We combine the WTG G? and the l-XTOPR M
into a single l-XTOPRf M
? that computes
?M ?(t, u) = LG?(t) ? ?M (t, u) = L(t) ? ?(t, u)
for every t ? T? and u ? T?. To this end, we
construct
M ? = (S,?,?, s0, G?G
?, (R \Rs0) ?R
?)
such that G ? G? is the classical product WTG
[see Proposition 5.1 of (Berstel and Reutenauer,
1982)] and for every rule `
a
?? r in Rs0 and
? : var(`)? Q?, the rule
`
a?wtG? (q
?
0,`?)??????????? r
is in R?, where ??(x) = ??(x), ?(x)? for every
x ? var(`).
Example 10. Let us illustrate the construction on
the WTG Gex of Example 2 and the l-XTOP Mex
of Example 8. According to our assumptions,
Gex should first be normalized (see Theorem 5).
We have two rules in Rs0 and they have the same
left-hand side `. It can be determined easily that
wtG?ex(qs, `?) 6= 0 only if
? ?(x1)?(x2)?(x3) = qnpqppqv or
? ?(x1)?(x2)?(x3) = qdtqnqv.
Figure 5 shows the two corresponding derivations
and their weights. Thus, the s0-rules are replaced
by the 4 rules displayed in Figure 6.
Theorem 11. For every t ? T? and u ? T?, we
have ?M ?(t, u) = L(t) ? ?(t, u).
Proof. We prove an intermediate property for
each derivation of M . Let
s0(t)
b1?M,?1 ? ? ?
bn?M,?n u
be a derivation of M . Let ?1 = `
a1?? r be the
first rule, which trivially must be in Rs0 . Then for
every ? : var(`)? Q?, there exists a derivation
s0(t)
c1?M ?,??1 ?2
b2?M ?,?2 ? ? ?
bn?M ?,?n u
in M ? such that
c1 = b1?wtG?(q
?
0, `?)?
?
x?var(`)
wtG?(?(x), ?
?(x)) ,
where ?? : var(`) ? T? is such that t = `??.
Since we sum over all such derivations and
?
? : var(`)?Q?
wtG?(q
?
0, `?) ?
?
x?var(`)
wtG?(?(x), ?
?(x))
= wtG?(q
?
0, t) = LG?(t)
by a straightforward extension of Lemma 4.1.8
of (Borchardt, 2005), we obtain that the deriva-
tions in M ? sum to LG?(t) ? b1 ? . . . ? bn as desired.
The main property follows trivially from the in-
termediate result.
5.2 Normalization
Currently, the weights of the input WTG are
only on the initial rules and in its look-ahead.
Next, we use essentially the same method as
in the previous section to remove the look-
ahead from all variables that are not deleted.
Let M ? = (S,?,?, s0, G ? G?, R) be the
l-XTOPRf constructed in the previous section and
6
s0
S
NP
x1 x2
VP
x3
??
0.6 ? c
S
NP
s1
x1
s2
x2
VP
s3
x3
?
?
?
0.4 ? c
S
s1
x1
VP
s3
x3
Figure 6: 4 new l-XTOPRf rules, where ? and c are
either (i) ?(x1)?(x2)?(x3) = qnpqppqv and c = 0.4
or (ii) ?(x1)?(x2)?(x3) = qdtqnqv and c = 0.6 (see
Example 10).
s0
S
NP
x1 x2
VP
x3
??
0.4 ? 0.4
S
?s1, qnp?
x1
VP
?s3, qv?
x3
?
?
?
0.4 ? 0.6
S
?s1, qdt?
x1
VP
?s3, qv?
x3
Figure 7: New l-XTOPR rules, where ?(x2) = qpp
[left] and ?(x2) = qn [right] (see Figure 6).
? = `
a
?? r ? R be a rule with ?(x) = ?>, q??
for some q? ? Q? \ {>} and x ? var(r). Note
that ?(x) = ?>, q?? for some q? ? Q? for all
x ? var(r) since M is an l-XTOPR. Then we
construct the l-XTOPRf M
??
(S ? S ?Q?,?,?, s0, G?G
?, (R \ {?}) ?R?)
such that R? contains the rule `
a
??? r?, where
??(x?) =
{
?>,>? if x = x?
?(x?) otherwise
for all x? ? var(`) and r? is obtained from r by re-
placing the subtree s(x) with s ? S by ?s, q??(x).
Additionally, for every rule `??
a??
???? r?? in Rs and
? : var(`??)? Q?, the rule
`??
a???wtG? (q
?,`???)
?????????????? r
??
is in R?, where ????(x) = ????(x), ?(x)? for ev-
ery x ? var(`). This procedure is iterated until
we obtain an l-XTOPR M ??. Clearly, the iteration
must terminate since we do not change the rule
shape, which yields that the size of the potential
rule set is bounded.
Theorem 12. The l-XTOPR M ?? and the
l-XTOPRf M
? are equivalent.
?s2, qn?
N
ADJ
x1
x2
??
0.32 ? 0.5
?s2, qn?
x2
?s1, qnp?
NP
x1 x2
???|???
0.5 ? 0.4
?s1, qnp?
x1
?
?
?
0.5 ? 0.6
?s1, qdt?
x1
Figure 8: New l-XTOPR rules, where ?(x1) is either
qold or qyoung , ??(x2) = qpp, and ???(x2) = qn.
Proof. It can be proved that the l-XTOPRf con-
structed after each iteration is equivalent to its
input l-XTOPRf in the same fashion as in Theo-
rem 11 with the only difference that the rule re-
placement now occurs anywhere in the derivation
(not necessarily at the beginning) and potentially
several times. Consequently, the finally obtained
l-XTOPR M ?? is equivalent to M ?.
Example 13. Let us reconsider the l-XTOPRf con-
structed in the previous section and apply the nor-
malization step. The interesting rules (i.e., those
rules l
a
?? r where var(r) 6= var(l)) are dis-
played in Figures 7 and 8.
5.3 Range projection
We now have an l-XTOPR M ?? with rules R??
computing ?M ??(t, u) = L(t) ? ?(t, u). In the fi-
nal step, we simply disregard the input and project
to the output. Formally, we want to construct a
WTG G?? such that
LG??(u) =
?
t?T?
?M ??(t, u) =
?
t?T?
L(t) ? ?(t, u)
for every u ? T?. Let us suppose that G is the
WTG inside M ??. Recall that the inside weight of
state q ? Q is
inG(q) =
?
t?T?
wtG(q, t) .
We construct the WTG
G?? = (S ? S ?Q?,?, s0, P
??)
such that `(?)
c
? r? is in P ?? for every rule
`
a
?? r ? R??, where
c = a ?
?
x?var(`)\var(r)
inG(?(x))
and r? is obtained from r by removing the vari-
ables of X . If the same production is constructed
from several rules, then we add the weights. Note
that the WTG G?? can be effectively computed if
inG(q) is computable for every state q.
7
qs qprp
qnp qn qadj
Figure 9: Dependency graph of the WTG Gex.
Theorem 14. For every u ? T?, we have
LG??(u) =
?
t?T?
L(t) ? ?(t, u) = (?(L))(u) .
Example 15. The WTG productions for the rules
of Figures 7 and 8 are
s0
0.4?0.4
? S(?s1, qnp?,VP(?s3, qv?))
s0
0.4?0.6
? S(?s1, qdt?,VP(?s3, qv?))
?s2, qn?
0.3?0.3
? ?s2, qn?
?s1, qnp?
0.5?0.4
? ?s1, qnp?
?s1, qnp?
0.5?0.6
? ?s1, qdt? .
Note that all inside weights are 1 in our exam-
ple. The first production uses the inside weight
of qpp, whereas the second production uses the in-
side weight of qn. Note that the third production
can be constructed twice.
6 Computation of inside weights
In this section, we address how to effectively com-
pute the inside weight for every state. If the WTG
G = (Q,?, q0, P ) permits only finitely many
derivations, then for every q ? Q, the inside
weight inG(q) can be computed according to Def-
inition 4 because wtG(q, t) = 0 for almost all
t ? T?. If P contains (useful) recursive rules,
then this approach does not work anymore. Our
WTG Gex of Example 2 has the following two re-
cursive rules:
qnp
0.4
? NP(qnp,PP(qprp, qnp)) (?4)
qn
0.3
? N(qadj , qn) . (?5)
The dependency graph of Gex, which is shown in
Figure 9, has cycles, which yields that Gex per-
mits infinitely many derivations. Due to the com-
pleteness of the semiring, even the infinite sum of
Definition 4 is well-defined, but we still have to
compute it. We will present two simple methods
to achieve this: (a) an analytic method and (b) an
approximation in the next sections.
6.1 Analytic computation
In simple cases we can compute the inside weight
using the stars a?, which we defined in Section 2.
Let us first list some interesting countably com-
plete semirings for NLP applications and their
corresponding stars.
? Probabilities: (R??0,+, ?, 0, 1) where R
?
?0
contains all nonnegative real numbers
and ?, which is bigger than every real
number. For every a ? R??0 we have
a? =
{
1
1?a if 0 ? a < 1
? otherwise
? VITERBI: ([0, 1],max, ?, 0, 1) where [0, 1] is
the (inclusive) interval of real numbers be-
tween 0 and 1. For every 0 ? a ? 1 we have
a? = 1.
? Tropical: (R??0,min,+,?, 0) where
a? = 0 for every a ? R??0.
? Tree unification: (2T?(X1),?,unionsq, ?, {x1})
where 2T?(X1) = {L | L ? T?(X1)} and
unionsq is unification (where different occurrences
of x1 can be replaced differently) extended
to sets as usual. For every L ? T?(Xk) we
have L? = {x1} ? (L unionsq L).
We can always try to develop a regular expres-
sion (Fu?lo?p and Vogler, 2009) for the weighted
forest recognized by a certain state, in which we
then can drop the actual trees and only compute
with the weights. This is particularly easy if our
WTG has only left- or right-recursive productions
because in this case we obtain classical regular
expressions (for strings). Let us consider produc-
tion ?5. It is right-recursive. On the string level,
we obtain the following unweighted regular ex-
pression for the string language generated by qn:
L(qadj)
?(man | hill | telescope)
where L(qadj) = {old, young} is the set of strings
generated by qadj . Correspondingly, we can de-
rive the inside weight by replacing the generated
string with the weights used to derive them. For
example, the production ?5, which generates the
state qadj , has weight 0.3. We obtain the expres-
sion
inG(qn) = (0.3 ? inG(qadj))
? ? (0.3 + 0.2 + 0.2) .
8
Example 16. If we calculate in the probability
semiring and inG(qadj) = 1, then
inG(qn) =
1
1? 0.3
? (0.3 + 0.2 + 0.2) = 1 ,
as expected (since our productions induce a prob-
ability distribution on all trees generated from
each state).
Example 17. If we calculate in the tropical semi-
ring, then we obtain
inG(qn) = min(0.3, 0.2, 0.2) = 0.2 .
It should be stressed that this method only
allows us to compute inG(q) in very simple
cases (e.g., WTG containing only left- or right-
recursive productions). The production ?4 has
a more complicated recursion, so this simple
method cannot be used for our full example WTG.
However, for extremal semirings the inside
weight always coincides with a particular deriva-
tion. Let us also recall this result. The semiring is
extremal if a+ a? ? {a, a?} for all a, a? ? A. The
VITERBI and the tropical semiring are extremal.
Recall that
inG(q) =
?
t?T?
wtG(q, t)
=
?
t?T?
?
?1,...,?n?P
q?
?1
G ????
?n
G t
wtG(?1 ? ? ? ?n) ,
which yields that inG(q) coincides with the
derivation weight wtG(?1 ? ? ? ?n) of some deriva-
tion q ??1G ? ? ? ?
?n
G t for some t ? T?. In
the VITERBI semiring this is the highest scor-
ing derivation and in the tropical semiring it is
the lowest scoring derivation (mind that in the
VITERBI semiring the production weights are
multiplied in a derivation, whereas they are added
in the tropical semiring). There are efficient algo-
rithms (Viterbi, 1967) that compute those deriva-
tions and their weights.
6.2 Numerical Approximation
Next, we show how to obtain a numerical ap-
proximation of the inside weights (up to any
desired precision) in the probability semiring,
which is the most important of all semirings
discussed here. A similar approach was used
by Stolcke (1995) for context-free grammars. To
keep the presentation simple, let us suppose that
G = (Q,?, q0, P ) is in normal form (see The-
orem 5). The method works just as well in the
general case.
We first observe an important property of the
inside weights. For every state q ? Q
inG(q) =
?
q
a
??(q1,...,qn)?P
a ? inG(q1) ? . . . ? inG(qn) ,
which can trivially be understood as a system of
equations (where each inG(q) with q ? Q is a
variable). Since there is one such equation for
each variable inG(q) with q ? Q, we have a
system of |Q| non-linear polynomial equations in
|Q| variables.
Several methods to solve non-linear systems of
equations are known in the numerical calculus lit-
erature. For example, the NEWTON-RAPHSON
method allows us to iteratively compute the roots
of any differentiable real-valued function, which
can be used to solve our system of equations be-
cause we can compute the JACOBI matrix for our
system of equations easily. Given a good starting
point, the NEWTON-RAPHSON method assures
quadratic convergence to a root. A good start-
ing point can be obtained, for example, by bisec-
tion (Corliss, 1977). Another popular root-finding
approximation is described by Brent (1973).
Example 18. For the WTG of Example 2 we ob-
tain the following system of equations:
inG(qs) = 1.0 ? inG(qnp)
inG(qnp) = 0.4 ? inG(qnp) ? inG(qprp) ? inG(qnp)
+ 0.6 ? inG(qn)
inG(qn) = 0.3 ? inG(qadj) ? inG(qn)
+ 0.3 + 0.2 + 0.2
inG(qadj) = 0.5 + 0.5
inG(qprp) = 0.5 + 0.5 .
Together with inG(qn) = 1, which we already
calculated in Example 16, the only interesting
value is
inG(qs) = inG(qnp) = 0.4 ? inG(qnp)
2 + 0.6 ,
which yields the roots inG(qnp) = 1 and
inG(qnp) = 1.5. The former is the desired solu-
tion. As before, this is the expected solution.
9
References
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene?s theorem.
Inf. Process. Lett., 24(1):1?4.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Jean Berstel and Christophe Reutenauer. 1982. Rec-
ognizable formal power series on trees. Theoret.
Comput. Sci., 18(2):115?148.
Bjo?rn Borchardt. 2005. The Theory of Recognizable
Tree Series. Ph.D. thesis, Technische Universita?t
Dresden.
Richard P. Brent. 1973. Algorithms for Minimization
without Derivatives. Series in Automatic Computa-
tion. Prentice Hall, Englewood Cliffs, NJ, USA.
George Corliss. 1977. Which root does the bisection
algorithm find? SIAM Review, 19(2):325?327.
Samuel Eilenberg. 1974. Automata, Languages, and
Machines ? Volume A, volume 59 of Pure and Ap-
plied Math. Academic Press.
Joost Engelfriet. 1975. Bottom-up and top-down tree
transformations ? a comparison. Math. Systems
Theory, 9(3):198?231.
Joost Engelfriet. 1977. Top-down tree transducers
with regular look-ahead. Math. Systems Theory,
10(1):289?303.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Hand-
book of Weighted Automata, EATCS Monographs
on Theoret. Comput. Sci., chapter 9, pages 313?
403. Springer.
Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.
2010. Preservation of recognizability for syn-
chronous tree substitution grammars. In Proc. 1st
Workshop Applications of Tree Automata in Natu-
ral Language Processing, pages 1?9. Association
for Computational Linguistics.
Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.
2011. Weighted extended tree transducers. Fun-
dam. Inform., 111(2):163?202.
Jonathan S. Golan. 1999. Semirings and their Appli-
cations. Kluwer Academic, Dordrecht.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391?427.
Jonathan Graehl, Mark Hopkins, Kevin Knight, and
Andreas Maletti. 2009. The power of extended
top-down tree transducers. SIAM J. Comput.,
39(2):410?430.
Udo Hebisch and Hanns J. Weinert. 1998. Semirings
? Algebraic Theory and Applications in Computer
Science. World Scientific.
Georg Karner. 2004. Continuous monoids and semi-
rings. Theoret. Comput. Sci., 318(3):355?372.
Kevin Knight and Jonathan Graehl. 2005. An over-
view of probabilistic tree transducers for natural
language processing. In Proc. 6th Int. Conf. Com-
putational Linguistics and Intelligent Text Process-
ing, volume 3406 of LNCS, pages 1?24. Springer.
Kevin Knight. 2007. Capturing practical natural
language transformations. Machine Translation,
21(2):121?133.
Karim Lari and Steve J. Young. 1990. The esti-
mation of stochastic context-free grammars using
the inside-outside algorithm. Computer Speech and
Language, 4(1):35?56.
Andreas Maletti and Giorgio Satta. 2009. Parsing al-
gorithms based on tree automata. In Proc. 11th Int.
Workshop Parsing Technologies, pages 1?12. Asso-
ciation for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. 46th Ann. Meeting of
the ACL, pages 192?199. Association for Computa-
tional Linguistics.
William C. Rounds. 1970. Mappings and grammars
on trees. Math. Systems Theory, 4(3):257?287.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Comput. Linguist., 21(2):165?201.
James W. Thatcher. 1970. Generalized2 sequential
machine maps. J. Comput. System Sci., 4(4):339?
367.
Andrew J. Viterbi. 1967. Error bounds for convo-
lutional codes and an asymptotically optimum de-
coding algorithm. IEEE Trans. Inform. Theory,
13(2):260?269.
10
