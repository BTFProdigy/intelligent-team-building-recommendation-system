Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 947?954,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Automatic Set Expansion for List Question Answering
Richard C. Wang, Nico Schlaefer, William W. Cohen, and Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh PA 15213
{rcwang,nico,wcohen,ehn}@cs.cmu.edu
Abstract
This paper explores the use of set expan-
sion (SE) to improve question answering (QA)
when the expected answer is a list of entities
belonging to a certain class. Given a small
set of seeds, SE algorithms mine textual re-
sources to produce an extended list including
additional members of the class represented
by the seeds. We explore the hypothesis that
a noise-resistant SE algorithm can be used to
extend candidate answers produced by a QA
system and generate a new list of answers that
is better than the original list produced by the
QA system. We further introduce a hybrid ap-
proach which combines the original answers
from the QA system with the output from the
SE algorithm. Experimental results for several
state-of-the-art QA systems show that the hy-
brid system performs better than the QA sys-
tems alone when tested on list question data
from past TREC evaluations.
1 Introduction
Question answering (QA) systems are designed to
retrieve precise answers to questions posed in nat-
ural language. A list question expects a list as its
answer, e.g. Name the coffee-producing countries in
South America. The ability to answer list questions
has been tested as part of the yearly TREC QA eval-
uation (Dang et al, 2006; Dang et al, 2007). This
paper focuses on the use of set expansion to improve
list question answering. A set expansion (SE) algo-
rithm receives as input a few members of a class or
set, and mines various textual resources (e.g. web
pages) to produce an extended list including addi-
tional members of the class or set that are not in the
input. A well-known online SE system is Google
Sets1. This system is publicly accessible, but since it
is a proprietary system that might be changed at any
time, its results cannot be replicated reliably. We ex-
plore the hypothesis that a SE algorithm, when care-
fully designed to handle noisy inputs, can be applied
to the output from a QA system to produce an overall
list of answers for a given question that is better than
the answers produced by the QA system itself. We
propose to exploit large, redundant sources of struc-
tured and/or semi-structured data and use linguistic
analysis to seed a shallow analysis of these sources.
This is a hard problem since the linguistic evidence
used as seeds is noisy. More precisely, we combine
the QA system Ephyra (Schlaefer et al, 2007) with
the SE system SEAL (Wang and Cohen, 2007) to
create a hybrid approach that performs better than
either system by itself when tested on data from the
TREC 13-15 evaluations. In addition, we apply our
SE algorithm to answers generated by the five QA
systems that performed the best on the list questions
in the TREC 15 evaluation and report improvements
in F1 scores for four of these systems.
Section 2 of the paper gives an overview of the
QA and SE systems used for our experiments. Sec-
tion 3 describes how the SE system was adapted to
deal with noisy seeds produced by QA systems, and
Section 4 presents the details of the experimental de-
sign. Experimental results are discussed in Section
5, and the paper concludes in Section 6 with a dis-
cussion of planned future work.
1http://labs.google.com/sets
947
2 System Overview
2.1 Ephyra Question Answering System
Ephyra (Schlaefer et al, 2006; Schlaefer et al,
2007) is a QA system that has been evaluated in
the TREC QA track (Dang et al, 2006; Dang et al,
2007). The system combines three answer extrac-
tion techniques for factoid and list questions: (1) an
answer type classification approach; (2) a syntactic
pattern learning and matching approach; and (3) a
semantic extractor that uses a semantic role label-
ing system. The answer type based extractor clas-
sifies questions by their answer types and extracts
candidates of the expected types. The Ephyra pat-
tern matching approach learns textual patterns that
relate question key terms to possible answers and
applies these patterns to candidate sentences to ex-
tract factoid answers. The semantic approach gener-
ates a semantic representation of the question that is
based on predicate-argument structures and extracts
answer candidates from similar structures in the cor-
pus. The source code of the answer extractors is in-
cluded in OpenEphyra, an open source release of the
system.2
The answer candidates from these extractors are
combined and ranked by a statistical answer selec-
tion framework (Ko et al, 2007), which estimates
the probability of an answer based on a number of
answer validation and similarity features. Valida-
tion features use resources such as gazetteers and
Wikipedia to verify an answer, whereas similarity
features measure the syntactic and semantic simi-
larity to other candidates, e.g. using string distance
measures and WordNet relations.
2.2 Set Expander for Any Language (SEAL)
Set expansion (SE) refers to expanding a given par-
tial set of objects into a more complete set. SEAL3
(Wang and Cohen, 2007) is a SE system which ac-
cepts input elements (seeds) of some target set St
and automatically finds other probable elements of
St in semi-structured documents such as web pages.
SEAL also works on unstructured text, but its ex-
traction mechanism benefits from structuring ele-
ments such as HTML tags. The algorithm is in-
dependent of the human language from which the
2http://www.ephyra.info/
3http://rcwang.com/seal
Figure 1: Examples of SEAL?s input and output. English
entities are reality TV shows, Chinese entities are popular
Taiwanese food, and Japanese entities are famous cartoon
characters.
Figure 2: An example graph constructed by SEAL. Every
edge from node x to y actually has an inverse relation
edge from node y to x that is not shown here (e.g. m1 is
extracted by w1).
seeds are taken, and also independent of the markup
language used to annotate the documents. Examples
of SEAL?s input and output are shown in Figure 1.
In more detail, SEAL comprises three major com-
ponents: the Fetcher, the Extractor, and the Ranker.
The Fetcher focuses on retrieving web pages. The
URLs of the web pages come from top results re-
trieved from Google and Yahoo! using the concate-
nation of all seeds as the query. The Extractor au-
tomatically constructs page-specific extraction rules,
or wrappers, for each page that contains the seeds.
Every wrapper is defined by two character strings,
which specify the left-context and right-context nec-
essary for an entity to be extracted from a page.
These strings are chosen to be maximally-long con-
texts that bracket at least one occurrence of every
seed string on a page. Most of the wrappers con-
948
tain HTML tags, which illustrates the importance
of structuring information in the source documents.
All entity mentions bracketed by these contextual
strings derived from a particular page are extracted
from the same page. Finally, the Ranker builds a
graph, and then ranks the extracted mentions glob-
ally based on the weights computed by performing a
random graph walk.
An example graph is shown in Figure 2, where
each node di represents a document, wi a wrapper,
and mi an extracted entity mention. The graph mod-
els the relationship between documents, wrappers,
and mentions. In order to measure the relative im-
portance of each node within the graph, the Ranker
performs a graph walk until all node weights con-
verge. The idea is that nodes are weighted higher
if they are connected to many other highly weighted
nodes.
We apply this SE algorithm to answer candidates
for list questions generated by Ephyra and other
TREC QA systems to find additional instances of
correct answers that were not in the original candi-
date set.
3 Proposed Approach
SEAL was originally designed to handle only rele-
vant input seeds. When provided with a mixture of
relevant and irrelevant answers from a QA system,
the performance would suffer. In this section, we
propose three modifications to SEAL to improve its
ability to handle noisy input seeds.
3.1 Aggressive Fetcher
For each expansion, SEAL?s fetcher concatenates all
seeds and sends them as one query to the search
engines. However, when the seeds are noisy, the
documents fetched are constrained by the irrele-
vant seeds, which decreases the chance of finding
good documents. To overcome this problem, we
designed an aggressive fetcher (AF) that increases
the chance of composing queries containing only
relevant seeds. It sends a two-seed query for ev-
ery possible pair of seeds to the search engines. If
there are n input seeds, then the total number of
queries sent would be (n2
). For example, suppose
SEAL is given a set of noisy seeds: Boston, Seattle
and Carnegie-Mellon (assuming Carnegie-Mellon is
irrelevant), then by using AF, one query will contain
only relevant seeds (as shown in Table 1). The docu-
ments are then collected and sent to SEAL?s extrac-
tor for learning wrappers.
Queries Quality
-AF #1: Boston Seattle Carnegie-Mellon Low
+AF
#1: Boston Seattle High
#2: Boston Carnegie-Mellon Low
#3: Seattle Carnegie-Mellon Low
Table 1: Example queries and their quality given
the seeds Boston, Seattle and Carnegie-Mellon, where
Carnegie-Mellon is assumed to be irrelevant.
3.2 Lenient Extractor
SEAL?s extractor requires the longest common
contexts to bracket at least one instance of every
seed per web page. However, when seeds are noisy,
such common contexts usually do not exist or
are too short to be useful. To solve this problem,
we propose a lenient extractor (LE) which only
requires the contexts to bracket at least one in-
stance of a minimum of two seeds, instead of every
seed. This increases the chance of finding longest
common contexts that bracket only relevant seeds.
For instance, suppose SEAL is given the seeds
from the previous example (Boston, Seattle and
Carnegie-Mellon) and the passage below. Then the
extractor would learn the wrappers shown in Table 2.
?While attending a hearing in Boston City
Hall, Alan, a professor at Boston University,
met Tina, his former student at Seattle Univer-
sity, who is studying at Carnegie-Mellon University
Art School and will be working in Seattle City Hall.?
Learned Wrappers
-LE #1: at [...] University
+LE #1: at [...] University#2: in [...] City Hall
Table 2: Wrappers learned by SEAL?s extractor when
given the passage in Section 3.2 and the seeds Boston,
Seattle and Carnegie-Mellon.
949
As illustrated, with lenient extraction, SEAL is
now able to learn the second wrapper because it
brackets one instance of at least two seeds (Boston
and Seattle). This can be very helpful if the list
question is asking for city names rather than univer-
sity names. The extractor then uses these wrappers
to extract additional answer candidates, by search-
ing for other strings that fit into the placeholders of
the wrappers. Note that the example was simplified
for ease of presentation. The wrappers are actually
character-based (as opposed to word-based) and are
likely to contain HTML tags when generated from
real web pages.
3.3 Hinted Expander
Most QA systems use keywords from the question to
guide the retrieval of relevant documents and the ex-
traction of answer candidates. We believe these key-
words are also important for SEAL to identify ad-
ditional instances of correct answers. For example,
if the seeds are George Washington, John Adams,
and Thomas Jefferson, then without using any con-
text from the question, SEAL would output a mix-
ture of founding fathers and presidents of the U.S.A.
To solve this problem, we devised a hinted expan-
sion (HE) technique that utilizes the context given
in the question to constrain SEAL?s search space on
the Web. This is achieved by appending keywords
from the question to every query that is sent to the
search engines. The rationale is that the retrieved
documents will also match the keywords, which may
increase the chance of finding those documents that
contain our desired set of answers.
4 Experimental Design
We conducted experiments in two phases. In the
first phase, we evaluated the SE approach by apply-
ing SEAL to answers generated by Ephyra. In the
second phase, we evaluated the approach by apply-
ing SEAL to the output from QA systems that per-
formed the best on the list questions in the TREC 15
evaluation. In both phases, the answers found by
SEAL were retrieved from the Web instead of the
AQUAINT newswire corpus used in the TREC eval-
uations. However, we rejected answers if they could
only be found in the Web and not in the AQUAINT
corpus to avoid an unfair advantage over the QA
systems: TREC participants were allowed to extract
candidates from the Web (or any other source), but
they had to identify a supporting document in the
AQUAINT corpus for each answer and thus could
not return answers that were not covered by the cor-
pus.
Preliminary experiments showed that we can ob-
tain a good balance between the amount and quality
of the documents fetched by using only rare ques-
tion terms as hint words. In particular, we select the
three question words that occur least frequently in a
sample of the AQUAINT corpus as hints. The can-
didate answers were evaluated by using the answer
keys, composed of regular expression patterns, ob-
tained from the TREC website. We did not extend
the patterns with additional correct answers found in
our experiments. These answer keys were not offi-
cially used in the TREC evaluation; thus the baseline
scores we computed for Ephyra and other QA sys-
tems in our experiments are slightly different from
those officially reported.
4.1 Ephyra
We evaluated our SE approach on Ephyra using the
list questions from TREC 13, 14, and 15 (55, 93, and
89 questions, respectively). For each question, the
top four answer candidates from Ephyra were given
as input seeds to SEAL. Initial experiments showed
that by adding additional seeds, the effectiveness of
our approach can be improved at the expense of a
longer runtime.
We report both mean average precision (MAP)
and F1 scores. For the F1 scores, we drop answer
candidates with low confidence scores by applying
a relative cut-off threshold: an answer candidate is
dropped if the ratio of its confidence score and the
score of the top answer is below a threshold. An
optimal threshold for a question is a threshold that
maximizes the F1 score for that particular question.
For each TREC dataset, we conducted three ex-
periments: (1) evaluation of answer candidates us-
ing MAP; (2) evaluation using average F1 with an
optimal threshold for each question; and (3) eval-
uation using average F1 with thresholds trained by
5-fold cross validation. For each of those 5-fold val-
idations, only one threshold was determined for all
questions in the training folds.
950
Ephyra Ephyra?s SEAL SEAL+LE SEAL+LE SEAL+LE
Top 4 Ans. +AF +AF+HE
TREC 13 25.95% 21.39% 23.76% 31.43% 34.22% 35.26%
TREC 14 14.45% 8.71% 14.47% 17.04% 16.58% 18.82%
TREC 15 13.42% 9.02% 13.17% 16.87% 17.12% 18.95%
Table 3: Mean average precision of Ephyra, its top four answers, and various SEAL configurations, where LE is
Lenient Extractor, AF is Aggressive Fetcher, and HE is Hinted Expander.
Ephyra Ephyra?s SEAL SEAL+LE SEAL+LE SEAL+LE
Top 4 Ans. +AF +AF+HE
TREC 13 35.74% 26.29% 30.53% 36.47% 40.08% 40.80%
TREC 14 22.83% 14.05% 20.62% 22.81% 22.66% 24.88%
TREC 15 22.42% 14.57% 19.88% 23.30% 24.04% 25.65%
Table 4: Average F1 of Ephyra, its top four answers, and various SEAL configurations when using an optimal threshold
for each question.
4.2 Top QA Systems
We evaluated two SE approaches, SEAL and Google
Sets, on the five QA systems that performed the best
on the list questions in TREC 15. For each question,
the top four answer candidates4 from those systems
were given as input seeds to SEAL and Google Sets.
Unlike the candidates found by Ephyra, these can-
didates were provided without confidence scores;
hence, we assumed they all have a score of 1.0. In
our experiments with SEAL, we first determined a
single threshold that optimizes the average of the F1
scores of the top five systems in both TREC 13 and
14. We then obtained evaluation results for the top
systems in TREC 15 by using this trained threshold.
When performing hinted expansion, the keywords
(or hint words) for each question were extracted by
Ephyra?s question analysis component. In our exper-
iments with Google Sets, we requested Small Sets of
items and again measured the performance in terms
of F1 scores. We also tried requesting Large Sets but
the results were worse.
5 Results and Discussion
In Tables 3 and 4, we present evaluation results for
all answers from Ephyra, only the top four answers,
and various configurations of SEAL using the top
four answers as seeds. Table 3 shows the MAP for
4Obtained from http://trec.nist.gov/results
each dataset (TREC 13, 14, and 15), and Table 4
shows for each dataset the average F1 score when
using optimal per-question thresholds. The results
indicate that SEAL achieves the best performance
when configured with all three proposed extensions.
In terms of MAP, the best-configured SEAL im-
proves the quality of the input answers (relatively)
by 65%, 116%, 110% for each dataset respectively,
and improves Ephyra?s overall performance by 36%,
30%, 41%. In terms of optimal F1, SEAL improves
the quality of the input answers by 55%, 77%, 76%
and Ephyra?s overall performance by 14%, 9%, 14%
respectively. These results illustrate that a SE sys-
tem is capable of improving a QA system?s perfor-
mance on list questions, if we know how to select
good thresholds.
In practice, the thresholds are unknown and must
be estimated from a training set. Table 5 shows eval-
uation results using 5-fold cross validation for each
dataset (TREC 13, 14, and 15) independently, and
the combination of all three datasets (All). For each
validation, we determine the threshold that maxi-
mizes the F1 score on the training folds, and we
also determine the F1 score on the test fold by ap-
plying the trained threshold. We repeat this valida-
tion for each of the five test folds and present the av-
erage threshold and F1 score for each configuration
and dataset. The F1 scores give an estimate of the
performance on unseen data and allow a fair com-
951
Ephyra SEAL+LE+AF+HE Hybrid
Avg. F1 Avg. Threshold Avg. F1 Avg. Threshold Avg. F1 Avg. Threshold
TREC 13 25.55% 0.3808 30.71% 0.3257 29.04% 0.0796
TREC 14 15.78% 0.2636 15.60% 0.1889 17.13% 0.0108
TREC 15 15.19% 0.1192 15.64% 0.2581 16.47% 0.0123
All 18.03% 0.2883 19.15% 0.2606 19.59% 0.0164
Table 5: Average F1 of Ephyra, the best-configured SEAL, and the hybrid system, along with thresholds trained by
5-fold cross validation.
TREC 15 Baseline Top 4 Ans. Google Sets SEAL+LE+AF+HE Hybrid
QA Systems Avg. F1 Avg. F1 Avg. F1 ?F1 Avg. F1 ?F1 Avg. F1 ?F1
lccPA06 44.96% 32.67% 37.89% -15.72% 40.00% -11.04% 45.30% 0.76%
cuhkqaepisto 18.27% 17.02% 15.96% -12.68% 19.75% 8.08% 19.13% 4.70%
NUSCHUAQA1 18.40% 14.99% 16.70% -9.21% 18.74% 1.86% 18.06% -1.81%
FDUQAT15A 19.71% 14.32% 18.79% -4.63% 19.78% 0.38% 20.61% 4.57%
QACTIS06C 17.52% 15.22% 17.05% -2.72% 18.45% 5.26% 18.38% 4.85%
Average 23.77% 18.84% 21.28% -10.49% 23.34% -1.81% 24.30% 2.20%
Table 6: Average F1 of the QA systems, their top four answers, Google Sets, the best-configured SEAL, the hybrid
system, and their relative improvements over the QA systems.
parison across systems. Here, we also introduce a
hybrid system (Hybrid) that intersects the answers
found by both systems by multiplying their proba-
bilistic scores.
Tables 3, 4, and 5 show that the effectiveness of
the SE approach depends on the quality of the initial
answer candidates. The improvements are most ap-
parent for the TREC 13 dataset, where Ephyra has
a much higher performance compared to TREC 14
and 15. However, the best-configured SEAL did not
improve the F1 score on TREC 14, as reported in
Table 5. We suspect that this is due to the compar-
atively low quality of Ephyra?s top four answers for
this dataset. The experiments also illustrate that by
intersecting the answer candidates found by Ephyra
and SEAL, we can eliminate poor answer candi-
dates and partially compensate for the low preci-
sion of Ephyra on the harder TREC datasets. How-
ever, this comes at the expense of a lower recall,
which slightly hurts the performance on the compar-
atively easier TREC 13 questions. We also evaluated
Google Sets on top four answers from Ephyra for
TREC 13-15 and obtained F1 scores of 12%, 11%,
and 9% respectively (compared to 29%, 17%, and
16% for our hybrid approach with trained thresh-
olds).
Table 6 shows F1 scores for the SE approach
applied to the output from the five QA systems
with the highest performance on the list questions
in TREC 15. Again, Hybrid intersects the answers
found by the QA system and SEAL by multiplying
their confidence scores. Two thresholds were trained
separately on the top five systems in both TREC 13
and 14; one for SEAL (0.2376) and another for Hy-
brid (0.2463). As shown, the performance of Google
Sets is worse than SEAL and Hybrid, but better than
the top four answers on average. We believe our SE
system outperforms Google Sets because we have
methods to handle noisy inputs (i.e. AF, LE) and a
method for guiding the SE algorithm to search in the
right space on the Web (i.e. HE).
The results show that both SEAL and Hybrid are
capable of improving four out of the five systems.
We observed that one reason why SEAL did not im-
prove ?lccPA06? was the incompleteness of the an-
swer keys. Table 7 shows one of many examples
where SEAL was penalized for finding additional
correct answers. As illustrated, Hybrid improved
all systems except ?NUSCHUAQA1?. The reason
is that even though SEAL improved the baseline,
their overlapping answer set is too small; thus hurt-
ing the recall of Hybrid substantially. Unfortunately,
952
Question 154.6: Name titles of movies, other than ?Superman? movies, that
Christopher Reeve acted in.
lccPA06 (F1: 75%) SEAL+LE+AF+HE (F1: 40%)
+Rear Window +Rear Window
+The Remains of the Day +The Remains of the Day
+Snakes and Ladders -The Bostonians
-Superman -Somewhere in Time
-Village of the Damned
-In the Gloaming
Table 7: Example of SEAL being penalized for finding correct answers (all are correct except the last one). Answers
found in the answer keys are marked with ?+?. All four answers from ?lccPA06? were used as seeds.
Question 170.6: What are the titles of songs written by John Prine?
NUSCHUAQA1 (F1: 25%) SEAL+LE+AF+HE (F1: 44%)
+I Just Want to Dance With You +I Just Want to Dance With You
-Titled In Spite of Ourselves +Christmas in Prison
+Christmas in Prison +Sam Stone
-Grammy - Winning -Grandpa was a Carpenter
-Sabu Visits the Twin Cities Alone
+Angel from Montgomery
Table 8: Example demonstrating SEAL?s ability to handle noisy input seeds. All four answers from ?NUSCHUAQA1?
were used as seeds. Again, SEAL is penalized for finding correct answers (all answers are correct).
for the top TREC 15 systems we only had access to
the answers that were actually submitted by the par-
ticipants, whereas for Ephyra we could utilize the
entire list of generated answer candidates, includ-
ing those that fell below the cutoff threshold for list
questions. Nevertheless, the hybrid approach could
improve the baseline by more than 2% on average
in terms of F1 score. Table 8 shows that the best-
configured SEAL is capable of expanding only the
relevant seeds even when given a set of noisy seeds.
Neither Google Sets nor the original SE algorithm
without the proposed extensions could expand these
seeds with additional candidates.
On average, SEAL required about 5 seconds for
querying the search engines, 10 seconds for crawl-
ing the Web, 20 seconds for extracting answer can-
didates from the web pages, and 5 seconds for rank-
ing the candidates. Note that the SE system has not
been optimized extensively. The runtime of the web
page retrieval step and much of the search is due to
network latency and can be reduced if the search is
performed locally.
6 Conclusion and Future Work
We have shown that our SE approach is capable of
improving the performance of QA systems on list
questions by utilizing only their top four answer can-
didates as seeds. We have also illustrated a feasible
and effective method for integrating a SE approach
into any QA system. We would like to emphasize
that for each of the experiments we conducted, all
that the SE system received as input were the top
four noisy answers from a QA system and three key-
words from the TREC questions. We have shown
that higher quality candidates support more effec-
tive set expansion. In the future, we will investigate
how to utilize more answer candidates from the QA
system and determine the minimal quality of those
candidates required for SE approach to make an im-
provement.
We have also shown that, in terms of F1 scores
with trained thresholds, the hybrid method improves
the Ephyra QA system on all datasets and also im-
proves four out of the five systems that performed
953
the best on the list questions in TREC 15. How-
ever, the final list of answers only comprises candi-
dates found by both the QA system and the SE al-
gorithm. In future experiments, we will investigate
other methods of merging answer candidates, such
as taking the union of answers from both systems.
We expect further improvements from adding can-
didates that are found only by the QA system, but
it is unclear how the confidence measures from the
two systems can be combined effectively.
We would also like to emphasize that the SE ap-
proach is entirely language independent, and thus
can be readily applied to answer candidates in other
languages. In future experiments, we will investi-
gate its performance on question answering tasks in
languages such as Chinese and Japanese.
As pointed out previously, the performance of the
SE approach highly depends on the accuracy of the
seeds. However, QA systems are usually not op-
timized to provide few high-precision results, but
treat precision and recall as equally important. This
leaves room for further improvements, e.g. by ap-
plying stricter answer validation techniques to the
seeds used for SE.
We also plan to analyze the effectiveness of our
approach across different question types and evalu-
ate it on more complex questions such as the rigid
list questions in the new TAC QA evaluation, which
ask for opinion holders and subjects.
Acknowledgements
This work was supported in part by the Google Re-
search Awards program, IBM Open Collaboration
Agreement #W0652159, and the Defense Advanced
Research Projects Agency (DARPA) under Contract
No. NBCHD030010.
References
H.T. Dang, J. Lin, and D. Kelly. 2006. Overview of the
TREC 2006 question answering track. Proceedings of
the Fifteenth Text REtrieval Conference.
H.T. Dang, D. Kelly, and J. Lin. 2007. Overview of the
TREC 2007 question answering track. Proceedings of
the Sixteenth Text REtrieval Conference.
J. Ko, L. Si, and E. Nyberg. 2007. A probabilistic frame-
work for answer selection in question answering. Pro-
ceedings of NAACL-HLT.
N. Schlaefer, P. Gieselmann, and G. Sautter. 2006. The
Ephyra QA system at TREC 2006. Proceedings of the
Fifteenth Text REtrieval Conference.
N. Schlaefer, G. Sautter, J. Ko, J. Betteridge, M. Pathak,
and E. Nyberg. 2007. Semantic extensions of the
Ephyra QA system in TREC 2007. To appear in: Pro-
ceedings of the Sixteenth Text REtrieval Conference.
R.C. Wang and W.W. Cohen. 2007. Language-
independent set expansion of named entities using the
web. Proceedings of IEEE International Conference
on Data Mining.
954
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1503?1512,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Character-level Analysis of Semi-Structured Documents for Set Expansion
Richard C. Wang
Language Technologies Institute
Carnegie Mellon University
rcwang@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
Set expansion refers to expanding a par-
tial set of ?seed? objects into a more com-
plete set. One system that does set ex-
pansion is SEAL (Set Expander for Any
Language), which expands entities auto-
matically by utilizing resources from the
Web in a language-independent fashion.
In this paper, we illustrated in detail the
construction of character-level wrappers
for set expansion implemented in SEAL.
We also evaluated several kinds of wrap-
pers for set expansion and showed that
character-based wrappers perform better
than HTML-based wrappers. In addition,
we demonstrated a technique that extends
SEAL to learn binary relational concepts
(e.g., ?x is the mayor of the city y?) from
only two seeds. We also show that the
extended SEAL has good performance on
our evaluation datasets, which includes
English and Chinese, thus demonstrating
language-independence.
1 Introduction
SEAL
1
(Set Expander for Any Language) is a
set expansions system that accepts input ele-
ments (seeds) of some target set S and automat-
ically finds other probable elements of S in semi-
structured documents such as web pages. SEAL
is a research system that has shown good perfor-
mance in previously published results (Wang and
Cohen, 2007). By using only three seeds and
top one hundred documents returned by Google,
SEAL achieved 90% in mean average precision
(MAP), averaged over 36 datasets from three lan-
guages: English, Chinese, and Japanese. Un-
like other published research work (Etzioni et al,
2005), SEAL focuses on finding small closed sets
1
http://rcwang.com/seal
of items (e.g., Disney movies) rather than large
and more open sets (e.g., scientists).
In this paper, we explore the impact on perfor-
mance of one of the innovations in SEAL, specif-
ically, the use of character-level techniques to de-
tect candidate regular structures, or wrappers, in
web pages. Although some early systems for
web-page analysis induce rules at character-level
(e.g., such as WIEN (Kushmerick et al, 1997) and
DIPRE (Brin, 1998)), most recent approaches for
set expansion have used either tokenized and/or
parsed free-text (Carlson et al, 2009; Talukdar et
al., 2006; Snow et al, 2006; Pantel and Pennac-
chiotti, 2006), or have incorporated heuristics for
exploiting HTML structures that are likely to en-
code lists and tables (Nadeau et al, 2006; Etzioni
et al, 2005).
In this paper, we experimentally evaluate
SEAL?s performance under two settings: 1) us-
ing the character-level page analysis techniques
of the original SEAL, and 2) using page analy-
sis techniques constrained to identify only HTML-
related wrappers. Our conjecture is that the less
constrained character-level methods will produce
more candidate wrappers than HTML-based tech-
niques. We also conjecture that a larger number of
candidate wrappers will lead to better performance
overall, due to SEAL?s robust methods for ranking
candidate wrappers.
The experiments in this paper largely vali-
date this conjecture. We show that the HTML-
restricted version of SEAL performs less well,
losing 13 points in MAP on a dozen Chinese-
language benchmark problems, 8 points in MAP
on a dozen English-language problems, and 2
points in MAP on a dozen Japanese-language
problems.
SEAL currently only handles unary relation-
ships (e.g., ?x? is a mayor). In this paper, we
show that SEAL?s character-level analysis tech-
niques can, like HTML-based methods, be read-
1503
ily extended to handle binary relationships. We
then demonstrate that this extension of SEAL can
learn binary concepts (e.g., ?x is the mayor of
the city y?) from a small number of seeds, and
show that, as with unary relationships, MAP per-
formance is 26 points lower when wrappers are
restricted to be HTML-related. Furthermore, we
also illustrate that the learning of binary concepts
can be bootstrapped to improve its performance.
Section 2.1 explains how SEAL constructs
wrappers and rank candidate items for unary re-
lations. Section 3 describes the experiments and
results for unary relations. Section 4 presents the
method for extending SEAL to handle binary re-
lationships, as well as their experimental results.
Related work is discussed in Section 5, and the
paper concludes in Section 6.
2 SEAL
2.1 Identifying Wrappers for Unary
Relations
When SEAL performs set expansion, it accepts a
small number of seeds from the user (e.g., ?Ford?,
?Nissan?, and ?Toyota?). It then uses a web
search engine to retrieve some documents that
contain these instances, and then analyzes these
documents to find candidate wrappers (i.e., regu-
lar structures on a page that contain the seed in-
stances). Strings that are extracted by a candidate
wrapper (but are not equivalent to any seed) are
called candidate instances. SEAL then statisti-
cally ranks the candidate instances (and wrappers),
using the techniques outlined below, and outputs a
ranked list of instances to the user.
One key step in this process is identifying can-
didate wrappers. In SEAL, a candidate wrapper is
defined by a pair of left and right character strings,
` and r. A wrapper ?extracts? items from a partic-
ular document by locating all strings in the docu-
ment that are bracketed by the wrapper?s left and
right strings, but do not contain either of the two
strings. In SEAL, wrappers are always learned
from, and applied to, a single document.
Table 1 illustrates some candidate wrappers
learned by SEAL. (Here, a wrapper is written as
`[...]r, with the [...] to be filled by an extracted
string.) Notice that the instances extracted by
wrappers can and do appear in surprising places,
such as embedded in URLs or in HTML tag at-
tributes. Our experience with these character-
based wrappers lead us to conjecture that exist-
ing heuristics for identifying structure in HTML
are fundamentally limited, in that many potentially
useful structures will not be identified by analyz-
ing HTML structure only.
SEAL uses these rules to find wrappers. Each
candidate wrapper `, r is a maximally long pair of
strings that bracket at least one occurrence of ev-
ery seed in a document: in other words, for each
pair `, r, the set of strings C extracted by `, r has
the properties that:
1. For every seed s, there exists some c ? C that
is equivalent to s; and
2. There are no strings `
?
, r
?
that satisfy property
(1) above such that ` is a proper suffix of `
?
and r is a proper prefix of r
?
.
SEAL?s wrappers can be found quite efficiently.
The algorithm we use has been described previ-
ously (Wang and Cohen, 2007), but will be ex-
plained again here for completeness. As an ex-
ample, below shows a mock document, written in
an unknown mark-up language, that has the seeds:
Ford, Nissan, and Toyota located (and boldfaced).
There are two other car makers hidden inside this
document (can you spot them?). In this section,
we will show you how to automatically construct
wrappers that reveal them.
GtpKxHnIsSaNxjHJglekuDialcLBxKHforDxkrpW
NaCMwAAHOFoRduohdEXocUvaGKxHaCuRAxjHjnOx
oTOyOTazxKHAUdIxkrOyQKxHToYotAxjHCRdmLxa
puRAPprtqOVKxHfoRdxjHaJAScRFrlaFoRDofwNL
WxKHtOYotaxkrHxQKlacXlGEKtxKHNisSanxkrEq
Given a set of seeds and a semi-structured doc-
ument, the wrapper construction algorithm starts
by locating all strings equivalent to a seed in the
document; these strings are called seed instances
below. (In SEAL, we always use case-insensitive
string matching, so a string is ?equivalent to? any
case variant of itself.) The algorithm then inserts
all the instances into a list and assigns a unique id
to each of them by its index in the list (i.e., the id
of an instance is its position in the list.)
For every seed instance in the document, its
immediate left character string (starting from the
first character of the document) and right charac-
ter string (ending at the last character of the docu-
ment) are extracted and inserted into a left-context
trie and a right-context trie respectively, where the
left context is inserted in reversed character or-
der. (Here, we implemented a compact trie called
1504
URL: http://www.shopcarparts.com/
Wrapper: .html" CLASS="shopcp">[...] Parts</A> <br>
Content: acura, audi, bmw, buick, cadillac, chevrolet, chevy, chrysler, daewoo, daihatsu, dodge, eagle, ford, ...
URL: http://www.allautoreviews.com/
Wrapper: </a><br> <a href="auto reviews/[...]/
Content: acura, audi, bmw, buick, cadillac, chevrolet, chrysler, dodge, ford, gmc, honda, hyundai, infiniti, isuzu, ...
URL: http://www.hertrichs.com/
Wrapper: <li class="franchise [...]"> <h4><a href="#">
Content: buick, chevrolet, chrysler, dodge, ford, gmc, isuzu, jeep, lincoln, mazda, mercury, nissan, pontiac, scion, ...
URL: http://www.metacafe.com/watch/1872759/2009 nissan maxima performance/
Wrapper: videos">[...]</a> <a href="/tags/
Content: avalon, cars, carscom, driving, ford, maxima, nissan, performance, speed, toyota
URL: http://www.worldstyling.com/
Wrapper: ?>[...] Accessories</option><option value=?
Content: chevy, ford, isuzu, mitsubishi, nissan, pickup, stainless steel, suv, toyota
Table 1: Examples of wrappers constructed from web pages given the seeds: Ford, Nissan, Toyota.
Patricia trie where every node stores a substring.)
Every node in the left-context trie maintains a list
of ids for keeping track of the seed instances that
follow the string associated with that node. Same
thing applies to the right-context trie symmetri-
cally. Figure 1 shows the two context tries and
the list of seed instances when provided the mock
document with the seeds: Ford, Nissan, and Toy-
ota.
Provided that the left and right context tries are
populated with all the contextual strings of ev-
ery seed instance, the algorithm then finds maxi-
mally long contextual strings that bracket at least
one seed instance of every seed. The pseudo-code
for finding these strings for building wrappers is
illustrated in Table 2, where Seeds is the set of
input seeds and ` is the minimum length of the
strings. We observed that longer strings produce
higher precision but lower recall. This is an in-
teresting parameter that is worth exploring, but
for this paper, we consider and use only a min-
imum length of one throughout the experiments.
The basic idea behind the pseudo-code is to first
find all the longest possible strings from one trie
given some constraints, then for every such string
s, find the longest possible string s
?
from another
trie such that s and s
?
bracket at least one occur-
rence of every given seed in a document.
The wrappers constructed as well as the items
extracted given the mock document and the exam-
ple seeds are shown below. Notice that Audi and
Acura are uncovered (did you spot them?).
Wrapper: xKH[...]xkr
Content: audi, ford, nissan, toyota
Wrapper: KxH[...]xjH
Content: acura, ford, nissan, toyota
Wrappers MakeWrappers(Trie `, Trie r)
Return Wraps(l, r) ?Wraps(r, l)
Wrappers Wraps(Trie t
1
, Trie t
2
)
For each n
1
? TopNodes(t
1
, `)
For each n
2
? BottomNodes(t
2
, n
1
)
For each n
1
? BottomNodes(t
1
, n
2
)
Construct a new Wrapper(Text(n
1
), Text(n
2
))
Return a union of all wrappers constructed
Nodes BottomNodes(Trie t
1
, Node n
?
)
Find node n ? t
1
such that:
(1) NumCommonSeeds(n, n
?
) == |Seeds|, and
(2) All children nodes of n (if exist) fail on (1)
Return a union of all nodes found
Nodes TopNodes(Trie t, int `)
Find node n ? t such that:
(1) Text(n).length ? `, and
(2) Parent node of n (if exist) fails on (1)
Return a union of all nodes found
String Text(Node n)
Return the textual string represented by the
path from root to n in the trie containing n
Integer NumCommonSeeds(Node n
1
, Node n
2
)
For each index i ? Intersect(n
1
, n
2
):
Find the seed at index i of seed instance list
Return the size of the union of all seeds found
Integers Intersect(Node n
1
, Node n
2
)
Return n
1
.indexes ? n
2
.indexes
Table 2: Pseudo-code for constructing wrappers.
Table 1 shows examples of wrappers con-
structed from real web documents. We have also
observed items extracted from plain text (.txt),
comma/tab-separated text (.csv/.tsv), latex (.tex),
and even Word documents (.doc) of which the
wrappers have binary character strings. These ob-
servations support our claim that the algorithm is
independent of mark-up language. In our experi-
mental results, we will show that it is independent
of human language as well.
1505
Figure 1: The context tries and the seed instance list constructed given the mock document presented in
Section 2.1 and the seeds: Ford, Nissan and Toyota.
2.2 Ranking Wrappers and Candidate
Instances
In previous work (Wang and Cohen, 2007), we
presented a graph-walk based technique that is
effective for ranking sets and wrappers. This
model encapsulates the relations between docu-
ments, wrappers, and extracted instances (entity
mentions). Similarly, our graph also consists of
a set of nodes and a set of labeled directed edges.
Figure 2 shows an example graph where each node
d
i
represents a document, w
i
a wrapper, and m
i
an extracted entity mention. A directed edge con-
nects a node d
i
to a w
i
if d
i
contains w
i
, a w
i
to a
m
i
ifw
i
extractsm
i
, and a d
i
to am
i
if d
i
contains
m
i
. Although not shown in the figure, every edge
from node x to y actually has an inverse relation
edge from node y to x (e.g., m
i
is extracted by w
i
)
to ensure that the graph is cyclic.
We will use letters such as x, y, and z to denote
nodes, and x
r
?? y to denote an edge from x to
y with labeled relation r. Each node represents an
object (document, wrapper, or mention), and each
edge x
r
?? y asserts that a binary relation r(x, y)
holds. We want to find entity mention nodes that
are similar to the seed nodes. We define the sim-
ilarity between two nodes by random walk with
restart (Tong et al, 2006). In this algorithm, to
walk away from a source node x, one first chooses
an edge relation r; then given r, one picks a target
node y such that x
r
?? y. When given a source
node x, we assume that the probability of picking
an edge relation r is uniformly distributed among
the set of all r, where there exist a target node y
such that x
r
?? y. More specifically,
Figure 2: Example graph built by Random Walk.
P (r|x) =
1
|r : ?y x
r
?? y|
(1)
We also assume that once an edge relation r is
chosen, a target node y is picked uniformly from
the set of all y such that x
r
?? y. More specifi-
cally,
P (y|r, x) =
1
|y : x
r
?? y|
(2)
In order to perform random walk, we will build
a transition matrix M where each entry at (x, y)
represents the probability of traveling one step
from a source node x to a target node y, or more
specifically,
M
xy
=
?
r
P (r|x)P (y|r, x) (3)
We will also define a state vector ~v
t
which rep-
resents the probability at each node after iterating
through the entire graph t times, where one itera-
tion means to walk one step away from every node.
The state vector at t+ 1 iteration is defined as:
~v
t+1
= ?~v
0
+ (1? ?)M~v
t
(4)
1506
Since we want to start our walk from the seeds,
we initialize v
0
to have probabilities uniformly
distributed over the seed nodes. In each step of
our walk, there is a small probability ? of tele-
porting back to the seed nodes, which prevents us
from walking too far away from the seeds. We
iterate our graph until the state vector converges,
and rank the extracted mentions by their probabil-
ities in the final state vector. We use a constant ?
of 0.01 in the experiments below.
2.3 Bootstrapping Candidate Instances
Bootstrapping refers to iterative unsupervised set
expansion. This process requires minimal super-
vision, but is very sensitive to the system?s perfor-
mance because errors can easily propagate from
one iteration to another. As shown in previous
work (Wang and Cohen, 2008), carefully designed
seeding strategies can minimize the propagated er-
rors. Below, we show the pseudo-code for our
bootstrapping strategy.
stats? ?, used? inputs
for i = 1 to M do
m = min(3, |used|)
seeds? select
m
(used) ? top(list)
stats? expand(seeds, stats)
list? rank(stats)
used? used ? seeds
end for
where M is the total number of iterations, inputs
are the two initial input seeds, select
m
(S) ran-
domly selects m different seeds from the set S,
used is a set that contains previously expanded
seeds, top(list) returns an item that has the high-
est rank in list, expand(seeds, stats) expands
the selected seeds using stats and outputs accu-
mulated statistics, and rank(stats) applies Ran-
dom Walk described in Section 2.2 on the accu-
mulated stats to produce a list of items. This
strategy dumps the highest-ranked item into the
used bucket after every iteration. It starts by ex-
panding two input seeds. For the second iteration,
it expands three seeds: two used plus one from
last iteration. For every successive iteration, it ex-
pands four seeds: three randomly selected used
ones plus one from last iteration.
3 Experiments with Unary Relations
We would like to determine whether character-
based or HTML-based wrappers are more suited
for the task of set expansion. In order to do that,
# L. Context [...]R. Context Eng Jap Chi Avg
1 .+[...].+ 87.6 96.9 95.4 93.3
2 .
*
[<>].
*
[...].
*
[<>].
*
85.7 96.8 90.7 91.1
3 .
*
>[...]<.
*
85.7 96.7 90.7 91.0
4 .
*
<.+?>.
*
[...].
*
<.+?>.
*
80.1 95.8 83.7 86.5
5 .
*
<.+?>[...]<.+?>.
*
79.6 94.9 82.4 85.6
Table 3: The performance (MAP) of various types
of wrappers on semi-structured web pages.
we introduce five types of wrappers, as illustrated
in Table 3. The first type is the character-based
wrapper that does not have any restriction on the
alphabets of its characters. Starting from the sec-
ond type, the allowable alphabets in a wrapper be-
come more restrictive. The fifth type requires that
an item must be tightly bracketed by two complete
HTML tags in order to be extracted.
All pure HTML-based wrappers are type 5, pos-
sibly with additional restrictions imposed (Nadeau
et al, 2006; Etzioni et al, 2005). SEAL cur-
rently does not use an HTML parser (or any other
kinds of parser), so restrictions cannot be easily
imposed. As far as we know, there isn?t an agree-
ment on what restrictions make the most sense
or work the best. Therefore, we evaluate perfor-
mance for varying wrapper constraints from type
1 (most general) to type 5 (most strict) in our ex-
periments.
For set expansion, we use the same evaluation
set as in (Wang and Cohen, 2007) which contains
36 manually constructed lists across three differ-
ent languages: English, Chinese, and Japanese (12
lists per language). Each list contains all instances
of a particular semantic class in a certain language,
and each instance contains a set of synonyms (e.g.,
USA, America).
Since the output of our system is a ranked list
of extracted instances, we choose mean average
precision (MAP) as our evaluation metric. MAP
is commonly used in the field of Information Re-
trieval for evaluating ranked lists because it is sen-
sitive to the entire ranking and it contains both re-
call and precision-oriented aspects. The MAP for
multiple ranked lists is simply the mean value of
average precisions calculated separately for each
ranked list. We define the average precision of a
single ranked list as:
AvgPrec(L) =
|L|
?
r=1
Prec(r)? isFresh(r)
Total # of Correct Instances
1507
where L is a ranked list of extracted instances, r
is the rank ranging from 1 to |L|, Prec(r) is the
precision at rank r, or the percentage of correct
synonyms above rank r (inclusively). isFresh(r)
is a binary function for ensuring that, if a list con-
tains multiple synonyms of the same instance (or
instance pair), we do not evaluate that instance (or
instance pair) more than once. More specifically,
the function returns 1 if a) the synonym at r is cor-
rect, and b) it is the highest-ranked synonym of its
instance in the list; it returns 0 otherwise.
We evaluate the performance of each type of
wrapper by conducting set expansion on the 36
datasets across three languages. For each dataset,
we randomly select two seeds, expand them by
bootstrapping ten iterations (where each iteration
retrieves at most 200 web pages only), and evalu-
ate the final result. We repeat this process three
times for every dataset and report the average
MAP for English, Japanese, and Chinese in Ta-
ble 3. As illustrated, the more restrictive a wrapper
is, the worse it performs. As a result, this indicates
that further restrictions on wrappers of type 5 will
not improve performance.
4 Set Expansion for Binary Relations
4.1 Identifying Wrappers for Binary
Relations
We extend the wrapper construction algorithm de-
scribed in Section 2.1 to support relational set ex-
pansion. The major difference is that we introduce
a third type of context called the middle context
that occurs between the left and right contexts of
a wrapper for separating any two items. We ex-
ecute the same algorithm as before, except that a
seed instance in the algorithm is now a seed in-
stance pair bracketing some middle context (i.e.,
?s
1
?middle? s
2
?).
Given some seed pairs (e.g., Ford and USA),
the algorithm first locates the seeds in some given
documents. For every pair of seeds located, it ex-
tracts their left, middle, and right contexts. The
left and right contexts are inserted into their corre-
sponding tries, while the middle context is inserted
into a list. Every middle context is assigned a flag
indicating whether the two instances bracketing it
were found in the same or reversed order as the
input seed pairs. Every entry in the seed instance
list described previously now stores a pair of in-
stances as one single string (e.g. ?Ford/USA?). An
id stored in a node now matches the index of a pair
of instances as well as a middle context.
Shown below is a mock example document of
which the seed pairs: Ford and USA, Nissan and
Japan, Toyota and Japan are located (and bold-
faced).
GtpKxHnIsSaNoKpjaPaNxjHJgleTuoLpBlcLBxKH
forDEFcuSAxkrpWNapnIkAAHOFoRdawHDaUSauoh
deQsKxHaCuRAoKpJapANxjHdIjWnOxoTOyOTaVaq
jApaNzxKHAUdIEFcgErmANyxkrOyQKxHToYotAoK
pJApaNxjHCRdmtqOVKxHfoRdoKpusAxjHaJASzEi
nSfrlaFoRDLMmpuSaofwNLWxKHtOYotaEFcjAPan
xkrHxQKzrHpoKdGEKtxKHNisSanEFcJApAnxkrEq
After performing the abovementioned proce-
dures on this mock document, we now have con-
text tries that are much more complicated than
those illustrated in Figure 1, as well as a list of
middle contexts similar to the one shown below:
id Seed Pairs r Middle Context
0 Nissan/Japan No oKp
1 Nissan/Japan No EFc
2 Nissan/Japan Yes xkrHxQKzrHpoKd...
4 Toyota/Japan No oKp
6 Toyota/Japan Yes xjHdIjWnOxo
9 Ford/USA No EFc
13 Ford/USA Yes xkrpWNapnIkAAHO
where r indicates if the two instances bracketing
the middle context were found in the reversed or-
der as the input seed pairs. In order to find the
maximally long contextual strings, the ?Intersect?
function in the set expansion pseudo-code pre-
sented in Table 2 needs to be replaced with the
following:
Integers Intersect(Node n
1
, Node n
2
)
Define S = n
1
.indexes ? n
2
.indexes
Return the largest subset s of S such that:
Every index ? s corresponds to same middle context
which returns those seed pairs that are bracketed
by the strings associated with the two input nodes
with the same middle context. A wrapper for re-
lational set expansion, or relational wrapper, is
defined by the left, middle, and right contextual
strings. The relational wrappers constructed from
the mock document given the example seed pairs
are shown below. Notice that Audi/Germany and
Acura/Japan are discovered.
Wrapper: xKH[.1.]EFc[.2.]xkr
Content: audi/germany, ford/usa, nissan/japan,
toyota/japan
Wrapper: KxH[.1.]oKp[.2.]xjH
Content: acura/japan, ford/usa, nissan/japan,
toyota/japan
1508
Dataset ID Item #1 vs. Item #2 Lang. #1 Lang. #2 Size Complete?
US Governor US State/Territory vs. Governor English English 56 Yes
Taiwan Mayor Taiwanese City vs. Mayor Chinese Chinese 26 Yes
NBA Team NBA Team vs. NBA Team Chinese English 30 Yes
Fed. Agency US Federal Agency Acronym vs. Full Name English English 387 No
Car Maker Car Manufacturer vs. Headquartered Country English English 122 No
Table 4: The five relational datasets for evaluating relational set expansion.
Mean Avg. Precision Precision@100
Datasets 1 2 3 4 5 1 2 3 4 5
US Governor 97.4 89.3 89.2 89.3 89.2 55 50 51 50 50
Taiwan Mayor 99.8 95.6 94.3 91.3 90.8 25 25 24 23 23
NBA Team 100.0 99.9 99.9 99.9 99.2 30 30 30 30 30
Fed. Agency 43.7 14.5 5.2 11.1 5.2 96 55 20 40 20
Car Maker 61.7 0.0 0.0 0.0 0.0 74 0 0 0 0
Average 80.5 59.9 57.7 58.3 56.9 56 32 25 29 25
Table 5: Performance of various types of wrappers on the five relational datasets after first iteration.
Mean Avg. Precision Precision@100
Datasets 1 2 3 4 5 1 2 3 4 5
US Governor 98.9 97.0 95.3 94.1 93.9 55 55 54 53 53
Taiwan Mayor 99.8 98.3 96.9 93.8 94.3 25 25 25 24 24
NBA Team 100.0 100.0 99.2 98.4 98.6 30 30 30 30 30
Fed. Agency 65.5 54.5 27.9 55.3 30.0 97 97 61 95 69
Car Maker 81.6 0.0 0.0 0.0 0.0 90 0 0 0 0
Average 89.2 70.0 63.9 68.3 63.4 59 41 34 40 35
Table 6: Performance of various types of wrappers on the five relational datasets after 10
th
iteration.
4.2 Experiments with Binary Relations
For binary relations, we performed the same ex-
periment as with unary relations described in Sec-
tion 3. A relational wrapper is of type t if the
wrapper?s left and right context match t?s con-
straint for left and right respectively, and also
that the wrapper?s middle context match both con-
straints.
For choosing the evaluation datasets for rela-
tional set expansion, we surveyed and obtained a
dozen relationships, from which we randomly se-
lected five of them and present in Table 4. Each
dataset was then manually constructed. For the
last two datasets, since there are too many items,
we tried our best to make the lists as exhaustive as
possible.
To evaluate relational wrappers, we performed
relational set expansion on randomly selected
seeds from the five relational datasets. For every
dataset, we select two seeds randomly and boot-
strap the relational set expansion ten times. The
results after the first iteration are shown in Table 5
and after the tenth iteration in Table 6. When com-
puting precision at 100 for each resulting list, we
kept only the top-most-ranked synonym of every
instance and remove all other synonyms from the
list; this ensures that every instance is unique. No-
tice that for the ?Car Maker? dataset, there exists
no wrappers of types 2 to 5; thus resulting in zero
performance for those wrapper types. In each ta-
ble, the results indicate that character-based wrap-
pers perform the best, while those HTML-based
wrappers that require tight HTML bracketing of
items (type 3 and 5) perform the worse.
In addition, the results illustrate that bootstrap-
ping is effective for expanding relational pairs of
items. As illustrated in Table 6, the result of find-
ing translation pairs of NBA team names is per-
fect, and it is almost perfect for finding pairs of
U.S. states/territories and governors, as well as
Taiwanese cities and mayors. In finding pairs of
acronyms and full names of federal agencies, the
precision at top 100 is nearly perfect (97%). The
results for finding pairs of car makers and coun-
tries is good as well, with a high precision of
90%. For the last two datasets, we believe that
MAP could be improved by increasing the number
of bootstrapping iterations. Table 7 shows some
example wrappers constructed and instances ex-
tracted for wrappers of type 1.
1509
Seeds: kentucky / steve beshear, north dakota / john hoeven
URL: http://wikifoia.pbworks.com/Alaska-Governor-Sarah-Palin
Wrapper: Governor [.2.]">[.1.] Governor
URL: http://blogs.suntimes.com/sweet/2008/02/sweet state dinner for governo.html
Wrapper: <br /> <br /> The Honorable [.2.], Governor of [.1.] <br /> <br />
URL: http://en.wikipedia.org/wiki/United States Senate elections, 2010
Wrapper: " title="Governor of [.1.]">Governor</a> <a href="/wiki/[.2.]" title="
URL: http://ballotbox.governing.com/2008/07/index.html
Wrapper: , [.1.]?s [.2.],
Content: alabama / bob riley, alaska / sarah palin, arizona / janet napolitano, arkansas / mike huckabee, california /
arnold schwarzenegger, colorado / bill ritter, connecticut / mary jodi rell, delaware / ruth ann minner, florida
/ charlie crist, georgia / sonny perdue, hawaii / linda lingle, idaho / butch otter, illinois / rod blagojevich. . .
Seeds: cia / central intelligence agency, usps / united states postal service
URL: http://www1.american.edu/dccampus/links/whitehouse.html
Wrapper: <a href="http://www.[.1.].gov" class="Links2nd">[.2.]</a><span class="Links2nd">
URL: http://www.usembassy.at/en/us/gov.htm
Wrapper: /" target=" blank">[.2.] ([.1.])</a> -
URL: http://www.nationmaster.com/encyclopedia/List-of-United-States-federal-agencies
Wrapper: The [.2.] ([.1.]) is
URL: http://www.nationmaster.com/encyclopedia/List-of-United-States-federal-agencies
Wrapper: </li> <li>[.1.]- <a href="/encyclopedia/[.2.]" onmouseover="pv(event, 2
Content: achp / advisory council on historic preservation, arc / appalachian regional commission, cftc / commod-
ity futures trading commission, cia / central intelligence agency, cms / centers for medicare and medicaid
services, exim bank / export import bank of the united states, ntrc / national transportation research center. . .
Seeds: mazda / japan, venturi / france
URL: http://www.jrfilters.com/filtres/index.php?lng=en
Wrapper: &page=filtres&lng=en">[.1.]&nbsp;&nbsp;&nbsp;([.2.])</option><option value="index.php?
URL: http://www.jrfilters.com/suspensions/index.php?famille=1&lng=en
Wrapper: &lng=en">[.1.]&nbsp;&nbsp;&nbsp;([.2.])</option><option value="index.php?famille=1&rubrique1
URL: http://www.street-car.net/forums/forumdisplay.php?f=10
Wrapper: "><strong>[.1.]</strong></a> </div> <div class="smallfont">Country of origin:[.2.].
URL: http://www.allcarcentral.com/
Wrapper: file.html">[.1.],[.2.]</a><br />
Content: abarth / italy, acura / japan, alfa romeo / italy, aston martin / england, auburn / usa, audi / germany, austin
healey / england, austin / england, auto union / germany, balwin / usa, bandini / italy, bentley / england, bmw
/ germany, brabham / england, bricklin / usa, bristol / england, brm / england, bucciali / france. . .
Table 7: Examples of (type 1) wrappers constructed and instances (contents) extracted.
1510
5 Related Work
In recent years, many research has been done
on extracting relations from free text (e.g., (Pan-
tel and Pennacchiotti, 2006; Agichtein and Gra-
vano, 2000; Snow et al, 2006)); however, al-
most all of them require some language-dependent
parsers or taggers for English, which restrict
the language of their extractions to English only
(or languages that have these parsers). There
has also been work done on extracting relations
from HTML-structured tables (e.g., (Etzioni et al,
2005; Nadeau et al, 2006; Cafarella et al, 2008));
however, they all incorporated heuristics for ex-
ploiting HTML structures; thus, they cannot han-
dle documents written in other mark-up languages.
Extracting relations at character-level from
semi-structured documents has been proposed
(e.g., (Kushmerick et al, 1997),(Brin, 1998)).
In particular, Brin?s approach (DIPRE) is the
most similar to ours in terms of expanding rela-
tional items. One difference is that it requires
maximally-long contextual strings to bracket al
seed occurrences. This technique has been experi-
mentally illustrated to perform worse than SEAL?s
approach on unary relations (Wang and Cohen,
2007). Brin presented five seed pairs of author
names and book titles that he used in the exper-
iment (unfortunately, he did not provide detailed
results). We input the top two seed pairs listed in
his paper into the relational SEAL, performed ten
bootstrapping iterations (took about 3 minutes),
and obtained 26,000 author name/book title pairs
of which the precision at 100 is perfect (100%).
6 Conclusions
In this paper, we have described in detail an al-
gorithm for constructing document-specific wrap-
pers automatically for set expansion. In the exper-
imental results, we have illustrated that character-
based wrappers are better suited than HTML-
based wrappers for the task of set expansion. We
also presented a method that utilizes an additional
middle context for constructing relational wrap-
pers. We also showed that our relational set ex-
pansion approach is language-independent; it can
be applied to non-English and even cross-lingual
seeds and documents. Furthermore, we have il-
lustrated that bootstrapping improves the perfor-
mance of relational set expansion. In the future,
we will explore automatic mining of binary con-
cepts given only the relation (e.g., ?mayor of?).
7 Acknowledgments
This work was supported by the Google Research
Awards program.
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In In Proceedings of the 5th ACM Inter-
national Conference on Digital Libraries, pages 85?
94.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In In WebDB Work-
shop at 6th International Conference on Extending
Database Technology, EDBT98, pages 172?183.
Michael J. Cafarella, Alon Y. Halevy, Daisy Z. Wang,
Eugene W. 0002, and Yang Zhang. 2008. Webta-
bles: exploring the power of tables on the web.
PVLDB, 1(1):538?549.
A. Carlson, J. Betteridge, E.R. Hruschka Junior, and
T.M. Mitchell. 2009. Coupling semi-supervised
learning of categories and relations. In NAACL HLT
Workshop on Semi-supervised Learning for Natural
Language Processing, pages 1?9. Association for
Computational Linguistics.
Oren Etzioni, Michael J. Cafarella, Doug Downey,
Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel S. Weld, and Alexander Yates. 2005.
Unsupervised named-entity extraction from the
web: An experimental study. Artif. Intell.,
165(1):91?134.
N. Kushmerick, D. Weld, and B. Doorenbos. 1997.
Wrapper induction for information extraction. In
Proc. Int. Joint Conf. Artificial Intelligence.
David Nadeau, Peter D. Turney, and Stan Matwin.
2006. Unsupervised named-entity recognition:
Generating gazetteers and resolving ambiguity. In
Luc Lamontagne and Mario Marchand, editors,
Canadian Conference on AI, volume 4013 of Lec-
ture Notes in Computer Science, pages 266?277.
Springer.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automat-
ically harvesting semantic relations. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 113?120, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In ACL ?06: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the ACL, pages 801?
808, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
1511
Partha P. Talukdar, Thorsten Brants, Mark Liberman,
and Fernando Pereira. 2006. A context pattern
induction method for named entity extraction. In
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X).
Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan.
2006. Fast random walk with restart and its appli-
cations. In ICDM, pages 613?622. IEEE Computer
Society.
Richard C. Wang and William W. Cohen. 2007.
Language-independent set expansion of named enti-
ties using the web. In ICDM, pages 342?350. IEEE
Computer Society.
Richard C. Wang and William W. Cohen. 2008. Iter-
ative set expansion of named entities using the web.
In ICDM, pages 1091?1096. IEEE Computer Soci-
ety.
1512
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 443?450, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Extracting Personal Names from Email: Applying Named Entity
Recognition to Informal Text
Einat Minkov and Richard C. Wang
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15217
{einat,rcwang}@cs.cmu.edu
William W. Cohen
Ctr for Automated Learning & Discovery
Carnegie Mellon University
Pittsburgh, PA 15217
wcohen@cs.cmu.edu
Abstract
There has been little prior work on Named
Entity Recognition for ?informal? docu-
ments like email. We present two meth-
ods for improving performance of per-
son name recognizers for email: email-
specific structural features and a recall-
enhancing method which exploits name
repetition across multiple documents.
1 Introduction
Named entity recognition (NER), the identification
of entity names in free text, is a well-studied prob-
lem. In most previous work, NER has been applied
to news articles (e.g., (Bikel et al, 1999; McCal-
lum and Li, 2003)), scientific articles (e.g., (Craven
and Kumlien, 1999; Bunescu and Mooney, 2004)),
or web pages (e.g., (Freitag, 1998)). These genres of
text share two important properties: documents are
written for a fairly broad audience, and writers take
care in preparing documents. Important genres that
do not share these properties include instant messag-
ing logs, newsgroup postings and email messages.
We refer to these genres as ?informal? text.
Informal text is harder to process automatically.
Informal documents do not obey strict grammatical
conventions. They contain grammatical and spelling
errors. Further, since the audience is more restricted,
informal documents often use group- and task-
specific abbreviations and are not self-contained.
Because of these differences, existing NER methods
may require modifications to perform well on infor-
mal text.
In this paper, we investigate NER for informal
text with an experimental study of the problem of
recognizing personal names in email?a task that is
both useful and non-trivial. An application of in-
terest is corpus anonymization. Automatic or semi-
automatic email anonymization should allow using
large amounts of informal text for research purposes,
for example, of medical files. Person-name extrac-
tion and other NER tasks are helpful for automatic
processing of informal text for a large variety of ap-
plications (Culotta et al, 2004; Cohen et al, 2005).
We first present four corpora of email text, anno-
tated with personal names, each roughly compara-
ble in size to the MUC-6 corpus1. We experimen-
tally evaluate the performance of conditional ran-
dom fields (CRF) (Lafferty et al, 2001), a state-
of-the art machine-learning based NER methods on
these corpora. We then turn to examine the special
attributes of email text (vs. newswire) and suggest
venues for improving extraction performance. One
important observation is that email messages often
include some structured, easy-to-recognize names,
such as names within a header, names appearing in
automatically-generated phrases, as well as names in
signature files or sign-offs. We therefore suggest a
set of specialized structural features for email; these
features are shown to significantly improve perfor-
mance on our corpora.
We also present and evaluate a novel method for
exploiting repetition of names in a test corpus. Tech-
niques for exploiting name repetition within docu-
ments have been recently applied to newswire text
1Two of these are publicly available. The others can not be
distributed due to privacy considerations.
443
(e.g., (Humphreys et al, 1998)), scientific abstracts
(e.g., (Bunescu and Mooney, 2004)) and seminar an-
nouncements (Sutton and Mccallum, 2004); how-
ever, these techniques rely on either NP analysis or
capitalization information to pre-identify candidate
coreferent name mentions, features which are not re-
liable in email. Furthermore, we argue that name
repetition in email should be inferred by examining
multiple documents in a corpus, which is not com-
mon practice. We therefore present an alternative
efficient scheme for increasing recall in email, us-
ing the whole corpus. This technique is shown to
always improve recall substantially, and to almost
always improve F1 performance.
2 Corpora
Two email corpora used in our experiments were
extracted from the CSpace email corpus (Kraut et
al., 2004), which contains email messages collected
from a management course conducted at Carnegie
Mellon University in 1997. In this course, MBA stu-
dents, organized in teams of four to six members,
ran simulated companies in different market scenar-
ios. We believe this corpus to be quite similar to
the work-oriented mail of employees of a small or
medium-sized company. This text corpus contains
three header fields: ?From?, ?Subject?, and ?Time?.
Mgmt-Game is a subcorpora consisting of all emails
written over a five-day period. In the experiments,
the first day worth of email was used as a training
set, the fourth for tuning and the fifth day as a test
set. Mgmt-Teams forms another split of this data,
where the training set contains messages between
different teams than in the test set; hence in Mgmt-
Teams, the person names appearing in the test set
are generally different than those that appear in the
training set.
The next two collections of email were extracted
from the Enron corpus (Klimt and Yang, 2004). The
first subset, Enron-Meetings, consists of messages in
folders named ?meetings? or ?calendar?2 . Most but
not all of these messages are meeting-related. The
second subset, Enron-Random, was formed by re-
peatedly sampling a user name (uniformly at random
among 158 users), and then sampling an email from
2with two exceptions: (a) six very large files were removed,
and (b) one very large ?calendar? folder was excluded.
that user (uniformly at random).
Annotators were instructed to include nicknames
and misspelled names, but exclude person names
that are part of an email address and names that are
part of a larger entity name like an organization or
location (e.g., ?David Tepper School of Business?).
The sizes of the corpora are given in Table 1. We
limited training size to be relatively small, reflecting
a real-world scenario.
Corpus # Documents #Words #NamesTrain Tune Test x1000
Mgmt-Teams 120 82 83 105 2,792
Mgmt-Game 120 216 264 140 2,993
Enron-Meetings 244 242 247 204 2,868
Enron-Random 89 82 83 286 5,059
Table 1: Summary of the corpora used in the experiments.
The number of words and names refer to the whole annotated
corpora.
3 Existing NER Methods
In our first set of experiments we apply CRF, a
machine-learning based probabilistic approach to la-
beling sequences of examples, and evaluate it on the
problem of extracting personal names from email.
Learning reduces NER to the task of tagging (i.e.,
classifying) each word in a document. We use a set
of five tags, corresponding to (1) a one-token entity,
(2) the first token of a multi-token entity, (3) the last
token of a multi-token entity, (4) any other token of
a multi-token entity and (5) a token that is not part
of an entity.
The sets of features used are presented in Table
2. All features are instantiated for the focus word, as
well as for a window of 3 tokens to the left and to the
right of the focus word. The basic features include
the lower-case value of a token t, and its capital-
ization pattern, constructed by replacing all capital
letters with the letter ?X?, all lower-case letters with
?x?, all digits with ?9? and compressing runs of the
same letter with a single letter. The dictionary fea-
tures define various categories of words including
common words, first names, last names 3 and ?roster
names? 4 (international names list, where first and
3We used US Census? lists of the most com-
mon first and last names in the US, available from
http://www.census.gov/genealogy/www/freqnames.html
4A dictionary of 16,623 student names across the country,
obtained as part of the RosterFinder project (Sweeney, 2003)
444
Basic Features
t, lexical value, lowercase (binary form, e.g. f(t=?hello?)=1)
capitalization pattern of t (binary form, e.g. f(t.cap=x+)=1)
Dictionary Features
inCommon: t in common words dictionary
inFirst: t in first names dictionary
inLast: t in last names dictionary
inRoster: t in roster names dictionary
First: inFirst ? ?isLast ? ?inCommon
Last: ?inFirst ? inLast ? ?inCommon
Name: (First ? Last ? inRoster) ? ? inCommon
Title: t in a personal prefixes/suffixes dictionary
Org: t in organization suffixes dictionary
Loc: t in location suffixes dictionary
Email Features
t appears in the header
t appears in the ?from? field
t is a probable ?signoff?
(? after two line breaks and near end of message)
t is part of an email address (regular expression)
does the word starts a new sentence
(? capitalized after a period, question or exclamation mark)
t is a probable initial (X or X.)
t followed by the bigram ?and I?
t capitalized and followed by a pronoun within 15 tokens
Table 2: Feature sets
last names are mixed.) In addition, we constructed
some composite dictionary features, as specified in
Table 2: for example, a word that is in the first-name
dictionary and is not in the common-words or last-
name dictionaries is designated a ?sure first name?.
The common-words dictionary used consists of
base forms, conjugations and plural forms of com-
mon English words, and a relatively small ad-hoc
dictionary representing words especially common in
email (e.g., ?email?, ?inbox?). We also use small
manually created word dictionaries of prefixes and
suffixes indicative of persons (e.g., ?mr?, ?jr?), loca-
tions (e.g., ?ave?) and organizations (e.g., ?inc?).
Email structure features: We perform a simplified
document analysis of the email message and use this
to construct some additional features. One is an in-
dicator as to whether a token t is equal to some to-
ken in the ?from? field. Another indicates whether
a token t in the email body is equal to some token
appearing in the whole header. An indicator feature
based on a regular expression is used to mark tokens
that are part of a probable ?sign-off? (i.e., a name at
the end of a message). Finally, since the annotation
rules do not consider email addresses to be names,
we added an indicator feature for tokens that are in-
side an email address.
l.2.mr l.1.president
l.2.mrs l.2.dr
l.1.jr r.2.who
l.1.judge r.2.jr
r.3.staff l.3.by
l.2.ms r.3.president
r.2.staff l.3.by
r.1.family l.3.rep
l.3.says l.2.rep
r.3.reporter r.1.administration
l.1.by r.2.home
l.2.by r.1.or
l.3.name l.1.with
l.2.name l.1.thanks
l.3.by r.1.picked
r.3.his l.3.meet
r.1.ps r.1.started
r.3.home r.1.told
r.1.and l.2.prof
l.1.called l.2.email
Figure 1: Predictive contexts for personal-name words for
MUC-6 (left) and Mgmt-Game (right) corpora. A features is
denoted by its direction comparing to the focus word (l/r), offset
and lexical value.
We experimented with features derived from POS
tags and NP-chunking of the email, but found the
POS assignment too noisy to be useful. We did in-
clude some features based on approximate linguistic
rules. One rule looks for capitalized words that are
not common words and are followed by a pronoun
within a distance of up to 15 tokens. (As an exam-
ple, consider ?Contact Puck tomorrow. He should be
around.?). Another rule looks for words followed by
the bigram ?and I?. As is common for hand-coded
NER rules, both these rules have high precision and
low recall.
3.1 Email vs Newswire
In order to explore some of the differences between
email and newswire NER problems, we stripped all
header fields from the Mgmt-Game messages, and
trained a model (using basic features only) from the
resulting corpus of email bodies. Figure 1 shows the
features most indicative of a token being part of a
name in the models trained for the Mgmt-Game and
MUC-6 corpora. To make the list easier to interpret,
it includes only the features corresponding to tokens
surrounding the focus word.
As one might expect, the important features from
the MUC-6 dataset are mainly formal name titles
such as ?mr?, ?mrs?, and ?jr?, as well as job ti-
tles and other pronominal modifiers such as ?pres-
ident? and ?judge?. However, for the Mgmt-Game
corpus, most of the important features are related
to email-specific structure. For example, the fea-
tures ?left.1.by? and ?left.2.by? are often associated
with a quoted excerpt from another email message,
which in the Mgmt-Game corpus is often marked
by mailers with text like ?Excerpts from mail: 7-
445
Sep-97 Re: paper deadline by Richard Wang?. Sim-
ilarly, features like ?left.1.thanks? and ?right.1.ps?
indicate a ?signoff? section of an email, as does
?right.2.home? (which often indicates proximity to
a home phone number appearing in a signature).
3.2 Experimental Results
We now turn to evaluate the usefulness of the fea-
ture sets described above. Table 3 gives entity-level
F1 performance 5 for CRF trained models for all
datasets, using the basic features alone (B); the ba-
sic and email-tailored features (B+E); the basic and
dictionary features (B+D); and, all of the feature sets
combined (B+D+E). All feature sets were tuned us-
ing the Mgmt-Game validation subset. The given
results relate to previously unseen test sets.
Dataset B B+E B+D B+D+E
Mgmt-Teams 68.1 75.7 82.0 87.9
Mgmt-Game 79.2 84.2 90.7 91.9
Enron-Meetings 59.0 71.5 78.6 76.9
Enron-Random 68.1 70.2 72.9 76.2
Table 3: F1 entity-leavel performance for the sets of features,
across all datasets, with CRF training.
The results show that the email-specific features
are very informative. In addition, they show that
the dictionary features are especially useful. This
can be explained by the relatively weak contextual
evidence in email. While dictionaries are useful in
named entities extraction in general, they are in fact
more essential when extracting names from email
text, where many name mentions are part of headers,
names lists etc. Finally, the results for the combined
feature set are superior in most cases to any subset
of the features.
Overall the level of performance using all fea-
tures is encouraging, considering the limited training
set size. Performance on Mgmt-Teams is somewhat
lower than for Mgmt-Game mainly because (by de-
sign) there is less similarity between training and
test sets with this split. Enron emails seem to be
harder than Mgmt-Game emails, perhaps because
they include fewer structured instances of names.
Enron-Meetings emails also contain a number of
constructs that were not encountered in the Mgmt-
Game corpus, notably lists (e.g., of people attending
a meeting), and also include many location and or-
5No credit awarded for partially correct entity boundaries.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 5  10  15  20  25  30  35
Mgmt Game
Enron-Meetings
Enron-Random
MUC-6
Figure 2: Cumulative percentage of person-name tokens w
that appear in at most K distinct documents as a function of K.
ganization names, which are rare in Mgmt-Game. A
larger set of dictionaries might improve performance
for the Enron corpora.
4 Repetition of named entities in email
In the experiments described above, the extractors
have high precision, but relatively low recall. This
typical behavior suggests that some sort of recall-
enhancing procedure might improve overall perfor-
mance.
One family of recall-enhancing techniques are
based on looking for multiple occurrences of names
in a document, so that names which occur in am-
biguous contexts will be more likely to be recog-
nized. It is an intuitive assumption that the ways in
which names repeat themselves in a corpus will be
different in email and newswire text. In news stories,
one would expect repetitions within a single docu-
ment to be common, as a means for an author to es-
tablish a shared context with the reader. In an email
corpus, one would expect names to repeat more fre-
quently across the corpus, in multiple documents?
at least when the email corpus is associated with a
group that works together closely. In this section we
support this conjecture with quantitative analysis.
In a first experiment, we plotted the percentage
of person-name tokens w that appear in at most
K distinct documents as a function of K. Figure
2 shows this function for the Mgmt-Game, MUC-
6, Enron-Meetings, and Enron-Random datasets.
There is a large separation between MUC-6 and
Mgmt-Game, the most workgroup-oriented email
corpus. In MUC-6, for instance, almost 80% of the
446
Single-Document Repetition
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Mgmt Game Mgmt
Teams
Enron
Meetings
Enron
Random
MUC-6
To
ke
n
 
R
ec
al
l
SDR
CRF
SDR+CRF
(a) SDR
Multiple-Document Repetition
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Mgmt Game Mgmt
Teams
Enron
Meetings
Enron
Random
MUC-6
To
ke
n
 
R
ec
al
l
MDR
CRF
MDR+CRF
(b) MDR
Figure 3: Upper bounds on recall and recall improvements
associated with methods that look for terms that re-occur within
a single document (SDR) or across multiple documents (MDR).
names appear in only a single document, while in
Mgmt-Game, only 30% of the names appear in only
a single document. At the other extreme, in MUC-6,
only 1.3% of the names appear in 10 or more docu-
ments, while in Mgmt-Game, almost 20% do. The
Enron-Random and Enron-Meetings datasets show
distributions of names that are intermediate between
Mgmt-Game and MUC-6.
As a second experiment, we implemented two
very simple extraction rules. The single document
repetition (SDR) rule marks every token that oc-
curs more than once inside a single document as a
name. Adding tokens marked by the SDR rule to
the tokens marked by the learned extractor generates
a new extractor, which we will denote SDR+CRF.
Thus, the recall of SDR+CRF serves as an upper
bound on the token recall6 of any recall-enhancing
6Token level recall is recall on the task of classifying tokens
as inside or outside an entity name.
method that improves the extractor by exploiting
repetition within a single document. Analogously,
the multiple document repetition (MDR) rule marks
every token that occurs in more than one document
as a name. Again, the token recall of MDR+CRF
rule is an upper bound on the token recall of any
recall-enhancing method that exploits token repeti-
tion across multiple documents.
The left bars in Figure 3 show the recall obtained
by the SDR (top) and the MDR rule (bottom). The
MDR rule has highest recall for the two Mgmt cor-
pora, and lowest recall for the MUC-6 corpus. Con-
versely, for the SDR rule, the highest recall level
obtained is for MUC-6. The middle bars show the
token recall obtained by the CRF extractor, using
all features. The right bars show the token recall
of the SDR+CRF and MDR+CRF extractors. Com-
paring them to the other bars, we see that the maxi-
mal potential recall gain from a SDR-like method is
on MUC-6. For MDR-like methods, there are large
potential gains on the Mgmt corpora as well as on
Enron-Meetings and Enron-Random to a lesser de-
gree. This probably reflects the fact that the Enron
corpora are from a larger and more weakly interact-
ing set of users, compared to the Mgmt datasets.
These results demonstrate the importance of ex-
ploiting repetition of names across multiple docu-
ments for entity extraction from email.
5 Improving Recall With Inferred
Dictionaries
Sequential learners of the sort used here classify to-
kens from each document independently; moreover,
the classification of a word w is independent of the
classification of other occurrences of w elsewhere in
the document. That is, the fact that a word w has ap-
peared somewhere in a context that clearly indicates
that it is a name does not increase the probability that
it will be classified as a name in other, more ambigu-
ous contexts.
Recently, sequential learning methods have been
extended to directly utilize information about name
co-occurrence in learning the sequential classifier.
This approach provides an elegant solution to mod-
eling repetition within a single document. However,
it requires identifying candidate related entities in
advance, applying some heuristic. Thus, Bunescu &
447
Mooney (2004) link between similar NPs (requiring
their head to be identical), and Sutton and Mccallum
(2004) connect pairs of identical capitalized words.
Given that in email corpora capitalization patterns
are not followed to a large extent, there is no ad-
equate heuristic that would link candidate entities
prior to extraction. Further, it is not clear if a col-
lective classification approach can scale to modeling
multiple-document repetition.
We suggest an alternative approach of recall-
enhancing name matching, which is appropriate for
email. Our approach has points of similarity to
the methods described by Stevenson and Gaizauskas
(2000), who suggest matching text against name dic-
tionaries, filtering out names that are also common
words or appear as non-names in high proportion
in the training data. The approach described here
is more systematic and general. In a nutshell, we
suggest applying the noisy dictionary of predicted
names over the test corpus, and use the approximate
(predicted) name to non-name proportions over the
test set itself to filter out ambiguous names. There-
fore, our approach does not require large amount of
annotated training data. It also does not require word
distribution to be similar between train and test data.
We will now describe our approach in detail.
5.1 Matching names from dictionary
First, we construct a dictionary comprised of all
spans predicted as names by the learned model. For
personal names, we suggest expanding this dictio-
nary further, using a transformation scheme. Such a
scheme would construct a family of possible varia-
tions of a name n: as an example, Figure 4 shows
name variations created for the name span ?Ben-
jamin Brown Smith?. Once a dictionary is formed,
a single pass is made through the corpus, and ev-
ery longest match to some name-variation is marked
as a name7. It may be that a partial name span n1
identified by the extractor is subsumed by the full
name span n2 identified by the dictionary-matching
scheme. In this case, entity-level precision is in-
creased, having corrected the entity?s boundaries.
7Initials-only variants of a name, e.g., ?bs? in Figure 4 are
marked as a name only if the ?inSignoff? feature holds?i.e., if
they appear near the end of a message in an apparent signature.
benjamin brown smith benjamin-brown-s. b. brown s. bbs
benjamin-brown smith benjamin-b. s. b. b. smith bs
benjamin brown-smith benjamin-smith b. brown-s.
benjamin-brown-smith benjamin smith benjamin
benjamin brown s. b. brown smith brown
benjamin-b. smith benjamin b. s. smith
benjamin b. smith b. brown-smith b. smith
benjamin brown-s. benjamin-s. b. b. s
benjamin-brown s. benjamin s. b. s.
Figure 4: Names variants created from the name ?Benjamin
Brown Smith?
5.2 Dictionary-filtering schemes
The noisy dictionary-matching scheme is suscepti-
ble to false positives. That is, some words predicted
by the extractor to be names are in fact non-names.
Presumably, these non-names could be removed by
simply eliminating low-confidence predictions of
the extractor; however, ambiguous words ?that are
not exclusively personal names in the corpus? may
need to be identified and removed as well. We note
that ambiguity better be evaluated in the context of
the corpus. For example, ?Andrew? is a common
first name, and may be confidently (and correctly)
recognized as one by the extractor. However, in the
Mgmt-Game corpus, ?Andrew? is also the name of
an email server, and most of the occurrences of this
name in this corpus are not personal names. The
high frequency of the word ?Andrew? in the cor-
pus, coupled with the fact that it is only sometimes a
name, means that adding this word to the dictionary
leads to a substantial drop in precision.
We therefore suggest a measure for filtering the
dictionary. This measure combines two metrics. The
first metric, predicted frequency (PF), estimates the
degree to which a word appears to be used consis-
tently as a name throughout the corpus:
PF (w) ? cpf(w)ctf(w)
where cpf(w) denotes the number of times that a
word w is predicted as part of a name by the extrac-
tor, and ctf(w) is the number of occurrences of the
word w in the entire test corpus (we emphasize that
estimating this statistic based on test data is valid, as
it is fully automatic ?blind? procedure).
Predicted frequency does not assess the likely cost
of adding a word to a dictionary: as noted above,
ambiguous or false dictionary terms that occur fre-
quently will degrade accuracy. A number of statis-
tics could be used here; for instance, practitioners
448
sometimes filter a large dictionary by simply dis-
carding all words that occur more than k times in a
test corpus. We elected to use the inverse document
frequency (IDF) of w to measure word frequency:
IDF (w) ?
log(N+0.5df(w) )
log(N + 1)
Here df(w) is the number of documents that contain
a word w, and N is the total number of documents
in the corpus. Inverse document frequency is often
used in the field of information retrieval (Allan et al,
1998), and the formula above has the virtue of being
scaled between 0 and 1 (like our PF metric) and of
including some smoothing. In addition to bounding
the cost of a dictionary entry, the IDF formula is in
itself a sensible filter, since personal names will not
appear as frequently as common English words.
The joint filter combines these two multiplica-
tively, with equal weights:
PF.IDF (w) : PF (w) ? IDF (w)
PF.IDF takes into consideration both the probability
of a word being a name, and how common it is in
the entire corpus. Words that get low PF.IDF scores
are therefore either words that are highly ambiguous
in the corpus (as derived from the extractors? pre-
dictions) or are common words, which were inaccu-
rately predicted as names by the extractor.
In the MDR method of Figure 3, we imposed
an artificial requirement that words must appear in
more than one document. In the method described
here, there is no such requirement: indeed, words
that appear in a small number of documents are
given higher weights, due to the IDF factor. Thus
this approach exploits both single-document and
multiple-document repetitions.
In a set of experiments that are not described here,
the PF.IDF measure was found to be robust to pa-
rameter settings, and also preferable to its separate
components in improving recall at minimal cost in
precision. As described, the PF.IDF values per word
range between 0 and 1. One can vary the threshold,
under which a word is to be removed from the dic-
tionary, to control the precision-recall trade-off. We
tuned the PF.IDF threshold using the validation sub-
sets, optimizing entity-level F1 (a threshold of 0.16
was found optimal).
In summary, our recall-enhancing strategy is as
follows:
1. Learn an extractor E from the training corpus Ctrain .
2. Apply the extractor E to a test corpus Ctest to assign a
preliminary labeling.
3. Build a dictionary S?? including the names n such that
(a) n is extracted somewhere in the preliminary label-
ing of the test corpus, or is derived from an extracted
name applying the name transformation scheme and (b)
PF.IDF (n) > ??.
4. Apply the dictionary-matching scheme of Section 5.1, us-
ing the dictionary S?? to augment the preliminary label-
ing, and output the result.
5.3 Experiments with inferred dictionaries
Table 4 shows results using the method described
above. We consider all of the email corpora and the
CRF learner, trained with the full feature set. The
results are given in terms of relative change, com-
pared to the baseline results generated by the extrac-
tors (scoreresult/scorebaseline ? 1) and final value.
As expected, recall is always improved. Entity-
level F1 is increased as well, as recall is increased
more than precision is decreased. The largest im-
provements are for the Mgmt corpora ?the two e-
mail datasets shown to have the largest potential im-
provement from MDR-like methods in Figure 3. Re-
call improvements are more modest for the Enron
datasets, as was anticipated by the MDR analysis.
Another reason for the gap is that extractor baseline
performance is lower for the Enron datasets, so that
the Enron dictionaries are noisier.
As detailed in Section 2, the Mgmt-Teams dataset
was constructed so that the names in the training
and test set have only minimal overlap. The perfor-
mance improvement on this dataset shows that rep-
etition of mostly-novel names can be detected using
our method. This technique is highly effective when
names are novel, or dense, and is optimal when ex-
tractor baseline precision is relatively high.
Dataset Precision Recall F1
Mgmt-Teams -0.9% / 92.9 +8.5% / 89.8 +3.9% / 91.3
Mgmt-Game -0.8% / 94.5 +8.4% / 96.2 +3.8% / 95.4
Enron-Meetings -2.5% / 81.1 +4.7% / 74.9 +1.2% / 77.9
Enron-Random -3.8% / 79.2 +4.9% / 74.3 +0.7% / 76.7
Table 4: Entity-level relative improvement and final result,
applying name-matching on models trained with CRF and the
full feature set (F1 baseline given in Table 3).
449
6 Conclusion
This work applies recently-developed sequential
learning methods to the task of extraction of named
entities from email. This problem is of interest as an
example of NER from informal text?text that has
been prepared quickly for a narrow audience.
We showed that informal text has different char-
acteristics from formal text such as newswire. Anal-
ysis of the highly-weighted features selected by the
learners showed that names in informal text have
different (and less informative) types of contextual
evidence. However, email also has some structural
regularities which make it easier to extract personal
names. We presented a detailed description of a set
of features that address these regularities and signif-
icantly improve extraction performance on email.
In the second part of this paper, we analyzed
the way in which names repeat in different types
of corpora. We showed that repetitions within a
single document are more common in newswire
text, and that repetitions that span multiple docu-
ments are more common in email corpora. Addi-
tional analysis confirms that the potential gains in
recall from exploiting multiple-document repetition
is much higher than the potential gains from exploit-
ing single-document repetition.
Based on this insight, we introduced a simple and
effective method for exploiting multiple-document
repetition to improve an extractor. One drawback of
the recall-enhancing approach is that it requires the
entire test set to be available: however, our test sets
are of only moderate size (83 to 264 documents),
and it is likely that a similar-size sample of unlabeled
data would be available in many practical applica-
tions. The approach substantially improves recall
and often improves F1 performance; furthermore, it
can be easily used with any NER method.
Taken together, extraction performance is sub-
stantially improved by this approach. The improve-
ments seem to be strongest for email corpora col-
lected from closely interacting groups. On the
Mgmt-Teams dataset, which was designed to reduce
the value of memorizing specific names appearing
in the training set, F1 performance is improved from
68.1% for the out-of-the-box system (or 82.0% for
the dictionary-augmented system) to 91.3%. For the
less difficult Mgmt-Game dataset, F1 performance
is improved from 79.2% for an out-of-the-box CRF-
based NER system (or 90.7% for a CRF-based sys-
tem that uses several large dictionaries) to 95.4%.
As future work, experiments should be expanded to
include additional entity types and other types of in-
formal text, such as blogs and forum postings.
References
J. Allan, J. Callan, W.B. Croft, L. Ballesteros, D. Byrd,
R. Swan, and J. Xu. 1998. Inquery does battle with trec-
6. In TREC-6.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. 1999. An
algorithm that learns what?s in a name. Machine Learning,
34:211?231.
R. Bunescu and R. J. Mooney. 2004. Relational markov net-
works for collective information extraction. In ICML-2004
Workshop on Statistical Relational Learning.
W. W. Cohen, E. Minkov, and A. Tomasic. 2005. Learning to
undertand website update requests. In IJCAI-05.
M. Craven and J. Kumlien. 1999. Constructing biologi-
cal knowledge bases by extracting information from text
sources. In ISMB-99.
A. Culotta, R. Bekkerman, and A. McCallum. 2004. Extracting
social networks and contact information from email and the
web. In CEAS-04.
D. Freitag. 1998. Information extraction from html: applica-
tion of a general machine learning approach. In AAAI-98.
K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck,
B. Mitchell, H. Cunningham, and Y. Wilks. 1998. Descrip-
tion of the LASIE-II system as used for MUC-7.
B. Klimt and Y. Yang. 2004. Introducing the Enron corpus. In
CEAS-04.
R. E. Kraut, S. R. Fussell, F. J. Lerch, and J. A. Espinosa. 2004.
Coordination in teams: evi-dence from a simulated manage-
ment game. To appear in the Journal of Organizational Be-
havior.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML-01.
A. McCallum and W. Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In CoNLL-2003.
M. Stevenson and R. Gaizauskas. 2000. Using corpus-derived
names lists for named entities recognition. In NAACL-2000.
C. Sutton and A. Mccallum. 2004. Collective segmentation
and labeling of distant entities in information extraction. In
ICML workshop on Statistical Relational Learning.
L. Sweeney. 2003. Finding lists of people on the web.
Technical Report CMU-CS-03-168, CMU-ISRI-03-104.
http://privacy.cs.cmu.edu/dataprivacy/ projects/rosterfinder/.
450
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 93?96,
New York, June 2006. c?2006 Association for Computational Linguistics
NER Systems that Suit User?s Preferences: Adjusting the Recall-Precision
Trade-off for Entity Extraction
Einat Minkov, Richard C. Wang
Language Technologies
Institute
Carnegie Mellon University
einat,rcwang@cs.cmu.edu
Anthony Tomasic
Inst. for Software Research
International
Carnegie Mellon University
tomasic@cs.cmu.edu
William W. Cohen
Machine Learning Dept.
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
We describe a method based on ?tweak-
ing? an existing learned sequential classi-
fier to change the recall-precision tradeoff,
guided by a user-provided performance
criterion. This method is evaluated on
the task of recognizing personal names in
email and newswire text, and proves to be
both simple and effective.
1 Introduction
Named entity recognition (NER) is the task of iden-
tifying named entities in free text?typically per-
sonal names, organizations, gene-protein entities,
and so on. Recently, sequential learning methods,
such as hidden Markov models (HMMs) and con-
ditional random fields (CRFs), have been used suc-
cessfully for a number of applications, including
NER (Sha and Pereira, 2003; Pinto et al, 2003; Mc-
callum and Lee, 2003). In practice, these methods
provide imperfect performance: precision and re-
call, even for well-studied problems on clean well-
written text, reach at most the mid-90?s. While
performance of NER systems is often evaluated in
terms of F1 measure (a harmonic mean of preci-
sion and recall), this measure may not match user
preferences regarding precision and recall. Further-
more, learned NER models may be sub-optimal also
in terms of F1, as they are trained to optimize other
measures (e.g., loglikelihood of the training data for
CRFs).
Obviously, different applications of NER have
different requirements for precision and recall. A
system might require high precision if it is designed
to extract entities as one stage of fact-extraction,
where facts are stored directly into a database. On
the other hand, a system that generates candidate ex-
tractions which are passed to a semi-automatic cu-
ration system might prefer higher recall. In some
domains, such as anonymization of medical records,
high recall is essential.
One way to manipulate an extractor?s precision-
recall tradeoff is to assign a confidence score to each
extracted entity and then apply a global threshold to
confidence level. However, confidence thresholding
of this sort cannot increase recall. Also, while confi-
dence scores are straightforward to compute in many
classification settings, there is no inherent mecha-
nism for computing confidence of a sequential ex-
tractor. Culotta and McCallum (2004) suggest sev-
eral methods for doing this with CRFs.
In this paper, we suggest an alternative simple
method for exploring and optimizing the relation-
ship between precision and recall for NER systems.
In particular, we describe and evaluate a technique
called ?extractor tweaking? that optimizes a learned
extractor with respect to a specific evaluation met-
ric. In a nutshell, we directly tweak the threashold
term that is part of any linear classifier, including se-
quential extractors. Though simple, this approach
has not been empirically evaluated before, to our
knowledge. Further, although sequential extractors
such as HMMs and CRFs are state-of-the-art meth-
ods for tasks like NER, there has been little prior re-
search about tuning these extractors? performance to
suit user preferences. The suggested algorithm op-
timizes the system performance per a user-provided
93
evaluation criterion, using a linear search procedure.
Applying this procedure is not trivial, since the un-
derlying function is not smooth. However, we show
that the system?s precision-recall rate can indeed be
tuned to user preferences given labelled data using
this method. Empirical results are presented for a
particular NER task?recognizing person names, for
three corpora, including email and newswire text.
2 Extractor tweaking
Learning methods such as VP-HMM and CRFs op-
timize criteria such as margin separation (implicitly
maximized by VP-HMMs) or log-likelihood (ex-
plicitly maximized by CRFs), which are at best indi-
rectly related to precision and recall. Can such learn-
ing methods be modified to more directly reward a
user-provided performance metric?
In a non-sequential classifier, a threshold on confi-
dence can be set to alter the precision-recall tradeoff.
This is nontrivial to do for VP-HMMs and CRFs.
Both learners use dynamic programming to find the
label sequence y = (y1, . . . , yi, . . . , yN ) for a word
sequence x = (x1, . . . , xi, . . . , xN ) that maximizes
the function W ?
?
i f(x, i, yi?1, yi) , where W is
the learned weight vector and f is a vector of fea-
tures computed from x, i, the label yi for xi, and the
previous label yi?1. Dynamic programming finds
the most likely state sequence, and does not output
probability for a particular sub-sequence. (Culotta
and McCallum, 2004) suggest several ways to gen-
erate confidence estimation in this framework. We
propose a simpler approach for directly manipulat-
ing the learned extractor?s precision-recall ratio.
We will assume that the labels y include one label
O for ?outside any named entity?, and let w0 be the
weight for the feature f0, defined as follows:
f0(x, i, yi?1, yi) ?
{
1 if yi = O
0 else
If no such feature exists, then we will create one.
The NER based on W will be sensitive to the value
of w0: large negative values will force the dynamic
programming method to label tokens as inside enti-
ties, and large positive values will force it to label
fewer entities1.
1We clarify that w0 will refer to feature f0 only, and not to
other features that may incorporate label information.
We thus propose to ?tweak? a learned NER by
varying the single parameter w0 systematically so as
to optimize some user-provided performance metric.
Specifically, we tune w0 using a a Gauss-Newton
line search, where the objective function is itera-
tively approximated by quadratics.2 We terminate
the search when two adjacent evaluation results are
within a 0.01% difference3.
A variety of performance metrics might be imag-
ined: for instance, one might wish to optimize re-
call, after applying some sort of penalty for pre-
cision below some fixed threshold. In this paper
we will experiment with performance metrics based
on the (complete) F-measure formula, which com-
bines precision and recall into a single numeric value
based on a user-provided parameter ?:
F (?, P,R) = (?
2 + 1)PR
?2P +R
A value of ? > 1 assigns higher importance to re-
call. In particular, F2 weights recall twice as much
as precision. Similarly, F0.5 weights precision twice
as much as recall.
We consider optimizing both token- and entity-
level F? ? awarding partial credit for partially ex-
tracted entities and no credit for incorrect entity
boundaries, respectively. Performance is optimized
over the dataset on which W was trained, and tested
on a separate set. A key question our evaluation
should address is whether the values optimized for
the training examples transfer well to unseen test ex-
amples, using the suggested approximate procedure.
3 Experiments
3.1 Experimental Settings
We experiment with three datasets, of both email
and newswire text. Table 1 gives summary statis-
tics for all datasets. The widely-used MUC-6 dataset
includes news articles drawn from the Wall Street
Journal. The Enron dataset is a collection of emails
extracted from the Enron corpus (Klimt and Yang,
2004), where we use a subcollection of the mes-
sages located in folders named ?meetings? or ?cal-
endar?. The Mgmt-Groups dataset is a second email
2from http://billharlan.com/pub/code/inv.
3In the experiments, this is usually within around 10 itera-
tions. Each iteration requires evaluating a ?tweaked? extractor
on a training set.
94
collection, extracted from the CSpace email cor-
pus, which contains email messages sent by MBA
students taking a management course conducted at
Carnegie Mellon University in 1997. This data was
split such that its test set contains a different mix of
entity names comparing to training exmaples. Fur-
ther details about these datasets are available else-
where (Minkov et al, 2005).
# documents # names
Train Test # tokens per doc.
MUC-6 347 30 204,071 6.8
Enron 833 143 204,423 3.0
Mgmt-Groups 631 128 104,662 3.7
Table 1: Summary of the corpora used in the experiments
We used an implementation of Collins? voted-
percepton method for discriminatively training
HMMs (henceforth, VP-HMM) (Collins, 2002) as
well as CRF (Lafferty et al, 2001) to learn a NER.
Both VP-HMM and CRF were trained for 20 epochs
on every dataset, using a simple set of features such
as word identity and capitalization patterns for a
window of three words around each word being clas-
sified. Each word is classified as either inside or out-
side a person name.4
3.2 Extractor tweaking Results
Figure 1 evaluates the effectiveness of the optimiza-
tion process used by ?extractor tweaking? on the
Enron dataset. We optimized models for F? with
different values of ?, and also evaluated each op-
timized model with different F? metrics. The top
graph shows the results for token-level F? , and the
bottom graph shows entity-level F? behavior. The
graph illustates that the optimized model does in-
deed roughly maximize performance for the target
? value: for example, the token-level F? curve for
the model optimized for ? = 0.5 indeed peaks at
? = 0.5 on the test set data. The optimization is
only roughly accurate5 for several possible reasons:
first, there are differences between train and test sets;
in addition, the line search assumes that the perfor-
mance metric is smooth and convex, which need
not be true. Note that evaluation-metric optimiza-
tion is less successful for entity-level performance,
4This problem encoding is basic. However, in the context of
this paper we focus on precision-recall trade-off in the general
case, avoiding settings? optimization.
5E.g, the token-level F2 curve peaks at ? = 5.
 50
 55
 60
 65
 70
 75
 80
 85
 90
5.02.01.00.50.2
F(
Be
ta)
Beta
0.2
0.5
1.0
2.0
5.0
 50
 55
 60
 65
 70
 75
 80
 85
 90
5.02.01.00.50.2
F(
Be
ta)
Beta
0.2
0.5
1.0
2.0
5.0
Figure 1: Results of token-level (top) and entity-level (bot-
tom) optimization for varying values of ?, for the Enron dataset,
VP-HMM. The y-axis gives F in terms of ?. ? (x-axis) is given
in a logarithmic scale.
which behaves less smoothly than token-level per-
formance.
Token Entity
? Prec Recall Prec Recall
Baseline 93.3 76.0 93.6 70.6
0.2 100 53.2 98.2 57.0
0.5 95.3 71.1 94.4 67.9
1.0 88.6 79.4 89.2 70.9
2.0 81.0 83.9 81.8 70.9
5.0 65.8 91.3 69.4 71.4
Table 2: Sample optimized CRF results, for the MUC-6
dataset and entity-level optimization.
Similar results were obtained optimizing baseline
CRF classifiers. Sample results (for MUC-6 only,
due to space limitations) are given in Table 2, opti-
mizing a CRF baseline for entity-level F? . Note that
as ? increases, recall monotonically increases and
precision monotonically falls.
The graphs in Figure 2 present another set of re-
sults with a more traditional recall-precision curves.
The top three graphs are for token-level F? opti-
mization, and the bottom three are for entity-level
optimization. The solid lines show the token-level
and entity-level precision-recall tradeoff obtained by
95
MUC-6 Enron M.Groups
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Pr
ec
is
io
n
Recall
Token-level
Entity-level
Token-level baseline
Entity-level baseline
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Pr
ec
is
io
n
Recall
Token-level
Entity-level
Token-level baseline
Entity-level baseline
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
Figure 2: Results for the evaluation-metric model optimization. The top three graphs are for token-level F (?) optimization,
and the bottom three are for entity-level optimization. Each graph shows the baseline learned VP-HMM and evaluation-metric
optimization for different values of ?, in terms of both token-level and entity-level performance.
varying6 ? and optimizing the relevant measure for
F? ; the points labeled ?baseline? show the precision
and recall in token and entity level of the baseline
model, learned by VP-HMM. These graphs demon-
strate that extractor ?tweaking? gives approximately
smooth precision-recall curves, as desired. Again,
we note that the resulting recall-precision trade-
off for entity-level optimization is generally less
smooth.
4 Conclusion
We described an approach that is based on mod-
ifying an existing learned sequential classifier to
change the recall-precision tradeoff, guided by a
user-provided performance criterion. This approach
not only allows one to explore a recall-precision
tradeoff, but actually allows the user to specify a
performance metric to optimize, and optimizes a
learned NER system for that metric. We showed
that using a single free parameter and a Gauss-
Newton line search (where the objective is itera-
tively approximated by quadratics), effectively op-
timizes two plausible performance measures, token-
6We varied ? over the values 0.2, 0.5, 0.8, 1, 1.2, 1.5, 2, 3
and 5
level F? and entity-level F? . This approach is in
fact general, as it is applicable for sequential and/or
structured learning applications other than NER.
References
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In EMNLP.
A. Culotta and A. McCallum. 2004. Confidence estimation for
information extraction. In HLT-NAACL.
B. Klimt and Y. Yang. 2004. Introducing the Enron corpus. In
CEAS.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML.
A. Mccallum and W. Lee. 2003. early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In CONLL.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Extracting
personal names from emails: Applying named entity recog-
nition to informal text. In HLT-EMNLP.
D. Pinto, A. Mccallum, X. Wei, and W. B. Croft. 2003. table
extraction using conditional random fields. In ACM SIGIR.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In HLT-NAACL.
96
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 441?449,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Automatic Set Instance Extraction using the Web
Richard C. Wang
Language Technologies Institute
Carnegie Mellon University
rcwang@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
An important and well-studied problem is
the production of semantic lexicons from
a large corpus. In this paper, we present
a system named ASIA (Automatic Set In-
stance Acquirer), which takes in the name
of a semantic class as input (e.g., ?car
makers?) and automatically outputs its in-
stances (e.g., ?ford?, ?nissan?, ?toyota?).
ASIA is based on recent advances in web-
based set expansion - the problem of find-
ing all instances of a set given a small
number of ?seed? instances. This ap-
proach effectively exploits web resources
and can be easily adapted to different
languages. In brief, we use language-
dependent hyponym patterns to find a
noisy set of initial seeds, and then use a
state-of-the-art language-independent set
expansion system to expand these seeds.
The proposed approach matches or outper-
forms prior systems on several English-
language benchmarks. It also shows ex-
cellent performance on three dozen addi-
tional benchmark problems from English,
Chinese and Japanese, thus demonstrating
language-independence.
1 Introduction
An important and well-studied problem is the pro-
duction of semantic lexicons for classes of in-
terest; that is, the generation of all instances of
a set (e.g., ?apple?, ?orange?, ?banana?) given
a name of that set (e.g., ?fruits?). This task is
often addressed by linguistically analyzing very
large collections of text (Hearst, 1992; Kozareva
et al, 2008; Etzioni et al, 2005; Pantel and
Ravichandran, 2004; Pasca, 2004), often using
hand-constructed or machine-learned shallow lin-
guistic patterns to detect hyponym instances. A hy-
ponym is a word or phrase whose semantic range
Figure 1: Examples of SEAL?s input and output.
English entities are reality TV shows, Chinese en-
tities are popular Taiwanese foods, and Japanese
entities are famous cartoon characters.
is included within that of another word. For exam-
ple, x is a hyponym of y if x is a (kind of) y. The
opposite of hyponym is hypernym.
In this paper, we evaluate a novel approach to
this problem, embodied in a system called ASIA1
(Automatic Set Instance Acquirer). ASIA takes a
semantic class name as input (e.g., ?car makers?)
and automatically outputs instances (e.g., ?ford?,
?nissan?, ?toyota?). Unlike prior methods, ASIA
makes heavy use of tools for web-based set ex-
pansion. Set expansion is the task of finding all
instances of a set given a small number of exam-
ple (seed) instances. ASIA uses SEAL (Wang and
Cohen, 2007), a language-independent web-based
system that performed extremely well on a large
number of benchmark sets ? given three correct
seeds, SEAL obtained average MAP scores in the
high 90?s for 36 benchmark problems, including a
dozen test problems each for English, Chinese and
Japanese. SEAL works well in part because it can
efficiently find and process many semi-structured
web documents containing instances of the set be-
ing expanded. Figure 1 shows some examples of
SEAL?s input and output.
SEAL has been recently extended to be robust
to errors in its initial set of seeds (Wang et al,
1http://rcwang.com/asia
441
2008), and to use bootstrapping to iteratively im-
prove its performance (Wang and Cohen, 2008).
These extensions allow ASIA to extract instances
of sets from the Web, as follows. First, given a
semantic class name (e.g., ?fruits?), ASIA uses a
small set of language-dependent hyponym patterns
(e.g., ?fruits such as ?) to find a large but noisy
set of seed instances. Second, ASIA uses the ex-
tended version of SEAL to expand the noisy set of
seeds.
ASIA?s approach is motivated by the conjecture
that for many natural classes, the amount of infor-
mation available in semi-structured documents on
the Web is much larger than the amount of infor-
mation available in free-text documents; hence, it
is natural to attempt to augment search for set in-
stances in free-text with semi-structured document
analysis. We show that ASIA performs extremely
well experimentally. On the 36 benchmarks used
in (Wang and Cohen, 2007), which are relatively
small closed sets (e.g., countries, constellations,
NBA teams), ASIA has excellent performance
for both recall and precision. On four additional
English-language benchmark problems (US states,
countries, singers, and common fish), we com-
pare to recent work by Kozareva, Riloff, and Hovy
(Kozareva et al, 2008), and show comparable or
better performance on each of these benchmarks;
this is notable because ASIA requires less infor-
mation than the work of Kozareva et al(their sys-
tem requires a concept name and a seed). We also
compare ASIA on twelve additional benchmarks
to the extended Wordnet 2.1 produced by Snow
et al(Snow et al, 2006), and show that for these
twelve sets, ASIA produces more than five times
as many set instances with much higher precision
(98% versus 70%).
Another advantage of ASIA?s approach is that it
is nearly language-independent: since the underly-
ing set-expansion tools are language-independent,
all that is needed to support a new target language
is a new set of hyponym patterns for that lan-
guage. In this paper, we present experimental re-
sults for Chinese and Japanese, as well as English,
to demonstrate this language-independence.
We present related work in Section 2, and ex-
plain our proposed approach for ASIA in Sec-
tion 3. Section 4 presents the details of our ex-
periments, as well as the experimental results. A
comparison of results are illustrated in Section 5,
and the paper concludes in Section 6.
2 Related Work
There has been a significant amount of research
done in the area of semantic class learning (aka
lexical acquisition, lexicon induction, hyponym
extraction, or open-domain information extrac-
tion). However, to the best of our knowledge, there
is not a system that can perform set instance ex-
traction in multiple languages given only the name
of the set.
Hearst (Hearst, 1992) presented an approach
that utilizes hyponym patterns for extracting can-
didate instances given the name of a semantic set.
The approach presented in Section 3.1 is based on
this work, except that we extended it to two other
languages: Chinese and Japanese.
Pantel et al(Pantel and Ravichandran, 2004)
presented an algorithm for automatically inducing
names for semantic classes and for finding their
instances by using ?concept signatures? (statistics
on co-occuring instances). Pasca (Pasca, 2004)
presented a method for acquiring named entities in
arbitrary categories using lexico-syntactic extrac-
tion patterns. Etzioni et al(Etzioni et al, 2005)
presented the KnowItAll system that also utilizes
hyponym patterns to extract class instances from
the Web. All the systems mentioned rely on either
a English part-of-speech tagger, a parser, or both,
and hence are language-dependent.
Kozareva et al(Kozareva et al, 2008) illustrated
an approach that uses a single hyponym pattern
combined with graph structures to learn semantic
class from the Web. Section 5.1 shows that our
approach is competitive experimentally; however,
their system requires more information, as it uses
the name of the semantic set and a seed instance.
Pasca (Pas?ca, 2007b; Pas?ca, 2007a) illustrated
a set expansion approach that extracts instances
from Web search queries given a set of input seed
instances. This approach is similar in flavor to
SEAL but, addresses a different task from that ad-
dressed here: for ASIA the user provides no seeds,
but instead provides the name of the set being ex-
panded. We compare to Pasca?s system in Sec-
tion 5.2.
Snow et al(Snow et al, 2006) use known hyper-
nym/hyponym pairs to generate training data for a
machine-learning system, which then learns many
lexico-syntactic patterns. The patterns learned are
based on English-language dependency parsing.
We compare to Snow et als results in Section 5.3.
442
3 Proposed Approach
ASIA is composed of three main components: the
Noisy Instance Provider, the Noisy Instance Ex-
pander, and the Bootstrapper. Given a semantic
class name, the Provider extracts a initial set of
noisy candidate instances using hand-coded pat-
terns, and ranks the instances by using a sim-
ple ranking model. The Expander expands and
ranks the instances using evidence from semi-
structured web documents, such that irrelevant
ones are ranked lower in the list. The Bootstrap-
per enhances the quality and completeness of the
ranked list by using an unsupervised iterative tech-
nique. Note that the Expander and Bootstrap-
per rely on SEAL to accomplish their goals. In
this section, we first describe the Noisy Instance
Provider, then we briefly introduce SEAL, fol-
lowed by the Noisy Instance Expander, and finally,
the Bootstrapper.
3.1 Noisy Instance Provider
Noisy Instance Provider extracts candidate in-
stances from free text (i.e., web snippets) us-
ing the methods presented in Hearst?s early work
(Hearst, 1992). Hearst exploited several patterns
for identifying hyponymy relation (e.g., such au-
thor as Shakespeare) that many current state-of-
the-art systems (Kozareva et al, 2008; Pantel and
Ravichandran, 2004; Etzioni et al, 2005; Pasca,
2004) are using. However, unlike all of those sys-
tems, ASIA does not use any NLP tool (e.g., parts-
of-speech tagger, parser) or rely on capitalization
for extracting candidates (since we wanted ASIA
to be as language-independent as possible). This
leads to sets of instances that are noisy; however,
we will show that set expansion and re-ranking can
improve the initial sets dramatically. Below, we
will refer to the initial set of noisy instances ex-
tracted by the Provider as the initial set.
In more detail, the Provider first constructs a
few queries of hyponym phrase by using a se-
mantic class name and a set of pre-defined hy-
ponym patterns. For every query, the Provider re-
trieves a hundred snippets from Yahoo!, and splits
each snippet into multiple excerpts (a snippet of-
ten contains multiple continuous excerpts from its
web page). For each excerpt, the Provider extracts
all chunks of characters that would then be used
as candidate instances. Here, we define a chunk
as a sequence of characters bounded by punctua-
tion marks or the beginning and end of an excerpt.
Figure 2: Hyponym patterns in English, Chinese,
and Japanese. In each pattern, <C> is a place-
holder for the semantic class name and <I> is a
placeholder for its instances.
Lastly, the Provider ranks each candidate instance
x based on its weight assigned by the simple rank-
ing model presented below:
weight(x) =
sf (x,S)
|S|
?
ef (x,E)
|E|
?
wcf (x,E)
|C|
where S is the set of snippets, E is the set of ex-
cerpts, and C is the set of chunks. sf (x,S) is
the snippet frequency of x (i.e., the number of
snippets containing x) and ef (x,E) is the excerpt
frequency of x. Furthermore, wcf (x,E) is the
weighted chunk frequency of x, which is defined
as follows:
wcf (x,E) =
?
e?E
?
x?e
1
dist(x, e) + 1
where dist(x, e) is the number of characters be-
tween x and the hyponym phrase in excerpt e.
This model weights every occurrence of x based
on the assumption that chunks closer to a hyponym
phrase are usually more important than those fur-
ther away. It also heavily rewards frequency, as
our assumption is that the most common instances
will be more useful as seeds for SEAL.
Figure 2 shows the hyponym patterns we use
for English, Chinese, and Japanese. There are two
types of hyponym patterns: The first type are the
ones that require the class name C to precede its
instance I (e.g., C such as I), and the second type
are the opposite ones (e.g., I and other C). In
order to reduce irrelevant chunks, when excerpts
were extracted, the Provider drops all characters
preceding the hyponym phrase in excerpts that
contain the first type, and also drops all charac-
ters following the hyponym phrase in excerpts that
contain the second type. For some semantic class
names (e.g., ?cmu buildings?), there are no web
443
documents containing any of the hyponym-phrase
queries that were constructed using the name. In
this case, the Provider turns to a back-off strategy
which simply treats the semantic class name as the
hyponym phrase and extracts/ranks all chunks co-
occurring with the class name in the excerpts.
3.2 Set Expander - SEAL
In this paper, we rely on a set expansion system
named SEAL (Wang and Cohen, 2007), which
stands for Set Expander for Any Language. The
system accepts as input a few seeds of some target
set S (e.g., ?fruits?) and automatically finds other
probable instances (e.g., ?apple?, ?banana?) of S
in web documents. As its name implies, SEAL
is independent of document languages: both the
written (e.g., English) and the markup language
(e.g., HTML). SEAL is a research system that
has shown good performance in published results
(Wang and Cohen, 2007; Wang et al, 2008; Wang
and Cohen, 2008). Figure 1 shows some examples
of SEAL?s input and output.
In more detail, SEAL contains three major com-
ponents: the Fetcher, Extractor, and Ranker. The
Fetcher is responsible for fetching web docu-
ments, and the URLs of the documents come from
top results retrieved from the search engine us-
ing the concatenation of all seeds as the query.
This ensures that every fetched web page contains
all seeds. The Extractor automatically constructs
?wrappers? (i.e. page-specific extraction rules) for
each page that contains the seeds. Every wrap-
per comprises two character strings that specify
the left and right contexts necessary for extract-
ing candidate instances. These contextual strings
are maximally-long contexts that bracket at least
one occurrence of every seed string on a page. All
other candidate instances bracketed by these con-
textual strings derived from a particular page are
extracted from the same page.
After the candidates are extracted, the Ranker
constructs a graph that models all the relations
between documents, wrappers, and candidate in-
stances. Figure 3 shows an example graph where
each node di represents a document, wi a wrapper,
and mi a candidate instance. The Ranker performs
Random Walk with Restart (Tong et al, 2006) on
this graph (where the initial ?restart? set is the
set of seeds) until all node weights converge, and
then ranks nodes by their final score; thus nodes
are weighted higher if they are connected to many
Figure 3: An example graph constructed by
SEAL. Every edge from node x to y actually has
an inverse relation edge from node y to x that is
not shown here (e.g., m1 is extracted by w1).
seed nodes by many short, low fan-out paths. The
final expanded set contains all candidate instance
nodes, ranked by their weights in the graph.
3.3 Noisy Instance Expander
Wang (Wang et al, 2008) illustrated that it is feasi-
ble to perform set expansion on noisy input seeds.
The paper showed that the noisy output of any
Question Answering system for list questions can
be improved by using a noise-resistant version of
SEAL (An example of a list question is ?Who
were the husbands of Heddy Lamar??). Since the
initial set of candidate instances obtained using
Hearst?s method are noisy, the Expander expands
them by performing multiple iterations of set ex-
pansion using the noise-resistant SEAL.
For every iteration, the Expander performs set
expansion on a static collection of web pages. This
collection is pre-fetched by querying Google and
Yahoo! using the input class name and words such
as ?list?, ?names?, ?famous?, and ?common? for
discovering web pages that might contain lists of
the input class. In the first iteration, the Expander
expands instances with scores of at least k in the
initial set. In every upcoming iteration, it expands
instances obtained in the last iteration that have
scores of at least k and that also exist in the ini-
tial set. We have determined k to be 0.4 based on
our development set2. This process repeats until
the set of seeds for ith iteration is identical to that
of (i? 1)th iteration.
There are several differences between the origi-
nal SEAL and the noise-resistant SEAL. The most
important difference is the Extractor. In the origi-
2A collection of closed-set lists such as planets, Nobel
prizes, and continents in English, Chinese and Japanese
444
nal SEAL, the Extractor requires the longest com-
mon contexts to bracket at least one instance of ev-
ery seed per web page. However, when seeds are
noisy, such common contexts usually do not ex-
ist. The Extractor in noise-resistant SEAL solves
this problem by requiring the contexts to bracket
at least one instance of a minimum of two seeds,
rather than every seed. This is implemented using
a trie-based method described briefly in the origi-
nal SEAL paper (Wang and Cohen, 2007). In this
paper, the Expander utilizes a slightly-modified
version of the Extractor, which requires the con-
texts to bracket as many seed instances as possible.
This idea is based on the assumption that irrelevant
instances usually do not have common contexts;
whereas relevant ones do.
3.4 Bootstrapper
Bootstrapping (Etzioni et al, 2005; Kozareva,
2006; Nadeau et al, 2006) is an unsupervised iter-
ative process in which a system continuously con-
sumes its own outputs to improve its own perfor-
mance. Wang (Wang and Cohen, 2008) showed
that it is feasible to bootstrap the results of set ex-
pansion to improve the quality of a list. The pa-
per introduces an iterative version of SEAL called
iSEAL, which expands a list in multiple iterations.
In each iteration, iSEAL expands a few candi-
dates extracted in previous iterations and aggre-
gates statistics. The Bootstrapper utilizes iSEAL
to further improve the quality of the list returned
by the Expander.
In every iteration, the Bootstrapper retrieves 25
web pages by using the concatenation of three
seeds as query to each of Google and Yahoo!.
In the first iteration, the Bootstrapper expands
randomly-selected instances returned by the Ex-
pander that exist in the initial set. In every upcom-
ing iteration, the Bootstrapper expands randomly-
selected unsupervised instances obtained in the
last iteration that also exist in the initial set. This
process terminates when all possible seed com-
binations have been consumed or five iterations3
have been reached, whichever comes first. No-
tice that from iteration to iteration, statistics are
aggregated by growing the graph described in Sec-
tion 3.2. We perform Random Walk with Restart
(Tong et al, 2006) on this graph to determine the
final ranking of the extracted instances.
3To keep the overall runtime minimal.
4 Experiments
4.1 Datasets
We evaluated our approach using the evaluation
set presented in (Wang and Cohen, 2007), which
contains 36 manually constructed lists across
three different languages: English, Chinese, and
Japanese (12 lists per language). Each list contains
all instances of a particular semantic class in a cer-
tain language, and each instance contains a set of
synonyms (e.g., USA, America). There are a total
of 2515 instances, with an average of 70 instances
per semantic class. Figure 4 shows the datasets
and their corresponding semantic class names that
we use in our experiments.
4.2 Evaluation Metric
Since the output of ASIA is a ranked list of ex-
tracted instances, we choose mean average pre-
cision (MAP) as our evaluation metric. MAP is
commonly used in the field of Information Re-
trieval for evaluating ranked lists because it is sen-
sitive to the entire ranking and it contains both re-
call and precision-oriented aspects. The MAP for
multiple ranked lists is simply the mean value of
average precisions calculated separately for each
ranked list. We define the average precision of a
single ranked list as:
AvgPrec(L) =
|L|?
r=1
Prec(r)? isFresh(r)
Total # of Correct Instances
where L is a ranked list of extracted instances, r
is the rank ranging from 1 to |L|, Prec(r) is the
precision at rank r. isFresh(r) is a binary function
for ensuring that, if a list contains multiple syn-
onyms of the same instance, we do not evaluate
that instance more than once. More specifically,
the function returns 1 if a) the synonym at r is cor-
rect, and b) it is the highest-ranked synonym of its
instance in the list; it returns 0 otherwise.
4.3 Experimental Results
For each semantic class in our dataset, the
Provider first produces a noisy list of candidate in-
stances, using its corresponding class name shown
in Figure 4. This list is then expanded by the Ex-
pander and further improved by the Bootstrapper.
We present our experimental results in Table 1.
As illustrated, although the Provider performs
badly, the Expander substantially improves the
445
Figure 4: The 36 datasets and their semantic class names used as inputs to ASIA in our experiments.
English Dataset NP Chinese Dataset NP Japanese Dataset NP
NP NP +NE NP NP +NE NP NP +NE
# NP +BS +NE +BS # NP +BS +NE +BS # NP +BS +NE +BS
1. 0.22 0.83 0.82 0.87 13. 0.09 0.75 0.80 0.80 25. 0.20 0.63 0.71 0.76
2. 0.31 1.00 1.00 1.00 14. 0.08 0.99 0.80 0.89 26. 0.20 0.40 0.90 0.96
3. 0.54 0.99 0.99 0.98 15. 0.29 0.66 0.84 0.91 27. 0.16 0.96 0.97 0.96
4. 0.48 1.00 1.00 1.00 *16. 0.09 0.00 0.93 0.93 *28. 0.01 0.00 0.80 0.87
5. 0.54 1.00 1.00 1.00 17. 0.21 0.00 1.00 1.00 29. 0.09 0.00 0.95 0.95
6. 0.64 0.98 1.00 1.00 *18. 0.00 0.00 0.19 0.23 *30. 0.02 0.00 0.73 0.73
7. 0.32 0.82 0.98 0.97 19. 0.11 0.90 0.68 0.89 31. 0.20 0.49 0.83 0.89
8. 0.41 1.00 1.00 1.00 20. 0.18 0.00 0.94 0.97 32. 0.09 0.00 0.88 0.88
9. 0.81 1.00 1.00 1.00 21. 0.64 1.00 1.00 1.00 33. 0.07 0.00 0.95 1.00
*10. 0.00 0.00 0.00 0.00 22. 0.08 0.00 0.67 0.80 34. 0.04 0.32 0.98 0.97
11. 0.11 0.62 0.51 0.76 23. 0.47 1.00 1.00 1.00 35. 0.15 1.00 1.00 1.00
12. 0.01 0.00 0.30 0.30 24. 0.60 1.00 1.00 1.00 36. 0.20 0.90 1.00 1.00
Avg. 0.37 0.77 0.80 0.82 Avg. 0.24 0.52 0.82 0.87 Avg. 0.12 0.39 0.89 0.91
Table 1: Performance of set instance extraction for each dataset measured in MAP. NP is the Noisy
Instance Provider, NE is the Noisy Instance Expander, and BS is the Bootstrapper.
quality of the initial list, and the Bootstrapper then
enhances it further more. On average, the Ex-
pander improves the performance of the Provider
from 37% to 80% for English, 24% to 82% for
Chinese, and 12% to 89% for Japanese. The Boot-
strapper then further improves the performance of
the Expander to 82%, 87% and 91% respectively.
In addition, the results illustrate that the Bootstrap-
per is also effective even without the Expander; it
directly improves the performance of the Provider
from 37% to 77% for English, 24% to 52% for
Chinese, and 12% to 39% for Japanese.
The simple back-off strategy seems to be effec-
tive as well. There are five datasets (marked with *
in Table 1) of which their hyponym phrases return
zero web documents. For those datasets, ASIA au-
tomatically uses the back-off strategy described in
Section 3.1. Considering only those five datasets,
the Expander, on average, improves the perfor-
mance of the Provider from 2% to 53% and the
Bootstrapper then improves it to 55%.
5 Comparison to Prior Work
We compare ASIA?s performance to the results
of three previously published work. We use the
best-configured ASIA (NP+NE+BS) for all com-
parisons, and we present the comparison results in
this section.
5.1 (Kozareva et al, 2008)
Table 2 shows a comparison of our extraction per-
formance to that of Kozareva (Kozareva et al,
2008). They report results on four tasks: US
states, countries, singers, and common fish. We
evaluated our results manually. The results in-
dicate that ASIA outperforms theirs for all four
datasets that they reported. Note that the input
to their system is a semantic class name plus one
seed instance; whereas, the input to ASIA is only
the class name. In terms of system runtime, for
each semantic class, Kozareva et alreported that
their extraction process usually finished overnight;
however, ASIA usually finished within a minute.
446
N Kozareva ASIA N Kozareva ASIA
US States Countries
25 1.00 1.00 50 1.00 1.00
50 1.00 1.00 100 1.00 1.00
64 0.78 0.78 150 1.00 1.00
200 0.90 0.93
300 0.61 0.67
323 0.57 0.62
Singers Common Fish
10 1.00 1.00 10 1.00 1.00
25 1.00 1.00 25 1.00 1.00
50 0.97 1.00 50 1.00 1.00
75 0.96 1.00 75 0.93 1.00
100 0.96 1.00 100 0.84 1.00
150 0.95 0.97 116 0.80 1.00
180 0.91 0.96
Table 2: Set instance extraction performance com-
pared to Kozareva et al We report our precision
for all semantic classes and at the same ranks re-
ported in their work.
5.2 (Pas?ca, 2007b)
We compare ASIA to Pasca (Pas?ca, 2007b) and
present comparison results in Table 3. There are
ten semantic classes in his evaluation dataset, and
the input to his system for each class is a set of
seed entities rather than a class name. We evaluate
every instance manually for each class. The results
show that, on average, ASIA performs better.
However, we should emphasize that for the
three classes: movie, person, and video game,
ASIA did not initially converge to the correct in-
stance list given the most natural concept name.
Given ?movies?, ASIA returns as instances strings
like ?comedy?, ?action?, ?drama?, and other kinds
of movies. Given ?video games?, it returns ?PSP?,
?Xbox?, ?Wii?, etc. Given ?people?, it returns
?musicians?, ?artists?, ?politicians?, etc. We ad-
dressed this problem by simply re-running ASIA
with a more specific class name (i.e., the first one
returned); however, the result suggests that future
work is needed to support automatic construction
of hypernym hierarchy using semi-structured web
documents.
5.3 (Snow et al, 2006)
Snow (Snow et al, 2006) has extended the Word-
Net 2.1 by adding thousands of entries (synsets)
at a relatively high precision. They have made
several versions of extended WordNet available4.
For comparison purposes, we selected the version
(+30K) that achieved the best F-score in their ex-
periments.
4http://ai.stanford.edu/?rion/swn/
Precision @
Target Class System 25 50 100 150 250
Cities Pasca 1.00 0.96 0.88 0.84 0.75
ASIA 1.00 1.00 0.97 0.98 0.96
Countries Pasca 1.00 0.98 0.95 0.82 0.60
ASIA 1.00 1.00 1.00 1.00 0.79
Drugs Pasca 1.00 1.00 0.96 0.92 0.75
ASIA 1.00 1.00 1.00 1.00 0.98
Food Pasca 0.88 0.86 0.82 0.78 0.62
ASIA 1.00 1.00 0.93 0.95 0.90
Locations Pasca 1.00 1.00 1.00 1.00 1.00
ASIA 1.00 1.00 1.00 1.00 1.00
Newspapers Pasca 0.96 0.98 0.93 0.86 0.54
ASIA 1.00 1.00 0.98 0.99 0.85
Universities Pasca 1.00 1.00 1.00 1.00 0.99
ASIA 1.00 1.00 1.00 1.00 1.00
Movies Pasca 0.92 0.90 0.88 0.84 0.79
Comedy Movies ASIA 1.00 1.00 1.00 1.00 1.00
People Pasca 1.00 1.00 1.00 1.00 1.00
Jazz Musicians ASIA 1.00 1.00 1.00 0.94 0.88
Video Games Pasca 1.00 1.00 0.99 0.98 0.98
PSP Games ASIA 1.00 1.00 1.00 0.99 0.97
Pasca 0.98 0.97 0.94 0.90 0.80
Average ASIA 1.00 1.00 0.99 0.98 0.93
Table 3: Set instance extraction performance com-
pared to Pasca. We report our precision for all se-
mantic classes and at the same ranks reported in
his work.
For the experimental comparison, we focused
on leaf semantic classes from the extended Word-
Net that have many hypernyms, so that a mean-
ingful comparison could be made: specifically, we
selected nouns that have at least three hypernyms,
such that the hypernyms are the leaf nodes in the
hypernym hierarchy of WordNet. Of these, 210
were extended by Snow. Preliminary experiments
showed that (as in the experiments with Pasca?s
classes above) ASIA did not always converge to
the intended meaning; to avoid this problem, we
instituted a second filter, and discarded ASIA?s re-
sults if the intersection of hypernyms from ASIA
and WordNet constituted less than 50% of those
in WordNet. About 50 of the 210 nouns passed
this filter. Finally, we manually evaluated preci-
sion and recall of a randomly selected set of twelve
of these 50 nouns.
We present the results in Table 4. We used a
fixed cut-off score5 of 0.3 to truncate the ranked
list produced by ASIA, so that we can compute
precision. Since only a few of these twelve nouns
are closed sets, we cannot generally compute re-
call; instead, we define relative recall to be the
ratio of correct instances to the union of correct
instances from both systems. As shown in the re-
sults, ASIA has much higher precision, and much
higher relative recall. When we evaluated Snow?s
extended WordNet, we assumed all instances that
5Determined from our development set.
447
Snow?s Wordnet (+30k) Relative ASIA Relative
Class Name # Right # Wrong Prec. Recall # Right # Wrong Prec. Recall
Film Directors 4 4 0.50 0.01 457 0 1.00 1.00
Manias 11 0 1.00 0.09 120 0 1.00 1.00
Canadian Provinces 10 82 0.11 1.00 10 3 0.77 1.00
Signs of the Zodiac 12 10 0.55 1.00 12 0 1.00 1.00
Roman Emperors 44 4 0.92 0.47 90 0 1.00 0.96
Academic Departments 20 0 1.00 0.67 27 0 1.00 0.90
Choreographers 23 10 0.70 0.14 156 0 1.00 0.94
Elected Officials 5 102 0.05 0.31 12 0 1.00 0.75
Double Stars 11 1 0.92 0.46 20 0 1.00 0.83
South American Countries 12 1 0.92 1.00 12 0 1.00 1.00
Prizefighters 16 4 0.80 0.23 63 1 0.98 0.89
Newspapers 20 0 1.00 0.23 71 0 1.00 0.81
Average 15.7 18.2 0.70 0.47 87.5 0.3 0.98 0.92
Table 4: Set instance extraction performance compared to Snow et al
Figure 5: Examples of ASIA?s input and out-
put. Input class for Chinese is ?holidays? and for
Japanese is ?dramas?.
were in the original WordNet are correct. The
three incorrect instances of Canadian provinces
from ASIA are actually the three Canadian terri-
tories.
6 Conclusions
In this paper, we have shown that ASIA, a SEAL-
based system, extracts set instances with high pre-
cision and recall in multiple languages given only
the set name. It obtains a high MAP score (87%)
averaged over 36 benchmark problems in three
languages (Chinese, Japanese, and English). Fig-
ure 5 shows some real examples of ASIA?s in-
put and output in those three languages. ASIA?s
approach is based on web-based set expansion
using semi-structured documents, and is moti-
vated by the conjecture that for many natural
classes, the amount of information available in
semi-structured documents on the Web is much
larger than the amount of information available
in free-text documents. This conjecture is given
some support by our experiments: for instance,
ASIA finds 457 instances of the set ?film direc-
tor? with perfect precision, whereas Snow et als
state-of-the-art methods for extraction from free
text extract only four correct instances, with only
50% precision.
ASIA?s approach is also quite language-
independent. By adding a few simple hyponym
patterns, we can easily extend the system to sup-
port other languages. We have also shown that
Hearst?s method works not only for English, but
also for other languages such as Chinese and
Japanese. We note that the ability to construct
semantic lexicons in diverse languages has obvi-
ous applications in machine translation. We have
also illustrated that ASIA outperforms three other
English systems (Kozareva et al, 2008; Pas?ca,
2007b; Snow et al, 2006), even though many of
these use more input than just a semantic class
name. In addition, ASIA is also quite efficient,
requiring only a few minutes of computation and
couple hundreds of web pages per problem.
In the future, we plan to investigate the pos-
sibility of constructing hypernym hierarchy auto-
matically using semi-structured documents. We
also plan to explore whether lexicons can be con-
structed using only the back-off method for hy-
ponym extraction, to make ASIA completely lan-
guage independent. We also wish to explore
whether performance can be improved by simul-
taneously finding class instances in multiple lan-
guages (e.g., Chinese and English) while learning
translations between the extracted instances.
7 Acknowledgments
This work was supported by the Google Research
Awards program.
448
References
Oren Etzioni, Michael J. Cafarella, Doug Downey,
Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel S. Weld, and Alexander Yates. 2005.
Unsupervised named-entity extraction from the
web: An experimental study. Artif. Intell.,
165(1):91?134.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In In Proceedings
of the 14th International Conference on Computa-
tional Linguistics, pages 539?545.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL-08: HLT, pages 1048?1056, Columbus, Ohio,
June. Association for Computational Linguistics.
Zornitsa Kozareva. 2006. Bootstrapping named entity
recognition with automatically generated gazetteer
lists. In EACL. The Association for Computer Lin-
guistics.
David Nadeau, Peter D. Turney, and Stan Matwin.
2006. Unsupervised named-entity recognition:
Generating gazetteers and resolving ambiguity. In
Luc Lamontagne and Mario Marchand, editors,
Canadian Conference on AI, volume 4013 of Lec-
ture Notes in Computer Science, pages 266?277.
Springer.
Marius Pas?ca. 2007a. Organizing and searching the
world wide web of facts ? step two: harnessing the
wisdom of the crowds. In WWW ?07: Proceedings
of the 16th international conference on World Wide
Web, pages 101?110, New York, NY, USA. ACM.
Marius Pas?ca. 2007b. Weakly-supervised discovery
of named entities using web search queries. In
CIKM ?07: Proceedings of the sixteenth ACM con-
ference on Conference on information and knowl-
edge management, pages 683?690, New York, NY,
USA. ACM.
Patrick Pantel and Deepak Ravichandran. 2004.
Automatically labeling semantic classes. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
321?328, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In CIKM ?04: Proceed-
ings of the thirteenth ACM international conference
on Information and knowledge management, pages
137?145, New York, NY, USA. ACM.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In ACL ?06: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the ACL, pages 801?
808, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan.
2006. Fast random walk with restart and its appli-
cations. In ICDM, pages 613?622. IEEE Computer
Society.
Richard C. Wang and William W. Cohen. 2007.
Language-independent set expansion of named enti-
ties using the web. In ICDM, pages 342?350. IEEE
Computer Society.
Richard C. Wang and William W. Cohen. 2008. Iter-
ative set expansion of named entities using the web.
In ICDM, pages 1091?1096. IEEE Computer Soci-
ety.
Richard C. Wang, Nico Schlaefer, William W. Co-
hen, and Eric Nyberg. 2008. Automatic set ex-
pansion for list question answering. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 947?954, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
449
