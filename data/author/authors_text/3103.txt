Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1162?1171,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Descriptive and Empirical Approaches to Capturing Underlying
Dependencies among Parsing Errors
Tadayoshi Hara1 Yusuke Miyao1
1Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JAPAN
2School of Computer Science, University of Manchester
3NaCTeM (National Center for Text Mining)
{harasan,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii1,2,3
Abstract
In this paper, we provide descriptive and
empirical approaches to effectively ex-
tracting underlying dependencies among
parsing errors. In the descriptive ap-
proach, we define some combinations of
error patterns and extract them from given
errors. In the empirical approach, on the
other hand, we re-parse a sentence with
a target error corrected and observe er-
rors corrected together. Experiments on
an HPSG parser show that each of these
approaches can clarify the dependencies
among individual errors from each point
of view. Moreover, the comparison be-
tween the results of the two approaches
shows that combining these approaches
can achieve a more detailed error analysis.
1 Introduction
For any kind of technology, analyzing causes of
errors given by a system is a very helpful process
for improving its performance. In recent sophisti-
cated parsing technologies, the process has taken
on more and more important roles since critical
ideas for parsing performance have already been
introduced and the researches are now focusing on
exploring the rest of the pieces for making addi-
tional improvements.
In most cases for parsers? error analysis, re-
searchers associate output errors with failures in
handling certain linguistic phenomena and attempt
to avoid them by adding or modifying correspond-
ing settings of their parsers. However, such an
analysis cannot been done so smoothly since pars-
ing errors sometimes depend on each other and the
underlying dependencies behind superficial phe-
nomena cannot be captured easily.
In this paper, we propose descriptive and em-
pirical approaches to effective extraction of de-
pendencies among parsing errors and engage in a
deeper error analysis with them. In our descriptive
approach, we define various combinations of error
patterns as organized error phenomena on the ba-
sis of linguistic knowledge, and then extract such
combinations from given errors. In our empirical
approach, on the other and, we re-parse a sentence
under the condition where a target error is cor-
rected, and errors which are additionally corrected
are regarded as dependent errors. By capturing de-
pendencies among parsing errors through system-
atic approaches, we can effectively collect errors
which are related to the same linguistic properties.
In the experiments, we applied both of our ap-
proaches to an HPSG parser Enju (Miyao and Tsu-
jii, 2005; Ninomiya et al, 2006), and then evalu-
ated the obtained error classes. After examining
the individual approaches, we explored the com-
bination of them.
2 Parser and its evaluation
A parser is a system which interprets structures
of given sentences from some grammatical or in
some cases semantical viewpoints, and interpreted
structures are utilized as essential information for
various natural language tasks such as informa-
tion extraction, machine translation, and so on.
In most cases, an output structure of a parser is
based on a certain grammatical framework such as
CFG, CCG (Steedman, 2000), LFG (Kaplan and
Bresnan, 1995) or HPSG (Pollard and Sag, 1994).
Since such a framework can usually produce more
than one probable structure for a sentence, a parser
1162
John aux_arg12
ARG1 ARG2
verb_arg1
ARG1
has : come :
Figure 1: Predicate argument relations
Abbr. Full Abbr. Full
aux auxiliary lgs logical subject
verb verb coord coordination
prep prepositional conj conjunction
det determiner argN
1
... take argument(s)
adj adjunction (N
1
th, ...)
app apposition mod modify a word
relative relative
Table 1: Descriptions for predicate types
often utilizes some kind of disambiguation model
for choosing the best one.
While various parsers take different manners
in capturing linguistic phenomena based on their
frameworks, they are at least required to obtain
some kinds of relations between the words in sen-
tences. On the basis of the requirements, a parser
is usually evaluated on how correctly it gives in-
tended linguistic relations. ?Predicate argument
relation? is one of the most common evaluation
measurements for a parser since it is a very fun-
damental linguistic behavior and is less dependent
on parser systems. This measure divides linguis-
tic structural phenomena in a sentence into min-
imal predicative events. In one predicate argu-
ment relation, a word which represents an event
(predicate) takes some words as participants (argu-
ments). Although no fixed formulation exists for
the relations, there are to a large extent common
conceptions for them based on linguistic knowl-
edge among researchers.
Figure 1 shows an example of predicate argu-
ment relations given by Enju. In the sentence
?John has come.?, ?has? is a predicate of type
?aux arg12? and takes ?John? and ?come? as the
first and second arguments. ?come? is also a pred-
icate of the type ?verb arg1? and takes ?John? as
the first and the only argument. In this formalism,
each predicate type is represented as a combina-
tion of ?the grammatical nature of a word? and
?the arguments which it takes,? which are repre-
sented by the descriptions in Table 1. ?aux arg12?
in Figure 1 indicates that it is an auxiliary word
and takes two arguments ?ARG1? and ?ARG2.?
In order to improve the performance of a parser,
analyzing parsing errors is very much worth the
I watched the girl on TV Correct answer:
ARG1 ARG2
ARG1 ARG2
I watched the girl on TV Parser output:
ARG1 ARG2
ARG1 ARG2
Obtain inconsistent outputs as errors
Error: I watched the girl on TV 
ARG1
ARG1 Error
Figure 2: An example of parsing errors
Error: The book on which read the shelf  I yesterdayARG1
ARG2
ARG2ARG1Figure 3: Co-occurring parsing errors
effort. Since the errors are output according to
a given evaluation measurement such as ?predi-
cate argument relation,? we researchers carefully
explore them and infer the linguistic phenom-
ena which cause the erroneous outputs. Figure 2
shows an example of parsing errors for sentence ?I
watched the girl on TV.? Note that the errors are
based on predicate argument relations as shown
above and that the predicate types are abbreviated
in this figure. When we focus on the error output,
we can observe that ?ARG1? of predicate ?on?
was mistaken by the parser. In this case, ?ARG1?
represents a modifiee of the preposition, and we
then conclude that the ill attachment of a prepo-
sitional phrase caused this error. By continuing
such error analysis, weak points of the parser are
revealed and can be useful clues for further im-
provements.
However, in most researches on parsing tech-
nologies, error analysis has been limited to narrow
and shallow explorations since there are various
dependencies behind erroneous outputs. In Fig-
ure 3, for example, two errors were given: wrong
outputs for ?ARG1? of ?which? and ?ARG2? of
?read.? Both of these two errors originated from
the fact that the relative clause took a wrong an-
tecedent ?the shelf.? In this sentence, the former
1163
Error:
ARG1ARG1
They completed the sale of for 
ARG1
ARG1
it to him $1,000 
Confliction
They completed the sale of for 
ARG1ARG1
it to him $1,000 
Analysis 2: (Impossible)
They completed the sale of for 
ARG1ARG1
it to him $1,000 
Analysis 1: (Possible)
Can each error occur independently?
ARG1
ARG1
ARG1 ARG1
Figure 4: Sketch of error propagation
?ARG1? directly corresponds to the antecedent
while the latter ?ARG2? indirectly referred to the
same antecedent as the object of the verb ?read.?
The two predicate argument relations thus took the
same word as their common arguments, and there-
fore the two errors co-occurred.
On the other hand, one-way inductive relations
also exist among errors. In Figure 4, ?ARG1? of
?for? and ?to? were mistaken by a parser. We can
know that each of the errors was caused by an ill
attachment of a prepositional phrase with the same
analysis as shown in Figure 2. What is important
in this example is the manner in their occurrences.
The former error can appear by itself (Analysis 1)
while the latter cannot because of the structural
conflict with the former error (Analysis 2). The
appearance of the latter error thus induces that of
the former error. In error analysis, we have to cor-
rectly capture such various relations, which leads
us to a costly and less rewarding analysis.
In order to make advancements on this prob-
lem, we propose two types of approaches to real-
izing a deeper error analysis on parsing. In the ex-
periments, we examine our approaches for actual
errors which are given by the HPSG parser Enju
(Miyao and Tsujii, 2005; Ninomiya et al, 2006).
Enju was developed for capturing detailed syntac-
tic or semantic properties and relations for a sen-
tence with an HPSG framework (Pollard and Sag,
1994). In this research, we focus on error analysis
based on predicate argument relations, and in the
experiments with Enju, utilize the relations which
Erroneous phenomena Matched patterns
[Argument selection]
Prepositional attachment ARG1 of prep arg
Adjunction attachment ARG1 of adj arg
Conjunction attachment ARG1 of conj arg
Head selection for ARG1 of det arg
noun phrase
Coordination ARG1/2 of coord arg
[Predicate type selection]
Preposition/Adjunction prep arg / adj arg
Gerund acts as modifier/not verb mod arg / verb arg
Coordination/conjunction coord arg / conj arg
# of arguments prep argX / prep argY
for preposition (X 6= Y )
Adjunction/adjunctive noun adj arg / noun arg
[More structural errors]
To-infinitive for see Figure 7
modifier/argument of verb
Subject for passive sentence see Figure 8
or not
[Others]
Comma any error around ?,?
Relative clause attachment see Figure 9
Table 2: Patterns defined for descriptive approach
are represented in parsed tree structures.
3 Two approaches for error analysis
In this section, we propose two approaches for er-
ror analysis which enable us to capture underlying
dependencies among parsing errors. Our descrip-
tive approach matches the patterns of error com-
binations with given parsing errors and collects
matched erroneous participants. Our empirical ap-
proach, on the other hand, detects co-occurring
errors by re-parsing a sentence under a situation
where each of the errors is forcibly corrected.
3.1 Descriptive approach
Our descriptive approach for capturing dependen-
cies among parsing errors is to extract certain rep-
resentative structures of errors and collect the er-
rors which involve them. Parsing errors have a ten-
dency to occur with certain patterns of structures
representing linguistic phenomena. We first define
such patterns through observations with a part of
error outputs, and then match them with the rest.
Table 2 summarizes the patterns for erroneous
phenomena which we defined for matching in
the experiments. In the table, the patterns for
14 phenomena are given and classified into four
types according to their matching manners. Each
of the patterns for ?Argument selection? examine
whether a focused argument for a certain predi-
cate type is erroneous or not. Figure 5 shows the
pattern for ?Prepositional attachment,? which col-
1164
prep_arg
ARG1 Error
Parser output: ?
They completed the sale of for :
ARG1ARG1
it to : him $1,000 
Pattern:
prep_arg12 prep_arg12Correct output:
ARG1ARG1
They completed the sale of for :it to : him $1,000 prep_arg12 prep_arg12Parser output:
Example:
Figure 5: Pattern for ?Prepositional attachment?
gerund:     verb_argParser output: gerund: verb_mod_argCorrect answer:
(Patterns of correct answer and parser output can be interchanged)Pattern:
Example:
The customers walk the door
a   package   for   them
expecting: verb_mod_arg123 you to havein ARG1
MOD ARG2
ARG3
Parser output:
Correct output:
The customers walk the door
a   package   for   them
expecting:     verb_arg123 you to haveinNot exist 
ARG2
ARG3
ARG1
(MOD)
Figure 6: Pattern for ?Gerund acts as modifier or
not?
lects wrong ARG1 for predicate type ?prep arg?.
From the sentence in the figure, we can obtain
two errors for ?Prepositional attachment? around
prepositions ?to? and ?for.? On the other hand,
each ?Predicate type selection? pattern collects er-
rors around a word whose predicate type is erro-
neous. Figure 6 shows the pattern for ?Gerund
acts as modifier or not,? which collects errors
around gerunds whose predicate types are erro-
neous. From the example sentence in the figure,
we can obtain an erroneous predicate type for ?ex-
pecting? and collect errors around it for ?Gerund
acts as modifier or not.?
We can implement more structural errors than
simple argument or predicate type selections. Fig-
ures 7 and 8 show the patterns for ?To-infinitive
for modifier/argument of verb? and ?Subject for
passive sentence or not? respectively. The pat-
tern for the latter phenomenon collects errors on
recognitions of prepositional phrases which be-
have as subjects for passive expressions. The pat-
tern collects errors not only around prepositions
but also around the verbs which take the preposi-
Parser output: aux_arg12to :verb1 ?ARG3 verb2
Correct output: aux_mod_arg12
MOD
to :
ARG2
Unknown subject ARG1 ARG1
verb1 ? verb2
The  figures  ? were  adjusted to : remove ...aux_arg12
Example:
Parser output:
Correct answer:
ARG3
The  figures  ? were  adjusted to : remove ...aux_mod_arg12 
MOD ARG2
Unknown subject ARG1 ARG1
Pattern: (Patterns of correct answer and parser output can be interchanged)
Figure 7: Pattern for ?To-infinitive for modi-
fier/argument of verb?
Example:
Pattern:
Parser output: prep_arg12Unknown subject verb1 ?ARG1ARG1 ?
Correct output: lgs_arg2 ARG2verb1 ? ?ARG1
A  50-state  study  released in  September  by : Friends  ?
Unknown subject ARG1ARG1 prep_arg12Parser output:
Correct answer: A  50-state  study  released in  September  by : Friends  ?ARG1ARG2 lgs_arg12ARG2
(Patterns of correct answer and parser output can be interchanged)
Figure 8: Pattern for ?Subject for passive sentence
or not?
tional phrases as a subject.
Since these patterns are based on linguistic
knowledge given by a human, the process could
provide a relatively precise analysis with a lower
cost than a totally manual analysis.
3.2 Empirical approach
Our empirical approach, on the other hand, briefly
traces the parsing process which results in each of
the target errors. We collect co-occurring errors
as strongly relevant ones, and then extract depen-
dencies among the obtained groups. Parsing errors
could originate from wrong processing at certain
stages in the parsing, and errors with a common
origin would by necessity appear together. We re-
parse a target sentence under the condition where a
certain error is forcibly corrected and then collect
errors which are corrected together as the ?rela-
tive? ones. An error group where all errors are
relative to each other can be regarded as a ?co-
occurring error group.? Errors in the same co-
1165
Example:
Pattern:
relative_arg1
ARG1
Parser output: ARG1/2
Error
Parser output:
Correct answer:
The book on relative_arg1 read ARG2the shelf  I yesterdayARG1
ARG2
ARG1
which :
The book on relative_arg1 read the shelf  I yesterdaywhich :
Figure 9: Pattern for ?Relative clause attachment?
our work force
Error 1
Re-parse a sentence under the condition whereeach error is forcibly corrected 
Error 1
Error 2
Error 3
Correct Error 2
Error 1
Error 1
Extract co-occurring error groups and inductive relations 
Error 4 Error 1
Error 4Error 3
Error 3Correct
Correct
Correct
corrected together
corrected together
corrected together
corrected together
,
,
,
Error 1 Error 2 Error 3 Error 4
today
ARG1
Correct answer:
It    has    no    bearing on
our work force todayonParser output: ARG1 ARG1ARG2
ARG2 ARG1 ARG1 ARG1
It    has    no    bearing
Error 2 Error 3 Error 4 Error 5
Error 5 Error 4Correct corrected togetherError 1 Error 3Error 2, , ,
Error 4,
Error 2 Error 4,,
Error 2 Error 3,,
Error 5Induce
Co-occurring error group Co-occurring error group
Figure 10: An image of our empirical approach
occurring error group are expected to participate
in the same phenomenon. Dependencies among
errors are then expected to be summarized with in-
ductions among co-occurring error groups.
Figure 10 shows an image of this approach. In
this example, ?today? should modify noun phrase
?our work force? while the parser decided that ?to-
day? was also in the noun phrase. As a result, there
are five errors: three wrong outputs for ?ARG2?
of ?on? (Error 1) and ?ARG1? of ?our? (Error 2)
and ?work? (Error 3), excess relation ?ARG1? of
?force? (Error 4), and missing relation ?ARG1? for
?today? (Error 5). By correcting each of the errors
1, 2, 3 and 4, all of these errors are corrected to-
gether, and therefore classified into the same co-
occurring error group. Although error 5 cannot
participate in the group, correcting error 5 can cor-
rect all of the errors in the group, and therefore an
# ofError types Errors Patterns
? Analyzed 2,078 1,671
[Argument selection]
Prepositional attachment 579 579
Adjunction attachment 261 261
Conjunction attachment 43 40
Head selection for noun phrase 30 30
Coordination 202 184
[Predicate type selection]
Preposition/Adjunction 108 54
Gerund acts as modifier or not 84 31
Coordination/conjunction 54 27
# of arguments for preposition 51 17
Adjunction/adjunctive noun 13 13
[More structural errors]
To-infinitive for 120 22
modifier/argument of verb
Subject for passive sentence 8 3
or not
[Others]
Comma 444 372
Relative clause attachment 102 38
? Unanalyzed 2,631 ?
Total 4,709 ?
Table 3: Errors extracted with descriptive analysis
inductive relation is given from error 5 to the co-
occurring error group. We can then finally obtain
the inductive relations as shown at the bottom of
Figure 10. This approach can trace the actual be-
havior of the parser precisely, and can therefore
capture underlying dependencies which cannot be
found only by observing error outputs.
4 Experiments
We applied our approaches to parsing errors given
by the HPSG parser Enju, which was trained on
the Penn Treebank (Marcus et al, 1994) section
2-21. We first examined each approach, and then
explored the combination of the approaches.
4.1 Evaluation of descriptive approach
We examined our descriptive approach. We first
parsed sentences in the Penn Treebank section 22
with Enju, and then observed the errors. Based on
the observation, we next described the patterns as
shown in Section 3. After that, we parsed section
0 and then applied the patterns to the errors.
Table 3 summarizes the extracted errors. As the
table shows, with the 14 error patterns, we suc-
cessfully matched 1,671 locations in error outputs
and covered 2,078 of 4,709 errors, which com-
prised of more than 40% of the total errors. This
was the first step of the application of our ap-
proach, and in the future work we would like to
1166
Evaluated sentences (erroneous) 1,811 (1,009)
Errors (Correctable) 4,709 (3,085)
Co-occurring errors 1,978
Extracted inductive relations 501
F-score (LP/LR) 90.69 (90.78/93.59)
Table 4: Summary of our empirical approach




       	 
 










Figure 11: Frequency of each size of co-occurring
error group
add more patterns for capturing more phenomena.
When we focused on individual patterns, we
could observe that the simple error phenomena
such as the attachments were dominant. The first
reason for this would be that such phenomena
were among minimal linguistic events. This would
make the phenomena components of other more
complex ones. The second reason for the dom-
inance would be that the patterns for these error
phenomena were easy to implement only with ar-
gument inconsistencies, and only one or a few pat-
terns could cover every probable error. Among
these dominant error types, the number of prepo-
sitional attachments was outstanding. The er-
ror types which required matching with predicate
types were fewer than the attachment errors since
the limited patterns on the predicate types would
narrow the possible linguistic behavior of the can-
didate words. When we focus on more structural
errors, the table shows that the rates of the partici-
pant errors to matched locations were much larger
than those for simpler pattern errors. Once our pat-
terns matches, they could collect many errors at
the same time.
4.2 Evaluation of empirical approach
Next, we applied our empirical approach in the
same settings as in the previous section. We first
parsed sentences in section 0 and then applied our
approach to the obtained errors. In the experi-
ments, some errors could not be forcibly corrected
by our approach. The parser ?cut off? less proba-
ble parse substructures before giving the predicate
Sentence: The  asbestos  fiber  ,  crocidolite ,  is  unusually  resilient  once  it  enters the    
lungs  ,  with  even  brief  exposures  to  it  causing  symptoms  that  show  up  decades  later
,  researchers  said
(a)(b)
(c) (d)
(a) fiber      , : crocidoliteapp_arg12
fiber      , : crocidolitecoord_arg12
Correct answer:
Parser output:
is     usually     resilient     ? the     lungs        ,        with(b)
symptoms    that     show : up    decades    later(c)
Parser output:
Correct answer: verb_arg1
symptoms    that     show : up    decades    laterverb_arg12
(d)
ARG1 ARG2
ARG1 ARG2
ARG1 ARG1
ARG1 ARG2
ARG1 ARG1
Correct answer:
Parser output: is     usually     resilient     ? the     lungs        ,        withARG1 ARG1
Correct answer:
Parser output:
It    causing    symptoms    that    show    up    decades    laterARG1
It    causing    symptoms    that    show    up    decades    laterARG1
Figure 12: Obtained co-occurring error groups
argument relation for reducing the cost of parsing.
In this research, we ignored the errors which were
subject to such ?cut off? as ?uncorrectable? ones,
and focused only on the remaining ?correctable?
errors. In our future work, we would like to con-
sider the ?uncorrectable? errors.
Table 4 shows the summary of the analysis with
our approach. Enju gave 4,709 errors for section
0. Among these errors, the correctable errors were
3,085, and from these errors, we successfully ob-
tained 1,978 co-occurring error groups and 501 in-
ductive relations. Figure 11 shows the frequency
for each size of co-occurring groups. About a half
of the groups contains only single errors, which
would indicate that the errors could have only one-
way inductive relations with other errors. The rest
of this section explores examples of the obtained
co-occurring error groups and inductive relations.
Figure 12 shows an example of the extracted co-
occurring error groups. For the sentence shown at
the top of the figure, Enju gave seven errors. By
introducing our empirical approach, these errors
were definitely classified into four co-occurring er-
ror groups (a) to (d), and there were no inductive
relations detected among them. Group (a) contains
two errors on the comma?s local behavior as ap-
position or coordination. Group (b) contains the
errors on the words which gave almost the same
attachment behaviors. Group (c) contains the er-
rors on whether the verb ?show? took ?decades?
1167
Error types # of correctable errors # of independent errors Correction effect (errors)
[Argument selection]
Prepositional attachment 531 397 766
Adjunction attachment 196 111 352
Conjunction attachment 33 12 79
Head selection for noun phrase 22 0 84
Coordination 146 62 323
[Predicate type selection]
Preposition/Adjunction 72 30 114
Gerund acts as modifier or not 39 18 62
Coordination/conjunction 36 16 61
# of arguments for preposition 24 23 26
Adjunction/adjunctive noun 8 6 10
[More structural errors]
To-infinitive for 75 27 87
modifier/argument of verb
Subject for passive sentence or not 8 3 9
[Others]
Comma 372 147 723
Relative clause attachment 84 27 119
Total 1,646 979 ?
Table 5: Induction relations between errors for each linguistic phenomenon and other errors
Sentence: She  says  she  offered  Mrs.  Yeargin a  quiet  resignation
and  thought  she  could  help  save  her  teaching  certificate(a) (b)
Correcting (a) induced correcting (b)
(b) Correct answer:
Parser output:
? thought  she  could  help   save : her  teaching  certificateverb_arg123
? thought  she  could  help   save : her  teaching  certificateverb_arg12
ARG1 ARG2
ARG1
ARG1 ARG2 ARG3
(a) Correct answer:
Parser output:
? thought   she   could     help : save   her   teaching   certificateverb_arg12
? thought   she   could     help : save   her   teaching   certificateaux_arg12
ARG1 ARG2
ARG2 ARG2
ARG1 ARG2
ARG2ARG2
Figure 13: Inductive relation between obtained co-
occurring error groups
as its object or not. Group (d) contains an error on
the attachment of the adverb ?later?. Regardless
of the overlap of the regions in the sentence for
(c) and (d), our approach successfully classified
the errors into the two independent groups. With
our approach, it would be empirically shown that
the errors in each group actually co-occurred and
the group was independent. This would enable us
to concentrate on each of the co-occurring error
groups without paying attention to the influences
from the errors in other groups.
Figure 13 shows another example of the anal-
ysis with our empirical approach. In this case, 8
errors for a sentence were classified into two co-
occurring error groups (a) and (b), and our ap-
proach showed that correction in group (a) re-
sulted in correcting group (b) together. The errors
in group (a) were on whether ?help? behaved as an
auxiliary or pure verbal role. The errors in group
(b) were on whether ?save? took only one object
?her teaching certificate,? or two objects ?her? and
?teaching certificate.? Between group (a) and (b),
no ?structural? conflict could be found when cor-
recting only each of the groups. We could then
guess that the inductive relation between these two
groups was implicitly given by the disambigua-
tion model of the parser. By dividing the errors
into minimum units and clarifying the effects of
correcting a target error, error analysis with our
empirical approach could suggest some policy for
parser improvements.
4.3 Combination of two approaches
On the basis of the experiments shown in the pre-
vious sections, we would like to explore possibili-
ties for obtaining a more detailed analysis by com-
bining the two approaches.
4.3.1 Interactions between a target linguistic
phenomenon and other errors
Our descriptive approach could classify the pars-
ing errors according to the linguistic phenomena
they participated in. We then attempt to reveal how
such classified errors interacted with other errors
from the viewpoints of our empirical approach. In
order to enable the analysis by our empirical ap-
proach, we focused only on the correctable errors.
1168
Sentence: It  invests  heavily  in  dollar-denominated  securities  overseas  and  is
currently  waiving  management  fees  ,  which  boosts  its  yield (a)(b)(a)
It  invests  heavily  in  dollar-denominated  securities    overseas :adj_arg1
?Adjunction attachment?
ARG1
ARG1
Pattern matched: 
is  currently  waiving  management  fees              ,         which           boosts   its  yield
(b)
?Comma? , ?Relative clause attachment?Pattern matched: 
ARG1ARG1ARG1
ARG1ARG1ARG1
Error:
Error:
Figure 14: Combination of results given by de-
scriptive and empirical approaches (1)
Table 5 reports the degree to which the classi-
fied errors were related to other individual errors.
The leftmost numbers show the numbers of cor-
rectable errors, which were the focused errors in
the experiments. The central numbers show the
numbers of ?independent? errors, that is, the errors
which could be corrected only by correcting them-
selves. The rightmost numbers show ?correction
effects,? that is, the number of errors which would
consequently be corrected if all of the errors for
the focused phenomena were forcibly corrected.
?Independent? errors are obtained by collecting
error phenomena groups which consist of unions
of co-occurring error groups and each error in
which is not induced by other errors. Figure 14
shows an example of ?independent? errors. For
the sentence at the top of the figure, the parser had
four errors on ARG1 of ?overseas,? the comma,
?which? and ?boosts.? Our empirical approach
then classified these errors into two co-occurring
error groups (a) and (b), and there was no induc-
tive relation between the groups. Our descrip-
tive approach, on the other hand, matched all of
the errors with the patterns for ?Adjunction at-
tachment,? ?Comma? and ?Relative clause attach-
ment.? Since the error for the ?Adjunction attach-
ment? equals to a co-occurring group (a) and is not
induced by other errors, the error is ?independent.?
Table 5 shows that, for ?Prepositional attach-
ment?, ?Adjunction attachments,? ?# of argu-
ments for preposition? and ?Adjunction/adjunctive
noun,? more than half of the errors for the focused
phenomena are ?independent.? Containing many
?independent? errors would mean that the parser
should handle these phenomena further more in-
tensively as an independent event.
Sentence: Clark  J.  Vitulli was  named  senior  vice  president  and  general  manager 
of  this  U.S.  sales  and  marketing  arm  of  Japanese  auto  Maker  Mazda  Motor  Corp(b) (a)
(b)
(a)
senior  vice  president  and  general  manager  of  this  U.S.  sales   and :coord_arg12
?Coordination? (fragment)
ARG1
ARG1
Pattern matched: 
Correcting (a) induced correcting (b)
manager   of     this : U.S.   sales    and : marketing  arm  of
?Coordination? (fragment),
?Head selection of noun phrase?Pattern matched: 
det_arg1 coord_arg12
ARG2ARG1ARG2 ARG1
ARG2 ARG1 ARG1 ARG1 ARG2
Error:
Error:
Figure 15: Combination of results given by de-
scriptive and empirical approaches (2)
The ?correction effect? for a focused linguistic
phenomenon can be obtained by counting errors in
the union of the correctable error set for the phe-
nomenon and the error sets which were induced by
the individual errors in the set. We would show an
example of correction effect in Figure 15. In the
figure, the parser had six errors for the sentence
at the top: three false outputs for ARG1 of ?and,?
?this? and ?U.S.,? two false outputs for ARG2 of
?of? and ?and,? and missing output for ARG1 of
?sales.? Our empirical approach classified these
errors into two co-occurring error groups (a) and
(b), and extracted an inductive relation from (a) to
(b). Our descriptive approach, on the other hand,
matched two errors on ?and? with pattern ?Coor-
dination? and one error on ?this? with ?Head se-
lection for noun phrase.? When we focus on the
error for ?Head selection of noun phrase? in co-
occurring group (a), the correction of the error in-
duced the rest of the errors in (a), and further in-
duced the error in (b) according to the inductive
relation from (a) to (b). Therefore, a ?correction
effect? for the error results in six errors.
Table 5 shows that, for ?Conjunction attach-
ment,? ?Head selection for noun phrase? and ?Co-
ordination,? each ?correction effect? results in
more than twice the forcibly corrected errors. Im-
proving the parser so that it can resolve such high-
correction-effect erroneous phenomena may ad-
ditionally improve the parsing performances to a
great extent. On the other hand, ?Head selection
for noun phrase? contains no ?independent? error,
and therefore could not be handled independently
of other erroneous phenomena at all. Consider-
1169
ing the effects from outer events might make the
treatment of ?Head selection for noun phrase? a
more complicated process than other phenomena,
regardless of its high ?correction effect.?
Table 5 would thus suggest which phenomenon
we should resolve preferentially from the three
points of view: the number of errors, the number
of ?independent? errors and its ?correction effect.?
Considering these points, ?Prepositional attach-
ment? seems most preferable for handling first.
4.3.2 Possibilities for further analysis
Since the errors for the phenomenon were system-
atically collected with our descriptive approach,
we can work on further focused error analyses
which would answer such questions as ?Which
preposition causes most errors in attachments??,
?Which pair of a correct answer and an erroneous
output for predicate argument relations can occur
most frequently??, and so on. Our descriptive ap-
proach would enable us to thoroughly obtain such
analyses with more closely-defined patterns. In
addition, our empirical approach would clarify the
influences of the obtained error properties on the
parser?s behaviors. The results of the focused anal-
yses might reasonably lead us to the features that
can be captured as parameters for model training,
or policies for re-ranking the parse candidates.
The combination of our approaches would give
us interesting clues for planning effective strate-
gies for improving the parser. Our challenges for
combining the two approaches are now in the pre-
liminary stage and there would be many possibili-
ties for further detailed analysis.
5 Related work
Although there have been many researches which
analyzed errors on their own systems in the part of
the experiments, there have been few researches
which focused mainly on error analysis itself.
In the field of parsing, McDonald and Nivre
(2007) compared parsing errors between graph-
based and transition-based parsers. They observed
the accuracy transitions from various points of
view, and the obtained statistical data suggested
that error propagation seemed to occur in the
graph structures of parsing outputs. Our research
proceeded for one step in this point, and attempted
to reveal the way of the propagations. In exam-
ining the combination of the two types of pars-
ing, McDonald and Nivre (2007) utilized similar
approaches to our empirical analysis. They al-
lowed a parser to give only structures given by
the parsers. They implemented the ideas for eval-
uating the parser?s potentials whereas we imple-
mented the ideas for observing error propagations.
Dredze et al (2007) showed the possibility
that many parsing errors in the domain adaptation
tasks came from inconsistencies between annota-
tion manners of training resources. Such findings
would further suggest that, comparing given errors
without considering the inconsistencies could lead
to the misunderstanding of what occurs in domain
transitions. The summarized error dependencies
given by our approaches would be useful clues for
extracting such domain-dependent error phenom-
ena.
Gime?nez and Ma`rquez (2008) proposed an au-
tomatic error analysis approach in machine trans-
lation (MT) technologies. They were developing
a metric set which could capture features in MT
outputs at different linguistic levels with different
levels of granularity. As we considered the parsing
systems, they explored the way to resolve costly
and non-rewarding error analysis in the MT field.
One of their objectives was to enable researchers
to easily access detailed linguistic reports on their
systems and to concentrate only on analyses for
the system improvements. From this point of view,
our research might provide an introduction into
such rewarding analysis in parsing.
6 Conclusions
We proposed empirical and descriptive approaches
to extracting dependencies among parsing errors.
In the experiments, with each of our approaches,
we successfully obtained relevant errors. More-
over, the possibility was shown that the combina-
tion of our approaches would give a more detailed
error analysis which would bring us useful clues
for parser improvements.
In our future work, we will improve the per-
formance of our approaches by adding more pat-
terns for the descriptive approach and by handling
uncorrectable errors for the empirical approach.
With the obtained robust information, we will ex-
plore rewarding ways for parser improvements.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
1170
References
Mark Dredze, John Blitzer, Partha Pratim Talukdar,
Kuzman Ganchev, Joa?o V. Grac?a, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proceedings of the
CoNLL Shared Task Session of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 1051?1055.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Towards
heterogeneous automatic MT error analysis. In
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC?08),
pages 1894?1901.
Ronald M. Kaplan and Joan Bresnan. 1995. Lexical-
functional grammar: A formal system for gram-
matical representation. Formal Issues in Lexical-
Functional Grammar, pages 29?130.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert Macintyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
ARPA Human Language Technology Workshop.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 83?90.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 155?163.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
1171
Adapting a Probabilistic Disambiguation Model
of an HPSG Parser to a New Domain
Tadayoshi Hara1, Yusuke Miyao1, and Jun?ichi Tsujii1,2,3
1 Department of Computer Science, University of Tokyo,
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
2 CREST, JST (Japan Science and Technology Agency),
Honcho, 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
3 School of Informatics, University of Manchester,
POBox 88, Sackville St, Manchester, M60 1QD, UK
Abstract. This paper describes a method of adapting a domain-inde-
pendent HPSG parser to a biomedical domain. Without modifying the
grammar and the probabilistic model of the original HPSG parser, we
develop a log-linear model with additional features on a treebank of the
biomedical domain. Since the treebank of the target domain is limited, we
need to exploit an original disambiguation model that was trained on a
larger treebank. Our model incorporates the original model as a reference
probabilistic distribution. The experimental results for our model trained
with a small amount of a treebank demonstrated an improvement in
parsing accuracy.
1 Introduction
Natural language processing (NLP) is being demanded in various fields, such
as biomedical research, patent application, and WWW, because an unmanage-
able amount of information is being published in unstructured data, i.e., natural
language texts. To exploit latent information in these, the assistance of NLP
technologies is highly required. However, an obstacle is the lack of portability
of NLP tools. In general, NLP tools specialized to each domain were developed
from scratch, or adapted by considerable human effort. This is because linguistic
resources for each domain, such as a treebank, have not been sufficiently devel-
oped yet. Since dealing with various kinds of domains is an almost intractable
job, sufficient resources can not be expected.
The method presented in this paper is the development of disambiguation
models of an HPSG parser by combining a disambiguation model of an original
parser with a new model adapting to a new domain. Although the training of a
disambiguation model of a parser requires a sufficient amount of a treebank, its
construction requires a considerable human effort. Hence, we exploit the original
disambiguation model that was trained with a larger, but domain-independent
treebank. Since the original disambiguation model contains rich information of
general grammatical constraints, we try to use its information in developing a
disambiguation model for a new domain.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 199?210, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
200 T. Hara, Y. Miyao, and J. Tsujii
Our disambiguation model is a log-linear model into which the original disam-
biguation model is incorporated as a reference distribution. However, we cannot
simply estimate this model, because of the problem that has been discussed in
studies of the probabilistic modeling of unification-based grammars [1,2]. That
is, the exponential explosion of parse candidates assigned by the grammar makes
the estimation intractable. The previous studies solved the problem by applying
a dynamic programming algorithm to a packed representation of parse trees. In
this paper, we borrow their idea, and define reference distribution on a packed
structure. With this method, the log-linear model with a reference distribution
can be estimated by using dynamic programming.
In the experiments, we used an HPSG parser originally trained with the
Penn Treebank [3], and evaluated a disambiguation model trained with the GE-
NIA treebank [4], which consisted of abstracts of biomedical papers. First, we
measured the accuracy of parsing and the time required for parameter estima-
tion. For comparison, we also examined other possible models other than our
disambiguation model. Next, we varied the size of a training corpus in order to
evaluate the size sufficient for domain adaptation. Then, we varied feature sets
used for training and examined the parsing accuracy. Finally, we compared the
errors in the parsing results of our model with those of the original parser.
In Section 2, we introduce the disambiguation model of an HPSG parser. In
Section 3, we describe a method of adopting reference distribution for adapting
a probabilistic disambiguation model to a new domain. In Section 4, we examine
our method through experiments on the GENIA treebank.
2 An HPSG Parser
The HPSG parser used in this study is Enju [5]. The grammar of Enju was ex-
tracted from the Penn Treebank [3], which consisted of sentences collected from
The Wall Street Journal [6]. The disambiguation model of Enju was trained
on the same treebank. This means that the parser has been adapted to The
Wall Street Journal, and would be difficult to apply to other domains such
as biomedical papers that include different distribution of words and
their constraints.
In this study, we attempted the adaptation of a probabilistic disambiguation
model by fixing the grammar and the disambiguation model of the original
parser. The disambiguation model of Enju is based on a feature forest model
[2], which is a maximum entropy model [7] on packed forest structure. The
probability, pE(t|s), of producing the parse result t for a given sentence s is
defined as
pE(t|s) =
1
Zs
exp
(
?
i
?ifi(t, s)
)
Zs =
?
t??T (s)
exp
(
?
i
?ifi(t?, s)
)
,
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 201
Fig. 1. Chart for parsing ?he saw a girl with a telescope?
where T (s) is the set of parse candidates assigned to s. The feature function
fi(t, s) represents the characteristics of t and s, while the corresponding model
parameter ?i is its weight. Model parameters were estimated so as to maximize
the log-likelihood of the training data.
Estimation of the above model requires a set of training pairs ?ts, T (s)?, where
ts is the correct parse for the sentence s. While ts is provided by a treebank, T (s)
is computed by parsing each s in the treebank. However, the simple enumeration
of T (s) is impractical because the size of T (s) is exponential to the length of s.
To avoid an exponential explosion, Enju represented T (s) in a packed form of
HPSG parse trees [5]. In chart parsing, partial parse candidates are stored in a
chart, in which phrasal signs are identified and packed into an equivalence class
if they are determined to be equivalent and dominate the same word sequence.
A set of parse trees is then represented as a set of relations among equivalence
classes. Figure 1 shows a chart for parsing ?he saw a girl with a telescope?, where
the modifiee (?saw? or ?girl?) of ?with? is ambiguous. Each feature structure
expresses an equivalence class, and the arrows represent immediate-dominance
relations. The phrase, ?saw a girl with a telescope?, has two ambiguous subtrees
(A in the figure). Since the signs of the top-most nodes are equivalent, they are
packed into the same equivalence class. The ambiguity is represented as two
pairs of arrows that come out of the node.
A packed chart can be interpreted as an instance of a feature forest [2]. A
feature forest represents a set of exponentially-many trees in an ?and/or? graph
of a tractable size. A feature forest is formally defined as a tuple ?C, D, R, ?, ??,
where C is a set of conjunctive nodes, D is a set of disjunctive nodes, R ? C
is a set of root nodes1, ? : D ? 2C is a conjunctive daughter function, and
? : C ? 2D is a disjunctive daughter function.
1 For the ease of explanation, the definition of root node is slightly different from the
original.
202 T. Hara, Y. Miyao, and J. Tsujii
HEAD  prep
MOD  NP
SUBCAT <>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  prep
MOD  VP
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT
<NP,NP>
he
saw
c1
c3
c2
c4
c5 c6
c8c7
Fig. 2. Packed representation of HPSG parse trees in Figure 1
Figure 2 shows (a part of) the HPSG parse trees in Figure 1 represented
as a feature forest. Square boxes are conjunctive nodes, dotted lines express a
disjunctive daughter function, and solid arrows represent a conjunctive daughter
function.
Based on the definition, parse tree t of sentence s can be represented as the
set of conjunctive nodes in the feature forest. The probability pE(t|s) is then
redefined as
pE(t|s) =
1
Zs
exp
(
?
c?t
?
i
?ifi(c)
)
Zs =
?
t??T (s)
exp
(
?
c?t?
?
i
?ifi(c)
)
,
where fi(c) are alternative feature functions assigned to conjunctive nodes c ? C.
By using this redefined probability, a dynamic programming algorithm can be
applied to estimate p(t|T (s)) without unpacking the packed chart [2].
Feature functions in feature forest models are designed to capture the char-
acteristics of a conjunctive node. In HPSG parsing, it corresponds to a tuple of a
mother and its daughters. Enju uses features that are combinations of the atomic
features listed in Table 1. The following combinations are used for representing
the characteristics of the binary/unary rule applications.
fbinary =
?rule,dist,comma,
spanh, symh,wordh, posh, leh,
spann, symn, wordn, posn, len
?
funary = ?rule,sym,word,pos,le?
where suffixh andnmeans a headdaughter anda non-headdaughter, respectively.
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 203
Table 1. Templates of atomic features
rule the name of the applied schema
dist the distance between the head words of the daughters
comma whether a comma exists between daughters and/or inside of daughter phrases
span the number of words dominated by the phrase
sym the symbol of the phrasal category (e.g. NP, VP)
word the surface form of the head word
pos the part-of-speech of the head word
le the lexical entry assigned to the head word
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  verb
SUBCAT <NP>HEAD  nounSUBCAT <>
he
transitiveVBD,saw,S,root =f
prep-mod-vpwith,IN,PP,3,
,transitiveVBD,saw,VP,3,
mod,3,0,-head
binary =f
c1
c2
c3 c4
Fig. 3. Example features
In addition, the following feature is used for expressing the condition of the
root node of the parse tree.
froot = ?sym,word,pos,le?
Figure 3 shows example features: froot is the feature for the root node, in
which the phrase symbol is S and the surface form, part-of-speech, and lexical
entry of the lexical head are ?saw?, VBD, and a transitive verb, respectively.
The fbinary is the feature for the binary rule application to ?saw a girl? and
?with a telescope?, in which the applied schema is the Head-Modifier Schema, the
head daughter is VP headed by ?saw?, and the non-head daughter is PP headed
by ?with?, whose part-of-speech is IN and the lexical entry is a VP-modifying
preposition.
3 Re-training of Disambiguation Models
The method of domain adaptation is to develop a new maximum entropy model
with incorporating an original model as a reference probabilistic distribution.
The idea of adaptation using a reference distribution has already been presented
204 T. Hara, Y. Miyao, and J. Tsujii
in several studies [8,9]. When we have a reference probabilistic model p0(t|s) and
are making a new model pM (t|s), the probability is defined as
pM (t|s) =
1
Z ?s
p0(t|s) exp
?
?
?
j
?jgj(t?, s)
?
?
where Z ?s =
?
t??T (s)
p0(t?|s) exp
?
?
?
j
?jgj(t?, s)
?
? .
Model parameters, ?j, are estimated so as to maximize the likelihood of the
training data as in ordinary maximum entropy models. The maximization of the
likelihood with the above model is equivalent to finding the model pM that is
closest to the reference probability p0 in terms of the Kullback-Leibler distance.
However, we cannot simply apply the above method to our task because the
parameter estimation requires the computation of the above probability for all
parse candidates T (s). As discussed in Section 2, the size of T (s) is exponentially
related to the length of s. This imposes a new problem, that is, we need to
enumerate p0(t|s) for all candidate parses. Obviously, this is intractable.
Since Enju represented a probabilistic disambiguation model in a packed
forest structure, we exploit that structure to represent our probabilistic model.
That is, we redefine pM with feature functions gj on conjunctive nodes as
pM (t|s) =
1
Z ?s
p0(t|s) exp
?
?
?
c?t
?
j
?jgj(c)
?
?
where Z ?s =
?
t??T (s)
p0(t|s) exp
?
?
?
c?t?
?
j
?jgj(c)
?
? .
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  verb
SUBCAT <NP>HEAD  nounSUBCAT <>
he
c1
c2
c3 c4
t1 selected
t2 selected
? j cgjj )( 1?
?i cfii )( 1?
? j cgjj )( 4?? j cgjj )( 3?? j cgjj )( 2?
?i cfii )( 2? ?i cfii )( 3? ?i cfii )( 4?
Fig. 4. Example of importing a reference distribution into each conjunctive node
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 205
As described in Section 2, the original model, pE(t|s), is expressed in a packed
structure as
pE(t|s) =
1
Zs
exp
(
?
c?t
?
i
?ifi(c)
)
where Zs =
?
t??T (s)
exp
(
?
c?t
?
i
?ifi(c)
)
.
Then, p0(t|s) is substituted by pE(t|s), and pM (t|s) is formulated as
pM (t|s) =
1
Z ?s
{
1
Zs
exp
(
?
c?t
?
i
?ifi(c)
)}
exp
?
?
?
c?t
?
j
?jgj(c)
?
?
=
1
Z ?s ? Zs
exp
?
?
?
c?t
?
i
?ifi(c) +
?
c?t
?
j
?jgj(c)
?
?
=
1
Z ??s
exp
?
?
?
?
c?t
?
?
?
i
?ifi(c) +
?
j
?jgj(c)
?
?
?
?
?
where Z ??s = Zs ? Z ?s =
?
t?T (s)
exp
?
?
?
?
c?t
?
?
?
i
?ifi(c) +
?
j
?jgj(c)
?
?
?
?
?
.
With this form of pM (t|s), a dynamic programing algorithm can be applied.
For example, we show how to obtain probabilities of parse trees in the case of
Figure 4. For ease, we assume that there are only two disjunctive daughters
(dotted lines) that are of the top conjunctive node. The left disjunctive node
introduces a parse tree t1 that consists of conjunctive nodes {c1, c2, c3, . . . },
and the right one, t2 that consists of {c1, c2, c4, . . . }. To each conjunctive node
ck, a weight from the reference distribution
?
i ?ifi(ck) is assigned. Probability
pM (t1|s) and pM (t2|s) are then given as
pM (t1|s)=
1
Z ??s
exp
?
?
?
?
?
?
i
?ifi(c1) +
?
j
?jgj(c1)
?
?+
?
?
?
i
?ifi(c2) +
?
j
?jgj(c2)
?
?
+
?
?
?
i
?ifi(c3) +
?
j
?jgj(c3)
?
? + ? ? ?
?
?
?
pM (t2|s)=
1
Z ??s
exp
?
?
?
?
?
?
i
?ifi(c1) +
?
j
?jgj(c1)
?
?+
?
?
?
i
?ifi(c2) +
?
j
?jgj(c2)
?
?
+
?
?
?
i
?ifi(c4) +
?
j
?jgj(c4)
?
? + ? ? ?
?
?
?
.
206 T. Hara, Y. Miyao, and J. Tsujii
4 Experiments
We implemented the method described in Section 3. The original parser, Enju,
was developed on Section 02-21 of the Penn Treebank (39,832 sentences)[5]. For
the training of our model, we used the GENIA treebank [4], which consisted of
500 abstracts (4,446 sentences) extracted from MEDLINE. We divided the GENIA
treebank into three sets of 400, 50, and 50 abstracts (3,524, 455, and 467 sentences),
and these setswere used respectively as training, development, and final evaluation
data. The method of Gaussian MAP estimation [10] was used for smoothing.
The meta parameter ? of the Gaussian distribution was determined so as
to maximize the accuracy on the development set. In the following experiments,
we measured the accuracy of predicate-argument dependencies on the evaluation
set. The measure is labeled precision/recall (LP/LR), which is the same measure
as previous work [11,5] that evaluated the accuracy of lexicalized grammars on
the Penn Treebank.
First, we measured the accuracy of parsing and the time required for pa-
rameter estimation. Table 2 compares the results of the following estimation
methods.
Table 2. Accuracy and time cost for various estimation methods
F-score Training Parsing time (sec.)
GENIA Corpus Penn Treebank time (sec.) GENIA Corpus Penn Treebank
Our method 86.87 86.81 2,278 611 3,165
Combined 86.32 86.09 29,421 424 2,757
GENIA only 85.72 42.49 1,694 332 8,183
Original model 85.10 87.16 137,038 515 2,554
85
85.2
85.4
85.6
85.8
86
86.2
86.4
86.6
86.8
87
0 500 1000 1500 2000 2500 3000 3500
training sentences
F-
sc
ore
RULE WORDh + WORDn RULE + WORDh + WORDn
Fig. 5. Corpus size vs. Accuracy
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 207
Table 3. Accuracy with atomic feature templates
Features LP LR F-score diff.
RULE 85.42 84.87 85.15 +0.05
DIST 85.29 84.77 85.03 ?0.07
COMMA 85.45 84.86 85.15 +0.05
SPANh+SPANn 85.58 85.02 85.30 +0.20
SYMBOLh+SYMBOLn 85.01 84.56 84.78 ?0.32
WORDh+WORDn 86.59 86.07 86.33 +1.23
WORDh 85.48 84.98 85.23 +0.13
WORDn 85.44 84.64 85.04 ?0.06
POSh+POSn 85.23 84.77 85.00 ?0.10
LEh+LEn 85.42 85.06 85.24 +0.14
None 85.39 84.82 85.10
Table 4. Accuracy with the combination of RULE and other features
Features LP LR F-score diff.
RULE+DIST 85.41 84.85 85.13 +0.03
RULE+COMMA 85.92 85.15 85.53 +0.43
RULE+SPANh+SPANn 85.33 84.82 85.07 ?0.03
RULE+SYMBOLh+SYMBOLn 85.43 85.00 85.21 +0.11
RULE+WORDh+WORDn 87.12 86.62 86.87 +1.77
RULE + WORDh 85.74 84.94 85.34 +0.24
RULE + WORDn 85.10 84.60 84.85 ?0.25
RULE+POSh+POSn 85.51 85.08 85.29 +0.19
RULE+LEh+LEn 85.48 85.08 85.28 +0.18
None 85.39 84.82 85.10
Our method: training with our method
Combined: training Enju model with the training corpus replaced by the com-
bination of the GENIA corpus and the Penn Treebank
GENIA only: training Enju model with the training corpus replaced by the
GENIA corpus only
Original Model: training an original Enju model
The table shows the accuracy and the parsing time for the GENIA corpus and
the Penn Treebank Section 23, and also shows the time required for the training
of the model. The additional feature used in our method was RULE+WORDh+
WORDn, which will be explained later. In the ?Combined? method, we could
not train the model with the original training parameters (n = 20,  = 0.98 in
[5]) because the estimator ran out of memory. Hence, we reduced the parameters
to n = 10,  = 0.95.
For the GENIA corpus, our model gave the higher accuracy than the origi-
nal model and the other estimation methods, while for the Penn Treebank, our
model gave a little lower accuracy than the original model. This result indicates
that our model was more adapted to the specific domain. The ?GENIA only?
208 T. Hara, Y. Miyao, and J. Tsujii
Table 5. Accuracy with the combination of WORD and another feature
Features LP LR F-score diff.
WORDh+WORDn+RULE 87.12 86.62 86.87 +1.77
WORDh+WORDn+DIST 86.41 85.86 86.14 +1.04
WORDh+WORDn+COMMA 86.91 86.38 86.64 +1.54
WORDh+WORDn+SPANh+SPANn 85.77 85.22 85.49 +0.39
WORDh+WORDn+SYMBOLh+SYMBOLn 86.58 85.70 86.14 +1.04
WORDh+WORDn+POSh+POSn 86.53 85.99 86.26 +1.16
WORDh+WORDn+LEh+LEn 86.16 85.68 85.92 +0.82
None 85.39 84.82 85.10
Table 6. Errors in our model and Enju
Total errors Common errors Errors not in
the other model
Our model 1179 1050 129
Original model 1338 1050 288
method gave significantly lower accuracy. We expect that the method clearly
lacked the amount of the training corpus for obtaining generic grammatical
information.
The ?Combined? method achieved the accuracy close to our method. How-
ever, it is notable that our method took much less time for the training of the
model since ours did not need to handle the Penn Treebank. Instead, our method
exploited the original model of Enju, which was trained on the Penn Treebank,
and this resulted in much less cost of training.
Next, we changed the size of the GENIA treebank for training: 40, 80, 120,
160, 200, 240, 280, 320, 360, and 400 abstracts. Figure 5 shows the accuracy when
the size of the training data was changed. We can say that, for those feature sets
giving remarkable accuracy in the experiments, the accuracy edged upwards with
the size of the training corpus, and the trend does not seem to converge even if
more than 400 abstracts exist. If we choose more complex feature sets for higher
accuracy, data sparseness will occur and an even larger corpus will be needed.
These findings indicate that we can further improve the accuracy by using a
larger treebank and a proper feature set.
Table 3 shows the accuracy of models with only atomic feature templates.
The bottom of the table gives the accuracy attained by the original parser.
When we focus on the WORD features, we can see the combination of WORDh
and WORDn improved the accuracy significantly, although each of the features
by itself did not improve so much. DIST, SYMBOL, and POS feature templates
lowered the accuracy. The other feature templates improved the accuracy, though
not as well as the WORD templates.
Table 4 shows that the RULE feature combined with one or more other
features often gave a little higher accuracy than the RULE feature gave by
itself, though not as well as the WORD features.
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 209
Table 5 shows that the WORD features combined with one or more other
features gave remarkable improvement to the accuracy as a whole. RULE and
COMMA features gave even higher accuracy than with only the WORD features.
Our results revealed that the WORD features were crucial for the adaptation to
the biomedical domain. We expect that this was because the biomedical domain
had a different distribution of words, while more generic grammatical constraints
were not significantly different from other domains.
Table 6 shows the comparison of the number of errors of our model with those
of the original model in parsing the GENIA corpus. Though our model gave less
errors than the original model, our model introduced a certain amount of new
errors. In future work, we need to investigate manually those errors to find more
suitable feature templates without losing the information in the original model.
5 Conclusions
We have presented a method of adapting a domain-independent HPSG parser
to a biomedical domain. Since the treebank of the new domain was limited,
we exploited an original disambiguation model. The new model was trained
on a biomedical treebank, and was combined with the original model by using
it as a reference distribution of a log-linear model. The experimental results
demonstrated our new model was adapted to the target domain, and was superior
to other adaptation methods in accuracy and the cost of training time. With our
model, the parsing accuracy for the target domain improved by 1.77 point with
the treebank of 3,524 sentences. Since the accuracy did not seem to saturate, we
will further improve the accuracy by increasing the size of the domain-dependent
treebank. In addition, the experimental results showed that the WORD feature
significantly contributed to the accuracy improvement.
We examined only a few feature templates, and we must search for further
more feature templates. Not only the new combinations of the atomic features
but also new types of features, which may be domain-dependent such as named
entities, will be possible.
References
1. Geman, S., Johnson, M.: Dynamic programming for parsing and estimation of
stochastic unification-based grammars. In: Proc. 40th ACL. (2002)
2. Miyao, Y., Tsujii, J.: Maximum entropy estimation for feature forests. In: Proc.
HLT 2002. (2002)
3. Marcus, M., Kim, G., Marcinkiewicz, M.A., MacIntyre, R., Bies, A., Ferguson, M.,
Katz, K., Schasberger, B.: The Penn Treebank: Annotating predicate argument
structure. In: ARPA Human Language Technology Workshop. (1994)
4. Kim, J.D., Ohta, T., Teteisi, Y., Tsujii, J.: Genia corpus - a semantically annotated
corpus for bio-textmining. Bioinformatics 19 (2003) i180?i182
5. Miyao, Y., Tsujii, J.: Probabilistic disambiguation models for wide-coverage HPSG
parsing. In: Proc. ACL 2005. (2005)
210 T. Hara, Y. Miyao, and J. Tsujii
6. Miyao, Y., Ninomiya, T., Tsujii, J.: Corpus-oriented grammar development for
acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. In:
Proc. IJCNLP-04. (2004)
7. Berger, A.L., Pietra, S.A.D., Pietra, V.J.D.: A maximum entropy approach to
natural language processing. Computational Linguistics 22 (1996) 39?71
8. Jelinek, F.: Statistical Methods for Speech Recognition. The MIT Press (1998)
9. Johnson, M., Riezler, S.: Exploiting auxiliary distributions in stochastic unification-
based grammars. In: Proc. 1st NAACL. (2000)
10. Chen, S., Rosenfeld, R.: A gaussian prior for smoothing maximum entropy models.
Technical Report CMUCS-99-108, Carnegie Mellon University (1999)
11. Clark, S., Curran, J.R.: Parsing the WSJ using CCG and log-linear models. In:
Proc. 42nd ACL. (2004)
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An intelligent search engine and GUI-based efficient MEDLINE search
tool based on deep syntactic parsing
Tomoko Ohta
Yoshimasa Tsuruoka??
Jumpei Takeuchi
Jin-Dong Kim
Yusuke Miyao
Akane Yakushiji?
Kazuhiro Yoshida
Yuka Tateisi?
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{okap, yusuke, ninomi, tsuruoka, akane, kmasuda, tj jug,
kyoshida, harasan, jdkim, yucca, tsujii}@is.s.u-tokyo.ac.jp
Takashi Ninomiya?
Katsuya Masuda
Tadayoshi Hara
Jun?ichi Tsujii
Abstract
We present a practical HPSG parser for
English, an intelligent search engine to re-
trieve MEDLINE abstracts that represent
biomedical events and an efficient MED-
LINE search tool helping users to find in-
formation about biomedical entities such
as genes, proteins, and the interactions be-
tween them.
1 Introduction
Recently, biomedical researchers have been fac-
ing the vast repository of research papers, e.g.
MEDLINE. These researchers are eager to search
biomedical correlations such as protein-protein or
gene-disease associations. The use of natural lan-
guage processing technology is expected to re-
duce their burden, and various attempts of infor-
mation extraction using NLP has been being made
(Blaschke and Valencia, 2002; Hao et al, 2005;
Chun et al, 2006). However, the framework of
traditional information retrieval (IR) has difficulty
with the accurate retrieval of such relational con-
cepts. This is because relational concepts are
essentially determined by semantic relations of
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
This paper proposes a practical HPSG parser
for English, Enju, an intelligent search engine for
the accurate retrieval of relational concepts from
?Current Affiliation:
?School of Informatics, University of Manchester
?Knowledge Research Center, Fujitsu Laboratories LTD.
?Faculty of Informatics, Kogakuin University
?Information Technology Center, University of Tokyo
F-Score
GENIA treebank Penn Treebank
HPSG-PTB 85.10% 87.16%
HPSG-GENIA 86.87% 86.81%
Table 1: Performance for Penn Treebank and the
GENIA corpus
MEDLINE, MEDIE, and a GUI-based efficient
MEDLINE search tool, Info-PubMed.
2 Enju: An English HPSG Parser
We developed an English HPSG parser, Enju 1
(Miyao and Tsujii, 2005; Hara et al, 2005; Ni-
nomiya et al, 2005). Table 1 shows the perfor-
mance. The F-score in the table was accuracy
of the predicate-argument relations output by the
parser. A predicate-argument relation is defined
as a tuple ??,wh, a, wa?, where ? is the predi-
cate type (e.g., adjective, intransitive verb), wh
is the head word of the predicate, a is the argu-
ment label (MOD, ARG1, ..., ARG4), and wa is
the head word of the argument. Precision/recall
is the ratio of tuples correctly identified by the
parser. The lexicon of the grammar was extracted
from Sections 02-21 of Penn Treebank (39,832
sentences). In the table, ?HPSG-PTB? means that
the statistical model was trained on Penn Tree-
bank. ?HPSG-GENIA? means that the statistical
model was trained on both Penn Treebank and GE-
NIA treebank as described in (Hara et al, 2005).
The GENIA treebank (Tateisi et al, 2005) consists
of 500 abstracts (4,446 sentences) extracted from
MEDLINE.
Figure 1 shows a part of the parse tree and fea-
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
17
Figure 1: Snapshot of Enju
ture structure for the sentence ?NASA officials
vowed to land Discovery early Tuesday at one
of three locations after weather conditions forced
them to scrub Monday?s scheduled return.?
3 MEDIE: a search engine for
MEDLINE
Figure 2 shows the top page of the MEDIE. ME-
DIE is an intelligent search engine for the accu-
rate retrieval of relational concepts from MED-
LINE 2 (Miyao et al, 2006). Prior to retrieval, all
sentences are annotated with predicate argument
structures and ontological identifiers by applying
Enju and a term recognizer.
3.1 Automatically Annotated Corpus
First, we applied a POS analyzer and then Enju.
The POS analyzer and HPSG parser are trained
by using the GENIA corpus (Tsuruoka et al,
2005; Hara et al, 2005), which comprises around
2,000 MEDLINE abstracts annotated with POS
and Penn Treebank style syntactic parse trees
(Tateisi et al, 2005). The HPSG parser generates
parse trees in a stand-off format that can be con-
verted to XML by combining it with the original
text.
We also annotated technical terms of genes and
diseases in our developed corpus. Technical terms
are annotated simply by exact matching of dictio-
2http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
nary entries and the terms separated by space, tab,
period, comma, hat, colon, semi-colon, brackets,
square brackets and slash in MEDLINE.
The entire dictionary was generated by apply-
ing the automatic generation method of name vari-
ations (Tsuruoka and Tsujii, 2004) to the GENA
dictionary for the gene names (Koike and Takagi,
2004) and the UMLS (Unified Medical Language
System) meta-thesaurus for the disease names
(Lindberg et al, 1993). It was generated by ap-
plying the name-variation generation method, and
we obtained 4,467,855 entries of a gene and dis-
ease dictionary.
3.2 Functions of MEDIE
MEDIE provides three types of search, seman-
tic search, keyword search, GCL search. GCL
search provides us the most fundamental and pow-
erful functions in which users can specify the
boolean relations, linear order relation and struc-
tural relations with variables. Trained users can
enjoy all functions in MEDIE by the GCL search,
but it is not easy for general users to write ap-
propriate queries for the parsed corpus. The se-
mantic search enables us to specify an event verb
with its subject and object easily. MEDIE auto-
matically generates the GCL query from the se-
mantic query, and runs the GCL search. Figure 3
shows the output of semantic search for the query
?What disease does dystrophin cause??. This ex-
ample will give us the most intuitive understand-
ings of the proximal and structural retrieval with a
richly annotated parsed corpus. MEDIE retrieves
sentences which include event verbs of ?cause?
and noun ?dystrophin? such that ?dystrophin? is the
subject of the event verbs. The event verb and its
subject and object are highlighted with designated
colors. As seen in the figure, small sentences in
relative clauses, passive forms or coordination are
retrieved. As the objects of the event verbs are
highlighted, we can easily see what disease dys-
trophin caused. As the target corpus is already
annotated with diseases entities, MEDIE can ef-
ficiently retrieve the disease expressions.
4 Info-PubMed: a GUI-based
MEDLINE search tool
Info-PubMed is a MEDLINE search tool with
GUI, helping users to find information about
biomedical entities such as genes, proteins, and
18
Figure 2: Snapshot of MEDIE: top page?
Figure 3: Snapshot of MEDIE: ?What disease does
dystrophin cause??
the interactions between them 3.
Info-PubMed provides information from MED-
LINE on protein-protein interactions. Given the
name of a gene or protein, it shows a list of the
names of other genes/proteins which co-occur in
sentences from MEDLINE, along with the fre-
quency of co-occurrence.
Co-occurrence of two proteins/genes in the
same sentence does not always imply that they in-
teract. For more accurate extraction of sentences
that indicate interactions, it is necessary to iden-
tify relations between the two substances. We
adopted PASs derived by Enju and constructed ex-
traction patterns on specific verbs and their argu-
ments based on the derived PASs (Yakusiji, 2006).
Figure 4: Snapshot of Info-PubMed (1)
Figure 5: Snapshot of Info-PubMed (2)
Figure 6: Snapshot of Info-PubMed (3)
4.1 Functions of Info-PubMed
In the ?Gene Searcher? window, enter the name
of a gene or protein that you are interested in.
For example, if you are interested in Raf1, type
?raf1? in the ?Gene Searcher? (Figure 4). You
will see a list of genes whose description in our
dictionary contains ?raf1? (Figure 5). Then, drag
3http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/
19
one of the GeneBoxes from the ?Gene Searcher?
to the ?Interaction Viewer.? You will see a list
of genes/proteins which co-occur in the same
sentences, along with co-occurrence frequency.
The GeneBox in the leftmost column is the one
you have moved to ?Interaction Viewer.? The
GeneBoxes in the second column correspond to
gene/proteins which co-occur in the same sen-
tences, followed by the boxes in the third column,
InteractionBoxes.
Drag an InteractionBox to ?ContentViewer? to
see the content of the box (Figure 6). An In-
teractionBox is a set of SentenceBoxes. A Sen-
tenceBox corresponds to a sentence in MEDLINE
in which the two gene/proteins co-occur. A Sen-
tenceBox indicates whether the co-occurrence in
the sentence is direct evidence of interaction or
not. If it is judged as direct evidence of interac-
tion, it is indicated as Interaction. Otherwise, it is
indicated as Co-occurrence.
5 Conclusion
We presented an English HPSG parser, Enju, a
search engine for relational concepts from MED-
LINE, MEDIE, and a GUI-based MEDLINE
search tool, Info-PubMed.
MEDIE and Info-PubMed demonstrate how the
results of deep parsing can be used for intelligent
text mining and semantic information retrieval in
the biomedical domain.
6 Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Proc.
of IJCNLP 2005.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid parsing
in probabilistic HPSG parsing. In Proc. of IWPT
2005, pages 103?114.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proc. of the IJCNLP 2005, Com-
panion volume, pp. 222?227.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proc. of ACL ?06,
to appear.
Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Part-of-speech tagger for
biomedical text. In Proc. of the 10th Panhellenic
Conference on Informatics.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Asako Koike and Toshihisa Takagi. 2004.
Gene/protein/family name recognition in biomed-
ical literature. In Proc. of HLT-NAACL 2004
Workshop: Biolink 2004, pages 9?16.
D.A. Lindberg, B.L. Humphreys, and A.T. McCray.
1993. The unified medical language system. Meth-
ods in Inf. Med., 32(4):281?291.
Akane Yakushiji. 2006. Relation Information Extrac-
tion Using Deep Syntactic Analysis. Ph.D. Thesis,
University of Tokyo.
20
Proceedings of the 10th Conference on Parsing Technologies, pages 11?22,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Evaluating Impact of Re-training a Lexical Disambiguation Model
on Domain Adaptation of an HPSG Parser
Tadayoshi Hara1 Yusuke Miyao1 Jun?ichi Tsujii1;2;3
1Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan
2School of Computer Science, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
3NaCTeM(National Center for Text Mining)
Manchester Interdisciplinary Biocentre, University of Manchester
131 Princess St, MANCHESTER M1 7DN, UK
E-mail: fharasan, yusuke, tsujiig@is.s.u-tokyo.ac.jp
Abstract
This paper describes an effective approach
to adapting an HPSG parser trained on the
Penn Treebank to a biomedical domain. In
this approach, we train probabilities of lex-
ical entry assignments to words in a tar-
get domain and then incorporate them into
the original parser. Experimental results
show that this method can obtain higher
parsing accuracy than previous work on do-
main adaptation for parsing the same data.
Moreover, the results show that the combi-
nation of the proposed method and the exist-
ing method achieves parsing accuracy that is
as high as that of an HPSG parser retrained
from scratch, but with much lower training
cost. We also evaluated our method in the
Brown corpus to show the portability of our
approach in another domain.
1 Introduction
Domain portability is an important aspect of the ap-
plicability of NLP tools to practical tasks. There-
fore, domain adaptation methods have recently been
proposed in several NLP areas, e.g., word sense dis-
ambiguation (Chan and Ng, 2006), statistical pars-
ing (Lease and Charniak, 2005; McClosky et al,
2006), and lexicalized-grammar parsing (Johnson
and Riezler, 2000; Hara et al, 2005). Their aim was
to re-train a probabilistic model for a new domain at
low cost, and more or less successfully improved the
accuracy for the domain.
In this paper, we propose a method for adapting
an HPSG parser (Miyao and Tsujii, 2002; Ninomiya
et al, 2006) trained on the WSJ section of the Penn
Treebank (Marcus et al, 1994) to a biomedical do-
main. Our method re-trains a probabilistic model of
lexical entry assignments to words in a target do-
main, and incorporates it into the original parser.
The model of lexical entry assignments is a log-
linear model re-trained with machine learning fea-
tures only of word n-grams. Hence, the cost for the
re-training is much lower than the cost of training
the entire disambiguation model from scratch.
In the experiments, we used an HPSG parser orig-
inally trained with the Penn Treebank, and evaluated
a disambiguation model re-trained with the GENIA
treebank (Kim et al, 2003), which consists of ab-
stracts of biomedical papers. We varied the size of
a training corpus, and measured the transition of the
parsing accuracy and the cost required for parameter
estimation. For comparison, we also examined other
possible approaches to adapting the same parser. In
addition, we applied our approach to the Brown cor-
pus (Kucera and Francis, 1967) in order to examine
portability of our approach.
The experimental results revealed that by sim-
ply re-training the probabilistic model of lexical en-
try assignments we achieve higher parsing accuracy
than with a previously proposed adaptation method.
In addition, combined with the existing adaptation
method, our approach achieves accuracy as high as
that obtained by re-training the original parser from
scratch, but with much lower training cost. In this
paper, we report these experimental results in detail,
and discuss how disambiguation models of lexical
entry assignments contribute to domain adaptation.
In recent years, it has been shown that lexical in-
11
formation plays a very important role for high accu-
racy of lexicalized grammar parsing. Bangalore and
Joshi (1999) indicated that, correct disambiguation
with supertagging, i.e., assignment of lexical entries
before parsing, enabled effective LTAG (Lexical-
ized Tree-Adjoining Grammar) parsing. Clark and
Curran (2004a) showed that supertagging reduced
cost for training and execution of a CCG (Combina-
tory Categorial Grammar) parser while keeping ac-
curacy. Clark and Curran (2006) showed that a CCG
parser trained on data derived from lexical category
sequences alone was only slightly less accurate than
one trained on complete dependency structures. Ni-
nomiya et al (2006) also succeeded in significantly
improving speed and accuracy of HPSG parsing by
using supertagging probabilities. These results indi-
cate that the probability of lexical entry assignments
is essential for parse disambiguation.
Such usefulness of lexical information has also
been shown for domain adaptation methods. Lease
and Charniak (2005) showed how existing domain-
specific lexical resources on a target domain may be
leveraged to augment PTB-training: part-of-speech
tags, dictionary collocations, and named-entities.
Our findings basically follow the above results. The
contribution of this paper is to provide empirical re-
sults of the relationships among domain variation,
probability of lexical entry assignment, training data
size, and training cost. In particular, this paper em-
pirically shows how much in-domain corpus is re-
quired for satisfiable performance.
In Section 2, we introduce an HPSG parser and
describe an existing method for domain adaptation.
In Section 3, we show our methods of re-training
a lexical disambiguation model and incorporating
it into the original model. In Section 4, we exam-
ine our method through experiments on the GENIA
treebank. In Section 5, we examine the portability
of our method through experiments on the Brown
corpus. In Section 6, we showed several recent re-
searches related to domain adaptation.
2 An HPSG Parser
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of grammar rules describe
general construction rules, and a large number of
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
Grammar Rule
3
1
Unification
HEAD 
SUBCAT < >
1
2
HEAD 
SUBCAT < >
3
2
HEAD 
SUBCAT < >
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
John has come
HEAD verb
SUBCAT <noun>
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
Lexical Entries
John has come
John has come
Figure 1: Parsing a sentence ?John has come.?
HEAD verb
SUBCAT <noun>
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
John has come
HEAD verb
SUBCAT <>
Figure 2: An HPSG parse tree for a sentence ?John
has come.?
lexical entries express word-specific characteristics.
The structures of sentences are explained using com-
binations of grammar rules and lexical entries.
Figure 1 shows an example of HPSG parsing of
the sentence ?John has come.? First, as shown at the
top of the figure, an HPSG parser assigns a lexical
entry to each word in this sentence. Next, a gram-
mar rule is assigned and applied to lexical entries. At
the middle of this figure, the grammar rule is applied
to the lexical entries for ?has? and ?come.? We then
obtain the structure represented at the bottom of the
figure. After that, the application of grammar rules
is done iteratively, and then we can finally obtain the
parse tree as is shown in Figure 2. In practice, since
two or more parse candidates can be given for one
sentence, a disambiguation model gives probabili-
ties to these candidates, and a candidate given the
highest probability is then chosen as a correct parse.
12
The HPSG parser used in this study is Ninomiya
et al (2006), which is based on Enju (Miyao and
Tsujii, 2005). Lexical entries of Enju were extracted
from the Penn Treebank (Marcus et al, 1994), which
consists of sentences collected from The Wall Street
Journal (Miyao et al, 2004). The disambiguation
model of Enju was trained on the same treebank.
The disambiguation model of Enju is based on
a feature forest model (Miyao and Tsujii, 2002),
which is a log-linear model (Berger et al, 1996) on
packed forest structure. The probability, p
E
(tjw),
of producing the parse result t for a given sentence
w = hw
1
; :::; w
u
i is defined as
p
E
(tjw) =
1
Z
s
Y
i
p
lex
(l
i
jw; i)  q
syn
(tjl);
Z
s
=
X
t2T (w)
Y
i
p
lex
(l
i
jw; i)  q
syn
(tjl)
where l = hl
1
; :::; l
u
i is a list of lexical entries as-
signed to w, p
lex
(l
i
jw; i) is a probabilistic model
giving the probability that lexical entry l
i
is assigned
to word w
i
, q
syn
(tjl) is an unnormalized log-linear
model of tree construction and gives the possibil-
ity that parse candidate t is produced from lexical
entries l, and T (w) is a set of parse candidates as-
signed to w. With a treebank of a target domain as
training data, model parameters of p
lex
and q
syn
are
estimated so as to maximize the log-likelihood of the
training data.
Probabilistic model p
lex
is defined as a log-linear
model as follows.
p
lex
(l
i
jw; i) =
1
Z
w
i
exp
 
X
j

j
f
j
(l
i
;w; i)
!
;
Z
w
i
=
X
l
i
2L(w
i
)
exp
 
X
j

j
f
j
(l
i
;w; i)
!
;
where L(w
i
) is a set of lexical entries which can
be assigned to word w
i
. Before training this model,
L(w
i
) for all w
i
are extracted from the training tree-
bank. The feature function f
j
(l
i
;w; i) represents the
characteristics of l
i
, w and w
i
, while corresponding

j
is its weight. For the feature functions, instead of
using unigram features adopted in Miyao and Tsujii
(2005), Ninomiya et al (2006) used ?word trigram?
and ?POS 5-gram? features which are listed in Ta-
ble 1. With the revised Enju model, they achieved
Table 1: Features for the probabilities of lexical en-
try selection
surrounding words w
 1
w
0
w
1
(word trigram)
surrounding POS tags p
 2
p
 1
p
0
p
1
p
2
(POS 5-gram)
combinations w
 1
w
0
; w
0
w
1
; p
 1
w
0
; p
0
w
0
;
p
1
w
0
; p
0
p
1
p
2
p
3
; p
 2
p
 1
p
0
;
p
 1
p
0
p
1
; p
0
p
1
p
2
; p
 2
p
 1
;
p
 1
p
0
; p
0
p
1
; p
1
p
2
parsing accuracy as high as Miyao and Tsujii (2005),
with around four times faster parsing speed.
Johnson and Riezler (2000) suggested the pos-
sibility of the method for adapting a stochastic
unification-based grammar including HPSG to an-
other domain. They incorporated auxiliary distribu-
tions as additional features for an original log-linear
model, and then attempted to assign proper weights
to the new features. With this approach, they suc-
ceeded in decreasing to a degree indistinguishable
sentences for a target grammar.
Our previous work proposed a method for adapt-
ing an HPSG parser trained on the Penn Treebank
to a biomedical domain (Hara et al, 2005). We
re-trained a disambiguation model of tree construc-
tion, i.e., q
syn
, for the target domain. In this ap-
proach, q
syn
of the original parser was used as a
reference distribution (Jelinek, 1998) of another log-
linear model, and the new model was trained using a
target treebank. Since re-training used only a small
treebank of the target domain, the cost was small and
parsing accuracy was successfully improved.
3 Re-training of a Disambiguation Model
of Lexical Entry Assignments
Our idea of domain adaptation is to train a disam-
biguation model of lexical entry assignments for the
target domain and then incorporate it into the origi-
nal parser. Since Enju includes the disambiguation
model of lexical entry assignments as p
lex
, we can
implement our method in Enju by training another
disambiguation model p0
lex
(l
i
jw; i) of lexical entry
assignments for the biomedical domain, and then re-
placing the original p
lex
with the newly trained p0
lex
.
In this paper, for p0
lex
, we train a disambigua-
tion model p
lex mix
(l
i
jw; i) of lexical entry assign-
ments. p
lex mix
is a maximum entropy model and
the feature functions for it is the same as p
lex
as
13
given in Table 1. With these feature functions, we
train p
lex mix
on the treebanks both of the original
and biomedical domains.
In the experiments, we examine the contribution
of our method to parsing accuracy. In addition, we
implement several other possible methods for com-
parison of the performances.
baseline: use the original model of Enju
GENIA only: execute the same method of training
the disambiguation model of Enju, using only
the GENIA treebank
Mixture: execute the same method of training the
disambiguation model of Enju, using both of
the Penn Treebank and the GENIA treebank (a
kind of smoothing method)
HMT05: execute the method proposed in our pre-
vious work (Hara et al, 2005)
Our method: replace p
lex
in the original model
with p
lex mix
, while leaving q
syn
as it is
Our method (GENIA): replace p
lex
in the original
model with p
lex genia
, which is a probabilistic
model of lexical entry assignments trained only
with the GENIA treebank, while leaving q
syn
as it is
Our method + GENIA: replace p
lex
in the original
model with p
lex mix
and q
syn
with q
syn genia
,
which is a disambiguation model of tree con-
struction trained with the GENIA treebank
Our method + HMT05: replace p
lex
in the orig-
inal model with p
lex mix
and q
syn
with the
model re-trained with our previous method
(Hara et al, 2005) (the combination of our
method and the ?HMT05? method)
baseline (lex): use only p
lex
as a disambiguation
model
GENIA only (lex): use only p
lex genia
as a disam-
biguation model, which is a probabilistic model
of lexical entry assignments trained only with
the GENIA treebank
Mixture (lex): use only p
lex mix
as a disambigua-
tion model
The ?baseline? method does no adaptation to the
biomedical domain, and therefore gives lower pars-
ing accuracy for the domain than for the original do-
main. This method is regarded as the baseline of
the experiments. The ?GENIA only? method relies
solely on the treebank for the biomedical domain,
and therefore it cannot work well with the small tree-
bank. The ?Mixture? method is a kind of smoothing
method using all available training data at the same
time, and therefore the method can give the highest
accuracy of the three, which would be regarded as
the ideal accuracy with the naive methods. However,
training this model is expected to be very costly.
The ?baseline (lex),? ?GENIA only (lex),? and
?Mixture (lex)? approaches rely solely on models of
lexical entry assignments, and show lower accuracy
than those that contain both of models of lexical en-
try assignments and tree constructions. These ap-
proaches can be utilized as indicators of importance
of combining the two types of models.
Our previous work (Hara et al, 2005) showed that
the model trained with the ?HMT05? method can
give higher accuracy than the ?baseline? method,
even with the small amount of the treebanks in the
biomedical domain. The model also takes much less
cost to train than with the ?Mixture? method. How-
ever, they reported that the method could not give as
high accuracy as the ?Mixture? method.
4 Experiments with the GENIA Corpus
4.1 Experimental Settings
We implemented the models shown in Section 3,
and then evaluated the performance of them. The
original parser, Enju, was developed on Section 02-
21 of the Penn Treebank (39,832 sentences) (Miyao
and Tsujii, 2005; Ninomiya et al, 2006). For
training those models, we used the GENIA tree-
bank (Kim et al, 2003), which consisted of 1,200
abstracts (10,848 sentences) extracted from MED-
LINE. We divided it into three sets of 900, 150, and
150 abstracts (8,127, 1,361, and 1,360 sentences),
and these sets were used respectively as training, de-
velopment, and final evaluation data. The method
of Gaussian MAP estimation (Chen and Rosenfeld,
1999) was used for smoothing. The meta parameter
 of the Gaussian distribution was determined so as
to maximize the accuracy on the development set.
14
  
  
  
  
  
   
  
 
                 
	 
                   
 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 180?191,
Paris, October 2009. c?2009 Association for Computational Linguistics
Effective Analysis of Causes and Inter-dependencies of Parsing Errors
Tadayoshi Hara1 Yusuke Miyao1
1Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JAPAN
2School of Computer Science, University of Manchester
3NaCTeM (National Center for Text Mining)
{harasan,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii1,2,3
Abstract
In this paper, we propose two methods for
analyzing errors in parsing. One is to clas-
sify errors into categories which grammar
developers can easily associate with de-
fects in grammar or a parsing model and
thus its improvement. The other is to
discover inter-dependencies among errors,
and thus grammar developers can focus on
errors which are crucial for improving the
performance of a parsing model.
The first method uses patterns of er-
rors to associate them with categories of
causes for those errors, such as errors in
scope determination of coordination, PP-
attachment, identification of antecedent of
relative clauses, etc. On the other hand,
the second method, which is based on re-
parsing with one of observed errors cor-
rected, assesses inter-dependencies among
errors by examining which other errors
were to be corrected as a result if a spe-
cific error was corrected.
Experiments show that these two meth-
ods are complementary and by being com-
bined, they can provide useful clues as to
how to improve a given grammar.
1 Introduction
In any kind of complex systems, analyzing causes
of errors is a crucial step for improving its perfor-
mance. In recent sophisticated parsing technolo-
gies, the step of error analysis has been becoming
more and more convoluted and time-consuming,
if not impossible. While common performance
evaluation measures such as F-values are useful to
compare the performance of systems or evaluate
improvement of a system, they hardly give useful
clues as to how to improve a system. Evaluation
measures usually assume uniform units such as the
number of correctly or incorrectly recognized con-
stituent boundaries and their labels, or in a similar
vein, dependency links among words and their la-
bels, and then compute single values such as the F-
value. These values do not give any insights as to
where the weaknesses exist in a parsing model. As
a result, the improvement process takes the form
of time consuming trial-error cycles.
Once grammar developers know the actual dis-
tribution of errors across different categories such
as PP-attachment, complement/adjunct distinc-
tion, gerund/participle distinction, etc., they can
think of focused and systematic improvement of
a parsing model.
Another problem of the F-value in terms of
uniform units is that it does not take inter-
dependencies among errors into consideration. In
particular, for parsers based on grammar for-
malisms such as LFG (Kaplan and Bresnan, 1995),
HPSG (Pollard and Sag, 1994), or CCG (Steed-
man, 2000), units (eg. single predicate-argument
links) are inter-related through hierarchical struc-
tures and structure sharing assumed by these for-
malisms. Single errors are inherently propagated
to other sets of errors. This is also the case, though
to a lesser extent, for parsing models in which
shallow parsing is followed by another component
for semantic label assignment.
In order to address these two issues, we propose
two methods in this paper. One is to recognize
cause categories of errors and the other is to cap-
ture inter-dependencies among errors. The former
method defines various patterns of errors to iden-
tify categories of error causes. The latter method
re-parses a sentence with a single target error cor-
rected, and regards the errors which are corrected
in re-parse as errors dependent on the target.
Although these two methods are implemented
for a specific parser using HPSG (Miyao and Tsu-
jii, 2005; Ninomiya et al, 2006), the same ideas
can be applied to any type of parsing models.
180
Predicate 
Sentence: John    has    come
Predicative event 1:
Predicative event 2:
Word:  hasGrammatical nature:  auxiliary# of arguments:  2
Argument 1 John
Argument 2 come
Predicate 
Word:  comeGrammatical nature:  verb# of arguments:  1
Argument 1 John
Predicate-argument relations
Figure 1: Predicate-argument relations
John aux_2args
ARG1 ARG2
verb_1arg
ARG1
has : come :
Figure 2: Representation of predicate-argument
relations
In the following, Section 2 introduces a parser
and its evaluation metrics, Section 3 illustrates dif-
ficulties in analyzing parsing errors based on com-
mon evaluation measures, and Section 4 proposes
the two methods for effective error analysis. Sec-
tion 5 presents experimental results which show
how our methods work for analyzing actual pars-
ing errors. Section 6 and Section 7 illustrate fur-
ther application of these methods to related topics.
Section 8 summarizes this research and indicates
some of future directions.
2 A parser and its evaluation
A parser is a system which interprets given sen-
tences in terms of structures derived from syn-
tactic or in some cases semantic viewpoints, and
structures constructed as a result are used as es-
sential information for various tasks of natural lan-
guage processing such as information extraction,
machine translation, and so on.
In this paper, we address issues involved in im-
proving the performance of a parser which pro-
duces structural representations deeper than sur-
face constituent structures. Such a parser is called
a ?deep parser.? In many deep parsers, the output
structure is defined by a linguistics-based gram-
mar framework such as CFG, CCG (Steedman,
2000), LFG (Kaplan and Bresnan, 1995) or HPSG
Abbr. Full Abbr. Full
aux auxiliary conj conjunction
prep prepositional lgs logical subject
verb verb app apposition
coord coordination relative relative
det determiner Narg(s) takes N arguments
adj adjunction mod modifies a word
Table 1: Descriptions for predicate types
(Pollard and Sag, 1994). Alternatively, some deep
parsing models assume staged processing in which
a stage of shallow parsing is followed by a stage of
semantic role labeling, which assigns labels indi-
cating semantic relationships between predicates
and their arguments. In either case, we assume a
parser to produce a single ?deep? structural rep-
resentation for a given sentence, which is chosen
from a set of possible interpretations as the most
probable one by a disambiguation model.
For evaluation of the performance of a parser,
various metrics have been introduced according
to the structure captured by a given grammar
formalism or a system of semantic labels. In
most cases, instead of examining correctness for
a whole structure, a parser is evaluated in terms of
the F-value which shows how correctly it recog-
nizes relationships among words and assigns ?la-
bels? to the relationships in the structure. In this
paper, we assume a certain type of ?predicate-
argument relation.?
In this measurement, a structure given for a
sentence is decomposed into a set of predicative
words and their arguments. A predicate takes
other words as its arguments. In our representa-
tion, the arguments are labeled by semantically
neutral labels such as ARGn(n = 1...5) and
MOD. In this representation, a basic unit is a
triplet, such as
<Predicate:PredicateType,
ArgumentLabel,
Argument>,
where ?Predicate? and ?Argument? are surface
words. As shown in the examples in Section 4,
?PredicateType? bears extra information concern-
ing the syntactic construction in which the triplet
is embedded. ARG1-ARG5 express relations be-
tween a Head and its complement, while MOD ex-
presses a relation between an Adjunct and its mod-
ifiee. Since all dependency relations are expressed
by triplets, triplets contain not only semantic de-
181
I saw a girl with a telescope Correct answer:
ARG1 ARG2
ARG1 ARG2
Parser output:
ARG1 ARG2
ARG1 ARG2Compare
Error (25%): 
ARG1
ARG1
Correct (75%):
ARG1 ARG2
ARG2ARG1 ARG2
ARG2
I saw a girl with a telescope 
I saw a girl with a telescope 
I saw a girl with a telescope 
Figure 3: An example of parsing performance
evaluations
pendencies but also many dependencies which are
essentially syntactic in nature. Figure 1 shows an
example used in Miyao and Tsujii (2005) and Ni-
nomiya et al (2006).
This example shows predicate-argument rela-
tions for ?John has come.? There are two pred-
icates in this sentence, ?has? and ?come?. The
word ?has?, which is used as an auxiliary verb,
takes two words, ?John? and ?come?, as its ar-
guments, and therefore two triplets of predicate-
argument relation, <has ARG1 John> and <has
ARG2 come>. As for the predicative word
?come?, we have one triplet <come ARG1 John>.
Note that, in this HPSG analysis, the auxiliary
verb ?has? is analyzed in such a way that it takes
one NP as subject and one VP as complement,
and that the subject of the auxiliary verb is shared
by the verb (?come?) in VP as its subject (Fig-
ure 2). The fact that ?has? in this sentence is an
auxiliary verb is indicated by the ?PredicateType?,
aux 2args. A ?PredicateType? consists of a type
and the number of arguments it takes (Table 1).
3 Difficulties in analyzing parsing errors
Figure 3 shows an example of the evaluation of
the parser based on these predicate-argument rela-
tions. Note that the predicate types are abbreviated
in this figure. In the sentence ?I saw a girl with a
telescope?, there should be four triplets for the two
predicates, ?saw? and ?with,? each of which takes
Error:
They completed the sale of for 
ARG1ARG1
it to him $1,000 
Conflict
Analysis 2: (Impossible)
They completed the sale of for ARG1ARG1
it to him $1,000 
Analysis 1: (Possible) ARG1ARG1
ARG1ARG1
Can each error occur independently?
They completed the sale of for ARG1 ARG1
it to him $1,000 
ARG1ARG1
Figure 4: Sketch of error propagation
The book on which read the shelf  I yesterdayARG1 ARG2
ARG2ARG1
Error:
Figure 5: Parsing errors around one relative clause
attachment
two arguments. Although the parser output does
indeed contain four triplets, the first argument of
?with? is not the correct one. Thus, this output is
erroneous, with the F-value of 75%.
While the F-value thus computed is fine for cap-
turing the performance of a parser, it does not offer
any help for improving its performance.
First, because it does not give any indica-
tion on what portion of erroneous triplets are in
PP-attachment, complement/adjunct distinction,
gerund/participle distinction, etc., one cannot de-
termine which part of a parsing model should be
improved. In order to identify error categories, we
have to manually compare a parsing output with
a correct parse and classify them. Consider again
the example in Figure 3. We can easily observe
that ?ARG1? of predicate ?with? was mistaken. In
this case, the word linked via ?ARG1? represents
a modifiee of the prepositional phrase, and thereby
we conclude that the error is in PP-attachment.
While the process looks straightforward for this
simple sentence and error, to perform such a man-
ual inspection for all sentences and more complex
types of errors is costly, and becomes inhibitive
when the size of a test set of sentences is realisti-
182
cally large.
Another problem with the F-value is that it ig-
nores inter-dependencies among errors. Since the
F-value does not consider inter-dependencies, one
cannot determine which errors are more crucial
than others in terms of the performance of the sys-
tem as a whole.
A simple example of inter-dependency is shown
in Figure 4. ?ARG1? of ?for? and ?to? were mis-
taken by a parser, both of which can be classified
as PP-attachments as in Figure 3. However, the
two errors are not independent. The former error
can occur by itself (Analysis 1) while the latter
cannot because of the structural conflict with the
former (Analysis 2). The occurrence of the latter
error thus forces the former.
Moreover, inter-dependency in a deep parser
based on linguistics-based formalisms can be
complicated. Error propagation is ingrained in
grammar itself. Consider Figure 5. In this exam-
ple, a wrong decision on the antecedent of a rela-
tive clause results in a wrong triplet of the predi-
cate in the embedded clause with the antecedent.
That is, the two erroneous triplets, one of the
?ARG1? of ?which? and the other of the ?ARG2?
of ?read,? were caused by a single wrong deci-
sion of the antecedent of a relative clause. Such
a propagation of errors can be even more compli-
cated, for example, when the predicate in the rela-
tive clause is a control verb.
In the following section we propose two meth-
ods for analyzing errors. Although both meth-
ods are implemented for the specific parser Enju
(Miyao and Tsujii, 2005; Ninomiya et al, 2006),
the same ideas can be implemented for any parsing
model.
4 Methods for effective error analysis
4.1 Recognizing categories of error causes
While the Enju parser produces rich feature struc-
tures as output, the performance is evaluated by
the F-value in terms of basic units of predicate-
argment structure. As we illustrated in Section 2,
the basic unit is a triplet in the following form.
<Predicate:PredicateType,
ArgumentLabel,
Argument>
We illustrated in Section 2 how we can identify
errors in PP-attachment simply by examining a
The  car  was  designed to : use   it  for ...
Correct output:
aux_2argsto :[ verb1 ] ?ARG3 [ verb2 ]Parser output:
aux_mod_2args
MOD
to :
ARG2
Unknown subject ARG1 ARG1
[ verb1 ] ? [ verb2 ]
aux_2args
Example:
Parser output:
Correct answer:
ARG3
The  car  was  designed to : use   it  for ...aux_mod_2args
MOD ARG2
Unknown subject ARG1 ARG1
Pattern:
(Patterns of correct answer and parser output can be interchanged)
Figure 6: Pattern for ?To-infinitive for modi-
fier/argument of verb?
triplet produced by the parser with the correspond-
ing triplet in the gold standard parse.
However, in more complex cases, we have to
consider a set of mismatched triplets collectively
in order to map errors to meaningful error causes.
The following are typical examples of error causes
and pattern rules which identify them.
(1) Interpretation of Infinitival Clauses as Adjunct
or Complement
Two different types of interpretations of the in-
finitival clauses are explicitly indicated by ?Predi-
cateType.? Consider the following two sentences.
(a) [Infinitival clause as an adjunct of the main
clause]
The car was designed (by John) to use it for
business trips.
(b) [Infinitival clause as an argument of catena-
tive verb]
The car is designed to run fast.
In both sentences, ?to? is treated as a predicate to
represent the infinitival clauses in triplets. How-
ever, Enju marks the ?PredicateType? of (a) as
?aux-mod-2args,? while it marks the predicate
simply as ?aux-2args? in (b). Furthermore, the
linkage between the main clause and the infinitival
clause is treated differently. In (a), the infinitival
clause takes the main clause with relation MOD,
while in (b) the main clause takes the infinitival
183
[ gerund ]: verb_Narg(s)Parser output: [ gerund ]: verb_mod_Narg(s)Correct answer:(Patterns of correct answer and parser output can be interchanged)
Pattern:
Example:
The customers walk the door
a   package   for   them
expecting: verb_mod_3args
you to have
in MOD
ARG1
ARG2 ARG3
Parser output:
Correct output:
The customers walk the door
a   package   for   them
expecting: verb_3args
you to have
in
Not exist 
ARG2 ARG3
ARG1 (MOD)
? ?
? ?
Figure 7: Pattern for ?Gerund acts as modifier or
not?
clause as ARG3. Furthermore, in the catenative
verb interpretation of ?designed?, the deep object
(the surface subject in this example) fills ARG1
of the verb in the infinitival clause (complement),
while in the adjunct interpretation, the deep sub-
ject which is missing in this sentence occupies
the same role. Consequently, a single erroneous
choice between these two interpretations results in
a set of mismatched triplets.
We recognize such a set of mismatched triplets
by a pattern rule (Figure 6) and map them to this
type of error cause.
(2) Interpretation of Gerund-Participle interpreta-
tions
A treatment similar to (1) is taken for different
interpretations of Gerund. Interpretation as Ad-
junct of a main clause is signaled by the ?Predi-
cateType? verb-mod-*, while an interpretation as
a modifier of a noun is represented by the ?Predi-
cateType? verb (Figure 7).
(3) Interpretation of ?by?
A prepositional phrase with ?by? in a passive
clause can be interpreted as a deep subject, while
the same phrase can be interpreted as an ordinary
PP phrase that is used as an adjunct. The first in-
terpretation is marked by the ?PredicateType? lgs
(logical subject) which takes only one argument.
The relationship between the passivized verb and
the deep subject is captured by ARG1 which goes
Example:
Pattern:
Correct output:
Parser output: prep_2args
Unknown subject
[ verb1 ] ?ARG1ARG1 ?
lgs_1arg ARG1[ verb1 ] ? ?ARG1
A 50-state study released in September  by : Friends ?
Unknown subject
? ARG1ARG1
prep_2argsParser output:
Correct answer:
A 50-state study released in September  by : Friends ??
ARG1ARG1
lgs_1argARG1
(Patterns of correct answer and parser output can be interchanged)
Figure 8: Pattern for ?Subject for passive sentence
or not?
Example:
Pattern:
relative_1arg
ARG1
Parser output: ARG1/2
Error
Parser output:
Correct answer:
The  book on which : read 
ARG1
the  shelf  I yesterday
ARG2
The  book on which : read 
ARG1
the  shelf  I yesterday
ARG2
relative_1arg
relative_1arg
Error
? ?
Figure 9: Pattern for ?Relative clause attachment?
from the verb to the noun phrase. On the other
hand, in the interpretation as an ordinary PP, the
preposition as predicate links the main verb and
NP via ARG1 and ARG2, respectively (Figure 8).
Again, a set of mismatched triplets should be
mapped to a single cause of errors via a pattern
rule.
(4) Antecedent of a Relative Clause
This type of error is manifested by two mis-
matched triplets with different predicates. This is
because a wrong choice of antecedent for a rela-
tive clause results in a wrong link for the trace of
the relative clause.
Since a relative clause pronoun is treated as a
184
Cause categories Patterns
[Argument selection]
Prepositional attachment ARG1 of prep *
Adjunction attachment ARG1 of adj *
Conjunction attachment ARG1 of conj *
Head selection for noun phrase ARG1 of det *
Coordination ARG1/2 of coord *
[Predicate type selection]
Preposition/Adjunction prep * ? adj *
Gerund acts as modifier/not verb mod Narg(s)
? verb Narg(s)
Coordination/conjunction coord * ? conj *
# of arguments for preposition prep Marg(s)
? prep Narg(s)
Adjunction/adjunctive noun adj * ? noun *
[More structural errors]
To-infinitive for see Figure 6
modifier/argument of verb
Subject for passive sentence/not see Figure 8
[Others]
Comma any error around ?,?
Relative clause attachment see Figure 9
Table 2: Defined patterns for cause categories
predicate which takes the antecedent as its single
argument, identification of error type can be done
simply by looking at ARG1. However, since the
errors usually propagate to the triplets that contain
their traces, we have to map them together to the
single error (Figure 9).
Table 2 shows the errors across different types
which our current version of pattern rules can
identify.
4.2 Capturing inter-dependencies among
errors
Some inter-dependencies among erroneous
triplets are ingrained in grammar, such as the case
of antecedent of a relative clause in (4) in Section
4.1. Some are caused by general constraints such
as the projection principle in dependency structure
(Figure 4 in Section 2).
Regardless of causes of dependencies, to recog-
nize inter-dependencies among errors is a crucial
step of effective error analysis.
Our method consists of the following four steps:
[Step 1] Re-parsing a target sentence: A given sen-
tence is re-parsed under the condition where an er-
ror is forcibly corrected.
[Step 2] Forming a network of inter-dependencies
of errors: By comparing the new parse result (a
set of triplets) with the initial parse result, this
step creates a directed graph of errors in the ini-
1
Re-parse a sentence under the condition whereeach error is forcibly corrected 
1
2
3
Correct 2
1
1
Form inter-dependent error groups anderror propagation network
4 1
433CorrectCorrect
Correct
disappear
disappear
disappear
disappear
,
,
,
1 2 3 4
ARG1our work force todayon
Errors:
ARG1 ARG1ARG2
ARG2 ARG1 ARG1 ARG1It  has  no  bearing
2 3 4
5
5 4Correct disappear1 32, , ,
4,
2 4,,
2 3,,
5Propagation
Resultant network:
Inter-dependent error group Inter-dependent error group
(a)
(b)
(c)
Inter-dependency among errors:re-parse
re-parse
re-parse
re-parse
re-parse
Figure 10: Schema of capturing inter-
dependencies
tial parse. A directed link shows that correction of
the error in the starting node produces a new parse
result in which the error in the receiving node of
the link disappears.
[Step 3] Forming groups of inter-dependent errors:
This step recognizes a group of inter-dependent er-
rors which forms a directed circle in the network
created by [Step 2].
[Step 4] Forming a network of error propagation:
This step creates a new network by reducing each
of inter-dependent error groups of [Step 3] to a sin-
gle node.
Figure 10 illustrates how these steps work. In
this example, while ?today? should modify the
noun phrase ?our work force?, the initial parse
wrongly takes ?today? as the head noun of the
whole noun phrase. As a result, there are five er-
rors; three wrong outputs, ?ARG2? of ?on? (Er-
ror 1), ?ARG1? of ?our? (Error 2) and ?ARG1?
of ?work? (Error 3). There is an extra triplet for
?ARG1? of ?force? (Error 4), and a triplet for
?ARG1? of ?today? (Error 5) is missing (Figure
10 (a)).
Figure 10 (b) shows inter-dependencies among
the errors recognized by [Step 2], and Figure 10
185
# ofCause categories of errors Errors Locations
Classified 2,078 1,671
[Argument selection]
Prepositional attachment 579 579
Adjunction attachment 261 261
Conjunction attachment 43 40
Head selection for noun phrase 30 30
Coordination 202 184
[Predicate type selection]
Preposition/Adjunction 108 54
Gerund acts as modifier/not 84 31
Coordination/conjunction 54 27
# of arguments for preposition 51 17
Adjunction/adjunctive noun 13 13
[More structural errors]
To-infinitive for 120 22
modifier/argument of verb
Subject for passive sentence/not 8 3
[Others]
Comma 444 372
Relative clause attachment 102 38
Unclassified 2,631 ?
Total 4,709 ?
Table 3: Errors classified into cause categories
(c) shows what the resultant network looks like.
An inter-dependent error group of 1, 2, 3 and 4 is
recognized by [Step 3] and represented as a single
node. Error 5 is propagated to this node in the final
network.
5 Experiments
We applied our methods to the analyses of actual
errors produced by Enju. This version of Enju was
trained on the Penn Treebank (Marcus et al, 1994)
Section 2-21.
5.1 Observation of identified cause categories
We first parsed sentences in PTB Section 22, and
based on the observation of errors, we defined the
patterns in Section 4. We then parsed sentences in
Section 0. The errors in Section 0 were mapped to
error cause categories by the pattern rules created
for Section 22.
Table 3 summarizes the distribution across the
causes of errors. The left and right numbers in the
table show the number of erroneous triplets clas-
sified into the categories and the frequency of the
patterns matched, respectively. The table shows
that, with the 14 pattern rules, we successfully ob-
served 1,671 hits and 2,078 erroneous triplets are
dealt with by these hits. This amounts to more
than 40% erroneous triplets (2,078/4,709). Since
this was the first attempt, we expect the coverage
can be easily improved by adding new patterns.
Evaluated sentences (erroneous) 1,811 (1,009)
Errors (Correctable) 4,709 (3,085)
Inter-dependent error groups 1,978
Correction propagations 501
F-score (LP/LR) 90.69 (90.78/90.59)
Table 4: Summary of inter-dependencies




       	 
 










Figure 11: Frequency of each size of inter-
dependent error group
From the table, we can observe that a signif-
icant portion of errors is covered by simple types
of error causes such as PP-attachment and Adjunct
attachment. They are simple in the sense that the
number of erroneous triplets treated and the fre-
quency of the pattern application coincide. How-
ever, their conceived significance may be over-
rated. These simple types may constitute parts of
more complex error causes. Furthermore, since
pattern rules for these simple causes are easy to
prepare and have already been covered by the cur-
rent version, most of the remaining 60% of the er-
roneous triplets are likely to require patterns for
more complex causes.
On the other hand, patterns for complex causes
collect more erroneous triplets once they are fired.
This tendency is more noticeable in structural pat-
terns of errors. For example, in ?To-infinitive for
modifier/argument of verb,? there were only 22
hits for the pattern, while the number of erroneous
triplets is 120. This implies five triplets per hit.
This is because, in a deep parser, a wrong choice
between adjunct or complement interpretations of
a to-infinitival clause affects the interpretation of
implicit arguments in the clause through control.
Though expected, such detailed observations show
how differences between shallow and deep parsers
may affect evaluation methods and the methods of
analyzing errors.
5.2 Observation of inter-dependencies
In the inter-dependency experiments we per-
formed, some errors could not be forcibly cor-
rected by our method. This was because the parser
186
The  asbestos  fiber  ,  crocidolite ,  is  unusually  resilient  once
it  enters  the  lungs  ,  with  even  brief  exposures  to  it causing
symptoms  that  show  up  decades  later  ,  researchers  said  .
(a) (b)(c) (d)
? fiber      , : crocidolite ?app_2args
? fiber      , : crocidolite ?coord_2args
Correct answer:
Parser output:
? is   usually   resilient   ? the   lungs      ,       with   ?
? symptoms   that    show : up   decades   later  ?
Parser output:
Correct answer: verb_1arg
? symptoms   that    show : up   decades   later  ?verb_2args
? it   causing   symptoms   that   show   up   decades   later  ?
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):
Inter-dependent error group (c):
Inter-dependent error group (d):
ARG1 ARG2
ARG1 ARG2
ARG1ARG1
ARG1ARG1
ARG1
ARG1
ARG1
ARG2
ARG1
ARG1
Figure 12: Obtained inter-dependent error groups
we use prunes less probable parse substructures
during parsing. In some cases, even if we gave a
large positive value to a triplet which should be in-
cluded in the final parse, parsing paths which can
contain the triplet were pruned before. In this re-
search, we ignored such errors as ?uncorrectable?
ones, and focused on the remaining ?correctable?
errors.
Table 4 shows a summary of the analysis. As the
previous experiment, Enju produced 4,709 errors
for Section 0, of which 3,085 were correctable. By
applying the method illustrated in Section 4.2, we
obtained 1,978 inter-dependent error groups and
501 correction propagation relationships among
the groups.
Figure 11 shows the frequency of the size of
inter-dependent error groups. About half of the
groups contain only single errors which could
have only one-way correction propagations with
other errors or were completely independent of
other errors.
Figure 12 shows an example of the extracted
inter-dependent error groups. For the sentence
shown at the top, Enju gave seven errors. By ap-
plying the method in Section 4.2, these errors were
grouped into four inter-dependent error groups (a)
to (d), and no correction propagations were de-
She  says  she  offered  Mrs.  Yeargin a  quiet  resignation  and
thought  she  could  help  save  her  teaching  certificate .(a) (b)
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):Correct answer:
Parser output:
? she  could  help  save : her  teaching  certificate .verb_3args
? she  could  help  save : her  teaching  certificate .verb_2args
Correct answer:
Parser output:
? thought   she  could   help : save   ?verb_2args
? thought   she  could   help : save   ?aux_2args
Correction propagation from (a) to (b)
ARG2
ARG2
ARG2
ARG2
ARG1 ARG2
ARG1 ARG2
ARG3ARG1
ARG1
ARG2
ARG2
ARG1
Figure 13: Correction propagation between ob-
tained inter-dependent error groups
tected among them. Group (a) contains two errors
on the comma?s local behavior as apposition or co-
ordination. Group (b) contains the errors on the
words which give almost the same attachment be-
haviors. Group (c) contains the errors on whether
the verb ?show? took ?decades? as its object or
not. Group (d) contains an error on the attachment
of the adverb ?later?. Regardless of the overlap
of the regions in the sentence for (c) and (d), our
approach successfully grouped the errors into two
independent groups. The method shows that the
errors in each group are inter-dependent, but er-
rors in one group are independent of those in an-
other. This enables us to concentrate on each of
the co-occurring error groups separately, without
minding the errors in other groups.
Figure 13 shows another example. In this ex-
ample, eight errors for a sentence were classified
into two inter-dependent error groups (a) and (b).
Moreover, it shows that the correction of group (a)
results in correction of group (b).
The errors in group (a) were related to the
choice as to whether ?help? had an auxiliary or
a pure verbal role. The errors in group (b) were
related with the choice as to whether ?save? took
only one object (?her teaching certificate?) or two
objects (?her? and ?teaching certificate?). Be-
tween group (a) and (b), no ?structural? con-
187
It  invests  heavily  in  dollar-denominated  securities  overseas  and
is  currently  waiving  management  fees  ,  which  boosts  its yield .
(a)(b)
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):
Adjunction attachmentCause categories: 
Comma, Relative clause attachmentCause categories: 
It  invests  heavily  in  ? securities  overseas : ?adj_1argARG1 ARG1
? is  currently  waiving  management  fees   ,   which   boosts  ?
ARG1ARG1ARG1
ARG1ARG1ARG1
Figure 14: Combining our two methods (1)
flict could arise when correcting only each of the
groups. We could then hypothesize that the cor-
rection propagation between the two groups were
caused by the disambiguation model.
By dividing the errors into minimal units and
clarifying the effects of correcting a target error,
we can conclude that the inter-dependent group
(a) should be handled first for effective improve-
ment of the parser. In such a way, obtained inter-
dependencies among errors can suggest an effec-
tive strategy for parser improvement.
5.3 Combination of the two methods
By combining the two methods described in Sec-
tion 4.1 and 4.2, we can see how each error cause
affects the performance of a parser. The results
are summarized in Table 5. The leftmost column
in the table shows the numbers of errors in terms
of triplets, which are the same as the leftmost col-
umn in Table 3.
The ?independence rate? shows the ratio of er-
roneous triplets in the category which are not af-
fected by correction of other erroneous triplets. On
the other hand, the ?correction effect? shows how
many erroneous triplets would be corrected if one
of the erroneous triplets in the category was cor-
rected. These two columns are computed by using
the error propagation network constructed in Sec-
tion 4.2. That is, by using the network we obtain
the number of erroneous triplets to be corrected if
a given erroneous triplet in the category was cor-
rected, sum up these numbers and then calculate
the average number of expected side-effect correc-
Clark  J.  Vitulli was  named  senior  vice  president  and  general  
manager  of   this  U.S.  sales  and  marketing  arm of Japanese
auto Maker Mazda Motor Corp .
(b)(a)
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):
Coordination (fragment)Head selection for noun phraseCause categories: 
? president  ? of  this  U.S.  sales   and : ?coord_2argsARG1ARG1
of   this : U.S. sales and : marketing armdet_1arg coord_2args
ARG2ARG1ARG2 ARG1
ARG2 ARG1 ARG1 ARG1 ARG2
Correction propagation from (a) to (b)
Coordination (fragment)Cause categories: 
Figure 15: Combining our two methods (2)
tion per erroneous triplet in the category.
Figure 14 shows an example of independent er-
rors. For the sentence at the top, the parser pro-
duced four errors. The method in Section 4.2
successfully discovered two inter-dependent error
groups (a) and (b). There was no error propaga-
tion relation between the two groups. On the other
hand, the method in Section 4.1 associated all of
these four errors with the categories of ?Adjunc-
tion attachment,? ?Comma? and ?Relative clause
attachment,? and the error for the ?Adjunction at-
tachment? corresponds to the inter-dependent er-
ror group (a). Because this group is not a receiving
node of any propagation in the network, the error
is regarded as an ?independent? one.
Independent errors mean that, if a new parsing
model could correct them, the correction would
not be destroyed by other errors which remain in
the new parsing model.
The correction effect shows the opposite and
desirable effect of the nature of the dependency
among errors which the propagation network rep-
resents. This means that, if one of erroneous
triplets in the category was corrected, the correc-
tion would be amplified through propagation, and
as a result other errors would also be corrected.
We show an example of the correction effect in
Figure 15. In the figure, the parser had six errors;
three false outputs for ARG1 of ?and,? ?this? and
188
Independence Correction Expected rangeCause categories of errors # of errors
rate (%) effect (%) of error correction
[Argument selection]
Prepositional attachment 579 74.8 144.3 625.0 - 835.5
Adjunction attachment 261 56.6 179.6 265.3 - 468.8
Conjunction attachment 43 36.4 239.4 37.5 - 102.9
Head selection for noun phrase 30 0.0 381.8 0.0 - 114.5
Coordination 202 42.5 221.2 189.9 - 446.8
[Predicate type selection]
Preposition/Adjunction 108 41.7 158.3 71.3 - 171.0
Gerund acts as modifier/not 84 46.2 159.0 61.7 - 133.0
Coordination/conjunction 54 44.4 169.4 40.6 - 91.5
# of arguments for preposition 51 95.8 108.3 52.9 - 55.2
Adjunction/adjunctive noun 13 75.0 125.0 12.2 - 16.3
[More structural errors]
To-infinitive for 120 36.0 116.0 50.1 - 139.2
modifier/argument of verb
Subject for passive sentence/not 8 37.5 112.5 3.4 - 9.0
[Others]
Comma 444 39.5 194.4 341.0 - 863.1
Relative clause attachment 102 32.1 141.7 46.4 - 144.5
Table 5: Correction propagations between errors for each cause category and the other errors
?U.S.,? two false outputs for ARG2 of ?of? and
?and,? and a missing output for ARG1 of ?sales.?
Our method for inter-dependencies classified these
errors into two inter-dependent error groups (a)
and (b), and extracted an correction propagation
from (a) to (b). Our method for cause categories,
on the other hand, associated two errors of ?and?
with the category ?Coordination? and one error of
?this? with the category ?Head selection for noun
phrase.? When we correct an error in the interde-
pendent error group (a), the correction leads to not
only correction of the other errors in (a) but also
correction of the error in (b) via correction prop-
agation from (a) to (b). Therefore, a correction
effect of an error in group (a) results in 6.0.
On the basis of the above considerations, we es-
timated the range of the effect which an error cor-
rection in each category has. The minimum of ex-
pected correction range in Table 5 is given by the
product of the number of erroneous triplets in the
category, the independence rate and the correction
effect. On the other hand, the maximum is given
by the product of the number of erroneous triplets
in the category and the correction effect. This as-
sumes that all corrections made in the category are
not cancelled by other errors, while the figure in
the minimum are based on the assumption that all
corrections made in the category, except for the in-
dependent ones, are cancelled by other errors.
Table 5 would thus suggest which categories
should be resolved with high priority, from three
points of view: the number of errors in the cat-
egory, the number of independent errors, and the
correction effect.
6 Further applications of our methods
In this section, as an example of the further ap-
plication of our methods, we attempt to analyze
parsing behaviors in domain adaptation from the
viewpoints of error cause categories.
In Hara et al (2007), we proposed a method for
adapting Enju to a target domain, and then suc-
ceeded in improving the parser performance for
the GENIA corpus (Kim et al, 2003), a biomed-
ical domain. Table 6 summarizes the parsing re-
sults for three types of settings respectively: pars-
ing PTB with Enju (?Enju for PTB?), parsing GE-
NIA with Enju (?Enju for GENIA?), and parsing
GENIA with the adapted model (?Adapted for GE-
NIA?). We then analyzed the performance transi-
tion among these settings from the viewpoint of
the cause categories given in Section 4.1 (Table 7).
In order to compare the error frequencies among
different settings, we took the percentage of target
errors in all of the evaluated triplets. The signed
values between the two settings show how much
the errors increased when moving from the left set-
tings to the right ones.
When we focus on the transition from ?Enju
for PTB? to ?Enju for GENIA,? we can observe
that the change in the domain resulted in a dif-
ferent distribution of error causes. The errors for
most categories increased, and in particular, the er-
rors for ?Prepositional attachment? and ?Coordi-
189
Enju for PTB Enju for GENIA Adapted for GENIA
Evaluated sentences 1,811 842 842
Evaluated triplets 44,934 22,230 22,230
Errors 4,709 3,120 2,229
F-score (LP/LR) 90.69 (90.78/90.59) 87.41 (87.60/87.22) 90.93 (91.10/90.76)
Table 6: Summary of parsing performances for domain and model variations
Rate of errors against total examined relations in test set (%)Cause categories of errors Enju for PTB ?? Enju for GENIA ?? Adapted for GENIA
Classified 4.62 +2.60? 7.22 ?1.80? 5.42
[Argument selection]
Prepositional attachment 1.29 +0.93? 2.22 ?0.64? 1.58
Adjunction attachment 0.58 +0.38? 0.96 ?0.20? 0.76
Conjunction attachment 0.10 ?0.04? 0.06 ?0.04? 0.02
Head selection for noun phrase 0.07 +0.17? 0.24 ?0.06? 0.18
Coordination 0.45 +0.59? 1.04 ?0.25? 0.79
[Predicate type selection]
Preposition/Adjunction 0.24 +0.08? 0.32 ?0.06? 0.26
Gerund acts as modifier/not 0.19 ?0.07? 0.12 +0.01? 0.13
Coordination/conjunction 0.12 ?0.00? 0.12 ?0.07? 0.05
# of arguments for preposition 0.11 ?0.02? 0.09 ?0.00? 0.09
Adjunction/adjunctive noun 0.03 +0.19? 0.22 ?0.08? 0.14
[More structural errors]
To-infinitive for 0.27 +0.02? 0.29 ?0.09? 0.20
modifier/argument of verb
Subject for passive sentence/not 0.02 +0.34? 0.36 +0.01? 0.37
[Others]
Comma 0.99 ?0.03? 0.96 ?0.31? 0.65
Relative clause attachment 0.23 +0.05? 0.28 ?0.03? 0.25
Unclassified 5.86 +0.96? 6.82 ?2.22? 4.60
Total (Classified + Unclassified) 10.48 +3.56? 14.04 ?4.01? 10.03
Table 7: Error distributions for domain and model variations
nation? increased remarkably. On the other hand,
the transition from ?Enju for GENIA? to ?Adapted
for GENIA? shows that their adaptation method
succeeded in reducing the errors for most cate-
gories to some extent. However, for ?Preposi-
tional attachment,? ?Coordination,? and ?Subject
for passive sentence or not,? there were still no-
ticeable gaps in error distribution between ?Enju
for PTB? and ?Adapted for GENIA.? This would
mean that the adapted model requires further per-
formance improvement if we expect the same level
of performances for those categories as the parser
originally obtained in PTB.
We could thus capture some biases of cause
categories which occur in domain transition or
in domain adaptation, which would not be clari-
fied by F-score evaluation methods. With inter-
dependencies given by the method described in
Section 4.2, the above analysis would be useful
for effectively exploring further adaptation.
7 Related works
Although there have been many researchers who
analyzed errors in their own systems in the experi-
ments, there has been little research which focused
on error analysis itself.
In the field of parsing, McDonald and Nivre
(2007) compared parsing errors between graph-
based and transition-based parsers. They consid-
ered accuracy transitions from various points of
view, and the obtained statistical data suggested
that error propagation seemed to occur in the
graph structures of parsing outputs. Our research
proceeded one step further and attempted to re-
veal the nature of the propagations. In examin-
ing the combination of the two types of parsing,
they utilized approaches similar to our method for
capturing inter-dependencies of errors. They al-
lowed a parser to give only structures produced by
the parsers and utilized the ideas for evaluating the
parser?s potentials, whereas we utilized it for ob-
serving error propagations.
Dredze et al (2007) showed that many of the
parsing errors in domain adaptation tasks may
come from inconsistencies between the annota-
tions of training resources. This would sug-
gest that just error comparisons without consider-
ing the inconsistencies could lead to a misunder-
190
standing of what happens in domain transitions.
The summarized error cause categories and inter-
dependencies given by our methods would be use-
ful clues for extracting such domain-dependent er-
ror phenomena.
When we look into other research areas in nat-
ural language processing, Gime?nez and Ma`rquez
(2008) proposed an automatic error analysis ap-
proach in machine translation (MT) technologies.
They developed a metric set which could capture
features in MT outputs at different linguistic lev-
els with different levels of granularity. Like we
considered parsing systems, they explored ways to
resolve costly and rewardless error analysis in the
MT field. One of their objectives was to enable
researchers to easily obtain detailed linguistic re-
ports on the behavior of their systems, and to con-
centrate on analyses for the system improvements.
8 Conclusion
We proposed two methods for analyzing parsing
errors. One is to assign errors to cause categories,
and the other is to capture inter-dependencies
among errors. The first method defines error pat-
terns to identify cause categories and then asso-
ciates errors involved in the patterns with the cor-
responding categories. The second method re-
parses a sentence with a target error corrected, and
regards errors corrected together as dependent on
the target.
In our experiments with an HPSG parser, we
successfully associated more than 40% of the er-
rors with 14 cause categories, and captured 1,978
inter-dependent error groups. Moreover, the com-
bination of our methods gave a more detailed error
analysis for effective improvement of the parser.
In our future work, we would give more pat-
tern rules for classifying a large percentage of er-
rors into cause categories, and incorporate uncor-
rectable errors into inter-dependency analysis. Af-
ter improving the analytical facilities of our indi-
vidual methods, we would explore the possibil-
ity of combining the methods for obtaining more
powerful and detailed clues on how to improve
parsing performance.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
References
Mark Dredze, John Blitzer, Partha Pratim Talukdar,
Kuzman Ganchev, Joa?o V. Grac?a, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proceedings of the
CoNLL Shared Task Session of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 1051?1055.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Towards het-
erogeneous automatic mt error analysis. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08), pages 1894?1901.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of 10th International Confer-
ence on Parsing Technologies (IWPT 2007), pages
11?22.
Ronald M. Kaplan and Joan Bresnan. 1995. Lexical-
functional grammar: A formal system for gram-
matical representation. Formal Issues in Lexical-
Functional Grammar, pages 29?130.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(suppl. 1):i180?i182.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert Macintyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
ARPA Human Language Technology Workshop.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 83?90.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 155?163.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
191
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 779?787,
Beijing, August 2010
Evaluating Dependency Representation for Event Extraction
Makoto Miwa1 Sampo Pyysalo1 Tadayoshi Hara1 Jun?ichi Tsujii1,2,3
1Department of Computer Science, the University of Tokyo
2School of Computer Science, University of Manchester
3National Center for Text Mining
{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jp
Abstract
The detailed analyses of sentence struc-
ture provided by parsers have been applied
to address several information extraction
tasks. In a recent bio-molecular event ex-
traction task, state-of-the-art performance
was achieved by systems building specif-
ically on dependency representations of
parser output. While intrinsic evalua-
tions have shown significant advances in
both general and domain-specific pars-
ing, the question of how these translate
into practical advantage is seldom con-
sidered. In this paper, we analyze how
event extraction performance is affected
by parser and dependency representation,
further considering the relation between
intrinsic evaluation and performance at
the extraction task. We find that good
intrinsic evaluation results do not always
imply good extraction performance, and
that the types and structures of differ-
ent dependency representations have spe-
cific advantages and disadvantages for the
event extraction task.
1 Introduction
Advanced syntactic parsing methods have been
shown to effective for many information extrac-
tion tasks. The BioNLP 2009 Shared Task, a re-
cent bio-molecular event extraction task, is one
such task: analysis showed that the application of
a parser correlated with high rank in the task (Kim
et al, 2009). The automatic extraction of bio-
molecular events from text is important for a num-
ber of advanced domain applications such as path-
way construction, and event extraction thus a key
task in Biomedical Natural Language Processing
(BioNLP).
Methods building feature representations and
extraction rules around dependency representa-
tions of sentence syntax have been successfully
applied to a number of tasks in BioNLP. Several
parsers and representations have been applied in
high-performing methods both in domain studies
in general and in the BioNLP?09 shared task in
particular, but no direct comparison of parsers or
representations has been performed. Likewise,
a number of evaluation of parser outputs against
gold standard corpora have been performed in the
domain, but the broader implications of the results
of such intrinsic evaluations are rarely considered.
The BioNLP?09 shared task involved documents
contained also in the GENIA treebank (Tateisi et
al., 2005), creating an opportunity for direct study
of intrinsic and task-oriented evaluation results.
As the treebank can be converted into various de-
pendency formats using existing format conver-
sion methods, evaluation can further be extended
to cover the effects of different representations.
In this this paper, we consider three types of de-
pendency representation and six parsers, evaluat-
ing their performance from two different aspects:
dependency-based intrinsic evaluation, and effec-
tiveness for bio-molecular event extraction with a
state-of-the-art event extraction system. Compar-
ison of intrinsic and task-oriented evaluation re-
779
	








	
 


 
	

 Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 37?45,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
A Comparative Study of Syntactic Parsers for Event Extraction
Makoto Miwa1 Sampo Pyysalo1 Tadayoshi Hara1 Jun?ichi Tsujii1,2,3
1Department of Computer Science, the University of Tokyo, Japan
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan.
2School of Computer Science, University of Manchester, UK
3National Center for Text Mining, UK
{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jp
Abstract
The extraction of bio-molecular events
from text is an important task for a number
of domain applications such as pathway
construction. Several syntactic parsers
have been used in Biomedical Natural
Language Processing (BioNLP) applica-
tions, and the BioNLP 2009 Shared Task
results suggest that incorporation of syn-
tactic analysis is important to achieving
state-of-the-art performance. Direct com-
parison of parsers is complicated by to dif-
ferences in the such as the division be-
tween phrase structure- and dependency-
based analyses and the variety of output
formats, structures and representations ap-
plied. In this paper, we present a task-
oriented comparison of five parsers, mea-
suring their contribution to bio-molecular
event extraction using a state-of-the-art
event extraction system. The results show
that the parsers with domain models using
dependency formats provide very similar
performance, and that an ensemble of dif-
ferent parsers in different formats can im-
prove the event extraction system.
1 Introduction
Bio-molecular events are useful for modeling and
understanding biological systems, and their au-
tomatic extraction from text is one of the key
tasks in Biomedical Natural Language Process-
ing (BioNLP). In the BioNLP 2009 Shared Task
on event extraction, participants constructed event
extraction systems using a variety of different
parsers, and the results indicated that the use of
a parser was correlated with high ranking in the
task (Kim et al, 2009). By contrast, the results
did not indicate a clear preference for a particular
parser, and there has so far been no direct compar-
ison of different parsers for event extraction.
While the outputs of parsers applying the same
out format can be compared using a gold standard
corpus, it is difficult to perform meaningful com-
parison of parsers applying different frameworks.
Additionally, it is still an open question to what ex-
tent high performance on a gold standard treebank
correlates with usefulness at practical tasks. Task-
based comparisons of parsers provide not only a
way to asses parsers across frameworks but also a
necessary measure of their practical applicability.
In this paper, five different parsers are com-
pared on the bio-molecular event extraction task
defined in the BioNLP 2009 Shared Task using a
state-of-the-art event extraction system. The data
sets share abstracts with GENIA treebank, and the
treebank is used as an evaluation standard. The
outputs of the parsers are converted into two de-
pendency formats with the help of existing conver-
sion methods, and the outputs are compared in the
two dependency formats. The evaluation results
show that different syntactic parsers with domain
models in the same dependency format achieve
closely similar performance, and that an ensemble
of different syntactic parsers in different formats
can improve the performance of an event extrac-
tion system.
2 Bio-molecular Event Extraction with
Several Syntactic Parsers
This paper focuses on the comparison of several
syntactic parsers on a bio-molecular event extrac-
tion task with a state-of-the-art event extraction
system. This section explains the details of the
comparison. Section 2.1 presents the event ex-
37
traction task setting, following that of the BioNLP
2009 Shared Task. Section 2.2 then summa-
rizes the five syntactic parsers and three formats
adopted for the comparison. Section 2.3 described
how the state-of-the-art event extraction system of
Miwa et al (2010) is modified and used for the
comparison.
2.1 Bio-molecular Event Extraction
The bio-molecular event extraction task consid-
ered in this study is that defined in the BioNLP
2009 Shared Task (Kim et al, 2009)1. The shared
task provided common and consistent task defi-
nitions, data sets for training and evaluation, and
evaluation criteria. The shared task consists of
three subtasks: core event extraction (Task 1),
augmenting events with secondary arguments
(Task 2), and the recognition of speculation and
negation of the events (Task 3) (Kim et al, 2009).
In this paper we consider Task 1 and Task 2. The
shared task defined nine event types, which can be
divided into five simple events (Gene expression,
Transcription, Protein catabolism, Phosphoryla-
tion, and Localization) that take one core argu-
ment, a multi-participant binding event (Bind-
ing), and three regulation events (Regulation, Pos-
itive regulation, and Negative regulation) that can
take other events as arguments.
In the two tasks considered, events are repre-
sented with a textual trigger, type, and arguments,
where the trigger is a span of text that states the
event in text. In Task 1 the event arguments that
need to be extracted are restricted to the core ar-
guments Theme and Cause, and secondary argu-
ments (locations and sites) need to be attached in
Task 2.
2.2 Parsers and Formats
Five parsers and three formats are adopted for
the evaluation. The parsers are GDep (Sagae and
Tsujii, 2007)2, the Bikel parser (Bikel) (Bikel,
2004)3, the Charniak-Johnson reranking parser,
using David McClosky?s self-trained biomedi-
cal parsing model (MC) (McClosky, 2009)4, the
C&C CCG parser, adapted to biomedical text
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask/
2http://www.cs.cmu.edu/?sagae/parser/
gdep/
3http://www.cis.upenn.edu/?dbikel/
software.html
4http://www.cs.brown.edu/?dmcc/
biomedical.html
	 
     Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 164?173,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Parsing Natural Language Queries for Life Science Knowledge
Tadayoshi Hara
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, JAPAN
harasan@nii.ac.jp
Yuka Tateisi
Faculty of Informatics, Kogakuin University
1-24-2 Nishi-shinjuku, Shinjuku-ku,
Tokyo 163-8677, JAPAN
yucca@cc.kogakuin.ac.jp
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku,
Tokyo 113-0032, JAPAN
jdkim@dbcls.rois.ac.jp
Yusuke Miyao
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, JAPAN
yusuke@nii.ac.jp
Abstract
This paper presents our preliminary work on
adaptation of parsing technology toward natu-
ral language query processing for biomedical
domain. We built a small treebank of natu-
ral language queries, and tested a state-of-the-
art parser, the results of which revealed that
a parser trained on Wall-Street-Journal arti-
cles and Medline abstracts did not work well
on query sentences. We then experimented
an adaptive learning technique, to seek the
chance to improve the parsing performance on
query sentences. Despite the small scale of the
experiments, the results are encouraging, en-
lightening the direction for effective improve-
ment.
1 Introduction
Recent rapid progress of life science resulted in a
greatly increased amount of life science knowledge,
e.g. genomics, proteomics, pathology, therapeutics,
diagnostics, etc. The knowledge is however scat-
tered in pieces in diverse forms over a large number
of databases (DBs), e.g. PubMed, Drugs.com, Ther-
apy database, etc. As more and more knowledge is
discovered and accumulated in DBs, the need for
their integration is growing, and corresponding ef-
forts are emerging (BioMoby1, BioRDF2, etc.).
Meanwhile, the need for a query language with
high expressive power is also growing, to cope with
1http://www.biomoby.org/
2http://esw.w3.org/HCLSIG BioRDF Subgroup
the complexity of accumulated knowledge. For ex-
ample, SPARQL3 is becoming an important query
language, as RDF4 is recognized as a standard in-
teroperable encoding of information in databases.
SPARQL queries are however not easy for human
users to compose, due to its complex vocabulary,
syntax and semantics. We propose natural language
(NL) query as a potential solution to the problem.
Natural language, e.g. English, is the most straight-
forward language for human beings. Extra training
is not required for it, yet the expressive power is
very high. If NL queries can be automatically trans-
lated into SPARQL queries, human users can access
their desired knowledge without learning the com-
plex query language of SPARQL.
This paper presents our preliminary work for
NL query processing, with focus on syntactic pars-
ing. We first build a small treebank of natural
language queries, which are from Genomics track
(Hersh et al, 2004; Hersh et al, 2005; Hersh et al,
2006; Hersh et al, 2007) topics (Section 2 and 3).
The small treebank is then used to test the perfor-
mance of a state-of-the-art parser, Enju (Ninomiya
et al, 2007; Hara et al, 2007) (Section 4). The
results show that a parser trained on Wall-Street-
Journal (WSJ) articles and Medline abstracts will
not work well on query sentences. Next, we ex-
periment an adaptive learning technique, to seek the
chance to improve the parsing performance on query
sentences. Despite the small scale of the experi-
ments, the results enlighten directions for effective
3http://www.w3.org/TR/rdf-sparql-query/
4http://www.w3.org/RDF/
164
GTREC
04 05 06 07
Declarative 1 0 0 0
Imperative 22 60 0 0
Infinitive 1 0 0 0
Interrogative
- WP/WRB/WDT 3 / 1 / 11 0 / 0 / 0 6 / 22 / 0 0 / 0 / 50
- Non-wh 5 0 0 0
NP 14 0 0 0
Total 58 60 28 50
Table 1: Distribution of sentence constructions
improvement (Section 5).
2 Syntactic Features of Query Sentences
While it is reported that the state-of-art NLP tech-
nology shows reasonable performance for IR or
IE applications (Ohta et al, 2006), NLP technol-
ogy has long been developed mostly for declara-
tive sentences. On the other hand, NL queries in-
clude wide variety of sentence constructions such
as interrogative sentences, imperative sentences, and
noun phrases. Table 1 shows the distribution of the
constructions of the 196 query sentences from the
topics of the ad hoc task of Genomics track 2004
(GTREC04) and 2005 (GTREC05) in their narra-
tive forms, and the queries for the passage retrieval
task of Genomics track 2006 (GTREC06) and 2007
(GTREC07).
GTREC04 set has a variety of sentence construc-
tions, including noun phrases and infinitives, which
are not usually considered as full sentences. In the
2004 track, the queries were derived from interviews
eliciting information needs of real biologists, with-
out any control on the sentence constructions.
GTREC05 consists only of imperative sentences.
In the 2005 track, a set of templates were derived
from an analysis of the 2004 track and other known
biologist information needs. The derived templates
were used as the commands to find articles describ-
ing biological interests such as methods or roles of
genes. Although the templates were in the form
?Find articles describing ...?, actual obtained imper-
atives begin with ?Describe the procedure or method
for? (12 sentences), ?Provide information about?
(36 sentences) or ?Provide information on? (12 sen-
tences).
GTREC06 consists only of wh-questions where a
wh-word constitutes a noun phrase by itself (i.e. its
 
S  
 
  
 
VP  
 
  
 
NP  
 
  
 
PP  
 
  
 
NP  
 
  
 
PP  
 
  
NP NP NP NP 
        VB NNS IN NN IN NN 
[ ] Find articles abut function of FancD2 
 
Figure 1: The tree structure for an imperative sentence
part-of-speech is the WP in Penn Treebank (Marcus
et al, 1994) POS tag set) or is an adverb (WRB). In
the 2006 track, the templates for the 2005 track were
reformulated into the constructions of questions and
were then utilized for deriving the questions. For ex-
ample, the templates to find articles describing the
role of a gene involved in a given disease is refor-
mulated into the question ?What is the role of gene
in disease??
GTREC07 consists only of wh-questions where a
wh-word serves as a pre-nominal modifier (WDT).
In the 2007 track, unlike in those of last two years,
questions were not categorized by the templates, but
were based on biologists? information needs where
the answers were lists of named entities of a given
type. The obtained questions begin with ?what +
entity type? (45 sentences), ?which + entity type? (4
sentences), or ?In what + entity type? (1 sentence).
In contrast, the GENIA Treebank Corpus (Tateisi
et al, 2005)5 is estimated to have no imperative sen-
tences and only seven interrogative sentences (see
Section 5.2.2). Thus, the sentence constructions in
GTREC04?07 are very different from those in the
GENIA treebank.
3 Treebanking GTREC query sentences
We built a treebank (with POS) on 196 query sen-
tences following the guidelines of the GENIA Tree-
bank (Tateisi and Tsujii, 2006). The queries were
first parsed using the Stanford Parser (Klein and
Manning, 2003), and manual correction was made
5http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/
wiki.cgi?page=GENIA+Treebank
165
 SBARQ  
 
  
 
SQ  
 
   
 VP    
WHNP[i168]   NP[i169?i168]  NP[?i169] PP               
WDT NNS VBP   VBN   IN NN 
What toxicities are [ ] associated [ ] with cytarabine 
 
Figure 2: The tree structure for an interrogative sentence
by the second author. We tried to follow the guide-
line of the GENIA Treebank as closely as possible,
but for the constructions that are rare in GENIA, we
used the ATIS corpus in Penn Treebank (Bies et al,
1995), which is also a collection of query sentences,
for reference.
Figure 1 shows the tree for an imperative sen-
tence. A leaf node with [ ] corresponds to a null
constituent. Figure 2 shows the tree for an inter-
rogative sentence. Coindexing is represented by
assigning an ID to a node and a reference to the
ID to the node which is coindexed. In Figure 2,
WHNP[i168] means that the WHNP node is indexed
as i168, NP[i169?i168] means that the NP node is
indexed as i169 and coindexed to the i168 node, and
NP[?i169] means that the node is coindexed to the
i169 node. In this sentence, which is a passive wh-
question, it is assumed that the logical object (what
toxicities) of the verb (associate) is moved to the
subject position (the place of i169) and then moved
to the sentence-initial position (the place of i168).
As most of the query sentences are either impera-
tive or interrogative, there are more null constituents
compared to the GENIA Corpus. In the GTREC
query treebank, 184 / 196 (93.9%) sentences con-
tained one or more null constituents, whereas in GE-
NIA, 12,222 / 18,541 (65.9%) sentences did. We ex-
pected there are more sentences with multiple null
constituents in GTREC compared to GENIA, due to
the frequency of passive interrogative sentences, but
on the contrary the number of sentences containing
more than one null constituents are 65 (33.1%) in
GTREC, and 6,367 (34.5%) in GENIA. This may be
due to the frequency of relative clauses in GENIA.
4 Parsing system and extraction of
imperative and question sentences
We introduce the parser and the POS tagger whose
performances are examined, and the extraction of
imperative or question sentences from GTREC tree-
bank on which the performances are measured.
4.1 HPSG parser
The Enju parser (Ninomiya et al, 2007)6 is a deep
parser based on the HPSG formalism. It produces
an analysis of a sentence that includes the syntac-
tic structure (i.e., parse tree) and the semantic struc-
ture represented as a set of predicate-argument de-
pendencies. The grammar is based on the standard
HPSG analysis of English (Pollard and Sag, 1994).
The parser finds a best parse tree scored by a max-
ent disambiguation model using a Cocke-Kasami-
Younger (CKY) style algorithm.
We used a toolkit distributed with the Enju parser
for training the parser with a Penn Treebank style
(PTB-style) treebank. The toolkit initially converts
the PTB-style treebank into an HPSG treebank and
then trains the parser on it. We used a toolkit dis-
tributed with the Enju parser for extracting a HPSG
lexicon from a PTB-style treebank. The toolkit ini-
tially converts the PTB-style treebank into an HPSG
treebank and then extracts the lexicon from it.
The HPSG treebank converted from the test sec-
tion was used as the gold-standard in the evaluation.
As the evaluation metrics of the Enju parser, we used
labeled and unlabeled precision/recall/F-score of the
predicate-argument dependencies produced by the
parser. A predicate-argument dependency is repre-
sented as a tuple of ?wp, wa, r?, where wp is the
predicate word, wa is the argument word, and r is
the label of the predicate-argument relation, such
as verb-ARG1 (semantic subject of a verb) and
prep-ARG1 (modifiee of a prepositional phrase).
4.2 POS tagger
The Enju parser assumes that the input is already
POS-tagged. We use a tagger in (Tsuruoka et al,
2005). It has been shown to give a state-of-the-art
accuracy on the standard Penn WSJ data set and also
on a different text genre (biomedical literature) when
trained on the combined data set of the WSJ data and
6http://www-tsujii.is.s.u-tokyo.ac.jp/enju
166
the target genre (Tsuruoka et al, 2005). Since our
target is biomedical domain, we utilize the tagger
adapted to the domain as a baseline, which we call
?the GENIA tagger?.
4.3 Extracting imperative and question
sentences from GTREC treebank
In GTREC sentences, two major constructions of
sentences can be observed: imperative and question
sentences. These two types of sentences have differ-
ent sentence constructions and we will observe the
impact of each or both of these constructions on the
performances of parsing or POS-tagging. In order
to do so, we collected imperative and question sen-
tences from our GTREC treebank as follows:
? GTREC imperatives - Most of the impera-
tive sentences in GTREC treebank begin with
empty subjects ?(NP-SBJ */-NONE-)?. We ex-
tracted such 82 imperative sentences.
? GTREC questions - Interrogative sentences
are annotated with the phrase label ?SBARQ?
or ?SQ?, where ?SBARQ? and ?SQ? respec-
tively denote a wh-question and an yes/no ques-
tion. We extracted 98 interrogative sentences
whose top phrase labels were either of them.
5 Experiments
We examine the POS-tagger and the parser for the
sentences in the GTREC corpus. They are adapted
to each of GTREC overall, imperatives, and ques-
tions. We then observe how the parsing or POS-
tagging accuracies are improved and analyze what
is critical for parsing query sentences.
5.1 Experimental settings
5.1.1 Dividing corpora
We prepared experimental datasets for the follow-
ing four domains:
? GENIA Corpus (GENIA) (18,541 sentences)
Divided into three parts for training (14,849
sentences), development test (1,850 sentences),
and final test (1,842 sentences).
? GTREC overall (196 sentences)
Divided into two parts: one for ten-folds cross
validation test (17-18 ? 10 sentences) and the
other for error analysis (17 sentences)
Target GENIA tagger Adapted tagger
GENIA 99.04% -
GTREC (overall) 89.98% 96.54%
GTREC (imperatives) 90.32% 97.30%
GRREC (questions) 89.25% 94.77%
Table 2: Accuracy of the POS tagger for each domain
? GTREC imperatives (82 sentences)
Divided into two parts: one for ten-folds cross
validation test (7-8 ? 10 sentences) and the
other for error analysis (7 sentences)
? GTREC questions (98 sentences)
Divided into two parts: one for ten-folds cross
validation test (9 ? 10 sentences) and the other
for error analysis (8 sentences)
5.1.2 Adaptation of POS tagger and parser
In order to adapt the POS tagger and the parser to
a target domain, we took the following methods.
? POS tagger - For the GTREC overall / impera-
tives / questions, we replicated the training data
for 100,000 times and utilized the concatenated
replicas and GENIA training data in (Tsuruoka
et al, 2005) for training. For POS tagger, the
number of replicas of training data was deter-
mined among 10n(n = 0, . . . , 5) by testing
these numbers on development test sets in three
of ten datasets of cross validation.
? Enju parser - We used a toolkit in the Enju
parser (Hara et al, 2007). As a baseline model,
we utilized the model adapted to the GENIA
Corpus. We then attempted to further adapt the
model to each domain. In this paper, the base-
line model is called ?the GENIA parser?.
5.2 POS tagger and parser performances
Table 2 and 3 respectively show the POS tagging and
the parsing accuracies for the target domains, and
Figure 3 and 4 respectively show the POS tagging
and the parsing accuracies for the target domains
given by changing the size of the target training data.
The POS tagger could output for each word either
of one-best POS or POS candidates with probabili-
ties, and the Enju parser could take either of the two
output types. The bracketed numbers in Table 3 and
167
Parser GENIA Adapted
POS Gold GENIA tagger Adapted tagger Gold GENIA tagger Adapted tagger
For GENIA 88.54 88.07 (88.00) - - - -
For GTREC overall 84.37 76.81 (72.43) 83.46 (81.96) 89.00 76.98 (74.44) 86.98 (85.42)
For GTREC imperatives 85.19 78.54 (77.75) 85.71 (85.48) 89.42 74.40 (74.84) 88.97 (88.67)
For GTREC questions 85.45 76.25 (67.27) 83.55 (80.46) 87.33 81.41 (71.90) 84.87 (82.70)
[ using POS candidates with probabilities (using only one best POS) ]
Table 3: Accuracy of the Enju parser for GTREC
70
75
80
85
90
0 20 40 60 80 100 120 140
F
-
s
c
o
r
e
Corpus size (sentences)
70
75
80
85
90
0 20 40 60
F
-
s
c
o
r
e
Corpus size (sentences)
65
70
75
80
85
90
95
0 20 40 60 80
F
-
s
c
o
r
e
Corpus size (sentences)
Adapted parser, gold POS
Adapted parser, adapted tagger (prob.)
GENIA parser, adapted tagger (prob.)
Adapted parser, GENIA tagger (prob.)
Adapted parser, adapted tagger (1best)
GENIA parser, adapted tagger (1best)
Adapted parser, GENIA tagger (1best)
For GTREC imperatives For GTREC questionsFor GTREC overall
Figure 4: Parsing accuracy vs. corpus size
88
90
92
94
96
98
0 50 100 150
A
c
c
u
r
a
c
y
 (
%
)
Corpus size (sentences)
GTREC overall
GTREC imperatives
GTREC questions
Figure 3: POS tagging accuracy vs. corpus size
the dashed lines in Figure 4 show the parsing accu-
racies when we utilized one-best POS given by the
POS tagger, and the other numbers and lines show
the accuracies given by POS candidates with proba-
bilities. In the rest of this section, when we just say
?POS tagger?, the tagger?s output is POS candidates
with probabilities.
Table 4 and 5 respectively compare the types of
POS tagging and parsing errors for each domain
between before and after adapting the POS tagger,
and Table 6 compares the types of parsing errors for
Correct ? Error GENIA tagger Adapted tagger
For GTREC overall (17 sentences)
NN ? NNP 4 0.6
VB ? NN 4 0
WDT ? WP 4 0
NN ? JJ 1 1.9
For GTREC imperative (seven sentences)
FW ? NNP / NN / JJ 7 4
VB ? NN 4 0
NN ? NNP 2 0
For GTREC question (eight sentences)
WDT ? WP 3 0
VB ? VBP 2 1
NNS ? VBZ 2 0
(The table shows only error types observed more than
once for either of the taggers)
Table 4: Tagging errors for each of the GTREC corpora
each domain between before and after adapting the
parser. The numbers of errors for the rightmost col-
umn in each of the tables were given by the average
of the ten-folds cross validation results.
In the following sections, we examine the im-
pact of the performances of the POS taggers or the
parsers on parsing the GTREC documents.
168
GENIA parserError types GENIA tagger Adapted tagger
For GTREC overall (17 sentences)
Failure in detecting verb 12 0.2
Root selection 6 0
Range of NP 5 5
PP-attachment 4 3
Determiner / pronoun 4 1
Range of verb subject 4 4
Range of verb object 3 3
Adjective / modifier noun 2 3
For GTREC imperatives (seven sentences)
Failure in detecting verb 8 0
Root selection 4 0
Range of NP 3 4
PP-attachment 3 1.8
Range of PP 2 2
For GTREC questions (eight sentences)
Range of coordination 5 3
Determiner / pronoun 3 0
PP-attachment 3 1
Range of PP 2 2
Subject for verb 2 1
(The table shows only the types of parsing errors observed more
than once for either of the parsers)
Table 5: Impact of adapting POS tagger on parsing errors
5.2.1 Impact of POS tagger on parsing
In Table 2, for each of the GTREC corpora,
the GENIA tagger dropped its tagging accuracy by
around nine points, and then recovered five to seven
points by the adaptation. According to this behav-
ior of the tagger, Table 3 shows that the GENIA and
the adapted parsers with the GENIA tagger dropped
their parsing accuracies by 6?15 points in F-score
from the accuracies with the gold POS, and then re-
covered the accuracies within two points below the
accuracies with the gold POS. The performance of
the POS tagger would thus critically affect the pars-
ing accuracies.
In Figure 3, we can observe that the POS tagging
accuracy for each corpus rapidly increased only for
first 20?30 sentences, and after that the improvement
speed drastically declined. Accordingly, in Figure 4,
the line for the adapted parser with the adapted tag-
ger (the line with triangle plots) rose rapidly for the
first 20?30 sentences, and after that slowed down.
We explored the tagging and parsing errors, and
analyze the cause of the initial accuracy jump and
the successive improvement depression.
Gold POSError types GENIA parser Adapted parser
For GTREC overall (17 sentences)
Range of NP 5 1.3
Range of verb subject 3 2.6
PP-attachment 3 2.7
Whether verb takes
object & complement 3 2.9
Range of verb object 2 1
For GTREC imperatives (seven sentences)
Range of NP 4 1.1
PP-attachment 2 1.6
Range of PP 2 0.3
Preposition / modifier 2 2
For GTREC questions (eight sentences)
Coordination / conjunction 2 2.2
Auxiliary / normal verb 2 2.6
Failure in detecting verb 2 2.6
(The table shows only the types of parsing errors observed more
than once for either of the parsers)
Table 6: Impact of adapting parser on parsing errors
Cause of initial accuracy jump
In Table 4, ?VB ? NN? tagging errors were
observed only in imperative sentences and drasti-
cally decreased by the adaptation. In a impera-
tive sentence, a verb (VB) usually appears as the
first word. On the other hand, the GENIA tagger
was trained mainly on the declarative sentences and
therefore would often take the first word in a sen-
tence as the subject of the sentence, that is, noun
(NN). When the parser received a wrong NN-tag for
a verb, the parser would attempt to believe the infor-
mation (?failure in detecting verb? in Table 6) and
could then hardly choose the NN-tagged word as a
main verb (?root selection? in Table 6). By adapting
the tagger, the correct tag was given to the verb and
the parser could choose the verb as a main verb.
?WDT ? WP? tagging errors were observed only
in the question sentences and also drastically de-
creased. For example, in the sentence ?What toxici-
ties are associated with cytarabine??, ?What? works
as a determiner (WDT) which takes ?toxicities?,
while the GENIA tagger often took this ?What? as a
pronoun (WP) making a phrase by itself. This would
be because the training data for the GENIA tagger
would contain 682 WP ?what? and only 27 WDT
?what?. WP ?what? could not make a noun phrase
by taking a next noun, and then the parsing of the
parsing would corrupt (?determiner / pronoun? in
Table 5). By adapting the tagger, ?WDT? tag was
169
given to ?What?, and the parser correctly made a
phrase ?What toxicities?.
Since the variation of main verbs in GTREC im-
peratives is very small (see Section 2) and that of
interrogatives is also very small, in order to cor-
rect the above two types of errors, we would require
only small training data. In addition, these types of
errors widely occurred among imperatives or ques-
tions, the accuracy improvement by correcting the
errors was very large. The initial rapid improvement
would thus occur.
Cause of improvement depression
?NN ? NNP? tagging errors would come from
the description style of words. In the GTREC
queries, technical terms, such as the names of dis-
eases or proteins, sometimes begin with capital char-
acters. The GENIA tagger would take the capi-
talized words not as a normal noun (NN) but as a
proper noun (NNP). By adaptation, the tagger would
have learned the capital usage for terms and the er-
rors then decreased.
However, in order to achieve such improvement,
we would have to wait until a target capitalized term
is added to the training corpus. ?FW ? NNP / NN
/ JJ?, ?NN ? JJ?, and several other errors would be
similar to this type of errors in the point that, they
would be caused by the difference in annotation pol-
icy or description style between the training data for
the GENIA tagger and the GTREC queries.
?VB ? VBP? errors were found in questions. For
example, ?affect? in the question ?How do muta-
tions in Sonic Hedgehog genes affect developmen-
tal disorders?? was base form (VB), while the GE-
NIA tagger took it as a present tense (VBP) since
the GENIA tagger would be unfamiliar with such
verb behavior in questions. By adaptation, the tag-
ger would learn that verbs in the domain tend to take
base forms and the errors then decreased.
However, the tagger model based on local context
features could not substantially solve the problem.
VBP of course could appear in question sentences.
We observed that a verb to be VBP was tagged with
VB by the adapted tagger. In order to distinguish
VB from VBP, we should capture longer distance
dependencies between auxiliary and main verbs.
In tagging, the fact that the above two types of
errors occupied most of the errors other than the er-
rors involved in the initial jump, would be related
to why the accuracy improvement got so slowly,
which would lead to the improvement depression of
the parsing performances. With the POS candidates
with probabilities, the possibilities of correct POSs
would increase, and therefore the parser would give
higher parsing performances than using only one-
best POSs (see Table 3 and Figure 4).
Anyway, the problems were not substantially
solved. For these tagging problems, just adding the
training data would not work. We might need re-
construct the tagging system or re-consider the fea-
ture designs of the model.
5.2.2 Impact of parser itself on parsing
For the GTREC corpora, the GENIA parser with
gold POSs lowered the parsing accuracy by more
than three points than for the GENIA Corpus, while
the adaptation of the parser recovered a few points
for each domain (second and fifth column in Table
3). Figure 4 would also show that we could improve
the parser?s performance with more training data for
each domain. For GTREC questions, the parsing ac-
curacy dropped given the maximum size of the train-
ing data. Our training data is small and therefore
small irregular might easily make accuracies drop or
rise. 7 We might have to prepare more corpora for
confirming our observation.
Table 6 would imply that the major errors for all
of these three corpora seem not straightforwardly as-
sociated with the properties specific to imperative or
question sentences. Actually, when we explored the
parse results, errors on the sentence constructions
specific to the two types of sentences would hardly
be observed. (?Failure in detecting verb? errors in
GTREC questions came from other causes.) This
would mean that the GENIA parser itself has poten-
tial to parse the imperative or question sentences.
The training data of the GENIA parser consists
of the WSJ Penn Treebank and the GENIA Corpus.
As long as we searched with our extraction method
in Section 4.3, the WSJ and GENIA Corpus seem
respectively contain 115 and 0 imperative, and 432
7This time we could not analyze which training data affected
the decrease, because through the cross validation experiments
each sentence was forced to be once final test data. However,
we would like to find the reason for this accuracy decrease in
some way.
170
and seven question sentences. Unlike the POS tag-
ger, the parser could convey more global sentence
constructions from these sentences.
Although the GENIA parser might understand the
basic constructions of imperative or question sen-
tences, by adaptation of the parser to the GTREC
corpora, we could further learn more local construc-
tion features specific to GTREC, such as word se-
quence constructing a noun phrase, attachment pref-
erence of prepositions or other modifiers. The error
reduction in Table 6 would thus be observed.
However, we also observed that several types of
errors were still mostly unsolved after the adapta-
tion. Choosing whether to add complements for
verbs or not, and distinguishing coordinations from
conjunctions seems to be difficult for the parser. If
two question sentences were concatenated by con-
junctions into one sentence, the parser would tend to
fail to analyze the sentence construction for the lat-
ter sentence. The remaining errors in Table 6 would
imply that we should also re-consider the model de-
signs or the framework itself for the parser in addi-
tion to just increasing the training data.
6 Related work
Since domain adaptation has been an extensive re-
search area in parsing research (Nivre et al, 2007),
a lot of ideas have been proposed, including un-
/semi-supervised approaches (Roark and Bacchiani,
2003; Blitzer et al, 2006; Steedman et al, 2003;
McClosky et al, 2006; Clegg and Shepherd, 2005;
McClosky et al, 2010) and supervised approaches
(Titov and Henderson, 2006; Hara et al, 2007).
Their main focus was on adapting parsing models
trained with a specific genre of text (in most cases
PTB-WSJ) to other genres of text, such as biomed-
ical research papers. A major problem tackled in
such a task setting is the handling of unknown words
and domain-specific ways of expressions. However,
as we explored, parsing NL queries involves a sig-
nificantly different problem; even when all words in
a sentence are known, the sentence has a very differ-
ent construction from declarative sentences.
Although sentence constructions have gained lit-
tle attention, a notable exception is (Judge et al,
2006). They pointed out low accuracy of state-of-
the-art parsers on questions, and proposed super-
vised parser adaptation by manually creating a tree-
bank of questions. The question sentences are anno-
tated with phrase structure trees in the PTB scheme,
although function tags and empty categories are
omitted. An LFG parser trained on the treebank then
achieved a significant improvement in parsing ac-
curacy. (Rimell and Clark, 2008) also worked on
question parsing. They collected question sentences
from TREC 9-12, and annotated the sentences with
POSs and CCG (Steedman, 2000) lexical categories.
They reported a significant improvement in CCG
parsing without phrase structure annotations.
On the other hand, (Judge et al, 2006) also im-
plied that just increasing the training data would not
be enough. We went further from their work, built
a small but complete treebank for NL queries, and
explored what really occurred in HPSG parsing.
7 Conclusion
In this paper, we explored the problem in parsing
queries. We first attempted to build a treebank on
queries for biological knowledge and successfully
obtained 196 annotated GTREC queries. We next
examined the performances of the POS tagger and
the HPSG parser on the treebank. In the experi-
ments, we focused on the two dominant sentence
constructions in our corpus: imperatives and ques-
tions, extracted them from our corpus, and then also
examined the parser and tagger for them.
The experimental results showed that the POS
tagger?s mis-tagging to main verbs in imperatives
and wh-interrogatives in questions critically de-
creased the parsing performances, and that our
small corpus could drastically decrease such mis-
tagging and consequently improve the parsing per-
formances. The experimental results also showed
that the parser itself could improve its own perfor-
mance by increasing the training data. On the other
hand, the experimental results suggested that the
POS tagger or the parser performance would stag-
nate just by increasing the training data.
In our future research, on the basis of our findings,
we would like both to build more training data for
queries and to reconstruct the model or reconsider
the feature design for the POS tagger and the parser.
We would then incorporate the optimized parser and
tagger into NL query processing applications.
171
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
style ? Penn Treebank project. Technical report, De-
partment of Linguistics, University of Pennsylvania.
John Blitzer, Ryan Mcdonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia.
A. B. Clegg and A. Shepherd. 2005. Evaluating and in-
tegrating treebank parsers on a biomedical corpus. In
Proceedings of the ACL 2005 Workshop on Software,
Ann Arbor, Michigan.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of 10th International Confer-
ence on Parsing Technologies (IWPT 2007), pages 11?
22.
William R. Hersh, Ravi Teja Bhupatiraju, L. Ross,
Aaron M. Cohen, Dale Kraemer, and Phoebe Johnson.
2004. TREC 2004 Genomics Track Overview. In Pro-
ceedings of the Thirteenth Text REtrieval Conference,
TREC 2004.
William R. Hersh, Aaron M. Cohen, Jianji Yang,
Ravi Teja Bhupatiraju, Phoebe M. Roberts, and
Marti A. Hearst. 2005. TREC 2005 Genomics Track
Overview. In Proceedings of the Fourteenth Text RE-
trieval Conference, TREC 2005.
William R. Hersh, Aaron M. Cohen, Phoebe M. Roberts,
and Hari Krishna Rekapalli. 2006. TREC 2006 Ge-
nomics Track Overview. In Proceedings of the Fif-
teenth Text REtrieval Conference, TREC 2006.
William R. Hersh, Aaron M. Cohen, Lynn Ruslen, and
Phoebe M. Roberts. 2007. TREC 2007 Genomics
Track Overview. In Proceedings of The Sixteenth Text
REtrieval Conference, TREC 2007.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. Questionbank: Creating a Corpus of Parsing-
Annotated Questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL, pages 497?504.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert Macintyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of ARPA Human Language Technology
Workshop.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 337?344, Sydney, Australia.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic Domain Adaptation for Parsing. In
Proceedings of the 2010 Annual Conference of the
North American Chapter of the ACL, pages 28?36, Los
Angeles, California.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao,
and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate hpsg pars-
ing. In Proceedings of 10th International Conference
on Parsing Technologies (IWPT 2007), pages 60?68.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Laura Rimell and Stephen Clark. 2008. Adapting a
Lexicalized-Grammar Parser to Contrasting Domains.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 475?
584.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised PCFG adaptation to novel domains.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 126?133, Edmonton, Canada.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the tenth conference on European chap-
ter of the Association for Computational Linguistics,
pages 331?338, Budapest, Hungary.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
Yuka Tateisi and Jun?ichi Tsujii. 2006. GENIA Anno-
tation Guidelines for Treebanking. Technical Report
TR-NLP-UT-2006-5, Tsujii Laboratory, University of
Tokyo.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the Second Interna-
tional Joint Conference on Natural Language Process-
172
ing (IJCNLP 2005), Companion volume, pages 222?
227.
Ivan Titov and James Henderson. 2006. Porting statis-
tical parsers with data-defined kernels. In Proceed-
ings of the Tenth Conference on Computational Natu-
ral Language Learning, pages 6?13, New York City.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics - 10th Panhellenic Conference on Infor-
matics, volume LNCS 3746, pages 382?392, Volos,
Greece, November. ISSN 0302-9743.
173
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 49?58,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling Comma Placement in Chinese Text for Better Readability using
Linguistic Features and Gaze Information
Tadayoshi Hara1 Chen Chen2? Yoshinobu Kano3,1 Akiko Aizawa1
1National Institute of Informatics, Japan 2The University of Tokyo, Japan
3PRESTO, Japan Science and Technology Agency
{harasan, kano, aizawa}@nii.ac.jp
Abstract
Comma placements in Chinese text are
relatively arbitrary although there are
some syntactic guidelines for them. In this
research, we attempt to improve the read-
ability of text by optimizing comma place-
ments through integration of linguistic fea-
tures of text and gaze features of readers.
We design a comma predictor for gen-
eral Chinese text based on conditional ran-
dom field models with linguistic features.
After that, we build a rule-based filter for
categorizing commas in text according to
their contribution to readability based on
the analysis of gazes of people reading text
with and without commas.
The experimental results show that our
predictor reproduces the comma distribu-
tion in the Penn Chinese Treebank with
78.41 in F1-score and commas chosen by
our filter smoothen certain gaze behaviors.
1 Introduction
Chinese is an ideographic language, with no natu-
ral apparent word boundaries, little morphology,
and no case markers. Moreover, most Chinese
sentences are quite long. These features make it
especially difficult for Chinese learners to identify
composition of a word or a clause in a sentence.
Punctuation marks, especially commas, are al-
lowed to be placed relatively arbitrarily to serve as
important segmentation cues (Yue, 2006) for pro-
viding syntactic and prosodic boundaries in text;
commas indicate not only phrase or clause bound-
aries but also sentence segmentations, and they
capture some of the major aspects of a writer?s
prosodic intent (Chafe, 1988). The combination
of both aspects promotes cognition when reading
text (Ren and Yang, 2010; Walker et al, 2001).
?The Japan Research Institute, Ltd. (from April, 2013)
Linguistic FeaturesCRF modelCRF model-based Comma Predictor
Gaze FeaturesHuman AnnotationRule-based Comma Filter Text with/without Commas
Parse Tree
Treebank
Comma Distribution for Readability
Comma Distribution in General Text
Input (Comma-less) Text
+
+
Figure 1: Our approach
However, although there are guidelines and re-
search on the syntactic aspects of comma place-
ment, prosodic aspects have not been explored,
since they are more related with cognition. It is
as yet unclear how comma placement should be
optimized for reading, and it has thus far been up
to the writer (Huang and Chen, 2011).
In this research, we attempt to optimize comma
placements by integrating the linguistic features of
text and the gaze features of readers. Figure 1 il-
lustrates our approach. First, we design a comma
predictor for general Chinese text based on con-
ditional random field (CRF) models with various
linguistic features. Second, we build a rule-based
filter for classifying commas in text into ones fa-
cilitating or obstructing readability, by comparing
the gaze features of persons reading text with and
without commas. These two steps are connected
by applying our rule-based filter to commas pre-
dicted by our comma predictor. The experimental
results for each step validate our approach.
Related work is described in Section 2. The
functions of Chinese commas are described in
Section 3. Our CRF model-based comma predic-
tor is examined in Section 4, and our rule-based
comma filter is constructed and examined in Sec-
tion 5 and 6. Section 7 contains a summary and
outlines future directions of this research.
49
[Case 1] When a pause between a subject and a predicate is needed. (? (,) means the original or comparative position of the comma in Chinese text.)
e.g. ????????????????????????(The stars we can see (,)? are mostly fixed stars that are far away from the earth.)
[Case 2] When a pause between an inner predicate and an object of a sentence is needed.
e.g. ?????????????????????(We should see that (,) science needs a person to devote all his/her life to it.)
[Case 3] When a pause after an inner (adverbial, prepositional, etc.) modifier of a sentence is needed.
e.g. ?????????????(He is no stranger (,) to this city.) (The order of the modifier and the main clause is opposite in the English translation.)
[Case 4] When a pause between clauses in a complex sentence is needed, besides the use of semicolon (?).
e.g. ??????????????????????(It is said that there are more than 100 Suzhou traditional gardens, (,) no more than 10 of which I
have been to.)
[Case 5] When a pause between phrases of the same syntactic type is needed.
e.g. ??????????????? (The students prefer young (,) and energetic teachers.)
Table 1: Five main usages of commas in Chinese text
(a) Screenshot of a material
Display PC Monitor SubjectEye TrackerHost PC Monitor
(b) Scene of the experiment (c) Window around a gaze point
Figure 3: Settings for eye-tracking experiments
WS Word surface
POS POS tag
DIP Depth of a word in the parse tree
STAG Syntactic tag
OIC Order of the clause in a sentence that a word belongs to
WL Word length
LOD Length of fragment with specific depth in a parsing tree
Table 2: Features used in our CRF model
2 Related Work
Previous work on Chinese punctuation prediction
mostly focuses on sentence segmentation in au-
tomatic speech recognition (Shriberg et al, 2000;
Huang and Zweig, 2002; Peitz et al, 2011).
Jin et al (2002) classified commas for sentence
segmentation and succeeded in improving pars-
ing performance. Lu and Ng (2010) proposed
an approach built on a dynamic CRF for predict-
ing punctuations, sentence boundaries, and sen-
tence types of speech utterances without prosodic
cues. Zhang et al (2006) suggested that a cascade
CRF-based approach can deal with ancient Chi-
nese prose punctuation better than a single CRF.
Guo et al (2010) implemented a three-tier max-
imum entropy model incorporating linguistically
motivated features for generating commonly used
Chinese punctuation marks in unpunctuated sen-
tences output by a surface realizer.
(a)
WS|POS|STAG|DIP|OIC|WL|LOD|IOB-tag
(b)
Figure 2: Example of a parse tree (a) and its cor-
responding training data (b) with the features
3 Functions of Chinese Commas
There are five main uses of commas in Chinese
text, as shown in Table 1. Cases 1 to 4 are from
ZDIC.NET (2005), and Case 5 obviously exists in
Chinese text. The first three serve the function of
emphasis, while the latter two indicate coordinat-
ing or subordinating clauses or phrases.
In Cases 1 and 2, a comma is inserted as a
kind of pause between a short subject and a long
predicate, or between a short remainder predicate,
such as?? (see/know),??/?? (indicate),?
50
Feature F1 (P/R) A
WS 59.32 (72.67/50.12) 95.45
POS 32.51 (69.06/21.26) 94.08
DIP 34.14 (68.65/22.72) 94.13
STAG 22.44 (64.00/13.60) 93.67
OIC 9.27 (66.56/ 4.98) 93.42
WL 10.70 (75.24/ 5.76) 93.52
LOD 35.32 (59.20/25.17) 93.81
WS+POS 63.75 (79.93/53.01) 96.03
WS +DIP 70.06 (83.27/60.47) 96.61
WS +STAG 57.42 (81.94/44.19) 95.67
WS +OIC 60.35 (77.98/49.22) 95.73
WS +WL 60.90 (76.39/50.63) 95.71
WS +LOD 70.85 (78.87/64.31) 96.53
WS+POS+DIP 73.41 (84.62/64.82) 96.93
WS+POS+DIP+STAG 74.58 (83.66/67.27) 97.01
WS+POS+DIP +OIC 76.87 (84.29/70.65) 97.23
WS+POS+DIP +WL 70.18 (83.33/60.62) 96.63
WS+POS+DIP +LOD 76.61 (82.61/71.43) 97.16
WS+POS+DIP+STAG+OIC 76.62 (84.48/70.09) 97.21
WS+POS+DIP+STAG +WL 74.12 (84.00/66.33) 96.98
WS+POS+DIP+STAG +LOD 77.64 (85.11/71.38) 97.33
WS+POS+DIP +OIC+WL 75.43 (84.76/67.95) 97.11
WS+POS+DIP +OIC +LOD 78.23 (84.23/73.03) 97.36
WS+POS+DIP +WL+LOD 74.01 (85.80/65.06) 97.02
WS+POS+DIP+STAG+OIC+WL 77.25 (83.97/71.53) 97.26
WS+POS+DIP+STAG+OIC +LOD 77.31 (86.36/69.97) 97.33
WS+POS+DIP+STAG +WL+LOD 76.55 (85.24/69.46) 97.23
WS+POS+DIP +OIC+WL+LOD 77.60 (84.30/71.89) 97.30
WS+POS+DIP+STAG+OIC+WL+LOD 78.41 (83.97/73.54) 97.36
F1: F1-Score, P: precision (%), R: recall (%), A: accuracy (%)
Table 3: Performance of the comma predictor
(A) #Characters,
Article (B) #Punctuations, (C) / (A) (C) / (B) Subjects
ID (C) #Commas
6 692 49 28 4.04% 57.14% L, T, C
7 335 30 15 4.48% 50.00% L, T, C
10 346 18 7 2.02% 38.89% L, T, C, Z
12 221 18 7 3.17% 38.89% L, T, C
14 572 33 14 2.45% 42.42% L, T, C
18 471 36 13 2.76% 36.11% C, Z
79 655 53 28 4.27% 52.83% Z
82 471 30 13 2.76% 43.33% Z
121 629 41 19 3.02% 46.34% Z
294 608 50 24 3.95% 48.00% Z
401 567 43 21 3.70% 48.84% L, T, C
406 558 39 18 3.23% 46.15% Z
413 552 52 22 3.99% 42.31% T, C, Z
423 580 49 26 4.48% 53.06% L, C, Z
438 674 46 28 4.15% 60.87% Z
Average 528.73 39.13 18.87 3.57% 48.22% -
Table 4: Materials assigned to each subject
? (find) etc., and following long clause-style ob-
jects. English commas, on the other hand, sel-
dom have such usages (Zeng, 2006). In Cases 3
and 4, commas instead of conjunctions sometimes
connect two clauses in a relation of either coordi-
nation or subordination. English commas, on the
other hand, are only required between independent
clauses connected by conjunctions (Zeng, 2006).
Liu et al (2010) proved that Chinese commas
can change the syntactic structures of sentences
by playing lexical or syntactic roles. Ren and
Yang (2010) claimed that inserting commas as
clause boundaries shortens the fixation time in
post-comma regions. Meanwhile, in computa-
tional linguistics, Xue and Yang (2011) showed
Figure 4: Obtained eye-movement trace map
0
100,000
200,000
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10
ith commas Without commas
Tota
l vie
win
g 
time
 
(sec
.)
0
1 0
2 0
Figure 5: Total viewing time
that Chinese sentence segmentation can be viewed
as detecting loosely coordinated clauses separated
by commas.
4 CRF Model-based Comma Predictor
We first predict comma placements in existing
text. The prediction is formalized as a task to an-
notate each word in a word sequence with an IOB-
style tag such as I-Comma (following a comma),
B-Comma (preceding a comma) or O (neither I-
Comma nor B-Comma). We utilize a CRF model
for this sequential labeling (Lafferty et al, 2001).
4.1 CRF Model for Comma Prediction
A conditional probability assigned to a label se-
quence Y for a particular sequence of words X in
a first-order linear-chain CRF is given by:
P?(Y |X) =
exp(
?n
w
?k
i ?ifi(Yw?1, Yw, X,w))
Z0(X)
where w is a word position in X , fi is a binary
function describing a feature for Yw?1, Yw, X , and
w, ?i is a weight for that feature, and Z0 is a nor-
malization factor over all possible label sequences.
The weight ?i for each fi is learned on training
data. For fi, the linguistic features shown in Ta-
ble 2 are derived from a syntactic parse of a sen-
tence1. The first three were used initially; the rest
were added after we got feedback from construc-
tion of our rule-based filters (see Section 5). Fig-
ure 2 shows an example of a parsing tree and its
corresponding training data.
1Some other features or tag formats which worked well in
the previous research, such as bi-/tri-gram, a preceding word
(L-1) or its POS (POS-1), and IO-style tag (Leaman and Gon-
zalez, 2008) were also examined, but they did not work that
well, probably because of the difference in task settings.
51
0
1,000
2,000
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10Fixa
tion
 
time
 
/
com
ma
 
(sec
.)
0.0
1.0
2.0 ith commas Without commas
Figure 6: Fixation time per comma
0.01.0
2.03.0
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10#reg
res
sion
s /
com
ma 0
ith co mas Without commas
12
3
Figure 7: Number of regressions per comma
4.2 Experimental Settings
The Penn Chinese Treebank (CTB) 7.0 (Nai-
wen Xue and Palmer, 2005) consists of 2,448
articles in five genres. It contains 1,196,329
words, and all sentences are annotated with parse
trees. We selected four genres for written Chi-
nese (newswire, news magazine, broadcast news
and newsgroups/weblogs) from this corpus as our
dataset. These were randomly divided into train-
ing (90%) and test data (10%). We also corrected
errors in tagging and inconsistencies in the dataset,
mainly by solving problems around strange char-
acters tagged as PU (punctuation). The commas
and characters after this preprocessing numbered
63,571 and 1,533,928 in the training data and
4,116 and 111,172 in the test data.
MALLET (McCallum, 2002) and its applica-
tion ABNER (Settles, 2005) were used to train the
CRF model. We evaluated the results in terms
of precision (P = tp/(tp + fp)), recall (R =
tp/(tp+fn)), F1-score (F1 = 2PR/(P+R)), and
accuracy (A = (tp + tn)/(tp + tn + fp + fn)),
where tp, tn, fp and fn are respectively the num-
ber of true positives, true negatives, false positives
and false negatives, based on whether the model
and the corpus provided commas at each location.
4.3 Performance of the CRF Model
Table 3 shows the performance of our CRF
model2. We can see that WS contributed much
more to the performance than other features, prob-
ably because a word surface itself has a lot of
information on both prosodic and syntactic func-
tions. Combining WS with other features greatly
improved performance, and as a result, with all
2Precision, recall, F1-score, and accuracy with WS + POS
+ DIP + L-1 + POS-1 were 82.96%, 65.04%, 72.91 and
96.84%, respectively (lower than those with WS+POS+DIP).
4080
120160
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10Sac
cad
e le
ngth
(1) / 
com
ma
ith co mas Without commas
4080
120160
(pixel)
Figure 8: Saccade length (1) per comma
30
60
90
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Commaith co mas Without commas
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z1030Sac
cad
e le
ngth
(2) / 
com
ma 60
90(pixel)
Figure 9: Saccade length (2) per comma
features (WS + POS + STAG + DIP + OIC + LOD
+ WL), precision, recall, F1-score and accuracy
were 83.97%, 73.54%, 78.41 and 97.36%.
We also found that a large number of false pos-
itives seemed helpful according to native speakers
(see the description of the subjects in Section 5 and
6). Although these commas do not appear in the
CTB text, they might smoothen the reading expe-
rience. We constructed a rule-based filter in order
to pick out such commas.
5 Rule-based Comma Filter
We constructed a rule-based comma filter for clas-
sifying commas in text into ones facilitating (pos-
itive) or obstructing (negative) the reading process
as follows:
[Step 1]: Collect gaze data from persons reading
text with or without commas (Section 5.1).
[Step 2]: Compare gaze features around commas
to find those features that reflect the effect of
comma placement. (Section 5.2).
[Step 3]: Annotate commas with categories based
on the obtained features (Section 5.3), and devise
rules to explain the annotation (Section 5.4).
5.1 Collecting Human Eye-movement Data
Eye-movements during reading contain rich infor-
mation on how the document is being read, what
the reader is interested in, where difficulties hap-
pen, etc. The movements are characterized by fix-
ations (short periods of steadiness), saccades (fast
movements), and regressions (backward saccades)
(Rayner, 1998). In order to analyze the effect of
commas on reading through the features, we col-
lected gaze data from subjects reading text in the
following settings.
[Subjects and Materials] Four native Man-
52
Categories Effect on readability Outward manifestation
Positive (?) Can improve readability. Presence would cause GF+.
Semi-positive (?) Might be necessary for readability, but the importance is not as obvious as a positive comma. Absence might cause GF-.
Semi-negative (2) Might be negative, but its severity is not as obvious as a negative comma. Absence might cause GF+.
Negative (?) Thought to reduce a document?s readability. Presence would cause GF-.
GF+/GF-: values of eye-tracking features that represent good/poor readability
Table 5: Comma categories
Subject Positive (?) Semi-positive (?) Semi-negative (2) Negative (?) Adjustment formula
L ?FT?>800 500<?FT??800 -100<?FT??500 ?FT?<-100 ?FT? = ?FT ? ?RT ? 200
C ?FT?>900 600<?FT??900 -200<?FT??600 ?FT?<-200 ?FT? = ?FT ? ?RT ? 275
T ?FT?>600 300<?FT??600 -300<?FT??300 ?FT?<-300 ?FT? = ?FT ? ?RT ? 250
Z ?FT?>650 350<?FT??650 -250<?FT??350 ?FT?<-250 ?FT? = ?FT ? ?RT ? 250
?FT = [ fixation time (without commas) [ms]]? [ fixation time (with commas) [ms]]
?RT = [ #regressions (without commas) ]? [ #regressions (with commas) ]
Table 6: Estimation formula for judging the contribution of commas to readability
ID ? ? 2 ?
6 13 6 4 5
7 8 6 1 0
10 5 0 1 1
12 1 4 2 0
14 4 4 5 1
18 5 1 4 3
79 11 4 9 4
82 5 6 2 0
ID ? ? 2 ?
121 11 2 6 0
294 9 9 4 1
401 10 7 2 2
406 5 6 5 2
413 8 5 6 3
423 11 4 7 4
438 6 16 6 0
Total 112 80 64 26
Table 7: Categories of annotated commas
darin Chinese speakers (graduate students and re-
searchers) read 15 newswire articles selected from
CTB 7.0 (included in the test data in Section 4.2).
Table 4 and Figure 3(a) show the materials as-
signed to each subject and a screenshot of one ma-
terial. Each article was presented in 12-15 points
of bold-faced Fang-Song font occupying 13?13,
14?15, 15?16 or 16?16 pixels along with a line
spacing of 5-10 pixels3.
[Apparatus] Figure 3(b) shows a scene of the
experiment. An EyeLink 1000 eye tracker (SR
Research Ltd., Toronto, Canada) with a desktop
mount monitored the movements of a right eye at
1,000 Hz. The subject?s head was supported at the
chin and forehead. The distance between the eyes
and the monitor was around 55 cm, and each Chi-
nese character subtended a visual angle 1?. Text
was presented on a 19? monitor at a resolution
of 800?600 pixels, with the brightness adjusted
to a comfortable level. The displayed article was
masked except for the area around a gaze point
(see Figure 3(c)) in order to confirm that the gaze
point was correctly detected and make the subject
concentrate on the area (adjusted for him/her).
[Procedure] Each article was presented twice
(once with/once without commas) to each subject.
3These values, as well as the screen position of the article,
were adjusted for each subject.
The one without commas was presented first4 (not
necessarily in a row). We did not give any compre-
hension test after reading; we just asked the sub-
jects to read carefully and silently at their normal
or lower speed, in order to minimize the effect of
the first reading on the second. The subjects were
informed of the presence or absence of commas
beforehand. The apparatus was calibrated before
the experiment and between trials. The experi-
ment lasted around two hours for each subject.
[Alignment of eye-tracking data to text] Figure 4
shows an example of the obtained eye-movement
trace map, where circles and lines respectively
mean fixation points and saccades, and color depth
shows their duration. The alignment of the data to
the text is a critical task, and although automatic
approaches have been proposed (Mart??nez-Go?mez
et al, 2012a; Mart??nez-Go?mez et al, 2012b), they
do not seem robust enough for our purpose. Ac-
cordingly, we here just compared the entire layout
of the gaze point distribution and that of the actual
text, and adjusted them to have relatively coherent
positions on the x-axis; i.e., the beginning and end
of the gaze point sequence in a line were made as
close as possible to those of the line in the text.
5.2 Analysis of Eye-movement Data
The gaze data were analyzed by focusing on re-
gions around each comma or where each one
should be (three characters left and right to the
comma5).
4If we had used the reversed order, the subject would have
knowledge about original comma distribution, and this would
cause abnormally quick reading of the text without commas.
With the order we set, conflicts between false segmentations
(made in first reading) and correct ones might bother the sub-
ject, which is trade-off (though minor) in the second reading.
5When a comma appeared at the beginning of a line, two
characters to the left and right of the comma and one charac-
53
1. If L Seg and R Seg are both very long, a comma must be put between them.
2. If two ? appear serially, one is necessary whereas the other might be optional or judged negative, but it still depends on the lengths of the siblings.
3. If two neighboring commas appear very close to each other, one of them is judged as negative whereas judgment on the other one is reserved.
4. If several (more than 2) ?s appear continually, one or more ?s might be reserved in consideration of the global condition.
5. A comma is always needed after a long sentence or clause without any syntactically significant punctuation with the function of segmentation.
6. If a ? appears near a ?, it might be judged as negative with a high probability. However, the judgment process is always from the bottom up, which
means ? ? 2? ???. For example, if a 2 appears near a ?, we judge 2 first (to be positive or negative), then judge the ? in the condition
with or without the comma of 2.
Table 8: General rules for reference
Figure 5, 6 and 7 respectively show the total
viewing time, fixation time (duration for all fix-
ations and saccades in a target region) per comma,
and number of regressions per comma6 for each
trial. We can see a general trend wherein the for-
mer two were shorter and the latter was smaller for
the articles with commas than without. The diver-
sity of the subjects was also observed in Figure 6.
Figure 8 and 9 show the saccade length per
comma for different measures. The former (lat-
ter) figure considers a saccade in which at least
one edge (both edges) was in the region. We can-
not see any global trend, probably because of the
difference in global layout of materials brought by
the presence or absence of commas.
5.3 Categorization of Commas
Using the features shown to be effective to repre-
sent the effect of comma placement, we analyzed
the statistics for each comma in order to manu-
ally construct an estimation formula for judging
the contribution of each comma to readability. The
contribution was classified into four categories
(Table 5), and the formula is described in Table 67.
The adjustment formula was based on our obser-
vation that the number of regressions could only
be regarded as an aid. For example, for subject
C, if ?FT=200ms and ?RT =?2, ?FT?=?350,
and therefore, the comma is annotated as negative.
All parameters were decided empirically and man-
ually checked twice (self-judgment and feedback
from the subjects).
On the basis of this estimation formula, all arti-
cles in Table 4 were manually annotated. Table 7
shows the distribution of the assigned categories8.
ter to the left and right of the final character of the last line
were analyzed.
6Calculated by counting the instances where the x-
position of [a fixation / end point of a saccade ] was ahead
of [the former fixation / its start point]. Although the counts
of these two types were almost the same, by counting both of
them, we expected to cover any possible regression.
7One or two features are used to judge the category of a
comma. We will explore more features in the future.
8In the case of severe contradictions, the annotators dis-
cussed them and resolved them by voting.
5.4 Implementation of Rule-based Filter
The annotated commas were classified into Cases
1 to 5 in Table 1, based on the types of left and
right segment conjuncts (L Seg and R Seg, which
were obtained from the parse trees in CTB). For
each of the five cases, the reason for the assign-
ment of a category (?, ?, 2 or ?) to each
comma was explained by a manually constructed
rule which utilized information about L Seg and
R Seg. The rules were constructed so that they
would cover as many instances as possible. Ta-
ble 8 shows the general rules utilized as a refer-
ence, and Table 9 shows the finally obtained rules.
The rightmost column in this table shows the num-
ber of commas matching each rule. These rules
were then implemented as a filter for classifying
commas in a given text.
For several rules (?10, 28, 210, 211 and
212), there were only single instances. In addi-
tion, although our rules were built carefully, a few
exceptions to the detailed threshold were found.
Collecting and investigating more gaze data would
help to make our rules more sophisticated.
6 Performance of the Rule-based Filter
We assumed that our comma predictor provides a
CTB text with the same distribution as the origi-
nal one in CTB (see Figure 1). Accordingly, we
examined the quality of the comma categorization
by our rule-based filter through gaze experiments.
6.1 Experimental Settings
Another five native Mandarin Chinese speakers
were invited as test subjects. The CTB articles as-
signed to the subjects are listed in Table 10. These
articles were selected from the test data in Sec-
tion 4.2 in such a way that 520<#characters<700,
#commas>17, #commas/#punctuations>38%,
and #commas/#characters>3.1%, since we
needed articles of appropriate length with a fair
number of commas. After that, we manually
chose articles that seemed to attract the subjects?
interest from those that satisfied the conditions.
54
Case 1: L Subject + R Predicate #commas
?6 L IP-SBJ + R VP (length both<14 (In Seg Len)) 2
?7 L IP-SBJ/NP-SBJ (Org Len>13, Ttl Len>15) 7
?6 L NP-SBJ/IP-SBJ (<14) + R VP (?25) 2
Case 2: L Predicate + R Object #commas
?9 Long frontings (Modifier/Subject, >7) + short L predicate (VV/VRD/VSB? ? ? , ?3) + Longer R object (IP-OBJ, >28) 6
?8 Short frontings (<5) + short L predicate (<3) + moderate-length R object (IP-SBJ, <20) 4
26 Short frontings (<6) + short L predicate (?3) + long R object (IP-SBJ, >23) 9
Case 3: L Modifier #commas
?3 Short frequently used L modifier (2-3,??,??, etc.) + moderate-length/long R SPO (?w18p10) 13
?7 Short L (PP/LCP)-TMP (5, 6) + long R NP (?10) 4
?10 Long L CP-CND (e.g.,??, >18) + moderate-length R Seg (SPO, IP, etc. <18) 1
?1 Long L modifier (PP(-XXX, P+Long NP/IP), IP-ADV, ?17) 6
?4 Moderate-length/short L modifier (PP(-XXX, P+IP, There is IP inside, >6<15, cf. 26 (NP)) 9
?9 Long L (PP/LCP)-TMP (Ttl Len?10), short R Seg (NP/ADVP, <3) 4
?10 Short L (LCP/PP)-LOC (<8) 2
22 Long L LOC (or there is LCP inside PP, >10) 5
23 Very short frequently used L ADVP/ADV (2) 8
25 Short L (PP/LCP/NP)-TMP (4;5-6, when R Seg is short (<10)) 12
24 Moderate-length PP(-XXX, P+NP, >8 ?13) + R Seg (SPO, IP, VO, MSPO, etc.) 6
28 Short L IP-CND (<8) 1
211 Long L PP-DIR (>20) + short R VO (?10) 1
?2 Very short L (QP/NP/LCP)-TMP (?3) 8
?5 Short frequently used L modifier (as in ?3, ?3) + short/moderate-length R Seg (SPO etc., <c20w9) 1
Case 4: L c + R c #commas
?2 L c & R c are both long (In Seg Len?15; or one>13, the other near 20) 39
?8 L c is the summary of R c 2
?2 Moderate-length L c + R c (both ?10?15; or one?17, the other?12) 25
?3 Moderate-length clause (>10), but connected with familiar CC or ADVP 6
?5 Three or more consecutive moderate-length clauses (all<15, and at least one ?10) 12
?7 Very short L c + R c (both <5), something like slogan) 1
Case 5: L p + R p #commas
?1 Short coordinate modifiers (Both side <5) 4
?4 Short L p+R p (both<c15w5, and at least one <10), but pre-L p (e.g., SBJ) is too long (>18) 2
?5 Between two moderate-length/long phrases (both ?15; or L p?17, R p=10-14; Or L p=10-14, R p>20) 39
?11 Long pre-L p (SBJ /ADV, etc. >16) + short L p (?5) + long R p (?18) 2
(?3 Moderate-length phrase (>10), but connected with familiar CC or ADVP) (6)
?6 Three or more consecutive short/moderate-length phrases (both<15, at least one<8) 5
21 Between short phrases (both ?c13w5), and pre-L p (SBJ/ADV, etc.) is short/moderate-length (<11) 13
27 Coordinate VPs, and L VP is a moderate-length VP (PP-MNR VP) 4
29 Phrasal coordination between a long (?18) and a short (<10) phrase 3
210 Moderate-length coordinate VPs (>10<15), and R VP has the structure like VP (MSP VP) 1
212 Between two short/moderate-length NP phrases (both ?15, e.g., L NP-TPC+R NP-SBJ) 1
?1 Moderate-length/short phrase ((i) c:one>10<18, The other >5?10, w:one?5, the other>5?10; (ii) c:both?10<15, 13
w:both>5?7), and pre-L p (SBJ/ADV, etc.) is short (?5)
? L x/R x: the left/right segment of a target comma which is x.
(x can be ?p? (phrase) / ?c? (clause), syntactic tags (with function tags) such as ?VP? and ?IP-SBJ?, or general functions such as ?Subject? and ?Predicate?.)
? Org Len: the number of characters in a segment (including other commas or punctuation inside).
? In Seg Len/Ttl Len: the number of characters between the comma and nearest punctuation (inside a long/outside a short target segment).
? SPO: subject + predicate + object, belonging to the outermost sentence. The length is defined in the similar way as In Seg Len.
? MSPO: modifier + subject + predicate + object. The length is defined in the similar way as In Seg Len.
? -XX or -XXX: arbitrary type of possible functional tag (or without any functional tag) connected with the former syntactic tag.
? ?ciwj: #characters?i and #words?j.
? In some cases (in Case 3, 4 and 5), the length is calculated after negative (or judged negative) commas are eliminated.
? The rules related with TMP are applied faster than ones related with LCP (in Case 3).
? ?3 appears in both Case 4 (clause) and Case 5 (phrase). The number of commas is given by the sum of those in both cases.
Table 9: Entire classification of rules based on traditional comma categories
(A) #Characters,
Article (B) #Punctuations, (C) / (A) (C) / (B) Subjects
ID (C) #Commas
6 692 49 28 4.04% 57.14% L, S, H
11 672 48 21 3.13% 43.75% L, S, F
15 674 67 26 3.86% 38.81% L, S, H
16 547 43 22 4.02% 51.16% L, S, F
56 524 43 18 3.44% 41.86% L, H, M
73 595 46 28 4.71% 60.87% S, H, F, M
79 655 53 28 4.27% 52.83% H, F, M
99 671 55 24 3.58% 43.64% F, M
Average 628.75 50.50 24.38 3.88% 48.27% -
Table 10: Materials assigned to each subject
Our rule-based filter was applied to the commas
of each article9, and the commas were classified
9Instances of incoherence among the applied rules were
0
40,000
80,000
120,000
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)Positive distribution Negative distribution
040
8012
Tot
al v
iew
ing
tim
e (se
c.)
Figure 10: Total viewing time for two distributions
into two distributions: a positive one (positive +
semi-positive commas) and a negative one (nega-
tive + semi-negative commas). Two types of ma-
terials were thus generated by leaving the commas
in one distribution and removing the others.
manually checked and corrected.
55
20
40
60
80
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
FFT
(10
0) Positive distribution Negative distribution
Figure 11: EMFFT for two distributions
46
810
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
FT
(80
0) Positive distribution Negative distribution
Figure 12: EMFT for two distributions
The apparatus and procedure were almost the
same as those in Section 5.1, whereas, on the ba-
sis of the feedback from the previous experiments,
the font size, number of characters in a line, and
line spacing were fixed to single optimized values,
respectively, 14-point Fang-Song font occupying
15?16 pixels, 33 characters and 7 pixels.
6.2 Evaluation Metrics
We examined whether our positive/negative distri-
butions really facilitated/obstructed the subjects?
reading process by using the following metrics:
TT, EMFFT = FFTFT
10
, EMFT = FTCN?TT
11
,
EMRT = RT2?CN
12
, EMSLO = SLO2?TT ,
where TT, FT, RT and CN are total viewing time,
fixation time, number of regressions, and num-
ber of commas respectively, as described in Sec-
tion 5.2. FFT and SLO are additionally introduced
metrics respectively for the ?total duration for all
first-pass fixations in a target region that exclude
any regressions? and for the ?length of saccades
from inside a target region to the outside?13. All of
the areas around commas appearing in the original
article were considered target areas for the metrics.
The other settings were the same as in Section 5.
6.3 Contribution of Categorized Commas
Figure 10, 11, 12, 13 and 14 respectively show TT,
EMFFT , EMFT , EMRT and EMSLO for two types
of comma distributions in each trial.
10Ratio to the total fixation time in the target areas (FT).
11Normalized by the total viewing time (TT).
12Two types of RT count (see Section 5.2) were averaged.
13Respectively to reflect ?the early-stage processing of the
region? and ?the information processed for a fixation and a
decision of the next fixation point? (Hirotani et al, 2006).
0
5
10
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
RT
(1
0) Positive distribution Nega ive distribution
Figure 13: EMRT for two distributions
0
5
10
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)Positive distribution Negative distribution
EM
SLO
(10
0)
Figure 14: EMSLO for two distributions
For TT, we cannot see any general trend, mainly
because this time, the reading order of the text
was random, which spread out the second reading
effect evenly between the two distributions. For
EMFFT , we cannot reach a conclusion either. In
contrast, in more than half of the trials, EMFFT
was larger for positive distributions, which would
imply that the positive commas helped to prevent
the reader?s gaze from revisiting the target regions.
For most trials, except for subject S whose cal-
ibration was poor and reading process was poor
in M56, EMFT and EMRT decreased and EMSLO
increased for positive distributions, which implies
that the positive commas smoothed the reading
process around the target regions.
7 Conclusion
We proposed an approach for modeling comma
placement in Chinese text for smoothing reading.
In our approach, commas are added to the text on
the basis of a CRF model-based comma predic-
tor trained on the treebank, and a rule-based filter
then classifies the commas into ones facilitating or
obstructing reading. The experimental results on
each part of this approach were encouraging.
In our future work, we would like see how com-
mas affect reading by using much more material,
and thereby refine our framework in order to bring
a better reading experience to readers.
Acknowledgments
This research was partially supported by Kakenhi,
MEXT Japan [23650076] and JST PRESTO.
56
References
Wallace Chafe. 1988. Punctuation and the prosody of
written language. Written Communication, 5:396?
426.
Yuqing Guo, Haifeng Wang, and Josef van Genabith.
2010. A linguistically inspired statistical model
for Chinese punctuation generation. ACM Trans-
actions on Asian Language Information Processing,
9(2):6:1?6:27, June.
Masako Hirotani, Lyn Frazier, and Keith Rayner. 2006.
Punctuation and intonation effects on clause and
sentence wrap-up: Evidence from eye movements.
Journal of Memory and Language, 54(3):425?443.
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Pause and
stop labeling for Chinese sentence boundary detec-
tion. In Proceedings of Recent Advances in Natural
Language Processing, pages 146?153.
Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proceedings of the International Conference on
Spoken Language Processing, pages 917?920.
Mei xun Jin, Mi-Young Kim, Dongil Kim, and Jong-
Hyeok Lee. 2002. Segmentation of Chinese
long sentences using commas. In Proceedings of
the Third SIGHAN Workshop on Chinese Language
Processing, pages 1?8.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survery of advances in biomed-
ical named entity recognition. In Pacific Symposium
on Biocomputing (PSB?08), pages 652?663.
Baolin Liu, Zhongning Wang, and Zhixing Jin. 2010.
The effects of punctuations in Chinese sentence
comprehension: An erp study. Journal of Neurolin-
guistics, 23(1):66?68.
Wei Lu and Hwee Tou Ng. 2010. Better punctuation
prediction with dynamic conditional random fields.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
?10), pages 177?186.
Pascual Mart??nez-Go?mez, Chen Chen, Tadayoshi Hara,
Yoshinobu Kano, and Akiko Aizawa. 2012a. Image
registration for text-gaze alignment. In Proceedings
of the 2012 ACM international conference on Intel-
ligent User Interfaces (IUI ?12), pages 257?260.
Pascual Mart??nez-Go?mez, Tadayoshi Hara, Chen
Chen, Kyohei Tomita, Yoshinobu Kano, and Akiko
Aizawa. 2012b. Synthesizing image representa-
tions of linguistic and topological features for pre-
dicting areas of attention. In Patricia Anthony, Mit-
suru Ishizuka, and Dickson Lukose, editors, PRICAI
2012: Trends in Artificial Intelligence, pages 312?
323. Springer.
Andrew Kachites McCallum. 2002. MALLET: A ma-
chine learning for language toolkit.
Fu-dong Chiou Naiwen Xue, Fei Xia and Marta
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Stephan Peitz, Markus Freitag, Arne Mauser, and Her-
mann Ney. 2011. Modeling punctuation prediction
as machine translation. In Proceedings of Interna-
tional Workshop on Spoken Language Translation,
pages 238?245.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124(3):372?422.
Gui-Qin Ren and Yufang Yang. 2010. Syntac-
tic boundaries and comma placement during silent
reading of Chinese text: evidence from eye move-
ments. Journal of Research in Reading, 33(2):168?
177.
Burr Settles. 2005. ABNER: an open source tool
for automatically tagging genes, proteins, and other
entity names in text. Bioinformatics, 21(14):3191?
3192.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-
Tu?r, and Go?khan Tu?r. 2000. Prosody-based au-
tomatic segmentation of speech into sentences and
topics. Speech Communication, 32(1-2):127?154.
Judy Perkins Walker, Kirk Fongemie, and Tracy
Daigle. 2001. Prosodic facilitation in the resolu-
tion of syntactic ambiguities in subjects with left
and right hemisphere damage. Brain and Language,
78(2):169?196.
Nianwen Xue and Yaqin Yang. 2011. Chinese sen-
tence segmentation as comma classification. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics:shortpapers,
pages 631?635.
Ming Yue. 2006. Discursive usage of six Chinese
punctuation marks. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages
43?48.
ZDIC.NET. 2005. Commonly used Chinese punctua-
tion usage short list. Long Wiki, Retrieved Dec 10,
2012, from http://www.zdic.net/appendix/f3.htm.
(in Chinese).
X. Y. Zeng. 2006. The comparison and the use
of English and Chinese comma. College English,
3(2):62?65. (in Chinese).
57
Kaixu Zhang, Yunqing Xia, and Hang Yu. 2006.
CRF-based approach to sentence segmentation and
punctuation for ancient Chinese prose. Jour-
nal of Tsinghua Univ (Science and Technology),
49(10):1733?1736. (in Chinese).
58
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 44?52,
Dublin, Ireland, August 23rd 2014.
Significance of Bridging Real-world Documents and NLP Technologies
Tadayoshi Hara Goran Topic? Yusuke Miyao Akiko Aizawa
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan
{harasan, goran topic, yusuke, aizawa}@nii.ac.jp
Abstract
Most conventional natural language processing (NLP) tools assume plain text as their input,
whereas real-world documents display text more expressively, using a variety of layouts, sentence
structures, and inline objects, among others. When NLP tools are applied to such text, users
must first convert the text into the input/output formats of the tools. Moreover, this awkwardly
obtained input typically does not allow the expected maximum performance of the NLP tools to
be achieved. This work attempts to raise awareness of this issue using XML documents, where
textual composition beyond plain text is given by tags. We propose a general framework for
data conversion between XML-tagged text and plain text used as input/output for NLP tools and
show that text sequences obtained by our framework can be much more thoroughly and efficiently
processed by parsers than naively tag-removed text. These results highlight the significance of
bridging real-world documents and NLP technologies.
1 Introduction
Recent advances in natural language processing (NLP) technologies have allowed us to dream about
applying these technologies to large-scale text, and then extracting a wealth of information from the text
or enriching the text itself with various additional information. When actually considering the realization
of this dream, however, we are faced with an inevitable problem. Conventional NLP tools usually assume
an ideal situation where each input text consists of a plain word sequence, whereas real-world documents
display text more expressively using a variety of layouts, sentence structures, and inline objects, among
others. This means that obtaining valid input for a target NLP tool is left completely to the users, who
have to program pre- and postprocessors for each application to convert their target text into the required
format and integrate the output results into their original text. This additional effort reduces the viability
of technologies, while the awkwardly obtained input does not allow the expected maximum benefit of
the NLP technologies to be realized.
In this research, we raise awareness of this issue by developing a framework that simplifies this con-
version and integration process. We assume that any textual composition beyond plain text is captured by
tags in XML documents, and focus on the data conversion between XML-tagged text and the input/output
formats of NLP tools. According to our observations, the data conversion process is determined by the
textual functions of the XML-tags utilized in the target text, of which there seem to be only four types.
We therefore devise a conversion strategy for each of the four types. After all tags in the XML tagset of
the target text have been classified by the user into the four types, data conversion and integration can be
executed automatically using our strategies, regardless of the size of the text (see Figure 1).
In the experiments, we apply our framework to several types of XML documents, and the results show
that our framework can extract plain text sequences from the target XML documents by classifying only
20% or fewer of the total number of tag types. Furthermore, with the obtained sequences, two typical
parsers succeed in processing entire documents with a much greater coverage rate and using much less
parsing time than with naively tag-removed text.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/44
Formatted Input(e.g. plain text)
Output analysis(e.g. parse trees)
<p> A user task is a scenario of use of the UI, ? the UML 
notation.  In our case, we use the CTT (Concur Task Tree) 
<cite>[ <bibref bibrefs="paterno-ctte-2001,paterno-ctt-2001" separator="," show="Number" yyseparator=","/>]</cite></p>
A user task is a scenario of use of the UI, ? the UML notation.In our case, we use the CTT (Concur Task Tree) [1]
<sentence id=?s0?><cons>?</cons></sentence>
<sentence id=?s1?>?<cons><tok>[</tok>
</cons><cons><cons><tok>1</tok></cons> ? 
<cons><tok>]</tok></cons>?</sentence>
Independent
Decoration
Object
Meta-info
Tag1
Tag2
?
(Classifyinto 4 types)
TextstructuredbyXML
TextstructuredbyXML
Textstructuredby XML
Tagset
Dataconversion NLP tools(Parser)
Figure 1: Proposed data conversion framework for applying NLP tools to text structured as XML
Tag type Criteria for classification Strategies for data conversion
Independent To represent a region syntactically Remove the tag and tagged region
independent from the surrounding text ? (apply tools to the tagged region independently)
? recover the (analyzed) tagged region after applying the tools
Decoration To set the display style of Remove only the tag
the tagged region at the same level as ? recover the tag after applying the tools
the surrounding text
Object To represent the minimal object Replace the tag (and the tagged region) with a plain word
unit that should be handled in the ? (do not process the tagged region further)
the same level as the surrounding text ? recover the tag (and region) after applying the tools
Meta-info To describe the display style Remove the tag and tagged region
setting or additional information ? (do not process the tagged region further)
? recover the tag and region after applying the tools
Table 1: Four types of tags and the data conversion strategy for each type
The contribution of this work is to demonstrate the significance of bridging real-world documents and
NLP technologies in practice. We show that, if supported by a proper framework, conventional NLP tools
already have the ability to process real-world text without significant loss of performance. We expect the
demonstration to promote further discussion on real-world document processing.
In Section 2, some related research attempts are introduced. In Section 3, the four types of textual
functions for XML tags and our data conversion strategies for each of these are described and imple-
mented. In Section 4, the efficiency of our framework and the adequacy of the obtained text sequences
for use in NLP tools are examined using several types of documents.
2 Related Work
To the best of our knowledge, no significant work on a unified methodology for data conversion between
target text and the input/output formats of NLP tools has been published. Some NLP tools provide
scripts for extracting valid input text for the tools from real-world documents; however, even these scripts
assume specific formats for the documents. For example, deep syntactic parsers such as the C&C Parser
(Clark and Curran, 2007) and Enju (Ninomiya et al., 2007) assume POS-tagged sentences as input, and
therefore the distributed packages for the parsers1 contain POS-taggers together with the parsers. The
POS-taggers assume plain text sentences as their input.
As the work most relevant to our study, UIMA (Ferruci et al., 2006) deals with various annotations
in an integrated framework. However, in this work, the authors merely proposed the framework and did
1[C&C Parser]: http://svn.ask.it.usyd.edu.au/trac/candc/wiki / [Enju]: http://kmcs.nii.ac.jp/enju/45
New UI is shown. The UI is more useful  than XYZ          , and ... .
text indexmark
Cite1
Notice that ? . notecite
<text>New UI</text> is shown. The UI is more useful than XYZ <indexmark>
? </indexmark> in <cite>[ ? ]</cite><note>Notice that ? .</note>, and ? .
sentence sentence
New UI is shown. The UI is more useful  than XYZ          , and ... .Cite1 sentence
<sentence><text>New UI</text> is shown.</sentence> <sentence>The UI is more useful  than XYZ<indexmark> 
? </indexmark> in <cite>[?]</cite><note><sentence>Notice that ? . </sentence></note>, and ? .</sentence> 
(a)
(c)
(d)
(b)
text indexmark notecite Notice that ? . 
Meta-infoDecoration
IndependentObject
Figure 2: Example of executing our strategy
not explain how the given text can be used in a target annotation process such as parsing. Some projects
based on the UIMA framework, such as RASP4UIMA (Andersen et al., 2008), U-compare (Kano et
al., 2011), and Kachako (Kano, 2012)2, have developed systems where the connections between various
documents and various tools are already established. Users, however, can utilize only the text and tool
pairs that have already been integrated into the systems. GATE (Cunningham et al., 2013) is based on
a similar concept to UIMA; it supports XML documents as its input, while the framework also requires
integration of tools into the systems.
In our framework, although availability of XML documents is assumed, a user can apply NLP tools to
the documents without modifying the tools; instead, this is achieved by merely classifying the XML-tags
in the documents into a small number of functional types.
3 Data Conversion Framework
We designed a framework for data conversion between tagged text and the input/output formats of NLP
tools based on the four types of textual functions of tags. First, we introduce the four tag types and
the data conversion strategy for each. Then, we introduce the procedure for managing the entire data
conversion process using the strategies.
3.1 Strategies for the Four Tag Types
The functions of the tags are classified into only four types, namely, Independent, Decoration, Object,
and Meta-info, and for each of these types, a strategy for data conversion is described, as given in Table
1. This section explains the types and their strategies using a simple example where we attempt to apply
a sentence splitter to the text given in Figure 2(a). The target text has four tags, ?<note>?, ?<text>?,
?<cite>?, and ?<indexmark>?, denoting, respectively, Independent, Decoration, Object, and Meta-
info tags. We now describe each of the types.
Regions enclosed by Independent tags contain syntactically independent text, such as titles, sections,
and so on. In some cases, a region of this type is inserted into the middle of another sentence, like
the ?<note>? tags in the example, which represent footnote text. The data conversion strategy for text
containing these tags is to split the enclosed region into multiple subregions and apply the NLP tools
separately to each subregion.
2[U-compare]: http://u-compare.org/ / [Kachako]: http://kachako.org/kano/46
<?xml ?><document ?><title>Formal approaches ? </title><creator> ? </creator><abstract>This research ? </abstract><section><title>Introduction</title><para><p><text>New UI</text> is shown. The UI is more useful than XYZ<indexmark> ? </indexmark> in <cite>[ ? ]</cite><note>Notice that ? </note> and ? .</p></para></section><bibliography> ? </bibliography></document>
(a) XML document
?xml document
creator section bibliographytitle abstractThis research ? Formal ?
paratitleIntroduction
p
textNew UI indexmark? cite[?] noteNotice ? . and ? .inis ? XYZ
(b) Structure of XML document
Figure 3: Example XML document
Decoration tags, on the other hand, do not necessarily guarantee the independence of the enclosed
text regions, and are utilized mainly for embossing the regions visually, such as changing the font or
color of the text (?<text>? in the example), paragraphing sections3, and so on. The data conversion
strategy for text containing these tags is to remove the tags before inputting the text into the NLP tools,
and then to recover the tags afterwards4.
Regions enclosed by Object tags contain special descriptions for representing objects treated as single
syntactic components in the surrounding text. The regions do not consist of natural language text, and
therefore cannot be analyzed by NLP tools5. The data conversion strategy for text containing this tag is
to replace the enclosed region with some proper character sequence before inputting the text into NLP
tools, and then to recover the replaced region afterwards.
Regions enclosed by Meta-info tags are not targets of NLP tools, mainly because the regions are not
displayed, but utilized for other purposes, such as creating index pages (like this ?<indexmark>?)6. The
data conversion strategy for text containing these tags is to delete the tagged region before inputting the
text into NLP tools, and then to recover the region afterwards.
According to the above strategies, which are summarized in Table 1, conversion of the example text
in Figure 2(a) is carried out as follows. In the first step, tags are removed from the text, whilst retaining
their offsets in the resulting tag-less sequence shown in (b). ?Cite1? in the sequence is a plain word
utilized to replace the ?<cite>? tag region. For the ?<note>? tag, we recursively apply our strategies
to its inner region, with two plain text regions ?New UI ...? and ?Notice that ...? consequently input into
the sentence splitter. Thereafter, sentence boundary information is returned as shown in (c), and finally,
using the retained offset information of the tags, the obtained analysis and original tag information are
integrated to produce the XML-tagged sequence shown in (d).
3.2 Procedure for Efficient Tag Classification and Data Conversion
In actual XML documents as shown in Figure 3(a), a number of tags are introduced and tagged regions
are multi-layered as illustrated in Figure 3(b) (where black and white boxes represent, respectively, XML
tags and plain text regions, and regions enclosed by tags are placed below the tags in the order they
appear.). We implemented a complete data conversion procedure for efficiently classifying tags in text
documents into the four types and simultaneously obtaining plain text sequences from such documents,
3In some types of scientific articles, one sentence can be split into two paragraph regions. It depends on the target text
whether a paragraph tag is classified as Independent or Decoration.
4The tags may imply that the enclosed regions constitute chunks of text, which may be suitable for use in NLP tools.
5In some cases, natural language text is used for parts of the descriptions, for example, itemization or tables in scientific
articles. How the inner textual parts are generally associated with the surrounding text would be discussed in our future work.
For the treatment of list structures, we can learn more from A??t-Mokhtar et al. (2003).
6If the tagged region contains analyzable text, it depends on the user policy whether NLP tools should be applied to the
region, that is, whether to classify the tag as Independent.47
@plain_text_sequences = ();   # plain text sequences input to NLP tools@recovery_info = ();                 # information for recovering original document after applying NLP tools@unknown = ();                        # unknown tags
function data_convert ($target_sequence, $seq_ID) {
if ($target_sequence contains any tags) {   # process one instance of tag usage in a target sequence$usage = (pick one instance of top-level tag usage in $target_sequence);$tag = (name of the top-level tag in $usage);@attributes = (attributes and their values for the top-level tag in $usage);$region = (region in $target_sequence enclosed by tag $tag in $usage); $tag_and_region =  (region in $target_sequence consisting of $region & tag $tag enclosing it);
if ($tag ?@independent)             { remove $tag_and_region from $target_sequence;add [?independent?, $tag, @attributes,  $seq_ID, $seq_ID + 1,(offset in $target_sequence where $tag_and_region should be inserted) ] to @recovery_info;data_convert($region,  $seq_ID + 1);  } # process the tagged region separatelyelse if ($tag ?@decoration)         { remove only tag $tag enclosing $region from $target_sequence; add [?decoration?, $tag, @attributes,(offsets in $target_sequence where $region begans and ends)] to @recovery_info; }else if ($tag ?@object)                 { replace $tag_and_region in $target_sequence with a unique plain word $uniq;add [?object?, $uniq, $tag_and_region] to @recovery_info; }else if ($tag ?@meta_info)          { remove $tag_and_region from $target_sequence;add [?meta_info?, $tag_and_region,(offset in $target_sequence where $tag_and_region should be inserted)] to @recovery_info; }else                                                     { replace $tag_and_region in $target_sequence with a unique plain word $uniq;add [?unknown?, $uniq, $tag_and_region] to @recovery_info;if ($tag ?@unknown) { add $tag to @unknown; }  }
data_convert($target_sequence, $seq_ID);  # process the remaining tags}else {  # a plain text sequence is obtainedadd [$seq_id,  $target_sequence] to @plain_text_sequences;} }
function main ($XML_document) {data_convert ($XML_document, 0);return @plain_text_sequences, @recovery_info, @unknown;}
Figure 4: Pseudo-code algorithm for data conversion from XML text to plain text for use in NLP tools
as given by the pseudo-code algorithm in Figure 4. In the remainder of this section, we explain how the
algorithm works.
Our data conversion procedure applies the strategies for the four types of tags recursively from the
top-level tags to the lower tags. The @independent, @decoration, @object and @meta info lists
contain, respectively, Independent, Decoration, Object, and Meta-info tags, which have already been
classified by the user. When applied to a target document, the algorithm uses the four lists and strategies
given in the previous section in its first attempt at converting the document into plain text sequences,
storing unknown (and therefore unprocessed) tags, if any, in @unknown. After the entire document has
been processed for the first time, the user classifies any reported unknown tags. This process is repeated
until no further unknown tags are encountered.
In the first iteration of processing the document in Figure 3(a) the algorithm is applied to the target
document with the four tag lists empty. In the function ?data convert?, top-level tags in the document,
?<?xml>? and ?<document>?, are detected as yet-to-be classified tags and added to @unknown.
The tags and their enclosed regions in the target document are replaced with unique plain text such as
?UN1? and ?UN2?, and the input text thus becomes a sequence consisting of only plain words like ?UN1
UN2?. The algorithm then adds the sequence to @plain text sequences and terminates. The user then
classifies the reported yet-to-be classified tags in @unknown into the four tag lists, and the algorithm48
Article # ar- # total tags # classified tags (# types) #obtain-
type ticles (# types) I D O M Total ed seq.
PMC 1,000 1,357,229( 421) 32,109(12) 62,414( 8) 48205( 9) 33,953(56) 176,681( 85) 25,679
ArX. 300 1,969,359(210?) 5,888(15) 46,962(12) 60,194( 8) 7,960(17) 121,004( 52) 4,167
ACL 67 130,861( 66?) 3,240(24) 14,064(29) 4,589(15) 2,304(19) 24,197( 87) 2,293
Wiki. 300 223,514( 60?) 3,530(12) 11,197( 8) 1,470(28) 11,360(67) 27,557(115) 2,286
(ArX.: arXiv.org, Wiki.: Wikipedia, I: Independent, D: Decoration, O: Object, M: Meta-info)
Table 2: Classified tags and obtained sequences for each type of article
Treat- Parsing with Enju parser Parsing with Stanford parserArticle ed tag * # sen- ** Time Avg. # failures * # sen- ** Time Avg. # failures
type classes tences (s) (**/*) (rate) tences (s) (**/*) (rate)
None 159,327 209,783 1.32 4,721 ( 2.96%) 170,999 58,865 0.39 18,621 (10.89%)
PMC O/M 112,285 135,752 1.21 810 ( 0.72%) 126,176 50,741 0.44 11,881 ( 9.42%)
All 126,215 132,250 1.05 699 ( 0.55%) 139,805 63,295 0.49 11,338 ( 8.11%)
None 74,762 108,831 1.46 2,047 ( 2.74%) 75,672 27,970 0.43 10,590 (13.99%)
ArX. O/M 41,265 89,200 2.16 411 ( 1.00%) 48,666 24,630 0.57 5,457 (11.21%)
All 43,208 87,952 2.04 348 ( 0.81%) 50,504 26,360 0.58 5,345 (10.58%)
None 19,571 15,142 0.77 115 ( 0.59%) 17,166 5,047 0.29 1,095 ( 6.38%)
ACL O/M 9,819 9,481 0.97 63 ( 0.64%) 11,182 4,157 0.37 616 ( 5.51%)
All 11,136 8,482 0.76 39 ( 0.35%) 12,402 4,871 0.39 587 ( 4.73%)
None 10,561 14,704 1.39 1,161 (10.99%) 14,883 3,114 0.24 1,651 (11.09%)
Wiki. O/M 5,026 6,743 1.34 67 ( 1.33%) 6,173 2,248 0.38 282 ( 4.57%)
All 6,893 6,058 0.88 61 ( 0.88%) 8,049 2,451 0.31 258 ( 3.21%)
(ArX.: arXiv, Wiki.: Wikipedia, O/M: Object and Meta-info)
Table 3: Impact on parsing performance of plain text sequences extracted using classified tags
starts its second iteration7.
In the case of Independent/Decoration tags, the algorithm splits the regions enclosed by the
tags/removes only the tags from the target text, and recursively processes the obtained text sequence(s)
according to our strategies. In the splitting/removal operation, the algorithm stores in @recovery info,
the locations (offsets) in the obtained text where the tags should be inserted in order to recover the tags
and textual structures after applying the NLP tools. In the case of Object/Meta-info tags, regions en-
closed by these tags are replaced with unique plain text/omitted from the target text, which means that the
inner regions are not unpacked and processed (with relevant information about the replacement/omitting
process also stored in @recovery info). This avoids unnecessary classification tasks for tags that are
utilized only in the regions, and therefore minimizes user effort.
When no further unknown tags are reported, sufficient tag classification has been done to obtain plain
text sequences for input into NLP tools, with the sequences already stored in @plain text sequences.
After applying NLP tools to the obtained sequences, @recovery info is used to integrate the anno-
tated output from the tools into the original XML document by merging the offset information8, and
consequently to recover the structure of the original document.
4 Experiments
We investigated whether the algorithm introduced in Section 3.2 is robustly applicable to different types
of XML documents and whether the obtained text sequences are adequate for input into NLP tools. The
results of this investigation highlight the significance of bridging real-world text and NLP technologies.
4.1 Target Documents
Our algorithm was applied to four types of XML documents: three types of scientific ar-
ticles, examples of which were, respectively, downloaded from PubMed Central (PMC)
(http://www.ncbi.nlm.nih.gov/pmc/tools/ftp/), arXiv.org (http://arxiv.org/) and ACL Anthology
7The user can delay the classification for some tags to later iterations.
8When crossover of tag regions occur, the region in the annotated output is divided into subregions at the crossover point.49
(http://anthology.aclweb.org/)9, and a web page type, examples of which were downloaded from
Wikipedia (http://www.wikipedia.org/). The articles obtained from PMC were originally given in an
XML format, while those from arXiv.org and ACL Anthology were given in XHTML (based on XML),
and those from Wikipedia were given in HTML, with the HTML articles generated via intermediate
XML files. These four types of articles were therefore more or less based on valid XML (or XML-like)
formats. For our experiments, we randomly selected 1,000 PMC articles, randomly selected 300
arXiv.org articles, collected 67 (31 long and 36 short) ACL 2014 conference papers without any
conversion errors (see Footnote 9), and randomly downloaded 300 Wikipedia articles.
Each of the documents contained a variety of textual parts; we decided to apply the NLP tools to the
titles of the articles and sections, abstracts, and body text of the main sections in the scientific articles,
and to the titles of the articles, body text headings, and the actual body text of the Wikipedia articles.
According to these policies, we classified the tags appearing in all articles of each type.
4.2 Efficiency of Tag Classification
Table 2 summarizes the classified tags and obtained sequences for each type of document. The second
to ninth columns give the numbers of utilized articles, tags (in tokens and types) in the documents, each
type of tag actually classified and processed, and obtained text sequences, respectively10. Using simple
regular-expression matching, we found no remaining tagged regions in the obtained sequences. From
this we concluded that our framework at least succeeded in converting XML-tagged text into plain text.
For the PMC articles, we obtained plain text sequences by classifying only a fifth or less of the total
number of tag types, that is, focusing on less than 15% of the total tag occurrences in the documents
(comparing the third and eighth columns). This is because the tags within the regions enclosed by
Object and Meta-info tags were not considered by our procedure. For each of the arXiv.org, ACL and
Wikipedia articles, a similar effect was implied by the fact that the number of classified tags was less
than 20% of the total occurrences of all tags.
4.3 Adequacy of Obtained Sequences for Use in NLP Tools
We randomly selected several articles from each article type, and confirmed that the obtained text se-
quences consisted of valid sentences, which could be directly input into NLP tools and which thoroughly
covered the content of the original articles. Then, to evaluate the impact of this adequacy in a more prac-
tical situation, we input the obtained sequences (listed in Table 2) into two typical parsers, namely, the
Enju parser for deep syntactic/semantic analysis, and the Stanford parser (de Marneffe et al., 2006)11 for
phrase structure and dependency analysis12. Table 3 compares the parsing performance on three types
of plain text sequences obtained by different strategies: simply removing all tags, processing Object
and Meta-info tags using our framework and removing the remaining tags, and processing all the tags
using our framework. For each combination of parser and article type, we give the number of detected
sentences13, the total parsing time, the average parsing time per sentence14, and the number/ratio of
sentences that could not be parsed15.
For all article types, the parsers, especially the Enju parser, succeeded in processing the entire article
with much higher coverage (see the fourth column for each parser) and in much less time (see the third
9The XHTML version of 178 ACL 2014 conference papers were available at ACL Anthology. Each of the XHTML files
was generated by automatic conversion of the original article using LaTeXML (http://dlmf.nist.gov/LaTeXML/).
10For Wikipedia, arXiv.org and ACL articles, since HTML/XHTML tag names represent more abstract textual functions,
the number of different tag types was much smaller than for PMC articles (see ? in the table). To better capture the textual
functions of the tagged regions, we used the combination of the tag name and its selected attributes as a single tag. The number
of classified tags for Wikipedia, arXiv.org and ACL given in the table reflects this decision.
11http://nlp.stanford.edu/software/lex-parser.shtml
12The annotated output from the parsers was integrated into the original XML documents by merging the offset information,
and the structures of the original documents were consequently recovered. The recovered structures were input to xmllint, a
UNIX tool for parsing XML documents, and the tool succeeded in parsing the structures without detecting any error.
13For the Enju parser, we split each text sequence into sentences using GeniaSS [http://www.nactem.ac.uk/y-matsu/geniass/].
14For the Enju parser, the time spent parsing failed sentences was also considered.
15For the Stanford parser, the maximum sentence length was limited to 50 words using the option settings because several
sentences caused parsing failures, even after increasing the memory size from 150 MB to 2 GB, which terminated the whole
process. 50
column for each parser) using the text sequences obtained by treating some (Object and Meta-info)
or all tags with our framework than with those sequences obtained by merely removing the tags. This
is mainly because the text sequences obtained by merely removing the tags contained some embedded
inserted sentences (specified by Independent tags), bare expressions consisting of non natural language
(non-NL) principles (specified by Object tags), and sequences not directly related to the displayed text
(specified by Meta-info tags), which confused the parsers. In particular, treating Object and Meta-info
tags drastically improved parsing performance, since non-NL tokens were excluded from the analysis.
Compared with treating Object/Meta-info tags, treating all tags, that is, additionally treating Inde-
pendent tags and removing the remaining tags as Decoration tags, increased the number of detected
sentences. This is because Independent tags provide solid information for separating text sequences
into shorter sequences and thus prompting the splitting of sequences into shorter sentences, which de-
creased parsing failure by preventing a lack of search space for the Enju parser and by increasing target
(? 50 word) sentences for the Stanford parser. Treating all tags increased the total time for the Stanford
parser since a decrease in failed (> 50 word) sentences directly implied an increase in processing cost,
whereas, for the Enju parser, the total time decreased since the shortened sentences drastically narrowed
the required search space.
4.4 Significance of Bridging Real-world Documents and NLP Technologies
As demonstrated above, the parsers succeeded in processing the entire article with much higher coverage
and in much less time with the text sequences obtained by our framework than with those sequences
obtained by merely removing the tags. Then, what does such thorough and efficient processing bring
about? If our target is shallow analysis of documents which can be achieved by simple approaches such
as counting words, removing tags will suffice; embedded sentences do not affect word count, and non-NL
sequences can be canceled by a large amount of valid sequences in the target documents.
Such shallow approaches, however, cannot satisfy the demands on more detailed or precise analysis
of documents: discourse analysis, translation, grammar extraction, and so on. In order to be sensitive to
subtle signs from the documents, information uttered even in small parts of text cannot be overlooked,
under the condition that sequences other than body text are excluded.
This process of plain text extraction is a well-established procedure in NLP research; in order to
concentrate on precise analysis of natural language phenomena, datasets have been arranged in the format
of plain text sequences, and, using those datasets, plenty of remarkable achievements have been reported
in various NLP tasks while brand-new tasks have been found and tackled.
But what is the ultimate goal of these challenges? Is it to just parse carefully arranged datasets? We all
know this to be just a stepping stone to the real goal: to parse real-world, richly-formatted documents. As
we demonstrated, if supported by a proper framework, conventional NLP tools already have the ability
to process real-world text without significant loss of performance. Adequately bridging target real-world
documents and NLP technologies is thus a crucial task for taking advantage of full benefit brought by
NLP technologies in ubiquitous application of NLP.
5 Conclusion
We proposed a framework for data conversion between XML-tagged text and input/output formats of
NLP tools. In our framework, once each tag utilized in the XML-tagged text has been classified as one
of the four types of textual functions, the conversion is automatically done according to the classification.
In the experiments, we applied our framework to several types of documents, and succeeded in obtaining
plain text sequences from these documents by classifying only a fifth of the total number of tag types
in the documents. We also observed that with the obtained sequences, the target documents were much
more thoroughly and efficiently processed by parsers than with naively tag-removed text. These results
emphasize the significance of bridging real-world documents and NLP technologies.
We are now ready for public release of a tool for conversion of XML documents into plain text se-
quences utilizing our framework. We would like to share further discussion on applying NLP tools to
various real-world documents for increased benefits from NLP.51
Acknowledgements
This research was partially supported by ?Data Centric Science: Research Commons? at the Research
Organization of Information and Systems (ROIS), Japan.
References
Salah A??t-Mokhtar, Veronika Lux, and ?Eva Ba?nik. 2003. Linguistic parsing of lists in structured documents. In
Proceedings of Language Technology and the Semantic Web: 3rd Workshop on NLP and XML (NLPXML-2003),
Budapest, Hungary, April.
?. Andersen, J. Nioche, E.J. Briscoe, and J. Carroll. 2008. The BNC parsed with RASP4UIMA. In Proceedings
of the 6th Language Resources and Evaluation Conference (LREC 2008), pages 865?869, Marrakech, Morocco,
May.
Stephen Clark and James R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?552.
H. Cunningham, V. Tablan, A. Roberts, and K. Bontcheva. 2013. Getting more out of biomedical documents with
GATE?s full lifecycle open source text analytics. PLoS Comput Biol, 9(2).
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the 5th Language Resources and Evaluation Conference
(LREC 2006), pages 449?454, Genoa, Italy, May.
David Ferruci, Adam Lally, Daniel Gruhl, Edward Epstein, Marshall Schor, J. William Murdock, Andy Frenkiel,
Eric W. Brown, Thomas Hampp, Yurdaer Doganata, Christopher Welty, Lisa Amini, Galina Kofman, Lev Koza-
kov, and Yosi Mass. 2006. Towards an interoperability standard for text and multi-modal analytics. Technical
Report RC24122, IBM Research Report.
Yoshinobu Kano, Makoto Miwa, Kevin Cohen, Larry Hunter, Sophia Ananiadou, and Jun?ichi Tsujii. 2011.
U-Compare: a modular NLP workflow construction and evaluation system. IBM Journal of Research and
Development, 55(3):11:1?11:10.
Yoshinobu Kano. 2012. Kachako: a hybrid-cloud unstructured information platform for full automation of service
composition, scalable deployment and evaluation. In Proceedings in the 1st International Workshop on Analyt-
ics Services on the Cloud (ASC), the 10th International Conference on Services Oriented Computing (ICSOC
2012), Shanghai, China, November.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate HPSG parsing. In Proceedings of the 10th International Conference
on Parsing Technologies (IWPT?07), Prague, Czech Republic, June.
52
