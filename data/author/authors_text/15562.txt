Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1477?1488, Dublin, Ireland, August 23-29 2014.
Predicting Interesting Things in Text 
 
 
Michael Gamon 
Microsoft Corp. 
One Microsoft Way 
Redmond, WA 98052 
mgamon@microsoft.com 
Arjun Mukherjee 
Department of Computer 
Science 
University of Houston 
Houston, TX 77004 
ar-
jun4787@gmail.com 
Patrick Pantel 
Microsoft Corp. 
One Microsoft Way 
 Redmond, WA 98052 
ppantel@microsoft.com 
 
  
 
Abstract 
While reading a document, a user may encounter concepts, entities, and topics that she is interested in 
exploring more. We propose models of ?interestingness?, which aim to predict the level of interest a user 
has in the various text spans in a document. We obtain naturally occurring interest signals by observing 
user browsing behavior in clicks from one page to another. We cast the problem of predicting interesting-
ness as a discriminative learning problem over this data. We leverage features from two principal sources: 
textual context features and topic features that assess the semantics of the document transition. We learn 
our topic features without supervision via probabilistic inference over a graphical model that captures the 
latent joint topic space of the documents in the transition. We train and test our models on millions of real-
world transitions between Wikipedia documents as observed from web browser session logs. On the task 
of predicting which spans are of most interest to users, we show significant improvement over various 
baselines and highlight the value of our latent semantic model. 
1 Introduction 
Reading inevitably leads people to discover interesting concepts, entities, and topics. Predicting what 
interests a user while reading a document has important applications ranging from augmenting the doc-
ument with supplementary information, to ad placement, to content recommendation. We define the task 
of predicting interesting things (ITs) as ranking text spans in an unstructured document according to 
whether a user would want to know more about them. This desire to learn more serves as our proxy for 
interestingness. 
There are many types of observable behavior that indicate user interest in a text span. The closest one 
to our problem definition is found in web browsing, where users click from one document to another 
via named anchors. The click process is generally governed by the user?s interest (modulo erroneous 
clicks). As such, the anchor name can be viewed as a text span of interest for that user. Furthermore, the 
frequency with which users, in aggregate, click on an anchor serves as a good proxy for the level of 
interest1. 
What is perceived as interesting is influenced by many factors. The semantics of the document and 
candidate IT are important. For example, we find that when users read an article about a movie, they are 
more likely to browse to an article about an actor or character than to another movie or the director. 
Also, user profile and geo-temporal information are relevant. For example, interests can differ depend-
ing on the cultural and socio-economic background of a user as well as the time of the session (e.g., 
weekday versus weekend, daytime versus late night, etc.). 
1 Other naturally occurring expressions of user interest, albeit less fitting to our problem, are found in web search queries, 
social media engagement, and others. 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
                                                 
1477
Strictly speaking, human interestingness is a psychological and cognitive process (Varela et al., 1991). 
Clicks and long dwell times are salient observed behavioral signals of interestingness that have been 
well accepted in the information retrieval literature (Claypool et al., 2001; Mueller and Lockerd, 2001). 
In this paper, we utilize the observed user?s browsing behavior as a supervision signal for modeling 
interestingness. Specifically, we cast the prediction of ITs as a discriminative learning task. We use a 
regression model to predict the likelihood of an anchor in a Wikipedia article to be clicked, which as we 
have seen above can serve as a proxy for interestingness. Based on an empirical study of a sample of 
our data, we use features in our model from the document context (such as the position of the anchor 
text, frequency of the anchor text in the current paragraph, etc.) as well as semantic features that aim to 
capture the latent topic space of the documents in the browsing transition. These semantic features are 
obtained in an unsupervised manner via a joint topic model of source and target documents in browsing 
transitions. We show empirical evidence that our discriminative model is effective in predicting ITs and 
we demonstrate that the automatically learned latent semantic features contribute significantly to the 
model performance. The main contributions of this paper are: 
? We introduce the task of predicting interesting things as identifying what a user likely wants to 
learn more about while reading a document. 
? We use browsing transitions as a proxy for interestingness and model our task using a discrimina-
tive training approach. 
? We propose a semantic probabilistic model of interestingness, which captures the latent aspects 
that drive a user to be interested in browsing from one document to another. Features derived from 
this semantic model are used in our discriminative learner. 
? We show empirical evidence of the effectiveness of our model on an application scenario. 
2 Related Work 
Salience: A notion that might at first glance be confused with interestingness is that of salience (Paranjpe 
2009; Gamon et al. 2013). Salience can be described as the centrality of a term to the content of a 
document. Put another way, it represents what the document is about. Though salience and interesting-
ness can interact, There are clear differences. For example, in a news article about President Obama?s 
visit to Seattle, Obama is salient, yet the average user would probably not be interested in learning more 
about Obama while reading that article. 
Click Prediction: Click prediction models are used pervasively by search engines. Query based click 
prediction aims at computing the probability that a given document in a search-result page is clicked on 
after a user enters some query (Joachims, 2002; Joachims et al., 2005; Agichtein et al., 2006; Guo et al., 
2009a). Click prediction for online advertising is a core signal for estimating the relevance of an ad to a 
search result page or a document (Chatterjee et al., 2003; Broder et al., 2007; Craswell et al., 2008; 
Graepel et al., 2010). Also related are personalized click models, e.g., (Shen et al., 2012), which use 
user-specific click through rate (CTR). Although these applications and our task share the use of CTR 
as a supervision signal, there is a key difference: Whereas in web search CTR is used as a predictor/fea-
ture at runtime, our task specifically aims at predicting interestingness in the absence of web usage 
features: Our input is completely unstructured and there is no assumption of prior user interaction data. 
Use of probabilistic models: Our semantic model is built over LDA (Blei et al., 2003) and has re-
semblances to Link-LDA models (Erosheva et al., 2004) and Comment-LDA models (Yano et al., 2009). 
However, these are tailored for blogs and associated comment discussions which is very different from 
our source to destination browsing transition logs. Guo et al., (2009b) used probabilistic models for 
discovering entity classes from query logs and in (Lin et al., 2012), latent intents in entity centric search 
were explored. Gao et al. (2011) employ statistical machine translation to connect two types of content, 
learning semantic translation of queries to document titles.  None of the above models, however, are 
directly applicable to the joint topic mappings that are involved in source to destination browsing tran-
sitions which are the focus of our work. 
Predicting Popular Content: Modeling interestingness is also related to predicting popular content 
in the Web and content recommenders (Lerman and Hogg, 2010; Szabo and Huberman, 2010; Bandari 
et al., 2012). In contrast to these tasks, we strive to predict what term a user is likely to be interested in 
when reading content. We do not rely on prior browsing history, since we aim to predict interestingness 
1478
in unstructured text with no interaction history. We show in our experiments that a popularity signal 
alone is not a sufficient predictor for interestingness. 
3 The Interestingness Task 
The process of identifying interesting things (ITs) on a page consists of two parts: (1) generating candi-
date things (e.g., entities, concepts, topics); and (2) scoring and ranking these according to interesting-
ness. In this paper, we fix step 1 and focus our effort on step 2, i.e., the assignment of an interestingness 
score to a candidate. We believe that this scope is appropriate in order to understand the factors that 
enter into what is perceived as interesting by a user. Once we have gained an understanding of the 
interestingness scoring problem, however, there are opportunities in identifying candidates automati-
cally, which we leave for future work. 
In this section we begin by formally defining our task. We then introduce our data set of naturally 
occurring interest signals, followed by an investigation of the factors that influence them. 
3.1 Formal Task Definition 
We define our task as follows. Let ??  be the set of all documents and ?? be the set of all candidate text 
spans in all documents in ?? , generated by some candidate generator. Let ???? ? ??  be the set of candi-
dates in ?? ? ?? . We formally define the interestingness task as learning the function below, where 
??(??, ??) is the interestingness of candidate ?? in 2: 
??:?? ? ?? ? ?  (1) 
3.2 Data Set 
User browsing events on the web (i.e., a user clicking from one document to another) form a naturally 
occurring collection of interestingness signals. That is when a user clicks on an anchor in a document, 
we can postulate that the user is interested in learning more about it, modulo erroneous clicks. 
We collect a large set of many millions of such user browsing events from session logs of a commer-
cial web browser. Specifically, we collect from these logs each occurrence of a user click from one 
Wikipedia page to another during a one month period, from all users in all parts of the world. We refer 
to each such event as a transition. For each transition, our browser log provides metadata, including user 
profile information, geo-location information and session information (e.g., time of click, source/target 
dwell time, etc.) Our data set includes millions of transitions between Wikipedia pages.  
For our task we require: (1) a mechanism for generating candidate things; (2) ample clicks to serve 
as a reliable signal of interestingness for training our models; and (3) accessible content. Our focus on 
Wikipedia satisfies all. First, Wikipedia pages tend to contain many anchors, which can serve as the set 
of candidate things to be ranked. Second, Wikipedia attracts enough traffic to obtain robust browsing 
transition data. Finally, Wikipedia provides full content3 dumps. It is important here to note that our 
choice of Wikipedia as a test bed for our experiments does not restrict the general applicability of our 
approach: We propose a semantic model (Section 4.2) for mining latent features relevant to the phenom-
enon of interestingness which is general and can be applied to generic Web document collections. 
Using uniform sampling, we split our data into three sets: a development set (20%), a training set 
(60%) and a test set (20%). We further subdivide the test set by assigning each transition as belonging 
to the HEAD, TORSO, or TAIL, which we compute using inverse CDF sampling on the test set. We do 
so by assigning the most frequently occurring transitions, accounting for 20% of the (source) traffic, to 
the HEAD. Similarly, the least frequently occurring transitions, accounting for 20% of the (source) traf-
fic, are assigned to the TAIL. The remaining transitions are assigned to the TORSO. This three-way 
split reflects a common practice in the IR community and is based on the observation that web traffic 
frequencies show a very skewed distribution, with a small set of web pages attracting a large amount of 
traffic, and a very long tail of infrequently visited sites. Different regions in that distribution often show 
marked differences in behaviour, and models that are useful in one region are not necessarily as useful 
in another. 
2 We fix ??(??, ??) = 0 for all ?? ? ????. 
3 We utilize the May 3, 2013 English Wikipedia dump from http://dumps.wikimedia.org, consisting of roughly 4.1 million 
articles. 
                                                 
1479
3.3 What Factors Influence Interestingness? 
We manually inspected 200 random transitions from our development set. Below, we summarize our 
observations. 
Only few things on a page are interesting: The average number of anchors on a Wikipedia page is 79. 
Of these, only very few are actually clicked by users. For example, the Wikipedia article on the TV 
series ?The Big Bang Theory? leads to clicks on anchors linking to the pages of the series? actors for 
90% of transitions (while these anchors account for only a small fraction of all unique anchors on that 
page). 
The semantics of source and destination pages is important: We manually determined the entity type 
of the Wikipedia articles in our sample, according to schema.org classes. 49% of all source urls in our 
data sample are of the Creative Work category, reflecting the strong popular interest in movies 
(37%), actors (22%), artists (18%), and television series (8%). The next three most prominent categories 
are Organization (12%), Person (11%) and Place (6%). We observed that transitions are influ-
enced by these categories. For example, when the source article category is Movie, the most frequently 
clicked pages are of category Actor (63%) and Character (13%). For source articles of the 
TVSeries category, Actor destination articles account for 86% of clicks. Actor articles lead to 
clicks on Movie articles (45%) and other Actor articles (26%), whereas Artist articles lead to 
clicks on other Artist articles (29%), Movie articles (17%) and MusicRecording articles (18%). 
The structure of the source page plays a role: It is well known that the position of a link on a page 
influences user click behavior: links that are higher on a page or in a more prominent position tend to 
attract more clicks. We noticed similar trends in our data. 
The user plays a role: We hypothesized that users from different geographic and cultural backgrounds 
might exhibit different interests, or that interests are time-bound (e.g., interests on weekends differ from 
those on week days, daytime from nighttime, etc.) Initial experiments showed small effects of these 
factors, however, a more thorough analysis on a larger sample is necessary, which we leave for future 
work. 
4 Modeling Interestingness 
We cast the problem of learning the interestingness function ?? (see Eq. 1) as a discriminative regression 
learning problem. Below, we first describe this model, and then we introduce our semantic topic model 
which serves to provide semantic features for the discriminative learner. 
4.1 Discriminative Model 
Although our task is to predict ITs from unstructured documents, we can leverage the user interactions 
in our data, described in Section 3.2 as our training signal. 
Given a source document ?? ? ?? , and an anchor in s leading to destination document d, we use the 
aggregate click frequency of this anchor as a proxy for its interestingness, i.e.: 
??(??, ??) = ??(??|??)                       (2) 
where ??(??|??) is the probability of a user clicking on the anchor to ?? when viewing ??3F4. We use ??(??|??) 
as our regression target computed from our training data. 
For our learning algorithm, we use boosted decision trees (Friedman, 1999). We tune our hyperpa-
rameters (i.e., number of iterations, learning rate, minimum instances in leaf nodes, and the maximum 
number of leaves) using cross-validation on the development set. Each transition in our training data is 
represented as a vector of features, where the features fall into three basic families: 
1 Anchor features (Anc): position of the anchor in the document, frequency of the anchor, anchor 
density in the paragraph, and whether the anchor text matches the title of the destination page. 
2 User session features (Ses): city, country, postal code, region, state and timezone of the user, as 
well as day of week, hour, and weekend vs. workday of the occurrence of the transition. 
4 For notational convenience, we use ??(??, ??) even though Eq. 1 defines its second argument as being a candidate text span. 
Here, it is implicit that d consists of both the target document and the anchor text (which serves as the candidate text span). 
                                                 
1480
3 Semantic features: sourced in various experimental configurations from (1) Wikipedia page cate-
gories as assigned by Wikipedia editors (Wiki) or from (2) an unsupervised joint topic transition 
model (JTT) of source and destination pages (described in detail in the next section). 
In some experimental configurations we use Wikipedia page categories as semantic features. We show 
in our experiments (see Section 5) that these are highly discriminative. It is important to note that editor-
labeled category information is available in the Wikipedia domain but not in others. In other words, we 
can use this information to verify that semantics indeed is influential for interestingness, but we should 
design our models to not rely on this. We thus build an unsupervised semantic model of source and 
destination pages, which serves the purpose of providing semantic information without any domain-
specific annotation. 
4.2 The Semantics of Interestingness 
As indicated in Section 3, the semantics of source and destination pages, ?? and ??, influence the likeli-
hood that a user is interested in ?? after viewing ??. In this section we propose an unsupervised method 
for modeling the transition semantics between ?? and ??. As outlined in the previous section, this model 
then serves to generate semantic features for our discriminative model of interestingness. 
Referring to the notations in Table 1, we start by positing a distribution over the joint latent transition 
topics (in the higher level of semantic space), ???? for each transition ??. The corresponding source ??(??) 
and destination ??(??) articles of a given transition ?? are assumed to be admixtures of latent topics that are 
conditioned on the joint topic transition distribution, ????. For ease of reference, we will refer to this model 
as the Joint Transition Topic Model (JTT). The variable names and their descriptions are provided in 
Table 1. Figure 1 shows the plate notation of our model and the generative process: 
 
        
     
                        T
? 
zd zs 
ws wd
              
            K
?  
? 
? 
Ns Nd
 
Figure 1. Generative Process and Plate Notation of JTT. 
1. For each topic ??, draw ???? ~ ??????(??) 
2. For each transition ??: 
a. Draw the joint topic transition distribution, ???? ~ 
??????(??) 
b. For each word token ?? ? {1? ????}: 
i. Draw ????,????  ~ ????????(????) 
ii. Emit ????,????  ~ ????????(????) 
c. For each word token ?? ? {1? ????}: 
i. Draw ????,????  ~ ????????(????) 
ii. Emit ????,????   ~ ????????(????) 
1481
Variable Description Variable Description 
?? A transition ?? ???? , ???? Set of all topics in src, dest pages 
??(??), ??(??) The src and dest pages of ?? ?? ??, ?? ?? Set of all word tokens in src, dest pages 
????~??????(??) Joint src/dest topic distribution ? = {????} 
Set of all latent joint transition topic dis-
tributions 
????, ???? Latent topics of  ??(??), ??(??) ? = {????} Set of all latent topics 
????, ???? Observed word tokens of  ??(??), ??(??) ????,?? Contribution of topic ?? in transition ?? 
???? ~ ??????(??  
Latent topic-word distributions for 
topic ?? 
????,??
?? , ????,??
??  ??th word of transition ?? in ??(??), ??(??) 
??, ??  Dirichlet parameters for ??, ?? ????,???? , ????,????   Latent topic of ??th word of ?? in ??(??), ??(??) 
????, ???? No. of terms in src and dest pgs of ?? ????(??),??
??  No. of words in ??(??) assigned to topic ?? 
?? = {??} Set of all transitions, ?? ????(??),??
??  No. of words in ??(??) assigned to ?? 
?? No. of topics ????,??
??  No. of times word ?? assigned to ?? in ?? ?? 
??  No. of unique terms in the vocab. ????,????  No. of times word ?? assigned to ?? in ?? ?? 
Table 1. List of notations. 
Exact inference for JTT is intractable. Hence, we use Markov Chain Monte Carlo (MCMC) Gibbs sam-
pling. Rao-Blackwellization (Bishop, 2006) is used to reduce sampling variance by collapsing latent 
variables ?? and ??. Owing to space constraints, we omit the full derivation details. The full joint can be 
written succinctly as follows: 
??(????,????,????, ????) = ??
???????(??),[ ]
??  + ????(??),[ ]
?? + ???
??(??)
??
??=1
? = ??
??(????,[ ]
??  + ????,[ ]
?? +??)
??(??)
??
??=1
? (3) 
Omission of a latter index in the count variables, denoted by [ ], corresponds to the row vector span-
ning over the latter index. The corresponding Gibbs conditional distributions for ???? and ???? are detailed 
below, where the subscript ??(??, ??)? denotes the value of the expression excluding the counts of the 
term (??, ??): 
???????,??
?? = ??| ? ? ?
?????(??),??
?? ?
?(??,??)
+ ????(??),??
?? +??
? ??????(??),??
?? ?
?(??,??)
+ ????(??),??
?? +?????
??=1
?
?????,??
?? ?
?(??,??)
+ ????,??
?? +??
? ??????,??
?? ?
?(??,??)
+ ????,??
?? +?????
??=1
  (4) 
???????,??
?? = ??| ? ? ?
????(??),??
??  +?????(??),??
?? ?
?(??,??)
+??
? ?????(??),??
??  +?????(??),??
?? ?
?(??,??)
+?????
??=1
?
????,??
??  + ?????,??
?? ?
?(??,??)
+??
? ?????,??
??  + ?????,??
?? ?
?(??,??)
+?????
??=1
 (5) 
We learn our joint topic model from a random traffic-weighted sample of 10,000 transitions, which are 
randomly sampled from the development set outlined in Section 3.25. The decision to use this sample 
of 10,000 transitions is based on the observation that there were no statistically significant performance 
gains for models trained on more than 10k transitions. The Dirichlet hyperparameters are set to ?? = 
50/?? and ?? = 0.1 according to the values suggested in (Griffiths and Steyvers, 2004). The number of 
topics, ??, is empirically set to 50. We also conducted pilot experiments with other hyperparameter set-
tings, larger transition sets and more topics but we found no substantial difference in the end-to-end 
performance. Although increasing the number of topics and modeling more volume usually results in 
lowering perplexities and better fitting in topic models (Blei et al., 2003), it can also result in redun-
dancy in topics which may not be very useful for downstream applications (Chen et al., 2013). For all 
reported experiments we use the posterior estimates of our joint model learned according to the above 
settings. In our discriminative interestingness model, we use three classes of features from JTT to cap-
ture the latent topic distributions of the source page, the destination page, and the joint topics for that 
transition. These correspond to source topic features (????, labeled as JTTsrc in charts), destination topic 
features (????, labeled as JTTdst), and transition topic features (?, labeled as JTTtrans). Each of these 
three sets comprises 50 features, for a total of 150.? is the distribution over joint src and dst topics that 
5 Note that we use the development set to train our semantic model since it is ultimately used to generate features for our dis-
criminative learner of Section 4. Since the learner is trained using the training set, this strategy avoids overfitting our seman-
tic model to the training set. 
                                                 
1482
appear in a particular transition. ???? and ???? are the actual topic assignments for individual words in src 
and dst. Upon learning the JTT model, for each K topics, we get a probability of that topic appearing in 
the transition, in the src, and in the dst document (by taking the posterior point estimates for latent 
variables  ?, ????, ???? respectively). The GBDT implementation we use for our discriminative model per-
forms binning of these real-valued features over an ensemble of DTs.  
5 Experiments 
We evaluate our interestingness model on the task of proposing ?? anchors on a page that the user will 
find interesting (highlighting task). Recall the interestingness function ?? from Eq. 1. In the highlighting 
task, a user is reading a document ?? ? ??  and is interested in learning more about a set of anchors. Our 
goal in this task is to select ?? anchors that maximize the cumulative degree of interest of the user, i.e.: 
argmax
??????=(??1,?,????|?????????)
? ??(??, ????)???????????
         (6)  
In other words, we consider the ideal selection to consist of the k most interesting anchors according to 
??(??, ??).We compare the interestingness ranking of our models against a gold standard function, ???, com-
puted from our test set. Recall that we use the aggregate click frequency of an anchor as a proxy for its 
interestingness. As such, the gold standard function for the test set is computed as: 
???(??, ??) = ??(??|??)                      (7) 
where ??(??|??) is the probability of a user clicking on the anchor ?? when viewing ??. 
Given a source document ??, we measure the quality of a model?s interestingness ranking against the 
ideal ranking defined above using the standard nDCG metric (Manning et al., 2008). We use the inter-
estingness score of the gold standard as the relevance score. 
Table 2 shows the nDCG results for two baselines and a range of different feature sets. The first high-
level observation is that the task is difficult, given the low baseline results. Since there are many anchors 
on an average page, picking a random set of anchors yields very low nDCG scores. The nDCG numbers 
of our baselines increase as we move from HEAD to TORSO to TAIL, due to the fact that the average 
number of links per page (not unique) decreases in these sets from 170 to 94 to 416. The second baseline 
illustrates that it is not sufficient to simply pick the top n anchors on a page. 
Next, we see that using our set of anchor features (see Section 4.1) in the regression model greatly 
improves performance over the baselines, with the strongest numbers on the HEAD set and decreasing 
effectiveness in TORSO and TAIL. This shows that the distribution of interesting anchors on a page 
differs according to the popularity of the source content, possibly also with the length of the page. Our 
best performing model is the one using anchor features and all three sets of latent semantic features 
(Table 2, row 6; source, destination, and transition topics). 
The biggest improvement is obtained on the HEAD data. This is not surprising given that the topic 
model is trained on a traffic weighted sample of Wikipedia articles and that HEAD pages tend to have 
more content, making the identification of topics more reliable. Regarding the individual contributions 
of the latent semantic features (Table 2, rows 4, 5), destination features alone hurt performance on the 
HEAD set. Latent semantic source features lead to a boost across the board, and the addition of latent 
semantic transition topic features produces the best model, with gains especially pronounced on the 
HEAD data. Figure 2 further shows the performance of our best configuration across ALL, HEAD, 
TORSO, and TAIL. Interestingly, the TAIL exhibits better performance of the model than the TORSO 
(with the exception of nDCG at rank 3 or higher). We hypothesize that this is because the average num-
ber of anchors in a TAIL page is less than half of that in a TORSO page. 
6 Wikipedia editors tend to spend more time on more frequently viewed documents, hence they tend contain more content and 
more anchors. 
                                                 
1483
 nDCG % HEAD TORSO TAIL 
n @1 @2 @5 @10 @1 @2 @5 @10 @1 @2 @5 @10 
Baseline: random 4.07 4.90 6.24 8.10 3.56 4.83 7.66 10.92 6.20 11.74 19.50 25.82 
Baseline: first n an-
chors 
9.99 12.47 17.72 24.33 7.17 9.87 17.06 23.97 9.06 16.66 27.35 34.82 
Anc 21.46 22.50 25.30 29.47 13.85 16.80 22.85 28.20 10.88 19.16 29.33 36.48 
Anc+JTTdst 13.97 16.33 19.69 23.78 11.37 14.17 19.67 24.66 11.62 19.69 29.69 36.35 
Anc+JTTdst+JTTsrc 26.62 30.03 34.82 39.38 17.05 20.82 27.15 32.48 12.27 21.56 31.88 38.85 
Anc+JTT-
dst+JTTsrc+JTTtrans 
34.49 35.21 38.01 41.80 18.32 21.69 28.03 33.22 13.06 21.68 32.13 38.01 
Table 2. Highlighting performance (% nDCG @ n) for different feature sets across HEAD, TORSO, 
and TAIL. Bold indicates statistically significant best systems (with 95% confidence). 
Not shown in these results are the effects of using user session features. We consistently found that 
these features did not improve upon the configurations where anchor and JTT features are used. We do 
not, however, rule out the potential of such features on this task, especially in light of our data analysis 
observations from Section 3.3, which suggest an effect from these factors. We leave a more in-depth 
study of the potential contribution of these types of features for future research. 
We now address the question how our unsupervised latent semantic features perform compared to the 
editor-assigned categories for Wikipedia pages, for two reasons. First, it is reasonable to consider the 
manually assigned Wikipedia categories as a (fine-grained) oracle for topic assignments. Second, out-
side of Wikipedia, we do not have the luxury of manually assigned categories/topics. As illustrated in 
Figure 3, we found that Wikipedia categories outperform the JTT topic features, but the latter can re-
cover about two thirds of the nDCG gain compared to Wikipedia categories. 
Finally, in the HEAD part of the data, we have enough historical clickthrough data that we could 
directly leverage for prediction. We conducted experiments where we used the prior probability ??(??|??) 
obtained from the development data (both smoothed and unsmoothed). Following this strategy we can 
achieve up to 65% nDCG@10 as shown in Figure 4 where the use of prior history (labeled ?History: 
Target | Source Prior?) is compared to our best model and to baselines. As stressed before, in most real-
life applications, this is not a viable option since anchors or user-interaction logs are unavailable. Even 
in web browsing scenarios, the TORSO and TAIL have no or only very sparse histories. Furthermore, 
the information is not available in a ?cold start? scenario involving new and unseen pages. We also 
examined whether the general popularity of a target page is sufficient to predict an anchor?s interesting-
ness, and we found that this signal performs better than the baselines, but significantly worse than our 
models. This series is labeled ?History: Target Prior? in Figure 4. 
 
Figure 2. NDCG comparison across overall performance (ALL) versus HEAD, TORSO, and TAIL 
subsets, on the Highlighting task. 
0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
N
D
CG
Rank
Best Configuration (Anc+JTT)
NDCG @ Rank (ALL vs. HEAD vs. TORSO vs. TAIL)
ALL
HEAD
TORSO
TAIL
1484
  
Figure 3. JTT features versus Wikipedia category features on Highlighting task. 
 
Figure 4. Highlighting task comparison between baselines, best configuration using JTT, and models 
with historical transitions. 
Our highlights task reflects the main goal of our paper, i.e., to predict interestingness in the context of 
any document, whether it be a web page, an email, or a book. A natural extension of our work, especially 
in our experimental setting with Wikipedia transitions, is to predict the next click of a user, i.e., click 
prediction. 
There is a subtle but important difference between the two tasks. Highlights aims to identify a set of 
interesting nuggets for a source document. A user may ultimately click on only a subset of the nuggets, 
and perhaps not in the order of most interest. Our experimental metric, nDCG, reflects this ranking task 
well. Click prediction is an inherently more difficult task, where we focus on predicting exactly the next 
click of a specific user. Unlike in the highlights task, there is no partial credit for retrieving other inter-
esting anchors. Only the exact clicked anchor is considered a correct result. As such, we utilize a differ-
ent metric than nDCG on this task. We measure our model?s performance on the task of click prediction 
using cumulative precision. Given a unique transition event ?(s,a,d) by a particular user at a particular 
time, we present the transition, minus the gold anchor a and destination d, to our models, which in turn 
predict an ordered list of most likely anchors on which the user will click. The cumulative precision at 
k of a model, is 1 if any of the predicted anchors matched a, and 0 otherwise. 
0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
N
D
CG
Rank
Semantic Features: JTT vs. Wikipedia Categories
NDCG @ Rank (ALL)
Baseline: random
Baseline: first
Anc
Anc+Wiki_Src+Wik
i_tar
Anc+JTT_src+JTT_t
ar+JTT_trans
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
N
D
CG
Rank
Interest Models vs. Access to Historical Transitions
NDCG @ Rank (ALL)
Baseline: random
Baseline: first
History: Target Prior
History: Target |
Source Prior
Anc+JTT_src+JTT_tar
+JTT_trans
1485
Table 3 outlines the results on this task and Figure 5 shows the corresponding chart for our best 
configuration. Note that in the click prediction task, the model performs best on the TAIL, followed by 
TORSO and HEAD. This seems to be a reflection of the fact that in this harder task, the total number of 
anchors per page is the most influential factor in model performance. 
Cumulative  
Precision % HEAD TORSO TAIL 
n @1 @2 @5 @10 @1 @2 @5 @10 @1 @2 @5 @10 
Baseline: random 1.07 2.08 5.29 10.55 1.94 3.91 9.71 19.00 5.97 11.66 26.43 44.94 
Baseline: first n an-
chors 
2.68 5.77 16.73 33.78 4.10 8.19 22.86 42.08 8.77 16.57 36.80 58.52 
Anc 8.40 12.55 22.04 34.22 8.70 14.37 27.56 42.68 10.59 19.08 38.27 59.04 
Anc+JTTdst 5.48 9.19 17.77 29.14 6.93 12.07 23.90 38.00 11.23 19.59 38.46 57.87 
Anc+JTTdst+JTTsrc 9.02 15.65 30.05 44.72 10.11 17.42 32.08 47.07 11.95 21.47 40.96 61.24 
Anc+JTT-
dst+JTTsrc+JTTtrans 
11.53 18.43 31.93 45.36 10.86 18.19 32.96 47.66 12.64 21.58 41.27 61.28 
Table 3. Click prediction results for different feature sets across HEAD, TORSO, and TAIL. Bold indicates sta-
tistically significant best systems (with 95% confidence). 
 
Figure 5. Overall performance (ALL) versus HEAD, TORSO, and TAIL subsets on click prediction. 
6 Conclusion and Future Directions 
We presented a notion of an IT on a page that is grounded in observable browsing behavior during 
content consumption. We implemented a model for prediction of interestingness that we trained and 
tested within the domain of Wikipedia. The model design is generic and not tied to our experimental 
choice of the Wikipedia domain and can be applied to other domains. Our model takes advantage of 
semantic features that we derive from a novel joint topic transition model. This semantic model takes 
into account the topic distributions for the source, destination, and transitions from source to destination. 
We demonstrated that the latent semantic features from our topic model contribute significantly to the 
performance of interestingness prediction, to the point where they perform nearly as well as using editor-
assigned Wikipedia categories as features. We also showed that the transition topics improve results 
over just using source and destination semantic features alone. 
A number of future directions immediately suggest themselves. First, for an application that marks 
interesting ITs on an arbitrary page, we would need a detector for IT candidates. A simple first approach 
would be to use a state-of-the-art Named Entity Recognition (NER) system to cover at least a subset of 
potential candidates. This does not solve the problem entirely, since we know that named entities are 
not the only interesting nuggets ? general terms and concepts can also be of interest to a reader. On the 
other hand we do have reason to believe that entities play a very prominent role in web content con-
sumption, based on the frequency with which entities are searched for (see, for example Lin et al. 2012 
and the references cited therein). Using an NER system as a candidate generator would also allow us to 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Cu
m
ul
at
iv
e 
Pr
ec
is
io
n
Rank
Best Configuration (Anc+JTT)
Click Prediction - Cumulative Precision @ Rank
ALL
HEAD
TORSO
TAIL
1486
add another potentially useful feature to our interestingness prediction model: the type of the entity. One 
could also envision jointly modeling interestingness and candidate detection. 
A second point concerns the observation from the previous section on the different regularities that 
seem to be at play according to the popularity and possibly the length of an article. More detailed ex-
periments are needed to tease out this influence and possibly improve the predictive power of the model. 
User session features did not contribute to model performance when used in conjunction with other 
feature families, but closer investigation of these features is warranted for more personalized models of 
interestingness. Finally, a number of options regarding JTT  could be explored further. Being trained on 
a traffic-weighted sample of articles, the topic model predominantly picks up on popular topics. This 
could be remedied by training on a non-weighted sample, or, more promisingly, on a larger non-
weighted sample with a larger ??, i.e. more permissible total topics. 
References 
Agichtein, E., Brill, E., and Dumais, S. 2006. Improving web search ranking by incorporating user behavior infor-
mation. In Proceedings of SIGIR. pp. 19-26. 
Bandari, R., Asur, S., and Huberman, B. A. 2012. The Pulse of News in Social Media: Forecasting Popularity. In 
Proceedings of ICWSM. 
Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent dirichlet allocation. Journal of Machine Learning Re-
search, 3, 993-1022. 
Broder, A., Fontoura, M., Josifovski, V., and Riedel, L. 2007. A semantic approach to contextual advertising. In 
Proceedings of SIGIR. pp. 559-566. 
Bishop, C.M. 2006. Pattern Recognition and Machine Learning. Springer.  
Chatterjee, P., Hoffman, D. L., and Novak, T. P. 2003. Modeling the clickstream: Implications for web-based 
advertising efforts. Marketing Science, 22(4), 520-541. 
Chen, Z., Mukherjee, A., Liu, B., Hsu, M., Castellanos, M. and Ghosh, R. 2013. Leveraging Multi-Domain Prior 
Knowledge in Topic Models. In Proceedings of IJCAI. pp. 2071-2077. 
Claypool, M., Le, P., Wased, M., & Brown, D. 2001a. Implicit interest indicators. In Proceedings of the 6th inter-
national conference on Intelligent user interfaces (pp. 33-40). ACM. 
Craswell, N., Zoeter, O., Taylor, M., and Ramsey, B. 2008. An experimental comparison of click position-bias 
models. In Proceedings of WSDM. pp. 87-94. 
Erosheva, E., Fienberg, S., and Lafferty, J. 2004. Mixed membership models of scientific publications. In Pro-
ceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1). pp. 5220-5227. 
Friedman, J. H. 1999. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189-
1232, 1999. 
Gamon, M., Yano, T., Song, X., Apacible, J. and Pantel, P. 2013. Identifying Salient Entities in Web Pages. In 
Proceedings CIKM. pp. 2375-2380. 
Gao, J., Toutanova, K., and Yih, W. T. 2011. Clickthrough-based latent semantic models for web search. In Pro-
ceedings of SIGIR. pp. 675-684. 
Graepel, T., Candela, J.Q., Borchert, T., and Herbrich, R. 2010. Web-scale Bayesian Click-Through Rate Predic-
tion for Sponsored Search Advertising in Microsoft?s Bing Search Engine. In Proceedings of ICML. pp. 13-20. 
Griffiths, T.L and Steyvers, M. 2004. Finding Scientific Topics. Proceedings of the National Academy of Science, 
101, suppl 1, 5228-5235. 
Guo, F., Liu, C., Kannan, A., Minka, T., Taylor, M., Wang, Y.-M, and Faloutsos, C. 2009a. Click chain model in 
web search. In Proceedings of WWW. pp. 11-20. 
Guo, J., Xu, G., Cheng, X., and Li, H. 2009b. Named entity recognition in query. In Proceedings of SIGIR. pp. 
267-274. 
Joachims, T. 2002. Optimizing search engines using clickthrough data. In Proceedings of KDD. pp. 133-142. 
Joachims, T., Granka, L., Pan, B., Hembrooke, H. and Gay, G. 2005. Accurately interpreting clickthrough data as 
implicit feedback. In Proceedings of SIGIR. pp. 154-161. 
1487
Lerman, K., and Hogg, T. 2010. Using a model of social dynamics to predict popularity of news. In Proceedings 
of WWW. pp. 621-630. 
Lin, T., Pantel, P., Gamon, M., Kannan, A., and Fuxman, A. 2012. Active objects: actions for entity-centric search. 
In Proceedings of WWW. pp. 589-598. 
Manning, C. D., Raghavan, P., and Schutze, H. 2008. Introduction to Information Retrieval. Cambridge University 
Press. 
Mueller, F., & Lockerd, A. 2001. Cheese: tracking mouse movement activity on websites, a tool for user modeling. 
In CHI'01 extended abstracts on Human factors in computing systems (pp. 279-280). ACM. 
Paranjpe, D. 2009. Learning document aboutness from implicit user feedback and document structure. In Proceed-
ings of CIKM. pp. 365-374. 
Shen, S., Hu, B., Chen, W., and Yang, Q. 2012. Personalized click model through collaborative filtering. In Pro-
ceedings of WSDM. pp. 323-333. 
Szabo, G., and Huberman, B. A. 2010. Predicting the popularity of online content. Com-munications of the 
ACM, 53(8), 80-88. 
Varela, F. J., Thompson, E. T., & Rosch, E. 1991. The embodied mind: Cognitive science and human experi-
ence. The MIT Press. 
Yano, T., Cohen, W. W., & Smith, N. A. 2009. Predicting response to political blog posts with topic models. In 
Proceedings of NAACL. pp. 477-485. 
1488
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 207?217,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improving Gender Classification of Blog Authors 
 
Arjun Mukherjee Bing Liu 
Department of Computer Science 
University of Illinois at Chicago 
851 South Morgan Street 
Chicago, IL 60607, USA 
amukherj@cs.uic.edu 
 
Department of Computer Science 
University of Illinois at Chicago 
851 South Morgan Street 
Chicago, IL 60607, USA 
liub@cs.uic.edu 
 
 
 
 
 
 
 
Abstract 
The problem of automatically classifying the 
gender of a blog author has important appli-
cations in many commercial domains. Exist-
ing systems mainly use features such as 
words, word classes, and POS (part-of-
speech) n-grams, for classification learning. 
In this paper, we propose two new techniques 
to improve the current result. The first tech-
nique introduces a new class of features 
which are variable length POS sequence pat-
terns mined from the training data using a se-
quence pattern mining algorithm. The second 
technique is a new feature selection method 
which is based on an ensemble of several fea-
ture selection criteria and approaches. Empir-
ical evaluation using a real-life blog data set 
shows that these two techniques improve the 
classification accuracy of the current state-of-
the-art methods significantly.  
1 Introduction 
Weblogs, commonly known as blogs, refer to on-
line personal diaries which generally contain in-
formal writings. With the rapid growth of blogs, 
their value as an important source of information 
is increasing. A large amount of research work 
has been devoted to blogs in the natural language 
processing (NLP) and other communities. There 
are also many commercial companies that exploit 
information in blogs to provide value-added ser-
vices, e.g., blog search, blog topic tracking, and 
sentiment analysis of people?s opinions on prod-
ucts and services. Gender classification of blog 
authors is one such study, which also has many 
commercial applications. For example, it can help 
the user find what topics or products are most 
talked about by males and females, and what 
products and services are liked or disliked by men 
and women. Knowing this information is crucial 
for market intelligence because the information 
can be exploited in targeted advertising and also 
product development. 
In the past few years, several authors have stu-
died the problem of gender classification in the 
natural language processing and linguistic com-
munities. However, most existing works deal with 
formal writings, e.g., essays of people, the Reuters 
news corpus and the British National Corpus 
(BNC). Blog posts differ from such text in many 
ways. For instance, blog posts are typically short 
and unstructured, and consist of mostly informal 
sentences, which can contain spurious information 
and are full of grammar errors, abbreviations, 
slang words and phrases, and wrong spellings. 
Due to these reasons, gender classification of blog 
posts is a harder problem than gender classifica-
tion of traditional formal text. 
Recent work has also attempted gender classi-
fication of blog authors using features such as 
content words, dictionary based content analysis 
results, POS (part-of-speech) tags and feature se-
lection along with a supervised learning algorithm 
(Schler et al, 2006; Argamon et al, 2007; Yan 
and Yan, 2006). This paper improves these exist-
ing methods by proposing two novel techniques. 
The first technique adds a new class of pattern 
based features to learning, which are not used in 
any existing work. The patterns are frequent se-
quences of POS tags which can capture complex 
stylistic characteristics of male and female au-
thors. We note that these patterns are very differ-
ent from the traditional n-grams because the 
207
patterns are of variable lengths and need to satisfy 
some criteria in order for them to represent signif-
icant regularities. We will discuss them in detail 
in Section 3.5. 
The second technique is a new feature selec-
tion algorithm which uses an ensemble of feature 
selection criteria and methods. It is well known 
that each individual feature selection criterion and 
method can be biased and tends to favor certain 
types of features. A combination of them should 
be able to capture the most useful or discrimina-
tive features. 
Our experimental results based on a real life 
blog data set collected from a large number of 
blog hosting sites show that the two new tech-
niques enable classification algorithms to signifi-
cantly improve the accuracy of the current state-
of-the-art techniques (Argamon et al, 2007; 
Schler et al, 2006; Yan and Yan, 2006). We also 
compare with two publicly available systems, 
Gender Genie (BookBlog, 2007) and Gender 
Guesser (Krawetz, 2006). Both systems imple-
mented variations of the method given in (Arga-
mon et al, 2003). Here, the improvement of our 
techniques is even greater. 
2 Related Work 
There have been several recent papers on gender 
classification of blogs (e.g., Schler et al, 2006, 
Argamon et al, 2007; Yan and Yan, 2006; Now-
son et al, 2005). These systems use func-
tion/content words, POS tag features, word classes 
(Schler et al, 2006), content word classes (Arga-
mon et al, 2007), results of dictionary based con-
tent analysis, POS unigram (Yan and Yan, 2006), 
and personality types (Nowson et al, 2005) to 
capture stylistic behavior of authors? writings for 
classifying gender. (Koppel et al 2002) also used 
POS n-grams together with content words on the 
British National Corpus (BNC). (Houvardas and 
Stamatatos, 2006) even applied character (rather 
than word or tag) n-grams to capture stylistic fea-
tures for authorship classification of news articles 
in Reuters.  
However, these works use only one or a subset 
of the classes of features. None of them uses all 
features for classification learning. Given the 
complexity of blog posts, it makes sense to apply 
all classes of features jointly in order to classify 
genders. Moreover, having many feature classes is 
very useful as they provide features with varied 
granularities and diversities. However, this also 
results in a huge number of features and many of 
them are redundant and may obscure classifica-
tion. Feature selection is thus needed. Following 
the idea, this paper proposes a new ensemble fea-
ture selection method which is capable of extract-
ing good features from different feature classes 
using multiple criteria.  
We also note some less relevant literature. For 
example, (Tannen, 1990) deals with gender differ-
ences in ?conversational style? and in ?formal 
written essays?, and (Gefen and Straub, 1997) 
reports differences in perception of males and fe-
males in the use of emails. 
Our new POS pattern features are related to 
POS n-grams used in (Koppel et al, 2002; Arga-
mon et al, 2007), which considered POS 3-grams, 
2-grams and unigrams as features. As shown in 
(Baayen et. al. 1996), POS n-grams are very ef-
fective in capturing the fine-grained stylistic and 
heavier syntactic information. In this work, we go 
further by finding POS sequence patterns. As dis-
cussed in the introduction, our patterns are entire-
ly different from POS n-grams. First of all, they 
are of variable lengths depending on whatever 
lengths can catch the regularities. They also need 
to satisfy some constraints to ensure that they tru-
ly represent some significant regularity of male or 
female writings. Furthermore, our POS sequence 
patterns can take care of n-grams and capture ad-
ditional sequence regularities. These automatical-
ly mined pattern features are thus more 
discriminating for classification. 
3 Feature Engineering and Mining 
There are different classes of features that have 
been experimented for gender classification, e.g., 
F-measure, stylistic features, gender preferential 
features, factor analysis and word classes (Now-
son et al, 2005; Schler et al, 2006; Corney et al, 
2002; Argamon et al, 2007). We use all these ex-
isting features and also propose a new class of 
features that are POS sequence patterns, which 
replace existing POS n-grams. Also, as mentioned 
before, using all feature classes gives us features 
with varied granularities. Upon extracting all 
these classes of features, a new ensemble feature 
selection (EFS) algorithm is proposed to select a 
subset of good or discriminative features.  
208
Below, we first introduce the existing features, 
and then present the proposed class of new pattern 
based features and how to discover them.  
3.1 F-measure 
The F-measure feature was originally proposed in 
(Heylighen and Dewaele, 2002) and has been used 
in (Nowson et al, 2005) with good results. Note 
that F-measure here is not the F-score or F-
measure used in text classification or information 
retrieval for measuring the classification or re-
trieval effectiveness (or accuracy). 
F-measure explores the notion of implicitness 
of text and is a unitary measure of text?s relative 
contextuality (implicitness), as opposed to its 
formality (explicitness). Contextuality and formal-
ity can be captured by certain parts of speech. A 
lower score of F-measure indicates contextuality, 
marked by greater relative use of pronouns, verbs, 
adverbs, and interjections; a higher score of F-
measure indicates formality, represented by great-
er use of nouns, adjectives, prepositions, and ar-
ticles. F-measure is defined based on the 
frequency of the POS usage in a text (freq.x below 
means the frequency of the part-of-speech x):  
F = 0.5 * [(freq.noun + freq.adj + freq.prep + 
freq.art) ? (freq.pron + freq.verb + 
freq.adv + freq.int) + 100] 
(Heylighen and Dewaele, 2002) applied the F-
measure to a corpus with known author genders 
and found a distinct difference between the sexes. 
Females scored lower preferring a more contex-
tual style while males scored higher preferring a 
more formal style. F-measure values for male and 
female writings reported in (Nowson et al, 2005) 
also demonstrated a similar trend. In our work, we 
also use F-measure as one of the features. 
3.2 Stylistic Features 
These are features which capture people?s writing 
styles. The style of writing is typically captured 
by three types of features: part of speech, words, 
and in the blog context, words such as lol, hmm, 
and smiley that appear with high frequency. In this 
work, we use words and blog words as stylistic 
features. Part of speech features are mined using 
our POS sequence pattern mining algorithm. POS 
n-grams can also be used as features. However, 
since we mine all POS sequence patterns and use 
them as features, most discriminative POS n-
grams are already covered. In Section 5, we will 
also show that POS n-grams do not perform as 
well as our POS sequence patterns. 
3.3 Gender Preferential Features  
Gender preferential features consist of a set of 
signals that has been used in an email gender clas-
sification task (Corney et al, 2002). These fea-
tures come from various studies that have been 
undertaken on the issue of gender and language 
use (Schiffman, 2002). It was suggested by these 
studies and also various other works that women?s 
language makes more frequent use of emotionally 
intensive adverbs and adjectives like ?so?, ?terri-
bly?, ?awfully?, ?dreadfully? and women?s lan-
guage is more punctuated. On the other hand, 
men?s conversational patterns express ?indepen-
dence? (Corney et al, 2002). In brief, the lan-
guage expressed by males is more proactive at 
solving problems while the language used by fe-
males is more reactive to the contribution of oth-
ers - agreeing, understanding and supporting. We 
used the gender preferential features listed in Ta-
ble 1, which indicate adjectives and adverbs based 
on the presence of suffixes and apologies as used 
in (Corney et al, 2002). The feature value as-
signment will be discussed in Section 5. 
f1 words ending with able  
f2 words ending with al  
f3 words ending with ful 
f4 words ending with ible 
f5 words ending with ic  
f6 words ending with ive  
f7 words ending with less  
f8 words ending with ly  
f9 words ending with ous  
f10 sorry words   
Table 1: Gender preferential features 
3.4 Factor Analysis and Word Classes 
Factor or word factor analysis refers to the process 
of finding groups of similar words that tend to 
occur in similar documents. This process is re-
ferred to as meaning extraction in (Chung and 
Pennebaker, 2007). Word lists for twenty factors, 
along with suggested labels/headings (for refer-
ence) were used as features in (Argamon et al, 
2007). Here we list some of those features (word 
209
classes) in Table 2. For the detailed list of such 
word classes, the reader is referred to (Argamon et 
al., 2007). We also used these word classes as fea-
tures in our work. In addition, we added three 
more new word classes implying positive, nega-
tive and emotional connotations and used them as 
features in our experiments. These are listed in 
Table 3. 
Factor Words 
Conversa-
tion 
know, people, think, person, tell, feel, friends, talk, 
new, talking, mean, ask, understand, feelings, care, 
thinking, friend, relationship, realize, question, an-
swer, saying 
Home 
woke, home, sleep, today, eat, tired, wake, watch, 
watched, dinner, ate, bed, day, house, tv, early, bor-
ing, yesterday, watching, sit 
Family 
years, family, mother, children, father, kids, parents, 
old, year, child, son, married, sister, dad, brother, 
moved, age, young, months, three, wife, living, col-
lege, four, high, five, died, six, baby, boy, spend, 
Christmas 
Food / 
Clothes 
food, eating, weight, lunch, water, hair, life, white, 
wearing, color, ice, red, fat, body, black, clothes, 
hot, drink, wear, blue, minutes, shirt, green, coffee, 
total, store, shopping 
Romance forget, forever, remember, gone, true, face, spent, times, love, cry, hurt, wish, loved 
Table 2: Words in factors 
Positive 
absolutely, abundance, ace, active, admirable, adore, 
agree, amazing, appealing, attraction, bargain, beam-
ing, beautiful, best, better, boost, breakthrough, breeze, 
brilliant, brimming, charming, clean, clear, colorful, 
compliment, confidence, cool, courteous, cuddly, daz-
zling, delicious, delightful, dynamic, easy, ecstatic, 
efficient, enhance, enjoy, enormous, excellent, exotic, 
expert, exquisite, flair, free, generous, genius, great, 
graceful, heavenly, ideal, immaculate, impressive, in-
credible, inspire, luxurious, outstanding, royal, speed, 
splendid, spectacular, superb, sweet, sure, supreme, 
terrific, treat, treasure, ultra, unbeatable, ultimate, 
unique, wow, zest 
Negative 
wrong, stupid, bad, evil, dumb, foolish, grotesque, 
harm, fear, horrible, idiot, lame, mean, poor, heinous, 
hideous, deficient, petty, awful, hopeless, fool, risk, 
immoral, risky, spoil, spoiled, malign, vicious, wicked, 
fright, ugly, atrocious, moron, hate, spiteful, meager, 
malicious, lacking 
Emotion 
aggressive, alienated, angry, annoyed, anxious, careful, 
cautious, confused, curious, depressed, determined, 
disappointed, discouraged, disgusted, ecstatic, embar-
rassed, enthusiastic, envious,  excited,  exhausted, 
frightened, frustrated, guilty, happy,  helpless, hopeful, 
hostile, humiliated, hurt, hysterical,  innocent, interest-
ed, jealous, lonely, mischievous,  miserable, optimistic, 
paranoid, peaceful, proud,  puzzled, regretful, relieved, 
sad, satisfied, shocked,  shy, sorry, surprised, suspi-
cious, thoughtful, undecided,  withdrawn 
Table 3: Words implying positive, negative and emo-
tional connotations 
3.5 Proposed POS Sequence Pattern Fea-
tures 
We now present the proposed POS sequence pat-
tern features and the mining algorithm. This re-
sults in a new feature class. A POS sequence 
pattern is a sequence of consecutive POS tags that 
satisfy some constraints (discussed below). We 
used (Tsuruoka and Tsujii, 2005) as our POS tag-
ger. 
As shown in (Baayen et. al., 1996), POS n-
grams are good at capturing the heavy stylistic 
and syntactic information. Instead of using all 
such n-grams, we want to discover all those pat-
terns that represent true regularities, and we also 
want to have flexible lengths (not fixed lengths as 
in n-grams). POS sequence patterns serve these 
purposes. Its mining algorithm mines all such pat-
terns that satisfy the user-specified minimum sup-
port (minsup) and minimum adherence 
(minadherence) thresholds or constraints. These 
thresholds ensure that the mined patterns represent 
significant regularities.   
The main idea of the algorithm is to perform a 
level-wise search for such patterns, which are 
POS sequences with minsup and minadherence. 
The support of a pattern is simply the proportion 
of documents that contain the pattern. If a pattern 
appears too few times, it is probably spurious. A 
sequence is called a frequent sequence if it satis-
fies minsup. The adherence of a pattern is meas-
ured using the symmetrical conditional 
probability (SCP) given in (Silva et al, 1999). 
The SCP of a sequence with two elements |xy| is 
the product of the conditional probability of each 
given the other, 
)()(
),(
)|()|(),(
2
yPxP
yxP
xyPyxPyxSCP ==  
Given a consecutive sequence of POS tags 
|x1?xn|, called a POS sequence of length n, a dis-
persion point defines two subparts of the se-
quence. A sequence of length n contains n-1 
possible dispersion points. The SCP of the se-
quence |x1?xn| given the dispersion point (denoted 
by *) |x1?xn-1*xn| is: 
)()...(
)...(
)),...((
11
2
1
11
nn
n
nn xPxxP
xxP
xxxSCP
?
? =  
The SCP measure can be extended so that all 
possible dispersion points are accounted for. 
210
Hence the fairSCP of the sequence |x1?xn| is giv-
en by: 
??
=
+?
=
1
1
11
2
1
1
)...()...(
1
1
)...(
)...(
n
i
nii
n
n
xxPxxP
n
xxP
xxfairSCP  
fairSCP measures the adherence strength of POS 
tags in a sequence. The higher the fairSCP value, 
the more dominant is the sequence. Our POS se-
quence pattern mining algorithm is given below. 
Input: Corpus D = {d | d is a document containing a 
sequence of POS tags}, Tagset T = {t | t is a POS 
tag}, and the user specified minimum support (min-
sup) and minimum adherence (minadherence). 
Output: All POS sequence patterns (stored in SP) 
mined from D that satisfy minsup and minadhe-
rence.  
Algorithm mine-POS-pats(D, T, minsup, minadhe-
rence) 
1.  C1 ? count each t (? T) in D;  
2.  F1 ? {f | f ? C1 , f .count / n ? minsup};    // n = |D| 
3. SP1 ? F1; 
4.  for (k = 2; k ? MAX-length; k++) 
5. Ck = candidate-gen(Fk-1); 
6. for each document d ? D 
7. for each candidate POS sequence c ? Ck 
8. if (c is contained in d) 
9. c.count++; 
10. endfor 
11.    endfor 
12.   Fk ? {c ? Ck | c.count / n ? minsup}; 
13 SPk ? {f ? Fk | fairSCP(f) ? minadherence} 
14.  endfor 
15.  return SP ? U
k
kSP ; 
Function candidate-gen(Fk-1) 
1.   Ck ? ?;  
2.   for each POS n-gram c ? Fk-1 
3.      for each t ? T 
4.         c?? addsuffix(c, t);  // adds tag t to c as suffix 
5.         add c?  to Ck ; 
6.      endfor 
7.   endfor 
We now briefly explain the mine-POS-pats algo-
rithm. The algorithm is based on level-wise 
search. It generates all POS patterns by making 
multiple passes over data. In the first pass, it 
counts the support of individual POS tags and de-
termines which of them have minsup (line 2). 
Multiple occurrences of a tag in a document are 
counted only once. Those in F1 are called length 1 
frequent sequences. All length 1 sequence patterns 
are stored in SP1. Since adherence is not defined 
for a single element, we have SP1 = F1 (line 3). In 
each subsequent pass k until MAX-length (which 
is the maximum length limit of the mined pat-
terns), there are three steps: 
1.  Using Fk-1 (frequent sequences found in the (k-
1) pass) as a set of seeds, the algorithm applies 
candidate-gen() to generate all possibly fre-
quent POS k-sequences (sequences of length k) 
(line 5). Those infrequent sequences (which are 
not in Fk-1) are discarded as adding more POS 
tags will not make them frequent based on the 
downward closure property in (Agrawal and 
Srikant, 1994). 
2.  D is then scanned to compute the actual sup-
port count of each candidate in Ck (lines 6-11).  
3.  At the end of each scan, it determines which 
candidate sequences have minsup and minad-
herence (lines 12 - 13). We compute Fk and SPk 
separately because adherence does not have the 
downward closure property as the support.   
Finally, the algorithm returns the set of all se-
quence patterns (line 15) that meet the minsup and 
minadherence thresholds.  
The candidate-gen() function generates all pos-
sibly frequent k-sequences by adding each POS 
tag t to c as suffix. c is a k-1-sequence in Fk-1.  
In our experiments, we used MAX-length = 7, 
minsup = 30%, and minadherence = 20% to mine 
all POS sequence patterns. All the mined patterns 
are used as features.   
Finally, it is worthwhile to note that mine-
POS-pat is very similar to the well-known GSP 
algorithm (Srikant and Agrawal, 1996). Likewise, 
it has linear scale up with data size. If needed, one 
can use MapReduce (Dean and Ghemawat, 2004) 
with suitable modifications in mine-POS-pats to 
speed things up by distributing to multiple ma-
chines for large corpora. Moreover, mining is a 
part of preprocessing of the algorithm and its 
complexity does not affect the final prediction, as 
it will be later shown that for model building and 
prediction, standard machine learning methods are 
used. 
4 Ensemble Feature Selection 
Since all classes of features discussed in Section 3 
are useful, we want to employ all of them. This 
results in a huge number of features. Many of 
211
them are redundant and even harmful. Feature 
selection thus becomes important. There are two 
common approaches to feature selection: the filter 
and the wrapper approaches (Blum and Langley, 
1997; Kohavi and John, 1997). In the filter ap-
proach, features are first ranked based on a feature 
selection criterion such as information gain, chi-
square (?2) test, and mutual information. A set of 
top ranked features are selected. On the contrary, 
the wrapper model chooses features and adds to 
the current feature pool based on whether the new 
features improve the classification accuracy.  
Both these approaches have drawbacks. While 
the wrapper approach becomes very time consum-
ing and impractical when the number of features 
is large as each feature is tested by building a new 
classifier. The filter approach often uses only one 
feature selection criterion (e.g., information gain, 
chi-square, or mutual information). Due to the 
bias of each criterion, using only a single one may 
result in missing out some good features which 
can rank high based on another criterion. In this 
work, we developed a novel feature selection me-
thod that uses multiple criteria, and combines both 
the wrapper and the filter approaches. Our method 
is called ensemble feature selection (EFS). 
4.1 EFS Algorithm 
EFS takes the best of both worlds. It first uses a 
number of feature selection criteria to rank the 
features following the filter model. Upon ranking, 
the algorithm generates some candidate feature 
subsets which are used to find the final feature set 
based on classification accuracy using the wrapper 
model. Since our framework generates much few-
er candidate feature subsets than the total number 
of features, using wrapper model with candidate 
feature sets is scalable. Also, since the algorithm 
generates candidate feature sets using multiple 
criteria and all feature classes jointly, it is able to 
capture most of those features which are discrimi-
nating. We now detail our EFS algorithm. 
The algorithm takes as input, a set of n features 
F = {f1, ?, fn}, a set of t feature selection criteria 
? = {?1, ?, ?t}, a set of t thresholds ? = {?1, ?, 
?t} corresponding to the criteria in ?, and a win-
dow w. ?i is the base number of features to be se-
lected for criterion ?i. w is used to vary ?i (thus the 
number of features) to be used by the wrapper 
approach. 
Algorithm: EFS (F, ?, ?, w) 
1. for each ?i ? ? 
2. Rank all features in F based on criterion ?i and 
let ?i denotes the ranked features  
3. endfor 
4. for i = 1 to t 
5. Ci ? ? 
6. for ? = ?i ? w to ? = ?i + w 
7. select first ? features ?i from ?i and add ?i to Ci 
in order 
8. endfor 
9. endfor 
10. // Ci = {?1,  ?, ?2w + 1}, where ?i is a set of fea-
tures 
11. OptCandFeatures ? ?; 
12. Repeat steps 13 ? 18 
13. ? ? ? 
14. for i = 1 to t 
15. select and remove the first feature set ?i ? Ci 
from Ci in order 
16. ? ? ? ? ?i  
17. endfor 
18. add ? to OptCandFeatures  
19. // ? is a set of features comprising of features in 
// feature sets ?i ? Ci in the same position ? i 
20. until Ci = ? ? i 
21. for each ? ? OptCandFeatures 
22. ?.score ? accuracy of 10-fold CV on training 
data on a chosen classifier (learning algo-
rithm) 
23. endfor 
24. return 
score.
maxarg
?
{ ? | ? ? OptCandFeatures} 
We now explain our EFS algorithm. Using a set of 
different feature selection measures, ?, we rank 
all features in our feature pool, F, using the set of 
criteria (lines 1?3). This is similar to the filter ap-
proach. In lines 4?9, we generate feature sets Ci, 1 
? i ? t for each of the t criteria. Each set Ci con-
tains feature subsets, and each subset ?i is the set of 
top ? features in ?i ranked based on criterion ?i in 
lines 1?2. ? varies from ?i ? w to ?i + w where ?i is 
the threshold for criterion ?i and w the window 
size. We vary ? and generate 2w + 1 feature sets 
and add all such feature sets ?i to Ci (in lines 6?8) 
in order. We do so because it is difficult to know 
the optimal threshold ?i for each criterion ?i. It 
should be noted that ?adding in order? ensures the 
ordering of feature sets ?i  as shown in line 10, 
which will be later used to ?select and remove in 
order? in line 15. In lines 11?20 we generate can-
didate feature sets using Ci and add each such 
212
candidate feature set ? to OptCandFeatures. Each 
candidate feature set ? is a collection of top 
ranked features based on multiple criteria. It is 
generated by unioning the features in the first fea-
ture subset ?i, which is then removed from Ci for 
each criterion ?i (lines 14-17). Each candidate fea-
ture set is added to OptCandFeatures in line 18. 
Since each Ci has 2w+1 feature subsets ?i, there 
are a total of 2w+1 candidate feature sets ? in 
OptCandFeatures. Lines 21?23 assign an accuracy 
to each candidate feature set ? ? OptCandFeatures 
by running 10-fold cross validation on the training 
data using a chosen classifier with the features in 
?. Finally, the optimal feature set ? ? OptCand-
Features is returned in line 24. 
An interesting question arising in the EFS al-
gorithm is: How does one select the threshold ?i 
for each criterion ?i and the window size w? Intui-
tively, suppose that for criterion ?i, the optimal 
subset of features is Sopt_i based on some optimal 
threshold ?i. Then the final feature set is a collec-
tion of all features f ? Sopt_i ? i. However, finding 
such optimal feature set Sopt_i or optimal threshold 
?i is a difficult problem. To counter this, we use 
the window w to select various feature subsets 
close to the top ?i features in ?i. Thus, the thre-
shold values ?i and window size w should be ap-
proximated by experiments. In our experiments, 
we used ?i = top 1/20th of the features ranked in ?i 
for ? i and window size w = |F|/100, and got good 
results. Fortunately, as we will see in Section 6.2, 
these parameters are not sensitive at all, and any 
reasonably large size feature set seems to work 
equally well.  
Finally, we are aware that there are some exist-
ing ensemble feature selection methods in the ma-
chine learning literature (Gargant? et al, 2007; Tuv 
et al, 2009). However, they are very different 
from our approach. They mainly use ensemble 
classification methods to help choose good fea-
tures rather than combining different feature se-
lection criteria and integrating different feature 
selection approaches as in our method.   
4.2 Feature Selection Criteria 
The set of feature selection criteria ? = {?1??t} 
used in our work are those commonly used indi-
vidual selection criteria in the filter approach.  
 Let C ={c1, c2, ?, cm} denotes the set of 
classes, and F = {f1, f2, ?, fn} the set of features. 
We list the criteria in ? used in our work below. 
Information Gain (IG): This is perhaps the most 
commonly used criterion, which is based on en-
tropy. The scoring function for information gain 
of a feature f is given by: 
? ??
==
+?=
ff
m
i
iii
m
i
i fcPfcPfPcPcPfIG
, 11
)|(log)|()()(log)()(
Mutual Information (MI): This metric is com-
monly used in statistical language modeling. The 
mutual information MI(f, c) between a class c and 
a feature f is defined as: 
??=
ff cc cPfP
cfP
cfPcfMI
, , )()(
),(
log),(),(  
The scoring function generally used as the crite-
rion is the max among all classes. MI(f) = maxi 
{MI (f, ci)} (which we use). The weighted average 
over all classes can also be applied as the scoring 
function. 
?2 Statistic: The ?2 statistic measures the lack of 
independence between a feature f and class c, and 
can be compared to the ?2 distribution with one 
degree of freedom. We use a 2x2 contingency ta-
ble of a feature f and a class c to introduce ?2 test. 
 c c  
f W X 
f  Y Z 
Table 4: Two-way contingency table of f and c 
In the table, W denotes the number of documents 
in the corpus in which feature f and class c co-
occur, X  the number of documents in which f oc-
curs without c, Y the number of documents in 
which c occurs without f, and Z the number of 
documents in which neither c nor f occurs. Thus, 
N = W + X + Y + Z is the total number of docu-
ments in the corpus. 
?2 test is defined as: 
))()()((
)(
),(
2
2
ZYXWZXYW
YXWZN
cf ++++
?=?  
The scoring function using the ?2 statistic is either 
the weighted average or max over all classes. In 
our experiments, we use the weighted average: 
?2(f) = ?=mi ii cfcP1 2 ),()( ?  
Cross Entropy (CE): This metric is similar to 
mutual information (Mladenic and Grobelnik, 
213
1998): 
?
=
=
m
i
i
i fP
fcP
fcPfPfCE
1 )(
)|(
log)|()()(  
Weight of Evidence for Text (WET): This crite-
rion is based on the average absolute weight of 
evidence (Mladenic and Grobelnik, 1998): 
|
))|(1)((
))(1)(|(
log|)()()(
1 fcPcP
cPfcP
fPcPfWET
ii
ii
m
i
i ?
?=?
=
 
5 Feature Value Assignments 
After selecting features belonging to different 
classes, values are assigned differently to different 
classes of features. There are three common ways 
of feature value assignments: Boolean, TF (Term 
Frequency) and TF-IDF (product of term and in-
verted document frequency). For details of feature 
value assignments, interested readers are referred 
to (Joachims, 1997). While the Boolean scheme 
assigns a 1 to the feature value if the feature is 
present in the document and a 0 otherwise, the TF 
scheme assigns the relative frequency of the num-
ber of times that the feature occurs in the docu-
ment. We did not use TF-IDF as it did not yield 
good results in our preliminary experiments.  
The feature value assignment to different 
classes of features is done as follows: The value 
of F-measure was assigned based on its actual 
value. Stylistic features such words, and blog 
words were assigned values 1 or 0 in the Boolean 
scheme and the relative frequency in the TF 
scheme (we experimented with both schemes). 
Feature values for gender preferential features 
were also assigned in a similar way. Factor and 
word class features were assigned values accord-
ing to the Boolean or TF scheme if any of the 
words belonging to the feature class exists (factor 
or word class appeared in that document). Each 
POS sequence pattern feature was assigned a val-
ue according to the Boolean (or TF) scheme based 
on the appearances of the pattern in the POS 
tagged document. 
6 Experimental Results 
This section evaluates the proposed techniques 
and sees how they affect the classification accura-
cy. We also compare with the existing state-of-
the-art algorithms and systems. For algorithms, 
we compared with three representatives in (Arga-
mon et al, 2007), (Schler et al, 2006) and (Yan 
and Yan, 2006). Since they do not have publicly 
available systems, we implemented them. Each of 
them just uses a subset of the features used in our 
system. Recall our system includes all their fea-
tures and our own POS pattern based features. For 
systems, we compared with two public domain 
systems, Gender Genie (BookBlog, 2007) and 
Gender Guesser (Krawetz, 2006), which imple-
mented variations of the algorithm in (Argamon 
et. al, 2003).  
We used SVM classification, SVM regression, 
and Na?ve Bayes (NB) as learning algorithms. 
Although SVM regression is not designed for 
classification, it can be applied based on the out-
put of positive or negative values. It actually 
worked better than SVM classification for our 
data. For SVM classification and regression, we 
used SVMLight (Joachims, 1999), and for NB we 
used (Borgelt, 2003). In all our experiments, we 
used accuracy as the evaluation measure as the 
two classes (male and female) are roughly ba-
lanced (see the data description below), and both 
classes are equally important.  
6.1 Blog Data Set  
To keep the problem of gender classification of 
informal text as general as possible, we collected 
blog posts from many blog hosting sites and blog 
search engines, e.g., blogger.com, technorati.com, 
etc. The data set consists of 3100 blogs. Each blog 
is labeled with the gender of its author. The gend-
er of the author was determined by visiting the 
profile of the author. Profile pictures or avatars 
associated with the profile were also helpful in 
confirming the gender especially when the gender 
information was not available explicitly. To en-
sure quality of the labels, one group of students 
collected the blogs and did the initial labeling, and 
the other group double-checked the labels by visit-
ing the actual blog pages. Out of 3100 posts, 1588 
(51.2%) were written by men and 1512 (48.8%) 
were written by women. The average post length 
is 250 words for men and 330 words for women.  
6.2 Results  
We used all features from different feature classes 
(Section 3) along with our POS patterns as our 
214
pool of features. We used ? and w values stated in 
Section 4.1 and criteria mentioned in Section 4.2 
for our EFS algorithm. EFS was compared with 
three commonly used feature selection methods 
on SVM classification (denoted by SVM), SVM 
regression (denoted by SVM_R) and the NB clas-
sifier. The results are shown in Table 5. All results 
were obtained through 10-fold cross validation. 
Also, the total number of features selected by 
IG, MI, ?2, and EFS were roughly the same. Thus, 
the improvement in accuracy brought forth by 
EFS was chiefly due to the combination of fea-
tures selected (based on multi-criteria). 
To measure the accuracy improvement of using 
our POS patterns over common POS n-grams, we 
also compared our results with those from POS n-
grams (Koppel et al, 2002). The comparison re-
sults are given in Table 6. Table 6 also includes 
results to show the overall improvement in accu-
racy with our two new techniques. We tested our 
system without any feature selection and without 
using the POS sequence patterns as features. 
The comparison results with existing algo-
rithms and public domain systems using our real-
life blog data set are tabulated in Table 7. 
Also, to see whether feature selection helps and 
how many features are optimal, we varied ? and w 
of the EFS algorithm and plotted the accuracy vs. 
no. of features. These results are shown in Figure 
1. 
Feature  
Selection  
Value 
Assignment NB SVM SVM_R 
IG Boolean 71.32 76.61 78.32 
IG TF 66.01 72.84 74.13 
MI  Boolean 72.01 78.62 79.48 
MI TF 70.86 73.14 74.58 
?2 Boolean 72.90 80.71 81.52 
?2 TF 71.84 73.57 75.24 
EFS Boolean 73.57 86.24 88.56 
EFS TF 72.82 82.05 83.53 
Table 5: Accuracies of SVM, SVM_R and NB with 
different feature selection methods 
 
Settings NB SVM SVM_R
All features 63.01 68.84 70.03 
All features, no POS patterns 60.73 65.17 66.17 
POS 1,2,3-grams + EFS 71.24 82.71 83.86 
POS Patterns + EFS 73.57 86.24 88.56 
Table 6: Accuracies of POS n-grams and POS patterns 
with or without EFS (Boolean value assignment) 
 
System Accuracy (%)
Gender Genie 61.69 
Gender Guesser 63.78 
(Argamon et al, 2007) 77.86 
(Schler et al, 2006) 79.63 
(Yan and Yan, 2006) 68.75 
Our method 88.56 
Table 7: Accuracy comparison with other systems 
50
60
70
80
90
100
25 12
8
21
0
35
0
18
07
64
68
23
97
4
26
02
9
No. of features
A
cc
ur
ac
y
SVM Classification with EFS
SVM Regression with EFS
Na?ve Bayes with EFS
Figure 1: Accuracy vs. no. of features using EFS 
6.3 Observations and Discussions  
Based on the results given in the previous section, 
we make the following observations:  
? SVM regression (SVM_R) performs the best 
(Table 5). SVM classification (SVM) also 
gives good accuracies. NB did not do so well.  
? Table 5 also shows that our EFS feature selec-
tion method brings about 6-10% improvement 
in accuracy over the other feature selection me-
thods based on SVM classification and SVM 
regression. The reason has been explained in 
the introduction section. Paired t-tests showed 
that all the improvements are statistically sig-
nificant at the confidence level of 95%. For 
NB, the benefit is less (3%).   
? Keeping all other parameters constant, Table 5 
also shows that Boolean feature values yielded 
better results than the TF scheme across all 
classifiers and feature selection methods.  
? Row 1 of Table 6 tells us that feature selection 
is very useful. Without feature selection (All 
features), SVM regression only achieves 70% 
accuracy, which is way inferior to the 88.56% 
accuracy obtained using EFS feature selection. 
Row 2 shows that without EFS and without 
POS sequence patterns, the results are even 
worse.  
215
? Keeping all other parameters intact, Table 6 
also demonstrated the effectiveness of our POS 
pattern features over POS n-grams. We have 
discussed the reason in Section 3.2 and 3.5.  
? From Tables 5 and 6, we can infer that the 
overall accuracy improvement using EFS and 
all feature classes described in Section 3 is 
about 15% for SVM classification and regres-
sion and 10% for NB. Also, using POS se-
quence patterns with EFS brings about a 5% 
improvement over POS n-grams (Table 6). The 
improvement is more pronounced for SVM 
based methods than NB. 
? Table 7 summarizes the accuracy improvement 
brought by our proposed techniques over the 
existing state-of-art systems. Our techniques 
have resulted in substantial (around 9%) accu-
racy improvement over the best of the existing 
systems. Note that (Argamon et al, 2007) used 
Logistic Regression with word classes and 
POS unigrams as features. (Schler et al, 2006) 
used Winnow classifier with function words, 
content word classes, and POS features. (Yan 
and Yan, 2006) used Naive Bayes with content 
words and blog-words as features. For all these 
systems, we used their features and ran their 
original classifiers and also the three classifiers 
in this paper and report the best results.  For 
example, for (Argamon et al, 2007), we ran 
Logistic Regression and our three methods. 
SVM based methods always gave slightly bet-
ter results. We could not run Winnow due to 
some technical issues. SVM and SVM_R gave 
comparable results to those given in their orig-
inal papers. These results again show that our 
techniques are useful. All the gains are statisti-
cally significant at the confidence level of 
95%. 
? From Figure 1, we see that when the number of 
features selected is small (<100) the classifica-
tion accuracy is lower than that obtained by us-
ing all features (no feature selection). 
However, the accuracy increases rapidly as the 
number of selected features increases. After 
obtaining the best case accuracy, it roughly 
maintains the accuracy over a long range. The 
accuracies then gradually decrease with the in-
crease in the number of features. This trend is 
consistent with the prior findings in (Mladenic, 
1998; Rogati and Yang, 2002; Forman 2003; 
Riloff et al, 2006; Houvardas and Stamatatos, 
2006).  
It is important to note here that over a long 
range of 2000 to 20000 features, the accuracy 
is high and stable. This means that the thre-
sholds of EFS are easy to set. As long as they 
are in the range, the accuracy will be good. 
Finally, we would like to mention that (Herring 
and Paolillo, 06) has used genre relationships with 
gender classification. Their finding that subgenre 
?diary? contains more ?female? and subgenre ?fil-
ter? having more ?male? stylistic features inde-
pendent of the author gender, may obscure gender 
classification as there are many factors to be con-
sidered. Herring and Paolillo referred only words 
as features which are not as fine grained as our 
POS sequence patterns. We are also aware of oth-
er factors influencing gender classification like 
genre, age and ethnicity. However, much of such 
information is hard to obtain reliably in blogs. 
They definitely warren some future studies. Also, 
EFS being a useful method for feature selection in 
machine learning, it would be useful to perform 
further experiments to investigate how well it per-
forms on a variety of classification datasets. This 
again will be an interesting future work. 
7  Conclusions 
This paper studied the problem of gender classifi-
cation. Although there have been several existing 
papers studying the problem, the current accuracy 
is still far from ideal. In this work, we followed 
the supervised approach and proposed two novel 
techniques to improve the current state-of-the-art. 
In particular, we proposed a new class of features 
which are POS sequence patterns that are able to 
capture complex stylistic regularities of male and 
female authors. Since there are a large number 
features that have been considered, it is important 
to find a subset of features that have positive ef-
fects on the classification task. Here, we proposed 
an ensemble feature selection method which takes 
advantage of many different types of feature se-
lection criteria in feature selection. Experimental 
results based on a real-life blog data set demon-
strated the effectiveness of the proposed tech-
niques. They help achieve significantly higher 
accuracy than the current state-of-the-art tech-
niques and systems.  
216
References 
Agrawal, R. and Srikant, R. 1994. Fast Algorithms for 
Mining Association Rules. VLDB. pp. 487-499. 
Argamon, S., Koppel, M., J Fine, AR Shimoni. 2003. 
Gender, genre, and writing style in formal written 
texts. Text-Interdisciplinary Journal, 2003. 
Argamon, S., Koppel, M., Pennebaker, J. W., Schler, J. 
2007. Mining the Blogosphere: Age, Gender and 
the varieties of self-expression, First Monday, 2007 
- firstmonday.org 
Baayen, H., H van Halteren, F Tweedie. 1996. Outside 
the cave of shadows: Using syntactic annotation to 
enhance authorship attribution, Literary and Lin-
guistic Computing, 11, 1996. 
Blum, A. and Langley, P. 1997. Selection of relevant 
features and examples in machine learning. Artifi-
cial Intelligence, 97(1-2):245-271. 
BookBlog, Gender Genie, Copyright 2003-2007, 
http://www.bookblog.net/gender/genie.html 
Borgelt, C. 2003. Bayes Classifier Induction. 
http://www.borgelt.net/doc/bayes/bayes.html 
Chung, C. K. and Pennebaker, J. W. 2007. Revealing 
people?s thinking in natural language: Using an au-
tomated meaning extraction method in open?ended 
self?descriptions, J. of Research in Personality. 
Corney, M., Vel, O., Anderson, A., Mohay, G. 2002. 
Gender Preferential Text Mining of E-mail Dis-
course. 18th annual Computer Security Applica-
tions Conference (ACSAC), 2002. 
J. Dean and S. Ghemawat. 2004. Mapreduce: Simpli-
fied data processing on large clusters, Operating 
Systems Design and Implementation, 2004. 
Forman, G., 2003. An extensive empirical study of fea-
ture selection metrics for text classification. JMLR, 
3:1289 - 1306 , 2003. 
Gargant?, R. A., Marchiori, T. E., and Kowalczyk, S. 
R. W., 2007. A Genetic Algorithm to Ensemble Fea-
ture Selection. Masters Thesis. Vrije Universiteit, 
Amsterdam. 
Gefen, D., D. W. Straub. 1997. Gender differences in 
the perception and use of e-mail: An extension to 
the technology acceptance model. MIS Quart. 21(4) 
389?400. 
Herring, S. C., & Paolillo, J. C. 2006. Gender and ge-
nre variation in weblogs, Journal of Sociolinguis-
tics, 10 (4), 439-459. 
Heylighen, F., and Dewaele, J. 2002. Variation in the 
contextuality of language: an empirical measure. 
Foundations of Science, 7, 293?340. 
Houvardas, J. and Stamatatos, E. 2006. N-gram Fea-
ture Selection for Authorship Identification, Proc. of 
the 12th Int. Conf. on Artificial Intelligence: Me-
thodology, Systems, Applications, pp. 77-86. 
Joachims, T. 1999. Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and 
A. Smola (ed.), MIT-Press, 1999. 
Joachims, T. 1997. Text categorization with support 
vector machines, Technical report, LS VIII Number 
23, University of Dortmund, 1997 
Kohavi, R. and John, G. 1997. Wrappers for feature 
subset selection. Artificial Intelligence, 97(1-
2):273-324. 
Koppel, M., Argamon, S., Shimoni, A. R.. 2002. Auto-
matically Categorizing Written Text by Author 
Gender. Literary and Linguistic Computing. 
Krawetz, N. 2006. Gender Guesser. Hacker Factor 
Solutions. http://www.hackerfactor.com/ Gender-
Guesser.html 
Mladenic, D. 1998. Feature subset selection in text 
learning. In Proc. of ECML-98, pp. 95?100. 
Mladenic, D. and Grobelnik, D.1998. Feature selection 
for classification based on text hierarchy. Proceed-
ings of the Workshop on Learning from Text and 
the Web, 1998 
Nowson, S., Oberlander J., Gill, A. J., 2005. Gender, 
Genres, and Individual Differences. In Proceedings 
of the 27th annual meeting of the Cognitive Science 
Society (p. 1666?1671). Stresa, Italy. 
Riloff, E., Patwardhan, S., Wiebe, J.. 2006. Feature 
Subsumption for opinion Analysis. EMNLP,  
Rogati, M. and Yang, Y.2002. High performing and 
scalable feature selection for text classification. In 
CIKM, pp. 659-661, 2002. 
Schiffman, H. 2002. Bibliography of Gender and Lan-
guage. http://ccat.sas.upenn.edu/~haroldfs/ pop-
cult/bibliogs/gender/genbib.htm 
Schler, J., Koppel, M., Argamon, S, and Pennebaker J. 
2006. Effects of age and gender on blogging, In 
Proc. of the AAAI Spring Symposium Computa-
tional Approaches to Analyzing Weblogs. 
Silva, J., Dias, F., Guillore, S., Lopes, G. 1999. Using 
LocalMaxs Algortihm for the Extraction of Conti-
guous and Noncontiguous Multiword Lexical Units. 
Springer Lecture Notes in AI 1695, 1999 
Srikant, R. and Agrawal, R. 1996. Mining sequential 
patterns: Generalizations and performance im-
provements, In Proc. 5th Int. Conf. Extending Data-
base Technology (EDBT?96), Avignon, France. 
Tannen, D. (1990). You just don?t understand, New 
York: Ballantine. 
Tsuruoka, Y. and Tsujii, J. 2005. Bidirectional Infe-
rence with the Easiest-First Strategy for Tagging 
Sequence Data, HLT/EMNLP 2005, pp. 467-474. 
Tuv, E., Borisov, A., Runger, G., and Torkkola, K. 
2009. Feature selection with ensembles, artificial 
variables, and redundancy elimination. JMLR, 10. 
Yan, X., Yan, L. 2006. Gender Classification of Web-
log Authors. Computational Approaches to Analyz-
ing Weblogs, AAAI. 
217
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1655?1667,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Domain Knowledge in Aspect Extraction 
 
 
Zhiyuan Chen, Arjun Mukherjee, 
Bing Liu 
Meichun Hsu, Malu Castellanos, 
Riddhiman Ghosh 
University of Illinois at Chicago HP Labs 
Chicago, IL 60607, USA Palo Alto, CA 94304, USA 
{czyuanacm,arjun4787}@gmail.com, 
liub@cs.uic.edu 
{meichun.hsu, malu.castellanos, 
riddhiman.ghosh}@hp.com 
 
 
 
Abstract 
Aspect extraction is one of the key tasks in 
sentiment analysis. In recent years, statistical 
models have been used for the task. However, 
such models without any domain knowledge 
often produce aspects that are not interpreta-
ble in applications. To tackle the issue, some 
knowledge-based topic models have been 
proposed, which allow the user to input some 
prior domain knowledge to generate coherent 
aspects. However, existing knowledge-based 
topic models have several major shortcom-
ings, e.g., little work has been done to incor-
porate the cannot-link type of knowledge or 
to automatically adjust the number of topics 
based on domain knowledge. This paper pro-
poses a more advanced topic model, called 
MC-LDA (LDA with m-set and c-set), to ad-
dress these problems, which is based on an 
Extended generalized P?lya urn (E-GPU) 
model (which is also proposed in this paper).  
Experiments on real-life product reviews 
from a variety of domains show that MC-
LDA outperforms the existing state-of-the-art 
models markedly. 
1 Introduction 
In sentiment analysis and opinion mining, aspect 
extraction aims to extract entity aspects or features 
on which opinions have been expressed (Hu and 
Liu, 2004; Liu, 2012). For example, in a sentence 
?The picture looks great,? the aspect is ?picture.? 
Aspect extraction consists of two sub-tasks: (1) 
extracting all aspect terms (e.g., ?picture?) from 
the corpus, and (2) clustering aspect terms with 
similar meanings (e.g., cluster ?picture? and ?pho-
to? into one aspect category as they mean the 
same in the domain ?Camera?). In this work, we 
adopt the topic modeling approach as it can per-
form both sub-tasks simultaneously (see ? 2). 
Topic models, such as LDA (Blei et al, 2003), 
provide an unsupervised framework for extracting 
latent topics in text documents. Topics are aspect 
categories (or simply aspects) in our context. 
However, in recent years, researchers have found 
that fully unsupervised topic models may not pro-
duce topics that are very coherent for a particular 
application. This is because the objective functions 
of topic models do not always correlate well with 
human judgments and needs (Chang et al, 2009). 
To address the issue, several knowledge-based 
topic models have been proposed. The DF-LDA 
model (Andrzejewski et al, 2009) incorporates 
two forms of prior knowledge, also called two 
types of constraints: must-links and cannot-links. 
A must-link states that two words (or terms) 
should belong to the same topic whereas a cannot-
link indicates that two words should not be in the 
same topic. In (Andrzejewski et al, 2011), more 
general knowledge can be specified using first-
order logic. In (Burns et al, 2012; Jagarlamudi et 
al., 2012; Lu et al, 2011; Mukherjee and Liu, 
2012), seeded models were proposed. They enable 
the user to specify prior knowledge as seed 
words/terms for some topics. Petterson et al (2010) 
also used word similarity as priors for guidance.  
However, none of the existing models is capable 
of incorporating the cannot-link type of knowledge 
except DF-LDA (Andrzejewski et al, 2009). Fur-
thermore, none of the existing models, including 
DF-LDA, is able to automatically adjust the num-
ber of topics based on domain knowledge. The 
domain knowledge, such as cannot-links, may 
change the number of topics. There are two types 
of cannot-links: consistent and inconsistent with 
the domain corpus. For example, in the reviews of 
1655
domain ?Computer?, a topic model may generate 
two topics Battery and Screen that represent two 
different aspects. A cannot-link {battery, screen} 
as the domain knowledge is thus consistent with 
the corpus. However, words Amazon and Price 
may appear in the same topic due to their high co-
occurrences in the Amazon.com review corpus. To 
separate them, a cannot-link {amazon, price} can 
be added as the domain knowledge, which is in-
consistent with the corpus as these two words have 
high co-occurrences in the corpus. In this case, the 
number of topics needs to be increased by 1 since 
the mixed topic has to be separated into two indi-
vidual topics Amazon and Price. Apart from the 
above shortcoming, earlier knowledge-based topic 
models also have some major shortcomings: 
Incapability of handling multiple senses: A 
word typically has multiple meanings or senses. 
For example, light can mean ?of little weight? or 
?something that makes things visible.? DF-LDA 
cannot handle multiple senses because its defini-
tion of must-link is transitive. That is, if A and B 
form a must-link, and B and C form a must-link, it 
implies a must-link between A and C, indicating A, 
B, and C should be in the same topic. This case 
also applies to the models in (Andrzejewski et al, 
2011), (Petterson et al, 2010), and (Mukherjee and 
Liu, 2012). Although the model in (Jagarlamudi et 
al., 2012) allows multiple senses, it requires that 
each topic has at most one set of seed words (seed 
set), which is restrictive as the amount of 
knowledge should not be limited. 
Sensitivity to the adverse effect of knowledge: 
When using must-links or seeds, existing models 
basically try to ensure that the words in a must-
link or a seed set have similar probabilities under a 
topic. This causes a problem: if a must-link com-
prises of a frequent word and an infrequent word, 
due to the redistribution of probability mass, the 
probability of the frequent word will decrease 
while the probability of the infrequent word will 
increase. This can harm the final topics because 
the attenuation of the frequent (often domain im-
portant) words can result in some irrelevant words 
being ranked higher (with higher probabilities). 
To address the above shortcomings, we define 
m-set (for must-set) as a set of words that should 
belong to the same topic and c-set (cannot-set) as a 
set of words that should not be in the same topic. 
They are similar to must-link and cannot-link but 
m-sets do not enforce transitivity. Transitivity is 
the main cause of the inability to handle multiple 
senses. Our m-sets and c-sets are also more con-
cise providing knowledge in the context of a set. 
As in (Andrzejewski et al, 2009), we assume that 
there is no conflict between m-sets and c-sets, i.e., 
if ?1  is a cannot-word of ?2  (i.e., shares a c-set 
with ?2), any word that shares an m-set with ?1 is 
also a cannot-word of ?2. Note that knowledge as 
m-sets has also been used in (Chen et al, 2013a) 
and (Chen et al, 2013b). 
We then propose a new topic model, called MC-
LDA (LDA with m-set and c-set), which is not on-
ly able to deal with c-sets and automatically adjust 
the number of topics, but also deal with the multi-
ple senses and adverse effect of knowledge prob-
lems at the same time. For the issue of multiple 
senses, a new latent variable ? is added to LDA to 
distinguish multiple senses (? 3). Then, we employ 
the generalized P?lya urn (GPU) model 
(Mahmoud, 2008) to address the issue of adverse 
effect of knowledge (? 4). Deviating from the 
standard topic modeling approaches, we propose 
the Extended generalized P?lya urn (E-GPU) 
model (? 5). E-GPU extends the GPU model to 
enable multi-urn interactions. This is necessary for 
handling c-sets and for adjusting the number of 
topics. E-GPU is the heart of MC-LDA. Due to the 
extension, a new inference mechanism is designed 
for MC-LDA (? 6). Note that E-GPU is generic 
and can be used in any appropriate application. 
In summary, this paper makes the following 
three contributions: 
1. It proposed a new knowledge-based topic mod-
el called MC-LDA, which is able to use both 
m-sets and c-sets, as well as automatically ad-
just the number of topics based on domain 
knowledge. At the same time, it can deal with 
some other major shortcomings of early exist-
ing models. To our knowledge, none of the ex-
isting knowledge-based models is as compre-
hensive as MC-LDA in terms of capabilities. 
2. It proposed the E-GPU model to enable multi-
urn interactions, which enables c-sets to be nat-
urally integrated into a topic model. To the best 
of our knowledge, E-GPU has not been pro-
posed and used before.  
3. A comprehensive evaluation has been conduct-
ed to compare MC-LDA with several state-of-
the-art models. Experimental results based on 
both qualitative and quantitative measures 
demonstrate the superiority of MC-LDA. 
1656
2  Related Work 
Sentiment analysis has been studied extensively in 
recent years (Hu and Liu, 2004; Pang and Lee, 
2008; Wiebe and Riloff, 2005; Wiebe et al, 2004). 
According to (Liu, 2012), there are three main ap-
proaches to aspect extraction: 1) Using word fre-
quency and syntactic dependency of aspects and 
sentiment words for extraction (e.g., Blair-
goldensohn et al, 2008; Hu and Liu, 2004; Ku et 
al., 2006; Popescu and Etzioni, 2005; Qiu et al, 
2011; Somasundaran and Wiebe, 2009; Wu et al, 
2009; Yu et al, 2011; Zhang and Liu, 2011; 
Zhuang et al, 2006); 2) Using supervised se-
quence labeling/classification (e.g., Choi and 
Cardie, 2010; Jakob and Gurevych, 2010; 
Kobayashi et al, 2007; Li et al, 2010); 3) Topic 
models (Branavan et al, 2008; Brody and Elhadad, 
2010; Fang and Huang, 2012; Jo and Oh, 2011; 
Kim et al, 2013; Lazaridou et al, 2013; Li et al, 
2011; Lin and He, 2009; Lu et al, 2009, 2012, 
2011; Lu and Zhai, 2008; Mei et al, 2007; 
Moghaddam and Ester, 2011; Mukherjee and Liu, 
2012; Sauper et al, 2011; Titov and McDonald, 
2008; Wang et al, 2010, 2011; Zhao et al, 2010). 
Other approaches include shallow semantic pars-
ing (Li et al, 2012b), bootstrapping (Xia et al, 
2009), Non-English techniques (Abu-Jbara et al, 
2013; Zhou et al, 2012), graph-based representa-
tion (Wu et al, 2011), convolution kernels 
(Wiegand and Klakow, 2010) and domain adap-
tion (Li et al, 2012). Stoyanov and Cardie (2011), 
Wang and Liu (2011), and Meng et al (2012) 
studied opinion summarization outside the reviews. 
Some other works related with sentiment analysis 
include (Agarwal and Sabharwal, 2012; Kennedy 
and Inkpen, 2006; Kim et al, 2009; Mohammad et 
al., 2009). 
In this work, we focus on topic models owing to 
their advantage of performing both aspect extrac-
tion and clustering simultaneously. All other ap-
proaches only perform extraction. Although there 
are several related works on clustering aspect 
terms (e.g., Carenini et al, 2005; Guo et al, 2009; 
Zhai et al, 2011), they all assume that the aspect 
terms have been extracted beforehand. We also 
notice that some aspect extraction models in sen-
timent analysis separately discover aspect words 
and aspect specific sentiment words (e.g., Sauper 
and Barzilay, 2013; Zhao et al, 2010). Our pro-
posed model does not separate them as most sen-
timent words also imply aspects and most adjec-
tives modify specific attributes of objects. For ex-
ample, sentiment words expensive and beautiful 
imply aspects price and appearance respectively. 
Regarding the knowledge-based models, be-
sides those discussed in ? 1, the model (Hu et al, 
2011) enables the user to provide guidance interac-
tively. Blei and McAuliffe (2007) and Ramage et 
al. (2009) used document labels in supervised set-
ting. In (Chen et al, 2013a), we proposed MDK-
LDA to leverage multi-domain knowledge, which 
serves as the basic mechanism to exploit m-sets in 
MC-LDA. In (Chen et al, 2013b), we proposed a 
framework (called GK-LDA) to explicitly deal 
with the wrong knowledge when exploring the 
lexical semantic relations as the general (domain 
independent) knowledge in topic models. But 
these models above did not consider the 
knowledge in the form of c-sets (or cannot-links). 
The generalized P?lya urn (GPU) model 
(Mahmoud, 2008) was first introduced in LDA by 
Mimno et al (2011). However, Mimno et al (2011) 
did not use domain knowledge. Our results in ? 7 
show that using domain knowledge can signifi-
cantly improve aspect extraction. The GPU model 
was also employed in topic models in our work of 
(Chen et al, 2013a, 2013b). In this paper, we pro-
pose the Extended GPU (E-GPU) model. The E-
GPU model is more powerful in handling complex 
situations in dealing with c-sets. 
3 Dealing with M-sets and Multiple Senses 
Since the proposed MC-LDA model is a major 
extension to our earlier work in (Chen et al, 
2013a), which can deal with m-sets, we include 
this earlier work here as the background.     
To incorporate m-sets and deal with multiple 
senses of a word, the MDK-LDA(b) model was 
proposed in (Chen et al, 2013a), which adds a 
new latent variable ? into LDA. The rationale here 
is that this new latent variable ? guides the model 
to choose the right sense represented by an m-set. 
The generative process of MDK-LDA(b) is (the 
notations are explained in Table 1): 
                     ?  ~ ?????????(?) 
                     ??|??  ~ ???????????(??) 
                     ? ~ ?????????(?) 
                     ??|??,? ~ ???????????????? 
                     ? ~ ?????????(?) 
                     ??|?? , ??,? ~ ???????????????,??? 
1657
The corresponding plate is shown in Figure 1. Un-
der MDK-LDA(b), the probability of word ? giv-
en topic ?, i.e., ??(?), is given by: 
 ??(?) = ? ??(?) ? ??,?(?)
?
?=1    (1) 
where ??(?)  denotes the probability of m-set ? 
occurring under topic ? and ??,?(?) is the proba-
bility of word ? appearing in m-set ? under topic ?. 
According to (Chen et al, 2013a), the condi-
tional probability of Gibbs sampler for MDK-
LDA(b) is given by (see notations in Table 1): 
    ???? = ?, ?? = ? ???? , ??? ,?,?,?, ?? ? 
??,?
?? + ?
? ???,??
?? + ?????=1
?
??,?
?? + ?
? ???,??
?? + ?????=1
?
??,?,??
?? + ??
? ???,?,??
?? + ???
?
??=1
 (2) 
The superscript ??  denotes the counts excluding 
the current assignments (?? and ??) for word ??. 
4 Handling Adverse Effect of Knowledge 
4.1 Generalized P?lya urn (GPU) Model 
The P?lya urn model involves an urn containing 
balls of different colors. At discrete time intervals, 
balls are added or removed from the urn according 
to their color distributions. 
In the simple P?lya urn (SPU) model, a ball is 
first drawn randomly from the urn and its color is 
recorded, then that ball is put back along with a 
new ball of the same color. This selection process 
is repeated and the contents of the urn change over 
time, with a self-reinforcing property sometimes 
expressed as ?the rich get richer.? SPU is actually 
exhibited in the Gibbs sampling for LDA.  
The generalized P?lya urn (GPU) model differs 
from the SPU model in the replacement scheme 
during sampling. Specifically, when a ball is ran-
domly drawn, certain numbers of additional balls 
of each color are returned to the urn, rather than 
just two balls of the same color as in SPU. 
4.2 Promoting M-sets using GPU 
To deal with the issue of sensitivity to the adverse 
effect of knowledge, MDK-LDA(b) is extended to 
MDK-LDA which employs the generalized P?lya 
urn (GPU) sampling scheme. 
As discussed in ? 1, due to the problem of the 
adverse effect of knowledge, important words may 
suffer from the presence of rare words in the same 
m-set. This problem can be dealt with the very 
sampling scheme of the GPU model (Chen et al, 
2013a). Specifically, by adding additional ??,??,? 
balls of color ? into ??
?  while keeping the drawn 
ball, we increase the proportion (probability) of 
seeing the m-set ? under topic ? and thus promote 
m-set ? as a whole. Consequently, each word in ? 
is more likely to be emitted. We define ??,??,? as: 
 ??,??,? = ?
1           ? = ??                                   
?           ? ? ?,?? ? ?,? ? ??        
 0           otherwise                             
 (3) 
The corresponding Gibbs sampler for MDK-LDA 
will be introduced in ? 6.  
Hyperparameters 
?, ?, ? Dirichlet priors for ?,  ?,  ? 
Latent & Visible Variables 
? Topic (Aspect) 
? M-set 
? Word 
? Document-Topic distribution 
?? Topic distribution of document ? 
? Topic-M-set distribution 
?? M-set distribution of topic ? 
? Topic-M-set-Word distribution 
??,? Word distribution of topic ?, m-set ? 
Cardinalities 
? Number of documents 
?? Number of words in document ? 
? Number of topics 
? Number of m-sets 
? The vocabulary size 
Sampling & Count Notations 
?? Topic assignment for word ??  
??  M-set assignment for word ??  
??? Topic assignments for all words except ??  
??? M-set assignments for all words except ??  
??,?  
Number of times that topic ? is assigned 
to word tokens in document ? 
??,? 
Number of times that m-set ? occurs un-
der topic ? 
??,?,? 
Number of times that word ? appears in 
m-set ? under topic ? 
Table 1. Meanings of symbols. 
 
 
 
 
 
 
 
 
 
Figure 1. Plate notation for MDK-LDA(b) and MC-LDA. 
T?S Nm 
M 
? 
T 
 ?  z 
 w 
 ? 
? 
 s ? 
 ? 
1658
5 Incorporating C-sets 
5.1 Extended Generalized P?lya urn Model 
To handle the complex situation resulted from in-
corporating c-sets, we propose an Extended gener-
alized P?lya urn (E-GPU) model. Instead of 
involving only one urn as in SPU and GPU, E-
GPU model considers a set of urns in the sampling 
process. The E-GPU model allows a ball to be 
transferred from one urn to another, enabling mul-
ti-urn interactions. Thus, during sampling, the 
populations of several urns will evolve even if on-
ly one ball is drawn from one urn. This capability 
makes the E-GPU model more powerful as it 
models relationships among multiple urns. 
We define three sets of urns which will be used 
in the new sampling scheme in the proposed MC-
LDA model. The first set of urns is the topic urns 
???{1??}
? , where each topic urn contains ? colors 
(topics) and each ball inside has a color ? ?
 {1 ??}. It corresponds to the document-topic dis-
tribution ? in Table 1. The second set of urns (m-
set urn ??? {1??}
? )  corresponds to the topic-m-set 
distribution ? , with balls of colors (m-sets) 
? ?  {1 ? ?}  in each m-set urn. The third set of 
urns is the word urns ??,?
? ,where ? ?  {1 ??} and 
? ?  {1 ? ?} . Each ball inside a word urn has a 
color (word) ? ?  {1 ??}. The distribution ? can 
be reflected in this set of urns. 
5.2 Handling C-sets using E-GPU 
As MDK-LDA can only use m-sets but not c-sets, 
we now extend MDK-LDA to the MC-LDA model 
in order to exploit c-sets. As pointed out in ? 1, c-
sets may be inconsistent with the corpus domain, 
which makes them considerably harder to deal 
with. To tackle the issue, we utilize the proposed 
E-GPU model and incorporate c-sets handling in-
side the E-GPU sampling scheme, which is also 
designed to enable automated adjustment of the 
number of topics based on domain knowledge. 
Based on the definition of c-set, each pair of 
words in a c-set cannot both have large probabili-
ties under the same topic. As the E-GPU model 
allows multi-urn interactions, when sampling a 
ball represents word ? from a word urn ??,?
? , we 
want to transfer the balls representing cannot-
words of ? (sharing a c-set with ?) to other urns 
(see Step 3 a below). That is, decrease the proba-
bilities of those cannot-words under this topic 
while increasing their corresponding probabilities 
under some other topics. In order to correctly 
transfer a ball that represents word ?, it should be 
transferred to an urn which has a higher proportion 
of ? and its related words (i.e., words sharing m-
sets with ?). That is, we randomly sample an urn 
that has a higher proportion of any m-set of ? to 
transfer ? to (Step 3 b below). However, the situa-
tion becomes more involved when a c-set is not 
consistent with the corpus. For example, aspects 
price and amazon may be mixed under one topic 
(say ?) in LDA. The user may want to separate 
them by providing a c-set {price, amazon}. In this 
case, according to LDA, word price has no topic 
with a higher proportion of it (and its related 
words) than topic ?. To transfer it, we need to in-
crement the number of topics by 1 and then trans-
fer the word to this new topic urn (step 3 c below). 
Based on these ideas, we propose the E-GPU sam-
pling scheme for the MC-LDA model below: 
1. Sample a topic ? from ??
? , an m-set ? from ??
?, and 
a word ?  from ??,?
?  sequentially, where ?  is the 
?th document. 
2. Record ?, ? and ?, put back two balls of color ? in-
to urn ??
? , one ball of color ? into urn ??
?, and two 
balls of color ? into urn ??,?
? . Given the matrix ? 
(in Equation 3), for each word ?? ? ?, we put back 
??,?? ,? number of balls of color ? into urn ??
?. 
3.  For each word ?? that shares a c-set with ?: 
a) Sample an m-set ??  from ??
?  which satisfies 
?? ? ?? . Draw a ball ? of color ?? (to be trans-
ferred) from ??,??
?  and remove it from ??,??
? . The 
document of ball ? is denoted by ??. If no ball 
of color ?? can be drawn (i.e., there is no ball 
of color ?? in ??,??
? ), skip steps b) to d). 
b) Produce an urn set {???,??
? } such that each urn in 
it satisfies the following conditions: 
i)   ?? ? ?, ?? ? ?
? 
ii) The proportion of balls of color ?? in ???
?  is    
higher than that of balls of color ??  in ??
?. 
c) If {???,??
? } is not empty, randomly select one urn 
???,??
?  from it. If {???,??
? } is empty, set ? = ? +
1, ?? = ?, draw an m-set ?? from ???
?  which sat-
isfies ?? ? ?
?. Record ?? for step d). 
d) Put the ball ? drawn from Step a) into ???,??
? , as 
well as a ball of color ?? into ???
?  and a ball of 
color ?? into ???
? . 
Note that the E-GPU model cannot be reflected in 
the graphical model in Figure 1 as it is essentially 
1659
sampling scheme, and hence MC-LDA shares the 
same plate as MDK-LDA(b). 
6 Collapsed Gibbs Sampling 
We now describe the collapsed Gibbs sampler 
(Griffiths and Steyvers, 2004) with the detailed 
conditional distributions and algorithms for MC-
LDA. Inference of ? and ? can be computationally 
expensive due to the non-exchangeability of words 
under the E-GPU models. We take the approach of 
(Mimno et al, 2011) which approximates the true 
Gibbs sampling distribution by treating each word 
as if it were the last. 
For each word ?? , we perform hierarchical 
sampling consisting of the following three steps 
(the detailed algorithms are given in Figures 2 and 
3): 
Step 1 (Lines 1-11 in Figure 2): We jointly 
sample a topic ??  and an m-set ??  (containing ??) 
for ?? , which gives us a blocked Gibbs sampler 
(Ishwaran and James, 2001), with the conditional 
probability given by: 
     ?(?? = ?, ?? = ?|?
?? , ??? ,?,?,?, ?,?) ?
     
??,?
?? +?
? ??
?,??
?? +???
??=1
?
? ? ??,??,?? ???,?,??
???
??=1
?
??=1 +?
? ?? ? ???,??,?????,??,??
???
??=1
?
??=1
+???
??=1
?
     
??,?,??
?? +??
? ??
?,?,??
?? +???
?
??=1
  
 (4) 
This step is the same as the Gibbs sampling for the 
MDK-LDA model. 
Step 2 (lines 1-5 in Figure 3): For every cannot-
word (say ??) of ??, randomly pick an urn ???,??
?  
from the urn set {???,??
? } where ??? ? ??. If there ex-
ists at least one ball of color ?? in urn ???,??
? , we 
sample one ball (say ?? ) of color ??  from urn 
???,??
? , based on the following conditional distribu-
tion: 
     ?(? = ??|?, ?,?,?,?, ?,?) ?
???,?+?
? ????,??
+???
??=1
  (5) 
where ??  denotes the document of the ball ??  of 
color ??. 
Step 3 (lines 6-12 in Figure 3): For each drawn 
ball ? from Step 2, resample a topic ? and an m-set 
?  (containing ?? ) based on the following condi-
tional distribution: 
?(?? = ?, ?? = ?|?
?? , ??? ,?,?,?, ?,?, ? = ??)
? ?
?0,???
?????
(????
??)?
????(??)? ?
??,?
?? + ?
? ???,??
?? + ?????=1
?
? ? ??,??,?? ? ??,?,??
???
??=1
?
??=1 + ?
? ?? ? ???,??,?? ? ??,??,??
???
??=1
?
??=1 + ??
?
??=1
?
??,?,??
?? + ??
? ???,?,??
?? + ???
?
??=1
 
(6) 
where ??  (same as ??  in Figure 3) and ??  are the 
original topic and m-set assignments. The super-
script ?? denotes the counts excluding the original 
Algorithm 1. GibbsSampling(?, ?? , ?, ?, ?) 
Input: Document ?, Word ?? , Matrix ?, 
           Transfer cannot-word flag ?, 
           A set of valid topics ? to be assigned to ??  
1:   ??,?? ? ??,?? ? 1; 
2:   for each word ?? in ?? do 
3:       ???,?? ? ???,?? ? ???,?? ,??; 
4:   end for 
5:   ???,??,?? ? ???,??,?? ? 1; 
6:   Jointly sample ?? ? ? and ?? ? ??  using Equation 2; 
7:   ??,?? ? ??,?? + 1; 
8:   for each word ?? in ?? do 
9:       ???,?? ? ???,?? + ???,?? ,??; 
10: end for 
11: ???,??,?? ? ???,??,?? + 1; 
12: if ? is true then 
13:     TransferCannotWords(?? , ??); 
14: end if 
Figure 2. Gibbs sampling for MC-LDA. 
Algorithm 2.TransferCannotWords(?? , ??) 
Input: Word ?? , Topic ??, 
1:   for each cannot-word ?? of ??  do 
2:       Randomly select an m-set ??  from all m-sets of ??; 
3:       Build a set ? containing all the instances of ?? 
from the corpus with topic and m-set assign-
ments being ?? and ??; 
4:       if ? is not empty then 
5:            Draw an instance of ?? from ? (denoting the 
document of this instance by ??) using 
Equation 5; 
6:            Generate a topic set ?? that each topic ?? inside 
satisfies ????????(???(?? )) > ???(??). 
7:            if ?? is not empty then 
8:                GibbsSampling(??, ??, ?, false, ??); 
9:            else 
10:              ????? = ? + 1; // ? is #Topics. 
11:              GibbsSampling(??, ??, ?, false, {?????}); 
12:          end if 
13:     end if 
14: end for 
Figure 3. Transfer cannot-words in Gibbs sampling. 
1660
assignments. ?()  is an indicator function, which 
restricts the ball to be transferred only to an urn 
that contains a higher proportion of its m-set. 
When no topic ? can be successfully sampled and 
the current sweep (iteration) of Gibbs sampling 
has the same number of topic (?) as the previous 
sweep, we increment ? by 1. And then assign ? to 
?? . The counts and parameters are also updated 
accordingly. 
7 Experiments 
We now evaluate the proposed MC-LDA model 
and compare it with state-of-the-art existing mod-
els. Two unsupervised baseline models that we 
compare with are:  
? LDA: LDA is the basic unsupervised topic 
model (Blei et al, 2003). 
? LDA-GPU: LDA with GPU (Mimno et al, 
2011). Specifically, LDA-GPU applies GPU in 
LDA using co-document frequency.  
As for knowledge-based models, we focus on 
comparing with DF-LDA model (Andrzejewski et 
al., 2009), which is perhaps the best known 
knowledge-based model and it allows both must-
links and cannot-links.  
For a comprehensive evaluation, we consider 
the following variations of MC-LDA and DF-LDA:  
? MC-LDA: MC-LDA with both m-sets and c-
sets. This is the newly proposed model.   
? M-LDA: MC-LDA with m-sets only. This is 
the MDK-LDA model in (Chen et al, 2013a). 
? DF-M: DF-LDA with must-links only. 
? DF-MC: DF-LDA with both must-links and 
cannot-links. This is the full DF-LDA model in 
(Andrzejewski et al, 2009).  
We do not compare with seeded models in (Burns 
et al, 2012; Jagarlamudi et al, 2012; Lu et al, 
2011; Mukherjee and Liu, 2012) as seed sets are 
special cases of must-links and they also do not 
allow c-sets (or cannot-links). 
7.1 Datasets and Settings 
Datasets: We use product reviews from four do-
mains (types of products) from Amazon.com for 
evaluation. The corpus statistics are shown in Ta-
ble 2 (columns 2 and 3). The domains are ?Cam-
era,? ?Food,? ?Computer,? and ?Care? (short for 
?Personal Care?). We have made the datasets pub-
lically available at the website of the first author. 
Pre-processing: We ran the Stanford Core NLP 
Tools1 to perform sentence detection and lemmati-
zation. Punctuations, stopwords 2 , numbers and 
words appearing less than 5 times in each corpus 
were removed. The domain name was also re-
moved, e.g., word camera in the domain ?Camera?, 
since it co-occurs with most words in the corpus, 
leading to high similarity among topics/aspects. 
Sentences as documents: As noted in (Titov and 
McDonald, 2008), when standard topic models are 
applied to reviews as documents, they tend to pro-
duce topics that correspond to global properties of 
products (e.g., brand name), which make topics 
overlapping with each other. The reason is that all 
reviews of the same type of products discuss about 
the same aspects of these products. Only the brand 
names and product names are different. Thus, us-
ing individual reviews for modeling is not very 
effective. Although there are approaches which 
model sentences (Jo and Oh, 2011; Titov and 
McDonald, 2008), we take the approach of (Brody 
and Elhadad, 2010), dividing each review into sen-
tences and treating each sentence as an independ-
ent document. Sentences can be used by all three 
baselines without any change to their models. Alt-
hough the relationships between sentences are lost, 
the data is fair to all models. 
Parameter settings: For all models, posterior in-
ference was drawn using 1000 Gibbs iterations 
with an initial burn-in of 100 iterations. For all 
models, we set ? = 1 and ? = 0.1. We found that 
small changes of ? and ? did not affect the results 
much, which was also reported in (Jo and Oh, 
2011) who also used online reviews. For the num-
ber of topics T, we tried different values (see ?7.2) 
as it is hard to know the exact number of topics. 
While non-parametric Bayesian approaches (Teh 
et al, 2006) aim to estimate ?  from the corpus, 
they are often sensitive to the hyper-parameters 
(Heinrich, 2009). 
1 http://nlp.stanford.edu/software/corenlp.shtml 
2 http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list 
Domain #Reviews #Sentences #M-sets #C-sets 
Camera 500 5171 173 18 
Food 500 2416 85 10 
Computer 500 2864 92 6 
Care 500 3008 119 13 
Average 500 3116 103 9 
Table 2. Corpus statistics with #m-sets and #c-sets 
having at least two words. 
                                                          
1661
For DF-LDA, we followed (Andrzejewski et al, 
2009) to generate must-links and cannot-links 
from our domain knowledge. We then ran DF-
LDA3 while keeping its parameters as proposed in 
(Andrzejewski et al, 2009) (we also experimented 
with different parameter settings but they did not 
produce better results). For our proposed model, 
we estimated the thresholds using cross validation 
in our pilot experiments. Estimated value ? = 0.2 
in equation 3 yielded good results. The second 
stage (steps 2 and 3) of the Gibbs sampler for MC-
LDA (for dealing with c-sets) is applied after 
burn-in phrase. 
Domain knowledge: User knowledge about a do-
main can vary a great deal. Different users may 
have very different knowledge. To reduce this var-
iance for a more reliable evaluation, instead of 
asking a human user to provide m-sets, we obtain 
the synonym sets and the antonym sets of each 
word that is a noun or adjective (as words of other 
parts-of-speech usually do not indicate aspects) 
from WordNet (Miller, 1995) and manually verify 
the words in those sets for the domain. Note that if 
a word ?  is not provided with any m-set, it is 
treated as a singleton m-set {?}. For c-sets, we ran 
LDA in each domain and provide c-sets based on 
the wrong results of LDA as in (Andrzejewski et 
al., 2009). Then, the knowledge is provided to 
each model in the format required by each model. 
The numbers of m-sets and c-sets are listed in col-
umns 4 and 5 of Table 2. Duplicate sets have been 
removed. 
7.2 Objective Evaluation 
In this section, we evaluate our proposed MC-
3 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html 
LDA model objectively. Topic models are often 
evaluated using perplexity on held-out test data. 
However, the perplexity metric does not reflect the 
semantic coherence of individual topics learned by 
a topic model (Newman et al, 2010). Recent re-
search has shown potential issues with perplexity 
as a measure: (Chang et al, 2009) suggested that 
the perplexity can sometimes be contrary to human 
judgments. Also, perplexity does not really reflect 
our goal of finding coherent aspects with accurate 
semantic clustering. It only provides a measure of 
how well the model fits the data. 
The Topic Coherence metric (Mimno et al, 
2011) (also called the ?UMass? measure (Stevens 
and Buttler, 2012)) was proposed as a better alter-
native for assessing topic quality. This metric re-
lies upon word co-occurrence statistics within the 
documents, and does not depend on external re-
sources or human labeling. It was shown that topic 
coherence is highly consistent with human expert 
labeling by Mimno et al (2011). Higher topic co-
herence score indicates higher quality of topics, 
i.e., better topic interpretability. 
Effects of Number of Topics 
Since our proposed models and the baseline mod-
els are all parametric models, we first compare 
each model given different numbers of topics. 
Figure 4 shows the average Topic Coherence score 
of each model given different numbers of topics. 
From Figure 4, we note the following: 
1. MC-LDA consistently achieves the highest To-
pic Coherence scores given different numbers 
of topics. M-LDA also works better than the 
other baseline models, but not as well as MC-
LDA. This shows that both m-sets and c-sets 
are beneficial in producing coherent aspects. 
2. DF-LDA variants, DF-M and DF-MC, do not 
perform well due to the shortcomings discussed 
 
Figure 4. Avg. Topic Coherence score of each model 
across different number of topics. 
 
Figure 5. Avg. Topic Coherence score for different 
proportions of knowledge. 
-1600
-1500
-1400
-1300
-1200
3 6 9 12 15
MC-LDA M-LDA LDA
DF-M DF-MC LDA-GPU
-1320
-1300
-1280
-1260
-1240
0% 25% 50% 100%
MC-LDA M-LDA DF-M DF-MC
                                                          
1662
in ? 1. It is slightly better than LDA when ? = 
15, but worse than LDA in other cases. We will 
further analyze the effects of knowledge on 
MC-LDA and DF-LDA shortly. 
3. LDA-GPU does not perform well due to its use 
of co-document frequency. As frequent words 
usually have high co-document frequency with 
many other words, the frequent words are 
ranked top in many topics. This shows that the 
guidance using domain knowledge is more ef-
fective than using co-document frequency. 
In terms of improvements, MC-LDA outperforms 
M-LDA significantly ( ? < 0.03 ) and all other 
baseline models significantly (? < 0.01) based on 
a paired t-test. It is important to note that by no 
means do we say that LDA-GPU and DF-LDA are 
not effective. We only say that for the task of as-
pect extraction and leveraging domain knowledge, 
these models do not generate as coherent aspects 
as ours because of their shortcomings discussed in 
? 1. In general, with more topics, the Topic Coher-
ence scores increase. We found that when ?  is 
larger than 15, aspects found by each model be-
came more and more overlapping, with several 
aspects expressing the same features of products. 
So we fix ? = 15 in the subsequent experiments.  
Effects of Knowledge 
To further analyze the effects of knowledge on 
models, in each domain, we randomly sampled 
different proportions of knowledge (i.e., different 
numbers of m-sets/must-links and c-sets/cannot-
links) as shown in Figure 5, where 0% means no 
knowledge (same as LDA and LDA-GPU, which 
do not incorporate knowledge) and 100% means 
all knowledge. From Figure 5, we see that MC-
LDA and M-LDA both perform consistently better 
than DF-MC and DF-M across different propor-
tions of knowledge. With the increasing number of 
knowledge sets, MC-LDA and M-LDA achieve 
higher Topic Coherence scores (i.e., produce more 
coherent aspects). In general, MC-LDA performs 
the best. For both DF-MC and DF-M, the Topic 
Coherence score increases from 0% to 25% 
knowledge, but decreases with more knowledge 
(50% and 100%). This shows that with limited 
amount of knowledge, the shortcomings of DF-
LDA are not very obvious, but with more 
knowledge, these issues become more serious and 
thus degrade the performance of DF-LDA.  
7.3 Human Evaluation 
Since our aim is to make topics more interpretable 
and conformable to human judgments, we worked 
with two judges who are familiar with Amazon 
products and reviews to evaluate the models sub-
jectively. Since topics from topic models are rank-
ings based on word probability and we do not 
know the number of correct topical words, a natu-
ral way to evaluate these rankings is to use Preci-
sion@n (or p@n) which was also used in 
(Mukherjee and Liu, 2012; Zhao et al, 2010), 
where n is the rank position. We give p@n for n = 
5 and 10. There are two steps in human evaluation: 
topic labeling and word labeling. 
Topic Labeling: We followed the instructions in 
(Mimno et al, 2011) and asked the judges to label 
each topic as good or bad. Each topic was present-
ed as a list of 10 most probable words in descend-
ing order of their probabilities under that topic. 
The models which generated the topics for label-
ing were obscure to the judges. In general, each 
topic was annotated as good if it had more than 
half of its words coherently related to each other 
representing a semantic concept together; other-
wise bad. Agreement of human judges on topic 
 
Figure 6. Avg. p@5 of good topics for each model 
across different domains. 
The models of each bar from left to rights are MC-LDA, M-
LDA, LDA, DF-M, DF-MC, LDA-GPU. (Same for Figure 7) 
 
Figure 7. Avg. p@10 of good topics for each model 
across different domains. 
0.5
0.6
0.7
0.8
0.9
1.0
Camera Food Computer Care
MC-LDA M-LDA LDA
DF-M DF-MC LDA-GPU
0.5
0.6
0.7
0.8
0.9
1.0
Camera Food Computer Care
MC-LDA M-LDA LDA
DF-M DF-MC LDA-GPU
1663
labeling using Cohen?s Kappa yielded a score of 
0.92 indicating almost perfect agreements accord-
ing to the scale in (Landis and Koch, 1977). This 
is reasonable as topic labeling is an easy task and 
semantic coherence can be judged well by humans. 
Word Labeling: After topic labeling, we chose 
the topics, which were labeled as good by both 
judges, as good topics. Then, we asked the two 
judges to label each word of the top 10 words in 
these good topics. Each word was annotated as 
correct if it was coherently related to the concept 
represented by the topic; otherwise incorrect. 
Since judges already had the conception of each 
topic in mind when they were labeling topics, la-
beling each word was not difficult which explains 
the high Kappa score for this labeling task (score = 
0.892). 
Quantitative Results 
Figures 6 and 7 give the average p@5 and p@10 
of all good topics over all four domains. The num-
bers of good topics generated by each model are 
shown in Table 3. We can see that the human 
evaluation results are highly consistent with Topic 
Coherence results in ?7.2. MC-LDA improves 
over M-LDA significantly (? < 0.01) and both 
MC-LDA and M-LDA outperforms the other base-
line models significantly ( ? < 0.005 ) using a 
paired t-test. We also found that when the domain 
knowledge is simple with one word usually ex-
pressing only one meaning/sense (e.g., in the do-
main ?Computer?), DF-LDA performs better than 
LDA. In other domains, it performs similarly or 
worse than LDA. Again, it shows that DF-LDA is 
not effective to handle complex knowledge, which 
is consistent with the results of effects of 
knowledge on DF-LDA in ?7.2. 
Qualitative Results 
We now show some qualitative results to give an 
intuitive feeling of the outputs from different mod-
els. There are a large number of aspects that are 
dramatically improved by MC-LDA. Due to space 
constraints, we only show some examples. To fur-
ther focus, we just show some results of MC-LDA, 
M-LDA and LDA. The results from LDA-GPU 
and DF-LDA were inferior and hard for the human 
judges to match them with aspects found by the 
other models for qualitative comparison. 
Table 4 shows three aspects Amazon, Price, 
Battery generated by each model in the domain 
?Camera?. Both LDA and M-LDA can only dis-
cover two aspects but M-LDA has a higher aver-
age precision. Given the c-set {amazon, price, 
battery}, MC-LDA can discover all three aspects 
with the highest average precision. 
8 Conclusion  
This paper proposed a new model to exploit do-
main knowledge in the form of m-sets and c-sets 
to generate coherent aspects (topics) from online 
reviews. The paper first identified and character-
ized some shortcomings of the existing 
knowledge-based models. A new model called 
MC-LDA was then proposed, whose sampling 
scheme was based on the proposed Extended GPU 
(E-GPU) model enabling multi-urn interactions. A 
comprehensive evaluation using real-life online 
reviews from multiple domains shows that MC-
LDA outperforms the state-of-the-art models sig-
nificantly and discovers aspects with high seman-
tic coherence. In our future work, we plan to 
incorporate aspect specific sentiments in the MC-
LDA model. 
Acknowledgments 
This work was supported in part by a grant from 
National Science Foundation (NSF) under grant no. 
IIS-1111092, and a grant from HP Labs Innova-
tion Research Program. 
#Good 
Topics 
MC-LDA M-LDA LDA DF-M DF-MC LDA-GPU 
Camera 15/18 12 11 9 7 3 
Food 8/16 7 7 5 4 5 
Computer 12/16 10 7 9 6 4 
Care 11/16 10 9 10 9 3 
Average 11.5/16.5 9.75/15 8.5/15 8.25/15 6.5/15 3.75/15 
Table 3. Number of good topics of each model.             
In x/y, x is the number of discovered good topics, and y is the 
total number of topics generated.  
MC-LDA M-LDA LDA 
Amazon Price Battery Price Battery Amazon Battery 
review price battery price battery card battery 
amazon perform life lot review day screen 
software money day money amazon amazon life 
customer expensive extra big life memory lcd 
month cost charger expensive extra product water 
support week water point day sd usb 
warranty cheap time cost power week cable 
package purchase power photo time month case 
product deal hour dot support item charger 
hardware product aa purchase customer class hour 
Table 4. Example aspects in the domain ?Camera?; 
errors are marked in red/italic. 
1664
References  
Amjad Abu-Jbara, Ben King, Mona Diab, and 
Dragomir Radev. 2013. Identifying Opinion 
Subgroups in Arabic Online Discussions. In 
Proceedings of ACL. 
Apoorv Agarwal and Jasneet Sabharwal. 2012. End-to-
End Sentiment Analysis of Twitter Data. In 
Proceedings of the Workshop on Information 
Extraction and Entity Analytics on Social Media 
Data, at the 24th International Conference on 
Computational Linguistics (IEEASMD-COLING 
2012), Vol. 2. 
David Andrzejewski, Xiaojin Zhu, and Mark Craven. 
2009. Incorporating domain knowledge into topic 
modeling via Dirichlet Forest priors. In Proceedings 
of ICML, pages 25?32. 
David Andrzejewski, Xiaojin Zhu, Mark Craven, and 
Benjamin Recht. 2011. A framework for 
incorporating general domain knowledge into latent 
Dirichlet alocation using first-order logic. In 
Proceedings of IJCAI, pages 1171?1177. 
Sasha Blair-goldensohn, Tyler Neylon, Kerry Hannan, 
George A. Reis, Ryan Mcdonald, and Jeff Reynar. 
2008. Building a sentiment summarizer for local 
service reviews. In Proceedings of In NLP in the 
Information Explosion Era. 
David M. Blei and Jon D. McAuliffe. 2007. Supervised 
Topic Models. In Proceedings of NIPS. 
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent Dirichlet Allocation. Journal of 
Machine Learning Research, 3, 993?1022. 
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and 
Regina Barzilay. 2008. Learning Document-Level 
Semantic Properties from Free-Text Annotations. In 
Proceedings of ACL, pages 263?271. 
Samuel Brody and Noemie Elhadad. 2010. An 
unsupervised aspect-sentiment model for online 
reviews. In Proceedings of NAACL, pages 804?812. 
Nicola Burns, Yaxin Bi, Hui Wang, and Terry 
Anderson. 2012. Extended Twofold-LDA Model for 
Two Aspects in One Sentence. Advances in 
Computational Intelligence, Vol. 298, pages 265?
275. Springer Berlin Heidelberg. 
Giuseppe Carenini, Raymond T. Ng, and Ed Zwart. 
2005. Extracting knowledge from evaluative text. In 
Proceedings of K-CAP, pages 11?18. 
Jonathan Chang, Jordan Boyd-Graber, Wang Chong, 
Sean Gerrish, and David Blei, M. 2009. Reading Tea 
Leaves: How Humans Interpret Topic Models. In 
Proceedings of NIPS, pages 288?296. 
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun 
Hsu, Malu Castellanos, and Riddhiman Ghosh. 
2013a. Leveraging Multi-Domain Prior Knowledge 
in Topic Models. In Proceedings of IJCAI, pages 
2071?2077. 
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun 
Hsu, Malu Castellanos, and Riddhiman Ghosh. 
2013b. Discovering Coherent Topics Using General 
Knowledge. In Proceedings of CIKM. 
Yejin Choi and Claire Cardie. 2010. Hierarchical 
Sequential Learning for Extracting Opinions and 
their Attributes, pages 269?274. 
Lei Fang and Minlie Huang. 2012. Fine Granular 
Aspect Analysis using Latent Structural Models. In 
Proceedings of ACL, pages 333?337. 
Thomas L. Griffiths and Mark Steyvers. 2004. Finding 
Scientific Topics. PNAS, 101 Suppl, 5228?5235. 
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang, 
and Zhong Su. 2009. Product feature categorization 
with multilevel latent semantic association. In 
Proceedings of CIKM, pages 1087?1096. 
Gregor Heinrich. 2009. A Generic Approach to Topic 
Models. In Proceedings of ECML PKDD, pages 517 
? 532. 
Minqing Hu and Bing Liu. 2004. Mining and 
Summarizing Customer Reviews. In Proceedings of 
KDD, pages 168?177. 
Yuening Hu, Jordan Boyd-Graber, and Brianna 
Satinoff. 2011. Interactive Topic Modeling. In 
Proceedings of ACL, pages 248?257. 
Hemant Ishwaran and LF James. 2001. Gibbs sampling 
methods for stick-breaking priors. Journal of the 
American Statistical Association, 96(453), 161?173. 
Jagadeesh Jagarlamudi, Hal Daum? III, and 
Raghavendra Udupa. 2012. Incorporating Lexical 
Priors into Topic Models. In Proceedings of EACL, 
pages 204?213. 
Niklas Jakob and Iryna Gurevych. 2010. Extracting 
Opinion Targets in a Single- and Cross-Domain 
Setting with Conditional Random Fields. In 
Proceedings of EMNLP, pages 1035?1045. 
Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment 
unification model for online review analysis. In 
Proceedings of WSDM, pages 815?824. 
Alistair Kennedy and Diana Inkpen. 2006. Sentiment 
Classification of Movie Reviews Using Contextual 
Valence Shifters. Computational Intelligence, 22(2), 
110?125. 
Jungi Kim, Jinji Li, and Jong-Hyeok Lee. 2009. 
Discovering the Discriminative Views: Measuring 
Term Weights for Sentiment Analysis. In 
Proceedings of ACL/IJCNLP, pages 253?261. 
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and 
Shixia Liu. 2013. A Hierarchical Aspect-Sentiment 
Model for Online Reviews. In Proceedings of AAAI. 
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 
2007. Extracting Aspect-Evaluation and Aspect-of 
Relations in Opinion Mining. In Proceedings of 
EMNLP, pages 1065?1074. 
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006. 
Opinion Extraction, Summarization and Tracking in 
1665
News and Blog Corpora. In Proceedings of AAAI 
Spring Symposium: Computational Approaches to 
Analyzing Weblogs, pages 100?107. 
JR Landis and GG Koch. 1977. The measurement of 
observer agreement for categorical data. biometrics, 
33. 
Angeliki Lazaridou, Ivan Titov, and Caroline Sporleder. 
2013. A Bayesian Model for Joint Unsupervised 
Induction of Sentiment, Aspect and Discourse 
Representations. In Proceedings of ACL. 
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, 
Yingju Xia, Shu Zhang, and Hao Yu. 2010. 
Structure-Aware Review Mining and 
Summarization. In Proceedings of COLING, pages 
653?661. 
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and 
Xiaoyan Zhu. 2012a. Cross-Domain Co-Extraction 
of Sentiment and Topic Lexicons. In Proceedings of 
ACL (1), pages 410?419. 
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011. 
Generating Aspect-oriented Multi-Document 
Summarization with Event-aspect model. In 
Proceedings of EMNLP, pages 1137?1146. 
Shoushan Li, Rongyang Wang, and Guodong Zhou. 
2012b. Opinion Target Extraction Using a Shallow 
Semantic Parsing Framework. In Proceedings of 
AAAI. 
Chenghua Lin and Yulan He. 2009. Joint 
sentiment/topic model for sentiment analysis. In 
Proceedings of CIKM, pages 375?384. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining. Morgan & Claypool Publishers. 
Bin Lu, Myle Ott, Claire Cardie, and Benjamin K. 
Tsou. 2011. Multi-aspect Sentiment Analysis with 
Topic Models. In Proceedings of ICDM Workshops, 
pages 81?88. 
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan 
Roth. 2012. Unsupervised discovery of opposing 
opinion networks from forum discussions. In 
Proceedings of CIKM, pages 1642?1646. 
Yue Lu and Chengxiang Zhai. 2008. Opinion 
integration through semi-supervised topic modeling. 
In Proceedings of WWW, pages 121?130. 
Yue Lu, ChengXiang Zhai, and Neel Sundaresan. 2009. 
Rated aspect summarization of short comments. In 
Proceedings of WWW, pages 131?140. 
Hosam Mahmoud. 2008. Polya Urn Models. Chapman 
& Hall/CRC Texts in Statistical Science. 
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, 
and ChengXiang Zhai. 2007. Topic sentiment 
mixture: modeling facets and opinions in weblogs. In 
Proceedings of WWW, pages 171?180. 
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, 
Sujian Li, and Houfeng Wang. 2012. Entity-centric 
topic-oriented opinion summarization in twitter. In 
Proceedings of KDD, pages 379?387. 
George A. Miller. 1995. WordNet: A Lexical Database 
for English. Commun. ACM, 38(11), 39?41. 
David Mimno, Hanna M. Wallach, Edmund Talley, 
Miriam Leenders, and Andrew McCallum. 2011. 
Optimizing semantic coherence in topic models. In 
Proceedings of EMNLP, pages 262?272. 
Samaneh Moghaddam and Martin Ester. 2011. ILDA: 
interdependent LDA model for learning latent 
aspects and their ratings from online product 
reviews. In Proceedings of SIGIR, pages 665?674. 
Saif Mohammad, Cody Dunne, and Bonnie J. Dorr. 
2009. Generating High-Coverage Semantic 
Orientation Lexicons From Overtly Marked Words 
and a Thesaurus. In Proceedings of EMNLP, pages 
599?608. 
Arjun Mukherjee and Bing Liu. 2012. Aspect 
Extraction through Semi-Supervised Modeling. In 
Proceedings of ACL, pages 339?348. 
David Newman, Youn Noh, Edmund Talley, Sarvnaz 
Karimi, and Timothy Baldwin. 2010. Evaluating 
topic models for digital libraries. In Proceedings of 
JCDL, pages 215?224. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, 2(1-2), 1?135. 
James Petterson, Alex Smola, Tib?rio Caetano, Wray 
Buntine, and Shravan Narayanamurthy. 2010. Word 
Features for Latent Dirichlet Allocation. In 
Proceedings of NIPS, pages 1921?1929. 
AM Popescu and Oren Etzioni. 2005. Extracting 
product features and opinions from reviews. In 
Proceedings of HLT, pages 339?346. 
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. 
Opinion Word Expansion and Target Extraction 
through Double Propagation. Computational 
Linguistics, 37(1), 9?27. 
Daniel Ramage, David Hall, Ramesh Nallapati, and 
Christopher D. Manning. 2009. Labeled LDA: a 
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of EMNLP, pages 
248?256. 
Christina Sauper and Regina Barzilay. 2013. Automatic 
Aggregation by Joint Modeling of Aspects and 
Values. J. Artif. Intell. Res. (JAIR), 46, 89?127. 
Christina Sauper, Aria Haghighi, and Regina Barzilay. 
2011. Content Models with Attitude. In Proceedings 
of ACL, pages 350?358. 
Swapna Somasundaran and J. Wiebe. 2009. 
Recognizing stances in online debates. In 
Proceedings of ACL, pages 226?234. 
Keith Stevens and PKDAD Buttler. 2012. Exploring 
Topic Coherence over many models and many 
topics. In Proceedings of EMNLP-CoNLL, pages 
952?961. 
Veselin Stoyanov and Claire Cardie. 2011. 
Automatically Creating General-Purpose Opinion 
1666
Summaries from Text. In Proceedings of RANLP, 
pages 202?209. 
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, 
and David M. Blei. 2006. Hierarchical dirichlet 
processes. Journal of the American Statistical 
Association, 1?30. 
Ivan Titov and Ryan McDonald. 2008. Modeling online 
reviews with multi-grain topic models. In 
Proceedings of WWW, pages 111?120. 
Dong Wang and Yang Liu. 2011. A Pilot Study of 
Opinion Summarization in Conversations. In 
Proceedings of ACL, pages 331?339. 
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. 
Latent aspect rating analysis on review text data: a 
rating regression approach. In Proceedings of KDD, 
pages 783?792. 
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. 
Latent aspect rating analysis without aspect keyword 
supervision. In Proceedings of KDD, pages 618?626. 
Janyce Wiebe and Ellen Riloff. 2005. Creating 
Subjective and Objective Sentence Classifiers from 
Unannotated Texts. In Proceedings of CICLing, 
pages 486?497. 
Janyce Wiebe, Theresa Wilson, Rebecca F. Bruce, 
Matthew Bell, and Melanie Martin. 2004. Learning 
Subjective Language. Computational Linguistics, 
30(3), 277?308. 
Michael Wiegand and Dietrich Klakow. 2010. 
Convolution Kernels for Opinion Holder Extraction. 
In Proceedings of HLT-NAACL, pages 795?803. 
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide 
Wu. 2009. Phrase dependency parsing for opinion 
mining. In Proceedings of EMNLP, pages 1533?
1541. 
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide 
Wu. 2011. Structural Opinion Mining for Graph-
based Sentiment Representation. In Proceedings of 
EMNLP, pages 1332?1341. 
Yunqing Xia, Boyi Hao, and Kam-Fai Wong. 2009. 
Opinion Target Network and Bootstrapping Method 
for Chinese Opinion Target Extraction. In 
Proceedings of AIRS, pages 339?350. 
Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-
Seng Chua. 2011. Aspect Ranking: Identifying 
Important Product Aspects from Online Consumer 
Reviews. In Proceedings of ACL, pages 1496?1505. 
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011. 
Constrained LDA for grouping product features in 
opinion mining. In Proceedings of the 15th Pacific-
Asia Conference on Knowledge Discovery and Data 
Mining (PAKDD), pages 448?459. 
Lei Zhang and Bing Liu. 2011. Identifying Noun 
Product Features that Imply Opinions. In 
Proceedings of ACL (Short Papers), pages 575?580. 
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and 
Xiaoming Li. 2010. Jointly Modeling Aspects and 
Opinions with a MaxEnt-LDA Hybrid. In 
Proceedings of EMNLP, pages 56?65. 
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2012. 
Cross-Language Opinion Target Extraction in 
Review Texts. In Proceedings of ICDM, pages 
1200?1205. 
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie 
review mining and summarization. In Proceedings of 
CIKM, pages 43?50. ACM Press. 
 
1667
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1139?1145,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Exploiting Social Relations and Sentiment for Stock Prediction 
 
Jianfeng Si* Arjun Mukherjee? Bing Liu? Sinno Jialin Pan* Qing Li? Huayi Li? 
* Institute for Infocomm Research, Singapore 
{ thankjeff@gmail.com, jspan@i2r.a-star.edu.sg} 
?Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, USA 
{ arjun4787@gmail.com, liub@cs.uic.edu, lhymvp@gmail.com} 
? Department of Computer Science, City University of Hong Kong, Hong Kong, China 
qing.li@cityu.edu.hk 
 
 
Abstract 
In this paper we first exploit cash-tags (?$? fol-
lowed by stocks? ticker symbols) in Twitter to 
build a stock network, where nodes are stocks 
connected by edges when two stocks co-occur 
frequently in tweets. We then employ a labeled 
topic model to jointly model both the tweets and 
the network structure to assign each node and 
each edge a topic respectively. This Semantic 
Stock Network (SSN) summarizes discussion 
topics about stocks and stock relations. We fur-
ther show that social sentiment about stock 
(node) topics and stock relationship (edge) topics 
are predictive of each stock?s market. For predic-
tion, we propose to regress the topic-sentiment 
time-series and the stock?s price time series. Ex-
perimental results demonstrate that topic senti-
ments from close neighbors are able to help im-
prove the prediction of a stock markedly. 
1 Introduction 
Existing research has shown the usefulness of 
public sentiment in social media across a wide 
range of applications. Several works showed so-
cial media as a promising tool for stock market 
prediction (Bollen et al., 2011; Ruiz et al., 2012; 
Si et al., 2013). However, the semantic relation-
ships between stocks have not yet been explored. 
In this paper, we show that the latent semantic 
relations among stocks and the associated social 
sentiment can yield a better prediction model.  
On Twitter, cash-tags (e.g., $aapl for Apple 
Inc.) are used in a tweet to indicate that the tweet 
talks about the stocks or some other related in-
formation about the companies. For example, 
one tweet containing cash-tags: $aapl and $goog 
(Google Inc.), is ?$AAPL is loosing customers. 
everybody is buying android phones! $GOOG?. 
Such joint mentions directly reflect some kind of 
latent relationship between the involved stocks, 
which motivates us to exploit such information 
for the stock prediction.  
We propose a notion of Semantic Stock Net-
work (SSN) and use it to summarize the latent 
semantics of stocks from social discussions. To 
our knowledge, this is the first work that uses 
cash-tags in Twitter for mining stock semantic 
relations. Our stock network is constructed based 
on the co-occurrences of cash-tags in tweets. 
With the SSN, we employ a labeled topic model 
to jointly model both the tweets and the network 
structure to assign each node and each edge a 
topic respectively. Then, a lexicon-based senti-
ment analysis method is used to compute a sen-
timent score for each node and each edge topic. 
To predict each stock?s performance (i.e., the 
up/down movement of the stock?s closing price), 
we use the sentiment time-series over the SSN 
and the price time series in a vector autoregres-
sion (VAR) framework.  
We will show that the neighbor relationships in 
SSN give very useful insights into the dynamics 
of the stock market. Our experimental results 
demonstrate that topic sentiments from close 
neighbors of a stock can help improve the predic-
tion of the stock market markedly. 
2 Related work 
2.1 Social Media & Economic Indices 
Many algorithms have been proposed to produce 
meaningful insights from massive social media 
data. Related works include detecting and sum-
marizing events (Weng and Lee, 2011; Weng et 
al., 2011; Baldwin et al., 2012; Gao et al., 2012) 
and analyzing sentiments about them (Pang and 
Lee, 2008; Liu, 2012), etc. Some recent literature 
also used Twitter as a sentiment source for stock 
market prediction (Bollen et al., 2011; Si et al., 
2013). This paper extends beyond the correlation 
between social media and stock market, but fur-
1139
ther exploits the social relations between stocks 
from the social media context. 
  Topic modeling has been widely used in social 
media. Various extensions of the traditional LDA 
model (Blei et al., 2003) has been proposed for 
modeling social media data (Wang et al., 2011, 
Jo and Oh, 2011; Liu et al., 2007; Mei et al., 
2007; Diao et al., 2012). Ramage et al. (2009; 
2011) presented a partially supervised learning 
model called Labeled LDA to utilize supervision 
signal in topic modeling. Ma et al. (2013) pre-
dicted the topic popularity based on hash-tags on 
Twitter in a classification framework. 
2.2 Financial Networks for Stock 
Financial network models study the correlations 
of stocks in a graph-based view (Tse et al., 2010; 
Mantegna, 1999; Vandewalle et al., 2001; On-
nela et al., 2003; Bonanno et al., 2001). The usu-
al approach is to measure the pairwise correla-
tion of stocks? historical price series and then 
connect the stocks based on correlation strengths 
to build a correlation stock network (CSN). 
However, our approach leverages social media 
posts on stock tickers. The rationale behind is 
that micro-blogging activities have been shown 
to be highly correlated with the stock market 
(Ruiz et al., 2012; Mao et al., 2012). It is more 
informative, granular to incorporate latest devel-
opments of the market as reflected in social me-
dia instead of relying on stocks? historical price.  
3 Semantic Stock Network (SSN) 
3.1 Construction of SSN 
We collected five months (Nov. 2 2012 - Apr. 3 
2013) of English tweets for a set of stocks in the 
Standard & Poor's 100 list via Twitter?s REST 
API, using cash-tags as query keywords. For 
preprocessing, we removed tweets mentioning 
more than five continuous stock tickers as such 
tweets usually do not convey much meaning for 
our task. Finally, we obtained 629,977 tweets in 
total. Table 1 shows the top five most frequent 
stocks jointly mentioned with Apple Inc. in our 
dataset. Formally, we define the stock network as 
an undirected graph ? = {? , ?}. The node set 
? comprises of stocks, ??,? ? ?  stands for the 
edge between stock nodes ? and ? and the edge 
weight is the number of co-occurrences. On ex-
ploring the co-occurrence statistics in pilot stud-
ies, we set a minimum weight threshold of 400 to 
filter most non-informative edges. Figure 1 
demonstrates a segment of the stock network 
constructed from our dataset. 
3.2 Semantic Topics over the Network 
Figure 2 illustrates our annotation for each tweet. 
For a tweet, ? with three cash-tags: {?1, ?2, ?3}, we annotate ?  with the label set, ?? =
 {?1, ?2, ?3, ?1,2, ?1,3, ?2,3}. (?1,2 is ?aapl_goog? 
if ?1is ?aapl? and ?2 is ?goog?). Then, the topic assignments of words in ? are constrained to top-
ics indexed by its label set, ??. Given the annota-tions as labels, we use the Labeled LDA model 
(Ramage et al., 2009) to jointly learn the topics 
over nodes and edges. Labeled-LDA assumes 
that the set of topics are the distinct labels in a 
labeled set of documents, and each label corre-
sponds to a unique topic. Similar to LDA (Blei et 
al., 2003), Labeled-LDA models each document 
as an admixture of latent topics and generates 
each word from a chosen topic. Moreover, La-
beled-LDA incorporates supervision by simply 
constraining the model to use only those topics 
that correspond to a document?s observed label 
set (Ramage et al., 2009). For model inference, 
we use collapsed Gibbs sampling (Bishop, 2006) 
and the symmetric Dirichlet Priors are set to: 
? = 0.01, ? = 0.01 as suggested in (Ramage et 
al., 2010). The Gibbs Sampler is given as: 
?(?? = ?|???)~
 ?(??,?)?1+ ?
?(??,?)?1+ |???|??
? ?(?,??)?1+??(?,?)?1+ |? |?? (1) 
where ?(??, ?) is the number of words in ?? as-signed to topic ?, while ?(??,?) is the marginal-ized sum. |??? | is the size of label subset of ??. 
 Figure 2. Tweet label design. 
$goog $amzn $ebay $msft $intc
43263 23266 14437 11891 2486
Table 1. co-occurrence statistics with $aapl. 
 
Figure 1. An example stock network. 
1140
?(?, ?) is the term frequency of word ? in topic 
?. |? | is the vocabulary size. The subscript -1 is 
used to exclude the count assignment of the cur-
rent word ?? . The posterior on the document?s topic distribution {??,?} and topic?s word distri-
bution {??,?} can be estimated as follows: 
??,? =  
?(??,?)+ ?
?(??,?)+ |???|??
                (2) 
??,? =  
?(?,??)+?
?(?,?)+ |? |??                   (3) 
Later, parameters {??,?} will be used to compute 
the sentiment score for topics. 
3.3 Leveraging Sentiment over SSN for 
Stock Prediction 
We define a lexicon based sentiment score in the 
form of opinion polarity for each node-indexed 
and edge-indexed topic as follows: 
?(?) = ? ??,?
|? |
?=1
?(?), ?(?) ? [?1,1]  (4) 
where ?(?) denotes the opinion polarity of word 
?. ??,?  is the word probability of ? in topic ? 
(Eq.3). Based on an opinion lexicon ?, ?(?) = 
+1 if ? ? ????, ?(?) = -1 if ? ? ???? and ?(?) 
= 0 otherwise. We use the opinion English lexi-
con contributed by Hu and Liu (2004).  
Considering the inherent dynamics of both the 
stock markets and social sentiment, we organize 
the tweets into daily based sub-sets according to 
their timestamps to construct one ????  ( ? ?
[1, ? ]) for each day. Then, we apply a Labeled 
LDA for each ???? and compute the sentiment scores for each ???? ?s nodes and edges. This yields a sentiment time series for the node, ? , 
{?(?)1, ?(?)2, ? , ?(?)? } and for the edge, ??,?, 
{?(??,?)1, ?(??,?)2, ? , ?(??,?)? } . We intro-
duce a vector autoregression model (VAR) 
(Shumway and Stoffer, 2011) by regressing sen-
timent time series together with the stock price 
time series to predict the up/down movement of 
the stock?s daily closing price. 
As usual in time series analysis, the regression 
parameters are learned during a training phase 
and then are used for forecasting under sliding 
windows, i.e., to train in period [?, ? + ?] and to 
predict on time ? + ? + 1. Here the window size 
? refers to the number of days in series used in 
model training. A VAR model for two variables 
{??} and {??} can be written as: 
?? =  ? (??????? + ???????)????=1 + ??  (5) where {?} are white noises, {?} are model pa-
rameters, and ??? notes the time steps of histori-
cal information to use. In our experiment, {??} is the target stock?s price time series, {??} is the covariate sentiment/price time series, and we will 
try ??? ? ?2,3?. We use the ?dse? library in R 
language to fit our VAR model based on least 
square regression. 
4 Experiments 
4.1 Tweets in Relation to the Stock Market 
Micro-blogging activities are well correlated 
with the stock market. Figure 3 shows us how the 
Twitter activities response to a report announce-
ment of $aapl (Jan. 23 2013). The report was 
made public soon after the market closed at 
4:00pm, while the tweets volume rose about two 
hours earlier and reached the peak at the time of 
announcement, then it arrived the second peak at 
the time near the market?s next opening (9:30am). 
By further accumulating all days? tweet volume 
in our dataset as hourly based statistics, we plot 
the volume distribution in Figure 4. Again, we 
note that trading activities are well reflected by 
tweet activities. The volume starts to rise drasti-
cally two or three hours before the market opens, 
and then reaches a peak at 9:00pm. It drops dur-
ing the lunch time and reaches the second peak 
around 2:00pm (after lunch). Above observations 
clearly show that market dynamics are discussed 
in tweets and the content in tweets? discussion 
very well reflects the fine-grained aspects of 
stock market trading, opening and closing. 
 
Figure 3. Tweet activity around $aapl?s earnings 
report date on Jan. 23 2013. 
 
Figure 4. Tweet volume distribution in our data 
over hours averaged across each day. 
0
500
1000
1500
2000
2500
Time (date-hour)
0
0.02
0.04
0.06
0.08
0.1
0 2 4 6 8 10 12 14 16 18 20 22
Time (hourly)
1141
4.2 Stock Prediction 
This section demonstrates the effectiveness of 
our SSN based approach for stock prediction. 
We leverage the sentiment time-series on two 
kinds of topics from SSN: 1). Node topic from 
the target stock itself, 2). Neighbor node/edge 
topics. We note that the price correlation stock 
network (CSN) (e.g., Bonanno et al., 2001; Man-
tegna, 1999) also defines neighbor relationships 
based on the Pearson's correlation coefficient 
(Tse et al., 2010) between pair of past price se-
ries (We get the stock dataset from Yahoo! Fi-
nance, between Nov. 2 2012 and Apr. 3 2013).  
 We build a two variables VAR model to pre-
dict the movement of a stock?s daily closing 
price. One variable is the price time series of the 
target stock ({??} in Eq.5); another is the covari-ate sentiment/price time series ({??}  in Eq.5). We setup two baselines according to the sources 
of the covariate time series as follows: 
1. Covariate price time series from CSN, we try 
the price time series from the target stock?s 
closest neighbor which takes the maximum 
historical price correlation in CSN. 
2. With no covariate time series, we try the tar-
get stock?s price only based on the univariate 
autoregression (AR) model. 
 To summarize, we try different covariate sen-
timent (?(. )) or price (?(. )) time series from 
SSN or CSN together with the target stock?s 
price time series (? ?) to predict the movement of 
one day ahead price (???). The accuracy is com-
puted based on the correctness of the predicted 
directions as follows, i.e., if the prediction ??? 
takes the same direction as the actual price value, 
we increment #(???????) by 1, #(?????????) is 
the total number of test.  
???????? = #(???????)#(?????????)       (6) 
 Figure 5 details the prediction of $aapl on dif-
ferent training window sizes of [15, 60] and lags. 
{?(????), ?(????), ?(????), ?(????_????)} are 
from SSN, ?(????)  is from CSN ($dell (Dell 
Inc.) takes the maximum price correlation score 
of 0.92 with $aapl), and ? ? =  ?(????)  is the 
univariate AR model, using the target stock?s 
price time series only. Table 2 further summariz-
es the performance comparison of different ap-
proaches reporting the average (and best) predic-
tion accuracies over all time windows and dif-
ferent lag settings. Comparing to the univariate 
AR model (?? only), we see that the sentiment 
based time-series improve performances signifi-
cantly. Among SSN sentiment based approach-
es, the ?(????) helps improve the performance 
mostly and gets the best accuracy of 0.78 on ??? 
2 and training window size of 53. On average, 
?(????) achieves a net gain over ?(????) in the 
range of 29% with lag 2 (0.62 = 1.29 x 0.48) and 
14% with lag 3 (0.57 = 1.14 x 0.50). Also, 
?(????_????)  performs better than ?(????) . 
The result indicates that $aapl?s stock perfor-
mance is highly influenced by its competitor. 
?(????) also performs well, but we will see rela-
tionships from CSN may not be so reliable. 
We further summarize some other prediction 
cases in Table 3 to show how different covariate 
sentiment sources ( ?(. ) ) and price sources 
(?(. )) from their closest neighbor nodes help 
predict their stocks, which gives consistent con-
clusions. We compute the ?-test for SSN based 
prediction accuracies against that of CSN or 
price only based approaches among all testing 
 Source Lag = 2 Lag = 3 
?? only self 0.49(0.57)	 0.47(0.52)
CSN: 
P(.)+??	 dell	 0.55(0.64)	 0.57(0.67)	
 
SSN: 
S(.)+?? 
aapl 0.48(0.56)	 0.50(0.61)
goog 0.62(0.78) 0.57(0.69) 
aapl_goog 0.55(0.65) 0.52(0.56) 
msft 0.52(0.65) 0.54(0.61) 
Table 2. Performance comparison of the average and 
best (in parentheses) prediction accuracies over all 
training window sizes for prediction on $aapl. 
 
 
Figure 5. Prediction on $aapl. (x-axis is the training 
window size, y-axis is the prediction accuracy) 
with different covariate sources. 
0.2
0.3
0.4
0.5
0.6
0.7
0.8
15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60
(a) Prediction of $aapl on lag 2
P* P(dell)+P*
S(aapl)+P* S(goog)+P*
S(aapl_goog)+P* S(msft)+P*
0.2
0.3
0.4
0.5
0.6
0.7
15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60
(b) Prediction of $aapl on lag 3
P* P(dell)+P*
S(aapl)+P* S(goog)+P*
S(aapl_goog)+P* S(msft)+P*
1142
window sizes ([15, 60]), and find that SSN based 
approaches are significantly (? -value < 0.001) 
better than others.  
We note that tweet volumes of most S&P100 
stocks are too small for effective model building, 
as tweets discuss only popular stocks, other 
stocks are not included due to their deficient 
tweet volume.  
We make the following observations: 
  1. CSN may find some correlated stock pairs 
like $ebay and $amzn, $wmt and $tgt, but some-
times, it also produces pairs without real-world 
relationships like $tgt and $vz, $qcom and $pfe, 
etc. In contrast, SSN is built on large statistics of 
human recognition in social media, which is like-
ly to be more reliable as shown. 
  2. Sentiment based approaches {?(?)} consist-
ently perform better than all price based ones 
{??, ? (?)}. For ?(?)  based predictions, senti-
ment discovered from the target stock?s closest 
neighbors in SSN performs best in general. This 
empirical finding dovetails with qualitative re-
sults in the financial analysis community (Mizik 
& Jacobson, 2003; Porter, 2008), where compa-
nies? market performances are more likely to be 
influenced by their competitors. But for Google, 
its stock market is not so much influenced by 
other companies (it gets the best prediction accu-
racy on ?(????), i.e., the internal factor). It can 
be explained by Google Inc.?s relatively stable 
revenue structure, which is well supported by its 
leading position in the search engine market. 
  3. The business of offline companies like Target 
Corp. ($tgt) and Wal-Mart Stores Inc. ($wmt) are 
highly affected by online companies like $amzn. 
Although competition exists between $tag and 
$wmt, their performances seem to be affected 
more by a third-party like $amzn (In Table 3, 
??????? predicts the best for both). Not surpris-
ingly, these offline companies have already been 
trying to establish their own online stores and 
markets. 
5 Conclusion 
This paper proposed to build a stock network 
from co-occurrences of ticker symbols in tweets. 
The properties of SSN reveal some close rela-
tionships between involved stocks, which pro-
vide good information for predicting stocks 
based on social sentiment. Our experiments show 
that SSN is more robust than CSN in capturing 
the neighbor relationships, and topic sentiments 
from close neighbors of a stock significantly im-
prove the prediction of the stock market.   
Acknowledgments 
This work was supported in part by a grant from 
the National Science Foundation (NSF) under 
grant no. IIS-1111092). 
Target ? ???? only CSN:  P(.)+?? SSN:  S(.)+?? 
 
goog 
  dis(0.96) goog aapl amzn 
2 0.48(0.59) 0.53(0.60) 0.59(0.65) 0.44(0.53) 0.42(0.49) 
3 0.46(0.54) 0.53(0.62) 0.56(0.67) 0.50(0.59) 0.43(0.49) 
 
amzn 
  csco(0.90) amzn goog msft 
2 0.48(0.54) 0.48(0.55) 0.47(0.54) 0.57(0.66) 0.60(0.68) 
3 0.46(0.53) 0.49(0.53) 0.43(0.50) 0.55(0.63) 0.57(0.66) 
 
ebay 
  amzn(0.81) ebay amzn goog 
2 0.49(0.55) 0.51(0.57) 0.44(0.53) 0.57(0.64) 0.56(0.62) 
3 0.48(0.58) 0.49(0.54) 0.45(0.58) 0.54(0.64) 0.54(0.61) 
 
tgt 
  vz(0.88) tgt wmt amzn 
2 0.43(0.53) 0.43(0.54) 0.46(0.55) 0.49(0.56) 0.49(0.59) 
3 0.44(0.50) 0.40(0.53) 0.44(0.48) 0.41(0.48) 0.48(0.54) 
 
wmt 
  tgt(0.86) wmt tgt amzn 
2 0.53(0.59) 0.53(0.63) 0.52(0.61) 0.52(0.60) 0.60(0.65) 
3 0.53(0.64) 0.48(0.57) 0.55(0.66) 0.48(0.58) 0.58(0.66) 
 
qcom 
  pfe(0.88) qcom aapl intc 
2 0.53(0.6) 0.55(0.63) 0.57(0.61) 0.46(0.54) 0.63(0.70) 
3 0.54(0.61) 0.48(0.55) 0.56(0.65) 0.51(0.61) 0.61(0.67) 
Table 3. Average and best (in parentheses) prediction accuracies (over window sizes of [15, 
60]) of some other cases with different covariates, cell of dis(0.96) means ?$dis? takes the 
maximum price correlation strength of 0.96 with ?$goog? (similar for others in column 
CSN). The best performances are highlighted in bold.  
1143
References 
Baldwin T., Cook P., Han B., Harwood A., Karuna-
sekera S., and Moshtaghi M. 2012. A support plat-
form for event detection using social intelligence. 
In Proceedings of the Demonstrations at the 13th 
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL '12). 
Association for Computational Linguistics, 
Stroudsburg, PA, USA, 69-72. 
Bishop C.M. 2006. Pattern Recognition and Machine 
Learning. Springer. 
Blei D., NG A., and Jordan M. 2003. Latent Dirichlet 
allocation. Journal of Machine Learning Research 
3:993-1022. 
Bollen J., Mao H., and Zeng X.J. 2011. Twitter mood 
predicts the stock market. Journal of Computer 
Science 2(1):1-8.  
Bonanno G., Lillo F., and Mantegna R.N. 2001. High- 
frequency cross-correlation in a set of stocks, 
Quantitative Finance, Taylor and Francis Journals, 
vol. 1(1), 96-104. 
Cohen J., Cohen P., West S.G., and Aiken L.S. 2003. 
Applied Multiple Regression/Correlation Analysis 
for the Behavioral Sciences, (3rd ed.) Hillsdale, NJ: 
Lawrence Erlbaum Associates. 
Diao Q., Jiang J., Zhu F., and Lim E.P. 2012. Finding 
bursty topics from microblogs. In Proceedings of 
the 50th Annual Meeting of the Association for 
Computational Linguistics: Long Papers - Volume 
1 (ACL '12), Vol. 1. Association for Computational 
Linguistics, Stroudsburg, PA, USA, 536-544. 
Gao W., Li P., and Darwish K. 2012. Joint topic mod-
eling for event summarization across news and so-
cial media streams. CIKM 2012: 1173-1182 
Hu M. and Liu B. 2004. Mining and summarizing 
customer reviews.  In Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining, 22-25. Seattle, Wash-
ington (KDD-2004). 
Jo Y. and Oh A. 2011. Aspect and sentiment unifica-
tion model for online review analysis. In ACM 
Conference in Web Search and Data Mining 
(WSDM-2011). 
Liu B. 2012. Sentiment analysis and opinion mining. 
Morgan & Claypool Publishers. 
Liu Y., Huang X., An A., and Yu X. 2007. ARSA: a 
sentiment-aware model for predicting sales per-
formance using blogs. In Proceedings of the 30th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retriev-
al, 607-614. ACM, New York, NY. 
Ma Z., Sun A., and Cong G. 2013. On predicting the 
popularity of newly emerging hashtags in Twitter. 
In Journal of the American Society for Information 
Science and Technology, 64(7): 1399-1410 (2013) 
Mantegna R. 1999. Hierarchical structure in financial 
markets, The European Physical Journal B - Con-
densed Matter and Complex Systems, Springer, vol. 
11(1), pages 193-197, September. 
Mao Y., Wei W., Wang B., and Liu B. 2012. Corre-
lating S&P 500 stocks with Twitter data. In Pro-
ceedings of the First ACM International Workshop 
on Hot Topics on Interdisciplinary Social Net-
works Research (HotSocial '12). ACM, New York, 
NY, USA, 69-72 
Mei Q., Ling X., Wondra M., Su H., and Zhai C. 2007. 
Topic sentiment mixture: modeling facets and 
opinions in weblogs. In Proceedings of Interna-
tional Conference on World Wide Web (WWW-
2007). 
Mizik N. and Jacobson R. 2003. Trading off between 
value creation and value appropriation: The finan-
cial implications of shifts in strategic emphasis. 
Journal of Marketing, 63-76. 
Onnela J.P., Chakraborti A., and Kaski K. 2003. Dy-
namics of market correlations: taxonomy and port-
folio analysis, Phys. Rev. E 68, 056110. 
Pang B. and Lee L. 2008. Opinion Mining and Senti-
ment Analysis. Now Publishers Inc. 
Porter M.E. 2008. The Five Competitive Forces That 
Shape Strategy.HBR, Harvard Business Review. 
Ramage D., Dumais S.T., and Liebling D. 2010. 
Characterizing microblogging using latent topic 
models. In Proceedings of ICWSM 2010. 
Ramage D., Hall D., Nallapati R., and Manning C.D. 
2009. Labeled LDA: A supervised topic model for 
credit attribution in multi-labeled corpora. In Pro-
ceedings of the 2009 Conference on Empirical 
Methods in Natural Language Processing (EMNLP 
2009). 
Ramage D., Manning C.D., and Dumais S.T. 2011. 
Partially labeled topic models for interpretable text 
mining. In Proceedings of KDD 2011 
Ruiz E.J., Hristidis V., Castillo C., Gionis A., and 
Jaimes A. 2012. Correlating financial time series 
with micro-blogging activity. In Proceedings of the 
fifth ACM international conference on Web search 
and data mining, pp. 513-522. ACM Press, NY 
(WSDM-2012). 
Shumway R.H. and Stoffer D.S. 2011. Time Series 
Analysis and Its Applications: With R Examples, 
3rd ed. 
Si J., Mukherjee A., Liu B., Li Q., Li H., and Deng X. 
2013. Exploiting Topic based Twitter Sentiment 
for Stock Prediction. In Proceedings of the 51st 
1144
Annual Meeting of the Association for Computa-
tional Linguistics. ACL?13, Sofia, Bulgaria, 24-29.   
Tse C.K., Liu J., and Lau F.C.M. 2010. A network 
perspective of the stock market, Journal of Empiri-
cal Finance. 17(4): 659-667. 
Vandewalle N., Brisbois F., and Tordoir X. 2001. 
Self-organized critical topology of stock markets, 
Quantit. Finan., 1, 372?375. 
Wang X., Wei F., Liu X., Zhou M., and Zhang M. 
2011. Topic sentiment analysis in twitter: a graph-
based hashtag sentiment classification approach. 
CIKM 2011: 1031-1040 
Weng J. and Lee B.S. 2011. Event Detection in Twit-
ter. In Proceedings of the International AAAI Con-
ference on Weblogs and Social Media 2011. 
Weng J.Y., Yang C.L., Chen B.N., Wang Y.K., and 
Lin S.D. 2011. IMASS: An Intelligent Microblog 
Analysis and Summarization System. ACL (Sys-
tem Demonstrations) 2011: 133-138. 
 
1145
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 320?329,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Modeling Review Comments 
 
 
Arjun Mukherjee Bing Liu 
Department of Computer Science Department of Computer Science 
University of Illinois at Chicago University of Illinois at Chicago 
Chicago, IL 60607, USA Chicago, IL 60607, USA 
arjun4787@gmail.com liub@cs.uic.edu 
 
 
 
 
 
 
Abstract 
Writing comments about news articles, 
blogs, or reviews have become a popular 
activity in social media. In this paper, we 
analyze reader comments about reviews. 
Analyzing review comments is important 
because reviews only tell the experiences 
and evaluations of reviewers about the 
reviewed products or services. Comments, 
on the other hand, are readers? evaluations 
of reviews, their questions and concerns. 
Clearly, the information in comments is 
valuable for both future readers and brands. 
This paper proposes two latent variable 
models to simultaneously model and 
extract these key pieces of information. 
The results also enable classification of 
comments accurately. Experiments using 
Amazon review comments demonstrate the 
effectiveness of the proposed models. 
1. Introduction 
Online reviews enable consumers to evaluate the 
products and services that they have used. These 
reviews are also used by other consumers and 
businesses as a valuable source of opinions.  
However, reviews only give the evaluations and 
experiences of the reviewers. Often a reviewer may 
not be an expert of the product and may misuse the 
product or make other mistakes. There may also be 
aspects of the product that the reviewer did not 
mention but a reader wants to know. Some 
reviewers may even write fake reviews to promote 
some products, which is called opinion spamming 
(Jindal and Liu 2008). To improve the online 
review system and user experience, some review 
hosting sites allow readers to write comments 
about reviews (apart from just providing a 
feedback by clicking whether the review is helpful 
or not). Many reviews receive a large number of 
comments. It is difficult for a reader to read them 
to get a gist of them. An automated comment 
analysis would be very helpful. Review comments 
mainly contain the following information:   
Thumbs-up or thumbs-down: Some readers may 
comment on whether they find the review 
useful in helping them make a buying decision.  
Agreement or disagreement: Some readers who 
comment on a review may be users of the 
product themselves. They often state whether 
they agree or disagree with the review. Such 
comments are valuable as they provide a second 
opinion, which may even identify fake reviews 
because a genuine user often can easily spot 
reviewers who have never used the product.  
Question and answer: A commenter may ask for 
clarification or about some aspects of the 
product that are not covered in the review. 
In this paper, we use statistical modeling to model 
review comments. Two new generative models are 
proposed. The first model is called the Topic and 
Multi-Expression model (TME). It models topics 
and different types of expressions, which represent 
different types of comment posts: 
1. Thumbs-up (e.g., ?review helped me?) 
2. Thumbs-down (e.g., ?poor review?) 
3. Question (e.g., ?how to?) 
320
4. Answer acknowledgement (e.g., ?thank you for 
clarifying?). Note that we have no expressions 
for answers to questions as there are usually no 
specific phrases indicating that a post answers 
a question except starting with the name of the 
person who asked the question. However, there 
are typical phrases for acknowledging answers, 
thus answer acknowledgement expressions.  
5. Disagreement (contention) (e.g., ?I disagree?)  
6. Agreement (e.g., ?I agree?). 
For ease of presentation, we call these 
expressions the comment expressions (or C-
expressions). TME provides a basic model for 
extracting these pieces of information and topics. 
Its generative process separates topics and C-
expression types using a switch variable and treats 
posts as random mixtures over latent topics and C-
expression types. The second model, called ME-
TME, improves TME by using Maximum-Entropy 
priors to guide topic/expression switching. In short, 
the two models provide a principled and integrated 
approach to simultaneously discover topics and C-
expressions, which is the goal of this work. Note 
that topics are usually product aspects in this work.  
The extracted C-expressions and topics from 
review comments are very useful in practice. First 
of all, C-expressions enable us to perform more 
accurate classification of comments, which can 
give us a good evaluation of the review quality and 
credibility. For example, a review with many 
Disagreeing and Thumbs-down comments is 
dubious. Second, the extracted C-expressions and 
topics help identify the key product aspects that 
people are troubled with in disagreements and in 
questions. Our experimental results in Section 5 
will demonstrate these capabilities of our models. 
With these pieces of information, comments for 
a review can be summarized. The summary may 
include, but not limited to, the following: (1) 
percent of people who give the review thumbs-up 
or thumbs-down; (2) percent of people who agree 
or disagree (or contend) with the reviewer; (3) 
contentious (disagreed) aspects (or topics); (4) 
aspects about which people often have questions. 
To the best of our knowledge, there is no 
reported work on such a fine-grained modeling of 
review comments. The related works are mainly in 
sentiment analysis (Pang and Lee, 2008; Liu 
2012), e.g., topic and sentiment modeling, review 
quality prediction and review spam detection. 
However, our work is different from them. We will 
compare with them in detail in Section 2.  
The proposed models have been evaluated both 
qualitatively and quantitatively using a large 
number of review comments from Amazon.com. 
Experimental results show that both TME and ME-
TME are effective in performing their tasks. ME-
TME also outperforms TME significantly.   
2. Related Work 
We believe that this work is the first attempt to 
model review comments for fine-grained analysis. 
There are, however, several general research areas 
that are related to our work. 
Topic models such as LDA (Latent Dirichlet 
Allocation) (Blei et al, 2003) have been used to 
mine topics in large text collections. There have 
been various extensions to multi-grain (Titov and 
McDonald, 2008a), labeled (Ramage et al, 2009), 
partially-labeled (Ramage et al, 2011), constrained 
(Andrzejewski et al, 2009) models, etc. These 
models produce only topics but not multiple types 
of expressions together with topics. Note that in 
labeled models, each document is labeled with one 
or multiple labels. For our work, there is no label 
for each comment. Our labeling is on topical terms 
and C-expressions with the purpose of obtaining 
some priors to separate topics and C-expressions. 
In sentiment analysis, researchers have jointly 
modeled topics and sentiment words (Lin and He, 
2009; Mei et al, 2007; Lu and Zhai, 2008; Titov 
and McDonald, 2008b; Lu et al, 2009; Brody and 
Elhadad, 2010; Wang et al, 2010; Jo and Oh, 
2011; Maghaddam and Ester, 2011; Sauper et al, 
2011; Mukherjee and Liu, 2012a). Our model is 
more related to the ME-LDA model in (Zhao et al, 
2010), which used a switch variable trained with 
Maximum-Entropy to separate topic and sentiment 
words. We also use such a variable. However, 
unlike sentiments and topics in reviews, which are 
emitted in the same sentence, C-expressions often 
interleave with topics across sentences and the 
same comment post may also have multiple types 
of C-expressions. Additionally, C-expressions are 
mostly phrases rather than individual words. Thus, 
a different model is required to model them. 
There have also been works aimed at putting 
authors in debate into support/oppose camps, e.g., 
(Galley et al, 2004; Agarwal et al, 2003; 
Murakami and Raymond, 2010), modeling debate 
discussions considering reply relations (Mukherjee 
and Liu, 2012b), and identifying stances in debates 
(Somasundaran and Wiebe, 2009; Thomas et al, 
321
2006; Burfoot et al, 2011). (Yano and Smith, 
2010) also modeled the relationship of a blog post 
and the number of comments it receives. These 
works are different as they do not mine C-
expressions or discover the points of contention 
and questions in comments. 
In (Kim et al, 2006; Zhang and Varadarajan, 
2006; Ghose and Ipeirotis, 2007; Liu et al, 2007; 
Liu et al, 2008; O?Mahony and Smyth, 2009; Tsur 
and Rappoport 2009), various classification and 
regression approaches were taken to assess the 
quality of reviews. (Jindal and Liu, 2008; Lim et 
al., 2010; Li et al 2011; Ott et al, 2011; 
Mukherjee et al, 2012) detect fake reviews and 
reviewers. However, all these works are not 
concerned with review comments. 
3. The Basic TME Model   
This section discusses TME. The next section 
discusses ME-TME, which improves TME. These 
models belong to the family of generative models 
for text where words and phrases (n-grams) are 
viewed as random variables, and a document is 
viewed as a bag of n-grams and each n-gram takes 
a value from a predefined vocabulary. In this work, 
we use up to 4-grams, i.e., n = 1, 2, 3, 4. For 
simplicity, we use terms to denote both words 
(unigrams or 1-grams) and phrases (n-grams). We 
denote the entries in our vocabulary by ???? where ? is the number of unique terms in the vocabulary. 
The entire corpus contains ???? documents. A document (e.g., comment post) ? is represented as 
a vector of terms ?? with ?? entries. ? is the set of all observed terms with cardinality, |?| ? ? ??? . The TME (Topic and Multi-Expression) model is 
a hierarchical generative model motivated by the 
joint occurrence of various types of expressions 
indicating Thumbs-up, Thumbs-down, Question, 
Answer acknowledgement, Agreement, and 
Disagreement and topics in comment posts. As 
before, these expressions are collectively called C-
expressions. A typical comment post mentions a 
few topics (using semantically related topical 
terms) and expresses some viewpoints with one or 
more C-expression types (using semantically 
related expressions). This observation motivates 
the generative process of our model where 
documents (posts) are represented as random 
mixtures of latent topics and C-expression types. 
Each topic or C-expression type is characterized by 
a distribution over terms (words/phrases). Assume 
we have ???? topics and ???? expression types in our corpus. Note that in our case of Amazon 
review comments, based on reading various posts, 
we hypothesize that E = 6 as in such review 
discussions, we mostly find 6 expression types 
(more details in Section 5.1). Let ?? denote the distribution of topics and C-expressions in a 
document ? with ??,? ? ???, ??? denoting the binary 
indicator variable (topic or C-expression) for the 
??? term of ?, ??,?. ??,?denotes the appropriate 
topic or C-expression type index to which ??,? 
belongs. We parameterize multinomials over topics 
using a matrix ????? whose elements ??,??  signify the 
probability of document ? exhibiting topic ?. For 
simplicity of notation, we will drop the latter 
subscript (? in this case) when convenient and use 
???  to stand for the ??? row of ??. Similarly, we define multinomials over C-expression types using 
a matrix ????? . The multinomials over terms associated with each topic are parameterized by a 
matrix ????? , whose elements ??,??  denote the 
probability of generating ? from topic ?. Likewise, 
multinomials over terms associated with each C-
expression type are parameterized by a matrix 
????? . We now define the generative process of TME (see Figure 1(a)). 
A. For each C-expression type ?, draw ???~??????? B. For each topic t, draw ??? ~??????? C. For each comment post ? ? ?1???: 
i. Draw ??~????????  
ii. Draw ???~??????? iii. Draw ???~??????? iv. For each term ??,?, ? ? ?1? ???: 
a. Draw ??,?~????????????? 
b. if (??,? ? ? ?? // ??,?is a C-expression term 
Draw ??,?~?????????) 
else  // ??,? ? ? ? ,???,?is a topical term 
Draw ??,?~????????? ) 
c. Emit ??,?~????????,?
??,?) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 (a) TME Model (b) ME-TME Model  
Figure 1: Graphical Models in plate notations.  
D 
?E?E 
?
? u 
?T ?T
?T T ?
E E ?E?T
z
r
w Nd
x 
? 
z 
r 
w Nd 
D 
?E ?E 
? 
?T ?T 
?T T ?
E E ?E?T
322
To learn the TME model from data, as exact 
inference is not possible, we resort to approximate 
inference using collapsed Gibbs sampling 
(Griffiths and Steyvers, 2004). Gibbs sampling is a 
form of Markov Chain Monte Carlo method where 
a Markov chain is constructed to have a particular 
stationary distribution. In our case, we want to 
construct a Markov chain which converges to the 
posterior distribution over ? and ? conditioned on 
the data. We only need to sample ? and ? as we use 
collapsed Gibbs sampling and the dependencies of 
? and ? have been integrated out analytically in the 
joint. Denoting the random variables ??, ?, ?? by 
singular subscripts???, ??, ???, ????, where ? ?
? ??? , a single iteration consists of performing the following sampling: 
???? ? ?, ?? ? ??| ???, ???, ???,?? ? ?? ? 
       ??
?
?????
???????????? ???
? ??,?
??
?????
??,????? ??????
? ??,?
??
?????
??,????? ??????
   (1) 
???? ? ?, ?? ? ??| ???, ???, ???, ?? ? ?? ? 
       ??
?
?????
???????????? ???
? ??,?
??
?????
??,????? ??????
? ??,?
??
?????
??,????? ??????
   (2) 
where ? ? ??, ?? denotes the ??? term of document 
? and the subscript ?? denotes assignments 
excluding the term at ??, ??. Counts??,??? and ??,???  
denote the number of times term ? was assigned to 
topic ? and expression type ? respectively. ??,??? and 
??,???  denote the number of terms in document ? that 
were assigned to topic ? and C-expression type ? 
respectively. Lastly, ??? and ??? are the number of terms in ? that were assigned to topics and C-
expression types respectively. Omission of the 
latter index denoted by ??? represents the 
marginalized sum over the latter index. We employ 
a blocked sampler jointly sampling ? and ? as this 
improves convergence and reduces autocorrelation 
of the Gibbs sampler (Rosen-Zvi et al, 2004). 
Asymmetric Beta priors: Based on our initial 
experiments with TME, we found that properly 
setting the smoothing hyper-parameter ?? is 
crucial as it governs the topic/expression switch. 
According to the generative process, ?? is the (success) probability (of the Bernoulli distribution) 
of emitting a topical/aspect term in a comment post 
??and1 ? ??, the probability of emitting a C-expression term in ?. Without loss of generality, 
we draw ??~???????? where ? is the concentration parameter and ? ? ???, ??? is the base measure. Without any prior belief, one resorts 
to uniform base measure ?? ? ?? ?0.5 (i.e., assumes that both topical and C-expression terms 
are equally likely to be emitted in a comment post). 
This results in symmetric Beta priors 
??~???????, ??? where ?? ? ???, ?? ? ??? and 
?? ? ?? ? 2/?. However, knowing the fact that topics are more likely to be emitted than 
expressions in a post apriori motivates us to take 
guidance from asymmetric priors (i.e., we now 
have a non-uniform base measure?).  This 
asymmetric setting of ? ensures that samples of ?? are more close to the actual distribution of topical 
terms in posts based on some domain knowledge. 
Symmetric ? cannot utilize any prior knowledge. In 
(Lin and He, 2009), a method was proposed to 
incorporate domain knowledge during Gibbs 
sampling initialization, but its effect becomes weak 
as the sampling progresses (Jo and Oh, 2011). 
For asymmetric priors, we estimate the hyper-
parameters from labeled data. Given a labeled set 
??, where we know the per post probability of C-expression emission (1 ? ???, we use the method of moments to estimate ? ? ???, ??? as follows: 
?? ? ? ???????? ? 1? , ?? ? ?? ?
?
? ? 1? ; ?? ? ?????, ? ? ???????   (3) 
4. ME-TME Model 
The guidance of Beta priors, although helps, is still 
relatively coarse and weak. We can do better to 
produce clearer separation of topical and C-
expression terms. An alternative strategy is to 
employ Maximum-Entropy (Max-Ent) priors 
instead of Beta priors. The Max-Ent parameters 
can be learned from a small number of labeled 
topical and C-expression terms (words and 
phrases) which can serve as good priors. The idea 
is motivated by the following observation: topical 
and C-expression terms typically play different 
syntactic roles in a sentence. Topical terms (e.g. 
?ipod? ?cell phone?, ?macro lens?, ?kindle?, etc.) 
tend to be noun and noun phrases while expression 
terms (?I refute?, ?how can you say?, ?great 
review?) usually contain pronouns, verbs, wh-
determiners, adjectives, and modals. In order to 
utilize the part-of-speech (POS) tag information, 
we move the topic/C-expression distribution ?? (the prior over the indicator variable ??,?) from the 
document plate to the word plate (see Figure 1 (b)) 
and draw it from a Max-Ent model conditioned on 
the observed feature vector ??,???????? associated with 
??,? and the learned Max-Ent parameters ??.??,? can 
323
encode arbitrary contextual features for learning. 
With Max-Ent priors, we have the new model ME-
TME. In this work, we encode both lexical and 
POS features of the previous, current and next POS 
tags/lexemes of the term ??,?. More specifically, 
 ??,???????? ? ??????,???, ?????,? , ?????,???, ??,? ? 1,??,?, ??,? ? 1? 
For phrasal terms (n-grams), all POS tags and 
lexemes of ??,?are considered as features. 
Incorporating Max-Ent priors, the Gibbs sampler 
of ME-TME is given by: 
???? ? ?, ?? ? ??| ???, ???, ???,?? ? ?? ? 
       ????? ???????,?,??????? ?? ????? ???????,?,?????? ??????,??? ?
??,????????
??,????? ??????
? ??,?
??
?????
??,????? ??????
    (4) 
???? ? ?, ?? ? ??| ???, ???, ???, ?? ? ?? ? 
       ????? ???????,?,??????? ?? ????? ???????,?,?????? ??????,??? ?
??,????????
??,????? ??????
? ??,?
??
?????
??,????? ??????
   (5) 
where ???? are the parameters of the learned Max-Ent model corresponding to the ? binary feature 
functions ???? from Max-Ent. 
5. Evaluation 
We now evaluate the proposed TME and ME-TME 
models. Specifically, we evaluate the discovered 
C-expressions, contentious aspects, and aspects 
often mentioned in questions. 
5.1 Dataset and Experiment SettingsWe crawled 
comments of reviews in Amazon.com for a variety 
of products. For each comment we extracted its id, 
the comment author id, the review id on which it 
commented, and the review author id. Our 
database consisted of 21,316 authors, 37,548 
reviews, and 88,345 comments with an average of 
124 words per comment post. 
For all our experiments, the hyper-parameters 
for TME and ME-TME were set to the heuristic 
values ?T = 50/T, ?E = 50/E, ?T = ?E = 0.1 as 
suggested in (Griffiths and Steyvers, 2004). For ?, 
we estimated the asymmetric Beta priors using the 
method of moments discussed in Section 3. We 
sampled 1000 random posts and for each post we 
identified the C-expressions emitted. We thus 
computed the per-post probability of C-expression 
emission (1 ? ??? and used Eq. (3) to get the final estimates, ?? = 3.66, ??= 1.21. To learn the Max-Ent parameters ?, we randomly sampled 500 terms 
from our corpus appearing at least 10 times and 
labeled them as topical (332) or C-expressions 
(168) and used the corresponding feature vector of 
each term (in the context of posts where it occurs) 
to train the Max-Ent model. We set the number of 
topics, T = 100 and the number of C-expression 
types, E = 6 (Thumbs-up, Thumbs-down, Question, 
Answer acknowledgement, Agreement and 
Disagreement) as in review comments, we usually 
find these six dominant expression types. Note that 
knowing the exact number of topics, T and 
expression types, E in a corpus is difficult. While 
non-parametric Bayesian approaches (Teh et al, 
2006) aim to estimate T from the corpus, in this 
work the heuristic values obtained from our initial 
experiments produced good results. We also tried 
increasing E to 7, 8, etc. However, it did not 
produce any new dominant expression type. 
Instead, the expression types became less specific 
as the expression term space became sparser. 
5.2 C-Expression Evaluation 
We now evaluate the discovered C-expressions. 
We first evaluate them qualitatively in Tables 1 
and 2. Table 1 shows the top terms of all 
expression types using the TME model. We find 
that TME can discover and cluster many correct C-
expressions, e.g., ?great review?, ?review helped 
me? in Thumbs-up; ?poor review?, ?very unfair 
review? in Thumbs-down; ?how do I?, ?help me 
decide? in Question; ?good reply?, ?thank you for 
clarifying? in Answer Acknowledgement; ?I 
disagree?, ?I refute? in Disagreement; and ?I 
agree?, ?true in fact? in Agreement. However, with 
the guidance of Max-Ent priors, ME-TME did 
much better (Table 2). For example, we find ?level 
headed review?, ?review convinced me? in 
Thumbs-up; ?biased review?, ?is flawed? in 
Thumbs-down; ?any clues?, ?I was wondering 
how? in Question; ?clears my?, ?valid answer? in 
Answer-acknowledgement; ?I don?t buy your?, 
?sheer nonsense? in Disagreement; ?agree 
completely?, ?well said? in Agreement. These 
newly discovered phrases by ME-TME are marked 
in blue in Table 3. ME-TME also has fewer errors. 
Next, we evaluate them quantitatively using the 
metric precision @ n, which gives the precision at 
different rank positions. This metric is appropriate 
here because the C-expressions (according to top 
terms in ?E) produced by TME and ME-TME are 
rankings. Table 3 reports the precisions @ top 25, 
50, 75, and 100 rank positions for all six 
expression types across both models. We evaluated 
till top 100 positions because it is usually 
324
important to see whether a model can discover and 
rank those major expressions of a type at the top. 
We believe that top 100 are sufficient for most 
applications. From Table 3, we observe that ME-
TME consistently outperforms TME in precisions 
across all expression types and all rank positions. 
This shows that Max-Ent priors are more effective 
in discovering expressions than Beta priors. Note 
that we couldn?t compare with an existing baseline 
because there is no reported study on this problem. 
5.3 Comment Classification 
Here we show that the discovered C-expressions 
can help comment classification. Note that since a 
comment can belong to one or more types (e.g., a 
comment can belong to both Thumbs-up and 
Agreement types), this task is an instance of multi-
label classification, i.e., an instance can have more 
than one class label. In order to evaluate all the 
expression types, we follow the binary approach 
which is an extension of one-against-all method for 
multi-label classification. Thus, for each label, we 
build a binary classification problem. Instances 
associated with that label are in one class and the 
rest are in the other class. To perform this task, we 
randomly sampled 2000 comments, and labeled 
each of them into one or more of the following 8 
labels: Thumbs-up, Thumbs-down, Disagreement, 
Agreement, Question, Answer-Acknowledgement, 
Answer, and None, which have 432, 401, 309, 276, 
305, 201, 228, and 18 comments respectively. We 
disregard the None category due to its small size. 
This labeling is a fairly easy task as one can almost 
certainly make out to which type a comment 
belongs. Thus we didn?t use multiple labelers. The 
distribution reveals that the labels are overlapping. 
For instance, we found many comments belonging 
to both Thumbs-down and Disagreement, Thumbs-up 
with Acknowledgement and with Question. 
For supervised classification, the choice of 
feature is a key issue. While word and POS n-
grams are traditional features, such features may 
not be the best for our task. We now compare such 
features with the C-expressions discovered by the 
proposed models. We used the top 1000 terms 
from each of the 6 C-expression rankings as 
features. As comments in Question type mostly use 
the punctuation ???, we added it in our feature set. 
We use precision, recall and F1 as our metric to 
compare classification performance using a trained 
SVM (linear kernel). All results (Table 4) were 
computed using 10-fold cross-validation (CV). We 
also tried Na?ve Bayes and Logistic Regression 
classifiers, but they were poorer than SVM. Hence 
their results are not reported due to space 
constraints. As a separate experiment (not shown 
here also due to space constraints), we analyzed 
the classification performance by varying the 
number of top terms from 200, 400,?, 1000, 1200, 
etc. and found that the F1 scores stabilized after top 
 
 
 
        Figure 5: Precision @ top 50,  
Thumbs-up (e1): review, thanks, great review, nice review, time, best review, appreciate, you, your review helped, nice, terrific, 
review helped me, good critique, very, assert, wrong, useful 
review, don?t, misleading, thanks a lot, ? 
Thumbs-down (e2): review, no, poor review, imprecise, you, complaint, very, suspicious, bogus review, absolutely, credible, 
very unfair review, criticisms, true, disregard this review, disagree 
with, judgment, without owning, ? 
Question (e3): question, my, I, how do I, why isn?t, please explain, good answer, clarify, don?t understand, my doubts, I?m confused, 
does not, understand, help me decide, how to,  yes, answer, how 
can I, can?t explain, ? 
Answer Acknowledgement (e4): my, informative, answer, good reply, thank you for clarifying, answer doesn?t, good answer, 
vague, helped me choose, useful suggestion, don?t understand, 
cannot explain, your answer, doubts, answer isn?t, ? 
Disagreement (e5): disagree, I, don?t, I disagree, argument claim, I reject, I refute, I refuse, oppose, debate, accept, don?t agree, quote, 
sense, would disagree, assertions, I doubt, right,  your, really, 
you, I?d disagree, cannot, nonsense,... 
Agreement (e6): yes, do, correct, indeed, no, right, I agree, you, agree, I accept, very, yes indeed, true in fact, indeed correct, I?d 
agree, completely, true, but, doesn?t, don?t, definitely, false, 
completely agree, agree with your, true, ? 
Table 1: Top terms (comma delimited) of six expression types 
e1, e2, e3, e4, e5, e6 (?E) using TME model. Red (bold) colored terms denote possible errors 
Thumbs-up (e1): review, you, great review, I'm glad I read, best review, review convinced me, review helped me,  good review, terrific 
review, job, thoughtful review, awesome review, level headed review, 
good critique, good job, video review,... 
Thumbs-down (e2): review, you, bogus review, con, useless review, ridiculous,  biased review, very unfair review, is flawed, completely, 
skeptical, badmouth, misleading review, cynical review, wrong, 
disregard this review, seemingly honest, ? 
Question (e3): question, I, how do I, why isn?t, please explain, clarify, any clues, answer, please explain, help me decide, vague, how to, how 
do I, where can I, how to set, I was wondering how, could you explain, 
how can I, can I use, ? 
Answer Acknowledgement (e4): my, good reply, , answer, reply, helped me choose, clears my,  valid answer, answer doesn?t, 
satisfactory answer, can you clarify, informative answer, useful 
suggestion, perfect answer, thanks for your reply, doubts, ? 
Disagreement (e5): disagree, I, don?t, I disagree, doesn?t, I don?t buy your, credible, I reject, I doubt, I refuse, I oppose, sheer nonsense, 
hardly, don?t agree, can you prove, you have no clue, how do you say, 
sense, you fail, contradiction, ? 
Agreement (e6): I, do, agree, point, yes, really, would agree, you, agree, I accept, claim, agree completely, personally agree, true in fact, 
indeed correct, well said, valid point, correct, never meant, might not, 
definitely agree,? 
Table 2: Top terms (comma delimited) of six expression types 
using ME-TME model. Red (bold) terms denote possible errors. 
Blue (italics) terms denote those newly discovered by the model; 
rest (black) were used in Max-Ent training. 
325
1000 terms. From Table 4, we see that F1 scores 
dramatically increase with C-expression (??) 
features for all expression types. TME and ME-
TME progressively improve the classification. 
Improvements of TME and ME-TME being 
significant (p<0.001) using a paired t-test across 
10-fold cross validations shows that the discovered 
C-expressions are of high quality and useful.  
We note that the annotation resulted in a new 
label ?Answer? which consists of mostly replies to 
comments with questions. Since an ?answer? to a 
question usually does not show any specific 
expression, it does not attain very good F1 scores. 
Thus, to improve the performance of the Answer 
type comments, we added three binary features for 
each comment c on top of C-expression features: 
i) Is the author of c the review author too? The 
idea here is that most of the times the reviewer 
answers the questions raised in comments. 
ii) Is there any comment posted before c by some 
author a which has been previously classified 
as a question post? 
iii) Is there any comment posted after c by author 
a that replies to c (using @name) and is an 
Answer-Acknowledgement comment (which 
again has been previously classified as such)? 
Using these additional features, we obtained a 
precision of 0.78 and a recall of 0.73 yielding an F1 
C-Expression Type P@25 P@50 P@75 P@100 
TME ME-TME TME ME-TME TME ME-TME TME ME-TME
Thumbs-up 0.60 0.80 0.66 0.78 0.60 0.69 0.55 0.64 
Thumbs-down 0.68 0.84 0.70 0.80 0.63 0.67 0.60 0.65 
Question 0.64 0.80 0.68 0.76 0.65 0.72 0.61 0.67 
Answer-Acknowledgement 0.68 0.76 0.62 0.72 0.57 0.64 0.54 0.58 
Disagreement 0.76 0.88 0.74 0.80 0.68 0.73 0.65 0.70 
Agreement 0.72 0.80 0.64 0.74 0.61 0.70 0.60 0.69 
Table 3: Precision @ top 25, 50, 75, and 100 rank positions for all C-expression types. 
Features Thumbs-up Thumbs-down Question Answer-Ack. Disagreement Agreement Answer 
 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 
W+POS 1-gram 0.68 0.66 0.67 0.65 0.65 0.65 0.71 0.68 0.69 0.64 0.61 0.62 0.73 0.72 0.72 0.67 0.65 0.66 0.58 0.57 0.57
W+POS 1-2 gram 0.72 0.69 0.70 0.68 0.67 0.67 0.74 0.69 0.71 0.69 0.63 0.65 0.76 0.75 0.75 0.71 0.69 0.70 0.60 0.57 0.58
W+POS, 1-3 gram 0.73 0.71 0.72 0.69 0.68 0.68 0.75 0.69 0.72 0.70 0.64 0.66 0.76 0.76 0.76 0.72 0.70 0.71 0.61 0.58 0.59
W+POS, 1-4 gram 0.74 0.72 0.73 0.71 0.68 0.69 0.75 0.70 0.72 0.70 0.65 0.67 0.77 0.76 0.76 0.73 0.70 0.71 0.61 0.58 0.59
C-Expr. ?E, TME 0.82 0.74 0.78 0.77 0.71 0.74 0.83 0.75 0.78 0.75 0.72 0.73 0.83 0.80 0.81 0.78 0.75 0.76 0.66 0.61 0.63
C-Expr. ?E, ME-TME 0.87 0.79 0.83 0.80 0.73 0.76 0.87 0.76 0.81 0.77 0.72 0.74 0.86 0.81 0.83 0.81 0.77 0.79 0.67 0.61 0.64
Table 4: Precision (P), Recall (R), and F1 scores of binary classification using SVM and different features. The improvements of our models are significant (p<0.001) over paired t-test across 10-fold cross validation. 
D 
?E  + Noun/Noun Phrase TME ME-TME 
J1 J2 J1 J2 J1 J2 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 
D1 0.62 0.70 0.66 0.58 0.67 0.62 0.66 0.75 0.70 0.62 0.70 0.66 0.67 0.79 0.73 0.64 0.74 0.69
D2 0.61 0.67 0.64 0.57 0.63 0.60 0.66 0.72 0.69 0.62 0.67 0.64 0.68 0.75 0.71 0.64 0.71 0.67
D3 0.60 0.69 0.64 0.56 0.64 0.60 0.64 0.73 0.68 0.60 0.67 0.63 0.67 0.76 0.71 0.63 0.72 0.67
D4 0.59 0.68 0.63 0.55 0.65 0.60 0.63 0.71 0.67 0.59 0.68 0.63 0.65 0.73 0.69 0.62 0.71 0.66
Avg. 0.61 0.69 0.64 0.57 0.65 0.61 0.65 0.73 0.69 0.61 0.68 0.64 0.67 0.76 0.71 0.63 0.72 0.67
Table 5 (a) 
D ?
E  + Noun/Noun Phrase TME ME-TME 
J1 J2 J1 J2 J1 J2 
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 D1 0.57 0.65 0.61 0.54 0.63 0.58 0.61 0.69 0.65 0.58 0.66 0.62 0.64 0.73 0.68 0.61 0.70 0.65
D2 0.61 0.66 0.63 0.58 0.61 0.59 0.64 0.68 0.66 0.60 0.64 0.62 0.68 0.70 0.69 0.65 0.69 0.67
D3 0.60 0.68 0.64 0.57 0.64 0.60 0.64 0.71 0.67 0.62 0.68 0.65 0.67 0.72 0.69 0.64 0.69 0.66
D4 0.56 0.67 0.61 0.55 0.65 0.60 0.60 0.72 0.65 0.58 0.68 0.63 0.63 0.75 0.68 0.61 0.71 0.66
Avg. 0.59 0.67 0.62 0.56 0.63 0.59 0.62 0.70 0.66 0.60 0.67 0.63 0.66 0.73 0.69 0.63 0.70 0.66
Table 5 (b) 
Table 5: Points of Contention (a), Questioned aspects (b). D1: Ipod, D2: Kindle, D3: Nikon, D4: Garmin. We report the 
average precision (P), recall (R), and F1 score over 100 comments for each particular domain.  
Statistical significance: Differences between Nearest Noun Phrase and TME for both judges (J1, J2) across all domains were significant at 97% confidence level (p<0.03). Differences among TME and ME-TME for both judges (J1, J2) across all domains were significant at 95% confidence level (p<0.05). A paired t-test was used for testing significance. 
326
score of 0.75 which is a dramatic increase beyond 
0.64 achieved by ME-TME in Table 4. 
5.4 Contention Points and Questioned Aspects  
We now turn to the task of discovering points of 
contention in disagreement comments and aspects 
(or topics) raised in questions. By ?points?, we 
mean the topical terms on which some contentions 
or disagreements have been expressed. Topics 
being the product aspects are also indirectly 
evaluated in this task. We employ the TME and 
ME-TME models in the following manner.  
We only detail the approach for disagreement 
comments. The same method is applied to question 
comments. Given a disagreement comment post ?, 
we first select the top k topics that are mentioned in 
d according to its topic distribution, ??? . Let ?? be the set of these top ? topics in ?. Then, for each 
disagreement expression?? ? ? ? ???????????????? , 
we emit the topical terms (words/phrases) of topics 
in ??which appear within a word window of ? from ? in ?. More precisely, we emit the set ? ? ??|? ?
? ? ??? , ? ? ??, |??????? ? ???????| ? ??, where posi(?) returns the position index of the word or 
phrase in document ?. To compute the intersection 
? ? ? ? ??? , we need a threshold. This is so because the Dirichlet distribution has a smoothing 
effect which assigns some non-zero probability 
mass to every term in the vocabulary for each topic 
?. So for computing the intersection, we considered 
only terms in ???  which have ???|?? ? ???,??  > 0.001 
as probability masses lower than 0.001 are more 
due to the smoothing effect of the Dirichlet 
distribution than true correlation. In an actual 
application, the values for ? and ? can be set 
according to the user?s need. In our experiment, we 
used ??= 3 and 5 = ?, which are reasonable because 
a post normally does not talk about many topics 
(?), and the contention points (aspect terms) appear 
quite close to the disagreement expressions. 
For comparison, we also designed a baseline. 
For each disagreement (or question) expression 
? ? ? ? ???????????????? (???????????? ), we emit the 
nouns and noun phrases within the same window ? 
as the points of contention (question) in ?. This 
baseline is reasonable because topical terms are 
usually nouns and noun phrases and are near 
disagreement (question) expressions. We note that 
this baseline cannot stand alone because it has to 
rely on our expression models ?? of ME-TME. 
Next, to evaluate the performance of these 
methods in discovering points of contention, we 
randomly selected 100 disagreement (contentious) 
(and 100 question) comment posts on reviews from 
each of the 4 product domains: Ipod, Kindle, 
Nikon Cameras, and Garmin GPS in our database 
and employed the aforementioned methods to 
discover the points of contention (question) in each 
post. Then we asked two human judges (graduate 
students fluent in English) to manually judge the 
results produced by each method for each post. We 
asked them to report the precision of the 
discovered terms for a post by judging them as 
being indeed valid points of contention and report 
recall in a post by judging how many of actually 
contentious points in the post were discovered. In 
Table 5 (a), we report the average precision and 
recall for 100 posts in each domain by the two 
judges J1 and J2 for different methods on the task 
of discovering points (aspects) of contention. In 
Table 5 (b), similar results are reported for the task 
of discovering questioned aspects in 100 question 
comments for each product domain. Since this 
judging task is subjective, the differences in the 
results from the two judges are not surprising. Our 
judges were made to work in isolation to prevent 
any bias. We observe that across all domains, ME-
TME again performs the best consistently. Note 
that agreement study using Kappa is not used here 
as our problem is not to label a fixed set of items 
categorically by the judges. 
6. Conclusion 
This paper proposed the problem of modeling 
review comments, and presented two models TME 
and ME-TME to model and to extract topics 
(aspects) and various comment expressions. These 
expressions enable us to classify comments more 
accurately, and to find contentious aspects and 
questioned aspects. These pieces of information 
also allow us to produce a simple summary of 
comments for each review as discussed in Section 
1. To our knowledge, this is the first attempt to 
analyze comments in such details. Our experiments 
demonstrated the efficacy of the models. ME-TME 
also outperformed TME significantly. 
Acknowledgments 
This work is supported in part by National Science 
Foundation (NSF) under grant no. IIS-1111092.  
327
References  
Agarwal, R., S. Rajagopalan, R. Srikant, Y. Xu. 2003. 
Mining newsgroups using networks arising from 
social behavior. Proceedings of International 
Conference on World Wide Web 2003. 
Andrzejewski, D., X. Zhu, M. Craven. 2009. 
Incorporating domain knowledge into topic 
modeling via Dirichlet forest priors. Proceedings of 
International Conference on Machine Learning.  
Blei, D., A. Ng, and M. Jordan. 2003. Latent Dirichlet 
Allocation. Journal of Machine Learning Research. 
Brody, S. and S. Elhadad. 2010. An Unsupervised 
Aspect-Sentiment Model for Online Reviews. 
Proceedings of the Annual Conference of the North 
American Chapter of the ACL. 
Burfoot, C., S. Bird, and T. Baldwin. 2011. Collective 
Classification of Congressional Floor-Debate 
Transcripts. Proceedings of the 49th Annual Meeting 
of the Association for Computational Linguistics.  
Galley, M., K. McKeown, J. Hirschberg, E. Shriberg. 
2004. Identifying agreement and disagreement in 
conversational speech: Use of Bayesian networks to 
model pragmatic dependencies. Proceedings of the 
42th Annual Meeting of the Association of 
Computational Linguistics.  
Ghose, A. and P. Ipeirotis. 2007. Designing novel 
review ranking systems: predicting the usefulness 
and impact of reviews. Proceedings of International 
Conference on Electronic Commerce. 
Griffiths, T. and M. Steyvers. 2004. Finding scientific 
topics. Proceedings of National Academy of 
Sciences.  
Kim, S., P. Pantel, T. Chklovski, and M. Pennacchiotti. 
2006. Automatically assessing review helpfulness. 
Proceedings of Empirical Methods in Natural 
Language Processing.  
Jindal, N. and B. Liu. 2008. Opinion spam and analysis. 
Proceedings of the ACM International Conference on 
Web Search and Web Data Mining.  
Jo, Y. and A. Oh. 2011. Aspect and sentiment 
unification model for online review analysis. 
Proceedings of the ACM International Conference 
on Web Search and Web Data Mining. 
Li, F., M. Huang, Y. Yang, and X. Zhu. 2011. Learning 
to Identify Review Spam. in Proceedings of the 
International Joint Conference on Artificial 
Intelligence. 
Lim, E., V. Nguyen, N. Jindal, B. Liu, and H. Lauw. 
2010. Detecting Product Review Spammers using 
Rating Behaviors. Proceedings of the ACM 
International Conference on Information and 
Knowledge Management. 
Lin, C. and Y. He. 2009. Joint sentiment/topic model for 
sentiment analysis. Proceedings of the ACM 
International Conference on Information and 
Knowledge Management.  
Liu, J., Y. Cao, C. Lin, Y. Huang, and M. Zhou. 2007. 
Low-quality product review detection in opinion 
summarization. Proceedings of Empirical Methods in 
Natural Language Processing.  
Liu, B. 2012. Sentiment Analysis and Opinion Mining. 
Morgan & Claypool publishers (to appear in June 
2012).  
Liu, Y., X. Huang, A. An, and X. Yu. 2008. Modeling 
and predicting the helpfulness of online reviews. 
Proceedings of IEEE International Conference on 
Data Mining. 
Lu, Y. and C. Zhai. 2008. Opinion integration through 
semi-supervised topic modeling. Proceedings of 
International Conference on World Wide Web.  
Lu, Y., C. Zhai, and N. Sundaresan. 2009. Rated aspect 
summarization of short comments. Proceedings of 
International Conference on World Wide.  
Mei, Q. X. Ling, M. Wondra, H. Su and C. Zhai. 2007. 
Topic sentiment mixture: modeling facets and 
opinions in weblogs.? Proceedings of International 
Conference on World Wide.  
Moghaddam, S. and M. Ester. 2011. ILDA: 
interdependent LDA model for learning latent 
aspects and their ratings from online product reviews. 
Proceedings of Annual ACM SIGIR Conference on 
Research and Development in Information Retrieval.  
Mukherjee, A. and B. Liu. 2012a. Aspect Extraction 
through Semi-Supervised Modeling. Proceedings of 
50th Annual Meeting of Association for 
Computational Linguistics (to appear in July 2012). 
Mukherjee, A. and B. Liu. 2012b. Mining Contentions 
from Discussions and Debates. Proceedings of ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining (to appear in August 
2012). 
Mukherjee, A., B. Liu and N. Glance. 2012. Spotting 
Fake Reviewer Groups in Consumer Reviews. 
Proceedings of International World Wide Web 
Conference. 
Murakami A., and R. Raymond, 2010. Support or 
Oppose? Classifying Positions in Online Debates 
from Reply Activities and Opinion Expressions. 
Proceedings of International Conference on 
328
Computational Linguistics. 
O'Mahony, M. P. and B. Smyth. 2009. Learning to 
recommend helpful hotel reviews. Proceedings of the 
third ACM conference on Recommender systems.  
Ott, M., Y. Choi, C. Cardie, and J. T. Hancock. 2011. 
Finding deceptive opinion spam by any stretch of the 
imagination. Proceedings of the 49th Annual Meeting 
of the Association for Computational Linguistics. 
Pang, B. and L. Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval. 
Ramage, D., D. Hall, R. Nallapati, and C. Manning. 
2009. Labeled LDA: A supervised topic model for 
credit attribution in multi-labeled corpora. 
Proceedings of Empirical Methods in Natural 
Language Processing.  
Ramage, D., C. Manning, and S. Dumais. 2011 Partially 
labeled topic models for interpretable text mining. 
Proceedings of the 17th ACM SIGKDD 
international conference on Knowledge discovery 
and data mining.  
Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smith. 
2004. The author-topic model for authors and 
documents. Uncertainty in Artificial Intelligence.  
Sauper, C. A. Haghighi and R. Barzilay. 2011. Content 
models with attitude. Proceedings of the 49th Annual 
Meeting of the Association for Computational 
Linguistics. 
Somasundaran, S., J. Wiebe. 2009. Recognizing stances 
in online debates. Proceedings of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP of the 
AFNLP 
Teh, Y., M. Jordan, M. Beal and D. Blei. 2006. 
Hierarchical Dirichlet Processes. Journal of the 
American Statistical Association. 
Thomas, M., B. Pang and L. Lee. 2006. Get out the 
vote: Determining support or opposition from 
Congressional floor-debate transcripts. Proceedings 
of Empirical Methods in Natural Language 
Processing.  
Titov, I. and R. McDonald. 2008a. Modeling online 
reviews with multi-grain topic models. Proceedings 
of International Conference on World Wide Web.  
Titov, I. and R. McDonald. 2008b. A joint model of text 
and aspect ratings for sentiment summarization. 
Proceedings of Annual Meeting of the Association 
for Computational Linguistics.  
Tsur, O. and A. Rappoport. 2009. Revrank: A fully 
unsupervised algorithm for selecting the most helpful 
book reviews. Proceedings of the International AAAI 
Conference on Weblogs and Social Media. 
Wang, H., Y. Lu, and C. Zhai. 2010. Latent aspect 
rating analysis on review text data: a rating 
regression approach. Proceedings of ACM SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining. 
Yano, T and N. Smith. 2010. What?s Worthy of 
Comment? Content and Comment Volume in 
Political Blogs. Proceedings of the International 
AAAI Conference on Weblogs and Social Media. 
Zhang, Z. and B. Varadarajan. 2006. Utility scoring of 
product reviews. Proceedings of ACM International 
Conference on Information and Knowledge 
Management.  
Zhao, X., J. Jiang, H. Yan, and X. Li. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-LDA 
hybrid. Proceedings of Empirical Methods in Natural 
Language Processing. 
329
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 339?348,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Aspect Extraction through Semi-Supervised Modeling 
 
 
Arjun Mukherjee Bing Liu 
Department of Computer Science Department of Computer Science 
University of Illinois at Chicago University of Illinois at Chicago 
Chicago, IL 60607, USA Chicago, IL 60607, USA 
arjun4787@gmail.com liub@cs.uic.edu 
 
  
 
 
 
 
Abstract 
Aspect extraction is a central problem in 
sentiment analysis. Current methods either 
extract aspects without categorizing them, 
or extract and categorize them using 
unsupervised topic modeling. By 
categorizing, we mean the synonymous 
aspects should be clustered into the same 
category. In this paper, we solve the 
problem in a different setting where the 
user provides some seed words for a few 
aspect categories and the model extracts 
and clusters aspect terms into categories 
simultaneously. This setting is important 
because categorizing aspects is a subjective 
task. For different application purposes, 
different categorizations may be needed. 
Some form of user guidance is desired. In 
this paper, we propose two statistical 
models to solve this seeded problem, which 
aim to discover exactly what the user 
wants. Our experimental results show that 
the two proposed models are indeed able to 
perform the task effectively.  
1 Introduction 
Aspect-based sentiment analysis is one of the main 
frameworks for sentiment analysis (Hu and Liu, 
2004; Pang and Lee, 2008; Liu, 2012). A key task 
of the framework is to extract aspects of entities 
that have been commented in opinion documents. 
The task consists of two sub-tasks. The first sub-
task extracts aspect terms from an opinion corpus. 
The second sub-task clusters synonymous aspect 
terms into categories where each category 
represents a single aspect, which we call an aspect 
category. Existing research has proposed many 
methods for aspect extraction. They largely fall 
into two main types. The first type only extracts 
aspect terms without grouping them into categories 
(although a subsequent step may be used for the 
grouping, see Section 2). The second type uses 
statistical topic models to extract aspects and group 
them at the same time in an unsupervised manner. 
Both approaches are useful. However, in practice, 
one also encounters another setting, where 
grouping is not straightforward because for 
different applications the user may need different 
groupings to reflect the application needs. This 
problem was reported in (Zhai et al, 2010), which 
gave the following example. In car reviews, 
internal design and external design can be regarded 
as two separate aspects, but can also be regarded as 
one aspect, called ?design?, based on the level of 
details that the user wants to study. It is also 
possible that the same word may be put in different 
categories based on different needs. However, 
(Zhai et al, 2010) did not extract aspect terms. It 
only categorizes a set of given aspect terms. 
In this work, we propose two novel statistical 
models to extract and categorize aspect terms 
automatically given some seeds in the user 
interested categories. It is thus able to best meet the 
user?s specific needs. Our models also jointly 
model both aspects and aspect specific sentiments. 
The first model is called SAS and the second 
model is called ME-SAS. ME-SAS improves SAS 
by using Maximum-Entropy (or Max-Ent for short) 
priors to help separate aspects and sentiment terms. 
However, to train Max-Ent, we do not need 
manually labeled training data (see Section 4).  
339
In practical applications, asking users to provide 
some seeds is easy as they are normally experts in 
their trades and have a good knowledge what are 
important in their domains.  
Our models are related to topic models in 
general (Blei et al, 2003) and joint models of 
aspects and sentiments in sentiment analysis in 
specific (e.g., Zhao et al, 2010). However, these 
current models are typically unsupervised. None of 
them can use seeds. With seeds, our models are 
thus semi-supervised and need a different 
formulation. Our models are also related to the DF-
LDA model in (Andrzejewski et al, 2009), which 
allows the user to set must-link and cannot-link 
constraints. A must-link means that two terms must 
be in the same topic (aspect category), and a 
cannot-link means that two terms cannot be in the 
same topic. Seeds may be expressed with must-
links and cannot-links constraints. However, our 
models are very different from DF-LDA. First of 
all, we jointly model aspect and sentiment, while 
DF-LDA is only for topics/aspects. Joint modeling 
ensures clear separation of aspects from sentiments 
producing better results. Second, our way of 
treating seeds is also different from DF-LDA. We 
discuss these and other related work in Section 2. 
The proposed models are evaluated using a large 
number of hotel reviews. They are also compared 
with two state-of-the-art baselines. Experimental 
results show that the proposed models outperform 
the two baselines by large margins. 
2 Related Work  
There are many existing works on aspect 
extraction. One approach is to find frequent noun 
terms and possibly with the help of dependency 
relations (Hu and Liu, 2004; Popescu and Etzioni, 
2005; Zhuang et al, 2006; Blair-Goldensohn et al, 
2008; Ku et al, 2006; Wu et al, 2009; 
Somasundaran and Wiebe, 2009; Qiu et al, 2011). 
Another approach is to use supervised sequence 
labeling (Liu, Hu and Cheng 2005; Jin and Ho, 
2009; Jakob and Gurevych, 2010; Li et al, 2010; 
Choi and Cardie, 2010; Kobayashi et al, 2007; Yu 
et al, 2011). Ma and Wan (2010) also exploited 
centering theory, and (Yi et al, 2003) used 
language models. However, all these methods do 
not group extracted aspect terms into categories. 
Although there are works on grouping aspect terms 
(Carenini et al, 2005; Zhai et al, 2010; Zhai et al, 
2011; Guo et al, 2010), they all assume that aspect 
terms have been extracted beforehand. 
In recent years, topic models have been used to 
perform extraction and grouping at the same time. 
Existing works are based on two basic models, 
pLSA (Hofmann, 1999) and LDA (Blei et al, 
2003). Some existing works include discovering 
global and local aspects (Titov and McDonald, 
2008), extracting key phrases (Branavan et al, 
2008), rating multi-aspects (Wang et al, 2010; 
Moghaddam and Ester, 2011), summarizing 
aspects and sentiments (Lu et al, 2009), and 
modeling attitudes (Sauper et al, 2011). In (Lu and 
Zhai, 2008), a semi-supervised model was 
proposed. However, their method is entirely 
different from ours as they use expert reviews to 
guide the analysis of user reviews. 
 Aspect and sentiment extraction using topic 
modeling come in two flavors: discovering aspect 
words sentiment wise (i.e., discovering positive 
and negative aspect words and/or sentiments for 
each aspect without separating aspect and 
sentiment terms) (Lin and He, 2009; Brody and 
Elhadad, 2010, Jo and Oh, 2011) and separately 
discovering both aspects and sentiments (e.g., Mei 
et al, 2007; Zhao et al, 2010). Zhao et al (2010) 
used Maximum-Entropy to train a switch variable 
to separate aspect and sentiment words. We adopt 
this method as well but with no use of manually 
labeled data in training. One problem with these 
existing models is that many discovered aspects 
are not understandable/meaningful to users. Chang 
et al (2009) stated that one reason is that the 
objective function of topic models does not always 
correlate well with human judgments. Our seeded 
models are designed to overcome this problem.   
Researchers have tried to generate ?meaningful? 
and ?specific? topics/aspects. Blei and McAuliffe 
(2007) and Ramage et al (2009) used document 
label information in a supervised setting. Hu et al 
(2011) relied on user feedback during Gibbs 
sampling iterations. Andrzejewski et al (2011) 
incorporated first-order logic with Markov Logic 
Networks. However, it has a practical limitation 
for reasonably large corpora since the number of 
non-trivial groundings can grow to O(N2) where N 
is the number of unique tokens in the corpus. 
Andrzejewski et al (2009) used another approach 
(DF-LDA) by introducing must-link and cannot-
link constraints as Dirichlet Forest priors. Zhai et 
al. (2011) reported that the model does not scale up 
340
when the number of cannot-links go beyond 1000 
because the number of maximal cliques Q(r) in a 
connected component of size |r| in the cannot-link 
graph is exponential in r. Note that we could still 
experiment with DF-LDA as our problem size is 
not so large. We will show in Section 4 that the 
proposed models outperform it by a large margin. 
3 Proposed Seeded Models  
The standard LDA and existing aspect and 
sentiment models (ASMs) are mostly governed by 
the phenomenon called ?higher-order co-
occurrence? (Heinrich, 2009), i.e., based on how 
often terms co-occur in different contexts1. This 
unfortunately results in many ?non-specific? terms 
being pulled and clustered. We employ seed sets to 
address this issue by ?guiding? the model to group 
semantically related terms in the same aspect thus 
making the aspect more specific and related to the 
seeds (which reflect the user needs). For easy 
presentation, we will use aspect to mean aspect 
category from now on. We replace the multinomial 
distribution over words for each aspect (as in 
ASMs) with a special two-level tree structured 
distribution. The generative process of ASMs 
assumes that each vocabulary word is 
independently (i.e., not dependent upon other 
word-aspect association) and equally probable to 
be associated with any aspect. Due to higher-order 
co-occurrences, we find conceptually different 
terms yet related in contexts (e.g., in hotel domain 
terms like stain, shower, walls in aspect 
                                                          1 w1 co-occurring with w2 which in turn co-occurs with w3 denotes a 
second-order co-occurrence between w1 and w3. 
Maintenance; bed, linens, pillows in aspect 
Cleanliness) equally probable of emission for any 
aspect. Figure 1(a) shows an example tree. Upon 
adding the seed sets {bed, linens, pillows} and 
{staff, service}, the prior structure now changes to 
the correlated distribution in Figure 1 (b). Thus, 
each aspect has a top level distribution over non-
seed words and seed sets. Each seed set in each 
aspect further has a second level distribution over 
seeds in that seed set. The aspect term (word) 
emission now requires two steps: first sampling at 
level one to obtain a non-seed word or a seed set. If 
a non-seed word is sampled we emit it else we 
further sample at the second seed set level and emit 
a seed word. This ensures that seed words together 
have either all high or low aspect associations. 
Furthermore, seed sets preserve conjugacy between 
related concepts and also shape more specific 
aspects by clustering based on higher order co-
occurrences with seeds rather than only with 
standard one level multinomial distribution over 
words (or terms) alone. 
3.1 SAS Model 
We now present the proposed Seeded Aspect and 
Sentiment model (SAS). Let ???? denote the entries in our vocabulary where ??is the number of 
unique non-seed terms. Let there be ? seed sets 
?????? where each seed set ?? is a group of semantically related terms. Let ??????? ,????????  denote T aspect and aspect specific sentiment 
models. Also let ??,? denote the aspect specific distribution of seeds in the seed set ??. Following the approach of (Zhao et al, 2010), we too assume 
that a review sentence usually talks about one 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Prior structure:  (a) Standard ASMs, (b) Two-level tree structured distribution. Graphical models in plate 
notation: (c) SAS and (d) ME-SAS. 
a? a? a? 
D 
aw 
ar 
au Nd, s 
az a? 
Sd 
a?A a?O 
T 
C a? a?
A 
T 
a?O 
a? a?A a?O 
T 
C a? a?
A 
T 
a?O 
a? 
a? 
a? 
a? 
D 
az 
a? 
Sd 
aw 
ar 
au Nd, s a
x 
(a) 
(b) (d) (c) 
Seeds: 
{staff, service} 
{linens, bed, pillows} 
room stain bed staff linens service shower pillows walls friendly
?A 
staff service bed linens pillows 
?A 
stain walls            shower            friendly room ? ?
341
aspect. A review document ???? comprises of ?? sentences and each sentence ? ? ?? has ??,?words. Also, let ?????? denote the sentence ? of document ?. To distinguish between aspect and sentiment 
terms, we introduce an indicator (switch) variable 
??,?,? ? ? ??, ??? for the ???term of ??????,???,?,?. Further, let ??,? denote the distribution of aspects and sentiments in ??????. The generative process of the SAS model (see Figure 1(c)) is given by: 
1. For each aspect ?? ? ?1, ? , ??: 
i. Draw    ????~???????? ii. Draw a distribution over terms and seed sets ????~???????? a) For each seed set ? ? ???, ? , ??? Draw a distribution over seeds ??,??~??????? 
2. For each (review) document ? ? ?1, ? , ??: 
i. Draw ???~??????? ii. For each sentence ? ? ?1, ? , ???: a) Draw ??,??~????????? 
b) Draw ??,??~???????? 
c) For each term ??,?,??where??? ? ?1, ? , ??,??: 
I. Draw ??,?,??~?????????????,??, ??,?,? ? ? ??, ??? 
II. if ??,?,? ? ? ?? // ??,?,? is a sentiment 
Emit ??,?,??~?????????,?? ? 
else // ??,?,? ? ? ???, ??,?,? is an aspect 
A. Draw ??,?,??~?????????,?? ? 
B. if ??,?,? ? ? ? // non-seed term 
Emit ??,?,? ? ???,?,? 
else // ??,?,? is some seed set index say ??,?,? 
Emit ??,?,??~????,??,??,?,? 
We employ collapsed Gibbs sampling (Griffiths 
and Steyvers, 2004) for posterior inference. As ? 
and ? are at different hierarchical levels, we derive 
their samplers separately as follows: 
????,? ? ??????,?, ???,?, ???,??, ???,?? ?
????,??? ? ???
? ???,??? ??,? ? ???
? 
????,???,??????
????,???,???,????
?? ? ?
????,?,???,? ????
????,?,???,? ??,?????
???? ?
??,?????.??,???
??,???????.??,????
  (1) 
????,?,? ? ??????,?, ???,?,?, ???,?,??, ???,?,?, ??,? ? ?, ??,?,? ? ?? ? 
??,?? ??,?,????
??,???? ??,?,??|??? ??? |??
? ??,?
?
??,?,????
??,?? ??,?,????????,?? ??,?,????
    (2) 
????,?,? ? ??? ? ? ?
?
??
?
?
??
?
? ??,?,?
?,?
??,?,???
??,?,????,? ??,?,??|??|?
? ??,?? ?????,???? ???????? ??
??,?? ??,?,????
??,?? ??,?,????????,?
?
??,?,????
??? ; ??? ? ??
??,??,?????
??,????,??????????
? ??,?
?
??,?,????
??,?? ??,?,????????,?
?
??,?,????
; ??, ? ? ??
 (3) 
where ????? ? ?? ???????????????????? ?????????????? ?is the multinomial Beta function. ??,??  is the number of times term ? was 
assigned to aspect ? as an opinion/sentiment word. 
??,??,? is the number of times non-seed term ? ? ?? was assigned to aspect ? as an aspect. ??,?,??,?  is the number of times seed term ? ? ? ?? was assigned to aspect ? as an aspect. ??,?????. is the number of sentences in document ? that were assigned to 
aspect ?. ??,??  and ??,??  denote the number of terms in ?????? that were assigned to aspects and opinions respectively. ??,??  is the number of times any term of seed set ?? was assigned to aspect ?. Omission of a latter index denoted by [] in the above notation 
represents the corresponding row vector spanning 
over the latter index. For example, ??,???,? ?
???,????,? , ? , ??,????,? ? and (?) denotes the marginalized sum over the latter index. The subscript ??, ? 
denotes the counts excluding assignments of all 
terms in ??????. ??, ?, ? denotes counts excluding 
??,?,?.We perform hierarchical sampling. First, an 
aspect is sampled for each sentence ??,? using Eq. (1). After sampling the aspect, we sample ??,?,?. 
The probability of ??,?,? being an opinion or 
sentiment term, ????,?,? ? ??? is given by Eq. (2). 
However, for ????,?,? ? ??? we have two cases: (a) 
the observed term ? ? ??,?,? ? ?? or (b) does not belong to any seed set, ??, ? ? ??, i.e., w is an non-seed term. These cases are dealt in Eq. (3). 
Asymmetric Beta priors: Hyper-parameters ?, ?O, 
?A are not very sensitive and the heuristic values 
suggested in (Griffiths and Steyvers, 2004) usually 
hold well in practice (Wallach et al 2009). 
However, the smoothing hyper-parameter ? 
(Figure 1(c)) is crucial as it governs the aspect or 
sentiment switch. Essentially, ??,?~????????? is the probability of emitting an aspect term2 in ?????? with concentration parameter ? and base measure 
?? ? ???, ???. Without any prior belief, uniform base measures ?? ? ?? ? 0.5 are used resulting in symmetric Beta priors. However, aspects are often 
more probable than sentiments in a sentence (e.g., 
?The beds, sheets, and bedding were dirty.?). Thus, 
it is more principled to employ asymmetric priors. 
Using a labeled set of sentences, ????????, where we know the per sentence probability of aspect 
emission (??,?), we can employ the method of moments to estimate the smoothing hyper-
parameter ? ? ???, ???: 
?? ? ? ???????? ? 1? , ?? ? ?? ?
?
? ? 1? ; ?? ? ????,??, ? ? ??????,?? 
(4) 
                                                          
2 ??,?,?~?????????????,??. ??,? , 1 ? ??,? are the success and failure 
probability of emitting an aspect/sentiment term. 
342
3.2 ME-SAS Model 
We can further improve SAS by employing 
Maximum Entropy (Max-Ent) priors for aspect and 
sentiment switching. We call this new model ME-
SAS. The motivation is that aspect and sentiment 
terms play different syntactic roles in a sentence. 
Aspect terms tend to be nouns or noun phrases 
while sentiment terms tend to be adjectives, 
adverbs, etc. POS tag information can be elegantly 
encoded by moving ??,? to the term plate (see Figure 1(d)) and drawing it from a Max-
Ent??,??,?; ?? model. Let  
???,?,??????????? ? ??????,?,???, ?????,?,? , ?????,?,???, ??,?,? ? 1, ??,?,?, ??,?,? ?
1?  denote the feature vector associated with ??,?,?? encoding lexical and POS features of the previous, 
current and next term. Using a training data set, we 
can learn Max-Ent priors. Note that unlike 
traditional Max-Ent training, we do not need 
manually labeled data for training (see Section 4 
for details). For ME-SAS, only the sampler for the 
switch variable r changes as follows: 
????,?,? ? ??????,?, ???,?,?, ???,?,??, ???,?,?, ??,? ? ?, ??,?,? ? ?? ? 
??,?? ??,?,????
??,???? ??,?,??|??? ??? |??
? ????? ???????,?,?,??????? ?? ????? ???????,?,?,?????? ??????,???       (5) 
????,?,? ? ??? ? ? ?
?
??
?
??
? ??,?,?
?,?
??,?,???
??,?,????,? ??,?,??|??|?
? ??,?? ?????,???? ???????? ??
????? ???????,?,?,??????? ?
? ????? ???????,?,?,?????? ??????,???
??? ; ??? ? ??
??,??,?????
??,????,??????????
? ????? ???????,?,?,??????? ?? ????? ???????,?,?,?????? ??????,??? ; ??, ? ? ??
   (6) 
where ???? are the parameters of the learned Max-Ent model corresponding to the ? binary feature 
functions ???? of Max-Ent. 
4 Experiments 
This section evaluates the proposed models. Since 
the focus in this paper is to generate high quality 
aspects using seeds, we will not evaluate 
sentiments although both SAS and ME-SAS can 
also discover sentiments. To compare the 
performance with our models, we use two existing 
state-of-the-art models, ME-LDA (Zhao et al 
2010) and DF-LDA (Andrzejewski et al, 2009). 
As discussed in Section 2, there are two main 
flavors of aspect and sentiment models. The first 
flavor does not separate aspect and sentiment, and 
the second flavor uses a switch to perform the 
separation. Since our models also perform a 
switch, it is natural to compare with the latter 
flavor, which is also more advanced. ME-LDA is 
the representative model in this flavor. DF-LDA 
adds constraints to LDA. We use our seeds to 
generate constraints for DF-LDA. While ME-LDA 
cannot consider constraints, DF-LDA does not 
separate sentiments and aspects. Apart from other 
modeling differences, our models can do both, 
which enable them to produce much better results. 
Dataset and Settings: We used hotel reviews from 
tripadvisor.com. Our corpus consisted of 101,234 
reviews and 692,783 sentences. Punctuations, stop 
words 3, and words appearing less than 5 times in 
the corpus were removed. 
For all models, the posterior inference was 
drawn after 5000 Gibbs iterations with an initial 
burn-in of 1000 iterations. For SAS and ME-SAS, 
we set ? = 50/T, ?A = ?O = 0.1 as suggested in 
(Griffiths and Steyvers, 2004). To make the seeds 
more effective, we set the seed set word-
distribution hyper-parameter ? to be much larger 
than ?A, the hyper-parameter for the distribution 
over seed sets and aspect terms. This results in 
higher weights to seeded words which in turn 
guide the sampler to cluster relevant terms better. 
A more theoretical approach would involve 
performing hyper-parameter estimation (Wallach 
et al, 2009) which may reveal specific properties 
of the dataset like the estimate of ? (indicating how 
different documents are in terms of their latent 
semantics), ? (suggesting how large the groups of 
frequently appearing aspect and sentiment terms 
are) and ? (giving a sense of which and how large 
groupings of seeds are good). These are interesting 
questions and we defer it to our future work. In this 
work, we found that the setting ? = 250, a larger 
value compared to ?A, produced good results. 
For SAS, the asymmetric Beta priors were 
estimated using the method of moments (Section 
3.1). We sampled 500 random sentences from the 
corpus and for each sentence identified the aspects. 
We thus computed the per-sentence probability of 
aspect emission (??,?) and used Eq. (4) to compute the final estimates, which give ?a = 2.35, ?b = 3.44.  To learn the Max-Ent parameters ? of ME-SAS, 
we used the sentiment lexicon 4 of (Hu and Liu, 
2004) to automatically generate training data (no 
manual labeling). We randomly sampled 1000 
terms from the corpus which have appeared at least 
                                                          3 http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-
list/english.stop 4 http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar 
343
20 times (to ensure that the training set is 
reasonably representative of the corpus). Of those 
1000 terms if they appeared in the sentiment 
lexicon, they were treated as sentiment terms, else 
aspect terms. Clearly, labeling words not in the 
sentiment lexicon as aspect terms may not always 
be correct. Even with this noisy automatically-
labeled data, the proposed models can produce 
good results. Since ME-LDA used manually 
labeled training data for Max-Ent, we again 
randomly sampled 1000 terms from our corpus 
appearing at least 20 times and labeled them as 
aspect terms or sentiment terms, so this labeled 
data clearly has less noise than our automatically 
labeled data. For both ME-SAS and ME-LDA we 
used the corresponding feature vector of each 
labeled term (in the context of sentences where it 
occurs) to train the Max-Ent model. As DF-LDA 
requires must-link and cannot-link constraints, we 
used our seed sets to generate intra-seed set must-
link and inter-seed set cannot-link constraints. For 
its hyper-parameters, we used the default values in 
the package5 (Andrzejewski et al, 2009). 
Setting the number of topics/aspects in topic 
models is often tricky as it is difficult to know the 
                                                          
5 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html 
exact number of topics that a corpus has. While 
non-parametric Bayesian approaches (Teh et al, 
2006) do exist for estimating the number of topics, 
T, they strongly depend on the hyper-parameters 
(Heinrich, 2009). As we use fixed hyper-
parameters, we do not learn T from Bayesian non-
parametrics. We used 9 major aspects (T = 9) 
based on commonsense knowledge of what people 
usually talk about hotels and some experiments. 
These are Dining, Staff, Maintenance, Check In, 
Cleanliness, Comfort, Amenities, Location and 
Value for Money (VFM). However, it is important 
to note that the proposed models are flexible and 
do not need to have seeds for every aspect/topic. 
Our experiments simulate the real-life situation 
where the user may not know all aspects or have 
no seeds for some aspects. Thus, we provided 
seeds only to the first 6 of the 9 aspects/topics. We 
will see that without seeds for all aspects, our 
models not only can improve the seeded aspects 
but also improve the non-seeded aspects. 
4.1 Qualitative Results  
This section shows some qualitative results to give 
an intuitive feeling of the results from different 
models. Table 1 shows the aspect terms and 
sentiment terms discovered by the 4 models for 
Aspect 
(seeds) 
ME-SAS SAS ME-LDA DF-LDA
Aspect Sentiment Aspect Sentiment Aspect Sentiment Topic
 Staff  (staff service waiter hospitality upkeep)  
attendantmanager waitress maintenance bartender waiters housekeepingreceptionist waitstaff janitor 
friendly attentive polite nice 
clean pleasant slow courteous rude professional 
attendantwaiter waitress manager maintenance 
helpful waiters housekeepingreceptionist 
polite
friendlynice dirty comfortablenice 
clean polite 
extremely courteous efficient
staffmaintenance room upkeep 
linens room-service receptionist 
wait pillow waiters
friendly nice courteous extremely nice 
clean polite 
little helpful 
better  
stafffriendly helpful beds front room comfortable large receptionist housekeeping
 Cleanliness 
 (curtains restroom floor beds cleanliness) 
carpets hall towels bathtub couch mattress linens wardrobe spa pillow 
clean dirty comfortable fresh wet filthy extra stain 
front worn 
hallcarpets towels pillow stain mattress 
filthy linens 
interior bathtub
cleandirty fresh old nice good 
enough new 
front friendly
cleanlinessfloor carpets bed lobby bathroom 
staff closet spa d?cor
clean good dirty 
hot large nice 
fresh thin new little 
cleanpool beach carpets parking bed bathroom nice comfortable suite
 Comfort 
 (comfort mattress furniture couch pillows) 
bedding bedcover sofa linens bedroom suites d?cor comforter blanket futon 
comfortable clean soft nice uncomfortable spacious hard comfy dirty quiet 
bedlinens sofa bedcover 
hard bedroom privacy double comfy futon
nicedirty comfortable large clean 
best spacious 
only big 
extra
bedmattress suites furniture 
lighting d?cor room bedroom hallway carpet
great clean awesome dirty best comfortable soft nice only extra 
bedmattress 
nice stay lighting lobby comfort room dirty sofa
Table 1: Top ranked aspect and sentiment words in three aspects (please see the explanation in Section 4.1).?
344
three aspects. Due to space limitations, we are 
unable to show all 6 aspects for which we have 
seeds. Since DF-LDA cannot separate aspects and 
sentiments, we only show its topics (aspects). Red 
(bold) colored words show semantic clustering 
errors or inappropriate terms for different groups.  
It is important to note that we judge the results 
based on how they are related to the user seeds 
(which represent the user need). The judgment is to 
some extent subjective. What we reported here are 
based on our judgments what are appropriate and 
what are not for each aspect. For SAS, ME-SAS 
and ME-LDA, we mark sentiment terms as errors 
when they are grouped under aspects as these 
models are supposed to separate sentiments and 
aspects. For DF-LDA, the situation is different as it 
is not meant to separate sentiment and aspect 
terms, we use red italic font to indicate those 
adjectives which are aspect specific adjectives (see 
more discussion below). Our judgment may be 
slightly unfair to ME-LDA and DF-LDA as their 
results may make sense in some other ways. 
However, that is precisely the purpose of this 
work, to produce results that suit the user?s need 
rather than something generic. 
We can see from Table 1 that ME-SAS performs 
the best. Next in order are SAS, ME-LDA, and 
DF-LDA. We see that only providing a handful of 
seeds (5) for the aspect Staff, ME-SAS can 
discover highly specific words like manager, 
attendant, bartender, and janitor. By specific, we 
mean they are highly related to the given seeds. 
While SAS also discovers specific words 
benefiting from seeds, relying on Beta priors for 
aspect and sentiment switching was less effective. 
Next in performance is ME-LDA which although 
produces reasonable results in general, several 
aspect terms are far from what the user wants 
based on the seeds, e.g., room, linens, wait, pillow. 
Finally, we observe that DF-LDA does not perform 
well either. One reason is that it is unable to 
separate aspects and sentiments. Although 
encoding the intra-seed set must-link and inter-
seed set cannot-link constraints in DF-LDA 
discovers some specific words as ME-SAS, they 
are much lower in the ranked order and hence do 
not show up in the top 10 words in Table 1. As 
DF-LDA is not meant to perform extraction and to 
group both aspect and sentiment terms, we relax 
the errors of DF-LDA due to correct aspect 
specific sentiments (e.g., friendly, helpful for Staff 
are correct aspect specific sentiments, but still 
regard incorrect sentiments like front, comfortable, 
large as errors) placed in aspect models. We call 
this model DF-LDA-Relaxed. 
4.2 Quantitative Results   
Topic models are often evaluated quantitatively 
using perplexity and likelihood on held-out test 
data (Blei et al, 2003). However, perplexity does 
not reflect our purpose since our aim is not to 
predict whether an unseen document is likely to be 
a review of some particular aspect. Nor are we 
trying to evaluate how well the unseen review data 
fits our seeded models. Instead our focus is to 
evaluate how well our learned aspects perform in 
clustering specific terms guided by seeds. So we 
directly evaluate the discovered aspect terms. Note 
again we do not evaluate sentiment terms as they 
are not the focus of this paper 6. Since aspects 
produced by the models are rankings and we do 
not know the number of correct aspect terms, a 
natural way to evaluate these rankings is to use 
precision @ n (or p@n), where n is a rank position. 
Varying number of seeds: Instead of a fixed 
number of seeds, we want to see the effect of the 
number of seeds on aspect discovery. Table 2 
reports the average p@n vs. the number of seeds. 
The average is a two-way averaging. The first 
average was taken over all combinations of actual 
seeds selected for each aspect, e.g., when the 
number of seeds is 3, out of the 5 seeds in each 
aspect, all ?53? combinations of seeds were tried and the results averaged. The results were further 
averaged over p@n for 6 aspects with seeds. We 
start with 2 seeds and progressively increase them 
to 5. Using only 1 seed per seed set (or per aspect) 
has practically no effect because the top level 
distribution ?? encodes which seed sets (and non-
seed words) to include; the lower-level distribution 
? constrains the probabilities of the seed words to 
be correlated for each of the seed sets. Thus, 
having only one seed per seed set will result in 
sampling that single word whenever that seed set is 
chosen which will not have the effect of correlating 
seed words so as to pull other words based on co-
occurrence with constrained seed words. From 
Table 2, we can see that for all models p@n 
progressively improves as the number of seeds 
increases. Again ME-SAS performs the best 
followed by SAS and DF-LDA. 
                                                          6 A qualitative evaluation of sentiment extraction based on Table 1 yields 
the following order: ME-SAS, SAS, ME-LDA. 
345
Effect of seeds on non-seeded aspects: Here we 
compare all models aspect wise and see the results 
of seeded models SAS and ME-SAS on non-
seeded aspects (Table 3).  Shaded cells in Table 3 
give the p@n values for DF-LDA, DF-LDA-
Relaxed, SAS, and ME-SAS on three non-seeded 
aspects (Amenities, Location, and VFM)7.  
We see that across all the first 6 aspects with (5) 
seeds ME-SAS outperforms all other models by 
large margins in all top 3 ranked buckets p@10, 
p@20 and p@30. Next in order are SAS, ME-LDA 
and DF-LDA. For the last three aspects which did 
not have any seed guidance, we find something 
interesting. Seeded models SAS and especially 
ME-SAS result in improvements of non-seeded 
aspects too. This is because as seeds facilitate 
clustering specific and appropriate terms in seeded 
aspects, which in turn improves precision on non-
seeded aspects. This phenomenon can be clearly 
seen in Table 1. In aspect Staff of ME-LDA, we 
find pillow and linens being clustered. This is not a 
?flaw? of the model per se, but the point here is 
pillow and linens happen to co-occur many times 
with other words like maintenance, staff, and 
upkeep because ?room-service? generally includes 
staff members coming and replacing linens and 
pillow covers. Although pillow and linens are 
related to Staff, strictly speaking they are 
semantically incorrect because they do not 
represent the very concept ?Staff? based on the 
seeds (which reflect the user need). Presence of 
                                                          
7 Note that Tables 2 and 3 are different runs of the model. The variations in the 
results are due to the random initialization of the Gibbs sampler. 
seed sets in SAS and ME-SAS result in pulling 
such words as linens and pillow (due to seeds like 
beds and cleanliness in the aspect Cleanliness) and 
ranking them higher in the aspect Cleanliness (see 
Table 1) where they make more sense than Staff. 
Lastly, we also note that the improvements in non-
seeded aspects are more pronounced for ME-SAS 
than SAS as SAS encounters more switching errors 
which counters the improvement gained by seeds.  
In summary, the averages over all aspects (Table 
3 last row) show that the proposed seeded models 
SAS and ME-SAS outperform ME-LDA, DF-LDA 
and even DF-LDA-Relaxed considerably. 
5 Conclusion 
This paper studied the issue of using seeds to 
discover aspects in an opinion corpus. To our 
knowledge, no existing work deals with this 
problem. Yet, it is important because in practice 
the user often has something in mind to find. The 
results obtained in a completely unsupervised 
manner may not suit the user?s need. To solve this 
problem, we proposed two models SAS and ME-
SAS which take seeds reflecting the user needs to 
discover specific aspects. ME-SAS also does not 
need any additional help from the user in its Max-
Ent training. Our results showed that both models 
outperformed two state-of-the-art existing models 
ME-LDA and DF-LDA by large margins. 
Acknowledgments 
This work is supported in part by National Science 
Foundation (NSF) under grant no. IIS-1111092.  
No. of Seeds DF-LDA DF-LDA-Relaxed SAS ME-SAS P@10 P@20 P@30 P@10 P@20 P@30 P@10 P@20 P@30 P@10 P@20 P@30 
2 0.51 0.53 0.49 0.67 0.69 0.70 0.69 0.71 0.67 0.74 0.72 0.70 
3 0.53 0.54 0.50 0.71 0.70 0.71 0.71 0.72 0.70 0.78 0.75 0.72 
4 0.57 0.56 0.53 0.73 0.73 0.73 0.75 0.74 0.73 0.83 0.79 0.76 
5 0.59 0.57 0.54 0.75 0.74 0.75 0.77 0.76 0.74 0.86 0.81 0.77 
Table 2: Average p@n of the seeded aspects with the no. of seeds. 
Aspect ME-LDA DF-LDA DF-LDA-Relaxed SAS ME-SAS
P@10 P@20 P@30 P@10 P@20 P@30 P@10 P@20 P@30 P@10 P@20 P@30 P@10 P@20 P@30
Dining 0.70 0.65 0.67 0.50 0.60 0.63 0.70 0.70 0.70 0.80 0.75 0.73 0.90 0.85 0.80
Staff 0.60 0.70 0.67 0.40 0.65 0.60 0.60 0.75 0.67 0.80 0.80 0.70 1.00 0.90 0.77
Maintenance 0.80 0.75 0.73 0.40 0.55 0.56 0.60 0.70 0.73 0.70 0.75 0.76 0.90 0.85 0.80
Check In 0.70 0.70 0.67 0.50 0.65 0.60 0.80 0.75 0.70 0.80 0.70 0.73 0.90 0.80 0.76
Cleanliness 0.70 0.75 0.67 0.70 0.70 0.63 0.70 0.75 0.70 0.80 0.75 0.70 1.00 0.85 0.83
Comfort 0.60 0.70 0.63 0.60 0.65 0.50 0.70 0.75 0.63 0.60 0.75 0.67 0.90 0.80 0.73
Amenities 0.80 0.80 0.67 0.70 0.65 0.53 0.90 0.75 0.73 0.90 0.80 0.70 1.00 0.85 0.73
Location 0.60 0.70 0.63 0.50 0.60 0.56 0.70 0.70 0.67 0.60 0.70 0.63 0.70 0.75 0.67
VFM 0.50 0.55 0.50 0.40 0.50 0.46 0.60 0.60 0.60 0.50 0.50 0.50 0.60 0.55 0.53
Avg. 0.67 0.70 0.65 0.52 0.62 0.56 0.70 0.72 0.68 0.72 0.72 0.68 0.88 0.80 0.74
Table 3: Effect of performance on seeded and non-seeded aspects (5 seeds were used for the 6 seeded aspects).?
346
References  
Andrzejewski, D., Zhu, X. and Craven, M. 2009. 
Incorporating domain knowledge into topic modeling 
via Dirichlet forest priors. Proceedings of 
International Conference on Machine Learning 
(ICML). 
 Andrzejewski, D., Zhu, X. and Craven, M. and Recht, 
B. 2011. A framework for incorporating general 
domain knowledge into latent Dirichlet alocation 
using first-order logic. Proceedings of the 22nd 
International Joint Conferences on Artificial 
Intelligence (IJCAI).  
Blair-Goldensohn, S., Hannan, K., McDonald, R., 
Neylon, T., Reis, G. A. and Reynar, J. 2008. Building 
a sentiment summarizer for local service reviews. 
Proceedings of WWW-2008 workshop on NLP in the 
Information Explosion Era. 
Blei, D., Ng, A. and Jordan, M. 2003. Latent dirichlet 
allocation. The Journal of Machine Learning 
Research 3: 993-1022. 
Blei D. and McAuliffe, J. 2007. Supervised topic 
models. Neural Information Processing Systems 
(NIPS). 
Branavan, S., Chen, H., Eisenstein J. and Barzilay, R. 
2008. Learning document-level semantic properties 
from free-text annotations. Proceedings of the 
Annual Meeting of the Association for 
Computational Linguistics (ACL). 
Brody, S. and Elhadad, S. 2010. An Unsupervised 
Aspect-Sentiment Model for Online Reviews. 
Proceedings of The 2010 Annual Conference of the 
North American Chapter of the ACL (NAACL). 
Carenini, G., Ng, R. and Zwart, E. 2005. Extracting 
knowledge from evaluative text. Proceedings of 
Third Intl. Conf. on Knowledge Capture (K-CAP-
05). 
Chang, J., Boyd-Graber, J., Wang, C.  Gerrish, S. and 
Blei, D. 2009. Reading tea leaves: How humans 
interpret topic models. In Neural Information 
Processing Systems (NIPS). 
Choi, Y. and Cardie, C. 2010. Hierarchical sequential 
learning for extracting opinions and their attributes. 
Proceedings of Annual Meeting of the Association 
for Computational (ACL). 
Griffiths, T. and Steyvers, M. 2004. Finding scientific 
topics. Proceedings of National Academy of Sciences 
(PNAS). 
Guo, H., Zhu, H., Guo, Z., Zhang, X. and Su, X. 2009. 
Product feature categorization with multilevel latent 
semantic association. Proceedings of ACM 
International Conference on Information and 
Knowledge Management (CIKM). 
Heinrich, G. 2009. A Generic Approach to Topic 
Models. Proceedings of the European Conference on 
Machine Learning and Principles and Practice of 
Knowledge Discovery in Databases (ECML/PKDD). 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. Proceedings of Conference on Uncertainty 
in Artificial Intelligence (UAI). 
Hu, Y., Boyd-Graber, J. and Satinoff, B. 2011. 
Interactive topic modeling. Proceedings of Annual 
Meeting of the Association for Computational 
Linguistics (ACL), 2011. 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. International Conference on 
Knowledge Discovery and Data Mining (ICDM). 
Jakob, N. and Gurevych, I. 2010. Extracting Opinion 
Targets in a Single-and Cross-Domain Setting with 
Conditional Random Fields. Proceedings of 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP). 
Jin, W. and Ho, H. 2009. A novel lexicalized HMM-
based learning framework for web opinion mining. 
Proceedings of International Conference on Machine 
Learning (ICML). 
Jo, Y. and Oh, A. 2011. Aspect and sentiment 
unification model for online review analysis. ACM 
Conference in Web Search and Data Mining 
(WSDM). 
Kobayashi, N., Inui, K. and Matsumoto, K. 2007. 
Extracting aspect-evaluation and aspect-of relations 
in opinion mining. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL). 
Ku, L., Liang, Y. and Chen, H. 2006. Opinion 
extraction, summarization and tracking in news and 
blog corpora. Proceedings of AAAI Symposium on 
Computational Approaches to Analyzing Weblogs 
(AAAI-CAAW'06). 
Li, F., Han, C., Huang, M., Zhu, X. Xia, Y., Zhang, S.  
and Yu, H. 2010. Structure-aware review mining and 
summarization. International Conference on 
Computational Linguistics (COLING). 
Lin, C. and He, Y. 2009. Joint sentiment/topic model for 
sentiment analysis. Proceedings of ACM 
International Conference on Information and 
Knowledge Management (CIKM). 
Liu, B. 2012. Sentiment Analysis and Opinion Mining. 
347
Morgan & Claypool publishers (to appear in June 
2012).  
Liu, B, M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and comparing opinions on the web. 
Proceedings of International Conference on World 
Wide Web (WWW).  
Lu, Y., Zhai, C. and Sundaresan, N. 2009. Rated aspect 
summarization of short comments. Proceedings of 
International Conference on World Wide Web 
(WWW). 
Lu, Y. and Zhai, C. 2008. Opinion Integration Through 
Semi-supervised Topic Modeling. Proceedings of the 
17th International World Wide Web Conference 
(WWW). 
Ma, T. and Wan, X. 2010. Opinion target extraction in 
Chinese news comments. Proceedings of Coling 
2010 Poster Volume (COLING). 
Mei, Q., Ling, X., Wondra, M., Su, H. and Zhai, C. 
2007. Topic sentiment mixture: modeling facets and 
opinions in weblogs. Proceedings of International 
Conference on World Wide Web (WWW). 
Moghaddam, S. and Ester, M. 2011. ILDA: 
interdependent LDA model for learning latent 
aspects and their ratings from online product reviews. 
Proceedings of the Annual ACM SIGIR International 
conference on Research and Development in 
Information Retrieval (SIGIR). 
Pang, B. and Lee, L. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval. 
Popescu, A. and Etzioni, O. 2005. Extracting product 
features and opinions from reviews. Proceedings of 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP). 
Qiu, G., Liu, B., Bu, J. and Chen, C. 2011. Opinion 
Word Expansion and Target Extraction through 
Double Propagation. Computational Linguistics. 
Ramage, D., Hall, D., Nallapati, R. and Manning, C. 
2009. Labeled LDA: a supervised topic model for 
credit attribution in multi-labeled corpora. 
Proceedings of the Conference on Empirical Methods 
in Natural Language Processing (EMNLP). 
Sauper, C., Haghighi, A. and Barzilay, R. 2011. Content 
models with attitude. Proceedings of the 49th Annual 
Meeting of the Association for Computational 
Linguistics (ACL). 
Somasundaran, S. and Wiebe, J. 2009. Recognizing 
stances in online debates, Proceedings of the 47th 
Annual Meeting of the ACL and the 4th IJCNLP of 
the AFNLP. 
Teh, Y., Jordan, M., Beal, M. and Blei, D. 2006. 
Hierarchical Dirichlet Processes. In Journal of the 
American Statistical Association (JASA). 
Titov, I. and McDonald, R. 2008. Modeling online 
reviews with multi-grain topic models. Proceedings 
of International Conference on World Wide Web 
(WWW). 
Wallach, H., Mimno, D. and McCallum, A. 2009. 
Rethinking LDA: Why priors matter. In Neural 
Information Processing Systems (NIPS). 
Wang, H., Lu, Y. and Zhai, C. 2010. Latent aspect 
rating analysis on review text data: a rating 
regression approach. Proceedings of ACM SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining (KDD). 
Wu, Y., Zhang, Q., Huang, X. and Wu, L. 2009. Phrase 
dependency parsing for opinion mining. Proceedings 
of Conference on Empirical Methods in Natural 
Language Processing (EMNLP). 
Yi, J., Nasukawa, T., Bunescu, R. and Niblack, W. 
2003. Sentiment analyzer: Extracting sentiments 
about a given topic using natural language processing 
techniques. Proceedings of IEEE International 
Conference on Data Mining (ICDM). 
Yu, J., Zha, Z. J., Wang, M. and Chua, T. S. 2011. 
Aspect ranking: identifying important product 
aspects from online consumer reviews. Proceedings 
of the 49th Annual Meeting of the Association for 
Computational Linguistics, Association for 
Computational Linguistics (ACL). 
Zhai, Z., Liu, B. Xu, H. and Jia, P. 2010. Grouping 
Product Features Using Semi-Supervised Learning 
with Soft-Constraints. Proceedings of International 
Conference on Computational Linguistics 
(COLING). 
Zhai, Z., Liu, B. Xu, H. and Jia, P. 2011. Constrained 
LDA for Grouping Product Features in Opinion 
Mining. Proceedings of Pacific-Asia Conference on 
Knowledge Discovery and Data Mining (PAKDD). 
Zhao, W., Jiang, J., Yan, Y. and Li, X. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-LDA 
hybrid. Proceedings of Conference on Empirical 
Methods in Natural Language Processing (EMNLP). 
Zhuang, L., Jing, F. and Zhu, X. 2006. Movie review 
mining and summarization. Proceedings of 
International Conference on Information and 
Knowledge Management (CIKM). 
 
348
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671?681,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discovering User Interactions in Ideological Discussions 
Arjun Mukherjee     Bing Liu 
Department of Computer Science 
University of Illinois at Chicago 
arjun4787@gmail.com  liub@cs.uic.edu 
 
Abstract 
Online discussion forums are a popular 
platform for people to voice their opinions on 
any subject matter and to discuss or debate 
any issue of interest. In forums where users 
discuss social, political, or religious issues, 
there are often heated debates among users or 
participants. Existing research has studied 
mining of user stances or camps on certain 
issues, opposing perspectives, and contention 
points. In this paper, we focus on identifying 
the nature of interactions among user pairs. 
The central questions are: How does each 
pair of users interact with each other? Does 
the pair of users mostly agree or disagree? 
What is the lexicon that people often use to 
express agreement and disagreement? We 
present a topic model based approach to 
answer these questions. Since agreement and 
disagreement expressions are usually multi-
word phrases, we propose to employ a 
ranking method to identify highly relevant 
phrases prior to topic modeling. After 
modeling, we use the modeling results to 
classify the nature of interaction of each user 
pair. Our evaluation results using real-life 
discussion/debate posts demonstrate the 
effectiveness of the proposed techniques.  
1 Introduction 
Online discussion/debate forums allow people 
with common interests to freely ask and answer 
questions, to express their views and opinions on 
any subject matter, and to discuss issues of 
common interest. A large part of such 
discussions is about social, political, and 
religious issues. On such issues, there are often 
heated discussions/debates, i.e., people agree or 
disagree and argue with one another. Such 
ideological discussions on a myriad of social and 
political issues have practical implications in the 
fields of communication and political science as 
they give social scientists an opportunity to study 
real-life discussions/debates of almost any issue 
and analyze participant behaviors in a large scale. 
In this paper, we present such an application, 
which aims to perform fine-grained analysis of 
user-interactions in online discussions.  
There have been some related works that focus 
on discovering the general topics and ideological 
perspectives in online discussions (Ahmed and 
Xing, 2010), placing users in support/oppose 
camps (Agarwal et al, 2003), and classifying 
user stances (Somasundaran and Wiebe, 2009). 
However, these works are at a rather coarser 
level and have not considered more fine-grained 
characteristics of debates/discussions where users 
interact with each other by quoting/replying each 
other to express agreement or disagreement and 
argue with one another. In this work, we want to 
mine the following information: 
1. The nature of interaction of each pair of users 
or participants who have engaged in the 
discussion of certain issues, i.e., whether the 
two persons mostly agree or disagree with 
each other in their interactions. 
2. What language expressions are often used to 
express agreement (e.g., ?I agree? and ?you?re 
right?) and disagreement (e.g., ?I disagree? 
and ?you speak nonsense?).  
We note that although agreement and 
disagreement expressions are distinct from 
traditional sentiment expressions (words and 
phrases) such as good, excellent, bad, and 
horrible, agreement and disagreement clearly 
express a kind of sentiment as well. They are 
usually emitted during interactive exchanges of 
arguments in ideological discussions. This idea 
prompted us to introduce the concept of AD-
sentiment. We define the polarity of agreement 
expressions as positive and the polarity of 
disagreement expressions as negative. We refer 
agreement and disagreement expressions as AD-
sentiment expressions, or AD-expressions for 
short. AD-expressions are crucial for the analysis 
of interactive discussions and debates just as 
sentiment expressions are instrumental in 
sentiment analysis (Liu, 2012). We thus regard 
this work as an extension to traditional sentiment 
671
analysis (Pang and Lee, 2008; Liu, 2012).  
In our earlier work (Mukherjee and Liu, 
2012a), we proposed three topic models to mine 
contention points, which also extract AD-
expressions. In this paper, we further improve the 
work by coupling an information retrieval 
method to rank good candidate phrases with topic 
modeling in order to discover more accurate AD-
expressions. Furthermore, we apply the resulting 
AD-expressions to the new task of classifying the 
arguing or interaction nature of each pair of 
users. Using discovered AD-expressions for 
classification has an important advantage over 
traditional classification because they are domain 
independent. We employ a semi-supervised 
generative model called JTE-P to jointly model 
AD-expressions, pair interactions, and discussion 
topics simultaneously in a single framework. 
With such complex interactions mined, we can 
produce many useful summaries of discussions. 
For example, we can discover the most 
contentious pairs for each topic and ideological 
camps of participants, i.e., people who often 
agree with each other are likely to belong to the 
same camp. The proposed framework also 
facilitates tracking users? ideology shifts and the 
resulting arguing nature. 
The proposed methods have been evaluated 
both qualitatively and quantitatively using a large 
number of real-life discussion/debate posts from 
four domains. Experimental results show that the 
proposed model is highly effective in performing 
its tasks and outperforms several baselines. 
2 Related Work 
There are several research areas that are related 
to our work. We compare with them below.  
Sentiment analysis: Sentiment analysis 
determines positive and negative opinions 
expressed on entities and aspects (Hu and Liu, 
2004). Main tasks include aspect extraction (Hu 
and Liu, 2004; Popescu and Etzioni, 2005), 
polarity identification (Hassan and Radev, 2010; 
Choi and Cardie, 2010) and subjectivity analysis 
(Wiebe, 2000). As discussed earlier, agreement 
and disagreement are a special form of 
sentiments and are different from the sentiment 
studied in the mainstream research. Traditional 
sentiment is mainly expressed with sentiment 
terms (e.g., great and bad), while agreement and 
disagreement are inferred by AD-expressions 
(e.g., I agree and I disagree), which we also call 
AD-sentiment expressions. Thus, this work 
expands the sentiment analysis research.  
Topic models: Our work is also related to topic 
modeling and joint modeling of topics and other 
information as we jointly model several aspects 
of discussions/debates.  
Topic models like pLSA (Hofmann, 1999) and 
LDA (Blei et al, 2003) have proved to be very 
successful in mining topics from large text 
collections. There have been various extensions 
to multi-grain (Titov and McDonald, 2008), 
labeled (Ramage et al, 2009), and sequential (Du 
et al, 2010) topic models. Yet other approaches 
extend topic models to produce author specific 
topics (Rosen-Zvi et al, 2004), author persona 
(Mimno and McCallum, 2007), social roles 
(McCallum et al, 2007), etc. However, these 
models do not model debates and hence are 
unable to discover AD-expressions and 
interaction natures of author pairs.  
Also related are topic models in sentiment 
analysis which are often referred to as Aspect 
and Sentiment models (ASMs). ASMs come in 
two main flavors: Type-1 ASMs discover aspect 
(or topic) words sentiment-wise (i.e., discovering 
positive and negative topic words and sentiments 
for each topic without separating topic and 
sentiment terms) (e.g., Lin and He, 2009; Brody 
and Elhadad, 2010, Jo and Oh, 2011). Type-2 
ASMs separately discover both aspects and 
sentiments (e.g., Mei et al, 2007; Zhao et al, 
2010). Recently, domain knowledge induced 
ASMs have also been proposed (Mukherjee and 
Liu, 2012b; Chen et al, 2013). The generative 
process of ASMs is, however, different from our 
model. Specifically, Type-1 ASMs use 
asymmetric hyper-parameters for aspects while 
Type-2 assumes that sentiments and aspects are 
emitted in the same sentence. However, AD-
expressions are emitted differently. They are 
mostly interleaved with users? topical viewpoints 
and span different sentences. Further, we capture 
the key characteristic of discussions by encoding 
pair-wise user interactions. Existing models do 
not model pair interactions. 
In terms of discussions and comments, Yano 
et al, (2009) proposed the CommentLDA model 
which builds on the work of LinkLDA (Erosheva 
et al, 2004). Mukherjee and Liu (2012d) mined 
comment expressions. These works, however, 
don?t model pair interactions in debates. 
Support/oppose camp classification: Several 
works have attempted to put debate authors into 
support/oppose camps. Agrawal et al (2003) 
used a graph based method. Murakami and 
Raymond (2010) used a rule-based method. In 
(Galley et al, 2004; Hillard et al, 2003), speaker 
672
utterances were classified into agreement, 
disagreement and backchannel classes. 
Stances in online debates: Somasundaran and 
Wiebe (2009), Thomas et al (2006), Bansal et al 
(2008), Burfoot et al (2011), and Anand et al 
(2011) proposed methods to recognize stances in 
online debates. Some other research directions 
include subgroup detection (Abu-Jbara et al, 
2012), tolerance analysis (Mukherjee et al, 
2013), mining opposing perspectives (Lin and 
Hauptmann, 2006), linguistic accommodation 
(Mukherjee and Liu, 2012c), and contention 
point mining (Mukherjee and Liu, 2012a). For 
this work, we adopt the JTE-P model in 
(Mukherjee and Liu, 2012a), and make two 
major advances. We propose a new method to 
improve the AD-expression mining  and a new 
task of classifying pair interaction nature to 
determine whether each pair of users who have 
interacted based on replying relations mostly 
agree or disagree with each other. 
3 Model  
We now introduce the JTE-P model with 
additional details. JTE-P is a semi-supervised 
generative model motivated by the joint 
occurrence of expression types (agreement and 
disagreement), topics in discussion posts, and 
user pairwise interactions. Before proceeding, we 
make the following observation about online 
discussions. 
In a typical debate/discussion post, the user 
(author) mentions a few topics (using 
semantically related topical terms) and expresses 
some viewpoints with one or more AD-
expression types (using agreement and 
disagreement expressions). AD-expressions are 
directed towards other user(s), which we call 
target(s). In this work, we focus on explicit 
mentions (i.e., using @name or quoting other 
authors? posts). In our crawled dataset, 77% of 
all posts exhibit explicit quoting/reply-to 
relations excluding the first posts of threads 
which start the discussions and usually have 
nobody to quote/reply-to. Such author-target 
exchanges usually go back and forth between 
pairs of users populating a thread of discussion. 
The discussion topics and AD-expressions 
emitted are thus caused by the author-pairs? 
topical interests and their nature of interaction 
(agreeing vs. disagreeing).  
In our discussion data obtained from 
Volconvo.com, we found that a pair of users 
typically exhibited a dominant arguing nature 
(agreeing vs. disagreeing) towards each other 
across various topics or threads. We believe this 
is because our data consists of topics like 
elections, theism, terrorism, vegetarianism, etc. 
which are often heated and attract people with 
pre-determined, strong, and polarized stances1. 
This observation motivates the generative 
process of our model. Referring to the notations 
in Table 1, we explain the generative process of 
JTE-P. Given a document (post) ?, its author, ??, 
and the list of targets to whom ?? replies/quotes 
                                                          
1 These hardened perspectives are supported by theoretical 
studies in communications like the polarization effect 
(Sunstein, 2002), and the hostile media effect, a scenario 
where partisans rigidly hold on to their stances (Hansen and 
Hyunjung, 2011). 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: JTE-P Model in plate notation. 
Variable/Function Description 
?; ?? 
A document (post) ? ; author ?  of 
document, ? 
?? = [?1? ??] 
List of targets to whom ?? 
replies/quotes in d. 
? = (?, ??) 
Pair of two authors interacting by 
reply/quote. 
??
?; 
??
?(??,??
?  , 
??,?????
? ) 
Pair ? ?s distribution over topics ; 
expression types (Agreement: ??,??
? , 
Disagreement: ??,?????
? ) 
??
?;  ???{??,?????}
?  Topic ? ?s ; Expression type ? ?s 
distribution over vocabulary terms 
?;? Total number  of topics; expression types 
?;? Total number of vocabulary terms; pairs 
??,?; ?? ??? term in ?;  Total # of terms in ? 
? ?,?  Distribution over topics and AD-
expressions 
??,? 
Associated feature context of the 
observed term ??,? 
? Learned Max-Ent parameters 
??,? ? {???, ???} 
Binary indicator/switch variable ( topic 
(???) or AD-expression (???) ) for ??,? 
??,? Topic/Expression type of ??,? 
??; ??; ??; ?? Dirichlet priors of ??
?;  ??
? ;??
?;  ??
? 
??,?
??; ??,?
??  
# of times topic ? ; expression type ? 
assigned to ? 
??,?
??; ??,?
??  
# of times term ?  appears in topic ? ; 
expression type ? 
Table 1: List of Notations 
?T 
 
  
T 
?E 
 
  
E  
?
E
 
 
?
T
 
   
?
E
 ?
E
  
?
T
  P  
?
T
 
  
z 
 
r  
w  
  
c 
 
p 
 
D 
  
N
d
 
? 
 
 
x 
 
? 
w 
 
a
d
 
 
bd 
673
in ? , ?? = [?1? ??] , the document ?  exhibits 
shared topics and arguing nature of various pairs, 
? = (?? , ?)   , where ? ? ?? . More precisely, the 
pair specific topic and AD-expression 
distributions (??
? ; ??
? ) ?shape? the topics and 
AD-expressions emitted in ?  as agreement and 
disagreement on topical viewpoints are directed 
towards certain target authors. Each topic (???) 
and AD-expression type (???) is characterized by 
a multinomial distribution over terms 
(words/phrases). Assume we have ? = 1 ?? 
topics and ? = 1 ??  expression types in our 
corpus. Note that in our case of discussion/debate 
forums, we hypothesize ? = 2 as in debates, we 
mostly find two expression types: agreement and 
disagreement (more details in ?6.1). Like most 
generative models for text, a post (document) is 
viewed as a bag of n-grams and each n-gram 
(word/phrase) takes one value from a predefined 
vocabulary. In this work, we use up to 4-grams, 
i.e., n = 1, 2, 3, 4. Instead of using all n-grams, a 
relevance based ranking method is proposed to 
select a subset of highly relevant n-grams for 
model building (details in ?4). For notational 
convenience, we use terms to denote both words 
(unigrams) and phrases (n-grams). 
JTE-P is a switching graphical model (Ahmed 
and Xing, 2010; Zhao et al, 2010) performing a 
switch between AD-expressions and topics. ??,? 
denotes the distribution over topics and AD-
expressions with ??,? ? {???, ???} denoting the binary 
indicator/switch variable (topic or AD-
expression) for the ? th term of ? , ??,? .  To 
perform the switch we use a maximum entropy 
(Max-Ent) model. The idea is motivated by the 
observation that topical and AD-expression terms 
usually play different roles in a sentence. Topical 
terms (e.g., ?elections? and ?income tax?) tend to 
be noun and noun phrases while AD-expression 
terms (?I refute?, ?how can you say?, and 
?probably agree?) usually contain pronouns, 
verbs, wh-determiners, and modals. In order to 
utilize the part-of-speech (POS) tag information, 
we place the topic/AD-expression distribution 
??,? (the prior over the indicator variable ??,?) in 
the term plate (see Figure 1) and set it from a 
Max-Ent model conditioned on the observed 
feature context ??,?  associated with ??,?  and the 
learned Max-Ent parameters, ? (details in ?6.1). 
In this work, we use both lexical and POS 
features of the previous, current, and next POS 
tags/lexemes of the term ??,?  as the contextual 
information, i.e., ??,? = [?????,??1 , ?????,? ,
?????,?+1 , ??,??1,??,? , ??,?+1], which is used to 
produce the feature functions for Max-Ent. For 
phrasal terms (n-grams), all POS tags and 
lexemes of ??,?  are considered as contextual 
information for computing feature functions in 
Max-Ent. We now detail the generative process 
of JTE-P (plate notation in Figure 1) as follows: 
1. For each AD-expression type ?, draw ???~???(??) 
2. For each topic ?, draw ???~???(??) 
3. For each pair ?, draw ???~???(??); ???~???(??) 
4. For each forum discussion post ? ? {1 ??}: 
i. Given the author ?? and the list of targets ?? , for 
each term ??,?, ? ? {1 ???}: 
a. Draw a target ?~???(??) 
b. Form pair ? = (?? , ?), ? ? ??   
c. Set ??,? ? ??????(??,?; ?) 
d. Draw ??,?~????(??,?) 
e. if (??,? = ???) // ??,? is an AD-expression term 
Draw ??,?~????(??
?) 
else // ??,? = ???, ??,? is a topical term 
Draw ??,?~????(??
?) 
f. Emit ??,?~????(???,?
??,?) 
??? , ???? , ???? , and ???  correspond to the 
Dirichlet, Multinomial, Bernoulli, and Uniform 
distributions respectively. To learn JTE-P, we 
employ approximate posterior inference using 
Monte Carlo Gibbs sampling. Denoting the 
random variables {?, ?,?, ?} associated with each 
term by singular subscripts {??, ??,??, ??}, ?1?? , 
? = ? ??? , a single Gibbs sweep consists of 
performing the following sampling. 
?(?? = ?,?? = ?, ?? = ???| ? ) ?
 
1
|??|
????? ???????,?,????
?
?=1 ?
? ????? ???????,?,??
?
?=1 ???{??,??}
?   
??,?
??
??
+??
??,(?)
??
??
+???
??,?
??
??
+??
??,(?)
??
??
+???
               (1) 
?(?? = ?,?? = ?, ?? = ???| ? ) ? 
  
1
|??|
????? ???????,?,????
?
?=1 ?
? ????? ???????,?,??
?
?=1 ???{??,??}
? 
??,?
??
??
+??
??,(?)
??
??
+???
??,?
??
??
+??
??,(?)
??
??
+???
                  (2) 
Count variables ??,??? , ??,??? , ??,??? , and ??,???   are 
detailed in Table 1. Omission of a latter index 
denoted by (?)  represents the marginalized sum 
over the latter index. ? = (?, ?)  denotes the ? th 
term of document ? and the subscript ?? denotes 
the counts excluding the term at (?, ?). ?1??  are 
the parameters of the learned Max-Ent model 
corresponding to the ?  binary feature functions 
?1??  for Max-Ent. These learned Max-Ent ? 
parameters in conjunction with the observed 
feature context, ??,?  feed the supervision signal 
for topic/expression switch parameter, r which is 
updated during inference in equations (1) and (2). 
674
4 Phrase Ranking based on Relevance 
We now detail our method of pre-processing n-
grams (phrases) based on relevance to select a 
subset of highly relevant n-grams for model 
building. This has two advantages: (i). A large 
number of irrelevant n-grams slow inference. (ii). 
Filtering irrelevant terms in the vocabulary 
improves the quality of AD-expressions. Before 
proceeding, we review some existing approaches. 
Topics in most topic models like LDA are 
usually unigram distributions. This offers a great 
computational advantage compared to more 
complex models which consider word ordering 
(Wallach, 2006; Wang et al, 2007). This thread 
of research models bigrams by encoding them 
into the generative process. For each word, a 
topic is sampled first, then its status as a unigram 
or bigram is sampled, and finally the word is 
sampled from a topic-specific unigram or bigram 
distribution. This method, however, is expensive 
computationally and has a limitation for arbitrary 
length n-grams. In (Tomokiyo and Hurst, 2003), 
a language model approach is used for bigram 
phrase extraction. 
Yet another thread of research post-processes 
the discovered topical unigrams to form multi-
word phrases using likelihood scores (Blei and 
Lafferty, 2009). This approach considers adjacent 
word pairs and identifies n-grams which occur 
much more often than one would expect by 
chance alone by computing likelihood ratios. 
While this is reasonable, a significant n-gram 
with high likelihood score may not necessarily be 
relevant to the problem domain. For instance, in 
our case of discovering AD-expressions, the 
likelihood score 2  of ?1  = ?the government of? 
happens to be more than ?2  = ?I completely 
disagree?. Clearly, the former is irrelevant for the 
task of discovering AD-expressions. The reason 
for this is that likelihood scores or other 
statistical test scores rely on the relative counts in 
the multi-way contingency table to compute 
significance. Since the relative counts of different 
fragments of the irrelevant phrase ?1 , e.g. ?the 
government?, and ?government of?, happen to 
appear more than the corresponding counts in the 
contingency table of ?2, the tests assign a higher 
score. This is nothing wrong per se because the 
statistical tests only judge significance of an n-
gram, but a significant n-gram may not 
necessarily be relevant in a given problem 
domain. 
                                                          
2 Computed using N-gram statistics package, NSP; http://n-
gram.sourceforge.net 
Thus, the existing approaches have some 
major shortcomings for our task. As our goal is 
to enhance the expressiveness of our models by 
considering relevant n-grams preserving the 
advantages of exchangeable modeling, we 
employ a pre-processing technique to rank n-
grams based on relevance and consider certain 
number of top ranked n-grams based on coverage 
(details follow) in our vocabulary. The idea 
works as follows. 
We first induce a unigram JTE-P whereby we 
cluster the relevant AD-expression unigrams in 
???
?  and ??????
? . Our notion of relevance of AD-
expressions is already encoded into the model 
using priors set from Max-Ent. Next, we rank the 
candidate phrases (n-grams) using our 
probabilistic ranking function. The ranking 
function is grounded on the following 
hypothesis: a relevant phrase is one whose 
unigrams are closely related to (or appear with 
high probabilities in) the given AD-expression 
type, ? : Agreement ( ?? ) or disagreement 
(?????). Continuing from the previous example, 
given the expression type ??=?????
? , ?2 is relevant 
while ?1 is not as ?government? and ?disagree? 
are highly unlikely and likely respectively to be 
clustered in ??=?????
? . Thus, we want to rank 
phrases based on ?(??? = 1|?,?) where ? denotes 
the expression type (Agreement/Disagreement), 
?  denotes a candidate phrase. Following the 
probabilistic relevance model in (Lafferty and 
Zhai, 2003), we use a similar technique to that in 
(Zhao et al, 2011) for deriving our relevance 
ranking function as follows: 
 ?(??? = 1|?,?) =
?(???=1|?,?)
?(???=0|?,?)+?(???=1|?,?)
=
1
1+
?(???=0|?,?)
?(???=1|?,?)
=
1
1+
?(???=0,?| ?)
?(???=1,?|?)
=
 
1
1+
[?(?|???=0,?)??(???=0|?)]
[?(?|???=1,?)??(???=1|?)]
               (3) 
We further define ? = ?(???=0|?)
?(???=1|?)
. Without loss of 
generality, one can say that ?(??? = 0|?) ?
?(??? = 1|?) , because there are many more 
irrelevant phrases than relevant ones, i.e., ? ? 1. 
Thus, taking log, from equation (3), we get, 
log?(??? = 1|?,?) = log?
1
1+??
?(?|???=0,?)
?(?|???=1,?)
? ?
log ?
?(?|???=1,?)
?(?|???=0,?)
?
1
?
? = log ?
?(?|???=1,?)
?(?|???=0,?)
? ? log ?    (4) 
Thus, our ranking function actually computes the 
relevance score log ??
(?|???=1,?)
?(?|???=0,?)
? . The last term, 
log ?  being a constant is ignored because it 
cancels out while comparing candidate n-grams. 
675
We now estimate the relevance score of a phrase 
? = (?1,?2, ? ,??). Using the conditional 
independence assumption of words given the 
indicator variable ??? and expression type ?, we 
have: 
log ?
?(?|???=1,?)
?(?|???=0,?)
? = ? log
?(??|???=1,?)
?(??|???=0,?)
?
?=1              (5) 
Given the expression model ???  previously 
learned by inducing the unigram JTE-P, it is 
intuitive to set ?(??|??? = 1, ?)  to the point 
estimate of the posterior on ??,??
? =
??,??
?? +??
??,(?)
?? +???
, 
where ??,??
??  is the number of times ??  was 
assigned to AD-expression type ?  and ??,(?)
??  
denotes the marginalized sum over the latter 
index. On the other hand, ?(??|??? = 0, ?) can be 
estimated using a Laplace smoothed ( ?  = 1) 
background model, i.e., (??|??? = 0, ?) =
???+?
??+??
 , 
where ???  denotes the number of times ?? 
appears in the whole corpus and ?? denotes the 
number of terms in the entire corpus. 
Next, we throw light on the issue of choosing 
the number of top k phrases from the ranked 
candidate n-grams. Precisely, we want to analyze 
the coverage of our proposed ranking based on 
relevance models. By coverage, we mean that 
having selected top k candidate n-grams based on 
the proposed relevance ranking, we want to get 
an estimate of how many relevant terms from a 
sample of the collection were covered. To 
compute coverage, we randomly sampled 500 
documents from the corpus and listed the 
candidate n-grams3 in the collection of sampled 
500 documents. For this and subsequent human 
judgment tasks, we use two judges (graduate 
students well versed in English). We asked our 
judges to mark all relevant AD-expressions. 
Agreement study yielded ?Cohen = 0.77 showing 
substantial agreement according to scale 4 
provided in (Landis and Koch, 1977). This is 
understandable as identifying AD-expressions is 
a relatively easy task. Finally, a term was 
considered to be relevant if both judges marked it 
so. We then computed the coverage to see how 
many of the relevant terms in the random sample 
were also present in top k phrases from the 
ranked candidate n-grams. We summarize the 
                                                          
3 These are terms appearing at least 20 times in the entire 
collection. We do this for computational reasons as there 
can be many n-grams and n-grams with very low frequency 
are less likely to be relevant. 
4 No agreement (? < 0), slight agreement (0 < ? ? 0.2), fair 
agreement (0.2 < ? ? 0.4), moderate agreement (0.4 < ? ? 
0.6), substantial agreement (0.6 < ? ? 0.8), and almost 
perfect agreement 0.8 < ? ? 1.0. 
coverage results below in Table 2. 
k 3000 4000 5000 
JTE-P 
Agreement 81.34 84.24 87.01 
Disagreement 84.96 87.86 89.64 
Table 2: Coverage (in %) of AD-expressions. 
We find that choosing top k = 5000 candidate n-
grams based on our proposed ranking, we obtain 
a coverage of 87% for agreement and 89.64 for 
disagreement expression types which are 
reasonably good. Thus, we choose top 5000 
candidate n-grams for each expression type and 
add them to the vocabulary beyond all unigrams.  
 Like expression types ?1?? , we also ranked 
candidate phrases for topics ?1??  using 
?(??? = 1|?,?). However, for topics, selecting k 
based on coverage of each topic is more difficult 
because we induce 50 topics and it is also much 
more difficult to manually find relevant topical 
phrases in the sampled data as a topical phrase 
may belong to more than one topic. We selected 
top 2000 ranked candidate phrases for each topic 
using ?(??? = 1|?,?) as we feel that is sufficient 
for a topic. Note that phrases for topics are not as 
crucial as for AD-expressions because topics can 
more or less be defined by unigrams. 
5 Classifying Pair Interaction Nature 
We now determine whether two users (also 
called a user pair) mostly agree or disagree with 
each other in their exchanges, i.e., their pair 
interaction or arguing nature. This is a relatively 
new task. We first summarize the closest related 
works. In (Galley et al, 2004; Hillard et al, 
2003; Thomas et al, 2006, Bansal et al, 2008), 
conversational speeches (i.e., U.S. Congress 
meeting transcripts) are classified into for or 
against an issue using various types of features: 
durational (e.g., time taken by a speaker; speech 
rate, etc.), structural (e.g., no. of speakers per 
side, no. of votes cast by a speaker on a bill, etc.), 
and lexical (e.g., first word, last word, n-grams, 
etc.). Burfoot et al, (2011) builds on the work of 
(Thomas et al, 2006) and proposes collective 
classification using speaker contextual features 
(e.g., speaker intentions based on vote labels). 
However, above works do not discover pair 
interactions (arguing nature) in debate authors. 
Online discussion forums are textual rather than 
conversational (e.g., U.S. Congress meeting 
transcripts). Thus, the durational, structural, and 
contextual features used in prior works are not 
directly applicable.  
Instead, the model posterior on ??
?  for JTE-P 
676
can actually give an estimate of the overall 
interaction nature of a pair, i.e., the probability 
masses assigned to expression types, ? =
??(Agreement) and ? = ????? (Disagreement). 
As ???~???(??), we have ??,?=??? + ??,?=?????
? = 1. 
Hence, if the probability mass assigned to any 
one of the expression types (agreement, 
disagreement) > 0.5 then according to the model 
posterior, that expression type is dominant, i.e., if 
??,??
?  > 0.5, the pair is agreeing else disagreeing.  
However, this approach is not the best. As we 
will see in the experiment section, supervised 
classification using labeled training data with 
discovered AD-expressions as features performs 
better.  
6 Empirical Evaluation 
We now evaluate the proposed techniques in the 
context of the JTE-P model. We first evaluate the 
discovered AD-expressions by comparing results 
with and without using the phrase ranking 
method in Section 4, and then evaluate the 
classification of interaction nature of pairs. 
6.1 Dataset and Experiment Settings 
We crawled debate/discussion forum posts from 
Volconvo.com. The forum is divided into various 
domains. Each domain consists of multiple 
threads of discussions. For each post, we 
extracted the post id, author, domain, ids of all 
posts to which it replies/quotes, and the post 
content. In all, we extracted 26137, 34986, 
22354, and 16525 posts from Politics, Religion, 
Society and Science domains respectively.  
Experiment Data: As it is not interesting to 
study pairs who only exchanged a few posts, we 
restrict to pairs with at least 20 post exchanges. 
This resulted in 1241 authors and 1461 pairs. The 
reduced dataset consists of 1095586 tokens (after 
n-gram preprocessing in ?4), 40102 posts with an 
average of 27 posts or interactions per pair. Data 
from all 4 domains are combined for modeling. 
Parameter Settings: For all our experiments, we 
set the hyper-parameters to the heuristic values 
??  = 50/?, ??  = 50/?, ??  = ??  = 0.1 suggested 
in (Griffiths and Steyvers, 2004). We set the 
number of topics, ? = 50 and the number of AD-
expression types, ? = 2 (agreement and 
disagreement) as in discussion/debate forums, 
there are usually two expression types5. To learn 
                                                          
5 Values for ? > 2 were also tried. However, they did not 
produce any new dominant expression type. There was also 
a slight increase in the model perplexity showing that values 
of ? > 2 do not fit the debate forum data well. 
the Max-Ent parameters ?, we randomly sampled 
500 terms from the held-out data (10 threads in 
our corpus which were excluded from the 
evaluation of tasks in ?6.2, ?6.3) appearing at 
least 10 times and labeled them as topical (361) 
or AD-expressions (139) and used the 
corresponding features of each term (in the 
context of posts where it occurs, ?3) to train the 
Max-Ent model. 
6.2 AD-Expression Evaluation 
We first list some discovered top AD-expressions 
in Table 3 for qualitative inspection. From Table 
3, we can see that JTE-P can cluster many correct 
AD-expressions, e.g., ?I accept?, ?I agree?, 
?you?re correct?, etc. in agreement and ?I 
disagree?, ?don?t accept?, ?I refute?, etc. in 
disagreement. In addition, it also discovers and 
clusters highly specific and more ?distinctive? 
expressions beyond those used in Max-Ent 
training, e.g., ?valid point?, ?I do support?, and 
?rightly said? in agreement; and phrases like ?can 
you prove?, ?I don?t buy your?, and ?you fail to? 
in disagreement. Note that terms in black in 
Table 3 were used in Max-Ent training. The 
newly discovered terms are marked blue in 
italics. Clustering errors are in red (bold). 
For quantitative evaluation, topic models are 
often compared using perplexity. However, 
perplexity does not reflect our purpose since we 
are not trying to evaluate how well the AD-
expressions in an unseen discussion data fit our 
learned models. Instead our focus is to evaluate 
how well our learned AD-expression types 
perform in clustering semantic phrases of 
agreement/disagreement. Since AD-expressions 
(according to top terms in ??) produced by JTE-
P are rankings, we choose precision @ n (p@n) 
as our metric. p@n is commonly used to evaluate 
a ranking when the total number of correct items 
is unknown (e.g., Web search results, aspect 
terms in topic models for sentiment analysis 
(Zhao et al, 2010), etc.). This situation is similar 
to our AD-expression rankings, ?? . Further, as 
??~???, the Dirichlet smoothing effect ensures 
that every term in the vocabulary has some non-
zero mass to agreement or disagreement 
expression type. Thus, it is the ranking of terms 
in each AD-expression type that matters (i.e., 
whether the model is able to rank highly relevant 
terms at the top).  
The above method evaluates the original 
ranking. Another way of evaluating the AD-
expression rankings is to evaluate only those 
newly discovered terms, i.e., beyond those 
677
labeled terms used in Max-Ent training. For this 
evaluation, we remove those terms that have 
been used in Max-Ent (ME) training. We report 
both results in Table 4. We also studied inter-
rater agreement using two judges who 
independently labeled the top n terms as correct 
or incorrect. A term was marked correct if both 
judges deemed it so which was then used to 
compute p@n. Agreement using ??????  was 
greater than 0.78 for all p@n computations 
implying substantial and good agreements as 
identifying whether a phrase implies agreement 
or disagreement or none is an easy task. P@n 
excluding ME labeled terms (Table 4, second 
column) are slightly lower than those using all 
terms but are still decent. This is because p@n 
excluding ME labeled terms removes many 
correct AD-expressions used in training. 
Further to evaluate the sensitivity of 
performance on the amount of labeled terms for 
Max-Ent, we computed p@n across different 
sizes of labeled terms. Table 4 shows p@n for 
agreement and disagreement expressions across 
different sizes of labeled terms (L). We find that 
more labeled terms improves p@n which is 
intuitive. We used 500 labeled terms in all our 
subsequent experiments. The result in Table 4 
uses relevance ranking (?4). 
Disagreement expressions (??=????????????
?  ) 
I, disagree, I don?t, I disagree, argument, reject, claim, I reject, I refute, and, your, I refuse, won?t, the claim, 
nonsense, I contest, dispute, I think, completely disagree, don?t accept, don?t agree, incorrect, doesn?t, hogwash, I 
don?t buy your, I really doubt, your nonsense, true, can you prove, argument fails, you fail to, your assertions, 
bullshit, sheer nonsense, doesn?t make sense, you have no clue, how can you say, do you even, contradict yourself, ? 
Agreement expressions (??=?????????
? ) 
agree, I, correct, yes, true, accept, I agree, don?t, indeed correct, your, I accept, point, that, I concede, is valid, your 
claim, not really, would agree, might, agree completely, yes indeed, absolutely, you?re correct, valid point, 
argument, the argument, proves, do accept, support, agree with you, rightly said, personally, well put, I do 
support, personally agree, doesn?t necessarily, exactly, very well put, kudos, point taken, ... 
Table 3: Top terms (comma delimited) of two expression types. Red (bold) terms denote possible errors. 
Blue (italics) terms are newly discovered; rest (black) terms have been used in Max-Ent training. 
    P@n 
 
     L 
JTE-P (all terms) JTE-P (excluding labeled ME terms) 
Agreement Disagreement Agreement Disagreement 
50 100 150 50 100 150 50 100 150 50 100 150 
100 0.62 0.63 0.61 0.64 0.62 0.63 0.58 0.56 0.57 0.60 0.59 0.58 
200 0.66 0.67 0.65 0.68 0.66 0.67 0.62 0.59 0.60 0.64 0.63 0.62 
300 0.70 0.70 0.71 0.70 0.68 0.67 0.66 0.66 0.65 0.66 0.66 0.65 
400 0.72 0.72 0.73 0.74 0.71 0.70 0.68 0.67 0.69 0.70 0.68 0.69 
500 0.76 0.77 0.75 0.76 0.73 0.74 0.70 0.71 0.70 0.72 0.71 0.70 
Table 4: Results using terms based on phrase relevance ranking for P @ n= 50, 100, 150 across 100, 200, 
?, 500 labeled examples (L) used for Max-Ent (ME) training.  
    P@n 
 
     L 
JTE-P (all terms) JTE-P (excluding ME terms) 
Agreement Disagreement Agreement Disagreement 
50 100 150 50 100 150 50 100 150 50 100 150 
500 0.66 0.69 0.69 0.72 0.70 0.70 0.66 0.65 0.64 0.68 0.66 0.65 
Table 5: Results using all tokens (without applying phrase relevance ranking) for P@50, 100, 150 and 500 
labeled examples were used for Max-Ent (ME) training). 
Feature Setting 
Agreeing Disagreeing 
P R F1 P R F1 
JTE-P-posterior 0.59 0.61 0.60 0.81 0.70 0.75 
W+POS 1-4 grams 0.63 0.66 0.64 0.83 0.82 0.82 
W+POS 1-4grams + IG (top 1%) 0.64 0.67 0.65 0.84 0.82 0.83 
W+POS 1-4 grams + IG (top 2%) 0.65 0.67 0.66 0.84 0.82 0.83 
W+POS 1-4 grams + ?2 (top 1%) 0.65 0.68 0.66 0.84 0.83 0.83 
W+POS 1-4 grams + ?2(top 2%) 0.64 0.68 0.69 0.84 0.82 0.83 
AD-Expressions, ?? (top 1000) 0.73 0.74 0.73 0.87 0.87 0.87 
AD-Expressions, ?? (top 2000) 0.77 0.81 0.78 0.90 0.88 0.89 
Table 6: Precision (P), recall (R), and F1 scores of pair interaction evaluation. Improvements in F1 using 
AD-expression features (??) are statistically significant (p<0.01) using paired t-test across 5-fold CV. 
 
678
We now compare with the performance of the 
model without using phrase relevance ranking. 
P@n results using all tokens (4356787) are 
shown in Table 5 (with 500 labeled terms for 
Max-Ent training). Clearly, P@n is lower than in 
Table 4 (last row; with phrase relevance ranking) 
because without phrase relevance ranking (Table 
5) many irrelevant terms can rank high due to co-
occurrences which may not be semantically 
related. This shows that relevance ranking of 
phrases is beneficial.   
6.3 Pair Interaction Nature 
We now evaluate the overall interaction nature of 
each pair of users. The evaluation of this task 
requires human judges to read all the posts where 
the two users forming the pair have interacted.  
Thus, it is hard to evaluate all 1461 pairs in our 
dataset. Instead, we randomly sampled 500 pairs 
(? 34% of the population) for evaluation. Two 
human judges were asked to independently read 
all the post interactions of 500 pairs and label 
each pair as overall ?disagreeing? or overall 
?agreeing? or ?none?. The ??????  for this task 
was 0.81. Pairs were finally labeled as agreeing 
or disagreeing if both judges deemed them so. 
This resulted in 320 disagreeing and 152 
agreeing pairs. Out of the rest 28 pairs, 10 were 
marked ?none? by both judges while 18 pairs had 
disagreement in labels. We only focus on the 472 
agreeing and disagreeing pairs. 
As we have labeled data for 472 pairs, we can 
treat identifying pair arguing nature as a text 
classification problem where all interactions 
between a pair are merged in one document 
representing the pair along with the label given 
by judges: agreeing or disagreeing. To compare 
classification performance, we use two feature 
sets: (i) standard word + POS 1-4 grams and (ii) 
AD-expressions from ??. We use TF-IDF as our 
feature value assignment scheme. We also try 
two well-known feature selection schemes Chi-
Squared Test (?2) and Information Gain (IG). We 
use the linear kernel6 SVM (SVMlight system in 
(Joachims, 1999)) as our text classifier. For 
feature selection using ?2 and IG, we use two 
settings: top 1% and 2% of all features ranked 
according to the selection metric. Also, for 
estimated AD-expressions (according to 
probabilities in ?? ), we experiment with top 
1000 and 2000 AD-expressions terms for both 
agreement and disagreement. We summarize 
                                                          
6  Other kernels polynomial, RBF, and sigmoid did not 
perform as well. 
comparison results using 5-fold Cross Validation 
(CV) with two classes: agreeing and disagreeing 
in Table 6. JTE-P-posterior represents the 
method using simply the model posterior on ??
? 
to make the decision (see ?5). From Table 6, we 
can make the following observations.  
Predicting agreeing arguing nature is harder 
than that of disagreeing across all feature 
settings. Feature selection improves performance. 
?2 and IG perform similarly. AD-expressions, 
??yields the best performance showing that the 
discovered AD-expressions are of high quality 
and reflect the user pair arguing nature well. 
Selecting certain top terms in ??  can also be 
viewed as a form of feature selection. Although 
prediction performance using model posterior 
(JTE-P-posterior) is slightly lower than 
supervised SVM (Table 6, second row), the F1 
scores are decent. Using the discovered AD-
expressions (Table 6, last low) as features 
renders a statistically significant (see Table 6 
caption) improvement over other baseline feature 
settings. This shows that discovered AD-
expressions are useful for downstream 
applications, e.g., the task of identifying pair 
interactions. 
7 Conclusion 
This paper studied the problem of modeling user 
pair interactions in online discussions with the 
purpose of discovering the interaction or arguing 
nature of each author pair and various AD-
expressions emitted in debates. A novel 
technique was also proposed to rank n-gram 
phrases where relevance based ranking was used 
in conjunction with a semi-supervised generative 
model. This method enables us to find better AD-
expressions. Experiments using real-life online 
debate data showed the effectiveness of the 
model. In our future work, we intend to extend 
the model to account for stances, and issue 
specific interactions which would pave the way 
for user profiling and behavioral modeling. 
Acknowledgments 
We would like thank Sharon Meraz (Department 
of communication, University of Illinois at 
Chicago) and Dennis Chong (Department of 
Political Science, Northwestern University) for 
several valuable discussions. This work was 
supported in part by a grant from the National 
Science Foundation (NSF) under grant no. IIS-
1111092. 
679
References  
Abu-Jbara, A., Dasigi, P., Diab, M. and Dragomir 
Radev. 2012. Subgroup detection in ideological 
discussions. In Proceedings of the Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2012). 
Agrawal, R., Rajagopalan, S., Srikant, R., and Xu. Y. 
2003. Mining newsgroups using networks arising 
from social behavior. In Proceedings of the 
International Conference on World Wide Web 
(WWW-2003). 
Ahmed, A and Xing, E. 2010. Staying informed: 
supervised and semi-supervised multi-view topical 
analysis of ideological perspective. In Proceedings 
of the Empirical Methods in Natural Language 
Processing (EMNLP-2010). 
Anand, P., Walker, M., Abbott, R., Tree, J., Bowmani, 
R., and Minor, M. 2011. Cats rule and dogs drool!: 
Classifying stance in online debate. In Proceedings 
of the 2nd Workshop on Computational Approaches 
to Subjectivity and Sentiment Analysis. 
Bansal, M., Cardie, C., and Lee, L. 2008. The power 
of negative thinking: Exploiting label disagreement 
in the min-cut classification framework. In 
Proceedings of the International Conference on 
Computational Linguistics (Short Paper). 
Blei, D., Ng, A., and Jordan, M. 2003. Latent 
Dirichlet Allocation. Journal of Machine Learning 
Research. 
Blei, D. and Lafferty J. 2009. Visualizing topics with 
multi-word expressions. Tech. Report. 
arXiv:0907.1013v1. 
Brody, S. and Elhadad, S. 2010. An Unsupervised 
Aspect-Sentiment Model for Online Reviews. In 
Proceedings of the Annual Conference of the North 
American Chapter of the ACL (NAACL-2010). 
Burfoot, C., Bird, S., and Baldwin, T. 2011. Collective 
Classification of Congressional Floor-Debate 
Transcripts. In Proceedings of the Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2001). 
Chang, J., Boyd-Graber, J., Wang, C.  Gerrish, S. 
Blei, D. 2009. Reading tea leaves: How humans 
interpret topic models. In Proceedings of the Neural 
Information Processing Systems (NIPS-2009). 
Chen, Z., Mukherjee, A., Liu, B., Hsu, M., 
Castellanos, M., Ghosh, R. 2013. Leveraging Multi-
Domain Prior Knowledge in Topic Models. In 
Proceedings of the International Joint Conference in 
Artificial Intelligence (IJCAI-2013). 
Choi, Y. and Cardie, C. 2010. Hierarchical sequential 
learning for extracting opinions and their attributes 
(Short Paper). In Proceedings of the Annual 
Meeting of the Association for Computational 
Linguistics (ACL-2010). 
Du, L., Buntine, W. L., and Jin, H. 2010. Sequential 
Latent Dirichlet Allocation: Discover Underlying 
Topic Structures within a Document. In 
Proceedings of the IEEE International Conference 
on Data Mining (ICDM-2010). 
Erosheva, E., Fienberg, S. and Lafferty, J. 2004. 
Mixed membership models of scientific 
publications. In Proceedings of the National 
Academy of Sciences (PNAS-2004). 
Galley, M., McKeown, K., Hirschberg, J., and 
Shriberg, E. 2004. Identifying agreement and 
disagreement in conversational speech: Use of 
Bayesian networks to model pragmatic 
dependencies. In Proceedings of the Annual 
Meeting of the Association for Computational 
Linguistics (ACL-2004). 
Griffiths, T. and Steyvers, M. 2004. Finding scientific 
topics. In Proceedings of the National Academy of 
Sciences (PNAS-2004). 
Hansen, G. J., and Hyunjung, K. 2011. Is the media 
biased against me? A meta-analysis of the hostile 
media effect research. Communication Research 
Reports, 28, 169-179. 
Hillard, D., Ostendorf, M., and Shriberg, E. 2003. 
Detection of agreement vs. disagreement in 
meetings: Training with unlabeled data. In 
Proceedings of the Conference of the North 
American Chapter of the Association for 
Computational Linguistics: Human Language 
Technologies (NAACL-HLT-2003). 
Hassan, A. and Radev, D. 2010. Identifying text 
polarity using random walks. In Proceedings of the 
Annual Meeting of the Association for 
Computational Linguistics (ACL-2010). 
Hofmann, T. 1999. Probabilistic latent semantic 
analysis. In Proceedings of the Conference on 
Uncertainty in Artificial Intelligence (UAI-1999). 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. In Proceedings of the SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining (KDD-2004). 
Jo, Y. and Oh, A. 2011. Aspect and sentiment 
unification model for online review analysis. In 
Proceedings of the International Conference on 
Web Search and Data Mining (WSDM-2011). 
Joachims, T. Making large-Scale SVM Learning 
Practical. 1999. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed.), MIT-Press, 1999. 
Lafferty, J. and Zhai, C. 2003. Probabilistic relevance 
models based on document and query generation. 
Language Modeling and Information Retrieval. 
Landis, J. R. and Koch, G. G. 1977. The measurement 
of observer agreement for categorical data. 
Biometrics. 
Lin, C. and He, Y. 2009. Joint sentiment/topic model 
for sentiment analysis. In Proceedings of the 
680
International Conference on Knowledge 
Management (CIKM-2009). 
Lin, W. H., and Hauptmann, A. 2006. Are these 
documents written from different perspectives?: a 
test of different perspectives based on statistical 
distribution divergence. In Proceedings of the 
Annual Meeting of the Association for 
Computational Linguistics (ACL-2006). 
Liu, B. 2012. Sentiment Analysis and Opinion Mining. 
Morgan & Claypool Publisher, USA. 
McCallum, A., Wang, X., and Corrada-Emmanuel, A. 
2007. Topic and Role Discovery in Social Networks 
with Experiments on Enron and Academic Email. 
Journal of Artificial Intelligence Research. 
Mei, Q., Ling, X., Wondra, M., Su, H., and Zhai, C. 
2007. Topic sentiment mixture: modeling facets and 
opinions in weblogs. In Proceedings of the 
International Conference on World Wide Web 
(WWW-2007). 
Mimno, D. and McCallum, A. 2007. Expertise 
modeling for matching papers with reviewers. In 
Proceedings of the SIGKDD International 
Conference on Knowledge Discovery and Data 
Mining (KDD-2007). 
Mukherjee, A., Venkataraman, V., Liu, B., Meraz, S. 
2013. Public Dialogue: Analysis of Tolerance in 
Online Discussions. In Proceedings of the Annual 
Meeting of the Association for Computational 
Linguistics (ACL-2013). 
Mukherjee, A. and Liu, B. 2012a. Mining Contentions 
from Discussions and Debates. Proceedings of 
SIGKDD Conference on Knowledge Discovery and 
Data Mining (KDD-2012). 
Mukherjee, A. and Liu, B. 2012b. Aspect Extraction 
through Semi-Supervised Modeling. In Proceedings 
of the Annual Meeting of the Association for 
Computational Linguistics (ACL-2012). 
Mukherjee, A. and Liu, B. 2012c. Analysis of 
Linguistic Style Accommodation in Online Debates. 
In Proceedings of the International Conference on 
Computational Linguistics (COLING-2012). 
Mukherjee, A. and Liu, B. 2012d. Modeling Review 
Comments. In Proceedings of the Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2012). 
Murakami A. and Raymond, R. 2010. Support or 
Oppose? Classifying Positions in Online Debates 
from Reply Activities and Opinion Expressions. In 
Proceedings of the International Conference on 
Computational Linguistics (Coling-2010).  
Pang, B. and Lee, L. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval. 
Popescu, A. and Etzioni, O. 2005. Extracting product 
features and opinions from reviews. In Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing (EMNLP-2005). 
Ramage, D., Hall, D., Nallapati, R, Manning, C. 2009. 
Labeled LDA: A supervised topic model for credit 
attribution in multi-labeled corpora. In Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing (EMNLP-2009). 
Rosen-Zvi, M., Griffiths, T., Steyvers, M., and Smith, 
P. 2004. The author-topic model for authors and 
documents. In Proceedings of the Conference on 
Uncertainty in Artificial Intelligence (UAI-2004). 
Sunstein, C. R. 2002. The law of group polarization. 
Journal of political philosophy.  
Somasundaran, S. and Wiebe, J. 2009. Recognizing 
stances in online debates. In Proceedings of the 
Joint Conference of the 47th Annual Meeting of the 
ACL and the 4th International Joint Conference on 
Natural Language Processing (ACL-IJCNLP-2009). 
Titov, I. and R. McDonald. 2008. Modeling online 
reviews with multi-grain topic models. In 
Proceedings of the International Conference on 
World Wide Web (WWW-2008). 
Thomas, M., Pang, B., and Lee, L. 2006. Get out the 
vote: Determining support or opposition from 
congressional floor-debate transcripts. In Proc. of 
the Conference on Empirical Methods in Natural 
Language Processing (EMNLP-2006). 
Tomokiyo, T., and Hurst, M. 2003. A language model 
approach to keyphrase extraction. In Proceedings of 
the ACL 2003 workshop on Multiword expressions: 
analysis, acquisition and treatment-Volume 18. 
Wallach, H. 2006. Topic modeling: Beyond bag of 
words. In Proceedings of the International 
Conference on Machine Learning (ICML-2006). 
Wang, X., McCallum, A., Wei, X. 2007. Topical N-
grams: Phrase and topic discovery, with an 
application to information retrieval. In Proceedings 
of the IEEE International Conference on Data 
Mining (ICDM-2007). 
Wiebe, J. 2000. Learning subjective adjectives from 
corpora. In Proc. of National Conference on AI 
(AAAI-2000). 
Yano, T., Cohen, W. and Smith, N. 2009. Predicting 
response to political blog posts with topic models. 
In Proceedings of the N. American Chapter of the 
Association for Computational Linguistics: Human 
Language Technologies (NAACL-HLT-2009). 
Zhao, X., J. Jiang, J. He, Y. Song, P. Achananuparp, 
E.P. LiM, and X. Li. 2011. Topical keyphrase 
extraction from twitter. In Proceedings of the 
Annual Meeting of the Association for 
Computational Linguistics (ACL-2011). 
Zhao, X., Jiang, J., Yan, H., and Li, X. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-
LDA hybrid. In Proceedings of the Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP-2010). 
681
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1680?1690,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Public Dialogue: Analysis of Tolerance in Online Discussions 
Arjun Mukherjee?   Vivek Venkataraman?   Bing Liu?   Sharon Meraz? 
?Department of Computer Science  ?Department of Communication 
University of Illinois at Chicago 
arjun4787@gmail.com {vvenka6, liub, smeraz}@uic.edu 
 
 
Abstract 
Social media platforms have enabled people to 
freely express their views and discuss issues of 
interest with others. While it is important to dis-
cover the topics in discussions, it is equally use-
ful to mine the nature of such discussions or de-
bates and the behavior of the participants. There 
are many questions that can be asked. One key 
question is whether the participants give rea-
soned arguments with justifiable claims via 
constructive debates or exhibit dogmatism and 
egotistic clashes of ideologies. The central idea 
of this question is tolerance, which is a key 
concept in the field of communications. In this 
work, we perform a computational study of tol-
erance in the context of online discussions. We 
aim to identify tolerant vs. intolerant partici-
pants and investigate how disagreement affects 
tolerance in discussions in a quantitative 
framework. To the best of our knowledge, this 
is the first such study. Our experiments using 
real-life discussions demonstrate the effective-
ness of the proposed technique and also provide 
some key insights into the psycholinguistic 
phenomenon of tolerance in online discussions. 
1 Introduction 
Social media platforms have enabled people 
from anywhere in the world to express their 
views and discuss any issue of interest in online 
discussions/debates. Existing works in this con-
text include recognition of support and oppose 
camps (Agrawal et al, 2003), mining of authori-
ties and subgroups (Mayfield and Ros?, 2011; 
Abu-Jbara et al (2012), dialogue act segmenta-
tion and classification (Morbini and Sagae, 2011; 
Boyer et al, 2011), etc. 
This paper probes further to study a different 
and important angle, i.e., the psycholinguistic 
phenomenon of tolerance in online discussions. 
Tolerance is an important concept in the field of 
communications. It is a subfacet of deliberation 
which refers to critical thinking and exchange of 
rational arguments on an issue among partici-
pants that seek to achieve consensus/solution 
(Habermas, 1984). 
Perhaps the most widely accepted definition 
of tolerance is that of Gastil (2005; 2007), who 
defines tolerance as a means to engage (in writ-
ten or spoken communication) in critical think-
ing, judicious argument, sound reasoning, and 
justifiable claims through constructive discus-
sion as opposed to mere coercion/egotistic clash-
es of ideologies.  
In this work, we adopt this definition, and also 
employ the following characteristics of tolerance 
(also known as ?code of conduct?) (Crocker, 
2005; Gutmann and Thompson, 1996) to guide 
our work.  
Reciprocity: Each member (or participant) offers 
proposals and justifications in terms that others 
could understand and accept. 
Publicity: Each member engages in a process 
that is transparent to all and each member 
knows with whom he is agreeing or disagree-
ing.  
Accountability: Each member gives acceptable 
and sound reasons to others on the various 
claims or proposals suggested by him. 
Mutual respect and civic integrity: Each mem-
ber?s speech should be morally acceptable, i.e., 
using proper language irrespective of agree-
ment or disagreement of views. 
The issue of tolerance has been actively re-
searched in the field of communications for the 
past two decades, and has been investigated in 
multiple dimensions. However, existing studies 
are typically qualitative and focus on theorizing 
the socio-linguistic aspects of tolerance (more 
details in ?2).  
With the rapid growth of social media, the 
large volumes of online discussions/debates offer 
a golden opportunity to investigate people?s im-
plicit psyche in discussions quantitatively based 
on the real-life data, i.e., their tolerance levels 
and their arguing nature, which are of fundamen-
tal interest to several fields, e.g., communica-
tions, marketing, politics, and sociology 
(Dahlgren, 2005; Gastil, 2005; Moxey and 
1680
Sanford, 2000). Communication and political 
scholars are hopeful that technologies capable of 
identifying tolerance levels of people on social 
issues (often discussed in online discussions) can 
render vital statistics which can be used in pre-
dicting political outcomes in elections and help-
ful in tailoring voting campaigns and agendas to 
maximize winning chances (Dahlgren, 2002). 
Objective: The objective of this work is two-
fold:  
1. Identifying tolerant and intolerant participants 
in discussions.  
2. Analyzing how disagreement affects toler-
ance and estimating the tipping point of such 
effects.  
To the best of our knowledge, these tasks have 
not been attempted quantitatively before. The 
first task is a classification/prediction problem. 
Due to the complex and interactive nature of dis-
cussions, the traditional n-gram features are no 
longer sufficient for accurate classification. We 
thus propose a generative model, called DTM, to 
discover some key pieces of information which 
characterize the nature of discussions and their 
participants, e.g., the arguing nature (agreeing 
vs. disagreeing), topic and expression distribu-
tions. These allow us to generate a set of novel 
features from the estimated latent variables of 
DTM capable of capturing authors? tolerance 
psyche during discussions. The features are then 
used in learning to identify tolerant and intoler-
ant authors. Our experimental results show that 
the proposed approach is effective and outper-
forms several strong baselines significantly. 
The second task studies the interplay of toler-
ance and disagreement. It is well-known that 
tolerance facilitates constructive disagreements, 
but sustained disagreements often result in a 
transition to destructive disagreement leading to 
polarization and intolerance (Dahlgren, 2005). 
An interesting question is: What is the tipping 
point of disagreement to exhibit intolerance? We 
take a Bayesian approach to seek an answer and 
discover issue-specific tipping points. Our em-
pirical results discover some interesting relation-
ships which are supported by theoretical studies 
in psychology and linguistic communications. 
Finally, this work also produces an annotated 
corpus of tolerant and intolerant users in online 
discussions across two domains: politics and re-
ligion. We believe this is the first such dataset 
and will be a valuable resource to the communi-
ty. 
2 Related Work 
Although limited work has been done on analy-
sis of tolerance in online discussions, there are 
several general research areas that are related to 
our work.  
Communications: Tolerance has been an active 
research area in the field of communications for 
the past two decades. Ryfe (2005) provided a 
comprehensive survey of the literature. The topic 
has been studied in multiple dimensions, e.g., 
opinion and attitude (Luskin et al, 2004; Price et 
al., 2002), public engagement (Escobar, 2012), 
psychoanalysis (Slavin and Kriegman, 1992), 
argument repertoire (Cappella et al, 2002), etc. 
Tolerance has also been investigated in the 
domain of political communications with an em-
phasis on political sophistication (Gastil and 
Dillard, 1999), civic culture (Dahlgren, 2002), 
and democracy (Fishkin, 1991). These existing 
works study tolerance from the qualitative per-
spective. Our focus is quantitative analysis. 
Sentiment analysis: Sentiment analysis deter-
mines positive or negative opinions expressed on 
topics (Liu, 2012; Pang and Lee, 2008). Main 
tasks include aspect extraction (Hu and Liu, 
2004; Popescu and Etzioni, 2005; Mukherjee and 
Liu, 2012c; Chen et al, 2013), opinion polarity 
identification (Hassan and Radev, 2010; Choi 
and Cardie, 2010) and subjectivity analysis 
(Wiebe, 2000). Although related, tolerance is 
different from sentiment. Sentiments are mainly 
indicated by sentiment terms (e.g., great, good, 
bad, and poor). Tolerance in discussions refers 
to the reception of certain views and often indi-
cated by agreement and disagreement expres-
sions and other features (?5). 
Online discussions or debates: Several works 
put authors in debate into support and oppose 
camps. Agrawal et al (2003) used a graph based 
method, and Murakami and Raymond (2010) 
used a rule-based method. In (Mukherjee and 
Liu, 2012a), contention points were identified, in 
(Mukherjee and Liu, 2012b), various expressions 
in review comment discussions were mined, and 
in (Galley et al, 2004; Hillard et al, 2003), 
speaker utterances were classified into agree-
ment, disagreement, and backchannel classes. 
Also related are studies on linguistic style ac-
commodation (Mukherjee and Liu, 2012d) and 
user pair interactions (Mukherjee and Liu, 2013) 
in online debates. However, these works do not 
consider tolerance analysis in debate discussions, 
which is the focus of this work. 
1681
In a similar vein, several classification meth-
ods have been proposed to recognize opinion 
stances and speaker sides in online debates (So-
masundaran and Wiebe, 2009; Thomas et al, 
2006; Bansal et al, 2008; Burfoot et al, 2011; 
Yessenalina et al, 2010). Lin and Hauptmann 
(2006) also proposed a method to identify oppos-
ing perspectives. Abu-Jbara et al (2012) identi-
fied subgroups. Kim and Hovy (2007) studied 
election prediction by analyzing online discus-
sions. Other related works studying dialogue and 
discourse in discussions include authority recog-
nition (Mayfield and Ros?, 2011), dialogue act 
segmentation and classification (Morbini and 
Sagae, 2011; Boyer et al, 2011), discourse struc-
ture prediction (Wang et al, 2011). 
All these prior works are valuable. But they 
are not designed to identify tolerance or to ana-
lyze tipping points of disagreements for intoler-
ance in discussions which are the focus of this 
work. 
3 Discussion/Debate Data 
For this research, we used discussion posts from 
Volconvo.com. This forum is divided into vari-
ous domains: Politics, Religion, Science, etc. 
Each domain consists of multiple discussion 
threads. Each thread consists of a list of posts. 
Our experimental data is from two domains, Pol-
itics and Religion. The data is summarized in 
Table 1(a). In this work, the terms users, authors 
and participants are used interchangeably. The 
full data is used for modeling, but 436 and 501 
authors from Politics and Religion domains were 
manually labeled as being tolerant or intolerant 
(Table 1(c)) respectively for classification exper-
iments.   
Two judges (graduate students) were used to 
label the data. The judges are fluent in English 
and were briefed on the definition of tolerance 
(see ?1). From each domain (Politics, Religion), 
we randomly sampled authors having not more 
than 60 posts in order to reduce the labeling bur-
den as the judges need to read all posts and see 
all interactions of each author before providing a 
label. Given all posts by an author, ? and his/her 
associated interactions (posts by other authors 
replying or quoting ?), the judges were asked to 
provide a label for author ? as being tolerant or 
intolerant. In our labeling, we found that users 
strongly exhibit one dominant trait: tolerant or 
intolerant, as our data consists of topics like elec-
tions, immigration, theism, terrorism, and vege-
tarianism across politics and religion domains, 
which are often heated and thus attract people 
with pre-determined, strong, and polarized 
stances1.  
The judges worked in isolation (to prevent bi-
as) during annotation/labeling and were also 
asked to provide a short reason for their judg-
ment. The agreement statistics using Cohen?s 
kappa are given in Table 1(b), which shows sub-
stantial agreements according to the scale 2  in 
(Landis and Koch, 1977). This shows that toler-
ance as defined in ?1 is quite decisive and one 
can decide whether a debater is exhibiting toler-
ant vs. intolerant quite well. To account for disa-
greements in labels, the judges discussed their 
reasons to reach a consensus. The final labeled 
data is reported in Table 1(c). 
4 Model 
We now present our generative model to capture 
the key aspects of discussions/debates and their 
intricate relationships, which enable us to (1) 
design sophisticated features for classification 
and (2) perform an in-depth analysis of the inter-
play of disagreement and tolerance. The model is 
called Debate Topic Model (DTM).  
DTM is a semi-supervised generative model 
motivated by the joint occurrence of various top-
ics; and agreement and disagreement expressions 
(abbreviated AD-expressions hereon) in debate 
posts. A typical debate post mentions a few top-
ics (using similar topical terms) and expresses 
some viewpoints with one or more AD-
expression types (Agreement and Disagreement) 
using semantically related expressions. This ob-
servation forms the basis of the generative pro-
cess of our model where documents (posts) are 
represented as admixtures of latent topics and 
AD-expression types (Agreement and Disagree-
ment). This key observation and the motivation 
of modeling debates are from our previous work 
in (Mukherjee and Liu, 2012a). In the new set-
                                                          
1  These hardened perspectives are theoretically supported 
by the polarization effect (Sunstein, 2002), and the hostile 
media effect, a scenario where partisans rigidly hold on to 
their stances (Hansen and Hyunjung, 2011). 
2  Agreement levels are as follows. ? ? [0, 0.2]: Poor, 
? ? (0.2, 0.4]:Fair, ? ? (0.4, 0.6]: Moderate, ? ? (0.6, 0.8]: 
Substantial, and ? ? (0.8, 1.0]: Almost perfect agreement. 
Domain Posts Authors  Cohen?s ?   Tol.  Intol. Total 
Politics 48605 1027  0.74    213  223  436 
 Religion 66835 1370  0.77    207  294  501 
             (a) Full Data     (b) Agreement   (c) Labeled data 
Table 1: Data statistics (Tol: Tolerant users; Intol: 
Intolerant users. Total = Tol. + Intol). 
 
1682
ting, we model topics and debate expression dis-
tributions specific to authors as this work is con-
cerned with modeling authors? (in)tolerance na-
ture. Making latent variable ?? and ?? author 
specific facilitates modeling user behaviors 
(?5.3). 
Assume we have ?1??  topics and ?1??  expres-
sion types in our corpus. In our case of debate 
posts, based upon reading various posts, we hy-
pothesize that ? = 2 as in debates as we mostly 
find 2 dominant expression types: Agreement 
and Disagreement. Meanings of variables used in 
the following discussion are detailed in Table 2. 
In this work, a document/post is viewed as a bag 
of n-grams and we use terms to denote both 
words (unigrams) and phrases (n-grams)3. DTM 
is a switching graphical model performing a 
switch between topics and AD-expressions simi-
lar to that in (Zhao et al, 2010). The switch is 
done using a learned maximum entropy (Max-
Ent) model. The rationale here is that topical and 
AD-expression terms usually play different syn-
tactic roles in a sentence. Topical terms (e.g., 
?U.S. elections,? ?government,? ?income tax?) 
tend to be noun and noun phrases while expres-
sion terms (?I refute,? ?how can you say,? ?I?d 
agree?) usually contain pronouns, verbs, wh-
determiners, and modals. In order to utilize the 
part-of-speech (POS) tag information, we place 
the topic/AD-expression distribution, ??,?,?  (the 
prior over the indicator variable ??,?,?) in the term 
plate (Figure 1)  and set it using a Max-Ent mod-
el conditioned on the observed context ??,?,?  as-
sociated with ??,?,?  and the learned Max-Ent 
parameters ? (details in ?4.1). In this work, we 
use both lexical and POS features of the previ-
ous, current and next POS tags/lexemes of the 
term ??,?,?  as the contextual information, 
i.e., ??,?,? = [?????,?,??1 , ?????,?,? , ?????,?,?+1 ,
??,?,??1,??,?,? , ??,?,?+1], which is used to produce 
feature functions for Max-Ent. For phrasal terms 
(n-grams), all POS tags and lexemes of ??,?  are 
considered as contextual information for compu-
ting feature functions in Max-Ent. DTM has the 
following generative process: 
A. For each AD-expression type ?, draw ???~???(??) 
B. For each topic t, draw ???~???(??) 
C. For each author ? ? {1 ??}: 
i. Draw ???~???(??) 
ii. Draw ???~???(??) 
iii. For each document/post ? ? {1 ???}: 
I. For each term ??,?,?, ? ? {1 ???,?}: 
a. Set ??,?,? ? ??????(??,?,?; ?) 
b. Draw ??,?,?~?????????(??,?,?) 
c. if (??,?,? =  ???) // ??,?is an AD-expression term 
Draw ??,?,?~ ????(??
?) 
else // ??,?,? =  ???, ??,?,?is a topical term 
Draw ??,?,?~ ????(??
?) 
d. Emit ??,?,?~????(???,?,?
??,?,?) 
4.1 Inference 
We employ posterior inference using Monte Car-
                                                          
3 Topics in most topic models (e.g., LDA (Blei et al, 2003)) 
are unigram distributions and a document is treated as an 
exchangeable bag-of-words. This offers a computational 
advantage over models considering word orders (Wallach, 
2006). As our goal is to enhance the expressiveness of 
DTM (rather than ?modeling? word order), we use 1-4 
grams preserving the advantages of exchangeable modeling. 
 
 
 
 
 
 
 
 
 
Figure 1: Plate notation of DTM 
Variable/Function Description 
?; ?; ? 
An author ?; set of all authors; docu-
ment, ? 
(?,?); ?? 
Post ? by author ?; Set of all posts by 
? 
?;?;? 
# of topics; expression types; vocabu-
lary 
??,?,?; ??,? 
??? term in (?,?); Total # of terms in 
(?,?) 
??,?,?  Distribution over topics and AD-
expressions 
??,?,? 
Associated feature context of observed 
??,?,? 
? Learned Max-Ent parameters 
??,?,? ? {???, ???} 
Binary indicator/switch variable ( topic 
(???) or AD-expression (???) ) for ??,?,? 
??
?; 
??
?(??,??
?  , 
??,?????
? ) 
??s distribution over topics ; expression 
types (Agreement: ??,??
? , Disagree-
ment: ??,?????
? ) 
??,?
? ;??,?,?
?  
Topic distribution of post ? by author 
?; Probability mass of topic ? in ??,?
? . 
??,?,??{??,?????}
?  
??,?
? ; 
Expression type distribution of post ? 
by author ?; Corresponding probability 
masses of Agreement: ??,?,?=??
?  and 
Disagreement in ??,?,?=?????
? . 
??,?,? Topic/Expression type of ??,?,? 
??
?;  ??
?  
Topic ??s ; Expression type ??s distri-
bution over vocabulary terms 
??; ??; ??; ?? Dirichlet priors of ??
?;  ??
? ;??
?;  ??
? 
??,?
??; ??,?
??  
# of times topic ?; expression type ? 
assigned to ? 
??,?
??; ??,?
??  
# of times term ? appears in topic ?; 
expression type ? 
Table 2: List of notations 
 
x 
? 
 
z 
 
r 
 
 
w Na, d 
? 
Da 
?E ?E  
A 
?T ?T  
?T  T 
?E  E 
?E ?T 
1683
lo Gibbs sampling. Denoting the random varia-
bles {?, ?, ?}  by singular 
scripts{??, ??, ??} ,?1?? , where ? = ? ? ??,??? , a 
single iteration consists of performing the fol-
lowing sampling: 
?(?? = ?, ?? = ???|???,???,???,?? = ?) ?
exp (? ????(??,?,?,???)
?
?=1 )
? exp (? ????(??,?,?,?)
?
?=1 )??{??,??}
?
??,?
??
??
+??
??,(?)
??
??
+???
?
??,?
??
??
+??
??,(?)
??
??
+???
  (1) 
?(?? = ?, ?? = ???|???,???,??? ,?? = ?) ?
exp (? ????(??,?,?,???)
?
?=1 )
? exp (? ????(??,?,?,?)
?
?=1 )??{??,??}
?
??,?
??
??
+??
??,(?)
??
??
+???
?
??,?
??
??
+??
??,(?)
??
??
+???
  (2) 
where ? = (?,?, ?) denotes the ???  term of docu-
ment ? by author ? and the subscript ?? denotes 
assignments excluding the term at (?,?, ?). Omis-
sion of the latter index denoted by (?) represents 
the marginalized sum over the latter index. 
Count variables are detailed in Table 1 (last two 
rows). ?1??  are the parameters of the learned 
Max-Ent model corresponding to the ?  binary 
feature functions ?1?? for Max-Ent. The learned 
Max-Ent ?  parameters in conjunction with the 
observed context, ??,?,? feed the supervision sig-
nal for updating the topic/expression switch pa-
rameter, ? in equations (1) and (2).  
The hyper-parameters for the model were set 
to the values ??= ??= 0.1 and ??  = 50/?, ??  = 
50/ ? , suggested in (Griffiths and Steyvers, 
2004). Model parameters were estimated after 
5000 Gibbs iterations with a burn-in of 1000 it-
erations. The Max-Ent parameters ?  were 
learned using 500 labeled terms in each domain 
(politics:- topical: 376 and AD-expression: 124; 
religion:- topical: 349 and AD-expression: 151) 
appearing at least 10 times in debate threads oth-
er than the data in Table 1 (we do so since the 
data in Table 1(c) is later used in the classifica-
tion experiments in ?6.1). 
Table 3 lists some top AD-expressions discov-
ered by DTM. We see that DTM can cluster 
many correct AD-expressions, e.g., ?I disagree?, 
?I refute?, ?don?t accept?, etc. in disagreement; 
and ?I agree?, ?you?re correct?, ?agree with 
you?, etc. in agreement. Further, it also discovers 
highly specific and more distinctive expressions 
beyond those used in Max-Ent training (marked 
blue in italics), e.g., ?I don?t buy your?, ?can you 
prove,? ?you fail to?, and ?you have no clue? in 
disagreement; and phrases like ?valid point?, 
?rightly said?, ?I do support?, and ?very well 
put? in agreement. In ?6.1, we will see that these 
AD-expressions serve as high quality features 
for predicting tolerance. 
Lastly, we note that DTM also estimates sev-
eral pieces of useful information (e.g., AD-
expressions, posterior estimates of author?s argu-
ing nature, ??? ; latent topics and expressions, 
??
?;  ??
? , etc.). These will be used to produce a 
rich set of user behavioral features for character-
izing tolerance in ?5.3. 
5 Feature Engineering 
We now propose features which will be used for 
model building to classify tolerant and intolerant 
authors in Table 1(c). We use three sets of fea-
tures. 
5.1 Language based Features of Tolerance 
Word and POS n-grams: As tolerance in com-
munication is directly reflected in language us-
age, word n-grams are obvious features. We also 
use POS tags (obtained using Stanford Tagger4) 
as features. The rationale of using POS tag based 
features is that intolerant communications are 
often characterized by hate/egotistic speech 
which have pronounced use of specific part of 
speech (e.g., pronouns) (Zingo, 1998). 
Heuristic Factor Analysis: In psycholinguistics, 
factor analysis refers to the process of finding 
groups of semantically similar linguistic con-
structs (words/phrases). It is also called meaning 
extraction in (Chung and Pennebaker, 2007). As 
tolerance in discussions is characterized by rea-
soned expressions which often accompany 
sourcing (e.g., providing a hyperlink, making an 
attempt to clarify with some evidence, etc.), we 
compiled a list of reasoned and sourced expres-
sions (shown in Table 4) from prior works 
                                                          
4 http://nlp.stanford.edu/software/tagger.shtml 
Disagreement expressions (??=????????????
?  ) 
I, disagree, I don?t, I disagree, argument, reject, claim, I reject, 
I refute, and, your, I refuse, won?t, the claim, nonsense, I con-
test, dispute, I think, completely disagree, don?t accept, don?t 
agree, incorrect, doesn?t, hogwash, I don?t buy your, I really 
doubt, your nonsense, true, can you prove, argument fails, you 
fail to, your assertions, bullshit, sheer nonsense, doesn?t make 
sense, you have no clue, how can you say, do you even, contra-
dict yourself, ? 
Agreement expressions (??=?????????
? ) 
agree, I, correct, yes, true, accept, I agree, don?t, indeed correct, 
your, point, that, I concede, is valid, your claim, not really, 
would agree, might, agree completely, yes indeed, absolutely, 
you?re correct, valid point, argument, the argument, proves, do 
accept, support, agree with you, rightly said, personally, well 
put, I do support, personally agree, doesn?t necessarily, exactly, 
very well put, absolutely correct, kudos, point taken,... 
Table 3: Top terms (comma delimited) of two expres-
sion types. Red (bold) terms denote possible errors. 
Blue (italics) terms are newly discovered; rest (black) 
terms have been used in Max-Ent training. 
 
1684
(Chung and Pennebaker, 2007; Flor and Hadar, 
2005; Moxey and Sanford, 2000; Pennebaker, et 
al.,  2007).  
5.2 Debate Expression Features 
AD-expressions: As we have seen in ?4, DTM 
can discover specific agreement and disagree-
ment expressions in debates. We use these ex-
pressions as another feature set. Estimated AD-
expressions (Table 3) serve as a principled way 
of performing factor analysis in debates instead 
of heuristic factor analysis as in Table 4 used in 
prior works.  
As the AD-expression types are modeled as 
Dirichlet distributions (??~???(??)), due to the 
smoothing effect, each term in the vocabulary 
has some non-zero probability mass associated 
with the expression types. To ensure that the dis-
covered expressions are representative AD-
expressions, we only consider the terms in ?? 
with ?(?|?) = ??,?
? > 0.001  as probability 
masses lower than 0.001 are more due to the 
smoothing effect of Dirichlet distribution than 
true correlation. 
5.3 User Behavioral Features 
Here we propose several features of user interac-
tion which reflect the socio-psychological state 
of tolerance while participating in discussions. 
We note that these features rely on the posterior 
estimates of latent variables ??, ?, and ? in DTM 
(?4) and are thus difficult to obtain without 
modeling. 
Overall Arguing Nature: The posterior on ??
? 
(Table 2) for each author, ? gives an estimate of 
??s overall arguing nature (agreeing or disagree-
ing). We use the probability mass assigned to 
each arguing nature type as a user behavioral 
feature. This gives us two features ?1, ?2 as fol-
lows: 
?1(?) =  ??,??
?  ;   ?2(?) =  ??,?????
?      (3) 
Behavioral Response: As intolerant users are 
likely to attract more disagreement, it is naturally 
useful to estimate the response (agreeing vs. dis-
agreeing) a user receives from other users. For 
computing behavioral response, we first use the 
posterior on ? to compute the distribution of AD-
expressions (i.e., the relative probability masses 
of agreeing and disagreeing expressions) in a 
document ? by an author ? as follows: 
??,?,??
? =
??????,?,?=??,1?????,???
??????,?,?=???,1?????,???
; 
??,?,?????
? =
??????,?,?=?????,1?????,???
??????,?,?=???,1?????,???
     (4) 
Now to get the overall behavioral response of an 
author, ?  we take the expected value of the 
agreeing and disagreeing responses that ?  re-
ceived from other authors ??  who replied to or 
quoted ? ?s posts. The expectations below are 
taken over all posts ??  by ??  which reply/quote 
posts of ?. 
?3(?) =  ?[??? ??,??
? ]; ?4(?) =  ????? ??,?????
? ?  (5) 
Equality of Speech: In communication literature 
(Dahlgren, 2005; Habermas, 1984), equality is 
theorized as an essential element of tolerance. 
Each participant must be able to participate on an 
equal footing with others without anybody domi-
nating the discussion. In online debates, we can 
measure this phenomenon using the following 
feature: 
?5(?) = ? ??
# ?? ????? ?? ? ?? ?????? ?
# ?? ????? ?? ?????? ?
? ?[??,?,?????
? ]?  (6) 
where the inner expectation is taken over all 
posts of ? in thread ? and the outer expectation is 
taken over all threads ?  in which ?  participated. 
The above definition computes the aggressive 
posting behavior of author ? whereby he tires to 
dominate the thread by posting more than others. 
The aggressive posting behavior is weighted by 
author?s disagreeing nature because a person 
usually exhibits a dominating nature when he 
pushes hard to establish his ideology (which is 
often in disagreement with others) (Moxey and 
Sanford, 2000).  
Topic Shifts: An interesting phenomenon of hu-
man (social) psyche is that when people are una-
ble to logically argue their stances and feel they 
are losing the debate, they often try to belit-
tle/deride others by pulling unrelated topics into 
discussion (Slavin and Kriegman, 1992). This is 
Factor: Reasoning words/phrases 
because, because of, since, reason, reason being, reason is, 
reason why, due to, owing to, as in, therefore, thus, hence-
forth, hence, implies, implies that, implying, hints, hinting, 
hints towards, it follows that, it turns out, conclude, conse-
quence, consequently, the cause, rationale, the rationale, justi-
fication, the justification, provided, premise, assumption, on 
the proviso, in spite, ? 
Factor: Sourcing words/phrases 
presence of hyperlinks/urls, source, reference, for example, 
for instance, namely, to explain, to detail, to clarify, to eluci-
date, to illustrate, to be precise, furthermore, moreover, apart 
from, besides, we find, ? 
 
Table 4: Heuristic Factor Analysis (HFA). 
Words/Phrases in each factor compiled from prior 
works in psycholinguistics. 
1685
referred to as topic shifts. Topic shifts thus have a 
relation with tolerance in deliberation. Stromer-
Galley (2005) reported that if the discussion is 
off topic, then tolerance or deliberation cannot 
meet its objective of deep consideration of an 
issue. Hence, the average topic shifts of an au-
thor, ? across various posts in a thread can serve 
as a good feature for measuring tolerance. We 
use the posterior on per-document topic distribu-
tion, ??,?,?? =
??????,?,?=?,1?????,???
??????,?,?=???,1?????,???
 to measure topic 
shifts using KL-Divergence as follows: 
?6 = ? ?avg?,??? ?????? ? ???????,?
? ||??,??
? ???     (7) 
We first compute author, ??s average topic shifts 
in a thread, ? which measures his topic shifts in ?. 
But this only gives us his behavior in one thread. 
To capture his overall behavior, we take the ex-
pected value of this behavior over all threads in 
which ?  participated. We take average KL-
divergence (KL-Div.) over all pairs of posts by ? 
in a given thread to account for the asymmetry of 
KL-Div. 
Finally, we note that by no means do we claim 
that the mere presence and a large value of any of 
the above features imply that a user is intolerant 
or tolerant. They are indicators of the phenome-
non of tolerance in discussions/debates. The ac-
tual prediction is done using the learned models 
in ?6.1. 
6 Experimental Evaluation 
We now detail the experiments that investigate 
the strengths of features in ?5. In particular, we 
first consider the task of classifying whether an 
author is tolerant or intolerant in discussions. 
Then, we analyze how disagreement affects tol-
erance. 
6.1 Tolerant and Intolerant Classification 
Here, we show that the features in ?5 can help 
build accurate models for predicting tolerance. 
We employ a linear kernel 5  SVM (using the 
SVMLight system (Joachims, 1999)) and report 5-
fold cross validation (CV) results on the task of 
predicting the socio-psychological nature of us-
ers? communication: tolerant vs. intolerant in 
politics and religion domains (Table 1(c)). Note 
that for each fold of 5-fold CV, DTM was run on 
the full data of each domain (Table 1(a)) exclud-
ing the users (and their associated posts) in the 
test set of that fold for generating the features of 
the training instances (users). The learned DTM 
                                                          
5 Other kernels (rbf, poly, sigmoid) did not perform as well. 
was then fitted (using the approach in (Hofmann, 
1999)) to the test set users and their posts for 
generating the features of the test instances.  
To investigate the effectiveness of the pro-
posed framework, we incrementally add feature 
sets starting with the baseline features.  Word 
unigrams and bigrams (inclusive of unigrams)6  
serve as our first baseline (B1a, B1b). Word + 
POS bigrams is our second baseline (B2). 
?Word? in B2 uses bigrams as B1b gives better 
results. B2 + Heuristic Factor Analysis (HFA) 
(Table 4) serve as our third baseline (B3). Table 
5 shows the experiment results. We note the fol-
lowing: 
1. Across both domains, adding POS bigrams 
slightly improves classification accuracy and 
F1-score beyond standard word unigrams and 
bigrams. Feature selection using information 
gain (IG) does not help much. 
2. Using heuristic factor analyses (HFA) of rea-
soned and sourced expressions (Table 4) 
brings about 1% and 2% improvement in ac-
curacy in politics and religion domains re-
spectively. 
3. Debate expression features (DE) in ?5.2 and 
user behavioral features (UB) in ?5.3 pro-
duced from DTM progressively improve clas-
sification accuracies by 4% and 8% in politics 
domains and 5% and 6% in religion domains. 
The improvements are also statistically signif-
icant.  
In summary, we can see that modeling made a 
major impact. It improved the accuracy by about 
10% than traditional unigram and bigram base-
lines. This shows that the debate expressions and 
user behaviors computed using the DTM model 
can capture various dimensions of (in)tolerance 
not captured by n-grams. 
6.2 How Disagreement affects Tolerance? 
We now quantitatively study the effect of disa-
greement on tolerance. We recall from ?1 that 
tolerance indicates constructive discussion and 
allows disagreement. Some level of disagree-
ment is often times an integral component of 
deliberation and tolerance (Cappella et al, 
2002). 
Disagreements, however, can be either con-
structive or destructive. The distinction is that 
the former is aimed at arriving at a consensus or 
solution, while the latter leads to polarization 
and intolerance (Sunstein, 2002). It was also 
shown in (Dahlgren, 2005) that sustained disa-
                                                          
6 Higher order n-grams did not result in better results. 
1686
greement often takes a transition towards de-
structive disagreement and is likely to lead to 
intolerance. Similar phenomena was also identi-
fied in psychology literature (Critchley, 1964). 
In such cases, the participants often stubbornly 
stick to an extreme attitude, which eventually 
results in intolerance and defeats the very pur-
pose of deliberative discussion.  
An intriguing research question is: What is the 
relationship between disagreement and intoler-
ance? The question is interesting from both the 
communication and psycholinguistic perspec-
tives. The best of our knowledge, this is the first 
attempt towards seeking an answer. We work in 
the context of five issues/threads in real-life 
online debates. To derive quantitative and defi-
nite conclusions, it is required to perform the 
following tasks: 
? For each issue, empirically investigate in ex-
pectation the tipping point of disagreement 
beyond which a user tends to be intolerant. 
? Further, investigate the confidence on the es-
timated tipping point (i.e., what is the likeli-
hood that the estimated tipping point is statis-
tically significant instead of chance alone). 
We formalize the above tasks in the Bayesian 
setting. Recall from Table 2 of ?4, that ??,???  (re-
spectively, ??,?????
? ) are the estimates of agreeing 
and disagreeing nature of an author and ??,???  + 
??,?????
?  = 1. Let ??(?) denote the event that in 
expectation a threshold value of 0 < ? < 1 
serves as a tipping point of disagreement beyond 
which intolerance is exhibited. Note that we em-
phasize the term ?in expectation? (taken over all 
authors). We do not mean that every author 
whose disagreement, ??,?????
? >  ? , is intolerant. 
The empirical likelihood of ??(?)  can be ex-
pressed by the following probability expression: 
????(?)? = 
??????,?????
? > ?|? = ?? ? ????,?????
? > ?|? = ??? (8) 
The events ? = ? and ? = ? denote that author 
? is intolerant and tolerant respectively. The ex-
pectation is taken over authors. Showing that ? 
indeed serves as the tipping point of disagree-
ment to exhibit intolerance corresponds is to re-
jecting the null hypothesis that the probabilities 
in (8) are equal. We employ a Fisher?s exact test 
to test significance and report confidence 
measures (using p-values) for the tipping point 
thresholds. The results are shown in Table 6. 
The threshold ? is computed using the entropy 
method in (Fayyad and Irani, 1993) as follows: 
We first fit our previously learned model (using 
the data in Table 1 (a)) to the new threads in Ta-
ble 6 and its users and posts to obtain the esti-
mates of ??,??????  and other latent variables for 
feature generation. The learned classifier in ?6.1 
is used to predict the nature of users (tolerant vs. 
Feature Setting 
Politics Religion 
Precision Recall F1 Accuracy Precision Recall F1 Accuracy 
B1a: Word unigrams 64.1 86.3 73.7 70.1 61.9 86.8 72.6 71.9 
Word unigram + IG 64.5 86.2 73.9 70.2 62.7 86.9 72.9 71.9 
B1b: Word bigrams 66.8 87.8 75.9 72.4 64.9 89.1 75.9 75.1 
B2: W+POS bigrams 68.5 86.8 76.4 73.7 66.6 88.4 76.8 76.7 
B3: B2 + HFA(Table 4) 69.2 90.5 78.1 75.2 66.4 90.6 76.8 77.5 
B3 + DE (?5.2) 74.7 91.3 82.4? 79.5? 70.2 92.8 80.8? 82.1? 
B3 + DE + UB (?5.3) 76.1 92.2 83.1? 83.2? 71.7 93.4 82.1? 83.3? 
Table 5: Precision, Recall, F1 score on the tolerant class, and Accuracy for different feature settings across 2 
domains. DE: Debate expression features (AD-expressions, Table3, ?5.2). UB: User behavioral features 
(?5.3). Improvements in F1 and Accuracy using DTM features (beyond baselines, B1-B3) are statistically 
significant (?: p<0.02; ?: p<0.01) using paired t-test with 5-fold CV. 
Thread/Issue # Posts # Users % InTol. ????,?,?????
? ? ? p-value 
Repeal Healthcare 1823 33 39.9 0.57 0.65 0.02 
Europe?s Collapse 1824 33 42.5 0.61 0.61 0.01 
Obama Euphoria 1244 26 30.7 0.66 0.71 0.01 
Socialism 831 49 44.8 0.69 0.48 0.03 
Abortion 1232 58 48.4 0.78 0.37 0.01 
Table 6: Tipping points of disagreements for intolerance (?) of different issues. ????,?,?????
? ?: the expected 
disagreement over all posts in each issue/thread, # Posts: the total number of posts, # Users: the total number 
of users/authors, % Intol: % of intolerant users in each thread, ?: the estimated tipping point, and p-value: 
computed from two-tailed Fisher?s exact test. 
1687
intolerant) in the new threads7. Then, for each 
user we have his predicted deliberative (social) 
psyche (Tolerant vs. Intolerant) and also his 
overall disagreeing nature exhibited in that 
thread (the posterior on ??,?????
? ? [0, 1]). For a 
thread, tolerant and intolerant users (data points) 
span the range [0, 1] attaining different values 
for ??,?????
? . Each candidate tipping point of disa-
greement, 0 ? ?? ? 1 results in a binary partition 
of the range with each partition containing some 
proportion of tolerant and intolerant users. We 
compute the entropy of the partition for every 
candidate tipping point in the range [0, 1]. The 
final tipping point threshold, ?  is chosen such 
that it minimizes the partition entropy based on 
the binary cut-point method in (Fayyad and 
Irani, 1993).  
Since we perform a thread level analysis, the 
results in Table 6 are thread/issue specific. We 
note the following from Table 6: 
1. Across all threads/issues, we find that the ex-
pected disagreement over all posts, ?, 
????,?,?????
? ? > 0.5 showing that in discussions 
of the reported issues, disagreement predomi-
nates. 
2. ????,?,?????
? ? also gives an estimate of overall 
heat in the issue being discussed. We find 
sensitive issues like abortion and socialism 
being more heated than healthcare, Obama, 
etc. 
3. The percentage of intolerant users increases 
with the expected overall disagreement in the 
issue except for the issue Obama euphoria. 
4. The estimated tipping point of disagreement 
to exhibit intolerance, ?  happens to vary in-
versely with the expected disagreement, 
????,?,?????
? ? except the issue Obama euphoria. 
This reflects that as overall disagreement in 
the issue increases, the tipping point of intol-
erance decreases, i.e., due to high discussion 
heat, people are likely to turn intolerant even 
with relatively small amount of disagreement. 
This finding dovetails with prior studies in 
psychology (Rokeach and Fruchter, 1956) that 
heated discussions are likely to reduce thresh-
                                                          
7 Although this prediction may not be perfect, it can be 
regarded as considerably reliable to study the trend of toler-
ance across different issues as our classifier (in ?6.1) attains 
a high (83%) classification accuracy using the full feature 
set. As judging all users across all threads would require 
reading about 7000 posts, for confirmation, we randomly 
sampled 30 authors across various threads for labeling by 
our judges. 28 out of 30 predictions produced by the classi-
fier correlated with the judges' labels, which should be suf-
ficiently accurate for our analysis. 
olds of reception leading to dogmatism, ego-
tism, and intolerance. Table 6 shows that for 
moderately heated issues (healthcare, Eu-
rope?s collapse), in expectation, author?s dis-
agreement ??,?????
?  should exceed 61-65% to 
exhibit intolerance. However, for sensitive is-
sues, we find that the tipping point is much 
lower, abortion: 37%; socialism: 48%. 
5. The issue Obama Euphoria is an exception to 
other issues? trends. Even though in expecta-
tion, it has ????,?,?????
? ?  = 66% overall disa-
greement, the percentage of intolerant users 
remains the lowest (30%) and the tipping 
point attains a highest value (? = 0.71), show-
ing more tolerance on the issue. A plausible 
reason could be that Obama is somewhat more 
liked and hence attracts less intolerance from 
users8. 
6. The p-values of the estimated tipping points, ? 
across all issues are statistically significant at 
98-99% confidence levels. 
7 Conclusion 
This work performed a deep analysis of the soci-
opsychological and psycholinguistic phenome-
non of tolerance in online discussions, which is 
an important concept in the field of communica-
tions. A novel framework is proposed, which is 
capable of characterizing and classifying toler-
ance in online discussions. Further, a novel tech-
nique was also proposed to quantitatively evalu-
ate the interplay of tolerance and disagreement. 
Our empirical results using real-life online dis-
cussions render key insights into the psycholin-
guistic process of tolerance and dovetail with 
existing theories in psychology and communica-
tions. To the best of our knowledge, this is the 
first such quantitative study. In our future work, 
we want to further this research and study the 
role of diversity of opinions in the context of 
tolerance and its relation to polarization. 
Acknowledgments 
This work was supported in part by a grant from 
National Science Foundation (NSF) under grant 
no. IIS-1111092. 
 
 
                                                          
8 This observation may be linked to the political phenome-
non of ?democratic citizenship through exposure to diverse 
perspectives? (Mutz, 2006) where it was shown that expo-
sure to heterogeneous opinions (i.e., greater disagreement), 
often enhances tolerance.  
1688
References 
Abu-Jbara, A., Dasigi, P., Diab, M. and Dragomir 
Radev. 2012. Subgroup detection in ideological 
discussions. ACL. 
Agrawal, R. Rajagopalan, S. Srikant, R. Xu. Y. 2003. 
Mining newsgroups using networks arising from 
social behavior. WWW.  
Bansal, M., Cardie, C., and Lee, L. 2008. The power 
of negative thinking: Exploiting label disagreement 
in the min-cut classification framework. In COL-
ING. 
Blei, D., A. Ng,  and M. Jordan. 2003. Latent Di-
richlet Allocation. In JMLR. 
Boyer, K.; Grafsgaard, J.; Ha, E. Y.; Phillips, R.; and 
Lester, J. 2011. An affect-enriched dialogue act 
classification model for task-oriented dialogue. In 
ACL. 
Burfoot, C.,  S. Bird,  and T. Baldwin. 2011. Collec-
tive Classification of Congressional Floor-Debate 
Transcripts. In ACL. 
Cappella, J. N., Price, V., and Nir, L. 2002. Argument 
repertoire as a reliable and valid measure of 
opinion quality: electronic dialogue during 
campaign 2000. Political Communication. Political 
Communication. 
Chen, Z., Mukherjee, A., Liu, B., Hsu, M., 
Castellanos, M., Ghosh, R. 2013. Leveraging 
Multi-Domain Prior Knowledge in Topic Models. 
In IJCAI. 
Chung, C. K., and Pennebaker, J. W. 2007. Revealing 
people?s thinking in natural language: Using an 
automated meaning extraction method in open?
ended self?descriptions,. J. of Research in 
Personality. 
Choi, Y. and Cardie, C. 2010. Hierarchical sequential 
learning for extracting opinions and their attributes. 
In ACL. 
Critchley, M. 1964. The neurology of psychotic 
speech. The British Journal of Psychiatry. 
Crocker, D. A. 2005. Tolerance and Deliberative 
Democracy. UMD Technical Report. 
Dahlgren, P. 2002. In search of the talkative public: 
Media, deliberative democracy and civic culture. 
Javnost/The Public. 
Dahlgren, Peter. 2005. The Internet, Public Spheres, 
and Political Communication: Dispersion and 
Deliberation. Political Communication. 
Escobar, O. 2012. Public Dialogue and Deliberation: 
A communication perspective for 
publicengagement practitioners. Handbook and 
Technical Report. 
Fayyad, U., and Irani, K. 1993. Multi-interval 
discretization of continuous-valued attributes for 
classification learning. In UAI. 
Fishkin, J. 1991. Democracy and deliberation. New 
Haven, CT: Yale University Press. 
Flor, M., and Hadar, U. 2005. The production of 
metaphoric expressions in spontaneous speech: A 
controlled-setting experiment. Metaphor and 
Symbol.  
Galley, M.,  K.  McKeown, J. Hirschberg,  E. 
Shriberg. 2004. Identifying agreement and 
disagreement in conversational speech: Use of 
Bayesian networks to model pragmatic 
dependencies. In ACL. 
Gastil, J. 2005. Communication as Deliberation: A 
Non-Deliberative Polemic on Communication 
Theory. Univ. of  Washington, Technical Report. 
Gastil, J., and Dillard, J. P. 1999. Increasing political 
sophistication through public deliberation. Political 
Communication. 
Gastil, John. 2007. Political communication and 
deliberation. Sage Publications. 
Griffiths, T. and Steyvers, M. 2004. Finding scientific 
topics. In PNAS. 
Gutmann, A., and Thompson, D. F. 1996. Democracy 
and disagreement. Harvard University Press. 
Habermas. 1984. The theory of communicative 
action: Reason and rationalization of society. (T. 
McCarthy, Trans. Vol. 1). Boston, MA: Beacon 
Press. 
Hillard, D., Ostendorf, M., and Shriberg, E. 2003. 
Detection of Agreement vs. Disagreement in 
Meetings: Training with Unlabeled Data. HLT-
NAACL. 
Hansen, G. J., and Hyunjung, K. 2011. Is the media 
biased against me? A meta-analysis of the hostile 
media effect research. Communication Research 
Reports, 28, 169-179. 
Hassan, A. and Radev, D. 2010. Identifying text 
polarity using random walks.In ACL. 
Hofmann, T. 1999. Probabilistic latent semantic 
analysis. In UAI.  
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. In SIGKDD.  
Joachims, T. Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and 
A. Smola (ed.), MIT-Press, 1999. 
Kim, S. and Hovy, E. 2007. Crystal: Analyzing 
predictive opinions on the web. In EMNLP-CoNLL.  
Landis, J. R. and Koch, G. G. 1977. The 
measurement of observer agreement for categorical 
data. Biometrics, 159?174.  
Lin, W. H., and Hauptmann, A. 2006. Are these 
documents written from different perspectives?: a 
test of different perspectives based on statistical 
distribution divergence. In ACL. 
Liu, B. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publisher, USA. 
Luskin, R. C., Fishkin, J. S., and Iyengar, S. 2004. 
Considered Opinions on U.S. Foreign Policy: Face-
to-Face versus Online Deliberative Polling. 
International Communication Association, New 
Orleans, LA.  
Mayfield, E. and Rose, C. P. 2011. Recognizing 
Authority in Dialogue with an Integer Linear 
Programming Constrained Model. In ACL.  
Moxey, L. M., and Sanford, A. J. 2000. 
Communicating quantities: A review of 
psycholinguistic evidence of how expressions 
determine perspectives. Applied Cognitive 
Psychology.  
Morbini, F. and Sagae, K. 2011. Joint Identification 
and Segmentation of Domain-Specific Dialogue 
Acts for Conversational Dialogue Systems. In ACL. 
1689
Murakami,  A.,  and Raymond, R. 2010. Support or 
Oppose? Classifying Positions in Online Debates 
from Reply Activities and Opinion Expressions. In 
COLING. 
Mukherjee, A. and Liu, B. 2013. Discovering User 
Interactions in Ideological Discussions. In ACL. 
Mukherjee, A. and Liu, B. 2012a. Mining 
Contentions from Discussions and Debates. In 
KDD. 
Mukherjee, A. and Liu, B. 2012b. Modeling review 
Comments. In ACL. 
Mukherjee, A. and Liu, B. 2012c. Aspect Extraction 
through Semi-Supervised Modeling. In ACL. 
Mukherjee, A. and Liu, B. 2012d. Analysis of 
Linguistic Style Accommodation in Online 
Debates. In COLING. 
Mutz, D. 2006. Hearing the Other Side: Deliberative 
Versus Participatory Democracy. Cambridge: 
Cambridge University Press, 2006.  
Pang, B. and Lee, L. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval.  
Pennebaker, J. W., Chung, C. K., Ireland, M., 
Gonzales, A., and Booth, R. J. 2007. The 
development and psychometric properties of 
LIWC2007. LIWC.Net.  
Popescu, A. and Etzioni, O. 2005. Extracting product 
features and opinions from reviews.  In EMNLP.  
Price, V., Cappella, J. N., and Nir, L. 2002. Does 
disagreement contribute to more deliberative 
opinion? Political Communication.  
Rokeach, M., and Fruchter, B. 1956. A factorial study 
of dogmatism and related concepts. The Journal of 
Abnormal and Social Psychology. 
Ryfe, D. M. (2005). Does deliberative democracy 
work? Annual review of political science.  
Slavin, M. O., and Kriegman, D. 1992. The adaptive 
design of the human psyche: Psychoanalysis, 
evolutionary biology, and the therapeutic process. 
Guilford Press.  
Somasundaran, S., J. Wiebe. 2009. Recognizing 
stances in online debates. In ACL-IJCNLP. 
Stromer-Galley, J. 2005. Conceptualizing and 
Measuring Coherence in Online Chat. Annual 
Meeting of the International Communication 
Association.  
Sunstein, C. R. 2002. The law of group polarization. 
Journal of political philosophy.  
Thomas, M.,  B.  Pang and  L.  Lee. 2006. Get out the 
vote: Determining support or opposition from 
Congressional floor-debate transcripts. In EMNLP. 
Wang, L., Lui, M., Kim, S. N., Nivre, J., and 
Baldwin, T. 2011. Predicting thread discourse 
structure over technical web forums. In EMNLP. 
Wiebe, J. 2000. Learning subjective adjectives from 
corpora. In Proc. of National Conference on AI. 
Yessenalina, A., Yue, A., Cardie, C. 2010. Multi-
level structured models for document-level 
sentiment classification. In EMNLP. 
Zhao, X.,  J.  Jiang, H. Yan,  and X.  Li. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-
LDA hybrid. In EMNLP. 
Zingo, M. T. (1998). Sex/gender Outsiders, Hate 
Speech, and Freedom of Expression: Can They Say 
that about Me? Praeger Publishers.  
1690
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 24?29,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploiting Topic based Twitter Sentiment for Stock Prediction 
Jianfeng Si* Arjun Mukherjee? Bing Liu? Qing Li* Huayi Li? Xiaotie Deng? 
*Department of Computer Science, City University of Hong Kong, Hong Kong, China 
*{ thankjeff@gmail.com, qing.li@cityu.edu.hk} 
?Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, USA 
?{ arjun4787@gmail.com, liub@cs.uic.edu, lhymvp@gmail.com} 
?AIMS Lab, Department of Computer Science, Shanghai Jiaotong University, Shanghai, China 
?deng-xt@cs.sjtu.edu.cn  
 
Abstract 
This paper proposes a technique to leverage 
topic based sentiments from Twitter to help 
predict the stock market. We first utilize a con-
tinuous Dirichlet Process Mixture model to 
learn the daily topic set. Then, for each topic 
we derive its sentiment according to its opin-
ion words distribution to build a sentiment 
time series. We then regress the stock index 
and the Twitter sentiment time series to predict 
the market. Experiments on real-life S&P100 
Index show that our approach is effective and 
performs better than existing state-of-the-art 
non-topic based methods. 
1 Introduction 
Social media websites such as Twitter, Facebook, 
etc., have become ubiquitous platforms for social 
networking and content sharing. Every day, they 
generate a huge number of messages, which give 
researchers an unprecedented opportunity to uti-
lize the messages and the public opinions con-
tained in them for a wide range of applications 
(Liu, 2012). In this paper, we use them for the 
application of stock index time series analysis. 
Here are some example tweets upon querying 
the keyword ?$aapl? (which is the stock symbol 
for Apple Inc.) in Twitter: 
1. ?Shanghai Oriental Morning Post confirm-
ing w Sources that $AAPL TV will debut 
in May, Prices range from $1600-$3200, 
but $32,000 for a 50"wow.? 
2. ?$AAPL permanently lost its bid for a ban 
on U.S. sales of the Samsung Galaxy Nex-
us http://dthin.gs/XqcY74.? 
3. ?$AAPL is loosing customers. everybody is 
buying android phones! $GOOG.? 
As shown, the retrieved tweets may talk about 
Apple?s products, Apple?s competition relation-
ship with other companies, etc. These messages 
are often related to people?s sentiments about 
Apple Inc., which can affect or reflect its stock 
trading since positive sentiments can impact 
sales and financial gains. Naturally, this hints 
that topic based sentiment is a useful factor to 
consider for stock prediction as they reflect peo-
ple?s sentiment on different topics in a certain 
time frame. 
This paper focuses on daily one-day-ahead 
prediction of stock index based on the temporal 
characteristics of topics in Twitter in the recent 
past. Specifically, we propose a non-parametric 
topic-based sentiment time series approach to 
analyzing the streaming Twitter data. The key 
motivation here is that Twitter?s streaming mes-
sages reflect fresh sentiments of people which 
are likely to be correlated with stocks in a short 
time frame. We also analyze the effect of training 
window size which best fits the temporal dynam-
ics of stocks. Here window size refers to the 
number of days of tweets used in model building. 
Our final prediction model is built using vec-
tor autoregression (VAR). To our knowledge, 
this is the first attempt to use non-parametric 
continuous topic based Twitter sentiments for 
stock prediction in an autoregressive framework. 
2 Related Work 
2.1 Market Prediction and Social Media 
Stock market prediction has attracted a great deal 
of attention in the past. Some recent researches 
suggest that news and social media such as blogs, 
micro-blogs, etc., can be analyzed to extract pub-
lic sentiments to help predict the market (La-
vrenko et al, 2000; Schumaker and Chen, 2009). 
Bollen et al (2011) used tweet based public 
mood to predict the movement of Dow Jones 
*   The work was done when the first author was visiting 
University of Illinois at Chicago. 
 
 
 
 
 
 
24
Industrial Average index. Ruiz et al (2012) stud-
ied the relationship between Twitter activities 
and stock market under a graph based view. 
Feldman et al (2011) introduced a hybrid ap-
proach for stock sentiment analysis based on 
companies? news articles.  
2.2 Aspect and Sentiment Models 
Topic modeling as a task of corpus exploration 
has attracted significant attention in recent years. 
One of the basic and most widely used models is 
Latent Dirichlet Allocation (LDA) (Blei et al, 
2003). LDA can learn a predefined number of 
topics and has been widely applied in its extend-
ed forms in sentiment analysis and many other 
tasks (Mei et al, 2007; Branavan et al, 2008; Lin 
and He, 2009; Zhao et al, 2010; Wang et al, 
2010; Brody and Elhadad, 2010; Jo and Oh, 2011; 
Moghaddam and Ester, 2011; Sauper et al, 2011; 
Mukherjee and Liu, 2012; He et al, 2012).  
The Dirichlet Processes Mixture (DPM) model 
is a non-parametric extension of LDA (Teh et al, 
2006), which can estimate the number of topics 
inherent in the data itself. In this work, we em-
ploy topic based sentiment analysis using DPM 
on Twitter posts (or tweets). First, we employ a 
DPM to estimate the number of topics in the 
streaming snapshot of tweets in each day.  
Next, we build a sentiment time series based 
on the estimated topics of daily tweets. Lastly, 
we regress the stock index and the sentiment 
time series in an autoregressive framework. 
3 Model 
We now present our stock prediction framework. 
3.1 Continuous DPM Model 
Comparing to edited articles, it is much harder to 
preset the number of topics to best fit continuous 
streaming Twitter data due to the large topic di-
versity in tweets. Thus, we resort to a non-
parametric approach: the Dirichlet Process Mix-
ture (DPM) model, and let the model estimate the 
number of topics inherent in the data itself. 
Mixture model is widely used in clustering and 
can be formalized as follows: 
   ?      (       )
 
              (1) 
where    is a data point,    is its cluster label, K 
is the number of topics,  (       ) is the sta-
tistical (topic) models: *  +   
  and     is the 
component weight satisfying      and  
?      . 
In our setting of DPM, the number of mixture 
components (topics) K is unfixed apriori but es-
timated from tweets in each day. DPM is defined 
as in (Neal, 2010): 
               (  )  
              
         (   )                 (2) 
where    is the parameter of the model that      
belongs to, and   is defined as a Dirichlet Pro-
cess with the base measure H and the concentra-
tion parameter   (Neal, 2010). 
We note that neighboring days may share the 
same or closely related topics because some top-
ics may last for a long period of time covering 
multiple days, while other topics may just last for 
a short period of time. Given a set of time-
stamped tweets, the overall generative process 
should be dynamic as the topics evolve over time. 
There are several ways to model this dynamic 
nature (Sun et al, 2010; Kim and Oh, 2011; 
Chua and Asur, 2012; Blei and Lafferty, 2006; 
Wang et al, 2008). In this paper, we follow the 
approach of Sun et al (2010) due to its generality 
and extensibility. 
Figure 1 shows the graphical model of our con-
tinuous version of DPM (which we call cDPM). 
As shown, the tweets set is divided into daily 
based collections: *         +  *    +   
     are the 
observed tweets and *    +   
     are the model pa-
rameters (latent topics) that generate these tweets. 
For each subset of tweets,    (tweets of day  ), 
we build a DPM on it. For the first day (   ), 
the model functions the same as a standard DPM, 
i.e., all the topics use the same base measure, 
      ( ). However, for later days (   ), 
besides the base measure,      ( ), we make 
use of topics learned from previous days as pri-
ors. This ensures smooth topic chains or links 
(details in ?3.2). For efficiency, we only consider 
topics of one previous day as priors. 
We use collapsed Gibbs sampling (Bishop, 
2006) for model inference. Hyper-parameters are 
set to:              ;       as in 
(Sun et al, 2010; Teh et al, 2006) which have 
been shown to work well. Because a tweet has at 
most 140 characters, we assume that each tweet 
contains only one topic. Hence, we only need to 
 
 
 
 
 
 
 
 
Figure 1: Continuous DPM. 
? 
 
   
 
   
        
 
   
 
     
 
     
 
   
 
    
    
    
   
 
 
     
 
     
 
 
     
 
           
25
sample the topic assignment    for each tweet   . 
According to different situations with respect 
to a topic?s prior, for each tweet    in   , the 
conditional distribution for    given all other 
tweets? topic assignments, denoted by    , can be 
summarized as follows: 
1.    is a new topic: Its candidate priors contain 
the symmetric base prior    ( )  and topics 
*      +   
     learned from            .  
? If    takes a symmetric base prior: 
 (     
           )  
 
     
 (    )
 (       )
?  (      )
   
   
?  ( )
   
   
           (3) 
where the first part denotes the prior proba-
bility according to the Dirichlet Process and 
the second part is the data likelihood (this 
interpretation can similarly be applied to the 
following three equations).  
? If    takes one topic k from *      +   
     as 
its prior: 
 (      
            )   
 
       
     
 (    )
 (       )
?  (         ( )     )
   
   
?  (         ( ))
   
   
 (4) 
2. k is an existing topic: We already know its 
prior. 
? If k takes a symmetric base prior: 
  (               )  
 
  
  
     
 (        ( )
  )
 (           ( )
  )
?  (           
  )
   
   
?  (      
  )
   
   
 (5) 
? If k takes topic        as its prior:  
  (               )  
  
  
     
 .        ( )
  /
 .           ( )
  /
?  (         ( )          
  )
   
   
?  (         ( )     
  )
   
   
 (6) 
Notations in the above equations are listed as 
follows: 
?      is the number of topics learned in day t-1. 
? |V| is the vocabulary size. 
?    is the document length of   . 
?      is the term frequency of word   in   . 
?       ( ) is the probability of word   in pre-
vious day?s topic k.  
?   
   is the number of tweets assigned to topic k 
excluding the current one   .  
?     
   is the term frequency of word   in topic k, 
with statistic from    excluded. While    ( )
   
denotes the marginalized sum of all words in 
topic k with statistic from    excluded. 
Similarly, the posteriors on *    ( )+  (topic 
word distributions) are given according to their 
prior situations as follows: 
? If topic k takes the base prior: 
           ( )   (      ) (         ( )) ?     (7) 
where      is the frequency of word   in topic 
k and    ( )  is the marginalized sum over all 
words. 
? otherwise, it is defined recursively as: 
    ( )  (          ( )      ) (         ( ))?  (8) 
where       serves as the topic prior for     . 
Finally, for each day we estimate the topic 
weights,    as follows:  
        ?      ?                              (9) 
where    is the number of tweets in topic k. 
3.2 Topic-based Sentiment Time Series 
Based on an opinion lexicon   (a list of positive 
and negative opinion words, e.g., good and bad), 
each opinion word,     is assigned with a po-
larity label  ( ) as ?+1? if it is positive and ?-1? 
if negative. We spilt each tweet?s text into opin-
ion part and non-opinion part. Only non-opinion 
words in tweets are used for Gibbs sampling. 
Based on DPM, we learn a set of topics from 
the non-opinion words space  . The correspond-
ing tweets? opinion words share the same topic 
assignments as its tweet. Then, we compute the 
posterior on opinion word probability,     
 ( ) 
for topic   analogously to equations (7) and (8). 
Finally, we define the topic based sentiment 
score  (   ) of topic   in day t as a weighted 
linear combination of the opinion polarity labels: 
 (   )   ?     
 ( )
   
    ( );  (   )   ,    -    (10) 
According to the generative process of cDPM, 
topics between neighboring days are linked if a 
topic k takes another topic as its prior. We regard 
this as evolution of topic k. Although there may 
be slight semantic variation, the assumption is 
reasonable. Then, the sentiment scores for each 
topic series form the sentiment time series {?, 
S(t-1, k), S(t, k), S(t+1, k), ...}. 
Figure 2 demonstrates the linking process 
where a triangle denotes a new topic (with base 
symmetric prior), a circle denotes a middle topic 
(taking a topic from the previous day as its prior, 
 
 
 
 
           0      ?       t-1          t         t+1    ?    N 
Figure 2: Linking the continuous topics via 
neighboring priors. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Continuous DPM  
 
 
? 
... 
 
 
? 
 
 
.
.
. 
 
 
? 
 
 
? ? 
26
while also supplying prior for the next day) and 
an ellipse denotes an end topic (no further topics 
use it as a prior). In this example, two continuous 
topic chains or links (via linked priors) exist for 
the time interval [t-1, t+1]: one in light grey color, 
and the other in black. As shown, there may be 
more than one topic chain/link (5-20 in our ex-
periments) for a certain time interval1.Thus, we 
sort multiple sentiment series according to their 
accumulative weights of topics over each link: 
?     
  
    
. In our experiments, we try the top 
five series and use the one that gives the best re-
sult, which is mostly the first (top ranked) series 
with a few exceptions of the second series. The 
topics mostly focus on hot keywords like: news, 
stocknews, earning, report, which stimulate ac-
tive discussions on the social media platform. 
3.3 Time Series Analysis with VAR 
For model building, we use vector autoregression 
(VAR). The first order (time steps of historical 
information to use: lag = 1) VAR model for two 
time series *  + and *  + is given by:  
                                                   
                                               (11) 
where * + are the white noises and * + are model 
parameters. We use the ?dse? library2 in the R 
language to fit our VAR model based on least 
square regression. 
 Instead of training in one period and predicting 
over another disjointed period, we use a moving 
training and prediction process under sliding 
windows3 (i.e., train in [t, t + w] and predict in-
dex on t + w + 1) with two main considerations: 
? Due to the dynamic and random nature of both 
the stock market and public sentiments, we are 
more interested in their short term relationship. 
? Based on the sliding windows, we have more 
training and testing points.  
Figure 3 details the algorithm for stock index 
prediction. The accuracy is computed based on 
the index up and down dynamics, the function 
     (    ) returns True only if   (our predic-
tion) and   (actual value) share the same index 
up or down direction. 
 
 
                                                 
1 The actual topic priors for topic links are governed by the 
four cases of the Gibbs Sampler. 
2 http://cran.r-project.org/web/packages/dse 
3  This is similar to the autoregressive moving average 
(ARMA) models. 
4 Dataset 
We collected the tweets via Twitter?s REST API 
for streaming data, using symbols of the Stand-
ard & Poor's 100 stocks (S&P100) as keywords. 
In this study, we focus only on predicting the 
S&P100 index. The time period of our dataset is 
between Nov. 2, 2012 and Feb. 7, 2013, which 
gave us 624782 tweets. We obtained the S&P100 
index?s daily close values from Yahoo Finance. 
5 Experiment 
5.1 Selecting a Sentiment Metric 
Bollen et al (2011) used the mood dimension, 
Calm together with the index value itself to pre-
dict the Dow Jones Industrial Average. However, 
their Calm lexicon is not publicly available. We 
thus are unable to perform a direct comparison 
with their system. We identified and labeled a 
Calm lexicon (words like ?anxious?, ?shocked?, 
?settled? and ?dormant?) using the opinion lexi-
con4 of Hu and Liu (2004) and computed the sen-
timent score using the method of Bollen et al 
(2011) (sentiment ratio). Our pilot experiments 
showed that using the full opinion lexicon of Hu 
and Liu (2004) actually performs consistently 
better than the Calm lexicon. Hence, we use the 
entire opinion lexicon in Hu and Liu (2004). 
5.2 S&P100INDEX Movement Prediction 
We evaluate the performance of our method by 
comparing with two baselines. The first (Index) 
uses only the index itself, which reduces the 
VAR model to the univariate autoregressive 
model (AR), resulting in only one index time 
series {  } in the algorithm of Figure 3.  
                                                 
4 http://cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar 
Parameter:  
w: training window size; lag: the order of VAR;  
Input:   : the date of time series; {  }: sentiment time 
series; {  }: index time series; 
Output: prediction accuracy. 
1. for t = 0, 1, 2, ?, N-w-1 
2. { 
3.        = VAR( ,     -  ,     -, lag); 
4.             
 =      .Predict(x[t+w+1-lag, t+w],  
  y[t+w+1-lag, t+w]); 
5.       if (     (      
        ) )   
 rightNum++;  
6.     } 
7.    Accuracy = rightNum / (N-w); 
8.    Return Accuracy; 
Figure 3: Prediction algorithm and accuracy 
 
 
 
 
 
 
 
 
 
Figure 1: Continuous DPM  
27
 When considering Twitter sentiments, existing 
works (Bollen et al, 2011, Ruiz et al, 2012) 
simply compute the sentiment score as ratio of 
pos/neg opinion words per day. This generates a 
lexicon-based sentiment time series, which is 
then combined with the index value series to give 
us the second baseline Raw.  
 In summary, Index uses index only with the 
AR model while Raw uses index and opinion 
lexicon based time series. Our cDPM uses index 
and the proposed topic based sentiment time se-
ries. Both Raw and cDPM employ the two di-
mensional VAR model. We experiment with dif-
ferent lag settings from 1-3 days. 
 We also experiment with different training 
window sizes, ranging from 15 - 30 days, and 
compute the prediction accuracy for each win-
dow size. Table 1 shows the respective average 
and best accuracies over all window sizes for 
each lag and Table 2 summarizes the pairwise 
performance improvements of averaged scores 
over all training window sizes. Figure 4 show the 
detailed accuracy comparison for lag 1 and lag 3.  
    From Table 1, 2, and Figure 4, we note: 
i. Topic-based public sentiments from tweets 
can improve stock prediction over simple sen-
timent ratio which may suffer from backchan-
nel noise and lack of focus on prevailing top-
ics. For example, on lag 2, Raw performs 
worse by 8.6% than Index itself. 
ii. cDPM outperforms all others in terms of both 
the best accuracy (lag 3) and the average ac-
curacies for different window sizes. The max-
imum average improvement reaches 25.0% 
compared to Index at lag 1 and 15.1% com-
pared to Raw at lag 3. This is due to the fact 
that cDPM learns the topic based sentiments 
instead of just using the opinion words? ratio 
like Raw, and in a short time period, some 
topics are more correlated with the stock mar-
ket than others. Our proposed sentiment time 
series using cDPM can capture this phenome-
non and also help reduce backchannel noise 
of raw sentiments.  
iii. On average, cDPM gets the best performance 
for training window sizes within [21, 22], and 
the best prediction accuracy is 68.0% on win-
dow size 22 at lag 3. 
6 Conclusions 
Predicting the stock market is an important but 
difficult problem. This paper showed that Twit-
ter?s topic based sentiment can improve the pre-
diction accuracy beyond existing non-topic based 
approaches. Specifically, a non-parametric topic-
based sentiment time series approach was pro-
posed for the Twitter stream. For prediction, vec-
tor autoregression was used to regress S&P100 
index with the learned sentiment time series. Be-
sides the short term dynamics based prediction, 
we believe that the proposed method can be ex-
tended for long range dependency analysis of 
Twitter sentiments and stocks, which can render 
deep insights into the complex phenomenon of 
stock market. This will be part of our future work. 
Acknowledgments 
This work was supported in part by a grant from 
the National Science Foundation (NSF) under 
grant no. IIS-1111092 and a strategic research 
grant from City University of Hong Kong (pro-
ject number: 7002770). 
Lag Index Raw cDPM 
1 0.48(0.54) 0.57(0.59) 0.60(0.64) 
2 0.58(0.65) 0.53(0.62) 0.60(0.63) 
3 0.52(0.56) 0.53(0.60) 0.61(0.68) 
Table 1: Average (best) accuracies over all 
training window sizes and different lags 1, 2, 3. 
Lag Raw vs. Index cDPM vs. Index cDPM vs. Raw 
1 18.8% 25.0% 5.3% 
2 -8.6% 3.4% 13.2% 
3 1.9% 17.3% 15.1% 
Table 2: Pairwise improvements among Index, 
Raw and cDPM averaged over all training win-
dow sizes. 
 
Figure 4: Comparison of prediction accuracy of 
up/down stock index on S&P 100 index for dif-
ferent training window sizes. 
0.25
0.45
0.65
15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
A
cc
u
ra
cy
 
Training window size 
Comparison on Lag 1 
Index Raw cDPM
0.25
0.35
0.45
0.55
0.65
0.75
18 19 20 21 22 23 24 25 26 27 28 29 30
A
cc
u
ra
cy
 
Training Window size 
Comparison on Lag 3 
Index Raw cDPM
28
References 
Bishop, C. M. 2006. Pattern Recognition and Machine 
Learning. Springer. 
Blei, D., Ng, A. and Jordan, M. 2003. Latent Dirichlet 
allocation. Journal of Machine Learning Research 
3:993?1022. 
Blei, D. and Lafferty, J. 2006. Dynamic topic models. 
In Proceedings of the 23rd International Confer-
ence on Machine Learning (ICML-2006). 
Bollen, J., Mao, H. N., and Zeng, X. J. 2011. Twitter 
mood predicts the stock market. Journal of Com-
puter Science 2(1):1-8.  
Branavan, S., Chen, H., Eisenstein J. and Barzilay, R. 
2008. Learning document-level semantic properties 
from free-text annotations. In Proceedings of the 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2008). 
Brody, S. and Elhadad, S. 2010. An unsupervised 
aspect-sentiment model for online reviews. In Pro-
ceedings of the 2010 Annual Conference of the 
North American Chapter of the ACL (NAACL-
2010). 
Chua, F. C. T. and Asur, S. 2012. Automatic Summa-
rization of Events from Social Media, Technical 
Report, HP Labs. 
Feldman, R., Benjamin, R., Roy, B. H. and Moshe, F. 
2011. The Stock Sonar - Sentiment analysis of 
stocks based on a hybrid approach. In Proceedings 
of 23rd IAAI Conference on Artificial Intelligence 
(IAAI-2011). 
He, Y., Lin, C., Gao, W., and Wong, K. F. 2012. 
Tracking sentiment and topic dynamics from social 
media. In Proceedings of the 6th International 
AAAI Conference on Weblogs and Social Media 
(ICWSM-2012). 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. In Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining (KDD-2004). 
Jo, Y. and Oh, A. 2011. Aspect and sentiment unifica-
tion model for online review analysis. In Proceed-
ings of ACM Conference in Web Search and Data 
Mining (WSDM-2011). 
Kim, D. and Oh, A. 2011. Topic chains for under-
standing a news corpus. CICLing (2): 163-176. 
Lavrenko, V., Schmill, M., Lawrie, D., Ogilvie, P., 
Jensen, D. and Allan, J. 2000. Mining of concur-
rent text and time series. In Proceedings of the 6th 
KDD Workshop on Text Mining, 37?44. 
Lin, C. and He, Y. 2009. Joint sentiment/topic model 
for sentiment analysis. In Proceedings of ACM In-
ternational Conference on Information and 
Knowledge Management (CIKM-2009). 
Liu, B. 2012. Sentiment analysis and opinion mining. 
Morgan & Claypool Publishers.  
Mei, Q., Ling, X., Wondra, M., Su, H. and Zhai, C. 
2007. Topic sentiment mixture: modeling facets 
and opinions in weblogs. In Proceedings of Interna-
tional Conference on World Wide Web (WWW-
2007). 
Moghaddam, S. and Ester, M. 2011. ILDA: Interde-
pendent LDA model for learning latent aspects and 
their ratings from online product reviews.  In Pro-
ceedings of the Annual ACM SIGIR International 
conference on Research and Development in In-
formation Retrieval (SIGIR-2011). 
Mukherjee A. and Liu, B. 2012. Aspect extraction 
through semi-supervised modeling. In Proceedings 
of the 50th Annual Meeting of the Association for 
Computational Linguistics (ACL-2012).  
Neal, R.M. 2000. Markov chain sampling methods for 
dirichlet process mixture models. Journal of Com-
putational and Graphical Statistics, 9(2):249-265. 
Ruiz, E. J., Hristidis, V., Castillo, C., Gionis, A. and 
Jaimes, A. 2012. Correlating financial time series 
with micro-blogging activity. In Proceedings of the 
fifth ACM international conference on Web search 
and data mining (WSDM-2012), 513-522.  
Sauper, C., Haghighi, A. and Barzilay, R. 2011. Con-
tent models with attitude. Proceedings of the 49th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL). 
Schumaker, R. P. and Chen, H. 2009. Textual analysis 
of stock market prediction using breaking financial 
news. ACM Transactions on Information Systems 
27(February (2)):1?19. 
Sun, Y. Z., Tang, J. Han, J., Gupta M. and Zhao, B. 
2010. Community Evolution Detection in Dynamic 
Heterogeneous Information Networks. In Proceed-
ings of KDD Workshop on Mining and Learning 
with Graphs (MLG'2010), Washington, D.C. 
Teh, Y., Jordan M., Beal, M. and Blei, D. 2006. Hier-
archical Dirichlet processes. Journal of the Ameri-
can Statistical Association, 101[476]:1566-1581. 
Wang, C. Blei, D. and Heckerman, D. 2008. Continu-
ous Time Dynamic Topic Models. Uncertainty in 
Artificial Intelligence (UAI 2008), 579-586 
Wang, H., Lu, Y.  and Zhai, C. 2010. Latent aspect 
rating analysis on review text data: a rating regres-
sion approach. Proceedings of ACM SIGKDD In-
ternational Conference on Knowledge Discovery 
and Data Mining (KDD-2010). 
Zhao, W. Jiang, J. Yan, Y. and Li, X. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-
LDA hybrid. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP-2010). 
29
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 347?358,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Aspect Extraction with Automated Prior Knowledge Learning
Zhiyuan Chen Arjun Mukherjee Bing Liu
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60607, USA
{czyuanacm,arjun4787}@gmail.com,liub@cs.uic.edu
Abstract
Aspect extraction is an important task in
sentiment analysis. Topic modeling is a
popular method for the task. However,
unsupervised topic models often generate
incoherent aspects. To address the is-
sue, several knowledge-based models have
been proposed to incorporate prior knowl-
edge provided by the user to guide mod-
eling. In this paper, we take a major
step forward and show that in the big data
era, without any user input, it is possi-
ble to learn prior knowledge automatically
from a large amount of review data avail-
able on the Web. Such knowledge can
then be used by a topic model to discover
more coherent aspects. There are two key
challenges: (1) learning quality knowl-
edge from reviews of diverse domains,
and (2) making the model fault-tolerant
to handle possibly wrong knowledge. A
novel approach is proposed to solve these
problems. Experimental results using re-
views from 36 domains show that the pro-
posed approach achieves significant im-
provements over state-of-the-art baselines.
1 Introduction
Aspect extraction aims to extract target entities
and their aspects (or attributes) that people have
expressed opinions upon (Hu and Liu, 2004, Liu,
2012). For example, in ?The voice is not clear,?
the aspect term is ?voice.? Aspect extraction has
two subtasks: aspect term extraction and aspect
term resolution. Aspect term resolution groups ex-
tracted synonymous aspect terms together. For ex-
ample, ?voice? and ?sound? should be grouped to-
gether as they refer to the same aspect of phones.
Recently, topic models have been extensively
applied to aspect extraction because they can per-
form both subtasks at the same time while other
existing methods all need two separate steps (see
Section 2). Traditional topic models such as
LDA (Blei et al, 2003) and pLSA (Hofmann,
1999) are unsupervised methods for extracting la-
tent topics in text documents. Topics are aspects
in our task. Each aspect (or topic) is a distribution
over (aspect) terms. However, researchers have
shown that fully unsupervised models often pro-
duce incoherent topics because the objective func-
tions of topic models do not always correlate well
with human judgments (Chang et al, 2009).
To tackle the problem, several semi-supervised
topic models, also called knowledge-based topic
models, have been proposed. DF-LDA (Andrze-
jewski et al, 2009) can incorporate two forms
of prior knowledge from the user: must-links
and cannot-links. A must-link implies that two
terms (or words) should belong to the same topic
whereas a cannot-link indicates that two terms
should not be in the same topic. In a similar but
more generic vein, must-sets and cannot-sets are
used in MC-LDA (Chen et al, 2013b). Other re-
lated works include (Andrzejewski et al, 2011,
Chen et al, 2013a, Chen et al, 2013c, Mukher-
jee and Liu, 2012, Hu et al, 2011, Jagarlamudi et
al., 2012, Lu et al, 2011, Petterson et al, 2010).
They all allow prior knowledge to be specified by
the user to guide the modeling process.
In this paper, we take a major step further. We
mine the prior knowledge directly from a large
amount of relevant data without any user inter-
vention, and thus make this approach fully au-
tomatic. We hypothesize that it is possible to
learn quality prior knowledge from the big data
(of reviews) available on the Web. The intuition
is that although every domain is different, there
is a decent amount of aspect overlapping across
domains. For example, every product domain
has the aspect/topic of ?price,? most electronic
products share the aspect ?battery? and some also
share ?screen.? Thus, the shared aspect knowl-
347
edge mined from a set of domains can poten-
tially help improve aspect extraction in each of
these domains, as well as in new domains. Our
proposed method aims to achieve this objective.
There are two major challenges: (1) learning qual-
ity knowledge from a large number of domains,
and (2) making the extraction model fault-tolerant,
i.e., capable of handling possibly incorrect learned
knowledge. We briefly introduce the proposed
method below, which consists of two steps.
Learning quality knowledge: Clearly, learned
knowledge from only a single domain can be er-
roneous. However, if the learned knowledge is
shared by multiple domains, the knowledge is
more likely to be of high quality. We thus propose
to first use LDA to learn topics/aspects from each
individual domain and then discover the shared as-
pects (or topics) and aspect terms among a sub-
set of domains. These shared aspects and aspect
terms are more likely to be of good quality. They
can serve as the prior knowledge to guide a model
to extract aspects. A piece of knowledge is a set
of semantically coherent (aspect) terms which are
likely to belong to the same topic or aspect, i.e.,
similar to a must-link, but mined automatically.
Extraction guided by learned knowledge: For
reliable aspect extraction using the learned prior
knowledge, we must account for possible errors
in the knowledge. In particular, a piece of au-
tomatically learned knowledge may be wrong or
domain specific (i.e., the words in the knowledge
are semantically coherent in some domains but
not in others). To leverage such knowledge, the
system must detect those inappropriate pieces of
knowledge. We propose a method to solve this
problem, which also results in a new topic model,
called AKL (Automated Knowledge LDA), whose
inference can exploit the automatically learned
prior knowledge and handle the issues of incorrect
knowledge to produce superior aspects.
In summary, this paper makes the following
contributions:
1. It proposes to exploit the big data to learn prior
knowledge and leverage the knowledge in topic
models to extract more coherent aspects. The
process is fully automatic. To the best of our
knowledge, none of the existing models for as-
pect extraction is able to achieve this.
2. It proposes an effective method to learn qual-
ity knowledge from raw topics produced using
review corpora from many different domains.
3. It proposes a new inference mechanism for
topic modeling, which can handle incorrect
knowledge in aspect extraction.
2 Related Work
Aspect extraction has been studied by many re-
searchers in sentiment analysis (Liu, 2012, Pang
and Lee, 2008), e.g., using supervised sequence
labeling or classification (Choi and Cardie, 2010,
Jakob and Gurevych, 2010, Kobayashi et al, 2007,
Li et al, 2010, Yang and Cardie, 2013) and us-
ing word frequency and syntactic patterns (Hu
and Liu, 2004, Ku et al, 2006, Liu et al, 2013,
Popescu and Etzioni, 2005, Qiu et al, 2011, So-
masundaran and Wiebe, 2009, Wu et al, 2009, Xu
et al, 2013, Yu et al, 2011, Zhao et al, 2012, Zhou
et al, 2013, Zhuang et al, 2006). However,
these works only perform extraction but not as-
pect term grouping or resolution. Separate aspect
term grouping has been done in (Carenini et al,
2005, Guo et al, 2009, Zhai et al, 2011). They
assume that aspect terms have been extracted be-
forehand.
To extract and group aspects simultaneously,
topic models have been applied by researchers
(Branavan et al, 2008, Brody and Elhadad, 2010,
Chen et al, 2013b, Fang and Huang, 2012, He
et al, 2011, Jo and Oh, 2011, Kim et al, 2013,
Lazaridou et al, 2013, Li et al, 2011, Lin and
He, 2009, Lu et al, 2009, Lu et al, 2012, Lu and
Zhai, 2008, Mei et al, 2007, Moghaddam and Es-
ter, 2013, Mukherjee and Liu, 2012, Sauper and
Barzilay, 2013, Titov and McDonald, 2008, Wang
et al, 2010, Zhao et al, 2010). Our proposed AKL
model belongs to the class of knowledge-based
topic models. Besides the knowledge-based topic
models discussed in Section 1, document labels
are incorporated as implicit knowledge in (Blei
and McAuliffe, 2007, Ramage et al, 2009). Ge-
ographical region knowledge has also been con-
sidered in topic models (Eisenstein et al, 2010).
All of these models assume that the prior knowl-
edge is correct. GK-LDA (Chen et al, 2013a) is
the only knowledge-based topic model that deals
with wrong lexical knowledge to some extent. As
we will see in Section 6, AKL outperformed GK-
LDA significantly due to AKL?s more effective er-
ror handling mechanism. Furthermore, GK-LDA
does not learn any prior knowledge.
Our work is also related to transfer learning to
some extent. Topic models have been used to help
348
Input: Corpora DL for knowledge learning
Test corpora DT
1: // STEP 1: Learning prior knowledge.
2: for r = 0 to R do // Iterate R+ 1 times.
3: for each domain corpus D
i
?DL do
4: if r = 0 then
5: A
i
? LDA(D
i
);
6: else
7: A
i
? AKL(D
i
,K);
8: end if
9: end for
10: A? ?
i
A
i
;
11: TC ? Clustering(A);
12: for each cluster T
j
? TC do
13: K
j
? FPM(T
j
);
14: end for
15: K ? ?
j
K
j
;
16: end for
17: // STEP 2: Using the learned knowledge.
18: for each test corpus D
i
?DT do
19: A
i
? AKL(D
i
,K);
20: end for
Figure 1: The proposed overall algorithm.
transfer learning (He et al, 2011, Pan and Yang,
2010, Xue et al, 2008). However, transfer learn-
ing in these papers is for traditional classification
rather than topic/aspect extraction. In (Kang et al,
2012), labeled documents from source domains
are transferred to the target domain to produce
topic models with better fitting. However, we do
not use any labeled data. In (Yang et al, 2011), a
user provided parameter indicating the technical-
ity degree of a domain was used to model the lan-
guage gap between topics. In contrast, our method
is fully automatic without human intervention.
3 Overall Algorithm
This section introduces the proposed overall algo-
rithm. It consists of two main steps: learning qual-
ity knowledge and using the learned knowledge.
Figure 1 gives the algorithm.
Step 1 (learning quality knowledge, Lines 1-
16): The input is the review corpora DL from
multiple domains, from which the knowledge is
automatically learned. Lines 3 and 5 run LDA on
each review domain corpus D
i
? DL to gener-
ate a set of aspects/topics A
i
(lines 2, 4, and 6-
9 will be discussed below). Line 10 unions the
topics from all domains to give A. Lines 11-14
cluster the topics in A into some coherent groups
(or clusters) and then discover knowledgeK
j
from
each group of topics using frequent pattern mining
(FPM) (Han et al, 2007). We will detail these in
Section 4. Each piece of the learned knowledge
is a set of terms which are likely to belong to the
same aspect.
Iterative improvement: The above process can
actually run iteratively because the learned knowl-
edge K can help the topic model learn better top-
ics in each domain D
i
? DL, which results in
better knowledge K in the next iteration. This it-
erative process is reflected in lines 2, 4, 6-9 and 16.
We will examine the performance of the process at
different iterations in Section 6.2. From the sec-
ond iteration, we can use the knowledge learned
from the previous iteration (lines 6-8). The learned
knowledge is leveraged by the new model AKL,
which is discussed below in Step 2.
Step 2 (using the learned knowledge, Lines 17-
20): The proposed model AKL is employed to use
the learned knowledge K to help topic modeling
in test domains DT , which can be DL or other
unseen domains. The key challenge of this step is
how to use the learned prior knowledge K effec-
tively in AKL and deal with possible errors in K.
We will elaborate them in Section 5.
Scalability: the proposed algorithm is naturally
scalable as both LDA and AKL run on each do-
main independently. Thus, for all domains, the
algorithm can run in parallel. Only the resulting
topics need to be brought together for knowledge
learning (Step 1). These resulting topics used in
learning are much smaller than the domain corpus
as only a list of top terms from each topic are uti-
lized due to their high reliability.
4 Learning Quality Knowledge
This section details Step 1 in the overall algorithm,
which has three sub-steps: running LDA (or AKL)
on each domain corpus, clustering the resulting
topics, and mining frequent patterns from the top-
ics in each cluster. Since running LDA is simple,
we will not discuss it further. The proposed AKL
model will be discussed in Section 5. Below we
focus on the other two sub-steps.
4.1 Topic Clustering
After running LDA (or AKL) on each domain cor-
pus, a set of topics is obtained. Each topic is
a distribution over terms (or words), i.e., terms
with their associated probabilities. Here, we use
only the top terms with high probabilities. As dis-
cussed earlier, quality knowledge should be shared
349
by topics across several domains. Thus, it is nat-
ural to exploit a frequency-based approach to dis-
cover frequent set of terms as quality knowledge.
However, we need to deal with two issues.
1. Generic aspects, such as price with aspect
terms like cost and pricy, are shared by many
(even all) product domains. But specific as-
pects such as screen, occur only in domains
with products having them. It means that dif-
ferent aspects may have distinct frequencies.
Thus, using a single frequency threshold in the
frequency-based approach is not sufficient to
extract both generic and specific aspects be-
cause the generic aspects will result in numer-
ous spurious aspects (Han et al, 2007).
2. A term may have multiple senses in different
domains. For example, light can mean ?of little
weight? or ?something that makes things visi-
ble?. A good knowledge base should have the
capacity of handling this ambiguity.
To deal with these two issues, we propose to
discover knowledge in two stages: topic clustering
and frequent pattern mining (FPM).
The purpose of clustering is to group raw topics
from a topic model (LDA or AKL) into clusters.
Each cluster contains semantically related topics
likely to indicate the same real-world aspect. We
then mine knowledge from each cluster using an
FPM technique. Note that the multiple senses of a
term can be distinguished by the semantic mean-
ings represented by the topics in different clusters.
For clustering, we tried k-means and k-
medoids (Kaufman and Rousseeuw, 1990), and
found that k-medoids performs slightly better.
One possible reason is that k-means is more sen-
sitive to outliers. In our topic clustering, each data
point is a topic represented by its top terms (with
their probabilities normalized). The distance be-
tween two data points is measured by symmetrised
KL-Divergence.
4.2 Frequent Pattern Mining
Given topics within each cluster, this step finds
sets of terms that appear together in multiple top-
ics, i.e., shared terms among similar topics across
multiple domains. Terms in such a set are likely
to belong to the same aspect. To find such sets of
terms within each cluster, we use frequent pattern
mining (FPM) (Han et al, 2007), which is suited
for the task. The probability of each term is ig-
nored in FPM.
FPM is stated as follows: Given a set of trans-
actions T, where each transaction t
i
? T is a set
of items from a global item set I , i.e., t
i
? I . In
our context, t
i
is the topic vector comprising the
top terms of a topic (no probability attached). T
is the collection of all topics within a cluster and
I is the set of all terms in T. The goal of FPM is
to find all patterns that satisfy some user-specified
frequency threshold (also called minimum support
count), which is the minimum number of times
that a pattern should appear in T. Such patterns
are called frequent patterns. In our context, a pat-
tern is a set of terms which have appeared multiple
times in the topics within a cluster. Such patterns
compose our knowledge base as shown below.
4.3 Knowledge Representation
As the knowledge is extracted from each cluster
individually, we represent our knowledge base as
a set of clusters, where each cluster consists of a
set of frequent 2-patterns mined using FPM, e.g.,
Cluster 1: {battery, life}, {battery, hour},
{battery, long}, {charge, long}
Cluster 2: {service, support}, {support, cus-
tomer}, {service, customer}
Using two terms in a set is sufficient to cover the
semantic relationship of the terms belonging to the
same aspect. Longer patterns tend to contain more
errors since some terms in a set may not belong to
the same aspect as others. Such partial errors hurt
performance in the downstream model.
5 AKL: Using the Learned Knowledge
We now present the proposed topic model AKL,
which is able to use the automatically learned
knowledge to improve aspect extraction.
5.1 Plate Notation
Differing from most topic models based on topic-
term distribution, AKL incorporates a latent clus-
ter variable c to connect topics and terms. The
plate notation of AKL is shown in Figure 2. The
inputs of the model are M documents, T top-
ics and C clusters. Each document m has N
m
terms. We model distribution P (cluster|topic)
as ? and distribution P (term|topic, cluster) as
? with Dirichlet priors ? and ? respectively.
P (topic|document) is modeled by ? with a
Dirichlet prior ?. The terms in each document are
assumed to be generated by first sampling a topic
z, and then a cluster c given topic z, and finally
350
? ? z c w NmM
? T ? TXC? ?
Figure 2: Plate notation for AKL.
a term w given topic z and cluster c. This plate
notation of AKL and its associated generative pro-
cess are similar to those of MC-LDA (Chen et al,
2013b). However, there are three key differences.
1. Our knowledge is automatically mined which
may have errors (or noises), while the prior
knowledge for MC-LDA is manually provided
and assumed to be correct. As we will see in
Section 6, using our knowledge, MC-LDA does
not generate as coherent aspects as AKL.
2. Our knowledge is represented as clusters. Each
cluster contains a set of frequent 2-patterns
with semantically correlated terms. They are
different from must-sets used in MC-LDA.
3. Most importantly, due to the use of the new
form of knowledge, AKL?s inference mecha-
nism (Gibbs sampler) is entirely different from
that of MC-LDA (Section 5.2), which results in
superior performances (Section 6). Note that
the inference mechanism and the prior knowl-
edge cannot be reflected in the plate notation
for AKL in Figure 2.
In short, our modeling contributions are (1) the
capability of handling more expressive knowledge
in the form of clusters, (2) a novel Gibbs sampler
to deal with inappropriate knowledge.
5.2 The Gibbs Sampler
As the automatically learned prior knowledge may
contain errors for a domain, AKL has to learn
the usefulness of each piece of knowledge dy-
namically during inference. Instead of assigning
weights to each piece of knowledge as a fixed prior
in (Chen et al, 2013a), we propose a new Gibbs
sampler, which can dynamically balance the use
of prior knowledge and the information in the cor-
pus during the Gibbs sampling iterations.
We adopt a Blocked Gibbs sampler (Rosen-Zvi
et al, 2010) as it improves convergence and re-
duces autocorrelation when the variables (topic z
and cluster c in AKL) are highly related. For each
term w
i
in each document, we jointly sample a
topic z
i
and cluster c
i
(containing w
i
) based on the
conditional distribution in Gibbs sampler (will be
detailed in Equation 4). To compute this distribu-
tion, instead of considering how well z
i
matches
with w
i
only (as in LDA), we also consider two
other factors:
1. The extent c
i
corroborates w
i
given the corpus.
By ?corroborate?, we mean whether those fre-
quent 2-patterns in c
i
containing w
i
are also
supported by the actual information in the do-
main corpus to some extent (see the measure in
Equation 1 below). If c
i
corroborates w
i
well,
c
i
is likely to be useful, and thus should also
provide guidance in determining z
i
. Otherwise,
c
i
may not be a suitable piece of knowledge for
w
i
in the domain.
2. Agreement between c
i
and z
i
. By agreement
we mean the degree that the terms (union of all
frequent 2-patterns of c
i
) in cluster c
i
are re-
flected in topic z
i
. Unlike the first factor, this is
a global factor as it concerns all the terms in a
knowledge cluster.
For the first factor, we measure how well c
i
corroborates w
i
given the corpus based on co-
document frequency ratio. As shown in (Mimno
et al, 2011), co-document frequency is a good in-
dicator of term correlation in a domain. Follow-
ing (Mimno et al, 2011), we define a symmetric
co-document frequency ratio as follows:
Co-Doc(w,w
?
) =
D(w,w
?
) + 1
(D(w) +D(w
?
))?
1
2
+ 1
(1)
where (w,w
?
) refers to each frequent 2-pattern in
the knowledge cluster c
i
. D(w,w
?
) is the number
of documents that contain both termsw andw
?
and
D(w) is the number of documents containing w.
A smoothing count of 1 is added to avoid the ratio
being 0.
For the second factor, if cluster c
i
and topic z
i
agree, the intuition is that the terms in c
i
(union of
all frequent 2-patterns of c
i
) should appear as top
terms under z
i
(i.e., ranked top according to the
term probability under z
i
). We define the agree-
ment using symmetrised KL-Divergence between
the two distributions (DIST
c
and DIST
z
) cor-
responding to c
i
and z
i
respectively. As there is
no prior preference on the terms of c
i
, we use
the uniform distribution over all terms in c
i
for
DIST
c
. For DIST
z
, as only top 20 terms un-
der z
i
are usually reliable, we use these top terms
351
with their probabilities (re-normalized) to repre-
sent the topic. Note that a smoothing probability
(i.e., a very small value) is also given to every term
for calculating KL-Divergence. GivenDIST
c
and
DIST
z
, the agreement is computed with:
Agreement(c, z) =
1
KL(DIST
c
, DIST
z
)
(2)
The rationale of Equation 2 is that the lesser di-
vergence between DIST
c
and DIST
z
implies the
more agreement between c
i
and z
i
.
We further employ the Generalized Plya urn
(GPU) model (Mahmoud, 2008) which was shown
to be effective in leveraging semantically related
words (Chen et al, 2013a, Chen et al, 2013b,
Mimno et al, 2011). The GPU model here ba-
sically states that assigning topic z
i
and cluster c
i
to term w
i
will not only increase the probability
of connecting z
i
and c
i
with w
i
, but also make
it more likely to associate z
i
and c
i
with term w
?
where w
?
shares a 2-pattern with w
i
in c
i
. The
amount of probability increase is determined by
matrixA
c,w
?
,w
defined as:
A
c,w
?
,w
=
?
??
??
1, if w = w
?
?, if (w,w
?
) ? c, w 6= w
?
0, otherwise
(3)
where value 1 controls the probability increase of
w by seeingw itself, and ? controls the probability
increase of w
?
by seeing w. Please refer to (Chen
et al, 2013b) for more details.
Putting together Equations 1, 2 and 3 into a
blocked Gibbs Sampler, we can define the follow-
ing sampling distribution in Gibbs sampler so that
it provides helpful guidance in determining the
usefulness of the prior knowledge and in selecting
the semantically coherent topic.
P (z
i
= t, c
i
= c|z?i, c?i,w, ?, ?, ?,A)
?
?
(w,w
?
)?c
Co-Doc(w,w
?
)?Agreement(c, t)
?
n
?i
m,t
+ ?
?
T
t
?
=1
(n
?i
m,t
?
+ ?)
?
?
V
w
?
=1
?
V
v
?
=1
A
c,v
?
,w
?
? n
?i
t,c,v
?
+ ?
?
C
c
?
=1
(
?
V
w
?
=1
?
V
v
?
=1
A
c
?
,v
?
,w
?
? n
?i
t,c
?
,v
?
+ ?)
?
?
V
w
?
=1
A
c,w
?
,w
i
? n
?i
t,c,w
?
+ ?
?
V
v
?
=1
(
?
V
w
?
=1
A
c,w
?
,v
?
? n
?i
t,c,w
?
+ ?)
(4)
where n
?i
denotes the count excluding the current
assignment of z
i
and c
i
, i.e., z
?i
and c
?i
. n
m,t
de-
notes the number of times that topic twas assigned
to terms in document m. n
t,c
denotes the times
that cluster c occurs under topic t. n
t,c,v
refers to
the number of times that term v appears in cluster
c under topic t. ?, ? and ? are predefined Dirichlet
hyperparameters.
Note that although the above Gibbs sampler is
able to distinguish useful knowledge from wrong
knowledge, it is possible that there is no cluster
corroborates for a particular term. For every term
w, apart from its knowledge clusters, we also add
a singleton cluster for w, i.e., a cluster with one
pattern {w,w} only. When no knowledge cluster
is applicable, this singleton cluster is used. As a
singleton cluster does not contain any knowledge
information but only the word itself, Equations 1
and 2 cannot be computed. For the values of sin-
gleton clusters for these two equations, we assign
them as the averages of those values of all non-
singleton knowledge clusters.
6 Experiments
This section evaluates and compares the pro-
posed AKL model with three baseline models
LDA, MC-LDA, and GK-LDA. LDA (Blei et
al., 2003) is the most popular unsupervised topic
model. MC-LDA (Chen et al, 2013b) is a re-
cent knowledge-based model for aspect extrac-
tion. GK-LDA (Chen et al, 2013a) handles wrong
knowledge by setting prior weights using the ratio
of word probabilities. Our automatically extracted
knowledge is provided to these models. Note that
cannot-set of MC-LDA is not used in AKL.
6.1 Experimental Settings
Dataset. We created a large dataset containing
reviews from 36 product domains or types from
Amazon.com. The product domain names are
listed in Table 1. Each domain contains 1, 000 re-
views. This gives us 36 domain corpora. We have
made the dataset publically available at the web-
site of the first author.
Pre-processing. We followed (Chen et al, 2013b)
to employ standard pre-processing like lemmatiza-
tion and stopword removal. To have a fair compar-
ison, we also treat each sentence as a document as
in (Chen et al, 2013a, Chen et al, 2013b).
Parameter Settings. For all models, posterior es-
timates of latent variables were taken with a sam-
pling lag of 20 iterations in the post burn-in phase
(first 200 iterations for burn-in) with 2, 000 itera-
tions in total. The model parameters were tuned
on the development set in our pilot experiments
352
Amplifier DVD Player Kindle MP3 Player Scanner Video Player
Blu-Ray Player GPS Laptop Network Adapter Speaker Video Recorder
Camera Hard Drive Media Player Printer Subwoofer Watch
CD Player Headphone Microphone Projector Tablet Webcam
Cell Phone Home Theater System Monitor Radar Detector Telephone Wireless Router
Computer Keyboard Mouse Remote Control TV Xbox
Table 1: List of 36 domain names.
and set to ? = 1, ? = 0.1, T = 15, and ? = 0.2.
Furthermore, for each cluster, ? is set proportional
to the number of terms in it. The other param-
eters for MC-LDA and GK-LDA were set as in
their original papers. For parameters of AKL, we
used the top 15 terms for each topic in the clus-
tering phrase. The number of clusters is set to
the number of domains. We will test the sensitiv-
ity of these clustering parameters in Section 6.4.
The minimum support count for frequent pattern
mining was set empirically to min(5, 0.4?#T),
where #T is the number of transactions (i.e., the
number of topics from all domains) in a cluster.
Test Settings: We use two test settings as below:
1. (Sections 6.2, 6.3 and 6.4) Test on the same cor-
pora as those used in learning the prior knowl-
edge. This is meaningful as the learning phrase
is automatic and unsupervised (Figure 1). We
call this self-learning-and-improvement.
2. (Section 6.5) Test on new/unseen domain cor-
pora after knowledge learning.
6.2 Topic Coherence
This sub-section evaluates the topics/aspects gen-
erated by each model based on Topic Coher-
ence (Mimno et al, 2011) in test setting 1. Tra-
ditionally, topic models have been evaluated us-
ing perplexity. However, perplexity on the held-
out test set does not reflect the semantic coher-
ence of topics and may be contrary to human judg-
ments (Chang et al, 2009). Instead, the met-
ric Topic Coherence has been shown in (Mimno
-1510
-1490
-1470
-1450
-1430
0 1 2 3 4 5 6
To
pic
 Co
her
enc
e
AKL GK-LDAMC-LDA LDA
Figure 3: Average Topic Coherence of each model
at different learning iterations (Iteration 0 is equiv-
alent to LDA).
et al, 2011) to correlate well with human judg-
ments. Recently, it has become a standard prac-
tice to use Topic Coherence for evaluation of topic
models (Arora et al, 2013). A higher Topic Coher-
ence value indicates a better topic interpretability,
i.e., semantically more coherent topics.
Figure 3 shows the average Topic Coherence of
each model using knowledge learned at different
learning iterations (Figure 1). For MC-LDA or
GK-LDA, this is done by replacing AKL in lines
7 and 19 of Figure 1 with MC-LDA or GK-LDA.
Each value is the average over all 36 domains.
From Figure 3, we can observe the followings:
1. AKL performs the best with the highest Topic
Coherence values at all iterations. It is actu-
ally the best in all 36 domains. These show that
AKL finds more interpretable topics than the
baselines. Its values stabilize after iteration 3.
2. Both GK-LDA and MC-LDA perform slightly
better than LDA in iterations 1 and 2. MC-
LDA does not handle wrong knowledge. This
shows that the mined knowledge is of good
quality. Although GK-LDA uses large word
probability differences under a topic to detect
wrong lexical knowledge, it is not as effective
as AKL. The reason is that as the lexical knowl-
edge is from general dictionaries rather than
mined from relevant domain data, the words
in a wrong piece of knowledge usually have a
very large probability difference under a topic.
However, our knowledge is mined from top
words in related topics including topics from
the current domain. The words in a piece of in-
correct (or correct) knowledge often have sim-
ilar probabilities under a topic. The proposed
dynamic knowledge adjusting mechanism in
AKL is superior.
Paired t-test shows that AKL outperforms all
baselines significantly (p < 0.0001).
6.3 User Evaluation
As our objective is to discover more coherent as-
pects, we recruited two human judges. Here we
also use the test setting 1. Each topic is annotated
as coherent if the judge feels that most of its top
353
0.6
0.7
0.8
0.9
1.0
Camera Computer Headphone GPS
Pre
cisi
on
@ 5
AKL GK-LDA MC-LDA LDA
0.6
0.7
0.8
0.9
1.0
Camera Computer Headphone GPS
Pre
cisi
on
@ 10
AKL GK-LDA MC-LDA LDA
Figure 4: Average Precision@5 (Left) and Precision@10 (Right) of coherent topics from four models
in each domain. (Headphone has a lot of overlapping topics in other domains while GPS has little.)
terms coherently represent a real-world product
aspect; otherwise incoherent. For a coherent topic,
each top term is annotated as correct if it reflects
the aspect represented by the topic; otherwise in-
correct. We labeled the topics of each model
at learning iteration 1 where the same pieces of
knowledge (extracted from LDA topics at learn-
ing iteration 0) are provided to each model. After
learning iteration 1, the gap between AKL and the
baseline models tends to widen. To be consistent,
the results later in Sections 6.4 and 6.5 also show
each model at learning iteration 1. We also notice
that after a few learning iterations, the topics from
AKL model tend to have some resemblance across
domains. We found that AKL with 2 learning it-
erations achieved the best topics. Note that LDA
cannot use any prior knowledge.
We manually labeled results from four domains,
i.e., Camera, Computer, Headphone, and GPS. We
chose Headphone as it has a lot of overlapping
of topics with other domains because many elec-
tronic products use headphone. GPS was cho-
sen because it does not have much topic overlap-
ping with other domains as its aspects are mostly
about Navigation and Maps. Domains Camera and
Computer lay in between. We want to see how
domain overlapping influences the performance of
AKL. Cohen?s Kappa scores for annotator agree-
ment are 0.918 (for topics) and 0.872 (for terms).
To measure the results, we compute
Precision@n (or p@n) based on the anno-
tations, which was also used in (Chen et al,
2013b, Mukherjee and Liu, 2012).
Figure 4 shows the precision@n results for
n = 5 and 10. We can see that AKL makes im-
provements in all 4 domains. The improvement
varies in domains with the most increase in Head-
phone and the least in GPS as Headphone overlaps
more with other domains than GPS. Note that if a
domain shares aspects with many other domains,
its model should benefit more; otherwise, it is rea-
sonable to expect lesser improvements. For the
baselines, GK-LDA and MC-LDA perform simi-
larly to LDA with minor variations, all of which
are inferior to AKL. AKL?s improvements over
other models are statistically significant based on
paired t-test (p < 0.002).
In terms of the number of coherent topics, AKL
discovers one more coherent topic than LDA in
Computer and one more coherent topic than GK-
LDA and MC-LDA in Headphone. For the other
domains, the numbers of coherent topics are the
same for all models.
Table 2 shows an example aspect (battery) and
its top 10 terms produced by AKL and LDA for
each domain to give a flavor of the kind of im-
provements made by AKL. The results for GK-
LDA and MC-LDA are about the same as LDA
(see also Figure 4). Table 2 focuses on the as-
pects generated by AKL and LDA. From Table 2,
we can see that AKL discovers more correct and
meaningful aspect terms at the top. Note that
those terms marked in red and italicized are er-
rors. Apart from Table 2, many aspects are dra-
matically improved by AKL, including some com-
monly shared aspects such as Price, Screen, and
Customer Service.
6.4 Sensitivity to Clustering Parameters
This sub-section investigates the sensitivity of the
clustering parameters of AKL (again in test setting
1). The top sub-figure in Figure 5 shows the aver-
age Topic Coherence values versus the top k terms
per topic used in topic clustering (Section 4.1).
The number of clusters is set to the number of
domains (see below). We can observe that using
k = 15 top terms gives the highest value. This is
intuitive as too few (or too many) top terms may
generate insufficient (or noisy) knowledge.
The bottom sub-figure in Figure 5 shows the
average Topic Coherence given different number
354
Camera Computer Headphone GPS
AKL LDA AKL LDA AKL LDA AKL LDA
battery battery battery battery hour long battery trip
life card hour cable long battery hour battery
hour memory life speaker battery hour long hour
long life long dvi life comfortable model mile
charge usb speaker sound charge easy life long
extra hour sound hour amp uncomfortable charge life
minute minute charge connection uncomfortable headset trip destination
charger sd dvi life comfortable life purchase phone
short extra tv hdmus period money older charge
aa device hdmus tv output hard compass mode
Table 2: Example aspect Battery from AKL and LDA in each domain. Errors are italicized in red.
-1510
-1490
-1470
-1450
-1430
5 10 15 20 25 30T
opi
c C
ohe
ren
ce
#Top Terms for Clustering
-1510
-1490
-1470
-1450
-1430
20 30 40 50 60 70T
opi
c C
ohe
ren
ce
#Clusters
Figure 5: Average topic coherence of AKL versus
#top k terms (Top) and #clusters (Bottom).
-1490
-1480
-1470
-1460
-1450
AKL GK-LDA MC-LDA LDA
To
pic
 Co
her
enc
e
Figure 6: Average topic coherence of each model
tested on new/unseen domain.
of clusters. We fix the number of top terms per
topic to 15 as it yields the best result (see the top
sub-figure in Figure 5). We can see that the per-
formance is not very sensitive to the number of
clusters. The model performs similarly for 30 to
50 clusters, with lower Topic Coherence for less
than 30 or more than 50 clusters. The significance
test indicates that using 30, 40, and 50 clusters,
AKL achieved significant improvements over all
baseline models (p < 0.0001). With more do-
mains, we should expect a larger number of clus-
ters. However, it is difficult to obtain the optimal
number of clusters. Thus, we empirically set the
number of clusters to the number of domains in
our experiments. Note that the number of clus-
ters (C) is expected to be larger than the number
of topics in one domain (T ) because C is for all
domains while T is for one particular domain.
6.5 Test on New Domains
We now evaluate AKL in test setting 2, i.e., the au-
tomatically extracted knowledge K (Figure 1) is
applied in new/unseen domains other than those in
domainsDL used in knowledge learning. The aim
is to see how K can help modeling in an unseen
domain. In this set of experiments, each domain
is tested by using the learned knowledge from the
rest 35 domains. Figure 6 shows the average Topic
Coherence of each model. The values are also av-
eraged over the 36 tested domains. We can see that
AKL achieves the highest Topic Coherence value
while LDA has the lowest. The improvements of
AKL over all baseline models are significant with
p < 0.0001.
7 Conclusions
This paper proposed an advanced aspect extraction
framework which can learn knowledge automati-
cally from a large number of review corpora and
exploit the learned knowledge in extracting more
coherent aspects. It first proposed a technique to
learn knowledge automatically by clustering and
FPM. Then a new topic model with an advanced
inference mechanism was proposed to exploit the
learned knowledge in a fault-tolerant manner. Ex-
perimental results using review corpora from 36
domains showed that the proposed method outper-
forms state-of-the-art methods significantly.
Acknowledgments
This work was supported in part by a grant from
National Science Foundation (NSF) under grant
no. IIS-1111092.
355
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet Forest priors. In Proceedings
of ICML, pages 25?32.
David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorpo-
rating general domain knowledge into latent Dirich-
let alocation using first-order logic. In Proceedings
of IJCAI, pages 1171?1177.
Sanjeev Arora, Rong Ge, Yonatan Halpern, David
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A Practical Algorithm for
Topic Modeling with Provable Guarantees. In Pro-
ceedings of ICML, pages 280?288.
David M. Blei and Jon D McAuliffe. 2007. Supervised
Topic Models. In Proceedings of NIPS, pages 121?
128.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
S R K Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning Document-Level
Semantic Properties from Free-Text Annotations. In
Proceedings of ACL, pages 263?271.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL, pages 804?812.
Giuseppe Carenini, Raymond T Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In Proceedings of K-CAP, pages 11?18.
Jonathan Chang, Jordan Boyd-Graber, Wang Chong,
Sean Gerrish, and David Blei, M. 2009. Reading
Tea Leaves: How Humans Interpret Topic Models.
In Proceedings of NIPS, pages 288?296.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013a. Discovering Coherent Topics Using General
Knowledge. In Proceedings of CIKM, pages 209?
218.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013b. Exploiting Domain Knowledge in Aspect
Extraction. In Proceedings of EMNLP, pages 1655?
1667.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013c. Leveraging Multi-Domain Prior Knowledge
in Topic Models. In Proceedings of IJCAI, pages
2071?2077.
Yejin Choi and Claire Cardie. 2010. Hierarchical Se-
quential Learning for Extracting Opinions and their
Attributes. In Proceedings of ACL, pages 269?274.
Jacob Eisenstein, Brendan O?Connor, Noah A Smith,
and Eric P Xing. 2010. A Latent Variable Model
for Geographic Lexical Variation. In Proceedings of
EMNLP, pages 1277?1287.
Lei Fang and Minlie Huang. 2012. Fine Granular As-
pect Analysis using Latent Structural Models. In
Proceedings of ACL, pages 333?337.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
and Zhong Su. 2009. Product feature categoriza-
tion with multilevel latent semantic association. In
Proceedings of CIKM, pages 1087?1096.
Jiawei Han, Hong Cheng, Dong Xin, and Xifeng Yan.
2007. Frequent pattern mining: current status and
future directions. Data Mining and Knowledge Dis-
covery, 15(1):55?86.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Au-
tomatically Extracting Polarity-Bearing Topics for
Cross-Domain Sentiment Classification. In Pro-
ceedings of ACL, pages 123?131.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Analysis. In Proceedings of UAI, pages 289?296.
Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of KDD,
pages 168?177.
Yuening Hu, Jordan Boyd-Graber, and Brianna Sati-
noff. 2011. Interactive Topic Modeling. In Pro-
ceedings of ACL, pages 248?257.
Jagadeesh Jagarlamudi, Hal Daum?e III, and Raghaven-
dra Udupa. 2012. Incorporating Lexical Priors into
Topic Models. In Proceedings of EACL, pages 204?
213.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
Opinion Targets in a Single- and Cross-Domain Set-
ting with Conditional Random Fields. In Proceed-
ings of EMNLP, pages 1035?1045.
Yohan Jo and Alice H. Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM, pages 815?824.
Jeon-hyung Kang, Jun Ma, and Yan Liu. 2012. Trans-
fer Topic Modeling with Ease and Scalability. In
Proceedings of SDM, pages 564?575.
L Kaufman and P J Rousseeuw. 1990. Finding groups
in data: an introduction to cluster analysis. John
Wiley and Sons.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A Hierarchical Aspect-Sentiment
Model for Online Reviews. In Proceedings of AAAI,
pages 526?533.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting Aspect-Evaluation and Aspect-of
Relations in Opinion Mining. In Proceedings of
EMNLP, pages 1065?1074.
356
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion Extraction, Summarization and
Tracking in News and Blog Corpora. In Proceed-
ings of AAAI-CAAW, pages 100?107.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A Bayesian Model for Joint
Unsupervised Induction of Sentiment, Aspect and
Discourse Representations. In Proceedings of ACL,
pages 1630?1639.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Yingju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-Aware Review Mining and Summariza-
tion. In Proceedings of COLING, pages 653?661.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011.
Generating Aspect-oriented Multi-Document Sum-
marization with Event-aspect model. In Proceed-
ings of EMNLP, pages 1137?1146.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of CIKM, pages 375?384.
Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic
Patterns versus Word Alignment: Extracting Opin-
ion Targets from Online Reviews. In Proceedings of
ACL, pages 1754?1763.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Yue Lu and Chengxiang Zhai. 2008. Opinion inte-
gration through semi-supervised topic modeling. In
Proceedings of WWW, pages 121?130.
Yue Lu, ChengXiang Zhai, and Neel Sundaresan.
2009. Rated aspect summarization of short com-
ments. In Proceedings of WWW, pages 131?140.
Bin Lu, Myle Ott, Claire Cardie, and Benjamin K Tsou.
2011. Multi-aspect Sentiment Analysis with Topic
Models. In Proceedings of ICDM Workshops, pages
81?88.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of CIKM, pages 1642?1646.
Hosam Mahmoud. 2008. Polya Urn Models. Chap-
man & Hall/CRC Texts in Statistical Science.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of WWW, pages 171?180.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of EMNLP, pages 262?272.
Samaneh Moghaddam and Martin Ester. 2013. The
FLDA Model for Aspect-based Opinion Mining:
Addressing the Cold Start Problem. In Proceedings
of WWW, pages 909?918.
Arjun Mukherjee and Bing Liu. 2012. Aspect Extrac-
tion through Semi-Supervised Modeling. In Pro-
ceedings of ACL, pages 339?348.
Sinno Jialin Pan and Qiang Yang. 2010. A Survey on
Transfer Learning. IEEE Trans. Knowl. Data Eng.,
22(10):1345?1359.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
James Petterson, Alex Smola, Tib?erio Caetano, Wray
Buntine, and Shravan Narayanamurthy. 2010. Word
Features for Latent Dirichlet Allocation. In Pro-
ceedings of NIPS, pages 1921?1929.
AM Popescu and Oren Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proceed-
ings of HLT, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion Word Expansion and Target Extrac-
tion through Double Propagation. Computational
Linguistics, 37(1):9?27.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: a su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of EMNLP, pages
248?256.
Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas
Griffiths, Padhraic Smyth, and Mark Steyvers.
2010. Learning author-topic models from text cor-
pora. ACM Transactions on Information Systems,
28(1):1?38.
Christina Sauper and Regina Barzilay. 2013. Auto-
matic Aggregation by Joint Modeling of Aspects and
Values. J. Artif. Intell. Res. (JAIR), 46:89?127.
Swapna Somasundaran and J Wiebe. 2009. Recog-
nizing stances in online debates. In Proceedings of
ACL, pages 226?234.
Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL, pages 308?316.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: a
rating regression approach. In Proceedings of KDD,
pages 783?792.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of EMNLP, pages 1533?1541.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Mining Opinion Words and Opinion
Targets in a Two-Stage Framework. In Proceedings
of ACL, pages 1764?1773.
GR Xue, Wenyuan Dai, Q Yang, and Y Yu. 2008.
Topic-bridged PLSA for cross-domain text classifi-
cation. In Proceedings of SIGIR, pages 627?634.
357
Bishan Yang and Claire Cardie. 2013. Joint Inference
for Fine-grained Opinion Extraction. In Proceed-
ings of ACL, pages 1640?1649.
Shuang Hong Yang, Steven P. Crain, and Hongyuan
Zha. 2011. Bridging the language gap: Topic adap-
tation for documents with different technicality. In
Proceedings of AISTATS, pages 823?831.
Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-
Seng Chua. 2011. Aspect Ranking: Identifying
Important Product Aspects from Online Consumer
Reviews. In Proceedings of ACL, pages 1496?1505.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011.
Constrained LDA for grouping product features in
opinion mining. In Proceedings of PAKDD, pages
448?459.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly Modeling Aspects and Opin-
ions with a MaxEnt-LDA Hybrid. In Proceedings of
EMNLP, pages 56?65.
Yanyan Zhao, Bing Qin, and Ting Liu. 2012. Col-
location polarity disambiguation using web-based
pseudo contexts. In Proceedings of EMNLP-
CoNLL, pages 160?170.
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2013.
Collective Opinion Target Extraction in Chinese Mi-
croblogs. In Proceedings of EMNLP, pages 1840?
1850.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of CIKM, pages 43?50.
358
