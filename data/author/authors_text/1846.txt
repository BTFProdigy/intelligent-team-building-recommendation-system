Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 89?96,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
Learning to interpret novel noun-noun compounds: evidence from a
category learning experiment
Barry Devereux & Fintan Costello
School of Computer Science and Informatics, University College Dublin,
Belfield, Dublin 4, IRELAND
{barry.devereux, fintan.costello}@ucd.ie
Abstract
The ability to correctly interpret and pro-
duce noun-noun compounds such as WIND
FARM or CARBON TAX is an important part
of the acquisition of language in various do-
mains of discourse. One approach to the
interpretation of noun-noun compounds as-
sumes that people make use of distributional
information about how the constituent words
of compounds tend to combine; another as-
sumes that people make use of information
about the two constituent concepts? features
to produce interpretations. We present an ex-
periment that examines how people acquire
both the distributional information and con-
ceptual information relevant to compound
interpretation. A plausible model of the in-
terpretation process is also presented.
1 Introduction
People frequently encounter noun-noun compounds
such as MEMORY STICK and AUCTION POLITICS
in everyday discourse. Compounds are particu-
larly interesting from a language-acquisition per-
spective: children as young as two can comprehend
and produce noun-noun compounds (Clark & Bar-
ron, 1988), and these compounds play an important
role in adult acquisition of the new language and ter-
minology associated with particular domains of dis-
course. Indeed, most new terms entering the English
language are combinations of existing words (Can-
non, 1987; consider FLASH MOB, DESIGNER BABY,
SPEED DATING and CARBON FOOTPRINT).
These noun-noun compounds are also interest-
ing from a computational perspective, in that they
pose a significant challenge for current computa-
tional accounts of language. This challenge arises
from the fact that the semantics of noun-noun com-
pounds are extremely diverse, with compounds uti-
lizing many different relations between their con-
stituent words (consider the examples at the end of
the previous paragraph). Despite this diversity, peo-
ple typically interpret even completely novel com-
pounds extremely quickly, in the order of hundredths
of seconds in reaction time studies.
One approach that has been taken in both cog-
nitive psychology and computational linguistics can
be termed the relation-based approach (e.g. Gagne?
& Shoben, 1997; Kim & Baldwin, 2005). In this
approach, the interpretation of a compound is rep-
resented as the instantiation of a relational link be-
tween the modifier and head noun of the compound.
Such relations are usually represented as a set of
taxonomic categories; for example the meaning of
STUDENT LOAN might be specified with a POSSES-
SOR relation (Kim & Baldwin, 2005) or MILK COW
might be specified by a MAKES relation (Gagne? &
Shoben, 1997). However, researchers are not close
to any agreement on a taxonomy of relation cate-
gories classifying noun-noun compounds; indeed a
wide range of typologies have been proposed (e.g.
Levi, 1977; Kim & Baldwin, 2005).
In these relation-based approaches, there is often
little focus on how the meaning of the relation inter-
acts with the intrinsic properties of the constituent
concepts. Instead, extrinsic information about con-
cepts, such as distributional information about how
often different relations are associated with a con-
cept, is used. For example, Gagne? & Shoben?s
CARIN model utilizes the fact that the modifier
MOUNTAIN is frequently associated with the LO-
CATED relation (in compounds such as MOUNTAIN
CABIN or MOUNTAIN GOAT); the model does not
utilize the fact that the concept MOUNTAIN has in-
89
trinsic properties such as is large and is a geological
feature: features which may in general precipitate
the LOCATION relation.
An approach that is more typical of psycholog-
ical theories of compound comprehension can be
termed the concept-based approach (Wisniewski,
1997; Costello and Keane, 2000). With such the-
ories, the focus is on the intrinsic properties of
the constituent concepts, and the interpretation of a
compound is usually represented as a modification
of the head noun concept. So, for example, the com-
pound ZEBRA FISH may involve a modification of
the FISH concept, by asserting a feature of the ZE-
BRA concept (e.g. has stripes) for it; in this way, a
ZEBRA FISH can be understood as a fish with stripes.
Concept-based theories do not typically use distrib-
utional information about how various relations are
likely to be used with concepts.
The information assumed relevant to compound
interpretation is therefore quite different in relation-
based and concept-based theories. However, neither
approach typically deals with the issue of how peo-
ple acquire the information that allows them to in-
terpret compounds. In the case of the relation-based
approaches, for example, how do people acquire the
knowledge that the modifier MOUNTAIN tends to
be used frequently with the LOCATED relation and
that this information is important in comprehend-
ing compounds with that modifier? In the case of
concept-based approaches, how do people acquire
the knowledge that features of ZEBRA are likely to
influence the interpretation of ZEBRA FISH?
This paper presents an experiment which exam-
ines how both distributional information about re-
lations and intrinsic information about concept fea-
tures influence compound interpretation. We also
address the question of how such information is ac-
quired. Rather than use existing, real world con-
cepts, our experiment used laboratory generated
concepts that participants were required to learn dur-
ing the experiment. As well as learning the meaning
of these concepts, participants also built up knowl-
edge during the experiment about how these con-
cepts tend to combine with other concepts via re-
lational links. Using laboratory-controlled concepts
allows us to measure and control various factors that
might be expected to influence compound compre-
hension; for example, concepts can be designed to
vary in their degree of similarity to one another, to
be associated with potential relations with a certain
degree of frequency, or to have a feature which is
associated with a particular relation. It would be ex-
tremely difficult to control for such factors, or in-
vestigate the aquisition process, using natural, real
world concepts.
2 Experiment
Our experiment follows a category learning para-
digm popular in the classification literature (Medin
& Shaffer, 1978; Nosofsky, 1984). The experiment
consists of two phases, a training phase followed
by a transfer phase. In the training phase, partic-
ipants learned to identify several laboratory gener-
ated categories by examining instances of these cat-
egories that were presented to them. These cate-
gories were of two types, conceptual and relational.
The conceptual categories consisted of four ?plant?
categories and four ?beetle? categories, which par-
ticipants learned to distinguish by attending to dif-
ferences between category instances. The relational
categories were three different ways in which a bee-
tle could eat a plant. Each stimulus consisted of
a picture of a beetle instance and a picture of a
plant instance, with a relation occurring between
them. The category learning phase of our experi-
ment therefore has three stages: one for learning to
distinguish between the four beetle categories, one
for learning to distinguish between the four plant
categories, and one for learning to distinguish be-
tween the three relation categories.
The training phase was followed by a transfer
phase consisting of two parts. In the first part par-
ticipants were presented with some of the beetle-
plant pairs that they had encountered in the train-
ing phase together with some similar, though previ-
ously unseen, pairs. Participants were asked to rate
how likely each of the three relations were for the
depicted beetle-plant pair. This part of the transfer
phase therefore served as a test of how well partic-
ipants had learned to identify the appropriate rela-
tion (or relations) for pairs of conceptual category
exemplars and also tested their ability to generalize
their knowledge about the learned categories to pre-
viously unseen exemplar pairs. In the second part of
the transfer phase, participants were presented with
90
pairs of category names (rather than pairs of cat-
egory items), presented as noun-noun compounds,
and were asked to rate the appropriateness of each
relation for each compound.
In the experiment, we aim to investigate three is-
sues that may be important in determining the most
appropriate interpretation for a compound. Firstly,
the experiment aims to investigate the influence of
concept salience (i.e. how important to participants
information about the two constituent concepts are,
or how relevant to finding a relation that information
is) on the interpretation of compounds. For example,
if the two concepts referenced in a compound are
identical with respect to the complexity of their rep-
resentation, how well they are associated with vari-
ous alternative relations (and so on), but are of dif-
fering levels of animacy, we might expect the rela-
tion associated with the more animate concept to be
selected by participants more often than a different
relation associated equally strongly with the less an-
imate concept. In our experiment, all three relations
involve a beetle eating a plant. Since in each case the
beetle is the agent in the EATS(BEETLE,PLANT) sce-
nario, it is possible that the semantics of the beetle
concepts might be more relevant to relation selection
than the semantics of the plant concepts.
Secondly, the experiment is designed to inves-
tigate the effect of the ordering of the two nouns
within the compound: given two categories named
A and B, our experiment investigates whether the
compound ?A B? is interpreted in the same way as
the compound ?B A?. In particular, we were in-
terested in whether the relation selected for a com-
pound would tend to be dependent on the concept in
the head position or the concept in the modifier posi-
tion. Also of interest was whether the location of the
more animate concept in the compound would have
an effect on interpretation. For example, since the
combined concept is an instance of the head concept,
we might hypothesize that compounds for which the
head concept is more animate than the modifier con-
cept may be easier to interpret correctly.
Finally, were interested in the effect of concept
similarity: would compounds consisting of similar
constituent categories tend to be interpreted in simi-
lar ways?
learn trans. Nr Rel Bcat Pcat B1 B2 B3 P1 P2 P3
l 1 1 1 3 4 1 1 3 2 3
l 2 1 1 3 4 4 1 2 3 3
l t 3 1 1 3 1 1 1 3 3 2
l t 4 1 1 3 4 1 2 3 3 3
l t 5 2 2 2 2 2 2 2 2 3
l 6 2 2 2 2 2 1 2 3 2
l 7 2 2 2 2 3 2 2 2 1
l t 8 2 2 2 2 2 3 2 2 2
l t 9 3 3 1 3 3 3 4 1 2
l t 10 3 3 1 3 3 2 1 1 1
l 11 3 3 1 2 3 3 4 4 1
l 12 3 3 1 3 2 3 4 1 1
l t 13 1 4 4 1 1 4 4 4 4
l t 14 2 4 4 4 1 4 4 1 4
l t 15 3 4 4 4 4 4 1 1 4
t 16 - 1 1 4 1 1 4 1 1
t 17 - 3 3 3 3 3 3 3 3
t 18 - 2 4 2 2 2 4 1 4
t 19 - 4 2 4 1 4 2 2 2
Table 1: The experiment?s abstract category struc-
ture
2.1 Method
2.1.1 Participants
The participants were 42 university students.
2.1.2 Materials
The abstract category structure used in the exper-
iment is presented in Table 1. There are 19 items
in total; the first and second columns in the table
indicate if the item in question was one of the 15
items used in the learning phase of the experiment
(l) or as one of the 13 items used in the transfer stage
of the experiment (t). There were four beetle cate-
gories (Bcat), four plant categories (Pcat) and three
relation categories used in the experiment. Both the
beetle and plant categories were represented by fea-
tures instantiated on three dimensions (B1, B2 & B3
and P1, P2 & P3, respectively). The beetle and plant
categories were identical with respect to their ab-
stract structure (so, for example, the four exemplars
of Pcat1 have the same abstract features as the four
exemplars of Bcat1).
Beetles and plants were associated with particu-
lar relations; Bcat1, Bcat2 and Bcat3 were associ-
ated with Relations 1, 2 and 3, respectively, whereas
Pcat1, Pcat2 and Pcat3 were associated with Rela-
tions 3, 2 and 1, respectively. Bcat4 and Pcat4 were
not associated with any relations; the three exemplar
91
instances of these categories in the learning phase
appeared once with each of the three relations. The
features of beetles and plants were sometimes diag-
nostic of a category (much as the feature has three
wheels is diagnostic for TRICYCLE); for example, a
particular feature associated with Bcat1 is a 1 on the
B3 dimension: 3 of the 4 Bcat1 training phase exem-
plars have a 1 on dimension B3 while only one of the
remaining 11 training phase exemplars do. Also, the
intrinsic features of beetles and plants are sometimes
diagnostic of a relation category (much as the intrin-
sic feature has a flat surface raised off the ground is
diagnostic for the relational scenario sit on); values
on dimensions B1, P1, B2 and P2 are quite diag-
nostic of relations. Participants learned to identify
the plant, beetle and relation categories used in the
experiment by attending to the associations between
beetle, plant and relation categories and feature di-
agnosticity for those categories.
The beetle and plant categories were also de-
signed to differ in terms of their similarity. For ex-
ample, categories Bcat1 and Bcat4 are more simi-
lar to each other than Bcat3 and Bcat4 are: the fea-
tures for Bcat1 and Bcat4 overlap to a greater extent
than the features for Bcat3 and Bcat4 do. The aim
of varying categories with respect to their similarity
was to investigate whether similar categories would
yield similar patterns of relation likelihood ratings.
In particular, Bcat4 (and Pcat4) occurs equally often
with the three relations; therefore if category simi-
larity has no effect we would expect people to select
each of the relations equally often for this category.
However, if similarity influences participants? rela-
tion selection, then we would expect that Relation 1
would be selected more often than Relations 2 or 3.
The abstract category structure was mapped to
concrete features in a way that was unique for each
participant. Each beetle dimension was mapped ran-
domly to the concrete dimensions of beetle shell
color, shell pattern and facial expression. Each plant
dimension was randomly mapped to the concrete di-
mensions of leaf color, leaf shape, and stem color.
The three relations were randomly mapped to eats
from leaf, eats from top, and eats from trunk.
2.1.3 Procedure
The experiment consisted of a training phase and
a transfer phase. The training phase itself consisted
Figure 1: Example of a relation learning stimulus
of three sub-stages in which participants learned to
distinguish between the plant, beetle and relation
categories. During each training sub-stage, the 15
training items were presented to participants sequen-
tially on a web-page in a random order. Underneath
each item, participants were presented with a ques-
tion of the form ?What kind of plant is seen in this
picture??, ?What type of beetle is seen in this pic-
ture?? and ?How does this ?Bcat? eat this ?Pcat???
in the plant learning, beetle learning, and relation
learning training sub-stages, respectively (e.g. Fig-
ure 1). Underneath the question were radio but-
tons on which participants could select what they
believed to be the correct category; after participants
had made their selection, they were given feedback
about whether their guess had been correct (with the
correct eating relation shown taking place). Each of
the three substages was repeated until participants
had correctly classified 75% or more of the items.
Once they had successfully completed the training
phase they moved on to the transfer phase.
The transfer phase consisted of two stages, an
exemplar transfer stage and a compound transfer
stage. In the exemplar transfer stage, participants
were presented with 13 beetle-plant items, some of
which had appeared in training and some of which
were new items (see Table 1). Underneath each
picture was a question of the form ?How does this
?Bcat? eat this ?Pcat??? and three 5-point scales
for the three relations, ranging from 0 (unlikely) to
4 (likely).
The materials used in the compound transfer stage
of the experiment were the 16 possible noun-noun
92
compounds consisting of a beetle and plant category
label. Participants were presented with a sentence of
the form ?There are a lot of ?Pcat? ?Bcat?s around
at the moment.? and were asked ?What kind of eat-
ing activity would you expect a ?Pcat? ?Bcat? to
have??. Underneath, participants rated the likeli-
hood of each of the three relations on 5-point scales.
One half of participants were presented with the
compounds in the form ??Bcat? ?Pcat?? whereas
the other half of participants saw the compounds in
the form ??Pcat? ?Bcat??.
2.2 Results
2.2.1 Performance during training
Two of the participants failed to complete the
training phase. For the remaining 40 participants,
successful learning took on average 5.8 iterations of
the training items for the plant categories, 3.9 itera-
tions for the beetle categories, and 2.1 iterations for
the relation categories. The participants therefore
learned to distinguish between the categories quite
quickly, which is consistent with the fact that the cat-
egories were designed to be quite easy to learn.
2.2.2 Performance during the exemplar
transfer stage
Participants? mean ratings of relation likelihood
for the nine previously seen exemplar items is pre-
sented in Figure 2 (items 3 to 15). For each of these
items there was a correct relation, namely the one
that the item was associated with during training.
The difference between the mean response for the
correct relation (M = 2.76) and the mean response
for the two incorrect relations (M = 1.42) was sig-
nificant (ts(39) = 7.50, p < .01; ti(8) = 4.07,
p < .01). These results suggest that participants
were able to learn which relations tended to co-occur
with the items in the training phase.
Participants? mean ratings of relation likelihood
for the four exemplar items not previously seen in
training are also presented in Figure 2 (items 16 to
19). Each of these four items consisted of a proto-
typical example of each of the four beetle categories
and each of the four plant categories (with each bee-
tle and plant category appearing once; see Table 1
for details). For these four items there was no cor-
rect answer; indeed, the relation consistent with the
beetle exemplar was always different to the relation
Figure 2: Participants? mean responses for the ex-
emplar transfer items.
suggested by the plant exemplar. For each trial, then,
one relation is consistent with the beetle exemplar
(rb), one is consistent with the plant exemplar (rp)
and one is neutral (rn). One-way repeated measures
ANOVAs with response type (rb, rp or rn) as a fixed
factor and either subject or item as a random factor
were used to investigate the data. There was a signif-
icant effect of response type in both the by-subjects
and by-items analysis (Fs(2, 39) = 19.10, p < .01;
Fi(2, 3) = 24.14, p < .01). Pairwise differences be-
tween the three response types were investigated us-
ing planned comparisons in both the by-subject and
by-items analyses (with paired t-tests used in both
cases). The difference between participants? mean
response for the relation associated with the beetle
exemplar, rb (M = 2.68), and their mean response
for the neutral relation, rn (M = 1.44) was sig-
nificant (ts(39) = 5.63, p < .001; ti(3) = 5.34,
p = .01). These results suggest that participants
were strongly influenced by the beetle exemplar
when making their category judgments. However,
the difference between participants? mean response
for the relation associated with the plant exemplar,
rp (M = 1.62), and their mean response for the
neutral relation was not significant (ts(39) = 1.11,
p = .27; ti(3) = 0.97, p = .40). These re-
sults suggest that participants were not influenced
by the plant exemplar when judging relation like-
lihood. Since the beetle and plant categories have
identical abstract structure, these results suggest that
other factors (such as the animacy of a concept or the
role it plays in the relation) are important to interpre-
tation.
The data from all 13 items were also analysed
taken together. To investigate possible effects of cat-
93
egory similarity, a repeated measures ANOVA with
beetle category and response relation taken as within
subject factors and subject taken as a random fac-
tor was undertaken. There was a significant effect
of the category that the beetle exemplar belonged to
on participants? responses for the three relations (the
interaction between beetle category and response re-
lation was significant; F (6, 39) = 26.83, p < .01.
Planned pairwise comparisons (paired t-tests) were
conducted to investigate how ratings for the cor-
rect relation (i.e. the relation consistent with train-
ing) differed for the ratings for the other two rela-
tions. For Bcat1, Bcat2 and Bcat3, the ratings for
the relation consistent with learning was higher than
the two alternative relations (p < .01 in all cases).
However, for the Bcat4 items, there was no evi-
dence that participants we more likely to rate Re-
lation 1 (M = 2.09) higher than either Relation 2
(M = 1.97; t(39) = 0.54, p = .59) or Relation
3 (M = 1.91; t(39) = 0.69, p > .50). Though
the difference is in the direction predicted by Bcat4?s
similarity to Bcat1, there is no evidence that partici-
pants made use of Bcat4?s similarity to Bcat1 when
rating relation likelihood for Bcat4.
In summary, the results suggest that participants
were capable of learning the training items. Partici-
pants appeared to be influenced by the beetle exem-
plar but not the plant exemplar. There was some evi-
dence that conceptual similarity played a role in par-
ticipants? judgments of relation likelihood for Bcat4
exemplars (e.g. the responses for item 19) but over
all Bcat4 exemplars this effect was not significant.
2.2.3 Performance on the noun-noun
compound transfer stage
In the noun-noun compound transfer stage, each
participant rated relation likelihood for each of the
16 possible noun-noun compounds that could be
formed from combinations of the beetle and plant
category names. Category name order was a be-
tween subject factor: half of the participants saw the
compounds with beetle in the modifier position and
plant in the head position whilst the other half of
participants saw the reverse. First of all, we were
interested in whether or not the training on exem-
plar items would transfer to noun-noun compounds.
Another question of interest is whether or not par-
ticipants? responses would be affected by the order
in which the categories were presented. For exam-
ple, perhaps it is the concept in the modifier position
that is most influential in determining the likelihood
of different relations for a compound. Alternatively
perhaps it is the concept in the head position that is
most influential.
To answer such questions a 4?4?3?2 repeated
measures ANOVA with beetle category, plant cate-
gory and response relation as within subject factors
and category label ordering as a between subject fac-
tor was used to analyze the data. The interaction
between beetle category and response relation was
significant (F (6, 38) = 59.79, p < .001). There-
fore, the beetle category present in the compound
tended to influence participants? relation selections.
The interaction between plant category and response
relation was weaker, but still significant (F (6, 38) =
5.35, p < 0.01). Therefore, the plant category
present in the compound tended to influence partic-
ipants? relation selections. These results answer the
first question above; training on exemplar items was
transferred to the noun-noun compounds. However,
there were no other significant interactions found. In
particular, the interaction between category order-
ing, beetle category and response relation was not
significant (F (6, 38) = 1.82, p = .09). In other
words, there is no evidence that the influence of bee-
tle category on participants? relation selections when
the beetle was in the modifier position differed from
the influence of beetle category on participants? rela-
tion selections when the beetle was in the head-noun
position. Similarly, the interaction between noun or-
dering, plant category and response relation was not
significant (F (6, 38) = 0.68, p = .67); there is no
evidence that the influence of the plant category on
relation selection differed depending on the location
of the plant category in the compound.
Planned pairwise comparisons (paired t-tests)
were used to investigate the significant interactions
further: for Bcat1, Bcat2 and Bcat3, the ratings
for the relation consistent with learning was sig-
nificantly higher than the two alternative relations
(p < .001 in all cases). However, for Bcat4, there
were no significant differences between the ratings
for the three relations (p > .31 for each of the three
comparisons). For the plants, however, the only sig-
nificant differences were between the response for
Relation 1 and Relation 2 for Pcat2 (t(39) = 2.12,
94
p = .041) and between Relation 2 and Relation 3 for
Pcat2 (t(39) = 3.08, p = .004), although the dif-
ferences for Pcat1 and Pcat3 are also in the expected
direction.
In summary, the results of the noun-noun com-
pound stage of the experiment show that partici-
pants? learning of the relations and their associa-
tions with beetle and plant categories during training
transferred to a task involving noun-noun compound
interpretation. This is important as it demonstrates
how the interpretation of compounds can be derived
from information about how concept exemplars tend
to co-occur together.
2.3 Modelling relation selection
One possible hypothesis about how people decide
on likely relations for a compound is that the men-
tion of the two lexemes in the compound activates
stored memory traces (i.e. exemplars) of the con-
cepts denoted by those lexemes. Exemplars differ
in how typical they are for particular conceptual cat-
egories and we would expect the likelihood of an
exemplar?s activation to be in proportion to its typ-
icality for the categories named in the compound.
As concept instances usually do not happen in isola-
tion but rather in the context of other concepts, this
naturally results in extensional relational informa-
tion about activated exemplars also becoming acti-
vated. This activated relational information is then
available to form a basis for determining the likely
relation or relations for the compound. A strength
of this hypothesis is that it incorporates both inten-
sional information about concepts? features (in the
form of concept typicality) and also extrinsic, dis-
tributional information about how concepts tend to
combine (in the form of relational information asso-
ciated with activated exemplars). In this section, we
present a model instantiating this hybrid approach.
The hypothesis proposed above assumes that ex-
tensional information about relations is associated
with exemplars in memory. In the context of our
experiment, the extensional, relational information
about beetle and plant exemplars participants held in
memory is revealed in how they rated relational like-
lihood during the exemplar transfer stage of the ex-
1This is not significant if Bonferroni correction is used to
control the familywise Type I error rate amongst the multiple
comparisons
periment. For each of the 13 beetle and plant exem-
plars, we therefore assume that the average ratings
for each of the relations describes our participants?
knowledge about how exemplars combine with other
exemplars. Also, we can regard the three relation
likelihood ratings as being a 3-dimensional vector.
Given that category ordering did not appear to have
an effect on participants? responses in the compound
transfer phase of the experiment, we can calculate
the relation vector ~rB,P for the novel compounds ?B
P ? or ?P B? as
~rB,P =
?
e?U
(typ(eb, B) + typ(ep, P ))
? ? ~re
?
e?U
(typ(eb, B) + typ(ep, P ))
?
where e denotes one of the 13 beetle-plant ex-
emplar items rated in the exemplar transfer stage,
typ(eb, B) denotes the typicality of the beetle ex-
emplar present in item e in beetle category B and
typ(ep, P ) denotes the typicality of the plant exem-
plar present in item e in plant category P . U is
the set of 13 beetle-plant exemplar pairs and ? is a
magnification parameter to be estimated empirically
which describes the relative importance of exemplar
typicality.
In this model, we require a measure of how typical
of a conceptual category an exemplar is (i.e. a mea-
sure of how good a member of a category a partic-
ular category instance is). In our model, we use the
Generalized Context Model (GCM) to derive mea-
sures of exemplar typicality. The GCM is a success-
ful model of category learning that implements an an
exemplar-based account of how people make judg-
ments of category membership in a category learn-
ing task. The GCM computes the probability Pr of
an exemplar e belonging in a category C as a func-
tion of pairwise exemplar similarity according to:
Pr(e, C) =
?
i?C
sim(e, i)
?
i?U
sim(e, i)
where U denotes the set of all exemplars in mem-
ory and sim(e, i) is a measure of similarity between
exemplars e and i. Similarity between exemplars is
in turn defined as a negative-exponential transforma-
95
tion of distance:
sim(i, j) = e?cdist(i,j) (1)
where c is a free parameter, corresponding to how
quickly similarity between the exemplars diminishes
as a function of their distance. The distance between
two exemplars is usually computed as the city-block
metric summed over the dimensions of the exem-
plars, with each term weighted by empirically esti-
mated weighting parameters constrained to sum to
one. According to the GCM, the probability that
a given exemplar belongs to a given category in-
creases as the average similarity between the exem-
plar and the exemplars of the category increases; in
other words, as it becomes a more typical member
of the category. In our model, we use the proba-
bility scores produced by the GCM as a means for
computing concept typicality (although other meth-
ods for measuring typicality could have been used).
We compared the relation vector outputted by the
model for the 16 possible compounds to the rela-
tion vectors derived from participants? ratings in the
compound transfer phase of the experiment. The
agreement between the model and the data was high
across the three relations (for Relation 1, r = 0.84,
p < 0.01; for Relation 2, r = 0.90, p < 0.01; for
Relation 3, r = 0.87, p < 0.01), using only one free
parameter, ?, to fit the data2.
3 Conclusions
The empirical findings we have described in this pa-
per have several important implications. Firstly, the
findings have implications for relation-based theo-
ries. In particular, the finding that only beetle exem-
plars tended to influence relation selection suggest
that factors other than relation frequency are rele-
vant to the interpretation process (since the beetle
and plants in our experiment were identical in their
degree of association with relations). Complex inter-
actions between concepts and relations (e.g. agency
in the EATS(AGENT,OBJECT) relation) is informa-
tion that is not possible to capture using a taxonomic
approach to relation meaning.
Secondly, the fact that participants could learn to
identify the relations between exemplars and also
2In the GCM, c was set equal to 1 and the three dimensional
weights in the distance calculation were set equal to 1/3
transfer that knowledge to a task involving com-
pounds has implications for concept-based theories
of compound comprehension. No concept-based
theory of conceptual combination has ever adopted
an exemplar approach to concept meaning; mod-
els based on concept-focused theories tend to rep-
resent concepts as frames or lists of predicates. Our
approach suggests an exemplar representation is a
viable alternative. Also, distributional knowledge
about relations forms a natural component of an ex-
emplar representation of concepts, as different con-
cept instances will occur with instances of other con-
cepts with varying degrees of frequency. Given the
success of our model, assuming an exemplar repre-
sentation of concept semantics would seen to offer a
natural way of incorporating both information about
concept features and information about relation dis-
tribution into a single theory.
References
G. Cannon. 1987. Historical change and English word
formation. New York: Lang.
E. V. Clark and B.J. Barron. 1988. A thrower-button or
a button-thrower? Children?s judgments of grammati-
cal and ungrammatical compound nouns. Linguistics,
26:3?19.
F. J. Costello & M.T. Keane. 2000. Efficient creativity:
Constraint guided conceptual combination.. Cognitive
Science, 24(2):299?349.
C. L. Gagne? and E.J. Shoben. 1997. Influence of the-
matic relations on the comprehension of modifier noun
combinations. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 23:71?78.
S. N. Kim and T. Baldwin. 2005. Automatic Interpreta-
tion of Noun Compounds Using WordNet Similarity.
Lecture Notes in Computer Science, 3651:945?956.
J. N. Levi. 1978. The Syntax and Semantics of Complex
Nominals. New York: Academic Press.
D. L. Medin & M.M. Schaffer. 1978. Context the-
ory of classification learning. Psychological Review,
85:207?238.
R. N. Nosofsky. 1984. Choice, similarity, and the con-
text theory of classification.. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition,
10(1):104?114.
E. J. Wisniewski 1997. When concepts combine. Psy-
chonomic Bulletin & Review, 4(2):167?183.
96
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 61?69,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Acquiring Human-like Feature-Based
Conceptual Representations from Corpora
Colin Kelly
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
colin.kelly
@cl.cam.ac.uk
Barry Devereux
Centre for Speech,
Language, and the Brain
University of Cambridge
Cambridge, CB2 3EB, UK
barry@csl.psychol.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
anna.korhonen
@cl.cam.ac.uk
Abstract
The automatic acquisition of feature-based
conceptual representations from text corpora
can be challenging, given the unconstrained
nature of human-generated features. We
examine large-scale extraction of concept-
relation-feature triples and the utility of syn-
tactic, semantic, and encyclopedic informa-
tion in guiding this complex task. Meth-
ods traditionally employed do not investi-
gate the full range of triples occurring in
human-generated norms (e.g. flute produce
sound), rather targeting concept-feature pairs
(e.g. flute ? sound) or triples involving specific
relations (e.g. is-a, part-of ). We introduce
a novel method that extracts candidate triples
(e.g. deer have antlers, flute produce sound)
from parsed data and re-ranks them using se-
mantic information. We apply this technique
to Wikipedia and the British National Corpus
and assess its accuracy in a variety of ways.
Our work demonstrates the utility of external
knowledge in guiding feature extraction, and
suggests a number of avenues for future work.
1 Introduction
In the cognitive sciences, theories about how con-
crete concepts such as ELEPHANT are represented in
the mind have often adopted a distributed, feature-
based model of conceptual knowledge (e.g. Ran-
dall et al (2004), Tyler et al (2000)). According
to such accounts, conceptual representations consist
of patterns of activation over sets of interconnected
semantic feature nodes (e.g. has eyes, has ears,
is large). To test these theories empirically, cogni-
tive psychologists require an accurate estimate of the
kinds of knowledge that people are likely to repre-
sent in such a system. To date, the most important
sources of such knowledge are property-norming
studies, where a large number of participants write
down lists of features for concepts. For example,
McRae et al (2005) collected a set of norms list-
ing features for 541 concrete concepts. In that study,
the features listed by different participants were nor-
malised by mapping different feature descriptions
with identical meanings to the same feature label.1
Table 1 gives the ten most frequent normed features
for two concepts in the norms.
elephant banana
Relation Feature Relation Feature
is large is yellow
has a trunk is a fruit
is an animal is edible
is grey is soft
lives in Africa grows on trees
has ears eaten by peeling
has tusks - grows
has legs eaten by monkeys
has four legs is long
has large ears tastes good
Table 1: Sample triples from McRae Norms
However, property norm data have certain weak-
nesses (these have been widely discussed; e.g. Mur-
phy (2002), McRae et al (2005)). One issue is
that participants tend to under-report features that
are present in many of the concepts in a given cat-
egory (McRae et al, 2005; Murphy, 2002). For ex-
ample, for the concept ELEPHANT, participants list
salient features like has trunk, but not less salient
features such as breathes air, even though presum-
ably all McRae et al?s participants knew that ele-
phants breathe air. Although the largest collection
1For example, for CAR, ?used for transportation? and
?people use it for transportation? were mapped to the same
used for transportation feature.
61
of norms lists features for over 500 concepts, the
relatively small size of property norm sets still gives
cause for concern. Larger sets of norms would be
useful to psycholinguists; however, large-scale prop-
erty norming studies are time-consuming and costly.
In NLP, researchers have developed methods for
extracting and classifying generic relationships from
data, e.g. Pantel and Pennacchiotti (2008), Davidov
and Rappoport (2008a, 2008b). In recent years,
researchers have also begun to develop methods
which can automatically extract feature norm-like
representations from corpora, e.g. Almuhareb and
Poesio (2005), Barbu (2008), Baroni et al (2009).
The automatic approach is capable of gathering
large-scale distributional data, and furthermore it is
cost-effective. Corpora contain natural-language in-
stances of words denoting concepts and their fea-
tures, and therefore serve as ideal material for fea-
ture generation tasks. However, current methods
are restricted to specific relations between concepts
and their features, or target concept-feature pairs
only. For example, Almuhareb and Poesio (2005)
proposed a method based on manually developed
lexico-syntactic patterns that extracts information
about attributes and values of concepts. They used
these syntactic patterns and two grammatical rela-
tions to create descriptions of nouns consisting of
vector entries and evaluated their approach based
on how well their vector descriptions clustered con-
cepts. This method performed well, but targeted
is-a and part-of relations only. Barbu (2008) com-
bined manually defined linguistic patterns with a co-
occurrence based method to extract features involv-
ing six classes of relations. He then split learning
for the property classes into two distinct paradigms.
One used a pattern-based approach (four classes)
with a seeded pattern-learning algorithm. The other
measured strength of association between the con-
cept and referring adjectives and verbs (two classes).
His pattern-based approach worked well for proper-
ties in the superordinate class, had reasonable recall
for stuff and location classes, but zero recall for part
class. His approach for the other two classes used
various association measures which he summed to
establish an overall score for potential properties.
The recent Strudel model (Baroni et al, 2009) re-
lies on more general linguistic patterns, ?connector
patterns?, consisting of sequences of part-of-speech
(POS) tags to look for candidate feature terms near
a target concept. The method assumes that ?the va-
riety of patterns connecting a concept and a poten-
tial property is a good indicator of the presence of
a true semantic link?. Thus, properties are scored
based on the count of distinct patterns connecting
them to a concept. When evaluated against the ESS-
LLI dataset (Baroni et al (2008); see section 3.1),
Strudel yields a precision of 23.9% ? this figure is
the best state-of-the-art result for unconstrained ac-
quisition of concept-feature pairs.
It seems unlikely that further development of the
shallow connector patterns will significantly im-
prove accuracy, as these already broadly cover most
POS sequences that are concept-feature connectors.
Because of the difficult nature of the task, we believe
that extraction of more accurate representations ne-
cessitates additional linguistic and world knowl-
edge. Furthermore, the utility of Strudel is limited
because it only produces concept-feature pairs, and
not concept-relation-feature triples similar to those
in human generated norms (although the distribution
of the connector patterns for a extracted pair does of-
fer clues about the broad class of semantic relation
that holds between concept and feature).
In this paper, we explore issues of both method-
ology and evaluation that arise when attempting
unconstrained, large-scale extraction of concept-
relation-feature triples in corpus data. Extracting
such human-like features is difficult, and we do not
anticipate a high level of accuracy in these early ex-
periments. We examine the utility of three types
of external knowledge in guiding feature extrac-
tion: syntactic, semantic and encyclopedic. We
build three automatically parsed corpora, two from
Wikipedia and one from the British National Cor-
pus. We introduce a method that (i) extracts concept-
relation-feature triples from grammatical depen-
dency paths produced by a parser and (ii) uses prob-
abilistic information about semantic classes of fea-
tures and concepts to re-rank the candidate triples
before filtering them. We then assess the accuracy
of our model using several different methods, and
demonstrate that external knowledge can help guide
the extraction of human-like features. Finally, we
highlight issues in both methodology and evaluation
that are important for further progress in this area of
research.
62
2 Extraction Method
2.1 Corpora
We used Wikipedia to investigate the usefulness of
world knowledge for our task. Almost all con-
cepts in the McRae norms have their own Wikipedia
articles, and the articles often include facts simi-
lar to those elicited in norming studies.2 Extrane-
ous data were removed from the articles (e.g. in-
foboxes, bibliographies) to create a plaintext version
of each article. The 1.84 million articles were then
compiled into two subcorpora. The first of these
(Wiki500) consists of the Wikipedia articles corre-
sponding to each of the McRae concepts. It con-
tains c. 500 articles (1.1 million words). The sec-
ond subcorpus is comprised of those articles where
the title is fewer than five words long and contains
one of the McRae concept words.3 This corpus,
called Wiki110K, holds 109,648 plaintext articles
(36.5 million words).
We also employ the 100-million word British Na-
tional Corpus (BNC) (Leech et al, 1994) which con-
tains written (90%) and spoken (10%) English. It
was designed to represent a broad cross-section of
modern British English. This corpus provides an in-
teresting contrast with Wikipedia, since we assume
that any features contained in such a wide-ranging
corpus would be presented in an incidental fashion
rather than explicitly. The BNC may contain use-
ful features which are encoded in everyday speech
and text but not in Wikipedia, perhaps due to their
ambiguity for encyclopedic purposes, or due to their
non-scientific but rather common-sense nature. For
example, eaten by monkeys is listed as a feature of
BANANA in the McRae norms, but the word monkey
does not appear in the Wikipedia banana article.
2.2 Candidate feature extraction
Using a modified, British English version of the
published norms, we recoded them to a uniform
concept-relation-feature representation suitable for
our experiments ? it is triples of this form that we
aim to extract. Our method for extracting concept-
2e.g. The article Elephant describes how elephants are large,
are mammals, and live in Africa.
3This was done in order to avoid articles on very specific
topics which are unlikely to contain basic information about the
target concept.
relation-feature triples consists of two main stages.
In the first stage, we extract large sets of candidate
concept-relation-feature triples for each target con-
cept from parsed corpus data. In the second stage,
we re-rank and filter these triples with the intention
of retaining only those triples which are likely to be
true semantic features.
In the first stage, the corpora are parsed using the
Robust Accurate Statistical Parsing (RASP) system
(Briscoe et al, 2006). For each sentence in the cor-
pora, this yields the most probable analysis returned
by the parser in the form of a set of grammatical
relations (GRs). The GR sets for each sentence con-
taining the target concept noun are then retrieved
from the corpus. These GRs form an undirected
acyclic graph, whose nodes are labelled with words
in the sentence and their POS, and whose edges are
labelled with the GR types linking the nodes to-
gether. Using this graph we generate all possible
paths which are rooted at our target concept node
using a breadth-first search.
We then examine whether any of these paths
match prototypical feature-relation GR structures
according to our manually-generated rules. The
rules were created by first extracting features from
the McRae norms for a small subset of the concepts
and extracting those sentences from the Wiki500
corpus which contained both concept and feature
terms. For each sentence, we then examined each
path through the graph (containing the GRs and POS
tags) linking the concept, the feature, and all inter-
mediate terms, and (providing no other rule already
generated the concept-relation-feature triple) manu-
ally generated a rule based on each path.
For example, the sentence There are also aprons
that will cover the sleeves should yield the triple
apron cover sleeve. We examine the tree structure
of the sentence rooted at the concept (apron):
apron+s:17_NN2
cmod-that cover:34_VV0
L--- dobj sleeve+s:44_NN2
L--- det the:40_AT
L--- aux will:29_VM
cmod-that cover:34_VV0
xcomp be+:8_VBR
L--- ncmod also:12_RR
L--- ncsubj There:2_EX
Here, the relation is relatively simple ? we merely
63
create a rule which requires that the relation is a verb
(i.e. has a V POS tag), the feature has an NN tag and
that there is a dobj GR linking the feature to the
concept. Our rules are effectively a constraint on (a)
which paths should be followed through the tree, and
(b) which items in that path should be noted in our
concept-relation-feature triple. By creating several
such rules and applying them to a large number of
sentences, we extract potential features and relations
for our concepts.
We avoided specifying too many POS tags and
GRs in rules since this could have resulted in too
few matching paths. In the above example, we could
have required also a cmod-that relation linking the
feature and concept ? but this would have excluded
sentences like the apron covered the sleeves. Con-
versely, we avoided making our rules too permis-
sive. For example, eliminating the dobj requirement
would have yielded the triple apron be steel from the
sentence the apron hooks were steel.
The application of this method to a number of
concepts in the Wiki500 corpus yielded 15 rules
which we employed in our experiments. We extract
triples using both singular and plural occurrences of
both the concept term and the feature term. We show
the first three of our rules in Table 2. The first stage
of our method uses the 15 rules to extract a very
large number of candidate triples from corpus data.
Rule: relation of concept has a VVN tag, feature
has a NN tag and they are linked by an xcomp
GR
S: This is an anchor which relies solely on be-
ing a heavy weight.
T: anchor be weight
Rule: relation of concept is a verb, feature is an ad-
jective and they are linked by an xcomp GR
S: Sliced apples turn brown with exposure to
air due to the conversion of natural pheno-
lic substances into melanin upon exposure to
oxygen.
T: apple turn brown
Rule: feature of concept has a VV0 tag, relation is
a verb and they are linked by an aux GR
S: Grassy bottoms may be good holding, but
only if the anchor can penetrate the foliage.
T: anchor can penetrate
Table 2: Three sample rules for a given concept, with
example sentence (S) and corresponding triple (T).
2.3 Re-ranking based on semantic information
The second stage of our method evaluates the quality
of the extracted candidates using semantic informa-
tion, with the aim of filtering out the poor quality
features generated in the first stage. We would ex-
pect the number of times a triple is extracted for a
given concept to be proportional to the likelihood
that the triple represents a true feature of that con-
cept. However, production frequency alone is not a
sufficient indicator of quality, because concept terms
can produce unexpected candidate feature terms.4
One may attempt to address this issue by intro-
ducing semantic categories. In other words, the
probability of a feature being part of a concept?s
representation is dependent on the semantic cate-
gory to which the concept belongs (for example,
used for-cutting would be expected to have low
probability for animal concepts). We analysed the
norms to quantify this type of semantic information
with the aim of identifying higher-order structure in
the distribution of semantic classes for features and
concepts. The overarching goal was to determine
whether this information can indeed improve the ac-
curacy of feature extraction.
In formal terms, we assume that there is a 2-
dimensional probability distribution over concept
and feature classes, P(C,F), where C is a concept
class (e.g. Apparel) and F is a feature class (e.g.
Materials). Knowing this distribution provides us
with a means of assessing how likely it is that a can-
didate feature f is true for a concept c, assuming that
we know that c ? C and f ? F . The McRae norms
may be considered to be a sample drawn from this
distribution, if the concept and feature terms appear-
ing in the norms can be assigned to suitable concept
and feature classes. These classes were identified
by way of clustering. The reranking step employed
the McRae norms so we could establish an upper
bound for the semantic analysis, although we could
also use other knowledge resources, e.g. the Open
Mind Common Sense database (Singh et al, 2002).
2.3.1 Clustering
We utilised Lin?s similarity measure (1998) for
our similarity metric, employing WordNet (Fell-
4For example, one of the extracted triples for TIGER is tiger
have squadron because of the RAF squadron called the Tigers.
64
k-means
banjo biscuit blackbird
bat cup ox
beehive kettle peacock
birch sailboat prawn
bookcase shoe prune
NMF
ashtray bouquet eel
bayonet cabinet grapefruit
cape card guppy
cat cellar moose
catfish chandelier otter
Hierarchical
Fruit/Veg Apparel Instruments
apple apron accordion
avocado armour bagpipes
banana belt banjo
beehive blouse cello
blueberry boot clarinet
Table 3: First five elements alphabetically from three
sample clusters for the three clustering methods.
baum, 1998) as the basis for calculating similarity.
This metric is suitable for our task as we would
like to generate appropriate superordinate classes for
which we can calculate distributional statistics. We
could merely cluster on the most frequent sense of
concept and feature words in WordNet, but the most
frequent sense in WordNet may not correspond to
the intended sense in our feature norm data.5 So we
consider also other senses of words in WordNet by
employing a manually-annotated list to choose the
correct sense in WordNet. This is only possible for
concept clustering since we don?t possess a manual
WordNet sense annotation for the 7000 McRae fea-
tures; for the feature clustering, we simply use the
most frequent sense in WordNet.
The concepts and feature-head terms appearing
in the recoded norms were each clustered indepen-
dently into 50 clusters using three methods: hi-
erarchical clustering, k-means clustering and non-
negative matrix factorization (NMF). We show the
first five alphabetical elements from three of the
clusters produced by our clustering methods in Table
3. The hierarchical clustering seems to be producing
5e.g. the first and second most frequent definitions of kite
refer to a slang meaning for the word cheque ? only the third
most frequent meaning refers to kite as a toy, which most people
would understand to be its predominant sense.
Hierarchical Clustering
Plant Parts Materials Activities
berry cotton annoying
bush fibre listening
core nylon music
plant silk showing
seed spandex looking
Table 4: Example members of feature clusters for hierar-
chical clustering.
Fruit/Veg Apparel Instruments
Plant Parts 0.144 0.037 0.008
Materials 0.006 0.148 0.008
Activities 0.009 0.074 0.161
Table 5: P(F |C) for C ? {Fruit/Veg, Apparel, Instru-
ments} and F ? {Plant Parts, Materials, Activities}
the most intuitive clusters.
We calculated the conditional probability P(F |C)
of a feature cluster given a concept cluster using the
data in the McRae norms. Table 5 gives the condi-
tional probability for each of the three feature clus-
ters given each of the three concept clusters that
were presented in Tables 3 and 4 for hierarchical
clustering. For example, P(Materials|Apparel) is
higher than P(Materials|Fruit/Veg): given a concept
in the Apparel cluster the probability of a Materials
feature is relatively high whereas given a concept in
the Fruit/Veg cluster the probability of a Materials
feature is low. The cluster analysis therefore sup-
ports our hypothesis that the likelihood of a partic-
ular feature for a particular concept is dependent on
the semantic categories that both belong to.
2.3.2 Reranking
We investigated whether this distributional semantic
information could be used to improve the quality of
the candidate triples, by using the conditional prob-
abilities of the appropriate feature cluster given the
concept cluster as a weighting factor. To obtain the
probabilities for a triple, we first find the clusters that
the concept and feature-head words belong to. If the
feature-head word of the extracted triple appears in
the norms, its cluster membership is drawn directly
from there; if not, we assign the feature-head to the
feature cluster with which it has the highest average
similarity.6 Having determined the concept and fea-
6We use average-linkage for hiearchical and k-means clus-
tering, and mean cosine similarity for NMF.
65
ture clusters for the triple, we reweight its raw cor-
pus occurrence frequency by multiplying it by the
conditional probability. In this way, incorrect triples
that occur frequently in the data are downgraded and
more plausible triples have their ranking boosted.
2.3.3 Baseline model
We also implemented as a baseline a co-occurrence-
based model, based on the ?SVD? model de-
scribed by Baroni and colleagues (Baroni and Lenci,
2008; Baroni et al, 2009) ? it is a simple, word-
association method, not tailored to extracting fea-
tures. A context-word-by-target-word frequency co-
occurrence matrix was constructed for both corpora,
with a sentence-sized window. Context words and
target words were defined to be the 5,000 and 10,000
most frequent content words in the corpus respec-
tively. The target words were supplemented with
the concept words from the recoded norms. The
co-occurrence matrix was reduced to 150 dimen-
sions by singular value decomposition, and cosine
similarity between pairs of target words was calcu-
lated. The 200 most similar target words to each
concept acted as the feature-head terms extracted by
this model.
3 Experimental Evaluation
3.1 Methods of Evaluation
We considered a number of methods for evaluating
the quality of the extracted feature triples. One pos-
sibility would be to calculate precision and recall
for the extracted triples with respect to the McRae
norms ?gold standard?. However, direct comparison
with the recoded norms is problematic, since there
may be extracted features which are semantically
equivalent to a triple in the norms but possessing a
different lexical form.7
Since semantically identical features can be lex-
ically different, we followed the approach taken in
the ESSLLI 2008 Workshop on semantic models
(Baroni et al, 2008). The gold standard for the ESS-
LLI task was the top 10 features for 44 of the McRae
concepts. For each concept-feature pair an expan-
sion set was generated containing synonyms of the
7For example, avocado have stone appears in the recoded
norms whilst avocado contain pit is extracted by our method;
direct comparison of these two triples results in avocado con-
tain pit being incorrectly marked as an error.
feature terms appearing in the norms. For example,
the feature lives on water was expanded to the set
{aquatic, lake, ocean, river, sea, water}.
We would expect to find in corpus data correct
features that do not appear in our ?gold standard?
(e.g. breathes air is listed for WHALE but for no
other animal). We therefore aim to attain high re-
call when evaluating against the ESSLLI set (since
ideally all features in the norms should be extracted)
but we are somewhat less concerned about achieving
high precision (since extracted features that are not
in the norms may still be correct, e.g. breathes air
for TIGER). To evaluate the ability of our model
to generate such novel features, we also conducted
a manual evaluation of the highest-ranked extracted
features that did not appear in the norms.
Extraction set Corpus Prec. Recall
SVD Baseline
Wiki500 0.0235 0.4712
Wiki110K 0.0140 0.2798
BNC 0.0131 0.2621
Method -
unfiltered
Wiki500 0.0242 0.6515
Wiki110K 0.0039 0.8944
BNC 0.0042 0.8813
Method - top 20
(unweighted)
Wiki500 0.1159 0.2326
Wiki110K 0.0761 0.1523
BNC 0.0841 0.1692
Method - top 20
(hierarchical
clustering)
Wiki500 0.1693 0.3394
Wiki110K 0.1733 0.3553
BNC 0.1943 0.3896
Method - top 20
(k-means
clustering)
Wiki500 0.1159 0.2323
Wiki110K 0.1000 0.2008
BNC 0.1216 0.2442
Method - top 20
(NMF
clustering)
Wiki500 0.1375 0.2755
Wiki110K 0.1409 0.2826
BNC 0.1500 0.3010
Table 6: Results when matching on features only.
3.2 Evaluation
Previous large-scale models of feature extraction
have been evaluated on pairs rather than triples e.g.
Baroni et al (2009). Table 6 presents the results
of our method when we evaluate using the feature-
head term alone (i.e. in calculating precision and re-
call we disregard the relation verb and require only
a match between the feature-head terms in the ex-
tracted triples and the recoded norms). Results for
six sets of extractions are presented. The first set
is the set of features extracted by the SVD baseline.
66
The second set of extracted triples consists of the
full set of triples extracted by our method, prior to
the reweighting stage. ?Top 20 unweighted? gives
the results when all but the top 20 most frequently
extracted triples for each concept are filtered out.
Note that the filtering criteria here is raw extraction
frequency, without reweighting by conditional prob-
abilities. ?Top 20 (clustering type)? are the corre-
sponding results when the features are weighted by
the conditional probability factors (derived from our
three clustering methods) prior to filtering; that is,
using the top 20 reranked features. The effective-
ness of using the semantic class-based analysis data
in our method can be assessed by comparing the fil-
tered results with and without feature weighting.
For the baseline implementation, the results are
better when we use the smaller Wiki500 corpus
compared to the larger Wiki110K corpus. This is
not surprising, since the smaller corpus contains
only those articles which correspond to the concepts
found in the norms. This smaller corpus thus min-
imises noise due to phenomena such as word poly-
semy which are more apparent in the larger corpus.
The results for the baseline model and the unfil-
tered method are quite similar for the Wiki500 cor-
pus, whilst the results for the unfiltered method us-
ing the Wiki110K corpus give the maximum recall
achieved by our method; 89.4% of the features are
extracted, although this figure is closely followed by
that of the BNC at 88.1%. As the unfiltered method
is deliberately greedy, a large number of features are
being extracted and therefore precision is low.
Extraction set Corpus Prec. Recall
Method - top 20
(hierarchical
clustering)
Wiki500 0.1011 0.2028
Wiki110K 0.1102 0.2210
BNC 0.0955 0.1917
Table 7: Results for our best method when matching on
features and relations.
For the results of the filtered method, where all
but the top 20 of features were discarded, we see the
benefit of reranking, with the reranked frequencies
for all three clustering types yielding much higher
precision and recall scores than the unweighted
method. Our best performance is achieved using the
BNC and hierarchical clustering, where we obtain
19.4% precision and 38.9% recall. Thus both gen-
eral and encyclopedic corpus data prove useful for
the task. An interesting question is whether these
two data types offer different, complementary fea-
ture types for the task. We discuss this point further
in section 3.3.
Using exactly the same gold standard, Baroni et
al. (2009) obtained precision of 23.9%. However,
this result is not directly comparable with ours, since
we define precision over the whole set of extracted
features while Baroni et al considered the top 10
extracted features only.
The innovation of our method is that it uses infor-
mation about the GR-graph of the sentence to also
extract the relation which appears in the path link-
ing the concept and feature terms in the sentence,
which is not possible in a purely co-occurrence-
based model. We therefore also evaluated the ex-
tracted triples using the full relation + feature-head
pair (i.e. both the feature and the relation verb have
to be correct). The results for our best method are
shown in Table 7. Unsurprisingly, because this task
is more difficult, precision and recall are reduced.
However, since we enforce no constraints on what
the relation may be and since we do not have ex-
panded synonym sets for our relations (as we do for
our features) it is actually impressive to have both
the exact relation verb and feature matching with the
recoded norms almost one in every five times. To our
knowledge, our work is the first to try to compare ex-
tracted features to the full relation and feature norm
parts of the triple.
3.3 Qualitative analysis
Since a key aim of our work is to learn novel features
in corpus data, we also performed a qualitative eval-
uation of the extracted features and relations. This
analysis revealed that many of the errors were not
true errors but potentially valid triples missing from
the gold standard. Table 8 shows the top 10 features
for two concepts extracted by our best method from
the Wiki500 corpus and the BNC corpus. We la-
bel those features that are correct according to the
norms as Correct (C), those which do not appear in
our norms but we believe to be plausible as Plausi-
ble (P), and those that do not appear in the norms
and are also implausible as Incorrect (I). We can see
that our method has detected several plausible fea-
tures not appearing in the norms (and thus our gold
standard), e.g. swan have chick and screwdriver be
67
swan
Wiki500 BNC
be bird C have number I
be black P have water C
have chick P have lake C
have plumage C be bird C
have feather C be white C
restrict water C have neck C
be mute P be wild P
eat grass P have duck I
turn elisa I have song I
have neck C have pair I
screwdriver
Wiki500 BNC
use handle C have tool C
have blade P have end P
use tool C have blade P
remedy problem P have hand I
have size P be sharp P
have head C have bit P
rotate end P have arm I
have plastic P be large P
achieve goal I be sonic P
have hand I have range P
Table 8: Top 10 returned features and relations for swan
and screwdriver.
sharp. Indeed, it could be argued that some ?incor-
rect? features (e.g. screwdriver achieve goal) could
be considered to be at least broadly accurate. We
recognise that the ideal evaluation for our method
would involve having human participants assess the
extracted features for a diverse cross-section of our
concepts, but this is beyond the scope of this paper.
When considering the top 20 features extracted
using our best method applied to the Wiki500 cor-
pus versus the BNC corpus, the overlap of features
is relatively low at 22.73%. When one also takes the
extracted relations into account, this figure descends
to 6.45%. It is clear that relatively distinct groups of
features are being extracted from the encyclopedic
and general corpus data. Future work could investi-
gate combining these for improved performance e.g.
using the intersection of the best features from the
BNC and Wiki110k corpora to improve precision
and the union to improve recall.
4 Discussion
This paper examined large-scale, unconstrained ac-
quisition of human-like feature norms from corpus
data. Our work was not limited to only a subset
of concepts, relation types or concept-feature pairs.
Rather, we investigated concepts, features and rela-
tions in conjunction, and extracted property norm-
like concept-relation-feature triples.
Our investigation shows that external knowledge
is highly useful in guiding this challenging task. En-
cyclopedic information proved useful for feature ex-
traction: although our Wikipedia corpora are consid-
erably smaller than the BNC, they performed almost
equally well. We also demonstrated the benefits of
employing syntactic information in feature extrac-
tion: our base extraction method operating on parsed
data outperforms the co-occurrence-based baseline
and permits us to extract relation verbs. This un-
derscores the usefulness of parsing for semantically
meaningful feature extraction. This is consistent
with recent work in the field of computational lex-
ical semantics, although GR data has not previously
been successfully applied to feature extraction.
We showed that semantic information about co-
occurring concept and feature clusters can be used
to enhance feature acquisition. We employed the
McRae norms for our analysis, however we could
also employ other knowledge resources and cluster
relation verbs using recent methods, e.g. Sun and
Korhonen (2009), Vlachos et al (2009).
Our paper has also investigated methods of eval-
uation, which is a critical but difficult issue for fea-
ture extraction. Most recent approaches have been
evaluated against the ESSLLI sub-set of the McRae
norms which expands the set of features in the norms
with their synonyms. Yet even expansion sets like
the ESSLLI norms do not facilitate adequate eval-
uation because they are not complete in the sense
that there are true features which are not included
in the norms. Our qualitative analysis shows that
many of the errors against the recoded norms are
in fact correct or plausible features. Future work
can aim for larger-scale qualitative evaluation using
multiple judges as well as investigating other task-
based evaluations. For example, we have demon-
strated that our automatically-acquired feature rep-
resentations can make predictions about fMRI activ-
ity associated with concept stimuli that are as pow-
erful as those produced by a manually-selected set
of features (Devereux et al, 2010).
68
Acknowledgments
This research was supported by EPSRC grant
EP/F030061/1 and the Royal Society University
Research Fellowship, UK. We are grateful to McRae and
colleagues for making their norms publicly available,
and to the anonymous reviewers for their input.
References
Abdulrahman Almuhareb and Massimo Poesio. 2005.
Concept learning and categorization from the web. In
Proceedings of the 27th Annual Meeting of the Cogni-
tive Science Society, pages 103?108.
Eduard Barbu. 2008. Combining methods to learn
feature-norm-like concept descriptions. In Proceed-
ings of the ESSLLI Workshop on Distributional Lexical
Semantics, pages 9?16.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. Italian Journal of Lin-
guistics, 20(1):55?88.
Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
tors. 2008. ESSLLI 2008 Workshop on Distributional
Lexical Semantics.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2009. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, pages 1?33.
Edward J. Briscoe, John Carroll, and Rebecca Wat-
son. 2006. The second release of the RASP sys-
tem. In Proceedings of the Interactive Demo Session
of COLING/ACL-06, pages 77?80.
D. Davidov and A. Rappoport. 2008a. Classification of
semantic relationships between nominals using pattern
clusters. ACL.08.
D. Davidov and A. Rappoport. 2008b. Unsupervised
discovery of generic relationships using pattern clus-
ters and its evaluation by automatically generated SAT
analogy questions. ACL.08.
Barry Devereux, Colin Kelly, and Anna Korhonen. 2010.
Using fmri activation to conceptual stimuli to evalu-
ate methods for extracting conceptual representations
from corpora. In Proceedings of the NAACL-HLT
Workshop on Computational Neurolinguistics.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
G. Leech, R. Garside, and M. Bryant. 1994. CLAWS4:
the tagging of the British National Corpus. In Pro-
ceedings of the 15th conference on Computational
linguistics-Volume 1, pages 622?628.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML?98, pages 296?
304.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37:547?559.
Gregory Murphy. 2002. The big book of concepts. The
MIT Press, Cambridge, MA.
Patrick Pantel and Marco Pennacchiotti. 2008. Automat-
ically harvesting and ontologizing semantic relations.
In Paul Buitelaar and Philipp Cimiano, editors, Ontol-
ogy learning and population. IOS press.
Billi Randall, Helen E. Moss, Jennifer M. Rodd, Mike
Greer, and Lorraine K. Tyler. 2004. Distinctive-
ness and correlation in conceptual structure: Behav-
ioral and computational studies. Journal of Experi-
mental Psychology: Learning, Memory & Cognition,
30(2):393?406.
P. Singh, T. Lin, E. Mueller, G. Lim, T. Perkins,
and W. Li Zhu. 2002. Open Mind Common
Sense: Knowledge acquisition from the general pub-
lic. On the Move to Meaningful Internet Systems 2002:
CoopIS, DOA, and ODBASE, pages 1223?1237.
Lin Sun and Anna Korhonen. 2009. Improving Verb
Clustering with Automatically Acquired Selectional
Preferences. Empirical Methods on Natural Language
Processing.
L. K. Tyler, H. E. Moss, M. R. Durrant-Peatfield, and J. P.
Levy. 2000. Conceptual structure and the structure of
concepts: A distributed account of category-specific
deficits. Brain and Language, 75(2):195?231.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and constrained dirichlet
process mixture models for verb clustering. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics, pages 74?82, Athens,
Greece.
69
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 70?78,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using fMRI activation to conceptual stimuli to evaluate methods for
extracting conceptual representations from corpora
Barry Devereux
Centre for Speech, Language and the Brain
Department of Experimental Psychology
University of Cambridge
barry@csl.psychol.cam.ac.uk
Colin Kelly & Anna Korhonen
Computer Laboratory
University of Cambridge
{ck329,alk23}@cam.ac.uk
Abstract
We present a series of methods for deriv-
ing conceptual representations from corpora
and investigate the usefulness of the fMRI
data and machine learning methodology of
Mitchell et al (2008) as a basis for evaluat-
ing the different models. Within this frame-
work, the quality of a semantic model is quan-
tified by its ability to predict the fMRI ac-
tivation associated with conceptual stimuli.
Mitchell et al used a manually-acquired set of
verbs as the basis for their semantic model; in
this paper, we also consider automatically ac-
quired feature-norm-like semantic representa-
tions. These models make different assump-
tions about the kinds of information avail-
able in corpora that is relevant to represent-
ing conceptual knowledge. Our results in-
dicate that automatically-acquired representa-
tions can make equally powerful predictions
about the brain activity associated with the
stimuli.
1 Introduction
Mitchell et al (2008) presented a novel approach for
predicting human brain activity associated with con-
ceptual stimuli. This approach represents a useful
development for interdisciplinary researchers inter-
ested in lexical semantics, for several reasons. Most
broadly, it is useful in testing the hypothesis that
distributional properties of words in corpora can re-
veal important information about the meanings of
words. A strong version of this hypothesis (i.e. that
children in part learn the meaning of concrete con-
cept words from co-occurring words in discourse
that they are exposed to) has formed the basis of
one class of probabilistic cognitive models of con-
ceptual representation (Andrews et al, 2005; An-
drews et al, 2009; Steyvers, 2010). Furthermore
this approach is useful for testing hypotheses about
the kind of co-occurring information that is useful
for representing conceptual semantics. In Mitchell
et al?s work (2008), for example, they adopt the po-
sition that the meaning of concrete concepts is en-
coded in the brain with information associated with
basic sensory and motor activities (such as actions
involving changes to spatial relationships and phys-
ical actions performed on objects).
At a more technical level, Mitchell et al?s fMRI
activation data1 give researchers developing feature-
based models of conceptual representation an im-
portant benchmark for evaluation. For these re-
searchers, a key problem is the lack of a reason-
able ?gold standard? against which the quality of the
representations generated by a computational model
may be evaluated. Previous research has adopted
two main approaches to evaluation. Firstly, some
models ? especially those aiming to extract repre-
sentations composed of psychologically meaningful
semantic feature units, such as Baroni et al (2009)
? have been evaluated against features gathered in
large scale property norming studies (e.g. McRae
et al (2005)).2 By comparing the system output
against features elicited by people, this kind of eval-
1fMRI data measures changes in oxygen concentrations in
the brain. These changes are tied to cognitive processes.
2In property norming studies, a group of human subjects are
asked to cite features which come to mind for a given concept.
These features are compiled by frequency (with a minimum fre-
quency cut-off) to generate a list of features for each concept.
70
uation aims to test the psychological validity of com-
putational methods. Furthermore, it allows a fine-
grained analysis of performance, for example by re-
vealing the classes of features (part-of, taxonomic,
etc) which a given model is particularly good at ex-
tracting (Baroni et al, 2008).
However, property norms come with important
caveats. One problem is that they tend to over-
represent informative or salient information about
concepts whilst under-representing other kinds of
features. For example, participants report that
camels have humps, but not that camels have hearts,
even though all participants are likely to have both
pieces of information accessible in their representa-
tion of the concept CAMEL. If a model is successful
in extracting these less salient features, there is no
way of evaluating their correctness using property
norms. A related issue is that participants can only
report verbalizable features, which may not repre-
sent the total sum of their conceptual knowledge
(Murphy, 2002; McRae et al, 2005).
A second problem with using property norms as
the basis of evaluation is that there is often no direct
lexical match between feature terms appearing in the
system output and the norms. Feature norms are typ-
ically normalized such that near-synonymous prop-
erties (e.g. is endangered, is an endangered species,
is almost extinct, etc., for WHALE) given by differ-
ent participants are mapped to the same feature la-
bel (e.g. is endangered). As a consequence, a model
may correctly extract endangered for WHALE, but
other lexical forms of the same feature will not
match any feature in the norms. One solution to this
is to create an expansion set for each feature which
includes its synonyms (Baroni et al, 2008). How-
ever, this is only a partial solution because lexical
variation in features is not limited to synonyms.
A second approach to evaluating semantic mod-
els uses classification or similarity data. For exam-
ple, Andrews et al (2009) evaluated their models by
calculating cosine similarity scores between seman-
tic representations and using these similarity scores
to predict behavioral data which are contingent on
the semantic similarity between pairs of concepts
(e.g. lexical substitution errors, semantic priming
latencies, word-association norms, etc). Although
this approach is psychologically motivated, it evalu-
ates a set of extracted features more indirectly than
comparison with norm data. In computational lin-
guistics, a similarly indirect evaluation method is to
cluster the extracted representations. This approach
avoids the difficulties in evaluating individual fea-
tures; however it only allows consideration along
one dimension of the data, namely the similarity be-
tween pairs of concepts.
fMRI data such as the Mitchell et al (2008)
dataset offers an advancement over both of these
evaluation techniques. Unlike, for example, prop-
erty norming data, fMRI data offers direct insight
into how the brain is functioning in response to given
stimuli. Its multidimensional nature makes it eas-
ier to inspect what aspects of meaning a particular
model is performing strongly or weakly on, and al-
lows for better control of experimental variation. Fi-
nally, it avoids the two major issues associated with
property norms, which we outlined above.
This paper is structured as follows. In the next
section, we briefly describe the models which we
used to extract conceptual representations for the 60
concepts in the Mitchell et al (2008) dataset. In
Section 3, we outline our experimental objectives,
and the framework we adopt for testing our seman-
tic models. In Section 4, we present the results of
our evaluation, which indicate above chance perfor-
mance for each of the models. Finally, we exam-
ine the differences between models by investigating
for which concepts prediction of the fMRI activity is
poorest, and discuss these differences with respect to
the differing assumptions made by the methods.
2 Semantic models
We consider four different semantic models in this
paper, which are described briefly below. These
models were selected as we were interested in the
various kinds of knowledge (part-of-speech, syntac-
tic, and semantic) in corpora available to the extrac-
tion process, and the extent to which the use of these
types of knowledge can affect the quality of the ex-
tracted conceptual representations.
2.1 Mitchell verb-based semantic model
The first semantic model we considered was that
of Mitchell et al (2008). This model assumes that
sensory-motor information is an important aspect of
conceptual representation, and that the information
71
relevant to a target concept?s representation can be
estimated from the concept word?s frequency of co-
occurrence with 25 sensory-motor verbs (eat, ma-
nipulate, push, etc) in a very large corpus. Our reim-
plementation of this method used the co-occurrence
statistics provided by Mitchell et al3 which were
extracted from the Google n-gram corpus consisting
of 1 trillion words of web text.
2.2 SVD model
Secondly, we implemented a co-occurrence-based
Singular Value Decomposition (SVD) model based
on the one described by Baroni and colleagues (Ba-
roni and Lenci, 2008; Baroni et al, 2009). This
model combines aspects of both the HAL (Landauer
et al, 1998) and LSA (Lund and Burgess, 1996)
models in constructing representations for words
based on their co-occurrences in texts. A word-
by-word co-occurrence matrix was constructed for
our corpus, storing how often each target word co-
occurred with each context word. The set of context
words consisted of the 5,000 most frequent content
words (i.e. words not occurring in a stop-list of func-
tion words) appearing in the corpus. The set of target
words consisted of the 60 concept terms appearing
in the fMRI dataset, supplemented with the 10,000
most frequent content words in the corpus (with the
exception of the top 10 most frequent words). For
calculating co-occurrence frequency between target
and context words, the context window was defined
by sentence boundaries: two words were considered
to co-occur if they appeared in the same sentence4.
Following Baroni and Lenci (2008), the dimen-
sionality of the target-word ? context-word co-
occurrence matrix was reduced to 150 columns by
singular value decomposition. That is, the singu-
lar value decomposition of the co-occurrence matrix
was computed and the 150 left singular vectors that
accounted for most of the variance, multiplied by the
corresponding singular values, were used as the 150-
dimensional representation of each target term. Sim-
3http://www.cs.cmu.edu/?tom/science2008/
semanticFeatureVectors.html
4In Baroni et al?s implementation a context window of 5
(Baroni and Lenci, 2008) or 20 (Baroni et al, 2009) words
either side of the target word was used instead; we chose a
sentence-based context window as it is analogous to the context
used in our experimental method (described in the following
section).
ilarity between pairs of target words was calculated
as the cosine between their vectors, and for each of
the 60 concept words in the experimental stimuli we
chose the 200 most similar target words to act as the
feature terms extracted by the model. The corpus
used with this model was the British National Cor-
pus (BNC) (Leech et al, 1994).
2.3 Novel extraction method
Finally we implemented a novel extraction method,
which aims to extract property-norm-like, psy-
chologically meaningful features from corpus data
(Kelly et al, 2010). The method aims to extract se-
mantically unconstrained feature triples of the form
concept-relation-feature , where feature is a feature
(either noun or adjective) of the target concept and
relation is a verb representing the semantic relation-
ship between them. Examples of extracted triples
include: swan be white, swan have neck and screw-
driver be tool. The model uses a corpus parsed for
grammatical relations (GRs) using Robust Accurate
Statistical Parsing (RASP) (Briscoe et al, 2006).
For each sentence containing a target concept, the
set of GRs for that sentence are examined to test
whether they match manually-created rules. These
rules include prototypical feature-relation GR struc-
tures connecting elements of the sentence and rep-
resent dependency patterns which encode potential
semantic relationships between the concept and can-
didate feature terms occurring in the sentence. A
large set of candidate triples are extracted by ap-
plying these rules to each sentence in the corpus
containing a target concept, and the triples for each
concept are ranked by their frequency of extraction.
In the second stage of the method, the extracted
triples are reweighted on the basis of probabilistic
high-level semantic information obtained from hu-
man property norm data. This subsequent stage has
the effect of increasing the weight associated with
more high-quality features and downgrading lower-
quality features. The extraction method is described
more fully in Kelly et al (2010). For this method
we also used the BNC. The top 200 triples ranked
by frequency (i.e. unweighted) and the top 200 fea-
tures after reweighting with the semantic data were
used in our experiments.
72
3 Experiment
As mentioned above, we are primarily interested in
using the fMRI data to evaluate the quality of the
different methods for extracting conceptual repre-
sentations from corpora (rather than being interested
in investigating methods for predicting fMRI activa-
tion). We make no attempt to build on the method
described by Mitchell et al (2008), although there
are likely to be many interesting avenues through
which that method could be extended.5 We therefore
followed the Mitchell et al methodology as closely
as possible, using the same multiple regression train-
ing and leave-two-out cross-validation paradigms as
presented in their paper and supporting online ma-
terial. The only parameter that we varied was the
extraction method (and corpus) that was used to gen-
erate the feature-vectors associated with the 60 con-
cepts that were used during the training phase. The
quality of the predictions generated for the concepts
using each semantic model can therefore be adopted
as an index of model performance.
The Mitchell et al method uses co-occurrence
with a specific set of 25 manually selected verbs
(eat, push, etc) that are the same for each concept.
This results in 25-dimensional feature vectors for in-
put into training. However, for both the SVD model
and our triple extraction models there are no a pri-
ori constraints on the number of unique features that
can be extracted for the concepts. For these mod-
els, we selected the top 200 features associated with
each concept; therefore, across all 60 concepts in the
Mitchell et al dataset, there are thousands of unique
features extracted which are used in the concepts?
representations. To ensure that the linear regres-
sion model for each method would be fitted using
the same number of free parameters during training
(thereby maximizing the comparability of the dif-
ferent methods), we reduced the dimensionality of
the generated feature spaces for the SVD method
and the two triple-extraction methods using Prin-
cipal Components Analysis (PCA). The concept ?
feature extraction frequency matrices for the three
models were submitted to PCA, and the first 25 com-
ponents (i.e. those components which best charac-
5For example, the method currently makes the simplifying
assumption that the activity in neighbouring voxels is indepen-
dent.
Triples (weighted) SVD
PCA1 PCA2 PCA1 PCA2
Highest-valued concepts
horse house coat butterfly
cat apartment skirt cow
cow dog shirt ant
dog igloo pants bee
beetle car dress lettuce
Lowest-valued concepts
knife pants car desk
door coat watch arm
hammer dress horse chair
saw skirt dog knife
chisel shirt fly leg
Table 1: Highest- and lowest-valued concepts for the
first two components for the SVD and weighted triple-
extraction methods.
terized the variance of the original features) for each
model were selected. In the case of the SVD model,
these 25 dimensions explained 77.7% of the vari-
ance in the original 3,061-dimensional vectors. For
our unweighted extraction method, the 25 extracted
components explained 63.0% of the original 5,525
dimensions; for the weighted method the compo-
nents explained 71.5% of the original 6,567 dimen-
sions.
It is interesting to consider the kind of seman-
tic information that is being captured by the resul-
tant PCA components. In particular, the compo-
nents appear to capture meaningful distinctions be-
tween stimuli. For example, the first PCA com-
ponent for our weighted triple extraction method
can be interpreted as the concepts? degree of ?an-
imalness? (animal stimuli have high values on this
component). Table 1 presents the five highest and
lowest-valued concepts for the first two components
for the SVD model and the weighted triple extrac-
tion model. Concepts which overlap with respect
to a specific set of semantic properties tend to have
high or low values on a given dimension, indicating
that that component is capturing a specific cluster of
co-occurring semantic features. For example, PCA1
for SVD can be interpreted as ?has features associ-
ated with clothing?.
Therefore, a key difference between the Michell
73
Method Feature Type POS Syntax Semantics
Mitchell 25 verbs no no no
SVD tuples (content-words) yes no no
triple-extraction method (unweighted) feature-triples yes yes no
triple-extraction method (weighted) feature-triples yes yes yes
Table 2: Comparison of the information available to each model.
et al model and our models is that while Mitchell
et al posit that certain sensory-motor function verbs
can act as important features of concepts, our models
instead place more importance on intrinsic semantic
features.
Finally, Table 2 gives a summary comparison of
the different models, in terms of whether or not each
uses part of speech (POS) data, syntactic informa-
tion (i.e. GRs), and semantic filtering (Section 2.3).
It should be noted that the BNC corpus (used with
the SVD model and our triple-extraction method) is
10,000 times smaller than the corpus from which
the Mitchell et al feature vectors are derived. As
such the semantic representations we extract with
our method need to make better use of the data avail-
able in the corpus if they are to compete with the
verb-based features used by Mitchell et al?s method.
4 Results
The accuracy for each of the four methods was eval-
uated using a leave-two-out validation paradigm.
There are 1,770 possible pairs of concepts that can
be drawn from the set of 60 concept stimuli. Train-
ing was performed separately for each participant
and for each of the 1,770 held-out pairs. Given
a particular participant and held-out pair, for each
voxel v we fit the activation at that voxel to the set
of 58 training items with multiple linear regression,
using as predictor variables the elements of the 25-
dimensional feature vectors associated with each of
the 58 concepts. Training therefore yields a set of
25 ?-coefficients, which can be used to generate a
prediction for the activation yv of voxel v for the
held-out word w using the equation
ypredv =
25?
i=1
?v,ifi,w (1)
where fi,w is the ith element of the feature vector for
word w (see Mitchell et al (2008) for details). Over
all voxels, this method gives a prediction for the ac-
tivation with respect to the held-out word w which
can then be compared to the observed activation for
that stimulus.
Rather than comparing the activity between pre-
dicted and observed images using all voxels, we
compared images using only the 500 most stable
voxels for each participant. For each participant, the
500 most stable voxels were the voxels which gave
the most consistent pattern of activation across the
six presentations of all 60 stimuli (see Mitchell et al
(2008) for details).
The top row of Figure 1 presents the learned co-
efficients for one feature dimension for each of the
four semantic models considered in our experiments
(for these images, all voxels rather then the 500
most stable voxels are used). For the Mitchell et al
method, the coefficients presented correspond to the
verb eat; for the other models the feature is the PCA
component that explained the most variance in the
original representations. We also present the pre-
dicted images for the concepts CELERY and AIR-
PLANE, calculated on the coefficients learned over
the remaining 58 concepts. Importantly, for the
Mitchell et al method (column (a)), the learned co-
efficients for eat and the predicted images for CEL-
ERY and AIRPLANE agree with those reported by
Mitchell et al (2008, Figure 2 & online supplemen-
tary material6).
We calculated similarity between predicted and
observed images using both cosine and Pearson cor-
relation and the 500 most stable voxels; we report
the results using Pearson correlation here as this
measure consistently gave slightly better accuracies
6http://www.cs.cmu.edu/?tom/science2008/
featureSignaturesP1.html
74
(a) ?eat? (b) PCA1/clothes (c) PCA1/clothes (d) PCA1/animals
(a) celery (b) celery (c) celery (d) celery
(a) airplane (b) airplane (c) airplane (d) airplane
Figure 1: Learned coefficients on a selected feature dimension (top row) and predicted activation for CELERY (middle
row) and AIRPLANE (bottom row) for four semantic models: (a) Mitchell et al (2008), (b) SVD (c) triple extraction
method (unweighted), and (d) triple extraction method (weighted). Warmer colours indicate higher values (i.e. larger
?-coefficients for the feature dimensions and higher predicted activation for the concepts). PCA components have been
given intuitive labels indicating the kind of information described by that component (see Table 1). As in Figure 2 of
Mitchell et al (2008), the figure shows just one slice in the horizontal plane (z = -12 in MNI space) for one participant
(P1). The predicted images for CELERY and AIRPLANE were generated from the feature coefficients learned on the
other 58 concepts using each of the four models; the corresponding observed images for CELERY and AIRPLANE can
be found in Mitchell et al (2008) Figure 2 B.
75
Method P1 P2 P3 P4 P5 P6 P7 P8 P9 Mean
Mitchell et al (2008) 0.84 0.83 0.76 0.81 0.79 0.66 0.73 0.64 0.68 0.75
SVD 0.82 0.67 0.79 0.83 0.74 0.64 0.64 0.70 0.75 0.73
Triple-extraction (unweighted) 0.82 0.71 0.79 0.80 0.70 0.69 0.65 0.53 0.78 0.72
Triple-extraction (weighted) 0.82 0.72 0.76 0.83 0.73 0.65 0.68 0.51 0.76 0.72
Table 3: Accuracy results for the four semantic models.
for each of the four models (the results are very simi-
lar using the cosine measure). Following Mitchell et
al. (2008; supplementary material), a match score
for each held out pair w1 and w2 was calculated
as the sum of the similarities between the correctly
aligned predicted and observed images:
a = sim(wpred1 , w
obs
1 ) + sim(w
pred
2 , w
obs
2 ) (2)
Similarly a mismatch score was calculated as
b = sim(wpred1 , w
obs
2 ) + sim(w
pred
2 , w
obs
1 ) (3)
Cases where the match score is greater than the mis-
match score (i.e. a > b) count as successes for the
model (i.e. the model correctly identifies the two
predicted images). Otherwise there is a failure by
the model (i.e. the model identifies the observed im-
age for w1 as being w2 and vice-versa).
Table 3 presents the results of the leave-two-
out cross-validation evaluation, giving the propor-
tion (across all 1,770 pairs) of predicted images for
the held-out pairs that were correctly matched to
the observed images.7 The original Mitchell et al
(2008) model has the best mean performance, al-
though across the nine participants, there is no sig-
nificant difference in accuracy between any of the
models (|t(8)| < 1.49, p > 0.17, for all pairwise
paired t-tests between Mitchell et al (2008), SVD,
and weighted triple extraction).
That there is no difference between the perfor-
mance of the Mitchell et al (2008), SVD and triple
7Our results for the Mitchell et al (2008) method are simi-
lar, though not identical, to those reported in that paper (where
the reported mean accuracy across all participants is 0.77, using
cosine similarity). Our implementation of the method for select-
ing the 500 most stable voxels yields slightly different voxels
from those obtained by Mitchell et al (2008; see supplemen-
tary material). In any case, the same set of 500 voxels for each
participant were used for generating the results of each model
presented here, and so we do not believe that this discrepancy
affects comparison of the different models.
extraction methods is surprising, given the different
kinds of information that are available to the dif-
ferent models. In particular, the models that auto-
matically acquire very general and semantically un-
constrained feature-based representations perform
as well as the model which uses a set of manually-
selected sensory-motor verbs, even though the rep-
resentations generated for these models are derived
from 10,000 times less corpus data.
As mentioned in our introduction, an advantage of
evaluating against the fMRI dataset is that this multi-
dimensional data allows us to investigate strengths
and weaknesses of different models in a way which
is not possible using similarity or clustering-based
evaluation. As a very simple investigation of spe-
cific differences in model performance, we present
in Table 4 the pairs of concepts for which each of the
models performs most poorly on. The Mitchell et al
(2008) method appears to do poorly on pairs of con-
cepts where a constituent word can be ambiguous
with respect to its part-of-speech (e.g. SAW, BEAR).
This is not surprising, given that part-of-speech data
is not available in the Google n-gram corpus used
with this method. The performance of the Mitchell
et al method might therefore be improved signifi-
cantly by applying heuristics to the n-gram data to
make inferences about the correct part-of-speech of
instances of words like SAW and BEAR. For the SVD
and weighted triple extraction methods, which both
use the BNC corpus, there is some evidence that
the models are performing poorly for relatively low
frequency words8 (e.g. CHISEL), words which are
semantically ambiguous as nouns (e.g. ARM), and
pairs which are semantically similar (e.g. SPOON &
KNIFE). This suggests that the SVD and triple ex-
traction methods may perform better with a larger
and more diverse corpus.
8AIRPLANE is relatively low frequency in the BNC; it may
be more sensible to use the word AEROPLANE with a British
corpus.
76
Mitchell et al SVD Triple Extraction (weighted)
Pair Nr. Pair Nr. Pair Nr.
bear saw 0 cup airplane 0 dresser chimney 0
bell carrot 0 cup lettuce 0 airplane chisel 0
bell saw 0 horse beetle 0 airplane hand 0
knife bear 0 chisel arm 0 airplane tomato 0
cup saw 1 hammer arm 1 spoon chisel 0
bear tomato 1 dresser arch 1 spoon knife 0
Table 4: Leave-out pairs for which each model performs least accurately, across the nine participants. Nr. = the number
of participants for which this leave-out pair was correctly matched.
5 Conclusion
The fMRI dataset and training and evaluation
methodology presented by Mitchell et al (2008)
gives researchers an interesting new framework with
which to evaluate the quality of feature-based con-
ceptual representations extracted from corpora. This
framework avoids some of the problems inherent in
evaluating extracted representations against a ?gold
standard? based on participant-generated property
norms. It also provides a rich multi-dimensional
dataset through which the strengths and weaknesses
of extraction methods can be identified.
We have applied this evaluation framework to
four feature extraction methods which use different
sources of information available in corpora to extract
conceptual representations. Surprisingly, in spite of
their major differences, we did not find any signifi-
cant difference in performance between the models.
This finding has interesting theoretical implica-
tions, given that previous research has suggested
that aspects of meaning defined by sensory-motor
verbs may have a somewhat distinctive role to play
in predicting the fMRI activation associated with
conceptual stimuli (Mitchell et al, 2008). Our re-
sults suggest that general feature-based representa-
tions of concepts, which place no a priori distinc-
tion on sensory-motor properties, may be equally
capable of predicting activation to conceptual stim-
uli. This highlights the potential for the Mitchell
et al method to be used to inform both distributed
and sensory-motor accounts of conceptual represen-
tation (e.g. McRae et al (1997), Cree et al (2006),
Tyler et al (2000), Tyler & Moss (2001), Moss et
al. (2007), Martin & Chao (2001)), as well as pro-
viding a benchmark with which to assess semantic
model development. In a similar vein, Murphy et
al. (2009) used a dependency-parsed corpus yielding
verb co-occurrence statistics to predict EEG9 activa-
tion patterns with significant accuracy.
The training and evaluation framework presented
by Mitchell et al (2008) represents just one point
in a large space of possibilities for using computa-
tional modelling to predict human brain activity as-
sociated with conceptual stimuli. In these initial ex-
periments, we have chosen to follow the Mitchell
et al approach as closely as possible, in order to
maximize comparability with their results. In future
work, we aim to investigate other methods for train-
ing and evaluation, other corpora and other sources
of imaging data. Furthermore, we aim to use the
evaluation results from such work to inform the de-
velopment of our extraction method.
Acknowledgments
Our work was funded by the EPSRC grant
EP/F030061/1, and the Royal Society University
Research Fellowship, UK. We thank Mitchell et
al. (2008) and McRae et al (2005) for making their
data publically available.
References
Mark Andrews, G. Vigliocco, and D. Vinson. 2005.
Integrating attributional and distributional informa-
tion in a probabilistic model of meaning representa-
tion. In Timo Honkela et al, editor, Proceedings of
AKRR?05, International and Interdisciplinary Confer-
ence on Adaptive Knowledge Representation and Rea-
9EEG measures voltages induced by neuronal firing across
the human scalp.
77
soning, pages 15?25, Espoo, Finland: Helsinki Uni-
versity of Technology.
Mark Andrews, Gabriella Vigliocco, and David Vinson.
2009. Integrating experiential and distributional data
to learn semantic representations. Psychological Re-
view, 116(3):463?498.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. From context to mean-
ing: Distributional models of the lexicon in linguis-
tics and cognitive science (Special issue of the Italian
Journal of Linguistics), 20(1):55?88.
Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
tors. 2008. ESSLLI 2008 Workshop on Distributional
Lexical Semantics.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2009. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, pages 1?33.
E. Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the Interactive Demo Session of COLING/ACL-
06, pages 77?80.
George S. Cree, Chris McNorgan, and Ken McRae.
2006. Distinctive features hold a privileged status
in the computation of word meaning: Implications
for theories of semantic memory. Journal of Experi-
mental Psychology. Learning, Memory, and Cognition,
32(4):643?58.
Colin Kelly, Barry Devereux, and Anna Korhonen. 2010.
Acquiring human-like feature-based conceptual repre-
sentations from corpora. In Brian Murphy, Kai min
Kevin Chang, and Anna Korhonen, editors, Proceed-
ings of the NAACL-HLT Workshop on Computational
Neurolinguistics, Los Angeles, USA.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
G. Leech, R. Garside, and M. Bryant. 1994. CLAWS4:
the tagging of the British National Corpus. In Pro-
ceedings of the 15th conference on Computational
linguistics-Volume 1, pages 622?628. Association for
Computational Linguistics.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28(2):203?208.
Alex Martin and Linda L. Chao. 2001. Semantic mem-
ory and the brain: structure and processes. Current
Opinion in Neurobiology, 11(2):194?201.
Ken McRae, Virginia R. de Sa, and Mark S. Seidenberg.
1997. On the nature and scope of featural representa-
tions of word meaning. Journal of Experimental Psy-
chology: General, 126(2):99?130.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37:547?559.
Tom M. Mitchell, Svetlana V. Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L. Malave, Robert A.
Mason, and Marcel A. Just. 2008. Predicting human
brain activity associated with the meanings of nouns.
Science, 320(5880):1191?1195.
Helen E. Moss, Lorraine K. Tyler, and Kirsten I. Taylor.
2007. Conceptual structure. In M. Gareth Gaskell, ed-
itor, The Oxford handbook of psycholinguistics, pages
217?234. Oxford University Press, Oxford, UK.
B. Murphy, M. Baroni, and M. Poesio. 2009. Eeg re-
sponds to conceptual stimuli and corpus semantics.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2009),
pages 619?627, East Stroudsburg, PA.
Gregory Murphy. 2002. The big book of concepts. The
MIT Press, Cambridge, MA.
Mark Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234?243.
Lorraine K. Tyler and Helen E. Moss. 2001. Towards a
distributed account of conceptual knowledge. Trends
in Cognitive Sciences, 5(6):244?252.
L. K. Tyler, H. E. Moss, M. R. Durrant-Peatfield, and J. P.
Levy. 2000. Conceptual structure and the structure of
concepts: A distributed account of category-specific
deficits. Brain and Language, 75(2):195?231.
78
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 11?20,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Semi-supervised learning for automatic conceptual property extraction
Colin Kelly
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
colin.kelly
@cl.cam.ac.uk
Barry Devereux
Centre for Speech,
Language, and the Brain
University of Cambridge
Cambridge, CB2 3EB, UK
barry@csl.psychol.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
anna.korhonen
@cl.cam.ac.uk
Abstract
For a given concrete noun concept, humans
are usually able to cite properties (e.g., ele-
phant is animal, car has wheels) of that con-
cept; cognitive psychologists have theorised
that such properties are fundamental to un-
derstanding the abstract mental representation
of concepts in the brain. Consequently, the
ability to automatically extract such properties
would be of enormous benefit to the field of
experimental psychology. This paper investi-
gates the use of semi-supervised learning and
support vector machines to automatically ex-
tract concept-relation-feature triples from two
large corpora (Wikipedia and UKWAC) for
concrete noun concepts. Previous approaches
have relied on manually-generated rules and
hand-crafted resources such as WordNet; our
method requires neither yet achieves bet-
ter performance than these prior approaches,
measured both by comparison with a property
norm-derived gold standard as well as direct
human evaluation. Our technique performs
particularly well on extracting features rele-
vant to a given concept, and suggests a number
of promising areas for future focus.
1 Introduction
The representation of concrete concepts (e.g., car,
banana, spanner) in the human brain has long been
an important area of investigation for cognitive psy-
chologists. Recent theories of this mental repre-
sentation have proposed a componential, property-
based and distributed model of conceptual knowl-
edge (e.g., Farah and McClelland (1991), Randall et
al. (2004), Tyler et al (2000)).
In order to empirically test these cognitive the-
ories, researchers have moved towards employing
real-world knowledge in their experiments. This
knowledge has usually been procured from human-
derived lists of properties taken from property norm-
ing studies (Garrard et al, 2001; McRae et al,
2005). In such studies, human participants are
asked to describe and note properties of a given
concept (e.g., has shell for turtle). Synonymous
responses are grouped together as a single prop-
erty and those meeting a certain minimum response-
frequency threshold are taken as valid properties.
The most wide-ranging study to date was that con-
ducted by McRae et al (2005): some sample prop-
erties from this set are in Table 1.
As others have noted (Murphy, 2002; McRae et
al., 2005), property norming studies are prone to a
number of deficiencies. One such weakness is the
incongruity of shared properties across even highly-
related concepts: human respondents exhibit a lack
of consistency when listing properties that are com-
mon to many similar concepts. For example, while
has legs is listed as a property of crocodile in the
McRae norms, it is absent as a property of alliga-
tor. A related issue is the non-comprehensive nature
of the generated norms ? although they may cover
the most salient properties for a given concept, they
are unlikely to comprise all of a concept?s properties
(e.g., has heart does not appear as a property of any
of the 92 animal concepts).
Our research aims to use NLP techniques to cre-
ate a system able to emulate the output of such
studies, and overcome some of the aforementioned
weaknesses. Our proposed system begins by search-
ing dependency-parsed corpora for those sentences
containing concept and feature terms which are
also found in a McRae norm-derived training set
of properties. For these sentences, the system
generates grammatical relation/part-of-speech struc-
tural attributes and applies support vector machines
(SVMs) to learn sets of attributes likely to indicate
the instantiation of a property in a sentence. These
11
turtle bowl
has a shell 25 is round 19
lays eggs 16 used for eating 12
swims 15 used for soup 11
is green 14 used for food 11
lives in water 14 used for liquids 10
is slow 13 used for eating cereal 10
an animal 11 made of plastic 8
walks 10 used for holding things 7
walks slowly 10 is curved 7
has 4 legs 9 found in kitchens 7
Table 1: Top ten properties from McRae norms with pro-
duction frequencies for turtle and bowl.
learned patterns of salient attributes are finally ap-
plied to a corpus to derive new properties for unseen
concepts.
Our task is a challenging one: the properties we
seek are extremely diverse in their form. They range
from the simple (e.g., banana is yellow) to the com-
plex (e.g., bayonet found at the end of a gun). Al-
though the properties can broadly be divided into
a number of categories (encyclopedic, taxonomic,
functional, etc) there is not a great deal of regular-
ity in the nature of the properties a given noun will
likely possess: it is highly concept-dependent.
Furthermore, we hope to derive these properties
from corpora, with the assumption that these prop-
erties will manifest themselves therein. Indeed, An-
drews et al (2005) discuss a theory of human knowl-
edge which relies on a combination of both dis-
tributional (i.e., derived from spoken and written
language) and experiential data (i.e., that derived
from our interactions with the real world), claiming
that the necessary contribution of each data-type for
a comprehensive human semantic representation is
non-trivial. Finally, there are difficulties associated
with evaluating our system?s output directly against
a set of human-generated property norms: we dis-
cuss these in further detail later.
Given their provenance, the properties found in
property norms are free-form. To simplify our task
we apply a more rigid representation to the proper-
ties we already have and to those we aim to seek. We
delineate each property into a concept relation fea-
ture triple (see Section 2.2) and our task becomes
one of finding valid relation feature pairs given a par-
ticular concept. This recoding renders our task more
well-defined and makes evaluation of our method
reptile1NNS
include2VBP
species0IN
of3IN
turtle5NN
dobj
five1DT
ncmod ncmod
dobj
marine0NNP
ncmod ncsubj
Figure 1: C&C-derived GR-POS graph for the sentence
Marine reptiles include five species of turtle.
more comparable to previous and related work.
Having framed our task in this way, there is an
obvious parallel with relation extraction: both ne-
cessitate the selection/classification of relationships
between individual entities (in our case, between
concept and feature). Hearst (1992) was the first
to propose a pattern-based approach to this task us-
ing lexico-syntactic patterns to automatically extract
hyponyms and this technique has frequently been
used for ontology learning. For example, Pantel and
Pennacchiotti (2008) linked instantiations of a set of
semantic relations into existing semantic ontologies
and Davidov et al (2007) employed seed concepts
from a given semantic class to discover relations
shared by concepts in that class.
Our task is more complex than classic relation ex-
traction for two main reasons: 1) the relations which
we aim to extract are not limited to a small set of
just a few well-defined relations (e.g., is-a and part-
of) nor to the relations of a specific semantic class
(e.g., capital-is for countries). Indeed the relations
can be as many and diverse as the concepts them-
selves (e.g., each concept could possess a unique
and distinguishing relation and feature). 2) We are
attempting to simultaneously extract two pieces of
information: features of the concept and those fea-
tures? defining relationship with the concept, but
only those relations and features which would be
classified as ?common-sense?, something which is
easy for humans to recognise but difficult (if not im-
possible) to describe rigorously or formally.
There has recently been work on the automatic ex-
12
traction of binary relations that scale to a web cor-
pus, for example the ReVerb (Etzioni et al, 2011)
and WOE (Wu and Weld, 2010) systems. These
systems are designed to extract legitimate relations
from a given sentence. In contrast, our aim is to cap-
ture more general relationships which are ?common-
sense?; just because an extracted relation is correct
in a given context does not automatically make it
true in general. Previous reasoned approaches to our
task have taken their lead from Hearst and her suc-
cessors, employing manually-created rulesets to ex-
tract such properties from corpora (e.g., Baroni et al
(2009), Devereux et al (2010), and our comparison
system (Kelly et al, 2010)). Baroni et al extract re-
lational information in the form of ?type-sketches?,
which give an approximate, implicit description of
the relationship whereas we are aiming to extract
explicit relations between the target concept and its
corresponding features. Devereux et al and Kelly
et al have attempted this, but both employ WordNet
(Fellbaum, 1998) to extract semantic relatedness in-
formation.
We use semi-supervised learning as it offers a
flexible technique of harnessing small amounts of
labelled data to derive information from unlabelled
datasets/corpora and allows us to guide the extrac-
tion towards our desired ?common-sense? output.
We chose SVMs as they have been used for a va-
riety of tasks in NLP (e.g., Joachims et al (1998),
Gime?nez and Marquez (2004)). We will demon-
strate that our system?s performance exceeds that of
Kelly et al (2010) and Etzioni et al (2011). It is, as
far as we are aware, the first work to employ semi-
supervised learning for this task.
2 Method
We will use SVMs to learn lexico-syntactic pat-
terns in our corpora corresponding to known prop-
erties in order to find new ones. Training an SVM
requires a labelled training set. To generate this
set we harness our already-known concepts/features
(and their relationships) from the McRae norms to
find instantiations of said relationships within our
corpora. We use parsed sentence information from
our corpora to create a set of attributes describing
each relationship, our learning patterns. In doing
so, we are assuming that across sentences in our
corpora containing a concept/feature pair found in
the McRae norms, there will be a set of consistent
lexico-syntactic patterns which indicate the same re-
lationship as that linking the pair in the norms.
Thus we iterate over our chosen corpora, parsing
each concept-containing sentence to yield grammat-
ical relation (GR) and part-of-speech (POS) infor-
mation from which we can create a GR-POS graph
relating the two. Then for each triple, we find any/all
paths through the graph which link the concept to its
feature and use the corresponding relation to label
this path. We collect descriptive information about
the path in the form of attributes describing it (e.g.,
path nodes, labels, length) to create a training pattern
specific to that concept relation feature triple and
sentence. It is these lists of attributes (and their rela-
tion labels) which we employ as the labelled training
set and as input for our SVM.
2.1 Corpora
We employ two corpora for our experiments:
Wikipedia and the UKWAC corpus (Ferraresi et al,
2008). These are both publicly available and web-
based: the former a source of encyclopedic infor-
mation and the latter a source of general text. Our
Wikipedia corpus is based on a Sep 2009 version
of English-language Wikipedia and contains around
1.84 million articles (>1bn words). Our UKWAC
corpus is an English-language corpus (>2bn words)
obtained by crawling the .uk internet domain.
2.2 Training data
Our experiments use a British-English version of
the McRae norms (see Taylor et al (2011) for de-
tails). We needed to recode the free-form McRae
properties into relation-classes and features which
would be usable for our learning algorithm. As
we will be matching the features from these prop-
erties with individual words in the training corpus
it was essential that the features we generated con-
tained only one lemmatised word. In contrast, the
relations were merely labels for the relationship de-
scribed (they did not need to occur in the sentences
we were training from) and therefore needed only
to be single-string relations. This allowed preposi-
tional verbs as distinct relations, something which
has not been attempted in previous work yet can be
semantically significant (e.g., the relations used-in,
used-for and used-by have dissimilar meanings).
We applied the following sequential multi-step
13
process to our set of free-form properties to distill
them to triples of the form concept relation feature,
where relation can be a multi-word string and feature
is a single word:
1. Translation of implicit properties to their correct re-
lations (e.g., pig an animal ? pig is an animal).
2. Removal of indefinite and definite articles.
3. Behavioural properties become ?does? properties
(e.g., turtle beh eats ? turtle does eats).
4. Negative properties given their own relation classes
(e.g., turkey does cannot fly ? turkey doesnt fly).
5. All numbers are translated to named cardinals (e.g.,
spider has 8 legs ? spider has eight legs).
6. Some of the norms already contained synonymous
terms: these were split into separate triples for each
synonym (e.g., pepper tastes hot/spicy ? pepper
tastes hot and pepper tastes spicy).
7. Prepositional verbs were translated to one-word,
hyphenated strings (e.g., made of ? made-of ).
8. Properties with present participles as the penulti-
mate word were split into one including the verb as
the feature and one including it in the relation (e.g.,
envelope used for sending letters ? envelope used-
for-sending letters and envelope used-for sending).
9. Any remaining multi-word properties were split
with the first term after the concept acting as the
relation (e.g., bull has ring in its nose ? bull has
ring, bull has in, bull has its and bull has nose).
10. All remaining stop-words were removed; properties
ending in stop-words (e.g., bull has in and bull has
its) were removed completely.
This yielded 7,518 property-triples with 254 distinct
relations and an average of 14.7 triples per concept.
2.3 Parsing
We parsed both corpora using the C&C parser
(Clark and Curran, 2007) as we employ both GR
and POS information in our learning method. To ac-
celerate this stage, we process only sentences con-
taining a form (e.g., singular/plural) of one of our
training/testing concepts. We lemmatise each word
using the WordNet NLTK lemmatiser (Bird, 2006).
Parsing our corpora yields around 10Gb and 12Gb
of data for UKWAC and Wikipedia respectively.
The C&C dependency parse output contains, for
a given sentence, a set of GRs forming an acyclic
graph whose nodes correspond to words from the
sentence, with each node also labelled with the POS
of that word. Thus the GR-POS graph interrelates all
lexical, POS and GR information for the entire sen-
tence. It is therefore possible to construct a GR-POS
graph rooted at our target term (the concept in ques-
tion), with POS-labelled words as nodes, and edges
labelled with GRs linking the nodes to one another.
An example graph can be seen in Figure 1.
2.4 Support vector machines
We use SVMs (Cortes and Vapnik, 1995) for our
experiments as they have been widely used in NLP
and their properties are well-understood, showing
good performance on classification tasks (Meyer et
al., 2003). In their canonical form, SVMs are non-
probabilistic binary linear classifiers which take a set
of input data and predict, for each given input, which
of two possible classes it corresponds to.
There are more than two possible relation-labels
to learn for our input patterns, so ours is a multi-class
classification task. For our experiments we use the
SVM Light Multiclass (v. 2.20) software (Joachims,
1999) which applies the fixed-point SVM algorithm
described by Crammer and Singer (2002) to solve
multi-class problem instances. Joachims? software
has been widely used to implement SVMs (Vinok-
ourov et al, 2003; Godbole et al, 2002).
2.5 Attribute selection
Previous techniques for our task have made use of
lexical, syntactic and semantic information. We are
deliberately avoiding the use of manually-created
semantic resources, so we rely only on lexical and
syntactic attributes for our learning stage (i.e., the
GR-POS paths described earlier).
A table of all the categories of attributes we ex-
tract for each GR-POS path are in Table 2.4, together
with attributes from the path linking turtle and reptile
in our example sentence (see Figure 1).
We ran our experiments with two vector-types
which we call our ?verb-augmented? and our ?non-
augmented? vector-types. The sets are identical ex-
cept the verb-augmented vector-type will also con-
tain an additional attribute category containing an
attribute for every instance of a relation verb (i.e.,
a verb which is found in our training set of relations,
e.g., become, cause, taste, use, have and so on) in
the lexical path. We do this to ascertain whether this
additional verb-information might be more informa-
tive to our system when learning relations (which
tend to be composed of verbs).
14
Attribute category Example attribute(s)
GR path-length LEN
lemmatised anchor node LEM=turtle
POS of anchor node POS=NN
GR path labels GR1=dobjR
from anchor GR2=ncmodR
(indexed) GR3=dobjR
GR4=ncsubjN
GR path labels GR1=ncsubjR
from target GR2=dobjN
(indexed) GR3=ncmodN
GR4=dobjN
POS of path nodes POS1=IN
from anchor POS2=NNS
(indexed) POS3=VBP
POS4=NNS
POS of path nodes POS1=NNS
from target POS2=VBP
(indexed) POS3=NNS
POS4=IN
lemmatised path nodes LEM=include
(bag of words) LEM=species
LEM=of
POS of all path nodes POS=IN
(set) POS=NNS
POS=VBP
Relation verbs N/A
GR path labels GR=dobjR
(set) GR=ncmodN
GR=ncsubjN
lemmatised target node LEM=reptile
POS of target node POS=NNS
Table 2: An example vector for an instance of the
relation-label is. The attributes are distinguished from
one another by their attribute category. Relation verbs
only appear in the verb-augmented vector-type and no
such verbs appear in our example sentence, so this cat-
egory of attribute is empty. All attributes in the table will
receive the value 1.0 except the LEN attribute which will
have the value 0.2 (the reciprocal of the path length, 5).
We considered allocating a ?no-rel? relation la-
bel to those sets of attributes corresponding to paths
through the GR-POS graph which did not link the
concept to a feature found in our training data;
however our initial experiments indicated the SVM
model would assign every pattern we tested to the
?no-rel? relation. Therefore we used only positive
instances in our training pattern data.
We cycle through all training concepts/features,
finding sentences containing both. For each such
sentence, our system generates the attributes from
the GR-POS path linking the concept to the fea-
ture (the linking-path) to create a pattern for that
pair, in the form of a relation-labelled vector con-
taining real-valued attributes. The system assigns
1.0 to all attributes occurring in a given path
and the LEN value receives the reciprocal of the
path-length.1 Each linking-path is collected into a
relation-labelled, sparse vector in this manner. In
the larger UKWAC corpus this corresponds to over
29 million unique attributes across all found linking-
paths (this figure corresponds to the dimensionality
of our vectors). We then pass all vectors to the learn-
ing module2 of SVM Light to generate a learned
model across all training concepts.
2.6 Extracting candidate patterns
Having trained our model, we must now find po-
tential features and relations for our test concepts
in our corpora. We again only examine sentences
which contain at least one of our test concepts. Fur-
thermore, to avoid a combinatorial explosion of pos-
sible paths rooted at those concepts we only permit
as candidates those paths whose anchor node is a
singular or plural noun and whose target node is ei-
ther a singular/plural noun or adjective. This filter-
ing corresponds to choosing patterns containing one
of the three most frequent anchor node POS tags
(NN, NNS and NNP) and target node POS tags (NN,
JJ and NNS) found during our training stage. These
candidate patterns constitute 92.6% and 87.7% of
all the vectors, respectively, from our training set
of patterns (on the UKWAC corpus). This pattern
pre-selection allows us to immediately ignore paths
which, despite being rooted at a test concept, are un-
likely to contain property norm-like information.
2.7 Generating and ranking triples
We next classified our test concepts? candidate
patterns using the learned model. SVM Light as-
signs each pattern a relation-class from the training
set and outputs the values of the decision functions
from the learned model when applied to that par-
ticular pattern. The sign of these values indicates
the binary decision function choice, and their mag-
nitude acts as a measure of confidence. We wanted
those vectors which the model was most confident
in across all decision functions, so we took the sum
of the absolute values of the decision values to gen-
erate a pattern score for each vector/relation-label.
1All other possible attributes are assigned the value 0.0.
2Using a regularisation parameter (C) value of 1.0 and de-
fault parameters otherwise.
15
Vector-type Corpus ?LL ?PMI ?SVM Prec. Recall F
Ignoring relation.
Non-augmented
Wikipedia 0.3 0.00 1.00 0.2214 0.3197 0.2564
UKWAC 0.10 0.05 0.60 0.2279 0.3330 0.2664
UKWAC-Wikipedia 0.35 0.00 0.75 0.2422 0.3533 0.2829
Verb-augmented
Wikipedia 0.20 0.00 0.65 0.2217 0.3202 0.2568
UKWAC 0.30 0.00 0.95 0.2326 0.3400 0.2720
UKWAC-Wikipedia 0.40 0.05 1.00 0.2444 0.3577 0.2859
With relation.
Non-augmented
Wikipedia 0.05 0.00 1.00 0.1199 0.1732 0.1394
UKWAC 0.05 0.00 1.00 0.1126 0.1633 0.1312
UKWAC-Wikipedia 0.05 0.00 0.65 0.1241 0.1808 0.1449
Verb-augmented
Wikipedia 0.05 0.00 1.00 0.1215 0.1747 0.1410
UKWAC 0.05 0.00 1.00 0.1190 0.1724 0.1387
UKWAC-Wikipedia 0.05 0.00 0.70 0.1281 0.1860 0.1494
Table 3: Parameter estimation both with and without relation, using our augmented and non-augmented vector-types
and across our two corpora and the combined corpora set.
From these patterns we derived an output set of
triples where the concept and feature of a triple cor-
responded to the anchor and target nodes of its pat-
tern and the relation corresponded to the pattern?s
relation-label. Identical triples from differing pat-
terns had their pattern scores summed to give a final
?SVM score? for that triple.
2.8 Calculating triple scores
A brief qualitative evaluation of our system?s out-
put indicates that although the higher-ranked (by
SVM score) features and relations were, for the most
part, quite sensible, there were some obvious output
errors (e.g., non-dictionary strings or verbs appear-
ing as features). Therefore we restricted our fea-
tures to those which appear as nouns or adjectives in
WordNet and excluded features containing an NLTK
(Bird, 2006) corpus stop-word. Despite these exclu-
sions, some general (and therefore less informative)
relation/feature combinations (e.g., is good, is new)
were still ranking highly. To mitigate this, we ex-
tract both log-likelihood (LL) and pointwise mutual
information (PMI) scores for each concept/feature
pair to assess the relative saliency of each extracted
feature, with a view to downweighting common but
less interesting features. To speed up this and later
stages, we calculate both statistics for the top 1,000
triples extracted for each concept only.
PMI was proposed by Church and Hanks (1990)
to estimate word association. We will use it to mea-
sure the strength of association between a concept
and its feature. We hope that emphasising concept-
feature pairs with high mutual information will ren-
der our triples more relevant/informative.
We also employ the LL measure across our set of
concept-feature pairs. Proposed by Dunning (1993),
LL is a measure of the distribution of linguistic phe-
nomena in texts and has been used to contrast the
relative corpus frequencies of words. Our aim is to
highlight features which are particularly distinctive
for a given concept, and hence likely to be features
of that concept alone.
We calculate an overall score for a triple, t, by a
weighted combination of the triple?s SVM, PMI and
LL scores using the following formula:
score(t) = ?PMI?PMI(t)+?LL?LL(t)+?SVM?SVM(t)
where the PMI, SVM and LL scores are normalised
so they are in the range [0, 1]. The relative ? weights
thus give an estimate of the three measures? impor-
tance relative to one another and allows us to gauge
which combination of these scores is optimal.
2.9 Datasets
We also wanted to ascertain the extent to which
the output from both our corpora could be combined
to improve results, balancing the encyclopedic but
somewhat specific nature of Wikipedia with the gen-
erality and breadth of the UKWAC corpus. We com-
bined the output by summing individual SVM scores
of each triple from both corpora to yield a combined
SVM score. PMI and LL scores were then calcu-
lated as usual from this combined set of triples.
3 Experimental Evaluation
3.1 Evaluation methodology
We employ ten-fold cross-validation to ascertain
optimal SVM, LL and PMI ? parameters for our fi-
nal system. We exclude 44 concepts from our set of
16
Relation Prec. Recall F
Kelly et al Without 0.1943 0.3896 0.2592With 0.1102 0.2210 0.1471
ReVerb Without 0.1142 0.2258 0.1514With 0.0431 0.0864 0.0576
Our method Without 0.2417 0.4847 0.3225With 0.1238 0.2493 0.1654
Table 4: Our best scores on the ESSLLI set compared to
Kelly et al (2010) and the ReVerb system (Etzioni et al,
2011). Our results are from the verb-augmented vector-
type, using the combined UKWAC-Wikipedia corpus and
using the ? parameters highlighted in Table 3.
510 to use in our final system testing and split the
remaining 466 concepts randomly and evenly into
10 folds. We apply the training steps above to nine
of the folds, generating predictions for the single
held-out fold. We repeat this for all ten folds, yield-
ing relations and features with SVM, LL and PMI
scores for our full set of 466 training concepts on
the UKWAC, Wikipedia and combined corpora.
We varied the ? values from our scoring equa-
tion in the range [0,1] (interval 0.05) and com-
pared the top twenty triples for each concept directly
against the held-out training set. The best F-scores
and their corresponding ? values (evaluating on full
triples and concept-feature pairs alone) are in Ta-
ble 3. We can see that our best results employ the
verb-augmented vector-type and the combined cor-
pus, with a best F-score of 0.2859 when ignoring
the relation term and 0.1494 when including it in the
evaluation. The main difference between these two
results is the relative contribution of the reweighting
factors: the SVM score is the most important over-
all, but the LL and PMI scores come into play when
evaluating without the relation. This could be ex-
plained by the fact that the PMI and LL scores do
not use any relation terms in their calculations.
3.2 Quantitative evaluation
The unseen subset of the McRae norms is a set
of human-generated common-sense properties with
which our extracted properties can be compared.
However, an issue with the McRae norms is that
semantically identical properties can be represented
by lexically different triples. This problem was ac-
knowledged by Baroni et al (2008) who created
a synonym-expanded set of properties for 44 con-
cepts (selected evenly across six semantic classes;
the 44 concepts we excluded for testing) to par-
Judge Judge
turtle A B bowl A B
is green c c is large p p
is small c c used for food c c
is species c c used for mixing c c
is marine c c used for storing food c c
used for sea r r used for storing soup r r
is animal c c is ceramic c c
is many p c is small p p
has shell c c used for storing cereal r r
is large c p used for storing spoon r r
is reptile c c used for storing sugar p c
Table 5: Our judges? assessments of the correctness of the
top ten relation/feature pairs for two concepts extracted
from our best system.
tially solve it. This expansion set comprises the con-
cepts? top ten properties from the McRae norms with
semi-automatically generated synonyms for each of
the ten distinct features. For example, the triple
turtle has shell was expanded to also include tur-
tle has shield and turtle has carapace.
We use the two best systems (i.e., including and
excluding the relation; highlighted in Table 3) to
generate two sets of top twenty output triples for
our 44 concepts. We then calculate precision, re-
call and F-scores for each against our synonym-
expanded set.3 Using this expanded set alows us
to compare our work with that of Kelly et al (2010).
We also compare with the top twenty output of the
Reverb system Etzioni et al (2011) using their pub-
licly available relations derived from the ClueWeb09
corpus, employing their normalized triples ranked
by frequency. All sets of results are in Table 4. We
note that even though Kelly et al optimised their
algorithm on the ESSLLI set to yield a theoretical
best-possible score?we are evaluating ?blind??our
performance still shows an advance on theirs: the
improvement on both sets when comparing the pop-
ulation of F-scores across all 44 concepts is statisti-
cally significant at the 0.5% level.4
3.3 Human evaluation
The above does not quite offer the full picture:
unlike the features, the relations are not synonym-
expanded. Furthermore, it is possible that there
3We note that we are incorporating an upper bound for pre-
cision of 0.500 by comparing with only the top ten properties.
4Paired t-tests. ?With relation?: t = 3.524, d.f.= 43, p =
0.0010. ?Without relation?: t = 3.503, d.f.= 43, p = 0.0011.
17
Relation A B ? Agreements
With c / p 146 161 0.7421 261 (87%)
r / w 153 138
Without c / p 226 235 0.5792 255 (85%)
r / w 74 65
Table 6: Inter-annotator agreement for our best system,
both including and excluding the relation.
are correct properties being generated which simply
don?t appear in the ESSLLI evaluation set.
In order to address these concerns, we also per-
formed a human evaluation on 15 of our concepts.5
We asked two native English-speaking judges to de-
cide whether a given triple was correct,6 plausible,7
wrong but related,8 or wrong.9 We executed the
human evaluation on our two best systems (as de-
scribed above). As there were shared triples and
concept-feature pairs across the two output sets,
each triple and pair was evaluated only once. The
judges were aware of the purposes of the study but
were blind to the source sets. Some example judge-
ments are in Table 5.
The agreement results across all 15 concepts to-
gether with their ? coefficients (Cohen, 1960) are
in Table 6. In our evaluation we conflated the cor-
rect/plausible and wrong but related/wrong cate-
gories (see also Kelly et al (2010) and Devereux et
al. (2010)). We did this because of the subjective na-
ture of the judgements, and because we are seeking
properties which are indeed correct or at least plausi-
ble. These results indicate that our system is extract-
ing correct or plausible triples 51.1% of the time (ris-
ing to 76.8% when considering features only). They
also demonstrate a marked discrepancy between the
results for our two evaluations, reflecting the neces-
sity of human evaluation when assessing our partic-
ular task.
4 Discussion
In this paper we have shown that semi-supervised
learning techniques can automatically learn lexico-
5The 44 evaluation concepts had been separated into super-
ordinate categories for unrelated psycholinguistic research and
we selected our 15 proportionally and at random from these su-
perordinate categories.
6A correct, valid, feature.
7A triple which is plausible but only in a specific set of cir-
cumstances or a feature which was correct but very general.
8The triple is incorrect but there existed some sort of rela-
tionship between the concept and relation and/or feature.
9When the triple is simply wrong.
syntactic patterns indicative of property norm-like
relations and features. Using these patterns, our
system can extract relevant and accurate properties
from any parsed corpus and allows for multi-word
relation labels, allowing greater semantic precision.
As already mentioned, the work of Baroni et
al. (2009) is relevant to our own. Their approach
achieves a precision score of 0.239 on the top ten
returned features evaluated against the ESSLLI set:
our best system offers precision of 0.370 on the same
evaluation. Moreover, Baroni et al do not explicitly
derive relation terms. We better the performance of
a comparable system (Kelly et al, 2010), even when
evaluating against an unseen set of concepts, and our
system does not use manually-generated rules or se-
mantic information. Furthermore, human evaluation
shows over half of our extracted properties are cor-
rect/plausible.
For future work, we have already mentioned that
we are ignoring a large amount of potentially in-
structive training data, specifically those GR-POS
paths in our corpus which don?t terminate on one of
our training features, as well as those paths through
sentences containing one of our concepts but none
of our training features. It might therefore be worth-
while investigating the use of this ?negative? infor-
mation. Another potential avenue for exploration
would be the expansion of the learning vector-types.
Although we already use a significant number of
learning attributes (an average of 37.9 per training
pattern), we could include more: there may be addi-
tional information not directly on the GR-POS path
linking a concept and feature (e.g., nodes adjacent
to said path) which might be indicative of their re-
lationship. We would also consider using active-
learning, introducing a feedback loop and human-
annotation to better distinguish between relations
which our algorithm tends to classify incorrectly.
For example, we could supplement input pattern
data with disambiguating POS-GR graphs, drawing
a distinction between valid and non-valid relations.
Finally, our system could also be evaluated in the
context of a psycholinguistic experiment. For exam-
ple, we could use our system output to predict con-
cept similarity by using our extracted triples to cre-
ate vector representations of each concept, calculat-
ing the distance between those vectors and compar-
ing these similarity ratings with human judgements.
18
Acknowledgements
This research was supported by EPSRC grant
EP/F030061/1. We are grateful to McRae and col-
leagues for making their norms publicly available,
and to the anonymous reviewers for their helpful in-
put.
References
M. Andrews, G. Vigliocco, and D. Vinson. 2005.
Integrating attributional and distributional informa-
tion in a probabilistic model of meaning representa-
tion. In Timo Honkela et al, editor, Proceedings of
AKRR?05, International and Interdisciplinary Confer-
ence on Adaptive Knowledge Representation and Rea-
soning, pages 15?25, Espoo, Finland: Helsinki Uni-
versity of Technology.
M. Baroni, S. Evert, and A. Lenci, editors. 2008. ESSLLI
2008 Workshop on Distributional Lexical Semantics.
M. Baroni, B. Murphy, Barbu E., and Poesio M. 2009.
Strudel: A corpus-based semantic model based on
properties and types. Cognitive Science, pages 1?33.
S. Bird. 2006. NLTK: The natural language toolkit. In
Proceedings of the COLING/ACL on Interactive pre-
sentation sessions, pages 69?72. Association for Com-
putational Linguistics.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
S. Clark and J.R. Curran. 2007. Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4):493?552.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and psychological measurement.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine learning, 20(3):273?297.
K. Crammer and Y. Singer. 2002. On the algorithmic
implementation of multiclass kernel-based vector ma-
chines. The Journal of Machine Learning Research,
2:265?292.
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by web mining. In Annual Meeting-Association
For Computational Linguistics, volume 45, page 232.
B. Devereux, N. Pilkington, T. Poibeau, and A. Korho-
nen. 2010. Towards unrestricted, large-scale acquisi-
tion of feature-based conceptual representations from
corpus data. Research on Language & Computation,
pages 1?34.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational linguistics,
19(1):61?74.
O. Etzioni, A. Fader, J. Christensen, S. Soderland, and
M.T. Center. 2011. Open information extraction: The
second generation. In Twenty-Second International
Joint Conference on Artificial Intelligence.
M.J. Farah and J.L. McClelland. 1991. A computational
model of semantic memory impairment: Modality
specificity and emergent category specificity. Journal
of Experimental Psychology: General, 120(4):339?
357.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. The MIT press.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini.
2008. Introducing and evaluating ukwac, a very large
web-derived corpus of english. In Proceedings of the
4th Web as Corpus Workshop (WAC-4) Can we beat
Google, pages 47?54.
P. Garrard, M.A.L. Ralph, J.R. Hodges, and K. Patterson.
2001. Prototypicality, distinctiveness, and intercorre-
lation: Analyses of the semantic attributes of living
and nonliving concepts. Cognitive Neuropsychology,
18(2):125?174.
J. Gime?nez and L. Marquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector ma-
chines. In In Proceedings of the 4th International
Conference on Language Resources and Evaluation.
Citeseer.
S. Godbole, S. Sarawagi, and S. Chakrabarti. 2002. Scal-
ing multi-class support vector machines using inter-
class confusion. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 513?518. ACM.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
conference on Computational linguistics-Volume 2,
pages 539?545. Association for Computational Lin-
guistics.
T. Joachims, C. Nedellec, and C. Rouveirol. 1998. Text
categorization with support vector machines: learning
with many relevant. In Machine Learning: ECML-
98 10th European Conference on Machine Learning,
Chemnitz, Germany, pages 137?142. Springer.
T. Joachims. 1999. Svmlight: Support vector machine.
SVM-Light Support Vector Machine http://svmlight.
joachims. org/, University of Dortmund, 19.
C. Kelly, B. Devereux, and A. Korhonen. 2010. Ac-
quiring human-like feature-based conceptual represen-
tations from corpora. In First Workshop on Computa-
tional Neurolinguistics, page 61. Citeseer.
K. McRae, G.S. Cree, M.S. Seidenberg, and C. McNor-
gan. 2005. Semantic feature production norms for
a large set of living and nonliving things. Behav-
ioral Research Methods, Instruments, and Computers,
37:547?559.
19
D. Meyer, F. Leisch, and K. Hornik. 2003. The support
vector machine under test. Neurocomputing, 55(1-
2):169?186.
G. Murphy. 2002. The big book of concepts. The MIT
Press, Cambridge, MA.
P. Pantel and M. Pennacchiotti. 2008. Automatically har-
vesting and ontologizing semantic relations. In Pro-
ceeding of the 2008 conference on Ontology Learning
and Population: Bridging the Gap between Text and
Knowledge, pages 171?195. IOS Press.
B. Randall, H.E. Moss, J.M. Rodd, M. Greer, and L.K.
Tyler. 2004. Distinctiveness and correlation in con-
ceptual structure: Behavioral and computational stud-
ies. Journal of Experimental Psychology Learning
Memory and Cognition, 30(2):393?406.
K.I. Taylor, B.J. Devereux, K. Acres, B. Randall, and
L.K. Tyler. 2011. Contrasting effects of feature-based
statistics on the categorisation and basic-level identifi-
cation of visual objects. Cognition.
L.K. Tyler, H.E. Moss, M.R. Durrant-Peatfield, and J.P.
Levy. 2000. Conceptual structure and the structure of
concepts: A distributed account of category-specific
deficits. Brain and Language, 75(2):195?231.
A. Vinokourov, J. Shawe-Taylor, and N. Cristianini.
2003. Inferring a semantic representation of text via
cross-language correlation analysis. Advances in neu-
ral information processing systems, 15:1473?1480.
F. Wu and D.S. Weld. 2010. Open information extraction
using wikipedia. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 118?127. Association for Computational
Linguistics.
20
