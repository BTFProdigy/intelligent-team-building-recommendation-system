Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 57?64, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Redundancy-based Correction of Automatically Extracted Facts
Roman Yangarber and Lauri Jokipii
Department of Computer Science
University of Helsinki, Finland
first.last@cs.helsinki.fi
Abstract
The accuracy of event extraction is lim-
ited by a number of complicating factors,
with errors compounded at all sages in-
side the Information Extraction pipeline.
In this paper, we present methods for re-
covering automatically from errors com-
mitted in the pipeline processing. Recov-
ery is achieved via post-processing facts
aggregated over a large collection of doc-
uments, and suggesting corrections based
on evidence external to the document. A
further improvement is derived from prop-
agating multiple, locally non-best slot fills
through the pipeline. Evaluation shows
that the global analysis is over 10 times
more likely to suggest valid corrections to
the local-only analysis than it is to suggest
erroneous ones. This yields a substantial
overall gain, with no supervised training.
1 Introduction
Information Extraction (IE) is a technology for find-
ing facts in plain text, and coding them in a logical
representation, such as, e.g., a relational database.
IE is typically viewed and implemented as a se-
quence of stages?a ?pipeline?:
1. Layout, tokenization, lexical analysis
2. Name recognition and classification
3. Shallow (commonly,) syntactic parsing
4. Resolution of co-reference among entities
5. Pattern-based event matching and role mapping
6. Normalization and output generation
While accuracy at the lowest levels can reach high
90?s, as the stages advance, complexity increases
and performance degrades considerably.
The problem of IE as a whole, as well each of
the listed subproblems, has been studied intensively
for well over a decade, in many flavors and varieties.
Key observations about much state-of-the-art IE are:
a. IE is typically performed by a pipeline process;
b. Only one hypothesis is propagated through the
pipeline for each fact?the ?best guess? the
system can make for each slot fill;
c. IE is performed in a document-by-document
fashion, applying a priori knowledge locally to
each document.
The a priori knowledge may be encoded in a set of
rules, an automatically trained model, or a hybrid
thereof. Information extracted from documents?
which may be termed a posteriori knowledge?
is usually not reused across document boundaries,
because the extracted facts are imprecise, and are
therefore not a reliable basis for future extraction.
Furthermore, locally non-best slot fills are not
propagated through the pipeline, and are conse-
quently not available downstream, nor for any global
analysis.
In most systems, these stages are performed in se-
quence. The locally-best slot fills are passed from
57
the ?lower-? to the ?higher-level? modules, with-
out feedback. Improvements are usually sought
(e.g., the ACE research programme, (ACE, 2004))
by boosting performance at the lower levels, to reap
benefits in the subsequent stages, where fewer errors
are propagated.
The point of departure for this paper is: the
IE process is noisy and imprecise at the single-
document level; this has been the case for some time,
and though there is much active research in the area,
the situation is not likely to change radically in the
immediate future?rather, we can expect slow, in-
cremental improvements over some years.
In our experiments, we approach the performance
problem from the opposite end: start with the ex-
tracted results and see if the totality of a posteri-
ori knowledge about the domain?knowledge gen-
erated by the same noisy process we are trying to
improve?can help recover from errors that stem
from locally insufficient a priori knowledge.
The aim of the research presented in this paper
is to improve performance by aggregating related
facts, which were extracted from a large document
collection, and to examine to what extent the cor-
rectly extracted facts can help correct those that were
extracted erroneously.
The rest of the paper is organized as follows. Sec-
tion 2 contains a brief review of relevant prior work.
Section 3 presents the experimental setup: the text
corpus, the IE process, the extracted facts, and what
aspects of the the extracted facts we try to improve
in this paper. Section 4 presents the methods for im-
proving the quality of the data using global analysis,
starting with a naive, baseline method, and proceed-
ing with several extensions. Each method is then
evaluated, and the results are examined in section 5.
In section 6, we present further extensions currently
under research, followed by the conclusion.
2 Prior Work
As we stated in the introduction, typical IE sys-
tems consist of modules arranged in a cascade, or
a pipeline. The modules themselves are be based
on heuristic rules or automatically trained, there is
an abundance of approaches in both camps (and ev-
erywhere in between,) to each of the pipeline stages
listed in the introduction.
It is our view that to improve performance we
ought to depart from the traditional linear, pipeline-
style design. This view is shared by others in the
research community; the potential benefits have pre-
viously been recognized in several contexts.
In (Nahm and Mooney, 2000a; Nahm and
Mooney, 2000b), it was shown that learning rules
from a fact base, extracted from a corpus of job post-
ings for computer programmers, improves future ex-
traction, even though the originally extracted facts
themselves are far from error-free. The idea is to
mine the data base for association rules, and then to
integrate these rules into the extraction process.
The baseline system is obtained by supervised
learning from a few hundred manually annotated ex-
amples. Then the IE system is applied to succes-
sively larger sets of unlabeled examples, and associ-
ation rules are mined from the extracted facts. The
resulting combined system (trained model plus as-
sociation rules) showed an improvement in perfor-
mance on a test set, which correlated with the size
of the unlabeled corpus.
In work on improving (Chinese) named entity tag-
ging, (Ji and Grishman, 2004; Ji and Grishman,
2005), show benefits to this component from in-
tegrating decisions made in later stages, viz. co-
reference, and relation extraction.1
Tighter coupling and integration between IE and
KDD components for mutual benefit is advocated by
(McCallum and Jensen, 2003), which present mod-
els based on CRFs and supervised training.
This work is related in spirit to the work pre-
sented in this paper, in its focus on leveraging cross-
document information that information?though it
is inherently noisy?to improve local decisions. We
expect that the approach could be quite powerful
when these ideas are used in combination, and our
experiments seem to confirm this expectation.
3 Experimental Setup
In this section we describe the text corpus, the un-
derlying IE process, the form of the extracted facts,
and the specific problem under study?i.e., which
aspects of these facts we first try to improve.
1Performance on English named entity tasks reaches mid to
high 90?s in many domains.
58
3.1 Corpus
We conducted experiments with redundancy-based
auto-correction over a large database of facts ex-
tracted from the texts in ProMED-Mail, a mailing
list which carries reports about outbreaks of infec-
tious epidemics around the world and the efforts
to contain them. This domain has been explored
earlier; see, e.g., (Grishman et al, 2003) for an
overview.
Our underlying IE system is described in (Yan-
garber et al, 2005). The system is a hybrid
automatically- and manually-built pattern base for
finding facts, an HMM-based name tagger, auto-
matically compiled and manually verified domain-
specific ontology, based in part on MeSH, (MeS,
2004), and a rule-based co-reference module, that
uses the ontology.
The database is live on-line, and is continuously
updated with new incoming reports; it can be ac-
cessed at doremi.cs.helsinki.fi/plus/.
Text reports have been collected by ProMED-
Mail for over 10 years. The quality of reporting (and
editing) has been rising over time, which is easy to
observe in the text data. The distribution of the data,
aggregated by month is shown in Figure 1, where
one can see a steady increase in volume over time.2
3.2 Extracted Facts
We now describe the makeup of the data extracted
from text by the IE process, with basic terminology.
Each document in the corpus, contains a single re-
port, which may contain one or more stories. Story
breaks are indicated by layout features, and are ex-
tracted by heuristic rules, tuned for this domain and
corpus. When processing a multi-story report, the
IE system treats each story as a separate document;
no information is shared among stories, except that
the text of the main headline of a multi-story report
is available to each story. 3
Since outbreaks may be described in complex
ways, it is not obvious how to represent a single fact
in this context. To break down this problem, we use
the notion of an incident. Each story may contain
2This is beneficial to the IE process, which operates better
with formulaic, well-edited text.
3The format of the documents in the archive can be exam-
ined by browsing the source site www.promedmail.org.
 0
 200
 400
 600
 800
 1000
1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005
 
Co
un
t 
 Date 
 
  Records   (46,317)
  Stories   (30,015)
  Documents (22,560)
Figure 1: Distribution of data in ProMED-Mail
multiple outbreak-related incidents/facts.4
We analyze an outbreak as a series of incidents.
The incidents may give ?redundant? information
about an outbreak, e.g., by covering overlapping
time intervals or geographic areas. For example, a
report may first state the number of cases within the
last month, and then give the total for the entire year.
We treat each of these statements as a separate inci-
dent; the containment relations among them are be-
yond the scope of our current goals.5
Thus each incident corresponds to a partial de-
scription of an outbreak, over a period of time and
geographic area. This makes it easy to represent
each incident/fact as a separate row in the table.
The key fields of the incident table are:
  Disease Name
  Location
  Date (start and end)
Where possible, the system also extracts informa-
tion about the victims affected in the incident?their
count, severity (affected or dead), and a descriptor
(people, animals, etc.). The system also extracts
bookkeeping information about each incident: loca-
tions of mentions of the key fields in the text, etc.
The system?s performance is currently at 71.16 F-
measure: 67% recall, 74% precision. This score is
obtained by a MUC scorer (Douthat, 1998) on a 50-
document test corpus, which was manually tagged
with correct incidents with these slots. We have
4In this paper, we use the terms fact, incident, and event
interchangeably.
5This problem is addressed in, e.g., (Huttunen et al, 2002).
59
no blind-test corpus at present, but prior experience
suggests that we ought to expect about a 10% reduc-
tion in F-measure on unseen data; this is approxi-
mately borne out by our informal evaluations.
Further, the system attempts to ?normalize? the
key fields. An alias for a disease name (e.g., ?bird
flu?) is mapped to a canonical name (?avian in-
fluenza.?)6 Date expressions are normalized to a
standard format yyyy.mm.dd?yyyy.mm.dd.7
Note that the system may not be able to normalize
some entities, which then remain un-normalized.
Such normalization is clearly helpful for search-
ing, but it is not only a user-interface issue. Normal-
izing reduces sparseness of data; and since our intent
is to aggregate related facts across a large fact base,
excessive variation in the database fields would re-
duce the effectiveness of the proposed methods.
3.3 Experimental Focus: Location
Normalization
A more complex problem arises out of the need to
normalize location names. For each record, we nor-
malize the location field?which may be a name of
a small village or a larger area?by relating it to the
name of the containing country; we also decided to
map locations in the United States to the name of the
containing state, (rather than the name of the coun-
try, ?USA?).8 This mapping will be henceforth re-
ferred to as ?location?state,? for short. The ideas
presented in the introduction are explored in the re-
mainder of this paper in the context of correcting the
location?state mapping.
Section 6 will touch upon our current work on ex-
tending the methodology to slots other than state.
(Please see Section 5 for further justification of this
choice for our initial experiments.)
To make the experiments interesting and fair, we
kept the size of the gazetteer small. The a priori geo-
graphic knowledge base contains names of countries
of the world (270), with aliases for several of them; a
list of capitals and other selected major cities (300);
a list of states in the USA and acronyms (50); major
6This is done by means of a set of scenario-specific patterns
and a dictionary of about 2500 disease names with aliases.
7Some date intervals may not have a starting date, e.g., if the
text states ?As of last Tuesday, the victim count is N...?
8This decision was made because otherwise records with
state = USA strongly skew the data, and complicate learning.
US cities (100); names of the (sub)continents (10),
and oceans. In our current implementation, conti-
nents are treated semantically as ?states? as well.9
The IE system operates in a local, document-by-
document fashion. Upon encountering a location
name that is not in its dictionaries, the system has
two ways to map it to the state name. One way is
by matching patterns over the immediate local con-
text, (?Milan, Italy?). Failing that, it tries to find
the corresponding state by positing an ?underspeci-
fied? state name (as if referred to by a kind of spe-
cial ?pronoun?) and mapping the location name to
that. The reference resolution module then finds the
most likely antecedent entity, of the semantic type
?state/country,? where likelihood is determined by
its proximity to the mention of the location name.
Note that the IE system outputs only a single, best
hypothesis for the state fill for each record.
3.4 The Data
The database currently contains about  
	 in-
dividual facts/incidents, extracted from  sto-
ries, from 

reports (cf. Fig. 1). Each incident
has a location and a state filler. We say a location
name is ?ambiguous? if it appears in the location slot
of at least two records, which have different names
in the state slot. The number of distinct ?ambigu-
ous? location names is



.
Note, this terminology is a bit sloppy: the fillers
to which we refer as ?ambiguous location names?,
may not be valid location names at all; they may
simply be errors in the IE process. E.g., at the name
classification stage, a disease name (especially if not
in the disease dictionary) may be misclassified, and
used as a filler for the location slot.
We further group together the location fills by
stripping lower-case words that are not part of the
proper name, from the front and the end of the fill.
E.g., we group together ?southern Mumbai? and
?the Mumbai area,? as referring to the same name.
After grouping and trimming insignificant words,
the number of distinct names appearing in location
fills is

, which covers a total of

records,
or
 
 of all extracted facts. As an estimate of
the potential for erroneous mapping from locations
to states, this is quite high, about  in 	 records.10
9By the same token, both Connecticut and USA are ?states.?
10Of course, it can be higher as well, if the IE system con-
60
4 Experiments and Results
We now present the methods of correcting possible
errors in the location?state relation. A method  
tries to suggest a new value for the state fill for every
incident I that contains an ambiguous location fill:
	
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 22?23,
Vancouver, October 2005.
Extracting Information about Outbreaks of Infectious Epidemics
Roman Yangarber Lauri Jokipii
Department of Computer Science
University of Helsinki, Finland
first.last@cs.helsinki.fi
Antti Rauramo
Index, Oy
Helsinki, Finland
Silja Huttunen
Department of Linguistics
University of Helsinki, Finland
Abstract
This work demonstrates the ProMED-
PLUS Epidemiological Fact Base. The
facts are automatically extracted from
plain-text reports about outbreaks of in-
fectious epidemics around the world. The
system collects new reports, extracts new
facts, and updates the database, in real
time. The extracted database is available
on-line through a Web server.
1 Introduction
Information Extraction (IE) is a technology for find-
ing facts in plain text, and coding them in a logical
representation, such as a relational database.
Much published work on IE reports on ?closed?
experiments; systems are built and evaluated based
on carefully annotated corpora, at most a few hun-
dred documents.1 The goal of the work presented
here is to explore the IE process in the large: the
system integrates a number of off-line and on-line
components around the core IE engine, and serves
as a base for research on a wide range of problems.
The system is applied to a large dynamic collec-
tion of documents in the epidemiological domain,
containing tens of thousands of documents. The
topic is outbreaks of infectious epidemics, affecting
humans, animals and plants. To our knowledge, this
is the first large-scale IE database in the epidemio-
logical domain publicly accessible on-line.2
1Cf., e.g., the MUC and ACE IE evaluation programmes.
2On-line IE databases do exist, e.g., CiteSeer, but none that
extract multi-argument events from plain natural-language text.
2 System Description
The architecture of the ProMED-PLUS system3 is
shown in Fig. 1. The core IE Engine (center) is im-
plemented as a sequence, or ?pipeline,? of stages:
  Layout analysis, tokenisation, lexical analysis;
  Name recognition and classification;
  Shallow syntactic analysis;
  Resolution of co-reference among entities;
  Pattern-based event matching and role mapping;
  Normalisation and output generation
The database (DB) contains facts extracted from
ProMED-Mail, a mailing list about epidemic out-
breaks.4
The IE engine is based in part on earlier work,
(Grishman et al, 2003). Novel components use ma-
chine learning at several stages to enhance the per-
formance of the system and the quality of the ex-
tracted data: acquisition of domain knowledge for
populating the knowledge bases (left side in Fig. 1),
and automatic post-validation of extracted facts for
detecting and reducing errors (upper right). Novel
features include the notion of confidence,5 and ag-
gregation of separate facts into outbreaks across
multiple reports, based on confidence.
Operating in the large is essential, because the
learning components in the system rely on the
availability of large amounts of data. Knowledge
3PLUS: Pattern-based Learning and Understanding System.
4ProMED, www.promedmail.org, is the Program for Mon-
itoring Emerging Diseases, of the International Society for In-
fectious Diseases. It is one of the most comprehensive sources
of reports about the spread of infectious epidemics around the
world, collected for over 10 years.
5Confidence for individual fields of extracted facts, and for
entire facts, is based on document-local and global information.
22
IE engine
Customization 
environment
Lexicon
Ontology
Patterns
Inference rules
Unsupervised 
learning
Extracted facts
Candidate knowledge
DB server
User query Response
publisher user
Data collection
Web server
customizer
Noise reduction/
Data correction/ 
Cross-validation
Other corpora
Text documents
Knowledge bases:
Figure 1: System architecture of ProMED-PLUS
acquisition, (Yangarber et al, 2002; Yangarber,
2003) requires a large corpus of domain-specific and
general-topic texts. On the other hand, automatic
error reduction requires a critical mass of extracted
facts. Tighter integration between IE and KDD com-
ponents, for mutual benefit, is advocated in recent
related research, e.g., (Nahm and Mooney, 2000;
McCallum and Jensen, 2003). In this system we
have demonstrated that redundancy in the extracted
data (despite the noise) can be leveraged to improve
quality, by analyzing global trends and correcting
erroneous fills which are due to local mis-analysis,
(Yangarber and Jokipii, 2005). For this kind of ap-
proach to work, it is necessary to aggregate over a
large body of extracted records.
The interface to the DB is accessible on-line
at doremi.cs.helsinki.fi/plus/ (lower-right
of Fig. 1). It allows the user to view, select and sort
the extracted outbreaks, as well as the individual in-
cidents that make up the aggregated outbreaks. All
facts in the database are linked back to the original
reports from which they were extracted. The dis-
tribution of the outbreaks may also be plotted and
queried through the Geographic Map view.
References
R. Grishman, S. Huttunen, and R. Yangarber. 2003. In-
formation extraction for enhanced access to disease
outbreak reports. J. of Biomed. Informatics, 35(4).
A. McCallum and D. Jensen. 2003. A note on the uni-
fication of information extraction and data mining us-
ing conditional-probability, relational models. In IJ-
CAI?03 Workshop on Learning Statistical Models from
Relational Data.
U. Y. Nahm and R. Mooney. 2000. A mutually beneficial
integration of data mining and information extraction.
In AAAI-2000, Austin, TX.
R. Yangarber and L. Jokipii. 2005. Redundancy-based
correction of automatically extracted facts. In Proc.
HLT-EMNLP 2005, Vancouver, Canada.
R. Yangarber, W. Lin, and R. Grishman. 2002. Un-
supervised learning of generalized names. In Proc.
COLING-2002, Taipei, Taiwan.
R. Yangarber. 2003. Counter-training in discovery of se-
mantic patterns. In Proc. ACL-2003, Sapporo, Japan.
23
Unsupervised Discovery of Scenario-Level Patterns for 
Information Extraction 
Roman Yangarber  Ra lph  Gr i shman 
roman?as, nyu. edu grishman?cs, nyu. edu 
Courant Inst i tute of Courant Inst i tute of 
Mathematical  Sciences Mathematical  Sciences 
New York University New York University 
Pas i  Tapana inen  )~ Si l ja  Hut tunen $ 
tapanain?conexor, fi sihuttun~ling.helsinki, fi 
t Conexor Oy :~ University of Helsinki 
Helsinki, F inland F in land 
Abst rac t  
Information Extraction (IE) systems are com- 
monly based on pattern matching. Adapting 
an IE system to a new scenario entails the 
construction of a new pattern base---a time- 
consuming and expensive process. We have 
implemented a system for finding patterns au- 
tomatically from un-annotated text. Starting 
with a small initial set of seed patterns proposed 
by the user, the system applies an incremental 
discovery procedure to identify new patterns. 
We present experiments with evaluations which 
show that the resulting patterns exhibit high 
precision and recall. 
0 I n t roduct ion  
The task of Information Extraction (I-E) is 
the selective extraction of meaning from free 
natural language text. I "Meaning" is under- 
stood here in terms of a fixed set of semantic 
objects--entities, relationships among entities, 
and events in which entities participate. The 
semantic objects belong to a small number of 
types, all having fixed regular structure, within 
a fixed and closely circumscribed subject do- 
main. The extracted objects are then stored in 
a relational database. In this paper, we use the 
nomenclature accepted in current IE literature; 
the term subject domain denotes a class of tex- 
tual documents to be processed, e.g., "business 
news," and scenario denotes the specific topic 
of interest within the domain, i.e., the set of 
facts to be extracted. One example of a sce- 
nario is "management succession," the topic of 
MUC-6 (the Sixth Message Understanding Con- 
ference); in this scenario the system seeks to 
identify events in which corporate managers left 
1For general references on IE, cf., e.g., (Pazienza, 
1997; muc, 1995; muc, 1993). 
their posts or assumed new ones. We will con- 
sider this scenario in detail in a later section 
describing experiments. 
IE systems today are commonly based on pat- 
tern matching. The patterns are regular ex- 
pressions, stored in a "pattern base" containing 
a general-purpose component and a substantial 
domain- and scenario-specific component. 
Portability and performance are two major 
problem areas which are recognized as imped- 
ing widespread use of IE. This paper presents a
novel approach, which addresses both of these 
problems by automatically discovering good 
patterns for a new scenario. The viability of 
our approach is tested and evaluated with an 
actual IE system. 
In the next section we describe the problem in 
more detail in the context of our IE system; sec- 
tions 2 and 3 describe our algorithm for pattern 
discovery; section 4 describes our experimental 
results, followed by comparison with prior work 
and discussion, in section 5. 
1 The  IE  Sys tem 
Our IE system, among others, contains a a back- 
end core engine, at the heart of which is a 
regular-e~xpression pattern matcher. The engine 
draws on attendant knowledge bases (KBs) of 
varying degrees of domain-specificity. The KB 
components are commonly factored out to make 
the systems portable to new scenarios. There 
are four customizable knowledge bases in our IE 
system: the Lexicon contains general dictionar- 
ies and scenario-specific terms; the concept base 
groups terms into classes; the predicate base de- 
scribes the logical structure of events to be ex- 
tracted, and the pattern base contains patterns 
that catch the events in text. 
Each KB has a. substantial domain-specific 
component, which must be modified when mov-  
282 
ing to new domains and scenarios. The system 
allows the user (i.e. scenario developer) to start 
with example sentences in text which contain 
events of interest, the candidates, and general- 
ize them into patterns. However, the user is 
ultimately responsible for finding all the can- 
didates, which amounts to manually processing 
example sentences in a very large training cor- 
pus. Should s/he fail to provide an example 
of a particular class of syntactic/semantic con- 
struction, the system has no hope of recovering 
the corresponding events. Our experience has 
shown that (1) the process of discovering candi- 
dates is highly expensive, and (2) gaps in pat- 
terns directly translate into gaps in coverage. 
How can the system help automate the pro- 
cess of discovering new good candidates? The 
system should find examples of all common lin- 
guistic constructs relevant o a scenario. While 
there has been prior research on identifying the 
primary lexical patterns of a sub-language or 
corpus (Grishman et al, 1986; Riloff, 1996), the 
task here is more complex, since we are typi- 
cally not provided in advance with a sub-corpus 
of relevant passages; these passages must them- 
selves be found as part of the discovery process. 
The difficulty is that one of the best indications 
of the relevance of the passages i precisely the 
presence of these constructs. Because of this 
circularity, we propose to acquire the constructs 
and passages in tandem. 
2 So lu t ion  
We outline our procedure for automatic ac- 
quisition of patterns; details are elaborated in 
later sections. The procedure is unsupervised 
in that it does not require the training corpus 
to be manually annotated with events of inter- 
est, nor a pro-classified corpus with relevance 
judgements, nor any feedback or intervention 
from the user 2. The idea is to combine IR-style 
document selection with an iterative relaxation 
process; this is similar to techniques used else- 
where in NLP, and is inspired in large part, if 
remotely, by the work of (Kay and RSscheisen, 
1993) on automatic alignment of sentences and 
words in a bilingual corpus. There, the reason- 
ing was: sentences that are translations of each 
2however, it may be supervised after each iteration, 
where the user can answer yes/no questions to improve 
the quality of the results 
other are good indicators that words they con- 
tain are translation pairs; conversely, words that 
are translation pairs indicate that the sentences 
which contain them correspond to one another. 
In our context, we observe that documents 
that are relevant to the scenario will neces- 
sarily contain good patterns; conversely, good 
patterns are strong indicators of relevant docu- 
ments. The outline of our approach is as follows. 
. 
. 
Given: (1) a large corpus of un-annotated 
and un-classified documents in the domain; 
(2) an initial set of trusted scenario pat- 
terns, as chosen ad hoc by the user--the 
seed; as will be seen, the seed can be quite 
small--two or three patterns eem to suf- 
fice. (3) an initial (possibly empty) set of 
concept classes 
The pattern set induces a binary partition 
(a split) on the corpus: on any document, 
either zero or more than zero patterns will 
match. Thus the universe of documents, U, 
is partitioned into the relevant sub-corpus, 
R, vs. the non-relevant sub-corpus, R = 
U - R, with respect o the given pattern 
set. Actually, the documents are assigned 
weights which are 1 for documents matched 
by the trusted seed, and 0 otherwise. 3 
2. Search for new candidate patterns: 
(a) Automatically convert each sentence 
in the corpus,into a set of candidate 
patterns, 4 
(b) Generalize each pattern by replacing 
each lexical item which is a member of 
a concept class by the class name. 
(c) Working from the relevant documents, 
select those patterns whose distribu- 
tion is strongly correlated with other 
relevant documents (i.e., much more 
3R represents he trusted truth through the discovery 
iterations, since it was induced by the manually-selected 
seed. 
4Here, for each clause in the sentence we extract a 
tuple of its major roles: the head of the subject, the 
verb group, the object, object complement, asdescribed 
below. This tuple is considered to be a pattern for the 
present purposes of discovery; it is a skeleton for the 
rich, syntactically transformed patterns our system uses 
in the extraction phase. 
283 
densely distributed among the rele- 
vant documents than among the non- 
relevant ones). The idea is to consider 
those candidate patterns, p, which 
meet the density, criterion: 
IHnRI IRI - - > >  
IHnUI IUI 
where H = H(p) is the set of docu- 
ments where p hits. 
(d) Based on co-occurrence with the cho- 
sen patterns, extend the concept 
classes. 
3. Optional: Present he new candidates and 
classes to the user for review, retaining 
those relevant o the scenario. 
4. The new pattern set induces a new parti- 
tion on the corpus. With this pattern set, 
return to step 1. Repeat he procedure un- 
til no more patterns can be added. 
3 Methodo logy  
3.1 Pre-proeess ing:  Normal i za t ion  
Before applying the discovery procedure, we 
subject the corpus to several stages o f  pre- 
processing. First, we apply a name recognition 
module, and replace each name with a token 
describing its class, e.g. C-Person, C-Company, 
etc. We collapse together all numeric expres- 
sions, currency values, dates, etc., using a single 
token to designate ach of these classes. 
3.2 Syntact ic  Analys is  
We then apply a parser to perform syntactic 
normalization to transform each clause into a 
common predicate-argument structure. We use 
the general-purpose d pendency parser of En- 
glish, based on the FDG formalism (Tapanainen 
and J~rvinen, 1997) and developed by the Re- 
search Unit for Multilingual Language Technol- 
ogy at the University of Helsinki, and Conexor 
Oy. The parser (modified to understand the 
name labels attached in the previous step) is 
used for reducing such variants as passive and 
relative clauses to a tuple, consisting of several 
elements. 
1. For each claus, the first element is the sub- 
ject, a "semantic" subject of a non-finite 
sentence or agent of the passive. 5 
2. The second element is the verb. 
3. The third element is the object, certain 
object-like adverbs, subject of the passive 
or subject complement 6 
4. The fourth element is a phrase which 
refers to the object or the subject. A 
typical example of such an argument is 
an object complement, such as Com- 
pany named John Smith pres ident .  An- 
other instance is the so-called copredica- 
tire (Nichols, 1978), in the parsing system 
(J~irvinen and Tapanainen, 1997). A co- 
predicative refers to a subject or an object, 
though this distinction is typically difficult 
to resolve automatically/ 
Clausal tuples also contain a locative modifier, 
and a temporal modifier. We used a corpus of 
5,963 articles from the Wall Street Journal, ran- 
domly chosen. The parsed articles yielded a to- 
tal of 250,000 clausal tuples, of which 135,000 
were distinct. 
3.3 Genera l i za t ion  and  Concept  Classes 
Because tuples may not repeat with sufficient 
frequency to obtain reliable statistics, each tu- 
ple is reduced to a set of pairs: e.g., a verb- 
object pair, a subject-object pair, etc. Each 
pair is used as a generalized pattern during 
the candidate selection stage. Once we have 
identified pairs which are relevant o the sce- 
nario, we use them to construct or augment con- 
cept classes, by grouping together the missing 
roles, (for example, a class of verbs which oc- 
cur with a relevant subject-object pair: "com- 
pany (hire/fire/expel...} person"). This is sim- 
ilar to work by several other groups which 
aims to induce semantic lasses through syn- 
tactic co-occurrence analysis (Riloff and Jones, 
1999; Pereira et al, 1993; Dagan et al, 1993; 
Hirschman et al, 1975), although in .our case 
the contexts are limited to selected patterns, 
relevant o the scenario. 
SE.g., " John sleeps", "John is appointed by 
Company" ,  "I saw a dog which sleeps", "She asked 
John  to buy a car". 
6E.g., " John is appointed by Company", "John is the 
pres ident  of Company", "I saw a dog which sleeps", 
The dog  which I saw sleeps. 
7For example, "She gave us our coffee black",  "Com- 
pany appointed John Smith as pres ident" .  
284 
3.4 Pattern Discovery 
Here we present he results from experiments 
we conducted on the MUC-6 scenario, "man- 
agement succession". The discovery procedure 
was seeded with a small pattern set, namely: 
Subject Verb Direct Object 
C-Company C-Appoint C-Person 
C-Person C-Resign 
Documents are assigned relevance scores on 
a scale between 0 and 1. The seed patterns 
are accepted as ground truth; thus the docu- 
ments they match have relevance 1. On sub- 
sequent iterations, the newly accepted patterns 
are not trusted as absolutely. On iteration um- 
ber i q- 1, each pattern p is assigned a precision 
measure, based on the relevance of the docu- 
ments it matches: 
Here C-Company and C-Person denote se- 
mantic classes containing named entities of the 
corresponding semantic types. C-Appoirlt de- 
notes a class of verbs, containing four verbs 
{ appoint, elect, promote, name}; C-Resign = 
{ resign, depart, quit, step-down }. 
During a single iteration, we compute the 
score s, L(p), for each candidate pattern p: 
L(p) = Pc(P)" log {H A R\] (1) 
where R denotes the relevant subset, and H -- 
H(p) the documents matching p, as above, and 
\[gnR\[ Pc(P) -- Igl is the conditional probability of 
relevance. We further impose two support cri- 
teria: we distrust such frequent patterns where 
\[HA U{ > a\[U\[ as uninformative, and rare pat- 
terns for which \[H A R\[ </3  as noise. ? At the 
end of each iteration, the system selects the pat- 
tern with the highest score, L(p), and adds it to 
the seed set. The documents which the winning 
pattern hits are added to the relevant set. The 
pattern search is then restarted. 
3.5 Re-computat lon  of  Document  
Relevance 
The above is a simplification of the actual pro- 
cedure, in several important respects. 
Only generalized patterns are considered for 
candidacy, with one or more slots filled with 
wild-cards. In computing the score of the gen- 
eralized pattern, we do not take into considera- 
tion all possible values of the wild-card role. We 
instead constrain the wild-card to those values 
which themselves in turn produce patterns with 
high scores. These values then become members 
of a new class, which is output in tandem with 
the winning pattern 1? 
Ssimilarly to (Riloff, 1996) 
?U denotes the universe of documents. We used c~ = 
0.i and ~----- 2. 
1?The classes are currently unused by subsequent i er- 
ations; this important issue is considered in future work. 
Preci+l(p) = 1 {H(p){ ~ Reli(d) (2) 
dEH(p) 
where Reli(d) is the relevance of the document 
from the previous iteration, and H(p) is the set 
of documents where p matched. More generally, 
if K is a classifier consisting of a set of patterns, 
we can define H(K) as the set of documents 
where all of patterns p E K match, and the 
"cumulative" precision 11 of K as 
Preci+l(K) = 1 ~ Reli(d) (3) 
IH(K)\[ riCH(K) 
Once the new winning pattern is accepted, 
the relevance scores of the documents are re- 
adjusted as follows. For each document d which 
is matched by some (non-empty) subset of the 
currently accepted patterns, we can view that 
subset of patterns as a classifier K d = {py}. 
These patterns determine the new relevance 
score of the document 
Reli+l(d) = max (Rel~(d),Prec~+l(Kd)) (4) 
This ensures that the relevance score grows 
monotonically, and only when there is sufficient 
positive evidence, as the patterns in effect vote 
"conjunctively" on the documents. The results 
which follow use this measure. 
Thus in the formulas above, R is not sim- 
ply the count of the relevant documents, but 
is rather their cumulative relevance. The two 
formulas, (3) and (4), capture the mutual de- 
pendency of patterns and documents; this re- 
computation and growing of precision and rele- 
vance scores is at the heart of the procedure. 
11Of course, this measure is defined only when 
H(K) # 0. 
285 
4 Resu l ts  1 
An objective measure of goodness of a pattern o. 9 
is not trivial to establish since the patterns can- 
not be used for extraction directly, without be- o. s 
ing properly incorporated into the knowledge 
base. Thus, the discovery procedure does not o. v 
lend itself easily to MUC-style evaluations, ince 
0.6  a pattern lacks information about which events 
it induces and which slots its arguments should 0.5  
fill. 
However, it is possible to apply some objec- o. a 
tive measures of performance. One way we eval- 
uated the system is by noting that in addition o.  
to growing the pattern set, the procedure also 
grows the relevance of documents. The latter o. 2 
can be objectively evaluated. 
0.1  
We used a test corpus of 100 MUC-6 formal- 
training documents (which were included in the o 
main development corpus of about 6000 docu- 
ments) plus another 150 documents picked at 
random from the main corpus and judged by 
hand. These judgements constituted the ground 
truth and were used only for evaluation, (not in 
the discovery procedure). 
4.1 Text  F i l ter ing 
Figure 1 shows the recall/precision measures 
with respect to the test corpus of 250 docu- 
ments, over a span of 60 generations, tarting 
with the seed set in table 3.4. The Seed pat- 
terns matched 184 of the 5963 documents, yield- 
ing an initial recall of .11 and precision of .93; 
by the last generation it searched through 982 
documents with non-zero relevance, and ended 
with .80 precision and .78 recall. This facet of 
the discovery procedure is closely related to the 
MUC '%ext-filtering" sub-task, where the sys- 
tems are judged at the level of documents rather 
than event slots. It is interesting to compare the 
results with other MUC-6 participants, shown 
anonymously in figure 2. Considering recall and 
precision separately, the discovery procedure at- 
tains values comparable to those achieved by 
some of the participants, all of which were ei- 
ther heavily-supervised or manually coded sys- 
tems. It is important o bear in mind that the 
discovery procedure had no benefit of training 
material, or any information beyond the seed 
pattern set. 
I I I I I I 
......i"{+X'N+~v i P ~ e c i s i o n  ' i 
" "  ... .  i . .~ .  i Re~aa - - -?- - -  
. . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . .  ~ . . . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . .  
...... iiiiiiiiiiiiiilEi   ........... ........ 
/... 
. . . . . . . . . .  ~ . . . . . . . . . .  '." . . . . . . . .  ": . . . . . . . . . .  ~ . . . . . . . . . . .  r . . . . . . . . . .  ~ .. . . . . . . . .  ! . . . . . . . . .  
2111111ji.. iii121122;1211111;ii122221ilSiiii12112121SiiiiSiii: . . . . . .
0 I0  20  30  40  50  60  70  
G e n e r a t i o n  # 
80 
Figure h Recall/Precision curves for Manage- 
ment Succession 
4.2 Cho ice  of  Test  Corpus  
Figure 2 shows two evaluations of our discovery 
procedure, tested against the original MUC-6 
corpus of 100 documents, and against our test 
corpus, which consists of an additional 150 doc- 
uments judged manually. The two plots in the 
figure show a slight difference in results, indi- 
cating that in some sense, the MUC corpus was 
more "random", or that our expanded corpus 
was somewhat skewed in favor of more common 
patterns that the system is able to find more 
easily. 
4.3 Cho ice  of  Eva luat ion  Met r i c  
The graphs shown in Figures 1 and 2 are based 
on an "objective" measure we adopted during 
the experiments. This is the same measure of 
relevance used internally by the discovery proce- 
dure on each iteration (relative to the "truth" of 
relevance scores of the previous iteration), and 
is not quite the standard measure used for text 
filtering in IR. According to this measure, the 
system gets a score for each document based on 
the relevance which it assigned to the document. 
Thus if the system .assigned relevance of X per- 
cent to a relevant document, it only received X 
286 
0 
( I )  
0 . 9  
0 . 8  
0 . 7  
I I  I I I I I I : : . , . : ? 
. . . . . . .  . . . . . . . .  . . . . . . . . . . . . . . .  - - -  ! . . . . . . . .  . . . . . . .  
i i i :: i i i i i ~ 
. . . . . .  i . . . . . . . .  i . . . . . . . .  \[ . . . . . . .  ? .. . . . . . .  f . . . . . . . . . . . .  T . . . . . . .  
. . . . . .  J . . . . . . . .  i . . . . . . . .  i .  . . . . . . .  i . . . . . . . .  ~ . . . . . . .  .; . . . . . . . . . . .  ..: . . . . . . . .  i . . . . :  
0 6 . . . . . . . . . . . . . . . . . . . . . . . .  .'7 . . . . . . .  ' . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  ~ . . . . . . . . . . . . . . . . . . . . . . .  
i 
i 
i 
0 . 5 . . . . . . .  '. . . . . . . .  , . . . . . . . .  ~" . . . . . . .  , . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  * . . . . . . .  "=. . . . . . . .  , . . . . . . .  
i z 
0 . 4  i I 
0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9  1 
R e c a l l  
Figure 2: Precision vs. Recall 
I 
i . i ~ iB  i 
. . . . . .  e . . . . . . . .  i . . . . . . . .  ! . . . . . . . .  ~ . . . . . . . .  ~ . . .  ' . . . . . . . .  ! . . . . . . . .  i . . . . . . .  0 . 9  
i i i i i im~ ! ! 
! i i D iE  c 
! i i i i i i i i 
0 . '7  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~" . . . . . . . . . . . . . . .  
C o n ~ i n ~ o u -  ? ~ut i - -o  f 
0 . 6 . . . . . .  ~ . . . . . . . .  ~ . . . . . . . .  ~ . . . . . . .  ' . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  ; . . . . . . .  ~ . . . . . . . .  ~ . . . . . . .  
0 . 5 ...... ~ ........ , ........ ~ ....... ~ ........ ~ ....... ~ ........ ! ........ : ........ 4 ....... 
0 . 4  
0 0 . i 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9  1 
R e c a l l  
Figure 3: Results on the MUC corpus 
percent on the recall score for classifying that 
document correctly. Similarly, if the system as- 
signed relevance Y to an irrelevant document, 
it was penalized only for the mis-classified Y
percent on the precision score. To make our re- 
sults more comparable to those of other MUC 
competitors, we chose a cut-off point and force 
the system to make a binary relevance decision 
on each document. The cut-off of 0.5 seemed 
optimal from empirical observations. Figure 3 
shows a noticeable improvement in scores, when 
using our continuous, "objective" measure, vs. 
the cut-off measure, with the entire graph essen- 
tially translated to the right for a gain of almost 
10 percentage points of recall. 
4.4 Eva luat ing  Pat terns  
Another effective, if simple, measure of perfor- 
mance is  how many of the patterns the pro- 
cedure found, and comparing them with those 
used by an extraction engine which was manu- 
ally constructed for the same task. Our MUC-6 
system used approximately 75 clause level pat- 
terns, with 30 distinct verbal heads. In one 
conservative experiment, we observed that the 
discovery procedure found 17 of these verbs, or 
57%. However, it also found at least 8 verbs the 
manual system lacked, which seemed relevant to 
the scenario: 
company-bring-person-\[as?officer\] 12 
person-come-\[to+eompanv\]-\[as+oZScer\] 
person-rejoin- company-\[as + o25cer\] 
person-{ ret  , conti,  e, remai, ,stay}-\[as + o25cer\] 
person-pursue-interest 
At the risk of igniting a philosophical de- 
bate over what is or is not relevant o a sce- 
nario, we note that the first four of these verbs 
are evidently essential to the scenario in the 
strictest definition, since they imply changes of 
post. The next three are "staying" verbs, and 
are actually also needed, since higher-level infer- 
ences required in tracking events for long-range 
merging over documents, require knowledge of 
persons occupying posts, rather than only as- 
suming or leaving them. The most curious one 
is "person-pursue-interesf'; urprisingly, it too 
is useful, even in the strictest MUC sense, cf., 
(muc, 1995). Systems are judged on filling a 
slot called "other-organization", i dicating from 
or to which company the person came or went. 
This pattern is consistently used in text to indi- 
nbracketed  const i tuents  a re  outs ide  o f  the  cent ra l  
SVO t r ip le t ,  inc luded here  fo r  c la r i ty .  
287 
cate that the person left to pursue other, undis- 
closed interests, the knowledge of which would 
relieve the system from seeking other informa- 
tion in order to fill this slot. This is to say that 
here strict evaluation is elusive. 
5 D iscuss ion  and  Cur rent  Work  
Some of the prior research as emphasized in- 
teractive tools to convert examples to extraction 
patterns, cf. (Yangarber and Grishman, 1997), 
while others have focused on methods for au- 
tomatically converting a corpus annotated with 
extraction examples into such patterns (Lehn- 
ert et al, 1992; Fisher et al, 1995; Miller et 
al., 1998). These methods, however, do not re- 
duce the burden of finding the examples to an- 
notate. With either approach, the portability 
bottleneck is shifted from the problem of build- 
ing patterns to that of finding good candidates. 
The prior work most closely related to this 
study is (Riloff, 1996), which, along with (Riloff, 
1993), seeks automatic methods for filling slots 
in event templates. However, the prior work 
differs from that presented here in several cru- 
cial respects; firstly, the prior work does not at- 
tempt to find entire events, after the fashion 
of MUC's highest-level scenario-template task. 
Rather the patterns produced by those systems 
identify NPs that fill individual slots, without 
specifying how these slots may be combined 
at a later stage into complete vent templates. 
The present work focuses on directly discovering 
event-level, multi-slot relational patterns. Sec- 
ondly, the prior work either relies on a set of 
documents with relevance judgements to find 
slot fillers where they are relevant o events, 
(Riloff, 1996), or utilizes an un-classified cor- 
pus containing a very high proportion of rele- 
vant documents o find all instances of a seman- 
tic class, (Riloff and Jones, 1999). By contrast, 
our procedure requires no relevance judgements, 
and works on the assumption that the corpus is 
balanced and the proportion of relevant docu- 
ments is small. Classifying documents by hand, 
although admittedly easier than tagging event 
instances in text for automatic training, is still 
a formidable task. When we prepared the test 
corpus, it took 5 hours to mark 150 short doc- 
uments. 
The presented results indicate that our 
method of corpus analysis can be used to rapidly 
identify a large number of relevant patterns 
without pre-classifying a large training corpus. 
We are at the early stages of understanding 
how to optimally tune these techniques, and 
there are number of areas that need refinement. 
We are working on capturing the rich informa- 
tion about concept classes which is currently re- 
turned as part of our pattern discovery proce- 
dure, to build up a concept dictionary in tandem 
with the pattern base. We are also consider- 
ing the proper selection of weights and thresh- 
olds for controlling the rankings of patterns and 
documents, criteria for terminating the itera- 
tion process, and for dynamic adjustments of 
these weights. We feel that the generalization 
technique in pattern discovery offers a great 
opportunity for combating sparseness of data, 
though this requires further research. Lastly, 
we are studying these algorithms under several 
unrelated scenarios to determine to what extent 
scenario-specific phenomena affect their perfor- 
mance. 
References 
Ido Dagan, Shaul Marcus, and Shaul 
Markovitch. 1993. Contextual word simi- 
larity and estimation from sparse data. In 
Proceedings of the 31st Annual Meeting of 
the Assn. for Computational Linguistics, 
pages 31-37, Columbus, OH, June. 
David Fisher, Stephen Soderland, Joseph Mc- 
Carthy, Fang-fang Feng, and Wendy Lehnert. 
1995. Description of the UMass system as 
used for MUC-6. In Proc. Si;zth Message Un- 
derstanding Conf. (MUC-6), Columbia, MD, 
November. Morgan Kaufmann. 
R. Grishman, L. Hirschman, and N.T. Nhan. 
1986. Discovery procedures for sublanguage 
selectional patterns: Initial experiments. 
Computational Linguistics, 12(3):205-16. 
Lynette Hirschman, Ralph Grishman, and 
Naomi Sager. 1975. Grammatically-based 
automatic word class formation. Information 
Processing and Management, 11(1/2):39-57. 
Timo J/irvinen and Pasi Tapanainen. 1997. A 
dependency parser for English. Technical Re- 
port TR-1, Department of General Linguis- 
tics, University of Helsinki, Finland, Febru- 
ary. 
Martin Kay and Martin RSscheisen. 1993. 
288 
Text-translation alignment. Computational 
Linguistics, 19(1). 
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, 
E. Riloff, and S. Soderland. 1992. Univer- 
sity of massachusetts: MUC-4 test results 
and analysis. In Proc. Fourth Message Un- 
derstanding Conf., McLean, VA, June. Mor- 
gan Kaufmann. 
Scott Miller, Michael Crystal, Heidi Fox, 
Lance Ramshaw, Richard Schwartz, Rebecca 
Stone, Ralph Weischedel, and the Annota- 
tion Group. 1998. Algorithms that learn to 
extract information; BBN: Description of the 
SIFT system as used for MUC-7. In Proc. of 
the Seventh Message Understanding Confer- 
ence, Fairfax, VA. 
1993. Proceedings of the Fifth Message Un- 
derstanding Conference (MUC-5), Baltimore, 
MD, August. Morgan Kaufmann. 
1995. Proceedings of the Sixth Message Un- 
derstanding Conference (MUC-6), Columbia, 
M_D, November. Morgan Kaufmann. 
Johanna Nichols. 1978. Secondary predicates. 
Proceedings of the 4th Annual Meeting of 
Berkeley Linguistics Society, pages 114-127. 
Maria Teresa Pazienza, editor. 1997. Infor- 
mation Extraction. Springer-Verlag, Lecture 
Notes in Artificial Intelligence, Rome. 
Fernando Pereira, Naftali Tishby, and Lillian 
Lee. 1993. Distributional clustering of En- 
glish words. In Proceedings of the 31st An- 
nual Meeting of the Assn. for Computational 
Linguistics, pages 183-190, Columbus, OH, 
June. 
Ellen Riloff and Rosie Jones. 1999. Learn- 
ing dictionaries for information extraction by 
multi-level bootstrapping. In Proceedings of
Sixteenth National Conference on Artificial 
Intelligence (AAAI-99), Orlando, Florida, 
Ellen Riloff. 1993. Automatically construct- 
ing a dictionary for information extraction 
tasks. In Proceedings of Eleventh National 
Conference on Artificial Intelligence (AAAI- 
93), pages 811-816. The AAAI Press/MIT 
Press. 
Ellen Riloff. 1996. Automatically generating 
extraction patterns from untagged text. In 
Proceedings of Thirteenth National Confer- 
ence on Artificial Intelligence (AAAL96), 
pages 1044-1049. The AAAI Press/MIT 
Press. 
Pasi Tapanainen and Timo J~rvinen. 1997. A 
non-projective dependency parser. In Pro- 
ceedings of the 5th Conference on Applied 
Natural Language Processing, pages 64-71, 
Washington, D.C., April. ACL. 
Roman Yangarber and Ralph Grishman. 1997. 
Customization of information extraction sys- 
tems. In Paola Velardi, editor, International 
Workshop on Lexically Driven Information 
Extraction, pages 1-11, Frascati, Italy, July. 
Universit?~ di Roma. 
289 
Automatic Acquisition of Domain Knowledge for Information 
Extraction 
Roman Yangarber, Ralph Grishman Past Tapanainen 
Courant  Inst i tute of Conexor oy 
Mathemat ica l  Sciences Helsinki, F in land 
New York University 
{roman \[ grishman}@cs, nyu. edu Pasi. Tapanainen@conexor. fi 
Si!ja Ituttunen 
University of Helsinki 
F inland 
sihuttun@ling.helsinki.fi 
Abstract  
In developing an Infbrmation Extraction tIE) 
system tbr a new class of events or relations, one 
of the major tasks is identifying the many ways 
in which these events or relations may be ex- 
pressed in text. This has generally involved the 
manual analysis and, in some cases, the anno- 
tation of large quantities of text involving these 
events. This paper presents an alternative ap- 
proach, based on an automatic discovery pro- 
cedure, ExDIsCO, which identifies a set; of rele- 
wmt documents and a set of event patterns from 
un-annotated text, starting from a small set of 
"seed patterns." We evaluate ExDIScO by com- 
paring the pertbrmance of discovered patterns 
against that of manually constructed systems 
on actual extraction tasks. 
0 Introduct ion 
Intbrmation Extraction is the selective xtrac- 
tion of specified types of intbrmation from nat- 
ural language text. The intbrmation to be 
extracted may consist of particular semantic 
classes of objects (entities), relationships among 
these entities, and events in which these entities 
participate. The extraction system places this 
intbrmation into a data base tbr retrieval and 
subsequent processing. 
In this paper we shall be concerned primar- 
ily with the extraction of intbrmation about 
events. In the terminology which has evolved 
ti'om the Message Understanding Conferences 
(muc, 1995; muc, 1993), we shall use the term 
subject domain to refer to a broad class of texts, 
such as business news, and tile term scenario to 
refer to tile specification of tile particular events 
to be extracted. For example, the "Manage- 
ment Succession" scenario for MUC-6, which we 
shall refer to throughout this paper, involves in- 
formation about corporate executives tarting 
and leaving positions. 
The fundamental problem we face in port- 
ing an extraction system to a new scenario is 
to identify the many ways in which intbrmation 
about a type of event may be expressed in the 
text;. Typically, there will be a few common 
tbrms of expression which will quickly come to 
nfind when a system is being developed. How- 
ever, the beauty of natural language (and the 
challenge tbr computational linguists) is that 
there are many variants which an imaginative 
writer cast use, and which the system needs to 
capture. Finding these variants may involve 
studying very large amounts of text; in the sub- 
ject domain. This has been a major impediment 
to the portability and performance of event ex- 
traction systems. 
We present; in this paper a new approach 
to finding these variants automatically fl'om a 
large corpus, without the need to read or amLo- 
tate the corpus. This approach as been evalu- 
ated on actual event extraction scenarios. 
In the next section we outline the strncture of 
our extraction system, and describe the discov- 
ery task in the context of this system. Sections 
2 and 3 describe our algorithm for pattern dis- 
covery; section 4 describes our experimental re- 
sults. This is tbllowed by comparison with prior 
work and discussion in section 5. 
1 The Extract ion System 
In the simplest terms, an extraction system 
identifies patterns within the text, and then 
mat)s some constituents of these patterns into 
data base entries. (This very simple descrip- 
lion ignores the problems of anaphora nd in- 
tersentential inference, which must be addressed 
by any general event extraction system.) AI- 
though these l)atterns could in principle be 
stated in terms of individual words, it is much 
940 
easier to state them in terms of larger SylltaC- 
tic constituents, uch as noun phrases and verb 
groups. Consequently, extraction ormally con- 
sists of an analysis of the l;e.xt in terms of general 
linguistic structures and dolnain-specifio con- 
structs, tbllowed by a search for the scenario- 
specific patterns. 
It is possible to build these constituent struc- 
tures through a flfll syntactic analysis of the 
text, and the discovery procedure we describe 
below woul(1 be applicable to such an architec- 
ture. Howe, ver, for re&sellS of slme,(t , coverage, 
and system rolmstness, the more (:ommon ap- 
t)roa(:h at present is to peribrni a t)artial syn- 
tactic analysis using a cascade of finite-state 
transducers. This is the at)t)roa(:h used by our 
e.xtraction system (Grishman, 1995; Yangarber 
and Grishman, 1998). 
At; the heart of our syslx'an is a regular ex- 
pression pattern matcher which is Cal)al)le of 
matching a set of regular exl)ressions against 
a partially-analyzed text and producing addi- 
tional annotations on the text. This core draws 
on a set of knowledge bases of w~rying degrees 
of domain- and task-specificity. The lexicon in- 
cludes both a general English dictionary and 
definitions of domain and scenario terms. The 
concept base arranges the domain terms into 
a semantic hierarchy. The predicate base. de- 
s('ribes the, logical structure of I;he events to be 
extracl;od. 'Fire pattern \])ase consists of sets of 
patterns (with associated actions), whi(;h make 
r(;ferollCO to information Kern the other knowl- 
e(lge bases. Some t)attorn sots, su(:h as those for 
n(mn and verb groups, are broadly apl)licable , 
wlfile other sets are spe(:ifio to the scenario. 
V~Ze, have previously (Yangarl)er and Grish- 
man, 1.997) (lescrit)ed a user interface which 
supt)orts the rapid cust;omization of the extrac- 
tion system to a new scenario. This interface 
allows the user to provide examples of role- 
wmt events, which are automatically converted 
into the appropriate patterns and generalized to 
cover syntactic variants (passive, relative clause, 
etc.). Through this internee, the user can also 
generalize l;he pattern semanti('ally (to (:over a 
broader class of words) and modify the concet)t 
base and lexicon as needed. Given an appro- 
priate set; of examples, thereibre, it; has become 
possible to adapt the extraction system quite 
ral)idly. 
However, the burden is still on the user to 
find the appropriate set of examples, which may 
require a painstaldng and expensive search of a 
large corpus. Reducing this cost is essential for 
enhanced system portability; this is the problem 
addressed by the current research. 
Ilow can we automatically discover a suitable 
set; of candidate patterns or examples (patterns 
which at least have a high likelihood of being 
relevant to the scenario)? The basic idea is to 
look for linguistic patterns which apt)ear with 
relatively high frequency in relevant documents. 
While there has been prior research oll idea|i- 
lying the primary lexical t)atterns of a sublan- 
guage or cortms (Orishman et al 1986; Riloff, 
1996), the task here is more complex, since we 
are tyt)ically not provided in advance with a 
sub-corpus of relevmlt passages; these passages 
must themselves be tbund as part of t;t1(; discov- 
ery i)rocedure. The difficulty is that one of the 
l)est imlic~tions of the relevance of the passages 
is t)recisely the t)resence of these constructs. Bo- 
(:ause of this (:ircularity, we l)ropose to a(:quire. 
the constructs and t)assagos in tandem. 
2 ExDISCO: the  D iscovery  P rocedure  
We tirst outline ExDIsco ,  our procedure for 
discovery of oxl,raction patterns; details of some 
of the stops arc l)rcse, nted in the section which 
follows, and an earlier t)~q)er on our at)l)roach 
(Yang~u:bcr ot al., 2000). ExDIscO is mi ml- 
supervised 1)rocedure: the training (:ortms does 
not need to t)e amlotated with the specific event 
intbrmatkm to be. e.xtracted, or oven with infor- 
mation as to whi(;h documents in the ('orpus are 
relevant o the scenario. 'i7tlo only intbrmation 
the user must provide, as described below, is a 
small set of seed patterns regarding the s(:enario. 
Starting with this seed, the system automati- 
(:ally pertbnns a repeated, automatic expansion 
of the pattern set. This is analogous to the pro- 
cess of automatic t;enn expansion used in s()me 
information retrieval systems, where, the terlns 
Dora the most relewmt doculncnts are added 
to the user query and then a new retriewfl is 
imrformed. However, by expanding in terms of 
1)atl;erns rather than individual terms, a more 
precise expansion is possit)le. This process pro- 
coeds as tbllows: 
0. We stm:t with a large, corlms of documents 
in the domain (which have not been anne- 
941 
tared or classified in any way) and an initial 
"seed" of scenario patterns selected by the 
user - -  a small set of patterns whose pres- 
ence reliably indicates thai; the document 
is relevant o the scenario. 
. The pattern set is used to divide the cor- 
tins U into a set of relewmt documents, R
(which contain at; least one instance of one 
of the patterns), and a set of non-relevant 
documents R = U - R. 
2. Search tbr new candidate patterns: 
? automatically convert each document 
in the eorIms into a set of candidate 
patterns, one for each clause 
? rank patterns by the degree to which 
their distribution is correlated with 
docmnent relevance (i.e., appears with 
higher frequency in relevant docu- 
ments than in non-relewmt ones). 
3. Add the highest ranking pattern to the pat- 
tern set. (Optionally, at this point, we may 
present he pattern to the user for review.) 
4. Use the new pattern set; to induce a new 
split of the corpus into relevant and non- 
relevant documents. More precisely, docu- 
ments will now be given a relevance confi- 
dence measure; documents containing one 
of the initial seed patterns will be given 
a score of 1, while documents which arc 
added to the relevant cortms through newly 
discovered patterns will be given a lower 
score. I/,epeat the procedure (from step 1) 
until some iteration limit is reached, or no 
more patterns can be added. 
3 Methodo logy  
3.1 Pre-processing: Syntact ic Analysis 
Before at)plying ExDIsco ,  we pre-proeessed 
the cortms using a general-purpose d pendency 
parser of English. The parser is based on 
the FDG tbrmalism (Tapanainen and Jgrvi- 
hen, 1997) and developed by the Research Unit 
for Multilingual Language Technology at the 
University of Helsinki, and Conexor Oy. The 
parser is used ibr reducing each clause or noun 
phrase to a tuple, consisting of the central ar- 
guments, ms described in detail in (Yangarber 
et al, 2000). We used a corlms of 9,224 articles 
from the Wall Street; Journal. The parsed arti- 
cles yielded a total of 440,000 clausal tuples, of 
which 215,000 were distinct. 
3.2 Normal izat ion 
We applied a name recognition module prior to 
parsing, and replaced each name with a token 
describing its (:lass, e.g. C-Person, C-Company, 
etc. We collapsed together all numeric expres- 
sions, currency wflues, dates, etc., using a single 
token to designate ach of these classes. Lastly, 
the parser performed syntactic normalization to 
transtbrm such variants ms the various passive 
and relative clauses into a common tbrm. 
3.3 General izat ion and Concept Classes 
Because tuples may not repeat with sufficient 
frequency to obtain reliable statistics, each tu- 
ple is reduced to a set of pints: e.g., a verb- 
object pair, a subject-object pair, etc. Each pair 
is used as a generalized pattern during the can- 
didate selection stage. Once we have identitied 
pairs which are relevant o the scenario, we use 
them to gather the set; of words for the miss- 
ing role(s) (tbr example, a class of verbs which 
occur with a relevant subject-ot@ct pair: "com- 
pany {hire/fire/expel...} person"). 
3.4 Pat tern  Discovery 
We (-onducte(1 exi)eriments in several scenarios 
within news domains such as changes in cor- 
porate ownership, and natural disasters. Itere 
we present results on the "Man~geme.nt Suc- 
cession" and "Mergers/Acquisitions" cenarios. 
ExDIsco  was seeded with lninimal pattern sets, 
namely: 
Subject Verb Direct Object 
C-Company C-At)point C-Person 
C-Person C-Resign 
ibr the Mmmgement task, and 
Subject Verb Direct Object 
* C-Buy C-Conlt)any 
C-Company merge * 
for Acquisitions. Here C-Company and C- 
Person denote semantic classes containing 
named entities of the corresponding types. C- 
Appoint denotes the list of verbs { appoint, elect, 
promote, name, nominate}, C-Resign = { re- 
sign, depart, quit }, and C-Buy = { buy , pur- 
chase }. 
942 
\ ])uring ~ single iter~tion, we conqmt(; the 
score, See're(p), for each cm~(lidate 1)attern p, 
using (;he fornmla~: 
S, :o ' , ' ,@)  = IH n l~l 
IHI - 1,,~ IHn  ~.1 (:t) 
where 12. (Icnotes (;h(', l'clewmt subsc(; of docu- 
ments, mid I t=  It(p) the, ( locmnents imttching 
p, as above; the Iirst (;erm a(:(:ounts for the con- 
(lition~fl t)robabil ity of relev;m('e oil p~ and |;11(; 
second tbr its support .  We further impose two 
support criteria: we distrust such frequent pat- 
(;,~.,-.~ w\]le,:e I1~ n UI > ,~IUI, ,~ uninforn,,~tive, 
mid rare patte.rns \['or which I1\] r-I \]~.1 < fl as 
noise. 2 At the end ot' (.aeh il;eratiol~, the sysl;em 
selects the pal;tern with the highest Sco'/'d(p)~ 
and adds it (;o (;lie seed scl;. The (to(:un~enl;s 
which t;he winning t)~(;t;ern hits are added (;o 
t;111(; relevant set. The  t)al;l;(;rn s(;areh is then 
r(;sl;m:l;(;d. 
3.5 Document  Re- rank ing  
Th(: above is a simt)lifi(';~l;ion of (;he a,(:tual pro- 
cedlll'(}~ in severa\] r(',st)e('(;s. 
Only generalized t)ntl;erns are (:onsidered fi)r 
(:audi(t~my, with one or mot(', slol;s fill(:(1 wi(;h 
wihl-cm'ds. In comput ing the score of th(', ge, n- 
(;raliz(:d \]);tttern, w(: do not take into ('onsi(h:r- 
;i,1;i()11 all possible va,hw, s of the, wil(1-('m:d role. 
\?e instea.d (:()llS(;raJll (;he wild-(:ar(l to thos(~ wd- 
u(:s wlli(:h l;ht',llls(;lv(;s ill (;llrH \]l;tV(: high scores. 
Th(:se v~du(:s l;lw, n |)e(:on~e lllClll\])(;l'S of }/. II(:W 
(:lass, whi(:h is l)rOdu(:ed in (;:tlldClll with the 
wimfing 1)att(:rn. 
\])o('umel~tS reh:wm('e is s(-ored (m ~ s(;ah: l)e- 
(;ween 0 and 1. Tlm seed t)atterns a.re a.(:cet)ted 
~,s trut\]~; the do('mlw, nts (;hey mat(:\]1 hnve rcle- 
vmme 1. On i(;er;~tion i + 1, e~mh t)a(;tern p is 
assigned a precision measure, t)ase(l on the rel- 
(':Vall(;e of |;11(; (locllnlelfl;s i|; 111a, l;(;ll(',,q: 
~ ".d~(d) (~) 
f f , , :d  +~ (v)  - -  IH(v) l ,~.(,,) 
where l~,eli(d) is the re, levmlce of' 1;11(: doeunmn(; 
fi'om t;t1(', previous iteration, ~md l I(p) is the set 
of documents where p matched, in general, if K 
is a classifier (:onsisting of ~ set of l)al;terns, w(', 
define H (K)  as the st:l; of documents  where all 
~similar to that used in (liiloff, 1996) 
~W(: used ,:-- 0.1 and fl = 2. 
of t)~d;terns p C K m~l;(:h, mid the "cunmlative" 
precision of K as 
1 ~ 1~4~(a,) (3) P~.~d +~(1() = IH U()I <.(K)  
Once the wimfing pa,l;l;ern is accepted, the rel- 
ewmee of the documents is re-adjusted. For 
(;~mh document  d which is matched by some 
subset of l;he currently accet)t('d pntterns, we 
can view thai; sul)s(',t; of  l)~tterns as ~ classitier 
Kd = {pj}. These  patterns (tel;ermilm the new 
reh;wmce score of the document  as 
J~, "~l,~ " ( ,0  : 111~x (:tc,,.1,*(,O,v,.,;, .~" (K , ) )  (~:) 
This ensures tha.(; l;he rclewmce score grows 
monotonical ly, and only when there is sufliei(mt 
positive evidence, as (;he i)ntterns in etl'e(:I; vote 
"conjmmtively" on the (loculncnl;s. 
We also tr ied an alternative, ::disjun(:tive" 
voting scheme, with weights wlfich accounts tbr 
vm:intion in support of the p~ttterns, 
J,.,.1, (d) . . . .  ~ "~ I I  (1 - ~',.~,.c~(p))"",' (5) 
~c K(d) 
where t;11(', weights ,wp arc (tetint;d using the tel- 
ewm(:(: of the (loeuments, a,s the total  SUl)l)or(; 
which the pa, I;I;ern p receives: 
% = log ~ l;.d,(d) 
dE 11 (p) 
and ;,7 is (;11(' largest weight. The  r(',cursive for- 
nmb~s ('apl;m:e (;he mul;u~fl dependency of t)~t- 
terns ~md documents;  this re-computat ion ~md 
growing of precision and relevmlce rmlks is the 
core of the t)rocedure. :~ 
4 Resu l ts  
4 .1  Event  Ext ract ion  
'l'he, most nal;m'a.l measm'e of efl'ecl;iveness of our 
discovery procedure is the performmme of ml ex- 
tract ion systmn using the, discovered t)~tterns. 
However, il; is not 1)ossil)le to apply this reel;- 
rio direei;ly because the discovered t)al;terns lack 
some of the information required tbr entries ill 
:{\V('. did not el)serve a significam; difl'erencc in 1)crfi)r- 
lIiHl\[CO, bet, ween the two tormulas 4 alt(t 5 in o111" experi- 
in(mrs; the results whit:h tbllow use 5. 
943 
the pattern base: information about the event 
type (predicate) associated with the pattern, 
and the mapping from pattern elements to pred- 
icate arguments. We have evaluated ExDIsco  
by manually incorporating the discovered pat- 
terns into the Proteus knowledge bases and run- 
ning a full MUC-style evaluation. 
We started with our extraction system, Pro- 
tens, which was used in MUC-6 in 1995, and 
has undergone continual improvements since 
the MUC evaluation. We removed all the 
scenario-specific clause and nominalization pat- 
terns. 4 We then reviewed all the patterns which 
were generated by the ExDIsco,  deleting those 
which were not relewmt to the task, or which 
did not correspond irectly to a predicate al- 
ready implemented tbr this task)  The remain- 
ing pat;terns were augmented with intbnnation 
about the corresponding predicate, and the re- 
lation between the pattern and the predicate 
al'guments, a The resulting variants of Proteus 
were applied to the formal training corpus and 
the (hidden) formal test corpus for MUC-6, and 
the output evaluated with the MUC scorer. 
The results on the training corpus are: 
Pattern Base Recall Precision 
Seed 38 83 
Ex I ) Isco 62 80 
Union 69 __79 
Manual-MUC ~ 71 L~1.9~ 
Manual-NOW 6(3~ 79 L7!~z\[)_t_j 
and on the test cortms: 
4There are also a few noun phrase patterns which can 
give rise to scenario events. For example, "Mr Smith, 
former president of IBM", may produce an event record 
where l%ed Smith left IBM. These patterns were left in 
Proteus for all the runs, and they make some contribu- 
tion to the relatively high baseline scores obtained using 
just the seed event patterns. 
~ExD~sco f und patterns which were relevant to the 
task lint could not be easily aceomodated in Proteus. 
For instance "X remained as president" could be rele- 
vant, particularly in the case of a merger creating anew 
corporate ntity, but Proteus was not equipped to trun- 
dle such iIfformation, and has not yet been extended to 
incorporate such patterns. 
6As with all clause-level patterns in Proteus, these 
patterns m-e automatically generalized tohandle syntac- 
tic wn'iants uch as passive, relative clause, etc. 
Pattern Base Recall Precision F 
Seed 27 74 39.58 
ExDIsco 52 72 60.16 
Union 57 73 63.56 
Manual-NOW -- 56 75 6404. 
The tables show the recall and precision mea- 
sures for the patterns, with F-measure being 
the harmonic mean of the two. The Seed pat- 
tern base consists of just the initial pattern set, 
given in the table on the previous page. ~ib this 
we added the patterns which the system discov- 
ered automatically after about 100 iterations, 
producing the pattern set called ExDIsco.  For 
comparison, M anual-MUC is the pattern base 
lnanually develot)ed on the MUC-6 training 
corpus-1)repared over the course of 1 month 
of full-time work by at least one computational 
linguist (during which the 100-document train- 
ing corpus was studied in detail). The last row, 
Manual-now, shows the current pertbrmance of
the Proteus system. The base called Ultiolt con- 
tains the union of ExDIScO and Manual-No'w. 
We find these results very encouraging: Pro- 
teus performs better with the patterns discov- 
ered by ExI)IscO than it did after one month 
of manual tinting and development; in fact, this 
perfi)rmance is close to current levels, which 
are the result of substantial additional devel- 
opmeut. These results umst be interpreted, 
however, with several caveats. First, Proteus 
performance depends on many fimtors besides 
the event patterns, such as the quality of name 
re, cognition, syntactic mmlysis, anaphora reso~ 
lution, inferencing, etc. Several of these were 
improved since the MUC formal evaluation, so 
some of the gain over the MUC formal evalua- 
tion score is attritmtable to these factors. How~ 
ever, all of the other scores are comparable in 
these regards. Second, as we noted above, the 
patterns were reviewed and augmented manu- 
ally, so the overall procedure is not entirely au- 
tomatic. However, the review and augmenta- 
tion process took little time, as compared to 
the manual corpus analysis and development of
the pattern base. 
4.2 Text  f i l ter ing 
We can obtain a second measure of pertbr- 
mance by noting that, in addition to growing 
the tmttern set, ExDIsco  also grows the rele- 
944 
0.9 
0.8 
0.7 
0.6 
0.5 
_ . r -~H . . . . . . . . . . . . . . . . .  r . . . . . . .  T ~ ~ : : ~  T 
;!\ >. g~t : 
- ' il 
%i 
\[!\] 
7 
\[!J 
Legend: 
Management/Test ? .-{~ ...... 
ManagemenVl-raie - :*: -- 
MUC-6 ? 
0.2 0.4 0.6 0.8 
Recall 
Figure l: Management Suc('cssion 
0.9 
0.8 
0.7 
0.6 
0.5 
L_~/r 
Legend: 
Acquisition 
0.2 0.4 0.6 
Recall 
0.8 
Figme 2: Mergers/A(:quisitions 
vance rankings of documents. The latter cnn be 
evahlated irectly, wil;hollt human intervention. 
We tested Exl)IsC, o ~tgainst wo cor\])orn: th(; 
100 documents from MUC-6 tbrmal training, 
a:nd the 100 documents from the MUC-6 for- 
mal test (both are contained anlong the 10,000 
ExDIsoO training set) r. Figure 1 shows recall 
t)\]otted against precision on the two corpora, 
over 100 iterations, starting with the seed pat- 
te, nls in section 3.d. This view on the discovery 
procedure is closely related to the MUC %ext- 
till;ering" task, in which the systems are jlulged 
at the \]evel of doc,wm, e,'nt.s rather thmt event slots. 
It; is interesting to (:omt)m:e Exl)IsCO's results 
with how other MUC-6 part\]tit)ants performed 
on the MUC-b '  test cortms , shown anonymously. 
ExDIscO attains values within the range of 
the MUC participald;S, all of which were either 
heavily-supervised or m~mually coded systems. 
II; is important to bear in mind that Ex I ) I sco  
had no benefit of training material, or any in- 
tbrmation beyond the seed pattern set. 
Figure 2 shows the 1)ertbrmance, of text fil- 
tering on the Acquisition task, again, given the 
seed in section 3.4. ExDisco  was trained on 
|;lie same WSJ eorlms, and tested against a set 
of 200 documents. We retrieved this set using 
keyword-based IR, search, and judged their rel- 
evance by halId. 
rThesc judgements constituted the truth which was 
used only for evaluation, not visible to ExDISCO 
5 Discuss ion  
The development of a w~riety of information 
extra(:tion systems over the last decade has 
demonstrated their feasibility but also the lim- 
itations on their portability and t)erformance. 
Prcl)aring good t)atterns tbr these syste, ms re- 
quires (:onsiderable skill, and achieving good 
(:overage requires |;lie analysis of a large amount 
of text. These t)rol)lems h~ve t)een impedinmnts 
to the -wide\].' use of extraction systenls. 
These dit\[iculties have stimulate.d resear('h on 
1)attel . 'n a ( : ( lu i s i t ion .  So lne  o f  th i s  work  has  en l -  
i)hasized il\]teractive tools to (:onvert examples 
to extractioi~ t)atterlls (Yangarber and Grish- 
man, 1997); nmch ot:' the re, search has focused on 
methods for automatically converting a cortms 
annotated with extraction examples into pat- 
terns (Lehnert et al, 1992; Fisher et al, 1995; 
Miller el; al., 1998). These techniques may re- 
duce the level of systeln expertise required to 
develop a new extraction N)plieation, but they 
do not lessen the lmrden of studying a large cor- 
lms in order to .find relevant candidates. 
The prior work most closely related to our 
own is that of (R.ilotf, 1996), who also seeks to 
lmild pattenls automatically without the need 
to annotate a corpus with the information to 
be extracted. Itowever, her work ditfers t'rom 
01217 own in several i lnportant respects. First, 
her patterns identit~y phrases that fill individual 
slots in the template, without specifying how 
these slots may be combined at a later stage 
into complete templates. In contrast, our pro- 
cedure discovers complete, multi-slot event pat- 
945 
terns. Second, her procedure relies on a cort)us 
in which |;tie documents have been classified for 
relevance by hand (it was applied to the MUC-3 
task, tbr which over 1500 classified documents 
are available), whereas ExDIsco requires no 
manual relevance judgements. While classify- 
ing documents tbr relevance is much easier than 
annotating docunlents with the information to 
be extracted, it; is still a significant ask, and 
places a limit on |:tie size of the training corpus 
that can be effectively used. 
Our research as demonstrated that for the 
studied scenarios automatic pattern discovery 
Call yield extraction perfi)rmance colnt)arabh~ to
that obtained through extensive corpus anal- 
ysis. There are many directions in which the 
work reported here needs to be extended: 
? nsing larger training corpora, in order to 
find less frequent exanlplcs, and in that way 
hopefully exceeding the i)erfornlancc of our 
best hand-trained system 
? cat)luring the word classes which are gen- 
erated as a by-product of our pattern dis- 
covery 1)rocedure (in a manner similar to 
(Riloff and ,Jones, 1999)) and using them 
to discover less frequent )atterns in subse- 
quent iterations 
- evaluating the effectiveness of the discov- 
cry procedure on other scenarios. In par- 
titular, we need to be able to identi\[y top- 
its which cast be most effbctively charac- 
terized by clause-level patterns (as was the 
case tbr the business domain), and topics 
which can be better characterized by other 
means. We. wouM also like to understand 
how the topic clusters (of documents and 
patterns) which are developed by our pro- 
cedure line up with pre-specified scenarios. 
References 
David Fisher, Stephen Soderland, Joseph Mc- 
Carthy, Fangfang Feng, and Wendy Lelmert. 
1995. Description of the UMass system as 
used fbr MUC-6. In Prec. Sixth Message Un- 
dcrstandin9 Conf. (MUC-6), Columbia, MD, 
November. Morgan Kauflnann. 
R.alph Grishman. 1995. The NYU systenl tbr 
MUC-6, or where's the syntax? Ill Prec. 
Sixth Message Understanding Conf. (MUC- 
6), pages 167 176, Columl)ia, MD, Novem- 
ber. Morgan Kauflnann. 
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, 
E. Riloff, and S. Soderland. 1992. Univer- 
sity of nlassachusetts: MUC-4 test results 
and analysis. Ill P,'oe. Fourth Message Un- 
der.standing Con.\[., McLean, VA, June. Mor- 
gan Kauflnaml. 
Scott Miller, Michael Crystal, Heidi Fox, 
Lance II,amshaw, R,ichard Schwartz, Rebecca 
Stone, Rall)h Weischedel, and the Annota- 
tion Group. 1998. Algorithms that learn to 
extract intbrmation; BBN: Description of the 
SIFT systenl as used for MUC-7. In PTve. 7th 
Mc.ssagc Understanding Co~:f., FMrfax, VA. 
1993. Proceedings of the F'~ifth Message UTz.- 
derstanding Confer(race (MUC-5), Baltimore, 
MD, August. Morgan Kauflnann. 
1995. PTveeedings of the Sixth Message U~I,- 
derstav, ding Conference (MUC-6), Colmnt)ia, 
MD, November. Morgan Kauflnaml. 
Ellen Rilotf and Rosie Jones. 1999. Learn- 
ing dictionaries for infbrmation extraction by 
multi-level bootstrat)ping. In Prec. 16th Nat'l 
Cord'erenee on Art'~i\[icial Intelli9enee (AAA I 
99), Orlando, Florida. 
Ellen Riloff. 1996. Automatically generating 
extraction patterns from m~tagged text. In 
Prec. I3th Nat'l Co~~:f. on Art~ificial Intel- 
ligence (AAAI-96). The AAAI Press/MIT 
Press. 
l?asi '\])~panainen a d Time .J/h:vinen. 1997. A 
non-t)rojectivc dependency parser. In P'mc. 
5th Conf. on Applied Nat'aral Language P~v- 
cessiu9, pages 64-71, Washington, D.C. ACL. 
Roman Yangarber and RalI)h Grishman. 1997. 
Customization of intbrmation extraction sys- 
tems. In Paola Velardi, editor, I~tt'l Work- 
shop on Lexically Driven I~7:forrnation Extrac- 
tion, Frascati, Italy. Universith di Roma. 
Roman Yangarl)er and Ralph Grishman. 1998. 
NYU: Description of thc Protens/PET sys- 
tem as used tbr MUC-7 ST. In 7th Message 
Understanding Conference, Columbia, MD. 
Roman Yangarl)er, Ralph Grishman, Past 
Tapanainen, and Silja Huttunen. 2000. Un- 
supervised discovery of scenario-level pat- 
terns tbr information extraction. Ill PTve. 
Co~@ on Applied Nat'aral Lang'aage Pr'ocess- 
tug (ANLP-NAACL), Seattle, WA. 
946 
Unsupervised Learning of Generalized Names
Roman Yangarber, Winston Lin, Ralph Grishman
Courant Institute of Mathematical Sciences
New York University
froman|winston|grishmang@cs.nyu.edu
Abstract
We present an algorithm, Nomen, for learning
generalized names in text. Examples of these
are names of diseases and infectious agents, such
as bacteria and viruses. These names exhibit
certain properties that make their identica-
tion more complex than that of regular proper
names. Nomen uses a novel form of bootstrap-
ping to grow sets of textual instances and of
their contextual patterns. The algorithm makes
use of competing evidence to boost the learning
of several categories of names simultaneously.
We present results of the algorithm on a large
corpus. We also investigate the relative merits
of several evaluation strategies.
1 Introduction
This research grew out of the Integrated Feasi-
bility Experiment on Biological Infectious Out-
breaks (IFE-BIO), a project to build an Infor-
mation Extraction (IE) system for identifying
events related to outbreaks and epidemics of in-
fectious disease, (Grishman et al, 2002).
IE generally relies on knowledge bases of sev-
eral kinds, and the most fundamental of these is
the domain-specic lexicon|lexical items that
are not likely to be found in general-purpose
dictionaries. This particular scenario requires
a comprehensive list of disease names. Other
requisite classes of names include: biological
agents causing disease, such as viruses and bac-
teria; vectors|organisms or animals capable of
transmitting infection; and possibly names of
drugs, used in treatment.
1.1 Generalized Names
Names of these kinds, generalized names (GNs),
dier from conventional proper names (PNs)
that have been studied extensively in the lit-
erature, e.g., as part of the traditional Named
Entity (NE) categorization task, which evolved
out of the MUC NE evaluation, (Wakao et al,
1996; Bikel et al, 1997; Borthwick et al, 1998;
Collins and Singer, 1999). The three main-
stream NE kinds are location, person, and or-
ganization, and much research has centered on
these \classical" kinds of proper names.
On the other hand, the vast eld of termi-
nology has traditionally dealt with identifying
single- and multi-word domain-specic expres-
sions, for various NLP tasks, and recent years
have seen a growing convergence between the
two elds.
In fact, good identication of names of both
kinds is essential for IE in general. In IFE-BIO,
for example, the text:
National Veterinary Services Director Dr.
Gideon Bruckner said no cases of mad cow
disease have been found in South Africa.
exhibits more than one problem of name identi-
cation and classication. We focus on general-
ized names, which pose numerous challenges.
The classication process usually starts with
identication, but the primary cue for a proper
name|capitalization (in English text)|is un-
available for generalized names. GNs are not al-
ways capitalized (\mad cow disease" or \tuber-
culosis") or may be partially capitalized (\Ebola
haemorrhagic fever", \E. coli"). GNs often have
multiple pre- and post-modiers|\(new) vari-
ant Creutzfeldt-Jacob disease," or may modify
the head of a noun group|\Bacillus anthracis
infection." Locating the boundaries of GNs is
much harder than for PNs.
The problem of ambiguity aects generalized
names, as it does proper names. E. coli can
refer to the organism or to the disease it causes;
encephalitis can mean a disease or a symptom.
1.2 Why Learning?
Why is it undesirable to rely on xed, special-
ized, domain-specic lists or gazetteers?
1. Comprehensive lists are not easy to ob-
tain.
2. Lists are never complete, since new names
(locations, diseases) periodically enter into ex-
istence and literature.
3. A typical text contains all the information
that is necessary for a human to infer the cate-
gory. This makes discovering names in text an
interesting research problem in its own right.
The following section introduces the learning
algorithm; Section 3 compares our approach to
related prior work; Section 4 presents an evalu-
ation of results; we conclude with a discussion
of evaluation and current work, in Section 5.
2 Nomen: The Learning Algorithm
Nomen is based on a bootstrapping approach,
similar in essence to that employed in (Yangar-
ber et al, 2000).
1
The algorithm is trained on
a large corpus of medical text, as described in
Section 4.
2.1 Pre-processing
A large text corpus is passed through a zoner,
a tokenizer/lemmatizer, and a part-of-speech
(POS) tagger. The zoner is a rule-based
program to extract textual content from the
mailing-list messages, i.e., stripping headers and
footers. The tokenizer produces lemmas for the
inected surface forms. The statistical POS tag-
ger is trained on the Wall Street Journal (pos-
sibly sub-optimal for texts about infectious dis-
ease). Unknown or foreign words are not lem-
matized and marked noun by the tagger.
2.2 Unsupervised Learning
0. Seeds: The user provides several trusted
seeds of each category we intend to learn. E.g.,
we selected the 10 most common diseases as
seeds for the disease category; the same for lo-
cations and several other categories.
2
1
For a detailed comparison of the algorithms,
cf. (Yangarber, 2002).
2
Frequency counts are computed from a large IE
database, of more than 10,000 records. The most com-
mon disease names: cholera, dengue, anthrax, BSE, ra-
bies, JE, Japanese encephalitis, inuenza, Nipah virus,
FMD (for foot-and-mouth disease).
For each category, the set of accepted names,
AcceptName, is initialized with the seeds.
1. Tagging: For each accepted name in each
category C to be learned, Nomen tags the lem-
matized, POS-tagged training corpus, placing
left and right tags around each occurrence of
the name|e.g., <disease> and </disease>.
2. Pattern Generation: For each tag T
inserted in the corpus on Step 1, Nomen gener-
ates a literal pattern p using a context window
of width w around the tag, e.g.,
p = [ l
 3
l
 2
l
 1
<T> l
+1
l
+2
l
+3
]
where l
i
are the context of p|the lemmas of
the surrounding words.
Note, the tag of the pattern, Tag(p) = T , in-
dicates both a direction, either \left" or \right,"
Dir(p) 2 fleft; rightg, and a category, Cat(p).
E.g., if Tag(p) = </disease>, then Dir(p) =
right and Cat(p) = disease.
Then p is transformed replacing each element
in the w-window by its generalization; in the
current simple scheme, the only generalization
can be a wildcard. These patterns form the set
of potential patterns, . Note that each pattern
matches on only one side of an instance, either
its beginning or its end.
3. Pattern Matching: Match every pat-
tern p 2  against the entire training corpus.
In a place where the context of p matches, p
predicts where one boundary of a name in text
would occur. Let pos
a
be the position of this
boundary. Then use a noun group (NG) regu-
lar expression
3
to search for the other, partner
boundary, say, at position pos
b
. For example,
suppose p matches in the text
the
  
z }| {
h
1
yellow feveri
2
vaccinei
3
| {z }
 !
to villagers
at pos
a
= 2 and Dir(p) = right; then pos
b
= 1.
However, if pos
a
= 1 and Dir(p) = left then
pos
b
= 3. (Note, the search proceeds in the
opposite direction of Dir(p).) Next, we check
whether the NG between positions pos
a
and
pos
b
has already been accepted as a name in
some category; the result can be:
3
Using heuristics, as in terminology discovery,
(Frantzi et al, 2000); we use a simple NG regular ex-
pression, [Adj* Noun+].
 positive: The NG has already been ac-
cepted as a name in the same category as
Cat(p);
 negative: The NG has already been ac-
cepted as a name in a dierent category,
C
0
6= Cat(p);
 unknown: The NG has not yet been ac-
cepted as a name in any category.
The unknown case is where a new candidate of
the category Cat(p) may potentially be discov-
ered.
4. Pattern Acquisition: For each pat-
tern p 2 , this gives us instance-based lists of
positive pos(p), negative neg(p) and unknown
unk(p) NGs. To compute Score(p), we rst de-
ne the corresponding type-based sets:
 pos

(p) = set of distinct names of category
Cat(p) from AcceptName that p matched.
 neg

(p) = set of distinct names of a wrong
category.
 unk

(p) = set of distinct NGs of unknown
type.
To score the patterns in , we currently use
the accuracy and condence measures:
acc

(p) =
jpos

j
jpos

j + jneg

j
conf

(p) =
jpos

j
jpos

j + jneg

j + junk

j
Patterns with accuracy below a precision
threshold acc

(p) < 
prec
, are removed from .
The remaining patterns are ranked as follows.
The score is computed as:
Score(p) = conf

(p)  log jpos

(p)j (1)
Add the n{best patterns for each target cate-
gory to the set of accepted patterns, AcceptPat.
In the rst term of the scoring function,
higher condence implies that we take less risk if
we acquire the pattern, since acquiring the pat-
tern aects the unknown population. The sec-
ond term favors patterns which select a greater
number of distinct names in AcceptName.
5. Application: Apply each pattern p 2
AcceptPat to the entire corpus.
The noun groups in the set unk

(p) are the
candidates for being added to the category
Cat(p). Let 	 be the list of candidate types:
	 =
[
p 2AcceptPat
unk

(p)
6. Candidate Acquisition: Compute a
score for each candidate type t 2 	, based on
 how many dierent patterns in AcceptPat
match an instance of type t,
 how reliable these patterns are.
To rank a candidate type t 2 	 consider the set
of patterns in AcceptPat which match on some
instance of t; let's call this set M
t
. If jM
t
j < 2,
the candidate is discarded.
4
Otherwise, com-
pute Rank(t) based on the quality of M
t
:
Rank(t) = 1  
Y
p2M
t

1   conf

(p)

(2)
This formula combines evidence by favoring
candidates matched by a greater number of pat-
terns; on the other hand, the term conf

(p) as-
signs more credit to the more reliable patterns.
For each target category, add the m best-
scoring candidate types to the set AcceptName.
7. Repeat: from Step 1, until no more
names can be learned.
3 Prior Work
The Nomen algorithm builds on some ideas
in previous research. Initially, NE classi-
cation centered on supervised methods, sta-
tistically learning from tagged corpora, using
Bayesian learning, ME, etc., (Wakao et al,
1996; Bikel et al, 1997; Borthwick et al,
1998). (Cucerzan and Yarowsky., 1999) present
an unsupervised algorithms for learning proper
names. AutoSlog-TS, (Rilo and Jones, 1999),
learns \concepts" (general NPs) for lling slots
in events, which in principle can include gen-
eralized names. The algorithm does not use
competing evidence. It uses syntactic heuristics
which mark whole noun phrases as candidate in-
stances, whereas Nomen also attempts to learn
names that appear as modiers within a NP.
4
Note, this means that the algorithm is unlikely to
learn a candidate which occurs only once in the corpus.
It can happen if the unique occurrence is anked by ac-
cepted patterns on both sides.
In the area of NE learning, (LP)
2
, (Ciravegna,
2001), is a recent high-performance, supervised
algorithm that learns contextual surface-based
rules separately for the left and the right side
of an instance in text. Separating the two sides
allows the learner to accept weaker rules, and
several correction phases compensate in cases
of insu?cient evidence by removing uncertain
items, and preventing them from polluting the
set of good seeds.
Research in automatic terminology acquisi-
tion initially focused more on the problem of
identication and statistical methods for this
task, e.g., (Justeson and Katz, 1995), the C-
Value/NC-Value method, (Frantzi et al, 2000).
Separately, the problem of classication or clus-
tering is addressed in, e.g., (Ushioda, 1996)
(Strzalkowski and Wang, 1996) presents an
algorithm for learning \universal concepts,"
which in principle includes both PNs and
generic NPs|a step toward our notion of gen-
eralized names. The \spotter" proceeds itera-
tively from a handful of seeds and learns names
in a single category.
DL-CoTrain, (Collins and Singer, 1999),
learns capitalized proper name NEs from a syn-
tactically analyzed corpus. This allows the rules
to use deeper, longer-range dependencies, which
are di?cult to express with surface-level infor-
mation alone. However, a potential problem
with using this approach for our task is that
the Penn-Treebank-based parser does not assign
structure to noun groups, so it is unclear that it
could discover generalized names, as these often
occur within a noun group, e.g., \the 4 yellow
fever cases." Our approach does not have this
limitation.
The salient features of Nomen: it learns
 generalized names, with no reliance on cap-
italization cues, as would be possible in the
case of proper names (in English).
 from an un-annotated corpus, bootstrap-
ping from a few manually-selected seeds
 rules for left and right contexts indepen-
dently (as (LP)
2
to boost coverage).
 several categories simultaneously, and uses
additional categories for negative evidence
to reduce overgeneration.
4 Results
The algorithm was developed using a corpus
drawn from the ProMed mailing list. ProMed is
a global forum where medical professionals post
information regarding outbreaks of infectious
disease (using at times informal language).
Our full training corpus contains 100,000 sen-
tences from 5,100 ProMed articles, from the be-
ginning of 1999 to mid-2001. A subset of that,
used for development, contains 26,000 sentences
from 1,400 documents (3.2Mb) from January to
July 1999.
Our evaluation strategy diers from those in
some of the prior work. We discuss the compet-
ing evaluation strategies in detail in Section 5.2.
To measure performance, we constructed sev-
eral reference lists as follows. First, a manual
list of disease names was hand-compiled from
multiple sources.
5
The manual list consists of
2,492 disease names.
The recall list is automatically derived from
the manual list by searching the training cor-
pus for disease names that surface more than
once.
6
The recall list for the 26,000-sentence
corpus contains 322 disease names, including
some aliases and common acronyms.
The precision list is constructed as the union
of the manual list with an automatically gener-
ated list of acronyms (made by collecting rst
letters of all multi-token names in the manual
list). We applied the same procedure to gener-
ate recall and precision lists for locations.
Then, we judge the recall of Nomen against
the recall lists, and precision against the preci-
sion lists. The list sizes are shown in Table 1.
We focus on two categories, diseases and lo-
cations, while learning several categories simul-
5
Using a disease IE database (Grishman et al, 2002),
the Gideon disease database, and Web search. The list
includes some common acronyms, like HIV and FMD.
6
This is justied because the current algorithm is un-
likely to discover a name that occurs only once.
Reference List Disease Location
Manual 2492 1785
Recall (26K corpus) 322 641
Recall (100K corpus) 616 1134
Precision 3588 2404
Table 1: Reference Lists
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
 
Pr
ec
is
io
n
 Diseases & Locations:  Recall 
Dis + Loc + Sym + Other:
Locations (100k)
Locations  (26k)
Diseases  (26k)
Diseases (100k)
Figure 1: Names: Recall vs. Precision
taneously.
7
We introduce a category for symp-
toms, discussed in the next section.
We also introduce a negative category for
learning terms belonging to none of the classes.
As seeds, we use the 10 most frequent NGs
in the corpus, excluding disease and location
names, and generic words for diseases or loca-
tions (\virus," \outbreak," \area").
8
The parameters in these experiments are:
number of seeds = 10 per category; pattern ac-
curacy threshold 
prec
= 0:80; n = m = 5 for
the number of retained patterns and candidates.
The learning curves in Figure 1 show how re-
call and precision for diseases and locations vary
across the iterations. The bold curves show the
result for diseases and locations on the devel-
opment corpus (26K); e.g., by the end, 70% of
diseases (from the recall list of 322 items) were
learned, at 50% precision|half of the learned
names were not on the precision list. On the
100K corpus (with 641 diseases on the recall
list) the precision was only slightly lower.
The precision measures, however, are under-
stated. Because it is not possible to get a full list
for measuring precision, we nd that Nomen is
penalized for nding correct answers. This is a
general problem of type-based evaluation.
To quantify this eect, we manually examined
the disease names learned by Nomen on the de-
velopment corpus and re-introduced those that
7
Locations seeds: United States, Malaysia, Australia,
Belgium, China, Europe, Taiwan, Hong Kong, Singa-
pore, France.
8
The negative seeds were: case, health, day, people,
year, patient, death, number, report, farm.
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
 
Pr
ec
is
io
n
 Disease Names:  Recall 
Dis + Loc + Sym + Other
Diseases (26K), as Figure 1
Enhanced precision list
Figure 2: Eect of Understated Precision
were incorrectly marked as errors, into the pre-
cision list only. The updated graph is shown in
Figure 2; at 70% recall the true precision is 65%.
Note that precision is similarly understated for
all type-based curves in this paper.
Among the re-introduced names there were
99 new diseases which were missed in the man-
ual compilation of reference lists.
9
This is an
encouraging result, since this is ultimately how
Nomen is intended to be used: for discovering
new, previously unknown names.
5 Discussion
5.1 Competing Categories
Figure 3 demonstrates the usefulness of com-
petition among target categories. All curves
show the performance of Nomen on the dis-
ease category, when the algorithm is seeded
only with diseases (the curve labeled Dis), when
seeded with diseases and locations (Dis+Loc),
and with symptoms, and the \other" category.
The curves Dis and Dis+Loc are very similar.
However, when more categories are added, pre-
cision and recall increase dramatically.
When only one category is being learned,
acc(p) = 1:0 for all patterns p. The lack of
an eective accuracy measure causes us to ac-
quire unselective disease name patterns that of-
ten also match non-diseases (e.g., \... X has
been conrmed"). This hurts precision.
9
Examples of new diseases: rinderpest, konzo,
Mediterranean spotted fever, coconut cadang-cadang,
swamp fever, lathyrism, PRRS (for \porcine reproduc-
tive and respiratory syndrome"); locations: Kinta, Ulu
Piah, Melilla, Anstohihy, etc.
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
 
Pr
ec
is
io
n
 Disease Names:  Recall 
Disease names:
Dis + Loc + Sym + Other
Dis + Loc + Other
Dis + Loc
Dis
Figure 3: Diseases: Eect of Competition
Recall also suers, (a) because some patterns
that are more selective (but have lower con-
dence or coverage) are neglected, and (b) be-
cause non-diseases contaminate the seed set and
generate useless patterns.
(Collins and Singer, 1999) also makes use of
competing categories (person, organization, and
location), which cover 96% of all the instances it
set out to classify. In our case, the sought cat-
egories, (diseases and locations), do not cover
the bulk of potential candidates for generalized
names|word sequences matching [ADJ* N+].
Introducing the \negative" category helps us
cover more of the potential candidates. This in
turn boosts the utility of the accuracy measure.
Additional competing categories may help to
prevent a category from \creeping" into an over-
lapping concept. E.g., we had mentioned that
the disease and symptom classes may overlap.
When the target categories include diseases but
not symptoms, Nomen learns some names that
can function as either. This leads to learning of
some patterns which tend to occur with symp-
toms only, resulting in precision errors. Figure 3
shows the improvement in precision from adding
the symptom category.
On the other hand, there may be disadvan-
tages to splitting categories too nely. For ex-
ample, one problem is metonymy among classes
of generalized names. It appears to be distinct
from the problem of ambiguity in PNs, e.g.,
when \Washington" may refer to a person, or a
location. In the case of PNs, there are usually
clues in the context to aord disambiguation.
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
 
Pr
ec
is
io
n
 Recall 
Dis+Sym+Loc+Other:
Locations (100k)
Locations  (26k)
Diseases  (26k)
Diseases (100k)
Figure 4: Token-based, MUC-style Evaluation
In the case of GNs, rather, the nature of ambi-
guity may be related to regular metonymy. For
example, names of agents regularly function as
the name of the disease they cause: \E. coli."
Therefore, in learning agents and diseases sepa-
rately, the algorithm will naturally confound the
two classes, which will inhibit learning. In these
experiments, we learn them as a single class.
It may then be more appropriate to apply an-
other procedure to separate the classes based on
a measure of prevalence of co-occurrence with
the respectively characteristic contexts.
5.2 Evaluation
The results in the preceding gures are not di-
rectly commensurate with those in the men-
tioned literature, e.g., (Strzalkowski and Wang,
1996; Collins and Singer, 1999). This relates to
the token-type dichotomy.
The evaluation in the prior work is token-
based, where the learner gets credit|recall
points|for identifying an instance correctly, for
every time it occurs in the corpus. In our type-
based evaluation, it gets credit only once per
name, no matter how many times it occurs.
We also conducted an instance-based evalua-
tion, more compatible with the mentioned prior
work. We manually tagged all diseases and lo-
cations in a 500-sentence test sub-corpus. Using
the output from the runs in Figure 1 we mea-
sured recall and precision using the standard
MUC NE scoring scheme, shown in Figure 4.
10
10
The sharp dip in the \diseases (100K)" curve is
due to several generic terms that were learned early on;
generics were not tagged in the test corpus.
Iteration Type-Based Instance-Based
0 0.03 0.35
20 0.18 0.68
40 0.31 0.85
60 0.42 0.85
300 0.69 0.86
Table 2: Evaluation of Disease Recall
Table 2 contrasts type-based and instance-
based recall across the iterations. The instance-
based evaluation can hardly distinguish between
an algorithm that learns 31% of the types vs.
one that learns 69% of the types. The algorithm
keeps learning lots of new, infrequent types until
iteration 340, but the instance-based evaluation
does not demonstrate this.
5.3 Current Work
Nomen can be improved in several respects.
The current regular-expression NG pattern is
very simplistic. In its present form, it does not
allow \foot and mouth disease" to be learned,
nor \legionnaires' disease"; this introduces in-
accuracy, since parts of these names are learned
and contaminate the pool.
The current pattern generalization scheme
could be expanded. (LP)
2
generalizes on sur-
face form, case, and semantic information. We
could use, e.g., parts of speech from the tagger,
as a level of generalization between lemmas and
wildcards. A complementary approach would
be to use a NP chunker, to capture longer-
distance relations, in the heads and prepositions
of adjacent phrases. ((Collins and Singer, 1999)
achieves this eect by full parsing.)
We are exploring acquisition of more types of
generalized names|agents and vectors, as well
as people and organizations. What is the eect
of learning possibly related classes simultane-
ously, what happens to the items in their inter-
section, and to what extent they inhibit learn-
ing, remains a practical question.
Acknowledgements
This research is supported by the Defense Advanced
Research Projects Agency as part of the Translin-
gual Information Detection, Extraction and Sum-
marization (TIDES) program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare Sys-
tems Center San Diego, and by the National Science
Foundation under Grant IIS-0081962.
References
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning
name-nder. In Proc. 5th Applied Natural Lan-
guage Processing Conf., Washington, DC.
A. Borthwick, J. Sterling, E. Agichtein, and R. Gr-
ishman. 1998. Exploiting diverse knowledge
sources via maximum entropy in named entity
recognition. In Proc. 6th Workshop on Very Large
Corpora, Montreal, Canada.
F. Ciravegna. 2001. Adaptive information extrac-
tion from text by rule induction and generalisa-
tion. In Proc. 17th Intl. Joint Conf. on AI (IJCAI
2001), Seattle, WA.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classication. In Proc. Joint
SIGDAT Conf. on EMNLP/VLC.
S. Cucerzan and D. Yarowsky. 1999. Language in-
dependent named entity recognition combining
morphological and contextual evidence. In Proc.
Joint SIGDAT Conf. on EMNLP/VLC.
K. Frantzi, S. Ananiadou, and H. Mima. 2000. Au-
tomatic recognition of multi-word terms: the C-
value/NC-value method. Intl. Journal on Digital
Libraries, 2000(3):115{130.
R. Grishman, S. Huttunen, and R. Yangarber. 2002.
Event extraction for infectious disease outbreaks.
In Proc. 2nd Human Lang. Technology Conf.
(HLT 2002), San Diego, CA.
J.S. Justeson and S.M. Katz. 1995. Technical ter-
minology: Some linguistic properties and an algo-
rithm for identication in text. Natural Language
Engineering, 1(1):9{27.
E. Rilo and R. Jones. 1999. Learning dictio-
naries for information extraction by multi-level
bootstrapping. In Proc. 16th Natl. Conf. on AI
(AAAI-99), Orlando, FL.
T. Strzalkowski and J. Wang. 1996. A self-learning
universal concept spotter. In Proc. 16th Intl.
Conf. Computational Linguistics (COLING-96).
A. Ushioda. 1996. Hierarchical clustering of words.
In Proc. 16th Intl. Conf. Computational Linguis-
tics (COLING-96), Copenhagen, Denmark.
T. Wakao, R. Gaizauskas, and Y. Wilks. 1996.
Evaluation of an algorithm for the recognition
and classication of proper names. In Proc. 16th
Int'l Conf. on Computational Linguistics (COL-
ING 96), Copenhagen, Denmark.
R. Yangarber, R. Grishman, P. Tapanainen, and
S. Huttunen. 2000. Automatic acquisition of do-
main knowledge for information extraction. In
Proc. 18th Intl. Conf. Computational Linguistics
(COLING 2000), Saarbrucken, Germany.
R. Yangarber. 2002. Acquisition of domain knowl-
edge. In M.T. Pazienza, editor, Information Ex-
traction. Springer-Verlag, LNAI, Rome.
Complexity of Event Structure in IE Scenarios
Silja Huttunen, Roman Yangarber, Ralph Grishman
Courant Institute of Mathematical Sciences
New York University
fsilja,roman,grishmang@cs.nyu.edu
Abstract
This paper presents new Information Extrac-
tion scenarios which are linguistically and struc-
turally more challenging than the traditional
MUC scenarios. Traditional views on event
structure and template design are not adequate
for the more complex scenarios.
The focus of this paper is to show the com-
plexity of the scenarios, and propose a way to
recover the structure of the event. First we
identify two structural factors that contribute
to the complexity of scenarios: the scattering
of events in text, and inclusion relationships
between events. These factors cause di?culty
in representing the facts in an unambiguous
way. Then we propose a modular, hierarchi-
cal representation where the information is split
in atomic units represented by templates, and
where the inclusion relationships between the
units are indicated by links. Lastly, we discuss
how we may recover this representation from
text, with the help of linguistic cues linking the
events.
1 Introduction
Information Extraction (IE) is a technology
used for locating and extracting specic pieces
of information from texts. The knowledge bases
are customized for each new topic or scenario,
as dened by ll rules that state which facts are
needed for constitution of an extractable event.
A scenario is a set of predened facts to be ex-
tracted from a large text corpus, such as news
articles, and organized in output templates.
Our experience with customizing our IE sys-
tem called Proteus (Grishman, 1997; Grishman
et al, 2002) to new scenarios suggests that the
lexical and structural properties of the scenario
aect the performance of the system. To make
an IE system exible for tasks of varying com-
plexity, it is essential to conduct a linguistic
analysis of the texts relating to dierent sce-
narios.
In this paper, we focus on the Infectious Dis-
ease Outbreak scenario (Grishman et al, 2002),
and the Natural Disaster scenario (Hirschman
et al, 1999) collectively called the \Nature" sce-
narios. During the customization of the IE sys-
tem to the Nature scenarios, we encountered
problems that did not arise in the traditional
scenarios of the Message Understanding Con-
ferences (MUCs). This included, in particular,
delimiting the scope of a single event and orga-
nizing the events into templates.
We identify two structural factors that con-
tribute to the complexity of a scenario: rst, the
scattering of events in text, and second, inclu-
sion relationships between events. These factors
cause di?culty in representing the facts in an
unambiguous way. We proposed that such event
relationships can be described with a modular,
hierarchical model (Huttunen et al, 2002).
The phenomenon of inclusion is widespread in
the Nature scenarios, and the types of inclusions
are numerous. In this paper we present prelim-
inary results obtained from our corpus analysis,
with a classication and distribution of inclu-
sion relationships. We discuss the potential for
recovery of these inclusions from text with the
help of the linguistic cues, of which we show
some examples.
This paper will argue that a thorough linguis-
tic analysis of the corpus is needed to help recov-
ery of the complex event structure in the text.
In the next section we give a brief description
of the scenarios we are investigating. In section
3 we review the problems of scattering, inclusion
and event denition, and propose a method for
representing template structure. In section 4
we present examples of the linguistic cues to
Disaster Date Location VictimDead Damage
tornado Sunday night Georgia one person motel
Disease Date Location VictimDead VictimSick
Ebola since September Uganda 156 people -
Table 1: Disaster Event and Disease Event
recover the complex event structure, followed
by discussion in section 5.
2 Background
2.1 Information Extraction
Our IE system has been previously customized
for several news topics, as part of the MUC
program, such as Terrorist Attacks (MUC,
1991; MUC, 1992) and Management Succession
(MUC, 1995; Grishman, 1995). Subsequently to
the MUCs, we customized Proteus to extract,
among other scenarios, Corporate Mergers and
Acquisitions, Natural Disasters and Infectious
Disease Outbreaks.
We contrasted the Nature scenarios with the
earlier MUC scenarios (Huttunen et al, 2002).
The \traditional" template structure is such
that all the information about the main event
can be presented within a single template. The
main events form separate instances, and there
are no links between them. Management Suc-
cession scenario presents a slightly more com-
plicated template structure, but it is still possi-
ble to present in one template. The traditional
representation is not adequate to represent the
complex structure of the Nature scenarios.
In the next section, we give a short descrip-
tion of the Nature scenarios.
2.2 Scenarios
For the Natural Disaster scenario, the task
is to nd occurrences of disasters (earthquakes,
storms, etc.) around the world, as reported in
newspaper articles. The information extracted
for each disaster should include the type of dis-
aster, date and location of the occurrence, and
the amount of human or material damage.
An example of a Natural Disaster template
is in table 1, extracted from the following news
fragment:
\[...] tornadoes that destroyed a Geor-
gia motel and killed one person in a
mobile home Sunday night."
For the Infectious Disease Outbreak sce-
nario, the task is to track the spread of epi-
demics of infectious diseases around the world.
The system has to nd the name of the disease,
the time and location of the outbreak, the num-
ber of victims (infected and dead), and type of
victims (e.g., human or animal). The next ex-
ample is a fragment of a disease outbreak report,
and the extracted facts are shown in table 1.
\Ebola fever has killed 156 people, [...],
in Uganda since September."
3 Structure of Events
The complex event structure in Nature scenar-
ios is partly due to the fact that the events are
reported in a scattered manner in the text.
By scattering of events we mean that their
components are not close to each other in the
text, and a typical text contains several related
events. This is partly because the articles are
often in a form of an update, where the latest
reported damages contribute to the total dam-
ages reported earlier, over several locations and
over dierent time spans.
The example in table 2 illustrates scattering
in the Disease scenario. It is a fragment of an
update about a cholera epidemic in Sudan, from
the World Health Organization's (WHO) web
report. The locations are highlighted in italics
and the victim counts are in boldface, to show
the scattering. In this example there are six
separate mentions|partial descriptions of the
event in text|giving the number of infected
and dead victims, in Sudan, and in two loca-
tions within Sudan. Paragraph (1) reports the
number of victims in Sudan, 2549 infected, and
186 dead. In paragraph (2), the focus is shifted
to another location in Sudan, and new numbers
are reported. Paragraph (3) gives the respective
(0) Meningococcal in Sudan
(1) A total of 2 549 cases of meningococcal disease, of which 186 were fatal, was reported to the
national health authorities between 1 January and 31 March 2000.
(2) Bahar aj Jabal State has been most aected to date, with 1 437 cases (including 99 deaths)
reported in the Juba city area.
(3) Other States aected include White Nile (197 cases, 15 deaths), [...]
Table 2: Example of a Disease Outbreak Report
Disease Location Infected Dead
Meningococcal Sudan 2549 186
Bahar aj Jabal State 1437 99
White Nile 197 15
Table 3: Facts from Disease Outbreak Report
numbers for yet another location in Sudan. The
mentions are summarized in table 3.
3.1 Inclusion Relationships
As we frequently observe in the Nature scenar-
ios, the information in the various mentions in
table 2 is overlapping, and the mentions par-
tially include each other.
For example, the numbers for infected victims
in paragraph (2) and (3), contribute to the total
number of infected cases in paragraph (1). The
extraction system should be able to extract all
the numbers for this text. The problem is how
to group these mentions into a template in an
unambiguous and coherent way. It is impossi-
ble to represent an event with overlapping in-
formation in a single template, since it consists
of multiple numbers of victims in several areas
and several time intervals.
For the purpose of handling this phenomenon,
we rst introduce a distinction between out-
breaks and incidents. An incident is a short de-
scription, or a mention, of one occurrence that
relates to an outbreak. It covers a single specic
span of time in a single specic area. An out-
break takes place over a longer period of time,
and possibly over wider geographical area: it
consists of multiple incidents.
In general, one incident may include others,
which give further detailed information.
Therefore, we analyze the news fragment in
table 2 as containing six incidents, with two
types of inclusions: rst, inclusion by status,
where the dead count contributes to the infected
count of the same area, and second, inclusion by
location, where the numbers of infected cases in
Bahar aj Jabal State, in paragraph (2), and in
White Nile, in (3), contribute to the infected
count in Sudan, in paragraph (1).
The Natural Disaster scenario poses further
complications for this schema. The scattering
is complicated by the relationship of causation:
the main disaster triggers derivative disasters
(sub-disasters), which in turn may cause dam-
ages that contribute to the overall damage. This
is illustrated by the news fragment in table 4,
from the New York Times. Names of disasters
are in bold, and the damages are italicized.
In table 4, paragraph (1), a disaster includes
rain and winds, which cause ooding. In para-
graph (3), the human damages caused by snow
are included in the total human damages caused
by the storm in (2). The derivative disasters
and their damages often take place in several lo-
cations, appearing relatively far in the text from
the rst mention of the main disaster. The -
nal logical representation of the event should be
such that the eects of the sub-disasters could
be traced back to the main event.
The following is a summary of the inclusion
relationships found in the two Nature scenarios:
 location: e.g, victim count in one city con-
tributes to the victim count in the whole
country.
 time: e.g. victim count for an update re-
port contributes to the overall victim count
since the beginning of the outbreak.
 status: dead or sick count is included in
(1) A brutal northeaster thrashed the Eastern Seaboard again Thursday with cold, slicing rain
and strong winds that caused ooding in coastal areas of New Jersey and Long Island. [...]
(2) Elsewhere along the East Coast, 19 deaths have been attributed to the storm since it began
on Monday.
(3) The 19 deaths include ve in accidents on snowy roads in Kentucky and two in Indiana. [...]
Table 4: Example of Disaster Reporting
the infected count, as in paragraph (2) of
table 2.
 victim type or descriptor: e.g., \people" in-
cludes \health workers", and \children".
 disease name (Disease scenario): e.g., the
number of Hepatitis C cases may be in-
cluded in the number of Hepatitis cases.
 disaster (Disaster scenario): e.g., damages
caused by rain may be included in the dam-
ages caused by rain and winds.
 causation (Disaster scenario): a disaster
can trigger derivative disasters.
3.2 Type and Distribution of Inclusions
To investigate the extent of inclusions and their
distribution by type, we analyzed 40 documents
related to Nature scenarios.
1
To conrm the feasibility and applicability of
this approach, we manually tagged the inclu-
sion relationships present in these documents.
Table 5 shows the number of incidents found in
the documents, as well as the number and the
types of inclusion. There are also multiple in-
clusions: e.g., infected health workers in a town
in Uganda are included in the total number of
infected people in the whole country: this is in-
clusion by both case-descriptor and location.
Multiple inheritance also occurs: in table 2,
the deaths in Bahar aj Jabal State contribute
to the infected count in that state, as well as to
the total number of deaths in Sudan. However,
in table 5, we show only the inclusion in the
immediately preceding parent.
3.3 Hierarchical Template Structure
Our proposed solution is to have a separate tem-
plate for each incident. Once we have broken
1
The training corpus was used to evaluate the per-
formance of our IE system on these tasks. For the Dis-
aster scenario we analyzed a total of 14 reports from
NYT, ABC, APW, CNN, VOA and WSJ. For Disease
Outbreaks, a total of 26 documents from NYT, Promed,
WHO, and ABC.
Scenario Disease Disaster
Documents 26 14
Words 9 500 6500
Incidents 125 112
Inclusions 57 81
time 6 6
location 19 20
status 19 1
case-descriptor 6 1
case-desc/location 3 {
disease 1 {
causation { 19
causation/location { 11
causation/time { 3
time/location { 7
disaster { 5
disaster/location { 2
damage { 4
others 3 2
Table 5: Type and Number of Inclusion
down the information into smaller incident tem-
plates, the inclusion relationship between them
is indicated by event pointers. This approach
makes it possible to represent the information
in a natural and intuitive way.
The nal template for the Infectious Disease
scenario is shown in table 6. Note that there is
a separate slot indicating the parent incident.
Disease Name
Date
Location
Victim Number
Victim Descriptor
Victim Status
Victim Type
Parent Event
Table 6: Infectious Disease Template
Figure 1: Infectious Disease Outbreak
Figure 2: Natural Disaster
Figure 1 is a graphical representation of the
inclusion relationships among the incidents ex-
tracted from the Disease report in table 2. The
gure shows the main incident with several sub-
incidents. Two of the sub-incidents have, in
turn, sub-incidents. The types of inclusions are
shown in the last row.
Figure 2 shows a graphical representation of
inclusion by causation in Natural Disaster sce-
nario. The incidents are extracted from ta-
ble 4.
2
There is a causation relationship be-
tween the incidents. It is important to recover
the long causation chains from the text.
As a result, the templates are simple, but
2
Note that the northeaster is not in causation rela-
tionship with storm, which began on Monday. The dam-
ages that the synonymous northeaster caused, are from
the following Thursday.
there are typically many templates per docu-
ment. The separation of incidents aects the
process of extraction, since we can now focus
on looking for smaller atomic pieces rst. Then
we must address the problem of linking together
related incidents as a separate problem in the
overall process of IE.
4 Linguistic cues
The process of tracking the inclusion relation-
ships between the incidents is not trivial. A
human reader uses the cohesive devices in the
text to construct the connections between parts
of text (see e.g., (Halliday and Hasan, 1976; Hal-
liday, 1985)). Finding the relationship between
incidents may be a less complex task than track-
ing cohesion through an entire text or discourse.
Our task is limited to nding the cohesive de-
vices connecting a small set of pre-dened facts,
that may occur nearby within one sentence, or
are separated by one or more sentence bound-
aries. Our goal is to locate the cues in the text,
and use them to automatically recover these re-
lationships.
An example of a linguistic cue is in the fol-
lowing fragment of an update from table 4:
Elsewhere along the East Coast, 19
deaths have been attributed to the
storm [...]
Elsewhere indicates a shift in the focus from
one location to another and there is probably
no inclusion between the following and immedi-
ately preceding mention of the damages.
We have identied several linguistic cues that
signal the presence or absence of an inclusion
relationship between two incidents. These cues
can be one of following types:
 Specic lexical items, which can be e.g.,
adverbs, verbs, prepositions, connectives.
Elsewhere in the previous example implies
that damages caused by the following dis-
aster do not contribute to the damages of
the immediately preceding disaster.
 Two expressions in separate incidents
which are related in the scenario-specic
concept hierarchy, may indicate the pres-
ence and also the direction of an inclusion,
e.g., health worker is included in people;
names of plants, animals and terms refer-
ring to human beings, are hyponyms of vic-
tim.
 Locative or temporal expressions that are
in a hierarchical relationship in a location
hierarchy or in the implicit time hierarchy,
often indicate presence or direction of in-
clusion.
 Elliptical elements create cohesion. Ellipsis
indicates the presence of a parent incident
earlier in the text. In paragraph (3) of table
4, in the parent incident we observe a case
descriptor, deaths, which is elided in the
two sub-incidents.
 Anaphora: anaphoric reference usually in-
dicates the absence of an inclusion between
two incidents, merging into one. For exam-
ple, in table 4, paragraph (3), the 19 deaths
is coreferential with 19 deaths caused by the
storm in paragraph (2).
 Coordination tends to indicate the absence
of inclusion relationship. For example,
when two incidents are conjoined by and
and do not share information about loca-
tion or time, there is typically no inclusion.
However, there are cases where other cues
override this general tendency.
These cues often do not appear in isolation,
and they may interact.
We give an example of three lexical items and
their role as an indicator of inclusion in the In-
fectious Disease Outbreak Scenario. Consider
the preposition with
3
, the participle including
and the nite verb include.
\More than 500 cases of dengue hem-
orrhagic fever were reported in Mexico
last year, with 30 deaths, Ruiz said."
The 30 deaths are included in the 500 cases.
The direction of the inclusion is reversed in the
following example:
"Disease has killed 10 persons, with
242 cases having already been re-
ported."
The latter incident includes the former. Here
additional cues are provided by the concept hi-
erarchy, and the numbers: a smaller number
cannot include a larger one.
The following illustrates the participle includ-
ing as cue:
Ebola fever has killed 156 people, in-
cluding 14 health workers, in Uganda
since September.
The incidents are connected by including,
which also indicates the direction explicitly. Ad-
ditional information is obtained from the case-
descriptors, related in the concept hierarchy.
The context for such \trigger" words as they
indicate inclusion, is that the trigger appears
between two incidents, preceding and preceded
3
In the case of with we look only at free prepositions,
that is, those not bound to a preceding verb (Biber et
al., 1999).
by a quantied NP
4
and optional phrases or
items from the concept hierarchy.
Q fcase-descriptor j statusg [reported
j get sick j time j location j disease] [,]
trigger Q fcase-descriptor j statusg
These triggers can indicate inclusion also in-
side a parenthetical phrase, preceding a quanti-
ed NP, as in table 2 in paragraph (2).
The trigger include (as a nite verb) functions
similarly, but can also occur between sentences:
[...] the Ugandan Ministry of Health
has reported [...] 370 cases and 140
deaths. This gure includes 16 new
conrmed cases in Gulu [...]
In our training corpus, when these cue words
occurred in this context, they consistently indi-
cated an event inclusion relation.
5 Discussion
Complexity of a scenario seems to depend of
multiple factors. The notion of complexity,
however, has not been investigated in great
depth. Some research on this was done by
(Bagga and Biermann, 1997; Bagga, 1997),
classifying scenarios according to di?culty by
counting distances between \components" of an
event in the text. In this way it attempts to ac-
count for variation in performance across the
MUC scenarios.
Our analysis suggests that the type and
amount of inclusion relationships depend on
the nature of the topic. In such scenarios as
Management Succession and Corporate Acqui-
sitions, an event usually occurs at one specic
point in time. By contrast, the Nature events
typically take place across a span of time and
space. As the event \travels" and evolves, its
manifestations are reported in a piecewise fash-
ion, sometimes on an hour-by-hour basis.
An extensive linguistic analysis of the cor-
pus is necessary to resolve these complex is-
sues. For evaluation and training, we are build-
ing test and training corpora, totaling 70 doc-
uments and annotated with inclusion relation-
ships.
4
Here the case descriptor or status can be elided:
however, one of quantiers should have a case descriptor
or a status.
Acknowledgments
This research is supported by the Defense Advanced
Research Projects Agency as part of the Translin-
gual Information Detection, Extraction and Sum-
marization (TIDES) program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare Sys-
tems Center San Diego, and by the National Science
Foundation under Grant IIS-0081962.
This paper does not necessarily reect the posi-
tion or the policy of the U.S. Government.
References
A. Bagga and A. W. Biermann. 1997. Analyzing
the complexity of a domain with respect to an
information extraction task. In Proc. 10th Intl.
Conf. on Research on Computational Linguistics
(ROCLING X).
A. Bagga. 1997. Analyzing the performance of
message understanding systems. In Proc. Natu-
ral Language Processing Pacic Rim Symposium
(NLPRS'97).
D. Biber, S. Johansson, G. Leech, S. Conrad, and
E. Finegan. 1999. Longman Grammar of Spoken
and Written English. Longman.
R. Grishman, S. Huttunen, and R. Yangarber. 2002.
Real-time event extraction for infectious disease
outbreaks. In Proc. HLT 2002: Human Language
Technology Conf., San Diego, CA.
R. Grishman. 1995. The NYU system for MUC-
6, or where's the syntax? In Proc. 6th Message
Understanding Conf. (MUC-6), Columbia, MD.
Morgan Kaufmann.
R. Grishman. 1997. Information extraction: Tech-
niques and challenges. In M. T. Pazienza, editor,
Information Extraction. Springer-Verlag, Lecture
Notes in Articial Intelligence, Rome.
M.A.K. Halliday and R. Hasan. 1976. Cohesion in
English. Longman, London.
M.A.K. Halliday. 1985. Introduction to Functional
Grammar. Edward Arnold, London.
L. Hirschman, E. Brown, N. Chinchor, A. Douthat,
L. Ferro, R. Grishman, P. Robinson, and B. Sund-
heim. 1999. Event99: A proposed event indexing
task for broadcast news. In Proc. DARPA Broad-
cast News Workshop, Herndon, VA.
S. Huttunen, R. Yangarber, and R. Grishman. 2002.
Diversity of scenarios in information extraction.
In Proc. 3rd Intl. Conf. of Language Resources
and Evaluation, LREC-2002, Las Palmas de Gran
Canaria, Spain.
1991. Proc. 3th Understanding Conf. (MUC-3).
Morgan Kaufmann.
1992. Proc. 4th Message Understanding Conf.
(MUC-4). Morgan Kaufmann.
1995. Proc. 6th Message Understanding Conf.
(MUC-6). Morgan Kaufmann.
Counter-Training in Discovery of Semantic Patterns
Roman Yangarber
Courant Institute of Mathematical Sciences
New York University
roman@cs.nyu.edu
Abstract
This paper presents a method for unsu-
pervised discovery of semantic patterns.
Semantic patterns are useful for a vari-
ety of text understanding tasks, in par-
ticular for locating events in text for in-
formation extraction. The method builds
upon previously described approaches to
iterative unsupervised pattern acquisition.
One common characteristic of prior ap-
proaches is that the output of the algorithm
is a continuous stream of patterns, with
gradually degrading precision.
Our method differs from the previous pat-
tern acquisition algorithms in that it intro-
duces competition among several scenar-
ios simultaneously. This provides natu-
ral stopping criteria for the unsupervised
learners, while maintaining good preci-
sion levels at termination. We discuss the
results of experiments with several scenar-
ios, and examine different aspects of the
new procedure.
1 Introduction
The work described in this paper is motivated by
research into automatic pattern acquisition. Pat-
tern acquisition is considered important for a variety
of ?text understanding? tasks, though our particular
reference will be to Information Extraction (IE). In
IE, the objective is to search through text for enti-
ties and events of a particular kind?corresponding
to the user?s interest. Many current systems achieve
this by pattern matching. The problem of recall, or
coverage, in IE can then be restated to a large ex-
tent as a problem of acquiring a comprehensive set
of good patterns which are relevant to the scenario
of interest, i.e., which describe events occurring in
this scenario.
Among the approaches to pattern acquisition
recently proposed, unsupervised methods1 have
gained some popularity, due to the substantial re-
duction in amount of manual labor they require. We
build upon these approaches for learning IE patterns.
The focus of this paper is on the problem of con-
vergence in unsupervised methods. As with a variety
of related iterative, unsupervised methods, the out-
put of the system is a stream of patterns, in which
the quality is high initially, but then gradually de-
grades. This degradation is inherent in the trade-off,
or tension, in the scoring metrics: between trying
to achieve higher recall vs. higher precision. Thus,
when the learning algorithm is applied against a ref-
erence corpus, the result is a ranked list of patterns,
and going down the list produces a curve which
trades off precision for recall.
Simply put, the unsupervised algorithm does not
know when to stop learning. In the absence of a
good stopping criterion, the resulting list of patterns
must be manually reviewed by a human; otherwise
one can set ad-hoc thresholds, e.g., on the number
of allowed iterations, as in (Riloff and Jones, 1999),
or else to resort to supervised training to determine
such thresholds?which is unsatisfactory when our
1As described in, e.g., (Riloff, 1996; Riloff and Jones, 1999;
Yangarber et al, 2000).
goal from the outset is to try to limit supervision.
Thus, the lack of natural stopping criteria renders
these algorithms less unsupervised than one would
hope. More importantly, this lack makes the al-
gorithms difficult to use in settings where training
must be completely automatic, such as in a general-
purpose information extraction system, where the
topic may not be known in advance.
At the same time, certain unsupervised learning
algorithms in other domains exhibit inherently natu-
ral stopping criteria. One example is the algorithm
for word sense disambiguation in (Yarowsky, 1995).
Of particular relevance to our method are the algo-
rithms for semantic classification of names or NPs
described in (Thelen and Riloff, 2002; Yangarber et
al., 2002).
Inspired in part by these algorithms, we introduce
the counter-training technique for unsupervised pat-
tern acquisition. The main idea behind counter-
training is that several identical simple learners run
simultaneously to compete with one another in dif-
ferent domains. This yields an improvement in pre-
cision, and most crucially, it provides a natural indi-
cation to the learner when to stop learning?namely,
once it attempts to wander into territory already
claimed by other learners.
We review the main features of the underlying un-
supervised pattern learner and related work in Sec-
tion 2. In Section 3 we describe the algorithm; 3.2
gives the details of the basic learner, and 3.3 in-
troduces the counter-training framework which is
super-imposed on it. We present the results with and
without counter-training on several domains, Sec-
tion 4, followed by discussion in Section 5.
2 Background
2.1 Unsupervised Pattern Learning
We outline those aspects of the prior work that are
relevant to the algorithm developed in our presenta-
tion.
  We are given an IE scenario

, e.g., ?Man-
agement Succession? (as in MUC-6). We have a
raw general news corpus for training, i.e., an un-
classified and un-tagged set of documents  . The
problem is to find a good set of patterns in 

, which
cover events relevant to

.
We presuppose the existence of two general-
purpose, lower-level language tools?a name recog-
nizer and a parser. These tools are used to extract all
potential patterns from the corpus.
  The user provides a small number of seed pat-
terns for

. The algorithm uses the corpus to itera-
tively bootstrap a larger set of good patterns for

.
  The algorithm/learner achieves this bootstrap-
ping by utilizing the duality between the space of
documents and the space of patterns: good extrac-
tion patterns select documents relevant to the chosen
scenario; conversely, relevant documents typically
contain more than one good pattern. This duality
drives the bootstrapping process.
  The primary aim of the learning is to train a
strong recognizer  for

;  is embodied in the set
of good patterns. However, as a result of training
 , the procedure also produces the set 
	 of doc-
uments that it deems relevant to

?the documents
selected by  .
  Evaluation: to evaluate the quality of discov-
ered patterns, (Riloff, 1996) describes a direct eval-
uation strategy, where precision of the patterns re-
sulting from a given run is established by manual re-
view. (Yangarber et al, 2000) uses an automatic but
indirect evaluation of the recognizer  : they retrieve
a test sub-set   	    from the training corpus
and manually judge the relevance of every document
in  	  ; one can then obtain standard IR-style recall
and precision scores for  
	 relative to   	  .
In presenting our results, we will discuss both
kinds of evaluation.
The recall/precision curves produced by the indi-
rect evaluation generally reach some level of recall
at which precision begins to drop. This happens be-
cause at some point in the learning process the al-
gorithm picks up patterns that are common in

, but
are not sufficiently specific to

alone. These pat-
terns then pick up irrelevant documents, and preci-
sion drops.
Our goal is to prevent this kind of degradation, by
helping the learner stop when precision is still high,
while achieving maximal recall.
2.2 Related Work
We briefly mention some of the unsupervised meth-
ods for acquiring knowledge for NL understanding,
in particular in the context of IE. A typical archi-
tecture for an IE system includes knowledge bases
(KBs), which must be customized when the system
is ported to new domains. The KBs cover different
levels, viz. a lexicon, a semantic conceptual hierar-
chy, a set of patterns, a set of inference rules, a set
of logical representations for objects in the domain.
Each KB can be expected to be domain-specific, to
a greater or lesser degree.
Among the research that deals with automatic ac-
quisition of knowledge from text, the following are
particularly relevant to us. (Strzalkowski and Wang,
1996) proposed a method for learning concepts be-
longing to a given semantic class. (Riloff and Jones,
1999; Riloff, 1996; Yangarber et al, 2000) present
different combinations of learners of patterns and
concept classes specifically for IE.
In (Riloff, 1996) the system AutoSlog-TS learns
patterns for filling an individual slot in an event tem-
plate, while simultaneously acquiring a set of lexical
elements/concepts eligible to fill the slot. AutoSlog-
TS, does not require a pre-annotated corpus, but
does require one that has been split into subsets that
are relevant vs. non-relevant subsets to the scenario.
(Yangarber et al, 2000) attempts to find extrac-
tion patterns, without a pre-classified corpus, start-
ing from a set of seed patterns. This is the ba-
sic unsupervised learner on which our approach is
founded; it is described in the next section.
3 Algorithm
We first present the basic algorithm for pattern ac-
quisition, similar to that presented in (Yangarber et
al., 2000). Section 3.3 places the algorithm in the
framework of counter-training.
3.1 Pre-processing
Prior to learning, the training corpus undergoes sev-
eral steps of pre-processing. The learning algorithm
depends on the fundamental redundancy in natural
language, and the pre-processing the text is designed
to reduce the sparseness of data, by reducing the ef-
fects of phenomena which mask redundancy.
Name Factorization: We use a name classifier to
tag all proper names in the corpus as belonging to
one of several categories?person, location, and or-
ganization, or as an unidentified name. Each name
is replaced with its category label, a single token.
The name classifier also factors out other out-of-
vocabulary (OOV) classes of items: dates, times,
numeric and monetary expressions. Name classifi-
cation is a well-studied subject, e.g., (Collins and
Singer, 1999). The name recognizer we use is based
on lists of common name markers?such as personal
titles (Dr., Ms.) and corporate designators (Ltd.,
GmbH)?and hand-crafted rules.
Parsing: After name classification, we apply a gen-
eral English parser, from Conexor Oy, (Tapanainen
and Ja?rvinen, 1997). The parser recognizes the
name tags generated in the preceding step, and treats
them as atomic. The parser?s output is a set of syn-
tactic dependency trees for each document.
Syntactic Normalization: To reduce variation in
the corpus further, we apply a tree-transforming pro-
gram to the parse trees. For every (non-auxiliary)
verb heading its own clause, the transformer pro-
duces a corresponding active tree, where possi-
ble. This converts for passive, relative, subordinate
clauses, etc. into active clauses.
Pattern Generalization: A ?primary? tuple is ex-
tracted from each clause: the verb and its main ar-
guments, subject and object.
The tuple consists of three literals [s,v,o]; if
the direct object is missing the tuple contains in its
place the subject complement; if the object is a sub-
ordinate clause, the tuple contains in its place the
head verb of that clause.
Each primary tuple produces three generalized tu-
ples, with one of the literals replaced by a wildcard.
A pattern is simply a primary or generalized tuple.
The pre-processed corpus is thus a many-many map-
ping between the patterns and the document set.
3.2 Unsupervised Learner
We now outline the main steps of the algorithm, fol-
lowed by the formulas used in these steps.
1. Given: a seed set of patterns, expressed as pri-
mary or generalized tuples.
2. Partition: divide the corpus into relevant
vs. non-relevant documents. A document  is
relevant?receives a weight of 1?if some seed
matches  , and non-relevant otherwise, receiving
weight 0. After the first iteration, documents are
assigned relevance weights between  and  . So
at each iteration, there is a distribution of relevance
weights on the corpus, rather than a binary partition.
3. Pattern Ranking: Every pattern appearing in
a relevant document is a candidate pattern. Assign
a score to each candidate; the score depends on how
accurately the candidate predicts the relevance of a
document, with respect to the current weight distri-
bution, and on how much support it has?the total
wight of the relevant documents it matches in the
corpus (in Equation 2). Rank the candidates accord-
ing to their score. On the  -th iteration, we select the
pattern  most correlated with the documents that
have high relevance. Add   to the growing set of
seeds 
ffProceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 29?37,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Assessment of Utility
in Web Mining for the Domain of Public Health
Peter von Etter, Silja Huttunen, Arto Vihavainen,
Matti Vuorinen and Roman Yangarber
Department of Computer Science
University of Helsinki, Finland
First.Last@cs.helsinki.fi
Abstract
This paper presents ongoing work on applica-
tion of Information Extraction (IE) technology
to domain of Public Health, in a real-world
scenario. A central issue in IE is the quality
of the results. We present two novel points.
First, we distinguish the criteria for quality:
the objective criteria that measure correctness
of the system?s analysis in traditional terms
(F-measure, recall and precision), and, on the
other hand, subjective criteria that measure the
utility of the results to the end-user.
Second, to obtain measures of utility, we build
an environment that allows users to interact
with the system by rating the analyzed con-
tent. We then build and compare several clas-
sifiers that learn from the user?s responses to
predict the relevance scores for new events.
We conduct experiments with learning to pre-
dict relevance, and discuss the results and their
implications for text mining in the domain of
Public Health.
1 Introduction
We describe an on-going project for text mining in
the domain of Public Health. The aim of the project
is to build a system for providing decision support
to Public Health (PH) professionals and officials, in
the task of Epidemic Surveillance.
Epidemic surveillance may be sub-divided into
indicator-based vs. event-based surveillance, (Hart-
ley et al, 2010). Whereas the former is based on
structured, quantitative data, which is collected, e.g.,
from national or international clinical laboratories
or databases, and is of reliable quality, the latter
is much more noisy, and relies on ?alert and ru-
mour scanning?, particularly from open-source me-
dia, such as on-line news sites. While the latter
kind of information sources are less reliable over-
all, they nonetheless constitute a crucial channel of
information in PH. This is because the media are ex-
tremely adept at picking up isolated cases and weak
signals?which may be indicative of emergence of
important events, such as an incipient epidemic or
critical change in a public-health situation?and in
many cases they can do so much more swiftly than
official channels. National and supra-national (e.g.,
European-level) Health Authorities require timely
information about threats posed to the public by
emerging infectious diseases and epidemics. There-
fore, these Agencies rely on media-monitoring as a
matter of routine, on a continual basis as part of their
day-to-day operations.
The system described in this paper, PULS, is de-
signed to support Epidemic Surveillance by moni-
toring open-source media for reports about events of
potential significance to Public Health (Yangarber
and Steinberger, 2009). We focus in this paper on
news articles mentioning incidents of infectious dis-
eases. The system does not make decisions, but pro-
vides decision support, by filtering massive volumes
of information and trying to identify those cases that
should be brought to the attention of epidemic intel-
ligence officers (EIO)?public health specialists en-
gaged in epidemic surveillance.
This is an inter-disciplinary effort. The system
builds on methods from text mining and computa-
tional linguistics to identify the items of potential
interest (Grishman et al, 2003). The EIOs, on the
other hand, are medical professionals, and are gen-
erally not trained in computational methods. There-
fore the tools that they use must be intuitive and must
29
not overwhelm the user with volume or complexity.
A convenient baseline for comparison is keyword-
based search, as provided by search engines and
news aggregators. Systems that rely on keyword-
matching to find articles related to infectious threats
and epidemics quickly overwhelm the user with a
vast amount of news items, much of which is noise.
We have tuned PULS, the ?Pattern-based Under-
standing and Learning System,? to support Epidemic
Surveillance in several phases. PULS is a collabo-
rative effort with MedISys, a system for gathering
epidemic intelligence built by the European Com-
mission (EC) at the Joint Research Centre (JRC)
in Ispra, Italy. First, MedISys finds news articles
from thousands of on-line sources around the world,
identifies articles potentially relevant to Epidemic
Surveillance, using a broad keyword-based Web
search, and sends them via an RSS feed to PULS
on a continual basis. Second, PULS employs ?fact-
finding? technology, Information Extraction (IE), to
determine exactly what happened in each article:
who was affected by what disease/condition, where
and when?creating a structured record that is stored
in the database. Articles that do not trigger cre-
ation of a database record are discarded. A third
component then determines the relevance of the se-
lected articles?and cases that they describe?to the
domain of Public Health, specifically to Epidemic
Surveillance.
Traditionally in IE research, performance has
been measured in terms of formal correctness?how
accurately the system is able to analyze the article
(Hirschman, 1998). In this paper we argue the need
for other measures of performance for text mining,
using as a case study our application of Web mining
to the domain of Public Health. In the next section,
we lay down criteria for judging quality, and present
the approach taken in our system. Section 3 out-
lines the organisation of the system, and Section 4
presents in detail our experiments with automatic as-
signment of relevance scores. In the final section we
discuss the results and outline next steps.
2 Criteria for quality
In this section we take a critical view at traditional
measures of quality, in text analysis in general, and
IE in particular. What defines quality most appropri-
ately for our application, and how should we mea-
sure quality? We propose the following taxonomy
of quality in our context:
? Objective: system?s perspective
? Correctness
? Confidence
? Subjective: user?s perspective
? Utility or relevance
? Reliability
At the top level, we distinguish objective vs. sub-
jective measures. Most IE research has focused on
correctness over the last two decades, e.g., in the
MUC and ACE initiatives (Hirschman, 1998; ACE,
2004). Correctness is a measure of how accurately
the system extracts the semantics from an article
of text, in terms of matching the system?s answers
to a set of answers pre-defined by human annota-
tors. In our context, a set of articles is annotated
with a ?gold-standard? set of database records, each
record containing fields like: the name of the dis-
ease/infectious agent, the location/country of the in-
cident, the date of the incident, the number of vic-
tims, whether they are human or animal, whether
they survived, etc. Then the system?s response can
be compared to the gold standard and correctness
can be computed in terms of recall and precision,
F-measure, accuracy, etc.?counting how many of
the fields in each record were correctly extracted.
This approach to quality is similar to the approach
taken in other areas of computational linguistics:
how many structures in the text were correctly iden-
tified, how many were missed, and how many spuri-
ous structures were introduced.
Confidence has been studied as well, to estimate
the probability of the correctness of the system?s an-
swer, e.g., in (Culotta and McCallum, 2004). Our
system computes confidence using discourse-level
cues, (Huttunen et al, 2002): e.g., confidence de-
creases as the distance between event trigger and
event attributes increases?the sentence that men-
tions that someone has fallen ill or died is far from
the mention of the disease. Confidence also de-
pends on uniqueness of attributes?e.g., if a doc-
ument mentions only one country, the system has
30
more confidence that an event referring to this coun-
try is correct.
On the subjective side, utility, or relevance, asks
how useful the result is to the user. There are several
points to note. First, it is clearly a highly subjective
measure, not easy to capture in exact terms. Sec-
ond, it is ?orthogonal? to correctness in the sense
that from the user?s perspective utility matters irre-
spective of correctness. For example, an extracted
case can be 100% correct, yet have very low utility
to the user, (for the task of epidemic surveillance)?
a perfectly extracted event that happened too long
ago would not matter in the current context. Con-
versely, every slot in the record may be extracted
erroneously, and yet the event may be of great im-
portance and value to the user. We focus specifically
on relevance vs. correctness.
Given the current performance ?ceilings? of 70-
80% F-measure in state-of-the-art IE, what does cor-
rectness of x% mean in practice? It likely means
that if x > y then a system achieving F-measure
x is better to have than one achieving y. But what
does it say about utility? In the best case, correct-
ness may be correlated with utility, in the worst case
it is independent of utility (e.g., if the system hap-
pens to achieve high correctness on events from the
past, which have low relevance). Since we are tar-
geting a specific user base, the user?s perspective
must be taken into account when estimating quality,
not (only) the system?s perspective. This implies the
need for automatic assignment of relevance scores
to analyzed events or documents.
Finally, reliability measures whether the reported
event is ?true?. The relevance of extracted fact may
be high, but is it credible? Can the information be
trusted? We list this criterion for quality for com-
pleteness, since it is the ultimate goal of any surveil-
lance process. However, answering this requires a
great deal of knowledge external to the system, that
can only be obtained by the human user through a
detailed down-stream verification process. The sys-
tem may provide some support for determining reli-
ability, e.g., by tracking the performance of different
information sources over time, since the reliability
of the facts extracted from an article is related to the
reliability of the source. It may be possible to clas-
sify Web-based sources according to their credibil-
ity; some sources may habitually withhold informa-
tion (for fear of impact to tourism, trade, etc.); other
sites may try to attract readership by exaggerated
claims (e.g., tabloids). On the other hand, clearly
disreputable sites may carry true information. This
measure of quality is beyond the scope of this paper.
3 The System: Background
PULS, the Pattern-based Understanding and Learn-
ing System, is developed at the University of
Helsinki to extract factual information from plain
text. PULS has been adapted to analyse texts for
Epidemic Surveillance.1
The components of PULS have been described
in detail previously, (Yangarber and Steinberger,
2009; Steinberger et al, 2008; Yangarber et al,
2007). In several respects, it is similar to other
existing systems for automated epidemic surveil-
lance, viz., BioCaster (Doan et al, 2008), MedISys
and PULS (Yangarber and Steinberger, 2009),
HealthMap (Freifeld et al, 2008), and others (Linge
et al, 2009).
PULS relies on EC-JRC?s MedISys for IR (in-
formation retrieval)?MedISys performs a broad
Web search, using a set of boolean keyword-based
queries, (Steinberger et al, 2008). The result is
a continuous stream of potentially relevant docu-
ments, updated every few minutes. Second, an IE
component, (Grishman et al, 2003; Yangarber and
Steinberger, 2009), analyzes each retrieved docu-
ment, to try to find events of potential relevance
to Public Health. The system stores the struc-
tured information about every detected event into a
database. The IE component uses a large set of lin-
guistic patterns, which in turn depend on a large-
scale public health ontology, similar to MeSH,2 that
contains concepts for diseases and infectious agents,
infectious vectors and animals, medical drugs, and
geographic locations.
From each article, PULS?s pattern matching en-
gine tries to extract a set of incidents, or ?facts??
detailed information related to instances of disease
outbreak. An incident is described by a set of fields,
or attributes: location and country of the incident,
disease name, the date of the incident, information
about the victims?their type (people, animals, etc.),
1puls.cs.helsinki.fi/medical
2www.nlm.nih.gov/mesh
31
number, whether they survived or died, etc.
The result of IE is a populated database of ex-
tracted items, that can be browsed and searched by
any attribute, according to the user?s interests. It is
crucial to note that the notion of a user?s focus or
interest is not the same as the notion of relevance,
introduced above. We take the view that the notion
of relevance is shared among the entire PH commu-
nity: an event is either relevant to PH or it is not.
Note also, that this view is upheld by several classic,
human-moderated PH surveillance systems, such as
ProMED-Mail3 or Canadian GPHIN. User?s inter-
est is individual, e.g., a user may have specific ge-
ographic, or medical focus (e.g., only viral or tropi-
cal illnesses), and given the structured database, s/he
can filter the content according to specific criteria.
But that is independent of the shared notion of rele-
vance to PH. User focus can be exploited for targeted
recommendation, using techniques such as collabo-
rative filtering; at present, this is beyond the scope
of our work.
The crawler and IE components have been in op-
eration and under refinement for some time. We next
build a classifier to assign relevance scores to each
extracted event and matched document.
4 Experimental Setup
We now present the work on automatic classification
of relevance scores. In collaboration with the end-
users, we defined guidelines for judging relevance
on a 6-point scale, summarized in Table 1.
Criteria Score
New information, highly relevant 5
Important updates, 4
on-going developments
Review of current events, 3
potential risk of disease
Historical/non-current events 2
Background information
Non-specific, non-factive events, 1
secondary topics, scientific studies
hypothetical risk
Unrelated to PH 0
Table 1: Guidelines for relevance scores in medical news
3www.promedmail.org
Note, the separation between the ?high-
relevance? scores, 4 and 5, vs. the rest; this
split is addressed in detail in Section 4.3.
4.1 Discourse features
It is clear that these guidelines are highly subjec-
tive, and cannot be encoded by rules directly. In
order to model the relevance judgements, we ex-
tracted features?the discourse features?from the
document that are indicative of, or mappable to,
the relevance scores. Discourse features try to cap-
ture higher-order information, including complex
and longer-range inter-dependencies and clues, in-
volving the physical layout of the document, and
deeper semantic and conceptual information found
in the document. Some examples of discourse fea-
tures are:
? Relative-position, which is represented by a
number from zero to 1 indicating the propor-
tion of the document one needs to read to reach
the event text;
? Disease-in-header is a binary value that indi-
cates whether the disease is mentioned in the
headline or the first two sentences;
? Disease-to-trigger-distance indicates how far
the disease is from the trigger sentence (same
as for confidence computation);
? Recency is the number of days between the re-
ported occurrence of the event and the publica-
tion date;
We compiled over two dozen discourse-level fea-
tures. It is clear that the discourse features do not
determine the relevance scores, but provide weak
indicators of relevance, so that probabilistic classi-
fication is appropriate. For example, a higher rel-
ative position of an event probably indicates lower
relevance, but there are often news summary arti-
cles that gather many unrelated news together, and
may contain very important items anywhere in the
article.4 A feature such as Victim-named, stating
whether the victim?s name is mentioned, often in-
dicates lower-relevance events (obituaries, stories
4Due to space limitations, we do not provide a detailed list
of the discourse features.
32
about public personalities, etc.). However, some-
times news articles about disease outbreaks deliber-
ately personify the victims, to give the reader a sense
of their background, lifestyle, to speculate about the
victims? common circumstances.
We describe two classifiers we have built for rel-
evance. A Naive Bayes classifier (NB) was used as
the baseline. We then tried to obtain improved per-
formance with Support Vector Machines (SVM).
4.2 Data
The dataset is the database of facts extracted by the
system. The system pre-assigns relevance to each
event, and users have the option to accept or cor-
rect the system?s relevance score, through the User
Interface, which also allows the users to correct er-
roneous fills, e.g., if a country, disease name, etc.,
was extracted incorrectly by the system.
Along with the users, members of the develop-
ment team also evaluated a sample of the extracted
events, and corrected relevance and erroneous fills.
The developers are computer scientists and linguists,
whereas the users are medics, and because they in-
terpreted the guidelines differently this had an im-
pact on the results, described in Tables 2 and 5.
?Cleaned data?: PULS?s user interface also per-
mits users to correct incorrect fills in the events (in
the two rightmost columns in the tables). This al-
lowed us to obtain two parallel sets of examples
with relevance labels: the raw examples, as they
were automatically extracted by the system, and the
?cleaned? examples, after users/developer correc-
tions. The raw set is more noisy, since it contains er-
rors introduced by the system. We used the cleaned
examples to train our classifiers, and tested them on
both the cleaned set and the raw set. Testing against
the cleaned set gives an ?idealized? performance, (as
if the IE system made no errors in analysis). True
performance is expected be closer to testing on the
raw set.
In total, there were just under 1000 examples la-
beled by the users and the developers (some exam-
ples were labeled by both, since the system allows
multiple users to attach different relevance judge-
ments to the same example. Most of the time
users agreed on the relevance judgements, but non-
developers were less likely to clean examples.)
4.3 Naive Bayes classifier
Initially, we planned to perform regression to the
complete [0?5] relevance scale. However, this
proved problematic, since the amount of labeled data
was not sufficient to cover the continuum between
highly relevant and not-so-relevant items. We there-
fore decided instead to build a binary classifier. This
decision is also justified in the context of our sys-
tem?s user interface, which provides the users with
two views:
? the Front Page View contains only high-
relevance items (rated 4 or 5), in case the user
wants to see only the most urgent items first;
? the Complete View shows the user all extracted
items, irrespective of relevance. (The user can
always filter the database by relevance value.)
Thus, the relevance score is also used to guide
a binary decision: whether to present a given
event/article to the user on the Front-Page View. The
NB classifier using the entire set of discourse fea-
tures did not perform well, because the discourse
features we have implemented are inherently not in-
dependent, which affects the performance of NB.
To try to reduce the mutual dependence among
the features, we added a simple, greedy feature-
selection phase during training. Feature selection
starts by training a classifier on the full set of fea-
tures, using leave-one-out (LOO) cross-validation to
estimate the classifier?s performance. In the next
phase, the algorithm in turn excludes the features
one by one, and runs the LOO cross-validation
again, once with each feature excluded. The feature
whose exclusion gives rise to the biggest increase in
performance is dropped out, and the selection step is
repeated with the reduced set of features. We con-
tinue to drop features until performance does not in-
crease for several iterations; in our experiments, we
used three steps beyond the top performance. We
then back up to the step that yielded peak perfor-
mance. The resulting subset of features is used to
train the final NB classifier.
The NB classifier is implemented in R Language.
Because relevance prediction is difficult for all
events, we also tried to predict the relevance of an
article, making the simplifying assumption that the
article is only as relevant as the first event found in
33
the article.5 The results are presented in Table 2.
The rows labeled Dev only refer to the data sets la-
beled by developers, and Users only to sets labeled
by (non-developer) users.
Testing on Number examples
Clean Raw Clean Raw
Event-level
Dev only 76.96 76.66 560 510
All 72.19 73.34 863 799
Users only 70.38 66.53 303 289
Document-level
Dev only 80.41 79.00 291 281
All 73.94 72.45 545 530
Users only 65.82 67.09 238 232
Table 2: Naive Bayes prediction accuracy
The event-level classification is shown in the top
portion of the table. Throughout, as expected, test-
ing on the cleaned data usually gives slightly bet-
ter (more idealized) performance estimates than test-
ing on the raw. Also, as expected, testing on
the first-only events (document-level) gives slightly
better performance, since it?s a simpler problem?
although there is less data to train/test on.
It is important to observe that using data la-
beled by developers gives significantly higher per-
formance. This is because coercing the users to fol-
low the guidelines strictly is not possible, and they
deviate from the rules that they themselves helped
articulate. The rows labeled ?all? show performance
when all combined available data was used?labeled
by both the developers and the users.
This performance is quite good for a baseline.6
The confusion matrices?for the developer-only
event-level raw data set?show the distribution of
true/false positives/negatives.
4.4 SVM Classifier
For comparison, we built two additional classifiers
using the SVMLight Toolkit.7 We first used a linear
5A manual check confirmed that there were no instances
where the first event in an article had lower relevance than a
subsequent event.
6Consider for comparison, that the correctness on a manu-
ally constructed, non-hidden set of articles used for system de-
velopment, is under 75% F-measure.
7http://svmlight.joachims.org/
True Labels
Predicted labels 4-5 0-3
High-relevance 4-5 125 77
Low-relevance 0-3 42 266
Table 3: NB confusion matrix
kernel as a baseline, and used a RBF kernel, which
is potentially more expressive. The conditions for
testing the SVM classifiers were same as the ones
for the NB classifiers, and same datasets were used
as for the NB.
As SVM with the RBF kernel can use non-linear
separating hyperplanes in the original feature space
by using the kernel trick (Aizerman et al, 1964),
we aimed to test whether it would provide an im-
provement over the linear kernel. (For more detailed
discussions of SVM and different kernel functions
for text classification, cf., for example, (Joachims,
1998).)
To regularize the input for SVM, all feature val-
ues were normalized to lie between 0 and 1 (for
continuous-valued features), and set to 0 or 1 for
binary features. Table 4 describes the accuracy
achieved with the linear kernel. Experiments labeled
All discourse features use the complete set of dis-
course features (over 20 features). Rows labeled Se-
lected discourse features show results from training
with exactly same features as resulted from the fea-
ture selection phase of NB.
Event-level Document-level
Clean Raw Clean Raw
All discourse features
Dev only 75.33 77.17 76.87 76.56
All 71.60 72.26 70.51 69.96
Selected discourse features only
Dev only 76.07 77.95 77.94 77.62
All 71.40 72.14 69.75 69.37
Table 4: SVM prediction accuracy using linear kernel
The difference when training with selected dis-
course features and all discourse features is not
large, since SVM is able to distinguish between rel-
evant and non-relevant features fairly well. The re-
sults from SVM using linear kernel appear compa-
34
rable with the results from the NB.
In addition to using the discourse features, we
also tried using lexical features. The lexical fea-
tures for a given example?extracted event?is sim-
ply the bag of words from the sentence containing
the event, plus the two surrounding sentences. To
reduce data sparsity, the sentences are pre-processed
by a lemmatizer, and passed through a named en-
tity (NE) recognizer (Grishman et al, 2003), which
replaces persons, organizations, locations and dis-
ease names with a special token indicating the NE?s
class. ?Stop-word? parts of speech were dropped?
prepositions, conjunctions, and articles.
Event-level Document-level
Clean Raw Clean Raw
All discourse features
Dev only 74.69 75.37 77.93 78.38
All 69.58 70.26 71.56 71.25
Selected discourse features only
Dev only 77.51 79.01 79.19 79.04
All 72.02 72.84 72.59 72.30
Lexical features only
Dev only 75.93 76.37 79.11 80.07
All 73.28 73.47 74.53 74.71
Lexical and selected discourse features
Dev only 78.87 79.24 82.66 81.83
All 76.48 76.58 76.52 76.19
Table 5: SVM prediction accuracy using RBF kernel
The performance of SVM with the RBF kernel
is strongly dependent on the values of SVM pa-
rameters C?the trade-off between training error
and margin? and ??the kernel width (Joachims,
1998). We tuned these parameters manually by
checking a grid of values against a development
dataset, and finding areas where the SVM performed
well. These areas were then further investigated. Af-
ter trying 40 combinations, we set C as 10000 and ?
to 0.001 for subsequent evaluations. The results for
SVM using RBF kernel are given in Table 5.
High accuracy of lexical features alone was some-
what surprising as lexical features consist only of the
bag of words in the event-bearing sentence, plus the
preceding and the following sentences. News arti-
cles often have various pieces of information related
to the event scattered around the document. For
example, the disease can appear only in the head-
line, the location/country in the middle of the doc-
ument, and the event-bearing sentence in a third lo-
cation, (Huttunen et al, 2002). Our lexical features,
as presented here, are not capable of capturing such
long-distance relationships.
The observed difference in performance on rele-
vance prediction between the data sets labeled by de-
velopers vs. non-developer users, likely arises from
the fact that developers follow the formal guidelines
more strictly (being computer scientists). Rows la-
beled all show performance against data sets la-
beled by real users, who work in different PH orga-
nizations in several different countries, each group
of users intuitively following their own, subjective
guidelines, despite the common guidelines agreed-
upon for this project. There may also be deviation
within organizations. For example, certain doctors
may find specific diseases or locations more inter-
esting, giving events containing them a high rele-
vance, thus injecting personal preference into docu-
ment relevance.
5 Discussion and Conclusions
The SVM performs somewhat better than the Naive
Bayes classifier, though there is still much to be ex-
plored and improved. One odd effect is that some-
times testing on the raw data gives slightly better
results than testing on the clean data, though this
is probably not significant, since the SVM classi-
fier is still not finely tuned (and the data contain
some noise). Using all discourse features performs
slightly worse than using a reduced set of features?
the same set of features that we obtained through
greedy feature selection for NB.
Although the lexical features alone seem to do
somewhat worse than the discourse features alone on
event-level classification, we still see that the lexical
features contain a great deal of information (which
the NB cannot use). As expected, adding the dis-
course features improves performance over lexical
features alone, since discourse features capture in-
formation about long-range dependencies that local
lexical features do not.
In forming splits for cross-validation or LOO, we
made sure not to split examples from the same doc-
35
ument across the training and test sets. That is, for
a given document, all events in it are either used for
training or for testing, to avoid biasing the testing.
To summarize, the points addressed in this paper:
? We have presented a language-technology-
based approach to a problem in Public Health,
specifically the problem of event-based epi-
demic surveillance through monitoring on-line
media.
? The user?s perspective needs to be taken into
account when estimating quality, not just the
system?s perspective. Utility to the user is at
least as important as (if not more important
than) correctness.
? We have presented an operational system that
suggests articles potentially relevant to the user,
and assigns relevance scores to each extracted
event.
? For now, we assume the users share same no-
tion of relevance of an event to Public Health.
? We have presented experiments and an initial
evaluation of assignment of relevance scores.
? Experiments indicate that relevance appears
to be a tractable measure of quality, at
least in principle. Marking document-level
relevance?only for the first event in the
document?appears to be easier. However,
making real users follow strict guidelines is dif-
ficult in practice.
On-going work includes refining the classification
approaches, especially, using Bayesian networks, re-
gression, using transductive SVMs to leverage unla-
beled data, and exploring collaborative filtering to
address users? individual interests.
Acknowledgments
This research was supported in part by: the Tech-
nology Development Agency of Finland (TEKES),
through the ContentFactory Project, and by the
Academy of Finland?s National Centre of Excel-
lence ?Algorithmic Data Analysis (ALGODAN).?
References
ACE. 2004. Automatic content extraction.
M. A. Aizerman, E. A. Braverman, and L. Rozonoer.
1964. Theoretical foundations of the potential func-
tion method in pattern recognition learning. In Au-
tomation and Remote Control, volume 25, pages 821?
837.
Aron Culotta and Andrew McCallum. 2004. Confi-
dence estimation for information extraction. In Pro-
ceedings of Human Language Technology Conference
and North American Chapter of the Association for
Computational Linguistics.
Son Doan, Quoc Hung-Ngo, Ai Kawazoe, and Nigel Col-
lier. 2008. Global Health Monitor?a web-based sys-
tem for detecting and mapping infectious diseases. In
Proceedings of the International Joint Conference on
Natural Language Processing (IJCNLP).
C.C. Freifeld, K.D. Mandl, B.Y. Reis, and J.S. Brown-
stein. 2008. HealthMap: Global infectious disease
monitoring through automated classification and visu-
alization of internet media reports. Journal of Ameri-
can Medical Informatics Association, 15:150?157.
Ralph Grishman, Silja Huttunen, and Roman Yangarber.
2003. Information extraction for enhanced access to
disease outbreak reports. Journal of Biomedical Infor-
matics, 35(4):236?246.
David Hartley, Noele Nelson, Ronald Walters, Ray
Arthur, Roman Yangarber, Larry Madoff, Jens Linge,
Abla Mawudeku, Nigel Collier, John Brownstein, Ger-
main Thinus, and Nigel Lightfoot. 2010. The land-
scape of international event-based biosurveillance.
Emerging Health Threats Journal, 3(e3).
Lynette Hirschman. 1998. Language understanding eval-
uations: Lessons learned from muc and atis. In Pro-
ceedings of the First International Conference on Lan-
guage Resources and Evaluation (LREC), pages 117?
122, Granada, Spain, May.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002. Complexity of event structure in information
extraction. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING
2002), Taipei, August.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In ECML: European Conference on Machine
Learning, pages 137?142.
J.P. Linge, R. Steinberger, T.P. Weber, R. Yangarber,
E. van der Goot, D.H. Al Khudhairy, and N.I. Stil-
ianakis. 2009. Internet surveillance systems for early
alerting of health threats. Eurosurveillance Journal,
14(13).
Ralf Steinberger, Flavio Fuart, Erik van der Goot, Clive
Best, Peter von Etter, and Roman Yangarber. 2008.
36
Text mining from the web for medical intelligence. In
Domenico Perrotta, Jakub Piskorski, Franoise Soulie?-
Fogelman, and Ralf Steinberger, editors, Mining Mas-
sive Data Sets for Security. OIS Press, Amsterdam, the
Netherlands.
Roman Yangarber and Ralf Steinberger. 2009. Auto-
matic epidemiological surveillance from on-line news
in MedISys and PULS. In Proceedings of IMED-
2009: International Meeting on Emerging Diseases
and Surveillance, Vienna, Austria.
Roman Yangarber, Clive Best, Peter von Etter, Flavio
Fuart, David Horby, and Ralf Steinberger. 2007.
Combining information about epidemic threats from
multiple sources. In Proceedings of the MMIES
Workshop, International Conference on Recent Ad-
vances in Natural Language Processing (RANLP
2007), Borovets, Bulgaria, September.
37
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 3?10,
Beijing, August 2010
Filtering news for epidemic surveillance:
towards processing more languages with fewer resources
Gae?l Lejeune1, Antoine Doucet1,
1GREYC, University of Caen
first.last@info.unicaen.fr
Roman Yangarber2, Nadine Lucas1
2CS department, University of Helsinki
yangarbe@cs.helsinki.fi
Abstract
Processing content for security be-
comes more and more important since
every local danger can have global
consequences. Being able to collect
and analyse information in different
languages is a great issue. This pa-
per addresses multilingual solutions
for analysis of press articles for epi-
demiological surveillance. The sys-
tem described here relies on pragmat-
ics and stylistics, giving up ?bag of
sentences? approach in favour of dis-
course repetition patterns. It only
needs light resources (compared to
existing systems) in order to process
new languages easily. In this pa-
per we present here results in En-
glish, French and Chinese, three lan-
guages with quite different character-
istics. These results show that simple
rules allow selection of relevant doc-
uments in a specialized database im-
proving the reliability of information
extraction.
1 Multilingual techniques in
information extraction
In natural language processing, information
extraction is a task where, given raw text, a
system is to give precise information fitting
in a predefined semantic template.
1.1 Epidemic surveillance
Automated news surveillance is an important
application of information extraction. The
detection of terrorist events and economic
surveillance were the first applications, in
particular in the framework of the evalua-
tion campaigns of the Message Understand-
ing Conference (MUC) (MUC, 1992; MUC,
1993). In MUC-3 (1991) and MUC-4 (1992),
about terrorism in Latin American countries,
the task of participants was, given a collec-
tion of news feed data, to fill in a prede-
termined semantic template containing the
name of the terrorist group that perpetrated
a terrorist event, the name of the victim(s),
the type of event, and the date and location
where it occurred. In economic surveillance,
one can for instance extract mergers or cor-
porate management changes.
An application of information extraction
that lately gained much importance is that
of epidemiological surveillance, with a spe-
cial emphasis on the detection of disease out-
breaks. Given news data, the task is to de-
tect epidemiological events, and extract the
location where they occurred, the name of
the disease, the number of victims, and the
?case?, that is, a text description of the
event, that may be the ?status? of victims
(sick, injured, dead, hospitalised . . . ) or a
written description of symptoms. Epidemio-
logical surveillance has become a crucial tool
with increasing world travel and the latest
crises of SARS, avian flu, H1N1 . . .
In this paper, we present an application to
epidemic surveillance, but it may be equally
applied to any subdomain of news surveil-
lance.
1.2 Multilingual information
extraction
As in many fields of NLP, most of the work in
information extraction long focused on En-
glish data (Etzioni et al, 2008).Multilingual
has often been understood as adding many
3
monolingual systems, except in pioneer mul-
tilingual parsing (Vergne, 2002). Whereas
English is nowadays the lingua franca in
many fields (in particular, business), we will
see that for several applications, this is not
sufficient. Most news agencies are translat-
ing part of their feed into English (e.g., AFP1
and Xinhua2 for which the source languages
are respectively French and Chinese), but a
good deal of the data is never translated,
while for the part that is, the translation
process naturally incurs a delay that is, by
essence, problematic in a field where exhaus-
tivity and early detection are crucial aspects.
Subsequently, the ability to simultane-
ously handle documents written in different
languages is becoming a more and more im-
portant feature (Poibeau et al, 2008; Gey et
al., 2009). Indeed, in the field of epidemio-
logical surveillance, it is especially important
to detect a new event the very first time it is
mentioned, and this very first occurrence will
almost always happen in the local language
(except for countries like Iraq for instance).
Therefore, it is not enough to be able to deal
with several languages : It is necessary to
handle many. For instance, the Medical In-
formation System (Medisys) of the European
Community gathers news data in 42 differ-
ent languages (Atkinson and der Goot, 2009)
(now 453).
1.3 Current approaches
There are currently 2 main approaches to
multilingual information extraction. The
first approach relies on the prior transla-
tion of all the documents into one com-
mon language (usually English), for which a
well-performing information extraction sys-
tem has been developed (Linge et al, 2009).
Whereas the simple design of this solution
is attractive, the current state of the art in
machine translation only allows for mediocre
results. Most monolingual information ex-
traction systems indeed rely on a combina-
1http://www.afp.com/afpcom/en
2http://www.xinhuanet.com/english2010/
3http://medusa.jrc.it/medisys/aboutMediSys.html
tion of grammatical patterns and specialized
lexicons (Grishman et al, 2002; Riloff, 1996).
The second main approach consists in leav-
ing documents in their original language but
to translate the lexicons and extraction pat-
terns into that language (Efimenko et al,
2004; Linge et al, 2009). However, the
same problems occur as in the first approach
because the patterns are strongly language-
related. Yet, to ?translate the system? seems
more realistic than to translate the docu-
ments, as it can be done manually, and of-
fline (once and for all, and not as docu-
ments arrive). The bottleneck is then that
the amount of work for each language is
enormous: it naturally requires the com-
plete translation of the lexicon (for all trig-
ger words), but the more challenging is-
sue is the translation of patterns, whose
language-dependence might well mean that
the amount of work needed to translate them
comes close to that required for writing them
from scratch. In addition, this task must
necessarily be achieved by a domain expert,
with excellent skills in the languages at hand.
One could want to tackle this problem by us-
ing machine learning but she will need train-
ing data in many languages. In practice,
this will often mean that only a few major
languages will be dealt with, whilst all the
others (amongst which all low-resource lan-
guages), will again be totally discarded. One
can then only wish that epidemics will chose
to occur in locations handled by surveillance
systems. . .
Both approaches additionally require a
number of linguistic processing tools, in a
number comparable to the number of lan-
guages to be dealt with: tokenizer, stem-
mer, syntactic analyzer, . . . One might there-
fore conclude that such techniques are not
properly multilingual but rather monolingual
methods that may be adapted to other lan-
guages individually.
In this paper, we explore a third approach
to multilingual information extraction. We
restrain ourselves to the sole use of truly mul-
4
tilingual elements, facts that are equally true
for any language. The approach hence relies
on universals, relying, e.g., on stylistics and
rhetorics.
2 Rationale of the experiment
The objective of the system is to monitor
news in a variety of languages to detect dis-
ease outbreaks which is an important issue
for an alert system in epidemic surveillance.
For this task a simple and clear framework is
needed in order to limit the amount of work
for new languages while keeping good relia-
bility. The main idea of our work is using
text granularity and discourse properties to
write rules that may be language indepen-
dent, fast and reliable (Vergne, 2002). For
this study, regularities at text level are ex-
ploited. These phenomena can be related to
stylistics and pragmatics. It has already been
shown that news discourse has its own con-
straints reflected in press articles of different
languages (Van Dijk, 1988; Lucas, 2004).
2.1 Stylistic rules
Journalists all over the world know how to
hook their potential readers. These meth-
ods are described in journalism schools (Itule
and Anderson, 2006). One very important
rule for journalists seems to be the?5W rule?
which emphasise on the fact that answering
to the questions?What?,?Where?,?When?,
?Why? and ?Who? is a priority at the start
of a paper. Only after that can journal-
ists develop and give secondary information.
This phenomenon is genre dependent and is
exploited for processing texts by searching
for repetitions.
Example 1 shows a piece of news where the
disease name is found in the beginning of the
news article and developed later on. No local
pattern is needed to detect what the article
is about, repetition phenomena is sufficient.
Example 2 is a counter example, where
a disease name is found but not repeated.
This French document reports on a pop mu-
sic band being the ?coqueluche? of Hip-Hop,
which can mean ?pertussis?, but here means
?fashion? in a figurative sense (underlining
the fast spread of the band?s popularity).
Usually, figurative meanings are not used
twice in the same article (Itule and Ander-
son, 2006) and hence the repetition criteria
allows one to rightfully ignore this article.
2.2 Pragmatic rules
As press articles are made for humans, strong
effort is exerted to ensure that readers will
understand the main information with as few
inferences as possible (Sperber and Wilson,
1998). In fact, the more inferences the reader
has to make, the more errors he is likely to
make and the more probability he will get
confused and not read the full article. Rep-
etitions are there to relieve the memory ef-
fort. A point that journalists pay much at-
tention to is leaving as few ambiguities on
main facts as possible. It means that poten-
tially unknown or complicated terms will be
used quite rarely. Only one main story will
be developed in an article, other facts that
are important will be developed elsewhere as
main stories.
3 Our system
The system is based on the comparison of
repetitions in the article to find documents
relevant for epidemic surveillance and extract
where the disease occurs and how many peo-
ple are concerned.
3.1 String repetitions: relevant
content
A system is not a human reader, so objec-
tive discourse marks are used by the sys-
tem. Repetitions are known since the an-
cient times as reflecting discourse structure.
A press article is divided into two parts,
roughly the head and the rest of the news.
The title and the first two sentences form
the head or thematic part and the rest of
the text is considered to be a development in
an expository discourse.
5
Measles outbreak spreads north in B.C.
Number of cases hits 44 provincewide B.C.?s measles outbreak appears to have spread to
northeastern areas of the province, after doctors confirmed two new cases of the disease
in the Fort St. John and Fort Nelson areas on Thursday.
The new cases bring the total number of confirmed cases in the province to 44, not
including suspected but unconfirmed cases, said the B.C. Centre for Disease Control.
Northern Health spokeswoman Eryn Collins said the virus had not been detected in the
north in more than six years and the two new cases involve people who weren?t immunized.
[...] ?It is suspected that at least two out-of-country visitors brought measles into Van-
couver sometime in February or early March, as two separate strains of the virus have been
identified,? said a statement from the B.C. Centre for Disease Control earlier this week. So
far, 17 cases of the measles have been detected in the Fraser Valley, 17 in the Vancouver
area, seven in the southern Interior, two in northern B.C. and one on Vancouver island.
Figure 1: Example in English: repetition of disease name and cases
Cameroun/Musique : X-Maleya nouvelle coqueluche du Hip-Hop camerounais !
Le trio Hip-Hop Cameounais X-Maleya, a le vent en poupe. Le groupe qui s?illustre dans
la tendance Hip-Hop, est aujourd?hui l?une des valeurs sres musicales gra?ce son second
opus Yelele.
Derrie`re ces trois pre?noms : Roger, Auguste et Ha??s, se cachent un trio camerounais
qui s?illustre dans le monde du Hip-Hop. [etc.] C?est donc, une nouvelle valeur su?re
qu?incarnent eux trois Roger, Auguste et Ha??s. Le groupe rencontre en effet, une ascension
fulgurante. Les trois faiseurs de Hip-Hop, ont une seule ide?e en te?te, continuer de se
produire pour ceux qui les appre?cient, toujours composer de belles me?lodies et, ne pas
oublier d?ou` ils viennent.
Figure 2: Example in French: no repetition
Strings that are present in both parts will
be referred to as?relevant content?. They are
found in the beginning of the news and re-
peated in the development. To process as
many languages as possible, repeated char-
acter strings will be searched (not words
because Chinese for instance does not use
graphic words).
3.2 Defining epidemic event
Epidemic events are captured through these
information slots:
? Disease (What)
? Location (Where)
? Case, i.e.,People concerned (Who)
3.3 Selecting potentially relevant
documents
This discourse related heuristic rule limits re-
sources needed by the system. Many char-
acter strings that are repeated in the text
reflect important terms. However, repeti-
tion alone does not allow to fill IE templates
with detailed information as required. Ac-
cordingly, a lexical filter is applied on the re-
peated strings. 200 common disease names
are used to filter information and find dis-
ease names. The idea behind the restricted
list is that a journalist will use a common
name to help his readers understand the mes-
sage. Similarly, for locations, a list of coun-
try names and capitals provided by UN is
6
WHO checks smallpox reports in Uganda
LONDON, Thursday
The World Health Organisation said today it was investigating reports of suspected cases
of the previously eradicated disease smallpox in eastern Uganda.
Smallpox is an acute contagious disease and was one of the worlds most feared sicknesses
until it was officially declared eradicated worldwide in 1979.
?WHO takes any report of smallpox seriously, Gregory Hartl, a spokesman for the Geneva-
based United Nations health agency, told Reuters via email.
?WHO is aware of the reports coming out of Uganda and is taking all the necessary
measures to investigate and verify.?[etc.]
Figure 3: Example in English: repetition and location
used (about 500 items). Finally, in order to
comply with a specific demand of partners,
blacklist terms were used to detect less rel-
evant articles (vaccination campaign for in-
stance).
When a disease name is found in the rele-
vant content, the article is selected as poten-
tially relevant and the system tries to extract
location and cases.
3.4 Extracting location and cases
To extract the location, the following heuris-
tic is applied: the relevant location corre-
sponds to a string in the?relevant content?.
For instance, Example 3 shows that it allows
for the system to find that the main event
concerns Uganda but not London.
If numerous locations match, the system
compares frequencies in the whole document:
if one location is more than twice as frequent
as others, it is considered as the relevant one.
If no location is found, the location of the
source is selected by default. In fact accord-
ing to pragmatic rules when one reads an
article in the Washington Post, she will be
sure that it is about the United States even
if it is not explicitly mentioned. To the con-
trary if the article is about Argentina it will
be clearly mentioned so the reader has less
chances of misunderstanding.
Concerning the cases, they are related to
the first numeric information found in the
document, provided the figures are not re-
lated to money or date (this is checked by a
blacklist and simple regular expressions).
Furthermore the extracted cases are con-
sidered more relevant if they appear twice in
the document, the system uses regular ex-
pressions to round up and compare them.
See Example 4 where the number of dead
people ?55? is the first numeric information
in the beginning and is repeated in the de-
velopment (we chose an example where it is
easy even for a non Chinese speaker to see
the repetition). One can also note that the
second repeated figure is ?19488? which is
the number of infected people.
4 Evaluation
It is important to insist on the fact that our
system extracts the main event from one ar-
ticle, considering that secondary events have
been or will be mentioned in another article.
Often, the more topics are presented in one
article, the less important each one is. In the
case of epidemic surveillance, review articles
or retrospectives are not first-hand, fresh and
valuable information.
4.1 Corpus and Languages
For each language we randomly extracted
documents from the Medisys website.
Medisys documents are gathered using key-
words: medical terms (including scientific
disease names), but also weaker keywords
such as casualties, hospital. . . This implies
that some news document not related
7
Figure 4: Example in Chinese: 55 deaths from H1N1
to epidemic surveillance, but to accident
reports for instance, are liable to be found
in the database.
We must underline that in this framework,
recall can only be estimated, notably because
the news documents are keyword-filtered be-
forehand. However, our aim is not to provide
an independent system, but to provide quick
sorting of irrelevant news, prior to detailed
analysis, which is the key issue of a surveil-
lance and alert system. 200 documents were
extracted for each language and manually
tagged by native speakers with the following
instructions:
? Is this article about an epidemic?
? If it is, please give when possible:
Disease(s)
Country (or Worldwide)
Number of cases
100 of these annotated documents were used
for fine-tuning the system, 100 others for
evaluating. We chose for this study 3 fairly
different languages for checking the generic-
ity of the approach
? French, with its rather rich morphology,
? English,a rather isolating language with
poor morphology,
? Chinese, a strict isolating language with
poor morphology.
4.2 Results
These results were computed from a set of
100 annotated documents, as described in
section 4. Table 1 shows recall, precision and
F-measure for document selection(more ex-
amples are available online 4 ) Table 2 com-
pares automatically extracted slots and hu-
man annotated slots, therefore if an event is
not detected by the system it will count as
an error for each slot.
Table 1 shows that selection of documents
is quite satisfactory and that recall is better
than precision. This is mostly due to the fact
that the system still extracts documents with
low relevance. We found it impossible to pre-
dict if this is a general bias and whether it
can be improved. The result analysis showed
that many false negatives are due to cases
when the piece of news is quite small, see
for instance Example 5 where ?Swine flu? is
only found in the first two sentences, which
implies the repetition criteria does not apply
(and the system misses the document).
Table 2 shows the accuracy of the infor-
mation entered into semantic slots, respec-
4http://sites.google.com/site/iesystemcoling2010
8
China has 100 cases of swine flu: state media
China has 100 confirmed cases of swine flu, state media said Tuesday, as data from the
World Health Organization showed the disease had spread to 73 countries.
?The health ministry has reported that so far, China has 100 confirmed cases of A(H1N1)
flu,? said a news report on state television CCTV. The report said the 100 cases were in
mainland China, which does not include Hong Kong or Macau.
Figure 5: Example in English: Disease name not in ?relevant content?
Language Recall Precision F-measure
French 93% 88% 90%
English 88% 84% 86%
Chinese 92% 85% 88%
Table 1: Selecting documents
Language Diseases Locations Cases
French 88% 87% 81%
English 81% 81% 78%
Chinese 82% 79% 77%
Table 2: Accuracy in filling slots
tively name of disease, location and number
of cases. It is important to say that the de-
scriptors extracted are really reliable in spite
of the fact that the annotated set used for
evaluation is fairly small: 100 documents per
language, 30 to 40 of which were marked as
relevant. The extraction of cases performs a
bit worse than that of locations but the loca-
tion is the most important to our end-users.
5 Discussion and Conclusion
Most research in Information Extraction (IE)
focuses on building independent systems for
each language, which is time and resource
consuming. To the contrary, using common
features of news discourse saves time. The
system is not quite independent, but it al-
lows filtering news feeds and it provides rea-
sonable information even when no resources
at all are available. Our results on English
are worse than some existing systems (about
93% precision for Global health Monitor for
instance) but these systems need strong re-
sources and are not multilingual. We then
really need a multilingual baseline to com-
pare both approaches.
Recall is important for an alert system,
but is very difficult to assess in the case of
epidemiological surveillance. This measure
is always problematic for web based docu-
ments, due to the fact that any randomly
checked sample would only by sheer luck con-
tain all the positive documents. The assump-
tion here is that no important news has been
missed by Medisys, and that no important
news filtered from Medisys has been rejected.
One explanation for missed articles lies
in the definition of the article header: it is
too rigid. While this is fine for standard
size news, it is inappropriate for short news,
hence meaningful repetitions are missed in
the short news. This is a flaw, because first
alerts are often short news. In the future, we
may wish to define a discourse wise detection
rule to improve the location slot filling. The
extraction of locations is currently plagued
by a very long list of countries and capitals,
most of which is not useful. Locations are ac-
tually mentioned in data according to states,
provinces, prefectures, etc. The country list
might be abandoned, since we do not favour
external resources.
The methods that are presented here
maintain good reliability in different lan-
guages, and the assumption that genre laws
are useful has not been challenged yet. Light
resources, about 750 items (to be compared
to tens of thousands in classical IE sys-
tems), make it possible to strongly divide the
amount of work needed for processing new
languages. It might be attempted to refine
the simple hypotheses underlying the pro-
9
gram and build a better system for filtering
relevant news. This approach is best suited
when combined with elaborate pattern-based
IE modules when available. Repetition can
be checked for selecting documents prior to
resource intensive semantic processing. It
can also provide a few, easily fixable and effi-
cient preliminary results where language re-
sources are scarce or not available at all.
References
Atkinson, Martin and Erik Van der Goot. 2009.
Near real time information mining in multilin-
gual news. In 18th International World Wide
Web Conference (WWW2009).
Efimenko, Irina, Vladimir Khoroshevsky, and
Victor Klintsov. 2004. Ontosminer family:
Multilingual ie systems. In SPECOM 2004:
9th Conference Speech and Computer.
Etzioni, Oren, Michele Banko, Stephen Soder-
land, and Daniel S. Weld. 2008. Open in-
formation extraction from the web. Commun.
ACM, 51(12):68?74.
Gey, Fredric, Jussi Karlgren, and Noriko Kando.
2009. Information access in a multilingual
world: transitioning from research to real-
world applications. SIGIR Forum, 43(2):24?
28.
Grishman, Ralph, Silja Huttunen, and Roman
Yangarber. 2002. Information extraction for
enhanced access to disease outbreak reports.
Journal of Biomedical Informatics, 35(4):236?
246.
Itule, Bruce and Douglas Anderson. 2006. News
Writing and Reporting for Today?s Media.
McGraw-Hill Humanities.
Linge, JP, R Steinberger, T P Weber, R Yan-
garber, E van der Goot, D H Al Khudhairy,
and N I Stilianakis. 2009. Internet surveil-
lance systems for early alerting of threats. Eu-
rosurveillance, 14.
Lucas, Nadine. 2004. The enunciative structure
of news dispatches, a contrastive rhetorical
approach. Language, culture, rhetoric, pages
154?164.
MUC. 1992. Proceedings of the 4th Confer-
ence on Message Understanding, MUC 1992,
McLean, Virginia, USA, June 16-18, 1992.
MUC. 1993. Proceedings of the 5th Conference
on Message Understanding, MUC 1993, Balti-
more, Maryland, USA, August 25-27, 1993.
Poibeau, Thierry, Horacio Saggion, and Roman
Yangarber, editors. 2008. MMIES ?08: Pro-
ceedings of the Workshop on Multi-source Mul-
tilingual Information Extraction and Summa-
rization, Morristown, NJ, USA. Association
for Computational Linguistics.
Riloff, Ellen. 1996. Automatically generating
extraction patterns from untagged text. In
AAAI/IAAI, Vol. 2, pages 1044?1049.
Sperber, Dan and Deirdre Wilson. 1998. Rele-
vance: Communication and cognition. Black-
well press, Oxford U.K.
Van Dijk, T.A. 1988. News as discourse.
Lawrence Erlbaum Associates, Hillsdale N.J.
Vergne, Jacques. 2002. Une me?thode pour
l?analyse descendante et calculatoire de corpus
multilingues: application au calcul des rela-
tions sujet-verbe. In TALN 2002, pages 63?74.
10
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 108?116,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Using context and phonetic features
in models of etymological sound change
Hannes Wettig1, Kirill Reshetnikov2 and Roman Yangarber1
1Department of Computer Science 2Institute of Linguistics
University of Helsinki, Finland Academy of Sciences
First.Last@cs.helsinki.fi Moscow, Russia
Abstract
This paper presents a novel method for
aligning etymological data, which mod-
els context-sensitive rules governing sound
change, and utilizes phonetic features of the
sounds. The goal is, for a given corpus of
cognate sets, to find the best alignment at
the sound level. We introduce an imputa-
tion procedure to compare the goodness of
the resulting models, as well as the good-
ness of the data sets. We present evalu-
ations to demonstrate that the new model
yields improvements in performance, com-
pared to previously reported models.
1 Introduction
This paper introduces a context-sensitive model
for alignment and analysis of etymological data.
Given a raw collection of etymological data (the
corpus)?we first aim to find the ?best? alignment
at the sound or symbol level. We take the corpus
(or possibly several different corpora) for a lan-
guage family as given; different data sets are typ-
ically conflicting, which creates the need to deter-
mine which is more correct. Etymological data
sets are found in digital etymological databases,
such as ones we use for the Uralic language fam-
ily. A database is typically organized into cog-
nate sets; all elements within a cognate set are
posited (by the database creators) to be derived
from a common origin, which is a word-form in
the ancestral proto-language.
Etymology encompasses several problems,
including: discovery of sets of cognates?
genetically related words; determination of ge-
netic relations among groups of languages, based
on linguistic data; discovering regular sound cor-
respondences across languages in a given lan-
guage family; and reconstruction of forms in the
proto-languages.
Computational methods can provide valuable
tools for the etymological community. The meth-
ods can be judged by how well they model certain
aspects of etymology, and by whether the auto-
matic analysis produces results that match theo-
ries established by manual analysis.
In this work, we allow all the data?and only
the data?to determine what rules underly it,
rather than relying on external (and possibly bi-
ased) rules that try to explain the data. This ap-
proach will provide a means of measuring the
quality of the etymological data sets in terms of
their internal consistency?a dataset that is more
consistent should receive a higher score. We seek
methods that analyze the data automatically, in
an unsupervised fashion, to determine whether a
complete description of the correspondences can
be discovered automatically, directly from raw
etymological data?cognate sets within the lan-
guage family. Another way to state the question
is: what alignment rules are ?inherently encoded?
in the given corpus itself.
At present, our aim is to analyze given etymo-
logical datasets, rather than to construct new ones
from scratch. Because our main goal is to de-
velop methods that are as objective as possible,
the models make no a priori assumptions or ?uni-
versal? principles?e.g., no preference to align
vowel with vowels, or a symbol with itself. The
models are not aware of the identity of a symbol
across languages, and do not try to preserve iden-
tity, of symbols, or even of features?rather they
try to find maximally regular correspondences.
In Section 2 we describe the data used in our
experiments, and review approaches to etymolog-
ical alignment over the last decade. We formalize
the problem of alignment in Section 3, give the
108
Uralic tree   
Figure 1: Finno-Ugric branch of Uralic language fam-
ily (the data used in the experiments in this paper)
technical details of our models in Section 4. We
present results and discussion in Sections 5 and 6.
2 Data and Related Work
We use two large Uralic etymological resources.
The StarLing database of Uralic, (Starostin,
2005), based on (Re?dei, 1988 1991), contains
over 2500 cognate sets. Suomen Sanojen Alku-
pera? (SSA), ?The Origin of Finnish Words?, a
Finnish etymological dictionary, (Itkonen and Ku-
lonen, 2000), has over 5000 cognate sets, (about
half of which are only in languages from the
Balto-Finnic branch, closest to Finnish). Most
importantly, for our models, SSA gives ?dictio-
nary? word-forms, which may contain extraneous
morphological material, whereas StarLing data is
mostly stemmed.
One traditional arrangement of the Uralic lan-
guages1 is shown in Figure 1. We model etymo-
logical processes using these Uralic datasets.
The methods in (Kondrak, 2002) learn regular
one-to-one sound correspondences between pairs
of related languages in the data. The methods
in (Kondrak, 2003; Wettig et al, 2011) find more
complex (one-to-many) correspondences. These
models operate on one language pair at a time;
also, they do not model the context of the sound
changes, while most etymological changes are
conditioned on context. The MCMC-based model
proposed in (Bouchard-Co?te? et al, 2007) explic-
itly aims to model the context of changes, and op-
1Adapted from Encyclopedia Britannica and (Anttila,
1989)
erates on more than a pair of languages.2
We should note that our models at present op-
erate at the phonetic level only, they leave seman-
tic judgements of the database creators unques-
tioned. While other work, e.g. (Kondrak, 2004),
has attempted to approach semantics by compu-
tational means as well, our model uses the given
cognate set as the fundamental unit. In our work,
we do not attempt the problem of discovering cog-
nates, addressed, e.g., in, (Bouchard-Co?te? et al,
2007; Kondrak, 2004; Kessler, 2001). We begin
instead with a set of etymological data (or more
than one set) for a language family as given. We
focus on the principle of recurrent sound corre-
spondence, as in much of the literature, includ-
ing (Kondrak, 2002; Kondrak, 2003), and others.
As we develop our alignment models at the
sound or symbol level, in the process of evalu-
ation of these models, we also arrive at model-
ing the relationships among groups of languages
within the family. Construction of phylogenies is
studied extensively, e.g., by (Nakhleh et al, 2005;
Ringe et al, 2002; Barbanc?on et al, 2009). This
work differs from ours in that it operates on manu-
ally pre-selected sets of characters, which capture
divergent features of languages within the family,
whereas we operate on the raw, complete data.
There is extensive work on alignment in the
machine-translation (MT) community, and it has
been observed that methods from MT alignment
may be projected onto alignment in etymology.
The intuition is that translation sentences in MT
correspond to cognate words in etymology, while
words in MT correspond to sounds in etymology.
The notion of regularity of sound change in et-
ymology, which is what our models try to cap-
ture, is loosely similar to contextually conditioned
correspondence of translation words across lan-
guages. For example, (Kondrak, 2002) employs
MT alignment from (Melamed, 1997; Melamed,
2000); one might employ the IBM models for
MT alignment, (Brown et al, 1993), or the HMM
model, (Vogel et al, 1996). Of the MT-related
models, (Bodrumlu et al, 2009) is similar to ours
in that it is based on MDL (the Minimum Descrip-
tion Length Principle, introduced below).
2Using this method, we found that the running time did
not scale well for more than three languages.
109
3 Aligning Pairs of Words
We begin with pairwise alignment: aligning pairs
of words, from two related languages in our
corpus of cognates. For each word pair, the
task of alignment means finding exactly which
symbols correspond. Some symbols may align
with ?themselves? (i.e., with similar or identi-
cal sounds), while others may have undergone
changes during the time when the two related lan-
guages have been evolving separately. The sim-
plest form of such alignment at the symbol level
is a pair (? : ?) ? ? ? T , a single symbol ?
from the source alphabet ? with a symbol ? from
the target alhabet T . We denote the sizes of the
alphabets by |?| and |T |.
To model insertions and deletions, we augment
both alphabets with a special empty symbol?
denoted by a dot?and write the augmented al-
phabets as ?. and T.. We can then align
word pairs such as vuosi?al (meaning ?year? in
Finnish and Xanty) , for example as any of:
v u o s i
| | | | |
a l . . .
v u o s i
| | | | |
. a . l .
etc...
The alignment on the right then consists of the
symbol pairs: (v:.), (u:a), (o:.), (s:l), (i:.).
4 Context Model with Phonetic Features
The context-aware alignment method we present
here is built upon baseline models published pre-
viously, (Wettig et al, 2011), where we presented
several models that do not use phonetic features
or context. Similarly to the earlier ones, the cur-
rent method is based on the Minimum Description
Length (MDL) Principle, (Gru?nwald, 2007).
We begin with a raw set of (observed) data?
the not-yet-aligned word pairs. We would like
to find an alignment for the data?which we
will call the complete data?complete with align-
ments, that make the most sense globally, in terms
of embodying regular correspondences. We are
after the regularity, and the more regularity we
can find, the ?better? our alignment will be (its
goodness will be defined formally later). MDL
tells us that the more regularity we can find in
the data, the fewer bits we will need to encode
it (or compress it). More regularity means lower
entropy in the distribution that describes the data,
and lower entropy allows us to construct a more
economical code. That is, if we have no knowl-
edge about any regularly of correspondence be-
tween symbols, the joint distribution over all pos-
sible pairs of symbols will be very flat (high en-
tropy). If we know that certain symbol pairs align
frequently, the joint distribution will have spikes,
and lower entropy. In (Wettig et al, 2011) we
showed how starting with a random alignment a
good joint distribution can be learned using MDL.
However the ?rules? those baseline models were
able to learn were very rudimentary, since they
could not use any information in the context, and
we know that many regular correspondences are
conditioned by context.
We now introduce models that leverage infor-
mation from the context to try to reduce the un-
certainty in the distributions further, lowering the
coding cost. To do that, we will code sounds
in terms of their phonetic features: rather than
coding the symbols (sounds) as atomic, we code
them as vectors of phonetic features. Rather than
aligning symbol pairs, we align the correspond-
ing features of the symbols. While coding each
feature, the model can make use of features of
other sounds in its context (environment), through
a special decision tree built for that feature.
4.1 Features
We will code each symbol, to be aligned in the
complete data, as a feature vector. First we code
the Type feature, with values: K (consonant), V
(vowel), dot, and word boundary, which we de-
note as #. Consonants and vowels have their own
sets of features, with 2?8 values per feature:
Consonant articulation
M Manner plosive, fricative, glide, ...
P Place labial, dental, ..., velar
X Voiced ? , +
S Secondary ? , affricate, aspirate, ...
Vowel articulation
V Vertical high?low
H Horizontal front?back
R Rounding ? , +
L Length 1?5
4.2 Contexts
While coding any symbol, the model will be al-
lowed to query a fixed, finite set of candidate con-
texts. A context is a triplet (L,P, F ), where L
is the level?either source or target,?and P is
110
one of the positions that the model may query?
relative to the position currently being coded; for
example, we may allow positions as in Fig. 2. F is
one of the possible features found at that position.
Therefore, we will have about 2 levels * 8 posi-
tions * 2?6 features ? 80 candidate contexts that
can be queried by the model, as explained below.
I itself,
?P previous position
?S previous non-dot symbol
?K previous consonant
?V previous vowel
+S previous or self non-dot symbol
+K previous or self consonant
+V previous or self vowel
Figure 2: An example of a set of possible positions
in the context?relative to the position currently being
coded?that can be queried by the context model.
4.3 The Two-Part Code
We code the complete (i.e., aligned) data using a
two-part code, following the MDL Principle. We
first code which particular model instance we se-
lect from our class of models, and then code the
data, given the defined model. Our model class
is defined as: a set of decision trees (forest), with
one tree to predict each feature on each level. The
model instance will define the particular struc-
tures for each of the trees.
The forest consists of 18 decision trees, one for
each feature on the source and the target level: the
type feature, 4 vowel and 4 consonant features,
times 2 levels. Each node in such tree will ei-
ther be a leaf, or will be split by querying one of
the candidate contexts defined above. The cost of
coding the structure of the tree is one bit for every
node?to encode whether this node was split (is
an internal node) or is a leaf?plus? log 80 times
the number of internal nodes?to encode which
particular context was chosen to split that node.
We will explain how the best context to split on is
chosen in Sec. 4.6.
Each feature and level define a tree, e.g., the
?voiced? (X) feature of the source symbols cor-
responds to the source-X tree. A node N in this
tree holds a distribution over the values of X of
only those symbol instances in the complete data
that have reached in N by following the context
queries, starting from the root. The tree struc-
ture tells us precisely which path to follow?
completely determined by the context. For exam-
ple, when coding a symbol ? based on another
symbol found in the context of ??at some level
(say, target), some position (say, ?K), and one of
its features (say, M)?the next edge down the tree
is determined by that feature?s value; and so on,
down to a leaf. For an example of an actual deci-
sion tree learned by the model, see Fig. 5.
To compute the code length of the complete
data, we only need to take into account the dis-
tributions at the leaves. We could choose from a
variety of coding methods; the crucial point is that
the chosen code will assign a particular number?
the cost?to every possible alignment of the data.
This code-length, or cost, will then serve as the
objective function?i.e., it will be the value that
the algorithm will try to optimize. Each reduc-
tion in cost will correspond directly to reduction
in the entropy of the probability distribution of
the symbols, which in turn corresponds to more
certainty (i.e., regularity) in the correspondences
among the symbols, and to improvement in the
alignment. This is the link to our goal, and the
reason for introducing code lengths?it gives us
a single number that describes the quality of an
alignment.
We use Normalized Maximum Likelihood
(NML), (Rissanen, 1996) as our coding scheme.
We choose NML because it has certain optimal-
ity properties. Using NML, we code the distri-
bution at each leaf node separately, and summing
the costs of all leaves gives the total cost of the
aligned data?the value of our objective function.
Suppose n instances end up in a leaf node N ,
of the ?-level tree, for feature F having k val-
ues (e.g., consonants satisfying N ?s context con-
straints in the source-X tree, with k = 2 values:
? and +), and the values are distributed so that
ni instances have value i (with i ? {1, . . . , k}).
Then this requires an NML code-length of
LNML(?;F ;N) = ? logPNML(?;F ;N)
= ? log
?
i
(ni
n
)ni
C(n, k)
(1)
Here
?
i
(ni
n
)ni is the maximum likelihood of the
multinomial data at node N , and the term
C(n, k) =
?
n?1+...+n
?
k=n
?
i
(
n?i
n
)n?i
(2)
111
is a normalizing constant to make PNML a prob-
ability distribution.
In the MDL literature, e.g., (Gru?nwald, 2007),
the term ? logC(n, k) is called the stochastic
complexity or the (minimax) regret of the model,
(in this case, the multinomial model). The NML
distribution provides the unique solution to the
minimax problem posed in (Shtarkov, 1987),
min
P?
max
xn
log
P (xn|??(xn))
P? (xn)
(3)
where ??(xn) = arg max? P(xn) are the maxi-
mum likelihood parameters for the data xn. Thus,
PNML minimizes the worst-case regret, i.e., the
number of excess bits in the code as compared to
the best model in the model class, with hind-sight.
For details on the computation of this code length
see (Kontkanen and Myllyma?ki, 2007).
Learning the model from the observed data now
means aligning the word pairs and building the
decision trees in such a way as to minimize the
two-part code length: the sum of the model?s code
length?to encode the structure of the trees,?
and the data?s code length?to encode the aligned
word pairs, using these trees.
4.4 Summary of the Algorithm
The full learning algorithm runs as follows:
We start with an initial random alignment for
each pair of words in the corpus, i.e., for each
word pair choose some random path through the
matrix depicted in Figure 3.
From then on we alternate between two steps:
A. re-build the decision trees for all features on
source and target levels, and B. re-align all word
pairs in the corpus. Both of these operations
monotonically decrease the two-part cost function
and thus compress the data.
We continue until we reach convergence.
4.5 Re-alignment Procedure
To align source word ~? consisting of symbols
~? = [?1...?n], ~? ? ?? with target word ~? =
[?1...?m] we use dynamic programming. The
tree structures are considered fixed, as are the
alignments of all word pairs, except the one cur-
rently being aligned?which is subtracted from
the counts stored at the leaf nodes.
We now fill the matrix V , left-to-right, top-to-
bottom. Every possible alignment of ~? and ~? cor-
Figure 3: Dynamic programming matrix V, to search
for the most probable alignment
responds to exactly one path through this matrix:
starting with cost equal to 0 in the top-left cell,
moving only downward or rightward, and termi-
nating in the bottom-right cell. In this Viterbi-like
matrix, every cell corresponds to a partially com-
pleted alignment: reaching cell (i, j) means hav-
ing read off i symbols of the source word and j
symbols of the target. Each cell V (i, j)?marked
X in the Figure?stores the cost of the most prob-
able path so far: the most probable way to have
scanned ~? through symbol ?i and ~? through ?j :
V (i, j) = min
?
??
??
V (i, j ? 1) +L(. : ?j)
V (i? 1, j) +L(?i : .)
V (i? 1, j ? 1) +L(?i : ?j)
Each term V (?, ?) has been computed earlier by
the dynamic programming; the term L(?)?the
cost of aligning the two symbols, inserting or
deleting?is determined by the change in data
code length it induces to add this event to the cor-
responding leaf in all the feature trees it concerns.
In particular, the cost of the most probable com-
plete alignment of the two words will be stored in
the bottom-right cell, V (n,m), marked .
4.6 Building Decision Trees
Given a complete alignment of the data, we need
to build a decision tree, for each feature on both
levels, yielding the lowest two-part cost. The term
?decision tree? is meant in a probabilistic sense
here: instead of a single value, at each node we
store a distribution of the corresponding feature
values, over all instances that reach this node. The
distribution at a leaf is then used to code an in-
stance when it reaches the leaf in question. We
code the features in some fixed, pre-set order, and
source level before target level.
112
We now describe in detail the process of build-
ing the tree for feature X, for the source level, (we
will need do the same for all other features, on
both levels, as well). We build this tree as follows.
First, we collect all instances of consonants on the
source level, and gather the the counts for feature
X; and build an initial count vector; suppose it is:
value of X: + ?
1001 1002
This vector is stored at the root of the tree; the
cost of this node is computed using NML, eq. 1.
Next, we try to split this node, by finding such
a context that if we query the values of the feature
in that context, it will help us reduce the entropy
in this count vector. We check in turn all possi-
ble candidate contexts, (L,P, F ), and choose the
best one. Each candidate refers to some symbol
found on the source (?) or the target (? ) level, at
some relative position P , and to one of that sym-
bol?s features F . We will condition the split on
the possible values of F . For each candidate, we
try to split on its feature?s values, and collect the
resulting alignment counts.
Suppose one such candidate is (?, ?V, H),
i.e., (source-level, previous vowel, Horizontal fea-
ture), and suppose that the H-feature has two val-
ues: front/back. The vector at the root node (re-
call, this tree is for the X-feature) would then split
into two vectors, e.g.:
value of X: + ?
X | H=front 1000 1
X | H=back 1 1001
This would likely be a very good split, since
it reduces the entropy of the distribution in each
row almost to zero. The criterion that guides the
choice of the best candidate to use for splitting a
node is the sum of the code lengths of the resulting
split vectors, and the code length is proportional
to the entropy.
We go through all candidates exhaustively, and
greedily choose the one that yields the greatest re-
duction in entropy, and drop in cost. We proceed
recursively down the tree, trying to split nodes,
and stop when the total tree cost stops decreasing.
This completes the tree for feature X on level ?.
We build trees for all features and levels similarly,
from the current alignment of the complete data.
We augment the set of possible values at ev-
ery node with two additional special branches: 6=,
meaning the symbol at the queried position is of
the wrong type and does not have the queried fea-
ture, and #, meaning the query ran past the be-
ginning of the word.
 
0
 
20
 
40
 
60
 
80
 
100
 
120
 
140
 
160  0
 
500
 
1000
 
1500
 
2000
 
2500
 
3000
 
3500
Compressed size x1000 bits
Data 
size: 
numb
er of 
word 
pairs 
(averag
e word
-length
: 5.5 by
tes)G
zip Bzip2 1-1 m
odel
2-2 m
odel
Conte
xt mo
del
Figure 4: Comparison of compression power: Finnish-
Estonian data from SSA, using the context model vs.
the baseline models and standard compressors.
5 Evaluation and Results
One way to evaluate the presented models would
require a gold-standard aligned corpus; the mod-
els produce alignments which could be compared
to the gold-standard alignments, and we could
measure performance quantitatively, e.g., in terms
of accuracy. However, building a gold-standard
aligned corpus for the Uralic data proved to be
extremely difficult. In fact, it quickly becomes
clear that this problem is at least as difficult as
building a full reconstruction for all internal nodes
in the family tree (and probably harder), since it
requires full knowledge of all sound correspon-
dences within the family. It is also compounded
by the problem that the word-forms in the corpus
may contain morphological material that is ety-
mologically unrelated: some databases give ?dic-
tionary? forms, which contain extraneous affixes,
and thereby obscure which parts of a given word
form stand in etymological relationship with other
members in the cognates set, and which do not.
We therefore introduce other methods to evaluate
the models.
Compression: In figure 4, we compare the
context model, and use as baselines the standard
data compressors, Gzip and Bzip, as well as the
more basic models presented in (Wettig et al,
2011), (labeled ?1x1 and ?2x2?). We test the
compression of up to 3200 Finnish-Estonian word
pairs, from SSA. Gzip and Bzip compress data
113
fin khn kom man mar mrd saa udm ugr
est 0.26 0.66 0.64 0.65 0.61 0.57 0.57 0.62 0.62
fin 0.63 0.64 0.65 0.59 0.56 0.50 0.62 0.63
khn 0.65 0.58 0.69 0.64 0.67 0.66 0.66
kom 0.63 0.68 0.66 0.70 0.39 0.66
man 0.68 0.65 0.72 0.62 0.62
mar 0.65 0.69 0.65 0.66
mrd 0.58 0.66 0.63
saa 0.67 0.70
udm 0.65
Table 1: Pairwise normalized edit distances for Finno-
Ugric languages, on StarLing data (symmetrized by
averaging over the two directions of imputation).
by finding regularities in it (i.e., frequent sub-
strings). The comparison with Gzip is a ?san-
ity check?: we would like to confirm whether
our models find more regularity in the data than
would an off-the-shelf data compressor, that has
no knowledge that the words in the data are ety-
mologically related. Of course, our models know
that they should align pairs of consecutive lines.
This test shows that learning about the ?vertical?
correspondences achieves much better compres-
sion rates?allows the models to extract greater
regularity from the data.
Figure 5: Part of a tree, showing the rule for voicing of
medial plosives in Estonian, conditioned on Finnish.
Rules of correspondence: One our main goals
is to model rules of correspondence among lan-
guages. We can evaluate the models based on how
good they are at discovering rules. (Wettig et al,
2011) showed that aligning multiple symbols cap-
tures some of the context and thereby finds more
complex rules than their 1-1 alignment model.
However, certain alignments, such as t?t/d,
p?p/b, and k?k/g between Finnish and Esto-
nian, cannot be explained by the multiple-symbol
model. This is due to the rule of voicing of
word-medial plosives in Estonian. This rule could
be expressed in terms of Two-level Morphol-
ogy, (Koskenniemi, 1983) as: a voiceless plosive
in Finnish, may correspond to voiced in Esto-
nian, if not word-initial.3 The context model
finds this rule, shown in Fig. 5. This tree codes
the Target-level (i.e., Estonian) Voiced consonant
feature. In each node, the counts of correspond-
ing feature values are shown in brackets. In
the root node?prior to knowing anything about
the environment?there is almost complete un-
certainty (i.e., high entropy) about the value of
Voiced feature of an Estonian consonant: 821
voiceless to 801 voiced in our data. Redder nodes
indicate higher entropy, bluer nodes?lower en-
tropy. The query in the root node tells us to check
the context Finnish Itself Voiced for the most in-
formative clue about whether the current Estonian
consonant is voiced or not. Tracing the options
down left to right from the root, we obtain the
rules. The leftmost branch says, if the Finnish
is voiced (?), then the Estonian is almost cer-
tainly voiced as well?615 voiced to 2 voiceless
in this case. If the Finnish is voiceless (Finnish
Itself Voiced = 	), it says voicing may occur, but
only in the red nodes?i.e., only if preceded by
a voiced consonant on Estonian level (the branch
marked by ?, 56 cases), or?if previous posi-
tion is not a consonant (the 6= branch indicates
that the candidate?s query does not apply: i.e., the
sound found in that position is not a consonant)?
it can be voiced only if the corresponding Finnish
is a plosive (P, 78 cases). The blue nodes in this
branch say that otherwise, the Estonian consonant
almost certainly remains voiceless.
The context models discover numerous com-
plex rules for different language pairs. For ex-
ample, they learn a rule that initial Finnish k
?changes? (corresponds) to h in Hungarian, if it
is followed by a back vowel; the correspondence
between Komi trills and Udmurt sibilants; etc.
Imputation: We introduce a novel test of the
quality of the models, by using them to impute
unseen data, as follows. For a given model,
and a language pair (L1, L2)?e.g., (Finnish,
Estonian)?hold out one word pair, and train the
model on the remaining data. Then show the
model the hidden Finnish word and let it guess
3In fact, phonetically, in modern spoken Estonian, the
consonants that are written using the symbols b,d,g are not
technically voiced, but that is a finer point, we use this rule
for illustration of the principle.
114
the corresponding Estonian. Imputation can be
done for all models with a simple dynamic pro-
gramming algorithm, similar to the Viterbi-like
search used during training. Formally, given the
hidden Finnish string, the imputation procedure
selects from all possible Estonian strings the most
probable Estonian string, given the model. We
then compute an edit distance between the im-
puted sting and the true withheld Estonian word
(e.g., using the Levenshtein distance). We repeat
this procedure for all word pairs in the (L1, L2)
data set, sum the edit distances and normalize by
the total size of the (true) L2 data?this yields the
Normalized Edit Distance NED(L2|L1,M) be-
tween L1 and L2, under model M .
Imputation is a more intuitive measure of the
model?s quality than code length, with a clear
practical interpretation. NED is also the ultimate
test of the model?s quality. If model M im-
putes better than M ??i.e., NED(L2|L1,M) <
NED(L2|L1,M ?)?then it is difficult to argue
that M could be in any sense ?worse? than M ??
it has learned more about the regularities between
L1 and L2, and it knows more about L2 given
L1. The context model, which has much lower
cost than the baseline, almost always has lower
NED. This also yields an important insight: it
is an encouraging indication that optimizing the
code length is a good approach?the algorithm
does not optimize NED directly, and yet the cost
correlates strongly with NED, which is a simple
and intuitive measure of the model?s quality.
6 Discussion
We have presented a novel feature-based context-
aware MDL model, and a comparison of its per-
formance against prior models for the task of
alignment of etymological data. We have eval-
uated the models by examining the the rules of
correspondence that they discovers, by comparing
compression cost, imputation power and language
distances induced by the imputation. The models
take only the etymological data set as input, and
require no further linguistic assumptions. In this
regard, they is as objective as possible, given the
data. The data set itself, of course, may be highly
subjective and questionable.
The objectivity of models given the data now
opens new possibilities for comparing entire data
sets. For example, we can begin to compare the
Finnish and Estonian datasets in SSA vs. Star-
Ling, although the data sets have quite different
characteristics, e.g., different size?3200 vs. 800
word pairs, respectively?and the comparison is
done impartially, relying solely on the data pro-
vided. Another direct consequence of the pre-
sented methods is that they enable us to quantify
uncertainty of entries in the corpus of etymologi-
cal data. For example, for a given entry x in lan-
guage L1, we can compute exactly the probabil-
ity that x would be imputed by any of the models,
trained on all the remaining data from L1 plus any
other set of languages in the family. This can be
applied equally to any entry, in particular to en-
tries marked dubious by the database creators.
We can use this method to approach the ques-
tion of comparison of ?competing? etymological
datasets. The cost of an optimal alignment ob-
tained over a given data set serves as a measure of
its internal consistency.
We are currently working to combine the con-
text model with 3- and higher-dimensional mod-
els, and to extend these models to perform di-
achronic imputation, i.e., reconstruction of proto-
forms. We also intend to test the models on
databases of other language families.
Acknowledgments
We are very grateful to the anonymous reviewers
for their thoughtful and helpful comments. We
thank Suvi Hiltunen for the implementation of the
models, and Arto Vihavainen for implementing
some of the earlier models. This research was
supported by the Uralink Project, funded by the
Academy of Finland and by the Russian Fund for
the Humanities.
References
Raimo Anttila. 1989. Historical and comparative lin-
guistics. John Benjamins.
Franc?ois G. Barbanc?on, Tandy Warnow, Don Ringe,
Steven N. Evans, and Luay Nakhleh. 2009. An ex-
perimental study comparing linguistic phylogenetic
reconstruction methods. In Proceedings of the Con-
ference on Languages and Genes, UC Santa Bar-
bara. Cambridge University Press.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi.
2009. A new objective function for word alignment.
In Proc. NAACL Workshop on Integer Linear Pro-
gramming for NLP.
Alexandre Bouchard-Co?te?, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2007. A probabilistic ap-
115
proach to diachronic phonology. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 887?896, Prague, June.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert. L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Peter Gru?nwald. 2007. The Minimum Description
Length Principle. MIT Press.
Erkki Itkonen and Ulla-Maija Kulonen. 2000.
Suomen Sanojen Alkupera? (The Origin of Finnish
Words). Suomalaisen Kirjallisuuden Seura,
Helsinki, Finland.
Brett Kessler. 2001. The Significance of Word Lists:
Statistical Tests for Investigating Historical Con-
nections Between Languages. The University of
Chicago Press, Stanford, CA.
Grzegorz Kondrak. 2002. Determining recur-
rent sound correspondences by inducing translation
models. In Proceedings of COLING 2002: 19th In-
ternational Conference on Computational Linguis-
tics, pages 488?494, Taipei, August.
Grzegorz Kondrak. 2003. Identifying complex sound
correspondences in bilingual wordlists. In A. Gel-
bukh, editor, Computational Linguistics and Intel-
ligent Text Processing (CICLing-2003), pages 432?
443, Mexico City, February. Springer-Verlag Lec-
ture Notes in Computer Science, No. 2588.
Grzegorz Kondrak. 2004. Combining evidence in
cognate identification. In Proceedings of the Sev-
enteenth Canadian Conference on Artificial Intelli-
gence (Canadian AI 2004), pages 44?59, London,
Ontario, May. Lecture Notes in Computer Science
3060, Springer-Verlag.
Petri Kontkanen and Petri Myllyma?ki. 2007. A
linear-time algorithm for computing the multino-
mial stochastic complexity. Information Processing
Letters, 103(6):227?233.
Kimmo Koskenniemi. 1983. Two-level morphol-
ogy: A general computational model for word-form
recognition and production. Ph.D. thesis, Univer-
sity of Helsinki, Finland.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In The
Second Conference on Empirical Methods in Nat-
ural Language Processing, pages 97?108, Hissar,
Bulgaria.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Luay Nakhleh, Don Ringe, and Tandy Warnow. 2005.
Perfect phylogenetic networks: A new methodol-
ogy for reconstructing the evolutionary history of
natural languages. Language (Journal of the Lin-
guistic Society of America), 81(2):382?420.
Ka?roly Re?dei. 1988?1991. Uralisches etymologisches
Wo?rterbuch. Harrassowitz, Wiesbaden.
Don Ringe, Tandy Warnow, and A. Taylor. 2002.
Indo-European and computational cladis-
tics. Transactions of the Philological Society,
100(1):59?129.
Jorma Rissanen. 1996. Fisher information and
stochastic complexity. IEEE Transactions on Infor-
mation Theory, 42(1):40?47, January.
Yuri M. Shtarkov. 1987. Universal sequential coding
of single messages. Problems of Information Trans-
mission, 23:3?17.
Sergei A. Starostin. 2005. Tower of babel: Etymolog-
ical databases. http://newstar.rinet.ru/.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in sta-
tistical translation. In Proceedings of 16th Confer-
ence on Computational Linguistics (COLING 96),
Copenhagen, Denmark, August.
Hannes Wettig, Suvi Hiltunen, and Roman Yangarber.
2011. MDL-based Models for Alignment of Et-
ymological Data. In Proceedings of RANLP: the
8th Conference on Recent Advances in Natural Lan-
guage Processing, Hissar, Bulgaria.
116
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 73?81,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Automatic Detection of Stable Grammatical Features in N-Grams
Mikhail Kopotev1 Lidia Pivovarova1,2 Natalia Kochetkova3 Roman Yangarber1
1 University of Helsinki, Finland
2 St.Petersburg State University, Russia
3 Moscow Institute of Electronics and Mathematics, NRU HSE, Russia
Abstract
This paper presents an algorithm that allows
the user to issue a query pattern, collects
multi-word expressions (MWEs) that match
the pattern, and then ranks them in a uniform
fashion. This is achieved by quantifying the
strength of all possible relations between the
tokens and their features in the MWEs. The al-
gorithm collects the frequency of morphologi-
cal categories of the given pattern on a unified
scale in order to choose the stable categories
and their values. For every part of speech, and
for all of its categories, we calculate a normal-
ized Kullback-Leibler divergence between the
category?s distribution in the pattern and its
distribution in the corpus overall. Categories
with the largest divergence are considered to
be the most significant. The particular values
of the categories are sorted according to a fre-
quency ratio. As a result, we obtain morpho-
syntactic profiles of a given pattern, which in-
cludes the most stable category of the pattern,
and their values.
1 Introduction
In n-grams, the relations among words and among
their grammatical categories cover a wide spectrum,
ranging from idioms to syntactic units, such as a
verb phrase. In most cases, the words are linked to-
gether by both grammatical and lexical relations. It
is difficult to decide, which relation is stronger in
each particular case. For example, in the idiomatic
phrase meet the eye, the relationship is lexical rather
than grammatical. A phrasal verb meet up is similar
to single-word verbs and has its own meaning. It can
be interpreted as one lexeme, spelled as two words.
On the other hand, phrases like meet the require-
ments, meet the specifications, meet the demands
are traditionally called ?collocations.? However, the
question arises about the role played by the noun fol-
lowing the verb: is it a lexically free direct object,
or a part of stable lexical unit, or to some extend
both? These words are bound by both grammatical
and lexical relations, and we assume that the major-
ity of word combinations in any language have such
a dual nature.
Lastly, the relationship between the words in the
English phrase meet her differs from those above in
that it may be described as purely grammatical?the
verb meet receives a direct object.
Distinguishing collocations, i.e. ?co-occurrences
of words? from colligations, i.e. ?co-occurrence of
word forms with grammatical phenomena? (Gries
and Divjak, 2009) is not always a simple task; there
is no clear boundary between various types of word
combinations inasmuch as they can be simultane-
ously a collocation and a colligation?this type of
MWE is called collostructions in (Stefanowitsch and
Gries, 2003). It was proposed that language as such
is a ?constructicon? (Goldberg, 2006), which means
that fusion is its core nature. For this reason, devis-
ing formal methods to measure the strength of mor-
phological or lexical relations between words be-
comes a challenge.
Our approach aims to treat multi-word expres-
sions (MWEs) of various nature?idioms, multi-
word lexemes, collocations and colligations?on an
equal basis, and to compare the strength of vari-
ous possible relations between the tokens in a MWE
quantitatively. We search for ?the underlying cause?
73
for the frequent co-occurrence of certain words:
whether it is due to their morphological categories,
or lexical compatibility, or a combination of both. In
this paper, however, we focus on colligations, ignor-
ing collocations and collostructions.
For languages with rich morphology the situation
is more complicated, because each word may have
several morphological categories that are not inde-
pendent and interact with each other. This paper fo-
cuses on Russian, which not only has free word or-
der and rich morphology,1 but is also a language that
is well-investigated. A good number of corpora and
reference grammars are available to be used for eval-
uation. The data we use in this work is the n-gram
corpus, extracted from a deeply annotated and care-
fully disambiguated (partly manually) sub-corpus of
the Russian National Corpus (RNC). The size of dis-
ambiguated corpus used in this paper is 5 944 188
words of running text.
2 Related Work
Much effort has been invested in automatic extrac-
tion of MWEs from text. A great variety of method
are used, depending on the data, the particular tasks
and the types of MWEs to be extracted. Pecina
(2005) surveys 87 statistical measures and meth-
ods, and even that is not a complete list. The
most frequently used metrics, inter alia, are Mu-
tual Information (MI), (Church and Hanks, 1990), t-
score (Church et al, 1991), and log-likelihood (Dun-
ning, 1993). The common disadvantage of these is
their dependency on the number of words included
in the MWE. Although there is a large number of
papers that use MI for bigram extraction, only a few
use the MI measure for three or more collocates,
e.g., (Tadic? and S?ojat, 2003; Wermter and Hahn,
2006; Kilgarriff et al, 2012),
Frantzi et al (2000) introduced the c-value and
nc-value measures to extract terms of different
lengths. Daudaravicius (2010) has developed a
promising method that recognizes collocations in
text. Rather than extracting MWEs, this method cuts
the text into a sequence of MWEs of length from
1 to 7 words; the algorithm may produce different
1The Multitext-East specification, which aims to create an
unified cross-language annotation scheme, defines 156 morpho-
syntactic tags for Russian as compared to 80 tags for English
(http://nl.ijs.si/ME/V4/msd/html).
chunking for the same segment of text within dif-
ferent corpora. Nevertheless, extraction of variable-
length MWE is a challenging task; the majority of
papers in the field still use measures that take the
number of collocates as a core parameter.
Entropy and other probabilistic measures have
been used for MWE extraction since the earliest
work. For example, the main idea in (Shimohata et
al., 1997; Resnik, 1997), is that the MWE?s idiosyn-
crasy, (Sag et al, 2002), is reflected in the distribu-
tions of the collocates. Ramisch et al (2008) intro-
duced the Entropy of Permutation and Insertion:
EPI = ?
m?
a=0
p(ngrama) log[p(ngrama)] (1)
where ngram0 is the original MWE, and ngrama
are its syntactically acceptable permutations.
Kullback-Leibler divergence was proposed
by Resnik (1997) to measure selective prefer-
ence for the word sense disambiguation (WSD)
task. Fazly and Stevenson (2007) applied a set of
statistical measures to classify verb+noun MWEs
and used Kullback-Leibler divergence, among other
methods, to measure the syntactic cohesion of a
word combination. Van de Cruys and Moiro?n
(2007) used normalized Kullback-Leibler diver-
gence to find idiomatic expression with verbs in
Dutch.
Russian MWE-studies have emerged over the last
decade. Khokhlova and Zakharov (2009) applied
MI, t-score and log-likelihood to extract verb collo-
cations; Yagunova and Pivovarova (2010) studied
the difference between Russian lemma/token col-
locations and also between various genres; Do-
brov and Loukachevitch (2011) implemented term
extraction algorithms. However, there is a lack of
study of both colligations and collostructions in Rus-
sian. The only work known to us is by Sharoff
(2004), who applied the MI-score to extract prepo-
sitional phrases; however, the only category he used
was the POS.
As far as we aware, the algorithm we present in
this paper has not been applied to Russian or to other
languages.
3 Method
The input for our system is any n-gram of length 2?
4, where one position is a gap?the algorithm aims
74
Figure 1: Distributions of noun cases in the corpus and in
a sample?following the preposition ??? (in)
Figure 2: Distributions of nominal gender in the corpus
and in a sample?following the preposition ??? (in)
to find the most stable morphological categories of
words that can fill this gap. Moreover, the user can
specify the particular properties of words that can fill
the gap?for example, specify that the output should
include only plural nouns. Thus, the combination of
the surrounding words and morphological constrains
form an initial query pattern for the algorithm.
Our model tries to capture the difference between
distributions of linguistic features in the general cor-
pus as compared to distributions within the given
pattern. For example, Figure 1 shows the distribu-
tion of cases in the corpus overall vs. their distribu-
tion in words following the preposition ??? (in/into).
Figure 2 shows the corresponding distributions of
gender. Gender is distributed similarly in the cor-
pus and in the sample restricted by the pattern; by
contrast, the distribution of cases is clearly different.
This is due to the fact that the preposition governs
the case of the noun, but has no effect on gender. To
measure this difference between the distributions we
use the Kullback-Leibler divergence:
Div(C) =
N?
i=1
P patterni ? log(
P patterni
P corpusi
) (2)
where C is the morphological category in a
pattern?e.g., case or gender,?having the values
1..N , P patterni is the relative frequency of value i
restricted by the pattern, and P corpusi is the relative
frequency of the same value in the general corpus.
Since the number of possible values for a category is
variable?e.g., eleven for case, four for gender, and
hundreds of thousands for lemmas?the divergence
needs to be normalized. The normalization could
be done in various ways, e.g., against the entropy or
some maximal divergence in the data; in our experi-
ments, the best results were obtained using a variant
proposed in (Bigi, 2003), where the divergence be-
tween the corpus distribution and the uniform distri-
bution is used as the normalizing factor:
NormDiv(C) =
Div(C)
E(C) + log(n)
(3)
where E(C) is the entropy of category C and n is
the number of possible values of C; the term log(n)
is the entropy of the uniform distribution over n out-
comes (which is the maximal entropy). The category
with the highest value of normalized divergence is
seen as maximally preferred by the pattern.
However, divergence is unable to determine the
exact values of the category, and some of these val-
ues are clearly unreliable even if they seem to ap-
pear in the pattern. For example, Figure 1 shows
that preposition ??? (in) in the data is sometimes
followed by the nominative case, which is grammat-
ically impossible. This is due to a certain amount of
noise, which is unavoidable in a large corpus due to
mark-up errors or inherent morphological ambigu-
ity. In Russian, the nominative and accusative cases
often syncretize (assume identical forms), which can
cause inaccuracies in annotation. On the other hand,
some values of a category can be extremely rare;
thus, they will be rare within patterns as well. For
instance, the so-called ?second accusative? case (la-
beled ?acc2? in Figure 1) is rare in modern Russian,
75
which is why its appearance in combination with
preposition ??? (in) is significant, even though its
frequency is not much higher than the frequency of
the (erroneous) nominative case in the same pattern.
To find the significant values of a particular cate-
gory we use the ratio between the frequencies of the
value in a sample and in the corpus:
frequency ratio =
P patterni
P corpusi
(4)
If frequentcy ratio > 1, then the category?s value
is assumed to be selected by the pattern.
Finally, we note that the distribution of POS varies
considerably within every pattern as compared to its
distribution in the corpus. For example, prepositions
can be followed only by noun groups and can never
be followed by verbs or conjunctions. This means
the Kullback-Leibler divergence for any POS, nat-
urally assumes the highest value in any pattern; for
this reason, we exclude the POS category from con-
sideration in our calculation, aiming to find more
subtle and interesting regularities in the data.
To summarize, the algorithm works as follows:
for a given query pattern
1. search all words that appear in the query pattern
and group them according to their POS tags.
2. for every POS, calculate the normalized
Kullback-Leibler divergence for all of its cat-
egories; categories that show the maximum di-
vergence are considered to be the most signifi-
cant for the given pattern;
3. for every relevant category, sort its values ac-
cording to the frequency ratio; if frequency ra-
tio is less than 1, the value considered to be ir-
relevant for this pattern.
4 Experiments
In this paper, we conduct an in-depth evaluation fo-
cusing on a limited number of linguistic phenom-
ena, namely: bigrams beginning with single-token
prepositions, which impose strong morpho-syntactic
constraints in terms of case government. We in-
vestigate 25 prepositions, such as ????? (without),
??? (in/to), etc. We evaluate the corpus of bi-
grams systematically against these queries, although
we expect that the model we propose here pro-
duces relevant results for a much wider range of
constructions?to be confirmed in further work.
4.1 Prepositions and Morphological Category
A syntactic property of prepositions in Russian is
that they govern nominal phrases, i.e., that we expect
the largest normalized divergence in queries such as
{ Preposition + X }, where the POS of X is noun,
to occur exactly with the category of case. Figure 3
shows the normalized divergence for four lexical and
morphological categories. Among them, Case has
the maximal divergence for all prepositions, which
matches our expectation with 100% accuracy.
According to the figure, the morphological cat-
egory of Animacy2 is also interesting, in that it
has a high value for some prepositions, like ???-
???? (from under), ????? (under), ????? (above).
A good example is the preposition ???-???? (from
under). Its semantic properties cause inanimate
nouns to appear much more frequently than ani-
mate ones. Consequently, we observe a higher diver-
gence, due to inanimate nouns like ???-??? ??????
(from under ground), ???-??? ?????? (from under
the snow), etc. Another good example of hidden
semantic properties is a pair of prepositions ?????
(under) and ????? (above). One can expect that
their syntactic behaviour is more or less similar,
but the histogram shows that Animacy (surprisingly)
has a much higher divergence for ????? (under) to
be ignored. Indeed, a deeper corpus-based anal-
ysis reveals a stable, frequently used construction,
which gives many points to animate nouns, e.g.,
???????????????? ??? ???????? (disguised as a
bride). It is notable that this particular effect is not
mentioned in any grammar book, (to the best of our
knowledge).
To conclude, the Case category is the clear win-
ner in terms of having the greatest normalized di-
vergence, and the output fully matches the expecta-
tion on all 25 common prepositions that we tested.
Other results are also clearly interesting due to their
links to semantic properties, that is, to colloca-
tions. The next task is, therefore to discriminate
2Animacy is a morphological category of Russian nouns
based on whether the referent of the noun is considered sen-
tient or living. Most nouns denoting humans and animals are
animate, while the majority of other nouns are inanimate.
76
Figure 3: Normalized divergence of noun categories (grammemes) for pattern preposition+X.
between the runners-up, like Animacy for ?????
(under), which seem to be interesting to some ex-
tent, and clear losers like Gender, in the example
above. To do that we need to find an appropriate
threshold?preferably automatically?between rel-
evant and non-relevant results. The algorithm ranks
the categories according to their divergence; the cat-
egory that has the top rank is certainly meaning-
ful. The question is how to determine which among
the rest are significant as well; this is left for future
work.
4.2 Specific Values of the Category with
Maximum Divergence
The next question we explore is which particular
values of the maximally divergent category?here,
Case?are selected by a given preposition. As we
mentioned above, we use the frequency ratio for this
task. We collected a list of cases3 that appear af-
ter the given preposition, according to the algorithm
with frequency ratio > 1; which cases are pos-
sible according to grammatical descriptions,4 which
3The current annotation scheme of our data has eleven case
tags, namely: nom, voc, gen, gen2, dat, acc, acc2, ins, loc, loc2,
adnum.
4Note, that not all possible prep+case combinations are rep-
resented in the corpus; for example, the combination { ??????
(for the sake of) + gen2 } does not appear in our data, and only
eight times in the RNC overall. For evaluation we take into
cases were produced by the algorithm, and the num-
ber of correct cases in the system?s response. We
expect that by using the frequency ratio we can re-
duce the noise; for example, of the eight cases that
match the pattern { ?c? (with) + Noun } only four
are relevant.
The algorithm predicts the correct relevant set for
21 of 25 prepositions, giving a total precision of
95%, recall of 89%, and F-measure of 92%. The
prepositions highlighted in bold in Table 1 are those
that were incorrectly processed for various reasons;
the error analysis is presented below.
14: ??? (about) The algorithm unexpectedly flags
the voc (vocative) as a possible case after this prepo-
sition. This is incorrect; checking the data we dis-
covered that this mistake was due to erroneous an-
notation: the interjection ?o? (oh), as in ?O ????!?
(Oh God!), is incorrectly annotated as the preposi-
tion ?o? (about). The error occurs twice in the data.
However, as the vocative is extremely rare in the data
(its frequency in the corpus is less than 0,0004), two
erroneous tags are sufficient to give it a high rank.
Similar annotation errors for more frequent cases are
eliminated by the algorithm. For example, as we
mentioned in the previous section, the nominative
consideration only those prep+case combinations that appear at
least once in our dataset.
77
Preposition Meaning Expected cases Response
1 ??? without gen/gen2 gen/gen2
2 ? in/into acc/acc2/loc/loc2 acc/acc2/loc/loc2
3 ??? for gen/gen2 gen/gen2
4 ?? until gen/gen2 gen/gen2
5 ?? behind acc/ins acc/ins
6 ?? from gen/gen2 gen/gen2
7 ??-?? from behind gen/gen2 gen/gen2
8 ??-??? from under gen/gen2 gen/gen2
9 ? to dat dat
10 ????? beyond gen gen
11 ????? between ins ins
12 ?? on acc/loc/loc2 acc/loc/loc2
13 ??? above ins ins
14 ? about acc/loc loc/voc
15 ?? from gen/gen2 gen/gen2
16 ????? in front of ins ins
17 ???? in front of ins ins
18 ?? by/up to dat/loc/acc dat
19 ??? under acc/ins acc/ins
20 ??? at/by loc loc
21 ??? about acc acc
22 ???? for gen gen
23 ? with gen/gen2/acc/ins gen2/ins
24 ? near gen gen
25 ????? through acc acc/adnum
Expected 45
Response 42
Correct 40
Precision 0.95
Recall 0.89
F-measure 0.92
Table 1: Noun cases expected and returned by the algorithm for Russian prepositions.
case after preposition ??? (in) appears 88 times in
our data; however this case is not returned by the al-
gorithm, since it is below the frequency ratio thresh-
old.
25: ??????? (through/past) The adnumerative
(adnum) is a rare case in our data, so even a single
occurrence in a sample is considered important by
the algorithm. A single bigram is found in the data,
where the token ?????? (hours)?correctly anno-
tated with the adnum tag?predictably depends on
the Numeral, i.e., ????? (two), rather than on prepo-
sition ??????? (through/past), see Figure 4. The
numeral appears in post-position?a highly marked
word order that is admissible in this colloquial con-
struction in Russian: ?????? ???? ???? (lit.: after
hours two = idiom: after about two hours), where
Figure 4: Distributions of cases in the corpus and in a
sample. (Arrows indicate syntactic dependency.)
the preposition governs the Case of the numeral, and
the numeral governs a noun that precedes it.
Because our algorithm at the moment processes
linear sequences, these kinds of syntactic inversion
phenomena in Russian will pose a challenge. In gen-
eral this problem can be solved by using tree-banks
for MWE extraction, (Seretan, 2008; Martens and
Vandeghinste, 2010). However, an appropriate tree-
78
bank is not always available for a given language; in
fact, we do not have access to any Russian tree-bank
suitable for this task.
23: ??? (with) This is a genuine error. The algo-
rithm misses two of four correct cases, Genitive and
Accusative, because both are widely used across the
corpus, which reduces their frequency ratio in the
sub-sample. Our further work will focus on finding
flexible frequency ratio thresholds, which is now set
to one. Two of the correct cases (Instrumental and
Gen2) are well over the threshold, while Genitive,
with 0.6924, and Accusative, with 0.0440, fall short.
18: ???? (by/along) For this preposition the al-
gorithm predicts 1 case out of 3. This situation is
slightly different from the previous ones, since the
accusative and locative cases are much more rare
with preposition ???? (by/along) than the dative:
245 instances out of 15387 for accusative, and 222
for locative in our data. We hypothesize that this
means that such ?Prep+case? combinations are con-
strained lexically to a greater extent than grammat-
ically. To check this hypothesis we calculate the
frequency ratio for all lemmas that appear with the
respective patterns { ???? (by/along) + acc } and
{ ???? (by/along) + loc }. As a result, 15 distinct
lemmas were extracted by { ???? (by) + acc }; 13
out of them have frequency ratio > 1. The major-
ity of the lemmas belong to the semantic class ?part
of the body? and are used in a very specific Rus-
sian construction, which indicates ?an approximate
level?, e.g. ??? ??????? (up to (one?s) elbow), cf.
English ?up to one?s neck in work?. This construc-
tion has limited productivity, and we are satisfied
that the Accusative is omitted in the output for gram-
matical categories, since the algorithm outputs all
tokens that appear in the { ???? (by/along) + acc }
as relevant lemmas.
The case of { ???? (by) + loc } is more com-
plex: 44 of 76 combinations return a frequency
greater than 1. Analysis of annotation errors reveals
a compact collection of bureaucratic cliches, like
??? ????????? (upon arrival), ??? ??????????
(upon completion), etc., which all share the seman-
tics of ?immediately following X?, and are pragmat-
ically related. These are expressions belonging to
the same bureaucratic jargon and sharing the same
morphological pattern, however, they are below the
threshold. Again, we are faced with need to tune the
threshold to capture this kind of potentially interest-
ing lexical combinations. In general, semantic and
pragmatic factors influence the ability of words to
combine, and the algorithm shows it in some way,
though these aspects of the problem are beyond the
scope of our experiments in the current stage.
5 Discussion and Future Work
5.1 Development of the algorithm
We have presented a part an overall system under de-
velopment. In the preceding sections, we investigate
an area where collocations and colligations meet. To
summarize, the algorithm, based on the corpus of n-
grams, treats both morpho-syntactic and lexical co-
occurrences as a unified continuum, which has no
clear borders. The evaluation of the morphological
output raises some new questions for further devel-
opment:
? At present, the low precision for both low- and
high-frequency tags depends on the threshold,
which needs to be studied further.
? The values of divergences are currently not
normalized among the different query patterns.
This may be a difficult question, and we plan to
investigate this further. The algorithm provides
a way to compare the strength of very diverse
collocations, which have nothing in common,
in terms of their degree of idiomatization.
? We observe that the longer the n-gram, the
more we expect it to be a collocation; stable
bigrams appear more frequently to be colliga-
tions, while stable 4-grams are more often col-
locations. The problem is that those colloca-
tions with a highly frequent first collocate, e.g.,
??? (in), cannot be found using our algorithm
as it stands now.
? Token/lexeme stability is the next task we will
concentrate on. Wermter and Hahn (2006) and
Kilgarriff et al (2012) proposed that sorting
tokens/lexemes according to plain frequency
works well if there is no grammatical knowl-
edge at hand. We do have such knowledge. To
improve the accuracy of lexeme/token extrac-
tion we rely on the idea of grammatical pro-
79
files, introduced by Gries and Divjak (2009).
We plan to develop this approach with the
further assumption that the distribution of to-
kens/lexemes within a pattern is based on rel-
evant grammatical properties, which are ob-
tained in an earlier step of our algorithm. For
instance, for ??? ?? X? (not up to X) we have
found that the grammatical profile for X is
N.gen/gen2, and the token frequency ratio is
greater than 1 as well. Building the list of to-
kens that are the most stable for this pattern, we
compare their distributions within the pattern to
all N.gen/gen2 tokens in the corpus. This yields
the following tokens as the most relevant: ???
?? ?????? (lit.: not up to laughter.gen = id-
iom: no laughing matter);??? ?? ????? (lit.
not up to fat.gen2 = idiom: no time/place for
complacency), which reveals an interesting set
of idioms.
5.2 Extensions and Applications
The model has no restriction on the length of data
to be used, and is applicable to various languages.
Finnish (which is morphologically rich) and English
(morphologically poor) will be examined next. As
for Russian, so far the algorithm has been systemat-
ically evaluated against bigrams, although we have
3-, 4- and 5-grams at our disposal for future work.
A reliable method that is able to determine pat-
terns of frequently co-occurring lexical and gram-
matical features within a corpus can have far-
reaching practical implications. One particular ap-
plication that we are exploring is the fine-tuning
of semantic patterns that are commonly used in in-
formation extraction (IE), (Grishman, 2003). Our
work on IE focuses on different domains and differ-
ent languages, (Yangarber et al, 2007; Atkinson et
al., 2011). Analysis of MWEs that occur in extrac-
tion patterns would provide valuable insights into
how the patterns depend on the particular style or
genre of the corpus, (Huttunen et al, 2002). Subtle,
genre-specific differences in expression can indicate
whether a given piece of text is signaling the pres-
ence an event of interest.
5.3 Creating Teaching-Support Tools
Instructors teaching a foreign language are regu-
larly asked how words co-occur: What cases and
word forms appear after a given preposition? Which
ones should I learn by rote and which ones follow
rules? The persistence of such questions indicates
that this is an important challenge to be addressed?
we should aim to build a system that can automati-
cally generate an integrated answer. A tool that pro-
duces answers to these questions would be of great
help for teachers as well as students. The presented
algorithm can support an easy-to-use Web-based ap-
plication, or an application for a mobile device. We
plan to develop a service, which is able to process
queries described in the paper. This service would
be an additional interface to a corpus, aimed at find-
ing not only the linear context of words but also their
collocational and constructional preferences. We be-
lieve that such an interface would be useful for both
research and language-learning needs.
Acknowledgments
We are very grateful to the Russian National Cor-
pus developers, especially E. Rakhilina and O. Lya-
shevskaya, for providing us with the data.
References
Martin Atkinson, Jakub Piskorski, Erik van der Goot, and
Roman Yangarber. 2011. Multilingual real-time event
extraction for border security intelligence gathering.
In U. Kock Wiil, editor, Counterterrorism and Open
Source Intelligence, pages 355?390. Springer Lecture
Notes in Social Networks, Vol. 2, 1st edition.
Brigitte Bigi. 2003. Using Kullback-Leibler distance
for text categorization. In Fabrizio Sebastiani, edi-
tor, Advances in Information Retrieval, volume 2633
of Lecture Notes in Computer Science, pages 305?319.
Springer Berlin, Heidelberg.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational linguistics, 16(1):22?29.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Kindle. 1991. Using statistics in lexical analy-
sis. Lexical acquisition: exploiting on-line resources
to build a lexicon.
Vidas Daudaravicius. 2010. Automatic identification of
lexical units. Computational Linguistics and Intelli-
gent text processing CICling-2009.
Boris Dobrov and Natalia Loukachevitch. 2011. Mul-
tiple evidence for term extraction in broad domains.
In Proceedings of the 8th Recent Advances in Natu-
ral Language Processing Conference (RANLP 2011).
Hissar, Bulgaria, pages 710?715.
80
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
Afsaneh Fazly and Suzanne Stevenson. 2007. Dis-
tinguishing subtypes of multiword expressions using
linguistically-motivated statistical measures. In Pro-
ceedings of the Workshop on A Broader Perspective on
Multiword Expressions, pages 9?16. Association for
Computational Linguistics.
Katerina Frantzi, Sophia Ananiadou, and Hideki Mima.
2000. Automatic recognition of multi-word terms:
the c-value/nc-value method. International Journal on
Digital Libraries, 3(2):115?130.
Adele Goldberg. 2006. Constructions at work: The na-
ture of generalization in language. Oxford University
Press, USA.
Stefan Th. Gries and Dagmar Divjak. 2009. Behavioral
profiles: a corpus-based approach to cognitive seman-
tic analysis. New directions in cognitive linguistics,
pages 57?75.
Ralph Grishman. 2003. Information extraction. In
The Handbook of Computational Linguistics and Nat-
ural Language Processing, pages 515?530. Wiley-
Blackwell.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002. Diversity of scenarios in information extraction.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC 2002),
Las Palmas de Gran Canaria, Spain, May.
Maria Khokhlova and Viktor Zakharov. 2009. Statistical
collocability of Russian verbs. After Half a Century
of Slavonic Natural Language Processing, pages 125?
132.
Adam Kilgarriff, Pavel Rychly`, Vojtech Kova?r, and V?t
Baisa. 2012. Finding multiwords of more than two
words. In Proceedings of EURALEX2012.
Scott Martens and Vincent Vandeghinste. 2010. An effi-
cient, generic approach to extracting multi-word ex-
pressions from dependency trees. In CoLing Work-
shop: Multiword Expressions: From Theory to Appli-
cations (MWE 2010).
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings of the
ACL Student Research Workshop, pages 13?18. Asso-
ciation for Computational Linguistics.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for
the extraction of multiword expressions. In Proceed-
ings of the LREC Workshop-Towards a Shared Task for
Multiword Expressions (MWE 2008), pages 50?53.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52?57. Washington, DC.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. Computational Lin-
guistics and Intelligent Text Processing, pages 189?
206.
Violeta Seretan. 2008. Collocation extraction based on
syntactic parsing. Ph.D. thesis, University of Geneva.
Serge Sharoff. 2004. What is at stake: a case study of
Russian expressions starting with a preposition. In
Proceedings of the Workshop on Multiword Expres-
sions: Integrating Processing, pages 17?23. Associ-
ation for Computational Linguistics.
Sayori Shimohata, Toshiyuki Sugio, and Junji Nagata.
1997. Retrieving collocations by co-occurrences and
word order constraints. In Proceedings of the eighth
conference on European chapter of the Association for
Computational Linguistics, pages 476?481. Associa-
tion for Computational Linguistics.
Anatol Stefanowitsch and Stefan Th Gries. 2003. Col-
lostructions: Investigating the interaction of words and
constructions. International journal of corpus linguis-
tics, 8(2):209?243.
Marko Tadic? and Kres?imir S?ojat. 2003. Finding multi-
word term candidates in Croatian. In Proceedings of
IESL2003 Workshop, pages 102?107.
Tim Van de Cruys and Begona Villada Moiro?n. 2007.
Lexico-semantic multiword expression extraction. In
Proceedings of the 17th Meeting of Computational
Linguistics in the Netherlands (CLIN), pages 175?190.
Joachim Wermter and Udo Hahn. 2006. You can?t beat
frequency (unless you use linguistic knowledge) ? a
qualitative evaluation of association measures for col-
location and term extraction. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 785?792.
Elena Yagunova and Lidia Pivovarova. 2010. The nature
of collocations in the Russian language. The experi-
ence of automatic extraction and classification of the
material of news texts. Automatic Documentation and
Mathematical Linguistics, 44(3):164?175.
Roman Yangarber, Clive Best, Peter von Etter, Flavio
Fuart, David Horby, and Ralf Steinberger. 2007.
Combining information about epidemic threats from
multiple sources. In Proceedings of the MMIES
Workshop, International Conference on Recent Ad-
vances in Natural Language Processing (RANLP
2007), Borovets, Bulgaria, September.
81
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 29?37,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Event representation across genre
Lidia Pivovarova, Silja Huttunen and Roman Yangarber
University of Helsinki
Finland
Abstract
This paper describes an approach for investi-
gating the representation of events and their
distribution in a corpus. We collect and
analyze statistics about subject-verb-object
triplets and their content, which helps us com-
pare corpora belonging to the same domain
but to different genre/text type. We argue that
event structure is strongly related to the genre
of the corpus, and propose statistical proper-
ties that are able to capture these genre differ-
ences. The results obtained can be used for the
improvement of Information Extraction.
1 Introduction
The focus of this paper is collecting data about
certain characteristics of events found in text, in
order to improve the performance of an Infor-
mation Extraction (IE) system. IE is a tech-
nology used for locating and extracting specific
pieces of information?or ?facts??from unstruc-
tured natural-language text, by transforming the
facts into abstract, structured objects, called events.
In IE we assume that events represent real-world
facts and the main objective is to extract them from
plain text; the nature of the events themselves rarely
receives in-depth attention in current research.
Events may have various relationships to real-
world facts, and different sources may have contra-
dictory views on the facts, (Saur?? and Pustejovsky,
2012). Similarly to many other linguistic units, an
event is a combination of meaning and form; the
structure and content of an event is influenced by
both the structure of the corresponding real-world
fact and by the properties of the surrounding text.
We use the notion of scenario to denote a set
of structured events of interest in a real-world do-
main: e.g., the MUC Management Succession sce-
nario, (Grishman and Sundheim, 1996), within the
broader Business domain.
The representation and the structure of events in
text depends on the scenario. For example, Huttunen
et al (2002a; Huttunen et al (2002b) points out that
?classic? MUC scenarios, such as Management Suc-
cession or Terrorist Attacks, describe events that oc-
cur in a specific point in time, whereas other sce-
narios like Natural Disaster or Disease Outbreak
describe processes that are spread out across time
and space. As a consequence, events in the latter,
?nature?-related scenarios are more complex, may
have a hierarchical structure, and may overlap with
each other in text. Linguistic cues that have been
proposed in Huttunen et al (2002a) to identify the
overlapping or partial events include specific lexical
items, locative and temporal expressions, and usage
of ellipsis and anaphora.
Grishman (2012) has emphasized that for fully
unsupervised event extraction, extensive linguistic
analysis is essential; such analysis should be able
to capture ?modifiers on entities, including quan-
tity and measure phrases and locatives; modifiers on
predicates, including negation, aspect, quantity, and
temporal information; and higher-order predicates,
including sequence and causal relations and verbs of
belief and reporting.? It is clear that such sophisti-
cated linguistic analysis increases the importance of
text style and genre for Information Extraction.
29
The idea of statistical comparison between text
types goes back at least as far as (Biber, 1991). It
was subsequently used in a number of papers on au-
tomatic text categorization (Kessler et al, 1997; Sta-
matatos et al, 2000; Petrenz and Webber, 2011).
Szarvas et al (2012) studied the linguistic cues
indicating uncertainty of events in three genres:
news, scientific papers and Wikipedia articles. They
demonstrate significant differences in lexical usage
across the genres; for example, such words as fear
or worry may appear relatively often in news and
Wikipedia, but almost never in scientific text. They
also investigate differences in syntactic cues: for
example, the relation between a proposition and a
real-word fact is more likely to be expressed in the
passive voice in scientific papers (it is expected),
whereas in news the same words are more likely ap-
pear in the active.
Because events are not only representations of
facts but also linguistic units, an investigation of
events should take into account the particular lan-
guage, genre, scenario and medium of the text?i.e.,
events should be studied in the context of a particu-
lar corpus. Hence, the next question is how corpus-
driven study of events should be organized in prac-
tice, or, more concretely, what particular statistics
are needed to capture the scenario-specific charac-
teristics of event representation in a particular cor-
pus, and what kind of markup is necessary to solve
this task. We believe that answers to these questions
will likely depend on the ultimate goals of event de-
tection. We investigate IE in the business domain?
thus, we believe that preliminary study of the corpus
should use exactly the same depth of linguistic anal-
ysis as would be later utilized by the IE system.
2 Problem Statement
2.1 Events in the Business domain
We investigate event structure in the context of
PULS,1 an IE System, that discovers, aggregates,
verifies and visualizes events in various scenarios.
This paper focuses on the Business domain, in which
scenarios include investments, contracts, layoffs and
other business-related events, which are collected in
a database to be used for decision support. In the
Business domain, PULS currently handles two types
1More information is available at: http://puls.cs.helsinki.fi
Figure 1: Distributions of document length in the news
and business analysts? reports corpora
of documents: news reports and short summaries
written by professional business analysts. Thus,
events extracted from both corpora relate to approx-
imately the same real-world facts.
Both corpora are in English (though some of the
analysts? reports are based on news articles written
in other languages). We collected a corpus of re-
ports containing 740 thousand documents over three
years 2010-2012, and a news corpus containing 240
thousand documents over the same period.
The two corpora demonstrate significant linguis-
tic differences. First, the documents have different
length: the average length of an analyst reports is 5.5
sentences including the title, and 80% of the docu-
ments have length between 4 and 7 sentences, (see
Figure 1). News articles are on average 19 sentences
long?and much more varied in length.
The topical structure is also quite different for the
two corpora. Each analyst report is most typically
dedicated to a particular single real-world event.
Also, the reports tend to have a standardized, formu-
laic structure. The analysts who generate these re-
ports tend to follow a specific, strict style and struc-
ture over time.
By contrast, documents in the news corpus are
much more heterogeneous. These texts can follow
a wide variety of different styles?short messages,
surveys, interviews, etc. News documents can focus
not only strictly on business events but on related
topics as well. For example, political events have
complex interaction with and impact on business ac-
30
tivity, and therefore political news frequently appear
in business news feeds.
PULS aims to use the same processing chain for
various types of input documents. One key goal of
the current work is to investigate whether different
IE processing approaches are needed for documents
belonging to different text types, as exemplified by
analyst reports vs. articles from news feeds.
To summarize, the goals of the present work are:
? investigate how text genre influences event rep-
resentation;
? find formal markers able to capture and mea-
sure the differences in corpus style/genre;
? propose a methodology for adaptating an IE
system to a different text genre.
2.2 System Description
In this section we describe how the IE system is used
in a ?pattern-mining mode,? to address the afore-
mentioned problems.
PULS is a pipeline of components, including:
a shallow parser/chunker; domain ontologies and
lexicons; low-level patterns for capturing domain-
specific entities and other semantic units, such as
dates and currency expressions; higher-level pat-
terns for capturing domain-specific relations and
events; inference rules, which combine fragments of
an event that may be scattered in text?that a pattern
may not have picked up in the immediate context
(e.g., the date of the event); reference resolution for
merging co-referring entities and events.
The ontology and the lexicon for the Business do-
main encode the taxonomic relations and support
merging of synonyms: e.g., the ontology stores the
information that cellphone and mobile phone are
synonymous, and that a super-concept for both is
PRODUCT.
Low-level patterns are used to extract entities
from text, such as company names, dates, and lo-
cations. On a slightly higher level, there are pat-
terns that match contexts such as range (collection,
line, etc.) of X and assign them the type of X. For
instance, the phrase a collection of watches would
be assigned semantic type watch, etc. The top-level
patterns in all IE scenarios are responsible for find-
ing the target events in text.
In the pattern-mining mode we use the gen-
eral pattern SUBJECT?VERB?OBJECT, where the
components may have any semantic type and are
constrained only by their deep syntactic function?
the system attempts to normalize many syntactic
variants of the basic, active form: including passive
clauses, relative clauses, etc.2
The idea of using very simple, local patterns
for obtaining information from large corpora in
the context of event extraction is similar to work
reported previously, e.g., the bootstrapping ap-
proaches in (Thelen and Riloff, 2002; Yangarber et
al., 2000; Riloff and Shepherd, 1997). Here, we
do not use iterative learning, and focus instead on
collecting and analyzing interesting statistics from
a large number of S-V-O patterns. We collected
all such ?generalized? S-V-O triplets from the cor-
pus and stored them in a database. In addition to
the noun groups, we save the head nouns and their
semantic classes. This makes it easy to use sim-
ple SQL queries to count instances of a particular
pattern, e.g., all objects of a particular verb, or all
actions that can be applied to an object of seman-
tic class ?PRODUCT.? For each triplet the database
stores a pointer the original sentence, making it pos-
sible to analyze specific examples in their context.
In the next two sections we present the statis-
tics that we collected using the pattern-mining
mode. This information reflects significant differ-
ences among the corpora genres and can be used to
measure variety of genre. We believe that in the fu-
ture such data analysis will support the adaptation of
PULS to new text genres.
3 Statistical Properties of the Corpora
3.1 Personal pronouns
Pronouns play a key role in anaphoric relations; the
more pronouns are present in the corpus, the more
crucial anaphora resolution becomes. Analysis of
relationships between frequencies of personal pro-
nouns in text and the genre of the text is not new;
it has been observed and studied extensively, going
2By normalization of syntactic variants we mean, for in-
stance, that clauses like ?Nokia releases a new cellphone? (ac-
tive), ?a new cellphone is released by Nokia? (passive), ?a new
cellphone, released by Nokia,...? (relative), etc., are all reduced
to the same S-V-O form.
31
Reports News
Pronoun Object Subject Object Subject
I/me 0.003 0.007 0.2 1.0
we/us 0.001 0.001 0.4 1.7
you 0.002 0.003 0.3 0.8
he/him 0.05 0.4 0.6 2.2
she/her 0.007 0.05 0.1 0.5
they/them 0.3 0.6 0.8 1.3
it 1.1 2.6 1.5 2.3
Total 1.5 3.6 4.0 9.8
Table 1: Personal pronouns appearing in the subject or
object position in the corpora. The numerical values are
proportions of the total number of verbs.
back as far as, e.g., (Karlgren and Cutting, 1994).
The analysis of pronoun distribution in our corpora
is presented in Table 1, which shows the proportions
of personal pronouns, as they appear in subject or
object position with verbs in the collected triples.
The numbers are relative to the count of all verb to-
kens in the corpus, i.e., the total number of the S?V?
O triplets extracted from the corpus. The total num-
ber of triplets is approximately 5.7M in the report
corpus and 11M in the news corpus.
It can be seen from Table 1 that personal pro-
nouns are much more rare in the report corpus than
in the news corpus. Only 1.5% of verbs in the re-
ports corpus have a pronoun as an object, and 3.6%
as a subject. By contrast, in the news corpus 4%
of verbs have a personal pronoun as an object, and
9.8% as a subject. This corresponds to the observa-
tion in (Szarvas et al, 2012), that ?impersonal con-
structions are hardly used in news media.?
It is interesting to note the distribution of the par-
ticular pronouns in the two corpora. Table 1 shows
that it is the most frequent pronoun, they and he are
less frequent; the remaining pronouns are much less
frequent in the report corpus, whereas in the news
the remaining personal pronouns have a much more
even distribution. This clearly reflects a more re-
laxed style of the news that may use rhetorical de-
vices more freely, including citing direct speech and
use a direct addressing the reader (you). It is also
interesting to note that in the third-person singular,
the feminine pronoun is starkly more rare in both
corpora than the masculine, but roughly twice more
rare among the analyst reports.
Reports News
Subject Object Subject Object
All 21.8 6.6 14.6 6.5
Business 27.1 8.1 20.1 9.5
Table 2: Distribution of proper names as subjects and ob-
jects, as a proportion the total number of all verbs (top
row) vs. business-related verbs (bottom row).
3.2 Proper Names
Proper names play a crucial role in co-reference res-
olution, by designating anaphoric relations in text,
similarly to pronouns. In the Business domain, e.g.,
a common noun phrase (NP) may co-refer with a
proper name, as ?the company? may refer to the
name of a particular firm. A correctly extracted
event can be much less useful for the end-user if it
does not contain the specific name of the company
involved in the event.
A verbs is often the key element of a pattern that
indicates to the IE system the presence of an event
of interest in the text. When the subject or ob-
ject of the verb is a common NP, the corresponding
proper name must be found in the surrounding con-
text, using reference resolution or domain-specific
inference rules. Since reference resolution is itself
a phase that contributes some amount of error to
the overall IE process, it is natural to expect that if
proper-name subjects and objects are more frequent
in the corpus, then the analysis can be more precise,
since all necessary information can be extracted by
pattern without the need for additional extra infer-
ence. Huttunen et al (2012) suggests that the com-
pactness of the event representation may be used as
one of the discourse cues that determine the event
relevance.
Table 2 shows the percentage of proper name ob-
jects and subjects for the two corpora. Proper-name
objects have comparable frequency in both corpora,
though proper-name subjects appear much more fre-
quently in analyst reports than in news. Further-
more, for the business verbs, introduced below in
section 4.1?i.e., the specific set of verbs that are
used in event patterns in the Business scenarios?as
seen in the second row of the table?proper-name
objects and subjects are more frequent still. This
suggests that business events tend to mention proper
names.
32
Percentage of business verbs
Corpus Total Title 1st sentence
Reports 49.5 7.6 13.8
News 31.8 0.6 1.1
Table 3: Business verbs in analyst reports and news cor-
pora, as a proportion of the total number of verbs.
4 Business Verbs
4.1 Distribution of Business verbs
The set of business-related verbs is an important part
of the system?s domain-specific lexicon for the Busi-
ness domain. These verbs are quite diverse: some
are strongly associated with the Business domain,
e.g., invest; some are more general, e.g., pay, make;
many are ambiguous, e.g., launch, fire. Inside ana-
lyst reports these verbs always function as markers
of certain business events or relations. The verbs
are the key elements of the top-level patterns and it
is especially crucial to investigate their usage in the
corpora to understand how the pattern base should
be fine-tune for the task.
Since the majority of these verbs fall in the am-
biguous category, none of these verbs can by them-
selves serve a sufficient indicator of the document?s
topic. Even the more clear-cut business verbs, such
as invest, can be used metaphorically in the non-
business context. However, their distribution in the
particular document and in the corpus as a whole can
reflect the genre specificity of the corpus.
Table 3 shows the overall proportion of the busi-
ness verbs, and their proportion in titles and in the
first sentence of a documents. It suggests that almost
50% of the verbs in the report corpus are ?business?
verbs, and almost half of them are concentrated in
the beginning of a document. By contrast, the frac-
tion of business verbs in the news corpus is less than
one third and they are more scattered through the
text. This fact is illustrated by the plot in Figure 2.
The first sentence is often the most informa-
tive part of text, since it introduces the topic of
the document to the reader and the writer must do
his/her best to attract the reader?s attention. It was
shown in (Huttunen et al, 2012) that 65% of highly-
relevant events in the domain of medical epidemics
appear in the title or in the first two sentences of a
news article; Lin and Hovy (1997) demonstrated that
Figure 2: Percentage of business verbs in the text; sen-
tence 0 refers to the title of the document. The fraction of
verbs is presented as a percent of all verb instances in the
corpus. Logarithmic scale is used for the x axis.
about 50% of topical keywords are concentrated in
the titles. We have noticed that some documents in
the news corpus have relevance to the business sce-
nario, although relevant events still can be extracted
from the second or third paragraphs of the text, men-
tioned incidentally. By contrast, each analyst report
is devoted to a specific business event, and these
events are frequently mentioned as early as in the
title.
4.2 Case study: is ?launch? a business verb?
A set of verbs such as launch, introduce, release,
present,3 etc., are used in the Business scenarios to
extract events about bringing new products to mar-
ket. In the domain ontology they are grouped under
a concept called LAUNCH-PRODUCT. An example
of a pattern that uses this concept is following:
np(COMPANY) vg(LAUNCH-PRODUCT)
np(ANYTHING)
This pattern matches when a NP designating a com-
pany is followed by a verb from the ontology, fol-
lowed by any other NP. This pattern matches, for
example, such sentence as: The Real Juice Company
has launched Pomegranate Blueberry flavour to its line
of 100% juices. However, this pattern also over-
generates by matching sentences such as, e.g.: Cen-
3Note, the S-V-O triplet extraction also handles phrasal
verbs, such as roll out, correctly, i.e., identifies them as a single
linguistic unit, and treats them the same as single-word verbs.
33
tral bank unveils effort to manage household debt. Even
among analyst reports, approximately 14% of the
NEW-PRODUCT events found by the system are
false positives. It is not feasible to collect a list of
all possible products to restrict the semantic type
of the object of the verb, since new, unpredictable
types of products can appear on the market every
day. It seemed more feasible to try to discover all
non-products that can appear in the object slot, due
to the ambiguity of the verbs in patterns?a kind of a
black-list. We introduce an ontology concept NON-
PRODUCT that groups nouns that can be matched
by the LAUNCH verbs but are in fact not products,
e.g., budget, effort, plan, report, study. The ontology
supports multiple inheritance, so any of these words
can be attached to other parents as well, if necessary.
If the <PRODUCT> slot in of event is filled by
one of the black-listed concepts, the event is also
black-listed, and not visible to the end-user. They
are used as discourse features by learning algorithms
that predict the relevance of other events from the
same documents (Huttunen et al, 2012).
The NON-PRODUCT class is populated in an ad-
hoc manner over time. The content of such a list
depends on the particular corpus; the more diverse
the topical and stylistic structure of the corpus, the
more time-consuming and the less tractable such de-
velopment becomes. Thus, an important task is to
adjust the patterns and the class of NON-PRODUCT
nouns to work for the news corpus, and to develop
a feasible methodology to address the false-positive
problem. We next show how we can use the pattern-
mining mode to address these problems.
We extract all instances of the LAUNCH-
PRODUCT verbs appearing in the corpora from the
S?V?O database. In total 27.5% of all verb instances
in reports corpus are verbs from this semantic class,
in comparison to 0.7% in the news corpus. The num-
ber of distinct objects are approximately the same in
both corpora: 3520 for reports and 3062 for news,
see Table 4. In total 247 different objects from the
report corpus attached to the semantic class PROD-
UCT in PULS ontology, and 158 objects have this
semantic class in the news corpus.
For 21% of launch verbs in the report corpus, and
34% in the news corpus, the system is not able to ex-
tract the objects, which may be a consequence of the
more diverse and varied language of news. Recall,
LAUNCH- distinct PRODUCT
Corpus PRODUCT objects objects
Reports 204193 3520 247
News 77463 3062 158
Table 4: Distributions of LAUNCH-PRODUCT verbs in
the corpora
that the system extracts a deep-structure verbal argu-
ments, i.e., for a sentence like ?A new cell-phone has
been launched by company XYZ? it identifies cell-
phone as the (deep) object, and the agent company
XYZ as the (deep) subject.
It is interesting to examine the particular sets of
words that can appear in the object position. We col-
lected the 50 most frequent objects of the LAUNCH-
PRODUCT verbs for each corpus; they are shown in
Table 5 ranked by frequency (we show the top 30
objects to save space). The table shows the semantic
class according to our ontology.
Of the 50 most frequent objects, 24 belong to
the semantic class PRODUCT in the report corpus,
while only 8 objects do in the general news cor-
pus. By contrast, 20 objects belong to the NON-
PRODUCT class in the news corpus and only 9 ob-
jects in reports. Moreover, 8 objects in the news cor-
pus are not found in the ontology at all, in compari-
son to only one such case from the report corpus.
Some object classes may mean that the event is
still relevant for the business domain, though it does
not belong to the NEW-PRODUCT scenario. For
example, when object is an advertising campaign the
event is likely to belong to the MARKETING sce-
nario, when the object is a facility (factory, outlet,
etc.) it is likely INVESTMENT. Inference rules may
detect such dependencies and adjust the scenario of
these events in the Business domain.
The inference rules are supported by the same do-
main ontology, but can test domain- and scenario-
specific conditions explicitly, and thus can be more
accurate than the generic reference resolution mech-
anism. However, this also means that inference rules
are more sensitive to the corpus genre and may not
easily transfer from one corpus to another.
In some cases an object type cannot be interpreted
as belonging to any reasonable event type, e.g., if
it is an ORGANIZATION or PERSON. Such cases
may arise due to unusual syntax in the sentence that
34
Rank Reports News
Object Freq Class Object Freq Class
1 Proper Name unspecified 19987 Proper Name unspecified 5971
2 product 7331 PROD report 1078 NON
3 service 6510 PROD result 851 NON
4 campaign 3537 CAMP plan 805 NON
5 project 2870 PROD product 792 PROD
6 range 2536 COLL service 648 PROD
7 plan 2524 NON it 618 PRON
8 organization 2450 ORG data 552
9 system 2166 FAC campaign 510 CAMP
10 line 1938 COLL organization 495 ORG
11 model 1920 PROD statement 467 NON
12 application 1345 PROD Proper Name person 449 PER
13 website 1321 PROD program 439
14 flight 1315 PROD Proper Name company 432 ORG
15 Proper Name company 1232 ORG information 411 NON
16 brand 1200 COLL detail 398 NON
17 offer 1187 NON investigation 380 NON
18 production 1112 NON website 373 PROD
19 programme 998 NON measure 368 NON
20 store 993 PROD they 363 PRON
21 currency 958 CUR he 358 PRON
22 route 954 PROD device 352 PROD
23 drink 891 PROD system 340 FAC
24 solution 883 NON smartphone 337 PROD
25 smartphone 852 PROD attack 335
26 fragrance 824 PROD figure 318 NON
27 card 802 PROD opportunity 295 INV
28 fund 801 PROD fund 290 NON
29 scheme 773 NON currency 287 CUR
30 facility 756 FAC model 286 COLL
Table 5: The most frequent objects of LAUNCH verbs. Class labels: PROD: product, NON: non-product (black-
listed), CAMP: advertising campaign, INV: investment. Domain independent labels: COLL: collective; PRON: pro-
noun, FAC: facility, ORG: organization, PER: person, CUR: currency,
confuses the shallow parser.
In summary, the results obtained from the S-V-O
pattern-mining can be used to improve the perfor-
mance of IE. First, the most frequent subjects and
objects for the business verbs can be added to the
ontology; second, inference rules and patterns are
adjusted to handle the new concepts and words.
It is very interesting to investigate?and we plan
to pursue this in the future?how this can be done
fully automatically; the problem is challenging since
the semantic classes for these news concepts de-
pend on the domain and task; for example, some
objects are of type PRODUCT (e.g., ?video?), and
others are of type NON-PRODUCT (e.g., ?attack?,
?report?, etc.). Certain words can be ambiguous
even within a limited domain: e.g., player may des-
ignate a COMPANY (?a major player on the mar-
ket?), a PRODUCT (DVD-player, CD-player, etc.),
or a person (tennis player, football player, etc.); the
last meaning is relevant for the Business domain
since sports personalities participate in promotion
campaigns, and can launch their own brands. Au-
tomating the construction of the knowledge bases is
a challenging task.
In practice, we found that the semi-automated ap-
proach and the pattern-mining tool can be helpful for
analyzing genre-specific event patterns; it provides
the advantages of a corpus-based study.
35
5 Conclusion
We have described an approach for collecting use-
ful statistics about event representation and distribu-
tion of event arguments in corpora. The approach
was easily implemented using pattern-based extrac-
tion of S-V-O triplets with PULS; it can be equally
efficiently implemented on top of a syntactic parser,
or a shallow parser of reasonable quality. An ontol-
ogy and lexicons are necessary to perform domain-
specific analysis. We have discussed how the results
of such analysis can be exploited for fine-tuning of
a practical IE scenario.
The pattern-mining process collects deep-
structure S?V?O triplets from the corpus?which
are ?potential? events. The triplets are stored in
a database, to facilitate searching and grouping
by words or by semantic class appearing as the
arguments of the triplets. This helps us quickly
find all realizations of a particular pattern?for
example, all semantic classes that appear in the
corpus as objects of verbs that have semantic class
LAUNCH-PRODUCT. The subsequent analysis of
the frequency lists can help improve the perfor-
mance of the IE system by suggesting refinements
to the ontology and the lexicon, as well as patterns
and inference rules appropriate for the particular
genre of the corpus.
Our current work includes the adaptation of the
IE system developed for the analyst reports to the
general news corpus devoted to the same topics. We
also plan to develop a hybrid methodology, to com-
bine the presented corpus-driven analysis with open-
domain techniques for pattern acquisition, (Cham-
bers and Jurafsky, 2011; Huang and Riloff, 2012).
The approach outlined here for analyzing the dis-
tributions of features in documents is useful for
studying events within the context of a corpus. It
demonstrates that event structure depends on the text
genre, and that genre differences can be easily cap-
tured and measured. By analyzing document statis-
tics and the output of the pattern-mining, we can
demonstrate significant differences between the gen-
res of analyst reports and general news, such as: sen-
tence length, distribution of the domain vocabulary
in the text, selectional preference in domain-specific
verbs, word co-occurrences, usage of pronouns and
proper names.
The pattern mining collects other statistical fea-
tures, beyond those that have been discussed in de-
tail above. For example, it showed that active voice
is used in 95% of the cases in the news corpus in
comparison to 88% in the analyst report corpus. It
is also possible to count and compare the usage of
other grammatical cues, such as verb tense, modal-
ity, etc. Thus, we should investigate not only lexical
and semantic cues, but also broader syntactic prefer-
ences and selectional constrains in the corpora.
In further research we plan to study how the for-
mal representation of the genre differences can be
used in practice, that is, for obtaining directly mea-
surable improvements in the quality of event extrac-
tion. Taking into account the particular genre of the
corpora from which documents are drawn will also
have implications for the work on performance im-
provements via cross-document merging and infer-
ence, (Ji and Grishman, 2008; Yangarber, 2006).
The frequency-based analysis described in Sec-
tion 4.2 seems to be effective. Sharpening the results
of the analysis as well as putting it to use in practical
IE applications will be the subject of further study.
Acknowledgements
We wish to thank Matthew Pierce and Peter von Et-
ter for their help in implementation of the pattern
mining more described in this paper. The work was
supported in part by the ALGODAN: Algorithmic
Data Analysis Centre of Excellence of the Academy
of Finland.
References
Douglas Biber. 1991. Variation across speech and writ-
ing. Cambridge University Press.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of ACL-HLT, pages 976?986.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference-6: A brief history. In Pro-
ceedings of COLING, volume 96, pages 466?471.
Ralph Grishman. 2012. Structural linguistics and un-
supervised information extraction. Automatic Knowl-
edge Base Construction and Web-scale Knowledge Ex-
traction (AKBC-WEKEX 2012), pages 57?61.
Ruihong Huang and Ellen Riloff. 2012. Bootstrapped
training of event extraction classifiers. EACL 2012,
pages 286?295.
36
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002a. Complexity of event structure in IE scenarios.
In Proceedings of the 19th International Conference
on Computational Linguistics (COLING 2002), Taipei,
August.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002b. Diversity of scenarios in information extrac-
tion. In Proceedings of the Third International Confer-
ence on Language Resources and Evaluation (LREC
2002), Las Palmas de Gran Canaria, Spain, May.
Silja Huttunen, Arto Vihavainen, Mian Du, and Roman
Yangarber. 2012. Predicting relevance of event ex-
traction for the end user. In T. Poibeau et al, edi-
tor, Multi-source, Multilingual Information Extraction
and Summarization, pages 163?177. Springer-Verlag,
Berlin.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL-2008: HLT, pages 254?262, June.
Jussi Karlgren and Douglas Cutting. 1994. Recogniz-
ing text genres with simple metrics using discriminant
analysis. In Proceedings of the 15th Conference on
Computational Linguistics, pages 1071?1075, Kyoto,
Japan, August.
Brett Kessler, Geoffrey Numberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Proceed-
ings of the 35th Annual Meeting of the Association for
Computational Linguistics and Eighth Conference of
the European Chapter of the Association for Computa-
tional Linguistics, pages 32?38. Association for Com-
putational Linguistics.
Chin-Yew Lin and Eduard Hovy. 1997. Identifying top-
ics by position. In Proceedings of the fifth conference
on Applied natural language processing, pages 283?
290. Association for Computational Linguistics.
Philipp Petrenz and Bonnie Webber. 2011. Stable clas-
sification of text genres. Computational Linguistics,
37(2):385?393.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117?124.
Association for Computational Linguistics, Somerset,
New Jersey.
Roser Saur?? and James Pustejovsky. 2012. Are you sure
that this happened? Assessing the factuality degree of
events in text. Computational Linguistics, 38(2):261?
299.
Efstathios Stamatatos, Nikos Fakotakis, and George
Kokkinakis. 2000. Text genre detection using com-
mon word frequencies. In Proceedings of the 18th
conference on Computational linguistics - Volume 2,
COLING ?00, pages 808?814, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,
Gyo?rgy Mo?fra, and Iryna Gurevych. 2012. Cross-
genre and cross-domain detection of semantic uncer-
tainty. Computational Linguistics, 38(2):335?367.
Mark Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2002).
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisi-
tion of domain knowledge for information extrac-
tion. In Proceedings of the 18th International Confer-
ence on Computational Linguistics (COLING 2000),
Saarbru?cken, Germany, August.
Roman Yangarber. 2006. Verification of facts across
document boundaries. In Proceedings of the Inter-
national Workshop on Intelligent Information Access
(IIIA-2006), Helsinki, Finland, August.
37
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 100?109,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Adapting the PULS Event Extraction Framework
to Analyze Russian Text
Lidia Pivovarova,1,2 Mian Du,1 Roman Yangarber1
1 Department of Computer Science
University of Helsinki, Finland
2St. Petersburg State University, Russia
Abstract
This paper describes a plug-in component
to extend the PULS information extraction
framework to analyze Russian-language
text. PULS is a comprehensive framework
for information extraction (IE) that is used
for analysis of news in several scenarios
from English-language text and is primar-
ily monolingual. Although monolingual-
ity is recognized as a serious limitation,
building an IE system for a new language
from the bottom up is very labor-intensive.
Thus, the objective of the present work is
to explore whether the base framework can
be extended to cover additional languages
with limited effort, and to leverage the pre-
existing PULS modules as far as possible,
in order to accelerate the development pro-
cess. The component for Russian analysis
is described and its performance is evalu-
ated on two news-analysis scenarios: epi-
demic surveillance and cross-border secu-
rity. The approach described in the paper
can be generalized to a range of heavily-
inflected languages.
1 Introduction
1.1 Problem Statement
PULS1 is a framework for information extraction
from text (IE), designed for decision support in
various domains and scenarios. To date, work
on PULS has mostly concentrated on English-
language text, though some effort has gone into
adapting PULS to other languages, (Du et al,
2011). This paper describes a component that is
used to extend PULS to analyze Russian-language
text, and demonstrates its performance on two IE
scenarios: infectious epidemics and cross-border
1http://puls.cs.helsinki.fi
security. The epidemics scenario is built to pro-
vide an early warning system for professionals
and organizations responsible for tracking epi-
demic threats around the world. Because infor-
mation related to outbreaks of infectious disease
often appears in news earlier than it does in offi-
cial sources, text mining from the Web for medi-
cal surveillance is a popular research topic, as dis-
cussed in, e.g., (Collier et al, 2008; Huttunen
et al, 2002; Rortais et al, 2010; Zamite et al,
2010). Similarly, in the security scenario, the sys-
tem tracks cross-border crime, including illegal
migration, smuggling, human trafficking, as well
as general criminal activity and crisis events; text
mining for this scenario has been previously re-
ported by (Ameyugo et al, 2012; Atkinson et al,
2011). The new component monitors open-source
media in Russian, searching for incidents related
to the given scenarios. It extracts information
from plain, natural-language text into structured
database records, which are used by domain spe-
cialists for daily event monitoring. The structure
of the database records (called templates) depends
on the scenario. For the epidemics scenario the
system extracts the fields: disease name, location
of the incident, date, number of victims, etc. In the
security domain, the template contains the type of
event, date and location, the perpetrator, number
of victims (if any), goods smuggled, etc.
Monolinguality is a serious limitation for IE,
since end-users are under growing pressure to
cover news from multiple languages, (Piskorski
et al, 2011). The Russian-language component
that we describe here is an experiment in extend-
ing PULS to multi-lingual coverage. Our aim is to
explore whether a such an extension can be built
with limited effort and resources.
1.2 Prior work on IE from Russian
IE in Russian has been the topic of several recent
studies. For example, (Piskorski et al, 2011) uses
100
Russian among other languages to study infor-
mation fusion across languages. Extraction tech-
niques are used for ontology learning in (Bocharov
et al, 2010) and (Schumann, 2012). The Uni-
versity of Sheffield?s GATE system, (Bontcheva
et al, 2003), which supports multi-lingual IE, has
been adapted to Russian as part of the MUSE-3
project, (though little is published on functional-
ity available in Russian). HP Labs have recently
started adaptation of their information extraction
solutions to Russian, (Solovyev et al, 2012).
Much literature devoted to Russian-language
information extraction is published only in Rus-
sian; a brief review can be found in (Khoro-
shevsky, 2010). The majority of existing appli-
cations for Russian IE, and Natural Language Pro-
cessing (NLP) in general, are commercially based,
and are either published in Russian only, or not
at all. One major player in Russian text min-
ing is Yandex, the leading Russian search engine.
Yandex uses IE to support its main search ser-
vice, e.g., to underline addresses and persons in
search results, and in a service called ?Press Por-
traits,?2 which builds profiles for various personal-
ities found in the news. A profile may include the
profession, biographical facts, news that s/he is in-
volved in, and related people?using information
automatically extracted from on-line Russian me-
dia. Yandex also recently unveiled an open-source
toolkit Tomita, for developing IE systems based on
context-free grammars.
Dictum, a company that builds applications for
NLP and sentiment analysis in Russian, provides
a toolkit for Russian morphological, syntactic and
semantic analysis. Their Fact Extraction compo-
nent3 finds persons, organizations, locations, etc.,
and creates simple facts about persons: corporate
posts, date of birth, etc.
RCO, a company focused on research and de-
velopment of text analysis solutions, provides the
RCO Fact Extractor tool4, which performs fact ex-
traction from unstructured text. One common us-
age scenario is setting up a list of target objects
(persons, companies) and extracting all events
where these objects are mentioned as participants.
The tool also includes a module that allows the
user to adjust search patterns.
With the exception of Tomita and AOT (see Sec-
2http://news.yandex.ru/people
3http://dictum.ru/en/object-extraction/blog
4http://www.rco.ru/eng/product.asp
tion 3), few resources are available in open-source.
2 The Baseline English System
The PULS news-tracking pipeline consists of three
main components: a Web-crawler that tries to
identify potentially relevant articles using a broad
keyword-based Web search; a rule-based Informa-
tion Extraction system that uses patterns acquired
through semi-supervised learning, that determines
exactly what happened in the article, creating a
structured record that is stored in the database;
and a relevance classifier that determines the rele-
vance of the selected articles?and events that they
describe?to the particular use-case scenario and
the users? needs. This paper will mostly focus on
the IE component, as other two components are
language-independent.
The IE system contains modules for lower-
level?morphological and syntactic?analysis, as
well as higher-level?semantic?analysis, and
produces filled templates on output, extracted
from an input document, (Du et al, 2011).
PULS follows a classic IE processing pipeline:
? Pre-processing,
? Lexical markup,
? Shallow syntactic analysis/chunking,
? Semantic pattern matching
? Reference resolution and logical inference
Pre-processing includes tokenization, part-of-
speech tagging, processing of punctuation, nu-
meric expressions, etc.
Lexical markup is tagging of lexical units found
in text with semantic information found in a dictio-
nary and/or ontology. PULS uses several domain-
independent and domain-specific lexicons and on-
tologies. The ontology is a network of con-
cepts organized in a hierarchy by several rela-
tions, among which the ?is-a? relation is the most
common. One key factor that enables the addi-
tion of new languages efficiently is that the on-
tology is language-independent. The system uses
the lexicons to map words into concepts. A lex-
icon consists of word-forms and some common
multi-word expressions (MWEs), which appear in
text and represent some ontology concept. We
assume that within a given domain each word or
101
MWE in the lexicon represents exactly one con-
cept, (Yarowsky, 1995). A concept may be rep-
resented by more than one word or MWE.5 Each
scenario has its own scenario-specific ontology
and lexicons; the Epidemics ontology consists of
more than 4000 concepts (which includes some
disease names). Diseases are organized in a hi-
erarchy, e.g., ?hepatitis? is a parent term for ?hep-
atitis A?. The Security ontology consists of 1190
concepts.
The domain-specific lexicon is a collection of
terms that are significant for a particular scenario,
mapped to their semantic types/concepts. The Se-
curity and Epidemics scenarios use a common lo-
cation lexicon, that contains approximately 2500
names of countries, cities and provinces. Loca-
tions are organized according the ?part-of? rela-
tion: cities are part-of provinces, which are part-of
states, etc.
Syntactic analysis is implemented as a cascade
of lower-level patterns. PULS uses shallow anal-
ysis (chunking), which does not try to build com-
plete syntactic tree for a sentence but recognizes
local grammatical structures?in particular, the
noun and verb groups. This phase also identi-
fies other common constructions needed for IE,
(names, dates, etc.). As a result of the syntactic
analysis, each sentence is represented as a set of
fragments.
The pattern base is the main component of the
IE system, responsible for finding factual informa-
tion in text. A pattern is a set of semantic, syntactic
and morphological constraints designed to match
pieces of natural-language text. When a pattern
fires it triggers an action, which creates an abstract
logical entity based on the text matched by the pat-
tern. The entity is added to an internal pool of
entities found in the document so far. Facts pro-
duced by the system are based on the entities in
this pool. The patterns are arranged in a cascade
such that the results produced by one pattern are
used by subsequent patterns to form more com-
plex entities.
Patterns operate on a sentence-by-sentence ba-
sis. To link information in the surrounding sen-
tences PULS uses concept-based reference reso-
lution and logical inference rules. The reference
resolution component is a set of rules for merging
5By default, words that appear only in the general-purpose
dictionary, and do not appear in any domain-specific lexicon,
are automatically identified with a concept having an identi-
cal name.
mentions of the same object and events.
Inference rules work on a logical level (rather
than text), operating on entities found at the pre-
ceding stages of analysis. These entities can be
used to fill slots in an event description, for exam-
ple, to find event time and location, or to perform
logical inference. For example, if the event type is
human-trafficking and a concept related to organ-
transplantation is mentioned in the sentence, an
inference rule may specialize the event type to
human-trafficking-organs.
3 Russian Morphology and Syntax
To speed development, we use pre-existing tools
for tokenization, morphological and syntactic
analysis in Russian. The range of freely-available,
open-source tools for Russian is quite narrow, es-
pecially for syntactic analysis. Significant efforts
for overcoming this situation have been the focus
of the recent ?Dialogue? series of conferences6,
which organized workshops on Russian morphol-
ogy, (Astaf?eva et al, 2010), and syntax, (Toldova
et al, 2012). Workshops take the form of compe-
titions, where the participants tackle shared tasks.
Eight teams participated in the latest workshop,
devoted to syntax. However, only one?AOT7?
offers their toolkit under the GNU LGPL license.
The AOT toolkit, (Sokirko, 2001) is a collec-
tion of modules for NLP, including libraries for
morphological, syntactic, and semantic analysis,
language generation, tools for working with dic-
tionaries, and GUIs for visualization of the anal-
ysis. Due to its open availability and high qual-
ity of linguistic analysis, AOT is currently a de-
facto standard for open-source Russian-language
processing.
The AOT morphological analyzer, called
?Lemm?, analyzes text word by word; its output
for each word contains: an index, the surface form,
the base lemma, part of speech, and morphologi-
cal tags. Lemm works on the morphological level
only, and leaves all morphological ambiguity in-
tact, to be resolved by later phases.
Lemm uses a general-purpose Russian mor-
phological dictionary, which can be edited and
extended (e.g., with neologisms, domain-specific
terms, etc.). To add a new lemma into the
dictionary, one needs to specify its inflectional
6Dialogue?International Conference of Computational
Linguistics (http://www.dialog-21.ru/en/)
7The AOT project (?Automatic Processing of Text? in
Russian)?www.aot.ru
102
paradigm. For Russian IE, we had to add to
the dictionary certain words and terms that des-
ignate scenario-specific concepts, for example
????????? (migrant) and ?????????????? (gas-
tarbaiter), which have become common usage in
recent Russian-language media.
The syntactic analyzer in AOT, ?Synan?, uses
a hybrid formalism, a mix of dependency trees
and constituent grammars. The output for a sen-
tence contains two types of syntactic units: binary
parent-child relations, and ?groups?, which are to-
ken sequences not analyzed further but treated as
an atomic expression. This approach is theoret-
ically natural, since certain syntactic units may
not have a clear root, for example, complex name
expressions (?Aleksey Sokirko?) or numeric ex-
pressions (?forty five?). To make it compatible
with the overall PULS structure, we transform all
Synan output into dependency-tree form; groups
simply become linked chains. Synan attempts to
produce a complete, connected parse structure for
the entire sentence; in practice, it produces a set of
fragments, consisting of relations and groups. In
the process, it resolves morphological ambiguity,
when possible.
To unify the results of Lemm and Synan, we
built a special ?wrapper,? (Du et al, 2011). The
wrapper takes every binary (syntactic) relation in
the Synan output, finds the items corresponding
to the relation?s parent and child in Lemm?s out-
put, and resolves their morphological ambiguity
(if any) by removing all other morphological read-
ings. If the lemma for parent or child is null?as,
e.g., when the corresponding element is a group?
we infer information from Lemm output for the
element that is missed in Synan. If a word does
not participate in any relation identified by Synan,
its analysis is based only on Lemm output, pre-
serving all unresolved morphological ambiguity?
to be potentially resolved at a later stage, typically
by scenario-specific patterns. Finally, the wrapper
assembles the resulting analysis for all words into
a set of tree fragments.
4 Russian Information Extraction
4.1 Ontology and Dictionaries
The ontology, a network of semantic classes, is
language-independent, and in Russian IE, we used
the pre-existing domain ontologies for the epi-
demics and security domains, with minor mod-
ifications. Most of the changes centered on re-
moving vestiges of English language-specific in-
formation, e.g., by making explicit the distinctions
among certain concepts that may be confounded
in English due to ambiguity of English lexical
units. For example, in English, the word ?convict?
means both the verb and the convicted person (pa-
tient nominalization), so it may be tempting to rep-
resent them by the same concept. In Russian, as in
many other languages, these are different concepts
as well as distinct lexemes.
A Russian domain-specific lexicon was added
to the system. Russian IE uses a shared lex-
icon for epidemics and security. The lexicon
contains not only translations of the correspond-
ing English words, but also includes MWEs that
appear in Russian media and correspond to the
domain-specific concepts. The current Russian
domain-specific lexicon contains approximately
1000 words and MWEs. Constructing the multi-
word lexicon for Russian is more complicated than
for English because Russian has a rich morphol-
ogy and complex grammatical agreement. For
example, to find a simple Adjective+Noun col-
location in text, the system needs to check that
the adjective agrees with the noun in gender,
case, and number. To resolve this problem, we
built a special set of low-level patterns, which
match MWEs. These patterns are subdivided
into several classes, according to their syntactic
form: Adjective+Noun, Noun+Noun, Verb+Noun,
Verb+Preposition+Noun, etc. The grammatical
constraints are coded only once for each class
of pattern, and apply to all patterns in the class.
For example, in the Noun+Noun class, the second
noun must be in genitive case (a genitive modifier
of the head noun), e.g., ?????? ??????? (cirrho-
sis of the liver), or in the instrumental case, e.g.,
????????? ??????? (human trafficking). This
simplifies adding new MWEs into the dictionary.
We use the multilingual GeoNames
database, (www.geonames.org) as the source
of geographic information in Russian. The
disease dictionary is mapped into Russian using
the International Classification of Diseases.8 The
system also identifies common animal diseases:
anthrax, African swine fever, rabies, etc.
4.2 Pattern Bases
The pattern base is the main component of the IE
system for extracting higher-level logical objects.
8ICD10: http://www.who.int/classifications/icd/en/
103
Syntactic variant Example Syntactic variant Example
I Verb + Object ?????????? ???????? II Object + Verb ???????? ??????????
(active clause) [someone] arrested a migrant (reverse word order) (same meaning)
III Participle + Object ????????? ??????? IV Object + Participle ??????? ?????????
(passive clause) migrant is arrested [by someone] (reverse word order) (same meaning)
V Noun + Object ????? ???????? VI (reverse word order is ?
(nominalization) arrest of a migrant rare, unlikely in news)
Table 1: Examples of syntactic variants for a single pattern Russian
Patterns are language-dependent and domain-
dependent, which means that patterns must cap-
ture the lexical, syntactic and stylistic features of
the analyzed text. It was not possible to directly
translate or map the English pattern base into Rus-
sian for at least two reasons.
The first reason is technical. PULS?s English
pattern base has over 150 patterns for the epi-
demics domain, and over 300 patterns for secu-
rity.9 These patterns were added to the system
through an elaborate pattern-acquisition process,
where semi-supervised pattern acquisition for En-
glish text was used, (Greenwood and Stevenson,
2006; Yangarber et al, 2000), to bootstrap many
pattern candidates from raw text based on a small
set of seed patterns; the candidates were sub-
sequently checked manually and included in the
system. Many of these patterns are typically in
?base-form?, i.e., simple active clauses; the En-
glish system takes each active-clause, ?subject-
verb-object? pattern, and generalizes it to multi-
ple syntactic variants, including passive clauses,
relative clauses, etc. Thus we created the Rus-
sian domain-specific patterns directly in PULS?s
pattern-specification language. A pattern consists
of a regular expression trigger and action code.
The second reason is theoretical. Unlike En-
glish, Russian is a heavily inflected, free word-
order language. In English, the active ?subject-
word-object? clause has only one form, whereas
in Russian all six permutations of the three el-
ements are possible, depending on the informa-
tion structure and pragmatic focus. This means
that we would need 6 pattern variants to match
a single active clause, and many more to process
other clausal types. The free word-order makes
it difficult to generate syntactic clausal variants;
it also complicates the bootstrapping of patterns
from seeds.
Therefore, for Russian we used a different strat-
9The difference is partly due to the fact that the security
scenario has several event types?illegal migration, human-
trafficking, smuggling, general crisis?and sub-types, while
epidemics deals with one event type.
egy, close to that used by (Tanev et al, 2009) for
Romance languages. In this approach, the patterns
first create ?shallow?, incomplete events where
only 1?2 slots are filled. Then, the inference rule
mechanism attempts to fill the remaining slots and
complete the events. The majority of Russian pat-
terns currently consist of two elements (such as
verb and object, or verb and subject), so that only
two word-order variants are possible. Currently,
the Russian patterns match five syntactic construc-
tions. These are listed in Table 1, along with ex-
amples from the security scenario. All example
phrases have the same meaning (?migrant was ar-
rested?) but different syntactic form. The active
clause and the passive clause in Russian may have
either V?O word order?types I and III?or O?
V,?types II and IV. The difference between the
active and the passive variants is in the grammati-
cal features only, which are marked by flexions.
Types I, III, and V in the table can be captured
by one simple pattern:
class(ARREST) noungroup(MIGRANT)
This pattern matches when a content phrase?
belonging to any part of speech (noun, verb,
or participle)?whose semantic head is the con-
cept ?ARREST? governs (i.e., in this case, pre-
cedes) a noun group headed by the concept ?MI-
GRANT?. The pattern primitives?class, noun-
group and others?build on top of the POS, syn-
tactic, and morphological tags that are returned by
the AOT wrapper. Types II and IV show variants
of the pattern in reverse order. Note that the pat-
terns use general ontology classes?shared with
English?rather than literal words.10
When a pattern fires, the system checks the con-
straints on grammatical features (e.g., case and
number agreement) on the matched phrases or
words. We introduce three types of constraints:
accusative object-case agreement for type I and
10NB: in practice, the patterns are more complex because
they allow various sentence modifiers to appear between verb
and object, which is a standard extension to this basic form
of the pattern.
104
Concept Event type
organ-transplant Human-Trafficking-Organs
border-guard Migration-Illegal-Entry
customs-officer Smuggling
Table 2: Examples of concepts found in context
that trigger rules to specialize the event type
II, for nominative subject-case agreement for type
III and IV, and and genitive-case nominalization
agreement for type V. If the constraints are satis-
fied, the event is created?that is, the same event
structure for any of the five pattern types.
For the security scenario the system currently
has 23 such ?basic? patterns. Most of them ini-
tially produce an event of a general class CRI-
SIS and fire when the text mentions that some-
one was arrested, sentenced, jailed, etc. If addi-
tional security-related concepts are found in text
nearby, inference rules will fill additional slots in
the event template, and specialize the type of the
event. The Russian security scenario uses exactly
the same set of inference rules as does the English
Security Scenario. Example rules are shown in Ta-
ble 2. For example, when an inference rule finds
in the context of an event a semantic concept that
is a sub-type of the type given in the left column,
the Type of the event is specialized to the corre-
sponding value in the right column, Table 2.
For the epidemics scenario, the system currently
uses only 7 patterns. Two produce an under-
specified event, when the text mentions that some-
one has become sick. The actual disease name is
again found by inference rules from nearby con-
text; if no disease is mentioned, the event is dis-
carded. Two additional patterns work ?in reverse?:
they match in cases when the text mentions an out-
break or case of a disease. Then the inference
rules try to find who is suffering from disease and
the number of victims. The inference rules are
again fully shared between English and Russian.
Some of the patterns are ?negative??they match
such statements as ?there is no threat of epidemic?,
which appear often in official reports.
In addition, the Russian pattern base contains
41 lower-level patterns, common for the security
and epidemics domains. These include, for exam-
ple, patterns to match date expressions, to analyze
collective-partitive noun groups (?a group of mi-
grants?, ?a team of doctors?, and so on), which
have general applicability.
Slot English system Russian system
rec pre F rec pre F
Event Type 67 72 69.41 70 57 62.83
Suspect 46 52 48.81 52 44 47.67
Total 27 71 46.47 44 37 40.20
Countries 56 55 55.49 48 40 43.63
Time 29 29 29.00 29 22 25.02
All 53 58 53.31 55 45 49.09
Table 3: Border Security scenario evaluation
English Russian
Event type test suite test suite
CRISIS 19 28
HUMAN-TRAFFICKING 4 4
ILLEGAL-MIGRATION 34 34
SMUGGLE 10 2
Total 67 68
Table 4: Distribution of event types in the test
suites for the Security scenario
5 Evaluation
5.1 Security
For evaluation, we used a test corpus of 64
Russian-language documents. Several assessors
annotated 65 events, and approximately one third
of the documents contained events. We compared
the Russian-language IE system with the English-
language system. The English test suite consists
of 50 documents with 70 events.
Evaluation results for the security domain are
presented in table 3, with scores given for the
main slots: Event Type (one of Migration, Human
Trafficking, Smuggling, and Crisis), Suspect, Total
(number of suspects), Countries (a list of one or
more countries involved in event), and Time (event
date). The table shows that currently the Rus-
sian system achieves a lower overall score than the
English system?the F-measure for all slots is 4?
5% lower, with precision being consistently lower
than recall for the Russian system.
Note that the development of a correct and
well-balanced test suite is in itself a challenging
task, and hence the evaluation numbers may be
biased. In the test suites used for these experi-
ments, shown in table 4, the English security sce-
nario includes more events of type SMUGGLE
than the Russian validation suite, and both vali-
dation suites contain few events of type HUMAN-
TRAFFICKING.
5.2 Epidemic Surveillance
For evaluation, we used a test corpus of 75 Rus-
sian documents. We asked several assessors to
105
Slot name English system Russian system
r p F r p F
Disease 74 74 74.00 93 81 86.58
Country 65 67 65.98 91 86 88.42
Total 68 79 73.09 30 78 43.33
Time 56 58 56.98 38 52 43.91
Status 77 75 75.99 93 81 86.58
All Slots 68 69 68.83 70 71 70.44
Table 5: Epidemics scenario evaluation.
correct events found by the system and add miss-
ing events in case they were not found by sys-
tem. Assessors annotated 120 events. We compare
the Russian-language IE system with the English-
language system. The PULS English validation
suite for Epidemics currently consists of 60 docu-
ments with 172 events.
Evaluation results are shown in table 5, where
the scores are given for the main slots: Dis-
ease, Country, Total (number of victims), Status
(?dead? or ?sick?) and Time. Results for the Rus-
sian system are somewhat better than for English.
This is due in part to the bias in the process which
we used to select documents for the test suite: the
assessors marked documents in which the system
found events, rather than searching and annotating
documents from scratch. (This aspect of the evalu-
ation will be corrected in future work.) The events
that the system found could be relevant, spurious,
or erroneous; in case the system missed an event,
the assessor?s job was to add it to the gold-standard
answers. Note that in general the amount of irrele-
vant documents processed by PULS is much larger
than the amount of relevant documents (only about
1% of all documents that contain keywords rele-
vant to epidemics contain useful events). Thus it
is impractical to ask assessors to read raw docu-
ments. As a consequence, the scores for the main
slots, such as Disease or Country, may be over-
stated: the majority of documents mention only
one disease, and since an event was found by the
system in most documents selected for the test
suite, the Disease slot is usually filled correctly.
The results for the auxiliary slots, e.g., Time, To-
tal, are closer to our expectation.
5.3 Comparison of Languages and Scenarios
In general, the epidemics scenario performs much
better than security, both in Russian and English.
This is due to fact that the task definition for epi-
demics is simpler, better formalized, and deals
with one type of event only. As noted in (Hut-
Event Type English Russian
Epidemic Surveillance
DISEASE 31 5
HARM 825 412
Total 856 417
Border Security
CRISIS 694 476
HUMAN-TRAFFICKING 10 12
ILLEGAL-MIGRATION 32 31
SMUGGLE 7 19
Total 743 538
Table 6: Number of events found by IE systems in
parallel English-Russian news corpus.
tunen et al, 2002), event representation in text
may have different structure depending on the sce-
nario: the ?classic? IE scenarios, such as the MUC
Management Succession or Terror Attacks, de-
scribe events that occur at a specific point in time,
whereas other scenarios, such as Natural Disasters
or Disease Outbreaks describe a process that is
spread out in time and space. Consequently, events
in the latter (?nature?) scenarios are more com-
plex, may have hierarchical structure, and may
even overlap in text. From the theoretical point of
view it would be interesting to compare how the
events representation, (Pivovarova et al, 2013),
differs in different languages. Moreover, such dif-
ferences can be important in cross-language infor-
mation summarization, (Ji et al, 2013).
We use a freely-available comparable news cor-
pus, (Klementiev and Roth, 2006), to investigate
the difference of event representation in English
and Russian. The corpus contains 2327 BBC mes-
sages from the time period from 1 January 2001 to
10 May 2005, and their approximate translations
from the Lenta.ru website; the translations may be
quite different from their English sources and are
stylistically similar to standard Russian news. We
processed the corpora with the security and epi-
demics IE systems, using the respective language;
the results are presented in the Table 6.
The table shows that for both scenarios the En-
glish system finds more events than the Russian,
which probably means that coverage of the Rus-
sian IE is lower. We have yet to conduct a thor-
ough evaluation of the events found. It is also clear
from the table that specific events are much more
rare than general events; for the security scenario,
the majority of events have type CRISIS, which is
a general type that indicates some incident related
to crime; in the epidemics scenario, the majority
of events have type HARM, i.e., which is a gen-
106
Figure 3: Monthly frequency of events for the four
top-reported diseases in Russia
eral type indicating that there are victims (e.g., hu-
mans) suffering from some cause, not only harm
caused by infections. The distributions of event
types are similar in English and Russian corpora,
which may hint that a lack of specific events may
be a property of the scenarios, irrespective of the
language. This agrees with the expectation that the
majority of retrieved documents are not relevant.
6 Discussion
The Russian-language processing pipeline pre-
sented above is compatible with the working, pre-
existing PULS IE system. It is worth noting again,
that the output of the Russian-language analysis
has the same form as that of the English-language
PULS event extraction, that is, all fills for the tem-
plate slots are output in English (except in the case
of person names). This is made possible by the
shared, language-independent ontology. An im-
portant benefit of this sharing is that the end-user
is not required to understand Russian in order to
determine whether the extracted facts and docu-
ments are relevant to her/his need. Thus, the slot
fills may be presented in English, as shown in Fig-
ure 1. The document text, however, may be pre-
sented in Russian; users who can read Russian can
see the original article text where event elements
are indicated (by highlighting or underlining).
Figure 2 shows a summary-style list of events
found from the news stream. The user can see
events extracted from documents in a mix of lan-
guages (identified by the language tag in the left-
most column). The database representation for
events is shared and independent of the language;
this permits the user get a grasp of current situa-
tion in the domain of interest, in more than one
language.
We checked the impact of the Russian compo-
nent on the system?s coverage over the geographic
area of the former USSR, which includes regions
(outside Russia) where Russian may be used as
a lingua franca, and may be common in press.
Figure 3 shows the total number of events found
in Russia, using both the Russian- and English-
language IE systems for the four most frequently
reported diseases. The check was conducted on
news streams over 2011?2012. The number of
events increases dramatically after deploying the
Russian component, at the end of 2011 (near the
middle of the timeline).
6.1 Conclusion
We have presented a ?plug-in? extension to PULS,
an English-language IE system, to cover Russian-
language text. We currently handle two scenarios:
Security and Epidemic Surveillance. The amount
of effort needed to develop the Russian component
was modest compared to the time and labour spent
on the English-language IE system. The Russian
system demonstrates a comparable level of per-
formance to the baseline English IE: F-measure
is about 4% lower for the Security scenario and
2% higher for the Epidemic Surveillance. We be-
lieve that this success is due to two main factors:
first, the re-use of as many existing modules and
knowledge bases as possible from the pre-existing
English-language system; second, the use of shal-
low, permissive patterns in Russian in combination
with logical inference rules.
In future research, we plan to further expand the
pattern sets and lexicons, to analyze more kinds of
syntactic and lexical phenomena in Russian. We
plan to compare structural differences between the
Security and Epidemics scenarios and their repre-
sentation in Russian and English, to find language-
dependent and language-independent features of
the event representations. We plan to use cross-
lingual analysis to obtain advances in two direc-
tions: first, pre-IE automatic pattern and para-
phrase acquisition for free-word-order languages;
second, post-IE aggregation of extracted informa-
tion to improve overall quality by use of cross-
document context, (Chen and Ji, 2009; Yangarber
and Jokipii, 2005; Yangarber, 2006).
Acknowledgements
We thank Peter von Etter and Mikhail Novikov for
help with the implementation; students of the De-
partment of Information Systems, St. Petersburg
State University, for annotating evaluation data.
Work was funded in part by Frontex, Project on
Automated Event Extraction, and the ALGODAN
Center of Excellence of the Academy of Finland.
107
Figure 1: Document view and template view: a Smuggling event from the Security domain
Figure 2: Summary view: a list of events in the Security domain. The tool-tip under the mouse shows a
snippet of the original text, from which the event was extracted.
References
Ameyugo, G., Art, M., Esteves, A. S., and Piskorski,
J. (2012). Creation of an EU-level information ex-
change network in the domain of border security.
In European Intelligence and Security Informatics
Conference (EISIC). IEEE.
Astaf?eva, I., Bonch-Osmolovskaya, A., Garejshina,
A., Grishina, J., D?jachkov, V., Ionov, M., Korol-
eva, A., Kudrinsky, M., Lityagina, A., Luchina,
E., Sidorova, E., Toldova, S., Lyashevskaya, O.,
Savchuk, S., and Koval?, S. (2010). NLP evaluation:
Russian morphological parsers. In Proceedings of
Dialog Conference, Moscow, Russia.
Atkinson, M., Piskorski, J., van der Goot, E., and Yan-
garber, R. (2011). Multilingual real-time event ex-
traction for border security intelligence gathering.
In Wiil, U. K., editor, Counterterrorism and Open
Source Intelligence, pages 355?390. Springer Lec-
ture Notes in Social Networks, Vol. 2.
Bocharov, V., Pivovarova, L., Rubashkin, V., and
Chuprin, B. (2010). Ontological parsing of encyclo-
pedia information. Computational Linguistics and
Intelligent Text Processing.
Bontcheva, K., Maynard, D., Tablan, V., and Cunning-
ham, H. (2003). GATE: A Unicode-based infras-
tructure supporting multilingual information extrac-
tion. In Proceedings of Workshop on Information
Extraction for Slavonic and other Central and East-
ern European Languages, Borovets, Bulgaria.
Chen, Z. and Ji, H. (2009). Can one language boot-
strap the other: a case study on event extraction. In
Proceedings of the NAACL-HLT Workshop on Semi-
Supervised Learning for Natural Language Process-
ing.
Collier, N., Doan, S., Kawazoe, A., Goodwin, R. M.,
Conway, M., Tateno, Y., Ngo, Q.-H., Dien, D.,
Kawtrakul, A., Takeuchi, K., Shigematsu, M., and
Taniguchi, K. (2008). BioCaster: detecting public
health rumors with a Web-based text mining system.
Bioinformatics, 24(24).
Du, M., von Etter, P., Kopotev, M., Novikov, M., Tar-
beeva, N., and Yangarber, R. (2011). Building sup-
108
port tools for Russian-language information extrac-
tion. In Habernal, I. and Matous?ek, V., editors, Text,
Speech and Dialogue, volume 6836 of Lecture Notes
in Computer Science. Springer Berlin / Heidelberg.
Greenwood, M. and Stevenson, M. (2006). Improv-
ing semi-supervised acquisition of relation extrac-
tion patterns. In Proceedings of Workshop on Infor-
mation Extraction Beyond The Document, COLING-
ACL, volume 3808, pages 29?35. Springer, Lecture
Notes in Artificial Intelligence, Sydney, Australia.
Huttunen, S., Yangarber, R., and Grishman, R. (2002).
Diversity of scenarios in information extraction.
In Proceedings of the Third International Confer-
ence on Language Resources and Evaluation (LREC
2002), Las Palmas de Gran Canaria, Spain.
Ji, H., Favre, B., Lin, W.-P., Gillick, D., Hakkani-
Tur, D., and Grishman, R. (2013). Open-domain
multi-document summarization via information ex-
traction: Challenges and prospects. In Multi-source,
Multilingual Information Extraction and Summa-
rization. Springer.
Khoroshevsky, V. F. (2010). Ontology driven multilin-
gual information extraction and intelligent analytics.
In Web Intelligence and Security: Advances in Data
and Text Mining Techniques for Detecting and Pre-
venting Terrorist Activities on the Web. IOS Press.
Klementiev, A. and Roth, D. (2006). Weakly su-
pervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, Syd-
ney, Australia.
Piskorski, J., Belyaeva, J., and Atkinson, M. (2011).
Exploring the usefulness of cross-lingual informa-
tion fusion for refining real-time news event ex-
traction: A preliminary study. In Proceedings of
RANLP: 8th Conference on Recent Advances in Nat-
ural Language Processing, Hissar, Bulgaria.
Pivovarova, L., Huttunen, S., and Yangarber, R. (2013).
Event representation across genre. In Proceedins of
the 1st Workshop on Events: Definition, Detection,
Coreference, and Representation, NAACL HLT, At-
lanta, Georgia.
Rortais, A., Belyaeva, J., Gemo, M., van der Goot, E.,
and Linge, J. P. (2010). Medisys: An early-warning
system for the detection of (re-)emerging food- and
feed-borne hazards. Food Research International,
43(5):1553?1556.
Schumann, A.-K. (2012). Towards the automated en-
richment of multilingual terminology databases with
knowledge-rich contexts?experiments with russian
eurotermbank data. In CHAT 2012: The Second
Workshop on Creation, Harmonization and Applica-
tion of Terminology Resources, Madrid, Spain.
Sokirko, A. (2001). Semantic dictionaries in automatic
text analysis, based on DIALING system materials.
PhD thesis, Russian State University for the Human-
ities, Moscow.
Solovyev, V., Ivanov, V., Gareev, R., Serebryakov,
S., and Vassilieva, N. (2012). Methodology for
building extraction templates for Russian language
in knowledge-based IE systems. Technical Report
HPL-2012-211, HP Laboratories.
Tanev, H., Zavarella, V., Linge, J., Kabadjov, M.,
Piskorski, J., Atkinson, M., and Steinberger, R.
(2009). Exploiting machine learning techniques to
build an event extraction system for Portuguese and
Spanish. Linguamatica, 2.
Toldova, S. J., Sokolova, E. G., Astaf?eva, I.,
Gareyshina, A., Koroleva, A., Privoznov, D.,
Sidorova, E., Tupikina, L., and Lyashevskaya, O. N.
(2012). NLP evaluation 2011?2012: Russian syn-
tactic parsers. In Proceedings of Dialog Conference,
Moscow, Russia.
Yangarber, R. (2006). Verification of facts across doc-
ument boundaries. In Proceedings of the Interna-
tional Workshop on Intelligent Information Access
(IIIA-2006), Helsinki, Finland.
Yangarber, R., Grishman, R., Tapanainen, P., and Hut-
tunen, S. (2000). Automatic acquisition of do-
main knowledge for information extraction. In
Proceedings of the 18th International Conference
on Computational Linguistics (COLING 2000),
Saarbru?cken, Germany.
Yangarber, R. and Jokipii, L. (2005). Redundancy-
based correction of automatically extracted facts. In
Proceedings of HLT-EMNLP: Conference on Empir-
ical Methods in Natural Language Processing, Van-
couver, Canada.
Yarowsky, D. (1995). Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, Cambridge,
MA. ACM Press.
Zamite, J., Silva, F., Couto, F., and Silva, M. (2010).
MEDCollector: Multisource epidemic data collec-
tor. In Khuri, S., Lhotska?, L., and Pisanti, N., ed-
itors, Information Technology in Bio- and Medical
Informatics, ITBAM 2010. Springer Berlin.
109
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 56?65,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Measuring Language Closeness by Modeling Regularity
Javad Nouri and Roman Yangarber
Department of Computer Science
University of Helsinki, Finland
first.last@cs.helsinki.fi
Abstract
This paper addresses the problems of mea-
suring similarity between languages?
where the term language covers any of the
senses denoted by language, dialect or lin-
guistic variety, as defined by any theory.
We argue that to devise an effective way to
measure the similarity between languages
one should build a probabilistic model that
tries to capture as much regular correspon-
dence between the languages as possible.
This approach yields two benefits. First,
given a set of language data, for any two
models, this gives a way of objectively
determining which model is better, i.e.,
which model is more likely to be accurate
and informative. Second, given a model,
for any two languages we can determine,
in a principled way, how close they are.
The better models will be better at judg-
ing similarity. We present experiments on
data from three language families to sup-
port these ideas. In particular, our results
demonstrate the arbitrary nature of terms
such as language vs. dialect, when applied
to related languages.
1 Introduction
In the context of building and applying NLP tools
to similar languages, language varieties, or di-
alects,
1
we are interested in principled ways of
capturing the notion of language closeness.
Starting from scratch to develop resources and
tools for languages that are close to each other
is expensive; the hope is that the cost can be re-
duced by making use of pre-existing resources and
tools for related languages, which are richer in re-
sources.
1
We use the term language to mean any of: language,
dialect, or linguistic variety, according to any definition.
In the context of this workshop, we assume that
we deal with some method, ?Method X,? that is
applied to two (or more) related languages. For
example, Method X may involve adapting/porting
a linguistic resource from one language to another;
or may be trying to translate between the lan-
guages; etc. We also assume that the success of
Method X directly depends in some way on how
similar?or close?the languages are: that is, the
similarity between the languages is expected to be
a good predictor of how successful the application
of the method will be. Thus, in such a setting, it is
worthwhile to devote some effort to devising good
ways of measuring similarity between languages.
This is the main position of this paper.
We survey some of the approaches to measur-
ing inter-language similarity in Section 2. We as-
sume that we are dealing with languages that are
related genetically (i.e., etymologically). Related
languages may be (dis)similar on many levels; in
this paper, we focus on similarity on the lexical
level. This is admittedly a potential limitation,
since, e.g., for Method X, similarity on the level of
syntactic structure may be more relevant than sim-
ilarity on the lexical level. However, as is done in
other work, we use lexical similarity as a ?general?
indicator of relatedness between the languages.
2
Most of the surveyed methods begin with align-
ment at the level of individual phonetic segments
(phones), which is seen as an essential phase in
the process of evaluating similarity. Alignment
procedures are applied to the input data, which
are sets of words which are judged to be similar
(cognate)?drawn from the related languages.
Once an alignment is obtained using some
method, the natural question arises: how effective
is the particular output alignment?
Once the data is aligned (and, hopefully, aligned
2
This is a well-studied subject in linguistics, with gen-
eral consensus that the lexical level has stronger resistance to
change than other levels.
56
well), it becomes possible to devise measures for
computing distances between the aligned words.
One of the simplest of such measures is the Leven-
shtein edit distance (LED), which is a crude count
of edit operations needed to transform one word
into one another. Averaging across LEDs between
individual word pairs gives an estimate of the dis-
tance between the languages. The question then
arises: how accurate is the obtained distance?
LED has obvious limitations. LED charges an
edit operation for substituting similar as well as
dissimilar phones?regardless of how regular (and
hence, probable) a given substitution is. Con-
versely, LED charges nothing for substituting a
phone x in language A for the same phone in lan-
guage B, even if x in A regularly (e.g., always!)
corresponds to y in B. More sophisticated variants
of LED are then proposed, which try to take into
account some aspects of the natural alignment set-
ting (such as assigning different weights to dif-
ferent edit operations, e.g., by saying that it is
cheaper to transform t into d than t into w).
Thus, in pursuit of effective similarity mea-
sures, we are faced with a sequence of steps:
procedures for aligning data produce alignments;
from the individual word-level alignments we de-
rive distance measures; averaging distances across
all words we obtain similarity measures between
languages; we then require methods for compar-
ing and validating the resulting language distance
measures. At various phases, these steps involve
subjectivity?typically in the form of gold stan-
dards. We discuss the kinds of subjectivity en-
countered with this approach in detail in Sec-
tion 2.1.
As an alternative approach, we advocate view-
ing closeness between languages in terms of regu-
larity in the data: if two languages are very close,
it means that either the differences between them
are very few, or?if they are many?then they
are very regular.
3
As the number of differences
grows and their nature becomes less regular, the
languages grow more distant. The goal then is to
build probabilistic models that capture regularity
in the data; to do this, we need to devise algorithms
to discover as much regularity as possible.
This approach yields several advantages. First,
a model assigns a probability to observed data.
This has deep implications for this task, since it
3
In the former case, the differences form a short list; in the
latter, the rules describing the differences form a short list.
allows us to quantify uncertainty in a principled
fashion, rather than commit to ad-hoc decisions
and prior assumptions. We will show that prob-
abilistic modeling requires us to make fewer sub-
jective judgements. Second, the probabilities that
the models assign to data allow us to build natu-
ral distance measures. A pair of languages whose
data have a higher probability under a given model
are closer than a pair with a lower probability, in
a well-defined sense. This also allows us to de-
fine distance between individual word pairs. The
smarter the model?i.e., the more regularity it cap-
tures in the data?the more we will be able to
trust in the distance measures based on the model.
Third?and equally important for this problem
setting?this offers a principled way of comparing
methods: if model X assigns higher probability to
real data than model Y, then model X is better, and
can be trusted more. The key point here is that
we can then compare models without any ?ground
truth? or gold-standard, pre-annotated data.
One way to see this is by using the model to
predict unobserved data. We can withhold one
word pair (w
A
, w
B
) from languages A and B be-
fore building the model (so the model does not
see the true correspondence); once the model is
built, show it w
A
, and ask what is the correspond-
ing word in B. Theoretically, this is simple: the
best guess for w?
B
is simply the one that maxi-
mizes the probability of the pair p
M
(w
A
, w?
B
) un-
der the model, over all possible strings w?
B
in B.
4
Measuring the distance between w
B
and w?
B
tells
how good M is at predicting unseen data. Now, if
model M
1
consistently predicts better than M
2
, it
is very difficult to argue thatM
1
is in any sense the
worse model; and it is able to predict better only
because it has succeeded in learning more about
the data and the regularities in it.
Thus we can compare different models for mea-
suring linguistic similarity. And this can be done
in a principled fashion?if the distances are based
on probabilistic models.
The paper is organized as follows. We continue
with a discussion of related work. In Section 3
we present one particular approach to modeling,
based on information-theoretic principles. In Sec-
tion 4 we show some applications of these models
to several linguistic data sets, from three different
language families. We conclude with plans for fu-
4
In practice, this can be done efficiently, using heuristics
to constrain the search over all strings w?
B
in B.
57
ture work, in Section 5.
2 Related work
In this section we survey related work on similar-
ity measures between languages, and contrast the
principles on which this work relies against the
principles which we advocate.
2.1 Subjectivity
Typically, alignment-based approaches use several
kinds of inputs that have a subjective nature.
One such input is the data itself, which is to
be aligned. For a pair of closely related di-
alects, deciding which words to align may ap-
pear ?self-evident.? However, as we take di-
alects/languages that are progressively more dis-
tant, such judgements become progressively less
self-evident; therefore, in all cases, we should
keep in mind that the input data itself is a source of
subjectivity in measuring similarity based on data
that is comprised of lists of related words.
Another source of subjectivity in some of the
related work is gold-standard alignments, which
accompany the input data. Again, for very close
languages, the ?correct? alignment may appear to
be obvious. However, we must recognize that this
necessarily involves subjective judgements from
the creators of the gold-standard alignment.
Further, many alignment methods pre-suppose
one-to-one correspondence between phones. On
one hand, this is due to limitations of the meth-
ods themselves (there exist methods for aligning
phones in other than one-to-one fashion); on an-
other hand, it violates accepted linguistic under-
standing that phones do not need to correspond
in a one-to-one fashion among close languages.
Another potential source of subjectivity comes in
the form of prior assumptions or restrictions on
permissible alignments.
5
Another common as-
sumption is insistence on consonant-to-consonant
and vowel-to-vowel alignments. More relaxed as-
sumptions may come in the form of prior proba-
bilities of phone alignments. Although these may
appear ?natural? in some sense, it is important to
keep in mind that they are ad hoc, and reflect a
subjective judgement which may not be correct.
After alignment and computation of language
distance, the question arises: which of the dis-
tance measures is more accurate? Again, one way
5
One-to-one alignment is actually one such restriction.
to answer this question is to resort to gold stan-
dards. For example, this can be done via phylo-
genetic clustering; if method A says language l
1
is closer to l
2
than to l
3
, and method B says the
opposite (that l
1
is closer to l
3
), and if we ?know?
the latter to be true?from a gold standard?then
we can prefer method B. Further, if we have a
gold-standard tree for the group of languages, we
can apply tree-distance measures
6
to check how
the trees generated by a given method differ from
the gold-standard. The method that deviates least
from the gold standard is then considered best.
2.2 Levenshtein-based algorithms
The Levenshtein algorithm is a dynamic program-
ming approach for aligning a word pair (A,B) us-
ing a least expensive set of insertion, deletion and
substitution operations required for transforming
A into B. While the original Levenshtein edit dis-
tance is based on these three operations without
any restrictions, later algorithms adapt this method
by additional edit operations or restrictions.
Wieling et al. (2009) compare several align-
ment algorithms applied to dialect pronunciation
data. These algorithms include several adaptations
of the Levenshtein algorithm and the Pair Hid-
den Markov Model. They evaluate the algorithms
by comparing the resulting pairwise alignments to
alignments generated from a set of manually cor-
rected multiple alignments. Standard Levenshtein
edit distance is used for comparing the output of
each algorithm to the gold standard alignment, to
determine which algorithm is preferred.
All alignment algorithms based on Levenshtein
distance evaluated by Wieling et al. (2009) restrict
aligning vowels with consonants.
VC-sensitive Levenshtein algorithm: uses the
standard Levenshtein algorithm, prohibits aligning
vowels with consonants, and assigns unit cost for
all edit operations. The only sense in which it cap-
tures regularities is the assumption that the same
symbol in two languages represents same sound,
which results in assigning a cost of 0 to aligning
a symbol to itself. It also prevents the algorithm
from finding vowel-to-consonant correspondences
(found in some languages), such as u?v, u?l, etc.
Levenshtein algorithm with Swap: adds an edit
operation to enable the algorithm to capture phe-
nomena such as metathesis, via a transposition:
6
Tree-distance measures are developed in the context of
work on phylogenetic trees in biological/genetic applications.
58
aligning ab inA to ba inB costs a single edit oper-
ation. This algorithm also forbids aligning vowels
to consonants, except in a swap.
Levenshtein algorithm with generated segment
distances based on phonetic features: The above
algorithms assign unit cost for all edit opera-
tions, regardless of how the segments are related.
Heeringa (2004) uses a variant where the distances
are obtained from differences between phonetic
features of the segment pairs. The authors observe
that this is subjective because one could choose
from different possible feature sets.
Levenshtein algorithm with generated segment
distances based on acoustic features: To avoid
subjectivity of feature selection, Heeringa (2004)
experiments with assigning different costs to dif-
ferent segment pairs based on how phonetically
close they are; segment distances are calculated
by comparing spectrograms of recorded pronun-
ciations. These algorithms do not attempt to dis-
cover regularity in data, since they only consider
the word pair at a time, using no information about
the rest of the data.
Levenshtein algorithm with distances based on
PMI: Wieling et al. (2009) use Point-wise Mutual
Information (PMI) as the basis for segment dis-
tances. They assign different costs to segments,
and use the entire dataset for each alignment. PMI
for outcomes x and y of random variables X and
Y is defined as:
pmi(x, y) = log
2
p(x, y)
p(x)p(y)
(1)
PMI is calculated using estimated probabilities of
the events. Since greater PMI shows higher ten-
dency of x and y to co-occur, it is reversed and
normalized to obtain a dissimilarity measure to
be used as segment distance. Details about this
method are in (Wieling and Nerbonne, 2011).
2.3 Other distance measures
Ellison and Kirby (2006) present a distance mea-
sure based on comparing intra-language lexica
only, arguing that there is no well-founded com-
mon language-independent phonetic space to be
used for comparing word forms across languages.
Instead, they focus on inferring the distances by
comparing how meanings in language A are likely
to be confused for each other, and comparing it to
the confusion probabilities in language B.
Given a lexicon containing mappings from a set
of meanings M to a set of forms F , confusion
probability P (m
1
|m
2
;L) for each pair of mean-
ings (m
1
,m
2
) in L is the probability of confus-
ing m
1
for m
2
. This probability is formulated
based on an adaptation of neighborhood activation
model, and depends on the edit distance between
the corresponding forms in the lexicon. Following
this approach, they construct a confusion probabil-
ity matrix for each language, which can be viewed
as a probability distribution. Inter-language dis-
tances are then calculated as the distance between
the corresponding distributions, using symmetric
Kullback-Liebler distance and Rao distance . The
inferred distances are used to construct a phylo-
genetic tree of the Indo-European languages. The
approach is evaluated by comparing the resulting
taxonomy to a gold-standard tree, which is re-
ported to be a good fit.
As with other presented methods, although
this method can be seen as measuring distances
between languages, there remain two problems.
First, they do not reflect the genetic differences
and similarities?and regularities?between the
languages in a transparent, easily interpretable
way. Second, they offer no direct way to com-
pare competing approaches, except indirectly, and
using (subjective) gold-standards.
3 Methods for measuring language
closeness
We now discuss an approach which follows the
proposal outlined in Section 1, and allows us to
build probabilistic models for measuring closeness
between languages. Other approaches that rely on
probabilistic modeling would serve equally well.
A comprehensive survey of methods for measur-
ing language closeness may be found in (Wiel-
ing and Nerbonne, 2015). Work that is proba-
bilistically oriented, similarly to our proposed ap-
proaches, includes (Bouchard-C?ot?e et al., 2007;
Kondrak, 2004) and others. We next review two
types of models (some of which are described
elsewhere), which are based on information-
theoretic principles. We discuss how these models
suit the proposed approach, in the next section.
3.1 1-1 symbol model
We begin with our ?basic? model, described
in (Wettig and Yangarber, 2011; Wettig et
al., 2011), which makes several simplifying
assumptions?which the subsequent, more ad-
vanced models relax (Wettig et al., 2012; Wettig
59
et al., 2013).
7
The basic model is based on align-
ment, similarly to much of the related work men-
tioned above: for every word pair in our data set?
the ?corpus??it builds a complete alignment for
all symbols (Wettig et al., 2011). The basic model
considers pairwise alignments only, i.e., two lan-
guages at a time; we call them the source and
the target languages. Later models relax this re-
striction by using N-dimensional alignment, with
N > 2 languages aligned simultaneously. The
basic model allows only 1-1 symbol alignments:
one source symbol
8
may correspond to one tar-
get symbol?or to the empty symbol  (which we
mark as ?.?). More advanced models align sub-
strings of more than one symbol to each other. The
basic model also ignores context, whereas in re-
ality symbol correspondences are heavily condi-
tioned on their context. Finally, the basic model
treats the symbols as atoms, whereas more ad-
vanced models treat the symbols as vectors of dis-
tinctive features.
We distinguish between the raw, observed data
and complete data?i.e., complete with the align-
ment; the hidden data is where the insertions and
deletions occur. For example, if we ask what is
the ?correct? alignment between Finnish vuosi and
Khanty al (cognate words from these two Uralic
languages, both meaning ?year?):
v u o . s i v u o s i
| | | | | | | | | | |
. a . l . . . . a l .
are two possible alignments, among many oth-
ers. From among all alignments, we seek the best
alignment: one that is globally optimal, i.e., one
that is consistent with as many regular sound cor-
respondences as possible. This leads to a chicken-
and-egg problem: on one hand, if we had the best
alignment for the data, we could simply read off
a set of rules, by observing which source sym-
bol corresponds frequently to which target sym-
bol. On the other hand, if we had a complete
set of rules, we could construct the best align-
ment, by using dynamic programming (`a la one of
the above mentioned methods, since the costs of
all possible edit operations are determined by the
rules). Since at the start we have neither, the rules
and the alignment are bootstrapped in tandem.
7
The models can be downloaded from ety-
mon.cs.helsinki.fi
8
In this paper, we equate symbols with sounds: we assume
our data to be given in phonetic transcription.
Following the Minimum Description Length
(MDL) principle, the best alignment is the one that
can be encoded (i.e., written down) in the shortest
space. That is, we aim to code the complete data?
for all word pairs in the given language pair?as
compactly as possible. To find the optimal align-
ment, we need A. an objective function?a way to
measure the quality of any given alignment?and
B. a search algorithm, to sift through all possible
alignments for one that optimizes the objective.
We can use various methods to code the com-
plete data. Essentially, they all amount to measur-
ing how many bits it costs to ?transmit? the com-
plete set of alignment ?events?, where each align-
ment event e is a pair of aligned symbols (? : ?)
e = (? : ?) ? ? ?
{
.,#
}
? T ?
{
.,#
}
drawn from the source alphabet ? and the target
alphabet T , respectively.
9
One possible coding
scheme is ?prequential? coding, or the Bayesian
marginal likelihood, see, e.g., (Kontkanen et al.,
1996), used in (Wettig et al., 2011); another is nor-
malized maximum likelihood (NML) code, (Ris-
sanen, 1996), used in (Wettig et al., 2012).
Prequential coding gives the total code length
L
base
(D) = ?
?
e?E
log c(e)!
+ log
[
?
e?E
c(e) +K ? 1
]
!? log(K ? 1)! (2)
for data D. Here, c(e) denotes the event count,
and K is the total number of event types.
To find the optimal alignments, the algorithm
starts with aligning word pairs randomly, and then
iteratively searching for the best alignment given
rest of the data for each word pair at a time. To do
this, we first exclude the current alignment from
our complete data. The best alignment in the re-
aligning process is found using a Dynamic Pro-
gramming matrix, with source word symbols in
the rows and target word symbols as the columns.
Each possible alignment of the word pair corre-
sponds to a path from top-left cell of the matrix
to the bottom-right cell. Each cell V (?
i
, ?
j
) holds
the cost of aligning sub-string ?
1
..?
i
with ?
1
..?
j
,
and is computed as:
V (?
i
, ?
j
) = min
{
V (?
i
, ?
j?1
) +L(. : ?
j
)
V (?
i?1
, ?
j
) +L(?
i
: .)
V (?
i?1
, ?
j?1
) +L(?
i
: ?
j
)
(3)
9
Note, that the alphabets need not be the same, or even
have any symbols in common. We add a special end-of-word
symbol, always aligned to itself: (# : #). Empty alignments
(. : .) are not allowed.
60
where L(e) is the cost of coding event e. The cost
of aligning the full word pair, is then found in the
bottom-right cell, and the corresponding path is
chosen as the new alignment, which is registered
back into the complete data.
We should mention that due to vulnerability of
the algorithm to local optima, we use simulated
annealing with (50) random restarts.
3.2 Context model
Context model is described in detail in (Wettig et
al., 2013). We use a modified version of this model
to achieve faster run-time.
One limitation of the basic model described
above is that it uses no information about the con-
text of the sounds, thus ignoring the fact that lin-
guistic sound change is regular and highly depends
on context. The 1-1 model also treats symbols of
the words as atoms, ignoring how two sounds are
phonetically close. The context model, addresses
both of these issues.
Each sound is represented as a vector of distinc-
tive phonetic features. Since we are using MDL as
the basis of the model here, we need to code (i.e.,
transmit) the data. This can be done by coding one
feature at a time on each level.
To code a feature F on a level L, we construct a
decision tree. First, we collect all instances of the
sounds in the data of the corresponding level that
have the current feature, and then build a count
matrix based on how many instances take each
value. Here is an example of such a matrix for
feature V (vertical articulation of a vowel).
V Close Mid-close Mid-open Open
10 25 33 24
This shows that there are 10 close vowels, 25
mid-close vowels, etc.
This serves as the root node of the tree. The tree
can then query features of the sounds in the current
context by choosing from a set of candidate con-
texts. Each candidate is a triplet (L,P, F ), repre-
senting Level, Position, and Feature respectively.
L can be either source or target, since we are
dealing with a pair of language varieties at a time.
P is the position of the sound that is being queried
relative to current sound, and F is the feature be-
ing queried. Examples of a Position are previ-
ous vowel, previous position, itself, etc. The tree
expands depending on the possible responses to
the query, resulting in child nodes with their own
count matrix. The idea here is to make the matri-
ces in the child nodes as sparse as possible in order
to code them with fewer bits.
This process continues until the tree cannot be
expanded any more. Finally the data in each leaf
node is coded using prequential coding as before
with the same cost explained in Equation 2.
Code length for the complete data consists of
cost of encoding the trees and the cost of encoding
the data given the trees. The search algorithm re-
mains the same as the 1-1 algorithm, but uses the
constructed trees to calculate the cost of events.
This method spends much time rebuilding the
trees on each iteration; its run-time is very high.
In the modified version used in this paper, the trees
are not allowed to expand initially, when the model
has just started and everything is random due to
simulated annealing. Once the simulated anneal-
ing phase is complete, the trees are expanded fully
normally. Our experiments show that this results
in trees that are equally good as the original ones.
3.3 Normalized Compression Distance
The cost of coding the data for a language pair un-
der a model reflects the amount of regularity the
model discovered, and thus is a means of measur-
ing the distance between these languages. How-
ever the cost also depends on the size of the data
for the language pair; thus, a way of normalizing
the cost is needed to make them comparable across
language pairs. We use ?Normalized Compres-
sion Distance? (NCD), described in (Cilibrasi and
Vitanyi, 2005) to achieve this.
Given a model that can compress a language
pair (a, b) with cost C(a, b), NCD of (a, b) is:
NCD(a, b) =
C(a, b)?min
(
C(a), C(b)
)
max
(
C(a), C(b)
)
(4)
Since NCD of different pairs are comparable un-
der the same model, it can be used as a distance
measure between language varieties.
3.4 Prediction of unobserved data
The models mentioned above are also able to
predict unobserved data as described in Sec-
tion 1 (Wettig et al., 2013).
For the basic 1-1 model, since no informa-
tion about the context is used, prediction sim-
ply means looking for the most probable symbol
in target language for each symbol of w
A
. For
the context model, a more sophisticated dynamic-
programming heuristic is needed to predict the un-
seen word, (Hiltunen, 2012). The predicted word
61
 
10000
 
15000
 
20000
 
25000
 
30000
 
10000
 
15000
 
20000
 
25000
 
30000
Context model
1x1 m
odel
MDL 
cost
azb bas blkx chv hak jak kaz krg nogx qum shr sjg tat tof trk trm tuv uig uzb y = x
Figure 1: Model comparison: MDL costs.
w?
B
is then compared to the real corresponding
word w
B
to measure how well the model per-
formed on the task.
Feature-wise Levenshtein edit distance is used
for this comparison. The edit distances for all
word pairs are normalized, resulting in Normal-
ized Feature-wise Edit Distance (NFED) which
can serve as a measure of model quality.
4 Experiments
To illustrate the principles discussed above, we
experiment with the two principal model types de-
scribed above?the baseline 1-1 model and the
context-sensitive model, using data from three dif-
ferent language families.
4.1 Data
We use data from the StarLing data bases,
(Starostin, 2005), for the Turkic and Uralic lan-
guage families, and for the Slavic branch of the
Indo-European family. For dozens of language
families, StarLing has rich data sets (going be-
yond Swadesh-style lists, as in some other lexical
data collections built for judging language and di-
alect distances). The databases are under constant
development, and have different quality. Some
datasets, (most notably the IE data) are drawn
from multiple sources, which use different nota-
tion, transcription, etc., and are not yet unified.
The data we chose for use is particularly clean.
For the Turkic family, StarLing at present con-
tains 2017 cognate sets; we use 19 (of the total 27)
languages, which have a substantial amount of at-
tested word-forms in the data collection.
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6  0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
Context model
1x1 m
odel
Norm
alized
 Featu
rewis
e Edit
 Dista
nce (N
FED)
azb bas blkx chv hak jak kaz krg nogx qum shr sjg tat tof trk trm tuv uig uzb y = x
Figure 2: Model comparison: NFED.
4.2 Model comparison
We first demonstrate how the ?best? model can be
chosen from among several models, in a principled
way. This is feasible if we work with probabilis-
tic models?models that assign probabilities to the
observed data. If the model is also able to perform
prediction (of unseen data), then we can measure
the model?s predictive power and select the best
model using predictive power as the criterion. We
will show that in the case of the two probabilistic
models presented above, these two criteria yield
the same result.
We ran the baseline 1-1 model and the context
model against the entire Turkic dataset, i.e., the
19 ? 18 language pairs,
10
(with 50 restarts for
each pair, a total of 17100 runs). For each lan-
guage pair, we select the best out of 50 runs for
each model, according to the cost it assigns to this
language pair. Figure 1 shows the costs obtained
by the best run: each point denotes a language
pair; X-coordinate is the cost according to the 1-
1 model, Y-coordinate is the cost of the context
model. The Figure shows that all 19?18 points lie
below the diagonal (x=y), i.e., for every language
pair, the context model finds a code with lower
cost?as is expected, since the context model is
?smarter,? uses more information from the data,
and hence finds more regularity in it.
Next, for each language pair, we take the run
that found the lowest cost, and use it to impute
unseen data, as explained in Section 3?yielding
NFED, the distance from the imputed string to the
10
Turkic languages in tables and figures are:
azb:Azerbaijani, bas:Bashkir, blk:Balkar, chv:Chuvash,
hak:Khakas, jak:Yakut, kaz:Kazakh, krg:Kyrgyz, nog:Nogaj,
qum:Qumyk, shr:Shor, sjg:Sary Uyghur, tat:Tatar,
tof:Tofalar, trk:Turkish, trm:Turkmen, tuv:Tuva, uig:Uyghur,
uzb:Uzbek.
62
ru ukr cz slk pl usrb lsrb bulg scr
ru 0 .41 .41 .39 .41 .51 .53 .48 .40
ukr .41 0 .48 .46 .51 .49 .50 .48 .47
cz .40 .48 0 .29 .38 .45 .52 .50 .39
slk .38 .45 .29 0 .38 .41 .44 .45 .38
pl .43 .51 .39 .41 0 .48 .50 .52 .45
usrb .50 .48 .44 .40 .46 0 .29 .49 .48
lsrb .52 .51 .49 .44 .47 .30 0 .52 .50
bulg .46 .47 .48 .45 .51 .47 .49 0 .41
scr .40 .47 .38 .38 .43 .49 .51 .44 0
Table 1: NCDs for 9 Slavic languages, StarLing
database: context model
actual, correct string in the target language. This
again yields 19?18 points, shown in Figure 2; this
time the X and Y values lie between 0 and 1, since
NFED is normalized. (In the figure, the points are
linked with line segments as follows: for any pair
(a,b) the point (a,b) is joined by a line to the point
(b,a). This is done for easier identification, since
the point (a,b) displays the legend symbol for only
language a.) Overall, many more points lie below
the diagonal, (approximately 10% of the points are
above). The context model performs better, and it
would therefore be a safer/wiser choice, if we wish
to measure language closeness; which agrees with
the result obtained using raw compression costs.
The key point here is that this compari-
son method can accommodate any probabilistic
model: for any new candidate model we check?
over the same datasets?what probability values
does the model assign to each data point. Probabil-
ities and (compression) costs are interchangeable:
information theory tells us that for a data set D and
model M, the probability P of data D under model
M and the cost (code length) L of D under M are
related by: L
M
(D) = ? logP
M
(D). If the new
model assigns higher probability (or lower cost) to
observed data, it is preferable?obviating the need
for gold-standards, or subjective judgements.
4.3 Language closeness
We next explore various datasets using the context
model?the better model we have available.
Uralic: We begin with Uralic data from Star-
Ling.
11
The Uralic database contains data from
more than one variant of many languages: we ex-
tracted data for the top two dialects?in terms of
counts of available word-forms?for Komi, Ud-
11
We use data from the Finno-Ugric sub-family. The
language codes are: est:Estonian, fin:Finnish, khn:Khanty,
kom:Komi, man:Mansi, mar:Mari, mrd:Mordva, saa:Saami,
udm:Udmurt.
Language pair NCD
kom s kom p .18
kom p kom s .19
udm s udm g .20
udm g udm s .21
mar b mar kb .28
mar kb mar b .28
mrd m mrd e .29
mrd e mrd m .29
est fin .32
fin est .32
man p man so .34
khn v khn dn .35
khn dn khn v .36
man so man p .36
saa n saa l .37
saa l saa n .37
Table 2: Comparison of Uralic dialect/language
pairs, sorted by NCD: context model.
murt, Mari, Mordva, Mansi, Khanty and Saami.
Table 2 shows the normalized compression dis-
tances for each of the pairs; the NCD costs for
Finnish and Estonian are given for comparison.
It is striking that the pairs that score below
Finnish/Estonian are all ?true? dialects, whereas
those that score above are not. E.g., the Mansi
variants Pelym and Sosva, (Honti, 1998), and
Demjanka and Vakh Khanty, (Abondolo, 1998),
are mutually unintelligible. The same is true for
North and Lule Saami.
Turkic: We compute NCDs for the Turkic lan-
guages under the context model. Some of the Tur-
kic languages are known to form a much tighter
dialect continuum, (Johanson, 1998), which is ev-
ident from the NCDs in Table 3. E.g., Tofa is most-
closely related to the Tuvan language and forms a
dialect continuum with it, (Johanson, 1998). Turk-
ish and Azerbaijani closely resemble each other
and are mutually intelligible. In the table we high-
light language pairs with NCD ? 0.30.
Slavic: We analyzed data from StarLing for 9
Slavic languages.
12
The NCDs are shown in Ta-
ble 1. Of all pairs, the normalized compression
costs for (cz, slk) and (lsrb, usrb) fall below the
.30 mark, and indeed these pairs have high mutual
intelligibility, unlike all other pairs.
When the data from Table 1 are fed into
the NeighborJoining algorithm, (Saitou and Nei,
1987), it draws the phylogeny in Figure 3, which
clearly separates the languages into the 3 ac-
cepted branches of Slavic: East (ru, ukr), South
12
The Slavic languages from StarLing: bulg:Bulgarian,
cz:Czech, pl:Polish, ru:Russian, slk:Slovak, scr:Serbo-
Croatian, ukr:Ukrainian, lsrb/usrb:Lower and Upper Sorbian.
63
0 0.05 0.1 0.15 0.2 0.25
SCR 
BULG
RU  
UKR 
SLK 
CZ  
PL  
USRB
LSRB
Figure 3: NeighborJoining tree for Slavic lan-
guages in Table 1.
(scr, bulg) and West (pl, cz, slk, u/lsrb). The
phylogeny also supports later separation (rela-
tive time depth > 0.05) of the pairs with higher
mutual intelligibility?Upper/Lower Sorbian, and
Czech/Slovak.
13
5 Conclusions and future work
We have presented a case for using probabilistic
modeling when we need reliable quantitative mea-
sures of language closeness. Such needs arise, for
example, when one attempts to develop methods
whose success directly depends on how close the
languages in question are. We attempt to demon-
strate two main points. One is that using proba-
bilistic models provides a principled and natural
way of comparing models?to determine which
candidate model we can trust more when mea-
suring how close the languages are. It also lets
us compare models without having to build gold-
standard datasets; this is important, since gold-
standards are subjective, not always reliable, and
expensive to produce. We are really interested in
regularity, and the proof of the model?s quality is
in its ability to assign high probability to observed
and unobserved data.
The second main point of the paper is show-
ing how probabilistic models can be employed to
measure language closeness. Our best-performing
model seems to provide reasonable judgements
of closeness when applied to languages/linguistic
variants from very different language families. For
all of Uralic, Turkic and Slavic data, those that fell
13
We should note that the NCDs produce excellent phylo-
genies also for the Turkic and Uralic data; not included here
due to space constraints.
below the 0.30 mark on the NCD axis are known
to have higher mutual intelligibility, while those
that are above the mark have lower or no mutual
intelligibility. Of course, we do not claim that 0.30
is a magic number; for a different model the line
of demarcation may fall elsewhere entirely. How-
ever, it shows that the model (which we selected
on the basis of its superiority according to our se-
lection criteria) is quite consistent in predicting the
degree of mutual intelligibility, overall.
Incidentally, these experiments demonstrate, in
a principled fashion, the well-known arbitrary na-
ture of the terms language vs. dialect?this dis-
tinction is simply not supported by real linguis-
tic data. More importantly, probabilistic methods
require us to make fewer subjective judgements,
with no ad hoc priors or gold-standards, which in
many cases are difficult to obtain and justify?and
rather rely on the observed data as the ultimate and
sufficient truth.
Acknowledgments
This research was supported in part by: the
FinUgRevita Project of the Academy of Finland,
and by the National Centre of Excellence ?Al-
gorithmic Data Analysis (ALGODAN)? of the
Academy of Finland.
References
Daniel Abondolo. 1998. Khanty. In Daniel Abon-
dolo, editor, The Uralic Languages, pages 358?386.
Routledge.
Alexandre Bouchard-C?ot?e, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2007. A probabilistic ap-
proach to diachronic phonology. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL:2007),
pages 887?896, Prague, Czech Republic.
Rudi Cilibrasi and Paul M.B. Vitanyi. 2005. Cluster-
ing by compression. IEEE Transactions on Infor-
mation Theory, 51(4):1523?1545.
T. Mark Ellison and Simon Kirby. 2006. Measuring
language divergence by intra-lexical comparison. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING/ACL-2006), pages 273?280, Syd-
ney, Australia.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
64
azb bas blk chv hak jak kaz krg nog qum shr sjg tat tof trk trm tuv uig uzb
azb 0 .40 .35 .60 .48 .59 .38 .39 .35 .34 .43 .41 .38 .45 .27 .33 .46 .40 .37
bas .39 0 .28 .58 .45 .59 .27 .31 .24 .26 .40 .41 .21 .46 .39 .38 .45 .36 .34
blk .35 .30 0 .57 .42 .57 .26 .28 .22 .19 .36 .40 .27 .42 .34 .36 .42 .35 .30
chv .59 .59 .56 0 .62 .67 .56 .58 .55 .53 .60 .57 .56 .60 .56 .60 .62 .61 .58
hak .47 .44 .41 .63 0 .58 .41 .40 .37 .40 .27 .43 .43 .39 .46 .50 .40 .46 .46
jak .57 .57 .57 .70 .58 0 .56 .57 .55 .54 .55 .54 .57 .51 .58 .57 .56 .58 .57
kaz .38 .28 .27 .57 .42 .57 0 .24 .16 .24 .38 .39 .29 .44 .37 .39 .41 .36 .33
krg .38 .31 .27 .60 .40 .57 .23 0 .21 .26 .35 .40 .32 .41 .36 .39 .40 .35 .33
nog .35 .25 .22 .57 .39 .55 .15 .22 0 .19 .36 .38 .26 .43 .33 .35 .41 .35 .31
qum .34 .27 .19 .57 .41 .55 .23 .26 .19 0 .35 .37 .26 .41 .33 .35 .41 .33 .31
shr .43 .40 .36 .63 .28 .55 .38 .36 .35 .34 0 .40 .40 .36 .43 .44 .38 .42 .42
sjg .43 .42 .41 .58 .45 .55 .40 .41 .39 .38 .40 0 .42 .44 .43 .43 .43 .41 .41
tat .36 .22 .27 .60 .44 .59 .28 .32 .26 .26 .40 .41 0 .45 .38 .38 .45 .36 .33
tof .47 .45 .42 .61 .39 .50 .42 .42 .42 .41 .36 .42 .45 0 .48 .46 .24 .44 .43
trk .28 .40 .35 .58 .48 .59 .37 .36 .33 .34 .43 .42 .39 .47 0 .34 .46 .40 .38
trm .32 .40 .36 .62 .51 .59 .39 .40 .36 .35 .44 .43 .39 .46 .34 0 .49 .41 .36
tuv .46 .46 .41 .63 .40 .56 .41 .40 .41 .41 .38 .42 .45 .23 .45 .48 0 .45 .46
uig .40 .39 .34 .60 .49 .58 .36 .36 .36 .33 .43 .40 .38 .45 .41 .42 .46 0 .33
uzb .37 .36 .31 .60 .48 .58 .34 .34 .32 .32 .43 .41 .34 .44 .38 .36 .47 .33 0
Table 3: Normalized compression distances for 19 Turkic languages (StarLing database): context model
Suvi Hiltunen. 2012. Minimum description length
modeling of etymological data. Master?s thesis,
University of Helsinki.
L?aszl?o Honti. 1998. Ob? Ugrian. In Daniel Abon-
dolo, editor, The Uralic Languages, pages 327?357.
Routledge.
Lars Johanson. 1998. The history of Turkic. In
Lars Johanson &
?
Eva
?
Agnes Csat?o, editor, The Tur-
kic Languages, pages 81?125. London, New York:
Routledge. Classification of Turkic languages (at
Turkiclanguages.com).
Grzegorz Kondrak. 2004. Combining evidence in
cognate identification. In Proceedings of the Sev-
enteenth Canadian Conference on Artificial Intelli-
gence (Canadian AI 2004), pages 44?59, London,
Ontario. Lecture Notes in Computer Science 3060,
Springer-Verlag.
Petri Kontkanen, Petri Myllym?aki, and Henry Tirri.
1996. Constructing Bayesian finite mixture models
by the EM algorithm. Technical Report NC-TR-97-
003, ESPRIT NeuroCOLT: Working Group on Neu-
ral and Computational Learning.
Jorma Rissanen. 1996. Fisher information and
stochastic complexity. IEEE Transactions on Infor-
mation Theory, 42(1):40?47.
Naruya Saitou and Masatoshi Nei. 1987. The
neighbor-joining method: a new method for recon-
structing phylogenetic trees. Molecular biology and
evolution, 4(4):406?425.
Sergei A. Starostin. 2005. Tower of Babel: StarLing
etymological databases. http://newstar.rinet.ru/.
Hannes Wettig and Roman Yangarber. 2011. Proba-
bilistic models for alignment of etymological data.
In Proceedings of NoDaLiDa: the 18th Nordic Con-
ference on Computational Linguistics, Riga, Latvia.
Hannes Wettig, Suvi Hiltunen, and Roman Yangarber.
2011. MDL-based Models for Alignment of Et-
ymological Data. In Proceedings of RANLP: the
8th Conference on Recent Advances in Natural Lan-
guage Processing, Hissar, Bulgaria.
Hannes Wettig, Kirill Reshetnikov, and Roman Yan-
garber. 2012. Using context and phonetic features
in models of etymological sound change. In Proc.
EACL Workshop on Visualization of Linguistic Pat-
terns and Uncovering Language History from Mul-
tilingual Resources, pages 37?44, Avignon, France.
Hannes Wettig, Javad Nouri, Kirill Reshetnikov, and
Roman Yangarber. 2013. Information-theoretic
modeling of etymological sound change. In Lars
Borin and Anju Saxena, editors, Approaches to mea-
suring linguistic differences, volume 265 of Trends
in Linguistics, pages 507?531. de Gruyter Mouton.
Martijn Wieling and John Nerbonne. 2011. Measur-
ing linguistic variation commensurably. In Dialec-
tologia Special Issue II: Production, Perception and
Attitude, pages 141?162.
Martijn Wieling and John Nerbonne. 2015. Advances
in dialectometry. In Annual Review of Linguistics,
volume 1. To appear.
Martijn Wieling, Jelena Proki?c, and John Nerbonne.
2009. Evaluating the pairwise string alignment of
pronunciations. In Proceedings of the EACL 2009
Workshop on Language Technology and Resources
for Cultural Heritage, Social Sciences, Humanities,
and Education, pages 26?34, Athens, Greece.
65
