Answer  Ext rac t ion  
Steven Abney  Michae l  Co l l ins  Ami t  S ingha l  
AT&T Shannon Laboratory  
180 Park  Ave. 
F lo rharn  Park ,  N J  07932 
{abney,  mco l l ins , s ingha l}@research .a t t . corn  
Abst ract  
Information retrieval systems have typically concen- 
trated on retrieving a set of documents which are rel- 
evant to a user's query. This paper describes a sys- 
tem that attempts to retrieve a much smaller section 
of text, namely, a direct answer to a user's question. 
The SMART IR system is used to extract a ranked 
set of passages that are relevant o the query. En- 
tities are extracted from these passages as potential 
answers to the question, and ranked for plausibility 
according to how well their type matches the query, 
and according to their frequency and position in the 
passages. The system was evaluated at the TREC-8 
question answering track: we give results and error 
analysis on these queries. 
1 Introduction 
In this paper, we describe and evaluate a question- 
answering system based on passage retrieval and 
entity-extraction technology. 
There has long been a concensus in the Informa- 
tion Retrieval (IR) community that natural anguage 
processing has little to offer for retrieval systems. 
Plausibly, this is creditable to the preeminence of ad 
hoc document retrieval as the task of interest in IR. 
However, there is a growing recognition of the lim- 
itations of ad hoc retrieval, both in the sense that 
current systems have reached the limit of achievable 
performance, and in the sense that users' informa- 
tion needs are often not well characterized by docu- 
ment retrieval. 
In many cases, a user has a question with a spe- 
cific answer, such as What city is it where the Euro- 
pean Parliament meets? or Who discovered Pluto? 
In such cases, ranked answers with links to support- 
ing documentation are much more useful than the 
ranked list of documents that standard retrieval en- 
gines produce. 
The ability to answer specific questions also pro- 
vides a foundation for addressing quantitative in- 
quiries such as How many times has the Fed raised 
interest rates this year? which can be interpreted 
as the cardinality of the set of answers to a specific 
question that happens to have multiple correct an- 
swers, like On what date did the Fed raise interest 
rates this year? 
We describe a system that extracts specific an- 
swers from a document collection. The system's per- 
formance was evaluated in the question-answering 
track that has been introduced this year at the 
TREC information-retrieval conference. The major 
points of interest are the following. 
? Comparison of the system's performance to a 
system that uses the same passage retrieval 
component, but no natural language process- 
ing, shows that NLP provides ignificant perfor- 
mance improvements on the question-answering 
task. 
? The system is designed to build on the strengths 
of both IR and NLP technologies. This makes 
for much more robustness than a pure NLP sys- 
tem would have, while affording much greater 
precision than a pure IR system would have. 
? The task is broken into subtasks that admit of 
independent development and evaluation. Pas- 
sage retrieval and entity extraction are both re- 
cognized independent tasks. Other subtasks are 
entity classification and query classification-- 
both being classification tasks that use features 
obtained by parsing--and entity ranking. 
In the following section, we describe the question- 
answering system, and in section 3, we quantify its 
performance and give an error analysis. 
2 The  Quest ion -Answer ing  System 
The system takes a natural-language query as input 
and produces a list of answers ranked in order of 
confidence. The top five answers were submitted to 
the TREC evaluation. 
Queries are processed in two stages. In the infor- 
mation retrieval stage, the most promising passages 
of the most promising documents are retrieved. In 
the linguistic processing stage, potential answers are 
extracted from these passages and ranked. 
The system can be divided into five main compo- 
nents. The information retrieval stage consists of a 
296 
single component, passage retrieval, and the linguis- 
tic processing stage circumscribes four components: 
entity extraction, entity classification, query classi- 
fication, and entity ranking. 
Passage Ret r ieva l  Identify relevant documents, 
and within relevant documents, identify the 
passages most likely to contain the answer to 
the question. 
Ent i ty  Ext ract ion  Extract a candidate set of pos- 
sible answers from the passages. 
Ent i ty  Classification The candidate set is a list of 
entities falling into a number of categories, in- 
cluding people, locations, organizations, quan- 
tities, dates, and linear measures. In some cases 
(dates, quantities, linear measures), entity clas- 
sification is a side effect of entity extraction, 
but in other cases (proper nouns, which may 
be people, locations, or organizations), there is 
a separate classification step after extraction. 
Query  Classi f icat ion Determine what category of 
entity the question is asking for. For example, 
if the query is 
Who is the author of the book, The 
Iron Lady: A Biography of Margaret 
Thatcher? 
the answer should be an entity of type Person. 
Ent i ty  Ranking Assign scores to entities, repre- 
senting roughly belief that the entity is the cor- 
rect answer. There are two components of the 
score. The most-significant bit is whether or 
not the category of the entity (as determined 
by entity classification) matches the category 
that the question is seeking (as determined by 
query classification). A finer-grained ranking is 
imposed on entities with the correct category, 
through the use of frequency and other infor- 
mation. 
The following sections describe these five compo- 
nents in detail. 
2.1 Passage Retrieval 
The first step is to find passages likely to contain the 
answer to the query. We use a modified version of 
the SMART information retrieval system (Buckley 
and Lewit, 1985; Salton, 1971) to recover a set of 
documents which are relevant o the question. We 
define passages as overlapping sets consisting of a 
sentence and its two immediate neighbors. (Pas- 
sages are in one-one correspondence with with sen- 
tences, and adjacent passages have two sentences in 
common.) The score for passage i was calculated as 
1 ?Si-z + ?Si + ~'S,+1 (1) 
where Sj, the score for sentence j, is the sum of IDF 
weights of non-stop terms that it shares with the 
query, plus an additional bonus for pairs of words 
(bigrams) that the sentence and query have in com- 
mon. 
The top 50 passages are passed on as input to 
linguistic processing. 
2.2 Ent i ty  Ext ract ion  
Entity extraction is done using the Cass partial pars- 
er (Abney, 1996). From the Cass output, we take 
dates, durations, linear measures, and quantities. 
In addition, we constructed specialized code for 
extracting proper names. The proper-name extrac- 
tor essentially classifies capitalized words as intrinsi- 
cally capitalized or not, where the alternatives to in- 
trinsic capitalization are sentence-initial capitaliza- 
tion or capitalization in titles and headings. The 
extractor uses various heuristics, including whether 
the words under consideration appear unambiguous- 
ly capitalized elsewhere in the document. 
2.3 Ent i ty  Classif ication 
The following types of entities were extracted as po- 
tential answers to queries. 
Person, Locat ion,  Organization, Other 
Proper names were classified into these cate- 
gories using a classifier built using the method 
described in (Collins and Singer, 1999). 1 This 
is the only place where entity classification was 
actually done as a separate step from entity 
extraction. 
Dates Four-digit numbers starting with 1 . . .  or 
20. .  were taken to be years. Cass was used to 
extract more complex date expressions ( uch as 
Saturday, January 1st, 2000). 
Quant i t ies  Quantities include bare numbers and 
numeric expressions' like The Three Stooges, 4 
1//2 quarts, 27~o. The head word of complex nu- 
meric expressions was identified (stooges, quarts 
or percent); these entities could then be later 
identified as good answers to How many ques- 
tions such as How many stooges were there ?
Durat ions,  Linear Measures  Durations and lin- 
ear measures are essentially special cases of 
quantities, in which the head word is a time 
unit or a unit of linear measure. Examples of 
durations are three years, 6 1/2 hours. Exam- 
ples of linear measures are 140 million miles, 
about 12 feet. 
We should note that this list does not exhaust he 
space of useful categories. Monetary amounts (e.g., 
~The classifier makes a three way distinction between 
Person, Location and Organization; names where the classi- 
fier makes no decision were classified as Other Named E~tity. 
297 
$25 million) were added to the system shortly after 
the Trec run, but other gaps in coverage remain. We 
discuss this further in section 3. 
2.4 Query  Classif ication 
This step involves processing the query to identify 
the category of answer the user is seeking. We parse 
the query, then use the following rules to determine 
the category of the desired answer: 
? Who, Whom -+ Person. 
? Where, Whence, Whither--+ Locat ion.  
? When -+ Date. 
? How few, great, little, many, much -+ 
Quemtity. We also extract the head word of 
the How expression (e.g., stooges in how many 
stooges) for later comparison to the head word 
of candidate answers. 
? How long --+ Duration or Linear Measure. 
How tall, wide, high, big, far --+ Linear 
Measure. 
? The wh-words Which or What typically appear 
with a head noun that describes the category 
of entity involved. These questions fall into two 
formats: What  X where X is the noun involved, 
and What  is the ... X. Here are a couple of 
examples: 
What  company is the largest Japanese 
ship builder? 
What  is the largest city in Germany? 
For these queries the head noun (e.g., compa- 
ny or city) is extracted, and a lexicon map- 
ping nouns to categories is used to identify the 
category of the query. The lexicon was partly 
hand-built (including some common cases such 
as number --+ Quant i ty  or year --~ Date). A 
large list of nouns indicating Person, Locat ion  
or Organ izat ion  categories was automatical- 
ly taken from the contextual (appositive) cues 
learned in the named entity classifier described 
in (Collins and Singer, 1999). 
? In queries containing no wh-word (e.g., Name 
the largest city in Germany), the first noun 
phrase that is an immediate constituent of the 
matrix sentence is extracted, and its head is 
used to determine query category, as for What 
X questions. 
? Otherwise, the category is the wildcard Any. 
2.5 Ent i ty  Rank ing  
Entity scores have two components. The first, most- 
significant, component is whether or not the entity's 
category matches the query's category. (If the query 
category is Any, all entities match it.) 
In most cases, the matching is boolean: either an 
entity has the correct category or not. However, 
there are a couple of special cases where finer distinc- 
tions are made. If a question is of the Date type, and 
the query contains one of the words day or month, 
then "full" dates are ranked above years. Converse- 
ly, if the query contains the word year, then years are 
ranked above full dates. In How many X questions 
(where X is a noun), quantified phrases whose head 
noun is also X are ranked above bare numbers or 
other quantified phrases: for example, in the query 
How many lives were lost in the Lockerbie air crash, 
entities such as 270 lives or almost 300 lives would 
be ranked above entities such as 200 pumpkins or 
150. 2 
The second component of the entity score is based 
on the frequency and position of occurrences of a 
given entity within the retrieved passages. Each oc- 
currence of an entity in a top-ranked passage counts 
10 points, and each occurrence of an entity in any 
other passage counts 1 point. ("Top-ranked pas- 
sage" means the passage or passages that received 
the maximal score from the passage retrieval compo- 
nent.) This score component is used as a secondary 
sort key, to impose a ranking on entities that are not 
distinguished by the first score component. 
In counting occurrences of entities, it is necessary 
to decide whether or not two occurrences are to- 
kens of the same entity or different entities. To this 
end, we do some normalization of entities. Dates 
are mapped to the format year-month-day: that is, 
last Tuesday, November 9, 1999 and 11/9/99 are 
both mapped to the normal form 1999 Nov 9 before 
frequencies are counted. Person names axe aliased 
based on the final word they contain. For example, 
Jackson and Michael Jackson are both mapped to 
the normal form Jackson. a 
3 Eva luat ion  
3.1 Resul ts  on the TREC-8  Evaluat ion 
The system was evaluated in the TREC-8 question- 
answering track. TREC provided 198 questions as a 
blind test set: systems were required to provide five 
potential answers for each question, ranked in or- 
der of plausibility. The output from each system 
was then scored by hand by evaluators at NIST, 
each answer being marked as either correct or in- 
correct. The system's core on a particular question 
is a function of whether it got a correct answer in the 
five ranked answers, with higher scores for the an- 
swer appearing higher in the ranking. The system 
receives a score of 1, 1/2, 1/3, 1/4, 1/5, or 0, re- 
2perhaps less desirably, people would not be recognized 
as a synonym of lives in this example: 200 people would be 
indistinguishable from 200 pumpkins. 
3This does introduce occasional errors, when two people 
with the same last name appear in retrieved passages. 
298 
System Mean Answer Mean 
Ans Len in Top 5 Score 
Entity 10.5 B 46% 0.356 
Passage 50 50 B 38.9% 0.261 
Passage 250 250 B 68% 0.545 
Figure 1: Results on the TREC-8 Evaluation 
spectively, according as the correct answer is ranked 
1st, 2nd, 3rd, 4th, 5th, or lower in the system out- 
put. The final score for a system is calculated as its 
mean score on the 198 questions. 
The TREC evaluation considered two question- 
answering scenarios: one where answers were lim- 
ited to be less than 250 bytes in length, the other 
where the limit was 50 bytes. The output from the 
passage retrieval component (section 2.1), with some 
trimming of passages to ensure they were less than 
250 bytes, was submitted to the 250 byte scenario. 
The output of the full entity-based system was sub- 
mitted to the 50 byte track. For comparison, we also 
submitted the output of a 50-byte system based on 
IR techniques alone. In this system single-sentence 
passages were retrieved as potential answers, their 
score being calculated using conventional IR meth- 
ods. Some trimming of sentences so that they were 
less than 50 bytes in length was performed. 
Figure 1 shows results on the TREC-8 evaluation. 
The 250-byte passage-based system found a correct 
answer somewhere in the top five answers on 68% of 
the questions, with a final score of 0.545. The 50- 
byte passage-based system found a correct answer 
on 38.9% of all questions, with an average score of 
0.261. The reduction in accuracy when moving from 
the 250-byte limit to the 50-byte limit is expected, 
because much higher precision is required; the 50- 
byte limit allows much less extraneous material to 
be included with the answer. The benefit of the 
including less extraneous material is that the user 
can interpret the output with much less effort. 
Our entity-based system found a correct answer in 
the top five answers on 46% of the questions, with 
a final score of 0.356. The performance is not as 
good as that of the 250-byte passage-based system. 
But when less extraneous material is permitted, the 
entity-based system outperforms the passage-based 
approach. The accuracy of the entity-based sys- 
tem is significantly better than that of the 50-byte 
passage-based system, and it returns virtually no ex- 
traneous material, as reflected in the average answer 
length of only 10.5 bytes. The implication is that 
NLP techniques become increasingly useful when 
short answers are required. 
3.2 Error Analysis of the Ent i ty-Based 
System 
3.2.1 Ranking of Answers 
As a first point, we looked at the performance ofthe 
entity-based system, considering the queries where 
the correct answer was found somewhere in the top 
5 answers (46% of the 198 questions). We found that 
on these questions, the percentage ofanswers ranked 
1, 2, 3, 4, and 5 was 66%, 14%, 11%, 4%, and 4% 
respectively. This distribution is by no means uni- 
form; it is clear that when the answer is somewhere 
in the top five, it is very likely to be ranked 1st or 
2nd. The system's performance is quite bimodah 
it either completely fails to get the answer, or else 
recovers it with a high ranking. 
3.2.2 Accuracy on Different Categories 
Figure 2 shows the distribution of question types 
in the TREC-8 test set ("Percentage of Q's"), and 
the performance ofthe entity-based system by ques- 
tion type ("System Accuracy"). We categorized the 
questions by hand, using the eight categories de- 
scribed in section 2.3, plus two categories that es- 
sentially represent types that were not handled by 
the system at the time of the TREC competition: 
Monetary Amount and Miscellaneous. 
"System Accuracy" means the percentage ofques- 
tions for which the correct answer was in the top five 
returned by the system. There is a sharp division in 
the performance on different question types. The 
categories Person, Location, Date and Quantity 
are handled fairly well, with the correct answer ap- 
pearing in the top five 60% of the time. These four 
categories make up 67% of all questions. In contrast, 
the other question types, accounting for 33% of the 
questions, are handled with only 15% accuracy. 
Unsurprisingly, the Miscellaneous and Other 
Named Ent i ty  categories are problematic; unfortu- 
nately, they are also rather frequent. Figure 3 shows 
some examples of these queries. They include a large 
tail of questions eeking other entity types (moun- 
tain ranges, growth rates, films, etc.) and questions 
whose answer is not even an entity (e.g., "Why did 
David Koresh ask the FBI for a word processor?") 
For reference, figure 4 gives an impression of the 
sorts of questions that the system does well on (cor- 
rect answer in top five). 
3.2.3 Errors by Component 
Finally, we performed an analysis to gauge which 
components represent performance bottlenecks in 
the current system. We examined system logs for 
a 50-question sample, and made a judgment of what 
caused the error, when there was an error. Figure 5 
gives the breakdown. Each question was assigned to 
exactly one line of the table. 
The largest body of errors, accounting for 18% of 
the questions, are those that are due to unhandled 
299 
Question I Rank I Output from System 
Who is the author of the book, The Iron Lady: A Biography of 2 
Margaret Thatcher? 
What is the name of the managing director of Apricot Computer? i 
What country is the biggest producer of tungsten? 
Who was the first Taiwanese President? 
When did Nixon visit China? 
How many calories are there in a Big Mac? 4 
What is the acronym for the rating system for air conditioner effi- 1 
ciency? 
Hugo Young 
Dr Peter Horne 
China 
Taiwanese President Li 
Teng hui 
1972 
562 calories 
EER 
Figure 4: A few TREC questions answered correctly by the system. 
Type Percent 
of Q's 
System 
Accuracy 
Person 28 62.5 
Location 18.5 67.6 
Date 11 45.5 
Quantity 9.5 52.7 
TOTAL 67 60 
Other Named Ent 
Miscellaneous 
Linear Measure 
Monetary Amt 
Organization 
Duration 
14.5 
8.5 
3.5 
3 
2 
1.5 
33 TOTAL 
31 
5.9 
0 
0 
0 
0 
15 
Errors 
Passage retrieval failed 
Answer is not an entity 
Answer of unhandled type: money 
Answer of unhandled type: misc 
Entity extraction failed 
Entity classification failed 
Query classification failed 
Entity ranking failed 
16% 
4% 
10% 
8% 
2% 
4% 
4% 
4% 
Successes 
Answer at Rank 2-5 I 16% 
Answer at Rank 1 I 32% 
TOTAL 
Figure 2: Performance ofthe entity-based system on 
different question types. "System Accuracy" means 
percent of questions for which the correct answer 
was in the top five returned by the system. "Good" 
types are in the upper block, "Bad" types are in the 
lower block. 
What does the Peugeot company manufacture? 
Why did David Koresh ask the FBI for a word 
processor? 
What are the Valdez Principles? 
What was the target rate for M3 growth in 1992? 
What does El Nino mean in spanish? 
Figure 5: Breakdown of questions by error type, in 
particular, by component responsible. Numbers are 
percent of questions in a 50-question sample. 
five, but not at rank one, are almost all due to fail- 
ures of entity ranking) Various factors contributing 
to misrankings are the heavy weighting assigned to 
answers in the top-ranked passage, the failure to ad- 
just frequencies by "complexity" (e.g., it is signifi- 
cant if 22.5 million occurs everal times, but not if 3 
occurs several times), and the failure of the system 
to consider the linguistic context in which entities 
appear. 
Figure 3: Examples of "Other Named Entity" and 
Miscellaneous questions. 
types, of which half are monetary amounts. (Ques- 
tions with non-entity answers account for another 
4%.) Another large block (16%) is due to the pas- 
sage retrieval component: the correct answer was 
not present in the retrieved passages. The linguistic 
components ogether account for the remaining 14% 
of error, spread evenly among them. 
The cases in which the correct answer is in the top 
4 Conc lus ions  and  Future  Work  
We have described a system that handles arbi- 
trary questions, producing a candidate list of an- 
swers ranked by their plausibility. Evaluation on 
the TREC question-answering track showed that the 
correct answer to queries appeared in the top five an- 
swers 46% of the time, with a mean score of 0.356. 
The average length of answers produced by the sys- 
tem was 10.5 bytes. 
4The sole exception was a query misclassification caused 
by a parse failure---miraculously, the correct answer made it 
to rank five despite being of the "wrong" type. 
300 
There are several possible areas for future work. 
There may be potential for improved performance 
through more sophisticated use of NLP techniques. 
In particular, the syntactic ontext in which a par- 
ticular entity appears may provide important infor- 
mation, but it is not currently used by the system. 
Another area of future work is to extend the 
entity-extraction component of the system to han- 
dle arbitrary types (mountain ranges, films etc.). 
The error analysis in section 3.2.2 showed that these 
question types cause particular difficulties for the 
system. 
The system is largely hand-built. It is likely that 
as more features are added a trainable statistical or 
machine learning approach to the problem will be- 
come increasingly desirable. This entails developing 
a training set of question-answer pairs, raising the 
question of how a relatively large corpus of questions 
can be gathered and annotated. 
Re ferences  
Steven Abney. 1996. Partial parsing via finite- 
state cascades. J Natural Language Engineering, 
2(4):337-344, December. 
C. Buckley and A.F. Lewit. 1985. Optimization of 
inverted vector searches. In Proe. Eighth Interna- 
tional ACM SIGIR Conference, pages 97-110. 
Michael Collins and Yoram Singer. 1999. Unsuper- 
vised models for named entity classification. In 
EMNLP. 
G. Salton, editor. 1971. The Smart Retrieval Sys- 
tem - Experiments in Automatic Document Pro- 
cessing. Prentice-Hall, Inc., Englewood Cliffs, NJ. 
301 
Information Extraction from Voicemail Transcripts
Martin Jansche
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
jansche.1@osu.edu
Steven P. Abney
AT&T Labs ? Research
180 Park Avenue
Florham Park, NJ 07932, USA
abney@research.att.com
Abstract
Voicemail is not like email. Even such ba-
sic information as the name of the caller/
sender or a phone number for returning
calls is not represented explicitly and must
be obtained from message transcripts or
other sources. We discuss techniques for
doing this and the challenges these tasks
present.
1 Introduction
When you?re away from the phone and someone
takes a message for you, at the very least you?d ex-
pect to be told who called and whether they left a
number for you to call back. If the same call is
picked up by a voicemail system, even such basic in-
formation like the name of the caller and their phone
number may not be directly available, forcing one to
listen to the entire message1 in the worst case. By
contrast, information about the sender of an email
message has always been explicitly represented in
the message headers, starting with early standard-
ization attempts (Bhushan et al, 1973) and including
the two decade old current standard (Crocker, 1982).
Applications that aim to present voicemail messages
through an email-like interface ? take as an example
the idea of a ?uniform inbox? presentation of email,
voicemail, and other kinds of messages2 ? must deal
with the problem of how to obtain information anal-
ogous to what would be contained in email headers.
1The average message length in the corpus described below
is 36 seconds.
2Similar issues arise with FAX messages, for example.
Here we will discuss one way of addressing this
problem, treating it exclusively as the task of extract-
ing relevant information from voicemail transcripts.
In practice, e.g. in the context of a sophisticated
voicemail front-end (Hirschberg et al, 2001) that is
tightly integrated with an organization-wide voice-
mail system and private branch exchange (PBX), ad-
ditional sources of information may be available: the
voicemail system or the PBX might provide infor-
mation about the originating station of a call, and
speaker identification can be used to match a caller?s
voice against models of known callers (Rosenberg
et al, 2001). Restricting our attention to voicemail
transcripts means that our focus and goals are sim-
ilar to those of Huang et al (2001), but the features
and techniques we use are very different.
While the present task may seem broadly similar
to named entity extraction from broadcast news (Go-
toh and Renals, 2000), it is quite distinct from the
latter: first, we are only interested in a small subset
of the named entities; and second, the structure of
the voicemail transcripts in our corpus is very dif-
ferent from broadcast news and certain aspects of
this structure can be exploited for extracting caller
names.
Huang et al (2001) discuss three approaches:
hand-crafted rules; grammatical inference of subse-
quential transducers; and log-linear classifiers with
bigram and trigram features used as taggers (Ratna-
parkhi, 1996). While the latter are reported to yield
the best overall performance, the hand-crafted rules
resulted in higher recall. Our phone number extrac-
tor is based on a two-phase procedure that employs a
small hand-crafted component to propose candidate
phrases, followed by a classifier that retains the de-
sirable candidates. This allows for more or less inde-
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 320-327.
                         Proceedings of the Conference on Empirical Methods in Natural
pendent optimization of recall and precision, some-
what similar to the PNrule classifier learner (Agar-
wal and Joshi, 2001; Joshi et al, 2001). We shall see
that hand-crafted rules achieve very good recall, just
as Huang et al (2001) had observed, and the prun-
ing phase successfully eliminates most undesirable
candidates without affecting recall too much. Over-
all performance of our method is better than if we
employ a log-linear model with trigram features.
The success of the method proposed here is also
due to the use of a rich set of features for candi-
date classification. For example, the majority of
phone numbers in voicemail messages has either
four, seven, or ten digits, whereas nine digits would
indicate a social security number. In our two-phase
approach it is straightforward for the second-phase
classifier to take the length of a candidate phone
number into account. On the other hand, standard
named entity taggers that use trigram features do not
exploit this information, and doing so would entail
significant changes to the underlying models and pa-
rameter estimation procedures.
The rest of this paper is organized as follows. A
brief overview of the data we used in ?2 is followed
by a discussion of methods for extracting two kinds
of caller information in ?3. Methods for extracting
telephone numbers are discussed in ?4, and ?5 sum-
marizes and concludes.
2 Voicemail Corpus
Development and evaluation was done using a pro-
prietary corpus of almost 10,000 voicemail mes-
sages that had been manually transcribed and
marked up for content. Some more details about
this corpus can be found in (Bacchiani, 2001). The
relevant content labeling is perhaps best illustrated
with an (anonymized) excerpt form a typical mes-
sage transcript:
?greeting? hi Jane ?/greeting? ?caller? this
is Pat Caller ?/caller? I just wanted to I
know you?ve probably seen this or maybe
you already know about it . . . so if you
could give me a call at ?telno? one two
three four five ?/telno? when you get the
message I?d like to chat about it hope
things are well with you ?closing? talk to
you soon ?/closing?
This transcript is representative of a large class of
messages that start out with a short greeting fol-
lowed by a phrase that identifies the caller either
by name as above or by other means (?hi, it?s me?).
A phone number may be mentioned as part of the
caller?s self-identification, or is often mentioned
near the end of the message. It may seem natu-
ral and obvious that voicemail messages should be
structured in this way, and this prototypical struc-
ture can therefore be exploited for purposes of lo-
cating caller information or deciding whether a digit
string constitutes a phone number. The next sections
discuss this in more detail.
The corpus was partitioned into two subsets, with
8120 messages used for development and 1869 for
evaluation. Approximately 5% of all messages are
empty. Empty messages were not discarded from
the evaluation set since they constitute realistic sam-
ples that the information extraction component has
to cope with. The development set contains 7686
non-empty messages.
3 Caller Information
Of the non-empty messages in the development set,
7065 (92%) transcripts contain a marked-up caller
phrase. Of those, 6731 messages mention a name in
the caller phrase. Extracting caller information can
be broken down into two slightly different tasks: we
might want to reproduce the existing caller annota-
tion as closely as possible, producing caller phrases
like ?this is Pat Caller? or ?it?s me?; or we might only
be interested in caller names such as ?Pat Caller? in
our above example. We make use of the fact that
for the overwhelming majority of cases, the caller?s
self-identification occurs somewhere near the begin-
ning of the message.
3.1 Caller Phrases
Most caller phrases tend to start one or two words
into the message. This is because they are typi-
cally preceded by a one-word (?hi?) or two-word
(?hi Jane?) greeting. Figure 1 shows the empiri-
cal distribution of the beginning of the caller phrase
across the 7065 applicable transcripts in the devel-
opment data. As can be seen, more than 97% of
all caller phrases start somewhere between one and
seven words from the beginning of the message,
though in one extreme case the start of the caller
phrase occurred 135 words into the message.
0
10
20
30
40
50
60
70
80
90
100
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
observed max.: 135
entropy: 1.48 bits
density
cumulative
Figure 1: Empirical probability of a caller phrase
starting x words into the message
These observations strongly suggest that when ex-
tracting caller phrases, positional cues should be
taken into account. This is good news, especially
since intrinsic features of the caller phrase may not
be as reliable: a caller phrase is likely to contain
names that are problematic for an automatic speech
recognizer. While this is less of a problem when
evaluating on manual transcriptions, the experience
reported in (Huang et al, 2001) suggests that the
relatively high error rate of speech recognizers may
negatively affect performance of caller name ex-
traction on automatically generated transcripts. We
therefore avoid using anything but a small number
of greetings and commonly occurring words like
?hi?, ?this?, ?is? etc. and a small number of common
first names for extracting caller phrases and use po-
sitional information in addition to word-based fea-
tures.
We locate caller phrases by first identifying their
start position in the message and then predicting
the length of the phrase. The empirical distribu-
tion of caller phrase lengths in the development data
is shown in Figure 2. Most caller phrases are be-
tween two and four words long (?it?s Pat?, ?this is
Pat Caller?) and there are moderately good lexical
indicators that signal the end of a caller phrase (?I?,
?could?, ?please?, etc.). Again, we avoid the use of
names as features and rely on a small set of fea-
tures based on common words, in addition to phrase
length, for predicting the length of the caller phrase.
0
10
20
30
40
50
60
70
80
90
100
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
observed max.: 47
entropy: 3.11 bits
density
cumulative
Figure 2: Empirical probability of a caller phrase
being x words long
We have thus identified two classes of features
that allow us to predict the start of the caller phrase
relative to the beginning of the message, as well as
the end of the caller phrase relative to its start. Since
we are dealing with discrete word indices in both
cases, we treat this as a classification task, rather
than a regression task. A large number of classifier
learners can be used to automatically infer classifiers
for the two subtasks at hand. We chose a decision
tree learner for convenience and note that this choice
does not affect the overall results nearly as much as
modifying our feature inventory.
Since a direct comparison to the log-linear named
entity tagger described in (Huang et al, 2001) (we
refer to this approach as HZP log-linear below) is
not possible due to the use of different corpora and
annotation standards, we applied a similar named
entity tagger based on a log-linear model with tri-
gram features to our data (we refer to this approach
as Col log-linear as the tagger was provided by
Michael Collins). Table 1 summarizes precision (P),
recall (R), and F-measure (F) for three approaches
evaluated on manual transcriptions: row HZP log-
linear repeats the results of the best model from
(Huang et al, 2001); row Col log-linear contains
the results we obtained using a similar named entity
tagger on our own data; and row JA classifiers shows
the performance of the classifier method proposed in
this section.
Like Huang et al (2001), we count a proposed
caller phrase as correct if and only if it matches
the annotation of the evaluation data perfectly. The
numbers could be made to look better by using con-
tainment as the evaluation criterion, i.e., we would
count a proposed phrase as correct if it contained an
actual phrase plus perhaps some additional material.
While this may be more useful in practice (see be-
low), it is not the objective that was maximized dur-
ing training, and so we prefer the stricter criterion
for evaluation on previously annotated transcripts.
P R F
HZP log-linear .89 .80 .84
Col log-linear .83 .78 .81
JA classifiers .73 .68 .71
Table 1: Performance of caller phrase extraction
(manual transcriptions)
While the results for the approach proposed
here appear clearly worse than those reported by
Huang et al (2001), we hasten to point out that this
is most likely not due to any difference in the cor-
pora that were used. This is corroborated by the fact
that we were able to obtain performance much closer
to that of the best, finely tuned log-linear model from
(Huang et al, 2001) by using a generic named entity
tagger that was not adapted in any way to the par-
ticular task at hand. The log-linear taggers employ
n-gram features based on family names and other
particular aspects of the development data that do
not necessarily generalize to other settings, where
the family names of the callers may be different or
may not be transcribed properly. In fact, it seems
rather likely that the log-linear models and the fea-
tures they employ over-fit the training data.
This becomes clearer when one evaluates on un-
seen transcripts produced by an automatic speech
recognizer (ASR),3 as summarized in Table 2. Rows
HZP strict and HZP containment repeat the figures
for the best model from (Huang et al, 2001) when
evaluated on automatic transcriptions. The differ-
ence is that HZP strict uses the strict evaluation cri-
terion described above, whereas HZP containment
uses the weaker criterion of containment, i.e., an
extracted phrase counts as correct if it contains ex-
actly one whole actual phrase. Row JA containment
summarizes the performance of our approach when
3An automatic transcription is the single best word hypoth-
esis of the ASR for a given voicemail message.
evaluated on 101 unseen automatically transcribed
messages. Since we did not have any labeled au-
tomatic transcriptions available to compare with the
predicted caller phrase labels using the strict crite-
rion, we only report results based on the weaker
criterion of containment. In fact, we count caller
phrases as correct as long as they contain the full
name of the caller, since this is the common denom-
inator in the otherwise somewhat heterogeneous la-
beling of our training corpus; more on this issue in
the next section.
P R F
HZP strict .24 .16 .19
HZP containment .73 .41 .52
JA containment .74 .66 .70
Table 2: Performance of caller phrase extraction (au-
tomatic transcriptions)
The difference between the approach in (Huang et
al., 2001) and ours may be partly due to the perfor-
mance of the ASR components: Huang et al (2001)
report a word error rate of ?about 35%?, whereas
we used a recognizer (Bacchiani, 2001) with a word
error rate of only 23%. Still, the reduced perfor-
mance of the HZP model on ASR transcripts com-
pared with manual transcripts is points toward over-
fitting, or reliance on features that do not generalize
to ASR transcripts. Our main approach, on the other
hand, uses classifiers that are extremely knowledge-
poor in comparison with the many features of the
log-linear models for the various named entity tag-
gers, employing no more than a few dozen categori-
cal features.
3.2 Caller Names
Extracting an entire caller phrase like ?this is Pat
Caller? may not be all that relevant in practice: the
prefix ?this is? does not provide much useful infor-
mation, so simply extracting the name of the caller
should suffice. This is more or less a problem with
the annotation standard used for marking up voice-
mail transcripts. We decided to test the effects of
changing that standard post hoc. This was relatively
easy to do, since proper names are capitalized in
the message transcripts. We heuristically identify
caller names as the leftmost longest contiguous sub-
sequence of capitalized words inside a marked-up
caller phrase. This leaves us with 6731 messages
with caller names in our development data.4
As we did for caller phrases, we briefly examine
the distributions of the start position of caller names
(see Figure 3) as well as their lengths (see Figure 4).
Comparing the entropies of the empirical distribu-
tions with the corresponding ones for caller phrases
suggests that we might be dealing with a simpler
extraction task here. The entropy of the empirical
name length distribution is not much more than one
bit, since predicting the length of a caller name is
mostly a question of deciding whether a first name
or full name was mentioned.
0
10
20
30
40
50
60
70
80
90
100
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
observed max.: 138
entropy: 2.20 bits
density
cumulative
Figure 3: Empirical probability of a caller name
starting x words into the message
0
10
20
30
40
50
60
70
80
90
100
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
observed max.: 8
entropy: 1.17 bits
density
cumulative
Figure 4: Empirical probability of a caller name be-
ing x words long
4The vast majority of messages that do not mention a name
as part of their caller phrase employ the caller phrase ?it?s me?,
which would be easy to detect and treat separately.
The performance comparison in Table 3 shows
that we are in fact dealing with a simpler task. No-
tice however that our method has not changed at all.
We still use one classifier to predict the beginning
of the caller name and a second classifier to predict
its length, with the same small set of lexical features
that do not include any names other than a handful
of common first names.
P R F
phrase .730 .684 .706
name .860 .871 .865
Table 3: Caller phrase vs. name extraction (manual
transcriptions)
4 Phone Numbers
The development data contain 5303 marked-up
phone numbers, for an average of almost 0.7 phone
numbers per non-empty message. These phone
numbers fall into the following categories based on
their realization:
? 4472 (84%) consist exclusively of spoken num-
bers
? 679 (13%) consist of spoken numbers and the
words ?area?, ?code?, and ?extension?
? 152 (3%) have additional material, due to cor-
rections, hesitations, fragments, and question-
able markup
Note that phone numbers in the North American
Numbering Plan are either ten or seven digits long,
depending on whether the Numbering Plan Area
code is included or not. Two other frequent lengths
for phone numbers in the development data are four
(for internal lines) and, to a lesser extent, eleven
(when the long distance dialing prefix is included,
as in ?one eight hundred . . . ?).
This allows us to formulate the following baseline
approach: find all maximal substrings consisting of
spoken digits (?zero? through ?nine?) and keep those
of length four, seven, and ten. Simple as it may
seem, this approach (which we call digits below)
performs surprisingly well. Its precision is more
than 78%, partly because in our corpus there do not
occur many seven or ten digit numbers that are not
phone numbers.
Named entity taggers based on conditional mod-
els with trigram features are not particularly suited
for this task. The reason is that trigrams do not pro-
vide enough history to allow the tagger to judge the
length of a proposed phone number: it inserts begin-
ning and end tags without being able to tell how far
apart they are. Data sparseness is another problem,
since we are dealing with 1000 distinct trigrams over
digits alone, so a different event model that replaces
all spoken digits with the same representative token
might be better suited, also because it avoids over-
fitting issues like accidentally learning area codes
and other number patterns that are frequent in the
development data.
However, there is a more serious problem. Even
if the distance between the start and end tags that a
named entity tagger predicts could be taken into ac-
count, this would not help with all spoken renditions
of phone numbers. For example, ?327-1025? could
be read aloud using only six words (?three two seven
ten twenty five?), and might be incorrectly rejected
because it appears to be of a length that is not very
common for phone numbers.
We therefore approach the phone number extrac-
tion task differently, using a two-phase procedure.
In the first phase we use a hand-crafted grammar to
propose candidate phone numbers. This avoids all
of the problems mentioned so far, yet the complex-
ity of the task remains manageable because of the
rather simple structure of most phone numbers in
our development data noted above. The advantage
is that it allows us to simultaneously convert spo-
ken digits and numbers to a numeric representation,
whose length can then be used as an important fea-
ture for deciding whether to keep or throw away a
candidate. Note that such a conversion process is
desirable in any case, since a text-based application
would presumably want to present digit strings like
?327-1025? to a user, rather than ?three two seven
ten twenty five?. This conversion step is not entirely
trivial, though: for example, one might transcribe
the spoken words ?three hundred fourteen ninety
nine? as either ?300-1499? or ?314.99? depending on
whether they are preceded by ?call me back at? vs. ?I
can sell it to you for?, for example. But since we are
only interested in finding phone numbers, the extrac-
tion component can treat all candidates it proposes
as if they were phone numbers.
Adjustments of the hand-crafted grammar were
only made in order to increase recall on the devel-
opment data. The grammar should locate as many
actual phone numbers in the development corpus as
possible, but was free to also propose spurious can-
didates that did not correspond to marked-up phone
numbers. While it has recently been argued that
such separate optimization of recall and precision is
generally desirable for certain learning tasks (Agar-
wal and Joshi, 2001; Joshi et al, 2001), the main
advantage in connection with hand-crafted compo-
nents is simplified development. Since we noted
above that 97% of all phone numbers in our devel-
opment data are expressed fairly straightforwardly
in terms of digits, numbers, and a few other words
particular to the phone number domain, we might
expect to achieve recall figures close to 97% without
doing anything special to deal with the remaining
3% of difficult cases. It was very easy to achieve this
recall figure on the development data, while the ratio
of proposed phone numbers to actual phone numbers
was about 3.2 at worst.5
A second phase is now charged with the task of
weeding through the set of candidates proposed dur-
ing the first phase, retaining those that correspond to
actual phone numbers. This is a simple binary clas-
sification task, and again many different techniques
can be applied. As a baseline we use a classifier
that accepts any candidate of length four or more
(now measured in terms of numeric digits, rather
than spoken words), and rejects candidates of length
three and less. Without this simple step (which we
refer to as prune below), the precision of our hand-
crafted extraction grammar is only around 30%, but
by pruning away candidate phone numbers shorter
than four digits precision almost doubles while re-
call is unaffected.
We again used a decision tree learner to automat-
ically infer a classifier for the second phase. The
features we made available to the learner were the
length of the phone number in numeric digits, its
5It would of course be trivial to achieve 100% recall by ex-
tracting all possible substrings of a transcript. The fact that our
grammar extracts only about three times as many phrases as
needed is evidence that it falls within the reasonable subset of
possible extraction procedures.
distance from the end of the message, and a small
number of lexical cues in the surrounding context of
a candidate number (?call?, ?number?, etc.). This ap-
proach (which we call classify below) increases the
precision of the combined two steps to acceptable
levels without hurting recall too much.
A comparison of performance results is presented
in Table 4. Rows HZP rules and HZP log-linear re-
fer to the rule-based baseline and the best log-linear
model of (Huang et al, 2001) and the figures are
simply taken from that paper; row Col log-linear
refers to the same named entity tagger we used in the
previous section and is included for comparison with
the HZP models; row JA digits refers to the simple
baseline where we extract strings of spoken digits of
plausible lengths. Our main results appear in the re-
maining rows. The performance of our hand-crafted
extraction grammar (in row JA extract) was about
what we had seen on the development data before,
with recall being as high as one could reasonably ex-
pect. As mentioned above, using a simple pruning
step in the second phase (see JA extract + prune)
results in a doubling of precision and leaves recall
essentially unaffected (a single fragmentary phone
number was wrongly excluded). Finally, if we use
a decision tree classifier in the second phase, we
can achieve extremely high precision with a minimal
impact on recall. Our two-phase procedure outper-
forms all other methods we considered.
P R F
HZP rules .81 .83 .82
HZP log-linear .90 .83 .86
Col log-linear .88 .93 .91
JA digits .78 .70 .74
JA extract .30 .96 .45
JA extract + prune .59 .96 .73
JA extract + classify .94 .94 .94
Table 4: Performance of phone number extraction
(manual transcriptions)
We evaluated the performance of our best models
on the same 101 unseen ASR transcripts used above
in the evaluation of the caller phrase extraction. The
results are summarized in Table 5, which also re-
peats the best results from (Huang et al, 2001), us-
ing the same terminology as earlier: rows HZP strict
and HZP containment refer to the best model from
(Huang et al, 2001) ? corresponding to row HZP
log-linear in Table 4 ? when evaluated using the
strict criterion and containment, respectively; and
row JA containment refers to our own best model
? corresponding to row JA extract + classify in Ta-
ble 4.
P R F
HZP strict .56 .52 .54
HZP containment .85 .79 .82
JA containment .95 .94 .95
Table 5: Performance of phone number extraction
(automatic transcriptions)
It is not very plausible that the differences be-
tween the approaches in Table 5 would be due to
a difference in the performance of the ASR compo-
nents that generated the message transcripts. From
inspecting our own data it is clear that ASR mistakes
inside phone numbers are virtually absent, and we
would expect the same to hold even of an automatic
recognizer with an overall much higher word error
rate. Also, for most phone numbers the labeling is
uncontroversial, so we expect the corpora used by
Huang et al (2001) and ourselves to be extremely
similar in terms of mark-up of phone numbers. So
the observed performance difference is most likely
due to the difference in extraction methods.
5 Conclusion and Outlook
The novel contributions of this paper can be summa-
rized as follows:
? We demonstrated empirically that positional
cues can be an important source of information
for locating caller names and phrases.
? We showed that good performance on the task
of extracting caller information can be achieved
using a very small inventory of lexical and po-
sitional features.
? We argued that for extracting telephone num-
bers it is extremely useful to take the length
of their numeric representation into account.
Our grammar-based extractor translates spoken
numbers into such a numeric representation.
? Our two-phase approach allows us to efficiently
develop a simple extraction grammar for which
the only requirement is high recall. This places
less of a burden on the grammar developers
than having to write an accurate set of rules like
the baseline of (Huang et al, 2001).
? The combined performance of our simple ex-
traction grammar and the second-phase clas-
sifier exceeded the performance of all other
methods, including the current state of the art
(Huang et al, 2001).
Our results point towards approaches that use a
small inventory of features that have been tailored
to specific tasks. Generic methods like the named
entity tagger used by Huang et al (2001) may not
be the best tools for particular tasks; in fact, we do
not expect the bigram and trigram features used by
such taggers to be sufficient for accurately extract-
ing phone numbers. We also believe that using all
available lexical information for extracting caller in-
formation can easily lead to over-fitting, which can
partly be avoid by not relying on names being tran-
scribed correctly by an ASR component.
In practice, determining the identity of a caller
might have to take many diverse sources of infor-
mation into account. The self-identification of a
caller and the phone numbers mentioned in the same
message are not uncorrelated, since there is usually
only a small number of ways to reach any particular
caller. In an application we might therefore try to use
a combination of speaker identification (Rosenberg
et al, 2001), caller name extraction, and recognized
phone numbers to establish the identity of the caller.
An investigation of how to combine these sources of
information is left for future research.
Acknowledgements
We would like to thank Michiel Bacchiani, Michael
Collins, Julia Hirschberg, and the SCANMail group
at AT&T Labs. Special thanks to Michiel Bacchiani
for help with ASR transcripts and to Michael Collins
for letting us use his named entity tagger.
References
Ramesh C. Agarwal and Mahesh V. Joshi. 2001. PNrule:
A new classification framework in data mining (A case
study in network intrusion detection). In First SIAM
International Conference on Data Mining, Chicago,
IL.
Michiel Bacchiani. 2001. Automatic transcription of
voicemail at AT&T. In International Conference on
Acoustics, Speech, and Signal Processing, Salt Lake
City, UT.
Abhay Bhushan, Ken Pogran, Ray Tomlinson, and Jim
White. 1973. Standardizing network mail headers.
Internet RFC 561.
David H. Crocker. 1982. Standard for the format of
ARPA internet text messages. Internet RFC 822,
STD 11.
Yoshihiko Gotoh and Steve Renals. 2000. Informa-
tion extraction from broadcast news. Philosophical
Transactions of the Royal Society of London, Series
A, 358:1295?1310.
Julia Hirschberg, Michiel Bacchiani, Don Hindle, Phil
Isenhour, Aaron Rosenberg, Litza Stark, Larry Stead,
Steve Whittaker, and Gary Zamchick. 2001. SCAN-
Mail: Browsing and searching speech data by content.
In 7th European Conference on Speech Communica-
tion and Technology, Aalborg, Denmark.
Jing Huang, Geoffrey Zweig, and Mukund Padmanab-
han. 2001. Information extraction from voicemail. In
39th Annual Meeting of the Association for Computa-
tional Linguistics, Toulouse, France.
Mahesh V. Joshi, Ramesh C. Agarwal, and Vipin Ku-
mar. 2001. Mining needles in a haystack: Classify-
ing rare classes via two-phase rule induction. In ACM
SIGMOD International Conference on Management of
Data, Santa Barbara, CA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Empirical Methods in
Natural Language Processing, Philadelphia, PA.
Aaron Rosenberg, Julia Hirschberg, Michiel Bacchiani,
S. Parthasarathy, Philip Isenhour, and Larry Stead.
2001. Caller identification for the SCANMail voice-
mail browser. In 7th European Conference on Speech
Communication and Technology, Aalborg, Denmark.
Automatically Inducing a Part-of-Speech Tagger
by Projecting from Multiple Source Languages
Across Aligned Corpora
Victoria Fossum1 and Steven Abney2
1 Dept. of EECS, University of Michigan, Ann Arbor MI 48105
vfossum@umich.edu
2 Dept. of Linguistics, University of Michigan, Ann Arbor MI 48105
abney@umich.edu
Abstract. We implement a variant of the algorithm described by
Yarowsky and Ngai in [21] to induce an HMM POS tagger for an ar-
bitrary target language using only an existing POS tagger for a source
language and an unannotated parallel corpus between the source and tar-
get languages. We extend this work by projecting from multiple source
languages onto a single target language. We hypothesize that systematic
transfer errors from differing source languages will cancel out, improving
the quality of bootstrapped resources in the target language. Our exper-
iments confirm the hypothesis. Each experiment compares three cases:
(a) source data comes from a single language A, (b) source data comes
from a single language B, and (c) source data comes from both A and B,
but half as much from each. Apart from the source language, other condi-
tions are held constant in all three cases ? including the total amount of
source data used. The null hypothesis is that performance in the mixed
case would be an average of performance in the single-language cases,
but in fact, mixed-case performance always exceeds the maximum of
the single-language cases. We observed this effect in all six experiments
we ran, involving three different source-language pairs and two different
target languages.
1 Introduction
1.1 Background
Statistical NLP techniques typically require large amounts of annotated data.
Labelling data by hand is time-consuming; a natural goal is therefore to generate
text analysis tools automatically, using minimal resources. Yarowsky et al [22]
present methods for automatically inducing various monolingual text analysis
tools for an arbitrary target language, using only the corresponding text analysis
tool for a source language and a parallel corpus between the source and target
languages. Hwa et al [15] induce a parser for Chinese text via projection from
English using a similar method to that of [22]. Cucerzan and Yarowsky [8] present
a method for bootstrapping a POS tagger for an arbitrary target language using
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 862?873, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Automatically Inducing a Part-of-Speech Tagger 863
only a bilingual dictionary between the source and target languages, a ?basic
library reference grammar? for the target language, and an existing corpus in
the target language.
While automatically induced text analysis tools use fewer resources, their
accuracy lags behind that of more resource-intensive tools. One solution to the
problem of error reduction on NLP tasks is to train multiple classifiers, then
compute a consensus classifier. Combining multiple classifiers is an effective way
to reduce error if the errors made by each classifier are independently distributed.
Such approaches have been successfully applied to a range of NLP tasks. Brill
and Wu [4], van Halteren et al [19], and Zavrel and Daelemans [23] investigate
various methods for improving the performance of statistical POS taggers by
combining multiple such taggers. Henderson and Brill [14] combine the Char-
niak, Collins, and Ratnaparkhi parsers to achieve an accuracy surpassing the
best previous results on the WSJ. Gollins and Sanderson [10] apply projec-
tion via multiple source languages to reduce error in cross-linguistic information
retrieval.
1.2 Motivation
We hypothesize that a large component of the error rate in the automatically in-
duced text analysis tools generated by [22] is due to morphosyntactic differences
between the source and target languages that are specific to each source-target
language pair. Therefore, training POS taggers on additional source languages
should result in multiple classifiers which produce independently distributed er-
rors on the target language.
Previous research in classifier combination for POS tagging has focused pri-
marily on combining various statistical classifiers trained on data in the same
language. Thus, our approach is novel in its exploitation of differences across
languages, rather than differences across statistical methods, to improve per-
formance on POS tagging. Our method is general in that it does not rely on
language-specific information, and requires no annotated resources in the target
language.
Our method is easily extensible to new languages. While it requires a parallel
corpus between each source language and the target language, the corpora used
to train each single-source tagger need not be translations of the same text.
Furthermore, our algorithm is applicable even to target languages belonging to
distinct language families from those of the source languages.
1.3 Task Overview
Using existing POS taggers for English, German, and Spanish, we generate
single-source taggers for Czech and French via projection across parallel trans-
lations of the Bible. To obtain a theoretical upper bound on the performance
improvement that is possible by combining multiple POS taggers, we measure
the complementarity between each pair of single-source taggers. We examine
864 V. Fossum and S. Abney
various ways to combine the output of these single-source taggers into a consen-
sus tagger, and measure the resulting performance improvement.
2 Methods
2.1 Single-Source POS Tagger Induction
We implement a variant of the algorithm described in [21] for constructing a
single-source bigram-based HMM POS tagger for a target language. First, we
identify a language (the ?source language?) for which a POS tagger exists, and
a sentence-aligned parallel corpus consisting of text in the source language and
its translation in the target language. We then align the parallel corpora at
the word-level using GIZA++ [1]. Next, we annotate the source text using an
existing POS tagger. Finally, we project these annotations across the parallel
text from the source text to the target text, smooth these projections, and use
the projected annotations to train an HMM POS tagger for the target language.
In more detail, we implement the following procedure, based on [21]:
1. Obtain a sentence-aligned parallel corpus in the source and target languages
(see Section 2.4).
2. Align the parallel corpus at the word-level using GIZA++1.
English: He(1) likes(2) cats(4).
French: Il(1) aime(2) les(3) chats(4).
3. Tag the source portion of the parallel corpus using an existing POS tagger for
the source language2. We use the Brill tagger3 for English [3], the TNT tagger4
for German, and the SVMTool tagger5 for Spanish.
English: He/PRP likes/VBP cats/NNS.
Since the POS tagger for each source language uses its own distinct tagset, we
convert the output of each tagger to a ?generic? tagset for comparison purposes.
Additionally, we label each POS tag as belonging to one of several more general
?core? tagset categories (see Table 1).
4. Using the mapping induced by the word-level alignments, project the POS
tags from the source language onto the target language.
French: Il/PRP aime/VB les/NULL chats/NNS.
1 GIZA++ is a component of EGYPT, an open-source implementation of IBM?s sta-
tistical machine translation system [1].
2 For all existing POS taggers, we use the default models provided with the tagger
for training in the source language. For taggers with variable parameter settings, we
use the default settings for all parameters.
3 A transformation-based tagger [3].
4 A bigram-based Markov tagger[2].
5 An SVM-based tagger [9].
Automatically Inducing a Part-of-Speech Tagger 865
Note that tag projection is complicated by the occurrence of many-to-one word
alignments from source to target. To handle such cases, we compute two esti-
mates of tag probabilities, P (ti|wi): one using only 1-to-1 alignments, and the
other using 1-to-n alignments. We then linearly combine the two estimators.
5. Before computing the P (wi|ti) model, several steps must be taken to smooth
the initial, noisy tag projections. First, P (wi|ti) can be decomposed as follows:
P (wi|ti) =
P (ti|wi) ? P (wi)
P (ti)
To smooth P (ti|wi), the simplifying assumption is made that in most natural
languages, each word has at most two possible POS tags at the core tagset
granularity. We count the relative frequency of each tag that is assigned to that
French word by the tag projection from English, then discard all but the two most
frequently assigned core tags. We then recursively smooth the tag probabilities
in favor of the two most probable subtags for each of the core tags, where the
subtags are members of the more finely grained ?generic? tagset. We compute
P (ti) and P (wi) using corpus frequency.
6. We estimate the probability of unknown words using the probability of words
appearing only once in the training corpus. We replace all words occurring only
once in the training corpus by the ?UNK? token.
7. Before computing the P (tj |ti) model, we filter the training data to remove
those sentence pairs whose alignment score (as determined by GIZA++) falls
into the lowest 25% of alignment scores. To estimate the probability of unknown
state transitions, we perform Witten-Bell smoothing [20] on P (tj |ti) to assign
non-zero probabilities to state transitions not seen in the training data.
8. The resulting model defines an HMM bigram-based tagger in the target lan-
guage. We use the Viterbi algorithm to determine the most likely sequence of
tags given a sentence in the target language [17].
2.2 Multiple-Source POS Tagger Induction
To compute a multiple-source consensus tagger, we train n single-source taggers
using n parallel texts, each pairing one of the source languages with the target
language. We then apply each single-source tagger to the test sentences. For
each word in the test sentences, we record the probability distribution Pi(t|w)
over possible tags that the ith single-source tagger produces. We then compute
two consensus taggers, Majority Tag and Linear Combination, by combining the
output from each of the n taggers, P1(t|w) . . . Pn(t|w) as follows:
Majority Tag: Each tagger outputs the most likely tag
tbesti = argmax
t
(Pi(t|w))
for w. We select the tag from tbest1 , . . . , t
best
n that receives the greatest number
of votes from single-source taggers. To break ties, we select the tag chosen with
the highest probability by the taggers that selected it.
866 V. Fossum and S. Abney
Linear Combination: Each tagger outputs a vector of probabilities over pos-
sible tags t given w. We take a linear combination of these vectors to compute
Plinear(T |w), then select the tag tlinear with the highest probability.
Plinear(T |w) =
n
?
i=1
ki ? (Pi(T |w))
tlinear = argmax
t
(Plinear(t|w))
In our experiments, we set ki = 1n , so we effectively average the probability
distributions of each tagger over possible tags t for w.
2.3 Tagsets
Two tagsets of different granularities are used in the experiments: the coarse-
grained ?core? and fine-grained ?generic? tagsets (see Table 1). While it can be
difficult to map fine-grained POS tags from one language directly onto another
another because of morphological differences between languages, languages tend
to agree on tags at a coarse-grained level.
2.4 Data Sets
We use two corpora in our experiments: the Bible (with translations in English,
Czech, French, German, and Spanish), and the Hansards parallel corpus of Cana-
dian parliamentary proceedings (with translations in English and French). For
Table 1. Generic and Core Tagsets
POS Generic Core
Noun NN N
Proper Noun NNP N
Verb, Inf. VB V
Verb, Present VBP V
Verb, Present Part. VBG V
Verb, Past Part. VBN V
Verb, Past VBD V
Determiner DT D
Wh-Determiner WDT D
Conjunction CC C
Number CD NUM
Adverb RB R
Wh-Adverb WRB R
Adjective JJ J
Pronoun PRP P
Preposition IN I
Automatically Inducing a Part-of-Speech Tagger 867
the Bible experiments, we use the entire 31,100-line text: training data con-
sists of either one 31,000-line excerpt or two 15,500-line excerpts, while testing
data consists of a held-out 100-sentence excerpt. For the Hansards experiments,
training data consists of a 85,000-line excerpt; testing data consists of a held-out
100-sentence excerpt.
We perform the following pre-processing steps. Each text is filtered to remove
punctuation and converted to lower case; accents are preserved. The English,
French, German, and Spanish texts are tokenized to expand elisions.6
3 Results
We report percent agreement with the correct tags, determined by compari-
son with the output of the Treetag tagger7 for French, and a hybrid rule-
based/HMM-based tagger8 for Czech. For French, agreement with the correctly
tagged text is measured on the generic and core tagsets. For Czech, agreement
is measured on the core tagset only, since this is the POS tagset provided by
the tagger we use for evaluation purposes. All experiments use 5-fold cross-
validation. For each iteration, the parallel corpus is divided randomly into train-
ing and testing sets. The accuracy of each single-source tagger is limited by the
accuracy of the tagger used to tag the source training text; the accuracy of the
evaluation of each tagger?s performance on French and Czech text is limited by
the accuracy of the reference tagger against which it is compared (Table 2).
Table 2. Reported Accuracy of Existing POS Taggers used to Train Single-Source
Taggers
Tagger Language % Accuracy F-measure Test Corpus
Brill English 96.6% ?- Penn Treebank (English)
TNT German ?- ?- ?-
SVMTool Spanish 96.89% ?- LEXESP (Spanish)
TreeTag French 96.36% ?- Penn Treebank (English)
Rules + HMM Czech ?- 95.38% PDT (Czech)
3.1 Single-Source
For each single-source tagger, we train on 31,000 lines of the parallel Bible be-
tween the source and target languages and test on 100 held-out lines of the Bible
in the target language. We report the accuracy of the induced taggers on French
(Tables 5 and 4) and Czech (Table 6).
6 e.g. ?doesn?t? ? ?does not?, ?qu?il? ? ?que il?, ?zum? ? ?zu dem?, and ?del? ?
?de el?. This tokenization represents the only step of our algorithm that requires
additional language-specific knowledge beyond the resources already given.
7 A decision-tree-based tagger [18].
8 [13].
868 V. Fossum and S. Abney
To compare our baseline single-source tagger performance against that of [21],
we conduct the following experiment, after the experimental procedure used by
[21]. We train a single-source English-projected tagger for French on a 2,000,000-
word (approximately 85,000-line) excerpt of the French-English Hansards cor-
pus and test it on a 100-line excerpt of the same corpus. We obtain accuracies
of 86.5% and 91.1% on the generic and core tagsets, respectively; [21] report
accuracies of 91% and 94% on the ?English Equivalent? and ?core? tagsets,
respectively.9
3.2 Multiple-Source
Complementarity: We compute the pairwise complementarity of each pair of
single-source taggers. Brill and Wu [4] define the complementarity of a pair of
taggers i and j as the percentage of cases when tagger i is wrong that tagger j
is correct (See Table 3):
Comp(i, j) = (1 ? errorsi ? errorsj
errorsi
) ? 100
Table 3. Complementarity (row,col) of Single-Source Taggers
French Bible Czech Bible
Generic Tagset Core Tagset Core Tagset
Source English German Spanish English German Spanish English German Spanish
English 0 38.95 32.87 0 32.75 37.13 0 22.08 18.71
German 42.40 0 44.93 30.49 0 38.95 15.47 0 17.31
Spanish 41.12 48.83 0 35.64 39.51 0 19.95 24.98 0
PairwiseCombination: To determine whether tagger performance improves by
using training data from two different source languages, without increasing the to-
tal amount of training data, we perform the following experiments. For each possi-
ble combination of two single-source taggers,wepartition theBible into two15,500-
line training sets (the first, a parallel corpus between one source language and the
target language; the second, a parallel corpus between the other source language
and the target language), and a 100-line held-out testing set. We train the first
single-source tagger on one half, train the second single-source tagger on the sec-
ond half, combine their output using the methods described in Section 2.2, and test
the resulting consensus tagger on a held-out 100-line excerpt of the French (Tables
4 and 5) or Czech (Table 6) Bibles. For each pairwise combination of taggers, we
report the percent error reduction of the combined tagger in comparison to the
average accuracy of the constituent single-source taggers.
9 Our ?generic? and ?core? tagsets correspond approximately to the ?English Equiva-
lent? and ?core? tagsets used by [21]. Since we do not have access to the same testing
set used by [21], we report results on a held-out excerpt of the Hansards corpus.
Automatically Inducing a Part-of-Speech Tagger 869
Table 4. % Accuracy of Single-Source, Pairwise-Combined, and n-way Combined Tag-
gers Using Generic Tagset on French Bible
Sources % Accuracy % Error Rate Reduction
Linear Majority
English 81.95 81.95 ?
German 81.21 81.21 ?
Spanish 79.76 79.76 ?
Eng. + Ger. 84.52 84.30 15.96
Eng. + Span. 84.42 84.48 18.91
Ger. + Span. 83.89 84.09 18.45
E. + G. + S. 85.80 85.61 25.38
Table 5. % Accuracy of Single-Source, Pairwise-Combined, and n-way Combined Tag-
gers Using Core Tagset on French Bible
Sources % Accuracy % Error Rate Reduction
Linear Majority
English 85.67 85.67 ?
German 86.66 86.66 ?
Spanish 86.54 86.54 ?
Eng. + Ger. 88.06 88.05 13.67
Eng. + Span. 88.13 88.12 14.54
Ger. + Span. 89.12 89.19 19.33
E. + G. + S. 89.87 89.43 26.11
n-Way Combination: To examine how much tagger performance can be im-
proved by increasing the total amount of training data n-fold and training each
of n single-source taggers on the full 31,000 lines of the Bible, then computing
a consensus tagger, we perform the following experiment. We train each single-
source tagger on 31,000 lines of the Bible, then compute the consensus output of
all 3 single-source taggers on a held-out 100-line excerpt of the French (Tables
4 and 5) or Czech (Table 6) Bibles. For each n-way combination of taggers, we
report the percent error reduction of the combined tagger in comparison to the
average accuracy of the constituent single-source taggers.
4 Discussion
All multiple-source taggers outperform the corresponding single-source taggers?
thus, incorporating multiple source languages improves performance, even when
the total amount of training data is held constant (as in the pairwise combination
experiments).
4.1 Single-Source Taggers
We expect performance to be highest for those source-target language pairs that
are most similar to each other, linguistically. At the generic tagset level, the
870 V. Fossum and S. Abney
Table 6. % Accuracy of Single-Source, Pairwise-Combined, and n-way Combined Tag-
gers Using Core Tagset on Czech Bible
Sources % Accuracy % Error Rate Reduction
Linear Majority
English 62.53 62.53 ?
German 65.27 65.27 ?
Spanish 63.27 63.27 ?
Eng. + Ger. 65.44 65.98 5.76
Eng. + Span. 65.41 65.28 6.77
Ger. + Span. 67.18 67.75 9.74
E. + G. + S. 67.13 67.36 10.12
poor performance of the Spanish-projected single-source tagger on French text
is partially due to a discrepancy between the SVMTool tagset [9] and our generic
tagset10. At the core tagset level, the distinction between verb tenses becomes
irrelevant, and the performance of the Spanish-projected tagger matches that of
the other single-source taggers more closely on French data; still, its performance
is lower than expected given the close morphosyntactic correspondence between
Spanish and French.11
For several reasons, we expect single-source tagger performance to be poorer
on Czech (Table 6) than on French (Tables 5 and 4). First, Czech is a ?highly
inflected? language: the role of function words in the Germanic and Romance
languages is typically filled by suffixes in Czech. Second, Czech exhibits a ?rela-
tively free word order? [7]. Since a great deal of the POS information exploited
by an HMM tagger is contained in sequences of function words12, these features
of Czech hinder the performance of an HMM POS tagger.13 Finally, Czech be-
longs to the Slavic language family, and is therefore further removed than French
from the Germanic and Romance families of the source languages used to train
the single-source taggers.
Although our single-source taggers do not replicate the performance results
reported by [21] (91% and 94% accuracy on generic and core tagsets, respec-
tively), our primary concern is not their absolute performance but rather their
10 e.g., SVMTool [9] does not make certain distinctions in verb tense that we make in
our generic tagset.
11 One likely explanation for this discrepancy is that we do not optimize the parameters
of the Spanish POS tagger used to annotate the source corpus to suit the input format
of our data set, but instead use the default settings. We estimate that optimizing
these parameters to match our data set could result in an increase of 1-2% accuracy in
the Spanish-projected source tagger for French and Czech; however, such an increase
in performance of one of the baseline experiments would not change our conclusion
in a significant way.
12 e.g., a ?DT? is likely to be followed by a ?NN? in English.
13 The Czech tagger we use for reference [13] combines a rule-based morphological
analyzer with an HMM POS tagger to combat these problems; our induced HMM
POS taggers, lacking any morphological analysis component, may not exploit the
correct type of information for such languages.
Automatically Inducing a Part-of-Speech Tagger 871
performance relative to the multiple-source taggers. We think it plausible that
the improvements we observe would also be observed with Yarowsky?s single-
source taggers, but it remains an open question.
4.2 Multiple-Source Taggers
Complementarity. Pairwise complementarity among single-source taggers is
relatively high on French at both tagset granularities (Table 3). The low pairwise
complementarity of taggers on Czech may indicate the existence of a ceiling on
the performance of the single-source tagger induction algorithm, imposed by the
limited degree of similarity between any of the source languages with the target
language. Even under such circumstances, we still see improvement (though
diminished) by combining single-source taggers for Czech.
One factor whose influence upon tagger complementarity must be acknowl-
edged is the diversity of the statistical models underlying each of the POS taggers
used to tag the source portion of the training text. Since we use a different type
of tagger to tag each source language, we cannot separate the component of
complementarity that is caused by the difference in statistical models among
sources from the component caused by the difference in languages.
Pairwise Combination. All pairwise combined taggers outperform the cor-
responding single-source taggers, though the total amount of training data is
unchanged. We observe this improvement on both French and Czech. This sug-
gests that our approach is likely to improve performance over single-source tag-
gers on a wide range of target languages, and does not depend upon a close
correspondence between any of the source and target languages.
n-Way Combination. As expected (given the n-fold increase in training data),
all n-way combined taggers outperform the corresponding single-source taggers,
suggesting that when parallel training data between a particular source-language
pair is limited, the performance of a POS tagger projected across that language
pair can be improved by the use of a parallel corpus between the target language
and a different source language.
5 Conclusion
Projection from multiple source languages significantly improves the perfor-
mance of automatically induced POS taggers on a target language. We observe
performance gains from incorporating multiple source taggers even when the to-
tal amount of training data is held constant, indicating that multiple languages
provide sources of information whose errors are independent and randomly dis-
tributed to a large extent. The approach presented here is general in that it does
not depend on any language-specific resources in the target language beyond
parallel corpora. Our results suggest that the performance of text analysis tools
induced using parallel corpora can benefit from the incorporation of resources
in other languages, even in the case of source languages belonging to distinct
linguistic families from the target language.
872 V. Fossum and S. Abney
6 Future Work
To further improve the accuracy of induced multiple-source taggers, we plan to in-
vestigate othermethods for combining the output of single-sourcePOS taggers.We
hypothesize that combining the models constructed by each tagger before applying
each tagger to the testing set would result in greater performance gains.
References
1. Yasser Al-Onaizan , Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan
Melamed, Franz-Josef Och, David Purdy, Noah Smith and David Yarowsky: Sta-
tistical machine translation. Johns Hopkins University 1999 Summer Workshop on
Language Engineering (1999)
2. Thorsten Brants: TnT ? a statistical part-of-speech tagger. In Proceedings of the
6th Applied NLP Conference, ANLP-2000, April 29 ? May 3, 2000, Seattle, WA.
(2000)
3. Eric Brill: Transformation-Based Error-Driven Learning and Natural Language
Processing: A Case Study in Part-of-Speech Tagging. Computational Linguistics
Vol. 21 No. 4 (1995) 543-565
4. Eric Brill and Jun Wu: Classifier Combination for Improving Lexical Disambigua-
tion. Proceedings of the ACL (1998)
5. Peter F. Brown, John Cocke, Stephen Della Pietra, Vincent J. Della Pietra, Freder-
ick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin: A Statistical
Approach to Machine Translation. Computational Linguistics Vol. 16 No. 2 (1990)
79?85
6. S. Clark, J. Curran, and M. Osborne: Bootstrapping POS taggers using unlabelled
data. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-
2003, Edmonton, Canada (2003) 49?55
7. Michael Collins, Jan Hajic, Lance Ramshaw, and Christoph Tillmann: A Statistical
Parser for Czech. Proceedings of the 37th Annual Meeting of the ACL, College
Park, Maryland (1999)
8. Silviu Cucerzan and David Yarowsky: Bootstrapping a Multilingual Part-of-speech
Tagger in One Person-day. Proceedings of the Sixth Conference on Natural Lan-
guage Learning (CoNLL) (2002)
9. Jesus Gimenez and Lluis Marquez: SVMTool: A general POS tagger generator
based on Support Vector Machines. Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation (LREC?04), Lisbon, Portugal (2004)
10. Tim Gollins and Mark Sanderson: Improving Cross Language Information Retrieval
with Triangulated Translation. Proceedings of the 24th annual international ACM
SIGIR conference 90?95 (2001)
11. French-English Hansards Corpus of Canadian Parliamentary Proceedings.
12. Jan Hajic and Barbora Hladka: Tagging Inflective Languages: Prediction of Mor-
phological Categories for a Rich, Structured Tagset, COLING-ACL (1998) 483?490
13. Jan Hajic, Pavel Krbec, Pavel Kevton, Karel Oliva, and Vladimir Petkevic: Serial
Combination of Rules and Statistics: A Case Study in Czech Tagging. Proceedings
of the ACL (2001)
14. John C. Henderson and Eric Brill: Exploiting Diversity in Natural Language Pro-
cessing: Combining Parsers. Proceedings of the 1999 Joint SIGDAT Conference
on Empirical Methods in Natural Language Processing and Very Large Corpora
(1999) 187?194
Automatically Inducing a Part-of-Speech Tagger 873
15. Rebecca Hwa, Philip Resnik, and Amy Weinberg: Breaking the Resource Bottle-
neck for Multilingual Parsing. Proceedings of the Workshop on Linguistic Knowl-
edge Acquisition and Representation: Bootstrapping Annotated Language Data
(2002)
16. Gideon Mann and David Yarowsky: Multipath translation lexicon induction via
bridge languages. In Proceedings of NAACL 2001: 2nd Meeting of the North Amer-
ican Chapter of the Association for Computational Linguistics (2001) 151?158
17. Lawrence Rabiner: A tutorial on hidden Markov models and selected applications
in speech recognition. Proceedings of the IEEE Vol. 77 No. 2 (1989)
18. Helmut Schmid: Probabilistic Part-of-Speech Tagging Using Decision Trees. Inter-
national Conference on New Methods in Language Processing, Manchester, UK.
(1994)
19. Hans van Halteren, Jakub Zavrel, and Walter Daelemans: Improving Data Driven
Wordclass Tagging by System Combination. Proceedings of the Thirty-Sixth An-
nual Meeting of the Association for Computational Linguistics (1998) 491?497
20. Ian Witten and Timothy Bell: The zero-frequency problem: Estimating the prob-
abilities of novel events in adaptive text compression. IEEE Transactions in Infor-
mation Theory, Vol. 37 No. 4 1085?1094 (1991)
21. David Yarowsky and Grace Ngai: Inducing Multilingual POS Taggers and NP
Bracketers via Robust Projection Across Aligned Corpora. Proceedings of NAACL
(2001) 200?207
22. David Yarowsky, Grace Ngai, and Richard Wicentowski: Inducing Multilingual
Text Analysis Tools via Robust Projection across Aligned Corpora. Proceedings of
HLT (2001)
23. Jakub Zavrel and Walter Daelemans: Bootstrapping a Tagged Corpus through
Combination of Existing Heterogeneous Taggers. Proceedings of LREC-2000,
Athens (2000)
c? 2004 Association for Computational Linguistics
Understanding the Yarowsky Algorithm
Steven Abney?
University of Michigan
Many problems in computational linguistics are well suited for bootstrapping (semisupervised
learning) techniques. The Yarowsky algorithm is a well-known bootstrapping algorithm, but
it is not mathematically well understood. This article analyzes it as optimizing an objective
function. More specifically, a number of variants of the Yarowsky algorithm (though not the
original algorithm itself) are shown to optimize either likelihood or a closely related objective
function K.
1. Introduction
Bootstrapping, or semisupervised learning, has become an important topic in com-
putational linguistics. For many language-processing tasks, there are an abundance
of unlabeled data, but labeled data are lacking and too expensive to create in large
quantities, making bootstrapping techniques desirable.
The Yarowsky (1995) algorithm was one of the first bootstrapping algorithms to be-
come widely known in computational linguistics. In brief, it consists of two loops. The
?inner loop? or base learner is a supervised learning algorithm. Specifically, Yarowsky
uses a simple decision list learner that considers rules of the form ?If instance x con-
tains feature f , then predict label j? and selects those rules whose precision on the
training data is highest.
The ?outer loop? is given a seed set of rules to start with. In each iteration, it uses
the current set of rules to assign labels to unlabeled data. It selects those instances
regarding which the base learner?s predictions are most confident and constructs a
labeled training set from them. It then calls the inner loop to construct a new classifier
(that is, a new set of rules), and the cycle repeats.
An alternative algorithm, co-training (Blum and Mitchell 1998), has subsequently
become more popular, perhaps in part because it has proven amenable to theoretical
analysis (Dasgupta, Littman, and McAllester 2001), in contrast to the Yarowsky al-
gorithm, which is as yet mathematically poorly understood. The current article aims
to rectify this lack of understanding, increasing the attractiveness of the Yarowsky
algorithm as an alternative to co-training. The Yarowsky algorithm does have the ad-
vantage of placing less of a restriction on the data sets it can be applied to. Co-training
requires data attributes to be separable into two views that are conditionally indepen-
dent given the target label; the Yarowsky algorithm makes no such assumption about
its data.
In previous work, I did propose an assumption about the data called precision
independence, under which the Yarowsky algorithm could be shown effective (Ab-
ney 2002). That assumption is ultimately unsatisfactory, however, not only because it
? 4080 Frieze Bldg., 105 S. State Street, Ann Arbor, MI 48109-1285. E-mail: abney.umich.edu.
Submission received: 26 August 2003; Revised submission received: 21 December 2003; Accepted for
publication: 10 February 2004
366
Computational Linguistics Volume 30, Number 3
Table 1
The Yarowsky algorithm variants. Y-1/DL-EM reduces H; the others
reduce K.
Y-1/DL-EM-? EM inner loop that uses labeled examples only
Y-1/DL-EM-X EM inner loop that uses all examples
Y-1/DL-1-R Near-original Yarowsky inner loop, no smoothing
Y-1/DL-1-VS Near-original Yarowsky inner loop, ?variable smoothing?
YS-P Sequential update, ?antismoothing?
YS-R Sequential update, no smoothing
YS-FS Sequential update, original Yarowsky smoothing
restricts the data sets on which the algorithm can be shown effective, but also for ad-
ditional internal reasons. A detailed discussion would take us too far afield here, but
suffice it to say that precision independence is a property that it would be preferable
not to assume, but rather to derive from more basic properties of a data set, and that
closer empirical study shows that precision independence fails to be satisfied in some
data sets on which the Yarowsky algorithm is effective.
This article proposes a different approach. Instead of making assumptions about
the data, it views the Yarowsky algorithm as optimizing an objective function. We will
show that several variants of the algorithm (though not the algorithm in precisely its
original form) optimize either negative log likelihood H or an alternative objective
function, K, that imposes an upper bound on H.
Ideally, we would like to show that the Yarowsky algorithm minimizes H. Un-
fortunately, we are not able to do so. But we are able to show that a variant of the
Yarowsky algorithm, which we call Y-1/DL-EM, decreases H in each iteration. It com-
bines the outer loop of the Yarowsky algorithm with a different inner loop based on
the expectation-maximization (EM) algorithm.
A second proposed variant of the Yarowsky algorithm, Y-1/DL-1, has the advan-
tage that its inner loop is very similar to the original Yarowsky inner loop, unlike
Y-1/DL-EM, whose inner loop bears little resemblance to the original. Y-1/DL-1 has
the disadvantage that it does not directly reduce H, but we show that it does reduce
the alternative objective function K.
We also consider a third variant, YS. It differs from Y-1/DL-EM and Y-1/DL-1
in that it updates sequentially (adding a single rule in each iteration), rather than in
parallel (updating all rules in each iteration). Besides having the intrinsic interest of
sequential update, YS can be proven effective when using exactly the same smoothing
method as used in the original Yarowsky algorithm, in contrast to Y-1/DL-1, which
uses either no smoothing or a nonstandard ?variable smoothing.? YS is proven to
decrease K.
The Yarowsky algorithm variants that we consider are summarized in Table 1. To
the extent that these variants capture the essence of the original algorithm, we have
a better formal understanding of its effectiveness. Even if the variants are deemed to
depart substantially from the original algorithm, we have at least obtained a family
of new bootstrapping algorithms that are mathematically understood.
2. The Generic Yarowsky Algorithm
2.1 The Original Algorithm Y-0
The original Yarowsky algorithm, which we refer to as Y-0, is given in table 2. It is
an iterative algorithm. One begins with a seed set ?(0) of labeled examples and a
367
Abney Understanding the Yarowsky Algorithm
Table 2
The generic Yarowsky algorithm (Y-0)
(1) Given: examples X, and initial labeling Y(0)
(2) For t ? {0, 1, . . .}
(2.1) Train classifier on labeled examples (?(t), Y(t)), where ?(t) = {x ? X|Y(t) = ?}
The resulting classifier predicts label j for example x with probability ?(t+1)x (j)
(2.2) For each example x ? X:
(2.2.1) Set y? = arg maxj ?
(t+1)
x (j)
(2.2.2) Set Y(t+1)x =
?
?
?
Y(0)x if x ? ?(0)
y? if ?(t+1)x (y?) > ?
? otherwise
(2.3) If Y(t+1) = Y(t), stop
set V(0) of unlabeled examples. At each iteration, a classifier is constructed from the
labeled examples; then the classifier is applied to the unlabeled examples to create a
new labeled set.
To discuss the algorithm formally, we require some notation. We assume first a set
of examples X and a feature set Fx for each x ? X. The set of examples with feature f
is Xf . Note that x ? Xf if and only if f ? Fx.
We also require a series of labelings Y(t), where t represents the iteration number.
We write Y(t)x for the label of example x under labeling Y(t). An unlabeled example is
one for which Y(t)x is undefined, in which case we write Y
(t)
x = ?. We write V(t) for
the set of unlabeled examples and ?(t) for the set of labeled examples. It will also be
useful to have a notation for the set of examples with label j:
?(t)j ? {x ? X|Y
(t)
x = j = ?}
Note that ?(t) is the disjoint union of the sets ?(t)j . When t is clear from context, we
drop the superscript (t) and write simply ?j, V, Yx, etc.
At the risk of ambiguity, we will also sometimes write ?f for the set of labeled
examples with feature f , trusting to the index to discriminate between ?f (labeled
examples with feature f ) and ?j (labeled examples with label j). We always use f and
g to represent features and j and k to represent labels. The reader may wish to refer
to Table 3, which summarizes notation used throughout the article.
In each iteration, the Yarowsky algorithm uses a supervised learner to train a clas-
sifier on the labeled examples. Let us call this supervised learner the base learning
algorithm; it is a function from (X, Y(t)) to a classifier ? drawn from a space of classi-
fiers ?. It is assumed that the classifier makes confidence-weighted predictions. That
is, the classifier defines a scoring function ?(x, j), and the predicted label for example
x is
y? ? arg max
j
?(x, j) (1)
Ties are broken arbitrarily. Technically, we assume a fixed order over labels and define
the maximization as returning the first label in the ordering, in case of a tie.
It will be convenient to assume that the scoring function is nonnegative and
bounded, in which case we can normalize it to make ?(x, j) a conditional distribution
over labels j for a given example x. Henceforward, we write ?x(j) instead of ?(x, j),
368
Computational Linguistics Volume 30, Number 3
Table 3
Summary of notation.
X set of examples, both labeled and unlabeled
Y the current labeling; Y(t) is the labeling at iteration t
? the (current) set of labeled examples
V the (current) set of unlabeled examples
x an example index
f , g feature indices
j, k label indices
Fx the features of example x
Yx the label of example x; value is undefined (?) if x is unlabeled
Xf , ?f , Vf examples, labeled examples, unlabeled examples that have feature f
?j, ?fj examples with label j, examples with feature f and label j
m the number of features of a given example: |Fx| (cf. equation (12))
L the number of labels
?x(j) labeling distribution (equation (5))
?x(j) prediction distribution (equation (12); except for DL-0, which uses equation (11))
?fj score for rule f ? j; we view ?f as the prediction distribution of f
y? label that maximizes ?x(j) for given x (equation (1)
[[?]] truth value of ?: value is 0 or 1
H objective function, negative log-likelihood (equation (6))
H(p) entropy of distribution p
H(p||q) cross entropy: ?
?
x p(x) log q(x) (cf. equations (2) and (3))
K objective function, upper bound on H (equation (20))
qf (j) precision of rule f ? j (equation (9))
q?f (j) smoothed precision (equation (10))
q?f (j) ?peaked? precision (equation (25))
j? the label that maximizes precision qf (j) for a given feature f (equation (26))
j? the label that maximizes rule score ?fj for a given feature f (equation (28))
u(?) uniform distribution
understanding ?x to be a probability distribution over labels j. We call this distribution
the prediction distribution of the classifier on example x.
To complete an iteration of the Yarowsky algorithm, one recomputes labels for
examples. Specifically, the label y? is assigned to example x if the score ?x(y?) exceeds a
threshold ?, called the labeling threshold. The new labeled set ?(t+1) contains all ex-
amples for which ?x(y?) > ?. Relabeling applies only to examples in V(0). The labels for
examples in ?(0) are indelible, because ?(0) constitutes the original manually labeled
data, as opposed to data that have been labeled by the learning algorithm itself.
The algorithm continues until convergence. The particular base learning algorithm
that Yarowsky uses is deterministic, in the sense that the classifier induced is a deter-
ministic function of the labeled data. Hence, the algorithm is known to have converged
at whatever point the labeling remains unchanged.
Note that the algorithm as stated leaves the base learning algorithm unspecified.
We can distinguish between the generic Yarowsky algorithm Y-0, for which the base
learning algorithm is an open parameter, and the specific Yarowsky algorithm, which
includes a specification of the base learner. Informally, we call the generic algorithm
the outer loop and the base learner the inner loop of the specific Yarowsky algorithm.
The base learner that Yarowsky assumes is a decision list induction algorithm. We
postpone discussion of it until Section 3.
369
Abney Understanding the Yarowsky Algorithm
2.2 An Objective Function
Machine learning algorithms are typically designed to optimize some objective func-
tion that represents a formal measure of performance. The maximum-likelihood crite-
rion is the most commonly used objective function. Suppose we have a set of examples
?, with labels Yx for x ? ?, and a parametric family of models ?? such that ?(j|x; ?)
represents the probability of assigning label j to example x, according to the model.
The likelihood of ? is the probability of the full data set according to the model, viewed
as a function of ?, and the maximum-likelihood criterion instructs us to choose the
parameter settings ?? that maximize likelihood, or equivalently, log-likelihood:
l(?) = log
?
x??
?(Yx|x; ?)
=
?
x??
log?(Yx|x; ?)
=
?
x??
?
j
[[j = Yx]] log?(j|x; ?)
(The notation [[?]] represents the truth value of the proposition ?; it is one if ? is true
and zero otherwise.)
Let us define
?x(j) = [[j = Yx]] for x ? ?
Note that ?x satisfies the formal requirements of a probability distribution over labels
j: Specifically, it is a point distribution with all its mass concentrated on Yx. We call it
the labeling distribution. Now we can write
l(?) =
?
x??
?
j
?x(j) log?(j|x; ?)
= ?
?
x??
H(?x||?x) (2)
In (2) we have written ?x for the distribution ?(?|x; ?), leaving the dependence on ?
implicit. We have also used the nonstandard notation H(p||q) for what is sometimes
called cross entropy. It is easy to verify that
H(p||q) = H(p) + D(p||q) (3)
where H(p) is the entropy of p and D is Kullback-Leibler divergence. Note that when
p is a point distribution, H(p) = 0 and hence H(p||q) = D(p||q). In particular:
l(?) = ?
?
x??
D(?x||?x) (4)
Thus when, as here, ?x is a point distribution, we can restate the maximum-likelihood
criterion as instructing us to choose the model that minimizes the total divergence
between the empirical labeling distributions ?x and the model?s prediction distribu-
tions ?x.
To extend l(?) to unlabeled examples, we need only observe that unlabeled exam-
ples are ones about whose labels the data provide no information. Accordingly, we
370
Computational Linguistics Volume 30, Number 3
revise the definition of ?x to treat unlabeled examples as ones whose labeling distribu-
tion is the maximally uncertain distribution, which is to say, the uniform distribution:
?x(j) =
{
[[j = Yx]] for x ? ?
1
L for x ? V
(5)
where L is the number of labels. Equivalently:
?x(j) = [[x ? ?j]] + [[x ? V]]
1
L
When we replace ? with X, expressions (2) and (4) are no longer equivalent; we
must use (2). Since H(?x||?x) = H(?x) + D(?x||?x), and H(?x) is minimized when x is
labeled, minimizing H(?x||?x) forces one to label unlabeled examples. On labeled ex-
amples, H(?x||?x) = D(?x||?x), and D(?x||?x) is minimized when the labels of examples
agree with the predictions of the model.
In short, we adopt as objective function
H ?
?
x?X
H(?x||?x) = ?l(?, ?) (6)
We seek to minimize H.
2.3 The Modified Algorithm Y-1
We can show that a modified version of the Yarowsky algorithm finds a local minimum
of H. Two modifications are necessary:
? The labeling function Y is recomputed in each iteration as before, but
with the constraint that an example once labeled stays labeled. The label
may change, but a labeled example cannot become unlabeled again.
? We eliminate the threshold ? or (equivalently) fix it at 1/L. As a result,
the only examples that remain unlabeled after the labeling step are those
for which ?x is the uniform distribution. The problem with an arbitrary
threshold is that it prevents the algorithm from converging to a
minimum of H. A threshold that gradually decreases to 1/L would also
address the problem but would complicate the analysis.
The modified algorithm, Y-1, is given in Table 4.
To obtain a proof, it will be necessary to make an assumption about the supervised
classifier ?(t+1) induced by the base learner in step 2.1 of the algorithm. A natural as-
sumption is that the base learner chooses ?(t+1) so as to minimize
?
x??(t) D(?
(t)
x ||?(t+1)x ).
A weaker assumption will suffice, however. We assume that the base learner reduces
divergence, if possible. That is, we assume
?D? ?
?
x??(t)
D(?(t)x ||?(t+1)x )?
?
x??(t)
D(?(t)x ||?(t)x ) ? 0 (7)
with equality only if there is no classifier ?(t+1) ? ? that makes ?D? < 0. Note
that any learning algorithm that minimizes
?
x??(t) D(?
(t)
x ||?(t+1)x ) satisfies the weaker
assumption (7), inasmuch as the option of setting ?(t+1)x = ?
(t)
x is always available.
371
Abney Understanding the Yarowsky Algorithm
Table 4
The modified generic Yarowsky algorithm (Y-1).
(1) Given: X, Y(0)
(2) For t ? {0, 1, . . .}
(2.1) Train classifier on (?(t), Y(t)); result is ?(t+1)
(2.2) For each example x ? X:
(2.2.1) Set y? = arg maxj ?
(t+1)
x (j)
(2.2.2) Set Y(t+1)x =
?
?
?
Y(0)x if x ? ?(0)
y? if x ? ?(t) ? ?(t+1)x (y?) > 1/L
? otherwise
(2.3) If Y(t+1) = Y(t), stop
We also consider a somewhat stronger assumption, namely, that the base learner
reduces divergence over all examples, not just over labeled examples:
?DX ?
?
x?X
D(?(t)x ||?(t+1)x )?
?
x?X
D(?(t)x ||?(t)x ) ? 0 (8)
If a base learning algorithm satisfies (8), the proof of theorem 1 is shorter; but (7) is
the more natural condition for a base learner to satisfy.
We can now state the main theorem of this section.
Theorem 1
If the base learning algorithm satisfies (7) or (8), algorithm Y-1 decreases H at each
iteration until it reaches a critical point of H.
We require the following lemma in order to prove the theorem:
Lemma 1
For all distributions p
H(p) ? log 1
maxj p(j)
with equality iff p is the uniform distribution.
Proof
By definition, for all k:
p(k) ? max
j
p(j)
log
1
p(k)
? log 1
maxj p(j)
Since this is true for all k, it is true if we take the expectation with respect to p:
?
k
p(k) log
1
p(k)
?
?
k
p(k) log
1
maxj p(j)
H(p) ? log 1
maxj p(j)
372
Computational Linguistics Volume 30, Number 3
We have equality only if p(k) = maxj p(j) for all k, that is, only if p is the uniform
distribution.
We now prove the theorem.
Proof of Theorem 1
The algorithm produces a sequence of labelings ?(0),?(1), . . . and a sequence of clas-
sifiers ?(1),?(2), . . . . The classifier ?(t+1) is trained on ?(t), and the labeling ?(t+1) is
created using ?(t+1).
Recall that
H =
?
x?X
[
H(?x) + D(?x||?x)
]
In the training step (2.1) of the algorithm, we hold ? fixed and change ?, and in the
labeling step (2.2), we hold ? fixed and change ?. We will show that the training step
minimizes H as a function of ?, and the labeling step minimizes H as a function of ?
except in examples in which it is at a critical point of H. Hence, H is nonincreasing in
each iteration of the algorithm and is strictly decreasing unless (?(t),?(t)) is a critical
point of H.
Let us consider the labeling step first. In this step, ? is held constant, but ? (pos-
sibly) changes, and we have
?H =
?
x?X
?H(x)
where
?H(x) ? H(?(t+1)x ||?(t+1)x )? H(?(t)x ||?(t+1)x )
We can show that ?H is nonpositive if we can show that ?H(x) is nonpositive for all
x.
We can guarantee that ?H(x) ? 0 if ?(t+1) minimizes H(p||?(t+1)x ) viewed as a
function of p. By definition:
H(p||?(t+1)x ) =
?
j
pj log
1
?(t+1)x (j)
We wish to find the distribution p that minimizes H(p||?(t+1)x ). Clearly, we accomplish
that by placing all the mass of p in pj?, where j? minimizes ? log?(t+1)x (j). If there
is more than one minimizer, H(p||?(t+1)x ) is minimized by any distribution p that dis-
tributes all its mass among the minimizers of ? log?(t+1)x (j). Observe further that
arg min
j
log
1
?(t+1)x (j)
= arg max
j
?(t+1)x (j)
= y?
That is, we minimize H(p||?(t+1)x ) by setting pj = [[j = y?]], which is to say, by labeling
x as predicted by ?(t+1). That is how algorithm Y-1 defines ?(t+1)x for all examples
x ? ?(t+1) whose labels are modifiable (that is, excluding x ? ?(0)).
Note that ?(t+1)x does not minimize H(p||?(t+1)x ) for examples x ? V(t+1), that is, for
examples x that remain unlabeled at t + 1. However, in algorithm Y-1, any example
that is unlabeled at t + 1 is necessarily also unlabeled at t, so for any such example,
373
Abney Understanding the Yarowsky Algorithm
?H(x) = 0. Hence, if any label changes in the labeling step, H decreases, and if no
label changes, H remains unchanged; in either case, H does not increase.
We can show further that even for examples x ? V(t+1), the labeling distribution
?(t+1)x assigned by Y-1 represents a critical point of H. For any example x ? V(t+1),
the prediction distribution ?(t+1)x is the uniform distribution (otherwise Y-1 would
have labeled x). Hence the divergence between ?(t+1) and ?(t+1) is zero, and thus at
a minimum. It would be possible to decrease H(?(t+1)x ||?(t+1)x ) by decreasing H(?(t+1)x )
at the cost of an increase in D(?(t+1)x ||?(t+1)x ), but all directions of motion (all ways of
selecting labels to receive increased probability mass) are equally good. That is to say,
the gradient of H is zero; we are at a critical point.
Essentially, we have reached a saddle point. We have minimized H with respect
to ?x(j) along those dimensions with a nonzero gradient. Along the remaining dimen-
sions, we are actually at a local maximum, but without a gradient to choose a direction
of descent.
Now let us consider the algorithm?s training step (2.1). In this step, ? is held
constant, so the change in H is equal to the change in D?recall that H(?||?) = H(?)+
D(?||?). By the hypothesis of the theorem, there are two cases: The base learner satisfies
either (7) or (8). If it satisfies (8), the base learner minimizes D as a function of ?, hence
it follows immediately that it minimizes H as a function of ?.
Suppose instead that the base learner satisfies (7). We can express H as
H =
?
x?X
H(?x) +
?
x??(t)
D(?x||?x) +
?
x?V(t)
D(?x||?x)
In the training step, the first term remains constant. The second term decreases, by
hypothesis. But the third term may increase. However, we can show that any increase
in the third term is more than offset in the labeling step.
Consider an arbitrary example x in V(t). Since it is unlabeled at time t, we know
that ?(t)x is the uniform distribution u:
u(j) =
1
L
Moreover, ?(t)x must also be the uniform distribution; otherwise example x would
have been labeled in a previous iteration. Therefore the value of H(x) = H(?x||?x) at
the beginning of iteration t is H0:
H0 =
?
j
?(t)x (j) log
1
?(t)x (j)
=
?
j
u(j) log
1
u(j)
= H(u)
After the training step, the value is H1:
H1 =
?
j
?(t)x (j) log
1
?(t+1)x (j)
If ?x remains unchanged in the training step, then the new distribution ?
(t+1)
x , like the
old one, is the uniform distribution, and the example remains unlabeled. Hence there
is no change in H, and in particular, H is nonincreasing, as desired. On the other hand,
if ?x does change, then the new distribution ?
(t+1)
x is nonuniform, and the example is
374
Computational Linguistics Volume 30, Number 3
labeled in the labeling step. Hence the value of H(x) at the end of the iteration, after
the labeling step, is H2:
H2 =
?
j
?(t+1)x (j) log
1
?(t+1)x (j)
= log
1
?(t+1)x (y?)
By Lemma 1, H2 < H(u); hence H2 < H0.
As we observed above, H1 > H0, but if we consider the change overall, we find
that the increase in the training step is more than offset in the labeling step:
?H(x) = H2 ? H1 + H1 ? H0 < 0
3. The Specific Yarowsky Algorithm
3.1 The Original Decision List Induction Algorithm DL-0
When one speaks of the Yarowsky algorithm, one often has in mind not just the generic
algorithm Y-0 (or Y-1), but an algorithm whose specification includes the particular
choice of base learning algorithm made by Yarowsky. Specifically, Yarowsky?s base
learner constructs a decision list, that is, a list of rules of form f ? j, where f is a
feature and j is a label, with score ?fj. A rule f ? j matches example x if x possesses
the feature f . The label predicted for a given example x is the label of the highest
scoring rule that matches x.
Yarowsky uses smoothed precision for rule scoring. As the name suggests,
smoothed precision q?f (j) is a smoothed version of (raw) precision qf (j), which is the
probability that rule f ? j is correct given that it matches
qf (j) ?
{
|?fj|/|?f | if |?f | > 0
1/L otherwise
(9)
where ?f is the set of labeled examples that possess feature f , and ?fj is the set of
labeled examples with feature f and label j.
Smoothed precision q?(j|f ; 	) is defined as follows:
q?(j|f ; 	) ?
|?fj|+ 	
|?f |+ L	
(10)
We also write q?f (j) when 	 is clear from context.
Yarowsky defines a rule?s score to be its smoothed precision:
?fj = q?f (j)
Anticipating later needs, we will also consider raw precision as an alternative: ?fj =
qf (j). Both raw and smoothed precision have the properties of a conditional probability
distribution. Generally, we view ?fj as a conditional distribution over labels j for a fixed
feature f .
Yarowsky defines the confidence of the decision list to be the score of the highest-
scoring rule that matches the instance being classified. This is equivalent to defining
?x(j) ? max
f?Fx
?fj (11)
(Recall that Fx is the set of features of x.) Since the classifier?s prediction for x is
defined, in equation (1), to be the label that maximizes ?x(j), definition (11) implies
375
Abney Understanding the Yarowsky Algorithm
Table 5
The decision list induction algorithm DL-0. The value accumulated in N[f , j] is |?fj|, and the
value accumulated in Z[f ] is |?f |.
(0) Given: a fixed value for  > 0
Initialize arrays N[f , j] = 0, Z[f ] = 0 for all f , j
(1) For each example x ? ?
(1.1) Let j be the label of x
(1.2) Increment N[f , j], Z[f ], for each feature f of x
(2) For each feature f and label j
(2.1) Set ?fj =
N[f ,j]+
Z[f ]+L
(*) Define ?x(j) ? maxf?Fx ?fj
that the classifier?s prediction is the label of the highest-scoring rule matching x, as
desired.
We have written ? in (11) rather than = because maximizing ?fj across f ? Fx for
each label j will not in general yield a probability distribution over labels?though the
scores will be positive and bounded, and hence normalizable. Considering only the
final predicted label y? for a given example x, the normalization will have no effect,
inasmuch as all scores ?fj being compared will be scaled in the same way.
As characterized by Yarowsky, a decision list contains only those rules f ? j whose
score q?f (j) exceeds the labeling threshold ?. This can be seen purely as an efficiency
measure. Including rules whose score falls below the labeling threshold will have no
effect on the classifier?s predictions, as the threshold will be applied when the classifier
is applied to examples. For this reason, we do not prune the list. That is, we represent
a decision list as a set of parameters {?fj}, one for every possible rule f ? j in the cross
product of the set of features and the set of labels.
The decision list induction algorithm used by Yarowsky is summarized in Table 5;
we refer to it as DL-0. Note that the step labeled (*) is not actually a step of the
induction algorithm but rather specifies how the decision list is used to compute a
prediction distribution ?x for a given example x.
Unfortunately, we cannot prove anything about DL-0 as it stands. In particular,
we are unable to show that DL-0 reduces divergence between prediction and labeling
distributions (7). In the next section, we describe an alternative decision list induc-
tion algorithm, DL-EM, that does satisfy (7); hence we can apply Theorem 1 to the
combination Y-1/DL-EM to show that it reduces H. However, a disadvantage of DL-
EM is that it does not resemble the algorithm DL-0 used by Yarowsky. We return in
section 3.4 to a close variant of DL-0 called DL-1 and show that though it does not
directly reduce H, it does reduce the upper bound K.
3.2 The Decision List Induction Algorithm DL-EM
The algorithm DL-EM is a special case of the EM algorithm. We consider two versions
of the algorithm: DL-EM-? and DL-EM-X. They differ in that DL-EM-? is trained on
labeled examples only, whereas DL-EM-X is trained on both labeled and unlabeled
examples. However, the basic outline of the algorithm is the same for both.
First, the DL-EM algorithms do not assume Yarowsky?s definition of ?, given in
(11). As discussed above, the parameters ?fj can be thought of as defining a prediction
distribution ?f (j) over labels j for each feature f . Hence equation (11) specifies how the
prediction distributions ?f for the features of example x are to be combined to yield a
376
Computational Linguistics Volume 30, Number 3
prediction distribution ?x for x. Instead of combining distributions by maximizing ?fj
across f ? Fx as in equation (11), DL-EM takes a mixture of the ?f :
?x(j) =
1
m
?
f?Fx
?fj (12)
Here m = |Fx| is the number of features that x possesses; for the sake of simplicity, we
assume that all examples have the same number of features. Since ?f is a probability
distribution for each f , and since any convex combination of distributions is also a
distribution, it follows that ?x as defined in (12) is a probability distribution.
The two definitions for ?x(j), (11) and (12), will often have the same mode y?, but
that is guaranteed only in the rather severely restricted case of two features and two
labels. Under definition (11), the prediction is determined entirely by the strongest
?f , whereas definition (12) permits a bloc of weaker ?f to outvote the strongest one.
Yarowsky explicitly wished to avoid the possibility of such interactions. Nonetheless,
definition (12), used by DL-EM, turns out to make analysis of other base learners more
manageable, and we will assume it henceforth, not only for DL-EM, but also for the
algorithms DL-1 and YS discussed in subsequent sections.
DL-EM also differs from DL-0 in that DL-EM does not construct a classifier ?from
scratch? but rather seeks to improve on a previous classifier. In the context of the
Yarowsky algorithm, the previous classifier is the one from the previous iteration of
the outer loop. We write ?oldfj for the parameters and ?
old
x for the prediction distributions
of the previous classifier.
Conceptually, DL-EM considers the label j assigned to an example x to be gen-
erated by choosing a feature f ? Fx and then assigning the label j according to the
feature?s prediction distribution ?f (j). The choice of feature f is a hidden variable. The
degree to which an example labeled j is imputed to feature f is determined by the old
distribution:
?old(f |x, j) =
[[f ? Fx]]?oldfj
?
g [[g ? Fx]]?oldgj
=
[[f ? Fx]] 1m?oldfj
?oldx (j)
One can think of ?old(f |x, j) either as the posterior probability that feature f was re-
sponsible for the label j, or as the portion of the labeled example (x, j) that is imputed
to feature f . We also write ?oldxj (f ) as a synonym for ?
old(f |x, j). The new estimate ?fj is
obtained by summing imputed occurrences of (f , j) and normalizing across labels. For
DL-EM-?, this takes the form
?fj =
?
x??j ?
old(f |x, j)
?
k
?
x??k ?
old(f |x, k)
The algorithm is summarized in Table 6.
The second version of the algorithm, DL-EM-X, is summarized in Table 7. It is like
DL-EM-?, except that it uses the update rule
?fj =
?
x??j ?
old(f |x, j) + 1L
?
x?V ?
old(f |x, j)
?
k
[
?
x??k ?
old(f |x, k) + 1L
?
x?V ?
old(f |x, k)
] (13)
Update rule (13) includes unlabeled examples as well as labeled examples. Concep-
tually, it divides each unlabeled example equally among the labels, then divides the
resulting fractional labeled example among the example?s features.
377
Abney Understanding the Yarowsky Algorithm
Table 6
DL-EM-? decision list induction algorithm.
(0) Initialize N[f , j] = 0 for all f , j
(1) For each example x labeled j
(1.1) Let Z =
?
g?Fx ?
old
gj
(1.2) For each f ? Fx, increment N[f , j] by 1Z?
old
fj
(2) For each feature f
(2.1) Let Z =
?
j N[f , j]
(2.2) For each label j, set ?fj = 1Z N[f , j]
Table 7
DL-EM-X decision list induction algorithm.
(0) Initialize N[f , j] = 0 and U[f , j] = 0, for all f , j
(1) For each example x labeled j
(1.1) Let Z =
?
g?Fx ?
old
gj
(1.2) For each f ? Fx, increment N[f , j] by 1Z?
old
fj
(2) For each unlabeled example x
(2.1) Let Z =
?
g?Fx ?
old
gj
(2.2) For each f ? Fx, increment U[f , j] by 1Z?
old
fj
(3) For each feature f
(3.1) Let Z =
?
j(N[f , j] +
1
L U[f , j])
(3.2) For each label j, set ?fj = 1Z
(
N[f , j] + 1L U[f , j]
)
We note that both variants of the DL-EM algorithm constitute a single iteration
of an EM-like algorithm. A single iteration suffices to prove the following theorem,
though multiple iterations would also be effective:
Theorem 2
The classifier produced by the DL-EM-? algorithm satisfies equation (7), and the clas-
sifier produced by the DL-EM-X algorithm satisfies equation (8).
Combining Theorems 1 and 2 yields the following corollary:
Corollary
The Yarowsky algorithm Y-1, using DL-EM-? or DL-EM-X as its base learning algo-
rithm, decreases H at each iteration until it reaches a critical point of H.
Proof of Theorem 2
Let ?old represent the parameter values at the beginning of the call to DL-EM, let ?
represent a family of free variables that we will optimize, and let ?old and ? be the
corresponding prediction distributions. The labeling distribution ? is fixed. For any
set of examples ?, let ?D? be the change in
?
x?? D(?x||?x) resulting from the change
in ?. We are obviously particularly interested in two cases: that in which ? is the set
of all examples X (for DL-EM-X) and that in which ? is the set of labeled examples
378
Computational Linguistics Volume 30, Number 3
? (for DL-EM-?). In either case, we will show that ?D? ? 0, with equality only if no
choice of ? decreases D.
We first derive an expression for ??D? that we will put to use shortly:
??D? =
?
x??
[
D(?x||?oldx )? D(?x||?x)
]
=
?
x??
[
H(?x||?oldx )? H(?x)? H(?x||?x) + H(?x)
]
=
?
x??
?
j
?x(j)
[
log?x(j)? log?oldx (j)
]
(14)
The EM algorithm is based on the fact that divergence is non-negative, and strictly
positive if the distributions compared are not identical:
0 ?
?
j
?
x??
?x(j)D(?oldxj ||?xj)
=
?
j
?
x??
?x(j)
?
f?Fx
?oldxj (f ) log
?oldxj (f )
?xj(f )
=
?
j
?
x??
?x(j)
?
f?Fx
?oldxj (f ) log
(
?oldfj
?oldx (j)
? ?x(j)
?fj
)
which yields the inequality
?
j
?
x??
?x(j)
[
log?x(j)? log?oldx (j)
]
?
?
j
?
x??
?x(j)
?
f?Fx
?oldxj (f )
[
log ?fj ? log ?oldfj
]
By (14), this can be written as
??D? ?
?
j
?
x??
?x(j)
?
f?Fx
?oldxj (f )
[
log ?fj ? log ?oldfj
]
(15)
Since ?oldfj is constant, by maximizing
?
j
?
x??
?x(j)
?
f?Fx
?oldxj (f ) log ?fj (16)
we maximize a lower bound on ??D?. It is easy to see that ??D? is bounded above
by zero: we simply set ?fj = ?oldfj . Since divergence is zero only if the two distributions
are identical, we have strict inequality in (15) unless the best choice for ? is ?old, in
which case no choice of ? makes ?D? < 0.
It remains to show that DL-EM computes the parameter set ? that maximizes (16).
We wish to maximize (16) under the constraints that the values {?fj} for fixed f sum to
unity across choices of j, so we apply Lagrange?s method. We express the constraints
in the form
Cf = 0
where
Cf ?
?
j
?fj ? 1
379
Abney Understanding the Yarowsky Algorithm
We seek a solution to the family of equations that results from expressing the gradient
of (16) as a linear combination of the gradients of the constraints:
?
??fj
?
k
?
x??
?x(k)
?
g?Fx
?oldxk (g) log ?gk = ?f
?Cf
??fj
(17)
We derive an expression for the derivative on the left-hand side:
?
??fj
?
k
?
x??
?x(k)
?
g?Fx
?oldxk (g) log ?gk =
?
x?Xf??
?x(j)?oldxj (f )
1
?fj
Similarly for the right-hand side:
?Cf
??fj
= 1
Substituting these into equation (17):
?
x?Xf??
?x(j)?oldxj (f )
1
?fj
= ?f
?fj =
?
x?Xf??
?x(j)?oldxj (f )
1
?f
(18)
Using the constraint Cf = 0 and solving for ?f :
?
j
?
x?Xf??
?x(j)?oldxj (f )
1
?f
? 1 = 0
?f =
?
j
?
x?Xf??
?x(j)?oldxj (f )
Substituting back into (18):
?fj =
?
x?Xf?? ?x(j)?
old
xj (f )
?
k
?
x?Xf?? ?x(k)?
old
xk (f )
(19)
If we consider the case where ? is the set of all examples and expand ?x in (19),
we obtain
?fj =
1
Z
?
?
?
x??fj
?oldxj (f ) +
1
L
?
x?Vf
?oldxj (f )
?
?
where Z normalizes ?f . It is not hard to see that this is the update rule that DL-EM-X
computes, using the intermediate values:
N[f , j] =
?
x??fj
?oldxj (f )
U[f , j] =
?
x?Vf
?oldxj (f )
380
Computational Linguistics Volume 30, Number 3
If we consider the case where ? is the set of labeled examples and expand ?x in (19),
we obtain
?fj =
1
Z
?
x??fj
?oldxj (f )
This is the update rule that DL-EM-? computes. Thus we see that DL-EM-X reduces
DX, and DL-EM-? reduces D?.
We note in closing that DL-EM-X can be simplified when used with algorithm Y-1,
inasmuch as it is known that ?fj = 1/L for all (f , j), where f ? Fx for some x ? V. Then
the expression for U[f , j] simplifies as follows:
?
x?Vf
?oldxj (f )
=
?
x?Vf
[
1/L
?
g?Fx 1/L
]
=
|Vf |
m
The dependence on j disappears, so we can replace U[f , j] with U[f ] in algorithm
DL-EM-X, delete step 2.1, and replace step 2.2 with the statement ?For each f ? Fx,
increment U[f ] by 1/m.?
3.3 The Objective Function K
Y-1/DL-EM is the only variation on the Yarowsky algorithm that we can show to
reduce negative log-likelihood, H. The variants that we discuss in the remainder of
the article, Y-1/DL-1 and YS, reduce an alternative objective function, K, which we
now define.
The value K (or, more precisely, the value K/m) is an upper bound on H, which
we derive using Jensen?s inequality, as follows:
H = ?
?
x?X
?
j
?xj log
?
g?Fx
1
m
?gj
? ?
?
x?X
?
j
?xj
?
g?Fx
1
m
log ?gj
=
1
m
?
x?X
?
g?Fx
H(?x||?g)
We define
K ?
?
x?X
?
g?Fx
H(?x||?g) (20)
By minimizing K, we minimize an upper bound on H. Moreover, it is in principle
possible to reduce K to zero. Since H(?x||?g) = H(?x) + D(?x||?g), K is reduced to zero
if all examples are labeled, each feature concentrates its prediction distribution in a
single label, and the label of every example agrees with the prediction of every feature
it possesses. In this limiting case, any minimizer of K is also a minimizer of H.
381
Abney Understanding the Yarowsky Algorithm
Table 8
The decision list induction algorithm DL-1-R.
(0) Initialize N[f , j] = 0, Z[f ] = 0 for all f , j
(1) For each example-label pair (x, j)
(1.1) For each feature f ? Fx, increment N[f , j], Z[f ]
(2) For each feature f and label j
(2.1) Set ?fj =
N[f ,j]
Z[f ]
(*) Define ?x(j) = 1m
?
f?Fx ?fj
Table 9
The decision list induction algorithm DL-1-VS.
(0) Initialize N[f , j] = 0, Z[f ] = 0, U[f ] = 0 for all f , j
(1) For each example-label pair (x, j)
(1.1) For each feature f ? Fx, increment N[f , j], Z[f ]
(2) For each unlabeled example x
(2.1) For each feature f ? Fx, increment U[f ]
(3) For each feature f and label j
(3.1) Set  = U[f ]/L
(3.2) Set ?fj =
N[f ,j]+
Z[f ]+U[f ]
(*) Define ?x(j) = 1m
?
f?Fx ?fj
We hasten to add a proviso: It is not possible to reduce K to zero for all data sets.
The following provides a necessary and sufficient condition for being able to do so.
Consider an undirected bipartite graph G whose nodes are examples and features.
There is an edge between example x and feature f just in case f is a feature of x.
Define examples x1 and x2 to be neighbors if they both belong to the same connected
component of G. K is reducible to zero if and only if x1 and x2 have the same label
according to Y(0), for all pairs of neighbors x1 and x2 in ?(0).
3.4 Algorithm DL-1
We consider two variants of DL-0, called DL-1-R and DL-1-VS. They differ from DL-0
in two ways. First, the DL-1 algorithms assume the ?mean? definition of ?x given in
equation (12) rather than the ?max? definition of equation (11). This is not actually a
difference in the induction algorithm itself, but in the way the decision list is used to
construct a prediction distribution ?x.
Second, the DL-1 algorithms use update rules that differ from the smoothed pre-
cision of DL-0. DL-1-R (Table 8) uses raw precision instead of smoothed precision.
DL-1-VS (Table 9) uses smoothed precision, but unlike DL-0, DL-1-VS does not use
a fixed smoothing constant 	; rather 	 varies from feature to feature. Specifically, in
computing the score ?fj, DL-1-VS uses |Vf |/L as its value for 	.
The value of 	 used by DL-1-VS can be expressed in another way that will prove
useful. Let us define
p(?|f ) ?
|?f |
|Xf |
p(V|f ) ?
|Vf |
|Xf |
382
Computational Linguistics Volume 30, Number 3
Lemma 2
The parameter values {?fj} computed by DL-1-VS can be expressed as
?fj = p(?|f )qf (j) + p(V|f )u(j) (21)
where u(j) is the uniform distribution over labels.
Proof
If |?f | = 0, then p(?|f ) = 0 and ?fj = u(j). Further, N[f , j] = Z[f ] = 0, so DL-1-VS
computes ?fj = u(j), and the lemma is proved. Hence we need only consider the case
|?f | > 0.
First we show that smoothed precision can be expressed as a convex combination
of raw precision (9) and the uniform distribution. Define ? = 	/|?f |. Then:
q?f (j) =
|?fj|+ 	
|?f |+ L	
=
|?fj|/|?f |+ ?
1 + L?
=
1
1 + L?
qf (j) +
L?
1 + L?
? ?
L?
=
1
1 + L?
qf (j) +
L?
1 + L?
u(j) (22)
Now we show that the mixing coefficient 1/(1 + L?) of (22) is the same as the mixing
coefficient p(?|f ) of the lemma, when 	 = |Vf |/L as in step 3.1 of DL-1-VS:
	 =
|Vf |
L
=
|?f |
L
? p(V|f )
p(?|f )
L? =
1
p(?|f ) ? 1
1
1 + L?
= p(?|f )
The main theorem of this section (Theorem 5) is that the specific Yarowsky al-
gorithm Y-1/DL-1 decreases K in each iteration until it reaches a critical point. It is
proved as a corollary of two theorems. The first (Theorem 3) shows that DL-1 min-
imizes K as a function of ?, holding ? constant, and the second (Theorem 4) shows
that Y-1 decreases K as a function of ?, holding ? constant. More precisely, DL-1-R
minimizes K over labeled examples ?, and DL-1-VS minimizes K over all examples
X. Either is sufficient for Y-1 to be effective.
Theorem 3
DL-1 minimizes K as a function of ?, holding ? constant. Specifically, DL-1-R minimizes
K over labeled examples ?, and DL-1-VS minimizes K over all examples X.
Proof
We wish to minimize K as a function of ? under the constraints
Cf ?
?
j
?fj ? 1 = 0
383
Abney Understanding the Yarowsky Algorithm
for each f . As before, to minimize K under the constraints Cf = 0, we express the
gradient of K as a linear combination of the gradients of the constraints and solve the
resulting system of equations:
?K
??fj
= ?f
?Cf
??fj
(23)
First we derive expressions for the derivatives of Cf and K. The variable ? represents
the set of examples over which we are minimizing K:
?Cf
??fj
= 1
?K
??fj
= ? ?
??fj
?
x??
?
g?Fx
?
k
?xk log ?gk
= ?
?
x?Xf??
?xj
1
?fj
We substitute these expressions into (23) and solve for ?fj:
?
?
x?Xf??
?xj
1
?fj
= ?f
?fj = ?
?
x?Xf??
?xj/?f
Substituting the latter expression into the equation for Cf = 0 and solving for f yields
?
j
?
??
?
x?Xf??
?xj/?f
?
? = 1
?|Xf ? ?| = ?f
Substituting this back into the expression for ?fj gives us
?fj =
1
|Xf ? ?|
?
x?Xf??
?xj (24)
If ? = ?, we have
?fj =
1
|?f |
?
x??f
[[x ? ?j]]
= qf (j)
This is the update computed by DL-1-R, showing that DL-1-R computes the parameter
values {?fj} that minimize K over the labeled examples ?.
384
Computational Linguistics Volume 30, Number 3
If ? = X, we have
?fj =
1
|Xf |
?
x??f
[[x ? ?j]] +
1
|Xf |
?
x?Vf
1
L
=
|?f |
|Xf |
?
|?fj|
|?f |
+
|Vf |
|Xf |
? 1
L
= p(?|f ) ? qf (j) + p(V|f ) ? u(j)
By Lemma 2, this is the update computed by DL-1-VS, hence DL-1-VS minimizes K
over the complete set of examples X.
Theorem 4
If the base learner decreases K over X or over ?, where the prediction distribution is
computed as
?x(j) =
1
m
?
f?Fx
?fj
then algorithm Y-1 decreases K at each iteration until it reaches a critical point, con-
sidering K as a function of ? with ? held constant.
Proof
The proof has the same structure as the proof of Theorem 1, so we give only a sketch
here. We minimize K as a function of ? by minimizing it for each example separately:
K(x) =
?
g?Fx
H(?x||?g)
=
?
j
?xj
?
g?Fx
log
1
?gj
To minimize K(x), we choose ?xj so as to concentrate all mass in
arg min
j
?
g?Fx
log
1
?gj
= arg max
j
?x(j)
This is the labeling rule used by Y-1.
If the base learner minimizes over ? only, rather than X, it can be shown that any
increase in K on unlabeled examples is compensated for in the labeling step, as in the
proof of Theorem 1.
Theorem 5
The specific Yarowsky algorithms Y-1/DL-1-R and Y-1/DL-1-VS decrease K at each
iteration until they reach a critical point.
Proof
Immediate from Theorems 3 and 4.
4. Sequential Algorithms
4.1 The Family YS
The Yarowsky algorithm variants we have considered up to now do ?parallel? updates
in the sense that the parameters {?fj} are completely recomputed at each iteration. In
385
Abney Understanding the Yarowsky Algorithm
this section, we consider a family YS of ?sequential? variants of the Yarowsky al-
gorithm, in which a single feature is selected for update at each iteration. The YS
algorithms resemble the ?Yarowsky-Cautious? algorithm of Collins & Singer (1999),
though they differ from that algorithm in that they update a single feature in each iter-
ation, rather than a small set of features, as in Yarowsky-Cautious. The YS algorithms
are intended to be as close to the Y-1/DL-1 algorithm as is consonant with single-
feature updates. The YS algorithms differ from one another, and from Y-1/DL-1, in
the choice of update rule. An interesting range of update rules work in the sequential
setting. In particular, smoothed precision with fixed 	, as in the original algorithm
Y-0/DL-0, works in the sequential setting, though with a proviso that will be spelled
out later.
Instead of an initial labeled set, there is an initial classifier consisting of a set of
selected features S0 and initial parameter set ?(0) with ?
(0)
fj = 1/L for all f ? S0. At
each iteration, one feature is selected to be added to the selected set. A feature, once
selected, remains in the selected set. It is permissible for a feature to be selected more
than once; this permits us to continue reducing K even after all features have been
selected. In short, there is a sequence of selected features f0, f1, . . . , and
St+1 = St ? {ft}
The parameters for the selected feature are also updated. At iteration t, the pa-
rameters ?gj, with g = ft, may be modified, but all other parameters remain constant.
That is:
?(t+1)gj = ?
(t)
gj for g = ft
It follows that, for all t:
?(t)gj =
1
L
for g ? St
However, parameters for features in S0 may not be modified, inasmuch as they play
the role of manually labeled data.
In each iteration, one selects a feature ft and computes (or recomputes) the predic-
tion distribution ?ft for the selected feature ft. Then labels are recomputed as follows.
Recall that y? ? arg maxj ?x(j), where we continue to assume ?x(j) to have the ?mix-
ture? definition (equation (12)). The label of example x is set to y? if any feature of x
belongs to St+1. In particular, all previously labeled examples continue to be labeled
(though their labels may change), and any unlabeled examples possessing feature ft
become labeled.
The algorithm is summarized in Table 10. It is actually an algorithm schema;
the definition for ?update? needs to be supplied. We consider three different update
functions: one that uses raw precision as its prediction distribution, one that uses
smoothed precision, and one that goes in the opposite direction, using what we might
call ?peaked precision.? As we have seen, smoothed precision can be expressed as a
mixture of raw precision and the uniform (i.e., maximum-entropy) distribution (22).
Peaked precision q?(f ) mixes in a certain amount of the point (i.e., minimum-entropy)
distribution that has all its mass on the label that maximizes raw precision:
q?f (j) ? p(?(t)|f )qf (j) + p(V(t)|f )[[j = j?]] (25)
where
j? ? arg max
j
qf (j) (26)
386
Computational Linguistics Volume 30, Number 3
Table 10
The sequential algorithm YS.
(0) Given: S(0), ?(0), with ?(0)gj = 1/L for g ? S
(0)
(1) Initialization
(1.1) Set S = S(0), ? = ?(0)
(1.2) For each example x ? X
If x possesses a feature in S(0), set Yx = y?, else set Yx = ?
(2) Loop:
(2.1) Choose a feature f ? S(0) such that ?f = ? and ?f = qf
If there is none, stop
(2.2) Add f to S
(2.3) For each label j, set ?fj = update(f , j)
(2.4) For each example x possessing a feature in S, set Yx = y?
Note that peaked precision involves a variable amount of ?peaking?; the mixing pa-
rameters depend on the relative proportions of labeled and unlabeled examples. Note
also that j? is a function of f , though we do not explicitly represent that dependence.
The three instantiations of algorithm YS that we consider are
YS-P (?peaked?) ?fj = q?f (j)
YS-R (?raw?) ?fj = qf (j)
YS-FS (?fixed smoothing?) ?fj = q?f (j)
We will show that the first two algorithms reduce K in each iteration. We will show
that the third algorithm, YS-FS, reduces K in iterations in which ft is a new feature,
not previously selected. Unfortunately, we are unable to show that YS-FS reduces K
when ft is a previously selected feature. This suggests employing a mixed algorithm
in which smoothed precision is used for new features but raw or peaked precision is
used for previously selected features.
A final issue with the algorithm schema YS concerns the selection of features
in step 2.1. The schema as stated does not specify which feature is to be selected.
In essence, the manner in which rules are selected does not matter, as long as one
selects rules that have room for improvement, in the sense that the current prediction
distribution ?f differs from raw precision qf . (The justification for this choice is given
in Theorem 9.) The theorems in the following sections show that K decreases in each
iteration, so long as any such rule can be found.
One could choose greedily by choosing the feature that maximizes gain G (equa-
tion (27)), though in the next section we give lower bounds for G that are rather more
easily computed (Theorems 6 and 7).
4.2 Gain
From this point on, we consider a single iteration of the YS algorithm and discard the
variable t. We write ?old and ?old for the parameter set and labeling at the beginning of
the iteration, and we write simply ? and ? for the new parameter set and new label-
ing. The set ? (respectively, V) represents the examples that are labeled (respectively,
unlabeled) at the beginning of the iteration. The selected feature is f .
We wish to choose a prediction distribution for f so as to guarantee that K decreases
in each iteration. The gain in the current iteration is
G =
?
x?X
?
g?Fx
[
H(?oldx ||?oldg )? H(?x||?g)
]
(27)
387
Abney Understanding the Yarowsky Algorithm
Gain is the negative change in K; it is positive when K decreases.
In considering the reduction in K from (?old, ?old) to (?, ?), it will be convenient to
consider the following intermediate values:
K0 =
?
x?X
?
g?Fx
H(?oldx ||?oldg )
K1 =
?
x?X
?
g?Fx
H(?x||?oldg )
K2 =
?
x?X
?
g?Fx
H(?x||?g)
K3 =
?
x?X
?
g?Fx
H(?x||?g)
where
?xj =
{
[[j = j?]] if x ? Vf
?oldxj otherwise
and
j? ? arg max
j
?fj (28)
One should note that
? ?f is the new prediction distribution for the candidate f ; ?gj = ?oldgj for
g = f .
? ? is the new label distribution, after relabeling. It is defined as
?xj =
{
[[j = y?(x)]] if x ? ? ? Xf
1
L otherwise
(29)
? for x ? Vf , the only selected feature at t + 1 is f , hence j? = y? for such
examples. It follows that ? and ? agree on examples in Vf . They also
agree on examples that are unlabeled at t + 1, assigning them the
uniform label distribution. If ? and ? differ, it is only on old labeled
examples (?) that need to be relabeled, given the addition of f .
The gain G can be represented as the sum of three intermediate gains, correspond-
ing to the intermediate values just defined:
G = GV + G? + G? (30)
where
GV = K0 ? K1
G? = K1 ? K2
G? = K2 ? K3
The gain GV intuitively represents the gain that is attributable to labeling previously
unlabeled examples in accordance with the predictions of ?. The gain G? represents the
gain that is attributable to changing the values ?fj, where f is the selected feature. The
388
Computational Linguistics Volume 30, Number 3
gain G? represents the gain that is attributable to changing the labels of previously
labeled examples to make labels agree with the predictions of the new model ?. The
gain G? corresponds to step 2.3 of algorithm YS, in which ? is changed but ? is held
constant; and the combined GV and G? gains correspond to step 2.4 of algorithm YS,
in which ? is changed while holding ? constant.
In the remainder of this section, we derive two lower bounds for G. In following
sections, we show that the updates YS-P, YS-R, and YS-FS guarantee that the lower
bounds given below are non-negative, and hence that G is non-negative.
Lemma 3
GV = 0
Proof
We show that K remains unchanged if we substitute ? for ?old in K0. The only property
of ? that we need is that it agrees with ?old on previously labeled examples.
Since ?x = ?oldx for x ? ?, we need only consider examples in V. Since these
examples are unlabeled at the beginning of the iteration, none of their features have
been selected, hence ?oldgj = 1/L for all their features g. Hence
K1 = ?
?
x?V
?
g?Fx
?
j
?xj log ?oldgj
= ?
?
x?V
?
g?Fx
?
?
?
j
?xj
?
? log
1
L
= ?
?
x?V
?
g?Fx
?
?
?
j
?oldxj
?
? log
1
L
= ?
?
x?V
?
g?Fx
?
j
?oldxj log ?
old
gj = K0
(Note that ?xj is not in general equal to ?oldxj , but
?
j ?xj and
?
j ?
old
xj both equal 1.) This
shows that K0 = K1, and hence that GV = 0.
Lemma 4
G? ? 0.
We must show that relabeling old labeled examples?that is, setting ?x(j) = [[j = y?]]
for x ? ??does not increase K. The proof has the same structure as the proof of
Theorem 1 and is omitted.
Lemma 5
G? is equal to
|?f |
[
H(qf ||?oldf )? H(qf ||?f )
]
+ |Vf |
[
log L ? log 1
?f j?
]
(31)
Proof
By definition, G? = K1?K2, and K1 and K2 are identical everywhere except on examples
389
Abney Understanding the Yarowsky Algorithm
in Xf . Hence
G? =
?
x?Xf
?
g?Fx
[
H(?x||?oldg )? H(?x||?g)
]
We divide this sum into three partial sums:
G? = A + B + C (32)
A =
?
x??f
[
H(?x||?oldf )? H(?x||?f )
]
B =
?
x?Vf
[
H(?x||?oldf )? H(?x||?f )
]
C =
?
x?Xf
?
g=f?Fx
[
H(?x||?oldg )? H(?x||?g)
]
We consider each partial sum separately:
A =
?
x??f
[
H(?x||?oldf )? H(?x||?f )
]
= ?
?
x??f
?
k
?xk
[
log ?oldfk ? log ?fk
]
= ?
?
x??f
?
k
[[x ? ?k]]
[
log ?oldfk ? log ?fk
]
= ?
?
k
|?fk|
[
log ?oldfk ? log ?fk
]
= ?|?f |
?
k
qf (k)
[
log ?oldfk ? log ?fk
]
= |?f |
[
H(qf ||?oldf )? H(qf ||?f )
]
(33)
B =
?
x?Vf
[
H(?x||?oldf )? H(?x||?f )
]
= ?
?
x?Vf
?
k
?xk
[
log ?oldfk ? log ?fk
]
= ?
?
x?Vf
?
k
[[k = j?]]
[
log ?oldfk ? log ?fk
]
= |Vf |
[
log
1
?oldf j?
? log 1
?f j?
]
= |Vf |
[
log L ? log 1
?f j?
]
(34)
The justification for the last step is a bit subtle. If f is a new feature, not previously
selected, then ?oldfk = 1/L for all k, and the substitution is valid. On the other hand, if
f is a previously selected feature, then |Vf | = 0, and even though the substitution of
390
Computational Linguistics Volume 30, Number 3
1/L for ?oldf j? may not be valid, it is innocuous.
C =
?
x?Xf
?
g=f?Fx
[
H(?x||?oldg )? H(?x||?g)
]
=
?
x?Xf
?
g=f?Fx
[
H(?x||?oldg )? H(?x||?oldg )
]
= 0 (35)
Combining (32), (33), (34), and (35) yields the lemma.
Theorem 6
G is bounded below by (31).
Proof
Combining (30) with Lemmas 3, 4, and 5.
Theorem 7
G is bounded below by
|?f |
[
H(qf ||?oldf )? H(qf ||?f )
]
Proof
The theorem follows immediately from Theorem 6 if we can show that
log L ? log 1
?f j?
? 0
Observe first that log L = H(u). (Recall that u(j) = 1/L is the uniform distribution over
labels.) By Lemma 1, we know that
H(u)? log 1
?f j?
? H(u)? H(?f )
? 0
The latter follows because the uniform distribution maximizes entropy.
Theorem 8
G is bounded below by
|?f |
[
D(qf ||?oldf )? D(qf ||?f )
]
Proof
Immediate from Theorem 7 and the fact that
H(qf ||?oldf )? H(qf ||?f ) = H(qf ) + D(qf ||?oldf )? H(qf )? D(qf ||?f )
= D(qf ||?oldf )? D(qf ||?f )
Theorem 9
If ?oldf = qf , then there is a choice of ?f that yields strictly positive gain.
391
Abney Understanding the Yarowsky Algorithm
Proof
If ?oldf = qf , then
D(qf ||?oldf ) > 0
Setting ?f = qf has the result that
|?f |
[
D(qf ||?oldf )? D(qf ||?f )
]
= |?f |D(qf ||?oldf ) > 0
Hence G > 0 by Theorem 8.
4.3 Algorithm YS-P
We now use the results of the previous section to show that the algorithm YS-P is
correct in the sense that it reduces K in every iteration.
Theorem 10
In each iteration of algorithm YS-P, K decreases.
Proof
We wish to show that G > 0. By Theorem 6, that is true if expression (31) is positive.
By Theorem 9, there exist choices for ?f that make (31) positive, hence in particular,
we guarantee G > 0 by maximizing (31). We maximize (31) by minimizing
|?f |H(qf ||?f ) + |Vf | log
1
?f j?
(36)
Since
H(qf ||?f ) = H(qf ) + D(qf ||?f )
we minimize (36) by minimizing
|?f |D(qf ||?f ) + |Vf | log
1
?f j?
(37)
Both terms are nonnegative. The first term is zero if ?f = qf . The second term is zero
for any distribution that concentrates all its mass in a single label j?; it is symmetric
in all choices of j? and decreases monotonically as ?f j? approaches one. Hence, the
minimum of (37) will have j? equal to the mode of qf , though it may be more peaked
than qf , at the cost of an increase in the first term, but offset by a decrease in the
second term.
Recall that j? = arg maxj qf (j). By the reasoning of the previous paragraph, we
know that j? = j? at the minimum of (37). Hence we can minimize (37) by minimizing
|?f |D(qf ||?f )? |Vf |
?
k
[[k = j?]] log ?fk (38)
We compute the gradient:
?
??fj
[
|?f |D(qf ||?f )? |Vf |
?
k
[[k = j?]] log ?fk
]
=
?
??fj
[
|?f |H(qf ||?f )? |?f |H(qf )? |Vf |
?
k
[[k = j?]] log ?fk
]
392
Computational Linguistics Volume 30, Number 3
=
?
??fj
|?f |H(qf ||?f )?
?
??fj
|Vf |
?
k
[[k = j?]] log ?fk
= ?|?f |
?
??fj
?
k
qf (k) log ?fk ? |Vf |
?
??fj
?
k
[[k = j?]] log ?fk
= ?|?f |
?
??fj
qf (j) log ?fj ? |Vf |
?
??fj
[[j = j?]] log ?fj
= ?|?f |qf (j)
1
?fj
? |Vf |[[j = j?]]
1
?fj
(39)
As before, the derivative of the constraint Cf = 0 is one, and we minimize (38) under
the constraint by solving
?|?f |qf (j)
1
?fj
? |Vf |[[j = j?]]
1
?fj
= ?
?fj =
(
?|?f |qf (j)? |Vf |[[j = j?]]
)
/? (40)
Substituting into the constraint gives us
?
j
(
?|?f |qf (j)? |Vf |[[j = j?]]
)
/? = 1
?|?f | ? |Vf | = ?
?|Xf | = ?
Substituting this back into (40) yields:
?fj = p(?|f )qf (j) + p(V|f )[[j = j?]] (41)
That is, the maximizing solution is peaked precision, which is the update rule for YS-P.
4.4 Algorithm YS-R
We now show that YS-R also decreases K in each iteration. In fact, it has essentially
already been proven.
Theorem 11
Algorithm YS-R decreases K in each iteration.
Proof
In the proof of Theorem 9, we showed that the choice
?f = qf
yields strictly positive gain. This is the update rule used by YS-R.
4.5 Algorithm YS-FS
The original Yarowsky algorithm YS-0/DL-0 used smoothed precision with fixed 	
as update rule. We have been unsuccessful at justifying this choice of update rule
in general. However, we are able at least to show that it does decrease K when the
selected feature is a new feature, not previously selected.
393
Abney Understanding the Yarowsky Algorithm
Theorem 12
Algorithm YS-FS has positive gain in each iteration in which the selected feature has
not been previously selected.
Proof
By Theorem 7, gain is positive if
H(qf ||?oldf ) > H(qf ||?f ) (42)
By the assumption that the selected feature f has not been previously selected, ?oldf is
the uniform distribution u, and the left-hand side of (42) is equal to H(qf ||u). It is easy
to verify that H(p||u) = H(u) for any distribution p; hence the left-hand side of (42) is
equal to H(u). Further, YS-FS uses smoothed precision as update rule, ?f = q?f , so (42)
can be rewritten as
H(u) > H(qf ||q?f )
This condition does not hold trivially, inasmuch as cross entropy, like divergence, is
unbounded. But we can show that it holds in this particular case.
We derive an upper bound for H(qf ||q?f ):
H(qf ||q?f ) = ?
?
j
qf (j) log q?f (j)
= ?
?
j
qf (j) log
[
1
1 + L	
qf (j) +
L	
1 + L	
u(j)
]
? ?
?
j
qf (j)
[
1
1 + L	
log qf (j) +
L	
1 + L	
log u(j)
]
=
1
1 + L	
H(qf ) +
L	
1 + L	
H(qf ||u)
=
1
1 + L	
H(qf ) +
L	
1 + L	
H(u) (43)
Observe that
H(u) >
1
1 + L	
H(qf ) +
L	
1 + L	
H(u) (44)
iff
[
1 ? L	
1 + L	
]
H(u) >
1
1 + L	
H(qf )
iff
H(u) > H(qf )
We know that H(u) ? H(qf ) because the uniform distribution maximizes entropy. We
know that the inequality is strict by the following reasoning. Since f is a new feature,
?oldf = u. Because of the restriction on step 2.1 in algorithm YS, ?
old
f = qf , hence qf = u,
and H(u) is strictly greater than H(qf ).
Hence (44) is true, and combining (44) with (43), we have shown (42) to be true,
proving the theorem.
394
Computational Linguistics Volume 30, Number 3
5. Minimization of Feature Entropy
At the beginning of the article, the co-training algorithm was mentioned as an alterna-
tive to the Yarowsky algorithm. There is in fact a connection between co-training and
the Yarowsky algorithm. In the original co-training paper (Blum and Mitchell 1998), it
was suggested that the algorithm be understood as seeking to maximize agreement on
unlabeled data between classifiers trained on two different ?views? of the data. Subse-
quent work (Dasgupta, Littman, and McAllester 2001) has proven a direct connection
between classifier error and such cross-view agreement on unlabeled data.
In the current context, there is also justification for pursuing agreement on unla-
beled data. However, the Yarowsky algorithm does not assume the existence of two
conditionally independent views of the data. Rather, there is a motivation for seeking
agreement on unlabeled data between arbitrary pairs of features.
Recall that our original objective function, H, can be expressed as the sum of an
entropy term and a divergence term:
H =
?
x?X
[
H(?x) + D(?x||?x)
]
As D(?x||?x) becomes small and H(?x) becomes small, H(?x) necessarily also becomes
small; hence we can limit H by limiting H(?x) and D(?x||?x). Intuitively, we wish to
reduce the uncertainty of the model?s predictions, while also improving the fit between
the model?s predictions and the known labels.
Let us focus now on the uncertainty of the model?s predictions:
H(?x) = ?
?
j
?x(j) log?x(j)
= ?
?
j
?x(j) log
?
?
?
g?Fx
1
m
?gj
?
?
? ?
?
j
?x(j)
?
g?Fx
1
m
log ?gj
= ?
?
j
?
?
?
f?Fx
1
m
?fj
?
?
?
g?Fx
1
m
log ?gj
= ? 1
m2
?
f?Fx
?
g?Fx
?
j
?fj log ?gj
=
1
m2
?
f?Fx
?
g?Fx
H(?f ||?g)
=
1
m2
?
f?Fx
?
g?Fx
[
H(?f ) + D(?f ||?g)
]
=
1
m
?
f?Fx
H(?f ) +
1
m2
?
f?Fx
?
g?Fx
D(?f ||?g) (45)
In other words, by decreasing the uncertainty of the prediction distributions of indi-
vidual features and simultaneously increasing the agreement among features (that is,
decreasing their pairwise divergence), we decrease an upper bound on H(?x). This
395
Abney Understanding the Yarowsky Algorithm
motivates interfeature agreement without recourse to an assumption of independent
views.
6. Conclusion
In this article, we have presented a number of variants of the Yarowsky algorithm,
and we have shown that they optimize natural objective functions. We considered
first the modified generic Yarowsky algorithm Y-1 and showed that it minimizes the
objective function H (which is equivalent to maximizing likelihood), provided that its
base learner reduces H.
We then considered three families of specific Yarowsky-like algorithms. The
Y-1/DL-EM algorithms (Y-1/DL-EM-? and Y-1/DL-EM-X) minimize H but have the
disadvantage that the DL-EM base learner has no similarity to Yarowsky?s original base
learner. A much better approximation to Yarowsky?s original base learner is provided
by DL-1, and the Y-1/DL-1 algorithms (Y-1/DL-1-R and Y-1/DL-1-VS) were shown to
minimize the objective function K, an upper bound for H. Finally, the YS algorithms
(YS-P, YS-R, and YS-FS) are sequential variants, reminiscent of the Yarowsky-Cautious
algorithm of Collins and Singer; we showed that they minimize K.
To the extent that these algorithms capture the essence of the original Yarowsky
algorithm, they provide a formal understanding of Yarowsky?s approach. Even if they
are deemed to diverge too much from the original to cast light on its workings, they
at least represent a new family of bootstrapping algorithms with solid mathematical
foundations.
References
Abney, Steven. 2002. Bootstrapping. In
Proceedings of 40th Annual Meeting of the
Association for Computational Linguistics
(ACL), Philadelphia, pages 360?367.
Blum, Avrim and Tom Mitchell. 1998.
Combining labeled and unlabeled data
with co-training. In Proceedings of the 11th
Annual Conference on Computational
Learning Theory (COLT), pages 92?100.
Morgan Kaufmann, San Francisco.
Collins, Michael and Yoram Singer. 1999.
Unsupervised models for named entity
classification. In Proceedings of Empirical
Methods in Natural Language Processing
(EMNLP), College Park, MD,
pages 100?110.
Dasgupta, Sanjoy, Michael Littman, and
David McAllester. 2001. PAC
generalization bounds for co-training. In
Proceedings of Advances in Neural
Information Processing Systems 14 (NIPS),
Vancouver, British Columbia, Canada.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics, Cambridge, MA, pages
189?196.
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 48?55,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Latent Features in Automatic Tense Translation between Chinese and
English
Yang Ye?, Victoria Li Fossum?, Steven Abney ? ?
? Department of Linguistics
? Department of Electrical Engineering and Computer Science
University of Michigan
Abstract
On the task of determining the tense to use
when translating a Chinese verb into En-
glish, current systems do not perform as
well as human translators. The main focus
of the present paper is to identify features
that human translators use, but which are
not currently automatically extractable.
The goal is twofold: to test a particu-
lar hypothesis about what additional infor-
mation human translators might be using,
and as a pilot to determine where to focus
effort on developing automatic extraction
methods for features that are somewhat be-
yond the reach of current feature extrac-
tion. The paper shows that incorporating
several latent features into the tense clas-
sifier boosts the tense classifier?s perfor-
mance, and a tense classifier using only the
latent features outperforms one using only
the surface features. Our findings confirm
the utility of the latent features in auto-
matic tense classification, explaining the
gap between automatic classification sys-
tems and the human brain.
1 Introduction
Language speakers make two types of distinctions
about temporal relations: the first type of relation
is based on precedence between events and can be
expanded into a finer grained taxonomy as pro-
posed by (Allen, 1981). The second type of re-
lation is based on the relative positioning between
the following three time parameters proposed by
(Reichenbach, 1947): speech time (S), event time
(E) and reference time (R). In the past couple of
decades, the NLP community has seen an emer-
gent interest in the first type of temporal relation.
In the cross-lingual context, while the first type of
relationship can be easily projected across a lan-
guage pair, the second type of relationship is of-
ten hard to be projected across a language pair. In
contrast to this challenge, cross-lingual temporal
reference distinction has been poorly explored.
Languages vary in the granularity of their
tense and aspect representations; some have finer-
grained tenses or aspects than others. Tense gener-
ation and tense understanding in natural language
texts are highly dynamic and context-dependent
processes, since any previously established time
point or interval, whether explicitly mentioned in
the context or not, could potentially serve as the
reference time for the event in question. (Bruce,
1972) captures this nature of temporal reference
organization in discourse through a multiple tem-
poral reference model. He defines a set (S
1
, S
2
, ...,
Sn) that is an element of tense. S1 corresponds to
the speech time, Sn is the event time, and (Si, i=2,
..., n-1) stand for a sequence of time references
from which the reference time of a particular event
could come. Given the elusive nature of reference
time shift, it is extremely hard to model the ref-
erence time point directly in temporal information
processing. The above reasons motivate classify-
ing temporal reference distinction automatically,
using machine learning algorithms such as Con-
ditional Random Fields (CRFs).
Many researchers in Natural Language Process-
ing seem to believe that an automatic system does
not have to follow the mechanism of human brain
in order to optimize its performance, for example,
the feature space for an automatic classification
system does not have to replicate the knowledge
sources that human beings utilize. There has been
very little research that pursues to testify this faith.
The current work attempts to identify which
features are most important for tense generation
in Chinese to English translation scenario, which
can point to direction of future research effort for
automatic tense translation between Chinese and
English.
48
The remaining part of the paper is organized
as follows: Section 2 summarizes the significant
related works in temporal information annotation
and points out how this study relates to yet dif-
fers from them. Section 3 formally defines the
problem, tense taxonomy and introduces the data.
Section 4 discusses the feature space and proposes
the latent features for the tense classification task.
Section 5 presents the classification experiments
in Conditional Random Fields as well as Classifi-
cation Tree and reports the evaluation results. Sec-
tion 6 concludes the paper and section 7 points out
directions for future research.
2 Related Work
There is an extensive literature on temporal infor-
mation processing. (Mani, et al, 2005) provides
a survey of works in this area. Here, we high-
light several works that are closely related to Chi-
nese temporal information processing. (Li, 2001)
describes a model of mining and organizing tem-
poral relations embedded in Chinese sentences,
in which a set of heuristic rules are developed to
map linguistic patterns to temporal relations based
on Allen?s thirteen relations. Their work shows
promising results via combining machine learning
techniques and linguistic features for successful
temporal relation classification, but their work is
concerned with another type of temporal relation-
ship, namely, the precedence-based temporal rela-
tion between a pair of events explicitly mentioned
in text.
A significant work worth mentioning is (Olsen
et. al. 2001)?s paper, where the authors exam-
ine the determination of tense for English verbs
in Chinese-to-English translation. In addition to
the surface features such as the presence of aspect
markers and certain adverbials, their work makes
use of the telicity information encoded in the lexi-
cons through the use of Lexical Conceptual Struc-
tures (LCS). Based on the dichotomy of grammat-
ical aspect and lexical aspect, they propose that
past tense corresponds to the telic (either inher-
ently or derived) LCS. They propose a heuristic
algorithm in which grammatical aspect markings
supersede the LCS, and in the absence of gram-
matical aspect marking, verbs that have telic LCS
are translated into past tense and present tense oth-
erwise. They report a significant performance im-
provement in tense resolution from adding a verb
telicity feature. They also achieve better perfor-
mance than the baseline system using the telic-
ity feature alone. This work, while alerting re-
searchers to the importance of lexical aspectual
feature in determination of tense for English verbs
in Chinese-to-English machine translation, is sub-
ject to the risk of adopting a one-to-one mapping
between grammatical aspect markings and tenses
hence oversimplifies the temporal reference prob-
lem in Chinese text. Additionally, their binary
tense taxonomy is too coarse for the rich tempo-
ral reference system in Chinese.
(Ye, et al 2005) reported a tense tagging case
study of training Conditional Random Fields on
a set of shallow surface features. The low inter-
annotator agreement rate reported in the paper il-
lustrates the difficulty of tense tagging. Neverthe-
less, the corpora size utilized is too small with only
52 news articles and none of the latent features was
explored, so the evaluation result reported in the
paper leaves room for improvement.
3 Problem Definition
3.1 Problem Formulation
The problem we are interested in can be formal-
ized as a standard classification or labeling prob-
lem, in which we try to learn a classifier
C : V ? T (1)
where V is a set of verbs (each described by a
feature vector), and T is the set of possible tense
tags.
Tense and aspect are morphologically merged
in English and coarsely defined, there can be
twelve combinations of the simple tripartite tenses
(present, past and future) with the progressive and
perfect grammatical aspects. For our classification
experiments, in order to combat sparseness, we ig-
nore the aspects and only deal with the three sim-
ple tenses: present, past and future.
3.2 Data
We use 152 pairs of parallel Chinese-English arti-
cles from LDC release. The Chinese articles come
from two news sources: Xinhua News Service and
Zaobao News Service, consisting of 59882 Chi-
nese characters in total with roughly 350 charac-
ters per article. The English parallel articles are
from Multiple-Translation Chinese (MTC) Corpus
from LDC with catalog number LDC2002T01.
We chose to use the best human translation out
49
of 9 translation teams as our gold-standard par-
allel English data. The verb tenses are obtained
through manual alignment between the Chinese
source articles and the English translations. In or-
der to avoid the noise brought by errors and be fo-
cused on the central question we try to answer in
the paper, we did not use automatic tools such as
GIZA++ to obtain the verb alignments, which typ-
ically comes with significant amount of errors. We
ignore Chinese verbs that are not translated into
English as verbs because of ?nominalization? (by
which verbal expressions in Chinese are translated
into nominal phrases in English). This exclusion is
based on the rationale that another choice of syn-
tactic structure might retain the verbal status in the
target English sentence, but the tense of those po-
tential English verbs would be left to the joint de-
cision of a set of disparate features. Those tenses
are unknown in our training data. This preprocess-
ing yields us a total of 2500 verb tokens in our data
set.
4 Feature Space
4.1 Surface Features
There are many heterogeneous features that con-
tribute to the process of tense generation for Chi-
nese verbs in the cross-lingual situation. Tenses in
English, while manifesting a distinction in tempo-
ral reference, do not always reflect this distinction
at the semantic level, as is shown in the sentence ?I
will leave when he comes.? (Hornstein, 1990) ac-
counts for this phenomenon by proposing the Con-
straints on Derived Tense Structures. Therefore,
the feature space we use includes the features that
contribute to the semantic level temporal reference
construction as well as those contributing to tense
generation from that semantic level. The follow-
ing is a list of the surface features that are directly
extractable from the training data:
1. Feature 1: Whether the verb is in quoted
speech or not.
2. Feature 2: The syntactic structure in which
the current verb is embedded. Possible struc-
tures include sentential complements, rel-
ative clauses, adverbial clauses, appositive
clauses, and null embedding structure.
3. Feature 3: Which of the following signal
adverbs occur between the current verb
and the previous verb: yi3jing1(already),
ceng2jing1(once), jiang1(future tense
marker), zheng4zai4(progressive aspect
marker), yi4zhi2(have always been).
4. Feature 4: Which of the following aspect
markers occur between the current verb and
the subsequent verb: le0, zhe0, guo4.
5. Feature 5: The distance in characters between
the current verb and the previously tagged
verb (We descretize the continuous distance
into three ranges: 0 < distance < 5, 5 ?
distance < 10, or 10 ? distance <?).
6. Feature 6: Whether the current verb is in the
same clause as the previous verb.
Feature 1 and feature 2 are used to capture the
discrepancy between semantic tense and syntactic
tense. Feature 3 and feature 4 are clues or triggers
of certain aspectual properties of the verbs. Fea-
ture 5 and feature 6 try to capture the dependency
between tenses of adjacent verbs.
4.2 Latent Features
The bottleneck in Artificial Intelligence is the un-
balanced knowledge sources shared by human be-
ings and a computer system. Only a subset of
the knowledge sources used by human beings can
be formalized, extracted and fed into a computer
system. The rest are less accessible and are very
hard to be shared with a computer system. De-
spite their importance in human language process-
ing, latent features have received little attention in
feature space exploration in most NLP tasks be-
cause they are impractical to extract. Although
there have not yet been rigorous psycholinguis-
tic studies demonstrating the extent to which the
above knowledge types are used in human tempo-
ral relation processing, we hypothesize that they
are very significant in assisting human?s temporal
relation decision. Nevertheless, a quantitative as-
sessment of the utility of the latent features in NLP
tasks has yet to be explored. (Olsen, et al, 2001)
illustrates the value of latent features by showing
how the telicity feature alone can help with tense
resolution in Chinese to English machine transla-
tion. Given the prevalence of latent features in hu-
man language processing, in order to emulate hu-
man beings performance of the disambiguation, it
is crucial to experiment with the latent features in
automatic tense classification.
(Pustejovsky, 2004) discusses the four basic
problems in event-temporal identification:
50
??????????????????????????????????
??????????
He said that Henan Province not only possesses the hardwares necessary for foreign 
investment, but also has, on the basis of the State policies and Henan's specific 
conditions, formulated its own preferential policies.
? ?? ???? ??
N/A include subsume
?? ??
Figure 1: Temporal Relations between Adjacent Events
1. Time-stamping of events (identifying an
event and anchoring it in time)
2. Ordering events with respect to one another
3. Reasoning with contextually under-specified
temporal expressions
4. Reasoning about the persistence of events
(how long does an event or the outcome of
an event last?)
While time-stamping of the events and reason-
ing with contextually under-specified temporal ex-
pressions might be too informative to be features
in tense classification, information concerning or-
derings between events and persistence of events
are relatively easier to be encoded as features in
a tense classification task. Therefore, we exper-
iment with these two latent knowledge sources,
both of which are heavily utilized by human be-
ings in tense resolution.
4.3 Telicity and Punctuality Features
Following (Vendler, 1947), temporal information
encoded in verbs is largely captured by some in-
nate properties of verbs, of which telicity and
punctuality are two very important ones. Telic-
ity specifies a verb?s ability to be bound in a cer-
tain time span, while punctuality specifies whether
or not a verb is associated with a point event in
time. Telicity and punctuality prepare verbs to be
assigned different tenses when they enter the con-
text in the discourse. While it is true that isolated
verbs are typically associated with certain telicity
and punctuality features, such features are contex-
tually volatile. In reaction to the volatility exhib-
ited in verb telicity and punctuality features, we
propose that verb telicity and punctuality features
should be evaluated only at the clausal or senten-
tial level for the tense classification task. We man-
ually obtained these two features for both the En-
glish and the Chinese verbs. All verbs in our data
set were manually tagged as ?telic? or ?atelic?, and
?punctual? or ?apunctual?, according to context.
4.4 Temporal Ordering Feature
(Allen, 1981) defines thirteen relations that could
possibly hold between any pair of situations. We
experiment with six temporal relations which we
think represent the most typical temporal relation-
ships between two events. We did not adopt all
of the thirteen temporal relationships proposed by
Allen for the reason that some of them would re-
quire excessive deliberation from the annotators
and hard to implement. The six relationships we
explore are as follows:
1. event A precedes event B
2. event A succeeds event B
3. event A includes event B
4. event A subsumes event B
5. event A overlaps with event B
6. no temporal relations between event A and
event B
For each Chinese verb in the source Chinese
texts, we annotate the temporal relation between
the verb and the previously tagged verb as belong-
ing to one of the above classes. The annotation
of the temporal relation classes mimics a deeper
semantic analysis of the Chinese source text. Fig-
ure 1 illustrates a sentence in which each verb is
tagged by the temporal relation class that holds be-
tween it and the previous verb.
51
5 Experiments and Evaluation
5.1 CRF learning algorithms
Conditional Random Fields (CRFs) are a formal-
ism well-suited for learning and prediction on se-
quential data in many NLP tasks. It is a prob-
abilistic framework proposed by (Lafferty et al,
2001) for labeling and segmenting structured data,
such as sequences, trees and lattices. The condi-
tional nature of CRFs relaxes the independence as-
sumptions required by traditional Hidden Markov
Models (HMMs). This is because the conditional
model makes it unnecessary to explicitly represent
and model the dependencies among the input vari-
ables, thus making it feasible to use interacting and
global features from the input. CRFs also avoid
the label bias problem exhibited by maximum en-
tropy Markov models (MEMMs) and other con-
ditional Markov models based on directed graph-
ical models. CRFs have been shown to perform
well on a number of NLP problems such as shal-
low parsing (Sha and Pereira, 2003), table extrac-
tion (Pinto et al, 2003), and named entity recog-
nition (McCallum and Li, 2003). For our exper-
iments, we use the MALLET implementation of
CRF?s (McCallum, 2002).
5.2 Experiments
5.2.1 Human Inter-Annotator Agreement
All supervised learning algorithms require a
certain amount of training data, and the reliability
of the computational solutions is intricately tied
to the accuracy of the annotated data. Human an-
notations typically suffer from errors, subjectivity,
and the expertise effect. Therefore, researchers
use consistency checking to validate human an-
notation experiments. The Kappa Statistic (Co-
hen, 1960) is a standard measurement of inter-
annotator agreement for categorical data annota-
tion. The Kappa score is defined by the following
formula, where P(A) is the observed agreement
rate from multiple annotators and P(E) is the ex-
pected rate of agreement due to pure chance:
k = P (A)? P (E)
1? P (E)
(2)
Since tense annotation requires disambiguating
grammatical meaning, which is more abstract than
lexical meaning, one would expect the challenge
posed by human annotators in a tense annota-
tion experiment to be even greater than for word
sense disambiguation. Nevertheless, the tense an-
notation experiment carried as a precursor to our
tense classification task showed a kappa Statistic
of 0.723 on the full taxonomy, with an observed
agreement of 0.798. In those experiments, we
asked three bilingual English native speakers who
are fluent in Chinese to annotate the English verb
tenses for the first 25 Chinese and English parallel
news articles from our training data.
We could also obtain a measurement of reliabil-
ity by taking one annotator as the gold standard
at one time, then averaging over the precisions of
the different annotators across different gold stan-
dards. While it is true that numerically, precision
would be higher than Kappa score and seems to
be inflating Kappa score, we argue that the dif-
ference between Kappa score and precision is not
limited to one measure being more aggressive than
the other. Rather, the policies of these two mea-
surements are different. The Kappa score cares
purely about agreement without any consideration
of trueness or falseness, while the procedure we
described above gives equal weight to each anno-
tator being the gold standard, and therefore con-
siders both agreement and truthness of the annota-
tion. The advantage of the precision-based agree-
ment measurement is that it makes comparison of
the system performance accuracy to the human
performance accuracy more direct. The precision
under such a scheme for the three annotators is
80% on the full tense taxonomy.
5.2.2 CRF Learning Experiments
We train a tense classifier on our data set in two
stages: first on the surface features, and then on
the combined space of both surface features (dis-
cussed in 4.1) and latent features (discussed in 4.2-
4.4). It is conceivable that the granularity of se-
quences may matter in learning from data with se-
quential relationship, and in the context of verb
tense tagging, it naturally maps to the granularity
of discourse. (Ye, et al, 2005) shows that there
is no significant difference between sentence-level
sequences and paragraph-level sequences. There-
fore, we experiment with only sentence-level se-
quences.
5.2.3 Classification Tree Learning
Experiments
To verify the stability of the utility of the la-
tent features, we also experiment with classifica-
tion tree learning on the same features space as
52
Tense Precision Recall F
Present tense 0.662 0.661 0.627
Past tense 0.882 0.915 0.896
Future tense 0.758 0.487 0.572
Table 1: Evaluation Results for CRFs Classifier in Precision, Recall and F Using All Features
Surface Features Latent Features Surface and Latent Features
Accuracy for Training Data 79.3% 82.9% 85.9%
Table 2: Apparent Accuracy for the Training Data of the Classification Tree Classifiers
discussed above. Classification Trees are used
to predict membership of cases or objects in the
classes of a categorical dependent variable from
their measurements on one or more predictor vari-
ables. The main idea of Classification Tree is to
do a recursive partitioning of the variable space
to achieve good separation of the classes in the
training dataset. We use the Recursive Partition-
ing and Regression Trees(Rpart) package provided
by R statistical computing software for the imple-
mentation of classification trees. In order to avoid
over-fitting, we prune the tree by setting the min-
imum number of objects in a node to attempt a
split and the minimum number of objects in any
terminal node to be 10 and 3 respectively. In the
constructed classification tree when we use all fea-
tures including both surface and latent features,
the top split at the root node in the tree is based
on telicity feature of the English verb, indicating
the importance of telicity feature for English verb
among all of the features.
5.3 Evaluation Results
All results are obtained by 5-fold cross validation.
The classifier?s performance is evaluated against
the tenses from the best-ranked human-generated
English translation. To evaluate the performance
of the CRFs tense classifier, we compute the pre-
cision, recall, general accuracy and F, which are
defined as follow.
Accuracy =
nprediction
Nprediction
(3)
Recall = nhit
S
(4)
Precision = nhit
Nhit
(5)
F = 2? Precision ?Recall
Precision + Recall
(6)
where
1. Nprediction: Total number of predictions;
2. nprediction: Number of correct predictions;
3. Nhit: Total number of hits;
4. nhit: Number of correct hits;
5. S: Size of perfect hitlist;
From Table 1, we see that past tense, which oc-
curs most frequently in the training data, has the
highest precision, recall and F. Future tense, which
occurs least frequently, has the lowest F. Precision
and recall do not show clear pattern across differ-
ent tense classes.
Table 2 presents the apparent classification ac-
curacies for the training data, we see that latent
features still outperform the surface features. Ta-
ble 3 summarizes the general accuracies of the
tense classification systems for CRFs and Classifi-
cation Trees. The CRFs classifier and the Classifi-
cation Tree classifier demonstrate similar scales of
improvement from surface features, latent features
to both surface and latent features.
53
Methodology Surface Features Latent Features Surface and Latent Features
CRFs 75.8% 80% 83.4%
Classification Tree 74.1% 81% 84.5%
Table 3: Evaluations in General Accuracy
5.4 Baseline Systems
To better evaluate our tense classifiers, we provide
two baseline systems here. The first baseline sys-
tem is the tense resolution from the best ranked
machine translation system?s translation results in
the MTC corpus mentioned above. When evalu-
ated against the reference tense tags from the best
ranked human translation team, the best MT sys-
tem yields a accuracy of 47%. The second base-
line system is a naive system that assigns the most
frequent tense in the training data set, which in our
case is past tense, to all verbs in the test data set.
Given the fact that we are deadling with newswire
data, this baseline system yields a high baseline
system with an accuracy of 69.5%.
6 Discussion and Conclusions
To the best of our knowledge, the current paper
is the first work investigating the utility of latent
features in the task of machine-learning based au-
tomatic tense classification. We significantly out-
perform the two baseline systems as well as the
automatic tense classifier performance reported by
(Ye, et al, 2005) by 15% in general accuracy. A
crucial finding of our experiments is that utility of
only three latent features, i.e. verb telicity, verb
punctuality and temporal ordering between adja-
cent events, outperforms that of all the surface
linguistic features we discussed earlier in the pa-
per. While one might think that the lack of exist-
ing techonology of latent feature extraction would
discount research effort on latent features? utili-
ties, we believe that such efforts guide the research
community to determine where to focus effort on
developing automatic extraction methods for fea-
tures that are beyond the reach of current tech-
nologies. Such research effort will also help to
shed light on the enigmatic research question of
whether automatic NLP systems should take ef-
fort to make use of the features employed by hu-
man beings to optimize the system performance
and shorten the gap between the system and hu-
man brain. The results of the current paper point
to the fact that bottleneck of cross-linguistic tense
classification is acquisition and modeling of the
more latent linguistic knowledge. To our surprise,
CRF tense classifier performance is consistently
tied with classification tree tense classifier perfor-
mance in all of our experiments. One might expect
that CRFs would accurately capture sequential de-
pendencies among verbs. Reflecting upon the sim-
ilar evaluation results of the CRFs classifier and
the Classification Tree classifier, it is unlikely for
this to be due to the over-fitting of the Classifi-
cation Tree because of the pruning we did to the
Classification Trees. Therefore, we speculate that
the dependencies between the tense tags of verbs
in the texts may not be strong enough for CRFs
to outperform Classification Tree. This might also
be contributable to the built-in variable selection
procedures of Classification Trees, which makes
it more robust to interacting and interdependent
features. A confirmative explanation towards the
equal performances between the CRFs and the
Classification Tree classifiers requires more exper-
iments with other machine learning algorithms.
In conclusion, this paper makes the following
contributions:
1. It demonstrates that an accurate tense classi-
fier can be constructed automatically by com-
bining off-the-shelf machine learning tech-
niques and inexpensive linguistic features.
2. It shows that latent features (such as verb
telicity, verb punctuality and temporal order-
ing between adjacent events) have higher util-
ity in tense classification than the surface lin-
guistic features.
3. It reveals that the sequential dependency be-
tween tenses of adjacent verbs in the dis-
course may be rather weak.
54
7 Future Work
Temporal reference is a complicated semantic do-
main with rich connections among the disparate
features. We investigate three latent features:
telicity, punctuality, and temporal ordering be-
tween adjacent verbs. We summarize several in-
teresting questions for future research in this sec-
tion. First, besides the latent features we examined
in the current paper, there are other interesting
latent features to be investigated under the same
theme, e.g. classes of temporal expression associ-
ated with the verbs and causal relationships among
disparate events. Second, currently, the latent fea-
tures are obtained through manual annotation by
a single annotator. In an ideal situation, multi-
ple annotators are desired to provide the reliabil-
ity of the annotations as well as reduce the noise
in annotations. Thirdly, it would be interesting to
examine the utility of the same latent features for
classification in the opposite direction, namely, as-
pect marker classification for Chinese verbs in the
English-to-Chinese translation scenario. Finally,
following our discussion of the degree of depen-
dencies among verb tenses in the texts, it is desir-
able to study rigorously the dependencies among
tenses and aspect markers for verbs in extensions
of the current research.
References
James Allen. 1981. Towards a General Theory of Ac-
tion and Time. Artificial Intelligence, 23(2): 123-
160.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Macmillan, New York, N.Y.
B. Bruce. 1972. A Model for Temporal Reference and
its Application in a Question Answering System, Ar-
tificial Intelligence. Vol. 3, No. 1, 1-25.
Inderjeet Mani, James Pustejovsky and Robert
Gaizauskas. 2005. The Language of Time, Oxford
Press.
Wenjie Li, Kam-Fai Wong, Caogui Hong, Chunfa
Yuan. 2004. Applying Machine Learning to Chinese
Temporal Relation Resolution. Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics, 582-588.
Mari Olson, David Traum, Carol Van-ess Dykema,
and AmyWeinberg. 2001. Implicit Cues for Explicit
Generation: Using Telicity as a Cue for Tense Struc-
ture in a Chinese to English MT System. Proceed-
ings Machine Translation Summit VIII, Santiago de
Compostela, Spain.
Yang Ye, Zhu Zhang. 2005. Tense Tagging for Verbs in
Cross-Lingual Context: a Case Study. Proceedings
of IJCNLP 2005, 885-895
Norbert Hornstein. 1990. As Time Goes By: Tense and
Universal Grammar. The MIT Press.
James Pustejovsky, Robert Ingria, Roser Sauri, Jose
Castano, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz, and Inderjeet Mani. 2004. The
Specification Language TimeML. The Language of
Time: A Reader. Oxford, 185-96.
Zeno Vendler. 1967. Verbs and Times. Linguistics in
Philosophy, 97-121.
Lafferty, J., McCallum, A. and Pereira, F. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML-01, 282-289.
Sha, F. and Pereira, F. 2003. Shallow Parsing with
Conditional Random Fields. Proceedings of the
2003 Human Language Technology Conference and
North American Chapter of the Association for
Computational Linguistics (HLT/NAACL-03)
Pinto, D., McCallum, A., Lee, X. and Croft, W. B.
2003. Table Extraction Using Conditional Random
Fields. Proceedings of the 26th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval (SIGIR 2003)
McCallum, A. and Li, W. 2003. Early Results for
Named Entity Recognition with Conditional Ran-
dom Fields, Feature Induction and Web-Enhanced
Lexicons. Proceedings of the Seventh Conference on
Natural Language Learning (CoNLL)
McCallum, A. K. 2002. MALLET: A Ma-
chine Learning for Language Toolkit,
http://mallet.cs.umass.edu.
Jacob Cohen, 1960. A Coefficient of Agreement for
Nominal Scales, Educational and Psychological
Measurement, 20, 37-46.
Ross Ihaka and Robert Gentleman. 1996. R: A Lan-
guage for Data Analysis and Graphics, Journal
of Computational and Graphical Statistics, Vol. 5.
299?14.
55
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 13?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
How and Where do People Fail with Time: Temporal Reference Mapping
Annotation by Chinese and English Bilinguals
Yang Ye?, Steven Abney??
?Department of Linguistics
?Department of Electrical Engineering and Computer Science
University of Michigan
Abstract
This work reports on three human tense
annotation experiments for Chinese verbs
in Chinese-to-English translation scenar-
ios. The results show that inter-annotator
agreement increases as the context of the
verb under the annotation becomes in-
creasingly specified, i.e. as the context
moves from the situation in which the tar-
get English sentence is unknown to the
situation in which the target lexicon and
target syntactic structure are fully speci-
fied. The annotation scheme with a fully
specified syntax and lexicon in the tar-
get English sentence yields a satisfactorily
high agreement rate. The annotation re-
sults were then analyzed via an ANOVA
analysis, a logistic regression model and a
log-linear model. The analyses reveal that
while both the overt and the latent linguis-
tic factors seem to significantly affect an-
notation agreement under different scenar-
ios, the latent features are the real driving
factors of tense annotation disagreement
among multiple annotators. The analy-
ses also find the verb telicity feature, as-
pect marker presence and syntactic em-
bedding structure to be strongly associated
with tense, suggesting their utility in the
automatic tense classification task.
1 Introduction
In recent years, the research community has seen
a fast-growing volume of work in temporal infor-
mation processing. Consequently, the investiga-
tion and practice of temporal information anno-
tation by human experts have emerged from the
corpus annotation research. To evaluate automatic
temporal relation classification systems, annotated
corpora must be created and validated, which mo-
tivates experiments and research in temporal infor-
mation annotation.
One important temporal relation distinction that
human beings make is the temporal reference dis-
tinction based on relative positioning between the
following three time parameters, as proposed by
(Reichenbach, 1947): speech time (S), event time
(E) and reference time (R). Temporal reference
distinction is linguistically realized as tenses. Lan-
guages have various granularities of tense repre-
sentations; some have finer-grained tenses or as-
pects than others. This poses a great challenge to
automatic cross-lingual tense mapping. The same
challenge holds for cross-lingual tense annotation,
especially for language pairs that have dramati-
cally different tense strategies. A decent solution
for cross-lingual tense mapping will benefit a va-
riety of NLP tasks such as Machine Translation,
Cross-lingual Question Answering (CLQA), and
Multi-lingual Information Summarization. While
automatic cross-lingual tense mapping has re-
cently started to receive research attention, such
as in (Olsen,et al, 2001) and (Ye, et al, 2005),
to the best of our knowledge, human performance
on tense and aspect annotation for machine trans-
lation between English and Chinese has not re-
ceived any systematic investigation to date. Cross-
linguistic NLP tasks, especially those requiring a
more accurate tense and aspect resolution, await
a more focused study of human tense and aspect
annotation performance.
Chinese and English are a language pair in
which tense and aspect are represented at differ-
ent levels of units: one being realized at the word
level and the other at the morpheme level.
This paper reports on a series of cross-linguistic
tense annotation experiments between Chinese
and English, and provides statistical inference for
different linguistic factors via a series of statisti-
cal modeling. Since tense and aspect are mor-
phologically merged in English, tense annotation
13
discussed in this paper also includes elements of
aspect. We only deal with tense annotation in
Chinese-to-English scenario in the scope of this
paper.
The remaining part of the paper is organized
as follows: Section 2 summarizes the significant
related works in temporal information annotation
and points out how this study relates to yet differs
from them. Section 3 reports the details of three
tense annotation experiments under three scenar-
ios. Section 4 discusses the inter-judge agree-
ment by presenting two measures of agreement:
the Kappa Statistic and accuracy-based measure-
ment. Section 5 investigates and reports on the
significance of different linguistic factors in tense
annotation via an ANOVA analysis, a logistic re-
gression analysis and a log-linear model analysis.
Finally, section 6 concludes the paper and points
out directions for future research.
2 Related Work
There are two basic types of temporal location re-
lationships. The first one is the ternary classifica-
tion of past, present and future. The second one
is the binary classification of ?BEFORE? versus
?AFTER?. These two types of temporal relation-
ships are intrinsically related but each stands as a
separate issue and is dealt with in different works.
While the ?BEFORE? versus ?AFTER? relation-
ship can easily be transferred across a language
pair, the ternary tense taxonomy is often very hard
to transfer from one language to another.
(Wilson, et al, 1997) describes a multilin-
gual approach to annotating temporal information,
which involves flagging a temporal expression in
the document and identifying the time value that
the expression designates. Their work reports an
inter-annotator reliability F-measure of 0.79 and
0.86 respectively for English corpora.
(Katz, et al, 2001) describes a simple and gen-
eral technique for the annotation of temporal rela-
tion information based on binary interval relation
types: precedence and inclusion. Their annotation
scheme could benefit a range of NLP applications
and is easy to carry out.
(Pustejovsky et al, 2004) reports an annotation
scheme, the TimeML metadata, for the markup of
events and their anchoring in documents. The an-
notation schema of TimeML is very fine-grained
with a wide coverage of different event types, de-
pendencies between events and times, as well as
?LINK? tags which encode the various relations
existing between the temporal elements of a doc-
ument. The challenge of human labeling of links
among eventualities was discussed at great length
in their paper. Automatic ?time-stamping? was
attempted on a small sample of text in an earlier
work of (Mani, 2003). The result was not partic-
ularly promising. It showed the need for a larger
quantity of training data as well as more predictive
features, especially on the discourse level. At the
word level, the semantic representation of tenses
could be approached in various ways depending
on different applications. So far, their work has
gone the furthest towards establishing a broad and
open standard metadata mark-up language for nat-
ural language texts.
(Setzer, et al, 2004) presents a method of eval-
uating temporal order relation annotations and an
approach to facilitate the creation of a gold stan-
dard by introducing the notion of temporal clo-
sure, which can be deduced from any annotations
through using a set of inference rules.
From the above works, it can be seen that the
effort in temporal information annotation has thus
far been dominated by annotating temporal rela-
tions that hold entities such as events or times
explicitly mentioned in the text. Cross-linguistic
tense and aspect annotation has so far gone un-
studied.
3 Chinese Tense Annotation
Experiments1
In current section, we present three tense annota-
tion experiments with the following scenarios:
1. Null-control situation by native Chinese
speakers where the annotators were provided
with the source Chinese sentences but not the
English translations;
2. High-control situation by native English
speakers where the annotators were provided
with the Chinese sentences as well as English
translations with specified syntax and lexi-
cons;
3. Semi-control situation by native English
speakers where the annotators were allowed
to choose the syntax and lexicons for the En-
glish sentence with appropriate tenses;
1All experiments in the paper are approved by Behav-
ioral Sciences Institutional Review Board at the University
of Michigan, the IRB file number is B04-00007481-I.
14
3.1 Experiment One
Experiment One presents the first scenario of
tense annotation for Chinese verbs in Chinese-to-
English cross-lingual situation. In the first sce-
nario, the annotation experiment was carried out
on 25 news articles from LDC Xinhua News re-
lease with category number LDC2001T11. The ar-
ticles were divided into 5 groups with 5 articles in
each group. There are a total number of 985 verbs.
For each group, three native Chinese speakers who
were bilingual in Chinese and English annotated
the tense of the verbs in the articles independently.
Prior to annotating the data, the annotators under-
went brief training during which they were asked
to read an example of a Chinese sentence for each
tense and make sure they understand the exam-
ples. During the annotation, the annotators were
asked to read the whole articles first and then se-
lect a tense tag based on the context of each verb.
The tense taxonomy provided to the annotators in-
clude the twelve tenses that are different combi-
nations of the simple tenses (present, past and fu-
ture), the prograssive aspect and the perfect aspect.
In cases where the judges were unable to decide
the tense of a verb, they were instructed to tag it
as ?unknown?. In this experiment, the annotators
were asked to tag the tense for all Chinese words
that were tagged as verbs in the Penn Treebank
corpora. Conceivably, the task under the current
scenario is meta-linguistic in nature for the reason
that tense is an elusive notion for Chinese speak-
ers. Nevertheless, the experiment provides a base-
line situation for human tense annotation agree-
ment. The following is an example of the anno-
tation where the annotators were to choose an ap-
propriate tense tag from the provided tense tags:
((IP (NP-TPC (NP-PN (NR ??))(NP (NN ??)(NN??)))(LCP-TMP (NP (NT ?
?))(LC?)) (NP-SBJ (NP (PP (P ?)(NP (NN ?)))(NP (NN ??)))(NP (NN ?
?)))(VP (ADVP (AD ???)) (VP (VV??)))(PU?)) ) 
1. simple present tense
2. simple past tense
3. simple future tense
4. present perfect tense
5. past perfect tense
6. future perfect tense
7. present progressive tense
8. past progressive tense
9. future progressive
10. present perfect progressive
11. past perfect progressive
3.2 Experiment Two
Experiment Two was carried out using 25 news
articles from the parallel Chinese and English
news articles available from LDC Multiple Trans-
lation Chinese corpora (MTC catalog number
LDC2002T01). In the previous experiment, the
annotators tagged all verbs. In the current experi-
mental set-up, we preprocessed the materials and
removed those verbs that lose their verbal status in
translation from Chinese to English due to nom-
inalization. After this preprocessing, there was
a total of 288 verbs annotated by the annotators.
Three native speakers, who were bilingually fluent
in English and Chinese, were recruited to annotate
the tense for the English verbs that were translated
from Chinese. As in the previous scenario, the an-
notators were encouraged to pay attention to the
context of the target verb when tagging its tense.
The annotators were provided with the full taxon-
omy illustrated by examples of English verbs and
they worked independently. The following is an
example of the annotation where the annotators
were to choose an appropriate tense tag from the
provided tense tags:
?????????????????????????????????????
?????
According to statistics, the cities (achieve) a combined gross domestic product of RMB19 
billion last year, an increase of more than 90% over 1991 before their opening. 
A. achieves
B. achieved 
C. will achieve 
D. are achieving 
E. were achieving 
F. will be achieving 
G. have achieved 
H. had achieved 
I. will have achieved 
J. have been achieving
K. had been achieving
L. will have been achieving
M. would achieve
3.3 Experiment Three
Experiment Three was an experiment simulated
on 52 Xinhua news articles from the Multiple
Translation Corpus (MTC) mentioned in the pre-
vious section. Since in the MTC corpora, each
Chinese article is translated into English by ten
human translation teams, conceptually, we could
view these ten translation teams as different an-
notators. They were making decisions about ap-
propriate tense for the English verbs. These an-
notators differ from those in Experiment Two de-
scribed above in that they were allowed to choose
any syntactic structure and verb lexicon. This is
because they were performing tense annotation in
a bigger task of sentence translation. Therefore,
their tense annotations were performed with much
less specification of the annotation context. We
manually aligned the Chinese verbs with the En-
glish verbs for the 10 translation teams from the
MTC corpora and thus obtained our third source
of tense annotation results. For the Chinese verbs
15
that were not translated as verbs into English, we
assigned a ?Not Available? tag. There are 1505
verbs in total including the ones that lost their ver-
bal status across the language.
4 Inter-Judge Agreement
Researchers use consistency checking to validate
human annotation experiments. There are vari-
ous ways of performing consistency checking de-
scribed in the literature, depending on the scale of
the measurements. Each has its advantages and
disadvantages. Since our tense taxonomy is nomi-
nal without any ordinal information, Kappa statis-
tics measurement is the most appropriate choice to
measure inter-judge agreement.
4.1 Kappa Statistic
Kappa scores were calculated for the three human
judges? annotation results. The Kappa score is the
de facto standard for evaluating inter-judge agree-
ment on tagging tasks. It reports the agreement
rate among multiple annotators while correcting
for the agreement brought about by pure chance.
It is defined by the following formula, where P(A)
is the observed agreement among the judges and
P(E) is the expected agreement:
k =
P (A)? P (E)
1? P (E)
(1)
Depending on how one identifies the expected
agreement brought about by pure chance, there are
two ways to calculate the Kappa score. One is the
?Seigel-Castellian? Kappa discussed in (Eugenio,
2004), which assumes that there is one hypotheti-
cal distribution of labels for all judges. In contrast,
the ?Cohen? Kappa discussed in (Cohen, 1960),
assumes that each annotator has an individual dis-
tribution of labels. This discrepancy slightly af-
fects the calculation of P(E). There is no consen-
sus regarding which Kappa is the ?right? one and
researchers use both. In our experiments, we use
the ?Seigel-Castellian? Kappa.
The Kappa statistic for the annotation results of
Experiment One are 0.277 on the full taxonomy
and 0.37 if we collapse the tenses into three big
classes: present, past and future. The observed
agreement rate,that is, P(A), is 0.42.
The Kappa score for tense resolution from the
ten human translation teams for the 52 Xinhua
news articles is 0.585 on the full taxonomy; we
expect the Kappa score to be higher if we exclude
the verbs that are nominalized. Interestingly, the
Kappa score calculated by collapsing the 13 tenses
into 3 tenses (present, past and future) is only
slightly higher: 0.595. The observed agreement
rate is 0.72.
Human tense annotation in the Chinese-to-
English restricted translation scenario achieved a
Kappa score of 0.723 on the full taxonomy with an
observed agreement of 0.798. If we collapse sim-
ple past and present perfect, the Kappa score goes
up to 0.792 with an observed agreement of 0.893.
The Kappa score is 0.81 on the reduced taxonomy.
4.2 Accuracy
The Kappa score is a relatively conservative mea-
surement of the inter-judge agreement rate. Con-
ceptually, we could also obtain an alternative mea-
surement of reliability by taking one annotator as
the gold standard at one time and averaging over
the accuracies of the different annotators across
different gold standards. While it is true that nu-
merically, this would yield a higher score than the
Kappa score and seems to be inflating the agree-
ment rate, we argue that the difference between
the Kappa score and the accuracy-based measure-
ment is not limited to one being more aggressive
than the other. The policies of these two mea-
surements are different. The Kappa score is con-
cerned purely with agreement without any consid-
eration of truthfulness or falsehood, while the pro-
cedure we described above gives equal weights to
each annotator being the gold standard. Therefore,
it considers both the agreement and the truthful-
ness of the annotation. Additionally, the accuracy-
based measurement is the same measurement that
is typically used to evaluate machine performance;
therefore it gives a genuine ceiling for machine
performance.
The accuracy under such a scheme for the three
annotators in Experiment One is 43% on the full
tense taxonomy.
The accuracy under such a scheme for tense
generation agreement from three annotators in Ex-
periment Two is 80% on the full tense taxonomy.
The accuracy under such a scheme for the ten
translation teams in Experiment Three is 70.8% on
the full tense taxonomy.
Table 1 summarizes the inter-judge agreement
for the three experiments.
Examining the annotation results, we identified
the following sources of disagreement. While the
16
Agreement Exp 1 Exp 2 Exp 3
Kappa Statistic 0.277 0.723 0.585
Kappa Statistic 0.37 0.81 0.595
(Reduced Taxonomy)
Accuracy 43% 80% 70.8%
Table 1: Inter-Annotator Agreement for the Three
Tense Annotation Experiments
first two factors can be controlled for by a clearly
pre-defined annotation guideline, the last two fac-
tors are intrinsically rooted in natural languages
and therefore hard to deal with:
1. Different compliance with Sequence of Tense
(SOT) principle among annotators;
2. ?Headline Effect?;
3. Ambiguous POS of the ?verb?: sometimes it
is not clear whether a verb is adjective or past
participle. e.g. The Fenglingdu Economic
Development Zone is the only one in China
that is/was built on the basis of a small town.
4. Ambiguous aspectual property of the verb:
the annotator?s view with respect to whether
or not the verb is an atelic verb or a telic verb.
e.g. ?statistics showed/show......?
Put abstractly, ambiguity is an intrinsic property
of natural languages. A taxonomy allows us to
investigate the research problem, yet any clearly
defined discrete taxonomy will inevitably fail on
boundary cases between different classes.
5 Significance of Linguistic Factors in
Annotation
In the NLP community, researchers carry out an-
notation experiments mainly to acquire a gold
standard data set for evaluation. Little effort has
been made beyond the scope of agreement rate
calculations. We propose that not only does fea-
ture analysis for annotation experiments fall un-
der the concern of psycholinguists, it also merits
investigation within the enterprise of natural lan-
guage processing. There are at least two ways
that the analysis of annotation results can help
the NLP task besides just providing a gold stan-
dard: identifying certain features that are respon-
sible for the inter-judge disagreement and model-
ing the situation of associations among the differ-
ent features. The former attempts to answer the
Figure 1: Interaction between Aspect Marker and
Temporal Modifier
question of where the challenge for human classi-
fication comes from, and thereby provides an ex-
ternal reference for an automatic NLP system, al-
though not necessarily in a direct way. The latter
sheds light on the structures hidden among groups
of features, the identification of which could pro-
vide insights for feature selection as well as of-
fer convergent evidence for the significance of cer-
tain features confirmed from classification practice
based on machine learning.
In this section, we discuss at some length a fea-
ture analysis for the results of each of the anno-
tation experiments discussed in the previous sec-
tions and summarize the findings.
5.1 ANOVA analysis of Agreement and
Linguistic Factors in Free Translation
Tense Annotation
This analysis tries to find the relationship be-
tween the linguistic properties of the verb and the
tense annotation agreement across the ten different
translation teams in Experiment Three. Specifi-
cally, we use an ANOVA analysis to explore how
the overall variance in the inconsistency of the
tenses of a particular verb with respect to differ-
ent translation teams can be attributed to different
linguistic properties associated with the Chinese
verb. It is a three-way ANOVA with three linguis-
tic factors under investigation: whether the sen-
tence contains a temporal modifier or not; whether
the verb is embedded in a relative clause, a senten-
tial complement, an appositive clause or none of
the above; and whether the verb is followed by as-
pect markers or not. The dependent variable is the
inconsistency of the tenses from the teams. The
17
inconsistency rate is measured by the ratio of the
number of distinct tenses over the number of tense
tokens from the ten translation teams.
Our ANOVA analysis shows that all of the three
main effects, i.e. the embedding structures of the
verb (p  0.001), the presence of aspect markers
(p  0.01), and the presence of temporal mod-
ifiers (p < 0.05) significantly affect the rate of
disagreement in tense generation among the dif-
ferent translation teams. The following graphs
show the trend: tense generation disagreement
rates are consistently lower when the Chinese as-
pect marker is present, whether there is a temporal
modifier present or not (Figure 1). The model also
suggested that the presence of temporal modifiers
is associated with a lower rate of disagreement
for three embedding structures except for verbs in
sentential complements (Figure 2, 0: the verb is
not in any embedding structures; 1: the verb is
embedded in a relative clause; 2: the verb is em-
bedded in an appositive clause; 3: the verb is em-
bedded in sentential complement). Our explana-
tion for this is that the annotators receive varying
degrees of prescriptive writing training, so when
there is a temporal modifier in the sentence as a
confounder, there will be a larger number, a higher
incidence of SOT violations than when there is
no temporal modifier present in the sentence. On
top of this, the rate of disagreement in tense tag-
ging between the case where a temporal modifier
is present in the sentence and the case where it is
not depends on different types of embedding struc-
tures (Figure 2, p value < 0.05).
We also note that the relative clause embed-
ding structure is associated with a much higher
disagreement rate than any other embedding struc-
tures (Figure 3).
5.2 Logistic Regression Analysis of
Agreement and Linguistic Factors in
Restricted Tense Annotation
The ANOVA analysis in the previous section is
concerned with the confounding power of the
overt linguistic features. The current section ex-
amines the significance of the more latent fea-
tures on tense annotation agreement when the SOT
effect is removed by providing the annotators a
clear guideline about the SOT principle. Specif-
ically, we are interested in the effect of verb telic-
ity and punctuality features on tense annotation
agreement. The telicity and punctuality features
Figure 2: Interaction between the Temporal Mod-
ifier and the Syntactic Embedding Structure
were obtained through manual annotation based
on the situation in the context. The data are from
Experiment Two. Since there are only three an-
notators, the inconsistency rate we discussed in
5.1 would have insufficient variance in the current
scenario, making logistic regression a more appro-
priate analysis. The response is now binary being
either agreement or disagreement (including par-
tial agreement and pure disagreement). To avoid a
multi-colinearity problem, we model Chinese fea-
tures and English features separately. In order
to truly investigate the effects of the latent fea-
tures, we keep the overt linguistic features in the
model as well. The overt features include: type of
syntactic embedding, presence of aspect marker,
presence of temporal expression in the sentence,
whether the verb is in a headline or not, and the
presence of certain signal adverbs including ?yi-
jing?(already), ?zhengzai? (Chinese pre-verb pro-
gressive marker), ?jiang?(Chinese pre-verbal ad-
verb indicating future tense). We used backward
elimination to obtain the final model.
The result showed that punctuality is the only
factor that significantly affects the agreement rate
among multiple judges in both the model of En-
glish features and the model of Chinese features.
The significance level is higher for the punctuality
of English verbs, suggesting that the source lan-
guage environment is more relevant in tense gener-
ation. The annotators are roughly four times more
likely to fail to agree on the tense for verbs as-
sociated with an interval event. This supports the
hypothesis that human beings use the latent fea-
tures for tense classification tasks. Surprisingly,
the telicity feature is not significant at all. We sus-
18
Figure 3: Effect of Syntactic Embedding Structure
on Tense Annotation Disagreement
pect this is partly due to the correlation between
the punctuality feature and the telicity feature. Ad-
ditionally, none of the overt linguistic features is
significant in the presence of the latent features,
which implies that the latent features drive dis-
agreement among multiple annotators.
5.3 Log-linear Model Analysis of
Associations between Linguistic Factors
in Free Translation Tense Annotation
This section discusses the association patterns be-
tween tense and the relevant linguistic factors via
a log-linear model. A log-linear model is a special
case of generalized linear models (GLMs) and has
been widely applied in many fields of social sci-
ence research for multivariate analysis of categor-
ical data. The model reveals the interaction be-
tween categorical variables. The log-linear model
is different from other GLMs in that it does not
distinguish between ?response? and ?explanatory
variables?. All variables are treated alike as ?re-
sponse variables?, whose mutual associations are
explored. Under the log-linear model, the ex-
pected cell frequencies are functions of all vari-
ables in the model. The most parsimonious model
that produces the smallest discrepancy between
the expected cell and the observed cell frequen-
cies is chosen as the final model. This provides
the best explanation of the observed relationships
among variables.
We use the data from Experiment Two for the
current analysis. The results show that three lin-
guistic features under investigation are signifi-
cantly associated with tense. First, there is a strong
association between aspect marker presence and
tense, independent of punctuality, telicity feature
and embedding structure. Second, there is a strong
association between telicity and tense, indepen-
dent of punctuality, aspect marker presence and
punctuality feature. Thirdly, there is a strong as-
sociation between embedding structure and tense,
independent of telicity, punctuality feature and as-
pect marker presence. This result is consistent
with (Olsen, 2001), in that the lexical telicity fea-
ture, when used heuristically as the single knowl-
edge source, can achieve a good prediction of verb
tense in Chinese to English Machine Translation.
For example, the odds of the verb being atelic in
the past tense is 2.5 times the odds of the verb
being atelic in the future tense, with a 95% con-
fidence interval of (0.9, 7.2). And the odds of a
verb in the future tense having an aspect marker
approaches zero when compared to the odds of a
verb in the past tense having an aspect marker.
Putting together the pieces from the logistic
analysis and the current analysis, we see that an-
notators fail to agree on tense selection mostly
with apunctual verbs, while the agreed-upon tense
is jointly decided by the telicity feature, aspect
marker feature and the syntactic embedding struc-
ture that are associated with the verb.
6 Conclusions and Future Work
As the initial attempt to assess human beings?
cross-lingual tense annotation, the current paper
carries out a series of tense annotation experi-
ments between Chinese and English under differ-
ent scenarios. We show that even if tense is an
abstract grammatical category, multiple annotators
are still able to achieve a good agreement rate
when the target English context is fully specified.
We also show that in a non-restricted scenario,
the overt linguistic features (aspect markers, em-
bedding structures and temporal modifiers), can
cause people to fail to agree with each other signif-
icantly in tense annotation. These factors exhibit
certain interaction patterns in the decision mak-
ing of the annotators. Our analysis of the anno-
tation results from the scenario with a fully speci-
fied context show that people tend to fail to agree
with each other on tense for verbs associated with
interval events. The disagreement seems not to
be driven by the overt linguistic features such as
embedding structure and aspect markers. Lastly,
among a set of overt and latent linguistic features,
aspect marker presence, embedding structure and
19
the telicity feature exhibit the strongest association
with tense, potentially indicating their high utility
in tense classification task.
The current analysis, while suggesting certain
interesting patterns in tense annotation, could be
more significant if the findings could be replicated
by experiments of different scales on different data
sets. Furthermore, the statistical analysis could be
more finely geared to capture the more subtle dis-
tinctions encoded in the features.
AcknowledgementAll of the annotation exper-
iments in this paper are funded by Rackham Grad-
uate School?s Discretionary Funds at the Univer-
sity of Michigan.
References
Hans Reichenbach,1947. Elements of Symbolic Logic,
Macmillan, New York, N.Y.
Mari Olson, David Traum, Carol Van Ess-Dykema,
and AmyWeinberg, 2001. Implicit Cues for Explicit
Generation: Using Telicity as a Cue for Tense Struc-
ture in a Chinese to English MT System, Proceed-
ings Machine Translation Summit VIII, Santiago de
Compostela, Spain.
Yang Ye, Zhu Zhang, 2005. Tense Tagging for Verbs
in Cross-Lingual Context: A Case Study. Proceed-
ings of 2nd International Joint Conference in Natural
Language Processing (IJCNLP), 885-895.
George Wilson, Inderjeet Mani, Beth Sundheim, and
Lisa Ferro, 2001. A Multilingual Approach to An-
notating and Extracting Temporal Information, Pro-
ceedings of the ACL 2001 Workshop on Temporal
And Spatial Information Processing, 39th Annual
Meeting of ACL, Toulouse, 81-87.
Graham Katz and Fabrizio Arosio, 2001. The Annota-
tion of Temporal Information in Natural Language
Sentences, Proceedings of the ACL 2001 Workshop
on Temporal And Spatial Information Processing,
39th Annual Meeting of ACL, Toulouse, 104-111.
James Pustejovsky, Robert Ingria, Roser Sauri, Jose
Castano, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz, and Inderjeet Mani. 2004. The
Specification Language TimeML. The Language of
Time: A Reader. Oxford, 185-96.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: A second look. Computational Lin-
guistics, 30(1): 95-101.
Inderjeet Mani, 2003. Recent Developments in Tem-
poral Information Extraction. In Nicolov, N. and
Mitkov, R., editors, Proceedings of RANLP?03.
John Benjamins.
Andrea Setzer, Robert Gaizauskas, and Mark Hep-
ple, 2003. Using Semantic Inferences for Tem-
poral Annotation Comparison, Proceedings of the
Fourth International Workshop on Inference in
Computational Semantics (ICOS-4), INRIA, Lor-
raine, Nancy, France, September 25-26, 185-96.
Jacob Cohen, 1960. A Coefficient of Agreement for
Nominal Scales, Educational and Psychological
Measurement, 20, 37-46.
20
Proceedings of the Third Workshop on Statistical Machine Translation, pages 44?52,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Syntax to Improve Word Alignment Precision for Syntax-Based
Machine Translation
Victoria Fossum
Dept. of Computer Science
University of Michigan
Ann Arbor, MI 48104
vfossum@umich.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Steven Abney
Dept. of Linguistics
University of Michigan
Ann Arbor, MI 48104
abney@umich.edu
Abstract
Word alignments that violate syntactic cor-
respondences interfere with the extraction
of string-to-tree transducer rules for syntax-
based machine translation. We present an
algorithm for identifying and deleting incor-
rect word alignment links, using features of
the extracted rules. We obtain gains in both
alignment quality and translation quality in
Chinese-English and Arabic-English transla-
tion experiments relative to a GIZA++ union
baseline.
1 Introduction
1.1 Motivation
Word alignment typically constitutes the first stage
of the statistical machine translation pipeline.
GIZA++ (Och and Ney, 2003), an implementation
of the IBM (Brown et al, 1993) and HMM (?)
alignment models, is the most widely-used align-
ment system. GIZA++ union alignments have been
used in the state-of-the-art syntax-based statistical
MT system described in (Galley et al, 2006) and in
the hierarchical phrase-based system Hiero (Chiang,
2007). GIZA++ refined alignments have been used
in state-of-the-art phrase-based statistical MT sys-
tems such as (Och, 2004); variations on the refined
heuristic have been used by (Koehn et al, 2003)
(diag and diag-and) and by the phrase-based system
Moses (grow-diag-final) (Koehn et al, 2007).
GIZA++ union alignments have high recall but
low precision, while intersection or refined align-
ments have high precision but low recall.1 There are
two natural approaches to improving upon GIZA++
alignments, then: deleting links from union align-
ments, or adding links to intersection or refined
alignments. In this work, we delete links from
GIZA++ union alignments to improve precision.
The low precision of GIZA++ union alignments
poses a particular problem for syntax-based rule ex-
traction algorithms such as (Quirk et al, 2005; Gal-
ley et al, 2006; Huang et al, 2006; Liu et al,
2006): if the incorrect links violate syntactic corre-
spondences, they force the rule extraction algorithm
to extract rules that are large in size, few in number,
and poor in generalization ability.
Figure 1 illustrates this problem: the dotted line
represents an incorrect link in the GIZA++ union
alignment. Using the rule extraction algorithm de-
scribed in (Galley et al, 2004), we extract the rules
shown in the leftmost column (R1?R4). Rule R1 is
large and unlikely to generalize well. If we delete
the incorrect link in Figure 1, we can extract the
rules shown in the rightmost column (R2?R9): Rule
R1, the largest rule from the initial set, disappears,
and several smaller, more modular rules (R5?R9) re-
place it.
In this work, we present a supervised algorithm
that uses these two features of the extracted rules
(size of largest rule and total number of rules), as
well as a handful of structural and lexical features,
to automatically identify and delete incorrect links
from GIZA++ union alignments. We show that link
1For a complete discussion of alignment symmetrization
heuristics, including union, intersection, and refined, refer to
(Och and Ney, 2003).
44
VP
VBZ
starts
PRT
RP
out
PP
IN
from
NP
NP
DT
the
NNS
needs
PP
IN
of
NP
PRP
its
JJ
own
NN
country
,
? ) ?  ? 
FROM OWN-COUNTRY NEEDS STARTS-OUT
Rules Extracted Using GIZA++ Union Alignments Rules Extracted After Deleting Dotted Link
R1: VP
VBZ
starts
PRT
RP
out
PP
x0:IN NP
NP
DT
the
NNS
needs
x1:PP
? x0 x1 ?  ?  R2: IN
from
? ,
R2: IN
from
? , R3: PP
IN
of
x0:NP
? x0
R3: PP
IN
of
x0:NP
? x0 R4: NP
PRP
its
JJ
own
NN
country
? ? )
R4: NP
PRP
its
JJ
own
NN
country
? ? ) R5: PP
x0:IN x1:NP
?x0 x1
R6: NP
x0:NP x1:PP
? x1 x0
R7: NP
DT
the
x0:NNS
? x0
R8: NNS
needs
? ? 
R9: VP
VBZ
starts
PRT
RP
out
x0:PP
? x0 ? 
Figure 1: The impact of incorrect alignment links upon rule extraction. Using the original alignment (including all
links shown) leads to the extraction of the tree-to-string transducer rules whose left hand sides are rooted at the solid
boxed nodes in the parse tree (R1, R2, R3, and R4). Deleting the dotted alignment link leads to the omission of rule
R1, the extraction of R9 in its place, the extraction of R2, R3, and R4 as before, and the extraction of additional rules
whose left hand sides are rooted at the dotted boxed nodes in the parse tree (R5, R6, R7, R8).
45
deletion improves alignment quality and translation
quality in Chinese-English and Arabic-English MT,
relative to a strong baseline. Our link deletion al-
gorithm is easy to implement, runs quickly, and has
been used by a top-scoring MT system in the Chi-
nese newswire track of the 2008 NIST evaluation.
1.2 Related Work
Recently, discriminative methods for alignment
have rivaled the quality of IBM Model 4 alignments
(Liu et al, 2005; Ittycheriah and Roukos, 2005;
Taskar et al, 2005; Moore et al, 2006; Fraser and
Marcu, 2007b). However, except for (Fraser and
Marcu, 2007b), none of these advances in align-
ment quality has improved translation quality of a
state-of-the-art system. We use a discriminatively
trained model to identify and delete incorrect links,
and demonstrate that these gains in alignment qual-
ity lead to gains in translation quality in a state-
of-the-art syntax-based MT system. In contrast to
the semi-supervised LEAF alignment algorithm of
(Fraser and Marcu, 2007b), which requires 1,500-
2,000 CPU days per iteration to align 8.4M Chinese-
English sentences (anonymous, p.c.), link deletion
requires only 450 CPU hours to re-align such a cor-
pus (after initial alignment by GIZA++, which re-
quires 20-24 CPU days).
Several recent works incorporate syntactic fea-
tures into alignment. (May and Knight, 2007) use
syntactic constraints to re-align a parallel corpus that
has been aligned by GIZA++ as follows: they extract
string-to-tree transducer rules from the corpus, the
target parse trees, and the alignment; discard the ini-
tial alignment; use the extracted rules to construct a
forest of possible string-to-tree derivations for each
string/tree pair in the corpus; use EM to select the
Viterbi derivation tree for each pair; and finally, in-
duce a new alignment from the Viterbi derivations,
using the re-aligned corpus to train a syntax-based
MT system. (May and Knight, 2007) differs from
our approach in two ways: first, the set of possible
re-alignments they consider for each sentence pair is
limited by the initial GIZA++ alignments seen over
the training corpus, while we consider all alignments
that can be reached by deleting links from the ini-
tial GIZA++ alignment for that sentence pair. Sec-
ond, (May and Knight, 2007) use a time-intensive
training algorithm to select the best re-alignment
for each sentence pair, while we use a fast greedy
search to determine which links to delete; in con-
trast to (May and Knight, 2007), who require 400
CPU hours to re-align 330k Chinese-English sen-
tence pairs (anonymous, p.c), link deletion requires
only 18 CPU hours to re-align such a corpus.
(Lopez and Resnik, 2005) and (Denero and Klein,
2007) modify the distortion model of the HMM
alignment model (Vogel et al, 1996) to reflect tree
distance rather than string distance; (Cherry and
Lin, 2006) modify an ITG aligner by introducing
a penalty for induced parses that violate syntac-
tic bracketing constraints. Similarly to these ap-
proaches, we use syntactic bracketing to constrain
alignment, but our work extends beyond improving
alignment quality to improve translation quality as
well.
2 Link Deletion
We propose an algorithm to re-align a parallel bitext
that has been aligned by GIZA++ (IBM Model 4),
then symmetrized using the union heuristic. We then
train a syntax-based translation system on the re-
aligned bitext, and evaluate whether the re-aligned
bitext yields a better translation model than a base-
line system trained on the GIZA++ union aligned
bitext.
2.1 Link Deletion Algorithm
Our algorithm for re-alignment proceeds as follows.
We make a single pass over the corpus. For each sen-
tence pair, we initialize the alignment A = Ainitial
(the GIZA++ union alignment for that sentence
pair). We represent the score of A as a weighted
linear combination of features hi of the alignment
A, the target parse tree parse(e) (a phrase-structure
syntactic representation of e), and the source string
f :
score(A) =
n
?
i=0
?i ? hi(A, parse(e), f)
We define a branch of links to be a contiguous 1-
to-many alignment.2 We define two alignments, A
2In Figure 1, the 1-to-many alignment formed by {? )-
its, ? )- own,? )-country} constitutes a branch, but the
1-to-many alignment formed by {? -starts,? -out,? -
needs} does not.
46
and A?, to be neighbors if they differ only by the
deletion of a link or branch of links. We consider all
alignments A? in the neighborhood of A, greedily
deleting the link l or branch of links b maximizing
the score of the resulting alignment A? = A \ l or
A? = A \ b. We delete links until no further increase
in the score of A is possible.3
In section 2.2 we describe the features hi, and in
section 2.4 we describe how to set the weights ?i.
2.2 Features
2.2.1 Syntactic Features
We use two features of the string-to-tree trans-
ducer rules extracted from A, parse(e), and f ac-
cording to the rule extraction algorithm described in
(Galley et al, 2004):
ruleCount: Total number of rules extracted from
A, parse(e), and f . As Figure 1 illustrates, in-
correct links violating syntactic brackets tend to de-
crease ruleCount; ruleCount increases from 4 to 8
after deleting the incorrect link.
sizeOfLargestRule: The size, measured in terms
of internal nodes in the target parse tree, of the single
largest rule extracted from A, parse(e), and f . In
Figure 1, the largest rules in the leftmost and right-
most columns are R1 (with 9 internal nodes) and R9
(with 4 internal nodes), respectively.
2.2.2 Structural Features
wordsUnaligned: Total number of unaligned
words.
1-to-many Links: Total number of links for which
one word is aligned to multiple words, in either di-
rection. In Figure 1, the links {? -starts,? -
out,? -needs} represent a 1-to-many alignment.
1-to-many links appear more frequently in GIZA++
union alignments than in gold alignments, and are
therefore good candidates for deletion. The cate-
gory of 1-to-many links is further subdivided, de-
pending on the degree of contiguity that the link ex-
hibits with its neighbors.4 Each link in a 1-to-many
3While using a dynamic programming algorithm would
likely improve search efficiency and allow link deletion to find
an optimal solution, in practice, the greedy search runs quickly
and improves alignment quality.
4(Deng and Byrne, 2005) observe that, in a manually aligned
Chinese-English corpus, 82% of the Chinese words that are
alignment can have 0, 1, or 2 neighbors, according
to how many links are adjacent to it in the 1-to-many
alignment:
zeroNeighbors: In Figure 1, the link ? -needs
has 0 neighbors.
oneNeighbor: In Figure 1, the links ? -starts
and ? -out each have 1 neighbor?namely, each
other.
twoNeighbors: In Figure 1, in the 1-to-many
alignment formed by {? )-its,? )-own,? )-
country}, the link ? )-own has 2 neighbors,
namely ? )-it and ? )-country.
2.2.3 Lexical Features
highestLexProbRank: A link ei-fj is ?max-
probable from ei to fj? if p(fj |ei) > p(fj? |ei) for
all alternative words fj? with which ei is aligned
in Ainitial. In Figure 1, p(? |needs) > p(?
|needs), so ? -needs is max-probable for
?needs?. The definition of ?max-probable from fj to
ei? is analogous, and a link is max-probable (nondi-
rectionally) if it is max-probable in either direction.
The value of highestLexProbRank is the total num-
ber of max-probable links. The conditional lexical
probabilities p(ei|fj) and p(fj |ei) are estimated us-
ing frequencies of aligned word pairs in the high-
precision GIZA++ intersection alignments for the
training corpus.
2.2.4 History Features
In addition to the above syntactic, structural,
and lexical features of A, we also incorporate
two features of the link deletion history itself into
Score(A):
linksDeleted: Total number of links deleted
Ainitial thus far. At each iteration, either a link or
a branch of links is deleted.
aligned to multiple English words are aligned to a contiguous
block of English words; similarly, 88% of the English words
that are aligned to multiple Chinese words are aligned to a con-
tiguous block of Chinese words. Thus, if a Chinese word is cor-
rectly aligned to multiple English words, those English words
are likely to be ?neighbors? of each other, and if an English
word is correctly aligned to multiple Chinese words, those Chi-
nese words are likely to be ?neighbors? of each other.
47
stepsTaken: Total number of iterations thus far in
the search; at each iteration, either a link or a branch
is deleted. This feature serves as a constant cost
function per step taken during link deletion.
2.3 Constraints
Protecting Refined Links from Deletion: Since
GIZA++ refined links have higher precision than
union links5, we do not consider any GIZA++ re-
fined links for deletion.6
Stoplist: In our Chinese-English corpora, the 10
most common English words (excluding punc-
tuation marks) include {a,in,to,of,and,the}, while
the 10 most common Chinese words include
{?,4,?,Z,{}. Of these, {a,the} and {?,{}
have no explicit translational equivalent in the other
language. These words are aligned with each other
frequently (and erroneously) by GIZA++ union, but
rarely in the gold standard. We delete all links in
the set {a, an, the} ? {{, ?} from Ainitial as a
preprocessing step.7
2.4 Perceptron Training
We set the feature weights ? using a modified ver-
sion of averaged perceptron learning with structured
outputs (Collins, 2002). Following (Moore, 2005),
we initialize the value of our expected most infor-
mative feature (ruleCount) to 1.0, and initialize all
other feature weights to 0. During each pass over the
discriminative training set, we ?decode? each sen-
tence pair by greedily deleting links from Ainitial in
order to maximize the score of the resulting align-
ment using the current settings of ? (for details, refer
to section 2.1).
5On a 400-sentence-pair Chinese-English data set, GIZA++
union alignments have a precision of 77.32 while GIZA++ re-
fined alignments have a precision of 85.26.
6To see how GIZA++ refined alignments compare to
GIZA++ union alignments for syntax-based translation, we
compare systems trained on each set of alignments for Chinese-
English translation task A. Union alignments result in a test set
BLEU score of 41.17, as compared to only 36.99 for refined.
7The impact upon alignment f-measure of deleting these
stoplist links is small; on Chinese-English Data Set A, the f-
measure of the baseline GIZA++ union alignments on the test
set increases from 63.44 to 63.81 after deleting stoplist links,
while the remaining increase in f-measure from 63.81 to 75.14
(shown in Table 3) is due to the link deletion algorithm itself.
We construct a set of candidate alignments
Acandidates for use in reranking as follows. Starting
with A = Ainitial, we iteratively explore all align-
ments A? in the neighborhood of A, adding each
neighbor to Acandidates, then selecting the neigh-
bor that maximizes Score(A?). When it is no
longer possible to increase Score(A) by deleting
any links, link deletion concludes and returns the
highest-scoring alignment, A1-best.
In general, Agold /? Acandidates; following
(Collins, 2000) and (Charniak and Johnson, 2005)
for parse reranking and (Liang et al, 2006) for trans-
lation reranking, we define Aoracle as alignment in
Acandidates that is most similar to Agold.8 We up-
date each feature weight ?i as follows: ?i = ?i +
hAoraclei ? h
A1-best
i .
9
Following (Moore, 2005), after each training
pass, we average all the feature weight vectors seen
during the pass, and decode the discriminative train-
ing set using the vector of averaged feature weights.
When alignment quality stops increasing on the dis-
criminative training set, perceptron training ends.10
The weight vector returned by perceptron training is
the average over the training set of all weight vectors
seen during all iterations; averaging reduces overfit-
ting on the training set (Collins, 2002).
3 Experimental Setup
3.1 Data Sets
We evaluate the effect of link deletion upon align-
ment quality and translation quality for two Chinese-
English data sets, and one Arabic-English data set.
Each data set consists of newswire, and contains a
small subset of manually aligned sentence pairs. We
divide the manually aligned subset into a training set
(used to discriminatively set the feature weights for
link deletion) and a test set (used to evaluate the im-
pact of link deletion upon alignment quality). Table
1 lists the source and the size of the manually aligned
training and test sets used for each alignment task.
8We discuss alignment similarity metrics in detail in Section
3.2.
9(Liang et al, 2006) report that, for translation reranking,
such local updates (towards the oracle) outperform bold updates
(towards the gold standard).
10We discuss alignment quality metrics in detail in Section
3.2.
48
Using the feature weights learned on the manually
aligned training set, we then apply link deletion to
the remainder (non-manually aligned) of each bilin-
gual data set, and train a full syntax-based statistical
MT system on these sentence pairs. After maximum
BLEU tuning (Och, 2003a) on a held-out tuning set,
we evaluate translation quality on a held-out test set.
Table 2 lists the source and the size of the training,
tuning, and test sets used for each translation task.
3.2 Evaluation Metrics
AER (Alignment Error Rate) (Och and Ney, 2003)
is the most widely used metric of alignment qual-
ity, but requires gold-standard alignments labelled
with ?sure/possible? annotations to compute; lack-
ing such annotations, we can compute alignment f-
measure instead.
However, (Fraser and Marcu, 2007a) show that,
in phrase-based translation, improvements in AER
or f-measure do not necessarily correlate with im-
provements in BLEU score. They propose two mod-
ifications to f-measure: varying the precision/recall
tradeoff, and fully-connecting the alignment links
before computing f-measure.11
Weighted Fully-Connected F-Measure Given a
hypothesized set of alignment links H and a gold-
standard set of alignment links G, we define H+ =
fullyConnect(H) and G+ = fullyConnect(G),
and then compute:
f -measure(H+) = 1?
precision(H+) +
1??
recall(H+)
For phrase-based Chinese-English and Arabic-
English translation tasks, (Fraser and Marcu, 2007a)
obtain the closest correlation between weighted
fully-connected alignment f-measure and BLEU
score using ?=0.5 and ?=0.1, respectively. We
use weighted fully-connected alignment f-measure
as the training criterion for link deletion, and to eval-
uate alignment quality on training and test sets.
Rule F-Measure To evaluate the impact of link
deletion upon rule quality, we compare the rule pre-
cision, recall, and f-measure of the rule set extracted
11In Figure 1, the fully-connected version of the alignments
shown would include the links ? -starts and ? - out.
Language Train Test
Chinese-English A 400 400
Chinese-English B 1500 1500
Arabic-English 1500 1500
Table 1: Size (sentence pairs) of data sets used in align-
ment link deletion tasks
from our hypothesized alignments and a Collins-
style parser against the rule set extracted from gold
alignments and gold parses.
BLEU For all translation tasks, we report case-
insensitive NIST BLEU scores (Papineni et al,
2002) using 4 references per sentence.
3.3 Experiments
Starting with GIZA++ union (IBM Model 4) align-
ments, we use perceptron training to set the weights
of each feature used in link deletion in order to opti-
mize weighted fully-connected alignment f-measure
(?=0.5 for Chinese-English and ?=0.1 for Arabic-
English) on a manually aligned discriminative train-
ing set. We report the (fully-connected) precision,
recall, and weighted alignment f-measure on a held-
out test set after running perceptron training, relative
to the baseline GIZA++ union alignments. Using
the learned feature weights, we then perform link
deletion over the GIZA++ union alignments for the
entire training corpus for each translation task. Us-
ing these alignments, which we refer to as ?GIZA++
union + link deletion?, we train a syntax-based trans-
lation system similar to that described in (Galley et
al., 2006). After extracting string-to-tree translation
rules from the aligned, parsed training corpus, the
system assigns weights to each rule via frequency
estimation with smoothing. The rule probabilities,
as well as trigram language model probabilities and
a handful of additional features of each rule, are used
as features during decoding. The feature weights are
tuned using minimum error rate training (Och and
Ney, 2003) to optimize BLEU score on a held-out
development set. We then compare the BLEU score
of this system against a baseline system trained us-
ing GIZA++ union alignments.
To determine which value of ? is most effective
as a training criterion for link deletion, we set ?=0.4
(favoring recall), 0.5, and 0.6 (favoring precision),
49
Language Train Tune Test1 Test2
Chinese-English A 9.8M/newswire 25.9k/NIST02 29.0k/NIST03 ?
Chinese-English B 12.3M/newswire 42.9k/newswire 42.1k/newswire ?
Arabic-English 174.8M/newswire 35.8k/NIST04-05 40.3k/NIST04-05 53.0k/newswire
Table 2: Size (English words) and source of data sets used in translation tasks
and compare the effect on translation quality for
Chinese-English data set A.
4 Results
For each translation task, link deletion improves
translation quality relative to a GIZA++ union base-
line. For each alignment task, link deletion tends to
improve fully-connected alignment precision more
than it decreases fully-connected alignment recall,
increasing weighted fully-connected alignment f-
measure overall.
4.1 Chinese-English
On Chinese-English translation task A, link deletion
increases BLEU score by 1.26 points on tuning and
0.76 points on test (Table 3); on Chinese-English
translation task B, link deletion increases BLEU
score by 1.38 points on tuning and 0.49 points on
test (Table 3).
4.2 Arabic-English
On the Arabic-English translation task, link dele-
tion improves BLEU score by 0.84 points on tuning,
0.18 points on test1, and 0.56 points on test2 (Ta-
ble 3). Note that the training criterion for Arabic-
English link deletion uses ?=0.1; because this pe-
nalizes a loss in recall more heavily than it re-
wards an increase in precision, it is more difficult
to increase weighted fully-connected alignment f-
measure using link deletion for Arabic-English than
for Chinese-English. This difference is reflected in
the average number of links deleted per sentence:
4.19 for Chinese-English B (Table 3), but only 1.35
for Arabic-English (Table 3). Despite this differ-
ence, link deletion improves translation results for
Arabic-English as well.
4.3 Varying ?
On Chinese-English data set A, we explore the ef-
fect of varying ? in the weighted fully-connected
93 187 375 750 1500
46
48
50
52
54
56
58
60
62
64
Training Sentence Pairs
Te
st
 S
et
 W
ei
gh
te
d 
Fu
lly
?C
on
ne
ct
ed
 A
lig
nm
en
t F
?M
ea
su
re
 
 
GIZA++ union
GIZA++ union + link deletion
Figure 2: Effect of discriminative training set size on link
deletion accuracy for Chinese-English B, ?=0.5
alignment f-measure used as the training criterion
for link deletion. Using ?=0.5 leads to a higher gain
in BLEU score on the test set relative to the base-
line (+0.76 points) than either ?=0.4 (+0.70 points)
or ?=0.6 (+0.67 points).
4.4 Size of Discriminative Training Set
To examine how many manually aligned sentence
pairs are required to set the feature weights reli-
ably, we vary the size of the discriminative training
set from 2-1500 sentence pairs while holding test
set size constant at 1500 sentence pairs; run per-
ceptron training; and record the resulting weighted
fully-connected alignment f-measure on the test set.
Figure 2 illustrates that using 100-200 manually
aligned sentence pairs of training data is sufficient
for Chinese-English; a similarly-sized training set is
also sufficient for Arabic-English.
4.5 Effect of Link Deletion on Extracted Rules
Link deletion increases the size of the extracted
grammar. To determine how the quality of the ex-
tracted grammar changes, we compute the rule pre-
50
Language Alignment Prec Rec ? F-measure Links Del/ Grammar BLEUSent Size Tune Test1 Test2
Chi-Eng A GIZA++ union 54.76 75.38 0.5 63.44 ? 23.4M 41.80 41.17 ?
Chi-Eng A GIZA++ union + 79.59 71.16 0.5 75.14 4.77 59.7M 43.06 41.93 ?link deletion
Chi-Eng B GIZA++ union 36.61 66.28 0.5 47.16 ? 28.9M 39.59 41.39 ?
Chi-Eng B GIZA++ union + 65.52 59.28 0.5 62.24 4.19 73.0M 40.97 41.88 ?link deletion
Ara-Eng GIZA++ union 35.34 84.05 0.1 73.87 ? 52.4M 54.73 50.9 38.16
Ara-Eng GIZA++ union + 52.68 79.75 0.1 75.85 1.35 64.9M 55.57 51.08 38.72link deletion
Table 3: Results of link deletion. Weighted fully-connected alignment f-measure is computed on alignment test sets
(Table 1); BLEU score is computed on translation test sets (Table 2).
Alignment Parse RulePrecision Recall F-measure Total Non-Unique
gold gold 100.00 100.00 100.00 12,809
giza++ union collins 50.49 44.23 47.15 11,021
giza++ union+link deletion, ?=0.5 collins 47.51 53.20 50.20 13,987
giza++ refined collins 44.20 54.06 48.64 15,182
Table 4: Rule precision, recall, and f-measure of rules extracted from 400 sentence pairs of Chinese-English data
cision, recall, and f-measure of the GIZA++ union
alignments and various link deletion alignments on
a held-out Chinese-English test set of 400 sentence
pairs. Table 4 indicates the total (non-unique) num-
ber of rules extracted for each alignment/parse pair-
ing, as well as the rule precision, recall, and f-
measure of each pair. As more links are deleted,
more rules are extracted?but of those, some are of
good quality and others are of bad quality. Link-
deleted alignments produce rule sets with higher rule
f-measure than either GIZA++ union or GIZA++ re-
fined.
5 Conclusion
We have presented a link deletion algorithm that im-
proves the precision of GIZA++ union alignments
without notably decreasing recall. In addition to lex-
ical and structural features, we use features of the ex-
tracted syntax-based translation rules. Our method
improves alignment quality and translation quality
on Chinese-English and Arabic-English translation
tasks, relative to a GIZA++ union baseline. The
algorithm runs quickly, and is easily applicable to
other language pairs with limited amounts (100-200
sentence pairs) of manually aligned data available.
Acknowledgments
We thank Steven DeNeefe and Wei Wang for assis-
tance with experiments, and Alexander Fraser and
Liang Huang for helpful discussions. This research
was supported by DARPA (contract HR0011-06-C-
0022) and by a fellowship from AT&T Labs.
51
References
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. The Mathematics of Sta-
tistical Machine Translation: Parameter Estimation.
Computational Linguistics, Vol. 19, No. 2, 1993.
Eugene Charniak and Mark Johnson. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
Proceedings of ACL, 2005.
Colin Cherry and Dekang Lin. Soft Syntactic Constraints
for Word Alignment through Discriminative Training.
Proceedings of ACL (Poster), 2006.
David Chiang. A Hierarchical Phrase-Based Model for
Statistical Machine Translation. Proceedings of ACL,
2005.
David Chiang. Hierarchical phrase-based translation.
Computational Linguistics, 2007.
Michael Collins. Discriminative Reranking for Natural
Language Parsing. Proceedings of ICML, 2000.
Michael Collins. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. Proceedings of EMNLP,
2002.
John DeNero and Dan Klein. Tailoring Word Align-
ments to Syntactic Machine Translation. Proceedings
of ACL, 2007.
Yonggang Deng and William Byrne. HMM word and
phrase alignment for statistical machine translation.
Proceedings of HLT/EMNLP, 2005.
Alexander Fraser and Daniel Marcu. Measuring Word
Alignment Quality for Statistical Machine Translation.
Computational Linguistics, Vol. 33, No. 3, 2007.
Alexander Fraser and Daniel Marcu. Getting the Struc-
ture Right for Word Alignment: LEAF. Proceedings of
EMNLP, 2007.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. What?s in a Translation Rule? Proceedings of
HLT/NAACL-04, 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. Proceedings of
ACL, 2006.
Liang Huang, Kevin Knight, and Aravind Joshi. Statis-
tical Syntax-Directed Translation with Extended Do-
main of Locality. Proceedings of AMTA, 2006.
Abraham Ittycheriah and Salim Roukos. A Maximum En-
tropy Word Aligner for Arabic-English Machine Trans-
lation. Proceedings of HLT/EMNLP, 2005.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
Statistical Phrase-Based Translation. Proceedings of
HLT/NAACL, 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. Moses: Open Source Toolkit for
Statistical Machine Translation. Proceedings of ACL
(demo), 2007.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and
Ben Taskar. An end-to-end discriminative approach to
machine translation. Proceedings of COLING/ACL,
2006.
Yang Liu, Qun Liu, and Shouxun Lin. Log-linear Models
for Word Alignment. Proceedings of ACL, 2005.
Yang Liu, Qun Liu, and Shouxun Lin. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. Proceedings of ACL, 2006.
Adam Lopez and Philip Resnik. Improved HMM Align-
ment Models for Languages with Scarce Resources.
Proceedings of the ACL Workshop on Parallel Text,
2005.
Jonathan May and Kevin Knight. Syntactic Re-Alignment
Models for Machine Translation. Proceedings of
EMNLP-CoNLL, 2007.
Robert C. Moore. A Discriminative Framework for Bilin-
gual Word Alignment. Proceedings of HLT/EMNLP,
2005.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. Im-
proved discriminative bilingual word alignment. Pro-
ceedings of ACL, 2006.
Franz Josef Och. Minimum Error Rate Training in Sta-
tistical Machine Translation. Proceedings of ACL,
2003.
Franz Josef Och and Hermann Ney. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, Vol. 29, No. 1, 2003.
Franz Josef Och and Hermann Ney. The alignment
template approach to statistical machine translation.
Computational Linguistics, 2004.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU:
a Method for Automatic Evaluation of Machine Trans-
lation. Proceedings of ACL, 2002.
Chris Quirk, Arul Menezes, and Colin Cherry. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. Proceedings of ACL, 2005.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. A
Discriminative Matching Approach to Word Align-
ment. Proceedings of HTL/EMNLP, 2005.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
HMM-Based Word Alignment in Statistical Transla-
tion Proceedings of COLING, 1996.
52
Proceedings of NAACL-HLT 2013, pages 1110?1119,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Labeling the Languages of Words in Mixed-Language Documents using
Weakly Supervised Methods
Ben King
Department of EECS
University of Michigan
Ann Arbor, MI
benking@umich.edu
Steven Abney
Department of Linguistics
University of Michigan
Ann Arbor, MI
abney@umich.edu
Abstract
In this paper we consider the problem of label-
ing the languages of words in mixed-language
documents. This problem is approached in a
weakly supervised fashion, as a sequence la-
beling problem with monolingual text sam-
ples for training data. Among the approaches
evaluated, a conditional random field model
trained with generalized expectation criteria
was the most accurate and performed consis-
tently as the amount of training data was var-
ied.
1 Introduction
Language identification is a well-studied problem
(Hughes et al, 2006), but it is typically only studied
in its canonical text-classification formulation, iden-
tifying a document?s language given sample texts
from a few different languages. But there are sev-
eral other interesting and useful formulations of the
problem that have received relatively little attention.
Here, we focus on the problem of labeling the lan-
guages of individual words within a multilingual
document. To our knowledge, this is the first paper
to specifically address this problem.
Our own motivation for studying this problem
stems from issues encountered while attempting to
build language resources for minority languages. In
trying to extend parts of Kevin Scannell?s Cru?bada?n
project (Scannell, 2007), which automatically builds
minority language corpora from the Web, we found
that the majority of webpages that contain text in
a minority language also contain text in other lan-
guages. Since Scannell?s method builds these cor-
pora by bootstrapping from the pages that were re-
trieved, the corpus-building process can go disas-
trously wrong without accounting for this problem.
And any resources, such as a lexicon, created from
the corpus will also be incorrect.
In this paper, we explore techniques for per-
forming language identification at the word level in
mixed language documents. Our results show that
one can do better than independent word language
classification, as there are clues in a word?s context:
words of one language are frequently surrounded by
words in the same language, and many documents
have patterns that may be marked by the presence of
certain words or punctuation. The methods in this
paper also outperform sentence-level language iden-
tification, which is too coarse to capture most of the
shifts between language.
To evaluate our methods, we collected and man-
ually annotated a corpus of over 250,000 words
of bilingual (though mostly non-parallel) text from
the web. After running several different weakly-
supervised learning methods, we found that a condi-
tional random field model trained with generalized
expectation criteria is the most accurate and per-
forms quite consistently as the amount of training
data is varied.
In section 2, we review the related work. In sec-
tion 3, we define the task and describe the data and
its annotation. Because the task of language identi-
fication for individual words has not been explicitly
studied in the literature, and because of its impor-
tance to the overall task, we examine the features
and methods that work best for independent word
language identification in section 4. We begin to ex-
1110
amine the larger problem of labeling the language
of words in context in section 5 by describing our
methods. In section 6, we describe the evaluation
and present the results. We present our error analy-
sis in section 7 and conclude in section 8.
2 Related Work
Language identification is one of the older NLP
problems (Beesley, 1988), especially in regards to
spoken language (House and Neuburg, 1977), and
has received a fair share of attention through the
years (Hughes et al, 2006). In its standard formu-
lation, language identification assumes monolingual
documents and attempts to classify each document
according to its language from some closed set of
known languages.
Many approaches have been proposed, such as
Markov models (Dunning, 1994), Monte Carlo
methods (Poutsma, 2002), and more recently sup-
port vector machines with string kernels, but nearly
all approaches use the n-gram features first sug-
gested by (Cavnar and Trenkle, 1994). Performance
of language identification is generally very high with
large documents, usually in excess of 99% accuracy,
but Xia et al (2009) mention that current methods
still can perform quite poorly when the class of po-
tential languages is very large or the texts to be clas-
sified are very short.
This paper attempts to address three of the on-
going issues specifically mentioned by Hughes et
al. (2006) in their survey of textual language iden-
tification: supporting minority languages, sparse or
impoverished training data, and multilingual docu-
ments.
A number of methods have been proposed in re-
cent years to apply to the problems of unsuper-
vised and weakly-supervised learning. Excluding
self- and co-training methods, these methods can
be categorized into two broad classes: those which
bootstrap from a small number of tokens (some-
times called prototypes) (Collins and Singer, 1999;
Haghighi and Klein, 2006), and those which impose
constraints on the underlying unsupervised learning
problem (Chang et al, 2007; Bellare et al, 2009;
Druck et al, 2008; Ganchev et al, 2010).
Constraint-based weakly supervised learning has
been applied to some sequence labeling problems,
through such methods as contrastive estimation
(Smith and Eisner, 2005), generalized expectation
criteria (Mann and McCallum, 2008), alternating
projections (Singh et al, 2010), and posterior reg-
ularization (Ganchev et al, 2010).
Perhaps the work that is most similar to this work
is the study of code-switching within NLP literature.
Most of the work done has been on automatically
identifying code-switch points (Joshi, 1982; Solorio
and Liu, 2008). The problem of identifying lan-
guage in the presence of code-switching has seen
the most attention in the realm of speech process-
ing (Chu et al, 2007; Lyu and Lyu, 2008), among
many others. Though code-switching has been well-
studied linguistically, it is only one possible rea-
son to explain why a document contains multiple
languages, and is actually one of the less common
causes observed in our corpus. For that reason, we
approach this problem more generally, assuming no
specific generative process behind multilingual text.
3 Task Definition
The task we describe in this paper is a sequence
labeling problem, labeling a word in running text
according to the language to which it belongs. In
the interest of being able to produce reliable hu-
man annotations, we limit ourselves to texts with
exactly two languages represented, though the tech-
niques developed in this paper would certainly be
applicable to documents with more than two lan-
guages. The two languages represented in the paper
are known a priori by the labeler and the only train-
ing data available to the labeler is a small amount
of sample text in each of the two languages repre-
sented.
In most NLP sequence labeling problems, the re-
searchers can safely assume that each sequence (but
not each item in the sequence) is independent and
identically distributed (iid) according to some un-
derlying distribution common to all the documents.
For example, it is safe to assume that a sentence
drawn from WSJ section 23 can be labeled by a
model trained on the other sections. With the task
of this paper we cannot assume that sequences from
different documents are iid, (e.g. One document
may have 90% of its words in Basque, while another
only has 20%), but we do make the simplifying as-
1111
sumption that sequences within the same document
are iid.
Because of this difference, the labeler is presented
each document separately and must label its words
independently of any other document. And the train-
ing data for this task is not in the form of labeled
sequences. Rather, the models in this task are given
two monolingual example texts which are used only
to learn a model for individual instances. Any se-
quential dependencies between words must be boot-
strapped from the document. It is this aspect of
the problem that makes it well-suited for weakly-
supervised learning.
It is worth considering whether this problem is
best approached at the word level, or if perhaps
sentence- or paragraph-level language identification
would suffice for this task. In those cases, we could
easily segment the text at the sentence or paragraph
level and feed those segments to an existing lan-
guage identifier. To answer this question we seg-
mented our corpus into sentences by splitting at ev-
ery period, exclamation point, or question mark (an
overly agressive approximation of sentence segmen-
tation). Even if every sentence was given the cor-
rect majority label under this sentence segmentation,
the maximum possible word-level accuracy that a
sentence-level classifier could achieve is 85.8%, and
even though this number reflects quite optimistic
conditions, it is still much lower than the methods
of this paper are able to achieve.
3.1 Evaluation Data
To build a corpus of mixed language documents, we
used the BootCat tool (Baroni and Bernardini, 2004)
seeded with words from a minority language. Boot-
Cat is designed to automatically collect webpages
on a specific topic by repeatedly searching for key-
words from a topic-specific set of seed words. We
found that this method works equally well for lan-
guages as for topics, when seeded with words from
a specific language. Once BootCat returned a col-
lection of documents, we manually identified docu-
ments from the set that contained text in both the tar-
get language and in English, but did not contain text
in any other languages. Since the problem becomes
trivial when the languages do not share a character
set, we limited ourselves to languages with a Latin
orthography.
Language # words Language # words
Azerbaijani 4114 Lingala 1359
Banjar 10485 Lombard 18512
Basque 5488 Malagasy 6779
Cebuano 17994 Nahuatl 1133
Chippewa 15721 Ojibwa 24974
Cornish 2284 Oromo 28636
Croatian 17318 Pular 3648
Czech 886 Serbian 2457
Faroese 8307 Slovak 8403
Fulfulde 458 Somali 11613
Hausa 2899 Sotho 8198
Hungarian 9598 Tswana 879
Igbo 11828 Uzbek 43
Kiribati 2187 Yoruba 4845
Kurdish 531 Zulu 20783
Table 1: Languages present in the corpus and their
number of words before separating out English text.
We found that there was an important balance to
be struck concerning the popularity of a language. If
a language is not spoken widely enough, then there
is little chance of finding any text in that language on
the Web. Conversely if a language is too widely spo-
ken, then it is difficult to find mixed-language pages
for it. The list of languages present in the corpus
and the number of words in each language reflects
this balance as seen in Table 1.
For researchers who wish to make use this data,
the set of annotations used in this paper is available
from the first author?s website1.
3.2 Annotation
Before the human annotators were presented with
the mixed-language documents fetched by Boot-
Cat, the documents were first stripped of all HTML
markup, converted to Unicode, and had HTML es-
cape sequences replaced with the proper Unicode
characters. Documents that had any encoding er-
rors (e.g. original page used a mixture of encodings)
were excluded from the corpus.
1http://www-personal.umich.edu/?benking/
resources/mixed-language-annotations-
release-v1.0.tgz
1112
ENG: because of LUTARU.Thank you ntate T.T! Sevice...
SOT: Retselisitsoemonethi ekare jwale hotla sebetswa ...
ENG: Lesotho is heading 4 development #big-ups Mr ...
SOT: Basotho bare monoana hao its?upe.
ENG: Just do the job and lets see what you are made ...
SOT: Malerato Mokoena Ntate Thabane, molimo ...
ENG: It is God who reigns and if God is seen in your ...
SOT: Mathabo Letsie http://www.facebook.com/taole. ...
ENG: As Zuma did he should introduce a way of we can ...
SOT: Msekhotho Matona a rona ha a hlomamisoe, re ...
Table 2: An example of text from an annotated
English-Sotho web page.
Since there are many different reasons that the
language in a document may change (e.g. code-
switching, change of authors, borrowing) and many
variations thereof, we attempted to create a broad
set of annotation rules that would cover many cases,
rather than writing a large number of very specific
rules. In cases when the language use was ambigu-
ous, the annotators were instructed simply to make
their best guess. Table 2 shows an example of an
annotated document.
Generally, only well-digested English loanwords
and borrowings were to be marked as belonging to
the foreign language. If a word appeared in the con-
text of both languages, it was permissible for that
word to receive different labels at different times,
depending on its context.
Ordinary proper names (like ?John Williams? or
?Chicago?) were to be marked as belonging to the
language of the context in which they appear. This
rule also applied to abbreviations (like ?FIFA? or
?BBC?). The exception to this rule was proper
names composed of common nouns (like ?Stairway
to Heaven? or ?American Red Cross?) and to abbre-
viations that spelled out English words, which were
to be marked as belonging to the language of the
words they were composed of.
The annotators were instructed not to assign la-
bels to numbers or punctuation, but they were al-
lowed to use numbers as punctuation as clues for as-
signing other labels.
3.3 Human Agreement
To verify that the annotation rules were reasonable
and led to a problem that could potentially be solved
by a computer, we had each of the annotators mark
Language # words Language # words
Azerbaijani 211 Lingala 1816
Banjar 450 Lombard 2955
Basque 1378 Malagasy 4038
Cebuano 1898 Nahuatl 3544
Chippewa 92 Ojibwa 167
Cornish 2096 Oromo 1443
Croatian 1505 Pular 1285
Czech 1503 Serbian 1515
English 16469 Slovak 1504
Faroese 1585 Somali 1871
Fulfulde 1097 Sotho 2154
Hausa 2677 Tswana 2191
Hungarian 1541 Uzbek 1533
Igbo 2079 Yoruba 2454
Kiribati 1891 Zulu 1075
Kurdish 1674
Table 3: Number of total words of training data for
each language.
up a small shared set of a few hundred words from
each of eight documents, in order to measure the
inter-annotator agreement.
The average actual agreement was 0.988, with 0.5
agreement expected by chance for a kappa of 0.975.
3.4 Training Data
Following Scannell (2007), we collected small
monolingual samples of 643 languages from four
sources: the Universal Declaration of Human
Rights2, non-English Wikipedias3, the Jehovah?s
Witnesses website4, and the Rosetta project (Lands-
bergen, 1989).
Only 30 of these languages ended up being used
in experiments. Table 3 shows the sizes of the mono-
lingual samples of the languages used in this paper.
2The Universal Declaration of Human Rights is a document
created by the United Nations and translated into many lan-
guages. As of February 2011 there were 365 versions available
from http://www.unicode.org/udhr/
3As of February 2011, there were 113 Wikipedias in differ-
ent languages. Current versions of Wikipedia can be accessed
from http://meta.wikimedia.org/wiki/List of
Wikipedias
4As of February 2011, there were 310 versions of the site
available at http://www.watchtower.org
1113
They range from 92 for Chippewa to 16469 for En-
glish. Most of the languages have between 1300 and
1600 words in their example text. To attempt to mit-
igate variation caused by the sizes of these language
samples, we sample an equal number of words with
replacement from each of English and a second lan-
guage to create the training data.
4 Word-level Language Classification
We shift our attention momentarily to a subproblem
of the overall task: independent word-level language
classification. While the task of language identifica-
tion has been studied extensively at the document,
sentence, and query level, little or no work has been
done at the level of an individual word. For this rea-
son, we feel it is prudent to formally evaluate the fea-
tures and classifiers which perform most effectively
at the task of word language classification (ignoring
any sequential dependencies at this point).
4.1 Features
We used a logistic regression classifier to experiment
with combinations of the following features: charac-
ter unigrams, bigrams, trigrams, 4-grams, 5-grams,
and the full word. For these experiments, the train-
ing data consisted of 1000 words sampled uniformly
with replacement from the sample text in the appro-
priate languages. Table 4 shows the accuracies that
the classifier achieved when using different sets of
features averaged over 10 independent runs.
Features Accuracy
Unigrams 0.8056
Bigrams 0.8783
Trigrams 0.8491
4-grams 0.7846
5-grams 0.6977
{1,2,3,4,5}-grams 0.8817
{1,2,3,4,5}-grams, word 0.8819
Table 4: Logistic regression accuracy when trained
using varying features.
The use of all available features seems to be the
best option, and we use the full set of features in
all proceeding experiments. This result also concurs
with the findigs of (Cavnar and Trenkle, 1994), who
0 200 400 600 800 1,000
0.7
0.8
0.9
Sampled Words
A
cc
u
ra
cy
logistic regression
na??ve Bayes
decision tree
winnow2
Figure 1: Learning curves for logistic regression,
na??ve Bayes, decision tree, and Winnow2 on the in-
dependent word classification problem as the num-
ber of sampled words in each training example
changes from 10 to 1000.
found 1-5-grams to be most effective for document
language classification.
4.2 Classifiers
Using all available features, we compare four MAL-
LET (McCallum, 2002) classifiers: logistic regres-
sion, na??ve Bayes, decision tree, and Winnow2. Fig-
ure 1 shows the learning curves for each classifier as
the number of sampled words comprising each train-
ing example is varied from 10 to 1000.
Since a na??ve Bayes classifier gave the best per-
formance in most experiments, we use na??ve Bayes
as a representative word classifier for the rest of the
paper.
5 Methods
Moving onto the main task of this paper, labeling
sequences of words in documents according to their
languages, we use this section to describe our meth-
ods.
Since training data for this task is limited and is
of a different type than the evaluation data (labeled
instances from monolingual example texts vs. la-
beled sequences from the multilingual document),
we approach the problem with weakly- and semi-
supervised methods.
1114
The sequence labeling methods are presented
with a few new sequence-relevant features, which
are not applicable to independent word classification
(since these features do not appear in the training
data):
? a feature for the presence of each possible non-
word character (punctuation or digit) between
the previous and the current words
? a feature for the presence of each possible non-
word character between the current and next
words
In addition to independent word classification,
which was covered in section 4, we also imple-
ment a conditional random field model trained with
generalized expectation criteria, a hidden Markov
model (HMM) trained with expectation maximiza-
tion (EM), and a logistic regression model trained
with generalized expectation criteria.
We had also considered that a semi-Markov CRF
(Sarawagi and Cohen, 2004) could be useful if
we could model segment lengths (a non-Markovian
feature), but we found that gold-standard segment
lengths did not seem to be distributed according to
any canonical distribution, and we did not have a re-
liable way to estimate these segment lengths.
5.1 Conditional Random Field Model trained
with Generalized Expectation
Generalized expectation (GE) criteria (Druck et al,
2008) are terms added to the objective function of
a learning algorithm which specify preferences for
the learned model. When the model is a linear
chain conditional random field (CRF) model, we can
straightforwardly express these criteria in the objec-
tive function with a KL-divergence term between the
expected values of the current model p? and the pre-
ferred model p? (Mann and McCallum, 2008).
O(?;D,U) =
?
d
log p?(y
(d)|x(d))?
?
k ?k
2?2
? ?D(p?||p??)
Practically, to compute these expectations, we
produce the smoothed MLE on the output label dis-
tribution for every feature observed in the training
data. For example, the trigram ?ter? may occur
27 times in the English sample text and 34 times
in the other sample text, leading to an MLE of
p?(eng|ter) ? 0.44.
Because we do not expect the true marginal label
distribution to be uniform (i.e. the document may
not have equal numbers of words in each language),
we first estimate the expected marginal label distri-
bution by classifying each word in the document in-
dependently using na??ve Bayes and taking the result-
ing counts of labels produced by the classifier as an
MLE estimate for it: p?(eng) and p?(non).
We use these terms to bias the expected label dis-
tributions over each feature. Let Feng and Fnon re-
spectively be the collections of all training data fea-
tures with the two labels. For every label l ? L =
{eng,non} and every feature f ? Feng?Fnon, we
calculate
p(l|f) =
count(f,Fl) + ?
count(f,
?
iFi) + ?|L|
?
p?(l)
puniform(l)
,
the biased maximum likelihood expected output
label distribution. To avoid having p(l|f) = 0,
which can cause the KL-divergence to be undefined,
we perform additive smoothing with ? = 0.5 on the
counts before multiplying with the biasing term.
We use the implementation of CRF with GE cri-
teria from MALLET (McCallum, 2002), which uses
a gradient descent algorithm to optimize the objec-
tive function. (Mann and McCallum, 2008; Druck,
2011)
5.2 Hidden Markov Model trained with
Expectation Maximization
A second method we used was a hidden Markov
model (HMM) trained iteratively using the Expec-
tation Maximization algorithm (Dempster et al,
1977). Here an HMM is preferable to a CRF be-
cause it is a generative model and therefore uses pa-
rameters with simple interpretations. In the case of
an HMM, it is easy to estimate emission and transi-
tion probabilities using an external method and then
set these directly.
To initialize the HMM, we use a uniform distri-
bution for transition probabilities, and produce the
emission probabilities by using a na??ve Bayes clas-
sifier trained over the two small language samples.
1115
In the expectation step, we simply pass the docu-
ment through the HMM and record the labels it pro-
duces for each word in the document.
In the maximization step, we produce maximum-
likelihood estimates for transition probabilities from
the transitions between the labels produced. To
estimate emission probabilities, we retrain a na??ve
Bayes classifier on the small language samples along
the set of words from the document that were labeled
as being in the respective language. We iterated this
process until convergence, which usually took fewer
than 10 iterations.
We additionally experimented with a na??ve Bayes
classifier trained by EM in the same fashion, except
that it had no transition probabilities to update. This
classifier?s performance was almost identical to that
of the GE-trained MaxEnt method mentioned in the
following section, so we omit it from the results and
analysis for that reason.
5.3 Logistic Regression trained with
Generalized Expectation
GE criteria can also be straightforwardly applied to
the weakly supervised training of logistic regression
models. The special case where the constraints spec-
ified are over marginal label distributions, is called
label regularization.
As with the CRF constraint creation, here we first
use an ordinary supervised na??ve Bayes classifier in
order to estimate the marginal label distributions for
the document, which can be used to create more ac-
curate output label expectations that are biased to
the marginal label distributions over all words in the
document.
We use the MALLET implementation of a GE-
trained logistic regression classifier, which opti-
mizes the objective function using a gradient descent
algorithm.
5.4 Word-level Classification
Our fourth method served as a baseline and did
not involve any sequence labeling, only independent
classification of words. Since na??ve Bayes was the
best performer among word classification methods,
we use that the representative of independent word
classification methods. The implementation of the
na??ve Bayes classifier is from MALLET.
0 200 400 600 800 1,000
0.7
0.75
0.8
0.85
0.9
0.95
Sampled Words
A
cc
u
ra
cy
na??ve Bayes
GE-trained logistic regression
EM-trained HMM
GE-trained CRF
Figure 2: Learning curves for na??ve Bayes, logistic
regression trained with GE, HMM trained with EM,
and CRF trained with GE as the number of sampled
words in each training example changes from 10 to
1000.
We also implemented a self-trained CRF, initially
trained on the output of this na??ve Bayes classifier,
and trained on its own output in subsequent itera-
tions. This method was not able to consistently out-
perform the na??ve Bayes classifier after any number
of iterations.
6 Evaluation and Results
We evaluated each method using simple token-level
accuracy, i.e. whether the correct label was assigned
to a word in the document. Word boundaries were
defined by punctuation or whitespace, and no tokens
containing a digit were included. Figure 2 displays
the accuracy for each method as the number of sam-
pled words from each language example is varied
from 10 to 1000.
In all the cases we tested, CRF trained with GE
is clearly the most accurate option among the meth-
ods examined, though the EM-trained HMM seemed
to be approaching a similar accuracy with large
amounts of training data. With a slight edge in ef-
ficiency also in its favor, we think the GE+CRF ap-
proach, rather than EM+HMM, is the best approach
for this problem because of its consistent perfor-
mance across a wide range of training data sizes.
In its favor, the EM+HMM approach has a slightly
1116
lower variance in its performance across different
files, though not at a statistically significant level.
Contrary to most of the results in (Mann and Mc-
Callum, 2010), a logistic regression classifier trained
with GE did not outperform a standard supervised
na??ve Bayes classifier. We suspect that this is due
to the different nature of this problem as compared
to most other sequence labeling problems, with the
classifier bootstrapping over a single document only.
In the problems studied by Mann and McCallum, the
GE-trained classifier was able to train over the entire
training set, which was on average about 50,000 in-
stances, far more than the number of words in the
average document in this set (2,500).
7 Error Analysis
In order to analyze the types of mistakes that the
models made we performed an error analysis on ten
randomly selected files, looking at each mislabeled
word and classifying the error according to its type.
The results of this analysis are in Table 5. The three
classes of errors are (1) named entity errors, when
a named entity is given a label that does not match
the label it was given in the original annotation, (2)
shared word errors, when a word that could belong
to either language is classified incorrectly, and (3)
other, a case that covers all other types of errors.
Method NE SW Other
GE+CRF 41% 10% 49%
EM+HMM 50% 14% 35%
GE+MaxEnt 37% 12% 51%
Na??ve Bayes 42% 17% 40%
Table 5: Types of errors and their proportions among
the different methods. NE stands for Named Entity,
SW stands for Shared Word, and Other covers all
other types of errors.
Our annotation rules for named entities specified
that named entities should be given a label match-
ing their context, but this was rather arbitrary, and
not explicitly followed by any of the methods, which
treat a named entity as if it was any other token. This
was the one of most frequent types of error made by
each of the methods and in our conclusion in sec-
tion 8, we discuss ways to improve it.
In a regression analysis to determine which fac-
tors had the greatest correlations with the GE-
trained CRF performance, the estimated proportion
of named entities in the document had by far the
greatest correlation with CRF accuracy of anything
we measured. Following that in decreasing order of
correlation strength were the cosine similarity be-
tween English and the document?s second language,
the number of words in the monolingual example
text (even though we sampled from it), and the aver-
age length of gold-standard monolingual sequences
in the document.
The learning curve for GE-trained CRF in Fig-
ure 2 is somewhat atypical as far as most machine
learning methods are concerned: performance is
typically non-decreasing as more training data is
made available.
We believe that the model is becoming over-
constrained as more words are used to create the
constraints. The GE method does not have a way
to specify that some of the soft constraints (for the
labels observed most frequently in the sample text)
should be more important than other constraints
(those observed less frequently). When we mea-
sure the KL-divergence between the label distribu-
tions predicted by the constraints and the true la-
bel distribution, we find that this divergence seems
to reach its minimum value between 600 and 800
words, which is where the GE+CRF also seems to
reach its maximum performance.
The step with a na??ve Bayes classifier estimating
the marginal label distribution ended up being quite
important overall. Without it, the accuracy dropped
by more than a full percentage point absolute. But
the problem of inaccurate constraint estimation is
one that needs further consideration. Some possible
ways to address it may be to prune the constraints
according to their frequency or perhaps according to
a metric like entropy, or to vary the GE-criteria coef-
ficient in the objective function in order to penalize
the model less for varying from the expected model.
8 Conclusion
This paper addresses three of the ongoing issues
specifically mentioned by Hughes et al (2006) in
their survey of textual language identification. Our
approach is able to support minority languages; in
1117
fact, almost all of the languages we tested on would
be considered minority languages. We also address
the issue of sparse or impoverished training data.
Because we use weakly-supervised methods, we are
able to successfully learn to recognize a language
with as few as 10 words of training data5. The last
and most obvious point we address is that of multi-
lingual documents, which is the focus of the paper.
We present a weakly-supervised system for iden-
tifying the languages of individual words in mixed-
language documents. We found that across a broad
range of training data sizes, a CRF model trained
with GE criteria is an accurate sequence classifier
and is preferable to other methods for several rea-
sons.
One major issue to be improved upon in future
work is how named entities are handled. A straight-
forward way to approach this may be to create an-
other label for named entities, which (for the pur-
poses of evaluation) would be considered not to be-
long to any of the languages in the document. We
could simply choose not to evaluate a system on the
named entity tokens in a document. Alternatively,
the problem of language-independent named entity
recognition has received some attention in the past
(Tjong Kim Sang and De Meulder, 2003), and it may
be beneficial to incorporate such a system in a robust
word-level language identification system.
Going forward, an issue that needs to be ad-
dressed with this method is its dependence on know-
ing the set of possible languages a priori. Because
we don?t see an easy way to adapt this method to ac-
curately label words in documents from a possible
set of thousands of languages when the document
itself may only contain two or three languages, we
would propose the following future work.
We propose a two-step approach to general word-
level language identification. The first step would be
to examine a multilingual document, and with high
accuracy, list the languages that are present in the
document. The second step would be identical to the
approach described in this paper (but with the two-
language restriction lifted), and would be responsi-
ble for labeling the languages of individual words,
using the set of languages provided by the first step.
5With only 10 words of each language as training data, the
CRF approach correctly labels 88% of words
References
Marco Baroni and Silvia Bernardini. 2004. Bootcat:
Bootstrapping corpora and terms from the web. In
Proceedings of the Fourth International Conference
on Langauge Resources and Evaluation (LREC 2004),
volume 4, pages 1313?1316, Lisbon, Portugal.
Kenneth R. Beesley. 1988. Language identifier: A com-
puter program for automatic natural-language identifi-
cation of on-line text. In Proceedings of the 29th An-
nual Conference of the American Translators Associa-
tion, volume 47, page 54.
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence,
pages 43?50. AUAI Press.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Annual Symposium on Document Analysis and
Information (SDAIR 94), pages 161?175, Las Vegas,
Nevada.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 280?287, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chyng-Leei Chu, Dau-cheng Lyu, and Ren-yuan Lyu.
2007. Language identification on code-switching
speech. In Proceedings of ROCLING.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 100?110.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), pages 1?38.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ized expectation criteria. In Proceedings of the 31st
annual international ACM SIGIR conference on Re-
search and development in information retrieval (SI-
GIR 2008), pages 595?602. ACM.
Gregory Druck. 2011. Generalized Expectation Criteria
for Lightly Supervised Learning. Ph.D. thesis, Univer-
sity of Massachusetts Amherst.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical report.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. The Journal of Machine
Learning Research, 11:2001?2049.
1118
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320?327, New York
City, USA, June. Association for Computational Lin-
guistics.
A.S. House and E.P. Neuburg. 1977. Toward automatic
identification of the language of an utterance. i. pre-
liminary methodological considerations. The Journal
of the Acoustical Society of America, 62:708.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proc. International Conference on Lan-
guage Resources and Evaluation, pages 485?488.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In Proceedings of the
9th conference on Computational linguistics-Volume
1, pages 145?150. Academia Praha.
Jan Landsbergen. 1989. The rosetta project. pages 82?
87, Munich, Germany.
Dau-Cheng Lyu and Ren-Yuan Lyu. 2008. Language
identification on code-switching utterances using mul-
tiple cues. In Ninth Annual Conference of the Interna-
tional Speech Communication Association.
Gideon S. Mann and Andrew McCallum. 2008. Gener-
alized expectation criteria for semi-supervised learn-
ing of conditional random fields. In Proceedings of
ACL-08: HLT, pages 870?878, Columbus, Ohio, June.
Association for Computational Linguistics.
Gideon S. Mann and Andrew McCallum. 2010. Gener-
alized expectation criteria for semi-supervised learn-
ing with weakly labeled data. The Journal of Machine
Learning Research, pages 955?984.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Arjen Poutsma. 2002. Applying monte carlo techniques
to language identification. Language and Computers,
45(1):179?189.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Advances in Neural Information Processing
Systems (NIPS 2004), 17:1185?1192.
Kevin P. Scannell. 2007. The cru?bada?n project: Cor-
pus building for under-resourced languages. In Build-
ing and Exploring Web Corpora: Proceedings of the
3rd Web as Corpus Workshop, volume 4, pages 5?15,
Louvain-la-Neuve, Belgium.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 73?81, Los Angeles, California, June. As-
sociation for Computational Linguistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 354?362, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Thamar Solorio and Yang Liu. 2008. Learning to pre-
dict code-switching points. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceed-
ings of the Seventh Conference on Natural Language
Learning at HLT-NAACL 2003, pages 142?147.
Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the context of harvesting language data
off the web. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 870?878, Athens, Greece, March. Association
for Computational Linguistics.
1119
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 88?97,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Human Language Project:
Building a Universal Corpus of the World?s Languages
Steven Abney
University of Michigan
abney@umich.edu
Steven Bird
University of Melbourne and
University of Pennsylvania
sbird@unimelb.edu.au
Abstract
We present a grand challenge to build a
corpus that will include all of the world?s
languages, in a consistent structure that
permits large-scale cross-linguistic pro-
cessing, enabling the study of universal
linguistics. The focal data types, bilin-
gual texts and lexicons, relate each lan-
guage to one of a set of reference lan-
guages. We propose that the ability to train
systems to translate into and out of a given
language be the yardstick for determin-
ing when we have successfully captured a
language. We call on the computational
linguistics community to begin work on
this Universal Corpus, pursuing the many
strands of activity described here, as their
contribution to the global effort to docu-
ment the world?s linguistic heritage before
more languages fall silent.
1 Introduction
The grand aim of linguistics is the construction of
a universal theory of human language. To a com-
putational linguist, it seems obvious that the first
step is to collect significant amounts of primary
data for a large variety of languages. Ideally, we
would like a complete digitization of every human
language: a Universal Corpus.
If we are ever to construct such a corpus, it must
be now. With the current rate of language loss, we
have only a small window of opportunity before
the data is gone forever. Linguistics may be unique
among the sciences in the crisis it faces. The next
generation will forgive us for the most egregious
shortcomings in theory construction and technol-
ogy development, but they will not forgive us if we
fail to preserve vanishing primary language data in
a form that enables future research.
The scope of the task is enormous. At present,
we have non-negligible quantities of machine-
readable data for only about 20?30 of the world?s
6,900 languages (Maxwell and Hughes, 2006).
Linguistics as a field is awake to the crisis. There
has been a tremendous upsurge of interest in doc-
umentary linguistics, the field concerned with the
the ?creation, annotation, preservation, and dis-
semination of transparent records of a language?
(Woodbury, 2010). However, documentary lin-
guistics alone is not equal to the task. For example,
no million-word machine-readable corpus exists
for any endangered language, even though such a
quantity would be necessary for wide-ranging in-
vestigation of the language once no speakers are
available. The chances of constructing large-scale
resources will be greatly improved if computa-
tional linguists contribute their expertise.
This collaboration between linguists and com-
putational linguists will extend beyond the con-
struction of the Universal Corpus to its exploita-
tion for both theoretical and technological ends.
We envisage a new paradigm of universal linguis-
tics, in which grammars of individual languages
are built from the ground up, combining expert
manual effort with the power tools of probabilis-
tic language models and grammatical inference.
A universal grammar captures redundancies which
exist across languages, constituting a ?universal
linguistic prior,? and enabling us to identify the
distinctive properties of specific languages and
families. The linguistic prior and regularities due
to common descent enable a new economy of scale
for technology development: cross-linguistic tri-
angulation can improve performance while reduc-
ing per-language data requirements.
Our aim in the present paper is to move beyond
generalities to a concrete plan of attack, and to
challenge the field to a communal effort to cre-
ate a Universal Corpus of the world?s languages,
in consistent machine-readable format, permitting
large-scale cross-linguistic processing.
88
2 Human Language Project
2.1 Aims and scope
Although language endangerment provides ur-
gency, the corpus is not intended primarily as
a Noah?s Ark for languages. The aims go be-
yond the current crisis: we wish to support cross-
linguistic research and technology development at
the largest scale. There are existing collections
that contain multiple languages, but it is rare to
have consistent formats and annotation across lan-
guages, and few such datasets contain more than a
dozen or so languages.
If we think of a multi-lingual corpus as con-
sisting of an array of items, with columns repre-
senting languages and rows representing resource
types, the usual focus is on ?vertical? processing.
Our particular concern, by contrast, is ?horizontal?
processing that cuts indiscriminately across lan-
guages. Hence we require an unusual degree of
consistency across languages.
The kind of processing we wish to enable is
much like the large-scale systematic research that
motivated the Human Genome Project.
One of the greatest impacts of having
the sequence may well be in enabling
an entirely new approach to biological
research. In the past, researchers stud-
ied one or a few genes at a time. With
whole-genome sequences . . . they can
approach questions systematically and
on a grand scale. They can study . . .
how tens of thousands of genes and pro-
teins work together in interconnected
networks to orchestrate the chemistry of
life. (Human Genome Project, 2007)
We wish to make it possible to investigate human
language equally systematically and on an equally
grand scale: a Human Linguome Project, as it
were, though we have chosen the ?Human Lan-
guage Project? as a more inviting title for the un-
dertaking. The product is a Universal Corpus,1 in
two senses of universal: in the sense of including
(ultimately) all the world?s languages, and in the
sense of enabling software and processing meth-
ods that are language-universal.
However, we do not aim for a collection that
is universal in the sense of encompassing all lan-
guage documentation efforts. Our goal is the con-
struction of a specific resource, albeit a very large
1http://universalcorpus.org/
resource. We contrast the proposed effort with
general efforts to develop open resources, stan-
dards, and best practices. We do not aim to be all-
inclusive. The project does require large-scale col-
laboration, and a task definition that is simple and
compelling enough to achieve buy-in from a large
number of data providers. But we do not need and
do not attempt to create consensus across the en-
tire community. (Although one can hope that what
proves successful for a project of this scale will
provide a good foundation for future standards.)
Moreover, we do not aim to collect data
merely in the vague hope that it will prove use-
ful. Although we strive for maximum general-
ity, we also propose a specific driving ?use case,?
namely, machine translation (MT), (Hutchins and
Somers, 1992; Koehn, 2010). The corpus pro-
vides a testing ground for the development of MT
system-construction methods that are dramatically
?leaner? in their resource requirements, and which
take advantage of cross-linguistic bootstrapping.
The large engineering question is how one can
turn the size of the task?constructing MT systems
for all the world?s languages simultaneously?to
one?s advantage, and thereby consume dramati-
cally less data per language.
The choice of MT as the use case is also driven
by scientific considerations. To explain, we re-
quire a bit of preamble.
We aim for a digitization of each human lan-
guage. What exactly does it mean to digitize an
entire language? It is natural to think in terms
of replicating the body of resources available for
well-documented languages, and the pre-eminent
resource for any language is a treebank. Producing
a treebank involves a staggering amount of man-
ual effort. It is also notoriously difficult to obtain
agreement about how parse trees should be defined
in one language, much less in many languages si-
multaneously. The idea of producing treebanks for
6,900 languages is quixotic, to put it mildly. But
is a treebank actually necessary?
Let us suppose that the purpose of a parse
tree is to mediate interpretation. A treebank, ar-
guably, represents a theoretical hypothesis about
how interpretations could be constructed; the pri-
mary data is actually the interpretations them-
selves. This suggests that we annotate sentences
with representations of meanings instead of syn-
tactic structures. Now that seems to take us out of
the frying pan into the fire. If obtaining consen-
89
sus on parse trees is difficult, obtaining consensus
on meaning representations is impossible. How-
ever, if the language under consideration is any-
thing other than English, then a translation into
English (or some other reference language) is for
most purposes a perfectly adequate meaning rep-
resentation. That is, we view machine translation
as an approximation to language understanding.
Here is another way to put it. One measure of
adequacy of a language digitization is the abil-
ity of a human?already fluent in a reference
language?to acquire fluency in the digitized lan-
guage using only archived material. Now it would
be even better if we could use a language digiti-
zation to construct an artificial speaker of the lan-
guage. Importantly, we do not need to solve the AI
problem: the speaker need not decide what to say,
only how to translate from meanings to sentences
of the language, and from sentences back to mean-
ings. Taking sentences in a reference language as
the meaning representation, we arrive back at ma-
chine translation as the measure of success. In
short, we have successfully captured a language if
we can translate into and out of the language.
The key resource that should be built for each
language, then, is a collection of primary texts
with translations into a reference language. ?Pri-
mary text? includes both written documents and
transcriptions of recordings. Large volumes of pri-
mary texts will be useful even without translation
for such tasks as language modeling and unsuper-
vised learning of morphology. Thus, we antici-
pate that the corpus will have the usual ?pyrami-
dal? structure, starting from a base layer of unan-
notated text, some portion of which is translated
into a reference language at the document level to
make the next layer. Note that, for maximally au-
thentic primary texts, we assume the direction of
translation will normally be from primary text to
reference language, not the other way around.
Another layer of the corpus consists of sentence
and word alignments, required for training and
evaluating machine translation systems, and for
extracting bilingual lexicons. Curating such anno-
tations is a more specialized task than translation,
and so we expect it will only be done for a subset
of the translated texts.
In the last and smallest layer, morphology is an-
notated. This supports the development of mor-
phological analyzers, to preprocess primary texts
to identify morpheme boundaries and recognize
allomorphs, reducing the amount of data required
for training an MT system. This most-refined
target annotation corresponds to the interlinear
glossed texts that are the de facto standard of anno-
tation in the documentary linguistics community.
We postulate that interlinear glossed text is suf-
ficiently fine-grained to serve our purposes. It
invites efforts to enrich it by automatic means:
for example, there has been work on parsing the
English translations and using the word-by-word
glosses to transfer the parse tree to the object lan-
guage, effectively creating a treebank automati-
cally (Xia and Lewis, 2007). At the same time, we
believe that interlinear glossed text is sufficiently
simple and well-understood to allow rapid con-
struction of resources, and to make cross-linguistic
consistency a realistic goal.
Each of these layers?primary text, translations,
alignments, and morphological glosses?seems to
be an unavoidable piece of the overall solution.
The fact that these layers will exist in diminishing
quantity is also unavoidable. However, there is an
important consequence: the primary texts will be
permanently subject to new translation initiatives,
which themselves will be subject to new align-
ment and glossing initiatives, in which each step
is an instance of semisupervised learning (Abney,
2007). As time passes, our ability to enhance the
quantity and quality of the annotations will only
increase, thanks to effective combinations of auto-
matic, professional, and crowd-sourced effort.
2.2 Principles
The basic principles upon which the envisioned
corpus is based are the following:
Universality. Covering as many languages as
possible is the first priority. Progress will be
gauged against concrete goals for numbers of lan-
guages, data per language, and coverage of lan-
guage families (Whalen and Simons, 2009).
Machine readability and consistency. ?Cover-
ing? languages means enabling machine process-
ing seamlessly across languages. This will sup-
port new types of linguistic inquiry and the devel-
opment and testing of inference methods (for mor-
phology, parsers, machine translation) across large
numbers of typologically diverse languages.
Community effort. We cannot expect a single
organization to assemble a resource on this scale.
It will be necessary to get community buy-in, and
90
many motivated volunteers. The repository will
not be the sole possession of any one institution.
Availability. The content of the corpus will be
available under one or more permissive licenses,
such as the Creative Commons Attribution Li-
cense (CC-BY), placing as few limits as possible
on community members? ability to obtain and en-
hance the corpus, and redistribute derivative data.
Utility. The corpus aims to be maximally use-
ful, and minimally parochial. Annotation will be
as lightweight as possible; richer annotations will
will emerge bottom-up as they prove their utility
at the large scale.
Centrality of primary data. Primary texts and
recordings are paramount. Secondary resources
such as grammars and lexicons are important, but
no substitute for primary data. It is desirable that
secondary resources be integrated with?if not de-
rived from?primary data in the corpus.
2.3 What to include
What should be included in the corpus? To some
extent, data collection will be opportunistic, but
it is appropriate to have a well-defined target in
mind. We consider the following essential.
Metadata. One means of resource identification
is to survey existing documentation for the lan-
guage, including bibliographic references and lo-
cations of web resources. Provenance and proper
citation of sources should be included for all data.
For written text. (1) Primary documents in
original printed form, e.g. scanned page images or
PDF. (2) Transcription. Not only optical charac-
ter recognition output, but also the output of tools
that extract text from PDF, will generally require
manual editing.
For spoken text. (1) Audio recordings. Both
elicited and spontaneous speech should be in-
cluded. It is highly desirous to have some con-
nected speech for every language. (2) Slow speech
?audio transcriptions.? Carefully respeaking a
spoken text can be much more efficient than writ-
ten transcription, and may one day yield to speech
recognition methods. (3) Written transcriptions.
We do not impose any requirements on the form
of transcription, though orthographic transcription
is generally much faster to produce than phonetic
transcription, and may even be more useful as
words are represented by normalized forms.
For both written and spoken text. (1) Trans-
lations of primary documents into a refer-
ence language (possibly including commentary).
(2) Sentence-level segmentation and transla-
tion. (3) Word-level segmentation and glossing.
(4) Morpheme-level segmentation and glossing.
All documents will be included in primary
form, but the percentage of documents with man-
ual annotation, or manually corrected annotation,
decreases at increasingly fine-grained levels of an-
notation. Where manual fine-grained annotation is
unavailable, automatic methods for creating it (at a
lower quality) are desirable. Defining such meth-
ods for a large range of resource-poor languages is
an interesting computational challenge.
Secondary resources. Although it is possible to
base descriptive analyses exclusively on a text cor-
pus (Himmelmann, 2006, p. 22), the following
secondary resources should be secured if they are
available: (1) A lexicon with glosses in a reference
language. Ideally, everything should be attested in
the texts, but as a practical matter, there will be
words for which we have only a lexical entry and
no instances of use. (2) Paradigms and phonol-
ogy, for the construction of a morphological ana-
lyzer. Ideally, they should be inducible from the
texts, but published grammatical information may
go beyond what is attested in the text.
2.4 Inadequacy of existing efforts
Our key desideratum is support for automatic pro-
cessing across a large range of languages. No data
collection effort currently exists or is proposed, to
our knowledge, that addresses this desideratum.
Traditional language archives such as the Audio
Archive of Linguistic Fieldwork (UC Berkeley),
Documentation of Endangered Languages (Max
Planck Institute, Nijmegen), the Endangered Lan-
guages Archive (SOAS, University of London),
and the Pacific And Regional Archive for Digi-
tal Sources in Endangered Cultures (Australia) of-
fer broad coverage of languages, but the majority
of their offerings are restricted in availability and
do not support machine processing. Conversely,
large-scale data collection efforts by the Linguis-
tic Data Consortium and the European Language
Resources Association cover less than one percent
of the world?s languages, with no evident plans for
major expansion of coverage. Other efforts con-
cern the definition and aggregation of language
resource metadata, including OLAC, IMDI, and
91
CLARIN (Simons and Bird, 2003; Broeder and
Wittenburg, 2006; Va?radi et al, 2008), but this is
not the same as collecting and disseminating data.
Initiatives to develop standard formats for lin-
guistic annotations are orthogonal to our goals.
The success of the project will depend on con-
tributed data from many sources, in many differ-
ent formats. Converting all data formats to an
official standard, such as the RDF-based models
being developed by ISO Technical Committee 37
Sub-committee 4 Working Group 2, is simply im-
practical. These formats have onerous syntactic
and semantic requirements that demand substan-
tial further processing together with expert judg-
ment, and threaten to crush the large-scale collab-
orative data collection effort we envisage, before
it even gets off the ground. Instead, we opt for a
very lightweight format, sketched in the next sec-
tion, to minimize the effort of conversion and en-
able an immediate start. This does not limit the
options of community members who desire richer
formats, since they are free to invest the effort in
enriching the existing data. Such enrichment ef-
forts may gain broad support if they deliver a tan-
gible benefit for cross-language processing.
3 A Simple Storage Model
Here we sketch a simple approach to storage of
texts (including transcribed speech), bitexts, inter-
linear glossed text, and lexicons. We have been
deliberately schematic since the goal is just to give
grounds for confidence that there exists a general,
scalable solution.
For readability, our illustrations will include
space-separated sequences of tokens. However,
behind the scenes these could be represented as
a sequence of pairs of start and end offsets into a
primary text or speech signal, or as a sequence of
integers that reference an array of strings. Thus,
when we write (1a), bear in mind it may be imple-
mented as (1b) or (1c).
(1) a. This is a point of order .
b. (0,4), (5,7), (8,9), (10,15), (16,18), . . .
c. 9347, 3053, 0038, 3342, 3468, . . .
In what follows, we focus on the minimal re-
quirements for storing and disseminating aligned
text, not the requirements for efficient in-memory
data structures. Moreover, we are agnostic about
whether the normalized, tokenized format is stored
entire or computed on demand.
We take an aligned text to be composed of a
series of aligned sentences, each consisting of a
small set of attributes and values, e.g.:
ID: europarl/swedish/ep-00-01-17/18
LANGS: swd eng
SENT: det ga?ller en ordningsfra?ga
TRANS: this is a point of order
ALIGN: 1-1 2-2 3-3 4-4 4-5 4-6
PROVENANCE: pharaoh-v1.2, ...
REV: 8947 2010-05-02 10:35:06 leobfld12
RIGHTS: Copyright (C) 2010 Uni...; CC-BY
The value of ID identifies the document and sen-
tence, and any collection to which the document
belongs. Individual components of the identi-
fier can be referenced or retrieved. The LANGS
attribute identifies the source and reference lan-
guage using ISO 639 codes.2 The SENT attribute
contains space-delimited tokens comprising a sen-
tence. Optional attributes TRANS and ALIGN
hold the translation and alignment, if these are
available; they are omitted in monolingual text.
A provenance attribute records any automatic or
manual processes which apply to the record, and
a revision attribute contains the version number,
timestamp, and username associated with the most
recent modification of the record, and a rights at-
tribute contains copyright and license information.
When morphological annotation is available, it
is represented by two additional attributes, LEX
and AFF. Here is a monolingual example:
ID: example/001
LANGS: eng
SENT: the dogs are barking
LEX: the dog be bark
AFF: - PL PL ING
Note that combining all attributes of these
two examples?that is, combining word-by-word
translation with morphological analysis?yields
interlinear glossed text.
A bilingual lexicon is an indispensable re-
source, whether provided as such, induced from
a collection of aligned text, or created by merg-
ing contributed and induced lexicons. A bilin-
gual lexicon can be viewed as an inventory of
cross-language correspondences between words
or groups of words. These correspondences are
just aligned text fragments, albeit much smaller
than a sentence. Thus, we take a bilingual lexicon
to be a kind of text in which each record contains
a single lexeme and its translation, represented us-
ing the LEX and TRANS attributes we have already
introduced, e.g.:
2http://www.sil.org/iso639-3/
92
ID: swedishlex/v3.2/0419
LANGS: swd eng
LEX: ordningsfra?ga
TRANS: point of order
In sum, the Universal Corpus is represented as
a massive store of records, each representing a
single sentence or lexical entry, using a limited
set of attributes. The store is indexed for effi-
cient access, and supports access to slices identi-
fied by language, content, provenance, rights, and
so forth. Many component collections would be
?unioned? into this single, large Corpus, with only
the record identifiers capturing the distinction be-
tween the various data sources.
Special cases of aligned text and wordlists,
spanning more than 1,000 languages, are Bible
translations and Swadesh wordlists (Resnik et al,
1999; Swadesh, 1955). Here there are obvious
use-cases for accessing a particular verse or word
across all languages. However, it is not neces-
sary to model n-way language alignments. In-
stead, such sources are implicitly aligned by virtue
of their structure. Extracting all translations of
a verse, or all cognates of a Swadesh wordlist
item, is an index operation that returns monolin-
gual records, e.g.:
ID: swadesh/47 ID: swadesh/47
LANGS: fra LANGS: eng
LEX: chien LEX: dog
4 Building the Corpus
Data collection on this scale is a daunting
prospect, yet it is important to avoid the paraly-
sis of over-planning. We can start immediately by
leveraging existing infrastructure, and the volun-
tary effort of interested members of the language
resources community. One possibility is to found
a ?Language Commons,? an open access reposi-
tory of language resources hosted in the Internet
Archive, with a lightweight method for commu-
nity members to contribute data sets.
A fully processed and indexed version of se-
lected data can be made accessible via a web ser-
vices interface to a major cloud storage facility,
such as Amazon Web Services. A common query
interface could be supported via APIs in multi-
ple NLP toolkits such as NLTK and GATE (Bird
et al, 2009; Cunningham et al, 2002), and also
in generic frameworks such as UIMA and SOAP,
leaving developers to work within their preferred
environment.
4.1 Motivation for data providers
We hope that potential contributors of data will
be motivated to participate primarily by agree-
ment with the goals of the project. Even some-
one who has specialized in a particular language
or language family maintains an interest, we ex-
pect, in the universal question?the exploration of
Language writ large.
Data providers will find benefit in the availabil-
ity of volunteers for crowd-sourcing, and tools for
(semi-)automated quality control, refinement, and
presentation of data. For example, a data holder
should be able to contribute recordings and get
help in transcribing them, through a combination
of volunteer labor and automatic processing.
Documentary linguists and computational lin-
guists have much to gain from collaboration. In re-
turn for the data that documentary linguistics can
provide, computational linguistics has the poten-
tial to revolutionize the tools and practice of lan-
guage documentation.
We also seek collaboration with communities of
language speakers. The corpus provides an econ-
omy of scale for the development of literacy mate-
rials and tools for interactive language instruction,
in support of language preservation and revitaliza-
tion. For small languages, literacy in the mother
tongue is often defended on the grounds that it pro-
vides the best route to literacy in the national lan-
guage (Wagner, 1993, ch. 8). An essential ingredi-
ent of any local literacy program is to have a sub-
stantial quantity of available texts that represent
familiar topics including cultural heritage, folk-
lore, personal narratives, and current events. Tran-
sition to literacy in a language of wider commu-
nication is aided when transitional materials are
available (Waters, 1998, pp. 61ff). Mutual bene-
fits will also flow from the development of tools
for low-cost publication and broadcast in the lan-
guage, with copies of the published or broadcast
material licensed to and archived in the corpus.
4.2 Roles
The enterprise requires collaboration of many in-
dividuals and groups, in a variety of roles.
Editors. A critical group are people with suffi-
cient engagement to serve as editors for particular
language families, who have access to data or are
able to negotiate redistribution rights, and oversee
the workflow of transcription, translation, and an-
notation.
93
CL Research. All manual annotation steps need
to be automated. Each step presents a challeng-
ing semi-supervised learning and cross-linguistic
bootstrapping problem. In addition, the overall
measure of success?induction of machine trans-
lation systems from limited resources?pushes the
state of the art (Kumar et al, 2007). Numerous
other CL problems arise: active learning to im-
prove the quality of alignments and bilingual lex-
icons; automatic language identification for low-
density languages; and morphology learning.
Tool builders. We need tools for annotation, for-
mat conversion, spidering and language identifica-
tion, search, archiving, and presentation. Innova-
tive crowd-sourcing solutions are of particular in-
terest, e.g. web-based functionality for transcrib-
ing audio and video of oral literature, or setting up
a translation service based on aligned texts for a
low-density language, and collecting the improved
translations suggested by users.
Volunteer annotators. An important reason for
keeping the data model as lightweight as possible
is to enable contributions from volunteers with lit-
tle or no linguistic training. Two models are the
volunteers who scan documents and correct OCR
output in Project Gutenberg, or the undergraduate
volunteers who have constructed Greek and Latin
treebanks within Project Perseus (Crane, 2010).
Bilingual lexicons that have been extracted from
aligned text collections might be corrected using
crowd-sourcing, leading to improved translation
models and improved alignments. We also see the
Universal Corpus as an excellent opportunity for
undergraduates to participate in research, and for
native speakers to participate in the preservation of
their language.
Documentary linguists. The collection proto-
col known as Basic Oral Language Documentation
(BOLD) enables documentary linguists to collect
2?3 orders of magnitude more oral discourse than
before (Bird, 2010). Linguists can equip local
speakers to collect written texts, then to carefully
?respeak? and orally translate the texts into a refer-
ence language. With suitable tools, incorporating
active learning, local speakers could further curate
bilingual texts and lexicons. An early need is pi-
lot studies to determine costings for different cat-
egories of language.
Data agencies. The LDC and ELRA have a cen-
tral role to play, given their track record in obtain-
ing, curating, and publishing data with licenses
that facilitate language technology development.
We need to identify key resources where negoti-
ation with the original data provider, and where
payment of all preparation costs plus compensa-
tion for lost revenue, leads to new material for the
Corpus. This is a new publication model and a
new business model, but it can co-exist with the
existing models.
Language archives. Language archives have a
special role to play as holders of unique materi-
als. They could contribute existing data in its na-
tive format, for other participants to process. They
could give bilingual texts a distinct status within
their collections, to facilitate discovery.
Funding agencies. To be successful, the Human
Language Project would require substantial funds,
possibly drawing on a constellation of public and
private agencies in many countries. However, in
the spirit of starting small, and starting now, agen-
cies could require that sponsored projects which
collect texts and build lexicons contribute them to
the Language Commons. After all, the most effec-
tive time to do translation, alignment, and lexicon
work is often at the point when primary data is
first collected, and this extra work promises direct
benefits to the individual project.
4.3 Early tasks
Seed corpus. The central challenge, we believe,
is getting critical mass. Data attracts data, and if
one can establish a sufficient seed, the effort will
snowball. We can make some concrete proposals
as to how to collect a seed. Language resources
on the web are one source?the Cru?bada?n project
has identified resources for 400 languages, for ex-
ample (Scannell, 2008); the New Testament of the
Bible exists in about 1200 languages and contains
of the order of 100k words. We hope that exist-
ing efforts that are already well-disposed toward
electronic distribution will participate. We partic-
ularly mention the Language and Culture Archive
of the Summer Institute of Linguistics, and the
Rosetta Project. The latter is already distributed
through the Internet Archive and contains material
for 2500 languages.
Resource discovery. Existing language re-
sources need to be documented, a large un-
94
dertaking that depends on widely distributed
knowledge. Existing published corpora from the
LDC, ELRA and dozens of other sources?a total
of 85,000 items?are already documented in the
combined catalog of the Open Language Archives
Community,3 so there is no need to recreate this
information. Other resources can be logged by
community members using a public access wiki,
with a metadata template to ensure key fields are
elicited such as resource owner, license, ISO 639
language code(s), and data type. This information
can itself be curated and stored in the form of an
OLAC archive, to permit search over the union of
the existing and newly documented items. Work
along these lines has already been initiated by
LDC and ELRA (Cieri et al, 2010).
Resource classification. Editors with knowl-
edge of particular language families will catego-
rize documented resources relative to the needs of
the project, using controlled vocabularies. This
involves examining a resource, determining the
granularity and provenance of the segmentation
and alignment, checking its ISO 639 classifi-
cations, assigning it to a logarithmic size cate-
gory, documenting its format and layout, collect-
ing sample files, and assigning a priority score.
Acquisition. Where necessary, permission will
be sought to lodge the resource in the repository.
Funding may be required to buy the rights to the
resource from its owner, as compensation for lost
revenue from future data sales. Funding may be
required to translate the source into a reference
language. The repository?s ingestion process is
followed, and the resource metadata is updated.
Text collection. Languages for which the avail-
able resources are inadequate are identified, and
the needs are prioritized, based on linguistic and
geographical diversity. Sponsorship is sought
for collecting bilingual texts in high priority lan-
guages. Workflows are developed for languages
based on a variety of factors, such as availability
of educated people with native-level proficiency
in their mother tongue and good knowledge of
a reference language, internet access in the lan-
guage area, availability of expatriate speakers in a
first-world context, and so forth. A classification
scheme is required to help predict which work-
flows will be most successful in a given situation.
3http://www.language-archives.org/
Audio protocol. The challenge posed by lan-
guages with no written literature should not be
underestimated. A promising collection method
is Basic Oral Language Documentation, which
calls for inexpensive voice recorders and net-
books, project-specific software for transcription
and sentence-aligned translation, network band-
width for upload to the repository, and suitable
training and support throughout the process.
Corpus readers. Software developers will in-
spect the file formats and identify high priority for-
mats based on information about resource priori-
ties and sizes. They will code a corpus reader, an
open source reference implementation for convert-
ing between corpus formats and the storage model
presented in section 3.
4.4 Further challenges
There are many additional difficulties that could
be listed, though we expect they can be addressed
over time, once a sufficient seed corpus is estab-
lished. Two particular issues deserve further com-
ment, however.
Licenses. Intellectual property issues surround-
ing linguistic corpora present a complex and
evolving landscape (DiPersio, 2010). For users, it
would be ideal for all materials to be available un-
der a single license that permits derivative works,
commercial use, and redistribution, such as the
Creative Commons Attribution License (CC-BY).
There would be no confusion about permissible
uses of subsets and aggregates of the collected cor-
pora, and it would be easy to view the Universal
Corpus as a single corpus. But to attract as many
data contributors as possible, we cannot make such
a license a condition of contribution.
Instead, we propose to distinguish between:
(1) a digital Archive of contributed corpora that
are stored in their original format and made avail-
able under a range of licenses, offering preserva-
tion and dissemination services to the language
resources community at large (i.e. the Language
Commons); and (2) the Universal Corpus, which
is embodied as programmatic access to an evolv-
ing subset of materials from the archive under
one of a small set of permissive licenses, licenses
whose unions and intersections are understood
(e.g. CC-BY and its non-commercial counterpart
CC-BY-NC). Apart from being a useful service in
its own right, the Archive would provide a staging
95
ground for the Universal Corpus. Archived cor-
pora having restrictive licenses could be evaluated
for their potential as contributions to the Corpus,
making it possible to prioritize the work of nego-
tiating more liberal licenses.
There are reasons to distinguish Archive and
Corpus even beyond the license issues. The Cor-
pus, but not the Archive, is limited to the formats
that support automatic cross-linguistic processing.
Conversely, since the primary interface to the Cor-
pus is programmatic, it may include materials that
are hosted in many different archives; it only needs
to know how to access and deliver them to the user.
Incidentally, we consider it an implementation is-
sue whether the Corpus is provided as a web ser-
vice, a download service with user-side software,
user-side software with data delivered on physical
media, or a cloud application with user programs
executed server-side.
Expenses of conversion and editing. We do not
trivialize the work involved in converting docu-
ments to the formats of section 3, and in manu-
ally correcting the results of noisy automatic pro-
cesses such as optical character recognition. In-
deed, the amount of work involved is one moti-
vation for the lengths to which we have gone to
keep the data format simple. For example, we have
deliberately avoided specifying any particular to-
kenization scheme. Variation will arise as a con-
sequence, but we believe that it will be no worse
than the variability in input that current machine
translation training methods routinely deal with,
and will not greatly injure the utility of the Corpus.
The utter simplicity of the formats also widens the
pool of potential volunteers for doing the manual
work that is required. By avoiding linguistically
delicate annotation, we can take advantage of mo-
tivated but untrained volunteers such as students
and members of speaker communities.
5 Conclusion
Nearly twenty years ago, the linguistics commu-
nity received a wake-up call, when Hale et al
(1992) predicted that 90% of the world?s linguis-
tic diversity would be lost or moribund by the year
2100, and warned that linguistics might ?go down
in history as the only science that presided oblivi-
ously over the disappearance of 90 per cent of the
very field to which it is dedicated.? Today, lan-
guage documentation is a high priority in main-
stream linguistics. However, the field of computa-
tional linguistics is yet to participate substantially.
The first half century of research in compu-
tational linguistics?from circa 1960 up to the
present?has touched on less than 1% of the
world?s languages. For a field which is justly
proud of its empirical methods, it is time to apply
those methods to the remaining 99% of languages.
We will never have the luxury of richly annotated
data for these languages, so we are forced to ask
ourselves: can we do more with less?
We believe the answer is ?yes,? and so we chal-
lenge the computational linguistics community to
adopt a scalable computational approach to the
problem. We need leaner methods for building
machine translation systems; new algorithms for
cross-linguistic bootstrapping via multiple paths;
more effective techniques for leveraging human
effort in labeling data; scalable ways to get bilin-
gual text for unwritten languages; and large scale
social engineering to make it all happen quickly.
To believe we can build this Universal Corpus is
certainly audacious, but not to even try is arguably
irresponsible. The initial step parallels earlier ef-
forts to create large machine-readable text collec-
tions which began in the 1960s and reverberated
through each subsequent decade. Collecting bilin-
gual texts is an orthodox activity, and many alter-
native conceptions of a Human Language Project
would likely include this as an early task.
The undertaking ranks with the largest data-
collection efforts in science today. It is not achiev-
able without considerable computational sophis-
tication and the full engagement of the field of
computational linguistics. Yet we require no fun-
damentally new technologies. We can build on
our strengths in corpus-based methods, linguis-
tic models, human- and machine-supplied annota-
tions, and learning algorithms. By rising to this,
the greatest language challenge of our time, we
enable multi-lingual technology development at a
new scale, and simultaneously lay the foundations
for a new science of empirical universal linguis-
tics.
Acknowledgments
We are grateful to Ed Bice, Doug Oard, Gary
Simons, participants of the Language Commons
working group meeting in Boston, students in
the ?Digitizing Languages? seminar (University of
Michigan), and anonymous reviewers, for feed-
back on an earlier version of this paper.
96
References
Steven Abney. 2007. Semisupervised Learning for
Computational Linguistics. Chapman & Hall/CRC.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media. http://nltk.org/book.
Steven Bird. 2010. A scalable method for preserving
oral literature from small languages. In Proceedings
of the 12th International Conference on Asia-Pacific
Digital Libraries, pages 5?14.
Daan Broeder and Peter Wittenburg. 2006. The IMDI
metadata framework, its current application and fu-
ture direction. International Journal of Metadata,
Semantics and Ontologies, 1:119?132.
Christopher Cieri, Khalid Choukri, Nicoletta Calzo-
lari, D. Terence Langendoen, Johannes Leveling,
Martha Palmer, Nancy Ide, and James Pustejovsky.
2010. A road map for interoperable language re-
source metadata. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Eval-
uation (LREC).
Gregory R. Crane. 2010. Perseus Digital Library:
Research in 2008/09. http://www.perseus.
tufts.edu/hopper/research/current.
Accessed Feb. 2010.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: an
architecture for development of robust HLT appli-
cations. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 168?175. Association for Computational
Linguistics.
Denise DiPersio. 2010. Implications of a permis-
sions culture on the development and distribution
of language resources. In FLaReNet Forum 2010.
Fostering Language Resources Network. http:
//www.flarenet.eu/.
Hale, M. Krauss, L. Watahomigie, A. Yamamoto, and
C. Craig. 1992. Endangered languages. Language,
68(1):1?42.
Nikolaus P. Himmelmann. 2006. Language documen-
tation: What is it and what is it good for? In
Jost Gippert, Nikolaus Himmelmann, and Ulrike
Mosel, editors, Essentials of Language Documenta-
tion, pages 1?30. Mouton de Gruyter.
Human Genome Project. 2007. The science
behind the Human Genome Project. http:
//www.ornl.gov/sci/techresources/
Human_Genome/project/info.shtml.
Accessed Dec. 2007.
W. John Hutchins and Harold L. Somers. 1992. An In-
troduction to Machine Translation. Academic Press.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar, Franz J. Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 42?50,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Mike Maxwell and Baden Hughes. 2006. Frontiers
in linguistic annotation for lower-density languages.
In Proceedings of the Workshop on Frontiers in Lin-
guistically Annotated Corpora 2006, pages 29?37,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Philip Resnik, Mari Broman Olsen, and Mona Diab.
1999. The Bible as a parallel corpus: Annotating
the ?book of 2000 tongues?. Computers and the Hu-
manities, 33:129?153.
Kevin Scannell. 2008. The Cru?bada?n Project: Corpus
building for under-resourced languages. In Cahiers
du Cental 5: Proceedings of the 3rd Web as Corpus
Workshop.
Gary Simons and Steven Bird. 2003. The Open Lan-
guage Archives Community: An infrastructure for
distributed archiving of language resources. Liter-
ary and Linguistic Computing, 18:117?128.
Morris Swadesh. 1955. Towards greater accuracy
in lexicostatistic dating. International Journal of
American Linguistics, 21:121?137.
Tama?s Va?radi, Steven Krauwer, Peter Wittenburg,
Martin Wynne, and Kimmo Koskenniemi. 2008.
CLARIN: common language resources and technol-
ogy infrastructure. In Proceedings of the Sixth Inter-
national Language Resources and Evaluation Con-
ference. European Language Resources Association.
Daniel A. Wagner. 1993. Literacy, Culture, and Devel-
opment: Becoming Literate in Morocco. Cambridge
University Press.
Glenys Waters. 1998. Local Literacies: Theory and
Practice. Summer Institute of Linguistics, Dallas.
Douglas H. Whalen and Gary Simons. 2009. En-
dangered language families. In Proceedings of the
1st International Conference on Language Docu-
mentation and Conservation. University of Hawaii.
http://hdl.handle.net/10125/5017.
Anthony C. Woodbury. 2010. Language documenta-
tion. In Peter K. Austin and Julia Sallabank, edi-
tors, The Cambridge Handbook of Endangered Lan-
guages. Cambridge University Press.
Fei Xia and William D. Lewis. 2007. Multilingual
structural projection across interlinearized text. In
Proceedings of the Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL). Association for Computational
Linguistics.
97
Towards a Data Model for the Universal Corpus
Steven Abney
University of Michigan
abney@umich.edu
Steven Bird
University of Melbourne and
University of Pennsylvania
sbird@unimelb.edu.au
Abstract
We describe the design of a comparable cor-
pus that spans all of the world?s languages and
facilitates large-scale cross-linguistic process-
ing. This Universal Corpus consists of text
collections aligned at the document and sen-
tence level, multilingual wordlists, and a small
set of morphological, lexical, and syntactic an-
notations. The design encompasses submis-
sion, storage, and access. Submission pre-
serves the integrity of the work, allows asyn-
chronous updates, and facilitates scholarly ci-
tation. Storage employs a cloud-hosted file-
store containing normalized source data to-
gether with a database of texts and annota-
tions. Access is permitted to the filestore, the
database, and an application programming in-
terface. All aspects of the Universal Corpus
are open, and we invite community participa-
tion in its design and implementation, and in
supplying and using its data.
1 Introduction
We have previously proposed a community dataset
of annotated text spanning a very large number of
languages, with consistent annotation and format
that enables automatic cross-linguistic processing
on an unprecedented scale (Abney and Bird, 2010).
Here we set out the data model in detail, and invite
members of the computational linguistics commu-
nity to begin work on the first version of the dataset.
The targeted annotation generalizes over three
widely-used kinds of data: (1) simple bitexts, that
is, tokenized texts and their translations, which are
widely used for training machine translation sys-
tems; (2) interlinear glossed text (IGT), which adds
lemmas, morphological features and parts of speech,
and is the de facto standard in the documentary lin-
guistics literature; and (3) dependency parses, which
add a head pointer and relation name for each word,
and are gaining popularity as representations of syn-
tactic structure. We do not expect all texts to have
equal richness of annotation; rather, these are the
degrees of annotation we wish to explicitly accom-
modate. Keeping the annotation lightweight is a pri-
mary desideratum.
We strive for inclusion of as many languages as
possible. We are especially interested in languages
outside of the group of 30 or so for which there
already exist non-trivial electronic resources. Op-
timistically, we aim for a universal corpus, in the
sense of one that covers a widely representative set
of the world?s languages and supports inquiry into
universal linguistics and development of language
technologies with universal applicability.
We emphasize, however, that even if completely
successful, it will be a universal corpus and not the
universal corpus. The term ?universal? should em-
phatically not be understood in the sense of encom-
passing all language annotation efforts. We are not
proposing a standard or a philosophy of language
documentation, but rather a design for one partic-
ular resource. Though the goals with regard to lan-
guage coverage are unusually ambitious, for the sake
of achievability we keep the targeted annotation as
simple as possible. The result is intended to be a sin-
gle, coherent dataset that is very broad in language
coverage, but very thin in complexity of annotation.
120
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 120?127,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Finally, the development of the corpus is an un-
funded, all-volunteer effort. It will only come about
if it wins community buy-in, in the spirit of collab-
orative efforts like Project Gutenberg. We formulate
it as a cooperation among data providers and host-
ing services to provide data in a manner that creates
a single, seamless dataset from the user perspective.
This paper is a first draft of a ?cooperative agree-
ment? that could achieve that goal.
2 A lightweight model for multilingual text
2.1 Media and annotation
In documentary linguistics, a distinction is made
between language documentation, whose concern
is the collection of primary documentation such as
speech recordings and indigenous written works,
and language description, whose concern is the an-
notation and organization of the primary material
(Himmelmann, 1998). We make a similar distinc-
tion between media files and annotation, where ?an-
notation? is understood broadly to include all pro-
cessing steps that make the linguistic contents more
explicit, including plain text rendering, sentence
segmentation, and alignment of translations.
The Corpus consists of annotated documents, in
the sense of primary documents with accompany-
ing annotation. There are many efforts at collecting
documentation for a broad range of languages; what
makes this Corpus distinct is its focus on annotation.
Accordingly, we assume that media files and anno-
tation are handled separately.
For media, the Language Commons collection in
the Internet Archive is a recently-established repos-
itory for redistributable language data that we view
as the primary host.1 For the annotation database, a
primary data host remains to be established, but we
have identified some options. For example, Amazon
Web Services and the Talis Connected Commons
have free hosting services for public data sets.
2.2 The data model in brief
In order to keep the barriers to participation as low as
possible, we have made our target for annotation as
simple as possible. The data model is summarized
in Figure 1. We distinguish between aligned texts
1http://www.archive.org/details/
LanguageCommons
(or parallel texts) and analyzed texts (comparable
texts).
Semantically, the entire collection of aligned texts
constitutes a matrix whose columns are languages
and whose rows are texts. We limit attention to three
levels of granularity: document, sentence, and word.
Each cell is occupied by a string, the typical length
of the string varying with the granularity. We expect
the matrix to be quite sparse: most cells are empty.
The collection of analyzed texts consists, semanti-
cally, of one table per language. The rows represent
words and the columns are properties of the words.
The words may either be tokens in a sentence anal-
ysis, as suggested by the examples, or types repre-
senting dictionary information. The tables are com-
parable, in the sense that they have a common format
and are conducive to language-independent process-
ing, but they are not parallel: the i-th word in the
German table has nothing to do with the i-th word
in the Spanish table.
The tables in Figure 1 constitute the bulk of the
data model. In addition, we assume some auxiliary
information (not depicted) that is primarily organi-
zational. It includes an association between docu-
ments and sentences, the location of documents and
sentences within media files (if applicable), a group-
ing of table rows into ?files,? and a grouping of files
into ?works.? Metadata such as revision information
is attached to files and works. We return below to
the characterization of this auxiliary information.
In contrast to current standard practice, we wish
to emphasize the status of aligned and analyzed text
as annotation of primary documents represented by
media files such as speech recordings or page im-
ages, and we wish to maintain explicit connections
between annotations and primary documents. We
do not insist that the underlying media files be avail-
able in all cases, but we hope to identify them when
possible. However, we focus on storage of the anno-
tation; we assume that media files are in a separate
store, and referenced by external URIs.
2.3 Two implementations: filestore and
database
The data model is abstract, and is implemented in a
couple of ways for different purposes. For distribu-
tion on physical medium or by download, it is most
convenient to implement the data model as actual
121
Aligned Texts Analyzed Texts
deu spa fra eng . . .
d1 sie.. ella.. elle.. she..
d2
...
s1
s2
...
w1
w2
...
deu
sent form lemma morph pos gloss head rel
w1 s1 Ku?he Kuh PL N cow 2 SBJ
w2 s1 sind sein PL V be 0 ROOT
...
spa
sent form lemma morph pos gloss head rel
w1 s2 estas este F.PL D this 2 SPC
w2 s2 floras flora F.PL N flower 3 SBJ
...
...
Figure 1: An overview of the targeted annotation: Aligned Texts in a single matrix having three levels of granularity
(document, sentence, word), and Analyzed Texts grouped by language and annotated down to the word level with
morphological, lexical and syntactic information.
files. Each file contains information corresponding
to some slice of a table, and the structure of the table
is encoded in the file format. On the other hand,
web services are often implemented as databases,
making an implementation of the abstract model as
a database desirable.
A file-based implementation is most familiar, and
most existing resources are available as file collec-
tions. However, even when different existing re-
sources have similar semantics, such as different
parallel text collections, there is considerable variety
in the organization and representation of the infor-
mation. In order to work with multiple such sources,
a substantial amount of housekeeping is required.
One can view our proposed filestore as a normal-
ized form that removes the diversity that only gets in
the way of efficient cross-language processing. In-
deed, our proposed format for analyzed text hews
intentionally close to the format used in the CoNLL
dependency-parsing shared tasks, which provided
a normal form into which data from multiple tree-
banks was mapped (Buchholz et al, 2006).
When an existing resource is included in the Cor-
pus, we assume that it remains externally available
in its original form, but a copy is imported into the
Corpus filestore in which every file has been pre-
processed into one of a set of simple file formats
implementing the model of Figure 1, following a
consistent scheme for filenames, with utf8 charac-
ter encoding, and capturing any available alignment
information in an auxiliary table. Distribution of the
Corpus via physical media or download simply in-
volves copying the filestore.
The filestore is organized around material pro-
vided by individual data providers, or ?authors,? and
maintains the identity of a data provider?s contribu-
tion as a distinct intellectual ?work.? Works provide
an appropriate unit to which to attach edition and
rights metadata.
In addition to the filestore, the texts and align-
ments are imported into a collection of database ta-
bles that can be queried efficiently.
In section 3 we describe a simple file-based im-
plementation of the data model, and show the variety
of familiar file types that find a natural place in the
model. In section 4 we describe the tabular storage
model.
3 Filestore implementation
Despite the simplicity of the data model, it captures
a substantial, even surprising, variety of commonly-
used textual data file types.
Document-aligned text. Parallel corpora are most
commonly aligned at the document level. Typically,
each translation of a document is contained in a file,
and there is some way of indicating which files are
mutual translations of the same document. The con-
122
tents of a file, as a single string, represents one cell
in the Aligned Text matrix in Figure 1 (at the ?doc-
ument? level of granularity). A document, compris-
ing a collection of mutual translations, corresponds
to a row of the matrix.
As normal form, we propose the convention of
using filenames that incorporate a language iden-
tifier and a document identifier. For example,
1001-eng.txt and 1001-deu.txt are the En-
glish and German files representing mutual transla-
tions of some hypothetical document 1001.
Language identifiers are ISO 639-3 language
code, supplemented by the Linguist List local-use
codes and subgroup and dialect identifiers.
Sentence-aligned text. At a finer grain, paral-
lel corpora may be aligned at the sentence level.
Each file contains the translation of one document,
segmented into one sentence per line. Our nor-
mal form uses the same filename convention as
for document-aligned text, to indicate which files
are mutual translations. We use the file suffix
?.snt? to indicate a file with one sentence per
line. This incidentally indicates which document
a set of sentences came from, since the filenames
share a document identifier. For example, the file
1001-deu.snt contains the sentence-segmented
version of 1001-deu.txt.
In the canonical case, each file in a group of
aligned files contains the same number of sentences,
and the sentences line up one-to-one. The group
of aligned files corresponds to a set of rows in the
Aligned Text matrix, at the ?sentence? level of gran-
ularity.
There are cases in which the sentence alignment
between documents is not one-to-one. Even in this
case, we can view the alignment as consisting of a
sequence of ?beads? that sometimes contain multi-
ple sentences in one language. If we normalize the
file to one in which the group of sentences belong-
ing to a single bead are concatenated together as a
?translational unit,? we reduce this case to the one-
to-one case, though we do lose the information about
orthographic sentence boundaries internal to a bead.
Preserving the original sentences would necessi-
tate an extension to the data model. A typical ap-
proach is to store the alignments in a table, where
n-way alignments are indicated using n-tuples of in-
tegers. We leave this as a point for future consider-
ation. We also put aside consideration of word-level
document alignment.
Translation dictionaries. A translation dictionary
contains word translations in multiple languages.
One representation looks just like sentence-aligned
text, except that each file contains one entry per line
instead of one sentence per line. Each file in an
aligned set contains the same number of entries, and
the entries line up one-to-one across files. This is
the representation we take as our normal form. We
also use the same filename convention, but with suf-
fix .tdi for translation dictionary.
A translation dictionary corresponds to a set of
rows in the Aligned Text matrix, at the ?word? level
of granularity. A translation dictionary would typ-
ically be derived from a large number of text doc-
uments, so each translation dictionary will typically
have a unique document identifier, and will not align
with files at the sentence or document granularity.
Transcriptions and segmentations. When one
begins with a sound recording or with page images
from a print volume that has been scanned, a first
step is conversion to plain text. We will call this a
?transcription? both for the case where the original
was a sound file and for the case where the origi-
nal was a page image. Transcriptions fit into our
data model as the special case of ?document-aligned
text? in which only one language is involved. We
assume that the Aligned Text matrix is sparse, and
this is the extreme case in which only one cell in a
row is occupied. The connection between the tran-
script?s document identifier and the original media
file is recorded in an auxiliary metadata file.
After transcription, the next step in processing is
to identify the parts of the text that are natural lan-
guage (as opposed to markup or tables or the like),
and to segment the natural language portion into
sentences. The result is sentence-segmented text.
Again, we treat this as the special case of sentence-
aligned text in which only one language is involved.
Analyzed text. A variety of different text file types
can be grouped together under the heading of an-
alyzed text. The richest example we consider is
dependency parse structure. One widely-used file
representation has one word token per line. Each
123
line consists of tab-separated fields containing at-
tributes of the word token. There is some varia-
tion in the attributes that are specified, but the ones
used in the Analyzed Text tables of our data model
are typical, namely: sentence identifier, wordform,
lemma, morphological form, gloss, part of speech,
head (also called governor), and relation (also called
role). Sentence boundaries are not represented as to-
kens; rather, tokens belonging to the same sentence
share the same value for sentence identifier. We con-
tinue with the same filename convention as before;
for Analyzed Text files, the suffix is .tab.
Many different linguistic annotations are natu-
rally represented as special cases of Analyzed Text.
? Tokenized text in ?vertical format? is the spe-
cial case in which the only column is the word-
form column. We include the sentence ID col-
umn as well, in lieu of sentence-boundary to-
kens.
? POS-tagged text adds the part of speech col-
umn.
? The information in the word-by-word part of
interlinear glossed text (IGT) typically includes
the wordform, lemma, morph, and gloss; again
we also include the sentence ID column.
? A dependency parse, as already indicated, is the
case in which all columns are present.
In addition, the format accommodates a variety
of monolingual and multilingual lexical resources.
Such lexical resources are essential, whether manu-
ally curated or automatically extracted.
? A basic dictionary consists of a sequence of en-
tries, each of which contains a lemma, part of
speech, and gloss. Hence a dictionary is nat-
urally represented as analyzed text containing
just those three columns. The entries in a dic-
tionary are word types rather than word tokens,
so the wordform and sentence ID columns are
absent.
? If two or more lexicons use the same glosses,
the lexicons are implicitly aligned by virtue of
the glosses and there is no need for overt align-
ment information. This is a more flexible repre-
sentation than a translation dictionary: unlike a
translation dictionary, it permits multiple words
to have the same gloss (synonyms), and it adds
parts of speech.
4 Database implementation
An alternative implementation, appropriate for de-
ployment of the Corpus as a web service, is as a
normalized, multi-table database. In this section
we drill down and consider the kinds of tables and
records that would be required in order to represent
our abstract data model. We will proceed by way
of example, for each of the kinds of data we would
like to accommodate. Each example is displayed as
a record consisting of a series of named fields.
Note that we make no firm commitment as to the
physical format of these records. They could be se-
rialized as XML when the database is implemented
as a web service. Equally, they could be represented
using dictionaries or tuples when the database is ac-
cessed via an application program interface (API).
We will return to this later.
4.1 The Aligned Text matrix
The Aligned Text matrix is extremely sparse. We
use the more flexible representation in which each
matrix cell is stored using a separate record, where
the record specifies (index, column) pairs. For ex-
ample, the matrix row
deu spa fra
d1 Sie... Ella...
d2 Mein... Mon...
is represented as
Document Table
DID LANG TEXT
1 deu Sie...
1 spa Ella...
2 deu Mein...
2 fra Mon...
(The ellipses are intended to indicate that each cell
contains the entire text of a document.) We have also
added an explicit document ID.
When we consider entries at the sentence and
word levels, we require both a document ID and sen-
tence or word IDs within the document. Figure 2
shows an example of two sentences from the same
document, translated into two languages. Note that
we can think of DID + LANG as an identifier for a
monolingual document instance, and DID + LANG +
SID identifies a particular sentence in a monolingual
document.
124
DID LANG SID TEXT
1 deu 1 Der Hund bellte.
1 eng 1 the dog barked.
1 deu 2 Mein Vater ist Augenarzt.
1 eng 2 My father is an optometrist.
Figure 2: Two sentences with two translations. These are
sentence table records.
In short, we implement the Aligned Text matrix as
three database tables. All three tables have columns
DID, LANG, and TEXT. The sentence table adds SID,
and the word table adds WID instead of SID. (The
words are types, not tokens, hence are not associated
with any particular sentence.)
4.2 The Analyzed Text tables
The implementation of the Analyzed Text tables is
straightforward. We add a column for the document
ID, and we assume that sentence ID is relative to
the document. We also represent the word token ID
explicitly, and take it to be relative to the sentence.
Finally, we add a column for LANG, so that we have
a single table rather than one per language.
The first record from the German table in Figure 1
is implemented as in Figure 3. This is a record from
a dependency parse. Other varieties of analyzed text
leave some of the columns empty, as discussed in the
previous section.
There is a subtlety to note. In the sentence table,
the entry with DID 1, SID 1, and LANG ?deu? is un-
derstood to be a translation of the entry with DID 1,
SID 1, and LANG ?eng.? That is not the case with
records in the analyzed-text table. Word 1 in the En-
glish sentence 1 of document 1 is not necessarily a
translation of word 1 in the German sentence 1 of
document 1.
A few comments are in order about the meanings
of the columns. The wordform is the attested, in-
flected form of the word token. The LEMMA pro-
vides the lexical form, which is the headword un-
der which one would find the word in a dictionary.
The MORPH field provides a symbolic indicator of
the relationship between the lemma and the word-
form. For example, ?Ku?he? is the PL form of the
lemma ?Kuh.?
This approach encompasses arbitrary morpholog-
ical processes. For example, Hebrew lomedet may
be represented as the PRESPTC.FEM.SG form of
lmd, (?to learn?).
When we represent dictionaries, the records are
word types rather than word tokens. We assign a
document ID to the dictionary as a whole, but by
convention take the SID to be uniformly 0.
Ultimately, the POS and GLOSS fields are in-
tended to contain symbols from controlled vocab-
ularies. For the present, the choice of controlled
vocabulary is up to the annotator. For the GLOSS
field, an option that has the benefit of simplicity is
to use the corresponding word from a reference lan-
guage, but one might equally well use synset identi-
fiers from WordNet, or concepts in some ontology.
4.3 The auxiliary tables
The auxiliary tables were not shown in the abstract
data model as depicted in Figure 1. They primar-
ily include metadata. We assume a table that asso-
ciates each document ID with a work, and a table
that provides metadata for each work. The Corpus
as a whole is the sum of the works.
In the spirit of not duplicating existing efforts, we
?outsource? the bulk of the metadata to OLAC (Si-
mons and Bird, 2003). If a work has an OLAC entry,
we only need to associate the internal document ID
to the OLAC identifier.
There is some metadata information that we
would like to include for which we cannot refer to
OLAC.
? Provenance: how the annotation was con-
structed, e.g., who the annotator was, or what
software was used if it was automatically cre-
ated.
? Rights: copyright holder, license category cho-
sen from a small set of interoperable licenses.
? Standards: allows the annotator to indicate
which code sets are used for the MORPH, POS,
and GLOSS fields. We would like to be able
to specify a standard code set for each, in the
same way that we have specified ISO 639-3 for
language codes. Consensus has not yet crystal-
lized around any one standard, however.
The auxiliary tables also associate documents
with media files. We assume a table associating
document IDs with a media files, represented by
125
DID LANG SID WID FORM LEMMA MORPH POS GLOSS HEAD REL
123 deu 1 1 Ku?he Kuh PL N cow 2 SBJ
Figure 3: A single word from a dependency parse. This is a record from the analyzed-text table.
their URLs, and a table associating sentences (DID
+ SID) with locations in media files.
Note that, as we have defined the file and tabu-
lar implementations, there is no need for an explicit
mapping between document IDs and filenames. A
filename is always of the form did-lang.suffix,
where the suffix is .txt for the document table,
.snt for the sentence table, .tdi for the word ta-
ble, and .tab for the analyzed-text table. Each file
corresponds to a set of records in one of the tables.
5 Cloud Storage and Interface
A third interface to the Corpus is via an applica-
tion programming interface. We illustrate a possi-
ble Python API using Amazon SimpleDB, a cloud-
hosted tuple store accessed via a web services in-
terface.2 An ?item? is a collection of attribute-
value pairs, and is stored in a ?domain.? Items,
attributes, and domains are roughly equivalent to
records, fields, and tables in a relational database.
Unlike relational databases, new attributes and do-
mains can be added at any time.
Boto is a Python interface to Amazon Web Ser-
vices that includes support for SimpleDB.3 The fol-
lowing code shows an interactive session in which a
connection is established and a domain is created:
>>> import boto
>>> sdb = boto.connect_sdb(PUBLIC_KEY, PRIVATE_KEY)
>>> domain = sdb.create_domain(?analyzed_text?)
We can create a new item, then use Python?s dic-
tionary syntax to create attribute-value pairs, before
saving it:
>>> item = domain.new_item(?123?)
>>> item[?DID?] = ?123?
>>> item[?LANG?] = ?deu?
>>> item[?FORM?] = ?Ku?he?
>>> item[?GLOSS?] = ?cow?
>>> item[?HEAD?] = ?2?
>>> item.save()
Finally, we can retrieve an item by name, or submit
a query using SQL-like syntax.
2http://aws.amazon.com/simpledb/
3http://code.google.com/p/boto/
>>> sdb.get_attributes(domain, ?123?)
?LANG?: ?deu?, ?HEAD?: ?2?, ?DID?: ?123?,
?FORM?: ?Ku?he?, ?GLOSS?: ?cow?
>>> sdb.select(domain,
... ?select DID, FORM from analyzed_text
... where LANG = "deu"?)
[?DID?: ?123?, ?FORM?: ?Ku?he?]
We have developed an NLTK ?corpus reader?
which understands the Giza and NAACL03 formats
for bilingual texts, and creates a series of records for
insertion into SimpleDB using the Boto interface.
Other formats will be added over time.
Beyond the loading of corpora, a range of query
and report generation functions are needed, as illus-
trated in the following (non-exhaustive) list:
? lookup(lang=ENG, rev="1.2b3", ...): find all
items which have the specified attribute val-
ues, returning a list of dictionaries; following
Python syntax, we indicate this variable num-
ber of keyword arguments with **kwargs.
? extract(type=SENT, lang=[ENG, FRA, DEU],
**kwargs): extract all aligned sentences involv-
ing English, French, and German, which meet
any further constraints specified in the keyword
arguments. (When called extract(type=SENT)
this will extract all sentence alignments across
all 7,000 languages, cf Figure 1.)
? dump(type=SENT, format="giza", lang=[ENG,
FRA], **kwargs): dump English-French bitext
in Giza format.
? extract(type=LEX, lang=[ENG, FRA, ...],
**kwargs): produce a comparative wordlist for
the specified languages.
? dump(type=LEX, format="csv", lang=[ENG,
FRA, ...], **kwargs): produce the wordlist in
comma-separated values format.
Additional functions will be required for discov-
ery (which annotations exist for an item?), naviga-
tion (which file does an item come from?), citation
(which publications should be cited in connection
with these items?), and report generation (what type
and quantity of material exists for each language?).
126
Further functionality could support annotation.
We do not wish to enable direct modification of
database fields, since everything in the Corpus
comes from contributed corpora. Instead, we could
foster user input and encourage crowdsourcing of
annotations by developing software clients that ac-
cess the Corpus using methods such as the ones al-
ready described, and which save any new annota-
tions as just another work to be added to the Corpus.
6 Further design considerations
Versioning. When a work is contributed, it comes
with (or is assigned) a version, or ?edition.? Multi-
ple editions of a work may coexist in the Corpus, and
each edition will have distinct filenames and identi-
fiers to avoid risk of collision. Now, it may hap-
pen that works reference each other, as when a base
text from one work is POS-tagged in another. For
this reason, we treat editions as immutable. Modi-
fications to a work are accumulated and released as
a new edition. When a new edition of a base text
is released, stand-off annotations of that text (such
as the POS-tagging in our example) will need to be
updated in turn, a task that should be largely auto-
mated. A new edition of the annotation, anchored to
the new edition of the base text, is then released. The
old editions remain unchanged, though they may be
flagged as obsolete and may eventually be deleted.
Licensing. Many corpora come with license con-
ditions that prevent them from being included. In
some cases, this is due to license fees that are paid
by institutional subscription. Here, we need to ex-
plore a new subscription model based on access. In
some cases, corpus redistribution is not permitted,
simply in order to ensure that all downloads occur
from one site (and can be counted as evidence of
impact), and so that users agree to cite the scholarly
publication about the corpus. Here we can offer data
providers a credible alternative: anonymized usage
tracking, and an automatic way for authors to iden-
tify the publications associated with any slice of the
Corpus, facilitating comprehensive citation.
Publication. The Corpus will be an online publi-
cation, with downloadable dated snapshots, evolv-
ing continually as new works and editions are added.
An editorial process will be required, to ensure that
contributions are appropriate, and to avoid spam-
ming. A separate staging area would facilitate
checking of incoming materials prior to release.
7 Conclusion
We have described the design and implementation
of a Universal Corpus containing aligned and anno-
tated text collections for the world?s languages. We
follow the same principles we set out earlier (Abney
and Bird, 2010, 2.2), promoting a community-level
effort to collect bilingual texts and lexicons for as
many languages as possible, in a consistent format
that facilitates machine processing across languages.
We have proposed a normalized filestore model that
integrates with current practice on the supply side,
where corpora are freestanding works in a variety
of formats and multiple editions. We have also de-
vised a normalized database model which encom-
passes the desired range of linguistic objects, align-
ments, and annotations. Finally, we have argued that
this model scales, and enables a view of the Univer-
sal Corpus as a vast matrix of aligned and analyzed
texts spanning the world?s languages, a radical de-
parture from existing resource creation efforts in lan-
guage documentation and machine translation.
We invite participation by the community in elab-
orating the design, implementing the storage model,
and populating it with data. Furthermore, we seek
collaboration in using such data as the basis for
large-scale cross-linguistic analysis and modeling,
and in facilitating the creation of easily accessible
language resources for the world?s languages.
References
Steven Abney and Steven Bird. 2010. The Human
Language Project: building a universal corpus of the
world?s languages. In Proc. 48th ACL, pages 88?97.
Association for Computational Linguistics.
Sabine Buchholz, Erwin Marsi, Yuval Krymolowski, and
Amit Dubey. 2006. CoNLL-X shared task: Multi-
lingual dependency parsing. http://ilk.uvt.
nl/conll/. Accessed May 2011.
Nikolaus P. Himmelmann. 1998. Documentary and de-
scriptive linguistics. Linguistics, 36:161?195.
Gary Simons and Steven Bird. 2003. The Open Lan-
guage Archives Community: An infrastructure for dis-
tributed archiving of language resources. Literary and
Linguistic Computing, 18:117?128.
127
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146?154,
Dublin, Ireland, August 23 2014.
Experiments in Sentence Language Identification with Groups of Similar
Languages
Ben King
Department of EECS
University of Michigan
Ann Arbor
benking@umich.edu
Dragomir Radev
Department of EECS
School of Information
University of Michigan
Ann Arbor
radev@umich.edu
Steven Abney
Department of Linguistics
University of Michigan
Ann Arbor
abney@umich.edu
Abstract
Language identification is a simple problem that becomes much more difficult when its usual
assumptions are broken. In this paper we consider the task of classifying short segments of text in
closely-related languages for the Discriminating Similar Languages shared task, which is broken
into six subtasks, (A) Bosnian, Croatian, and Serbian, (B) Indonesian and Malay, (C) Czech
and Slovak, (D) Brazilian and European Portuguese, (E) Argentinian and Peninsular Spanish,
and (F) American and British English. We consider a number of different methods to boost
classification performance, such as feature selection and data filtering, but we ultimately find that
a simple na??ve Bayes classifier using character and word n-gram features is a strong baseline that
is difficult to improve on, achieving an average accuracy of 0.8746 across the six tasks.
1 Introduction
Language identification constitutes the first stage of many NLP pipelines. Before applying tools trained
on specific languages, one must determine the language of the text. It is also is often considered to be a
solved task because of the high accuracy of language identification methods in the canonical formulation
of the problem with long monolingual documents and a set of mostly dissimilar languages to choose
from. We consider a different setting with much shorter text in the form of single sentences drawn from
very similar languages or dialects.
This paper describes experiments related to and our submissions to the Discriminating Similar Lan-
guages (DSL) shared task. This shared task has six subtasks, each a classification task in which a sentence
must be labeled as belonging to a small set of related languages:
? Task A: Bosnian vs. Croatian vs. Serbian
? Task B: Indonesian vs. Malay
? Task C: Czech vs. Slovak
? Task D: Brazilian vs. European Portuguese
? Task E: Argentinian vs. Peninsular Spanish
? Task F: American vs. British English
The first three tasks involve classes that could be rightly called separate languages or dialects. The
classes of each of the final three tasks have high mutual intelligibility and are so similar that some
linguists may not even classify them as separate dialects. We will use the term ?language variant? to
refer to such classes.
In this paper we experiment with several types of methods aimed at improving the classification ac-
curacy of these tasks: machine learning methods, data pre-processing, feature selection, and additional
training data. We find that a simple na??ve Bayes classifier using character and word n-gram features is
a strong baseline that is difficult to improve on. Because this paper covers so many different types of
methods, its format eschews the standard ?Results? section, instead providing comparisons of methods
as they are presented.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
146
2 Related Work
Recent directions in language identification have included finer-grained language identification (King
and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs
(Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely
related languages.
Language identification for closely related languages has been considered by several researchers,
though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguish-
ing Croatian from Serbian and Slovenian is explored by Ljube?si?c et al. (2007), who used a list of most
frequent words along with a Markov model and a word blacklist, a list of words that are not allowed
to appear in a certain language. A similar approach was later used by Tiedemann and Ljube?si?c (2012)
to distinguish Bosnian, Croatian, and Serbian. They further develop the idea of a blacklist classifier,
loosening the binary restriction of the earlier work?s blacklist and considering the frequencies of words
rather than their absolute counts. This blacklist classifier is able to outperform a na??ve Bayes classifier
with large amounts of training data. They also find training on parallel data to be important, as it al-
lows the machine learning methods to pick out features relating to the differences between the languages
themselves, rather than learning differences in domain.
Zampieri et al. consider classes that would be most often classified as language varieties rather than
separate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al.,
2013). A similar problem of distinguishing among Chinese text from mainland China, Singapore, and
Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity
between a document and a corpus according to the size of the intersection between the sets of types in
each.
A similar, but somewhat different problem of automatically identifying lexical variants between
closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they
are able to identify Netherlandic Dutch synonyms for words from Belgian Dutch.
3 Data
This paper?s training data and evaluation data both come from the DSL corpus collection (DSLCC)
(Tan et al., 2014). We use the training section of this data for training and the development section for
evaluation. The training section consists of 18,000 labeled instances per class, while the development
section has 2,000 labeled instances per class.
In order to try to increase classifier accuracy (and to avoid the problems with the task F training
data), we decided to collect additional training data for each open-class task. For each task, we collected
newspaper text from the appropriate websites for each of the 2?3 languages. We used regular expressions
to split the text into sentences, and created a set of rules to filter out strings that were unlikely to be good
sentences. Because the pages on the newspaper websites tended to have some boilerplate text, we collated
all the sentences and only kept one copy of each sentence.
Task Language/Dialect Newspaper Sentences Words
A
Bosnian Nezavisne Novine 175,741 3,250,648
Croatian Novi List 231,271 4,591,318
Serbian Ve?cernje Novosti 239,390 5,213,507
B
Indonesian Kompas 114,785 1,896,138
Malay Berita Harian 36,144 695,597
C
Czech Den??k 160,972 2,432,393
Slovak Denn??k SME 62,908 970,913
D
Brazilian Portuguese O Estado de S. Paulo 558,169 11,199,168
European Portuguese Correio da Manh?a 148,745 2,979,904
E
Argentinian Spanish La Naci?on 333,246 7,769,941
Peninsular Spanish El Pa??s 195,897 4,329,480
F
American English The New York Times 473,350 10,491,641
British English The Guardian 971,097 20,288,294
Table 1: Sources and amounts of training data collected for the open track for each task.
147
In order to create balanced training data, for each task we downsampled the number of sentences of
the larger collection(s) to match the number of sentences in the smaller collection. For example, we
downsampled the British English collection to 473,350 sentences and combined it with the American
English sentences to create the training data for English. Figure 1 shows results of training using this
external data.
3.1 Features
We use many types of features that have been found to be useful in previous language identification
work: word unigrams, word bigrams, and character n-grams (2 ? n ? 6). Character n-grams are simply
substrings of the sentence and may include in addition to letters, whitespace, punctuation, digits, and
anything else that might be in the sentence. Words, for the purpose of word unigrams and bigrams, are
simply maximal tokens not containing any punctuation, digit, or whitespace.
When instances are encoded into feature vectors, each feature has a value equal to the number of times
it occured in the corresponding sentence, so the majority of features have a value of 0 for any given
instance, but it is possible for a feature to occur multiple times in a sentence and have a value greater
than 1.0 in the feature vector. Table 2 below compares the performance of a na??ve Bayes classifier using
each of the different feature groups below.
Word Character
Task All 1 2 2 3 4 5 6
Bosnian/Croatian/Serbian 0.9348 0.9290 0.8183 0.7720 0.8808 0.9412 0.9338 0.9323
Indonesian/Malay 0.9918 0.9943 0.9885 0.8545 0.9518 0.9833 0.9908 0.9930
Czech/Slovak 0.9998 1.0000 0.9985 0.9980 0.9998 0.9998 1.0000 1.0000
Portuguese 0.9535 0.9468 0.9493 0.7935 0.8888 0.9318 0.9468 0.9570
Spanish 0.8623 0.8738 0.8625 0.7673 0.8273 0.8513 0.8610 0.8660
English 0.4970 0.4948 0.5005 0.4825 0.4988 0.5010 0.5048 0.4993
Average 0.8732 0.8731 0.8529 0.7780 0.8412 0.8681 0.8729 0.8746
Table 2: Accuracies compared for different sets of features compared. The classifier used here is na??ve
Bayes.
4 Methods
Our baseline method against which we compare all other models is a na??ve Bayes classifier using word
unigram features trained on the DSL-provided training data. The methods we compare to it can be
broken into three classes: other machine learning methods, feature selection methods, and data filtering
methods.
The classification pipeline used here has the following stages: (1) data filtering, (2) feature extraction,
(3) feature selection, (4) training, and (5) classification.
4.1 Machine Learning Methods
We will use the following notation throughout this section. An instance x, that is, a sentence to be
classified, with a corresponding class label y is encoded into a feature vector f(x), where each entry
is an integer denoting how many times the feature corresponding to that entry?s index occurred in the
sentence. The class label here is a language and it?s drawn from a small set y ? Y .
In addition to the na??ve Bayes classifier, we also experiment with two versions of logistic regression
and a support vector machine classifier. The MALLET machine learning library implementations are
used for the first three classifiers (McCallum, 2002) and SVMLight is used for the fourth (Joachims, ).
Na??ve Bayes A na??ve Bayes classifier models the class label as an independent combination of input
features.
148
P (y|f(x)) =
1
P (f(x))
P (y)
n
?
i=1
P (f(x)
i
|y) (1)
As na??ve Bayes is a generative classifier, it has been shown to be able to outperform discriminative
classifiers when the number of training instances is small compared to the number of features (Ng and
Jordan, 2002). This classifier is additionally advantageous in that it has a simple closed-form solution
for maximizing its log likelihood.
Logistic Regression A logistic regression classifier is a discriminative classifier whose parameters are
encoded in a vector ?. The conditional probability of a class label over an instance (x, y) is modeled as
follows:
P (y|x; ?) =
1
Z(x; ?)
exp {f(x, y) ? ?} ; Z(x, ?) =
?
y?Y
exp {f(x, y) ? ?} (2)
The parameter vector ? is commonly estimated by maximizing the log-likelihood of this function over
the set of training instances (x, y) ? T in the following way:
? = argmax
?
?
(x,y)?T
logP (y
i
|x
i
; ?)? ?R(?) (3)
The term R(?) above is a regularization term. It is common for such a classifier to overfit the pa-
rameters to the training data. To keep this from happening, a regularization term can be added which
keeps the parameters in ? from growing too large. Two common choices for this function are L2 and L1
normalization:
R
L2
= ||?||
2
2
=
n
?
i=1
?
2
i
, R
L1
= ||?||
1
=
n
?
i=1
|?
i
| (4)
L2 regularization is well-grounded theoretically, as it is equivalent to a model with a Gaussian prior
on the parameters (Rennie, 2004). But L1 regularization has a reputation for enforcing sparsity on the
parameters. In fact, it has been shown to be quite effective when the number of irrelevant dimensions is
greater than the number of training examples, which we expect to be the case with many of the tasks in
this paper (Ng, 2004).
Support Vector Machines A support vector machine (SVM) is a type of linear classifier that attempts
to find a boundary that linearly separates the training data with the maximum possible margin. SVMs
have been shown to be a very efficient and high accuracy method to classify data across a wide variety
of different types of tasks (Tsochantaridis et al., 2004).
Table 3 below compares these machine learning methods. Because of its consistently good perfor-
mance across tasks, we use a na??ve Bayes classifier throughout the rest of the paper.
4.2 Feature Selection Methods
We expect that the majority of features are not relevant to the classification task, and so we experimented
with several methods of feature selection, both manual and automatic.
Information Gain As a fully automatic method of feature extraction, we used information gain to
score features according to their expected usefulness. Information gain (IG) is an information theoretic
concept that (colloquially) measures the amount of knowledge about the class label that is gained by
having access to a specific feature. If f is the occurence an individual feature and
?
f the non-occurence
of a feature, we measure its information gain by the following formula:
G(f) = P (f)
?
?
?
y?Y
P (y|f)logP (y|f)
?
?
+ P (
?
f)
?
?
?
y?Y
logP (y|
?
f)logP (y|
?
f)
?
?
(5)
149
Task
Logistic
Regression
(L2-norm)
Logistic
Regression
(L1-norm)
Na??ve Bayes SVM
Bosnian/Croatian/Serbian 0.9138 0.9135 0.9290 0.9100
Indonesian/Malay 0.9878 0.9810 0.9943 0.9873
Czech/Slovak 0.9983 0.9958 1.0000 0.9985
Portuguese 0.9383 0.9368 0.9468 0.9325
Spanish 0.8843 0.8770 0.8738 0.8768
English 0.5000 0.4945 0.4948 0.4958
Average 0.8704 0.8648 0.8731 0.8668
Table 3: Comparison of different machine learning methods using word unigram features on the six
tasks.
To reduce the number of features being used in classification (and to hopefully remove irrelevant
features), we choose the 10,000 features with the highest IG scores. IG considers each feature indepen-
dently, so it is possible that redundant feature sets could be chosen. For example, it might happen that
both the quadrigram ther and the trigram the score highly according to IG and are both selected, even
though they are highly correlated with one another.
Parallel Text Feature Selection Because IG feature selection often seemed to choose features more
related to differences in domain than to differences in language (see Table 7), we wanted to try to isolate
features that are specific to language differences. It has been shown in previous work that training on
parallel text can help to isolate language differences since the domains of the languages are identical
(Tiedemann and Ljube?si?c, 2012). For each of the tasks,
1
we use translations of the complete Bible as a
parallel corpus, running IG feature selection exactly as above. Table 4 below gives more details about
the texts used.
Task Language/Dialect Bible
B
Indonesian Alkitab dalam Bahasa Indonesia Masa Kini
Malay 2001 Today?s Malay Version
C
Czech Cesk?y studijn?? preklad
Slovak Slovensk?y Ekumenick?y Biblia
D
Brazilian Portuguese a B
?
IBLIA para todos
European Portuguese Almeida Revista e Corrigida (Portugal)
E
Argentinian Spanish La Palabra (versi?on hispanoamericana)
Peninsular Spanish La Palabra (versi?on espa?nola)
F
American English New International Version
British English New International Version Anglicized
Table 4: Bibles used as parallel corpora for feature selection.
Manual Feature Selection We also used manual feature selection, selecting features to use in the clas-
sifiers from lists published on Wikipedia comparing the two languages. Of course some of the features in
lists like these are features that are quite difficult to detect using NLP (especially before the language has
been identified) such as characteristic passive or genitive constructions. But there are many features that
we are able to detect and use in a list of manually selected features, such as character n-grams relating
to morphology and spelling and word n-grams relating to vocabulary differences.
Table 5 below compares these feature selection methods on each task. Since the manual feature selec-
tion suggested all types of features, including character n-gram and word unigram and bigram features,
the experiments in this section use all features described in Section 3.1. The results show that any type
of feature selection consistently hurts performance, though IG hurts the least, and it should be noted
that in certain cases with other machine learning methods, IG feature selection actually yielded better
1
excluding Task A, for which we were unable to find a Bible in Latin-script Serbian or any Bible in Bosnian
150
performance than all features. That the feature selection methods designed to isolate language-specific
features performed so poorly is one indicator that the labeled data has additional differences that are not
tied to the languages themselves. We discuss this idea further in Section 5.
Task No feature selection IG Parallel Manual
Bosnian/Croatian/Serbian 0.9348 0.9300 ? 0.6328
Indonesian/Malay 0.9918 0.9768 0.8093 0.8485
Czech/Slovak 0.9998 0.9995 0.9940 0.8118
Portuguese 0.9535 0.9193 0.7215 0.6888
Spanish 0.8623 0.8310 0.5210 0.7023
English 0.4970 0.4978 0.5020 0.5053
Average 0.8732 0.8590 ? 0.6982
Table 5: Comparison of manual and automatic feature selection methods. IG and parallel feature selec-
tion both use the 10,000 features with the highest IG scores.
4.3 Data Filtering Methods
English Word Removal In looking through the training data for the non-English tasks, we observed
that it was not uncommon for sentences in these languages to contain English words and phrases. Be-
cause foreign words should be independent of the language/dialect used, English words included in the
sentences for other tasks should just be noise that, if removed will improve classification performance.
For each of the non-English tasks (A, B, C, D, and E), we create a new training set for identifying
English/non-English words by mixing together 1,000 random English words with 10,000 random task-
language words. The imbalance in the classes is a compromise, approximating the actual proportions in
the test without leading to a degenerate classifier. Because English and the other classes are so dissimilar,
the performance of the English word classifier is very insensitive to the actual ratio. From this data, we
train a na??ve Bayes classifier using character 3-grams, 4-grams, and 5-grams.
We manually labeled the words of 150 sentences from the five non-English tasks in order to evaluate
the English word classifier. Across the five tasks, the precision was 0.76 and the recall was 0.66, leading
to an F1-score of 0.70. Any words labeled as English by the classifier were removed from the sentence
and it was passed on to the feature extraction, classification, and training stages.
Named Entity Removal We also observed another common class of word that could potentially act
as a noise source: named entities. Across all the languages listed studied here, it is common for named
entities to begin with a capital letter. Lacking named entity recognizers for all the languages here, we
instead used the property of having an initial capital letter as a surrogate for recognizing a word as a
named entity. Because all the languaes studied here also have the convention of capitalizing the first
word of a sentence, we remove all words beginning with a capital letter except for the first and pass this
abridged sentence on to the feature extraction, classification, and training stages.
Task No data filtering
English Word
Removal
Named Entity
Removal
Bosnian/Croatian/Serbian 0.9138 0.9105 0.9003
Indonesian/Malay 0.9878 0.9885 0.9778
Czech/Slovak 0.9983 0.9980 0.9973
Portuguese 0.9383 0.9365 0.9068
Spanish 0.8843 0.8835 0.8555
English 0.5000 0.5000 0.5050
Average 0.8704 0.8695 0.8571
Table 6: Comparison of data filtering methods using word unigram features on the six tasks.
151
(A)
0 0.2 0.4 0.6 0.8 1
?10
5
0.4
0.6
0.8
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(B)
0 0.5 1 1.5 2 2.5
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(C)
0 0.5 1 1.5
?10
5
0.6
0.7
0.8
0.9
1
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(D)
0 0.2 0.4 0.6 0.8 1
?10
5
0.5
0.6
0.7
0.8
0.9
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(E)
0 0.5 1 1.5
?10
5
0.5
0.6
0.7
0.8
0.9
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(F)
0 0.5 1 1.5 2
?10
5
0.5
0.6
0.7
0.8
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
Figure 1: Learning curves for the six tasks as the number of training instances per language is varied.
The line marked ?DSL? is the learning curve for the DSL-provided training data evaluated against the
developement data. The line marked ?external? is our external newspaper training data evaluated against
the development data. The line marked ?external (CV)? is our external training data evaluated using
10-fold cross-validation.
152
Bosnian/Croatian/Serbian Indonesian/Malay Czech/Slovak Portuguese Spanish English
da bisa sa Portugal the I
kako berkata se R Rosario you
sa kerana aj euros han The
kazao karena ako Brasil euros said
takode daripada ve cento Argentina Obama
rekao saat pre governo PP your
evra dari pro Lusa Fe If
tijekom beliau ktor?e PSD Rajoy that
posle selepas s?u Ele Espa?na but
posto bahwa ktor?y Governo Madrid It
Table 7: The ten word-unigram features given the highest weight by information gain feature selection
for each of the six tasks.
5 Discussion
Across many of the tasks, there was evidence that performance was tied more strongly to domain-specific
features of the two classes rather than to language- (or language-variant-) specific features. For example,
Table 7 shows the best word-unigram features selected by information gain feature selection for each of
the tasks. The Portuguese, Spanish, and English tasks specifically have as many of their most important
features named entities and other non-language specific features.
It seems that for many of the tasks, it is easier to distinguish the subject matter written about than it is to
distinguish the languages/dialects themselves. With Portuguese, for example, Brazilian dialect speakers
were much more likely to discuss places in Brazil and mention Brazilian reais (currency, abbreviated
as R), while European speakers mentioned euros, places in Portugal, and discussed Portuguese politics.
While there are definite linguistic differences between Brazilian and European Portuguese, these seem
to be less pronounced than the superficial differences in subject matter.
Practically, this is not necessarily a bad thing for this shared task, as the domain information gives extra
clues that allow the task to be completed with higher accuracy than would otherwise be possible. This
would become problematic if one wanted to apply a classifier trained on this data to general domains,
where the classifier may not be able to rely on the speaker talking about a certain subject matter. To
address this, the classifier would either need to focus on features specific to the language pair itself or
would need to be trained on data that spanned many domains.
Further evidence of domain overfitting comes from the fact that the larger training sets drawn from
newspaper text were not able to improve performance on the development set over the provided training
data, which is presumably drawn from the same collection as the development data. Figure 1 shows
learning curves for each of the six tasks. Though all the external text is self-consistent (cross-validation
results in high accuracy), in none of the cases does training on a large amount of external data allow the
classifier to exceed the accuracy achieved by training on the DSL data.
6 Conclusion
In this paper we experimented with several methods for classification of sentences in closely-related lan-
guages for the DSL shared task. Our analysis showed that, when dealing with closely related languages,
the task of classifying text according to its language was difficult to untie from the taks of classifying
other text characteristics, such as the domain. Across all our types of methods, we found that a na??ve
Bayes classifier using character n-gram, word unigram, and word bigram features was a strong baseline.
In future work, we would like to try to improve on these results by incorporating features that try to
capture syntactic relationships. Certainly some of the pairs of languages considered here are close enough
that they could be chunked, tagged, or parsed before knowing exactly which variety they belong to. This
would allow for the inclusion of features related to transitivity, agreement, complementation, etc. For
example, in British English, the verb ?provide? is monotransitive, but ditransitive in American English. It
is unclear how much features like these would improve accuracy, but it is likely that they would ultimately
be necessary to improve classification of similar languages to human levels of performance.
153
References
Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language identi-
fication for creating language-specific twitter collections. In Proceedings of the Second Workshop on Language
in Social Media, pages 65?74. Association for Computational Linguistics.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias. 2013. Microblog language identification: Overcoming
the limitations of short, unedited and idiomatic text. Language Resources and Evaluation, 47(1):195?215.
Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive approach towards text source classification based on
top-bag-of-word similarity. pages 404?410.
Thorsten Joachims. Svmlight: Support vector machine. http://svmlight. joachims. org/.
Ben King and Steven Abney. 2013. Labeling the languages of words in mixed-language documents using weakly
supervised methods. In Proceedings of NAACL-HLT, pages 1110?1119.
Nikola Ljube?si?c, Nives Mikeli?c, and Damir Boras. 2007. Language identication: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces, pages
541?546.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multi-
lingual documents. Transactions of the Association for Computational Linguistics, 2:27?40.
Andrew K. McCallum. 2002. Mallet: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Andrew Y Ng and Michael I Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic
regression and naive bayes. Advances in neural information processing systems, 2:841?848.
Andrew Y Ng. 2004. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings of the
twenty-first international conference on Machine learning, page 78. ACM.
Dong-Phuong Nguyen and A Seza Dogruoz. 2013. Word level language identification in online multilingual
communication. Association for Computational Linguistics.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation
between language varieties. Natural Language Engineering, 16(4):469?491.
Jason Rennie. 2004. On l2-norm regularization and the gaussian prior.
http://people.csail.mit.edu/jrennie/writing.
Liling Tan, Marcos Zampieri, Nikola Ljube?sic, and J?org Tiedemann. 2014. Merging comparable data sources
for the discrimination of similar languages: The dsl corpus collection. In Proceedings of The 7th Workshop on
Building and Using Comparable Corpora (BUCC).
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
COLING, pages 2619?2634.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector ma-
chine learning for interdependent and structured output spaces. In Proceedings of the twenty-first international
conference on Machine learning, page 104. ACM.
Marcos Zampieri and Binyam Gebrekidan. 2012. Automatic identification of language varieties: The case of
portuguese. In Proceedings of KONVENS, pages 233?237.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2012. Classifying pluricentric languages:
Extending the monolingual model. In Proceedings of the Fourth Swedish Language Technlogy Conference
(SLTC2012), pages 79?80.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and pos
distribution for the identification of spanish varieties. Proceedings of TALN2013, Sable dOlonne, France, pages
580?587.
154
