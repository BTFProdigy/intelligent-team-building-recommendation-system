Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1542?1551,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Polynomial to Linear: Efficient Classification with Conjunctive Features
Naoki Yoshinaga
Institute of Industrial Science
University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo
ynaga@tkl.iis.u-tokyo.ac.jp
Masaru Kitsuregawa
Institute of Industrial Science
University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo
kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
This paper proposes a method that speeds
up a classifier trained with many con-
junctive features: combinations of (prim-
itive) features. The key idea is to pre-
compute as partial results the weights of
primitive feature vectors that appear fre-
quently in the target NLP task. A trie
compactly stores the primitive feature vec-
tors with their weights, and it enables the
classifier to find for a given feature vec-
tor its longest prefix feature vector whose
weight has already been computed. Ex-
perimental results for a Japanese depen-
dency parsing task show that our method
speeded up the SVM and LLM classifiers
of the parsers, which achieved accuracy of
90.84/90.71%, by a factor of 10.7/11.6.
1 Introduction
Deep and accurate text analysis based on discrimi-
native models is not yet efficient enough as a com-
ponent of real-time applications, and it is inade-
quate to process Web-scale corpora for knowledge
acquisition (Pantel, 2007; Saeger et al, 2009) or
semi-supervised learning (McClosky et al, 2006;
Spoustov? et al, 2009). One of the main reasons
for this inefficiency is attributed to the inefficiency
of core classifiers trained with many feature com-
binations (e.g., word n-grams). Hereafter, we refer
to features that explicitly represent combinations
of features as conjunctive features and the other
atomic features as primitive features.
The feature combinations play an essential role
in obtaining a classifier with state-of-the-art ac-
curacy for several NLP tasks; recent examples in-
clude dependency parsing (Koo et al, 2008), parse
re-ranking (McClosky et al, 2006), pronoun reso-
lution (Nguyen and Kim, 2008), and semantic role
labeling (Liu and Sarkar, 2007). However, ?ex-
plicit? feature combinations significantly increase
the feature space, which slows down not only
training but also testing of the classifier.
Kernel-based methods such as support vector
machines (SVMs) consider feature combinations
space-efficiently by using a polynomial kernel
function (Cortes and Vapnik, 1995). The kernel-
based classification is, however, known to be very
slow in NLP tasks, so efficient classifiers should
sum up the weights of the explicit conjunctive fea-
tures (Isozaki and Kazawa, 2002; Kudo and Mat-
sumoto, 2003; Goldberg and Elhadad, 2008).
`
1
-regularized log-linear models (`
1
-LLMs), on
the other hand, provide sparse solutions, in which
weights of irrelevant features are exactly zero, by
assuming a Laplacian prior on the weights (Tibshi-
rani, 1996; Kazama and Tsujii, 2003; Goodman,
2004; Gao et al, 2007). However, as Kazama and
Tsujii (2005) have reported in a text categorization
task and we later confirm in a dependency pars-
ing task, when most features regarded as irrelevant
during training `
1
-LLMs appear rarely in the task,
we cannot greatly reduce the number of active fea-
tures in each classification. In the end, when effi-
ciency is a major concern, we must use exhaustive
feature selection (Wu et al, 2007; Okanohara and
Tsujii, 2009) or even restrict the order of conjunc-
tive features at the expense of accuracy.
In this study, we provide a simple, but effective
solution to the inefficiency of classifiers trained
with higher-order conjunctive features (or polyno-
mial kernel), by exploiting the Zipfian nature of
language data. The key idea is to precompute the
weights of primitive feature vectors and use them
as partial results to compute the weight of a given
feature vector. We use a trie called the feature
sequence trie to efficiently find for a given fea-
ture vector its longest prefix feature vector whose
weight has been computed. The trie is built from
feature vectors generated by applying the classifier
to actual data in the classification task. The time
complexity of the classifier approaches time that
1542
is linear with respect to the number of primitive
features when the retrieved feature vector covers
most of the features in the input feature vector.
We implemented our algorithm for SVM and
LLM classifiers and evaluated the performance of
the resulting classifiers in a Japanese dependency
parsing task. Experimental results show that it
successfully speeded up classifiers trained with
higher-order conjunctive features by a factor of 10.
The rest of this paper is organized as follows.
Section 2 introduces LLMs and SVMs. Section 3
proposes our classification algorithm. Section 4
presents experimental results. Section 5 concludes
with a summary and addresses future directions.
2 Preliminaries
In this paper, we focus on linear classifiers that cal-
culate the probability (or score) by summing up
weights of individual features. Examples include
not only log-linear models but also support vec-
tor machines with kernel expansion (Isozaki and
Kazawa, 2002; Kudo and Matsumoto, 2003). Be-
low, we introduce these two classifiers and their
ways to consider feature combinations.
In classification-based NLP, the target task is
modeled as one or more classification steps. For
example in part-of-speech (POS) tagging, each
classification decides whether to assign a partic-
ular label (POS tag) to a given sample (each word
in a given sentence). Each sample is then repre-
sented by a feature vector x, whose element x
i
is
a value of a feature function f
i
? F .
Here, we assume a binary feature function
f
i
(x) ? {0, 1}, in which a non-zero value means
that particular context data appears in the sample.
We say that a feature f
i
is active in sample xwhen
x
i
= f
i
(x) = 1 and |x| represents the number of
active features in x (|x| = |{f
i
|f
i
(x) = 1}|).
2.1 Log-Linear Models
The log-linear model (LLM), or also known as
maximum-entropy model (Berger et al, 1996), is
a linear classifier widely used in the NLP literature.
Let the training data of LLMs be {?x
i
, y
i
?}
L
i=1
,
where x
i
? {0, 1}
n
is a feature vector and y
i
is a
class label associated with x
i
. We assume a binary
label y
i
? {?1} here to simplify the argument.
The classifier provides conditional probability
p(y|x) for a given feature vector x and a label y:
p(y|x) =
1
Z(x)
exp
?
i
w
i,y
f
i,y
(x, y), (1)
where f
i,y
(x, y) is a feature function that returns
a non-zero value when f
i
(x) = 1 and the label is
y, w
i,y
? R is a weight associated with f
i,y
, and
Z(x) =
?
y
exp
?
i
w
i,y
f
i,y
(x, y) is the partition
function. We can consider feature combinations in
LLMs by explicitly introducing a new conjunctive
feature f
F
?
,y
(x, y) that is activated when a partic-
ular set of features F
?
? F to be combined is acti-
vated (namely, f
F
?
,y
(x, y) =
?
f
i,y
?F
?
f
i,y
(x, y)).
We then introduce an `
1
-regularized LLM (`
1
-
LLM), in which the weight vector w is tuned so
as to maximize the logarithm of the a posteriori
probability of the training data:
L(w) =
L
?
i=1
log p(y
i
|x
i
)? C?w?
1
. (2)
Hyper-parameter C thereby controls the degree of
over-fitting (solution sparseness). Interested read-
ers may refer to the cited literature (Andrew and
Gao, 2007) for the optimization procedures.
2.2 Support Vector Machines
A support vector machine (SVM) is a binary clas-
sifier (Cortes and Vapnik, 1995). Training with
samples {?x
i
, y
i
?}
L
i=1
where x
i
? {0, 1}
n
and
y
i
? {?1} yields the following decision function:
y(x) = sgn(g(x) + b)
g(x) =
?
x
j
?SV
y
j
?
j
?(x
j
)
T
?(x), (3)
where b ? R, ? : R
n
7? R
H
and support vec-
tors x
j
? SV (subset of training samples), each
of which is associated with weight ?
j
? R. We
hereafter call g(x) the weight function. Nonlinear
mapping function ? is chosen to make the train-
ing samples linearly separable in R
H
space. Ker-
nel function k(x
j
,x) = ?(x
j
)
T
?(x) is then in-
troduced to compute the dot product in R
H
space
without mapping x to ?(x).
To consider combinations of primitive features
f
j
? F , we use a polynomial kernel k
d
(x
j
,x) =
(x
T
j
x + 1)
d
. From Eq. 3, we obtain the weight
function for the polynomial kernel as:
g(x) =
?
x
j
?SV
y
j
?
j
(x
T
j
x+ 1)
d
. (4)
Since we assumed that x
i
is a binary value repre-
senting whether a (primitive) feature f
i
is active
in the sample, the polynomial kernel of degree d
implies a mapping ?
d
from x to ?
d
(x) that has
1543
H =
?
d
k=0
(
n
k
)
dimensions. Each dimension rep-
resents a (weighted) conjunction of d features in
the original sample x.
1
Kernel Expansion (SVM-KE) The time com-
plexity of Eq. 4 is O(|x| ? |SV|). This cost is usu-
ally high for classifiers used in NLP tasks because
they often have many support vectors (|SV| >
10, 000). Kernel expansion (KE) was proposed
by Isozaki and Kazawa (2002) to convert Eq. 4
into the linear sum of the weights in the mapped
feature space as in LLM (p(y|x) in Eq. 1):
g(x) = w
T
x
d
=
?
i
w
i
x
d
i
, (5)
where x
d
is a binary feature vector whose element
x
d
i
has a non-zero value when (?
d
(x))
i
> 0, w
is the weight vector for x
d
in the expanded fea-
ture space F
d
and is precalculated from the sup-
port vectors x
j
and their weights ?
j
. Interested
readers may refer to Kudo and Matsumoto (2003)
for the detailed computation for obtaining w.
The time complexity of Eq. 5 (and Eq. 1) is
O(|x
d
|), which is linear with respect to the num-
ber of active features in x
d
within the expanded
feature space F
d
.
Heuristic Kernel Expansion (SVM-HKE) To
make the weight vector sparse, Kudo and Mat-
sumoto (2003) proposed a heuristic method that
filters out less useful features whose absolute
weight values are less than a pre-defined threshold
?.
2
They reported that increased threshold value ?
resulted in a dramatically sparse feature space F
d
,
which had the side-effects of accuracy degradation
and classifier speed-up.
3 Proposed Method
In this section, we propose a method that speeds
up a classifier trained with many conjunctive fea-
tures. Below, we focus on a kernel-based classifier
trained with a polynomial kernel of degree d (here,
1
For example, given an input vector x = (x
1
, x
2
)
T
and a support vector x
?
= (x
?
1
, x
?
2
)
T
, the 2nd-order
polynomial kernel returns k
2
(x
?
,x) = (x
?
1
x
1
+ x
?
2
x
2
+
1)
2
= 3x
?
1
x
1
+ 3x
?
2
x
2
+ 2x
?
1
x
1
x
?
2
x
2
+ 1 (? x
?
i
, x
i
?
{0, 1}). This function thus implies a mapping ?
2
(x) =
(1,
?
3x
1
,
?
3x
2
,
?
2x
1
x
2
)
T
. In the following argument, we
ignore the dimension of the constant in the mapped space and
assume constant b is set to include it.
2
Precisely speaking, they set different thresholds to posi-
tive (?
j
> 0) and negative (?
j
< 0) support vectors, consid-
ering the proportion of positive and negative support vectors.
Figure 1: Efficient computation of g(x).
SVMs), but an analogous argument is possible for
linear classifiers (e.g., LLMs).
3
We hereafter represent a binary feature vector x
as a set of active features {f
i
|f
i
(x) = 1}. x can
thereby be represented as an element of the power
set 2
F
of the set of features F .
3.1 Idea
Let us remember that weight function g(x) in
Eq. 5 maps x ? 2
F
to W ? R. If we could cal-
culate W
x
= g(x) for all possible x in advance,
we could obtain g(x) by simply checking |x| ele-
ments, namely, in O(|x|) time. However, because
|{x|x ? 2
F
}| = 2
|F|
and |F| is likely to be very
large (often |F| > 10, 000 in NLP tasks), this cal-
culation is impractical.
We then compute and store weight W
x
?
=
g(x
?
) for x
?
? V
c
(? 2
F
), a certain subset of
the possible value space, and compute g(x) for
x /? V
c
by using precalculated weight W
x
c
for
x
c
?
4
x in the following way:
g(x) = W
x
c
+
?
f
i
?x
d
?x
d
c
w
i
. (6)
Intuitively speaking, starting from partial weight
W
x
c
, we add up remaining weights of primitive
features f ? F that are not active in x
c
but active
in x and conjunctive features that combine f and
the other active features in x.
An example of this computation (d = 2) is de-
picted in Figure 1. We can efficiently compute
g(x) for a vector x that has four active features
f
1
, f
2
, f
3
, and f
4
(and x
2
has their six conjunc-
tive features) using precalculated weight W
{1,2,3}
;
we should first check the three features f
1
, f
2
, and
f
3
to retrieve W
{1,2,3}
and next check the remain-
ing four features related to f
4
, namely f
4
, f
1,4
,
f
2,4
, and f
3,4
, in order to add up the remaining
3
When a feature vector x includes (explicit) conjunctive
features f ? F
d
, we assume weight function g
?
(y|x
?
) =
g(y|x), where x
?
is a projection of x (by ?
?1
d
: F
d
? F ).
4
This means that all active features in x
c
are active in x.
1544
weights, while the normal computation in Eq. 5
should check the four primitive and six conjunc-
tive features to get the individual weights.
Expected time complexity Counting the num-
ber of features to be checked in the computation,
we obtain the time complexity f(x, d) of Eq. 6 as:
f(x, d) = O(|x
c
|+ |x
d
| ? |x
d
c
|), (7)
where |x
d
| =
d
?
k=1
(
|x|
k
)
(8)
(e.g., |x
2
| =
|x|
2
+|x|
2
and |x
3
| =
|x|
3
+5|x|
6
).
5
Note
that when |x
c
| becomes close to |x|, this time
complexity actually approaches O(|x|).
Thus, to minimize this computational cost, x
c
is to be chosen from V
c
as follows:
x
c
= argmin
x
?
?V
c
,x
?
?x
(|x
?
|+ |x
d
| ? |x
?d
|). (9)
3.2 Construction of Feature Sequence Trie
There are two issues with speeding up the classi-
fier by the computation shown in Eq. 6. First, since
we can store weights for only a small fraction of
possible feature vectors (namely, |V
c
|  2
|F|
), we
should choose V
c
so as to maximize its impact on
the speed-up. Second, we should quickly find an
optimal x
c
from V
c
for a given feature vector x.
The solution to the first problem is to enumer-
ate partial feature vectors that frequently appear in
the target task. Note that typical linguistic features
used in NLP tasks usually consist of disjunctive
sets of features (e.g., word surface and POS), in
which each set is likely to follow Zipf?s law (Zipf,
1949) and correlate with each other. We can ex-
pect the distribution of feature vectors, the mixture
of Zipf distributions, to be Zipfian. This has been
confirmed for word n-grams (Egghe, 2000) and
itemset support distribution (Chuang et al, 2008).
We can thereby expect that a small set of partial
feature vectors commonly appear in the task.
To solve the second problem, we introduce a
feature sequence trie (fstrie), which represents a
hierarchy of feature vectors, to enable the clas-
sifier to efficiently retrieve (sub-)optimal x
c
(in
Eq. 9) for a given feature vector x. We build an
fstrie in the following steps:
Step 1: Apply the target classifier to actual (raw)
data in the task to enumerate possible feature
vectors (hereafter, source feature vectors).
5
This is the maximum number of conjunctive features.
Figure 2: Feature sequence trie and completion of
prefix feature vector weights.
Step 2: Sort the features in each source feature
vector according to their frequency in the
training data (in descending order).
Step 3: Build a trie from the source feature vec-
tors by regarding feature indices as characters
and store weights of all prefix feature vectors.
An fstrie built from six source feature vectors is
shown in Figure 2. In fstries, a path from the root
to another node represents a feature vector. An
important point here is that the fstrie stores the
weights of all prefix feature vectors of the source
feature vectors, and the trie structure enables us to
retrieve for a given feature vector x the weight of
its longest prefix vector x
c
? x in O(|x
c
|) time.
To handle feature functions in LLMs (Eq. 1), we
store partial weight W
x
c
,y
=
?
i
w
i,y
f
i,y
(x
c
, y)
for each label y on the node that expresses x
c
.
Since we sort the features in the source fea-
ture vectors according to their frequency, the pre-
fix feature vectors exclude less frequent features
in the source feature vectors. Lexical features or
finer-grained features (e.g., POS-subcategory) are
usually less frequent than coarse-grained features
(e.g., POS), so they lie in the latter part of the
feature vectors. This sorting helps us to retrieve
longer feature vector x
c
for input feature vector x
that will have diverse infrequent features. It also
minimizes the size of fstrie by sharing the com-
mon frequent prefix (e.g., {f
1
, f
2
} in Figure 2).
Pruning nodes from fstrie We have so far de-
scribed the way to construct an fstrie from the
source feature vectors. However, a naive enumer-
ation of source feature vectors will result in the
explosion of the fstrie size, and we want to have
a principled way to control the fstrie size rather
than reducing the processed data size. Below, we
present a method that prunes useless prefix feature
vectors (nodes) from the constructed fstrie to max-
imize its impact on the classifier efficiency.
1545
Algorithm 1 PRUNE NODES FROM FSTRIE
Input: fstrie T , node_limit N ? N
Output: fstrie T
1: while # of nodes in T > N do
2: x
c
? argmin
x
?
?leaf(T )
u(x
?
)
3: remove x
c
, T
4: end while
5: return T
We adopt a greedy strategy that iteratively
prunes a leaf node (one prefix feature vector and
its weight) from the fstrie built from all the source
feature vectors, according to a certain utility score
calculated for each node. In this study, we con-
sider two metrics for each prefix feature vector x
c
to calculate its utility score.
Probability p(x
c
), which denotes how often the
stored weight W
x
c
will be used in the tar-
get task. The maximum-likelihood estima-
tion provides probability:
p(x
c
) =
?
x
?
?x
c
n
x
?
?
x
n
x
, (10)
where n
x
? N is the frequency count of a
source feature vector x in the processed data.
Computation reduction ?
d
(x
c
), which denotes
how much computation is reduced by W
x
c
to
calculate a weight of x ? x
c
. This can be es-
timated by counting the number of conjunc-
tive features we additionally have to check
when we remove x
c
. Since the fstrie stores
the weight of a prefix feature vector x
c-
? x
c
such that |x
c-
| = |x
c
| ? 1 (e.g., in Figure 2,
x
c-
= {f
1
, f
2
} for x
c
= {f
1
, f
2
, f
4
}), we
can define the computation reduction as:
?
d
(x
c
) = (|x
d
c
| ? |x
d
c-
|)? (|x
c
| ? |x
c-
|)
=
d
?
k=2
(
|x
c
|
k
)
?
d
?
k=2
(
|x
c
| ? 1
k
)
(? Eq. 8).
?
2
(x
c
) = |x
c
| ? 1 and ?
3
(x
c
) =
|x
c
|
2
?|x
c
|
2
.
We calculate utility score of each node x
c
in the
fstrie as u(x
c
) = p(x
c
) ? ?
d
(x
c
), which means
the expected computation reduction by x
c
in the
target task, and prune the lowest-utility-score leaf
nodes from the fstrie one by one (Algorithm 1). If
several prefix vectors have the same utility score,
we eliminate them in numerical descending order.
Algorithm 2 COMPUTE WEIGHT WITH FSTRIE
Input: fstrie T , weight vector w ? R
|F
d
|
feature vector x ? 2
F
Output: weight W = g(x) ? R
1: x? sort(x)
2: ?x
c
,W
x
c
? ? prefix_search(T , x)
3: W ?W
x
c
4: for all feature f
j
? x
d
? x
d
c
do
5: W ?W + w
j
6: end for
7: return W
3.3 Classification Algorithm
Our classification algorithm is shown in detail in
Algorithm 2. The classifier first sorts the active
features in input feature vectorx according to their
frequency in the training data. Then, for x, it re-
trieves the longest common prefix vector x
c
from
the fstrie (line 2 in Algorithm 2). It then adds the
weights of the remaining features to partial weight
W
x
c
(line 5 in Algorithm 2).
Note that the remaining features whose weights
we sum up (line 4 in Algorithm 2) are primitive
and conjunctive features that relate to f ? x?x
c
,
which appear less frequently than f
?
? x
c
in the
training data. Thus, when we apply our algorithm
to classifiers with the sparse solution (e.g., SVM-
HKEs or `
1
-LLMs), |x
d
|?|x
d
c
| can be much smaller
than the theoretical expectation (Eq. 8). We con-
firmed this in the following experiments.
4 Evaluation
We applied our algorithm to SVM-KE, SVM-HKE,
and `
1
-LLM classifiers and evaluated the resulting
classifiers in a Japanese dependency parsing task.
To the best of our knowledge, there are no previous
reports of an exact weight calculation faster than
linear summation (Eqs. 1 and 5). We also com-
pared our SVM classifier with a classifier called
polynomial kernel inverted (PKI: Kudo and Mat-
sumoto (2003)), which uses the polynomial kernel
(Eq. 4) and inverted indexing to support vectors.
4.1 Experimental Settings
A Japanese dependency parser inputs bunsetsu-
segmented sentences and outputs the correct head
(bunsetsu) for each bunsetsu; here, a bunsetsu is
a grammatical unit in Japanese consisting of one
or more content words followed by zero or more
function words. A parser generates a feature vec-
1546
Modifier,
modifiee
bunsetsu
head word (surface-form, POS, POS-subcategory,
inflection form), functional word (surface-form,
POS, POS-subcategory, inflection form), brackets,
quotation marks, punctuation marks, position in
sentence (beginning, end)
Between
bunsetsus
distance (1, 2?5, 6?), case-particles, brackets,
quotation marks, punctuation marks
Table 1: Feature set used for experiments.
tor for a particular pair of bunsetsus (modifier and
modifiee candidates) by exploiting the head-final
and projective (Nivre, 2003) nature of dependency
relations in Japanese. The classifier then outputs
label y = ?+1? (dependent) or ??1? (independent).
Since our classifier is independent of individ-
ual parsing algorithms, we targeted speeding up
(a classifier in) the shift-reduce parser proposed
by Sassano (2004), which has been reported to be
the most efficient for this task, with almost state-
of-the-art accuracy (Iwatate et al, 2008). This
parser decreases the number of classification steps
by using the fact that a bunsetsu is likely to modify
a bunsetsu close to itself. Due to space limitations,
we omit the details of the parsing algorithm.
We used the standard feature set tailored for this
task (Kudo and Matsumoto, 2002; Sassano, 2004;
Iwatate et al, 2008) (Table 1). Note that features
listed in the ?Between bunsetsus? row represent
contexts between the target pair of bunsetsus and
appear independently from other features, which
will become an obstacle to finding the longest pre-
fix vector. This task is therefore a better measure
of our method than simple sequential labeling such
as POS tagging or named-entity recognition.
For evaluation, we used Kyoto Text Corpus Ver-
sion 4.0 (Kurohashi and Nagao, 2003), Mainichi
news articles in 1995 that have been manually an-
notated with dependency relations.
6
The train-
ing, development, and test sets included 24,283,
4833, and 9284 sentences, and 234,685, 47,571,
and 89,874 bunsetsus, respectively. The training
samples generated from the training set included
150,064 positive and 146,712 negative samples.
The following experiments were performed on
a server with an Intel
R
? Xeon
TM
3.20-GHz CPU.
We used TinySVM
7
and a simple C++ library for
maximum entropy classification
8
to train SVMs
and `
1
-LLMs, respectively. We used Darts-Clone,
9
6
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus-e.html
7
http://chasen.org/?taku/software/TinySVM/
8
http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/maxent/
9
http://code.google.com/p/darts-clone/
Model type Model statistics Dep. Sent.
Model d ? / ? |F
d
| |x
d
| acc. acc.
SVM-KE 1 0 39712 27.3 88.29 46.49
SVM-KE 2 0 1478109 380.6 90.76 53.83
SVM-KE 3 0 26194354 3286.7 90.93

54.43

SVM-HKE 3 0.001 13247675 2725.9 90.92

54.39

SVM-HKE 3 0.002 2514385 2238.1 90.91

54.32
>
SVM-HKE 3 0.003 793195 1855.4 90.83 54.21
SVM-KE 4 0 293416102 20395.4 90.91

54.69

SVM-HKE 4 0.0002 96522236 15282.1 90.93

54.53
>
SVM-HKE 4 0.0004 19245076 11565.0 90.96

54.64

SVM-HKE 4 0.0006 7277592 8958.2 90.84 54.48
>
`
1
-LLM 1 1.0 9268 26.5 88.22 46.06
`
1
-LLM 2 2.0 32575 309.8 90.62 53.46
`
1
-LLM 3 3.0 129503 2088.3 90.71 54.09
>
`
1
-LLM 3 4.0 85419 1803.0 90.61 53.79
`
1
-LLM 3 5.0 63046 1699.5 90.59 53.55
Table 2: Specifications of LLMs and SVMs. The
accuracy marked with ?? or ?>? was signifi-
cantly better than the d = 2 counterpart (p < 0.01
or 0.01 ? p < 0.05 by McNemar?s test).
a double-array trie (Aoe, 1989; Yata et al, 2008),
as a compact trie implementation. All these li-
braries and algorithms are implemented in C++.
The code for building fstries occupies 100 lines,
while the code for the classifier occupies 20 lines
(except those for kernel expansion).
4.2 Results
Specifications of SVMs and LLMs used here are
shown in Table 2; |F
d
| is the number of active fea-
tures, while |x
d
| is the average number of active
features in each classification for the test corpus.
Dependency accuracy is the ratio of dependency
relations correctly identified by the parser, while
sentence accuracy is the exact match accuracy of
complete dependency relations in a sentence.
For LLM training, we designed explicit conjunc-
tive features for all the d or lower-order feature
combinations to make the results comparable to
those of SVMs. We could not train d = 4 LLMs
due to parameter explosion. We varied SVM soft
margin parameter c from 0.1 to 0.000001 and LLM
width factor parameter ?,
10
which controls the im-
pact of the prior, from 1.0 to 5.0, and adjusted
the values to maximize dependency accuracy for
the development set: (d, c) = (1, 0.1), (2, 0.005),
(3, 0.0001), (4, 0.000005) for SVMs and (d, ?) =
(1, 1.0), (2, 2.0), (3, 4.0) for `
1
-LLMs.
The accuracy of around 90.9% (SVM-KE, d =
3, 4) is close to the performance of state-of-the-
10
The parameter C of `
1
-LLM in Eq. 2 was set to ?/L
(referred to in Kazama and Tsujii (2003) as ?single width?).
1547
Model PKI Baseline Proposed w/ fstrie
S
Proposed w/ fstrie
M
Proposed w/ fstrie
L
Speed
type d classify Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] up
[ms/sent.] (MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total)
SVM-KE 1 13.480 0.2 0.003 (0.015) +0.6 0.006 (0.018) +20.2 0.007 (0.018) +662.9 0.016 (0.029) NA
SVM-KE 2 10.313 13.5 0.041 (0.054) +0.5 0.020 (0.032) +18.0 0.021 (0.034) +662.4 0.023 (0.036) 2.1
SVM-KE 3 10.945 142.2 0.345 (0.361) +0.5 0.163 (0.178) +18.2 0.108 (0.123) +667.0 0.079 (0.093) 4.4
SVM-KE 4 12.603 648.0 2.338 (2.363) +0.5 1.156 (1.178) +18.6 0.671 (0.690) +675.9 0.415 (0.432) 5.6
Table 3: Parsing results for test corpus: SVM-KE classifiers with dense feature space.
art parsers (Iwatate et al, 2008), and the model
statistics are considered to be complex (or re-
alistic) enough to evaluate our classifier?s util-
ity. The number of support vectors of SVMs was
71, 766 ? 9.2%, which is twice as many as those
used by Kudo and Matsumoto (2003) (34,996) in
their experiments on the same task.
We could clearly observe that the number of ac-
tive features |x
d
| increased dramatically according
to the order d of feature combinations. The den-
sity of |x
d
| for SVMs was very high (e.g., |x
3
| =
3286.7, close to the maximum shown in Eq. 8:
(27.3
3
+ 5? 27.3)/6 ' 3414.
For d ? 3 models, we attempted to control
the size of the feature space |F
d
| by changing
the model?s hyper-parameters: threshold ? for the
SVM-HKE and width factor ? for the `
1
-LLM. Al-
though we successfully reduced the size of the fea-
ture space |F
d
|, we could not dramatically reduce
the average number of active features |x
d
| in each
classification while keeping the accuracy advan-
tage. This confirms that the solution sparseness
does not suffice to obtain an efficient classifier.
We obtained source feature vectors to build
fstries by applying parsers with the target clas-
sifiers to a raw corpus in the target domain,
3,258,313 sentences of 1991?94 Mainichi news
articles that were morphologically analyzed by
JUMAN
6
and segmented into bunsetsus by KNP.
6
We first built fstrie
L
using all the source feature
vectors. We then attempted to reduce the number
of prefix feature vectors in fstrie
L
to 1/2
n
the size
by Algorithm 1. We refer to fstries built from 1/32
and 1/1024 of the prefix feature vectors in fstrie
L
as fstrie
M
and fstrie
S
in the following experiments.
Because we exploited Algorithm 2 to calcu-
late the weights of the prefix feature vectors, it
took less than one hour (59 min. 29 sec.) on the
3.20-GHz server to build fstrie
L
(and calculate the
utility score for all the nodes in it) for the slow-
est SVM-KE (d = 4) from the 40,409,190 source
feature vectors (62,654,549 prefix feature vectors)
generated by parsing the 3,258,313 sentences.
0
0.5
1
1.5
2
2.5
0 100 200 300 400 500 600 700A
ve
.
cla
ss
ific
atio
n
tim
e[m
s/s
en
t.]
Size of fstrie [MB]
SVM-KE (d = 1)
SVM-KE (d = 2)
SVM-KE (d = 3)
SVM-KE (d = 4)
Figure 3: Average classification time per sentence
plotted against size of fstrie: SVM-KE.
Results for SVM-KE with dense feature space
The performances of parsers having SVM-KE clas-
sifiers with and without the fstrie are given in Ta-
ble 3. The ?speed-up? column shows the speed-up
factor of the most efficient classifier (bold) ver-
sus the baseline classifier without fstries. Since
each classifier solved a slightly different num-
ber of classification steps (112, 853? 0.15%), we
show the (average) cumulative classification time
for a sentence. The Mem. columns show the size
of weight vectors for SVM-KE classifiers and the
size of fstries
S
, fstries
M
, and fstries
L
, respectively.
The fstries successfully speeded up SVM-KE
classifiers with the dense feature space.
11
The
SVM-KE classifiers without fstries were still faster
than PKI, but as expected from a large |x
d
| value,
the classifiers with higher conjunctive features
were much slower than the classifier with only
primitive features by factors of 13 (d = 2), 109
(d = 3) and 738 (d = 4) and the classification
time accounted for most of the parsing time.
The average classification time of our classifiers
plotted against fstrie size is shown in Figure 3.
Surprisingly, we obtained a significant speed-up
even with tiny fstrie sizes of < 1 MB. Further-
more, we naively controlled the fstrie size by sim-
11
The inefficiency of the classifier (d = 1) results from the
cost of the additional sort function (line 1 in Algorithm 2) and
CPU cache failure due to random accesses to the huge fstries.
1548
Model Baseline Proposed w/ fstrie
S
Proposed w/ fstrie
M
Proposed w/ fstrie
L
Speed
type d ? / ? Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] up
(MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total)
SVM-HKE 3 0.001 64.6 0.348 (0.363) +0.5 0.151 (0.166) +17.6 0.097 (0.111) +638.0 0.070 (0.084) 5.0
SVM-HKE 3 0.002 13.9 0.332 (0.346) +0.5 0.123 (0.137) +17.0 0.074 (0.088) +612.2 0.053 (0.067) 6.2
SVM-HKE 3 0.003 4.2 0.314 (0.328) +0.4 0.102 (0.115) +14.7 0.057 (0.070) +526.2 0.041 (0.054) 7.8
SVM-HKE 4 0.0002 235.0 2.258 (2.280) +0.5 1.022 (1.042) +17.7 0.558 (0.575) +637.1 0.330 (0.346) 6.8
SVM-HKE 4 0.0004 82.8 2.038 (2.058) +0.5 0.816 (0.835) +16.8 0.414 (0.430) +601.7 0.234 (0.249) 8.7
SVM-HKE 4 0.0006 32.2 1.802 (1.820) +0.4 0.646 (0.662) +15.7 0.311 (0.326) +558.9 0.168 (0.183) 10.7
`
1
-LLM 1 1.0 0.1 0.004 (0.016) +0.8 0.006 (0.018) +25.0 0.007 (0.019) +787.7 0.016 (0.029) NA
`
1
-LLM 2 2.0 0.4 0.043 (0.055) +0.6 0.016 (0.028) +20.5 0.015 (0.027) +698.0 0.018 (0.030) 2.9
`
1
-LLM 3 3.0 1.0 0.314 (0.326) +0.5 0.091 (0.103) +17.8 0.041 (0.054) +601.0 0.027 (0.040) 11.6
`
1
-LLM 3 4.0 0.7 0.300 (0.313) +0.5 0.082 (0.094) +16.3 0.036 (0.049) +550.1 0.024 (0.037) 12.4
`
1
-LLM 3 5.0 0.5 0.290 (0.302) +0.5 0.076 (0.088) +15.1 0.032 (0.045) +510.7 0.022 (0.035) 13.3
Table 4: Parsing results for test corpus: SVM-HKE and `
1
-LLM classifiers with sparse feature space.
0
0.5
1
1.5
2
2.5
0 10 20 30 40 50 60 70 80A
ve
.
cla
ss
ific
atio
n
tim
e[m
s/se
nt.
]
Size of fstrie [MB]
0.671 ms/sent.(18.6 MB)
0.680 ms/sent.(67.1 MB)
naive
utility score
Figure 4: Fstrie reduction: utility score vs. pro-
cessed sentence reduction for SVM-KE (d = 4).
ply reducing the number of sentences processed to
1/2
n
. The impact on the speed-up of the resulting
fstries (naive) and the fstries constructed by our
utility score (utility-score) on SVM-KE (d = 4)
is shown in Figure 4. The Zipfian nature of lan-
guage data let us obtain a substantial speed-up
even when we naively reduced the fstrie size, and
the utility score further decreased the fstrie size
required to obtain the same speed-up. We needed
less than 1/3 size fstries to achieve the same speed-
up: 0.671 ms./sent. (18.6 MB) (utility-score) vs.
0.680 ms./sent. (67.1 MB) (naive).
Results for SVM-HKE and `
1
-LLM classifiers
with sparse feature space The performances of
parsers having SVM-HKE and `
1
-LLM classifiers
with and without the fstrie are given in Table 4.
The fstries successfully speeded up the SVM-HKE
and `
1
-LLM classifiers by factors of 10.7 (SVM-
HKE, d = 4, ? = 0.0006) and 11.6 (`
1
-LLM,
d = 3, ? = 3.0). We obtained more speed-
up when we used fstries for classifiers with more
sparse feature space F
d
(Figures 5 and 6). The
parsing speed with d = 3 models are now compa-
rable to the parsing speed with d = 2 models.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 100 200 300 400 500 600 700A
ve
.
cla
ss
ific
atio
n
tim
e[m
s/s
en
t.]
Size of fstrie [MB]
SVM-KE (d = 3)
SVM-HKE (d = 3, ? = 0.001)
SVM-HKE (d = 3, ? = 0.002)
SVM-HKE (d = 3, ? = 0.003)
Figure 5: Average classification time per sentence
plotted against size of fstrie: SVM-HKE (d = 3).
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 100 200 300 400 500 600 700A
ve
.
cla
ss
ific
atio
n
tim
e[m
s/s
en
t.]
Size of fstrie [MB]
?1-LLM (d = 3, ? = 3.0)?1-LLM (d = 3, ? = 4.0)?1-LLM (d = 3, ? = 5.0)
Figure 6: Average classification time per sentence
plotted against size of fstrie: `
1
-LLM (d = 3).
Without fstries, little speed-up of SVM-HKE
classifiers versus the SVM-KE classifiers (in Ta-
ble 3) was obtained due to the mild reduction in
the average number of active features |x
d
| in the
classification. This result conforms to the results
reported in (Kudo and Matsumoto, 2003).
The parsing speed reached 14,937 sentences
per second with accuracy of 90.91% (SVM-HKE,
d = 3, ? = 0.002). We used this parser to pro-
cess 1,005,918 sentences (5,934,184 bunsetsus)
randomly extracted from Japanese weblog feeds
1549
updated in November 2008, to see how much the
impact of fstries lessens when the test data and
the data processed to build fstries mismatch. The
parsing time was 156.4 sec. without fstrie
L
, while
it was just 35.9 sec. with fstrie
L
. The speed-up
factor of 4.4 on weblog feeds was slightly worse
than that on news articles (0.346/0.067 = 5.2)
but still evident. This implies that sorting features
in building fstries yielded prefix features vectors
that commonly appear in this task, by excluding
domain-specific features such as lexical features.
In summary, our algorithm successfully mini-
mized the efficiency gap among classifiers with
different degrees of feature combinations and
made accurate classifiers trained with higher-order
feature combinations practical.
5 Conclusion and Future Work
Our simple method speeds up a classifier trained
with many conjunctive features by using precal-
culated weights of (partial) feature vectors stored
in a feature sequence trie (fstrie). We experimen-
tally demonstrated that it speeded up SVM and
LLM classifiers for a Japanese dependency pars-
ing task by a factor of 10. We also confirmed that
the sparse feature space provided by `
1
-LLMs and
SVM-HKEs contributed much to size reduction of
the fstrie required to achieve the same speed-up.
The implementations of the proposed algorithm
for LLMs and SVMs (with a polynomial kernel) and
the Japanese dependency parser will be available
at http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/.
We plan to apply our method to wider range of
classifiers used in various NLP tasks. To speed up
classifiers used in a real-time application, we can
build fstries incrementally by using feature vec-
tors generated from user inputs. When we run our
classifiers on resource-tight environments such as
cell-phones, we can use a random feature mix-
ing technique (Ganchev and Dredze, 2008) or a
memory-efficient trie implementation based on a
succinct data structure (Jacobson, 1989; Delpratt
et al, 2006) to reduce required memory usage.
We will combine our method with other tech-
niques that provide sparse solutions, for example,
kernel methods on a budget (Dekel and Singer,
2007; Dekel et al, 2008; Orabona et al, 2008) or
kernel approximation (surveyed in Kashima et al
(2009)). It is also easy to combine our method
with SVMs with partial kernel expansion (Gold-
berg and Elhadad, 2008), which will yield slower
but more space-efficient classifiers. We will in
the future consider an issue of speeding up decod-
ing with structured models (Lafferty et al, 2001;
Miyao and Tsujii, 2002; Sutton et al, 2004).
Acknowledgment The authors wish to thank
Susumu Yata and Yoshimasa Tsuruoka for letting
the authors to use their pre-release libraries. The
authors also thank Nobuhiro Kaji and the anony-
mous reviewers for their valuable comments.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L
1
-regularized log-linear models. In Proc.
ICML 2007, pages 33?40.
Jun?ichi Aoe. 1989. An efficient digital search al-
gorithm by using a double-array structure. IEEE
Transactions on Software Engineering, 15(9):1066?
1077, September.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71, March.
Kun-Ta Chuang, Jiun-Long Huang, and Ming-Syan
Chen. 2008. Power-law relationship and
self-similarity in the itemset support distribution:
analysis and applications. The VLDB Journal,
17(5):1121?1141, August.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?
297, September.
Ofer Dekel and Yoram Singer. 2007. Support vec-
tor machines on a budget. In Bernhard Sch?lkopf,
John Platt, and Thomas Hofmann, editors, Advances
in Neural Information Processing Systems 19, pages
345?352. The MIT Press.
Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer.
2008. The forgetron: A kernel-based perceptron on
a budget. SIAM Journal on Computing, 37(5):1342?
1372, January.
O?Neil Delpratt, Naila Rahman, and Rajeev Raman.
2006. Engineering the LOUDS succinct tree rep-
resentation. In Proc. WEA 2006, pages 134?145.
Leo Egghe. 2000. The distribution of n-grams. Scien-
tometrics, 47(2):237?252, February.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In Proc.
ACL 2008 Workshop on Mobile Language Process-
ing, pages 19?20.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study
1550
of parameter estimation methods for statistical natu-
ral language processing. In Proc. ACL 2007, pages
824?831.
Yoav Goldberg and Michael Elhadad. 2008.
splitSVM: Fast, space-efficient, non-heuristic, poly-
nomial kernel computation for NLP applications. In
Proc. ACL 2008, Short Papers, pages 237?240.
Joshua Goodman. 2004. Exponential priors for max-
imum entropy models. In Proc. HLT-NAACL 2004,
pages 305?311.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. COLING 2002, pages 1?7.
Masakazu Iwatate, Masayuki Asahara, and Yuji Mat-
sumoto. 2008. Japanese dependency parsing using
a tournament model. In Proc. COLING 2008, pages
361?368.
Guy Jacobson. 1989. Space-efficient static trees and
graphs. In Proc. FOCS 1989, pages 549?554.
Hisashi Kashima, Tsuyoshi Id?, Tsuyoshi Kato, and
Masashi Sugiyama. 2009. Recent advances and
trends in large-scale kernel methods. IEICE Trans-
actions on on Information and Systems, E92-D. to
appear.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. EMNLP 2003, pages
137?144.
Jun?ichi Kazama and Jun?ichi Tsujii. 2005. Maxi-
mum entropy models with inequality constraints: A
case study on text categorization. Machine Learn-
ing, 60(1-3):159?194.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proc. ACL 2008, pages 595?603.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. CoNLL 2002, pages 1?7.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In Proc. ACL 2003,
pages 24?31.
Sadao Kurohashi and Makoto Nagao. 2003. Build-
ing a Japanese parsed corpus. In Anne Abeill?, edi-
tor, Treebank: Building and Using Parsed Corpora,
pages 249?260. Kluwer Academic Publishers.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proc. ICML 2001, pages 282?289.
Yudong Liu and Anoop Sarkar. 2007. Experimental
evaluation of LTAG-based features for semantic role
labeling. In Proc. EMNLP 2007, pages 590?599.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proc. HLT-NAACL 2006, pages 152?159.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. HLT
2002, pages 292?297.
Ngan L.T. Nguyen and Jin-Dong Kim. 2008. Explor-
ing domain differences for the design of a pronoun
resolution system for biomedical texts. In Proc.
COLING 2008, pages 625?632.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proc. IWPT 2003,
pages 149?160.
Daisuke Okanohara and Jun?ichi Tsujii. 2009. Learn-
ing combination features with L
1
regularization. In
Proc. HLT-NAACL 2009, Short Papers, pages 97?
100.
Francesco Orabona, Joseph Keshet, and Barbara Ca-
puto. 2008. The projectron: a bounded kernel-based
perceptron. In Proc. ICML 2008, pages 720?727.
Patrick Pantel. 2007. Data catalysis: Facilitating large-
scale natural language data processing. In Proc.
ISUC, pages 201?204.
Stijn De Saeger, Kentaro Torisawa, and Jun?ichi
Kazama. 2009. Mining web-scale treebanks. In
Proc. NLP 2009, pages 837?840.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proc. COLING 2004, pages
8?14.
Drahom?ra ?Johanka? Spoustov?, Jan Haji?c, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised
training for the averaged perceptron POS tagger. In
Proc. EACL 2009, pages 763?771.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional ran-
dom fields: factorized probabilistic models for label-
ing and segmenting sequence data. In Proc. ICML
2004, pages 783?790.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B, 58(1):267?288, April.
Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2007.
An approximate approach for training polynomial
kernel SVMs in linear time. In Proc. ACL 2007
Poster and Demo Sessions, pages 65?68.
Susumu Yata, Kazuhiro Morita, Masao Fuketa, and
Jun?ichi Aoe. 2008. Fast string matching with
space-efficient word graphs. In Proc. Innovations
in Information Technology 2008, pages 79?83.
George K. Zipf. 1949. Human Behavior and the Prin-
ciple of Least-Effort. Addison-Wesley.
1551
A Debug Tool for Practical Grammar Development
Akane Yakushiji? Yuka Tateisi?? Yusuke Miyao?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{akane,yucca,yusuke,yoshinag,tsujii}@is.s.u-tokyo.ac.jp
Naoki Yoshinaga? Jun?ichi Tsujii??
Abstract
We have developed willex, a tool that
helps grammar developers to work effi-
ciently by using annotated corpora and
recording parsing errors. Willex has two
major new functions. First, it decreases
ambiguity of the parsing results by com-
paring them to an annotated corpus and
removing wrong partial results both au-
tomatically and manually. Second, willex
accumulates parsing errors as data for the
developers to clarify the defects of the
grammar statistically. We applied willex
to a large-scale HPSG-style grammar as
an example.
1 Introduction
There is an increasing need for syntactical parsers
for practical usages, such as information extrac-
tion. For example, Yakushiji et al (2001) extracted
argument structures from biomedical papers using
a parser based on XHPSG (Tateisi et al, 1998),
which is a large-scale HPSG. Although large-scale
and general-purpose grammars have been devel-
oped, they have a problem of limited coverage.
The limits are derived from deficiencies of gram-
mars themselves. For example, XHPSG cannot treat
coordinations of verbs (ex. ?Molybdate slowed but
did not prevent the conversion.?) nor reduced rel-
atives (ex. ?Rb mutants derived from patients with
retinoblastoma.?). Finding these grammar defects
and modifying them require tremendous human ef-
fort.
Hence, we have developed willex that helps to im-
prove the general-purpose grammars. Willex has two
major functions. First, it reduces a human workload
to improve the general-purpose grammar through
using language intuition encoded in syntactically
tagged corpora in XML format. Second, it records
data of grammar defects to allow developers to have
a whole picture of parsing errors found in the target
corpora to save debugging time and effort by priori-
tizing them.
2 What Is the Ideal Grammar Debugging?
There are already other grammar developing tools,
such as a grammar writer of XTAG (Paroubek et al,
1992), ALEP (Schmidt et al, 1996), ConTroll (Go?tz
and Meurers, 1997), a tool by Nara Institute of Sci-
ence and Technology (Miyata et al, 1999), and [incr
tsdb()] (Oepen et al, 2002). But these tools have
following problems; they largely depend on human
debuggers? language intuition, they do not help users
to handle large amount of parsing results effectively,
and they let human debuggers correct the bugs one
after another manually and locally.
To cope with these shortcomings, willex proposes
an alternative method for more efficient debugging
process.
The workflow of the conventional grammar devel-
oping tools and willex are different in the following
ways. With the conventional tools, human debug-
gers must check each sentence to find out grammar
defects and modify them one by one. On the other
hand, with willex human debuggers check sentences
that are tagged with syntactical structure, one by
one, find grammar defects, and record them, while
willex collects the whole grammar defect records.
Then human debuggers modify the found grammar
defects. This process allows human debuggers to
make priority over defects that appear more fre-
quently in the corpora, or defects that are more crit-
ical for purposes of syntactical parsing. Indeed, it
is possible for human debuggers using the conven-
tional tools to collect and modify the defects but
willex saves the trouble of human debuggers to col-
lect defects to modify them more efficiently.
3 Functions of willex
To create the new debugging tool, we have extended
will (Imai et al, 1998). Will is a browser of parsing
results of grammars based on feature structures. Will
and willex are implemented in JAVA.
3.1 Using XML Tagged Corpora
Willex uses sentence boundaries, word chunking,
and POSs/labels encoded in XML tagged corpora.
First, with the information of sentence boundaries
and word chunking, ambiguity of sentences is re-
duced, and ambiguity at parsing phase is also re-
duced. A parser connected to willex is assumed to
produce only results consistent with the information.
An example is shown in Figure 1 (<su> is a senten-
tial tag and <np> is a tag for noun phrases).
I  saw  a girl  with a telescope
I  saw  a girl  with a telescope


<su> I saw <np> a girl with a telescope </np></su>
Figure 1: An example of pa sing results along with
word chunking
Next, willex compares POSs/labels encoded in
XML tags and parsing results, and deletes improper
parsing trees. Therefore, it reduces numbers of par-
tial parsing trees, which appear in the way of parsing
and should be checked by human debuggers. In ad-
dition, human debuggers can delete partial parsing
trees manually later. Figure 2 shows a concrete ex-
ample. (NP and S are labels for noun and sentential
phrases respectively.)
POS/label from Tagged Corpus
POSs/labels from Partial Results
<NP> A cat </NP> knows everything
A      cat
D      N N       V
A      cat
NP S  
Figure 2: An example of deletion by using
POSs/labels
3.2 Output of Grammar Defects
Willex has a function to output information of gram-
mar defects into a file in order to collect the de-
fects data and treat them statistically. In addition,
we can save a log of debugging experiences which
show what grammar defects are found.
An example of an output file is shown in Table
1. It includes sentence numbers, word ranges in
which parsing failed, and comments input by a hu-
man debugger. For example, the first row of the ta-
ble means that the sentence #0 has coordinations of
verb phrases at position #3?#12, which cannot be
parsed. ?OK? in the second row means the sen-
tence is parsed correctly (i.e., no grammar defects
are found in the sentence). The third row means that
the word #4 of the sentence #2 has no proper lexical
entry.
The word ranges are specified by human debug-
gers using a GUI, which shows parsing results in
CKY tables and parse trees. The comments are input
by human debuggers in a natural language or chosen
from the list of previous comments. A postprocess-
ing module of willex sorts the error data by the com-
ments to help statistical analysis.
Table 1: An example of file output
Sentence # Word # comment
0 3?12 V-V coordination
1 ? OK
2 4 no lexical entry
4 Experiments and Discussion
We have applied willex to rental-XTAG, an HPSG-
style grammar converted from the XTAG English
grammar (The XTAG Research Group, 2001) by a
grammar conversion (Yoshinaga and Miyao, 2001).1
The corpus used is MEDLINE abstracts with tags
based on a slightly modified version of GDA-
DTD2 (Hasida, 2003). The corpus is ?partially
parsed?; the attachments of prepositional phrases are
annotated manually.
The tags do not always specify the correct struc-
tures based on rental-XTAG (i.e., the grammar as-
sumed by tags is different from rental-XTAG), so we
prepared a POS/label conversion table. We can use
tagged corpora based on various grammars different
from the grammar that the parser is assuming by us-
ing POS/label conversion tables.
We investigated 208 sentences (average 24.2
words) from 26 abstracts. 73 sentences were parsed
successfully and got correct results. Thus the cover-
age was 35.1%.
4.1 Qualitative Evaluation
Willex received three major positive feedbacks from
a user; first, the function of restricting partial results
was helpful, as it allows human debuggers to check
fewer results, second, the function to delete incorrect
partial results manually was useful, because there
are some cases that tags do not specify POSs/labels,
and third, human debuggers could use the record-
ing function to make notes to analyze them carefully
later.
However, willex also received some negative eval-
uations; the process of locating the cause of pars-
ing failure in a sentence was found to be a bit trou-
blesome. Also, willex loses its accuracy if the hu-
man debuggers themselves have trouble understand-
ing the correct syntactical structure of a sentence.3
1Since XTAG and rental-XTAG generate equivalent parse
results for the same input, debugging rental-XTAG means de-
bugging XTAG itself.
2GDA has no tags which specify prepositional phrases, so
we add <prep> and <prepp>.
3Thus, we divided the process of identifying grammar de-
fects to two steps. First, a non-expert roughly classifies pars-
ing errors and records temporary memorandums. Then, the
non-expert shows typical examples of sentences in each class
to experts and identifies grammar defects based on experts? in-
ference. Here, we can make use of the recording function of
We found from these evaluations that the func-
tions of willex can be used effectively, though more
automation is needed.
4.2 Quantitative Evaluation
Figure 3 shows the decrease in partial parsing trees
caused by using the tagged corpus. (Data of 10 sen-
tences among the 208 sentences are shown.) The
graph shows that human workload was reduced by
using the tagged corpus.
0
5000
10000
15000
20000
25000
30000
35000
10 15 20 25 30 35 40
n
u
m
b
e
r
 
o
f
 
p
a
r
t
i
a
l
 
r
e
s
u
l
t
s
length of a sentence (number of words)
without any info.with chunk info.with chunk and POS/label info.
Figure 3: Examples of numbers of partial results
4.3 Defects of rental-XTAG
Table 2 shows the defects of rental-XTAG which are
found by using willex.
Table 2: The defects of rental-XTAG
the defects of rental-XTAG #
no lexical entry 62
cannot handle reduced relative 35
cannot handle V-V coordination 22
Adjective does not post-modify NP 9
cannot parse ?, but not? 4
cannot handle objective to-infinitive 3
?, which ...? does not post-modify NP 3
cannot handle reduced as-relative clause 2
cannot parse ?greater than?(?>?) 2
misc. 17
From this table, it is inferred that (1) lack of lexi-
cal entries, (2) inability to parse reduced relative and
willex.
(3) inability to parse coordinations of verbs are seri-
ous problems of rental-XTAG.
4.4 Conflicts Between the Modified GDA and
rental-XTAG
Conflicts between rental-XTAG and the grammar on
which the modified GDA based cause parsing fail-
ures. Statistics of the conflicts is shown in Table 3.
Table 3: Conflicts between the modified GDA and
rental-XTAG
modified GDA rental-XTAG #
adjectival phrase verbal phrase 36
bracketing except ?,? 10
bracketing of ?,? 8
treatment of omitted words 2
misc. 5
These conflicts cannot be resolved by a simple
POS/label conversion table. One resolution is insert-
ing a preprocess module that deletes and moves tags
which cause conflicts.
We do not consider these conflicts as grammar de-
fects but the difference of grammars to be absorbed
in the conversion phase.
5 Conclusion and Future Work
We developed a debug tool, willex, which uses XML
tagged corpora and outputs information of grammar
defects. By using tagged corpora, willex succeeded
to reduce human workload. And by recording gram-
mar defects, it provides debugging environment with
a bigger perspective. But there remains a prob-
lem that a simple POS/label conversion table is not
enough to resolve conflicts of a debugged grammar
and a grammar assumed by tags. The tool should
support to handle the complicated conflicts.
In the future, we will try to modify willex to infer
causes of parsing errors (semi-)automatically. It is
difficult to find a point of parsing failure automati-
cally, because subsentences that have no correspon-
dent partial results are not always the failed point.
Hence, we will expand willex to find the longest
subsentences that are parsed successfully. Words,
POS/labels and features of the subsentences can be
clues to infer the causes of parsing errors.
References
Thilo Go?tz and Walt Detmar Meurers. 1997. The Con-
Troll system as large grammar development platform.
In Proc. of Workshop on Computational Environments
for Grammar Development and Linguistic Engineer-
ing, pages 38?45.
Koiti Hasida. 2003. Global docu-
ment annotation (GDA). available in
http://www.i-content.org/GDA/.
Hisao Imai, Yusuke Miyao, and Jun?ichi Tsujii. 1998.
GUI for an HPSG parser. In Information Processing
Society of Japan SIG Notes NL-127, pages 173?178,
September. In Japanese.
Takashi Miyata, Kazuma Takaoka, and Yuji Mat-
sumoto. 1999. Implementation of GUI debugger for
unification-based grammar. In Information Process-
ing Society of Japan SIG Notes NL-129, pages 87?94,
January. In Japanese.
Stephan Oepen, Emily M. Bender, Uli Callmeier, Dan
Flickinger, and Melanie Siegel. 2002. Parallel dis-
tributed grammar engineering for practical applica-
tions. In Proc. of the Workshop on Grammar Engi-
neering and Evaluation, pages 15?21.
Patrick Paroubek, Yves Schabes, and Aravind K. Joshi.
1992. XTAG ? a graphical workbench for developing
Tree-Adjoining grammars. In Proc. of the 3rd Confer-
ence on Applied Natural Language Processing, pages
216?223.
Paul Schmidt, Axel Theofilidis, Sibylle Rieder, and
Thierry Declerck. 1996. Lean formalisms, linguis-
tic theory, and applications. Grammar development in
ALEP. In Proc. of COLING ?96, volume 1, pages
286?291.
Yuka Tateisi, Kentaro Torisawa, Yusuke Miyao, and
Jun?ichi Tsujii. 1998. Translating the XTAG english
grammar to HPSG. In Proc. of TAG+4 workshop,
pages 172?175.
The XTAG Research Group. 2001. A Lex-
icalized Tree Adjoining Grammar for English.
Technical Report IRCS Research Report 01-03,
IRCS, University of Pennsylvania. available in
http://www.cis.upenn.edu/?xtag/.
Akane Yakushiji, Yuka Tateisi, Yusuke Miyao, and
Jun?ichi Tsujii. 2001. Event extraction from biomedi-
cal papers using a full parser. In Pacific Symposium on
Biocomputing 2001, pages 408?419, January.
Naoki Yoshinaga and Yusuke Miyao. 2001. Grammar
conversion from LTAG to HPSG. In Proc. of the sixth
ESSLLI Student Session, pages 309?324.
Comparison between CFG filtering techniques for LTAG and HPSG
Naoki Yoshinaga?
? University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo, 113-0033, Japan
yoshinag@is.s.u-tokyo.ac.jp
Kentaro Torisawa?
? Japan Advanced Institute
of Science and Technology
1-1 Asahidai, Tatsunokuchi,
Ishikawa, 923-1292, Japan
torisawa@jaist.ac.jp
Jun?ichi Tsujii??
? CREST, JST (Japan Science
and Technology Corporation)
Hon-cho 4-1-8, Kawaguchi-shi,
Saitama, 332-0012, Japan
tsujii@is.s.u-tokyo.ac.jp
Abstract
An empirical comparison of CFG filtering
techniques for LTAG and HPSG is pre-
sented. We demonstrate that an approx-
imation of HPSG produces a more effec-
tive CFG filter than that of LTAG. We also
investigate the reason for that difference.
1 Introduction
Various parsing techniques have been developed
for lexicalized grammars such as Lexicalized
Tree Adjoining Grammar (LTAG) (Schabes et al,
1988), and Head-Driven Phrase Structure Gram-
mar (HPSG) (Pollard and Sag, 1994). Along with
the independent development of parsing techniques
for individual grammar formalisms, some of them
have been adapted to other formalisms (Schabes et
al., 1988; van Noord, 1994; Yoshida et al, 1999;
Torisawa et al, 2000). However, these realiza-
tions sometimes exhibit quite different performance
in each grammar formalism (Yoshida et al, 1999;
Yoshinaga et al, 2001). If we could identify an al-
gorithmic difference that causes performance differ-
ence, it would reveal advantages and disadvantages
of the different realizations. This should also allow
us to integrate the advantages of the realizations into
one generic parsing technique, which yields the fur-
ther advancement of the whole parsing community.
In this paper, we compare CFG filtering tech-
niques for LTAG (Harbusch, 1990; Poller and
Becker, 1998) and HPSG (Torisawa et al, 2000;
Kiefer and Krieger, 2000), following an approach to
parsing comparison among different grammar for-
malisms (Yoshinaga et al, 2001). The key idea
of the approach is to use strongly equivalent gram-
mars, which generate equivalent parse results for the
same input, obtained by a grammar conversion as
demonstrated by Yoshinaga and Miyao (2001). The
parsers with CFG filtering predict possible parse
trees by a CFG approximated from a given grammar.
Comparison of those parsers are interesting because
effective CFG filters allow us to bring the empirical
time complexity of the parsers close to that of CFG
parsing. Investigating the difference between the
ways of context-free (CF) approximation of LTAG
and HPSG will thereby enlighten a way of further
optimization for both techniques.
We performed a comparison between the exist-
ing CFG filtering techniques for LTAG (Poller and
Becker, 1998) and HPSG (Torisawa et al, 2000),
using strongly equivalent grammars obtained by
converting LTAGs extracted from the Penn Tree-
bank (Marcus et al, 1993) into HPSG-style. We
compared the parsers with respect to the size of the
approximated CFG and its effectiveness as a filter.
2 Background
In this section, we introduce a grammar conver-
sion (Yoshinaga and Miyao, 2001) and CFG filter-
ing (Harbusch, 1990; Poller and Becker, 1998; Tori-
sawa et al, 2000; Kiefer and Krieger, 2000).
2.1 Grammar conversion
The grammar conversion consists of a conversion
of LTAG elementary trees to HPSG lexical entries
and an emulation of substitution and adjunction by
S
NP VP
V NP
S
NP VP
V S5.1
5.?
5.2
5.2.1 5.2.2
9.1
9.?
9.2
9.2.1 9.2.2
Tree 5: Tree 9: SCFG rulesNP VP
VP V NPVP V S
5.? 5.1 5.29.? 9.1 9.2
5.2 5.2.1 5.2.2
9.2 9.2.1 9.2.2
Figure 1: Extraction of CFG from LTAG
pre-determined grammar rules. An LTAG elemen-
tary tree is first converted into canonical elementary
trees which have only one anchor and whose sub-
trees of depth n (?1) contain at least one anchor. A
canonical elementary tree is then converted into an
HPSG lexical entry by regarding the leaf nodes as
arguments and by storing them in a stack.
We can perform a comparison between LTAG and
HPSG parsers using strongly equivalent grammars
obtained by the above conversion. This is because
strongly equivalent grammars can be a substitute for
the same grammar in different grammar formalisms.
2.2 CFG filtering techniques
An initial offline step of CFG filtering is performed
to approximate a given grammar with a CFG. The
obtained CFG is used as an efficient device to com-
pute the necessary conditions for parse trees.
The CFG filtering generally consists of two steps.
In phase 1, the parser first predicts possible parse
trees using the approximated CFG, and then filters
out irrelevant edges by a top-down traversal starting
from roots of successful context-free derivations. In
phase 2, it then eliminates invalid parse trees by us-
ing constraints in the given grammar. We call the
remaining edges that are used for the phase 2 pars-
ing essential edges.
The parsers with CFG filtering used in our ex-
periments follow the above parsing strategy, but are
different in the way the CF approximation and the
elimination of impossible parse trees in phase 2 are
performed. In the following sections, we briefly de-
scribe the CF approximation and the elimination of
impossible parse trees in each realization.
2.2.1 CF approximation of LTAG
In CFG filtering techniques for LTAG (Harbusch,
1990; Poller and Becker, 1998), every branching of
elementary trees in a given grammar is extracted as
a CFG rule as shown in Figure 1.
Grammar rule
lexicalSYNSEM  ?
signSYNSEM  ?signSYNSEM  ?
phrasalSYNSEM  ?
Grammar rule
phrasalSYNSEM  ?
signSYNSEM  ?signSYNSEM  ?
phrasalSYNSEM  ?
phrasalSYNSEM  ?
A
B
C
X
Y
B X AC Y B
signSYNSEM  ?
signSYNSEM  ?
CFG rules
Figure 2: Extraction of CFG from HPSG
Because the obtained CFG can reflect only local
constraints given in each local structure of the el-
ementary trees, it generates invalid parse trees that
connect local trees in different elementary trees. In
order to eliminate such parse trees, a link between
branchings is preserved as a node number which
records a unique node address (a subscript attached
to each node in Figure 1). We can eliminate these
parse trees by traversing essential edges in a bottom-
up manner and recursively propagating ok-flag from
a node number x to a node number y when a connec-
tion between x and y is allowed in the LTAG gram-
mar. We call this propagation ok-prop.
2.2.2 CF approximation of HPSG
In CFG filtering techniques for HPSG (Torisawa
et al, 2000; Kiefer and Krieger, 2000), the extrac-
tion process of a CFG from a given HPSG gram-
mar starts by recursively instantiating daughters of a
grammar rule with lexical entries and generated fea-
ture structures until new feature structures are not
generated as shown in Figure 2. We must impose
restrictions on values of some features (i.e., ignor-
ing them) and/or the number of rule applications in
order to guarantee the termination of the rule appli-
cation. A CFG is obtained by regarding each initial
and generated feature structures as nonterminals and
transition relations between them as CFG rules.
Although the obtained CFG can reflect local and
global constraints given in the whole structure of
lexical entries, it generates invalid parse trees be-
cause they do not reflect upon constraints given by
the values of features that are ignored in phase 1.
These parse trees are eliminated in phase 2 by apply-
ing a grammar rule that corresponds to the applied
CFG rule. We call this rule application rule-app.
Table 1: The size of extracted LTAGs (tree tem-
plates) and approximated CFGs (above: the number
of nonterminals; below: the number of rules)
Grammar G2 G2-4 G2-6 G2-8 G2-10 G2-21
LTAG 1,488 2,412 3,139 3,536 3,999 6,085
CFGPB 65 66 66 66 67 67
716 954 1,090 1,158 1,229 1,552
CFGTNT 1,989 3,118 4,009 4,468 5,034 7,454
18,323 35,541 50,115 58,356 68,239 118,464
Table 2: Parsing performance (sec.) with the
strongly equivalent grammars for Section 2 of WSJ
Parser G2 G2-4 G2-6 G2-8 G2-10 G2-21
PB 1.4 9.1 17.4 24.0 34.2 124.3
TNT 0.044 0.097 0.144 0.182 0.224 0.542
3 Comparison with CFG filtering
In this section, we compare a pair of CFG filter-
ing techniques for LTAG (Poller and Becker, 1998)
and HPSG (Torisawa et al, 2000) described in Sec-
tion 2.2.1 and 2.2.2. We hereafter refer to PB and
TNT for the C++ implementations of the former and
a valiant1 of the latter, respectively.2
We first acquired LTAGs by a method pro-
posed in Miyao et al (2003) from Sections 2-21 of
the Wall Street Journal (WSJ) in the Penn Tree-
bank (Marcus et al, 1993) and its subsets.3 We then
converted them into strongly equivalent HPSG-style
grammars using the grammar conversion described
in Section 2.1. Table 1 shows the size of CFG ap-
proximated from the strongly equivalent grammars.
Gx, CFGPB, and CFGTNT henceforth refer to the
LTAG extracted from Section x of WSJ and CFGs
approximated from Gx by PB and TNT, respectively.
The size of CFGTNT is much larger than that of
CFGPB. By investigating parsing performance using
these CFGs, we show that the larger size of CFGTNT
resulted in better parsing performance.
Table 2 shows the parse time with 254 sentences
of length n (?10) from Section 2 of WSJ (the av-
erage length is 6.72 words).4 This result shows not
only that TNT achieved a drastic speed-up against
1All daughters of rules are instantiated in the approximation.
2In phase 1, PB performs Earley (Earley, 1970) parsing
while TNT performs CKY (Younger, 1967) parsing.
3The elementary trees in the LTAGs are binarized.
4We used a subset of the training corpus to avoid the com-
plication of using default lexical entries for unknown words.
Table 3: The numbers of essential edges with the
strongly equivalent grammars for Section 02 of WSJ
Parser G2 G2-4 G2-6 G2-8 G2-10 G2-21
PB 791 1,435 1,924 2,192 2,566 3,976
TNT 63 121 174 218 265 536
Table 4: The success rate (%) of phase 2 operations
Operations G2 G2-4 G2-6 G2-8 G2-10 G2-21
ok-prop (PB) 38.5 34.3 33.1 32.3 31.7 31.0
rule-app (TNT) 100 100 100 100 100 100
PB, but also that performance difference between
them increases with the larger size of the grammars.
In order to estimate the degree of CF approxima-
tion, we measured the number of essential (inactive)
edges of phase 1. Table 3 shows the number of the
essential edges. The number of essential edges pro-
duced by PB is much larger than that produced by
TNT . We then investigated the effect on phase 2
as caused by the different number of the essential
edges. Table 4 shows the success rate of ok-prop
and rule-app. The success rate of rule-app is 100%,5
whereas that of ok-prop is quite low.6 These results
indicate that CFGTNT is superior to CFGPB with re-
spect to the degree of the CF approximation.
We can explain the reason for this difference by
investigating how TNT approximates HPSG-style
grammars converted from LTAGs. As described
in Section 2.1, the grammar conversion preserves
the whole structure of each elementary tree (pre-
cisely, a canonical elementary tree) in a stack, and
grammar rules manipulate a head element of the
stack. A generated feature structure in the approxi-
mation process thus corresponds to the whole unpro-
cessed portion of a canonical elementary tree. This
implies that successful context-free derivations ob-
tained by CFGTNT basically involve elementary trees
in which all substitution and adjunction have suc-
ceeded. However, CFGPB (also a CFG produced
by the other work (Harbusch, 1990)) cannot avoid
generating invalid parse trees that connect two lo-
5This means that the extracted LTAGs should be compatible
with CFG and were completely converted to CFGs by TNT .
6Similar results were obtained in preliminary experiments
using the XTAG English grammar (The XTAG Research Group,
2001) without features (parse time (sec.)/success rate (%) for
PB and TNT were 15.3/30.6 and 0.606/71.2 with the same sen-
tences), though space limitations preclude complete results.
cal structures where adjunction takes place between
them. We measured with G2-21 the proportion of the
number of ok-prop between two node numbers of
nodes that take adjunction and its success rate. It
occupied 87% of the total number of ok-prop and
its success rate was only 22%. These results sug-
gest that the global contexts in a given grammar is
essential to obtain an effective CFG filter.
It should be noted that the above investigation also
tells us another way of CF approximation of LTAG.
We first define a unique way of tree traversal such as
head-corner traversal (van Noord, 1994) on which
we can perform a sequential application of substitu-
tion and adjunction. We then recursively apply sub-
stitution and adjunction on that traversal to an ele-
mentary tree and a generated tree structure. Because
the processed portions of generated tree structures
are no longer used later, we regard the unprocessed
portions of the tree structures as nonterminals of
CFG. We can thereby construct another CFG filter-
ing for LTAG by combining this CFG filter with an
existing LTAG parsing algorithm (van Noord, 1994).
4 Conclusion and future direction
We presented an empirical comparison of LTAG and
HPSG parsers with CFG filtering. We compared the
parsers with strongly equivalent grammars obtained
by converting LTAGs extracted from the Penn Tree-
bank into HPSG-style. Experimental results showed
that the existing CF approximation of HPSG (Tori-
sawa et al, 2000) produced a more effective filter
than that of LTAG (Poller and Becker, 1998). By in-
vestigating the different ways of CF approximation,
we concluded that the global constraints in a given
grammar is essential to obtain an effective filter.
We are going to integrate the advantage of the CF
approximation of HPSG into that of LTAG in order
to establish another CFG filtering for LTAG. We will
also conduct experiments on trade-offs between the
degree of CF approximation and the size of approx-
imated CFGs as in Maxwell III and Kaplan (1993).
Acknowledgment
We thank Yousuke Sakao for his help in profiling
TNT parser and anonymous reviewers for their help-
ful comments. This work was partially supported by
JSPS Research Fellowships for Young Scientists.
References
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
K. Harbusch. 1990. An efficient parsing algorithm for
Tree Adjoining Grammars. In Proc. of ACL, pages
284?291.
B. Kiefer and H.-U. Krieger. 2000. A Context-Free ap-
proximation of Head-Driven Phrase Structure Gram-
mar. In Proc. of IWPT, pages 135?146.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguistics,
19(2):313?330.
J. T. Maxwell III and R. M. Kaplan. 1993. The interface
between phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?590.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2003. Lexicalized
grammar acquisition. In Proc. of EACL companion
volume, pages 127?130.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
P. Poller and T. Becker. 1998. Two-step TAG parsing
revisited. In Proc. of TAG+4, pages 143?146.
Y. Schabes, A. Abeille?, and A. K. Joshi. 1988. Pars-
ing strategies with ?lexicalized? grammars: Applica-
tion to Tree Adjoining Grammars. In Proc. of COL-
ING, pages 578?583.
The XTAG Research Group. 2001. A Lexicalized Tree
Adjoining Grammar for English. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
K. Torisawa, K. Nishida, Y. Miyao, and J. Tsujii. 2000.
An HPSG parser with CFG filtering. Natural Lan-
guage Engineering, 6(1):63?80.
G. van Noord. 1994. Head corner parsing for TAG.
Computational Intelligence, 10(4):525?534.
M. Yoshida, T. Ninomiya, K. Torisawa, T. Makino, and
J. Tsujii. 1999. Efficient FB-LTAG parser and its par-
allelization. In Proc. of PACLING, pages 90?103.
N. Yoshinaga and Y. Miyao. 2001. Grammar conver-
sion from LTAG to HPSG. In Proc. of ESSLLI Student
Session, pages 309?324.
N. Yoshinaga, Y. Miyao, K. Torisawa, and J. Tsujii.
2001. Efficient LTAG parsing using HPSG parsers. In
Proc. of PACLING, pages 342?351.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 2(10):189?208, February.
Improving the Accuracy of Subcategorizations Acquired from Corpora
Naoki Yoshinaga
Department of Computer Science,
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033
yoshinag@is.s.u-tokyo.ac.jp
Abstract
This paper presents a method of improv-
ing the accuracy of subcategorization
frames (SCFs) acquired from corpora to
augment existing lexicon resources. I
estimate a confidence value of each SCF
using corpus-based statistics, and then
perform clustering of SCF confidence-
value vectors for words to capture co-
occurrence tendency among SCFs in the
lexicon. I apply my method to SCFs
acquired from corpora using lexicons
of two large-scale lexicalized grammars.
The resulting SCFs achieve higher pre-
cision and recall compared to SCFs ob-
tained by naive frequency cut-off.
1 Introduction
Recently, a variety of methods have been proposed
for acquisition of subcategorization frames (SCFs)
from corpora (surveyed in (Korhonen, 2002)).
One interesting possibility is to use these tech-
niques to improve the coverage of existing large-
scale lexicon resources such as lexicons of lexi-
calized grammars. However, there has been little
work on evaluating the impact of acquired SCFs
with the exception of (Carroll and Fang, 2004).
The problem when we integrate acquired SCFs
into existing lexicalized grammars is lower qual-
ity of the acquired SCFs, since they are acquired
in an unsupervised manner, rather than being man-
ually coded. If we attempt to compensate for the
poor precision by being less strict in filtering out
less likely SCFs, then we will end up with a larger
number of noisy lexical entries, which is problem-
atic for parsing with lexicalized grammars (Sarkar
et al, 2000). We thus need some method of select-
ing the most reliable set of SCFs from the system
output as demonstrated in (Korhonen, 2002).
In this paper, I present a method of improving
the accuracy of SCFs acquired from corpora in or-
der to augment existing lexicon resources. I first
estimate a confidence value that a word can have
each SCF, using corpus-based statistics. To cap-
ture latent co-occurrence tendency among SCFs
in the target lexicon, I next perform clustering of
SCF confidence-value vectors of words in the ac-
quired lexicon and the target lexicon. Since each
centroid value of the obtained clusters indicates
whether the words in that cluster have each SCF,
we can eliminate SCFs acquired in error and pre-
dict possible SCFs according to the centroids.
I applied my method to SCFs acquired from
a corpus of newsgroup posting about mobile
phones (Carroll and Fang, 2004), using the
XTAG English grammar (XTAG Research Group,
2001) and the LinGO English Resource Grammar
(ERG) (Copestake, 2002). I then compared the
resulting SCFs with SCFs obtained by naive fre-
quency cut-off to observe the effects of clustering.
2 Background
2.1 SCF Acquisition for Lexicalized
Grammars
I start by acquiring SCFs for a lexicalized gram-
mar from corpora by the method described in (Car-
roll and Fang, 2004).
#S(EPATTERN :TARGET |yield|
:SUBCAT (VSUBCAT NP)
:CLASSES ((24 51 161) 5293)
:RELIABILITY 0
:FREQSCORE 0.26861903
:FREQCNT 1 :TLTL (VV0)
:SLTL ((|route| NN1))
:OLT1L ((|result| NN2))
:OLT2L NIL
:OLT3L NIL :LRL 0))
Figure 1: An acquired SCF for a verb ?yield?
In their study, they first acquire fine-grained
SCFs using the unsupervised method proposed by
Briscoe and Carroll (1997) and Korhonen (2002).
Figure 1 shows an example of one acquired SCF
entry for a verb ?yield.? Each SCF entry has
several fields about the observed SCF. I explain
here only its portion related to this study. The
TARGET field is a word stem, the first number in
the CLASSES field indicates an SCF type, and the
FREQCNT field shows how often words derivable
from the word stem appeared with the SCF type in
the training corpus. The obtained SCFs comprise
the total 163 SCF types which are originally based
on the SCFs in the ANLT (Boguraev and Briscoe,
1987) and COMLEX (Grishman et al, 1994) dic-
tionaries. In this example, the SCF type 24 corre-
sponds to an SCF of transitive verb.
They then obtain SCFs for the target lexicalized
grammar (the LinGO ERG (Copestake, 2002) in
their study) using a handcrafted translation map
from these 163 types to the SCF types in the target
grammar. They reported that they could achieve
a coverage improvement of 4.5% but that aver-
age parse time was doubled. This is because they
did not use any filtering method for the acquired
SCFs to suppress an increase of the lexical ambi-
guity. We definitely need some method to control
the quality of the acquired SCFs.
Their method is extendable to any lexicalized
grammars, if we could have a translation map from
these 163 types to the SCF types in the grammar.
2.2 Clustering of Verb SCF Distributions
There is some related work on clustering of
verbs according to their SCF probability distri-
butions (Schulte im Walde and Brew, 2002; Ko-
rhonen et al, 2003). Schulte im Walde and
(true) probability distribution
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
NP None NP_to-PP NP_PP PP
subcategorization frame
pr
o
ba
bi
lit
y
apply
recognition 
threshold
Figure 2: SCF probability distributions for apply
Brew (2002) used the k-Means (Forgy, 1965) al-
gorithm to cluster SCF distributions for monose-
mous verbs while Korhonen et al (2003) applied
other clustering methods to cluster polysemic SCF
data. These studies aim at obtaining verb seman-
tic classes, which are closely related to syntactic
behavior of argument selection (Levin, 1993).
Korhonen (2002) made use of SCF distributions
for representative verbs in Levin?s verb classes to
obtain accurate back-off estimates for all the verbs
in the classes. In this study, I assume that there
are classes whose element words have identical
SCF types. I then obtain these classes by clus-
tering acquired SCFs, using information available
in the target lexicon, and directly use the obtained
classes to eliminate implausible SCFs.
3 Method
3.1 Estimation of Confidence Values for SCFs
I first create an SCF confidence-value vector vi for
each word wi, an object for clustering. Each el-
ement vi j in vi represents a confidence value of
SCF s j for a word wi, which expresses how strong
the evidence is that the word wi has SCF s j. Note
that a confidence value con fi j is not a probability
that a word wi appears with SCF s j but a proba-
bility of existence of SCF s j for the word wi. In
this study, I assume that a word wi appears with
each SCF s j with a certain (non-zero) probabil-
ity ?i j(= p(si j|wi)> 0 where ? j ?i j = 1), but only
SCFs whose probabilities exceed a certain thresh-
old are recognized in the lexicon. I hereafter call
this threshold recognition threshold. Figure 2 de-
picts a probability distribution of SCF for apply.
In this context, I can regard a confidence value of
each SCF as a probability that the probability of
that SCF exceeds the recognition threshold.
One intuitive way to estimate a confidence value
is to assume an observed probability, i.e., relative
frequency, is equal to a probability ?i j of SCF s j
for a word wi (?i j = f reqi j/? j f reqi j where f reqi j
is a frequency that a word wi appears with SCF s j
in corpora). When the relative frequency of s j for
a word wi exceeds the recognition threshold, its
confidence value con fi j is set to 1, and otherwise
con fi j is set to 0. However, an observed probabil-
ity is unreliable for infrequent words. Moreover,
when we want to encode confidence values of re-
liable SCFs in the target grammar, we cannot dis-
tinguish the confidence values of those SCFs with
confidence values of acquired SCFs.
The other promising way to estimate a confi-
dence value, which I adopt in this study, is to as-
sume a probability ?i j as a stochastic variable in
the context of Bayesian statistics (Gelman et al,
1995). In this context, a posteriori distribution of
the probability ?i j of an SCF s j for a word wi is
given by:
p(?i j|D) =
P(?i j)P(D|?i j)
P(D)
=
P(?i j)P(D|?i j)
? 1
0 P(?i j)P(D|?i j)d?i j
, (1)
where P(?i j) is a priori distribution, and D is the
data we have observed. Since every occurrence
of SCFs in the data D is independent with each
other, the data D can be regarded as Bernoulli tri-
als. When we observe the data D that a word wi
appears n times in total and x(? n) times with SCF
s j,1 its conditional distribution is represented by
binominal distribution:
P(D|?i j) =
(n
x
)
? xi j(1??i j)(n?x). (2)
To calculate this a posteriori distribution, I need
to define the a priori distribution P(?i j). The ques-
tion is which probability distribution of ?i j can
appropriately reflects prior knowledge. In other
words, it should encode knowledge we use to es-
timate SCFs for unknown words. I simply deter-
mine it from distributions of observed probability
values of s j for words seen in corpora2 by using
1The values of FREQCNT is used to obtain n and x.
2I estimated a priori distribution separately for each type
of SCF from words that appeared more than 50 times in the
training corpus in the following experiments.
a method described in (Tsuruoka and Chikayama,
2001). In their study, they assume a priori distri-
bution as the beta distribution defined as:
p(?i j|? ,? ) =
? ??1i j (1??i j)??1
B(? ,? ) , (3)
where B(? ,? ) = ? 10 ? ??1i j (1 ? ?i j)??1d?i j. The
value of ? and ? is determined by moment esti-
mation.3 By substituting Equations 2 and 3 into
Equation 1, I finally obtain the a posteriori distri-
bution p(?i j|D) as:
p(?i j|? ,? ,D)= c ?? x+??1i j (1??i j)n?x+??1,(4)
where c =
(
n
x
)
/(B(? ,? )? 10 P(?i j)P(D|?i j)d?i j).
When I regard the recognition threshold as t, I
can calculate a confidence value con fi j that a word
wi can have s j by integrating the a posteriori dis-
tribution p(?i j|D) from the threshold t to 1:
con fi j =
? 1
t
c ?? x+??1i j (1??i j)n?x+??1d?i j.(5)
By using this confidence value, I represent an SCF
confidence-value vector vi for a word wi in the ac-
quired SCF lexicon (vi j = con fi j).
In order to combine SCF confidence-value vec-
tors for words acquired from corpora and those for
words in the lexicon of the target grammar, I also
represent an SCF confidence-value vector v?i for a
word w?i in the target grammar by:
v?i j =
{
1? ? w?i has s j in the lexicon
? otherwise, (6)
where ? expresses an unreliability of the lexicon.
In this study, I trust the lexicon as much as possible
by setting ? to the machine epsilon.
3.2 Clustering of SCF Confidence-Value
Vectors
I next present a clustering algorithm of words
according to their SCF confidence-value vectors.
Given k initial representative vectors called cen-
troids, my algorithm iteratively updates clusters by
assigning each data object to its closest centroid
3The expectation and variance of the beta distribution are
made equal to those of the observed probability values.
Input: a set of SCF confidence-value
vectors V= {v1,v2, . . . ,vn} ? Rm
a distance function d : Rm ?Zm ? R
a function to compute a centroid
? : {v j1 ,v j2 , . . . ,v jl }? Z
m
initial centroids C= {c1,c2, . . . ,ck} ? Zm
Output: a set of clusters {Cj}
while cluster members are not stable do
foreach cluster Cj
Cj = {vi |?cl ,d(vi,c j)? d(vi,cl)} (1)
end foreach
foreach clusters Cj
c j = ?(Cj) (2)
end foreach
end while
return {Cj}
Figure 3: Clustering algorithm for SCF
confidence-value vectors
and recomputing centroids until cluster members
become stable, as depicted in Figure 3.
Although this algorithm is roughly based on the
k-Means algorithm, it is different from k-Means in
important respects. I assume the elements of the
centroids of the clusters as a discrete value of 0 or
1 because I want to obtain clusters whose element
words have the exactly same set of SCFs.
I then derive a distance function d to calculate
a probability that a data object vi should have an
SCF set represented by a centroid cm as follows:
d(vi,cm) = ?
cm j=1
vi j ? ?
cm j=0
(1? vi j). (7)
By using this function, I can determine the closest
cluster as argmax
Cm
d(vi,cm) ((1) in Figure 3).
After every assignment, I calculate a next cen-
troid cm of each cluster Cm ((2) in Figure 3) by
comparing a probability that the words in the clus-
ter have an SCF s j and a probability that the words
in the cluster do not have the SCF s j as follows:
cm j =
?
?
?
1 when ?
vi?Cm
vi j > ?
vi?Cm
(1? vi j)
0 otherwise.
(8)
I next address the way to determine the num-
ber of clusters and initial centroids. In this study,
I assume that the most of the possible set of SCFs
for words are included in the lexicon of the tar-
get grammar,4 and make use of the existing sets of
4When the lexicon is less accurate, I can determine the
number of clusters using other algorithms (Hamerly, 2003).
SCFs for the words in the lexicon to determine the
number of clusters and initial centroids. I first ex-
tract SCF confidence-value vectors from the lexi-
con of the grammar. By eliminating duplications
from them and regarding ? = 0 in Equation 6, I ob-
tain initial centroids cm. I then initialize the num-
ber of clusters k to the number of cm.
I finally update the acquired SCFs using the ob-
tained clusters and the confidence values of SCFs
in this order. I call the following procedure cen-
troid cut-off t when the confidence values are es-
timated under the recognition threshold t. Since
the value cm j of a centroid cm in a cluster Cm rep-
resents whether the words in the cluster can have
SCF s j, I first obtain SCFs by collecting SCF s j
for a word wi ? Cm when cm j is 1. I then elimi-
nate implausible SCFs s j for wi from the resulting
SCFs according to their confidence values con fi j.
In the following, I compare centroid cut-off
with frequency cut-off and confidence cut-off t,
which use relative frequencies and confidence val-
ues calculated under the recognition threshold t,
respectively. Note that these cut-offs use only
corpus-based statistics to eliminate SCFs.
4 Experiments
I applied my method to SCFs acquired from
135,902 sentences of mobile phone newsgroup
postings archived by Google.com, which is the
same data used in (Carroll and Fang, 2004). The
number of acquired SCFs was 14,783 for 3,864
word stems, while the number of SCF types in
the data was 97. I then translated the 163 SCF
types into the SCF types of the XTAG English
grammar (XTAG Research Group, 2001) and the
LinGO ERG (Copestake, 2002)5 using translation
mappings built by Ted Briscoe and Dan Flickinger
from 23 of the SCF types into 13 (out of 57 possi-
ble) XTAG SCF types, and 129 into 54 (out of 216
possible) ERG SCF types.
To evaluate my method, I split each lexicon of
the two grammars into the training SCFs and the
testing SCFs. The words in the testing SCFs were
included in the acquired SCFs. When I apply
my method to the acquired SCFs using the train-
ing SCFs and evaluate the resulting SCFs with the
5I used the same version of the LinGO ERG as (Carroll
and Fang, 2004) (1.4; April 2003) but the map is updated.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
Precision
A
B C D
A: frequency cut-off
B: confidence cut-off 0.01
C: confidence cut-off 0.03
D: confidence cut-off 0.05
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
Precision
A
B
C
D
A: frequency cut-off
B: confidence cut-off 0.01
C: confidence cut-off 0.03
D: confidence cut-off 0.05
XTAG ERG
Figure 4: Precision and recall of the resulting SCFs using confidence cut-offs and frequency cut-off: the
XTAG English grammar (left) the LinGO ERG (right)
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
Precision
A
B
C
D
A: frequency cut-off
B: centroid cut-off* 0.05
C: centroid cut-off 0.05
D: confidence cut-off 0.05
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
Precision
A B
C
D
A: frequency cut-off
B: centroid cut-off* 0.05
C: centroid cut-off 0.05
D: confidence cut-off 0.05
XTAG ERG
Figure 5: Precision and recall of the resulting SCFs using confidence cut-off and centroid cut-off: the
XTAG English grammar (left) the LinGO ERG (right)
testing SCFs, we can estimate to what extent my
method can preserve reliable SCFs for words un-
known to the grammar.6 The XTAG lexicon was
split into 9,437 SCFs for 8,399 word stems as
training and 423 SCFs for 280 word stems as test-
ing, while the ERG lexicon was split into 1,608
SCFs for 1,062 word stems as training and 292
SCFs for 179 word stems as testing. I extracted
SCF confidence-value vectors from the training
SCFs and the acquired SCFs for the words in the
testing SCFs. The number of the resulting data
objects was 8,679 for XTAG and 1,241 for ERG.
The number of initial centroids7 extracted from
the training SCFs was 49 for XTAG and 53 for
ERG. I then performed clustering of 8,679 data
objects into 49 clusters and 1,241 data objects into
6I here assume that the existing SCFs for the words in the
lexicon is more reliable than the other SCFs for those words.
7I used the vectors that appeared for more than one word.
53 clusters, and then evaluated the resulting SCFs
by comparing them to the testing SCFs.
I first compare confidence cut-off with fre-
quency cut-off to observe the effects of Bayesian
estimation. Figure 4 shows precision and recall
of the SCFs obtained using frequency cut-off and
confidence cut-off 0.01, 0.03, and 0.05 by varying
threshold for the confidence values and the relative
frequencies from 0 to 1.8 The graph indicates that
the confidence cut-offs achieved higher recall than
the frequency cut-off, thanks to the a priori distri-
butions. When we compare the three confidence
cut-offs, we can improve precision using higher
recognition thresholds while we can improve re-
call using lower recognition thresholds. This is
quite consistent with our expectations.
8 Precision=
Correct SCFs for the words in the resulting SCFs
All SCFs for the words in the resulting SCFs
Recall = Correct SCFs for the words in the resulting SCFsAll SCFs for the words in the test SCFs
I then compare centroid cut-off with confidence
cut-off to observe the effects of clustering. Fig-
ure 5 shows precision and recall of the resulting
SCFs using centroid cut-off 0.05 and the confi-
dence cut-off 0.05 by varying the threshold for the
confidence values. In order to show the effects
of the use of the training SCFs, I also performed
clustering of SCF confidence-value vectors in the
acquired SCFs with random initialization (k = 49
(for XTAG) and 53 (for ERG); centroid cut-off
0.05*). The graph shows that clustering is mean-
ingful only when we make use of the reliable SCFs
in the manually-coded lexicon. The centroid cut-
off using the lexicon of the grammar boosted pre-
cision compared to the confidence cut-off.
The difference between the effects of my
method on XTAG and ERG would be due to the
finer-grained SCF types of ERG. This resulted
in lower precision of the acquired SCFs for ERG,
which prevented us from distinguishing infrequent
(correct) SCFs from SCFs acquired in error. How-
ever, since unusual SCFs tend to be included in the
lexicon, we will be able to have accurate clusters
for unknown words with smaller SCF variations as
we achieved in the experiments with XTAG.
5 Concluding Remarks and Future Work
In this paper, I presented a method to improve
the quality of SCFs acquired from corpora using
existing lexicon resources. I applied my method
to SCFs acquired from corpora using lexicons of
the XTAG English grammar and the LinGO ERG,
and have shown that it can eliminate implausible
SCFs, preserving more reliable SCFs.
In the future, I need to evaluate the quality of
the resulting SCFs by manual analysis and by us-
ing the extended lexicons to improve parsing. I
will investigate other clustering methods such as
hierarchical clustering, and use other information
for clustering such as semantic preference of argu-
ments of SCFs to have more accurate clusters.
Acknowledgments
I thank Yoshimasa Tsuruoka and Takuya Mat-
suzaki for their advice on probabilistic modeling,
Alex Fang for his help in using the acquired SCFs,
and Anna Korhonen for her insightful suggestions
on evaluation. I am also grateful to Jun?ichi Tsujii,
Yusuke Miyao, John Carroll and the anonymous
reviewers for their valuable comments. This work
was supported in part by JSPS Research Fellow-
ships for Young Scientists and in part by CREST,
JST (Japan Science and Technology Agency).
References
B. Boguraev and T. Briscoe. 1987. Large lexicons for natural
language processing: utilising the grammar coding system
of LDOCE. Computational Linguistics, 13(4):203?218.
T. Briscoe and J. Carroll. 1997. Automatic extraction of
subcategorization from corpora. In Proc. the fifth ANLP,
pages 356?363.
J. Carroll and A. C. Fang. 2004. The automatic acquisition
of verb subcategorizations and their impact on the perfor-
mance of an HPSG parser. In Proc. the first ijc-NLP, pages
107?114.
A. Copestake. 2002. Implementing typed feature structure
grammars. CSLI publications.
E. W. Forgy. 1965. Cluster analysis of multivariate data: Ef-
ficiency vs. interpretability of classifications. Biometrics,
21:768?780.
A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, editors.
1995. Bayesian Data Analysis. Chapman and Hall.
R. Grishman, C. Macleod, and A. Meyers. 1994. Comlex
syntax: Building a computational lexicon. In Proc. the
15th COLING, pages 268?272.
G. Hamerly. 2003. Learning structure and concepts in data
through data clustering. Ph.D. thesis, University of Cali-
fornia, San Diego.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003. Clus-
tering polysemic subcategorization frame distributions se-
mantically. In Proc. the 41st ACL, pages 64?71.
A. Korhonen. 2002. Subcategorization Acquisition. Ph.D.
thesis, University of Cambridge.
B. Levin. 1993. English Verb Classes and Alternations.
Chicago University Press.
A. Sarkar, F. Xia, and A. K. Joshi. 2000. Some experiments
on indicators of parsing complexity for lexicalized gram-
mars. In Proc. the 18th COLING workshop, pages 37?42.
S. Schulte im Walde and C. Brew. 2002. Inducing German
semantic verb classes from purely syntactic subcategorisa-
tion information. In Proc. the 41st ACL, pages 223?230.
Y. Tsuruoka and T. Chikayama. 2001. Estimating reliability
of contextual evidences in decision-list classifiers under
Bayesian learning. In Proc. the sixth NLPRS, pages 701?
707.
XTAG Research Group. 2001. A Lexicalized Tree Adjoin-
ing Grammar for English. Technical Report IRCS-01-03,
IRCS, University of Pennsylvania.
Resource sharing among HPSG and LTAG communities
by a method of grammar conversion from FB-LTAG to HPSG
Naoki Yoshinaga Yusuke Miyao
Department of Information Science, Graduate school of Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
fyoshinag, yusukeg@is.s.u-tokyo.ac.jp
Kentaro Torisawa
School of Information Science, Japan Advanced Institute of Science and Technology
Asahidai 1-1, Tatsunokuchi-cho, Noumi-gun, Ishikawa, 923-1292, Japan
Information and Human Behavior, PRESTO, Japan Science and Technology Corporation
Kawaguchi Hon-cho 4-1-8, Kawaguchi-shi, Saitama, 332-0012, Japan
torisawa@jaist.ac.jp
Jun?ichi Tsujii
Department of Computer Science, Graduate school of Information Science and Technology, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
CREST, JST (Japan Science and Technology Corporation)
Kawaguchi Hon-cho 4-1-8, Kawaguchi-shi, Saitama, 332-0012, Japan
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper describes the RenTAL sys-
tem, which enables sharing resources
in LTAG and HPSG formalisms by a
method of grammar conversion from
an FB-LTAG grammar to a strongly
equivalent HPSG-style grammar. The
system is applied to the latest version
of the XTAG English grammar. Ex-
perimental results show that the ob-
tained HPSG-style grammar success-
fully worked with an HPSG parser, and
achieved a drastic speed-up against an
LTAG parser. This system enables to
share not only grammars and lexicons
but also parsing techniques.
1 Introduction
This paper describes an approach for shar-
ing resources in various grammar formalisms
such as Feature-Based Lexicalized Tree Adjoin-
ing Grammar (FB-LTAG1) (Vijay-Shanker, 1987;
Vijay-Shanker and Joshi, 1988) and Head-Driven
Phrase Structure Grammar (HPSG) (Pollard and
Sag, 1994) by a method of grammar conver-
sion. The RenTAL system automatically converts
an FB-LTAG grammar into a strongly equiva-
lent HPSG-style grammar (Yoshinaga and Miyao,
2001). Strong equivalence means that both gram-
mars generate exactly equivalent parse results,
and that we can share the LTAG grammars and
lexicons in HPSG applications. Our system can
reduce considerable workload to develop a huge
resource (grammars and lexicons) from scratch.
Our concern is, however, not limited to the
sharing of grammars and lexicons. Strongly
equivalent grammars enable the sharing of
ideas developed in each formalism. There
have been many studies on parsing tech-
niques (Poller and Becker, 1998; Flickinger et
al., 2000), ones on disambiguation models (Chi-
ang, 2000; Kanayama et al, 2000), and ones
on programming/grammar-development environ-
1In this paper, we use the term LTAG to refer to FB-
LTAG, if not confusing.
LTAG Resources
Grammar: 
Elementary tree 
templates
Lexicon
Type hierarchy 
extractor
Tree 
converter
Lexicon 
converter
RenTAL System
HPSG Resources
Grammar: 
Lexical entry 
templates
Lexicon
LTAG parsers HPSG parsers
Derivation trees Parse trees
Derivation 
translator
LTAG-based application
HPSG-based application
Figure 1: The RenTAL System: Overview
ment (Sarkar and Wintner, 1999; Doran et al,
2000; Makino et al, 1998). These works are re-
stricted to each closed community, and the rela-
tion between them is not well discussed. Investi-
gating the relation will be apparently valuable for
both communities.
In this paper, we show that the strongly equiv-
alent grammars enable the sharing of ?parsing
techniques?, which are dependent on each com-
putational framework and have never been shared
among HPSG and LTAG communities. We ap-
ply our system to the latest version of the XTAG
English grammar (The XTAG Research Group,
2001), which is a large-scale FB-LTAG gram-
mar. A parsing experiment shows that an efficient
HPSG parser with the obtained grammar achieved
a significant speed-up against an existing LTAG
parser (Yoshinaga et al, 2001). This result im-
plies that parsing techniques for HPSG are also
beneficial for LTAG parsing. We can say that the
grammar conversion enables us to share HPSG
parsing techniques in LTAG parsing.
Figure 1 depicts a brief sketch of the RenTAL
system. The system consists of the following four
modules: Tree converter, Type hierarchy extrac-
tor, Lexicon converter and Derivation translator.
The tree converter module is a core module of the
system, which is an implementation of the gram-
mar conversion algorithm given in Section 3. The
type hierarchy extractor module extracts the sym-
bols of the node, features, and feature values from
the LTAG elementary tree templates and lexicon,
and construct the type hierarchy from them. The
lexicon converter module converts LTAG elemen-
tary tree templates into HPSG lexical entries. The
derivation translator module takes HPSG parse
S
NP VP
V
run
VP
VP
V
can
*
NP
N
We
?1
?2
?1
anchor
foot node
*
substitution node
Initial tree
Auxiliary tree
Figure 2: Elementary trees
trees, and map them to LTAG derivation trees. All
modules other than the last one are related to the
conversion process from LTAG into HPSG, and
the last one enables to obtain LTAG analysis from
the obtained HPSG analysis.
Tateisi et al also translated LTAG into
HPSG (Tateisi et al, 1998). However, their
method depended on translator?s intuitive analy-
sis of the original grammar. Thus the transla-
tion was manual and grammar dependent. The
manual translation demanded considerable efforts
from the translator, and obscures the equiva-
lence between the original and obtained gram-
mars. Other works (Kasper et al, 1995; Becker
and Lopez, 2000) convert HPSG grammars into
LTAG grammars. However, given the greater ex-
pressive power of HPSG, it is impossible to con-
vert an arbitrary HPSG grammar into an LTAG
grammar. Therefore, a conversion from HPSG
into LTAG often requires some restrictions on the
HPSG grammar to suppress its generative capac-
ity. Thus, the conversion loses the equivalence of
the grammars, and we cannot gain the above ad-
vantages.
Section 2 reviews the source and the tar-
get grammar formalisms of the conversion algo-
rithm. Section 3 describes the conversion algo-
rithm which the core module in the RenTAL sys-
tem uses. Section 4 presents the evaluation of
the RenTAL system through experiments with the
XTAG English grammar. Section 5 concludes this
study and addresses future works.
2 Background
2.1 Feature-Based Lexicalized Tree
Adjoining Grammar (FB-LTAG)
LTAG (Schabes et al, 1988) is a grammar formal-
ism that provides syntactic analyses for a sentence
by composing elementary trees with two opera-
Arg :
we
can run
ID grammar rule
unify
Sym : NP
Arg : 
Sym : VP
Arg :   VP
Sym : VP
Arg :   NP
Arg :
Sym :
Arg :
2
3
2
unify
3
unify
ID grammar rule
we
can run
Sym : NP
Arg :
Sym : VP
Arg :   VP
Sym : VP
Arg :   NP
Arg :   NP
Sym : 
Arg :
Arg :
1
1
|
2
Arg :
2
unify
we can run
Sym : NP
Arg :
Sym : VP
Arg :   VP
Sym : VP
Arg :   NP
Arg :   NP
Arg :
Figure 6: Parsing with an HPSG grammar
S
NP
VP
V
run
NP
N
We
substitution
?1
?2
S
NP VP
V
run
N
We
Figure 3: Substitution
VP
VP
V
can
*
adjunction
?1
S
NP VP
V
run
N
We
S
NP
VP
VP
V
can
N
We
V
run
Figure 4: Adjunction
tions called substitution and adjunction. Elemen-
tary trees are classified into two types, initial trees
and auxiliary trees (Figure 2). An elementary tree
has at least one leaf node labeled with a terminal
symbol called an anchor (marked with ). In an
auxiliary tree, one leaf node is labeled with the
same symbol as the root node and is specially
marked as a foot node (marked with ). In an el-
ementary tree, leaf nodes with the exception of
anchors and the foot node are called substitution
nodes (marked with #).
Substitution replaces a substitution node with
another initial tree (Figure 3). Adjunction grafts
an auxiliary tree with the root node and foot
node labeled x onto an internal node of another
tree with the same symbol x (Figure 4). FB-
LTAG (Vijay-Shanker, 1987; Vijay-Shanker and
Joshi, 1988) is an extension of the LTAG formal-
ism. In FB-LTAG, each node in the elementary
trees has a feature structure, containing grammat-
ical constraints on the node. Figure 5 shows a
result of LTAG analysis, which is described not
derived tree
?2
?1?1
derivation tree
S
NP VP
VP
V
can
N
We
V
run
Figure 5: Derived trees and derivation trees
only by derived trees (i.e., parse trees) but also by
derivation trees. A derivation tree is a structural
description in LTAG and represents the history of
combinations of elementary trees.
There are several grammars developed in the
FB-LTAG formalism, including the XTAG En-
glish grammar, a large-scale grammar for En-
glish (The XTAG Research Group, 2001). The
XTAG group (Doran et al, 2000) at the Univer-
sity of Pennsylvania is also developing Korean,
Chinese, and Hindi grammars. Development of
a large-scale French grammar (Abeille? and Can-
dito, 2000) has also started at the University of
Pennsylvania and is expanded at University of
Paris 7.
2.2 Head-Driven Phrase Structure
Grammar (HPSG)
An HPSG grammar consists of lexical entries and
ID grammar rules, each of which is described
with typed feature structures (Carpenter, 1992). A
lexical entry for each word expresses the charac-
teristics of the word, such as the subcategorization
frame and the grammatical category. An ID gram-
mar rule represents a relation between a mother
and its daughters, and is independent of lexical
characteristics. Figure 6 illustrates an example of
bottom-up parsing with an HPSG grammar. First,
lexical entries for ?can? and ?run? are unified re-
spectively with the daughter feature structures of
Canonical elementary trees Non-canonical elementary trees
think
S
NP VP
V S
*
it
S
NP VP
N
V
VP
V
?
is
Non-anchored subtree
S
NP VP
V PP
P NP
for
look
PP S
P NP
a) Exception for Condition 1
b) Exception for Condition 2
Figure 7: A canonical elementary tree and exceptions
an ID grammar rule. The feature structure of the
mother node is determined as a result of these uni-
fications. The center of Figure 6 shows a rule ap-
plication to ?can run? and ?we?.
There are a variety of works on efficient pars-
ing with HPSG, which allow the use of HPSG-
based processing in practical application con-
texts (Flickinger et al, 2000). Stanford Univer-
sity is developing the English Resource Gram-
mar, an HPSG grammar for English, as a part
of the Linguistic Grammars Online (LinGO)
project (Flickinger, 2000). In practical con-
text, German, English, and Japanese HPSG-based
grammars are developed and used in the Verb-
mobil project (Kay et al, 1994). Our group
has developed a wide-coverage HPSG grammar
for Japanese (Mitsuishi et al, 1998), which is
used in a high-accuracy Japanese dependency an-
alyzer (Kanayama et al, 2000).
3 Grammar conversion
The grammar conversion from LTAG to
HPSG (Yoshinaga and Miyao, 2001) is the
core portion of the RenTAL system. The
conversion algorithm consists of:
1. Conversion of canonical elementary trees to
HPSG lexical entries.
2. Definition of ID grammar rules to emulate
substitution and adjunction.
3. Conversion of non-canonical elementary
trees to canonical ones.
The left-hand side of Figure 7 shows a canoni-
cal elementary tree, which satisfies the following
conditions:
Condition 1 A tree must have only one anchor.
Sym:
Arg:
Sym  :
Leaf :
Dir    :
right left
,
Foot?:
+
_
*
think
V S
VP
S
NP
V
think:
S
VP S
NP
foot node
anchor
trunk
*
substitution node
Sym  :
Leaf :
Dir    :
Foot?:
Figure 8: A conversion from a canonical elemen-
tary tree into an HPSG lexical entry
mother

Sym : 1
Arg : 2






h
Sym : 3
Arg : h i
i
substitution node
X
X
X
X
X
2
4
Arg :
*
2
4
Sym : 1
Leaf : 3
Dir : left
Foot? :  
3
5
j 2
+
3
5
trunk node
Figure 9: Left substitution rule
Condition 2 All branchings in a tree must con-
tain trunk nodes.
Trunk nodes are nodes on a trunk, which is a path
from an anchor to the root node (the thick lines in
Figure 7) (Kasper et al, 1995). Condition 1 guar-
antees that a canonical elementary tree has only
one trunk, and Condition 2 guarantees that each
branching consists of a trunk node, a leaf node,
and their mother (also a trunk node). The right-
hand side of Figure 7 shows elementary trees vi-
olating the conditions.
Canonical elementary trees can be directly con-
verted to HPSG lexical entries by regarding each
leaf node as a subcategorization element of the
anchor, and by encoding them into a list. Fig-
ure 8 shows an example of the conversion. By
following the trunk from the anchor ?think? to the
mother

Sym : 1
Arg : 2  3







Sym : 4
Arg : 3

foot node
P
P
P
P
P
2
4
Arg :
*
2
4
Sym : 1
Leaf : 4
Dir : left
Foot? : +
3
5
j 2
+
3
5
trunk node
 append
Figure 10: Left adjunction rule
root node labeled S, we store each branching in
a list. As shown in Figure 8, each branching is
specified by a leaf node and the mother node. A
feature Sym represents the non-terminal symbol
of the mother node. Features Leaf, Dir, Foot?
represent the leaf node; the non-terminal symbol,
the direction (on which side of the trunk node the
leaf node is), and the type (whether a foot node or
a substitution node), respectively.
Figures 9 and 10 show ID grammar rules to em-
ulate substitution and adjunction. These grammar
rules are independent of the original grammar be-
cause they don?t specify any characteristics spe-
cific to the original grammar.
In the substitution rule, the Sym feature of the
substitution node must have the value of the Leaf
feature 3 of the trunk node. The Arg feature of
the substitution node must be a null list, because
the substitution node must be unified only with
the node corresponding to the root node of the ini-
tial tree. The substitution rule percolates the tail
elements 2 of the Arg feature of a trunk node to
the mother in order to continue constructing the
tree.
In the adjunction rule, the Sym feature of a
foot node must have the same value as the Leaf
feature 4 . The value of the Arg feature of the
mother node is a concatenation list of both Arg
features 2 and 3 of its daughters because we
first construct the tree corresponding to the ad-
joining tree and next continue constructing the
tree corresponding to the adjoined tree. The value
?+? or ? ? of the Foot? feature explicitly de-
termines whether the next rule application is the
adjunction rule or the substitution rule.
Figure 11 shows an instance of rule applica-
tions. The thick line indicates the adjoined tree
(1) and the dashed line indicates the adjoining
Sym : NP
Arg : 
Sym : S
Arg : 
Sym : S
?1
2
1
5
3
Sym :        S
Leaf :        NP
Dir :  left
Foot? :  
2
1
Sym :        VP
Leaf :        S 
Dir :  right
Foot? :  +
Sym : NP
Arg : 
Sym : NP
Arg : 
Sym : V
Sym : S
Sym : VP
Sym : V
think:
loves:
you
? A
*
? B
4
4
7
7
8
6
Sym :        S
Leaf :        NP 
Dir :  left
Foot? :  
5
Sym :        S
Leaf :        NP 
Dir :  left
Foot? :  
2
1
5
Sym :        S
Leaf :        NP
Dir :  left
Foot? :  
2
1
3
6
8
Sym :        S
Leaf :        NP
Dir :  left
Foot? :  
3
6
Sym :        S
Leaf :        NP
Dir :  left
Foot? : 
,
5
Sym :        S
Leaf :        NP 
Dir :  left
Foot? :  
2
1
,
4
9
9
?1
he
?2
?4
?3
Arg :
Arg :
Arg : Arg :
Arg :
what
? C
Figure 11: An example of rule applications
S
NP
VP
V PP
P NP
for
S
NP VP
V
P NP
for
look look
cut off
PP
look_for
PP
look_for
identifier
Figure 12: Division of a multi-anchored elemen-
tary tree into single-anchored trees
tree (2). The adjunction rule is applied to con-
struct the branching marked with ?, where ?think?
takes as an argument a node whose Sym feature?s
value is S. By applying the adjunction rule, the
Arg feature of the mother node (B) becomes a
concatenation list of both Arg features of 1 ( 8 )
and 1 ( 5 ). Note that when the construction of
1 is completed, the Arg feature of the trunk node
(C) will be its former state (A). We can continue
constructing 1 as if nothing had happened.
Multi-anchored elementary trees, which violate
Condition 1, are divided into multiple canonical
elementary trees. We call the cutting nodes in the
divided trees cut-off nodes (Figure 12). Note that
a cut-off node is marked by an identifier to pre-
serve a co-occurrence relation among the multiple
anchors. Figure 12 shows an example of the con-
version of a multi-anchored elementary tree for a
compound expression ?look for?. We first select
an anchor ?look? as the syntactic head, and tra-
verse the tree along the trunk from the root node
S to the anchor ?look?. We then cut off the multi-
PAd
P
P
substitution
all candidate initial trees 
for substitution
, ?
non-anchored subtree
multi-anchored trees without non-anchored subtrees
it
S
NP VP
N
V
is
VP
V
?
PP S
P NP
breaking points
on
tonext
it
S
NP VP
N
V
is
VP
V
?
PP S
P NP
it
S
NP VP
N
V
is
VP
V
?
PP S
P NP
, ?
Ad
P
on
tonext
Figure 13: Combination of a non-anchored subtree into anchored trees
anchored elementary tree at the node PP, and cut-
off nodes PP in resulting single-anchored trees are
marked by an identifier look for.
Non-canonical elementary trees violating Con-
dition 2 have a non-anchored subtree which is
a subtree of depth 1 or above with no anchor.
A non-anchored subtree is converted into multi-
anchored trees by substituting the deepest node
(Figure 13). Substituted nodes are marked as
breaking points to remember that the nodes orig-
inate from the substitution nodes. In the resulting
trees, all subtrees are anchored so that we can ap-
ply the above conversion algorithms. Figure 13
shows a conversion of a non-canonical elemen-
tary tree for it-cleft. A substitution node P in the
non-anchored subtree is selected, and is substi-
tuted by each initial tree. The substituted node
P in resulting multi-anchored trees are marked as
breaking points.
The above algorithm gives the conversion of
LTAG, and it can be easily extended to handle an
FB-LTAG grammar by merely storing a feature
structure of each node into the Sym feature and
Leaf feature together with the non-terminal sym-
bol. Feature structure unification is executed by
ID grammar rules.
The strong equivalence is assured because only
substitution/adjunction operations performed in
LTAG are performed with the obtained HPSG-
style grammar. This is because each element
in the Arg feature selects only feature structures
corresponding to trees which can substitute/be
adjoined by each leaf node of an elementary
tree. By following a history of rule applications,
each combination of elementary trees in LTAG
derivation trees can be readily recovered. The
strong equivalence holds also for conversion of
non-canonical elementary trees. For trees violat-
ing Condition 1, we can distinguish the cut-off
Table 1: The classification of elementary tree
templates in the XTAG English grammar (LTAG)
and converted lexical entry templates correspond-
ing to them (HPSG): A: canonical elementary
trees, B: elementary trees violating only Condi-
tion 1, C: elementary trees violating only Condi-
tion 2, D: elementary trees violating both condi-
tions
Grammar A B C D Total
LTAG 326 764 54 50 1,194
HPSG 326 1,992 1,083 2,474 5,875
nodes from the substitution nodes owing to iden-
tifiers, which recover the co-occurrence relation
in the original elementary trees between the di-
vided trees. For trees violating Condition 2, we
can identify substitution nodes in a combined tree
because they are marked as breaking points, and
we can consider the combined tree as two trees in
the LTAG derivation.
4 Experiments
The RenTAL system is implemented in LiL-
FeS (Makino et al, 1998)2. LiLFeS is one of
the fastest inference engines for processing fea-
ture structure logic, and efficient HPSG parsers
have already been built on this system (Nishida
et al, 1999; Torisawa et al, 2000). We ap-
plied our system to the XTAG English gram-
mar (The XTAG Research Group, 2001)3, which
is a large-scale FB-LTAG grammar for English.
2The RenTAL system is available at:
http://www-tsujii.is.s.u-tokyo.ac.jp/rental/
3We used the grammar attached to the latest distribution
of an LTAG parser which we used for the parsing experi-
ment. The parser is available at:
ftp://ftp.cis.upenn.edu/pub/xtag/lem/lem-0.13.0.i686.tgz
Table 2: Parsing performance with the XTAG En-
glish grammar for the ATIS corpus.
Parser Parse Time (sec.)
lem 19.64
TNT 0.77
The XTAG English grammar consists of 1,194 4
elementary tree templates and around 45,000 lex-
ical items5. We successfully converted all the
elementary tree templates in the XTAG English
grammar to HPSG lexical entry templates. Ta-
ble 1 shows the classifications of elementary tree
templates of the XTAG English grammar, ac-
cording to the conditions we introduced in Sec-
tion 3, and also shows the number of correspond-
ing HPSG lexical entry templates. Conversion
took about 25 minutes CPU time on a 700 Mhz
Pentium III Xeon with four gigabytes main mem-
ory.
The original and the obtained grammar gener-
ated exactly the same number of derivation trees
in the parsing experiment with 457 sentences
from the ATIS corpus (Marcus et al, 1994)6 (the
average length is 6.32 words). This result empir-
ically attested the strong equivalence of our algo-
rithm.
Table 2 shows the average parsing time with
the LTAG and HPSG parsers. In Table 2, lem
refers to the LTAG parser (Sarkar et al, 2000),
ANSI C implementation of the two-phase pars-
ing algorithm that performs the head corner pars-
ing (van Noord, 1994) without features (phase
1), and then executes feature unification (phase
2). TNT refers to the HPSG parser (Torisawa et
al., 2000), C++ implementation of the two-phase
parsing algorithm that performs filtering with a
compiled CFG (phase 1) and then executes fea-
ture unification (phase 2). Table 2 clearly shows
that the HPSG parser is significantly faster than
the LTAG parser. This result implies that parsing
techniques for HPSG are also beneficial for LTAG
4We eliminated 32 elementary trees because the LTAG
parser cannot produce correct derivation trees with them.
5These lexical items are a subset of the original XTAG
English grammar distribution.
6We eliminated 59 sentences because of a time-out of
the parsers, and 61 sentences because the LTAG parser does
not produce correct derivation trees because of bugs in its
preprocessor.
parsing. We can say that the grammar conversion
enables us to share HPSG parsing techniques in
LTAG parsing. Another paper (Yoshinaga et al,
2001) describes the detailed analysis on the factor
of the difference of parsing performance.
5 Conclusion
We described the RenTAL system, a grammar
converter from FB-LTAG to HPSG. The grammar
conversion guarantees the strong equivalence, and
hence we can obtain an HPSG-style grammar
equivalent to existing LTAG grammars. Experi-
mental result showed that the system enabled to
share not only LTAG grammars, but also HPSG
parsing techniques. This system will enable a
variety of resource sharing such as the sharing
of the programming/grammar-development envi-
ronment (Makino et al, 1998; Sarkar and Wint-
ner, 1999) and grammar extraction methods from
bracketed corpora (Xia, 1999; Chen and Vijay-
Shanker, 2000; Neumann, 1998). Although our
system connects only FB-LTAG and HPSG, we
believe that our approach can be extended to other
formalisms such as Lexical-Functional Gram-
mar (Kaplan and Bresnan, 1982).
Acknowledgment The authors are indebted
to Mr. Anoop Sarkar for his help in using his
parser in our experiment. The authors would like
to thank anonymous reviewers for their valuable
comments and criticisms on this paper.
References
Anne Abeille? and Marie-He?le`ne Candito. 2000.
FTAG: A Lexicalized Tree Adjoining Grammar for
French. In Anne Abeille? and Owen Rambow, edi-
tors, Tree Adjoining Grammars: Formal, Computa-
tional and Linguistic Aspects, pages 305?329. CSLI
publications.
Tilman Becker and Patrice Lopez. 2000. Adapting
HPSG-to-TAG compilation to wide-coverage gram-
mars. In Proc. of TAG+5, pages 47?54.
Bob Carpenter. 1992. The Logic of Typed Feature
Structures. Cambridge University Press.
John Chen and K. Vijay-Shanker. 2000. Automated
extraction of TAGs from the Penn Treebank. In
Proc. of IWPT 2000.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar.
In Proc. of ACL 2000, pages 456?463.
Christy Doran, Beth Ann Hockey, Anoop Sarkar,
B. Srinivas, and Fei Xia. 2000. Evolution of the
XTAG system. In Anne Abeille? and Owen Ram-
bow, editors, Tree Adjoining Grammars: Formal,
Computational and Linguistic Aspects, pages 371?
403. CSLI publications.
Dan Flickinger, Stephen Oepen, Jun?ichi Tsujii, and
Hans Uszkoreit, editors. 2000. Natural Language
Engineering ? Special Issue on Efficient Processing
with HPSG: Methods, Systems, Evaluation. Cam-
bridge University Press.
Dan Flickinger. 2000. On building a more effi-
cient grammar by exploiting types. Natural Lan-
guage Engineering ? Special Issue on Efficient Pro-
cessing with HPSG: Methods, Systems, Evaluation,
6(1):15?28.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsu-
isi, and Jun?ichi Tsujii. 2000. Hybrid Japanese
parser with hand-crafted grammar and statistics. In
Proc. of COLING 2000, pages 411?417.
Ronald Kaplan and Joan Bresnan. 1982. Lexical-
Functional Grammar: A formal system for gram-
matical representation. In Joan Bresnan, editor, The
Mental Representation of Grammatical Relations,
pages 173?281. The MIT Press.
Robert Kasper, Bernd Kiefer, Klaus Netter, and
K. Vijay-Shanker. 1995. Compilation of HPSG to
TAG. In Proc. of ACL ?94, pages 92?99.
M. Kay, J. Gawron, and P. Norvig. 1994. Verbmo-
bil: A Translation System for Face-to-Face Dialog.
CSLI Publications.
Takaki Makino, Minoru Yoshida, Kentaro Torisawa,
and Jun?ichi Tsujii. 1998. LiLFeS ? towards a
practical HPSG parsers. In Proc. of COLING?ACL
?98, pages 807?811.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Yutaka Mitsuishi, Kentaro Torisawa, and Jun?ichi Tsu-
jii. 1998. HPSG-style underspecified Japanese
grammar with wide coverage. In Proc. of
COLING?ACL ?98, pages 876?880.
Gu?ter Neumann. 1998. Automatic extraction of
stochastic lexcalized tree grammars from treebanks.
In Proc. of TAG+4, pages 120?123.
Kenji Nishida, Kentaro Torisawa, and Jun?ichi Tsujii.
1999. An efficient HPSG parsing algorithm with ar-
ray unification. In Proc. of NLPRS ?99, pages 144?
149.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press and CSLI Publications.
Peter Poller and Tilman Becker. 1998. Two-step TAG
parsing revisited. In Proc. of TAG+4, pages 143?
146.
Anoop Sarkar and Shuly Wintner. 1999. Typing as a
means for validating feature structures. In Proc.of
CLIN ?99, pages 159?167.
Anoop Sarkar, Fei Xia, and Aravind Joshi. 2000.
Some experiments on indicators of parsing com-
plexity for lexicalized grammars. In Proc. of COL-
ING 2000, pages 37?42.
Yves Schabes, Anne Abeille, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized? gram-
mars: Application to Tree Adjoining Grammars. In
Proc. of 12th COLING ?92, pages 578?583.
Yuka Tateisi, Kentaro Torisawa, Yusuke Miyao, and
Jun?ichi Tsujii. 1998. Translating the XTAG En-
glish grammar to HPSG. In Proc. of TAG+4, pages
172?175.
The XTAG Research Group. 2001. A Lex-
icalized Tree Adjoining Grammar for English.
http://www.cis.upenn.edu/?xtag/.
Kentaro Torisawa, Kenji Nishida, Yusuke Miyao, and
Jun?ichi Tsujii. 2000. An HPSG parser with CFG
filtering. Natural Language Engineering ? Special
Issue on Efficient Processing with HPSG: Methods,
Systems, Evaluation, 6(1):63?80.
Gertjan van Noord. 1994. Head corner parsing for
TAG. Computational Intelligence, 10(4):525?534.
K. Vijay-Shanker and Aravind K. Joshi. 1988. Fea-
ture structures based Tree Adjoining Grammars. In
Proc. of 12th COLING ?92, pages 714?719.
K. Vijay-Shanker. 1987. A Study of Tree Adjoining
Grammars. Ph.D. thesis, Department of Computer
& Information Science, University of Pennsylvania.
Fei Xia. 1999. Extracting Tree Adjoining Grammars
from bracketed corpora. In Proc. of NLPRS ?99,
pages 398?403.
Naoki Yoshinaga and Yusuke Miyao. 2001. Grammar
conversion from FB-LTAG to HPSG. In Proc. of
ESSLLI 2001 Student Session. To appear.
Naoki Yoshinaga, Yusuke Miyao, Kentaro Torisawa,
and Jun?ichi Tsujii. 2001. Efficient LTAG parsing
using HPSG parsers. In Proc. of PACLING 2001.
To appear.
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1245?1253,
Beijing, August 2010
Kernel Slicing: Scalable Online Training with Conjunctive Features
Naoki Yoshinaga
Institute of Industrial Science,
the University of Tokyo
ynaga@tkl.iis.u-tokyo.ac.jp
Masaru Kitsuregawa
Institute of Industrial Science,
the University of Tokyo
kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
This paper proposes an efficient online
method that trains a classifier with many
conjunctive features. We employ kernel
computation called kernel slicing, which
explicitly considers conjunctions among
frequent features in computing the poly-
nomial kernel, to combine the merits of
linear and kernel-based training. To im-
prove the scalability of this training, we
reuse the temporal margins of partial fea-
ture vectors and terminate unnecessary
margin computations. Experiments on de-
pendency parsing and hyponymy-relation
extraction demonstrated that our method
could train a classifier orders of magni-
tude faster than kernel-based online learn-
ing, while retaining its space efficiency.
1 Introduction
The past twenty years have witnessed a growing
use of machine-learning classifiers in the field of
NLP. Since the classification target of complex
NLP tasks (e.g., dependency parsing and relation
extraction) consists of more than one constituent
(e.g., a head and a dependent in dependency pars-
ing), we need to consider conjunctive features,
i.e., conjunctions of primitive features that fo-
cus on the particular clues of each constituent, to
achieve a high degree of accuracy in those tasks.
Training with conjunctive features involves a
space-time trade-off in the way conjunctive fea-
tures are handled. Linear models, such as log-
linear models, explicitly estimate the weights of
conjunctive features, and training thus requires a
great deal of memory when we take higher-order
conjunctive features into consideration. Kernel-
based models such as support vector machines, on
the other hand, ensure space efficiency by using
the kernel trick to implicitly consider conjunctive
features. However, training takes quadratic time
in the number of examples, even with online algo-
rithms such as the (kernel) perceptron (Freund and
Schapire, 1999), and we cannot fully exploit am-
ple ?labeled? data obtained with semi-supervised
algorithms (Ando and Zhang, 2005; Bellare et al,
2007; Liang et al, 2008; Daume? III, 2008).
We aim at resolving this dilemma in train-
ing with conjunctive features, and propose online
learning that combines the time efficiency of lin-
ear training and the space efficiency of kernel-
based training. Following the work by Goldberg
and Elhadad (2008), we explicitly take conjunc-
tive features into account that frequently appear in
the training data, and implicitly consider the other
conjunctive features by using the polynomial ker-
nel. We then improve the scalability of this train-
ing by a method called kernel slicing, which al-
lows us to reuse the temporal margins of partial
feature vectors and to terminate computations that
do not contribute to parameter updates.
We evaluate our method in two NLP tasks: de-
pendency parsing and hyponymy-relation extrac-
tion. We demonstrate that our method is orders of
magnitude faster than kernel-based online learn-
ing while retaining its space efficiency.
The remainder of this paper is organized as fol-
lows. Section 2 introduces preliminaries and no-
tations. Section 3 proposes our training method.
Section 4 evaluates the proposed method. Sec-
tion 5 discusses related studies. Section 6 con-
cludes this paper and addresses future work.
1245
Algorithm 1 BASE LEARNER: KERNEL PA-I
INPUT: T = {(x, y)t}|T |t=1, k : Rn ? Rn 7? R, C ? R+OUTPUT: (S|T |,?|T |)
1: initialize: S0 ? ?, ?0 ? ?
2: for t = 1 to |T | do
3: receive example (x, y)t : x ? Rn, y ? {?1,+1}
4: compute margin: mt(x) =
?
si?St?1
?ik(si,x)
5: if `t = max {0, 1? ymt(x)} > 0 then
6: ?t ? min
?
C, `t?x?2
ff
7: ?t ? ?t?1 ? {?ty}, St ? St?1 ? {x}
8: else
9: ?t ? ?t?1, St ? St?1
10: end if
11: end for
12: return (S|T |,?|T |)
2 Preliminaries
This section first introduces a passive-aggressive
algorithm (Crammer et al, 2006), which we use
as a base learner. We then explain fast methods of
computing the polynomial kernel.
Each example x in a classification problem is
represented by a feature vector whose element xj
is a value of a feature function, fj ? F . Here, we
assume a binary feature function, fj(x) ? {0, 1},
which returns one if particular context data appear
in the example. We say that feature fj is active in
example x when xj = fj(x) = 1. We denote a
binary feature vector, x, as a set of active features
x = {fj | fj ? F , fj(x) = 1} for brevity; fj ? x
means that fj is active in x, and |x| represents the
number of active features in x.
2.1 Kernel Passive-Aggressive Algorithm
A passive-aggressive algorithm (PA) (Crammer et
al., 2006) represents online learning that updates
parameters for given labeled example (x, y)t ?
T in each round t. We assume a binary label,
y ? {?1,+1}, here for clarity. Algorithm 1
is a variant of PA (PA-I) that incorporates a ker-
nel function, k. In round t, PA-I first computes
a (signed) margin mt(x) of x by using the ker-
nel function with support set St?1 and coefficients
?t?1 (Line 4). PA-I then suffers a hinge-loss,
`t = max {0, 1? ymt(x)} (Line 5). If `t > 0,
PA-I adds x to St?1 (Line 7). Hyperparameter C
controls the aggressiveness of parameter updates.
The kernel function computes a dot product in
RH space without mapping x ? Rn to ?(x) ?
RH (k(x,x?) = ?(x)T?(x?)). We can implic-
itly consider (weighted) d or less order conjunc-
tions of primitive features by using polynomial
kernel function kd(s,x) = (sTx + 1)d. For ex-
ample, given support vector s = (s1, s2)T and
input example x = (x1, x2)T, the second-order
polynomial kernel returns k2(s,x) = (s1x1 +
s2x2 +1)2 = 1+3s1x1 +3s2x2 +2s1x1s2x2 (?
si, xi ? {0, 1}). This function thus implies map-
ping ?2(x) = (1,
?
3x1,
?
3x2,
?
2x1x2)T.
Although online learning is generally efficient,
the kernel spoils its efficiency (Dekel et al, 2008).
This is because the kernel evaluation (Line 4)
takes O(|St?1||x|) time and |St?1| increases as
training continues. The learner thus takes the most
amount of time in this margin computation.
2.2 Kernel Computation for Classification
This section explains fast, exact methods of com-
puting the polynomial kernel, which are meant to
test the trained model, (S,?), and involve sub-
stantial computational cost in preparation.
2.2.1 Kernel Inverted
Kudo and Matsumoto (2003) proposed polyno-
mial kernel inverted (PKI), which builds inverted
indices h(fj) ? {s | s ? S, fj ? s} from each
feature fj to support vector s ? S to only con-
sider support vector s relevant to given x such
that sTx 6= 0. The time complexity of PKI is
O(B ? |x| + |S|) where B ? 1|x|
?
fj?x |h(fj)|,
which is smaller than O(|S||x|) if x has many
rare features fj such that |h(fj)|  |S|.
To the best of our knowledge, this is the only
exact method that has been used to speed up mar-
gin computation in the context of kernel-based on-
line learning (Okanohara and Tsujii, 2007).
2.2.2 Kernel Expansion
Isozaki and Kazawa (2002) and Kudo and Mat-
sumoto (2003) proposed kernel expansion, which
explicitly maps both support set S and given ex-
ample x ? Rn into RH by mapping ?d imposed
by kd:
m(x) =
(
?
si?S
?i?d(si)
)T
?d(x) =
?
fi?xd
wi,
1246
where xd ? {0, 1}H is a binary feature vector
in which xdi = 1 for (?d(x))i 6= 0, and w is a
weight vector in the expanded feature space, Fd.
The weight vector w is computed from S and ?:
w =
?
si?S
?i
d?
k=0
ckdIk(sdi ), (1)
where ckd is a squared coefficient of k-th order con-
junctive features for d-th order polynomial kernel
(e.g., c02 = 1, c12 = 3, and c22 = 2)1 and Ik(sdi ) is
sdi ? {0, 1}H whose dimensions other than those
of k-th order conjunctive features are set to zero.
The time complexity of kernel expansion is
O(|xd|) where |xd| = ?dk=0
(|x|
k
)
? |x|d, which
can be smaller than O(|S||x|) in usual NLP tasks
(|x|  |S| and d ? 4).
2.2.3 Kernel Splitting
Since kernel expansion demands a huge mem-
ory volume to store the weight vector, w, in RH
(H =?dk=0
(|F|
k
)), Goldberg and Elhadad (2008)
only explicitly considered conjunctions among
features fC ? FC that commonly appear in sup-
port set S, and handled the other conjunctive fea-
tures relevant to rare features fR ? F \ FC by
using the polynomial kernel:
m(x) = m(x?) + m(x)?m(x?)
=
?
fi?x?d
w?i +
?
si?SR
?ik?d(si,x, x?), (2)
where x? is x whose dimensions of rare features
are set to zero, w? is a weight vector computed
with Eq. 1 for FdC , and k?d(s,x, x?) is defined as:
k?d(s,x, x?) ? kd(s,x)? kd(s, x?).
We can space-efficiently compute the first term
of Eq. 2 since |w?|  |w|, while we can
quickly compute the second term of Eq. 2 since
k?d(si,x, x?) = 0 when sTi x = sTi x?; we only
need to consider a small subset of the support set,
SR =
?
fR?x\x? h(fR), that has at least one of therare features, fR, appearing in x\x? (|SR|  |S|).
Counting the number of features examined, the
time complexity of Eq. 2 is O(|x?d|+ |SR||x?|).
1Following Lemma 1 in Kudo and Matsumoto (2003),
ckd =
?d
l=k
`d
l
? `?k
m=0(?1)k?m ?ml
` k
m
??.
3 Algorithm
This section first describes the way kernel splitting
is integrated into PA-I (Section 3.1). We then pro-
pose kernel slicing (Section 3.2), which enables
us to reuse the temporal margins computed in the
past rounds (Section 3.2.1) and to skip unneces-
sary margin computations (Section 3.2.2).
In what follows, we use PA-I as a base learner.
Note that an analogous argument can be applied
to other perceptron-like online learners with the
additive weight update (Line 7 in Algorithm 1).
3.1 Base Learner with Kernel Splitting
A problem in integrating kernel splitting into the
base learner presented in Algorithm 1 is how to
determine FC , features among which we explic-
itly consider conjunctions, without knowing the
final support set, S|T |. We heuristically solve
this by ranking feature f according to their fre-
quency in the training data and by using the top-
N frequent features in the training data as FC
(= {f | f ? F , RANK(f) ? N}).2 Since S|T |
is a subset of the examples, this approximates the
selection from S|T |. We empirically demonstrate
the validity of this approach in the experiments.
We then useFC to construct a base learner with
kernel splitting; we replace the kernel computa-
tion (Line 4 in Algorithm 1) with Eq. 2 where
(S,?) = (St?1,?t?1). To compute mt(x?) by
using kernel expansion, we need to additionally
maintain the weight vector w? for the conjunctions
of common features that appear in St?1.
The additive parameter update of PA-I enables
us to keep w? to correspond to (St?1,?t?1).
When we add x to support set St?1 (Line 7 in
Algorithm 1), we also update w? with Eq. 1:
w? ? w? + ?ty
d?
k=0
ckdIk(x?d).
Following (Kudo and Matsumoto, 2003), we
use a trie (hereafter, weight trie) to maintain con-
junctive features. Each edge in the weight trie is
labeled with a primitive feature, while each path
2The overhead of counting features is negligible com-
pared to the total training time. If we want to run the learner
in a purely online manner, we can alternatively choose first
N features that appear in the processed examples as FC .
1247
represents a conjunctive feature that combines all
the primitive features on the path. The weights
of conjunctive features are retrieved by travers-
ing nodes in the trie. We carry out an analogous
traversal in updating the parameters of conjunc-
tive features, while registering a new conjunctive
feature by adding an edge to the trie.
The base learner with kernel splitting combines
the virtues of linear training and kernel-based
training. It reduces to linear training when we in-
crease N to |F|, while it reduces to kernel-based
training when we decrease N to 0. The output
is support set S|T | and coefficients ?|T | (option-
ally, w?), to which the efficient classification tech-
niques discussed in Section 2.2 and the one pro-
posed by Yoshinaga and Kitsuregawa (2009) can
be applied.
Note on weight trie construction The time and
space efficiency of this learner strongly depends
on the way the weight trie is constructed. We
need to address two practical issues that greatly
affect efficiency. First, we traverse the trie from
the rarest feature that constitutes a conjunctive
feature. This rare-to-frequent mining helps us to
avoid enumerating higher-order conjunctive fea-
tures that have not been registered in the trie, when
computing margin. Second, we use RANK(f)
encoded into a dlog128 RANK(f)e-byte string by
using variable-byte coding (Williams and Zobel,
1999) as f ?s representation in the trie. This en-
coding reduces the trie size, since features with
small RANK(f) will appear frequently in the trie.
3.2 Base Learner with Kernel Slicing
Although a base learner with kernel splitting can
enjoy the merits of linear and kernel-based train-
ing, it can simultaneously suffer from their demer-
its. Because the training takes polynomial time
in the number of common features in x (|x?d| =?d
k=0
(|x?|
k
)
? |x?|d) at each round, we need to set
N to a smaller value when we take higher-order
conjunctive features into consideration. However,
since the margin computation takes linear time in
the number of support vectors |SR| relevant to rare
features fR ? F\FC , we need to setN to a larger
value when we handle a larger number of training
examples. The training thereby slows down when
we train a classifier with high-order conjunctive
features and a large number of training examples.
We then attempt to improve the scalability of
the training by exploiting a characteristic of la-
beled data in NLP. Because examples in NLP tasks
are likely to be redundant (Yoshinaga and Kitsure-
gawa, 2009), the learner computes margins of ex-
amples that have many features in common. If we
can reuse the ?temporal? margins of partial feature
vectors computed in past rounds, this will speed
up the computation of margins.
We propose kernel slicing, which generalizes
kernel splitting in a purely feature-wise manner
and enables us to reuse the temporal partial mar-
gins. Starting from the most frequent feature f1 in
x (f1 = argminf?x RANK(f)), we incrementally
compute mt(x) by accumulating a partial mar-
gin, mjt (x) ? mt(xj)?mt(xj?1), when we add
the j-th frequent feature fj in x:
mt(x) = m0t +
|x|?
j=1
mjt (x), (3)
where m0t =
?
si?St?1 ?ikd(si,?) =
?
i ?i, and
xj has the j most frequent features in x (x0 = ?,
xj =
?j?1
k=0{argminf?x\xk RANK(f)}).
Partial margin mjt (x) can be computed by us-
ing the polynomial kernel:
mjt (x) =
?
si?St?1
?ik?d(si,xj ,xj?1), (4)
or by using kernel expansion:
mjt (x) =
?
fi?xdj \xdj?1
w?i. (5)
Kernel splitting is a special case of kernel slicing,
which uses Eq. 5 for fj ? FC and Eq. 4 for fj ?
F \ FC .
3.2.1 Reuse of Temporal Partial Margins
We can speed up both Eqs. 4 and 5 by reusing
a temporal partial margin, ?jt? = mjt?(x) that hadbeen computed in past round t?(< t):
mjt (x) = ?
j
t? +
?
si?Sj
?ik?d(si,xj ,xj?1), (6)
where Sj = {s | s ? St?1 \ St??1, fj ? s}.
1248
Algorithm 2 KERNEL SLICING
INPUT: x ? 2F , St?1, ?t?1, FC ? F , ? : 2F 7? N? R
OUTPUT: mt(x)
1: initialize: x0 ? ?, j ? 1, mt(x)? m0t
2: repeat
3: xj ? xj?1 unionsq {argminf?x\xj?1 RANK(f)}
4: retrieve partial margin: (t?, ?jt?)? ?(xj)5: if fj ? F \ FC or Eq. 7 is true then
6: compute mjt(x) using Eq. 6 with ?jt?7: else
8: compute mjt(x) using Eq. 59: end if
10: update partial margin: ?(xj)? (t,mjt(x))
11: mt(x)? mt(x) + mjt(x)12: until xj 6= x
13: return mt(x)
Eq. 6 is faster than Eq. 4,3 and can even be
faster than Eq. 5.4 When RANK(fj) is high, xj ap-
pears frequently in the training examples and |Sj |
becomes small since t? will be close to t. When
RANK(fj) is low, xj rarely appears in the training
examples but we can still expect |Sj | to be small
since the number of support vectors in St?1\St??1
that have rare feature fj will be small.
To compute Eq. 3, we now have the choice to
choose Eq. 5 or 6 for fj ? FC . Counting the
number of features to be examined in computing
mjt (x), we have the following criteria to deter-
mine whether we can use Eq. 6 instead of Eq. 5:
1 + |Sj ||xj?1| ? |xdj \ xdj?1| =
d?
k=1
(j ? 1
k ? 1
)
,
where the left- and right-hand sides indicate the
number of features examined in Eq. 6 for the for-
mer and Eq. 5 for the latter. Expanding the right-
hand side for d = 2, 3 and dividing both sides with
|xj?1| = j ? 1, we have:
|Sj | ?
{
1 (d = 2)
j
2 (d = 3)
. (7)
If this condition is met after retrieving the tem-
poral partial margin, ?jt? , we can compute partial
margin mjt (x) with Eq. 6. This analysis reveals
3When a margin of xj has not been computed, we regard
t? = 0 and ?jt? = 0, which reduces Eq. 6 to Eq. 4.4We associate partial margins with partial feature se-
quences whose features are sorted by frequent-to-rare order,
and store them in a trie (partial margin trie). This enables us
to retrieve partial margin ?jt? for given xj in O(1) time.
that we can expect little speed-up for the second-
order polynomial kernel; we will only use Eq. 6
with third or higher-order polynomial kernel.
Algorithm 2 summarizes the margin computa-
tion with kernel slicing. It processes each feature
fj ? x in frequent-to-rare order, and accumulates
partial margin mjt (x) to have mt(x). Intuitively
speaking, when the algorithm uses the partial mar-
gin, it only considers support vectors on each fea-
ture that have been added since the last evaluation
of the partial feature vector, to avoid the repetition
in kernel evaluation as much as possible.
3.2.2 Termination of Margin Computation
Kernel slicing enables another optimization that
exploits a characteristic of online learning. Be-
cause we need an exact margin, mt(x), only when
hinge-loss `t = 1?ymt(x) is positive, we can fin-
ish margin computation as soon as we find that the
lower-bound of ymt(x) is larger than one.
When ymt(x) is larger than one after pro-
cessing feature fj in Eq. 3, we quickly examine
whether this will hold even after we process the
remaining features. We can compute a possible
range of partial margin mkt (x) with Eq. 4, hav-
ing the upper- and lower-bounds, k??d and k??d, of
k?d(si,xk,xk?1) (= kd(si,xk)? kd(si,xk?1)):
mkt (x) ? k??d
?
si?S+k
?i + k??d
?
si?S?k
?i (8)
mkt (x) ? k??d
?
si?S+k
?i + k??d
?
si?S?k
?i, (9)
where S+k = {si | si ? St?1, fk ? si, ?i > 0},
S?k = {si | si ? St?1, fk ? si, ?i < 0}, k??d =
(k+1)d? kd and k??d = 2d? 1 (? 0 ? sTi xk?1 ?
|xk?1| = k ? 1, sTi xk = sTi xk?1 + 1 for all
si ? S+k ? S?k ).We accumulate Eqs. 8 and 9 from rare to fre-
quent features, and use the intermediate results
to estimate the possible range of mt(x) before
Line 3 in Algorithm 2. If the lower bound of
ymt(x) turns out to be larger than one, we ter-
minate the computation of mt(x).
As training continues, the model becomes dis-
criminative and given x is likely to have a larger
margin. The impact of this termination will in-
crease as the amount of training data expands.
1249
4 Evaluation
We evaluated the proposed method in two NLP
tasks: dependency parsing (Sassano, 2004) and
hyponymy-relation extraction (Sumida et al,
2008). We used labeled data included in open-
source softwares to promote the reproducibility of
our results.5 All the experiments were conducted
on a server with an Intel R? XeonTM 3.2 GHz CPU.
We used a double-array trie (Aoe, 1989; Yata et
al., 2009) as an implementation of the weight trie
and the partial margin trie.
4.1 Task Descriptions
Japanese Dependency Parsing A parser inputs
a sentence segmented by a bunsetsu (base phrase
in Japanese), and selects a particular pair of bun-
setsus (dependent and head candidates); the clas-
sifier then outputs label y = +1 (dependent) or
?1 (independent) for the pair. The features con-
sist of the surface form, POS, POS-subcategory
and the inflection form of each bunsetsu, and sur-
rounding contexts such as the positional distance,
punctuations and brackets. See (Yoshinaga and
Kitsuregawa, 2009) for details on the features.
Hyponymy-Relation Extraction A hyponymy
relation extractor (Sumida et al, 2008) first ex-
tracts a pair of entities from hierarchical listing
structures in Wikipedia articles (hypernym and
hyponym candidates); a classifier then outputs la-
bel y = +1 (correct) or ?1 (incorrect) for the
pair. The features include a surface form, mor-
phemes, POS and the listing type for each entity,
and surrounding contexts such as the hierarchical
distance between the entities. See (Sumida et al,
2008) for details on the features.
4.2 Settings
Table 1 summarizes the training data for the two
tasks. The examples for the Japanese dependency
parsing task were generated for a transition-based
parser (Sassano, 2004) from a standard data set.6
We used the dependency accuracy of the parser
5The labeled data for dependency parsing is available
from: http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/pecco/, and
the labeled data for hyponymy-relation extraction is avail-
able from: http://nlpwww.nict.go.jp/hyponymy/.
6Kyoto Text Corpus Version 4.0:
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus-e.html.
DATA SET DEP REL
|T | 296,776 201,664
(y = +1) 150,064 152,199
(y = ?1) 146,712 49,465
Ave. of |x| 27.6 15.4
Ave. of |x2| 396.1 136.9
Ave. of |x3| 3558.3 798.7
|F| 64,493 306,036
|F2| 3,093,768 6,688,886
|F3| 58,361,669 64,249,234
Table 1: Training data for dependency parsing
(DEP) and hyponymy-relation extraction (REL).
as model accuracy in this task. In the hyponymy-
relation extraction task, we randomly chosen two
sets of 10,000 examples from the labeled data for
development and testing, and used the remaining
examples for training. Note that the number of
active features, |Fd|, dramatically grows when we
consider higher-order conjunctive features.
We compared the proposed method, PA-I SL
(Algorithm 1 with Algorithm 2), to PA-I KER-
NEL (Algorithm 1 with PKI; Okanohara and Tsu-
jii (2007)), PA-I KE (Algorithm 1 with kernel ex-
pansion; viz., kernel splitting with N = |F|),
SVM (batch training of support vector machines),7
and `1-LLM (stochastic gradient descent training
of the `1-regularized log-linear model: Tsuruoka
et al (2009)). We refer to PA-I SL that does not
reuse temporal partial margins as PA-I SL?. To
demonstrate the impact of conjunctive features on
model accuracy, we also trained PA-I without con-
junctive features. The number of iterations in PA-I
was set to 20, and the parameters of PA-I were av-
eraged in an efficient manner (Daume? III, 2006).
We explicitly considered conjunctions among top-
N (N = 125 ? 2n;n ? 0) features in PA-I SL
and PA-I SL?. The hyperparameters were tuned to
maximize accuracy on the development set.
4.3 Results
Tables 2 and 3 list the experimental results for
the two tasks (due to space limitations, Tables 2
and 3 list PA-I SL with parameter N that achieved
the fastest speed). The accuracy of the models
trained with the proposed method was better than
`1-LLMs and was comparable to SVMs. The infe-
7http://chasen.org/?taku/software/TinySVM/
1250
METHOD d ACC. TIME MEMORY
PA-I 1 88.56% 3s 55MB
`1-LLM 2 90.55% 340s 1656MB
SVM 2 90.76% 29863s 245MB
PA-I KERNEL 2 90.68% 8361s 84MB
PA-I KE 2 90.67% 41s 155MB
PA-I SL?N=4000 2 90.71% 33s 95MB
`1-LLM 3 90.76% 4057s 21,499MB
SVM 3 90.93% 25912s 243MB
PA-I KERNEL 3 90.90% 8704s 83MB
PA-I KE 3 90.90% 465s 993MB
PA-I SLN=250 3 90.89% 262s 175MB
Table 2: Training time for classifiers used in de-
pendency parsing task.
0
300
600
900
1200
1500
102 103 104 105
Tra
inin
gti
m
e[s
]
N: # of expanded primitive features
PA-I SP
PA-I SL?
PA-I SL
Figure 1: Training time for PA-I variants as a func-
tion of the number of expanded primitive features
in dependency parsing task (d = 3).
rior accuracy of PA-I (d = 1) confirmed the ne-
cessity of conjunctive features in these tasks. The
minor difference among the model accuracy of the
three PA-I variants was due to rounding errors.
PA-I SL was the fastest of the training meth-
ods with the same feature set, and its space effi-
ciency was comparable to the kernel-based learn-
ers. PA-I SL could reduce the memory footprint
from 993MB8 to 175MB for d = 3 in the depen-
dency parsing task, while speeding up training.
Although linear training (`1-LLM and PA-I KE)
dramatically slowed down when we took higher-
order conjunctive features into account, kernel
slicing alleviated deterioration in speed. Espe-
cially in the hyponymy-relation extraction task,
PA-I SL took almost the same time regardless of
the order of conjunctive features.
8`1-LLM took much more memory than PA-I KE mainly
because `1-LLM expands conjunctive features in the exam-
ples prior to training, while PA-I KE expands conjunctive fea-
tures in each example on the fly during training. Interested
readers may refer to (Chang et al, 2010) for this issue.
METHOD d ACC. TIME MEMORY
PA-I 1 91.75% 2s 28MB
`1-LLM 2 92.67% 136s 1683MB
SVM 2 92.85% 12306s 139MB
PA-I KERNEL 2 92.91% 1251s 54MB
PA-I KE 2 92.96% 27s 143MB
PA-I SL?N=8000 2 92.88% 17s 77MB
`1-LLM 3 92.86% 779s 14,089MB
SVM 3 93.09% 17354s 140MB
PA-I KERNEL 3 93.14% 1074s 49MB
PA-I KE 3 93.11% 103s 751MB
PA-I SLN=125 3 93.05% 17s 131MB
Table 3: Training time for classifiers used in
hyponymy-relation extraction task.
0
30
60
90
120
150
102 103 104 105 106
Tra
inin
gti
m
e[s
]
N: # of expanded primitive features
PA-I SP
PA-I SL?
PA-I SL
Figure 2: Training time for PA-I variants as a func-
tion of the number of expanded primitive features
in hyponymy-relation extraction task (d = 3).
Figures 1 and 2 plot the trade-off between the
number of expanded primitive features and train-
ing time with PA-I variants (d = 3) in the two
tasks. Here, PA-I SP is PA-I with kernel slicing
without the techniques described in Sections 3.2.1
and 3.2.2, viz., kernel splitting. The early termi-
nation of margin computation reduces the train-
ing time when N is large. The reuse of temporal
margins makes the training time stable regardless
of parameter N . This suggests a simple, effec-
tive strategy for calibrating N ; we start the train-
ing with N = |F|, and when the learner reaches
the allowed memory size, we shrink N to N/2
by pruning sub-trees rooted by rarer features with
RANK(f) > N/2 in the weight trie.
Figures 3 and 4 plot training time with PA-I
variants (d = 3) for the two tasks as a function
of the training data size. PA-I SP inherited the de-
merit of PA-I KERNEL which takes quadratic time
in the number of examples, while PA-I SL took al-
most linear time in the number of examples.
1251
0
100
200
300
400
500
600
0 50000 100000 150000 200000 250000 300000
Tra
inin
gti
m
e[s
]
|T |: # of training examples
PA-I KERNEL
PA-I SPN=250
PA-I SLN=250
Figure 3: Training time for PA-I variants as a func-
tion of the number of training examples in depen-
dency parsing task (d = 3).
5 Related Work
There are several methods that learn ?simpler?
models with fewer variables (features or support
vectors), to ensure scalability in training.
Researchers have employed feature selection
to assure space-efficiency in linear training. Wu
et al (2007) used frequent-pattern mining to se-
lect effective conjunctive features prior to train-
ing. Okanohara and Tsujii (2009) revised graft-
ing for `1-LLM (Perkins et al, 2003) to prune use-
less conjunctive features during training. Iwakura
and Okamoto (2008) proposed a boosting-based
method that repeats the learning of rules repre-
sented by feature conjunctions. These methods,
however, require us to tune the hyperparameter to
trade model accuracy and the number of conjunc-
tive features (memory footprint and training time);
note that an accurate model may need many con-
junctive features (in the hyponymy-relation ex-
traction task, `1-LLM needed 15,828,122 features
to obtain the best accuracy, 92.86%). Our method,
on the other hand, takes all conjunctive features
into consideration regardless of parameter N .
Dekel et al (2008) and Cavallanti et al (2007)
improved the scalability of the (kernel) percep-
tron, by exploiting redundancy in the training data
to bound the size of the support set to given thresh-
old B (? |St|). However, Orabona et al (2009)
reported that the models trained with these meth-
ods were just as accurate as a naive method that
ceases training when |St| reaches the same thresh-
old, B. They then proposed budget online learn-
ing based on PA-I, and it reduced the size of the
support set to a tenth with a tolerable loss of accu-
0
50
100
150
200
0 50000 100000 150000 200000
Tra
inin
gti
m
e[s
]
|T |: # of training examples
PA-I KERNEL
PA-I SPN=125PA-I SLN=125
Figure 4: Training time for PA-I variants as a
function of the number of training examples in
hyponymy-relation extraction task (d = 3).
racy. Their method, however, requiresO(|St?1|2)
time in updating the parameters in round t, which
disables efficient training. We have proposed an
orthogonal approach that exploits the data redun-
dancy in evaluating the kernel to train the same
model as the base learner.
6 Conclusion
In this paper, we proposed online learning with
kernel slicing, aiming at resolving the space-time
trade-off in training a classifier with many con-
junctive features. The kernel slicing generalizes
kernel splitting (Goldberg and Elhadad, 2008) in
a purely feature-wise manner, to truly combine the
merits of linear and kernel-based training. To im-
prove the scalability of the training with redundant
data in NLP, we reuse the temporal partial margins
computed in past rounds and terminate unneces-
sary margin computations. Experiments on de-
pendency parsing and hyponymy-relation extrac-
tion demonstrated that our method could train a
classifier orders of magnitude faster than kernel-
based learners, while retaining its space efficiency.
We will evaluate our method with ample la-
beled data obtained by the semi-supervised meth-
ods. The implementation of the proposed algo-
rithm for kernel-based online learners is available
from http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/.
Acknowledgment We thank Susumu Yata for
providing us practical lessons on the double-array
trie, and thank Yoshimasa Tsuruoka for making
his `1-LLM code available to us. We are also in-
debted to Nobuhiro Kaji and the anonymous re-
viewers for their valuable comments.
1252
References
Ando, Rie Kubota and Tong Zhang. 2005. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. Journal of Machine
Learning Research, 6:1817?1853.
Aoe, Jun?ichi. 1989. An efficient digital search al-
gorithm by using a double-array structure. IEEE
Transactions on Software Engineering, 15(9):1066?
1077.
Bellare, Kedar, Partha Pratim Talukdar, Giridhar Ku-
maran, Fernando Pereira, Mark Liberman, Andrew
McCallum, and Mark Dredze. 2007. Lightly-
supervised attribute extraction. In Proc. NIPS 2007
Workshop on Machine Learning for Web Search.
Cavallanti, Giovanni, Nicolo` Cesa-Bianchi, and Clau-
dio Gentile. 2007. Tracking the best hyperplane
with a simple budget perceptron. Machine Learn-
ing, 69(2-3):143?167.
Chang, Yin-Wen, Cho-Jui Hsieh, Kai-Wei Chang,
Michael Ringgaard, and Chih-Jen Lin. 2010. Train-
ing and testing low-degree polynomial data map-
pings via linear SVM. Journal of Machine Learning
Research, 11:1471?1490.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Daume? III, Hal. 2006. Practical Structured Learn-
ing Techniques for Natural Language Processing.
Ph.D. thesis, University of Southern California.
Daume? III, Hal. 2008. Cross-task knowledge-
constrained self training. In Proc. EMNLP 2008,
pages 680?688.
Dekel, Ofer, Shai Shalev-Shwartz, and Yoram Singer.
2008. The forgetron: A kernel-based percep-
tron on a budget. SIAM Journal on Computing,
37(5):1342?1372.
Freund, Yoav and Robert E. Schapire. 1999. Large
margin classification using the perceptron algo-
rithm. Machine Learning, 37(3):277?296.
Goldberg, Yoav and Michael Elhadad. 2008.
splitSVM: fast, space-efficient, non-heuristic, poly-
nomial kernel computation for NLP applications. In
Proc. ACL-08: HLT, Short Papers, pages 237?240.
Isozaki, Hideki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. COLING 2002, pages 1?7.
Iwakura, Tomoya and Seishi Okamoto. 2008. A fast
boosting-based learner for feature-rich tagging and
chunking. In Proc. CoNLL 2008, pages 17?24.
Kudo, Taku and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In Proc. ACL 2003,
pages 24?31.
Liang, Percy, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: trading structure for fea-
tures. In Proc. ICML 2008, pages 592?599.
Okanohara, Daisuke and Jun?ichi Tsujii. 2007. A dis-
criminative language model with pseudo-negative
samples. In Proc. ACL 2007, pages 73?80.
Okanohara, Daisuke and Jun?ichi Tsujii. 2009. Learn-
ing combination features with L1 regularization. In
Proc. NAACL HLT 2009, Short Papers, pages 97?
100.
Orabona, Francesco, Joseph Keshet, and Barbara Ca-
puto. 2009. Bounded kernel-based online learning.
Journal of Machine Learning Research, 10:2643?
2666.
Perkins, Simon, Kevin Lacker, and James Theiler.
2003. Grafting: fast, incremental feature selection
by gradient descent in function space. Journal of
Machine Learning Research, 3:1333?1356.
Sassano, Manabu. 2004. Linear-time dependency
analysis for Japanese. In Proc. COLING 2004,
pages 8?14.
Sumida, Asuka, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in Wikipedia. In Proc. LREC 2008, pages
2462?2469.
Tsuruoka, Yoshimasa, Jun?ichi Tsujii, and Sophia
Ananiadou. 2009. Stochastic gradient descent
training for L1-regularized log-linear models with
cumulative penalty. In Proc. ACL-IJCNLP 2009,
pages 477?485.
Williams, Hugh E. and Justin Zobel. 1999. Compress-
ing integers for fast file access. The Computer Jour-
nal, 42(3):193?201.
Wu, Yu-Chieh, Jie-Chi Yang, and Yue-Shi Lee. 2007.
An approximate approach for training polynomial
kernel SVMs in linear time. In Proc. ACL 2007, In-
teractive Poster and Demonstration Sessions, pages
65?68.
Yata, Susumu, Masahiro Tamura, Kazuhiro Morita,
Masao Fuketa, and Jun?ichi Aoe. 2009. Sequential
insertions and performance evaluations for double-
arrays. In Proc. the 71st National Convention of
IPSJ, pages 1263?1264. (In Japanese).
Yoshinaga, Naoki and Masaru Kitsuregawa. 2009.
Polynomial to linear: efficient classification with
conjunctive features. In Proc. EMNLP 2009, pages
1542?1551.
1253
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1091?1102, Dublin, Ireland, August 23-29 2014.
A Self-adaptive Classifier for Efficient Text-stream Processing
Naoki Yoshinaga
Institute of Industrial Science,
the University of Tokyo,
Meguro-ku, Tokyo 153-8505, Japan
ynaga@tkl.iis.u-tokyo.ac.jp
Masaru Kitsuregawa
Institute of Industrial Science,
the University of Tokyo,
Meguro-ku, Tokyo 153-8505, Japan
and
National Institute of Informatics,
Chiyoda-ku, Tokyo 101-8430, Japan
kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
A self-adaptive classifier for efficient text-stream processing is proposed. The proposed classifier
adaptively speeds up its classification while processing a given text stream for various NLP tasks.
The key idea behind the classifier is to reuse results for past classification problems to solve
forthcoming classification problems. A set of classification problems commonly seen in a text
stream is stored to reuse the classification results, while the set size is controlled by removing the
least-frequently-used or least-recently-used classification problems. Experimental results with
Twitter streams confirmed that the proposed classifier applied to a state-of-the-art base-phrase
chunker and dependency parser speeds up its classification by factors of 3.2 and 5.7, respectively.
1 Introduction
The rapid growth in popularity of microblogs (e.g., Twitter) is enabling more and more people to in-
stantly publish their experiences or thoughts any time they want from mobile devices. Since information
in text posted by hundreds of millions of those people covers every space and time in the real world,
analyzing such a text stream tells us what is going on in the real world and is therefore beneficial for re-
ducing damage caused by natural disasters (Sakaki et al., 2010; Neubig et al., 2011a; Varga et al., 2013),
monitoring political sentiment (Tumasjan et al., 2010) and disease epidemics (Aramaki et al., 2011), and
predicting stock market (Gilbert and Karahalios, 2010) and criminal incident (Wang et al., 2012).
Text-stream processing, however, faces a new challenge; namely, the quality (content) and quantity
(volume of flow) changes dramatically, reflecting a change in the real world. Current studies on pro-
cessing microblogs have focused mainly on the difference between the quality of microblogs (or spoken
languages) and news articles (or written languages) (Gimpel et al., 2011; Foster et al., 2011; Ritter et al.,
2011; Han and Baldwin, 2011), and they have not addressed the issue of so-called ?bursts? that increase
the volume of text. Although it is desirable to use NLP analyzers with the highest possible accuracy for
processing a text stream, high accuracy is generally attained by costly structured classification or classi-
fication with rich features, typically conjunctive features (Liang et al., 2008). It is therefore inevitable to
trade accuracy for speed by using only a small fraction of features to assure real-time processing.
In this study, the aforementioned text-quantity issue concerning processing a text stream is addressed,
and a self-adaptive algorithm that speeds up an NLP classifier trained with many conjunctive features (or
with a polynomial kernel) for a given text stream is proposed and validated. Since globally-observable
events such as natural disasters or sports events incur a rapid growth in the number of posts (Twitter, Inc.,
2011), a text stream is expected to contain similar contents concerning these events when the volume of
flow in a text stream increases. To adaptively speed up the NLP classifier, the proposed algorithm thus
enumerates common classification problems from seen classification problems and keeps their classifi-
cation results as partial results for use in solving forthcoming classification problems.
The proposed classifier was evaluated by applying it to streams of classification problems generated
during the processing of the Twitter streams on the day of the 2011 Great East Japan Earthquake and on
another day in March 2012 using a state-of-the-art base-phrase chunker (Sassano, 2008) and dependency
parser (Sassano, 2004), and the obtained results confirm the effectiveness of the proposed algorithm.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1091
2 Related work
A sentence is the processing unit used for fundamental NLP tasks such as word segmentation, part-of-
speech tagging, phrase chunking, syntactic parsing, and semantic role labeling. Most efficient algorithms
solving these tasks thus aim at speeding up the processing based on this unit (Kaji et al., 2010; Koo et
al., 2010; Rush and Petrov, 2012), and few studies have attempted to speed up the processing of a given
text (a set of sentences) as a whole. In the following, reported algorithms that adaptively speed up NLP
analyzers for a given text are introduced.
A method of speeding up a classifier trained with many conjunctive features by using precomputed
results for common classification problems was proposed by Yoshinaga and Kitsuregawa (2009; 2012).
It solves classification problems that commonly appear in the processing of a large amount of text in
advance and stores the results in a trie, so that they can be reused as partial results for solving new classi-
fication problems. This method was reported to achieve speed-up factors of 3.3 and 10.6 for base-phrase
chunking and dependency parsing, respectively. An analogous algorithm for integer linear program (ILP)
used to solve structured classification was proposed by Srikumar, Kundu, and Roth (2012; 2013). The
algorithm was reported to achieve speed-up factors of 2.6 and 1.6 for semantic role labeling and entity-
relation extraction, respectively. Although these two algorithms can be applied to various NLP tasks that
can be solved by using a linear classifier or an ILP solver, how effective they are for processing a text
stream is not clear.
A method of feature sharing for beam-search incremental parsers was proposed by Goldberg et al.
(2013). Motivated by the observation that beam parsers solve similar classification problems in different
parts of the beam, this method reuses partial results computed in the previous beam items. It reportedly
achieved a speed-up factor of 1.2 for arc-standard and arc-eager dependency parsers. The key differences
between the method proposed in this study and their feature-sharing method are twofold. First, the feature
sharing in Goldberg et al. (2013) is performed in a token-wise manner in the sense that a key to retrieve a
cached result is represented by a bag of tokens that invoke features, which manner prevents fine-grained
caching. Second, the feature sharing is dynamically performed during parsing, but the cached results are
cleared after processing each sentence.
An adaptive pruning method for fast HPSG parsing was proposed by van Noord (2009). This method
preprocesses a large amount of text by using a target parser to collect derivation steps that are unlikely to
contribute to the best parse, and it speeds up the parser by filtering out those unpromising derivation steps.
Although this method was reported to attain a speed-up factor of four while keeping parsing accuracy, it
needs to be tuned to trade parsing accuracy and speed for each domain. It is difficult to derive the true
potential of their method in regard to processing a text stream whose domain shifts from time to time.
It has been demonstrated by Wachsmuth et al. (2011) that tuning a pipeline schedule of an information
extraction (IE) system improves the efficiency of the system. Furthermore, the self-supervised learning
algorithm devised by Wachsmuth et al. (2013) predicts the processing time for each possible pipeline
schedule of an IE system, and the prediction is used to adaptively change the pipeline schedule for a given
text stream. This method and the proposed method for speeding up an NLP classifier are complementary,
and a combination of both methods is expected to synergistically speed up various NLP-systems.
In this study, based on the classifier proposed by Yoshinaga and Kitsuregawa (2009), a self-adaptive
classifier that enumerates common classification problems from a given text stream and reuses their
results is proposed. As a result, the proposed classifier adaptively speeds up the classification of forth-
coming classification problems.
3 Preliminaries
As the basis of the proposed classifier, the previously-presented classifier that uses results of common
classification problems (Yoshinaga and Kitsuregawa, 2009) is described as follows. This base classifier
targets a linear classifier trained with many conjunctive features (including one converted from a classifier
trained with polynomial kernel (Isozaki and Kazawa, 2002)) that are widely used for many NLP tasks.
Although this classifier (and also the one proposed in this paper) can handle a multi-class classification
problem, a binary classification problem is assumed here for brevity.
1092
A binary classifier such as a perceptron and a support vector machine determines label y ? {+1,?1}
of input classification problem x by using the following equation (from which the bias term is omitted
for brevity):
m(x;?,w) =w
T
?(x) =
?
i
w
i
?
i
(x) (1)
y =
{
+1 (m(x;?,w) ? 0)
?1 (m(x;?,w) < 0).
(2)
Here, ?
i
is a feature function, w
i
is a weight for ?
i
obtained as a result of training, and m(x;?,w) is a
margin between x and the separating hyperplane.
In most NLP tasks, feature functions are mostly indicator (or binary) functions that typically represent
particular linguistic constraints. Here, feature functions are assumed to be indicator functions that return
{0, 1}, and margin m(x;?,w) is represented by the following equation:
m(x;?,w) =
?
i
w
i
?
i
(x) =
?
i,?
i
(x)=1
w
i
. (3)
Feature function ?
i
is hereafter referred to as feature ?
i
; when ?
i
(x) = 1 for given x, x is said to
?include? feature ?
i
or feature ?
i
is ?active? in x (denoted as ?
i
? x). Having the number of active
features, |?(x)| ? |{?
i
| ?
i
? x}|, Eq. 3 requires O(|?(x)|) when the weights for the active features
are summed up.
To speed up the summation in Eq. 3, classification results for some classification problems x
c
are
precomputed as M
x
c
? m(x
c
;?,w) in advance, and then these precomputed results are reused as
partial results for solving an input classification problem, x:
m(x;x
c
,?,w) =M
x
c
+
?
i,?
i
?x,?
i
/?x
c
w
i
(4)
where ??
j
? x
c
, ?
j
? x.
Note that for Eq. 4 to be computed faster than Eq. 3, M
x
c
must be retrieved in time less thanO(|?(x
c
)|).
It is actually possible when x
c
includes conjunctive feature ?
i,j
(x
c
) = ?
i
(x
c
)?
j
(x
c
). If it is necessary
to retrieve margin M
x
c
precomputed for x
c
including ?
i
, ?
j
, and ?
i,j
, it is necessary to check only
non-conjunctive (or primitive) features ?
i
and ?
j
, since ?
i,j
is active whenever ?
i
and ?
j
are active (so
checking ?
i,j
can be skipped). The second term of Eq. 4 sums up the weights of the remaining features
that are not included in x
c
but are included in x. For example, under the assumption that x includes
features ?
i
, ?
j
, ?
k
, ?
i,j
, ?
i,k
, and ?
j,k
and that margin M
x
c
has been obtained for x
c
(including ?
i
,
?
j
, and ?
i,j
), five features must be checked (two to retrieve M
x
c
and three to sum up the weights of
the remaining features ?
k
, ?
i,k
and ?
j,k
) by using Eq. 4. On the other hand, to compute m(x;?,w) by
Eq. 3, the weights for the six features must be checked.
To maximize the speed-up obtained by Eq. 4, reuse of margin M
x
c
of common classification problem
x
c
should minimize the number of remaining features included only in x. In other words, x
c
should
be as similar to x as possible (ideally, x
c
= x). It is not, however, realistic to precompute margin
M
x
? m(x;?,w) for every possible classification problem x since it requires O(2|??|) space where
|?
?
| is the number of primitive features (?? ? ?) and |??| is usually more than 10,000 in NLP tasks
due to lexical features. Yoshinaga and Kitsuregawa (2009) therefore preprocess a large amount of text
to enumerate possible classification problems, and select common classification problems, X
c
? 2
?
?
,
according to their probability and the reduction in the number of features to be checked by Eq 4.
Yoshinaga and Kitsuregawa (2009) then represent the common classification problems x
c
? X
c
by
sequences of active primitive feature indices, and store those feature (index) sequences as keys in a
prefix trie with precomputed margin M(x
c
) as their values. To reuse a margin of common classification
problem that is similar to input x in Eq. 4, features are ordered according to their frequency to form a
feature sequence of x
c
. A longest-prefix search for the trie thereby retrieves a common classification
problem similar to the input classification problem in linear time with respect to the number of primitive
features in x
c
, O(|?
?
(x
c
)|).
1093
Algorithm 1 A self-adaptive classifier for enumerating common classification problems
INPUT: x, ?, ?? ? ?,w ? R|?|, X
c
? 2
?
?
, k > 0
OUTPUT: m(x;?,w) ? R, X
c
1: INITIALIZE: x
c
s.t. ??(x
c
) = 0, M
x
c
? 0
2: repeat
3: xold
c
? x
c
4: ??
i
= argmax
?
?
i
?x,?
?
i
/?x
c
FREQ(??
i
) (extract a primitive feature according to its frequency)
5: ??
i
(x
c
)? 1 (construct a new common-classification problem)
6: if x
c
/? X
c
then
7: M
x
c
? m(x
c
;x
old
c
,?,w) (compute margin by using Eq. 4)
8: if |X
c
| = k then
9: X
c
? X
c
? {USELESS(X
c
)}
10: X
c
? X
c
? {x
c
}
11: until ??(x
c
) 6= ?
?
(x)
12: return m(x;?,w) = M
x
c
, X
c
4 Proposed method
The classifier described in Section 3 is extended so that it dynamically enumerates common classification
problems from a given text stream1 to adaptively speed up the classification. This classification ?speed
up? faces two challenges: which (partial) classification problems should be chosen to reuse their results
from a given stream of classification problems, and how to efficiently maintain the extracted common
classification problems. These two challenges are addressed in Sections 4.1 and 4.2, respectively.
4.1 Enumerating common classification problems dynamically from a text stream
Although Yoshinaga and Kitsuregawa (2009) select common classification problems according to their
probability, such statistics cannot be known before a target text stream is entirely seen. A set of common
classification problems was thus kept updated adaptively while processing a text stream; that is, classifi-
cation problems are added when they will be useful, while they are removed when they will be useless,
so that the number of common classification problems, |X
c
|, does not exceed a pre-defined threshold, k.
Algorithm 1 depicts the proposed self-adaptive classifier for enumerating common classification prob-
lems from an input classification problem, x. To incrementally construct common classification problem
x
c
(Line 4-5), the algorithm extracts the primitive features (??
i
) included in x one by one according to
their probability of appearing in the training data of the classifier. When the resulting x
c
is included in
the current set of common classification problems, X
c
, stored margin M
x
c
is reused. Otherwise, margin
M
x
c
= m(x
c
;x
old
c
,?,w) is computed by using Eq. 4 (Line 7), and x
c
is registered in X
c
as a new
common classification problem (Line 10).
An important issue is how to define function USELESS, which selects a common classification problem
that will not contribute to speeding up the forthcoming classification, when the number of common
classification problems, |X
c
|, reaches the pre-defined threshold k. To address this issue, the following
two policies (designed originally for CPU caching) are proposed and compared in terms of the efficiency
of the classifier in experiments:
Least Frequently Used (LFU) This policy counts frequency of common classification problems in a
seen text stream, and it maintains only the top-k common classification problems by removing the
least-common classification problem from X
c
:
USELESSLFU(Xc) = argmin
x
c
?X
c
FREQ(x
c
) (5)
1More precisely, a stream of classification problems generated during the analysis of a text stream.
1094
A space-saving algorithm, (Metwally et al., 2005), is used to efficiently count the approximated
frequency of k classification problems at most and to remove the common classification problem
rejected by the space-saving algorithm.
Least Recently Used (LRU) When the volume of flow in a text stream rapidly increases, it is likely to
relate to a burst of a certain topic. To exploit this characteristics, this policy preserves k common
classification problems whose results are most recently reused:
USELESSLRU(Xc) = argmin
x
c
?X
c
TIME(x
c
) (6)
Common classification problems are associated with the last timing when their results are reused,
and the least-recently-reused common classification problem is removed when |X
c
| = k. To realize
this policy, a circular linked-list of size k is used to maintain precomputed results, and the oldest
element is just overwritten while the corresponding classification problem is removed.
Fixed threshold k is used throughout the processing of a text stream, and its impact on classification
speed was evaluated by experiments.
Since Algorithm 1 naively constructs common classification problems using all the active primitive
features in input classification problem x, it might repeatedly add and remove classification problems
that include rare primitive features such as lexical features. This will incur serious overhead costs. To
avoid this situation, the margin computation is terminated as soon as it is determined that the remaining
computation does not change the sign of margin (namely, classification label y) of x.
When x and x
c
are given, lower- and upper-bounds of m(x;?,w) can be computed by accumulating
bounds of a partial margin computed by adding remaining active primitive features, {??
j
? x | ?
?
j
/? x
c
},
one by one to x
c
. It is assumed that primitive feature ??
i
is newly activated in x
c
and xold
c
refers to x
c
without ??
i
being activated. The partial margin, m(x
c
;?,w)?m(x
old
c
;?,w), is computed by summing
up the weights of primitive feature ??
i
and conjunctive features that are composed of ??
i
and one or more
primitive features ??
j
? x
old
c
. This partial margin is upper- and lower-bounded as follows:
m(x
c
;?,w)?m(x
old
c
;?,w)?max(w
min
i
|{?
j
? x
c
| ?
j
/? x
old
c
}|,W
?
i
) (7)
m(x
c
;?,w)?m(x
old
c
;?,w)?min(w
max
i
|{?
j
? x
c
| ?
j
/? x
old
c
}|,W
+
i
), (8)
where wmin
i
and wmax
i
refer to minimum and maximum weights among all the features regarding ??
i
,
while W+
i
and W?
i
refer to summations of all the features regarding ??
i
with positive and negative
weights, respectively; that is, this upper or lower-bound is computed by assuming all the features regard-
ing ??
i
to have a maximum or minimum weight (each bounded by W+
i
or W?
i
). Accumulating these
bounds for each remaining primitive feature makes it possible to obtain the bounds of m(x;?,w) and
thereby judge whether the sign of the margin can be changed by processing the remaining features.
4.2 Maintaining common classification problems with dynamic double-array trie
To maintain the enumerated common classification problems, a double-array trie (Aoe, 1989) is applied.
The trie associates common classification problem x
c
with unique index i(1 ? i ? k), which is fur-
ther associated with computed margin M
x
c
and frequency or access time as described in Section 4.1.
Although a double-array trie provides an extremely fast look-up, it had been considered that update
operation (adding a new key to a double-array trie) is slow. However, in a recent study (Yata et al.,
2009), the update speed of a double-array trie approaches that of a hash table. In the following section, a
double-array trie similar to that of Yata et al. (2009) is used to maintain common classification problems.
Efficient dynamic double-array trie with deletion
A double-array trie (Aoe, 1989) and an algorithm that allows a fast update (Yata et al., 2009) are briefly
introduced in the following. A double array is a data structure for a compact trie, which consists of two
one-dimensional arrays called BASE and CHECK. In a double-array trie, each trie node occupies one
element in BASE and CHECK, respectively.2 For each node, p, BASE stores the offset address of its child
2Although the original double-array (Aoe, 1989) realizes a minimal-prefix trie by using another array (called TAIL) to store
suffix nodes with only one child, TAIL is not adopted here since it is difficult to support space-efficient deletion with TAIL.
1095
nodes, so a child node takes the address c = BASE[p] XOR3 l when the node is traversed from p by label l.
For each node, c, CHECK stores the address of its parent node, p, and is used to confirm the validity of
the traversal by checking whether CHECK[c] = p is held after the node is reached by c = BASE[p] XOR l.
Adding a new node to a trie could cause a conflict, meaning that the newly added node could be
assigned to the address taken by an existing node in the trie. In such a case, it is necessary to collect all
the sibling nodes of either the newly added node or the existing node that took the conflicting address,
and then relocate either branching (with a lower number of child nodes) to empty addresses that are not
taken by other nodes in the trie. This relocation is time-consuming, and is the reason for the slow update.
To perform this relocation quickly, Yata et al. (2009) introduced two additional one-dimensional ar-
rays, called NLINK (node link) and BLOCK. For each node, NLINK stores the label needed to reach its
first child and the label needed to reach from its parent the sibling node next to the node. It thereby
makes it possible to quickly enumerate the sibling nodes for relocation. BLOCK stores information on
empty addresses within each 256 consecutive addresses called a block4 in BASE and CHECK. Each block
is classified into three types, called ?full,? ?closed? and ?open.? Full blocks have no empty addresses and
are excluded from the target of relocation. Closed blocks have only one empty address or have failed to
be relocated more times than a pre-specified threshold. Open blocks are other blocks, which have more
than one empty address. The efficient update of the trie is enabled by choosing appropriate blocks to
relocate a branching; a branching with one child node is relocated to a closed block, while a branching
with multiple child nodes is relocated to an open block.
The above-described double-array trie was modified to support a deletion operation, which simply
registers to each block empty addresses resulting from deletion. In consideration that a new key (common
classification problem) will be stored immediately after the deletion (Line 10 in Algorithm 1), the double-
array trie is not packed as in Yata et al. (2007) after a key is deleted.
Engineering a double-array trie to reduce trie size
To effectively maintain common classification problems in a trie, it is critical to reduce the number of
trie nodes accessed in look-up, update, and deletion operations. The number of trie nodes was therefore
reduced as much as possible by adopting a more compact representation of keys (common classification
problems) and by elaborating the way to store values for the keys in the double-array trie.
Gap-based key representation To compress representations of common classification problems (fea-
ture sequences) in the trie, frequency-based indices are allocated to primitive features (Yoshinaga and
Kitsuregawa, 2010). A gap representation (used to compress posting lists in information retrieval (Man-
ning et al., 2008, Chapter 5)) is used to encode feature sequences. Each feature index is replaced with a
gap from the preceding feature index (the first feature index is used as is). Each gap is then encoded by
variable-byte coding (Williams and Zobel, 1999) to obtain shorter representations of feature sequences.
A reduced double-array trie The standard implementation of a double-array trie stores an (integer)
index with a key at a child node (value node) traversed by a terminal symbol ?\0? (or an alphabet
not included in a key, e.g., ?#?) from the node reached after reading the entire key (Yoshinaga and
Kitsuregawa, 2009; Yasuhara et al., 2013). However, when a key is not a prefix to the other keys, the
value node has no sibling node, so a value can be directly embedded on the BASE of the node reached
after reading the entire key instead of the offset address of the child (value) node. All the value nodes for
the longest prefixes are thereby eliminated from the trie. The resulting double-array trie is referred to as
a reduced double-array trie.
These two tricks reduce the number of trie nodes (memory usage), and make the trie operations faster.
A reduced double-array trie is also used to compactly store the weights of conjunctive features, as de-
scribed in Yoshinaga and Kitsuregawa (2010). Interested readers may refer to cedar,5 open-source soft-
ware of a dynamic double-array trie, for further implementation details of the reduced double-array trie.
3The original double-array (Aoe, 1989) uses addition instead of XOR operation to obtain a child address.
4Note that the XOR operation guarantees that all the child nodes are located within a certain block i (assuming 1 byte (0-255)
for each label, l, child nodes of a node, p, are all located in addresses (256i ? c = BASE[p] XOR l < 256(i + 1)).
5http://www.tkl.iis.u-tokyo.ac.jp/
?
ynaga/cedar/
1096
base-phrase chunking dependency parsing
Number of features |?| 645,951 2,084,127
Number of primitive features |??| 11,509 27,063
Accuracy (partial) 99.01% 92.23%
Accuracy (complete) 94.16% 58.38%
Table 1: Model statistics for base-phrase chunking and dependency parsing.
5 Experiments
The proposed self-adaptive classifier was experimentally evaluated by applying it to streams of classifi-
cation problems. The streams of classification problems were generated by processing Twitter streams
using a state-of-the-art base-phrase chunker and dependency parser. All experiments were conducted
with an Intel R? CoreTM i7-3720QM 2.6-GHz CPU server with 16-GB main memory.
5.1 Setup
Since March 11, 2011 (the day of the Great East Japan Earthquake; ?3.11 earthquake? hereafter), Twitter
streams were crawled by using Twitter API.6 Tweets from famous Japanese users were crawled first.
Next, timelines of those users were obtained. Then, the set of users were repeatedly expanded by tracing
retweets and mentions in their timelines to collect as many tweets as possible. In the following experi-
ments, two sets of 24-hour Twitter streams from the crawled tweets were used. The first Twitter stream
was taken from the day of 3.11 earthquake (12:00 on Friday, March 11, 2011 to 12:00 on Saturday,
March 12, 2011), and the second one was taken from the second weekend in March, 2012 (12:00 on
Friday, March 9, 2012 to 12:00 on Saturday, March 10, 2012). The first Twitter stream is intended to
evaluate the classifier performance on days with a significant, continuous burst, while the second one is to
evaluate the performance on days without such a burst. No special events, other than a small earthquake
(02:25 on March 10), occurred from March 9 to 10, 2012. Because the input to base-phrase chunking
and dependency parsing is a sentence, each post was split by using punctuations as clues.
Although it might be better to evaluate the chunking and parsing speed with the proposed classifier
for a text stream, the classification speed was evaluated for streams of classification problems generated
in processing the Twitter streams by a deterministic base-phrase chunker (Sassano, 2008) and a shift-
reduce dependency parser (Sassano, 2004), which are implemented in J.DepP.12 Note that the chunker
and parser are known to spend most of the time for classification (Yoshinaga and Kitsuregawa, 2012),
and reducing the classification time leads to efficient processing of Twitter streams.
The base-phrase chunker processes each token in a sentence identified by a morphological analyzer,
MeCab,7 and judges whether the token is the beginning of a base-phrase chunk in Japanese (called a
bunsetsu8) or not. The shift-reduce dependency parser processes each chunk in the chunked sentences
and determines whether the head candidate chosen by the parser is correct head or not.
The classifiers for base-phrase chunking and dependency parsing were trained by using a variant of a
passive-aggressive algorithm (PA-I) (Crammer et al., 2006) with a standard split9 of the Kyoto-University
Text Corpus (Kawahara et al., 2002) Version 4.0.10 A third-order polynomial kernel was used to consider
combinations of up-to three primitive features. The features used for training the classifiers were identical
to those implemented in J.DepP. The polynomial kernel expanded (Kudo and Matsumoto, 2003) was
used to make the number of resulting conjunctive features tractable without harming the accuracy.
Table 1 lists the statistics of the models trained for chunking and parsing. In Table 1, ?accuracy
(partial)? is the ratio of chunks (or dependency arcs) correctly identified by the chunker (or the parser),
while ?accuracy (complete)? is the exact-match accuracy of complete chunks (or dependency arcs) in a
sentence. The accuracy of the resulting parser on the standard split was better than any published results
6https://dev.twitter.com/docs/api
7http:://mecab.sourceforge.net/
8A bunsetsu is a linguistic unit consisting of one or more content words followed by zero or more function words.
924,263, 4,833 and 9,284 sentences (234,685, 47,571 and 89,874 base phrases) for training, development, and testing.
10http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?Kyoto%20University%20Text%20Corpus
1097
March 11-12, 2011 March 9-10, 2012
Number of posts 9,291,767 6,360,392
Number of posts/s 108 74
Number of sentences 24,722,596 13,521,196
Number of classification problems (chunking) 220,490,401 109,452,133
Number of classification problems (parsing) 70,096,105 34,380,385
Table 2: Twitter stream used for evaluation.
0
1
2
3
4
5
6
1
2
:
0
0
1
4
:
0
0
1
6
:
0
0
1
8
:
0
0
2
0
:
0
0
2
2
:
0
0
0
0
:
0
0
0
2
:
0
0
0
4
:
0
0
0
6
:
0
0
0
8
:
0
0
1
0
:
0
0
1
2
:
0
0
R
e
l
a
t
i
v
e
#
p
o
s
t
s
/
m
i
n
.
time
March 11-12, 2011
March 9-10, 2012
(a) Change in number of posts per min.
0
1
2
3
4
5
6
1
2
:
0
0
1
4
:
0
0
1
6
:
0
0
1
8
:
0
0
2
0
:
0
0
2
2
:
0
0
0
0
:
0
0
0
2
:
0
0
0
4
:
0
0
0
6
:
0
0
0
8
:
0
0
1
0
:
0
0
1
2
:
0
0
R
e
l
a
t
i
v
e
#
p
o
s
t
s
/
m
i
n
.
time
March 11-12, 2011
March 9-10, 2012
(b) Change in number of classifications per min. (parsing)
Figure 1: Volume of flow of Twitter streams from March 11 to 12, 2011 and from March 9 to 10, 2012.
March 11-12, 2011 March 9-10, 2012
method space speed ratio space speed ratio
[MiB] [ms/sent.] [MiB] [ms/sent.]
baseline 12.01 0.0221 1.00 12.01 0.0188 1.00
Y&K ?09 30.46 0.0118 1.87 30.46 0.0112 1.69
proposed k = 216 18.05 0.0092 2.40 17.93 0.0098 1.93
(LFU) k = 220 90.70 0.0088 2.51 90.78 0.0089 2.12
k = 2
24 463.04 0.0081 2.73 473.60 0.0076 2.48
proposed k = 216 17.32 0.0086 2.57 17.32 0.0093 2.02
(LRU) k = 220 85.89 0.0077 2.88 86.09 0.0085 2.22
k = 2
24 399.17 0.0070 3.16 409.59 0.0068 2.76
Table 3: Experimental results obtained with the reduced double array trie: base phrase chunking.
for this dataset other than those reported for a parser based on ?stacking? (Iwatate, 2012).11
Table 2 lists the detail of the Twitter streams used for evaluating the proposed classifier. Figures 1(a)
and 1(b) show the change in the number of posts and classifications for parsing per minute, when the
average number of posts and classifications per minute before the 3.11 earthquake is counted as one,
respectively. The dataset shows a rapid growth in the number of posts after the 3.11 earthquake occurred
(14:46:18). This event also incurs a rapid growth in the number of classifications for parsing. Although
space limitations precluded the number of classifications for chunking, it had the same tendency as for
parsing. It should be noted that the official retweets (reposts) occupied 25.8% (2,394,025) and 8.5%
(542,726) of the entire posts from March 11 to 12, 2011 and from March 9 to 10, 2012, respectively.
5.2 Results
Tables 3 and 4 list the timings needed to solve the classification problems generated for each sentence by
processing the Twitter streams listed in Table 2 using the base-phrase chunker and the dependency parser,
respectively. In Table 3, ?baseline? refers to the classifier using Eq. 3, while ?Y&K ?09? refers to Yoshi-
naga and Kitsuregawa (2009) who used Eq. 4, and enumerates common classification problems from the
training corpus9 of the classifier in advance. To highlight the performance gap caused by the algorithmic
differences and make the memory consumptions comparable, the experiments were conducted using the
same implementation of the reduced double-array trie, described in Section 4.2, for all the methods. The
proposed classifier (with 65,536 (k = 216) common classification problems) achieved higher classifica-
11The best reported accuracy of a non-stacking parser is 91.96% (partial) and 57.44% (complete) for Kyoto-University Text
Corpus Version 3.0 (Iwatate et al., 2008), and is better than that achieved by the MST algorithm (McDonald et al., 2005).
1098
March 11-12, 2011 March 9-10, 2012
method space speed ratio space speed ratio
[MiB] [ms/sent.] [MiB] [ms/sent.]
baseline 31.50 0.1187 1.00 31.50 0.0979 1.00
Y&K ?09 99.91 0.0738 1.61 99.91 0.0651 1.51
proposed k = 216 43.21 0.0469 2.53 43.01 0.0542 1.81
(LFU) k = 220 113.40 0.0293 4.06 113.27 0.0399 2.45
k = 2
24 904.32 0.0222 5.35 905.62 0.0285 3.44
proposed k = 216 42.68 0.0497 2.39 42.66 0.0546 1.79
(LRU) k = 220 108.88 0.0283 4.20 108.94 0.0421 2.32
k = 2
24 840.85 0.0208 5.71 840.93 0.0280 3.50
Table 4: Experimental results obtained with the reduced double array trie: dependency parsing.
0
0.5
1.0
1.5
2.0
2
10
2
12
2
14
2
16
2
18
2
20
2
22
2
24
2
26
A
v
e
r
a
g
e
t
i
m
e
[
?
s
]
/
c
l
a
s
s
i
f
y
k
lru
lfu
Y&K ?09
(a) base-phrase chunking
0
5
10
15
20
25
30
2
10
2
12
2
14
2
16
2
18
2
20
2
22
2
24
2
26
A
v
e
r
a
g
e
t
i
m
e
[
?
s
]
/
c
l
a
s
s
i
f
y
k
lru
lfu
Y&K ?09
(b) dependency parsing
Figure 2: Average classification time per classification problem as a function of number of common
classification problems k (2011 tweet stream).
tion speed than that achieved by Y&K ?09 (with 943,864 (chunking) and 2,902,679 (parsing) common
classification problems). Although the speed up is evident for both tweet datasets, the speed-up is more
obvious in the case of the 2011 tweet stream. In the following experiments, in view of space limitations
and redundancy, the 2011 tweet stream was used; however, note that the same conclusions are drawn
from the results with the 2012 twitter stream.
Figure 2 shows the time needed for solving each classification problem for chunking and parsing of
the 2011 tweet stream when the threshold to the number of common classification problems k is varied
from 210 to 226, respectively. In both tasks, the proposed classifier with the LRU policy outperforms the
proposed classifier with the LFU policy when k was increased. This is not only because the LFU policy
has higher overheads than the LRU policy but also because the LFU policy selects useless classification
problems that include lexical features related to a burst in the past. The speed-up is saturated in the case
of base-phrase chunking at k = 222 (Figure 2(a)). This is because the proposed classifier often terminates
margin computation without seeing lexical features for base phrase chunking, so it rarely reuses results
of common classification problems including lexical features that are preserved when k is increased.
On the other hand in dependency parsing, the classifier relies on lexical features to resolve semantic
ambiguities, so it cannot terminate margin computation without seeing lexical features and thus exploits
common classification problems including lexical features.
Figure 3 shows the change in the time needed for solving classification problems generated from a
one-minute text stream for chunking and parsing of the 2011 tweet stream. The y-axis shows the relative
classification time, when the average classification time of the baseline method before the 3.11 earth-
quake is counted as one. The classification time of the baseline method and Yoshinaga and Kitsuregawa
(2009)?s method rapidly increased in response to the increase of the number of classification problems,
while the proposed classifier suppressed the increase in classification time. It is thus concluded that the
proposed classifier is more robust in terms of real-time processing for a text stream.
Finally, the contributions of the three tricks of the proposed classifier to the classification performance
for dependency parsing were evaluated. The three tricks are a gap-based key representation and a reduced
double-array trie (Section 4.1), as well as the early termination of margin computation (Section 4.1).
1099
01
2
3
4
5
6
1
2
:
0
0
1
4
:
0
0
1
6
:
0
0
1
8
:
0
0
2
0
:
0
0
2
2
:
0
0
0
0
:
0
0
0
2
:
0
0
0
4
:
0
0
0
6
:
0
0
0
8
:
0
0
1
0
:
0
0
1
2
:
0
0
R
e
l
a
t
i
v
e
t
i
m
e
/
m
i
n
.
p
o
s
t
s
time
baseline
Y&K ?09
lru (k = 2
16
)
lru (k = 2
20
)
lru (k = 2
24
)
(a) base-phrase chunking
0
1
2
3
4
5
6
1
2
:
0
0
1
4
:
0
0
1
6
:
0
0
1
8
:
0
0
2
0
:
0
0
2
2
:
0
0
0
0
:
0
0
0
2
:
0
0
0
4
:
0
0
0
6
:
0
0
0
8
:
0
0
1
0
:
0
0
1
2
:
0
0
R
e
l
a
t
i
v
e
t
i
m
e
/
m
i
n
.
p
o
s
t
s
time
baseline
Y&K ?09
lru (k = 2
16
)
lru (k = 2
20
)
lru (k = 2
24
)
(b) dependency parsing
Figure 3: Change in classification time of one-minute posts (2011 tweet stream).
plain (no tricks) + gap-based key + reduced double array + early termination
method space speed ratio space speed ratio space speed ratio space speed ratio
[MiB] [ms/sent.] [MiB] [ms/sent.] [MiB] [ms/sent.] [MiB] [ms/sent.]
baseline 39.88 0.1413 0.84 n/a n/a n/a 31.50 0.1187 1.00 38.21 0.0745 1.59
Y&K ?09 117.66 0.0922 1.29 110.52 0.0904 1.31 99.91 0.0738 1.61 106.61 0.0406 2.93
proposed k = 216 44.64 0.1037 1.14 44.50 0.1009 1.18 36.09 0.0845 1.41 42.68 0.0497 2.39
(LRU) k = 220 117.21 0.0590 2.01 114.57 0.0572 2.07 105.21 0.0492 2.41 108.88 0.0283 4.20
k = 2
24 969.48 0.0412 2.88 923.96 0.0398 2.98 897.70 0.0350 3.39 840.85 0.0208 5.71
Table 5: Contribution of each trick to classification performance; 2011 tweet dataset (underlined numbers
are quoted from Table 4).
Table 5 lists the classification times per sentence in the case of dependency parsing, when each trick
is cumulatively applied to plain classifiers without all the tricks. Classification is significantly speeded
up by early termination of the margin computation and the reduced double-array trie. These tricks also
contribute to speeding up the baseline method and Yoshinaga and Kitsuregawa (2009)?s method.
6 Conclusion
Aiming to efficiently process a real-world text stream (such as a Twitter stream) in real-time, a self-
adaptive classifier that becomes faster for a given text stream is proposed. It enumerates common clas-
sification problems that are generated during the processing of a text stream, and reuses the results of
those classification problems as partial results for solving forthcoming classification problems.
The proposed classifier was evaluated by applying it to the streams of classification problems generated
by processing two sets of Twitter streams on the day of the 2011 Great East Japan Earthquake and the
second weekend in March 2012 using a state-of-the-art base-phrase chunker and dependency parser. The
proposed classifier speeds up the classification by factors of 3.2 (chunking) and 5.7 (parsing), which are
significant factors in regard to processing a massive text stream.
It is planned to evaluate the classifier on other NLP tasks. A linear classifier with conjunctive fea-
tures is widely used for NLP tasks such as word segmentation, part-of-speech tagging (Neubig et al.,
2011b), and dependency parsing (Nivre and McDonald, 2008). Even for NLP tasks in which structured
classification is effective (e.g., named entity recognition), structure compilation (Liang et al., 2008) (or
?uptraining? (Petrov et al., 2010)) gives state-of-the-art accuracy when a linear classifier with many
conjunctive features is used. The proposed classifier is expected to be applied to a range of NLP tasks.
All the codes have been available for the research community as open-source software, including
pecco (a self-adaptive classifier)12 and J.DepP (a base-phrase chunker and dependency parser).13
Acknowledgments
This work was supported by the Research and Development on Real World Big Data Integration and
Analysis program of the Ministry of Education, Culture, Sports, Science, and Technology, JAPAN.
12http://www.tkl.iis.u-tokyo.ac.jp/
?
ynaga/pecco/
13http://www.tkl.iis.u-tokyo.ac.jp/
?
ynaga/jdepp/
1100
References
Jun?ichi Aoe. 1989. An efficient digital search algorithm by using a double-array structure. IEEE Transactions on
Software Engineering, 15(9):1066?1077.
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita. 2011. Twitter catches the flu: Detecting influenza epidemics
using Twitter. In Proceedings of EMNLP, pages 1568?1576.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 7:551?585, March.
Jennifer Foster, ?Ozlem C?etinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim Nivre, Deirdre
Hogan, and Josef van Genabith. 2011. #hardtoparse: POS tagging and parsing the Twitterverse. In Proceedings
of the AAAI-11 Workshop on Analyzing Microtext.
Eric Gilbert and Karrie Karahalios. 2010. Widespread worry and the stock market. In Proceedings of ICWSM,
pages 58?65.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter:
Annotation, features, and experiments. In Proceedings of ACL-HLT, pages 42?47.
Yoav Goldberg, Kai Zhao, and Liang Huang. 2013. Efficient implementation of beam-search incremental parsers.
In Proceedings of ACL, Short Papers, pages 628?633.
Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In
Proceedings of ACL-HLT, pages 368?378.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. In
Proceedings of COLING, pages 1?7.
Masakazu Iwatate, Masayuki Asahara, and Yuji Matsumoto. 2008. Japanese dependency parsing using a tourna-
ment model. In Proceedings of COLING, pages 361?368.
Masakazu Iwatate. 2012. Development of Pairwise Comparison-based Japanese Dependency Parsers and Appli-
cation to Corpus Annotation. Ph.D. thesis, Graduate School of Information Science, Nara Institute of Science
and Technology.
Nobuhiro Kaji, Yasuhiro Fujiwara, Naoki Yoshinaga, and Masaru Kitsuregawa. 2010. Efficient staggered decod-
ing for sequence labeling. In Proceedings of ACL, pages 485?494.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida. 2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of LREC, pages 2008?2013.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition
for parsing with non-projective head automata. In Proceedings of EMNLP, pages 1288?1298.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL, pages
24?31.
Gourab Kundu, Vivek Srikumar, and Dan Roth. 2013. Margin-based decomposed amortized inference. In Pro-
ceedings of EMNLP, pages 905?913.
Percy Liang, Hal Daume? III, and Dan Klein. 2008. Structure compilation: trading structure for features. In
Proceedings of ICML, pages 592?599.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schu?tze. 2008. Introduction to Information Retrieval.
Cambridge University Press.
Twitter, Inc. 2011. Twitter?s 2011 year in review. http://yearinreview.twitter.com/en/tps.
html.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 523?530.
Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi. 2005. Efficient computation of frequent and top-k
elements in data streams. In Proceedings of ICDT, pages 398?412.
1101
Graham Neubig, Yuichiroh Matsubayashi, Masato Hagiwara, and Koji Murakami. 2011a. Safety information
mining - what can NLP do in a disaster -. In Proceedings of IJCNLP, pages 965?973.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011b. Pointwise prediction for robust, adaptable japanese
morphological analysis. In Proceedings of ACL, pages 529?533.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In
Proceedings of ACL-HLT, pages 950?958.
Slav Petrov, Pi-Chuan Chang,Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate deterministic
question parsing. In Proceedings of EMNLP, pages 705?713.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental
study. In Proceedings of EMNLP, pages 1524?1534.
Alexander Rush and Slav Petrov. 2012. Vine pruning for efficient multi-pass dependency parsing. In Proceedings
of NAACL-HLT, pages 498?507.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes Twitter users: Real-time event
detection by social sensors. In Proceedings of WWW, pages 851?860.
Manabu Sassano. 2004. Linear-time dependency analysis for Japanese. In Proceedings of COLING, pages 8?14.
Manabu Sassano. 2008. An experimental comparison of the voted perceptron and support vector machines in
Japanese analysis tasks. In Proceedings of IJCNLP, pages 829?834.
Vivek Srikumar, Gourab Kundu, and Dan Roth. 2012. On amortizing inference cost for structured prediction. In
Proceedings of EMNLP-CoNLL, pages 1114?1124.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sandner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about political sentiment. In Proceedings of ICWSM, pages 178?185.
Gertjan van Noord. 2009. Learning efficient parsing. In Proceeding of EACL, pages 817?825.
Istva?n Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-Hoon
Oh, and Stijn De Saeger. 2013. Aid is out there: Looking for help from tweets during a large scale disaster. In
Proceedings of ACL, pages 1619?1629.
Henning Wachsmuth, Benno Stein, and Gregor Engels. 2011. Constructing efficient information extraction
pipelines. In Proceedings of CIKM, pages 2237?2240.
Henning Wachsmuth, Benno Stein, and Gregor Engels. 2013. Learning efficient information extraction on hetero-
geneous texts. In Proceedings of IJCNLP, pages 534?542.
Xiaofeng Wang, Matthew S. Gerber, and Donald E. Brown. 2012. Automatic crime prediction using events
extracted from Twitter posts. In Proceedings of SBP, pages 231?238.
Hugh E. Williams and Justin Zobel. 1999. Compressing integers for fast file access. The Computer Journal,
42(3):193?201.
Makoto Yasuhara, Toru Tanaka, Jun ya Norimatsu, and Mikio Yamamoto. 2013. An efficient language model
using double-array structures. In Proceedings of EMNLP, pages 222?232.
Susumu Yata, Masaki Oono, Kazuhiro Morita, Masao Fuketa, and Jun ichi Aoe. 2007. An efficient deletion
method for a minimal prefix double array. Journal of Software: Practice and Experience, 37(5):523?534.
Susumu Yata, Masahiro Tamura, Kazuhiro Morita, Masao Fuketa, and Jun?ichi Aoe. 2009. Sequential insertions
and performance evaluations for double-arrays. In Proceedings of the 71st National Convention of IPSJ, pages
1263?1264. (In Japanese).
Naoki Yoshinaga and Masaru Kitsuregawa. 2009. Polynomial to linear: Efficient classification with conjunctive
features. In Proceedings of EMNLP, pages 1542?1551.
Naoki Yoshinaga and Masaru Kitsuregawa. 2010. Kernel slicing: Scalable online training with conjunctive fea-
tures. In Proceedings of COLING, pages 1245?1253.
Naoki Yoshinaga and Masaru Kitsuregawa. 2012. Efficient classification with conjunctive features. Journal of
Information Processing, 20(1):228?227.
1102
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 883?892, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Identifying Constant and Unique Relations by using Time-Series Text
Yohei Takaku?
Toyo Keizai Inc.
Chuo-ku, Tokyo 103-8345, Japan
takaku.yohei@gmail.com
Nobuhiro Kaji and Naoki Yoshinaga
Institute of Industrial Science,
University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
{kaji,ynaga}@tkl.iis.u-tokyo.ac.jp
Masashi Toyoda
Institute of Industrial Science,
University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
toyoda@tkl.iis.u-tokyo.ac.jp
Abstract
Because the real world evolves over time, nu-
merous relations between entities written in
presently available texts are already obsolete
or will potentially evolve in the future. This
study aims at resolving the intricacy in con-
sistently compiling relations extracted from
text, and presents a method for identifying
constancy and uniqueness of the relations in
the context of supervised learning. We ex-
ploit massive time-series web texts to induce
features on the basis of time-series frequency
and linguistic cues. Experimental results con-
firmed that the time-series frequency distribu-
tions contributed much to the recall of con-
stancy identification and the precision of the
uniqueness identification.
1 Introduction
We have witnessed a number of success stories in
acquiring semantic relations between entities from
ever-increasing text on the web (Pantel and Pennac-
chiotti, 2006; Banko et al2007; Suchanek et al
2007; Wu et al2008; Zhu et al2009; Mintz et al
2009; Wu and Weld, 2010). These studies have suc-
cessfully revealed to us millions of relations between
real-world entities, which have been proven to be
beneficial in solving knowledge-rich problems such
as question answering and textual entailment (Fer-
rucci et al2010).
?This work was conducted while the first author was a grad-
uate student at University of Tokyo.
There exists, however, a great challenge to com-
pile consistently relations extracted from text by
these methods, because they assume a simplifying
assumption that relations are time-invariant. In other
words, they implicitly disregard the fact that state-
ments in texts actually reflect the state of the world
at the time when they were written, which follows
that relations extracted from such texts eventually
become outdated as the real world evolves over time.
Let us consider that relations are extracted from
the following sentences:1
(1) a. 1Q84 is written by Haruki Murakami.
b. Moselle river flows through Germany.
c. U.S.?s president is George Bush.
d. Pentax sells K-5, a digital SLR.
Here, italicized predicates represent the relations,
while underlined entities are their arguments. The
relations in statements 1a and 1b are true across
time, so we can simply accumulate all the relation
instances. The relations in 1c and 1d in contrast
evolve over time. The relation written in 1c be-
comes outdated when the other person takes the
position, so we need to supersede it when a new
relation is extracted from text (e.g., U.S?s president
is Barack Obama). For the relation in 1d, we do not
always need to supersede it with a new relation.
This study is motivated from the above consider-
1Since our task settings are language-independent, we here-
after employ English examples as much as possible to widen
the potential readership of the paper, although we conducted
experiments with relations between entities in Japanese.
883
ations and proposes a method for identifying con-
stancy and uniqueness of relations in order to se-
lect an appropriate strategy to maintain relation in-
stances extracted from text. For example, the rela-
tions written in statements 1a and 1b are constant,
while those in 1c and 1d are non-constant; the re-
lation in 1c is unique,2 whereas the relation in 1d
is non-unique. With these properties of relations in
mind, we can accumulate constant relations while
appropriately superseding non-constant, unique re-
lations with newly acquired relations.
We locate each identification task in the context
of supervised classification. The key challenge in
solving these classification tasks is how to induce an
effective feature that identifies unique, non-constant
relations (statement 1c) that seemingly appear as
non-unique relations on text (statement 1b). We ex-
ploit massive time-series web text to observe actual
evolutions of relation instances and induce features
from the relation instances taken from a time sliding
window and linguistic cues modifying the predicate
and arguments of the target relation.
We evaluated our method on 1000 relations ex-
tracted from 6-year?s worth of Japanese blog posts
with 2.3-billion sentences. We have thereby con-
firmed that the features induced from this time-series
text contributed much to improve the classification
accuracy.
The main contributions of this paper are twofold:
? We have introduced a novel task for identify-
ing constancy relations. Since most of the ex-
isting studies assume that relations are time-
invariant as discussed by Weikum et al2011),
non-constant relations prevalent in their out-
come incur a serious problem in maintaining
the acquired relations. The notion of constancy
is meant to resolve this stalemate.
? We have for the first time demonstrated the
usefulness of a time-series text in relation ac-
quisition and confirmed its impact in the two
relation classification tasks. The features in-
duced from the time-series text have greatly
contributed to the accuracy of the classification
based on uniqueness as well as the recall of the
classification based on constancy.
2This kind of relation is referred to as functional relation in
the literature (Ritter et al2008; Lin et al2010).
Constant Non-constant
arg1 was born in arg2 arg1?s president is arg2
arg1 is a father of arg2 arg1 belongs to arg2
arg1 is written by arg2 arg1 lives in arg2
Table 1: Examples of constant, non-constant relations.
The reminder of this paper is structured as fol-
lows. Section 2 introduces the two properties of
relations (constancy and uniqueness) and then de-
fines the task setting of this study. Sections 3 and 4
describe the features induced from time-series text
for constancy and uniqueness classification, respec-
tively. Section 5 reports experimental results. Sec-
tion 6 addresses work related to this study. Section 7
concludes this study and mentions future work.
2 Classification of Relations based on
Constancy and Uniqueness
2.1 Constancy and uniqueness
We introduce two properties of relations: constancy
and uniqueness.
A relation is constant if, for most values of arg1,
the value of arg2 is independent of time (Table 1).
For example, ?arg1 was born in arg2? is a constant
relation since one?s birthplace never changes. On the
other hand, ?arg1 ?s president is arg2? is an example
of non-constant relations. This can be checked by
noting that, for example, the president of the United
States was Barack Obama in 2011 but was previ-
ously George Bush and Bill Clinton before him.
A relation is unique if, for most values of arg1,
there exists, at any given point in time, only one
value of arg2 that satisfies the relation (Table 2). For
example, ?arg1 was born in arg2? is obviously a
unique relation. The relation ?arg1 is headquartered
in arg2? is also unique, while it is non-constant. No-
tice that there is usually only one headquarters at any
point in time, although the location of a headquarters
can change. In contrast, the relation ?arg1 is funded
by arg2? is a non-unique relation since it is likely
that there exist more than one funder.
2.2 Discussion
Both constancy and uniqueness are properties that
usually, not always, hold for most, not all, of the
arg1?s values. To see this, let us examine the relation
?arg1 ?s president is arg2?. Although this relation is
884
Unique Non-unique
arg1 was born in arg2 arg1 is funded by arg2
arg1 is headquartered in arg2 arg1 consists of arg2
arg1?s president is arg2 arg1 borders on arg2
Table 2: Examples of unique and non-unique relations.
non-constant and unique (Table 1 and 2), it is still
possible to find exceptional cases. For example, a
country might exist in which the president has never
changed; a country might have more than one pres-
ident at the same time during civil war. However,
since such situations are rare, the relation ?arg1 ?s
president is arg2? is considered as neither constant
nor non-unique.
The above discussion implies that the constancy
and uniqueness of relations can not be determined
completely objectively. We, nevertheless, claim that
these properties of relations are intuitively accept-
able and thus they can be identified with moderate
agreement by different people (see section 5).
2.3 Task and our approach
This paper explores classifying given relations on
the basis of constancy and uniqueness. We treat
the problem as two independent binary classification
tasks, and train supervised classifiers.
The technical challenge we address in this paper
is how to design features for the two tasks. Section
3 presents features based on time-series frequency
and linguistic cues for classifying constant and non-
constant relations. Similarly, section 4 presents
analogous features for classifying unique and non-
unique relations.
3 Features for Constancy Classification
3.1 Time-series frequency
It is intuitive to identify constant relations by com-
paring frequency distributions over arg2 in different
time periods. This idea leads us to use frequency
estimates from time-series text as features.
Time-series text For a time-series text, we used
Japanese blog posts that had been gathered from
Feb. 2006 to Sep. 2011 (68 months). These data in-
clude 2.3 billions of sentences. These posts were ag-
gregated on a monthly basis by using time stamps at-
tached with them, i.e., the unit of time is one month
0
2
4
6
8
10
12
Mar-08 Sep-08 Mar-09 Sep-09 Mar-10 Sep-10 Mar-11 Sep-11
Freq
uenc
y
PAO ChelseaChairman Haiberuden Luzhniki StadiumDutch league ItalyVVV VVV VenloCSKA Moscow
Figure 1: Time-series frequency distribution of ?arg1 be-
longs to arg2? when arg1 takes Keisuke Honda.
in our corpus.
Basic idea For constant relations (e.g., ?arg1 was
born in arg2?), we can expect that the frequency dis-
tributions over arg2 for a given arg1 (e.g., Mozart)
are similar to each other irrespective of the time win-
dows that are used to estimate frequency.
In the case of non-constant relations (e.g., ?arg1
belongs to arg2?), on the other hand, the frequency
distributions over arg2 for a given arg1 significantly
differ depending on the time window. For exam-
ple, Figure 1 illustrates the frequency distributions
of arg2s for ?arg1 belongs to arg2? in which arg1
takes Keisuke Honda, a famous football player. We
can clearly observe that due to Keisuke Honda being
sold from VVV Venlo to CSKA Moscow, the distri-
butions differ greatly between 2008 and 2010.
As is evident from the above discussions, the sta-
bility/change in the distribution over arg2 is a good
indicator of constant/non-constant relations. The
following subsection addresses how to encode such
information as features.
Feature computation Let us examine using as
features the cosine similarity between frequency dis-
tributions over arg2. Averaging such similarities
over representative values of arg1, we have
1
N
?
e?EN (r)
cos(Fw1(r, e), Fw2(r, e)),
where r is a relation (e.g., ?arg1 ?s president is
arg2?), e is a named entity (e.g., United States) ap-
pearing in arg1, and Fw(r, e) is the frequency distri-
bution over arg2 when arg1 takes e. The subscripts
885
w1 and w2 denote the time window (e.g., from Jan.
2011 to Feb. 2011) used to estimate the frequency
distribution. EN (r) denotes a set of top N frequent
entities appearing in arg1. We use the entire time-
series text to obtain EN (r).
Unfortunately, this idea is not suitable for our pur-
pose. The problem is that it is not clear how to deter-
mine the two time windows, w1 and w2. To identify
non-constant relations, arg2 must have different val-
ues in the two time periods. Such time windows are,
however, impossible to know of in advance.
We propose avoiding this difficulty by using av-
erage, maximum and minimum similarity over all
possible time windows:
1
N
?
e?EN (r)
ave
w1,w2?WT
cos(Fw1(r, e), Fw2(r, e)),
1
N
?
e?EN (r)
max
w1,w2?WT
cos(Fw1(r, e), Fw2(r, e)),
1
N
?
e?EN (r)
min
w1,w2?WT
cos(Fw1(r, e), Fw2(r, e)),
where WT is a set of all time windows of the size
T . For example, if we set T to 3 (months) in the
68-month?s worth of blog posts, WT consists of 66
(= 68?3+1) time windows. Although we still have
to specify the number of entities N and the window
size T , this is not a serious problem in practice. We
set N to 100. We use four window sizes (1, 3, 6, and
12 months) and induce different features for each
window size. As a result, we have 12 real-valued
features.
3.2 Linguistic cues
This subsection presents two types of linguistically-
motivated features for discriminating between con-
stant and non-constant relations.
Nominal modifiers We observe that non-constant
relations could be indicated by some nominal modi-
fiers:
(2) a. George Bush, ex-president of USA.
b. Lincoln is the first president of the USA.
The use of the prefix ex- and the adjective first im-
plies that the president changes, and hence the rela-
tion ?arg1 ?s president is arg2? is not constant.
? (ex-),? (present),?? (next),? (former),? (new),
? (old),?? (successive),?? (first),? (first)
Table 3: Japanese prefixes and adjectives indicating non-
constant relations. The translations are provided in the
parentheses.
We propose making use of such modifiers as fea-
tures. Although the above examples are in English,
we think modifiers also exist that have similar mean-
ings in other languages including Japanese, our tar-
get language.
Our new features are induced as follows:
? First, we manually list eight nominal modifiers
that indicate the non-constancy (Table 3).
? Next, we extract nouns from a relation to
be classified (e.g., president), and count the
frequency with which each modifier modifies
those nouns. We use the same blog posts as in
section 3.1 for counting the frequency. Since
time information is not important in this case,
the frequency is simply accumulated over the
entire time span.
? We then generate eight features, one for each of
the eight modifiers. The value of the features
is one if the frequency exceeds threshold ?1,3
otherwise it is zero. Note that the value of this
feature is always zero if the relation includes no
nouns.
Tense and aspect Tense and aspect of verbs are
also important indicators of the non-constancy:
(3) The U.S. president was George Bush.
If a relation, such as ?arg1 ?s president is arg2?, can
often be rephrased in the past tense as in (3), it is
likely to be, if not always, a non-constant relation.
It is, fortunately, straightforward to recognize
tense and aspect in Japanese, because they are ex-
pressed by attaching suffixes to verbs. In this study,
we use three common suffixes: ???, ?????, and
????. The first suffix expresses past tense, while
the other two express present continuous or progres-
sive aspects depending on context.
3?1 = 10 in our experiment.
886
A given relation is transformed into different
forms by attaching the suffixes to a verb in the rela-
tion, and their frequencies are counted. By using the
frequency estimates, we generate three new features,
each of which corresponds to one of the three suf-
fixes. The value of the new features is one if the fre-
quency exceeds threshold ?2,4 otherwise it is zero.
The frequency is counted in the same way as in
the case of the nominal modifiers. The value of
this feature is always zero if the relation includes no
verbs.
4 Features for Uniqueness Classification
This section provides features for identifying unique
relations. These features are also based on the time-
series text and linguistic cues, as in the case of con-
stancy classification.
4.1 Time-series frequency
Number of entity types A straightforward ap-
proach to identifying unique relations is, for a given
arg1, to count the number of entity types appear-
ing in arg2 (Lin et al2010). For unique relations,
the number of entity types should be one in an ideal
noiseless situation. Even if the estimate is contam-
inated by noise, a small number of entity types can
still be considered to indicate the uniqueness of the
relation.
A shortcoming of such a simple approach is that
it never considers the (non-)constancy of relations.
Presume counting the number of entity types in arg2
of the relation ?arg1 is headquartered in arg2?,
which is non-constant and unique. If we use large
size of time window to obtain counts, we will ob-
serve multiple types of entities in arg2, not because
the relation is non-unique, but because it is non-
constant. This problem cannot be resolved by triv-
ially using very small windows, since a time win-
dow that is too small in turn causes a data sparseness
problem.
This problem is attributed to the difficulty in de-
termining the appropriate size of the time window.
We tackle this problem by using the same technique
presented in section 3.1. Specifically, we use the fol-
4?2 = 3000 in our experiment.
lowing three measures as features:
1
N
?
e?EN (r)
ave
w?WT
#type(Fw(r, e)),
1
N
?
e?EN (r)
max
w?WT
#type(Fw(r, e)),
1
N
?
e?EN (r)
min
w?WT
#type(Fw(r, e)),
where the function #type(?) denotes the number of
entity types appearing in arg2.
Ratio of entity frequency Since it is not reliable
enough to use only the number of entity types, we
also exploit the frequency of the entity. Let e1st and
e2nd be the most and the second most frequent enti-
ties found in arg2. If the frequency of e1st is much
larger than that of e2nd, the relation is likely to be
constant.
To encode this intuition, the following measures
are used as features:
1
N
?
e?EN (r)
ave
w?WT
fw(e, r, e1st)
fw(e, r, e2nd)
1
N
?
e?EN (r)
max
w?WT
fw(e, r, e1st)
fw(e, r, e2nd)
1
N
?
e?EN (r)
min
w?WT
fw(e, r, e1st)
fw(e, r, e2nd)
where the fw(e, r, e?) is the frequency of the relation
r in which arg1 and arg2 take e and e?, respectively.
The subscript w denotes the time window.
4.2 Linguistic cues
Coordination structures and some keywords indicate
non-unique relations:
(4) a. France borders on Italy and Spain.
b. France borders on Italy etc.
The coordination structure in the first example im-
plies an entity can border on more than one entity,
and hence the relation ?arg1 borders on arg2? is not
unique. The keyword etc. in the second example also
indicates the non-uniqueness.
887
?,??,?,??,??,??,?
Table 4: List of Japanese particles that are used to form
coordination structures.
To capture this intuition, we introduce two types
of linguistic features for classifying unique and non-
unique relations. The first feature checks whether
entities in arg2 form coordination structures. The
feature is fired if the number of times that coordina-
tion structures are found in arg2 exceeds threshold
?3.5 Coordination structures are identified by a list
of Japanese particles, which roughly correspond to
and or or in English (Table 4). If two entities are
connected by one of those particles, they are seen as
forming a coordination structure.
The second feature exploits such keywords as etc.
for identifying non-unique relations. We list four
Japanese keywords that have similar meaning to the
English word etc., and induce another binary fea-
ture6. The feature is fired if the number of times that
an entity in arg2 is followed by one of the four key-
words exceeds threshold ?3.
5 Experiments and discussions
We built labeled data and examine the classification
performance of the proposed method. We also an-
alyzed the influence of window size T on the per-
formance, as well as major errors caused by our
method.
5.1 Data
We built a dataset for evaluation by extracting rela-
tions from the time-series text (section 3.1) and then
manually annotating 1000 relations. The detailed
procedure is as follows.
First, we parsed the time-series text and extracted
as relation dependency paths connecting two named
entities. We used J.DepP,7 an efficient shift-reduce
parser with feature sequence trie (Yoshinaga and
Kitsuregawa, 2009; Yoshinaga and Kitsuregawa,
2010), for parsing. All Japanese words that conju-
gate were normalized into standard forms.
5?3 = 10 in our experiment.
6The keywords we used are?,?,??, and?.
7http://www.tkl.iis.u-tokyo.ac.jp/
?ynaga/jdepp/
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
Proposed
Baseline
Figure 2: Recall-precision curve (constancy classifica-
tion).
Then, annotators were asked to label 1000 rela-
tions as not only constant or non-constant but also
unique or non-unique. Three annotators were as-
signed to each relation, and the goldstandard label
is determined by majority vote. The Fleiss kappa
(Fleiss, 1971) was 0.346 for constancy classification
and was 0.428 for uniqueness classification. They
indicate fair and moderate agreement, respectively
(Landis and Koch, 1977).
We have briefly investigated the relations whose
labels assigned by the annotators conflicted. The
major cause was that the annotators sometimes as-
sumed different types of named entities as values
of arguments. A typical case in which this problem
arises is that the relation has polysemous meanings,
e.g., ?arg1 was born in arg2?, or a vague meaning,
e.g., ?arg1makes arg2?. For example, arg2 of ?arg1
was born in arg2? can be filled with different types
of entities such as date and place. We can address
this problem by typing arguments (Lin et al2010).
5.2 Result
Using the dataset, we performed 5-fold cross-
validation for both classification tasks. We used
the passive-aggressive algorithm for our classifier
(Crammer et al2006).
Constancy classification Figure 2 illustrates the
recall-precision curve in constancy classification.
Because we are unaware of any previous methods
for classifying constant and non-constant relations,
a simple method based on the cosine similarity was
888
00.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
Proposed
Baseline
Figure 3: Recall-precision curve (uniqueness classifica-
tion).
used as a baseline:
1
N
?
e?EN (r)
cos(Fw1(r, e), Fw2(r, e)),
where the time windows w1 and w2 are determined
as the first and last month in which the relation r
is observed. A given relation is classified as non-
constant if the above similarity exceeds a threshold.
The recall-precision curve was drawn by changing
the threshold.
The results demonstrated that our method outper-
forms the baseline. This indicates the effectiveness
of using time-series frequency and linguistic cues as
features.
The poor performance of the baseline was mainly
due to data sparseness. Since the baseline method is
dependent on the frequency estimates obtained from
only two months of texts, it is less reliable than the
proposed method.
Uniqueness classification Figure 3 illustrates the
recall-precision curve in uniqueness classification.
As a baseline we implemented the method proposed
by Lin et al2010). While they have presented
three methods (KLFUNC, KLDIFF, and their aver-
age), we report the results of the last one because it
performed the best among the three in our experi-
ment.
From the figure, we can again see that the pro-
posed method outperforms the baseline method.
Lin?s method is similar to ours, but differs in that
they do not exploit time-series information at all.
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
N = 2N = 10N = 20N = 100
Figure 4: Comparison with the methods varying a value
of N for constancy classification.
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
N = 2N = 10N = 20N = 100
Figure 5: Comparison with the methods varying a value
of N for uniqueness classification.
We hence conclude time-series information is use-
ful for classifying not only constant but also unique
relations.
5.3 Investigation into the number of entities, N
We ranged the value of N in {2, 10, 20, 100}. Set-
ting N to a larger value yields the better recall for
constancy classification and the better precision for
uniqueness classification (Figures 4 and 5). These
results meet our expectations, since features derived
from frequency distributions of arg2 over various
arg1s capture the generic nature of the target rela-
tion.
889
00.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
T = 1, 3, 6, 12T = 1T = 3T = 6T = 12
Figure 6: Comparison with the methods using only a sin-
gle value of T for constancy classification.
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
T = 1, 3, 6, 12T = 1T = 3T = 6T = 12
Figure 7: Comparison with the methods using only a sin-
gle value of T for uniqueness classification.
5.4 Investigation into the window size, T
Our method uses multiple time windows of different
sizes (i.e., different values of T ) to induce features,
as detailed in sections 3.1 and 4.1. To confirm the
effect of this technique, we investigated the perfor-
mance when we use only a single value of T (Fig-
ures 6 and 7).
The results in the uniqueness classification task
demonstrated that our method achieves better over-
all results than the methods using a single value of
T . We can therefore consider that using multiple
values of T as features is a reasonable strategy. On
the other hand, we could not confirm the effect of
using multiple time windows of different sizes in the
constancy classification task.
5.5 Error analysis
We randomly selected and analyzed 200 misclassi-
fied relations for both tasks. The analysis revealed
four types of errors.
Paraphrases We observed that constant relations
are prone to be miss-classified as non-constant when
more than one paraphrase appear in arg2 and thus
the value of arg2 is pretended to change. For exam-
ple, America was also referred to as USA or United
States of America. A similar problem was observed
for unique relations as well.
Topical bias Topics mentioned in the blog posts
are sometimes biased, and such bias can have a neg-
ative effect on classification, especially when a rela-
tion takes a small number of entity types in arg2 for
given arg1. For example, Jaden Smith, who is one
of Will Smith?s sons, is frequently mentioned in our
time-series text because he co-starred with his father
in a movie, while Will Smith?s other sons never ap-
peared in our text. We consider this a possible rea-
son for our method wrongly identifying ?arg1 ?s son
is arg2? as a unique relation.
Short-/Long-term evolution Since we have ag-
gregated on a monthly basis the 6-year?s worth of
blog posts, the induced features cannot capture evo-
lutions that occur in shorter or longer intervals. For
example, consider relation ?arg1 beats arg2? tak-
ing Real Madrid as arg1. Since Real Madrid usually
have more than one football match in a month, they
can beat several teams in a month, which misleads
the classifier to recognize the relation as non-unique.
Similarly when a relation takes more than 6 years to
evolve, it will be regarded as constant.
Reference to past, future, or speculative facts
The blog authors sometimes refer to relations that do
not occur around when they write their posts; such
relations actually occurred in the past, will occur in
the future, or even speculative. Since our method
exploits the time stamps attached to the posts to as-
sociate the relations with time, those relations in-
troduce noises in the frequency distributions. Al-
though our robust feature induction could in most
cases avoid an adverse effect caused by these noises,
they sometimes leaded to misclassification.
890
6 Related Work
In recent years, much attention has been given to
extracting relations from a massive amount of tex-
tual data, especially the web (cf. section 1). Most of
those studies, however, explored just extracting re-
lations from text. Only a few studies, as described
below, have discussed classifying those relations.
There has been no previous work on identify-
ing the constancy of relations. The most relevant
research topic is the temporal information extrac-
tion (Verhagen et al2007; Verhagen et al2010;
Ling and Weld, 2010; Wang et al2010; Hovy et
al., 2012). This is the task of extracting from textual
data an event and the time it happened, e.g., Othello
was written by Shakespeare in 1602. Such tempo-
ral information alone is not sufficient for identifying
the constancy of relations, while we think it would
be helpful.
On the other hand, the uniqueness of relations has
so far been discussed in some studies. Ritter et al
(2008) have pointed out the importance of identi-
fying unique relations for various NLP tasks such
as contradiction detection, quantifier scope disam-
biguation, and synonym resolution. They proposed
an EM-style algorithm for scoring the uniqueness
of relations. Lin et al2010) also proposed three
algorithms for identifying unique relations. While
those studies discussed the same problem as this pa-
per, they did not point out the importance of the
constancy in identifying unique relations (cf. sec-
tion 4.1).
7 Conclusion
This paper discussed that the notion of constancy
is essential in compiling relations between enti-
ties extracted from real-world text and proposed a
method for classifying relations on the basis of con-
stancy and uniqueness. The time-series web text
was fully exploited to induce frequency-based fea-
tures from time-series frequency distribution on re-
lation instances as well as language-based features
tailored for individual classification tasks. Exper-
imental results confirmed that the frequency-based
features contributed much to the precision and recall
in both identification tasks.
We will utilize the identified properties of the re-
lations to adopt an appropriate strategy to compile
their instances. We also plan to start a spin-off re-
search that acquires paraphrases by grouping values
of arg2s for each value of arg1 in a constant, unique
relation.
We consider that the notion of constancy will even
be beneficial in acquiring world knowledge, other
than relations between entities, from text; we aim
at extending the notion of constancy to other types
of knowledge involving real-world entities, such as
concept-instance relations.
Acknowledgments
This work was supported by the Multimedia Web
Analysis Framework towards Development of So-
cial Analysis Software program of the Ministry of
Education, Culture, Sports, Science and Technol-
ogy, Japan. The authors thank the annotators for
their hard work. The authors are also indebted to the
three anonymous reviewers for their valuable com-
ments.
References
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI, pages 2670?2676.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shawartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?583.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John Prager,
Nico Schlaefer, and Chris Welty. 2010. Building Wat-
son: An overview of the DeepQA project. AI Maga-
zine, 31(3):59?79.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Patward-
han, and Christopher Welty. 2012. When did that hap-
pen? ? linking events and relations to timestamps. In
Proceedings of EACL, pages 185?193.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 1(33):159?174.
Thomas Lin, Mausam, and Oren Etzioni. 2010. Identify-
ing functional relation in web text. In Proceedings of
EMNLP, pages 1266?1276.
891
Xiao Ling and Daniel S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of AAAI, pages 1385?
1390.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-IJCNLP,
pages 1003?1011.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of ACL, pages
113?120.
Alan Ritter, Doug Downey, Stephen Soderland, and Oren
Etzioni. 2008. It?s a contradiction?no, it?s not: A
case study using functional relations. In Proceedings
of EMNLP, pages 11?20.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge unifying WordNet and Wikipedia. In Proceed-
ings of WWW, pages 697?706.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal
relation identification. In Proceedings of SemEval,
pages 75?80.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 task 13:
TempEval-2. In Proceedings of SemEval, pages 57?
62.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely YAGO: har-
vesting, querying, and visualizing temporal knowledge
from Wikipedia. In Proceedings of EDBT, pages 697?
700.
Gerhard Weikum, Srikanta Bedathur, and Ralf Schenkel.
2011. Temporal knowledge for timely intelligence. In
Proceedings of BIRTE, pages 1?6.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of ACL,
pages 118?127.
Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008.
Information extraction from Wikipedia: moving down
the long tail. In Proceedings of KDD, pages 731?739.
Naoki Yoshinaga and Masaru Kitsuregawa. 2009. Poly-
nomial to linear: Efficient classification with conjunc-
tive features. In Proceedings of EMNLP, pages 1542?
1551.
Naoki Yoshinaga andMasaru Kitsuregawa. 2010. Kernel
slicing: Scalable online training with conjunctive fea-
tures. In Proceedings of COLING, pages 1245?1253.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-
Rong Wen. 2009. StatSnowball: a statistical approach
to extracting entity relationships. In Proceedings of
WWW, pages 101?110.
892
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 485?494,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Staggered Decoding for Sequence Labeling
Nobuhiro Kaji Yasuhiro Fujiwara Naoki Yoshinaga Masaru Kitsuregawa
Institute of Industrial Science,
The University of Tokyo,
4-6-1, Komaba, Meguro-ku, Tokyo, 153-8505 Japan
{kaji,fujiwara,ynaga,kisture}@tkl.iis.u-tokyo.ac.jp
Abstract
The Viterbi algorithm is the conventional
decoding algorithm most widely adopted
for sequence labeling. Viterbi decoding
is, however, prohibitively slow when the
label set is large, because its time com-
plexity is quadratic in the number of la-
bels. This paper proposes an exact decod-
ing algorithm that overcomes this prob-
lem. A novel property of our algorithm is
that it efficiently reduces the labels to be
decoded, while still allowing us to check
the optimality of the solution. Experi-
ments on three tasks (POS tagging, joint
POS tagging and chunking, and supertag-
ging) show that the new algorithm is sev-
eral orders of magnitude faster than the
basic Viterbi and a state-of-the-art algo-
rithm, CARPEDIEM (Esposito and Radi-
cioni, 2009).
1 Introduction
In the past decade, sequence labeling algorithms
such as HMMs, CRFs, and Collins? perceptrons
have been extensively studied in the field of NLP
(Rabiner, 1989; Lafferty et al, 2001; Collins,
2002). Now they are indispensable in a wide range
of NLP tasks including chunking, POS tagging,
NER and so on (Sha and Pereira, 2003; Tsuruoka
and Tsujii, 2005; Lin and Wu, 2009).
One important task in sequence labeling is how
to find the most probable label sequence from
among all possible ones. This task, referred to as
decoding, is usually carried out using the Viterbi
algorithm (Viterbi, 1967). The Viterbi algorithm
has O(NL2) time complexity,1 where N is the
input size and L is the number of labels. Al-
though the Viterbi algorithm is generally efficient,
1The first-order Markov assumption is made throughout
this paper, although our algorithm is applicable to higher-
order Markov models as well.
it becomes prohibitively slow when dealing with
a large number of labels, since its computational
cost is quadratic in L (Dietterich et al, 2008).
Unfortunately, several sequence-labeling prob-
lems in NLP involve a large number of labels. For
example, there are more than 40 and 2000 labels
in POS tagging and supertagging, respectively
(Brants, 2000; Matsuzaki et al, 2007). These
tasks incur much higher computational costs than
simpler tasks like NP chunking. What is worse,
the number of labels grows drastically if we jointly
perform multiple tasks. As we shall see later,
we need over 300 labels to reduce joint POS tag-
ging and chunking into the single sequence label-
ing problem. Although joint learning has attracted
much attention in recent years, how to perform de-
coding efficiently still remains an open problem.
In this paper, we present a new decoding algo-
rithm that overcomes this problem. The proposed
algorithm has three distinguishing properties: (1)
It is much more efficient than the Viterbi algorithm
when dealing with a large number of labels. (2) It
is an exact algorithm, that is, the optimality of the
solution is always guaranteed unlike approximate
algorithms. (3) It is automatic, requiring no task-
dependent hyperparameters that have to be manu-
ally adjusted.
Experiments evaluate our algorithm on three
tasks: POS tagging, joint POS tagging and chunk-
ing, and supertagging2. The results demonstrate
that our algorithm is up to several orders of mag-
nitude faster than the basic Viterbi algorithm and a
state-of-the-art algorithm (Esposito and Radicioni,
2009); it makes exact decoding practical even in
labeling problems with a large label set.
2 Preliminaries
We first provide a brief overview of sequence la-
beling and introduce related work.
2Our implementation is available at http://www.tkl.iis.u-
tokyo.ac.jp/?kaji/staggered
485
2.1 Models
Sequence labeling is the problem of predicting la-
bel sequence y = {yn}Nn=1 for given token se-
quence x = {xn}Nn=1. This is typically done by
defining a score function f(x,y) and locating the
best label sequence: ymax = argmax
y
f(x,y).
The form of f(x,y) is dependent on the learn-
ing model used. Here, we introduce two models
widely used in the literature.
Generative models HMM is the most famous
generative model for labeling token sequences
(Rabiner, 1989). In HMMs, the score function
f(x,y) is the joint probability distribution over
(x,y). If we assume a one-to-one correspondence
between the hidden states and the labels, the score
function can be written as:
f(x,y) = log p(x,y)
= log p(x|y) + log p(y)
=
N
?
n=1
log p(xn|yn)+
N
?
n=1
log p(yn|yn?1).
The parameters log p(xn|yn) and log p(yn|yn?1)
are usually estimated using maximum likelihood
or the EM algorithm. Since parameter estimation
lies outside the scope of this paper, a detailed de-
scription is omitted.
Discriminative models Recent years have seen
the emergence of discriminative training methods
for sequence labeling (Lafferty et al, 2001; Tasker
et al, 2003; Collins, 2002; Tsochantaridis et al,
2005). Among them, we focus on the perceptron
algorithm (Collins, 2002). Although we do not
discuss the other discriminative models, our algo-
rithm is equivalently applicable to them. The ma-
jor difference between those models lies in param-
eter estimation; the decoding process is virtually
the same.
In the perceptron, the score function f(x,y) is
given as f(x,y) = w ? ?(x,y) where w is the
weight vector, and ?(x,y) is the feature vector
representation of the pair (x,y). By making the
first-order Markov assumption, we have
f(x,y) = w ? ?(x,y)
=
N
?
n=1
K
?
k=1
wk?k(x, yn?1, yn),
where K = |?(x,y)| is the number of features, ?k
is the k-th feature function, and wk is the weight
corresponding to it. Parameter w can be estimated
in the same way as in the conventional perceptron
algorithm. See (Collins, 2002) for details.
2.2 Viterbi decoding
Given the score function f(x,y), we have to lo-
cate the best label sequence. This is usually per-
formed by applying the Viterbi algorithm. Let
?(yn) be the best score of the partial label se-
quence ending with yn. The idea of the Viterbi
algorithm is to use dynamic programming to com-
pute ?(yn). In HMMs, ?(yn) can be can be de-
fined as
max
y
n?1
{?(yn?1) + log p(yn|yn?1)} + log p(xn|yn).
Using this recursive definition, we can evaluate
?(yn) for all yn. This results in the identification
of the best label sequence.
Although the Viterbi algorithm is commonly
adopted in past studies, it is not always efficient.
The computational cost of the Viterbi algorithm is
O(NL2), where N is the input length and L is
the number of labels; it is efficient enough if L
is small. However, if there are many labels, the
Viterbi algorithm becomes prohibitively slow be-
cause of its quadratic dependence on L.
2.3 Related work
To the best of our knowledge, the Viterbi algo-
rithm is the only algorithm widely adopted in the
NLP field that offers exact decoding. In other
communities, several exact algorithms have al-
ready been proposed for handling large label sets.
While they are successful to some extent, they de-
mand strong assumptions that are unusual in NLP.
Moreover, none were challenged with standard
NLP tasks.
Felzenszwalb et al (2003) presented a fast
inference algorithm for HMMs based on the as-
sumption that the hidden states can be embed-
ded in a grid space, and the transition probabil-
ity corresponds to the distance on that space. This
type of probability distribution is not common in
NLP tasks. Lifshits et al (2007) proposed a
compression-based approach to speed up HMM
decoding. It assumes that the input sequence is
highly repetitive. Amongst others, CARPEDIEM
(Esposito and Radicioni, 2009) is the algorithm
closest to our work. It accelerates decoding by
assuming that the adjacent labels are not strongly
correlated. This assumption is appropriate for
486
some NLP tasks. For example, as suggested in
(Liang et al, 2008), adjacent labels do not provide
strong information in POS tagging. However, the
applicability of this idea to other NLP tasks is still
unclear.
Approximate algorithms, such as beam search
or island-driven search, have been proposed for
speeding up decoding. Tsuruoka and Tsujii (2005)
proposed easiest-first deterministic decoding. Sid-
diqi and Moore (2005) presented the parameter ty-
ing approach for fast inference in HMMs. A simi-
lar idea was applied to CRFs as well (Cohn, 2006;
Jeong et al, 2009).
In general, approximate algorithms have the ad-
vantage of speed over exact algorithms. However,
both types of algorithms are still widely adopted
by practitioners, since exact algorithms have mer-
its other than speed. First, the optimality of the so-
lution is always guaranteed. It is hard for most of
the approximate algorithms to even bound the er-
ror rate. Second, approximate algorithms usually
require hyperparameters, which control the trade-
off between accuracy and efficiency (e.g., beam
width), and these have to be manually adjusted.
On the other hand, most of the exact algorithms,
including ours, do not require such a manual ef-
fort.
Despite these advantages, exact algorithms are
rarely used when dealing with a large number of
labels. This is because exact algorithms become
considerably slower than approximate algorithms
in such situations. The paper presents an exact al-
gorithm that avoids this problem; it provides the
research community with another option for han-
dling a lot of labels.
3 Algorithm
This section presents the new decoding algorithm.
The key is to reduce the number of labels ex-
amined. Our algorithm locates the best label se-
quence by iteratively solving labeling problems
with a reduced label set. This results in signifi-
cant time savings in practice, because each itera-
tion becomes much more efficient than solving the
original labeling problem. More importantly, our
algorithm always obtains the exact solution. This
is because the algorithm allows us to check the op-
timality of the solution achieved by using only the
reduced label set.
In the following discussions, we restrict our fo-
cus to HMMs for presentation clarity. Extension to
A A A A
B B B B
C
D
C
D
C
D
C
D
E E E E
F F F F
G G G G
H H H H
(a)
A
B
C
D
A A
B
A
B
C
D
(b)
Figure 1: (a) An example of a lattice, where the
letters {A, B, C, D, E, F, G, H} represent labels
associated with nodes. (b) The degenerate lattice.
the perceptron algorithm is presented in Section 4.
3.1 Degenerate lattice
We begin by introducing the degenerate lattice,
which plays a central role in our algorithm. Con-
sider the lattice in Figure 1(a). Following conven-
tion, we regard each path on the lattice as a label
sequence. Note that the label set is {A, B, C, D,
E, F, G, H}. By aggregating several nodes in the
same column of the lattice, we can transform the
original lattice into a simpler form, which we call
the degenerate lattice (Figure 1(b)).
Let us examine the intuition behind the degen-
erate lattice. Aggregating nodes can be viewed as
grouping several labels into a new one. Here, a
label is referred to as an active label if it is not ag-
gregated (e.g., A, B, C, and D in the first column
of Figure 1(b)), and otherwise as an inactive label
(i.e., dotted nodes). The new label, which is made
by grouping the inactive labels, is referred to as
a degenerate label (i.e., large nodes covering the
dotted ones). Two degenerate labels can be seen
as equivalent if their corresponding inactive label
sets are the same (e.g., degenerate labels in the first
and the last column). In this approach, each path
of the degenerate lattice can also be interpreted as
a label sequence. In this case, however, the label to
be assigned is either an active label or a degenerate
label.
We then define the parameters associated with
degenerate label z. For reasons that will become
clear later, they are set to the maxima among the
parameters of the inactive labels:
log p(x|z) = max
y??I(z)
log p(x|y?), (1)
log p(z|y) = max
y??I(z)
log p(y?|y), (2)
log p(y|z) = max
y??I(z)
log p(y|y?), (3)
log p(z|z?) = max
y??I(z),y???I(z?)
log p(y?|y??), (4)
487
A A A A
B B B B
C
D
C
D
C
D
C
D
E E E E
F F F F
G G G G
H H H H
(a)
A
B
C
D
A A
B
A
B
C
D
(b)
Figure 2: (a) The path y = {A, E, G, C} of the
original lattice. (b) The path z of the degenerate
lattice that corresponds to y.
where y is an active label, z and z? are degenerate
labels, and I(z) denotes one-to-one mapping from
z to its corresponding inactive label set.
The degenerate lattice has an important prop-
erty which is the key to our algorithm:
Lemma 1. If the best path of the degenerate lat-
tice does not include any degenerate label, it is
equivalent to the best path of the original lattice.
Proof. Let zmax be the best path of the degenerate
lattice. Our goal is to prove that if zmax does not
include any degenerate label, then
?y ? Y, log p(x,y) ? log p(x,zmax) (5)
where Y is the set of all paths on the original lat-
tice. We prove this by partitioning Y into two dis-
joint sets: Y
0
and Y
1
, where Y
0
is the subset of
Y appearing in the degenerate lattice. Notice that
zmax ? Y0. Since zmax is the best path of the
degenerate lattice, we have
?y ? Y
0
, log p(x,y) ? log p(x,zmax).
The equation holds when y = zmax. We next ex-
amine the label sequence y such that y ? Y
1
. For
each path y ? Y
1
, there exists a unique path z on
the degenerate lattice that corresponds to y (Fig-
ure 2). Therefore, we have
?y ? Y
1
, ?z ? Z, log p(x,y) ? log p(x,z)
< log p(x,zmax)
where Z is the set of all paths of the degenerate
lattice. The inequality log p(x,y) ? log p(x,z)
can be proved by using Equations (1)-(4). Using
these results, we can complete (5).
A A A A
(a)
A A
B
A
B
A
BB
(b)
A A
B
C
D
A
B
A
B
C
D
B
C
D
C
D
(c)
Figure 3: (a) The best path of the initial degenerate
lattice, which is denoted by the line, is located. (b)
The active labels are expanded and the best path is
searched again. (c) The best path without degen-
erate labels is obtained.
3.2 Staggered decoding
Nowwe can describe our algorithm, which we call
staggered decoding. The algorithm successively
constructs degenerate lattices and checks whether
the best path includes degenerate labels. In build-
ing each degenerate lattice, labels with high prob-
ability p(y), estimated from training data, are pref-
erentially selected as the active label; the expecta-
tion is that such labels are likely to belong to the
best path. The algorithm is detailed as follows:
Initialization step The algorithm starts by build-
ing a degenerate lattice in which there is only
one active label in each column. We select la-
bel y with the highest p(y) as the active label.
Search step The best path of the degenerate lat-
tice is located (Figure 3(a)). This is done
by using the Viterbi algorithm (and pruning
technique, as we describe in Section 3.3). If
the best path does not include any degenerate
label, we can terminate the algorithm since it
is identical with the best path of the original
lattice according to Lemma 1. Otherwise, we
proceed to the next step.
Expansion step We double the number of the ac-
tive labels in the degenerate lattice. The new
active labels are selected from the current in-
active label set in descending order of p(y).
If the inactive label set becomes empty, we
simply reconstructed the original lattice. Af-
ter expanding the active labels, we go back to
the previous step (Figure 3(b)). This proce-
dure is repeated until the termination condi-
tion in the search step is satisfied, i.e., the best
path has no degenerate label (Figure 3(c)).
Compared to the Viterbi algorithm, staggered
decoding requires two additional computations for
488
training. First, we have to estimate p(y) so as to
select active labels in the initialization and expan-
sion step. Second, we have to compute the pa-
rameters regarding degenerate labels according to
Equations (1)-(4). Both impose trivial computa-
tion costs.
3.3 Pruning
To achieve speed-up, it is crucial that staggered
decoding efficiently performs the search step. For
this purpose, we can basically use the Viterbi algo-
rithm. In earlier iterations, the Viterbi algorithm is
indeed efficient because the label set to be han-
dled is much smaller than the original one. In later
iterations, however, our algorithm drastically in-
creases the number of labels, making Viterbi de-
coding quite expensive.
To handle this problem, we propose a method of
pruning the lattice nodes. This technique is moti-
vated by the observation that the degenerate lattice
shares many active labels with the previous itera-
tion. In the remainder of Section3.3, we explain
the technique by taking the following steps:
? Section 3.3.1 examines a lower bound l such
that l ? max
y
log p(x,y).
? Section 3.3.2 examines the maximum score
MAX(yn) in case token xn takes label yn:
MAX(yn) = max
y?
n
=y
n
log p(x,y?).
? Section 3.3.3 presents our pruning procedure.
The idea is that if MAX(yn) < l, then the
node corresponding to yn can be removed
from consideration.
3.3.1 Lower bound
Lower bound l can be trivially calculated in the
search step. This can be done by retaining the
best path among those consisting of only active
labels. The score of that path is obviously the
lower bound. Since the search step is repeated un-
til the termination criteria is met, we can update
the lower bound at every search step. As the it-
eration proceeds, the degenerate lattice becomes
closer to the original one, so the lower bound be-
comes tighter.
3.3.2 Maximum score
The maximum score MAX(yn) can be computed
from the original lattice. Let ?(yn) be the best
score of the partial label sequence ending with yn.
Presuming that we traverse the lattice from left to
right, ?(yn) can be defined as
max
y
n?1
{?(yn?1) + log p(yn|yn?1)} + log p(xn|yn).
If we traverse the lattice from right to left, an anal-
ogous score ??(yn) can be defined as
log p(xn|yn) + max
y
n+1
{??(yn+1) + log p(yn|yn+1)}.
Using these two scores, we have
MAX(yn) = ?(yn) + ??(yn) ? log p(xn|yn).
Notice that updating ?(yn) or ??(yn) is equivalent
to the forward or backward Viterbi algorithm, re-
spectively.
Although it is expensive to compute ?(yn) and
??(yn), we can efficiently estimate their upper
bounds. Let ?(yn) and ??(yn) be scores analogous
to ?(yn) and ??(yn) that are computed using the
degenerate lattice. We have ?(yn) ? ?(yn) and
??(yn) ? ??(yn), by following similar discussions
as raised in the proof of Lemma 1. Therefore, we
can still check whether MAX(yn) is smaller than l
by using ?(yn) and ??(yn):
MAX(yn) = ?(yn) + ??(yn)? log p(xn|yn)
? ?(yn) + ??(yn) ? log p(xn|yn)
< l.
For the sake of simplicity, we assume that yn is an
active label. Although we do not discuss the other
cases, our pruning technique is also applicable to
them. We just point out that, if yn is an inactive
label, then there exists a degenerate label zn in the
n-th column such that yn ? I(zn), and we can use
?(zn) and ??(zn) instead of ?(yn) and ??(yn).
We compute ?(yn) and ??(yn) by using the
forward and backward Viterbi algorithm, respec-
tively. In the search step immediately following
initialization, we perform the forward Viterbi al-
gorithm to find the best path, that is, ?(yn) is
updated for all yn. In the next search step, the
backward Viterbi algorithm is carried out, and
??(yn) is updated. In the succeeding search steps,
these updates are alternated. As the algorithm pro-
gresses, ?(yn) and ??(yn) become closer to ?(yn)
and ??(yn).
3.3.3 Pruning procedure
We make use of the bounds in pruning the lattice
nodes. To do this, we keep the values of l, ?(yn)
489
and ??(yn). They are set as l = ?? and ?(yn) =
??(yn) = ? in the initialization step, and are up-
dated in the search step. The lower bound l is up-
dated at the end of the search step, while ?(yn)
and ??(yn) can be updated during the running of
the Viterbi algorithm. When ?(yn) or ??(yn) is
changed, we check whether MAX(yn) < l holds
and the node is pruned if the condition is met.
3.4 Analysis
We provide here a theoretical analysis of staggered
decoding. In the following proofs, L, V , and N
represent the number of original labels, the num-
ber of distinct tokens, and the length of input token
sequence, respectively. To simplify the discussion,
we assume that log
2
L is an integer (e.g., L = 64).
We first introduce three lemmas:
Lemma 2. Staggered decoding requires at most
(log
2
L + 1) iterations to terminate.
Proof. We have 2m?1 active labels in the m-th
search step (m = 1, 2 . . . ), which means we have
L active labels and no degenerate labels in the
(log
2
L + 1)-th search step. Therefore, the algo-
rithm always terminates within (log
2
L + 1) itera-
tions.
Lemma 3. The number of degenerate labels is
log
2
L.
Proof. Since we create one new degenerate label
in all but the last expansion step, we have log
2
L
degenerate labels.
Lemma 4. The Viterbi algorithm requires O(L2+
LV ) memory space and has O(NL2) time com-
plexity.
Proof. Since we need O(L2) and O(LV ) space to
keep the transition and emission probability ma-
trices, we need O(L2 + LV ) space to perform
the Viterbi algorithm. The time complexity of the
Viterbi algorithm is O(NL2) since there are NL
nodes in the lattice and it takes O(L) time to eval-
uate the score of each node.
The above statements allow us to establish our
main results:
Theorem 1. Staggered decoding requires O(L2+
LV ) memory space.
Proof. Since we have L original labels and log
2
L
degenerate labels, staggered decoding requires
O((L+log
2
L)2+(L+log
2
L)V ) = O(L2+LV )
A A A A
(a)
A A
B
A
B
A
B
(b)
A A
B
C
D
A
B
A
B
C
D
(c)
Figure 4: Staggered decoding with column-wise
expansion: (a) The best path of the initial degen-
erate lattice, which does not pass through the de-
generate label in the first column. (b) Column-
wise expansion is performed and the best path is
searched again. Notice that the active label in the
first column is not expanded. (c) The final result.
memory space to perform Viterbi decoding in the
search step.
Theorem 2. Staggered decoding has O(N) best
case time complexity and O(NL2)worst case time
complexity.
Proof. To perform the m-th search step, staggered
decoding requires the order of O(N4m?1) time
because we have 2m?1 active labels. Therefore, it
has O(
?M
m=1 N4
m?1) time complexity if it termi-
nates after the M -th search step. In the best case,
M = 1, the time complexity is O(N). In the worst
case, M = log
2
L + 1, the time complexity is the
order of O(NL2) because
?
log
2
L+1
m=1 N4
m?1 <
4
3
NL2.
Theorem 1 shows that staggered decoding
asymptotically requires the same order of mem-
ory space as the Viterbi algorithm. Theorem 2 re-
veals that staggered decoding has the same order
of time complexity as the Viterbi algorithm even
in the worst case.
3.5 Heuristic techniques
We present two heuristic techniques for further
speeding up our algorithm.
First, we can initialize the value of lower bound
l by selecting a path from the original lattice in
some way, and then computing the score of that
path. In our experiments, we use the path lo-
cated by the left-to-right deterministic decoding
(i.e., beam search with a beam width of 1). Al-
though this method requires an additional cost to
locate the path, it is very effective in practice. If
l is initialized in this manner, the best case time
complexity of our algorithm becomes O(NL).
490
The second technique is for the expansion step.
Instead of the expansion technique described in
Section 3.2, we can expand the active labels in a
heuristic manner to keep the number of active la-
bels small:
Column-wise expansion step We double the
number of the active labels in the column
only if the best path of the degenerate lattice
passes through the degenerate label of that
column (Figure 4).
A drawback of this strategy is that the algorithm
requires N(log
2
L+1) iterations in the worst case.
As the result, we can no longer derive a reasonable
upper bound for the time complexity. Neverthe-
less, column-wise expansion is highly effective in
practice as we will demonstrate in the experiment.
Note that Theorem 1 still holds true even if we use
column-wise expansion.
4 Extension to the Perceptron
The discussion we have made so far can be applied
to perceptrons. This can be clarified by comparing
the score functions f(x,y). In HMMs, the score
function can be written as
N
?
n=1
{
log(xn|yn) + log(yn|yn?1)
}
.
In perceptrons, on the other hand, it is given as
N
?
n=1
{
?
k
w1k?
1
k(x, yn) +
?
k
w2k?
2
k(x, yn?1, yn)
}
where we explicitly distinguish the unigram fea-
ture function ?1k and bigram feature function ?2k.
Comparing the form of the two functions, we can
see that our discussion on HMMs can be extended
to perceptrons by substituting
?
k w
1
k?
1
k(x, yn)
and
?
k w
2
k?
2
k(x, yn?1, yn) for log p(xn|yn) and
log p(yn|yn?1).
However, implementing the perceptron algo-
rithm is not straightforward. The problem is
that it is difficult, if not impossible, to compute
?
k w
1
k?
1
k(x, y) and
?
k w
2
k?
2
k(x, y, y
?) offline be-
cause they are dependent on the entire token se-
quence x, unlike log p(x|y) and log p(y|y?). Con-
sequently, we cannot evaluate the maxima analo-
gous to Equations (1)-(4) offline either.
For unigram features, we compute the maxi-
mum, maxy
?
k w
1
k?
1
k(x, y), as a preprocess in
the initialization step (cf. Equation (1)). This pre-
process requires O(NL) time, which is negligible
compared with the cost required by the Viterbi al-
gorithm.
Unfortunately, we cannot use the same tech-
nique for computing maxy,y?
?
k w
2
k?
2
k(x, y, y
?)
because a similar computation would take
O(NL2) time (cf. Equation (4)). For bigram fea-
tures, we compute its upper bound offline. For ex-
ample, the following bound was proposed by Es-
posito and Radicioni (2009):
max
y,y?
?
k
w2k?
2
k(x, y, y
?) ? max
y,y?
?
k
w2k?(0 < w
2
k)
where ?(?) is the delta function and the summa-
tions are taken over all feature functions associated
with both y and y?. Intuitively, the upper bound
corresponds to an ideal case in which all features
with positive weight are activated.3 It can be com-
puted without any task-specific knowledge.
In practice, however, we can compute better
bounds based on task-specific knowledge. The
simplest case is that the bigram features are inde-
pendent of the token sequence x. In such a situ-
ation, we can trivially compute the exact maxima
offline, as we did in the case of HMMs. Fortu-
nately, such a feature set is quite common in NLP
problems and we could use this technique in our
experiments. Even if bigram features are depen-
dent on x, it is still possible to compute better
bounds if several features are mutually exclusive,
as discussed in (Esposito and Radicioni, 2009).
Finally, it is worth noting that we can use stag-
gered decoding in training perceptrons as well, al-
though such application lies outside the scope of
this paper. The algorithm does not support train-
ing acceleration for other discriminative models.
5 Experiments and Discussion
5.1 Setting
The proposed algorithm was evaluated with three
tasks: POS tagging, joint POS tagging and chunk-
ing (called joint tagging for short), and supertag-
ging. To reduce joint tagging into a single se-
quence labeling problem, we produced the labels
by concatenating the POS tag and the chunk tag
(BIO format), e.g., NN/B-NP. In the two tasks
other than supertagging, the input token is the
word. In supertagging, the token is the pair of the
word and its oracle POS tag.
3We assume binary feature functions.
491
Table 1: Decoding speed (sent./sec).
POS tagging Joint tagging Supertagging
VITERBI 4000 77 1.1
CARPEDIEM 8600 51 0.26
SD 8800 850 121
SD+C-EXP. 14,000 1600 300
The data sets we used for the three experiments
are the Penn TreeBank (PTB) corpus, CoNLL
2000 corpus, and an HPSG treebank built from the
PTB corpus (Matsuzaki et al, 2007). We used sec-
tions 02-21 of PTB for training, and section 23 for
testing. The number of labels in the three tasks is
45, 319 and 2602, respectively.
We used the perceptron algorithm for train-
ing. The models were averaged over 10 itera-
tions (Collins, 2002). For features, we basically
followed previous studies (Tsuruoka and Tsujii,
2005; Sha and Pereira, 2003; Ninomiya et al,
2006). In POS tagging, we used unigrams of the
current and its neighboring words, word bigrams,
prefixes and suffixes of the current word, capital-
ization, and tag bigrams. In joint tagging, we also
used the same features. In supertagging, we used
POS unigrams and bigrams in addition to the same
features other than capitalization.
As the evaluation measure, we used the average
decoding speed (sentences/sec) to two significant
digits over five trials. To strictly measure the time
spent for decoding, we ignored the preprocessing
time, that is, the time for loading the model file
and converting the features (i.e., strings) into inte-
gers. We note that the accuracy was comparable to
the state-of-the-art in the three tasks: 97.08, 93.21,
and 91.20% respectively.
5.2 Results and discussions
Table 1 presents the performance of our algo-
rithm. SD represents the proposed algorithm with-
out column-wise expansion, while SD+C-EXP.
uses column-wise expansion. For comparison, we
present the results of two baseline algorithms as
well: VITERBI and CARPEDIEM (Esposito and
Radicioni, 2009). In almost all settings, we see
that both of our algorithms outperformed the other
two. We also find that SD+C-EXP. performed con-
sistently better than SD. This indicates the effec-
tiveness of column-wise expansion.
Following VITERBI, CARPEDIEM is the most
relevant algorithm, for sequence labeling in NLP,
as discussed in Section 2.3. However, our results
Table 2: The average number of iterations.
POS tagging Joint tagging Supertagging
SD 6.02 8.15 10.0
SD+C-EXP. 6.12 8.62 10.6
Table 3: Training time.
POS tagging Joint tagging Supertagging
VITERBI 100 sec. 20 min. 100 hour
SD+C-EXP. 37 sec. 1.5 min. 5.3 hour
demonstrated that CARPEDIEM worked poorly in
two of the three tasks. We consider this is because
the transition information is crucial for the two
tasks, and the assumption behind CARPEDIEM is
violated. In contrast, the proposed algorithms per-
formed reasonably well for all three tasks, demon-
strating the wide applicability of our algorithm.
Table 2 presents the average iteration num-
bers of SD and SD+C-EXP. We can observe
that the two algorithms required almost the same
number of iterations on average, although the
iteration number is not tightly bounded if we
use column-wise expansion. This indicates that
SD+C-EXP. virtually avoided performing extra it-
erations, while heuristically restricting active label
expansion.
Table 3 compares the training time spent by
VITERBI and SD+C-EXP. Although speeding up
perceptron training is a by-product, it is interest-
ing to see that our algorithm is in fact effective at
reducing the training time as well. The result also
indicates that the speed-up is more significant at
test time. This is probably because the model is
not predictive enough at the beginning of training,
and the pruning is not that effective.
5.3 Comparison with approximate algorithm
Table 4 compares two exact algorithms (VITERBI
and SD+E-XP.) with beam search, which is the ap-
proximate algorithm widely adopted for sequence
labeling in NLP. For this experiment, the beam
width, B, was exhaustively calibrated: we tried B
= {1, 2, 4, 8, ...} until the beam search achieved
comparable accuracy to the exact algorithms, i.e.,
the difference fell below 0.1 in our case.
We see that there is a substantial difference in
the performance between VITERBI and BEAM.
On the other hand, SD+C-EXP. reached speeds
very close to those of BEAM. In fact, they
achieved comparable performance in our exper-
iment. These results demonstrate that we could
successfully bridge the gap in the performance be-
492
Table 4: Comparison with beam search (sent./sec).
POS tagging Joint tagging Supertagging
VITERBI 4000 77 1.1
SD+C-EXP. 14,000 1600 300
BEAM 18,000 2400 180
tween exact and approximate algorithms, while re-
taining the advantages of exact algorithms.
6 Relation to coarse-to-fine approach
Before concluding remarks, we briefly examine
the relationship between staggered decoding and
coarse-to-fine PCFG parsing (2006). In coarse-to-
fine parsing, the candidate parse trees are pruned
by using the parse forest produced by a coarse-
grained PCFG. Since the degenerate label can be
interpreted as a coarse-level label, one may con-
sider that staggered decoding is an instance of
coarse-to-fine approach. While there is some re-
semblance, there are at least two essential differ-
ences. First, coarse-to-fine approach is a heuristic
pruning, that is, it is not an exact algorithm. Sec-
ond, our algorithm does not always perform de-
coding at the fine-grained level. It is designed to
be able to stop decoding at the coarse-level.
7 Conclusions
The sequence labeling algorithm is indispensable
to modern statistical NLP. However, the Viterbi
algorithm, which is the standard decoding algo-
rithm in NLP, is not efficient when we have to
deal with a large number of labels. In this paper
we presented staggered decoding, which provides
a principled way of resolving this problem. We
consider that it is a real alternative to the Viterbi
algorithm in various NLP tasks.
An interesting future direction is to extend the
proposed technique to handle more complex struc-
tures than the Markov chains, including semi-
Markov models and factorial HMMs (Sarawagi
and Cohen, 2004; Sutton et al, 2004). We hope
this work opens a new perspective on decoding al-
gorithms for a wide range of NLP problems, not
just sequence labeling.
Acknowledgement
We wish to thank the anonymous reviewers for
their helpful comments, especially on the com-
putational complexity of our algorithm. We also
thank Yusuke Miyao for providing us with the
HPSG Treebank data.
References
Thorsten Brants. 2000. TnT - a statistical part-of-
speech tagger. In Proceedings of ANLP, pages 224?
231.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine
Hill, R. Shrivaths, Jeremy Moore, Michael Pozar,
and Theresa Vu. 2006. Multi-level coarse-to-fine
PCFG parsing. In Proceedings of NAACL, pages
168?175.
Trevor Cohn. 2006. Efficient inference in large con-
ditional random fields. In Proceedings of ECML,
pages 606?613.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Thomas G. Dietterich, Pedro Domingos, Lise Getoor,
Stephen Muggleton, and Prasad Tadepalli. 2008.
Structured machine learning: the next ten years.
Machine Learning, 73(1):3?23.
Roberto Esposito and Daniele P. Radicioni. 2009.
CARPEDIEM: Optimizing the Viterbi algorithm
and applications to supervised sequential learning.
Jorunal of Machine Learning Research, 10:1851?
1880.
Pedro F. Felzenszwalb, Daniel P. Huttenlocher, and
Jon M. Kleinberg. 2003. Fast algorithms for large-
state-space HMMs with applications to Web usage
analysis. In Proceedings of NIPS, pages 409?416.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Efficient inference of CRFs for large-scale
natural language data. In Proceedings of ACL-
IJCNLP Short Papers, pages 281?284.
John Lafferty, Andrew McCallum, and Fernand
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: Trading structure for fea-
tures. In Proceedings of ICML, pages 592?599.
Yury Lifshits, ShayMozes, OrenWeimann, andMichal
Ziv-Ukelson. 2007. Speeding up HMM decod-
ing and training by exploiting sequence repetitions.
Computational Pattern Matching, pages 4?15.
Dekang Lin and Xiaoyun Wu. 2009. Phrae clustering
for discriminative training. In Proceedings of ACL-
IJCNLP, pages 1030?1038.
493
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertagging
and CFG-filtering. In Proceedings of IJCAI, pages
1671?1676.
Takashi Ninomiya, TakuyaMatsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of EMNLP, pages
155?163.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. In Proceedings of The IEEE, pages
257?286.
Sunita Sarawagi and Willian W. Cohen. 2004. Semi-
Markov conditional random fields for information
extraction. In Proceedings of NIPS, pages 1185?
1192.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL, pages 134?141.
Sajid M. Siddiqi and Andrew W. Moore. 2005. Fast
inference and learning in large-state-space HMMs.
In Proceedings of ICML, pages 800?807.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional ran-
dom fields: Factorized probabilistic models for la-
beling and segmenting sequence data. In Proceed-
ings of ICML.
Ben Tasker, Carlos Guestrin, and Daphe Koller. 2003.
Max-margin Markov networks. In Proceedings of
NIPS, pages 25?32.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453?1484.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of
HLT/EMNLP, pages 467?474.
Andrew J. Viterbi. 1967. Error bounds for convo-
lutional codes and an asymeptotically optimum de-
coding algorithm. IEEE Transactios on Information
Theory, 13(2):260?267.
494
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 964?972,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Predicting and Eliciting Addressee?s Emotion in Online Dialogue
Takayuki Hasegawa?
GREE Inc.
Minato-ku, Tokyo 106-6101, Japan
takayuki.hasegawa@gree.net
Nobuhiro Kaji and Naoki Yoshinaga
Institute of Industrial Science,
the University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
{kaji,ynaga}@tkl.iis.u-tokyo.ac.jp
Masashi Toyoda
Institute of Industrial Science,
the University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
toyoda@tkl.iis.u-tokyo.ac.jp
Abstract
While there have been many attempts to
estimate the emotion of an addresser from
her/his utterance, few studies have ex-
plored how her/his utterance affects the
emotion of the addressee. This has mo-
tivated us to investigate two novel tasks:
predicting the emotion of the addressee
and generating a response that elicits a
specific emotion in the addressee?s mind.
We target Japanese Twitter posts as a
source of dialogue data and automatically
build training data for learning the pre-
dictors and generators. The feasibility of
our approaches is assessed by using 1099
utterance-response pairs that are built by
five human workers.
1 Introduction
When we have a conversation, we usually care
about the emotion of the person to whom we
speak. For example, we try to cheer her/him up
if we find out s/he feels down, or we avoid saying
things that would trouble her/him.
To date, the modeling of emotion in a dialogue
has extensively been studied in NLP as well as re-
lated areas (Forbes-Riley and Litman, 2004; Ayadi
et al, 2011). However, the past attempts are vir-
tually restricted to estimating the emotion of an
addresser1 from her/his utterance. In contrast, few
studies have explored how the emotion of the ad-
dressee is affected by the utterance. We consider
the insufficiency of such research to be fatal for
?This work was conducted while the first author was a
graduate student at the University of Tokyo.
1We use the terms addresser/addressee rather than a
speaker/listener, because we target not spoken but online di-
alogue.
I have had a high fever for 3 days.
JOY
I hope you feel better soon.
I have had a high fever for 3 days.
SADNESS
Sorry, but you can?t join us today.
Figure 1: Two example pairs of utterances and re-
sponses. Those responses elicit certain emotions,
JOY or SADNESS, in the addressee?s mind. The ad-
dressee in this example refers to the left-hand user,
who receives the response.
computers to support human-human communica-
tions or to provide a communicative man-machine
interface.
With this motivation in mind, the paper inves-
tigates two novel tasks: (1) prediction of the ad-
dressee?s emotion and (2) generation of the re-
sponse that elicits a prespecified emotion in the ad-
dressee?s mind.2 In the prediction task, the system
is provided with a dialogue history. For simplic-
ity, we consider, as a history, an utterance and a
response to it (Figure 1). Given the history, the
system predicts the addressee?s emotion that will
be caused by the response. For example, the sys-
tem outputs JOY when the response is I hope you
feel better soon, while it outputs SADNESS when
the response is Sorry, but you can?t join us today
2We adopt Plutchik (1980)?s eight emotional categories in
both tasks.
964
(Figure 1).
In the generation task, on the other hand, the
system is provided with an utterance and an emo-
tional category such as JOY or SADNESS, which is
referred to as goal emotion. Then the system gen-
erates the response that elicits the goal emotion in
the addressee?s mind. For example, I hope you feel
better soon is generated as a response to I have had
a high fever for 3 days when the goal emotion is
specified as JOY, while Sorry, but you can?t join us
today is generated for SADNESS (Figure 1).
Systems that can perform the two tasks not only
serve as crucial components of dialogue systems
but also have interesting applications of their own.
Predicting the emotion of an addressee is use-
ful for filtering flames or infelicitous expressions
from online messages (Spertus, 1997). The re-
sponse generator that is aware of the emotion of
an addressee is also useful for text completion in
online conversation (Hasselgren et al, 2003; Pang
and Ravi, 2012).
This paper explores a data-driven approach to
performing the two tasks. With the recent emer-
gence of social media, especially microblogs, the
amount of dialogue data available is rapidly in-
creasing. Therefore, we are taking this opportu-
nity to building large-scale training data from mi-
croblog posts automatically. This approach allows
us to perform the two tasks in a large-scale with
little human effort.
We employ standard classifiers for predicting
the emotion of an addressee. Our contribution here
is to investigate the effectiveness of new features
that cannot be used in ordinary emotion recog-
nition, the task of estimating the emotion of a
speaker (or writer) from her/his utterance (or writ-
ing) (Ayadi et al, 2011; Bandyopadhyay and Oku-
mura, 2011; Balahur et al, 2011; Balahur et al,
2012). We specifically extract features from the
addressee?s last utterance (e.g., I have had a high
fever for 3 days in Figure 1) and explore the effec-
tiveness of using such features. Such information
is characteristic of a dialogue situation.
To perform the generation task, we build a sta-
tistical response generator by following (Ritter et
al., 2011). To improve on the previous study, we
investigate a method for controlling the contents
of the response for, in our case, eliciting the goal
emotion. We achieve this by using a technique in-
spired by domain adaptation. We learn multiple
models, each of which is adapted for eliciting one
specific emotion. Also, we perform model inter-
polation for addressing data sparseness.
In our experiment, we automatically build train-
ing data consisting of over 640 million dialogues
from Japanese Twitter posts. Using this data set,
we train the classifiers that predict the emotion
of an addressee, and the response generators that
elicit the goal emotion. We evaluate our methods
on the test data that are built by five human work-
ers, and confirm the feasibility of the proposed ap-
proaches.
2 Emotion-tagged Dialogue Corpus
The key in making a supervised approach to pre-
dicting and eliciting addressee?s emotion success-
ful is to obtain large-scale, reliable training data
effectually. We thus automatically build a large-
scale emotion-tagged dialogue corpus from mi-
croblog posts, and use it as the training data in the
prediction and generation tasks.
This section describes a method for construct-
ing the emotion-tagged dialogue corpus. We first
describe how to extract dialogues from posts in
Twitter, a popular microblogging service. We then
explain how to automatically annotate utterances
in the extracted dialogues with the addressers?
emotions by using emotional expressions as clues.
2.1 Mining dialogues from Twitter
We have first crawled utterances (posts) from
Twitter by using the Twitter REST API.3 The
crawled data consist of 5.5 billion utterances in
Japanese tweeted by 770 thousand users from
March 2011 to December 2012. We next cleaned
up the crawled utterances by handling Twitter-
specific expressions; we replaced all URL strings
to ?URL?, excluded utterances with the symbols
that indicate the re-posting (RT) or quoting (QT)
of others? tweets, and erased @user name ap-
pearing at the head and tail of the utterances, since
they are usually added to make a reply. We ex-
cluded utterances given by any user whose name
included ?bot.?
We then extracted dialogues from the resulting
utterances, assuming that a series of utterances
interchangeably made by two users form a dia-
logue. We here exploited ?in reply to status id?
field of each utterance provided by Twitter REST
API to link to the other, if any, utterance to which
it replied.
3https://dev.twitter.com/docs/api/
965
# users 672,937
# dialogues 311,541,839
# unique utterances 1,007,403,858
ave. # dialogues / user 463.0
ave. # utterances / user 1497.0
ave. # utterances / dialogue 3.2
Table 1: Statistics of dialogues extracted from
Twitter.
2,000,000
40,000,000
60,000,000
80,000,000
100,000,000
120,000,000
140,000,000
160,000,000
180,000,000
0 2 3 4 5 6 7 8 9 10 11+
#
Di
alo
gu
es
Dialogue length (# utterances in dialogue)
Figure 2: The number of dialogues plotted against
the dialogue length.
Utterance Emotion
A: Would you like to go for dinner with me?
B: Sorry, I can?t. I have a fever of 38 degrees.
A: Oh dear. I hope you feel better soon. SURPRISE
B: Thanks. I?m happy to hear you say that. JOY
Table 2: An illustration of an emotion-tagged dia-
logue: The first column shows a dialogue (a series
of utterances interchangeably made by two users),
while the second column shows the addresser?s
emotion estimated from the utterance.
Table 1 lists the statistics of the extracted di-
alogues, while Figure 2 plots the number of di-
alogues plotted against the dialogue length (the
number of utterances in dialogue). Most dialogues
(98.2%) consist of at most 10 utterances, although
the longest dialogue includes 1745 utterances and
spans more than six weeks.
2.2 Tagging utterances with addressers?
emotions
We then automatically labeled utterances in the
obtained dialogues with the addressers? emotions
by using emotional expressions as clues (Table 2).
In this study, we have adopted Plutchik (1980)?s
eight emotional categories (ANGER, ANTICIPA-
TION, DISGUST, FEAR, JOY, SADNESS, SUR-
PRISE, and TRUST) as the targets to label, and
manually tailored around ten emotional expres-
sions for each emotional category. Table 3 lists
examples of the emotional expressions, while the
Emotion Emotional expressions
ANGER frustrating, irritating, nonsense
ANTICIPATION exciting, expecting, looking forward
DISGUST disgusting, unpleasant, hate
FEAR afraid, anxious, scary
JOY glad, happy, delighted
SADNESS sad, lonely, unhappy
SURPRISE surprised, oh dear, wow
TRUST relieved, reliable, solid
Table 3: Example of clue emotional expressions.
Emotion # utterances Precision
Worker A Worker B
ANGER 190,555 0.95 0.95
ANTICIPATION 2,548,706 0.99 0.99
DISGUST 475,711 0.93 0.93
FEAR 2,671,222 0.96 0.96
JOY 2,725,235 0.94 0.96
SADNESS 712,273 0.97 0.97
SURPRISE 975,433 0.97 0.97
TRUST 359,482 0.97 0.98
Table 4: Size and precision of utterances labeled
with the addressers? emotions.
rest are mostly their spelling variations.4
Because precise annotation is critical in the su-
pervised learning scenario, we annotate utterances
with the addressers? emotions only when the emo-
tional expressions do not:
1. modify content words.
2. accompany an expression of negation, condi-
tional, imperative, interrogative, concession,
or indirect speech in the same sentence.
For example, I saw a frustrated teacher is re-
jected by the first condition, while I?ll be happy
if it rains is rejected by the second condition. The
second condition was judged by checking whether
the sentence includes trigger expressions such as
??? (not/never)?, ??? (if-clause)?, ???, ???
((al)though)?, and ?? (that-clause)?.
Table 4 lists the size and precision of the utter-
ances labeled with the addressers? emotions. Two
human workers measured the precision of the an-
notation by examining 100 labeled utterances ran-
domly sampled for each emotional category. The
inter-rater agreement was ? = 0.85, indicating al-
most perfect agreement. The precision of the an-
notation exceeded 0.95 for most of the emotional
categories.
4Note that the clue emotional expressions are language-
specific but can be easily tailored for other languages. Here,
Japanese emotional expressions are translated into English to
widen the potential readership of the paper.
966
3 Predicting Addressee?s Emotion
This section describes a method for predicting
emotion elicited in an addressee when s/he re-
ceives a response to her/his utterance. The input
to this task is a pair of an utterance and a response
to it, e.g., the two utterances in Figure 1, while
the output is the addressee?s emotion among the
emotional categories of Plutchik (1980) (JOY and
SADNESS for the top and bottom dialogues in Fig-
ure 1, respectively).
Although a response could elicit multiple emo-
tions in the addressee, in this paper we focus on
predicting the most salient emotion elicited in the
addressee and cast the prediction as a single-label
multi-class classification problem.5 We then con-
struct a one-versus-the-rest classifier6 by combin-
ing eight binary classifiers, each of which predicts
whether the response elicits each emotional cate-
gory. We use online passive-aggressive algorithm
to train the eight binary classifiers.
We exploit the emotion-tagged dialogue corpus
constructed in Section 2 to collect training exam-
ples for the prediction task. For each emotion-
tagged utterance in the corpus, we assume that the
tagged emotion is elicited by the (last) response.
We thereby extract the pair of utterances preced-
ing the emotion-tagged utterance and the tagged
emotion as one training example. Taking the di-
alogue in Table 2 as an example, we obtain one
training example from the first two utterances and
SURPRISE as the emotion elicited in user A.
We extract all the n-grams (n ? 3) in the re-
sponse to induce (binary) n-gram features. The
extracted n-grams could indicate a certain action
that elicits a specific emotion (e.g., ?have a fever?
in Table 2), or a style or tone of speaking (e.g.,
?Sorry?). Likewise, we extract word n-grams from
the addressee?s utterance. The extracted n-grams
activate another set of binary n-gram features.
Because word n-grams themselves are likely to
be sparse, we estimate the addressers? emotions
from their utterances and exploit them to induce
emotion features. The addresser?s emotion has
been reported to influence the addressee?s emotion
5Because microblog posts are short, we expect emotions
elicited by a response post not to be very diverse and a multi-
class classification to be able to capture the essential crux of
the prediction task.
6We should note that a one-versus-the-rest classifier can
be used in the multi-label classification scenario, just by al-
lowing the classifier to output more than one emotional cate-
gory (Ghamrawi and McCallum, 2005).
strongly (Kim et al, 2012), while the addressee?s
emotion just before receiving a response can be a
reference to predict her/his emotion in question af-
ter receiving the response.
To induce emotion features, we exploit the rule-
based approach used in Section 2.2 to estimate
the addresser?s emotion. Since the rule-based ap-
proach annotates utterances with emotions only
when they contain emotional expressions, we in-
dependently train for each emotional category
a binary classifier that estimates the addresser?s
emotion from her/his utterance and apply it to the
unlabeled utterances. The training data for these
classifiers are the emotion-tagged utterances ob-
tained in Section 2, while the features are n-grams
(n ? 3)7 in the utterance.
We should emphasize that the features induced
from the addressee?s utterance are unique to this
task and are hardly available in the related tasks
that predicted the emotion of a reader of news ar-
ticles (Lin and Hsin-Yihn, 2008) or personal sto-
ries (Socher et al, 2011). We will later confirm the
impact of these features on the prediction accuracy
in the experiments.
4 Eliciting Addressee?s Emotion
This section presents a method for generating a re-
sponse that elicits the goal emotion, which is one
of the emotional categories of Plutchik (1980), in
the addressee. In section 4.1, we describe a statis-
tical framework for response generation proposed
by (Ritter et al, 2011). In section 4.2, we present
how to adapt the model in order to generate a
response that elicits the goal emotion in the ad-
dressee.
4.1 Statistical response generation
Following (Ritter et al, 2011), we apply the sta-
tistical machine translation model for generating a
response to a given utterance. In this framework,
a response is viewed as a translation of the input
utterance. Similar to ordinary machine translation
systems, the model is learned from pairs of an ut-
terance and a response by using off-the-shelf tools
for machine translation.
We use GIZA++8 and SRILM9 for learning
translation model and 5-gram language model, re-
7We have excluded n-grams that matched the emotional
expressions used in Section 2 to avoid overfitting.
8http://code.google.com/p/giza-pp/
9http://www.speech.sri.com/projects/
srilm/
967
spectively. As post-processing, some phrase pairs
are filtered out from the translation table as fol-
lows. When GIZA++ is directly applied to di-
alogue data, it frequently finds paraphrase pairs,
learning to parrot back the input (Ritter et al,
2011). To avoid using such pairs for response gen-
eration, a phrase pair is removed if one phrase is
the substring of the other.
We use Moses decoder10 to search for the best
response to a given utterance. Unlike machine
translation, we do not use reordering models, be-
cause the positions of phrases are not considered
to correlate strongly with the appropriateness of
responses (Ritter et al, 2011). In addition, we do
not use any discriminative training methods such
as MERT for optimizing the feature weights (Och,
2003). They are set as default values provided by
Moses (Ritter et al, 2011).
4.2 Model adaptation
The above framework allows us to generate appro-
priate responses to arbitrary input utterances. On
top of this framework, we have developed a re-
sponse generator that elicits a specific emotion.
We use the emotion-tagged dialogue corpus to
learn eight translation models and language mod-
els, each of which is specialized in generating
the response that elicits one of the eight emo-
tions (Plutchik, 1980). Specifically, the models
are learned from utterances preceding ones that are
tagged with emotional category. As an example,
let us examine to learn models for eliciting SUR-
PRISE from the dialogue in Table 2. In this case,
the first two utterances are used to learn the trans-
lation model, while only the second utterance is
used to learn the language model.
However, this simple approach is prone to suf-
fer from the data sparseness problem. Because
not all the utterances are tagged with the emotion
in emotion-tagged dialogue corpus, only a small
fraction of utterances can be used for learning the
adapted models.
We perform model interpolation for addressing
this problem. In addition to the adapted mod-
els described above, we also use a general model,
which is learned from the entire corpus. The two
models are then merged as the weighted linear in-
terpolation.
Specifically, we use tmcombine.py script
provided by Moses for the interpolation of trans-
10http://www.statmt.org/moses/
lation models (Sennrich, 2012). For all the four
features (i.e., two phrase translation probabilities
and two lexical weights) derived from transla-
tion model, the weights of the adapted model are
equally set as ? (0 ? ? ? 1.0). On the other
hand, we use SRILM for the interpolation of lan-
guage models. The weight of the adapted model is
set as ? (0 ? ? ? 1.0).
The parameters ? and ? control the strength of
the adapted models. Only adapted models are used
when ? (or ?)= 1.0, while the adapted models are
not at all used when ? (or ?) = 0. When both ?
and ? are specified as 0, the model becomes equiv-
alent to the original one described in section 4.1.
5 Experiments
5.1 Test data
To evaluate the proposed method, we built, as test
data, sets of an utterance paired with responses
that elicit a certain goal emotion (Table 5). Note
that they were used for evaluation in both of the
two tasks. Each utterance in the test data has
more than one responses that elicit the same goal
emotion, because they are used to compute BLEU
score (see section 5.3).
The data set was built in the following manner.
We first asked five human worker to produce re-
sponses to 80 utterances (10 utterances for each
goal emotion). Note that the 80 utterances do not
have overlap between workers and that the worker
produced only one response to each utterance.
To alleviate the burden on the workers, we ac-
tually provided each worker with the utterances
in the emotion-tagged corpus. Then we asked
each worker to select 80 utterances to which s/he
thought s/he could easily respond. The selected
utterances were removed from the corpus during
training.
As a result, we obtained 400 utterance-response
pairs (= 80 utterance-response pairs ? 5 work-
ers). For each of those 400 utterances, two ad-
ditional responses are produced. We did not al-
low the same worker to produce more than one
response to the same utterance. In this way, we
obtained 1200 responses for the 400 utterances in
total.
Finally, we assessed the data quality to remove
responses that were unlikely to elicit the goal emo-
tion. For each utterance-response pair, we asked
two workers to judge whether the response elicited
the goal emotion. If both workers regarded the
968
Goal emotion: JOY
U: 16???????????????????
?????
(I?m turning 16. Hope to get alng with you as
well as ever!)
R1:??????????????
(Happy birthday!)
R2:?????????????????????
(Congratulations! I?ll give you a birthday present.)
R3:???????????????
(Congratulations! I hope you have a happy year!)
Table 5: Example of the test data. English transla-
tions are attached in the parenthesis.
Emotion # utterance pairs
ANGER 119,881
ANTICIPATION 1,416,847
DISGUST 333,972
FEAR 1,662,998
JOY 1,724,198
SADNESS 436,668
SURPRISE 589,790
TRUST 228,974
GENERAL 646,429,405
Table 6: The number of utterance pairs used
for training classifiers in emotion prediction and
learning the translation models and language mod-
els in response generation.
response as inappropriate, it was removed from
the data. The resulting test data consist of 1099
utterance-response pairs for 396 utterances.
This data set is submitted as supplementary ma-
terial to support the reproducibility of our experi-
mental results.
5.2 Prediction task
We first report experimental results on predicting
the addressee?s emotion within a dialogue. Table 6
lists the number of utterance-response pairs used
to train eight binary classifiers for individual emo-
tional categories, which form a one-versus-the rest
classifier for the prediction task. We used opal11
as an implementation of online passive-aggressive
algorithm to train the individual classifiers.
To investigate the impact of the features that are
uniquely available in a dialogue data, we com-
pared classifiers trained with the following two
sets of features in terms of precision, recall, and
F1 for each emotional category.
RESPONSE The n-gram and emotion features in-
duced from the response.
11http://www.tkl.iis.u-tokyo.ac.jp/
?ynaga/opal/.
Emotion RESPONSE RESPONSE/UTTER.
PREC REC F1 PREC REC F1
ANGER 0.455 0.476 0.465 0.600 0.548 0.573
ANTICIPA. 0.518 0.526 0.522 0.614 0.637 0.625
DISGUST 0.275 0.519 0.359 0.378 0.511 0.435
FEAR 0.484 0.727 0.581 0.459 0.706 0.556
JOY 0.690 0.417 0.519 0.720 0.590 0.649
SADNESS 0.711 0.467 0.564 0.670 0.562 0.611
SURPRISE 0.511 0.348 0.414 0.584 0.437 0.500
TRUST 0.695 0.452 0.548 0.682 0.514 0.586
average 0.542 0.492 0.497 0.588 0.563 0.567
Table 7: Predicting addressee?s emotion: Results.
PREDICTED EMOTION
AN
GE
R
AN
TI
CI
PA
.
DI
SG
US
T
FE
AR
JO
Y
SA
DN
ES
S
SU
RP
RI
SE
TR
US
T
tot
al
ANGER 69 0 26 20 0 8 2 1 126
ANTICIPA. 1 86 11 7 13 0 6 11 135
DISGUST 25 1 68 18 2 8 7 4 133
FEAR 3 0 22 101 1 5 9 2 143
JOY 1 28 9 4 85 1 7 9 144
SADNESS 6 3 25 14 5 77 5 2 137
SURPRISE 7 10 9 32 5 7 59 6 135
TRUST 3 12 10 24 7 9 6 75 146CO
RR
EC
T
EM
OT
IO
N
total 115 140 180 220 118 115 101 110 1099
Table 8: Confusion matrix of predicting ad-
dressee?s emotion, with mostly predicted emo-
tions bold-faced and mostly confused emotions
underlined for each emotional category.
RESPONSE/UTTER. The n-gram and emotion
features induced from the response and the
addressee?s utterance.
Table 7 lists prediction results. We can see that
the features induced from the addressee?s utter-
ance significantly improved the prediction perfor-
mance, F1, for emotions other than FEAR. FEAR is
elicited instantly by the response, and the features
induced from the addressee?s utterance thereby
confused the classifier.
Table 8 shows a confusion matrix of the classi-
fier using all the features, with mostly predicted
emotions bold-faced and mostly confused emo-
tions underlined for each emotional category. We
can find some typical confusing pairs of emotions
from this matrix. The classifier confuses DISGUST
with ANGER and vice versa, while it confuses JOY
with ANTICIPATION. These confusions conform
to our expectation, since they are actually similar
emotions. The classifier was less likely to confuse
positive emotions (JOY and ANTICIPATION) with
negative emotion (ANGER, DISGUST, FEAR, and
SADNESS) vice versa.
969
Goal emotion: ANGER (predicted as SADNESS)
U:????????????????
(You have phone calls every day, I envy you.)
R:????????????????????????
(I envy you have a lot of time ?cause no one calls you.)
Goal emotion: SURPRISE (predicted as FEAR)
U:????????????
(Is it true that dark-haired girls are popular with boys?)
R:???????????????????
(About 80% of boys seem to prefer dark-haired girls.)
Table 9: Examples of utterance-response pairs to
which the system predicted wrong emotions.
We have briefly examined the confusions and
found the two major types of errors, each of which
is exemplified in Table 9. The first (top) one is sar-
casm or irony, which has been reported to be diffi-
cult to capture by lexical features alone (Gonza?lez-
Iba?n?ez et al, 2011). The other (bottom) one is due
to lack of information. In this example, only if the
addressee does not know the fact provided by the
response, s/he will surprise at it.
5.3 Generation task
We next demonstrate the experimental results for
eliciting the emotion of the addressee.
We use the utterance pairs summarized in Ta-
ble 6 to learn the translation models and language
models for eliciting each emotional category. We
also use the 640 million utterances pairs in the
entire emotion-tagged corpus for learning general
models. However, for learning the general transla-
tion models, we currently use 4 millions of utter-
ance pairs sampled from the 640 millions of pairs
due to the computational limitation.
Automatic evaluation
We first use BLEU score (Papineni et al, 2002)
to perform automatic evaluation (Ritter et al,
2011). In this evaluation, the system is pro-
vided with the utterance and the goal emotion
in the test data and the generated responses are
evaluated through BLEU score. Specifically, we
conducted two-fold cross-validation to optimize
the weights of our method. We tried ? and
? in {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} and selected the
weights that achieved the best BLEU score. Note
that we adopted different values of the weights for
different emotional categories.
Table 10 compares BLEU scores of three meth-
ods including the proposed one. The first row
represents a method that does not perform model
adaptation at all. It corresponds to the special case
System BLEU
NO ADAPTATION 0.64
PROPOSED 1.05
OPTIMAL 1.57
Table 10: Comparison of BLEU scores.
(i.e., ? = ? = 0.0) of the proposed method. The
second row represents our method, while the last
row represents the result of our method when the
weights are set as optimal, i.e., those achieving the
best BLEU on the test data. This result can be con-
sidered as an upper bound on BLEU score.
The results demonstrate that model adaptation
is useful for generating the responses that elicit
the goal emotion. We can clearly observe the im-
provement in the BLEU from 0.64 to 1.05.
On the other hand, there still remains a gap be-
tween the last two rows (i.e., proposed and opti-
mal). We think this is partly because the current
test data is too small to reliably tune parameters.
Human evaluation
We next asked two human workers to manually
evaluate the generation results.
In this evaluation, the baseline (no adaptation
in Table 10) and proposed method generated a re-
sponse for each of the 396 utterances in the test
data. For the resulting 792 utterance-response
pairs, the two workers manually assessed the ap-
propriateness of the response. Each response was
judged whether it is grammatical and meaningful.
If the response was regarded as so by either of the
workers, it was further judged whether it elicits the
goal emotion or not. To make the comparison fair,
we did not expose to the workers which system
generated the response. In addition, the responses
generated by the two systems were presented in a
random order.
As the result, 147 and 157 responses of the
baseline and proposed method were regarded as
appropriate, i.e., ecliting the goal emotion, by ei-
ther of the workers; 74 and 92 responses were
regarded as appropirate by both of the workers.
These results suggest the effectiveness of the pro-
posed method. Especially, we can confirm that
the proposed method can generate responses that
elicit addresee?s emotion more clearly. We inves-
tigated the agreement between the two workers in
this evaluation. We found that the ? coefficient is
0.59, which indicates moderate agreement. This
supports the reliability of our evaluation.
970
Goal emotion: JOY
Input: ???????????????????2? 7?????
(I wooooon the outstanding award at the photo competition! The ceremony is on Feb. 7!)
NO ADAPTATION: ?????????????? (Sorry to say, only the first day.)
PROPOSED: ????????????????????
(Congratulations on winning the gold prize!!! Congrats.)
Goal emotion: TRUST
Input: ???????????? (Do you get desperate? )
NO ADAPTATION: ???? (I?m looking forward to it!)
PROPOSED: ???????? (Maybe still OK.)
Goal emotion: ANTICIPATION
Input: ?????????????ww
(Huh! It?s gonna be all right! lol)
?????????????????????????????????? (???)
(I gotta buy the goods, so I?ll be glad if you can take the time :-))
NO ADAPTATION: ????????????????? (Since I?ve not bought it, I feel worried.)
PROPOSED: ???????????????? (Good! I?ll buy it too!!!)
Table 11: Examples of the responses generated by the two systems, NO ADAPTATION and PROPOSED.
Examples
Table 11 illustrates examples of the responses gen-
erated by the no adaptation baseline and proposed
method. In the first two examples, the proposed
method successfully generates responses that elicit
the goal emotions: JOY and TRUST. From these
examples, we can consider that the adapted model
assigns large probability to phrases such as con-
gratulations or OK. In the last example, the sys-
tem also succeeded in eliciting the goal emotion:
ANTICIPATION. For this example, we can interpret
that the speaker of the response (i.e., the system)
feels anticipation, and consequently the emotion
of the addressee is affected by the emotion of the
speaker (i.e., the system). Interestingly, a similar
phenomenon is also observed in real conversation
(Kim et al, 2012).
6 Related Work
There have been a tremendous amount of stud-
ies on predicting the emotion from text or speech
data (Ayadi et al, 2011; Bandyopadhyay and Oku-
mura, 2011; Balahur et al, 2011; Balahur et al,
2012). Unlike our prediction task, most of them
have exclusively focused on estimating the emo-
tion of a speaker (or writer) from her/his utterance
(or writing).
Analogous to our prediction task, Lin and Hsin-
Yihn (2008) and Socher et al (2011) investigated
predicting the emotion of a reader from the text
that s/he reads. Our work differs from them in that
we focus on dialogue data, and we exploit fea-
tures that are not available within their task set-
tings, e.g., the addressee?s previous utterance.
Tokuhisa et al (2008) proposed a method for
extracting pairs of an event (e.g., It rained sud-
denly when I went to see the cherry blossoms) and
an emotion elicited by it (e.g., SADNESS) from the
Web text. The extracted data are used for emotion
classification. A similar technique would be use-
ful for prediction the emotion of an addressee as
well.
Response generation has a long research history
(Weizenbaum, 1966), although it is only very re-
cently that a fully statistical approach was intro-
duced in this field (Ritter et al, 2011). At this mo-
ment, we are unaware of any statistical response
generators that model the emotion of the user.
Some researchers have explored generating
jokes or humorous text (Dybala et al, 2010;
Labtov and Lipson, 2012). Those attempts are
similar to our work in that they also aim at elic-
iting a certain emotion in the addressee. They are,
however, restricted to elicit a specific emotion.
The linear interpolation of translation and/or
language models is a widely-used technique for
adapting machine translation systems to new do-
mains (Sennrich, 2012). However, it has not been
touched in the context of response generation.
7 Conclusion and Future Work
In this paper, we have explored predicting and
eliciting the emotion of an addressee by using a
large amount of dialogue data obtained from mi-
croblog posts. In the first attempt to model the
emotion of an addressee in the field of NLP, we
demonstrated that the response of the dialogue
partner and the previous utterance of the addressee
are useful for predicting the emotion. In the gen-
eration task, on the other hand, we showed that the
971
model adaptation approach successfully generates
the responses that elicit the goal emotion.
For future work, we want to use longer dialogue
history in both tasks. While we considered only
two utterances as a history, a longer history would
be helpful. We also plan to personalize the pro-
posed methods, exploiting microblog posts made
by users of a certain age, gender, occupation, or
even character to perform model adaptation.
Acknowledgment
This work was supported by the FIRST program of
JSPS. The authors thank the anonymous review-
ers for their valuable comments. The authors also
thank the student annotators for their hard work.
References
Moataz El Ayadi, Mohamed S. Kamel, and Fakhri Kar-
ray. 2011. Survey on speech emotion recognition:
Features, classification schemes, and databases.
Pattern Recognition, 44:572?587.
Alexandra Balahur, Ester Boldrini, Andres Montoyo,
and Patricio Martinez-Barco, editors. 2011. Pro-
ceedings of the 2nd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis.
Association for Computational Linguistics.
Alexandra Balahur, Andres Montoyo, Patricio Mar-
tinez Barco, and Ester Boldrini, editors. 2012. Pro-
ceedings of the 3rd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis.
Association for Computational Linguistics.
Sivaji Bandyopadhyay and Manabu Okumura, editors.
2011. Proceedings of the Workshop on Sentiment
Analysis where AI meets Psychology. Asian Federa-
tion of Natural Language Processing.
Pawel Dybala, Michal Ptaszynski, Jacek Maciejewski,
Mizuki Takahashi, Rafal Rzepka, and Kenji Araki.
2010. Multiagent system for joke generation: Hu-
mor and emotions combined in human-agent conver-
sation. Journal of Ambient Intelligence and Smart
Environments, 2(1):31?48.
Kate Forbes-Riley and Diane J. Litman. 2004. Pre-
dicting emotion in spoken dialogue from multiple
knowledge sources. In Proceedings of NAACL,
pages 201?208.
Nadia Ghamrawi and Andrew McCallum. 2005. Col-
lective multi-label classification. In Proceedings of
CIKM, pages 195?200.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: a closer look. In Proceedings of ACL, pages
581?586.
Jon Hasselgren, Erik Montnemery, Pierre Nugues, and
Markus Svensson. 2003. HMS: A predictive text
entry method using bigrams. In Proceedings of
EACL Workshop on Language Modeling for Text En-
try Methods, pages 43?50.
Suin Kim, JinYeong Bak, and Alice Haeyun Oh. 2012.
Do you feel what I feel? social aspects of emotions
in Twitter conversations. In Proceedings of ICWSM,
pages 495?498.
Igor Labtov and Hod Lipson. 2012. Humor as circuits
in semantic networks. In Proceedings of ACL (Short
Papers), pages 150?155.
Kevin Lin and Hsin-Hsi Hsin-Yihn. 2008. Ranking
reader emotions using pairwise loss minimization
and emotional distribution regression. In Proceed-
ings of EMNLP, pages 136?144.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Bo Pang and Sujith Ravi. 2012. Revisiting the pre-
dictability of language: Response completion in so-
cial media. In Proceedings of EMNLP, pages 1489?
1499.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Robert Plutchik. 1980. A general psychoevolutionary
theory of emotion. In Emotion: Theory, research,
and experience: Vol. 1. Theories of emotion, pages
3?33. New York: Academic.
Alan Ritter, Colin Cherry, andWilliam B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of EMNLP, pages 583?593.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of EACL, pages
539?549.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151?161.
Ellen Spertus. 1997. Smokey: Automatic recognition
of hostile messages. In Proceedings of IAAI, pages
1058?1065.
Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classification using massive exam-
ples extracted from the Web. In Proceedings of
COLING, pages 881?888.
JosephWeizenbaum. 1966. ELIZA? a computer pro-
gram for the study of natural language communica-
tion between man and machine. Communications of
the ACM, 9(1):36?45.
972
