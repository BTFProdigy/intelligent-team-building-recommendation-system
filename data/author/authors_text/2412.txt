Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 112?121, Prague, June 2007. c?2007 Association for Computational Linguistics
A Comparative Evaluation of Deep and Shallow Approaches to the
Automatic Detection of Common Grammatical Errors
Joachim Wagner, Jennifer Foster, and Josef van Genabith?
National Centre for Language Technology
School of Computing, Dublin City University, Dublin 9, Ireland
{jwagner, jfoster, josef}@computing.dcu.ie
Abstract
This paper compares a deep and a shallow
processing approach to the problem of clas-
sifying a sentence as grammatically well-
formed or ill-formed. The deep processing
approach uses the XLE LFG parser and En-
glish grammar: two versions are presented,
one which uses the XLE directly to perform
the classification, and another one which
uses a decision tree trained on features con-
sisting of the XLE?s output statistics. The
shallow processing approach predicts gram-
maticality based on n-gram frequency statis-
tics: we present two versions, one which
uses frequency thresholds and one which
uses a decision tree trained on the frequen-
cies of the rarest n-grams in the input sen-
tence. We find that the use of a decision tree
improves on the basic approach only for the
deep parser-based approach. We also show
that combining both the shallow and deep
decision tree features is effective. Our eval-
uation is carried out using a large test set of
grammatical and ungrammatical sentences.
The ungrammatical test set is generated au-
tomatically by inserting grammatical errors
into well-formed BNC sentences.
1 Introduction
This paper is concerned with the task of predict-
ing whether a sentence contains a grammatical er-
ror. An accurate method for carrying out automatic
?Also affiliated to IBM CAS, Dublin.
grammaticality judgements has uses in the areas of
computer-assisted language learning and grammar
checking. Comparative evaluation of existing error
detection approaches has been hampered by a lack
of large and commonly used evaluation error cor-
pora. We attempt to overcome this by automatically
creating a large error corpus, containing four dif-
ferent types of frequently occurring grammatical er-
rors. We use this corpus to evaluate the performance
of two approaches to the task of automatic error de-
tection. One approach uses low-level detection tech-
niques based on POS n-grams. The other approach
is a novel parser-based method which employs deep
linguistic processing to discriminate grammatical in-
put from ungrammatical. For both approaches, we
implement a basic solution, and then attempt to im-
prove upon this solution using a decision tree clas-
sifier. We show that combining both methods im-
proves upon the individual methods.
N-gram-based approaches to the problem of error
detection have been proposed and implemented in
various forms by Atwell(1987), Bigert and Knutsson
(2002), and Chodorow and Leacock (2000) amongst
others. Existing approaches are hard to compare
since they are evaluated on different test sets which
vary in size and error density. Furthermore, most of
these approaches concentrate on one type of gram-
matical error only, namely, context-sensitive or real-
word spelling errors. We implement a vanilla n-
gram-based approach which is tested on a very large
test set containing four different types of error.
The idea behind the parser-based approach to er-
ror detection is to use a broad-coverage hand-crafted
precision grammar to detect ungrammatical sen-
112
tences. This approach exploits the fact that a pre-
cision grammar is designed, in the traditional gen-
erative grammar sense (Chomsky, 1957), to dis-
tinguish grammatical sentences from ungrammati-
cal sentences. This is in contrast to treebank-based
grammars which tend to massively overgenerate and
do not generally aim to discriminate between the
two. In order for our approach to work, the coverage
of the precision grammars must be broad enough to
parse a large corpus of grammatical sentences, and
for this reason, we choose the XLE (Maxwell and
Kaplan, 1996), an efficient and robust parsing sys-
tem for Lexical Functional Grammar (LFG) (Kaplan
and Bresnan, 1982) and the ParGram English gram-
mar (Butt et al, 2002) for our experiments. This sys-
tem employs robustness techniques, some borrowed
from Optimality Theory (OT) (Prince and Smolen-
sky, 1993), to parse extra-grammatical input (Frank
et al, 1998), but crucially still distinguishes between
optimal and suboptimal solutions.
The evaluation corpus is a subset of an un-
grammatical version of the British National Cor-
pus (BNC), a 100 million word balanced corpus of
British English (Burnard, 2000). This corpus is ob-
tained by automatically inserting grammatical errors
into the original BNC sentences based on an analysis
of a manually compiled ?real? error corpus.
This paper makes the following contributions to
the task of automatic error detection:
1. A novel deep processing XLE-based approach
2. An effective and novel application of decision
tree machine learning to both shallow and deep
approaches
3. A novel combination of deep and shallow pro-
cessing
4. An evaluation of an n-gram-based approach on
a wider variety of errors than has previously
been carried out
5. A large evaluation error corpus
The paper is organised as follows: in Section 2,
we describe previous approaches to the problem of
error detection; in Section 3, a description of the
error corpus used in our evaluation experiments is
presented, and in Section 4, the two approaches to
error detection are presented, evaluated, combined
and compared. Section 5 provides a summary and
suggestions for future work.
2 Background
2.1 Precision Grammars
A precision grammar is a formal grammar designed
to distinguish ungrammatical from grammatical sen-
tences. This is in contrast to large treebank-induced
grammars which often accept ungrammatical input
(Charniak, 1996). While high coverage is required,
it is difficult to increase coverage without also in-
creasing the amount of ungrammatical sentences
that are accepted as grammatical by the grammar.
Most publications in grammar-based automatic error
detection focus on locating and categorising errors
and giving feedback. Existing grammars are re-used
(Vandeventer Faltin, 2003), or grammars of limited
size are developed from scratch (Reuer, 2003).
The ParGram English LFG is a hand-crafted
broad-coverage grammar developed over several
years with the XLE platform (Butt et al, 2002). The
XLE parser uses OT to resolve ambiguities (Prince
and Smolensky, 1993). Grammar constraints re-
sulting in rare constructions can be marked as ?dis-
preferred? and constraints resulting in common un-
grammatical constructions can be marked as ?un-
grammatical?. The use of constraint ordering and
marking increases the robustness of the grammar,
while maintaining the grammatical / ungrammati-
cal distinction (Frank et al, 1998). The English
Resource Grammar (ERG) is a precision Head-
Driven Phrase Structure Grammar (HPSG) of En-
glish (Copestake and Flickinger, 2000; Pollard and
Sag, 1994). Its coverage is not as broad as the XLE
English grammar. Baldwin et al (2004) propose a
method to identify gaps in the grammar. Blunsom
and Baldwin (2006) report ongoing development.
There has been previous work using the ERG and
the XLE grammars in the area of computer-assisted
language learning. Bender et al (2004) use a ver-
sion of the ERG containing mal-rules to parse ill-
formed sentences from the SST corpus of Japanese
learner English (Emi et al, 2004). They then use
the semantic representations of the ill-formed input
to generate well-formed corrections. Khader et al
(2004) study whether the ParGram English LFG can
be used for computer-assisted language learning by
113
adding additional OT marks for ungrammatical con-
structions observed in a learner corpus. However,
the evaluation is preliminary, on only 50 test items.
2.2 N-gram Methods
Most shallow approaches to grammar error detection
originate from the area of real-word spelling error
correction. A real-word spelling error is a spelling
or typing error which results in a token which is an-
other valid word of the language in question.
The (to our knowledge) oldest work in this area
is that of Atwell (1987) who uses a POS tagger to
flag POS bigrams that are unlikely according to a
reference corpus. While he speculates that the bi-
gram frequency should be compared to how often
the same POS bigram is involved in errors in an error
corpus, the proposed system uses the raw frequency
with an empirically established threshold to decide
whether a bigram indicates an error. In the same
paper, a completely different approach is presented
that uses the same POS tagger to consider spelling
variants that have a different POS. In the example
sentence I am very hit the POS of the spelling vari-
ant hot/JJ is added to the list NN-VB-VBD-VBN of
possible POS tags of hit. If the POS tagger chooses
hit/JJ, the word is flagged and the correction hot is
proposed to the user. Unlike most n-gram-based ap-
proaches, Atwell?s work aims to detect grammar er-
rors in general and not just real-word spelling errors.
However, a complete evaluation is missing.
The idea of disambiguating between the elements
of confusion sets is related to word sense disam-
biguation. Golding (1995) builds a classifier based
on a rich set of context features. Mays et al (1991)
apply the noisy channel model to the disambiguation
problem. For each candidate correction S? of the
input S the probability P (S?)P (S|S?) is calculated
and the most likely correction selected. This method
is re-evaluated by Wilcox-O?Hearn et al (2006) on
WSJ data with artificial real-word spelling errors.
Bigert and Knutsson (2002) extend upon a basic
n-gram approach by attempting to match n-grams of
low frequency with similar n-grams in order to re-
duce overflagging. Furthermore, n-grams crossing
clause boundaries are not flagged and the similarity
measure is adapted in the case of phrase boundaries
that usually result in low frequency n-grams.
Chodorow and Leacock (2000) use a mutual in-
formation measure in addition to raw frequency of n-
grams. Apart from this, their ALEK system employs
other extensions to the basic approach, for exam-
ple frequency counts from both generic and word-
specific corpora are used in the measures. It is not
reported how much each of these contribute to the
overall performance.
Rather than trying to implement all of the pre-
vious n-gram approaches, we implement the basic
approach which uses rare n-grams to predict gram-
maticality. This property is shared by all previous
shallow approaches. We also test our approach on a
wider class of grammatical errors.
3 Ungrammatical Data
In this section, we discuss the notion of an artifi-
cial error corpus (Section 3.1), define the type of
ungrammatical language we are dealing with (Sec-
tion 3.2), and describe our procedure for creating a
large artificial error corpus derived from the BNC
(Section 3.3).
3.1 An Artificial Error Corpus
In order to meaningfully evaluate a shallow ver-
sus deep approach to automatic error detection, a
large test set of ungrammatical sentences is needed.
A corpus of ungrammatical sentences can take the
form of a learner corpus (Granger, 1993; Emi et al,
2004), i. e. a corpus of sentences produced by lan-
guage learners, or it can take the form of a more gen-
eral error corpus comprising sentences which are not
necessarily produced in a language-learning context
and which contain competence and performance er-
rors produced by native and non-native speakers of
the language (Becker et al, 1999; Foster and Vogel,
2004; Foster, 2005). For both types of error corpus,
it is not enough to collect a large set of sentences
which are likely to contain an error - it is also neces-
sary to examine each sentence in order to determine
whether an error has actually occurred, and, if it has,
to note the nature of the error. Thus, like the cre-
ation of a treebank, the creation of a corpus of un-
grammatical sentences requires time and linguistic
knowledge, and is by no means a trivial task.
A corpus of ungrammatical sentences which is
large enough to be useful can be created auto-
matically by inserting, deleting or replacing words
114
in grammatical sentences. These transformations
should be linguistically realistic and should, there-
fore, be based on an analysis of naturally produced
grammatical errors. Automatically generated error
corpora have been used before in natural language
processing. Bigert (2004) and Wilcox-O?Hearn et
al. (2006), for example, automatically introduce
spelling errors into texts. Here, we generate a large
error corpus by automatically inserting four different
kinds of grammatical errors into BNC sentences.
3.2 Commonly Produced Grammatical Errors
Following Foster (2005), we define a sentence to be
ungrammatical if all the words in the sentence are
well-formed words of the language in question, but
the sentence contains one or more error. This er-
ror can take the form of a performance slip which
can occur due to carelessness or tiredness, or a com-
petence error which occurs due to a lack of knowl-
edge of a particular construction. This definition in-
cludes real-word spelling errors and excludes non-
word spelling errors. It also excludes the abbrevi-
ated informal language used in electronic communi-
cation. Using the above definition as a guideline, a
20,000 word corpus of ungrammatical English sen-
tences was collected from a variety of written texts
including newspapers, academic papers, emails and
website forums (Foster and Vogel, 2004; Foster,
2005). The errors in the corpus were carefully anal-
ysed and classified in terms of how they might be
corrected using the three word-level correction op-
erators: insert, delete and substitute. The following
frequency ordering of the three word-level correc-
tion operators was found:
substitute (48%) > insert (24%) > delete (17%) >
combination (11%)
Stemberger (1982) reports the same ordering of the
substitution, deletion and insertion correction oper-
ators in a study of native speaker spoken language
slips. Among the grammatical errors which can be
corrected by substituting one word for another, the
most common errors are real-word spelling errors
and agreement errors. In fact, 72% of all errors fall
into one of the following four classes:
1. missing word errors:
What are the subjects? > What the subjects?
2. extra word errors:
Was that in the summer? > Was that in the sum-
mer in?
3. real-word spelling errors:
She could not comprehend. > She could no
comprehend.
4. agreement errors:
She steered Melissa round a corner. > She
steered Melissa round a corners.
A similar classification was adopted by Nicholls
(1999), having analysed the errors in a learner cor-
pus. Our research is currently limited to the four er-
ror types given above, i. e. missing word errors, ex-
tra word errors, real-word spelling errors and agree-
ments errors. However, it is possible for it to be ex-
tended to handle a wider class of errors.
3.3 Automatic Error Creation
The error creation procedure takes as input a part-
of-speech-tagged corpus of sentences which are as-
sumed to be well-formed, and outputs a corpus of
ungrammatical sentences. The automatically intro-
duced errors take the form of the four most com-
mon error types found in the manually created cor-
pus, i. e. missing word errors, extra word errors, real-
word spelling errors and agreement errors. For each
sentence in the original tagged corpus, an attempt is
made to automatically produce four ungrammatical
sentences, one for each of the four error types. Thus,
the output of the error creation procedure is, in fact,
four error corpora.
3.3.1 Missing Word Errors
In the manually created error corpus of Foster
(2005), missing word errors are classified based on
the part-of-speech (POS) of the missing word. 98%
of the missing parts-of-speech come from the fol-
lowing list (the frequency distribution in the error
corpus is given in brackets):
det (28%) > verb (23%) > prep (21%) > pro (10%)
> noun (7%) > ?to? (7%) > conj (2%)
We use this information when introducing missing
word errors into the BNC sentences. For each sen-
tence, all words with the above POS tags are noted.
One of these is selected and deleted. The above
frequency ordering is respected so that, for exam-
ple, missing determiner errors are produced more of-
ten than missing pronoun errors. No ungrammatical
115
sentence is produced if the original sentence con-
tains just one word or if the sentence contains no
words with parts-of-speech in the above list.
3.3.2 Extra Word Errors
We introduce extra word errors in the following
three ways:
1. Random duplication of any token within a sen-
tence: That?s the way we we learn here.
2. Random duplication of any POS within a sen-
tence: There it he was.
3. Random insertion of an arbitrary token into the
sentence: Joanna drew as a long breadth.
Apart from the case of duplicate tokens, the extra
words are selected from a list of tagged words com-
piled from a random subset of the BNC. Again, our
procedure for inserting extra words is based on the
analysis of extra word errors in the 20,000 word er-
ror corpus of Foster (2005).
3.3.3 Real-Word Spelling Errors
We classify an error as a real-word spelling er-
ror if it can be corrected by replacing the erroneous
word with another word with a Levenshtein distance
of one from the erroneous word, e.g. the and they.
Based on the analysis of the manually created er-
ror corpus (Foster, 2005), we compile a list of com-
mon English real-word spelling error word pairs.
For each BNC sentence, the error creation proce-
dure records all tokens in the sentence which appear
as one half of one of these word pairs. One token
is selected at random and replaced by the other half
of the pair. The list of common real-word spelling
error pairs contains such frequently occurring words
as is and a, and the procedure therefore produces an
ill-formed sentence for most input sentences.
3.3.4 Agreement Errors
We introduce subject-verb and determiner-noun
number agreement errors into the BNC sentences.
We consider both types of agreement error equally
likely and introduce the error by replacing a singular
determiner, noun or verb with its plural counterpart,
or vice versa. For English, subject-verb agreement
errors can only be introduced for present tense verbs,
and determiner-noun agreement errors can only be
introduced for determiners which are marked for
number, e.g. demonstratives and the indefinite ar-
ticle. The procedure would be more productive if
applied to a morphologically richer language.
3.3.5 Covert Errors
James (1998) uses the term covert error to de-
scribe a genuine language error which results in a
sentence which is syntactically well-formed under
some interpretation different from the intended one.
The prominence of covert errors in our automati-
cally created error corpus is estimated by manually
inspecting 100 sentences of each error type. The per-
centage of grammatical structures that are inadver-
tently produced for each error type and an example
of each one are shown below:
? Agreement Errors, 7%
Mary?s staff include Jones,Smith and Murphy
> Mary?s staff includes Jones,Smith and Mur-
phy
? Real-Word Spelling Errors, 10%
And then? > And them?
? Extra Word Errors, 5%
in defiance of the free rider prediction > in de-
fiance of the free rider near prediction
? Missing Word Errors, 13%
She steered Melissa round a corner > She
steered round a corner
The occurrence of these covert errors can be re-
duced by fine-tuning the error creation procedure but
they can never be completely eliminated. Indeed,
they should not be eliminated from the test data,
because, ideally, an optimal error detection system
should be sophisticated enough to flag syntactically
well-formed sentences containing covert errors as
potentially ill-formed.1
4 Error Detection Evaluation
In this section we present the error detection eval-
uation experiments. The experimental setup is ex-
plained in Section 4.1, the results are presented in
Section 4.2 and they are analysed in Section 4.3.
1An example of this is given in the XLE User Documen-
tation (http://www2.parc.com/isl/groups/nltt/
xle/doc/). The authors remark that an ungrammatical read-
ing of the sentence Lets go to the store in which Lets is missing
an apostrophe, is preferable to the grammatical yet implausible
analysis in which Lets is a plural noun.
116
4.1 Experimental Setup
4.1.1 Test Data and Evaluation Procedure
The following steps are carried out to produce
training and test data for this experiment:
1. Speech material, poems, captions and list items
are removed from the BNC. 4.2 million sen-
tences remain. The order of sentences is ran-
domised.
2. For the purpose of cross-validation, the corpus
is split into 10 parts.
3. Each part is passed to the 4 automatic error in-
sertion modules described in Section 3.3, re-
sulting in 40 additional sets of varying size.
4. The first 60,000 sentences of each of the 50
sets, i. e. 3 million sentences, are parsed with
XLE.2
5. N-gram frequency information is extracted for
the first 60,000 sentences of each set. An addi-
tional 20,000 is extracted as held-out data.
6. 10 sets with mixed error types are produced by
joining a quarter of each respective error set.
7. For each error type (including mixed errors)
and cross-validation set, the 60,000 grammat-
ical and 60,000 ungrammatical sentences are
joined.
8. Each cross-validation run uses one set out of
the 10 as test data (120,000 sentences) and the
remaining 9 sets for training (1,080,000 sen-
tences).
The experiment is a standard binary classification
task. The methods classify the sentences of the test
sets as grammatical or ungrammatical. We use the
standard measures of precision, recall, f-score and
accuracy (Figure 1). True positives are understood
to be ungrammatical sentences that are identified as
such. The baseline precision and accuracy is 50%
as half of the test data is ungrammatical. If 100%
of the test data is classified as ungrammatical, re-
call will be 100% and f-score 2/3. Recall shows
the accuracy we would get if the grammatical half
of the test data was removed. Parametrised methods
2We use the XLE command parse-testfile with parse-
literally set to 1, max xle scratch storage set to 1,000 MB, time-
out to 60 seconds, and the XLE English LFG. Skimming is not
switched on and fragments are.
Measure Formula
precision tp/(tp + fp)
recall tp/(tp + fn)
f-score 2pr ? re/(pr + re)
accuracy (tp + tn)/(tp + tn + fp + fn)
Figure 1: Evaluation measures: tp = true positives,
fp = false positives, tn = true negatives, fn = false
negatives, pr = precision, re = recall
are first optimised for accuracy and then the other
measures are taken. Therefore, f-scores below the
artificial 2/3 baseline are meaningful.
4.1.2 Method 1: Precision Grammar
According to the XLE documentation, a sentence
is marked with a star (*) if its optimal solution uses
a constraint marked as ungrammatical. We use this
star feature, parser exceptions and zero number of
parses to classify a sentence as ungrammatical.
4.1.3 Method 2: POS N-grams
In each cross-validation run, the full data of the
remaining 9 sets of step 2 of the data generation
(see Section 4.1.1) is used as a reference corpus of
0.9?4, 200, 000 = 3, 800, 000 assumedly grammat-
ical sentences. The reference corpora and data sets
are POS tagged with the IMS TreeTagger (Schmidt,
1994). Frequencies of POS n-grams (n = 2, . . . , 7)
are counted in the reference corpora. A test sentence
is flagged as ungrammatical if it contains an n-gram
below a fixed frequency threshold. Method 2 has
two parameters: n and the frequency threshold.
4.1.4 Method 3: Decision Trees on XLE Output
The XLE parser outputs additional statistics for
each sentence that we encode in six features:
? An integer indicating starredness (0 or 1) and
various parser exceptions (-1 for time out, -2
for exceeded memory, etc.)
? The number of optimal parses3
? The number of unoptimal parses
? The duration of parsing
? The number of subtrees
? The number of words
3The use of preferred versus dispreferred constraints are
used to distinguish optimal parses from unoptimal ones.
117
Training data for the decision tree learner is com-
posed of 9?60, 000 = 540, 000 feature vectors from
grammatical sentences and 9 ? 15, 000 = 135, 000
feature vectors from ungrammatical sentences of
each error type, resulting in equal amounts of gram-
matical and ungrammatical training data.
We choose the weka implementation of machine
learning algorithms for the experiments (Witten and
Frank, 2000). We use a J48 decision tree learner
with the default model.
4.1.5 Method 4: Decision Trees on N-grams
Method 4 follows the setup of Method 3. How-
ever, the features are the frequencies of the rarest
n-grams (n = 2, . . . , 7) in the sentence. Therefore,
the feature vector of one sentence contains 6 num-
bers.
4.1.6 Method 5: Decision Trees on Combined
Feature Sets
This method combines the features of Methods 3
and 4 for training a decision tree.
4.2 Results
Table 1 shows the results for Method 1, which uses
XLE starredness, parser exceptions4 and zero parses
to classify grammaticality. Table 2 shows the re-
sults for Method 2, the basic n-gram approach. Ta-
ble 3 shows the results for Method 3, which classi-
fies based on a decision tree of XLE features. The
results for Method 4, the n-gram-based decision tree
approach, are shown in Table 4. Finally, Table 5
shows the results for Method 5 which combines n-
gram and XLE features in decision trees.
In the case of Method 2, we first have to find opti-
mal parameters. As only very limited integer values
for n and the threshold are reasonable, an exhaustive
search is feasible. We considered n = 2, . . . , 7 and
frequency thresholds below 20,000. Separate held-
out data (400,000 sentences) is used in order to avoid
overfitting. Best accuracy is achieved with 5-grams
and a threshold of 4. Table 2 reports results with
these parameters.
4XLE parsing (see footnote 2 for configuration) runs out
of time for 0.7 % and out of memory for 2.5 % of sentences,
measured on training data of the first cross-validation run, i. e.
540,000 grammatical sentence and 135,000 of each error type.
14 sentences of 3 million caused the parser to terminate abnor-
mally.
Error type Pr. Re. F-Sc. Acc.
Agreement 66.2 64.6 65.4 65.8
Real-word 63.5 57.3 60.3 62.2
Extra word 64.4 59.7 62.0 63.4
Missing word 59.2 47.8 52.9 57.4
Mixed errors 63.5 57.3 60.3 62.2
Table 1: Classification results with XLE starredness,
parser exceptions and zero parses (Method 1)
Error type Pr. Re. F-Sc. Acc.
Agreement 58.6 51.7 55.0 57.6
Real-word 64.0 64.9 64.5 64.2
Extra word 64.8 67.3 66.0 65.4
Missing word 57.2 48.8 52.7 56.1
Mixed errors 61.5 58.2 59.8 60.8
Table 2: Classification results with 5-gram and fre-
quency threshold 4 (Method 2)
The standard deviation of results across cross-
validation runs is below 0.006 on all measures, ex-
cept for Method 4. Therefore we only report average
percentages. The highest observed standard devia-
tion is 0.0257 for recall of Method 4 on agreement
errors.
For Methods 3, 4 and 5, the decision tree learner
optimises accuracy and, in doing so, chooses a trade-
off between precision and recall.
4.3 Analysis
Both Method 1 (Table 1) and Method 2 (Table 2)
achieve above baseline accuracy for all error types.
However, Method 1, which uses the XLE starred
feature, parser exceptions and zero parses to de-
termine whether or not a sentence is grammatical,
slightly outperforms Method 2, which uses the fre-
Error type Pr. Re. F-Sc. Acc.
Agreement 67.0 79.3 72.6 70.1
Real-word 63.4 67.6 65.4 64.3
Extra word 63.0 66.4 64.7 63.7
Missing word 59.7 57.8 58.7 59.4
Mixed errors 63.4 67.8 65.6 64.4
Table 3: Classification results with decision tree on
XLE output (Method 3)
118
Error type Pr. Re. F-Sc. Acc.
Agreement 61.2 53.8 57.3 59.9
Real-word 65.3 64.3 64.8 65.1
Extra word 66.4 67.4 66.9 66.7
Missing word 59.1 49.2 53.7 57.5
Mixed errors 63.3 58.7 60.9 62.3
Table 4: Classification results with decision tree on
vectors of frequency of rarest n-grams (Method 4)
Error type Pr. Re. F-Sc. Acc.
Agreement 67.1 75.2 70.9 69.2
Real-word 65.8 70.7 68.1 67.0
Extra word 65.9 71.2 68.5 67.2
Missing word 61.2 58.0 59.5 60.6
Mixed errors 65.2 68.8 66.9 66.0
Table 5: Classification results with decision tree on
joined feature set (Method 5)
quency of POS 5-grams to detect an error. The
XLE deep-processing approach is better than the n-
gram-based approach for agreement errors (f-score
+10.4). Examining the various types of agree-
ment errors, we can see that this is especially the
case for singular subjects followed by plural cop-
ula verbs (recall +37.7) and determiner-noun num-
ber mismatches (recall +23.6 for singular nouns and
+18.0 for plural nouns), but not for plural subjects
followed by singular verbs (recall -24.0). The rela-
tively poor performance of Method 2 on agreement
errors involving determiners could be due to the lack
of agreement marking on the Penn Treebank deter-
miner tag used by TreeTagger.
Method 1 is outperformed by Method 2 for real-
word spelling and extra word errors (f-score -4.2,
-4.0). Unsurprisingly, Method 2 has an advantage
on those real-word spelling errors that change the
POS (recall -8.8 for Method 1). Both methods per-
form poorly on missing word errors. For both meth-
ods there are only very small differences in perfor-
mance between the various missing word error sub-
types (identified by the POS of the deleted word).
Method 3, which uses machine learning to exploit
all the information returned by the XLE parser, im-
proves performance from Method 1, the basic XLE
method, for all error types.5 The general improve-
ment comes from an improvement in recall, mean-
ing that more ungrammatical sentences are actu-
ally flagged as such without compromising preci-
sion. The improvement is highest for agreement
errors (f-score +7.2). Singular subject with plural
copula errors (e. g. The man are) peak at a recall of
91.0. The Method 3 results indicate that information
on the number of solutions (optimal and unoptimal),
the number of subtrees, the time taken to parse the
sentence and the number of words can be used to
predict grammaticality. It would be interesting to
investigate this approach with other parsers.
Method 4, which uses a decision tree with n-
gram-based features, confirms the results of Method
2. The decision trees? root nodes are similar or even
identical (depending on cross-validation run) to the
decision rule of Method 2 (5-gram frequency below
4). However, the 10 decision trees have between
1,111 and 1,905 nodes and draw from all features,
even bigrams and 7-grams that perform poorly on
their own. The improvements are very small though
and they are not significant according the criterion of
non-overlapping cross-validation results. The main
reason for the evaluation of Method 4 is to provide
another reference point for comparison of the final
method.
The overall best results are those for Method 5,
the combined XLE, n-gram and machine-learning-
based method, which outperforms the next best
method, Method 3, on all error types apart from
agreement errors (f-score -1.7, +2.7, +3.8, +0.8).
For agreement errors, it seems that the relatively
poor results for n-grams have a negative effect on the
relatively good results for the XLE. Figure 2 shows
that the performance is almost constant on ungram-
matical data in the important sentence length range
from 5 to 40. However, there is a negative correla-
tion of accuracy and sentence length for grammati-
cal sentences. Very long sentences of any kind tend
to be classified as ungrammatical, except for missing
word errors which remain close to the 50% baseline
of coin-flipping.
For all methods, missing word errors are the
worst-performing, particularly in recall (i. e. the ac-
5The +0.3 increase in average accuracy for extra word errors
is not clearly significant as the results of cross-validation runs
overlap.
119
Figure 2: Accuracy by sentence length for Method 5
measured on separate grammatical and ungrammat-
ical data: Gr = Grammatical, AG = Agreement, RW
= Real-Word, EW = Extra Word, MW = Missing
Word
curacy on ungrammatical data alone). This means
that the omission of a word is less likely to result in
the sentence being flagged as erroneous. In contrast,
extra word errors perform consistently and relatively
well for all methods.
5 Conclusion and Future Work
We evaluated a deep processing approach and a POS
n-gram-based approach to the automatic detection of
common grammatical errors in a BNC-derived arti-
ficial error corpus. The results are broken down by
error type. Together with the deep approach, a deci-
sion tree machine learning algorithm can be used ef-
fectively. However, extending the shallow approach
with the same learning algorithm gives only small
improvements. Combining the deep and shallow ap-
proaches gives an additional improvement on all but
one error type.
Our plan is to investigate why all methods per-
form poorly on missing word errors, to extend the
error creation procedure so that it includes a wider
range of errors, to try the deep approach with other
parsers, to integrate additional features from state-
of-the-art shallow techniques and to repeat the ex-
periments for languages other than English.
Acknowledgements
This work is supported by the IRCSET Embark Ini-
tiative (basic research grant SC/02/298 and postdoc-
toral fellowship P/04/232). The training and test
data used in this reseach is based on the British Na-
tional Corpus (BNC), distributed by Oxford Univer-
sity Computing Services on behalf of the BNC Con-
sortium. We thank Djame? Seddah for helping us to
run the XLE parsing on the SFI/HEA Irish Centre
for High-End Computing (ICHEC) and the authors
wish to acknowledge ICHEC for the provision of
computational facilities and support.
References
Eric Atwell. 1987. How to detect grammatical errors in
a text without parsing it. In Proceedings of the 3rd
EACL, pages 38?45, Morristown, NJ.
Timothy Baldwin, John Beavers, Emily M. Bender, Dan
Flickinger, Ara Kim, and Stephan Oepen. 2004.
Beauty and the beast: What running a broad-coverage
precision grammar over the BNC taught us about the
grammar - and the corpus. In Pre-Proceedings of the
International Conference on Linguistic Evidence: Em-
pirical, Theoretical and Computational Perspectives,
pages 21?26.
Markus Becker, Andrew Bredenkamp, Berthold Crys-
mann, and Judith Klein. 1999. Annotation of error
types for German news corpus. In Proceedings of the
ATALA Workshop on Treebanks, Paris, France.
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Timothy Baldwin. 2004. Arboretum: Using a preci-
sion grammar for grammar checking in CALL. In Pro-
ceedings of the InSTIL/ICALL Symposium: NLP and
Speech Technologies in Advanced Language Learning
Systems, Venice, Italy.
Johnny Bigert and Ola Knutsson. 2002. Robust error
detection: a hybrid approach combining unsupervised
error detection and linguistic knowledge. In Proceed-
ings RO-MAND-02, Frascati, Italy.
Johnny Bigert. 2004. Probabilistic detection of context-
sensitive spelling errors. In Proceedings of LREC-04,
volume Five, pages 1633?1636, Lisbon, Portugal.
Phil Blunsom and Timothy Baldwin. 2006. Multilingual
deep lexical acquisition for HPSGs via supertagging.
In Proceedings of EMNLP-06, pages 164?171, Syd-
ney.
Lou Burnard. 2000. User reference guide for the British
national corpus. Technical report, Oxford University
Computing Services.
120
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The par-
allel grammar project. In Proceedings of COLING-
2002 Workshop on Grammar Engineering and Evalu-
ation, pages 1?7, Morristown, NJ, USA.
Eugene Charniak. 1996. Tree-bank grammars. Tech-
nical Report CS-96-02, Department of Computer Sci-
ence, Brown University.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proceedings of NAACL-00, pages 140?147, San Fran-
cisco, CA.
Noam Chomsky. 1957. Syntactic Structures. Mouton.
Ann Copestake and Dan Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of LREC-02, Athens, Greece.
Izumi Emi, Kiyotaka Uchimoto, and Hitoshi Isahara.
2004. The overview of the SST speech corpus of
Japanese learner English and evaluation through the
experiment on automatic detection of learners? er-
rors. In Proceedings of LREC-04, volume Four, pages
1435?1439, Lisbon, Portugal.
Jennifer Foster and Carl Vogel. 2004. Good reasons
for noting bad grammar: Constructing a corpus of un-
grammatical language. In Stephan Kepser and Marga
Reis, editors, Pre-Proceedings of the International
Conference on Linguistic Evidence: Empirical, The-
oretical and Computational Perspectives, pages 151?
152, Tu?bingen, Germany.
Jennifer Foster. 2005. Good Reasons for Noting Bad
Grammar: Empirical Investigations into the Parsing
of Ungrammatical Written English. Ph.D. thesis, Uni-
versity of Dublin, Trinity College, Dublin, Ireland.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John Maxwell. 1998. Optimality theory style con-
straint ranking in large-scale LFG grammars. In Pro-
ceedings of LFG-98, Brisbane, Australia.
Andrew R. Golding. 1995. A Bayesian hybrid method
for context-sensitive spelling correction. In Proceed-
ings of the Third Workshop on Very Large Corpora,
pages 39?53, Boston, MA.
Sylviane Granger. 1993. International corpus of learner
English. In J. Aarts, P. de Haan, and N.Oostdijk, ed-
itors, English Language Corpora: Design, Analysis
and Exploitation, pages 57?71. Rodopi, Amsterdam.
Carl James. 1998. Errors in Language Learning and
Use: Exploring Error Analysis. Addison Wesley
Longman.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar: a formal system for grammatical represen-
tation. In Joan Bresnan, editor, The Mental Represen-
tation of Grammatical Relations, pages 173?281. MIT
Press.
Rafiq Abdul Khader, Tracy Holloway King, and Miriam
Butt. 2004. Deep CALL grammars: The LFG-
OT experiment. http://ling.uni-konstanz.de/pages/
home/butt/dgfs04call.pdf.
John Maxwell and Ron Kaplan. 1996. An Efficient
Parser for LFG. In Proceedings of LFG-96, Grenoble.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Information
Processing and Management, 23(5):517?522.
D. Nicholls. 1999. The Cambridge learner corpus ? error
coding and analysis. In Summer Workshop on Learner
Corpora, Tokyo, Japan.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press and
CSLI Publications.
Alan Prince and Paul Smolensky. 1993. Optimality The-
ory. MIT Press, Cambridge, Massachusetts.
Veit Reuer. 2003. PromisD - Ein Analyseverfahren
zur antizipationsfreien Erkennung und Erkla?rung von
grammatischen Fehlern in Sprachlehrsystemen. Ph.D.
thesis, Humboldt-Universita?t zu Berlin, Berlin, Ger-
many.
Helmut Schmidt. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, England.
J.P. Stemberger. 1982. Syntactic errors in speech. Jour-
nal of Psycholinguistic Research, 11(4):313?45.
Anne Vandeventer Faltin. 2003. Syntactic Error Diag-
nosis in the context of Computer Assisted Language
Learning. Ph.D. thesis, Universite? de Gene`ve.
L. Amber Wilcox-O?Hearn, Graeme Hirst, and Alexan-
der Budanitsky. 2006. Real-word spelling correc-
tion with trigrams: A reconsideration of the Mays,
Damerau, and Mercer model. http://ftp.cs.toronto.edu/
pub/gh/WilcoxOHearn-etal-2006.pdf.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann Publishers.
121
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 221?224,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Adapting a WSJ-Trained Parser to Grammatically Noisy Text
Jennifer Foster, Joachim Wagner and Josef van Genabith
National Centre for Language Technology
Dublin City University
Ireland
jfoster, jwagner, josef@computing.dcu.ie
Abstract
We present a robust parser which is trained on
a treebank of ungrammatical sentences. The
treebank is created automatically by modify-
ing Penn treebank sentences so that they con-
tain one or more syntactic errors. We eval-
uate an existing Penn-treebank-trained parser
on the ungrammatical treebank to see how it
reacts to noise in the form of grammatical er-
rors. We re-train this parser on the training
section of the ungrammatical treebank, lead-
ing to an significantly improved performance
on the ungrammatical test sets. We show how
a classifier can be used to prevent performance
degradation on the original grammatical data.
1 Introduction
The focus in English parsing research in recent years
has moved from Wall Street Journal parsing to im-
proving performance on other domains. Our re-
search aim is to improve parsing performance on
text which is mildly ungrammatical, i.e. text which
is well-formed enough to be understood by people
yet which contains the kind of grammatical errors
that are routinely produced by both native and non-
native speakers of a language. The intention is not
to detect and correct the error, but rather to ignore
it. Our approach is to introduce grammatical noise
into WSJ sentences while retaining as much of the
structure of the original trees as possible. These
sentences and their associated trees are then used
as training material for a statistical parser. It is im-
portant that parsing on grammatical sentences is not
harmed and we introduce a parse-probability-based
classifier which allows both grammatical and un-
grammatical sentences to be accurately parsed.
2 Background
Various strategies exist to build robustness into the
parsing process: grammar constraints can be relaxed
(Fouvry, 2003), partial parses can be concatenated to
form a full parse (Penstein Rose? and Lavie, 1997),
the input sentence can itself be transformed until a
parse can be found (Lee et al, 1995), and mal-rules
describing particular error patterns can be included
in the grammar (Schneider and McCoy, 1998). For a
parser which tends to fail when faced with ungram-
matical input, such techniques are needed. The over-
generation associated with a statistical data-driven
parser means that it does not typically fail on un-
grammatical sentences. However, it is not enough
to return some analysis for an ungrammatical sen-
tence. If the syntactic analysis is to guide semantic
analysis, it must reflect as closely as possible what
the person who produced the sentence was trying to
express. Thus, while statistical, data-driven parsing
has solved the robustness problem, it is not clear that
it is has solved the accurate robustness problem.
The problem of adapting parsers to accurately
handle ungrammatical text is an instance of the do-
main adaptation problem where the target domain is
grammatically noisy data. A parser can be adapted
to a target domain by training it on data from the new
domain ? the problem is to quickly produce high-
quality training material. Our solution is to simply
modify the existing training material so that it re-
sembles material from the noisy target domain.
In order to tune a parser to syntactically ill-formed
text, a treebank is automatically transformed into an
ungrammatical treebank. This transformation pro-
cess has two parts: 1. the yield of each tree is trans-
formed into an ungrammatical sentence by introduc-
ing a syntax error; 2. each tree is minimally trans-
formed, but left intact as much as possible to reflect
the syntactic structure of the original ?intended? sen-
221
tence prior to error insertion. Artificial ungrammati-
calities have been used in various NLP tasks (Smith
and Eisner, 2005; Okanohara and Tsujii, 2007)
The idea of an automatically generated ungram-
matical treebank was proposed by Foster (2007).
Foster generates an ungrammatical version of the
WSJ treebank and uses this to train two statistical
parsers. The performance of both parsers signifi-
cantly improves on the artificially created ungram-
matical test data, but significantly degrades on the
original grammatical test data. We show that it
is possible to obtain significantly improved perfor-
mance on ungrammatical data without a concomi-
tant performance decline on grammatical data.
3 Generating Noisy Treebanks
Generating Noisy Sentences We apply the error
introduction procedure described in detail in Foster
(2007). Errors are introduced into sentences by ap-
plying the operations of word substitution, deletion
and insertion. These operations can be iteratively
applied to generate increasingly noisy sentences.
We restrict our attention to ungrammatical sentences
with a edit-distance of one or two words from the
original sentence, because it is reasonable to expect
a parser?s performance to degrade as the input be-
comes more ill-formed. The operations of substitu-
tion, deletion and insertion are not carried out en-
tirely at random, but are subject to some constraints
derived from an empirical study of ill-formed En-
glish sentences (Foster, 2005). Three types of word
substitution errors are produced: agreement errors,
real word spelling errors and verb form errors. Any
word that is not an adjective or adverb can be deleted
from any position within the input sentence, but
some part-of-speech tags are favoured over others,
e.g. it is more likely that a determiner will be deleted
than a noun. The error creation procedure can insert
an arbitrary word at any position within a sentence
but it has a bias towards inserting a word directly af-
ter the same word or directly after a word with the
same part of speech. The empirical study also in-
fluences the frequency at which particular errors are
introduced, with missing word errors being the most
frequent, followed by extra word errors, real word
spelling errors, agreement errors, and finally, verb
form errors. Table 1 shows examples of the kind of
ill-formed sentences that are produced when we ap-
ply the procedure to Wall Street Journal sentences.
Generating Trees for Noisy Sentences The tree
structures associated with the modified sentences are
also modified, but crucially, this modification is min-
imal, since a truly robust parser should return an
analysis for a mildly ungrammatical sentence that
remains as similar as possible to the analysis it re-
turns for the original grammatical sentence.
Assume that (1) is an original treebank tree for the
sentence A storm is brewing. Example (2) is then the
tree for the ungrammatical sentence containing an
is/it confusion. No part of the original tree structure
is changed apart from the yield.
(1) (S (NP A storm) (VP (VBZ is) (VP (VBG brewing))))
(2) (S (NP A storm) (VP (VBZ it) (VP (VBG brewing))))
An example of a missing word error is shown in
(3) and (4). A pre-terminal dominating an empty
node is introduced into the tree at the point where
the word has been omitted.
(3) (S (NP Annotators) (VP (VBP parse) (NP the sentences)))
(4) (S (NP Annotators) (VP (-NONE- 0) (NP the sentences)))
An example of an extra word error is shown in (5),
(6) and (7). For this example, two ungrammatical
trees, (6) and (7), are generated because there are
two possible positions in the original tree where the
extra word can be inserted which will result in a tree
with the yield He likes of the cake and which will not
result in the creation of any additional structure.
(5) (S (NP He) (VP (VBZ likes) (NP (DT the) (NN cake))))
(6) (S (NP He) (VP (VBZ likes) (IN of) (NP (DT the) (NN
cake))))
(7) (S (NP He) (VP (VBZ likes) (NP (IN of) (DT the) (NN
cake))))
4 Parser Adaptation Experiments
In order to obtain training data for our parsing ex-
periments, we introduce syntactic noise into the
usual WSJ training material, Sections 2-21, using
the procedures outlined in Section 3, i.e. for every
sentence-tree pair in WSJ2-21, we introduce an er-
ror into the sentence and then transform the tree so
that it covers the newly created ungrammatical sen-
tence. For 4 of the 20 sections in WSJ2-21, we apply
the noise introduction procedure to its own output to
222
Error Type WSJ00
Missing Word likely to bring new attention to the problem ? likely to new attention to the problem
Extra Word the $ 5.9 million it posted ? the $ 5.9 million I it posted
Real Word Spell Mr Vinken is chairman of Elsevier? Mr. Vinken if chairman of Elsevier
Agreement this event took place 35 years ago? these event took place 35 years ago
Verb Form But the Soviets might still face legal obstacles? But the Soviets might still faces legal obstacles
Table 1: Automatically Generated Ungrammatical Sentences
create even noisier data. Our first development set is
a noisy version of WSJ00, Noisy00, produced by ap-
plying the noise introduction procedure to the 1,921
sentences in WSJ00. Our second development set is
an even noisier version of WSJ00, Noisiest00, which
is created by applying our noise introduction proce-
dure to the output of Noisy00. We apply the same
process to WSJ23 to obtain our two test sets.
For all our parsing experiments, we use the June
2006 version of the two-stage parser reported in
Charniak and Johnson (2005). Evaluation is carried
out using Parseval labelled precision/recall. For ex-
tra word errors, there may be more than one gold
standard tree (see (6) and (7)). When this happens
the parser output tree is evaluated against all gold
standard trees and the maximum f-score is chosen.
We carry out five experiments. In the first ex-
periment, E0, we apply the parser, trained on well-
formed data, to noisy input. The purpose of E0 is to
ascertain how well a parser trained on grammatical
sentences, can ignore grammatical noise. E0 pro-
vides a baseline against which the subsequent ex-
perimental results can be judged. In the E1 experi-
ments, the parser is retrained using the ungrammati-
cal version of WSJ2-21. In experiment E1error, the
parser is trained on ungrammatical material only,
i.e. the noisy version of WSJ2-21. In experiment
E1mixed, the parser is trained on grammatical and
ungrammatical material, i.e. the original WSJ2-21 is
merged with the noisy WSJ2-21. In the E2 experi-
ments, a classifier is applied to the input sentence.
If the sentence is classified as ungrammatical, a ver-
sion of the parser that has been trained on ungram-
matical data is employed. In the E2ngram experi-
ment, we train a J48 decision tree classifier. Follow-
ing Wagner et al (2007), the decision tree features
are part-of-speech n-gram frequency counts, with n
ranging from 2 to 7 and with a subset of the BNC
as the frequency reference corpus. The decision tree
is trained on the original WSJ2-21 and the ungram-
matical WSJ2-21. In the E2prob experiment, the in-
put sentence is parsed with two parsers, the origi-
nal parser (the E0 parser) and the parser trained on
ungrammatical material (either the E1error or the
E1mixed parser). A very simple classifier is used
to decide which parser output to choose: if the E1
parser returns a higher parse probability for the most
likely tree than the E0 parser, the E1 parser output is
returned. Otherwise the E0 parser output is returned.
The baseline E0 results are in the first column of
Table 2. As expected, the performance of a parser
trained on well-formed input degrades when faced
with ungrammatical input. It is also not surprising
that its performance is worse on Noisiest00 (-8.8%
f-score) than it is on Noisy00 (-4.3%) since the Nois-
iest00 sentences contain two errors rather than one.
The E1 results occupy the second and third
columns of Table 2. An up arrow indicates a sta-
tistically significant improvement over the baseline
results, a down arrow a statistically significant de-
cline and a dash a change which is not statistically
significant (p < 0.01). Training the parser on un-
grammatical data has a positive effect on its perfor-
mance on Noisy00 and Noisiest00 but has a negative
effect on its performance on WSJ00. Training on a
combination of grammatical and ungrammatical ma-
terial gives the best results for all three development
sets. Therefore, for the E2 experiments we use the
E1mixed parser rather than the E1error parser.
The E2 results are shown in the last two columns
of Table 2 and the accuracy of the two classifiers in
Table 3. Over the three test sets, the E2prob classi-
fier outperforms the E2ngram classifier. Both classi-
fiers misclassify approximately 45% of the Noisy00
sentences. However, the sentences misclassified by
the E2prob classifier are those that are handled well
by the E0 parser, and this is reflected in the pars-
ing results for Noisy00. An important feature of the
223
Dev Set P R F P R F P R F P R F P R F
E0 E1-error E1-mixed E2prob E2ngram
WSJ00 91.5 90.3 90.9 91.0? 89.4 ? 90.2 91.3? 89.8 ? 90.5 91.5? 90.2? 90.9 91.3? 89.9? 90.6
Noisy00 87.5 85.6 86.6 89.4 ? 86.6 ? 88.0 89.4 ? 86.8 ? 88.1 89.1 ? 86.8 ? 87.9 88.7? 86.2? 87.5
Noisiest00 83.5 80.8 82.1 87.6 ? 83.6 ? 85.6 87.6 ? 83.8 ? 85.7 87.2 ? 83.7 ? 85.4 86.6? 83.0? 84.8
Table 2: Results of Parsing Experiments
Development Set E2prob E2ngram
WSJ00 76.7% 63.3%
Noisy00 55.1% 55.6%
Noisiest00 70.2% 66.0%
Table 3: E2 Classifier Accuracy
Test Set P R F P R F
E0 E2prob
WSJ23 91.7 90.8 91.3 91.7? 90.7? 91.2
Noisy23 87.4 85.6 86.5 89.2 ? 87.0 ? 88.1
Noisiest23 83.2 80.8 82.0 87.4 ? 84.1 ? 85.7
Table 4: Final Results for Section 23 Test Sets
E2prob classifier is that its use results in a constant
performance on the grammatical data - with no sig-
nificant degradation from the baseline.
Taking the E2prob results as our optimum, we
carry out the same experiment again on our WSJ23
test sets. The results are shown in Table 4. The same
effect can be seen for the test sets as for the devel-
opment sets - a significantly improved performance
on the ungrammatical data without an accompany-
ing performance decrease for the grammatical data.
The Noisy23 breakdown by error type is shown in
Table 5. The error type which the original parser is
most able to ignore is an agreement error. For this er-
ror type alone, the ungrammatical training material
seems to hinder the parser. The biggest improve-
ment occurs for real word spelling errors.
5 Conclusion
We have shown that it is possible to tune a WSJ-
trained statistical parser to ungrammatical text with-
Error Type P R F P R F
E0 E2-prob
Missing Word 88.5 83.7 86.0 88.9 84.3 86.5
Extra Word 87.2 89.4 88.3 89.2 89.7 89.4
Real Word Spell 84.3 83.0 83.7 89.5 88.2 88.9
Agreement 90.4 88.8 89.6 90.3 88.6 89.4
Verb Form 88.6 87.0 87.8 89.1 87.9 88.5
Table 5: Noisy23: Breakdown by Error Type
out affecting its performance on grammatical text.
This has been achieved using an automatically gen-
erated ungrammatical version of the WSJ treebank
and a simple binary classifier which compares parse
probabilities. The next step in this research is to see
how the method copes on ?real? errors - this will re-
quire manual parsing of a suitably large test set.
Acknowledgments We thank the IRCSET Em-
bark Initiative (postdoctoral fellowship P/04/232)
for supporting this research.
References
Eugene Charniak and Mark Johnson. 2005. Course-to-fine n-
best-parsing and maxent discriminative reranking. In Pro-
ceedings of ACL-2005.
Jennifer Foster. 2005. Good Reasons for Noting Bad Gram-
mar: Empirical Investigations into the Parsing of Ungram-
matical Written English. Ph.D. thesis, University of Dublin,
Trinity College.
Jennifer Foster. 2007. Treebanks gone bad: Parser evaluation
and retraining using a treebank of ungrammatical sentences.
IJDAR, 10(3-4), December.
Frederik Fouvry. 2003. Robust Processing for Constraint-
based Grammar Formalisms. Ph.D. thesis, University of Es-
sex.
Kong Joo Lee, Cheol Jung Kweon, Jungyun Seo, and Gil Chang
Kim. 1995. A robust parser based on syntactic information.
In Proceedings of EACL-1995.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A discrimi-
native language model with pseudo-negative examples. In
Proceedings of ACL-2007.
Carolyn Penstein Rose? and Alon Lavie. 1997. An efficient dis-
tribution of labor in a two stage robust interpretation process.
In Proceedings of EMNLP-1997.
David Schneider and Kathleen McCoy. 1998. Recognizing
syntactic errors in the writing of second language learners.
In Proceedings of ACL/COLING-1998.
Noah A. Smith and Jason Eisner. 2005. Contrastive Estima-
tion: Training Log-Linear Models on Unlabeled Data. In
Proceedings of ACL-2005.
Joachim Wagner, Jennifer Foster, and Josef van Genabith.
2007. A comparative evaluation of deep and shallow ap-
proaches to the automatic detection of common grammatical
errors. In Proceedings of EMNLP-CoNLL-2007.
224
Proceedings of the 10th Conference on Parsing Technologies, pages 33?35,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Adapting WSJ-Trained Parsers to the British National Corpus Using
In-Domain Self-Training
Jennifer Foster, Joachim Wagner, Djame? Seddah and Josef van Genabith
National Centre for Language Technology
School of Computing, Dublin City University, Dublin 9, Ireland
{jfoster, jwagner, josef}@computing.dcu.ie, dseddah@paris4.sorbonne.fr?
Abstract
We introduce a set of 1,000 gold standard
parse trees for the British National Corpus
(BNC) and perform a series of self-training
experiments with Charniak and Johnson?s
reranking parser and BNC sentences. We
show that retraining this parser with a com-
bination of one million BNC parse trees
(produced by the same parser) and the orig-
inal WSJ training data yields improvements
of 0.4% on WSJ Section 23 and 1.7% on the
new BNC gold standard set.
1 Introduction
Given the success of statistical parsing models on
the Wall Street Journal (WSJ) section of the Penn
Treebank (PTB) (Charniak, 2000; Collins, 2003, for
example), there has been a change in focus in recent
years towards the problem of replicating this success
on genres other than American financial news sto-
ries. The main challenge in solving the parser adap-
tation problem are the resources required to con-
struct reliable annotated training examples.
A breakthrough has come in the form of research
by McClosky et al (2006a; 2006b) who show that
self-training can be used to improve parser perfor-
mance when combined with a two-stage reranking
parser model (Charniak and Johnson, 2005). Self-
training is the process of training a parser on its own
output, and earlier self-training experiments using
generative statistical parsers did not yield encour-
aging results (Steedman et al, 2003). McClosky et
al. (2006a; 2006b) proceed as follows: sentences
?Now affiliated to Lalic, Universite? Paris 4 La Sorbonne.
from the LA Times newspaper are parsed by a first-
stage generative statistical parser trained on some
seed training data (WSJ Sections 2-21) and the n-
best parse trees produced by this parser are reranked
by a discriminative reranker. The highest ranked
parse trees are added to the training set of the parser
and the parser is retrained. This self-training method
gives improved performance, not only on Section
23 of the WSJ (an absolute f-score improvement of
0.8%), but also on test sentences from the Brown
corpus (Francis and Kuc?era, 1979) (an absolute f-
score improvement of 2.6%).
In the experiments of McClosky et al (2006a;
2006b), the parse trees used for self-training come
from the same domain (American newspaper text)
as the parser?s original seed training material. Bac-
chiani et al (2006) find that self-training is ef-
fective when the parse trees used for self-training
(WSJ parse trees) come from a different domain to
the seed training data and from the same domain as
the test data (WSJ sentences). They report a per-
formance boost of 4.2% on WSJ Section 23 for a
generative statistical parser trained on Brown seed
data when it is self-trained using 200,000 WSJ parse
trees. However, McCloskey et al (2006b) report a
drop in performance for their reranking parser when
the experiment is repeated in the opposite direction,
i.e. with Brown data for self-training and testing,
and WSJ data for seed training. In contrast, we re-
port successful in-domain1 self-training experiments
with the BNC data as self-training and test material,
and with the WSJ-trained reranking parser used by
McCloskey et al (2006a; 2006b).
We parse the BNC (Burnard, 2000) in its entirety
1We refer to data as being in-domain if it comes from the
same domain as the test data and out-of-domain if it does not.
33
using the reranking parser of Charniak and Johnson
(2005). 1,000 BNC sentences are manually anno-
tated for constituent structure, resulting in the first
gold standard set for this corpus. The gold standard
set is split into a development set of 500 parse trees
and a test set of 500 parse trees and used in a series
of self-training experiments: Charniak and John-
son?s parser is retrained on combinations of WSJ
treebank data and its own parses of BNC sentences.
These combinations are tested on the BNC devel-
opment set and Section 00 of the WSJ. An optimal
combination is chosen which achieves a Parseval la-
belled bracketing f-score of 91.7% on Section 23
and 85.6% on the BNC gold standard test set. For
Section 23 this is an absolute improvement of 0.4%
on the baseline results of this parser, and for the
BNC data this is a statistically significant improve-
ment of 1.7%.
2 The BNC Data
The BNC is a 100-million-word balanced part-of-
speech-tagged corpus of written and transcribed
spoken English. Written text comprises 90% of the
BNC: 75% non-fictional and 25% fictional. To fa-
cilitate parsing with a WSJ-trained parser, some re-
versible transformations were applied to the BNC
data, e.g. British English spellings were converted
to American English and neutral quotes disam-
biguated. The reranking parser of Charniak and
Johnson (2005) was used to parse the BNC. 99.8%
of the 6 million BNC sentences obtained a parse,
with an average parsing speed of 1.4s per sentence.
A gold standard set of 1,000 BNC sentences was
constructed by one annotator by correcting the out-
put of the first stage of Charniak and Johnson?s
reranking parser. The sentences included in the gold
standard were chosen at random from the BNC, sub-
ject to the condition that they contain a verb which
does not occur in the training sections of the WSJ
section of the PTB (Marcus et al, 1993). A deci-
sion was made to select sentences for the gold stan-
dard set which differ from the sentences in the WSJ
training sections, and one way of finding different
sentences is to focus on verbs which are not attested
in the WSJ Sections 2-21. It is expected that these
gold standard parse trees can be used as training
data although they are used only as test and develop-
ment data in this work. Because they contain verbs
which do not occur in the parser?s training set, they
are likely to represent a hard test for WSJ-trained
parsers. The PTB bracketing guidelines (Bies et al,
1995) and the PTB itself were used as references by
the BNC annotator. Functional tags and traces were
not annotated. The annotator noticed that the PTB
parse trees sometimes violate the PTB bracketing
guidelines, and in these cases, the annotator chose
the analysis set out in the guidelines. It took approx-
imately 60 hours to build the gold standard set.
3 Self-Training Experiments
Charniak and Johnson?s reranking parser (June 2006
version) is evaluated against the BNC gold stan-
dard development set. Labelled precision (LP), re-
call (LR) and f-score measures2 for this parser are
shown in the first row of Table 1. The f-score of
83.7% is lower than the f-score of 85.2% reported
by McClosky et al (2006b) for the same parser on
Brown corpus data. This difference is reasonable
since there is greater domain variation between the
WSJ and the BNC than between the WSJ and the
Brown corpus, and all BNC gold standard sentences
contain verbs not attested in WSJ Sections 2-21.
We retrain the first-stage generative statistical
parser of Charniak and Johnson using combinations
of BNC trees (parsed using the reranking parser)
and WSJ treebank trees. We test the combinations
on the BNC gold standard development set and on
WSJ Section 00. Table 1 shows that parser accu-
racy increases with the size of the in-domain self-
training material.3 The figures confirm the claim of
McClosky et al (2006a) that self-training with a
reranking parsing model is effective for improving
parser accuracy in general, and the claim of Gildea
(2001) that training on in-domain data is effective
for parser adaption. They confirm that self-training
on in-domain data is effective for parser adaptation.
The WSJ Section 00 results suggest that, in order
to maintain performance on the seed training do-
main, it is necessary to combine BNC parse trees
2All scores are for the second stage of the parsing process,
i.e. the evaluation takes place after the reranking. All evalua-
tion is carried out using the Parseval labelled bracketing metrics,
with evalb and parameter file new.prm.
3The notation bnc500K+5wsj refers to a set of 500,000
parser output parse trees of sentences taken randomly from the
BNC concatenated with five copies of WSJ Sections 2-21.
34
BNC Development WSJ Section 00
Self-Training LP LR LF LP LR LF
- 83.6 83.7 83.7 91.6 90.5 91.0
bnc50k 83.7 83.7 83.7 90.0 88.0 89.0
bnc50k+1wsj 84.4 84.4 84.4 91.6 90.3 91.0
bnc250k 84.7 84.5 84.6 91.1 89.3 90.2
bnc250k+5wsj 85.0 84.9 85.0 91.8 90.5 91.2
bnc500k+5wsj 85.2 85.1 85.2 91.9 90.4 91.2
bnc500k+10wsj 85.1 85.1 85.1 91.9 90.6 91.2
bnc1000k+5wsj 86.5 86.2 86.3 91.7 90.3 91.0
bnc1000k+10wsj 86.1 85.9 86.0 92.0 90.5 91.3
bnc1000k+40wsj 85.5 85.5 85.5 91.9 90.6 91.3
BNC Test WSJ Section 23
- 84.0 83.7 83.9 91.8 90.9 91.3
bnc1000k+10wsj 85.7 85.4 85.6 92.3 91.1 91.7
Table 1: In-domain Self-Training Results
with the original seed training material during the
self-training phase.
Of the self-training combinations with above-
baseline improvements for both development sets,
the combination of 1,000K BNC parse trees and
Section 2-21 of the WSJ (multiplied by ten) yields
the highest improvement for the BNC data, and we
present final results with this combination for the
BNC gold standard test set and WSJ Section 23.
There is an absolute improvement on the original
reranking parser of 1.7% on the BNC gold standard
test set and 0.4% on WSJ Section 23. The improve-
ment on BNC data is statistically significant for both
precision and recall (p < 0.0002, p < 0.0002). The
improvement on WSJ Section 23 is statistically sig-
nificant for precision only (p < 0.003).
4 Conclusion and Future Work
We have introduced a set of 1,000 gold standard
parse trees for the BNC. We have performed self-
training experiments with Charniak and Johnson?s
reranking parser and sentences from the BNC. We
have shown that retraining this parser with a com-
bination of one million BNC parse trees (produced
by the same parser) and the original WSJ train-
ing data yields improvements of 0.4% on WSJ Sec-
tion 23 and 1.7% on the BNC gold standard sen-
tences. These results indicate that self-training on
in-domain data can be used for parser adaptation.
Our BNC gold standard set consists of sentences
containing verbs which are not in the WSJ train-
ing sections. We suspect that this makes the gold
standard set a hard test for WSJ-trained parsers, and
our results are likely to represent a lower bound for
WSJ-trained parsers on BNC data. When used as
training data, we predict that the novel verbs in the
BNC gold standard set add to the variety of train-
ing material, and will further help parser adaptation
from the WSJ domain ? a matter for further research.
Acknowledgments We thank the IRCSET Em-
bark Initiative (basic research grant SC/02/298
and postdoctoral fellowship P/04/232), Science
Foundation Ireland (Principal Investigator grant
04/IN.3/I527) and the Irish Centre for High End
Computing for supporting this research.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and Richard
Sproat. 2006. Map adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41?68.
Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre.
1995. Bracketing guidelines for treebank II style, Penn Tree-
bank project. Technical Report MS-CIS-95-06, University
of Pennsylvania.
Lou Burnard. 2000. User reference guide for the British Na-
tional Corpus. Technical report, Oxford University.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best-parsing and maxent discriminative reranking. In Pro-
ceedings of ACL-05, pages 173?180, Barcelona.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL-00, pages 132?139, Seattle.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):499?637.
W. Nelson Francis and Henry Kuc?era. 1979. Brown Corpus
Manual. Technical report, Brown University, Providence.
Daniel Gildea. 2001. Corpus variation and parser performance.
In Proceedings of EMNLP-01, pages 167?202, Barcelona.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguistics,
19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson. 2006a.
Effective self-training for parsing. In Proceedings of HLT-
NAACL-06, pages 152?159, New York.
David McClosky, Eugene Charniak, and Mark Johnson. 2006b.
Reranking and self-training for parser adaptation. In Pro-
ceedings of COLING-ACL-06, pages 337?344, Sydney.
Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Boot-strapping
statistical parsers from small datasets. In Proceedings of
EACL-03, Budapest.
35
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82?90,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
GenERRate: Generating Errors for Use in Grammatical Error Detection
Jennifer Foster
National Centre for Language Technology
School of Computing
Dublin City University, Ireland
jfoster@computing.dcu.ie
?istein E. Andersen
Computer Laboratory
University of Cambridge
United Kingdom
oa223@cam.ac.uk
Abstract
This paper explores the issue of automatically
generated ungrammatical data and its use in
error detection, with a focus on the task of
classifying a sentence as grammatical or un-
grammatical. We present an error generation
tool called GenERRate and show how Gen-
ERRate can be used to improve the perfor-
mance of a classifier on learner data. We de-
scribe initial attempts to replicate Cambridge
Learner Corpus errors using GenERRate.
1 Introduction
In recent years automatically generated ungrammat-
ical data has been used in the training and evalu-
ation of error detection systems, in evaluating the
robustness of NLP tools and as negative evidence
in unsupervised learning. The main advantage of
using such artificial data is that it is cheap to pro-
duce. However, it is of little use if it is not a real-
istic model of the naturally occurring and expensive
data that it is designed to replace. In this paper we
explore the issues involved in generating synthetic
data and present a tool called GenERRate which can
be used to produce many different kinds of syntacti-
cally noisy data. We use the tool in two experiments
in which we attempt to train classifiers to distinguish
between grammatical and ungrammatical sentences.
In the first experiment, we show how GenERRate
can be used to improve the performance of an ex-
isting classifier on sentences from a learner corpus
of transcribed spoken utterances. In the second ex-
periment we try to produce a synthetic error corpus
that is inspired by the Cambridge Learner Corpus
(CLC)1, and we evaluate the difference between a
classifier?s performance when trained on this data
and its performance when trained on original CLC
material. The results of both experiments provide
pointers on how to improve GenERRate, as well as
highlighting some of the challenges associated with
automatically generating negative evidence.
The paper is organised as follows: In Section 2,
we discuss the reasons why artificial ungrammatical
data has been used in NLP and we survey its use
in the field, focussing mainly on grammatical error
detection. Section 3 contains a description of the
GenERRate tool. The two classification experiments
which use GenERRate are described in Section 4.
Problematic issues are discussed in Section 5 and
avenues for future work in Section 6.
2 Background
2.1 Why artifical error data is useful
Before pointing out the benefit of using artificial
negative evidence in grammatical error detection, it
is worth reminding ourselves of the benefits of em-
ploying negative evidence, be it artificial or natu-
rally occurring. By grammatical error detection, we
mean either the task of distinguishing the grammat-
ical from the ungrammatical at the sentence level
or more local targeted error detection, involving the
identification, and possibly also correction, of par-
ticular types of errors. Distinguishing grammati-
cal utterances from ungrammatical ones involves the
use of a binary classifier or a grammaticality scor-
1http://www.cambridge.org/elt/corpus/
learner_corpus2.htm
82
ing model. Examples are Andersen (2006; 2007),
Okanohara and Tsujii (2007), Sun et al (2007) and
Wagner et al (2007). In targeted error detection,
the focus is on identifying the common errors made
either by language learners or native speakers (de-
pending on the application). For ESL applications,
this includes the detection of errors involving ar-
ticles (Han et al, 2006; De Felice and Pulman,
2008; Gamon et al, 2008), prepositions (De Felice
and Pulman, 2008; Gamon et al, 2008; Tetreault
and Chodorow, 2008), verb forms (Lee and Seneff,
2008b), mass/count noun confusions (Brockett et al,
2006) and word order (Metcalf and Meurers, 2006).
The presence of a pattern in a corpus of well-
formed language is positive evidence that the pat-
tern is well-formed. The presence of a pattern in an
corpus of ill-formed language is negative evidence
that the pattern is erroneous. Discriminative tech-
niques usually lead to more accurate systems than
those based on one class alone. The use of the two
types of evidence can be seen at work in the system
described by Lee and Seneff (2008b): Verb phrases
are parsed and their parse trees are examined. If
the parse trees resemble the ?disturbed? trees that
statistical parsers typically produce when an incor-
rect verb form is used, the verb phrase is consid-
ered a likely candidate for correction. However, to
avoid overcorrection, positive evidence in the form
of Google n-gram statistics is also employed: a cor-
rection is only applied if its n-gram frequency is
higher than that of the original uncorrected n-gram.
The ideal situation for a grammatical error de-
tection system is one where a large amount of la-
belled positive and negative evidence is available.
Depending on the aims of the system, this labelling
can range from simply marking a sentence as un-
grammatical to a detailed description of the error
along with a correction. If an error detection sys-
tem employs machine learning, the performance of
the system will improve as the training set size in-
creases (up to a certain point). For systems which
employ learning algorithms with large feature sets
(e.g. maximum entropy, support vector machines),
the size of the training set is particularly important
so that overfitting is avoided. The collection of a
large corpus of ungrammatical data requires a good
deal of manual effort. Even if the annotation only
involves marking the sentence as correct/incorrect,
it still requires that the sentence be read and a gram-
maticality judgement applied to it. If more detailed
annotation is applied, the process takes even longer.
Some substantially-sized annotated error corpora do
exist, e.g. the Cambridge Learner Corpus, but these
are not freely available.
One way around this problem of lack of availabil-
ity of suitably large error-annotated corpora is to in-
troduce errors into sentences automatically. In order
for the resulting error corpus to be useful in an error
detection system, the errors that are introduced need
to resemble those that the system aims to detect.
Thus, the process is not without some manual effort:
knowing what kind of errors to introduce requires
the inspection of real error data, a process similar
to error annotation. Once the error types have been
specified though, the process is fully automatic and
allows large error corpora to be compiled. If the set
of well-formed sentences into which the errors are
introduced is large and varied enough, it is possi-
ble that this will result in ungrammatical sentence
structures which learners produce but which have
not yet been recorded in the smaller naturally occur-
ring learner corpora. To put it another way, the same
type of error will appear in lexically and syntacti-
cally varied contexts, which is potentially advanta-
geous when training a classifier.
2.2 Where artificial error data has been used
Artificial errors have been employed previously in
targeted error detection. Sjo?bergh and Knutsson
(2005) introduce split compound errors and word or-
der errors into Swedish texts and use the resulting
artificial data to train their error detection system.
These two particular error types are chosen because
they are frequent errors amongst non-native Swedish
speakers whose first language does not contain com-
pounds or has a fixed word order. They compare the
resulting system to three Swedish grammar check-
ers, and find that their system has higher recall at the
expense of lower precision. Brockett et al (2006)
introduce errors involving mass/count noun confu-
sions into English newswire text and then use the re-
sulting parallel corpus to train a phrasal SMT system
to perform error correction. Lee and Seneff (2008b)
automatically introduce verb form errors (subject?
verb agreement errors, complementation errors and
errors in a main verb after an auxiliary) into well-
83
formed text, parse the resulting text and examine the
parse trees produced.
Both Okanohara and Tsujii (2007) and Wagner et
al. (2007) attempt to learn a model which discrimi-
nates between grammatical and ungrammatical sen-
tences, and both use synthetic negative data which
is obtained by distorting sentences from the British
National Corpus (BNC) (Burnard, 2000). The meth-
ods used to distort the BNC sentences are, however,
quite different. Okanohara and Tsujii (2007) gener-
ate ill-formed sentences by sampling a probabilistic
language model and end up with ?pseudo-negative?
examples which resemble machine translation out-
put more than they do learner texts. Indeed, ma-
chine translation is one of the applications of their
resulting discriminative language model. Wagner
et al (2007) introduce grammatical errors of the
following four types into BNC sentences: context-
sensitive spelling errors, agreement errors, errors in-
volving a missing word and errors involving an extra
word. All four types are considered equally likely
and the resulting synthetic corpus contains errors
that look like the kind of slips that would be made
by native speakers (e.g. repeated adjacent words) as
well as errors that resemble learner errors (e.g. miss-
ing articles). Wagner et al (2009) report a drop in
accuracy for their classification methods when ap-
plied to real learner texts as opposed to held-out syn-
thetic test data, reinforcing the earlier point that ar-
tificial errors need to be tailored for the task at hand
(we return to this in Section 4.1).
Artificial error data has also proven useful in
the automatic evaluation of error detection systems.
Bigert (2004) describes how a tool called Missplel
is used to generate context-sensitive spelling errors
which are then used to evaluate a context-sensitive
spelling error detection system. The performance
of general-purpose NLP tools such as part-of-speech
taggers and parsers in the face of noisy ungrammat-
ical data has been automatically evaluated using ar-
tificial error data. Since the features of machine-
learned error detectors are often part-of-speech n-
grams or word?word dependencies extracted from
parser output (De Felice and Pulman, 2008, for ex-
ample), it is important to understand how part-of-
speech taggers and parsers react to particular gram-
matical errors. Bigert et al (2005) introduce artifi-
cial context-sensitive spelling errors into error-free
Swedish text and then evaluate parsers and a part-
of-speech tagger on this text using their performance
on the error-free text as a reference. Similarly, Fos-
ter (2007) investigates the effect of common English
grammatical errors on two widely-used statistical
parsers using distorted treebank trees as references.
The procedure used by Wagner et al (2007; 2009) is
used to introduce errors into the treebank sentences.
Finally, negative evidence in the form of automat-
ically distorted sentences has been used in unsuper-
vised learning. Smith and Eisner (2005a; 2005b)
generate negative evidence for their contrastive es-
timation method by moving or removing a word in a
sentence. Since the aim of this work is not to detect
grammatical errors, there is no requirement to gener-
ate the kind of negative evidence that might actually
be produced by either native or non-native speakers
of a language. The negative examples are used to
guide the unsupervised learning of a part-of-speech
tagger and a dependency grammar.
We can conclude from this survey that synthetic
error data is useful in a variety of NLP applications,
including error detection and evaluation of error de-
tectors. In Section 3, we describe an automatic error
generation tool, which has a modular design and is
flexible enough to accommodate the generation of
the various types of synthetic data described above.
3 Error Generation Tool
GenERRate is an error generation tool which ac-
cepts as input a corpus and an error analysis file
consisting of a list of errors and produces an error-
tagged corpus of syntactically ill-formed sentences.
The sentences in the input corpus are assumed to be
grammatically well-formed. GenERRate is imple-
mented in Java and will be made available to down-
load for use by other researchers.2
3.1 Supported Error Types
Error types are defined in terms of their corrections,
that is, in terms of the operations (insert, delete, sub-
stitute and move) that are applied to a well-formed
sentence to make it ill-formed. As well as being
a popular classification scheme in the field of er-
ror analysis (James, 1998), it has the advantage of
2http://www.computing.dcu.ie/?jfoster/
resources/generrate.html
84
being theory-neutral. This is important in this con-
text since it is hoped that GenERRate will be used
to create negative evidence of various types, be it
L2-like grammatical errors, native speaker slips or
more random syntactic noise. It is hoped that Gen-
ERRate will be easy to use for anyone working in
linguistics, applied linguistics, language teaching or
computational linguistics.
The inheritance hierarchy in Fig. 1 shows the er-
ror types that are supported by GenERRate. We
briefly describe each error type.
Errors generated by removing a word
? DeletionError: Generated by selecting a word
at random from the sentence and removing it.
? DeletionPOSError: Extends DeletionError by
allowing a specific POS to be specified.
? DeletionPOSWhereError: Extends Deletion-
POSError by allowing left and/or right context
(POS tag or start/end) to be specified.
Errors generated by inserting a word
? InsertionError: Insert a random word at a ran-
dom position. The word is chosen either from
the sentence itself or from a word list, and this
choice is also random.
? InsertionFromFileOrSentenceError: This
differs from the InsertionError in that the de-
cision of whether to use the sentence itself or a
word list is not made at random but supplied in
the error type specification.
? InsertionPOSError: Extends InsertionFrom-
FileOrSentenceError by allowing the POS of
the new word to be specified.
? InsertionPOSWhereError: Analogous to the
DeletionPOSWhereError, this extends Inser-
tionPOSError by allowing left and/or right con-
text to be specified.
Errors generated by moving a word
? MoveError: Generated by randomly selecting
a word in the sentence and moving it to another
position, randomly chosen, in the sentence.
? MovePOSError: A word tagged with the
specified POS is randomly chosen and moved
to a randomly chosen position in the sentence.
? MovePOSWhereError: Extends Move-
POSError by allowing the change in position
subst,word,an,a,0.2
subst,NNS,NN,0.4
subst,VBG,TO,0.2
delete,DT,0.1
move,RB,left,1,0.1
Figure 2: GenERRate Toy Error Analysis File
to be specified in terms of direction and
number of words.
Errors generated by substituting a word
? SubstError: Replace a random word by a
word chosen at random from a word list.
? SubstWordConfusionError: Extends Sub-
stError by allowing the POS to be specified
(same POS for both words).
? SubstWordConfusionNewPOSError: Simi-
lar to SubstWordConfusionError, but allows
different POSs to be specified.
? SubstSpecificWordConfusionError:
Replace a specific word with another
(e.g. be/have).
? SubstWrongFormError: Replace a word
with a different form of the same word. The
following changes are currently supported:
noun number (e.g. word/words), verb number
(write/writes), verb form (writing/written), ad-
jective form (big/bigger) and adjective/adverb
(quick/quickly). Note that this is the only er-
ror type which is language-specific. At the mo-
ment, only English is supported.
3.2 Input Corpus
The corpus that is supplied as input to GenERRate
must be split into sentences. It does not have to be
part-of-speech tagged, but it will not be possible to
generate many of the errors if it is not. GenERRate
has been tested using two part-of-speech tagsets,
the Penn Treebank tagset (Santorini, 1991) and the
CLAWS tagset (Garside et al, 1987).
3.3 Error Analysis File
The error analysis file specifies the errors that Gen-
ERRate should attempt to insert into the sentences
in the input corpus. A toy example with the Penn
tagset is shown in Fig. 2. The first line is an instance
of a SubstSpecificWordConfusion error. The second
85
Error
Deletion
DeletionPOS
DeletionPOSWhere
Insertion
InsertionFromFileOrSentence
InsertionPOS
InsertionPOSWhere
Move
MovePOS
MovePOSWhere
Subst
SubstWordConfusion
SubstWordConfusionNewPOS SubstSpecificWordConfusion
SubstWrongForm
Figure 1: GenERRate Error Types
and third are instances of the SubstWrongFormEr-
ror type. The fourth is a DeletionPOSError, and the
fifth is a MovePOSWhereError. The number in the
final column specifies the desired proportion of the
particular error type in the output corpus and is op-
tional. However, if it is present for one error type, it
must be present for all. The overall size of the out-
put corpus is supplied as a parameter when running
GenERRate.
3.4 Error Generation
When frequency information is not supplied in the
error analysis file, GenERRate iterates through each
error in the error analysis file and each sentence in
the input corpus, tries to insert an error of this type
into the sentence and writes the resulting sentence
to the output file together with a description of the
error. GenERRate includes an option to write the
sentences into which an error could not be inserted
and the reason for the failure to a log file. When the
error analysis file does include frequency informa-
tion, a slightly different algorithm is used: for each
error, GenERRate selects sentences from the input
file and attempts to generate an instance of that error
until the desired number of errors has been produced
or all sentences have been tried.
4 Classification Experiments
We describe two experiments which involve the
use of GenERRate in a binary classification task
in which the classifiers attempt to distinguish be-
tween grammatically well-formed and ill-formed
sentences or, more precisely, to distinguish between
sentences in learner corpora which have been anno-
tated as erroneous and their corrected counterparts.
In the first experiment we use GenERRate to cre-
ate ungrammatical training data using information
about error types gleaned from a subset of a corpus
of transcribed spoken utterances produced by ESL
learners in a classroom environment. The classifier
is one of those described in Wagner et al (2007).
In the second experiment we try to generate a CLC-
inspired error corpus and we use one of the simplest
classifiers described in Andersen (2006). Our aim
is not to improve classification performance, but to
test the GenERRate tool, to demonstrate how it can
be used and to investigate differences between syn-
thetic and naturally occurring datasets.
4.1 Experiments with a Spoken Language
Learner Corpus
Wagner et al (2009) train various classifiers to
distinguish between BNC sentences and artificially
produced ungrammatical versions of BNC sentences
(see ?2). They report a significant drop in accuracy
when they apply these classifiers to real learner data,
including the sentences in a corpus of transcribed
spoken utterances. The aim of this experiment is to
investigate to what extent this loss in accuracy can
be reduced by using GenERRate to produce a more
realistic set of ungrammatical training examples.
The spoken language learner corpus contains over
4,000 transcribed spoken sentences which were pro-
duced by learners of English of all levels and with
a variety of L1s. The sentences were produced in
a classroom setting and transcribed by the teacher.
The transcriptions were verified by the students. All
86
of the utterances have been marked as erroneous.
4.1.1 Setup
A 200-sentence held-out section of the corpus is
analysed by hand and a GenERRate error analysis
file containing 89 errors is compiled. The most fre-
quent errors are those involving a change in noun or
verb number or an article deletion. GenERRate then
applies this error analysis file to 440,930 BNC sen-
tences resulting in the same size set of synthetic ex-
amples (?new-ungram-BNC?). Another set of syn-
thetic sentences (?old-ungram-BNC?) is produced
from the same input using the error generation pro-
cedure used by Wagner et al (2007; 2009). Table 1
shows examples from both sets.
Two classifiers are then trained, one on the orig-
inal BNC sentences and the old-ungram-BNC sen-
tences, and the other on the original BNC sentences
and the new-ungram-BNC sentences. Both classi-
fiers are tested on 4,095 sentences from the spo-
ken language corpus (excluding the held-out sec-
tion). 310 of these sentences are corrected, resulting
in a small set of grammatical test data. The classi-
fier used is the POS n-gram frequency classifier de-
scribed in Wagner et al (2007).3 The features are
the frequencies of the least frequent n-grams (2?7)
in the input sentence. The BNC (excluding those
sentences that are used as training data) is used as
reference data to compute the frequencies. Learning
is carried out using the Weka implementation of the
J48 decision tree algorithm.4
4.1.2 Results
The results of the experiment are displayed in Ta-
ble 2. The evaluation measures used are precision,
recall, total accuracy and accuracy on the grammat-
ical side of the test data. Recall is the same as accu-
racy on the ungrammatical side of the test data.
The results are encouraging. There is a signifi-
cant increase in accuracy when we train on the new-
ungram-BNC set instead of the old-ungram-BNC
set. This increase is on the ungrammatical side of
3Wagner et al (2009) report accuracy figures in the range
55?70% for their various classifiers (when tested on synthetic
test data), but the best performance is obtained by combining
parser-output and n-gram POS frequency features using deci-
sion trees in a voting scheme.
4http://www.cs.waikato.ac.nz/ml/weka/
the test data, i.e. an increase in recall, demonstrat-
ing that by analysing a small set of data from our
test domain, we can automatically create more effec-
tive training data. This is useful in a scenario where
a small-to-medium-sized learner corpus is available
but which is not large enough to be split into a train-
ing/development/test set. These results seem to indi-
cate that reasonably useful training data can be cre-
ated with minimum effort. Of course, the accuracy is
still rather low but we suspect that some of this dif-
ference can be explained by domain effects ? the
sentences in the training data are BNC written sen-
tences (or distorted versions of them) whereas the
sentences in the learner corpus are transcribed spo-
ken utterances. Re-running the experiments using
the spoken language section of the BNC as training
data might yield better results.
4.2 A CLC-Inspired Corpus
We investigate to what extent it is possible to cre-
ate a large error corpus inspired by the CLC using
the current version of GenERRate. The CLC is a
30-million-word corpus of learner English collected
from University of Cambridge ESOL exam papers
at different levels. Approximately 50% of the CLC
has been annotated for errors and corrected.
4.2.1 Setup
We attempt to use GenERRate to insert errors
into corrected CLC sentences. In order to do
this, we need to create a CLC-specific error anal-
ysis file. In contrast to the previous experiment,
we do this automatically by extracting erroneous
POS trigrams from the error-annotated CLC sen-
tences and encoding them as GenERRate errors.
This results in approximately 13,000 errors of the
following types: DeletionPOSWhereError, Inser-
tionPOSWhereError, MovePOSWhereError, Sub-
stWordConfusionError, SubstWordConfusionNew-
POSError, SubstSpecificWordConfusionError and
SubstWrongFormError. Frequencies are extracted,
and errors occurring only once are excluded.
Three classifiers are trained. The first is trained
on corrected CLC sentences (the grammatical sec-
tion of the training set) and original CLC sentences
(the ungrammatical section). The second classifier
is trained on corrected CLC sentences and the sen-
tences that are generated from the corrected CLC
87
Old-Ungram-BNC New-Ungram-BNC
Biogas production production is growing rapidly Biogas productions is growing rapidly
Emil as courteous and helpful Emil courteous and was helpful
I knows what makes you tick I know what make you tick
He did n?t bother to lift his eyes from the task hand He did n?t bother lift his eyes from the task at hand
Table 1: Examples from two synthetic BNC sets
Training Data Precision Recall Accuracy Accuracy on Grammatical
BNC/old-ungram-BNC 95.5 37.0 39.8 76.8
BNC/new-ungram-BNC 94.9 51.6 52.4 63.2
Table 2: Spoken Language Learner Corpus Classification Experiment
sentences using GenERRate (we call these ?faux-
CLC?). The third is trained on corrected CLC sen-
tences and a 50/50 combination of CLC and faux-
CLC sentences. In all experiments, the grammat-
ical section of the training data contains 438,150
sentences and the ungrammatical section 454,337.
The classifiers are tested on a held-out section of
the CLC containing 43,639 corrected CLC sen-
tences and 45,373 original CLC sentences. To train
the classifiers, the Mallet implementation of Naive
Bayes is used.5 The features are word unigrams
and bigrams, as well as part-of-speech unigrams, bi-
grams and trigrams. Andersen (2006) experimented
with various learning algorithms and, taking into ac-
count training time and performance, found Naive
Bayes to be optimal. The POS-tagging is carried out
by the RASP system (Briscoe and Carroll, 2002).
4.2.2 Results
The results of the CLC classification experiment
are presented in Table 3. There is a 6.2% drop in
accuracy when we move from training on original
CLC sentences to artificially generated sentences.
This is somewhat disappointing since it means that
we have not completely succeeded in replicating the
CLC errors using GenERRate. Most of the accu-
racy drop is on the ungrammatical side, i.e. the cor-
rect/faux model classifies more incorrect CLC sen-
tences as correct than the correct/incorrect model.
This drop in accuracy occurs because some fre-
quently occurring error types are not included in the
error analysis file. One reason for the gap in cover-
age is the failure of the part-of-speech tagset to make
some important distinctions. The corrected CLC
5http://mallet.cs.umass.edu/
sentences which were used to generate the faux-
CLC set were tagged with the CLAWS tagset, and
although more fine-grained than the Penn tagset, it
does not, for example, make a distinction between
mass and count nouns, a common source of error.
Another important reason for the drop in accuracy
are the recurrent spelling errors which occur in the
incorrect CLC test set but not in the faux-CLC test
set. It is promising, however, that much of the per-
formance degradation is recovered when a mixture
of the two types of ungrammatical training data is
used, suggesting that artificial data could be used to
augment naturally occurring training sets
5 Limitations of GenERRate
We present three issues that make the task of gener-
ating synthetic error data non-trivial.
5.1 Sophistication of Input Format
The experiments in ?4 highlight coverage issues
with GenERRate, some of which are due to the sim-
plicity of the supported error types. When linguis-
tic context is supplied for deletion or insertion er-
rors, it takes the form of the POS of the words im-
mediately to the left and/or right of the target word.
Lee and Seneff (2008a) analysed preposition errors
made by Japanese learners of English and found that
a greater proportion of errors in argument preposi-
tional phrases (look at him) involved a deletion than
those in adjunct PPs (came at night). The only way
for such a distinction to be encoded in a GenERRate
error analysis file is to allow parsed input to be ac-
cepted. This brings with it the problem, however,
that parsers are less accurate than POS-taggers. An-
other possible improvement would be to make use
88
Training Data Precision Recall Accuracy Accuracy on Grammatical
Held-Out Test Data
Correct/Incorrect CLC 69.7 42.6 61.3 80.8
Correct/Faux CLC 62.0 30.7 55.1 80.5
Correct/Incorrect+Faux CLC 69.7 38.2 60.0 82.7
Table 3: CLC Classification Experiment
of WordNet synsets in order to choose the new word
in substitution errors.
5.2 Covert Errors
A covert error is an error that results in a syntac-
tically well-formed sentence with an interpretation
different from the intended one. Covert errors are a
natural phenomenon, occurring in real corpora. Lee
and Seneff (2008b) give the example I am preparing
for the exam which has been annotated as erroneous
because, given its context, it is clear that the per-
son meant to write I am prepared for the exam. The
problems lie in deciding what covert errors should
be handled by an error detection system and how to
create synthetic data which gets the balance right.
When to avoid: Covert errors can be produced
by GenERRate as a result of the sparse linguistic
context provided for an error in the error analysis
file. An inspection of the new-ungram-BNC set
shows that some error types are more likely to re-
sult in covert errors. An example is the SubstWrong-
FormError when it is used to change a noun from
singular to plural. This results in the sentence But
there was no sign of Benny?s father being changed
to the well-formed but more implausible But there
was no sign of Benny?s fathers. The next version of
GenERRate should include the option to change the
form of a word in a certain context.
When not to avoid: In the design of GenERRate,
particularly in the design of the SubstWrongFormEr-
ror type, the decision was made to exclude tense er-
rors because they are likely to result in covert er-
rors, e.g. She walked home? She walks home. But
in doing so we also avoid generating examples like
this one from the spoken language learner corpus:
When I was a high school student, I go to bed at one
o?clock. These tense errors are common in L2 data
and their omission from the faux-CLC training set
is one of the reasons why the performance of this
model is inferior to the real-CLC model.
5.3 More complex errors
The learner corpora contain some errors that are
corrected by applying more than one transforma-
tion. Some are handled by the SubstWrongFormEr-
ror type (I spend a long time to fish?I spend a long
time fishing) but some are not (She is one of reason
I became interested in English ? She is one of the
reasons I became interested in English).
6 Conclusion
We have presented GenERRate, a tool for automati-
cally introducing syntactic errors into sentences and
shown how it can be useful for creating synthetic
training data to be used in grammatical error detec-
tion research. Although we have focussed on the
binary classification task, we also intend to test Gen-
ERRate in targeted error detection. Another avenue
for future work is to explore whether GenERRate
could be of use in the automatic generation of lan-
guage test items (Chen et al, 2006, for example).
Our immediate aim is to produce a new version of
GenERRate which tackles some of the coverage is-
sues highlighted by our experiments.
Acknowledgments
This paper reports on research supported by the Uni-
versity of Cambridge ESOL Examinations. We are
very grateful to Cambridge University Press for giv-
ing us access to the Cambridge Learner Corpus and
to James Hunter from Gonzaga College for sup-
plying us with the spoken language learner corpus.
We thank Ted Briscoe, Josef van Genabith, Joachim
Wagner and the reviewers for their very helpful sug-
gestions.
89
References
?istein E. Andersen. 2006. Grammatical error detection.
Master?s thesis, Cambridge University.
?istein E. Andersen. 2007. Grammatical error detection
using corpora and supervised learning. In Ville Nurmi
and Dmitry Sustretov, editors, Proceedings of the 12th
Student Session of the European Summer School for
Logic, Language and Information, Dublin.
Johnny Bigert, Jonas Sjo?bergh, Ola Knutsson, and Mag-
nus Sahlgren. 2005. Unsupervised evaluation of
parser robustness. In Proceedings of the 6th CICling,
Mexico City.
Johnny Bigert. 2004. Probabilistic detection of context-
sensitive spelling errors. In Proceedings of the 4th
LREC, Lisbon.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd LREC, Las Palmas.
Chris Brockett, William B. Dolan, and Michael Gamon.
2006. Correcting ESL errors using phrasal SMT tech-
niques. In Proceedings of the 21st COLING and the
44th ACL, Sydney.
Lou Burnard. 2000. User reference guide for the British
National Corpus. Technical report, Oxford University
Computing Services.
Chia-Yin Chen, Liou Hsien-Chin, and Jason S. Chang.
2006. Fast ? an automatic generation system for
grammar tests. In Proceedings of the COLING/ACL
2006 Interactive Presentation Sessions, Sydney.
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceedings
of the 22nd COLING, Manchester.
Jennifer Foster. 2007. Treebanks gone bad: Parser evalu-
ation and retraining using a treebank of ungrammatical
sentences. International Journal on Document Analy-
sis and Recognition, 10(3-4):129?145.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitriy Belenko,
and Lucy Vanderwende. 2008. Using contextual
speller techniques and language modelling for ESL er-
ror correction. In Proceedings of the 3rd IJCNLP, Hy-
derabad.
Roger Garside, Geoffrey Leech, and Geoffrey Sampson,
editors. 1987. The Computational Analysis of En-
glish: a Corpus-Based Approach. Longman, London.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Carl James. 1998. Errors in Language Learning and
Use: Exploring Error Analysis. Addison Wesley
Longman.
John Lee and Stephanie Seneff. 2008a. An analysis of
grammatical errors in non-native speech in English. In
Proceedings of the 2008 Spoken Language Technology
Workshop, Goa.
John Lee and Stephanie Seneff. 2008b. Correcting mis-
use of verb forms. In Proceedings of the 46th ACL,
Columbus.
Vanessa Metcalf and Detmar Meurers. 2006. Towards
a treatment of word order errors: When to use deep
processing ? and when not to. Presentation at the NLP
in CALL Workshop, CALICO 2006.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proceedings of the 45th ACL, Prague.
Beatrice Santorini. 1991. Part-of-speech tagging guide-
lines for the Penn Treebank project. Technical report,
University of Pennsylvania, Philadelphia, PA.
Jonas Sjo?bergh and Ola Knutsson. 2005. Faking errors to
avoid making errors. In Proceedings of RANLP 2005,
Borovets.
Noah A. Smith and Jason Eisner. 2005a. Contrastive
Estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd ACL, Ann Arbor.
Noah A. Smith and Jason Eisner. 2005b. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In Proceedings of the IJCAI Workshop on Gram-
matical Inference Applications, Edinburgh.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proceedings of the
45rd ACL, Prague.
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL writ-
ing. In Proceedings of the 22nd COLING, Manchester.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2007. A comparative evaluation of deep and
shallow approaches to the automatic detection of com-
mon grammatical errors. In Proceedings of the joint
EMNLP/CoNLL, Prague.
Joachim Wagner, Jennifer Foster, and Josef van Genabith.
2009. Judging grammaticality: Experiments in sen-
tence classification. CALICO Journal. Special Issue
on the 2008 Automatic Analysis of Learner Language
CALICO Workshop. To Appear.
90
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 176?179,
Paris, October 2009. c?2009 Association for Computational Linguistics
The effect of correcting grammatical errors on parse probabilities
Joachim Wagner
CNGL
School of Computing
Dublin City University, Ireland
jwagner@computing.dcu.ie
Jennifer Foster
NCLT
School of Computing
Dublin City University, Ireland.
jfoster@computing.dcu.ie
Abstract
We parse the sentences in three parallel er-
ror corpora using a generative, probabilis-
tic parser and compare the parse probabil-
ities of the most likely analyses for each
grammatical sentence and its closely re-
lated ungrammatical counterpart.
1 Introduction
The syntactic analysis of a sentence provided by
a parser is used to guide the interpretation process
required, to varying extents, by applications such
as question-answering, sentiment analysis and ma-
chine translation. In theory, however, parsing also
provides a grammaticality judgement as shown in
Figure 1. Whether or not a sentence is grammati-
cal is determined by its parsability with a grammar
of the language in question.
The use of parsing to determine whether a sen-
tence is grammatical has faded into the back-
ground as hand-written grammars aiming to de-
scribe only the grammatical sequences in a lan-
guage have been largely supplanted by treebank-
derived grammars. Grammars read from treebanks
tend to overgenerate. This overgeneration is un-
problematic if a probabilistic model is used to rank
analyses and if the parser is not being used to pro-
vide a grammaticality judgement. The combina-
tion of grammar size, probabilistic parse selection
and smoothing techniques results in high robust-
ness to errors and broad language coverage, de-
sirable properties in applications requiring a syn-
tactic analysis of any input, regardless of noise.
However, for applications which rely on a parser?s
ability to distinguish grammatical sequences from
ungrammatical ones, e.g. grammar checkers, over-
generating grammars are perhaps less useful as
they fail to reject ungrammatical strings.
A naive solution might be to assume that the
probability assigned to a parse tree by its proba-
bilistic model could be leveraged in some way to
Figure 1: Grammaticality and formal languages
determine the sentence?s grammaticality. In this
paper, we explore one aspect of this question by
using three parallel error corpora to determine the
effect of common English grammatical errors on
the parse probability of the most likely parse tree
returned by a generative probabilistic parser.
2 Related Work
The probability of a parse tree has been used be-
fore in error detection systems. Sun et al (2007)
report only a very modest improvement when they
include a parse probability feature in their system
whose features mostly consist of linear sequential
patterns. Lee and Seneff (2006) detect ungram-
matical sentences by comparing the parse proba-
bility of a possibly ill-formed input sentence to the
parse probabilities of candidate corrections which
are generated by arbitrarily deleting, inserting and
substituting articles, prepositions and auxiliaries
and changing the inflection of verbs and nouns.
Foster et al (2008) compare the parse probabil-
ity returned by a parser trained on a regular tree-
bank to the probability returned by the same parser
trained on a ?noisy? treebank and use the differ-
ence to decide whether the sentence is ill-formed.
Research in the field of psycholinguistics has
explored the link between frequency and gram-
maticality, often focusing on borderline acceptable
sentences (see Crocker and Keller (2006) for a dis-
cussion of the literature). Koonst-Garboden and
Jaeger (2003) find a weak correlation between the
176
frequency ratios of competing surface realisations
and human acceptability judgements. Hale (2003)
calculates the information-theoretic load of words
in sentences assuming that they were generated ac-
cording to a probabilistic grammar and finds that
these values are good predictors for observed read-
ing time and other measures of cognitive load.
3 Experimental Setup
The aim of this experiment is to find out to
what extent ungrammatical sentences behave dif-
ferently from correct sentences as regards their
parse probabilities. There are two types of corpora
we study: two parallel error corpora that consist
of authentic ungrammatical sentences and manual
corrections, and a parallel error corpus that con-
sists of authentic grammatical sentences and auto-
matically induced errors. Using parallel corpora
allows us to compare pairs of sentences that have
the same or very similar lexical content and dif-
fer only with respect to their grammaticality. A
corpus with automatically induced errors is in-
cluded because such a corpus is much larger and
controlled error insertion allows us to examine di-
rectly the effect of a particular error type.
The first parallel error corpus contains 1,132
sentence pairs each comprising an ungrammatical
sentence and a correction (Foster, 2005). The sen-
tences are taken from written texts and contain ei-
ther one or two grammatical errors. The errors in-
clude those made by native English speakers. We
call this the Foster corpus. The second corpus
is a learner corpus. It contains transcribed spo-
ken utterances produced by learners of English of
varying L1s and levels of experience in a class-
room setting. Wagner et al (2009) manually cor-
rected 500 sentences of the transcribed utterances,
producing a parallel error corpus which we call
Gonzaga 500. The third parallel corpus contains
199,600 sentences taken from the British National
Corpus and ungrammatical sentences produced by
introducing errors of the following five types into
the original BNC sentences: errors involving an
extra word, errors involving a missing word, real-
word spelling errors, agreement errors and errors
involving an incorrect verbal inflection.
All sentence pairs in the three parallel cor-
pora are parsed using the June 2006 version
of the first-stage parser of Charniak and John-
son (2005), a lexicalised, generative, probabilistic
parser achieving competitive performance on Wall
Street Journal text. We compare the probability of
the highest ranked tree for the grammatical sen-
tence in the pair to the probability of the highest
ranked tree for the ungrammatical sentence.
4 Results
Figure 2 shows the results for the Foster corpus.
For ranges of 4 points on the logarithmic scale,
the bars depict how many sentence pairs have a
probability ratio within the respective range. For
example, there are 48 pairs (5th bar from left) for
which the correction has a parse probability which
is between 8 and 12 points lower than the parse
probability of its erroneous original, or, in other
words, for which the probability ratio is between
e?12 and e?8. 853 pairs show a higher probabil-
ity for the correction vs. 279 pairs which do not.
Since the probability of a tree is the product of
its rule probabilities, sentence length is a factor.
If we focus on corrections that do not change the
sentence length, the ratio sharpens to 414 vs. 90
pairs. Ungrammatical sentences do often receive
lower parse probabilities than their corrections.
Figure 3 shows the results for the Gonzaga 500.
Here we see a picture similar to the Foster cor-
pus although the peak for the range from e0 = 1
to e4 ? 54.6 is more pronounced. This time
there are more cases where the parse probability
drops despite a sentence being shortened and vice
versa. Overall, 348 sentence pairs show an in-
creased parse probability, 152 do not. For sen-
tences that stay the same length the ratio is 154
to 34, or 4.53:1, for this corpus which is almost
identical to the Foster corpus (4.60:1).
How do these observations translate to the artifi-
cial parallel error corpus created from BNC data?
Figure 4 shows the results for the BNC data. In
order to keep the orientation of the graph as be-
fore, we change the sign by looking at decrements
instead of increments. Also, we swap the keys
for shortened and lengthened sentences. Clearly,
the distribution is wider and moved to the right.
The peak is at the bar labelled 10. Accordingly,
the ratio of the number of sentence pairs above
and below the zero line is much higher than be-
fore (overall 32,111 to 167, 489 = 5.22, for same
length only 8,537 to 111,171 = 13.02), suggest-
ing that our artificial errors might have a stronger
effect on parse probability than authentic errors.
Another possible explanation is that the BNC data
only contains five error types, whereas the range of
177
Figure 2: Effect of correcting erroneous sentences (Foster corpus) on the probability of the best parse.
Each bar is broken down by whether and how the correction changed the sentence length in tokens. A
bar labelled x covers ratios from ex?2 to ex+2 (exclusive).
Figure 3: Effect of correcting erroneous sentences (Gonzaga 500 corpus) on the probability of the best
parse.
Figure 4: Effect of inserting errors into BNC sentences on the probability of the best parse.
178
errors in the Foster and Gonzaga corpus is wider.
Analysing the BNC data by error type and look-
ing firstly at those error types that do not involve a
change in sentence length, we see that:
? 96% of real-word spelling errors cause a re-
duction in parse probability.
? 91% of agreement errors cause a reduction in
parse probability. Agreement errors involving
articles most reliably decrease the probability.
? 92% of verb form errors cause a reduction.
Changing the form from present participle to
past participle1 is least likely to cause a reduc-
tion, whereas changing it from past participle
to third singular is most likely.
The effect of error types which change sentence
length is more difficult to interpret. Almost all of
the extra word errors cause a reduction in parse
probability and it is difficult to know whether this
is happening because the sentence length has in-
creased or because an error has been introduced.
The errors involving missing words do not system-
atically result in an increase in parse probability
? 41% of them cause a reduction in parse proba-
bility, and this is much more likely to occur if the
missing word is a function word (article, auxiliary,
preposition).
Since the Foster corpus is also error-annotated,
we can also examine its results by error type. This
analysis broadly agrees with that of the BNC data,
although the percentage of ill-formed sentences
for which there is a reduction in parse probability
is generally lower (see Fig. 2 vs. Fig. 4).
5 Conclusion
We have parsed the sentences in three parallel er-
ror corpora using a generative, probabilistic parser
and examined the parse probability of the most
likely analysis of each sentence. We find that
grammatical errors have some negative effect on
the probability assigned to the best parse, a find-
ing which corroborates previous evidence linking
sentence grammaticality to frequency. In our ex-
periment, we approximate sentence probability by
looking only at the most likely analysis ? it might
be useful to see if the same effect holds if we sum
1This raises the issue of covert errors, resulting in gram-
matical sentence structures. Lee and Seneff (2008) give the
example I am prepared for the exam which was produced by
a learner of English instead of I am preparing for the exam.
These occur in authentic error corpora and cannot be com-
pletely avoided when automatically introducing errors.
over parse trees. To fully exploit parse or sentence
probability in an error detection system, it is nec-
essary to fully account for the effect on probability
of 1) non-structural factors such as sentence length
and 2) particular error types. This study repre-
sents a contribution towards the latter.
Acknowledgements
We are grateful to James Hunter from Gonzaga
University for providing us with a learner corpus.
We thank Josef van Genabith and the reviewers for
their comments and acknowledge the Irish Cen-
tre for High-End Computing for the provision of
computational facilities. The BNC is distributed
by Oxford University Computing Services.
References
Eugene Charniak and Mark Johnson. 2005. Course-
to-fine n-best-parsing and maxent discriminative
reranking. In Proceedings of ACL.
Matthew W. Crocker and Frank Keller. 2006. Prob-
abilistic grammars as models of gradience in lan-
guage processing. In Gisbert Fanselow, C. Fe?ry,
R. Vogel, and M. Schlesewsky, editors, Gradience
in Grammar: Generative Perspectives, pages 227?
245. Oxford University Press.
Jennifer Foster, Joachim Wagner, and Josef van Gen-
abith. 2008. Adapting a WSJ-trained parser to
grammatically noisy text. In Proceedings of ACL.
Jennifer Foster. 2005. Good Reasons for Noting Bad
Grammar: Empirical Investigations into the Pars-
ing of Ungrammatical Written English. Ph.D. the-
sis, University of Dublin, Trinity College.
John Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101?123.
Andrew Koontz-Garboden and T. Florian Jaeger.
2003. An empirical investigation of the frequency-
grammaticality correlation hypothesis. Student es-
say received or downloaded on 2006-03-13.
John Lee and Stephanie Seneff. 2006. Automatic
grammar correction for second-language learners.
In Interspeech 2006 - 9th ICSLP, pages 1978?1981.
John Lee and Stephanie Seneff. 2008. Correcting mis-
use of verb forms. In Proceedings of ACL.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proc. of ACL.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experiments
in sentence classification. CALICO Journal, 26(3).
179
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2052?2063, Dublin, Ireland, August 23-29 2014.
Quality Estimation of English-French Machine Translation:
A Detailed Study of the Role of Syntax
Rasoul Kaljahi
??
, Jennifer Foster
?
, Raphael Rubino
??
, Johann Roturier
?
?
NCLT, School of Computing, Dublin City University, Ireland
{rkaljahi, jfoster, rrubino}@computing.dcu.ie
?
Symantec Research Labs, Dublin, Ireland
johann roturier@symantec.com
Abstract
We investigate the usefulness of syntactic knowledge in estimating the quality of English-French
translations. We find that dependency and constituency tree kernels perform well but the error
rate can be further reduced when these are combined with hand-crafted syntactic features. Both
types of syntactic features provide information which is complementary to tried-and-tested non-
syntactic features. We then compare source and target syntax and find that the use of parse trees
of machine translated sentences does not affect the performance of quality estimation nor does
the intrinsic accuracy of the parser itself. However, the relatively flat structure of the French
Treebank does appear to have an adverse effect, and this is significantly improved by simple
transformations of the French trees. Finally, we provide further evidence of the usefulness of
these transformations by applying them in a separate task ? parser accuracy prediction.
1 Introduction
Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output
of an MT system given an input and no reference translation (Blatz et al., 2003; Ueffing et al., 2003;
Specia et al., 2009). An accurate QE-for-MT system would mean that reliable decisions could be made
regarding whether to publish a machine translation as is or to re-direct it to a translator, either for post-
editing or to be translated from scratch. The scores produced by a QE system can also be used to choose
between translations, in a system combination framework or in n-best list reranking. The work presented
here takes place in the context of a wider study, the aim of which is to develop an English-French QE
system so that technical support material that is produced on a daily basis by a company?s English-
speaking customers can be translated automatically into French and made available with confidence to
the company?s French-speaking customer base.
It is reasonable to assume that syntactic features are useful in QE for MT as a way of capturing
the syntactic complexity of the source sentence, the grammaticality of the target translation and the
syntactic symmetry between the source sentence and its translation. This assumption has been borne out
by previous research which has demonstrated the usefulness of syntactic features for English-Spanish
QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role
of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002;
Moschitti, 2006), and by teasing apart the contribution of target and source syntax.
We find that both tree kernels and manually engineered features produce statistically significantly
better results than a strong set of non-syntactic features provided as a baseline by the organisers of the
2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic
features can be combined fruitfully with this baseline. Furthermore, we show that it is worthwhile to
combine tree kernels with hand-crafted features. Our tree kernel features are the complete set of tree
fragments of both the constituency and dependency trees of the source and target sentences. Our hand-
crafted feature set consists of an initial set of 489 constituency and dependency features which are then
reduced to a set of 144 with no significant loss in performance.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2052
We then show that source (English) constituency trees significantly outperform target (French) transla-
tion constituency trees in this task. We hypothesise that this is happening because a) the French parser has
a lower accuracy compared to the English, or b) the target trees sentences are harder to parse, represent-
ing, as they do, potentially ill-formed machine translations which may result in noisier parse trees which
are harder to learn from. If the first hypothesis were true, we would expect to see a drop in the accuracy
of our QE system when we use lower-accuracy parses. We do not observe this. If the second hypothesis
were true, we would expect to observe that the target trees were also less useful than the source trees in
the opposite translation direction (French-English). Instead, we find that the target (English) constituency
trees significantly outperform the source (French) constituency trees, suggesting that the difference be-
tween source and target that we observe in the original English-French experiment is related neither to
intrinsic parser accuracy nor to translation direction but rather to the languages/treebanks.
We explore the extent to which the difference between French and English constituency trees is due
to the relatively flatter structure of the French treebank. We use simple transformation heuristics to
introduce more nodes into the French trees and significantly improve the performance. We also apply
these heuristics in a second task, parser accuracy prediction. This task is similar to QE for MT except
we are predicting the quality of a parse tree in the absence of a reference parse tree. We also find here
that the modified trees also outperform the original trees, suggesting that one must proceed with caution
when using French Treebank tree fragments in a machine-learning task.
The paper?s novel contributions are as follows:
1. Evidence that syntactic information is useful in English-French QE for MT and further evidence
that it is useful in QE for MT in general
2. A comparison of two methods of representing syntactic information in QE
3. A more comprehensive set of syntactic features than has been previously been used in QE for MT
4. A comparison of the role of source and target syntax in English-French QE for MT
5. A set of heuristics that can be applied to French Treebank trees resulting in performance improve-
ments in the tasks of both QE for MT and parser accuracy prediction
The rest of this paper is organised as follows: we discuss related work in using syntax in QE in
Section 2, we describe the data in Section 3, and we then go on to describe the QE framework and the
systems built in Section 4. We follow this with an investigation of the role of source and target syntax in
Section 5 before presenting our heuristics to modify the French constituency trees in Section 6.
2 Related Work
Features extracted from parser output have been used before in training QE for MT systems. Quirk
(2004) uses a single syntax-based feature which indicates whether a full parse for the source sentence
could be found. Hardmeier et al. (2012) employ tree kernels to predict the 1-to-5 post-editing cost of a
machine-translated sentence. They use tree kernels derived from syntactic constituency and dependency
trees of the source side (English) and only dependency trees of the translation side (Spanish). The tree
kernels are used both alone and combined with non-syntactic features. The combined setting ranked
second in the 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore
a variety of syntactic features extracted from the output of both a hand-crafted broad-coverage gram-
mar/parser and a statistical constituency parser on the WMT 2012 data set. They find that the syntactic
features make an important contribution to the overall system. In a framework for combining QE and
automatic metrics to evaluate MT output, Specia and Gim?enez (2010) use part-of-speech (POS) tag lan-
guage model probabilities of the MT output 3-grams as features for QE and features built upon syntactic
chunks, dependencies and constituent structure to build automatic MT evaluation metrics. Avramidis
(2012) builds a series of models for estimating post-editing effort using syntactic features such as parse
probabilities and syntactic label frequency. In a similar vein, Gamon et al. (2005) use POS tag trigrams,
CFG rules and features derived from a semantic analysis of the MT output to classify it as fluent or
disfluent.
2053
In this work, we compare the use of tree kernels and hand-crafted features extracted from the con-
stituency and dependency trees of the source and target sides of a translation pair, as well as comparing
the role of source and target syntax. In addition, we conduct a more in-depth analysis of these approaches
and compare the utility of syntactic information extracted from the source side and target sides of the
translation.
3 Data
While there is evidence to suggest that predicting human evaluation scores is superior to predicting
automatic metrics in QE for ME (Quirk, 2004), it has also been shown that human judgements are not
necessarily consistent (Snover et al., 2006). A more practical consideration is that human evaluation
exists for just a few language pairs and domains. To the best of our knowledge, the only available
English-to-French data set which contains human judgements of translation quality are as follows:
? CESTA (Hamon et al., 2007), which is selected from the Official Journal of the European Commis-
sion and also from the health domain. In addition to the domain (and style) difference to newswire
(the domain on which our parsers are trained), a major stumbling block which prevents us from
using this data set is its small size: only 1135 segments have been evaluated manually.
? WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each
with approx. 5 translations) only half of which is in the news domain.
? FAUST
1
, which is out-of-domain and difficult to apply to our setting as the evaluations and post-
edits are user feedbacks, often in the form of phrases/fragments.
Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel
text for our language pair and domain. We use BLEU
2
(Papineni et al., 2002), TER
3
(Snover et al., 2006)
and METEOR
4
(Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics.
All metrics are applied at the segment level.
5
We randomly select 4500 parallel segments from the News development data sets released for the
WMT13 translation task (Bojar et al., 2013). In order to be independent of any one translation system,
we translate the data set with the following three systems and randomly choose 1500 distinct segments
from each:
? ACCEPT
6
: a phrase-based Moses system trained on training sets of WMT12 releases of Europarl
and News Commentary plus data from Translators Without Borders (TWB)
? SYSTRAN: a proprietary rule-based system
? Bing
7
: an online translation system
The data set is randomly split into 3000 training, 500 development and 1000 test segments. We use the
development set for tuning model parameters and building hand-crafted feature sets, and the test set for
testing model performance and analyses purposes.
4 Syntax-based QE
One way to employ syntactic information in a machine-learning task is to manually compile a set of
features that can be extracted automatically from a parse tree. An example of one such feature is the
label of the root of the tree. Another method is to directly use these trees in a tree kernel (Collins and
Duffy, 2002; Moschitti, 2006). This approach allows exponentially-sized feature spaces (e.g. all subtrees
1
http://www.faust-fp7.eu/faust/Main/DataReleases
2
Version 13a of MTEval script was used at the segment level.
3
TER COMpute 0.7.25: http://www.cs.umd.edu/
?
snover/tercom/
4
METEOR 1.4: http://www.cs.cmu.edu/
?
alavie/METEOR/
5
We present 1-TER to be more easily comparable to BLEU and METEOR. There is no upper bound for TER scores unlike
the other two metrics. Scores higher than 1 occur when the number of errors is higher than the segment length. To avoid this,
scores higher than 1 are cut-off to 1 before being converted to 1-TER.
6
http://www.accept.unige.ch/Products/D_4_1_Baseline_MT_systems.pdf
7
http://www.bing.com/translator
2054
of a tree) to be efficiently modelled using dynamic programming and has shown to be effective in many
natural language processing tasks including parsing and named entity recognition (Collins and Duffy,
2002), semantic role labelling (Moschitti, 2006), sentiment analysis (Wiegand and Klakow, 2010) and
QE for MT (Hardmeier et al., 2012). Although there can be overlap between the information captured by
the two approaches, each can capture information that the other one cannot. In addition, while tree ker-
nels involve minimal feature engineering, hand-crafted features offer more flexibility. Moschitti (2006)
shows that combining the two is beneficial. We use both hand-crafted features and tree kernels, applied
separately and combined together.
For parsing the English and French data into their constituency structures, a PCFG-LA parser
8
is
used. We train the English parser on the training section of the Wall Street Journal (WSJ) section of the
Penn Treebank (PTB) (Marcus et al., 1993). The French parser is trained on the training section of the
French Treebank (FTB) (Abeill?e et al., 2003). We obtain dependency parses by converting the English
constituency parses using the Stanford converter (de Marneffe and Manning, 2008) and the French
parses using Const2Dep (Candito et al., 2010). We evaluate the performance of the QE models using
Root Mean Square Error (RMSE) and Pearson correlation coefficient (r). To compute the statistical
significance of the performance differences between QE models, we use paired bootstrap resampling
following Koehn (2004). We randomly resample (with replacement) a set of N instances from the
predictions of each of the two given systems, where N is the size of the test set. We repeat this sampling
N times and count the number of times each of the two settings is better in terms of each measure (RMSE
and Pearson r). If a setting is better more than 95% of the time, we consider it statistically significant
at p < 0.05.
In the following sections, we first describe our baseline systems and then the quality estimation sys-
tems build using tree kernels, hand-crafted features and a combination of both.
4.1 Baseline QE Systems
In order to verify the usefulness of syntax-based QE, we build two baselines. The first baseline (BM) uses
the mean of the segment-level evaluation scores in the training set for all instances. In the second baseline
(BW), the 17 baseline features of the WMT12 QE Shared Task are used. BW is considered a strong baseline
as the system that used only these features was ranked higher than many of the participating systems.
We use support vector regression implemented in the SVMLight toolkit
9
to build BW. The Radial Basis
Function (RBF) kernel is used. The results for both baselines are presented in the first two rows of
Table 1. Since BW is a stronger baseline than BM, we will compare all syntax-based systems to BW only.
4.2 Syntax-based QE with Tree Kernels
Tree kernels are kernel functions that compute the similarity between two instances of data represented
as trees based on the number of common fragments between them. Therefore, the need for explicitly en-
coding an instance in terms of manually-designed and extracted features is eliminated, while benefitting
from a very high-dimensional feature space. Moschitti (2006) introduces an efficient implementation
of tree kernels within a support vector machine framework. Instead of extracting all possible tree frag-
ments, the algorithm compares only tree fragments rooted in two similar nodes. This algorithm is made
available through SVMLight-TK software
10
, which is used in this work.
In order to extract tree kernels from dependency trees, the labels on the arcs must be removed. Fol-
lowing Tu et al. (2012), the nodes in the resulting tree representation are word forms and dependency
relations, omitting POS tag information. An example is shown in Figure 1. A word is a child of its
dependency relation to its head. The dependency relation in turn is the child of the head word. This
continues until the root of the tree.
Based on preliminary experiments on our development set, we use subset tree kernels, where the tree
fragments are subtrees rooted at any node in the tree so that no production rule expanding a node in the
8
https://github.com/CNGLdlab/LORG-Release. The Lorg parser is very similar to the Berkeley parser (Petrov
et al., 2006), the main difference being its unknown word handling mechanism (Attia et al., 2010).
9
http://svmlight.joachims.org/
10
http://disi.unitn.it/moschitti/Tree-Kernel.htm
2055
BLEU 1-TER METEOR
RMSE r RMSE r RMSE r
BM 0.1626 0 0.1965 0 0.1657 0
BW 0.1601 0.1766 0.1949 0.1565 0.1625 0.2047
TK 0.1581 0.2437 0.1888 0.2774 0.1595 0.2715
BW+TK 0.1570 0.2696 0.1879 0.2939 0.1576 0.3111
HC 0.1603 0.1998 0.1913 0.2365 0.1610 0.2516
BW+HC 0.1587 0.2418 0.1899 0.2611 0.1585 0.2964
SyQE 0.1577 0.2535 0.1887 0.2797 0.1594 0.2743
BW+SyQE 0.1568 0.2802 0.1879 0.2937 0.1576 0.3127
Table 1: QE performances measured by RMSE and Pearson r; BM: Mean baseline, BW: WMT 17 base-
line features, TK: tree kernels, HC: hand-crafted features, SyQE: full syntax-based systems (TK+HC).
Statistically significantly better scores compared to their counterpart (upper row in the row block) are in
bold.
rootcamecc      advmod      nsubj      punctAnd       then           era            .                      det      amod                        the    American 
Figure 1: Tree Kernel Representation of Dependency Structure for And then the American era came.
subtree is split. Unlike subtree kernels, subset tree kernels allow tree fragments with non-terminals as
leaves. We tune the C parameter for Pearson r on the development set, with all other parameters left as
default.
We build a system with all four parse trees for every training instance, which includes the constituency
and dependency trees of the source and target side of the translation. The third row of Table 1 shows
the performance of this system which is named TK. The results achieved using this system represent a
statistically significant improvement over the BW baseline results. In order to examine their complemen-
tarity, we combine these tree kernels and the baseline features (BW+TK) in the fourth row of Table 1.
This combined system performs better than the two individual systems.
While BLEU prediction is the most accurate (lowest RMSE), METEOR prediction appears to be the
easiest to learn (highest Pearson r). TER prediction seems to be more difficult than BLEU and METEOR
prediction, especially in terms of prediction error. This is probably related to the distribution of each of
these metric scores in our data set. The standard deviations (?) of BLEU, TER and METEOR scores are
0.1620, 0.1943 and 0.1652 respectively. The substantially higher ? of TER scores makes them harder to
predict accurately leading to higher prediction error.
4.3 Syntax-based QE with Hand-crafted Features
We design a set of constituency and dependency feature types, some of which have previously been used
by the works described in Section 2 and some introduced here. Each feature type contains at least two
features, one extracted from the source and the other from the translation. Numerical feature types can
be further instantiated by extracting the ratio and differences between the source and target side feature
values. Some feature types are parametric meaning that they can be varied by changing the value of a pa-
rameter. For example, the non-terminal label is a parameter for the non-terminal-label-count
2056
Constituency
?1 Label of the root node of the constituency tree
2 Height of the constituency tree which is the number of edges from root node to the farthest terminal (leaf) node
?3 Number of nodes in the constituency tree
4 Log probability of the constituency parse assigned by the parser
?5 Parseval F
1
score of the tree with respect to a tree produced by the Stanford parser (Klein and Manning, 2003)
?6 Right hand side of the CFG production rule expanding the root node
7 All non-lexical and lexical CFG production rules expanding the tree nodes
?8 Average arity of the non-lexical CFG production rules expanding the constituency tree nodes
9 Counts of each non-terminal label in the tree
?10 POS unigrams, 3-grams and 5-grams
11 POS n-gram scores against language models trained on the POS tags of the respective treebanks using the SRILM
toolkit (http://www.speech.sri.com/projects/srilm/) with Witten-Bell smoothing
?12 Counts of each 12 universal POS tags (Petrov et al., 2012)
?13 Location of the first verb in the sentence in terms of the token distance from the beginning
?14 Average number of POS n-grams in each n-gram frequency quartile of the POS corpora of the respective treebanks
Dependency
?1 POS tag of the top node (dependent of the dummy root node) of the dependency tree
?2 Number of dependents of the top node
?3 Sequence of all dependency relations which modify the top node
?4 Sequence of the POS tags of the dependents of the top node
?5 Average number of dependents per node
?6 Height of the tree computed in the same way for the constituency tree
?7 3- and 5-gram sequences of dependency relations of the tokens to their head
?8 Number of most frequent dependency relations in our News training set
?9 Dependency relation n-gram scores against language models trained on the respective treebanks for each language
?10 Average number of dependency relation n-grams in each n-gram frequency quartile of the respective treebanks
?11 Pairs of tokens and their dependency relations to their head
Table 2: Constituency and dependency feature types
feature type. Therefore, it instantiates as several features, one for each non-terminal-label.
As in BW, we use support vector machines (SVM) to build the QE systems using these hand-crafted
features. We keep only those features which fire for more than a threshold which is set empirically on
the development set. Table 2 lists our syntax-based feature types and their descriptions. Those that have,
to the best of our knowledge, not been used in QE for MT before are marked with an asterisk.
The total number of feature-value pairs in the full feature set is 489. Since this feature set is large
and contains many sparse features, we attempt to reduce it through ablation experiments in which we
directly compare the effect of leaving out features that we suspect may be redundant. For example, we
investigate whether either the ratio or difference of the source and target numerical features or both of
them are redundant by building three systems, one without ratio features, one without difference features
and one with neither. This process is also carried out for log probability and perplexity features, original
and universal POS-tag-based features, n-gram and language model score features, lexical and non-lexical
CFG rules, and n-gram orders (i.e. 3-gram vs. 5-gram features). This process proved useful: we found,
for example, that either 3- or 5-grams worked better than both together and features based on universal
POS tags better than those based on original POS tags.
The final reduced feature set contains 144 features-value pairs. We build one QE system with all 489
features HC-all and one with the reduced set of 144 features HC . Table 3 compares the performance on
the development and test set. The system with the reduced feature set performs consistently better than
the HC-all system on the development set, mostly with statistically significant differences. However,
on the test set, the performance degrades albeit not statistically significantly. Considering a more than
70% reduction in feature set size, this relatively small degradation is tolerable. We use the reduced
feature set as our hand-crafted feature set for the rest of the work.
Compared to TK in Table 1 (third and fourth versus fifth and sixth rows), the performances are lower
for all MT metrics, though not statistically significantly. It is worth noting that we observed an opposite
2057
BLEU 1-TER METEOR
RMSE r RMSE r RMSE r
Development Set
HC-all 0.1567 0.3026 0.1851 0.2746 0.1575 0.2996
HC 0.1540 0.3398 0.1819 0.3263 0.1547 0.3452
Test Set
HC-all 0.1603 0.2108 0.1902 0.2510 0.1607 0.2493
HC 0.1603 0.1998 0.1913 0.2365 0.1610 0.2516
Table 3: QE performance with all hand-crafted syntactic features HC-all and the reduced feature set
HC. Statistically significantly better scores compared to their counterpart (upper row) are in bold.
RMSE r
TK-CD-ST 0.1581 0.2437
TK-CD-S 0.1584 0.2294
TK-CD-T 0.1597 0.2101
TK-C-S 0.1583 0.2312
TK-C-T 0.1608 0.1479
TK-D-S 0.1598 0.1869
TK-D-T 0.1598 0.2102
Table 4: BLEU prediction performances with tree kernels of only source S or translation T side trees.
The scores in bold are statistically better than their counterparts in the same row block. The original
result with source and target combined is provided for reference in the first row.
behaviour on the development set, where hand-crafted features largely outperform tree kernels. This
suggests that the tree kernels are more generalisable. We also combine these features with the WMT
17 baseline features (BW+HC). This combination also improves over both syntax-based and baseline
systems, confirming again the usefulness of syntactic information in addition to surface features.
We combine tree kernels and hand-crafted features to build a full syntax-based QE system (SyQE),
which improves over both TK and HC (Table 1) . The improvements for TER and METEOR prediction
are slight but statistically significant for BLEU prediction. This system is also combined with BW in
BW+SyQE (the last row of Table 1), resulting in statistically significant gains for all metrics.
5 Source and Target Syntax in Syntax-based QE
We now turn our attention to the parts played by source and target syntax in QE for MT. To save space,
we present only the BLEU scores for the tree kernel systems. Table 4 shows the results achieved by
systems built using either the source or target side of the translations.
At a glance, it can be seen that the source side constituency tree kernels outperform the target side
ones, while the opposite is the case for dependency tree kernels. The differences for constituency trees
are however substantially bigger. When both constituency and dependency trees are combined, the source
side trees perform better (TK-CD-S vs. TK-CD-T).
The following three hypotheses could explain this difference between TK-C-S and TK-C-T:
1. The Role of Parser Accuracy: The fact that French parsing models do not reach the high Parseval
F1s achieved by English parsing models could explain the difference in usefulness between the
French and English consistuency trees. On the standard parsing test sets, the English parsing model
achieves an F1 of 89.6 and the French an F
1
of 83.4.
2. Parsing Machine Translation Output: The difference between the source and target could be
happening because the target side is machine translation output and (presumably) represents a lower
2058
Sombre  Matter  Affects  de  vol  Probes   spatiale
NP
NP
NP
PP
SENT
NC
ET ET P
ET ET ET
Figure 2: Parse tree of the machine translation of Dark Matter Affects Flight of Space Probes to French
quality set of sentences than the source (see Figure 2 for an example of a parse tree for a poor
translation).
3. Differences in Annotation Strategies: The difference between the source and target could be due
to the idiosyncrasies of the underlying treebanks which is not carried over via the conversion tools
to the dependency structure.
Hypotheses 1 and 2 relate the usefulness of parse trees in QE to the intrinsic quality of the parse trees.
French constituency trees are less accurate than English ones, either because the French parsing model
is not as accurate as the English one (Hypothesis 1) or because the possibly ungrammatical nature of the
French parsing input adversely affects the quality of the parse tree (Hypothesis 2). Although this low
quality would be expected to affect the dependency trees in the same way since they are directly derived
from the consistency trees, this is not the case and it appears that the problematic aspects of the French
parses are abstracted away from the dependency trees.
To test the first hypothesis, we investigate the role of parser accuracy in QE. For both languages, we
substitute the standard parsing models used in all our prior experiments with ?lower-accuracy? mod-
els trained using only a fraction of the training data (following Quirk and Corston-Oliver (2006)). The
English parsing model achieves an F
1
of 72.5 and the French an F
1
of 66.5, representing drops of ap-
proximately 17 points from the original models. The RMSE and Pearson r of the new QE model are
0.1583 and 0.2350 compared to 0.1581 and 0.2437 of the one trained with original trees (see also the
third row of Table 1). These results show that the use of these lower-accuracy models has only a minimal
and statistically insignificant effect on QE performance, suggesting that intrinsic parser accuracy is not
the reason why the target constituency trees are less useful than the source constituency trees.
11
To investigate the second hypothesis, we switch the translation direction to French-to-English. There-
fore, we now parse the well-formed French input sentences and the machine-translated English segments.
If the second hypothesis were true, the target side parse trees in this direction would still underperform
the source side ones. The results are shown in Table 5. All the systems using target trees outperform
those using source trees. The difference between source and target in the models that use constituency
trees is especially substantial and statistically significant. Thus, it is apparent that the suspected lower
quality of constituency parse trees of MT output is not the reason for the lower QE performance.
We now seek the answer in our third hypothesis, i.e. in the difference between the annotation schemes
of the PTB and the FTB. One major difference, noted by, for example, Schluter and van Genabith (2007),
is that the FTB has a relatively flatter structure. It lacks a verb phrase (VP) node and phrases modifying
the verb are the sibling of the verb nucleus. We investigate this further in the next section.
6 Modifying French Parse Trees
In order to test whether the annotation strategy is a reason for the lower performance of French con-
stituency tree kernels, we apply a set of three heuristics which introduce more structure to the French
parse trees (1&2) or simply make them more PTB-like (3):
? Heuristic 1 automatically adds a VP node above the verb node (VN) and at most 3 of its immediate
adjacent nodes if they are noun or prepositional phrases (NP or PP).
11
See (Kaljahi et al., 2013) for a more detailed exploration of the role of parser accuracy in QE for MT.
2059
RMSE r
TK-FE/CD-ST 0.1561 0.2334
TK-FE/CD-S 0.1574 0.1830
TK-FE/CD-T 0.1559 0.2423
TK-FE/C-S 0.1581 0.1578
TK-FE/C-T 0.1556 0.2336
TK-FE/D-S 0.1577 0.1655
TK-FE/D-T 0.1579 0.1886
Table 5: BLEU prediction performances with tree kernels for Fr-En direction (FE) (C: constituency, D:
dependency, S: source, T: translation)
RMSE r
TK-C-T 0.1608 0.1479
TK-C-T
m
0.1591 0.2143
TK-CD-ST 0.1581 0.2437
TK-CD-ST
m
0.1574 0.2609
Table 6: QE with tree kernels using original and modified French trees (
m
)
? Heuristic 2 stratifies some of the production rules in the tree by grouping together every two equal
adjacent POS tags under a new node with a tag made of the POS tag suffixed with St.
? Heuristic 3 moves coordinated nodes (the immediate left sibling of the COORD node) under COORD.
Figure 3 shows examples of the application of each of these methods. We apply these heuristics to
the parsed MT output in the English-French translation direction and rebuild the tree kernel system with
translation side constituency trees (TK-C-T) and the full tree kernel system (TK-CD-ST) with the mod-
ified trees. The results are presented in Table 6. Despite the possibility of introducing linguistic errors,
these heuristics yield a statistically significant improvement in QE performance. Unsurprisingly, the
changes are bigger for the system with only translation side constituency trees as in the full system there
are three other tree types involved. These results suggest that the structure of the French constituency
trees is a factor in the lower performance of its tree kernels in QE.
12
The gain achieved by applying these heuristics is related to the fact that there are more similar frag-
ments extracted from the modified structure which are useful for the tree kernel system. For example, in
the original top left tree in Figure 3, there is no chance that a fragment consisting only of VN and NP ?
a very common structure and thus useful in calculating tree similarity ? will be extracted by the subset
tree kernel. The reason is that this kernel type does not allow the production rule to be split (in this case
the rule expanding the S node). However, after applying Heuristic 1, the fragment equivalent to VP ->
VN NP production rule can be easily extracted. Among the three heuristics, the first one contributes the
largest part of the improvement; the other two have a very slight effect according to the results of their
individual application, though they contribute to the overall performance when all three are combined.
The success of using modified French trees in improving tree kernel performance may of course de-
pend on the data set and even the task in hand, and may not be generalisable. We next explore this
question by applying the modification to a different task and a different data set.
6.1 Parser Accuracy Prediction
The task we choose is parser accuracy prediction, the aim of which is to predict the accuracy of a parse
tree without a reference (QE for parsing). The task was previously explored for English by Ravi et al.
12
We also see a slightly smaller improvement for the hand-crafted features using the modified French trees. The combina-
tion of tree kernels and hand-crafted features with the modified trees leads to a statistically significant improvement over the
combination with the original trees.
2060
Figure 3: Application of tree modification heuristics on example French translation parse trees
RMSE r
PAP 0.1239 0.4035
PAP
m
0.1233 0.4197
Table 7: Parser Accuracy Prediction (PAP) performance with tree kernels using original and modified
French trees (
m
)
(2008). We build a tree kernel model to predict the accuracy of French parses. To train the system, we
parse the training section of FTB with our French parser and score them using F
1
. We use the FTB
development set to tune the SVM C parameter and test the model on the FTB test set. Two parser
accuracy prediction models are then built using this setting, one with the original parse trees and the
second with the modified parse trees produced using the three heuristics listed above. The results are
presented in Table 7.
Both RMSE and Pearson r improve with the modified trees, where the r improvement is statistically
significant. Although the improvement we observe is not as large as the one we observed for the QE for
MT task, the results add weight to our claim that the structure of the FTB trees should be optimised for
use in tree kernel learning.
7 Conclusion
We analysed the utility of syntactic information in QE of English-French MT and found it useful both
individually and combined with standard QE features. We found that tree kernels are a convenient and
effective way of encoding syntactic knowledge but that our hand-crafted feature set also brings additional,
useful information. As a result of comparing the role of source and target syntax, we also found that the
constituent structure in the FTB could be amended to be more useful in QE for MT and parser accuracy
prediction. Now that we have explored the role of syntax in this project, our next step is try to further
improve our QE system by adding semantic information. However, there are many other ways in which
the research in this paper could be further extended. Our focus is on the language pair English-French
and the QE task but it would certainly be interesting to perform a similar analysis on the role of syntax
in QE for other language pairs, or to investigate the impact of French tree modification on other tasks.
2061
Acknowledgments
This research has been supported by the Irish Research Council Enterprise Partnership Scheme (EP-
SPG/2011/102 and EPSPD/2011/135) and the computing infrastructure of the Centre for Next Gener-
ation Localisation at Dublin City University. We are grateful to Djam?e Seddah for useful discussions
about the French Treebank. We also thank the reviewers for their helpful comments.
References
Anne Abeill?e, Lionel Cl?ement, and Franc?ois Toussenel. 2003. Building a treebank for French. In Treebanks:
Building and Using Syntactically Annotated Corpora. Kluwer Academic Publishers.
Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi, and Josef van Genabith. 2010.
Handling unknown words in statistical latent-variable parsing models for Arabic, English and French. In Pro-
ceedings of the 1st Workshop on Statistical Parsing of Morphologically Rich Languages.
Eleftherios Avramidis. 2012. Quality estimation for machine translation output using linguistic analysis and
decoding features. In Proceedings of WMT.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2003. Confidence estimation for machine translation. In JHU/CLSP Summer Workshop Final
Report.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 workshop on statistical machine
translation. In Proceedings of the 8th WMT.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation,
pages 136?158.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh WMT.
Marie Candito, Benot Crabb, and Pascal Denis. 2010. Statistical French dependency parsing: treebank conversion
and first results. In Proceedings of LREC.
Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of the ACL.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of WMT.
Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level MT evaluation without reference trans-
lations: beyond language modeling. In EAMT.
Olivier Hamon, Antony Hartley, Andr?ei Popescu-Belis, and Khalid Choukri. 2007. Assessing human and auto-
mated quality judgments in the french MT evaluation campaign CESTA. In Proceedings of the MT Summit.
Christian Hardmeier, Joakim Nivre, and J?org Tiedemann. 2012. Tree kernels for machine translation quality
estimation. In Proceedings of the WMT.
Rasoul Samed Zadeh Kaljahi, Jennifer Foster, Raphael Rubino, Johann Roturier, and Fred Hollowood. 2013.
Parser accuracy in quality estimation of machine translation: A tree kernel approach. In Proceedings of IJCNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the ACL.
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics, 19(2):313?330.
Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proceedings of EACL.
2062
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of the ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact and interpretable
tree annotation. In Proceedings of the 21st COLING-ACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of
LREC.
Chris Quirk and Simon Corston-Oliver. 2006. The impact of parse quality on syntactically-informed statistical
machine translation. In Proceedings of EMNLP.
Chris Quirk. 2004. Training a sentence-level machine translation confidence measure. In Proceedings of LREC.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Automatic prediction of parser accuracy. In Proceedings of
EMNLP.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Johann Roturier, Rasoul Kaljahi, and Fred Hollowood. 2012.
DCU-Symantec submission for the WMT 2012 quality estimation task. In Proceedings of WMT.
Natalie Schluter and Josef van Genabith. 2007. Preparing, restructuring, and augmenting a french treebank:
Lexicalised parsers or coherent treebanks? In Proceedings of the 10th Conference of the Pacific Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In Proceedings of AMTA.
Lucia Specia and Jes?us Gim?enez. 2010. Combining confidence estimation and reference-based metrics for seg-
ment level mt evaluation. In Proceedings of AMTA.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc Dymetman, and Nello Cristianini. 2009. Estimating the
sentence-level quality of machine translation systems. In EAMT, pages 28?35.
Zhaopeng Tu, Yifan He, Jennifer Foster, Josef van Genabith, Qun Liu, and Shouxun Lin. 2012. Identifying high-
impact sub-structures for convolution kernels in document-level sentiment classification. In Proceedings of the
ACL.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003. Confidence measures for statistical machine transla-
tion. In Machine Translation Summit IX.
Michael Wiegand and Dietrich Klakow. 2010. Convolution kernels for opinion holder extraction. In Proceedings
of NAACL-HLT.
2063
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1158?1169,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Combining PCFG-LA Models with Dual Decomposition: A Case Study with
Function Labels and Binarization
Joseph Le Roux?, Antoine Rozenknop?, Jennifer Foster?
? Universite? Paris 13, Sorbonne Paris Cite?, LIPN, F-93430, Villetaneuse, France
? NCLT/CNGL, School of Computing, Dublin City University, Dublin 9, Ireland
joseph.leroux@lipn.fr antoine.rozenknop@lipn.fr jfoster@computing.dcu.ie
Abstract
It has recently been shown that different NLP
models can be effectively combined using
dual decomposition. In this paper we demon-
strate that PCFG-LA parsing models are suit-
able for combination in this way. We exper-
iment with the different models which result
from alternative methods of extracting a gram-
mar from a treebank (retaining or discarding
function labels, left binarization versus right
binarization) and achieve a labeled Parseval
F-score of 92.4 on Wall Street Journal Sec-
tion 23 ? this represents an absolute improve-
ment of 0.7 and an error reduction rate of 7%
over a strong PCFG-LA product-model base-
line. Although we experiment only with bina-
rization and function labels in this study, there
is much scope for applying this approach to
other grammar extraction strategies.
1 Introduction
Because of the large amount of possibly contra-
dictory information contained in a treebank, learn-
ing a phrase-structure-based parser implies making
several choices regarding the prevalent annotations
which have to be kept ? or discarded ? in order to
guide the learning algorithm. These choices, which
include whether to keep function labels and empty
nodes, how to binarize the trees and whether to alter
the granularity of the tagset, are often motivated em-
pirically by parsing performance rather than by the
different aspects of the language they may be able to
capture.
Recently Rush et al (2010), Martins et al (2011)
and Koo et al (2010) have shown that Dual De-
composition or Lagrangian Relaxation is an elegant
S
fedcb
(a) Original example
S
?S?
?S?
?S?
fe
d
c
b
(b) Left Binarized example
S
f?S?
e?S?
d?S?
cb
(c) Right Binarized example
Figure 1: Binarization with markovization
framework for combining different types of NLP
tasks or for building parsers from simple slave pro-
cesses that only check partial well-formedness. Here
we propose to follow this idea, but with a different
objective. We want to mix different parsers trained
on different versions of a treebank each of which
makes some annotation choices in order to learn
more specific or richer information. We will use
state-of-the-art unlexicalized probabilistic context-
free grammars with latent annotations (PCFG-LA)
in order to compare our approach with a strong base-
line of high-quality parses. Dual Decomposition is
used to mix several systems (between two and four)
that may in turn be combinations of grammars, here
products of PCFG-LAs (Petrov, 2010). The systems
being combined make different choices with regard
to i) function labels and ii) grammar binarization.
Common sense would suggest that information in
the form of function labels ? syntactic labels such as
SBJ and PRD and semantic labels such as TMP and
LOC ? might help in obtaining a fine-grained anal-
ysis. On the other hand, the independence hypothe-
1158
sis on which CFGs rely and on which most popular
parsers are based may be too strong to learn the de-
pendencies between functions across the parse trees.
Also, the number of parameters increases with the
use of function labels and this can affect the learn-
ing process.
At first glance, binarization need not be an is-
sue, as CFGs admit a binarized form recognizing
exactly the same language. But binarization can be
associated with horizontal markovization and in this
case the recognized language will differ. Further-
more this can impose an unwanted emphasis on what
frontier information is more relevant to learning (be-
ginning or end of constituents). In the toy exam-
ple of Figure 1, the original grammar consisting of a
unique rule extracted from one tree only recognizes
the string bcdef, while the grammar learned from
the left binarized and markovized tree recognizes
(among others) bcdef and bdcef and the gram-
mar learned from the right binarized and markovized
tree recognizes (among others) bcdef and bcedf.
We find that i) retaining the function labels in non-
terminal categories loses its negative impact on pars-
ing as the number of grammars increases in PCFG-
LA product models, ii) the function labels them-
selves can be recovered with near state-of-the-art-
accuracy, iii) combining grammars with and without
function labels using dual decomposition is bene-
ficial, iv) combining left and right-binarized gram-
mars using dual decomposition also leads to bet-
ter trees and, v) our best results (a Parseval la-
beled F-score of 92.4, a Stanford labeled attach-
ment score (LAS) of 93.0 and a penn2malt unla-
beled attachment score (UAS) of 94.3 on Section 23
of the Wall Street Journal) are obtained by combin-
ing three grammars which encode different function
label/binarization decisions.
The paper is organized as follows. ? 2 reviews
related work. ? 3 presents approximate PCFG-LA
parsers as linear models, while ? 4 shows how we
can use dual decomposition to derive an algorithm
for combining these models. Experimental results
are presented and discussed in ? 5.
2 Related Work
Parser Model Combination It is well known that
improved parsing performance can be achieved by
leveraging the alternative perspectives provided by
several parsing models rather than relying on just
one. Examples are parser co-training (Steedman
et al, 2003; Sagae and Tsujii, 2007), voting over
phrase structure constituents or dependency arcs
(Henderson and Brill, 1999; Sagae and Tsujii, 2007;
Surdeanu and Manning, 2010), dependency pars-
ing stacking (Nivre and McDonald, 2008), product
model PCFG-LA parsing (Petrov, 2010), using dual
decomposition to combine dependency and phrase
structure models (Rush et al, 2010) or several non-
projective dependency parsing models (Koo et al,
2010; Martins et al, 2011), and using expecta-
tion propagation, a related approach to dual decom-
position, to combine lexicalized, unlexicalized and
PCFG-LA models (Hall and Klein, 2012). In this
last example, the models must factor in the same
way: in other words, the grammars must use the
same binarization scheme. In our study, we employ
PCFG-LA product models with dual decomposition,
and we relax the constraints on factorization, as we
require only a loose coupling of the models.
Function Label Parsing Although function labels
have been available in the Penn Treebank (PTB) for
almost twenty years (Marcus et al, 1994), they have
been to a large extent overlooked in English parsing
research ? most studies that report parsing results
on Section 23 of the Wall Street Journal (WSJ) use
parsing models that are trained on a version of the
WSJ trees where the function labels have been re-
moved. Notable exceptions are Merlo and Musillo
(2005) and Gabbard et al (2006) who each trained
a parsing model on a version of the PTB with func-
tion labels intact. Gabbard et al (2006) found that
parsing accuracy was not affected by keeping the
function labels. There have also been attempts to
use machine learning to recover the function labels
post-parsing (Blaheta and Charniak, 2000; Chrupala
et al, 2007). We recover function labels as part of
the parsing process, and use dual decomposition to
combine parsing models with and without function
labels. We are not aware of any other work that
leverages the benefits of both types of models.
Grammar Binarization Matsuzaki et al (2005)
compare binarization strategies for PCFG-LA pars-
ing, and conclude that the differences between them
have a minor effect on parsing accuracy as the num-
1159
ber of latent annotations increases beyond two. Hall
and Klein (2012) are forced to use head binarization
when combining their lexicalized and unlexicalized
parsers. Dual decomposition allows us to combine
models with different binarization schemes.
3 Approximation of PCFG-LAs as Linear
Models
In this section, we explain how we can use PCFG-
LAs to devise linear models suitable for the dual de-
composition framework.
3.1 PCFG-LA
Let us recall that PCFG-LAs are defined as tuples
G = (N , T ,H,RH, S, p) where:
? N is a set of observed non-terminals, among
which S is the distinguished initial symbol,
? T is a set of terminals (words),
? H is a set of latent annotations or hidden states,
? RH is a set of annotated rules, of the form
a[h1] ? b[h2] c[h3] for internal rules1 and
a[h1] ? w for lexical rules. Here a, b, c ? N
are non-terminals, w ? T is a terminal and
h1, h2, h3 ? H are latent annotations. Follow-
ing Cohen et al (2012) we also define the set of
skeletal rules R, in other words, rules without
hidden states, of the form a? b c or a? w.
? p : RH ? R?0 defines the probabilities asso-
ciated with rules conditioned on their left-hand
side. Like Petrov and Klein (2007), we impose
that the initial symbol S has only one latent an-
notation. In other words, among rules with S
on the left-hand side, only those of the form
S[0]? ? are inRH.
With such a grammar G we can define probabil-
ities over trees in the following way. We will con-
sider two types of trees, annotated trees and skeletal
trees. An annotated tree is a sequence of rules from
RH, while a skeletal tree is a sequence of skeletal
rules from R. An annotated tree TH is obtained by
left-most derivation from S[0]. Its probability is:
1For brevity and without loss of generality, we omit unary
and n-ary rules, as PCFG-LA admit a Chomsky normal form.
p(TH) =
?
r?TH
p(r) (1)
We define a projection ? from annotated trees to
skeletal trees. ?(TH) is a tree T isomorphic to TH
with the same terminal and non-terminal symbols la-
beling nodes, without hidden states. The probability
of a skeletal tree T is a sum of the probabilities of
all annotated trees that admit T as their projection.
p(T ) =
?
TH???1(T )
?
r?TH
p(r) (2)
PCFG-LA parsing amounts to, given a sequence
of words, finding the most probable skeletal tree
with this sequence as its yield according to a gram-
mar G:
T ? = arg max
T
?
TH???1(T )
?
r?TH
p(r) (3)
Because of this alternation of sum and products,
the parsing problem is intractable. Moreover, the
PCFG-LAs do not belong to the family of linear
models that are assumed in the Lagrangian frame-
work of (Rush and Collins, 2012). We now turn to
approximations for the parsing problem in order to
address both issues.
3.2 Variational Inference and MaxRule
Variational inference is a common technique to ap-
proximate a probability distribution p with a cruder
one q, as close as possible to the original one,
by minimizing the Kullback-Liebler divergence be-
tween the two ? see for instance (Smith, 2011),
chapter 5 for an introduction. Matsuzaki et al
(2005) showed that one can easily find such a cruder
distribution for PCFG-LAs and demonstrated exper-
imentally that this approximation gives good results.
More precisely, they find a PCFG that only rec-
ognizes the input sentence where the probabilities
q(rs) of the rules are set according to their marginal
probabilities in the original PCFG-LA parse forest.
The parameters rs are skeletal rules with span infor-
mation. Distribution q is defined in Figure 2.
Other approximations are possible. In particu-
lar, Petrov and Klein (2007) found that normaliz-
ing by the forest probability (in other words the in-
side probability of the root node) give better exper-
1160
score(a? b c, i, j, k) =
?
x,y,z?H
P i,kout
(
a[x]
)
? p
(
a[x]? b[y] c[z]
)
? P i,jin
(
b[y]
)
? P j,kin
(
c[z]
)
norm(a? b c, i, j, k) =
?
x?H
P i,kin
(
a[x]
)
? P i,kout
(
a[x]
)
score(a? w, i) =
?
x?H
P i,iout
(
a[x]
)
? p
(
a[x]? w
)
norm(a? w, i) =
?
x?H
P i,iin
(
a[x]
)
? P i,iout
(
a[x]
)
q(rs) =
[
score(rs)
norm(rs)
(Variational Inference)
]
or
[
score(rs)
P 0,nin (S[0])
(MaxRule-Product)
]
Figure 2: Variational Inference for PCFG-LA. Pin and Pout denote inside and outside probabilities.
imental results although its interpretation as varia-
tional inference is still unclear. This approximation
is called MaxRule-Product and amounts to replacing
the norm function (see Figure 2).
In both cases, the probability of a skeletal tree
now becomes a simple product of parameters asso-
ciated with anchored skeletal rules. For our purpose,
the consequence is twofold:
1. The parsing problem becomes tractable by ap-
plying standard PCFG algorithms relying on
dynamic programming (CKY for example).
2. Equivalent to probability, a score ? can be de-
fined as the logarithm of the probability. The
parsing problem becomes2:
T ? = arg max
T
?
rs?T
q(rs)
= arg max
T
?
rs?T
log q(rs)
= arg max
T
?
rs?F
wrs ? 1{rs ? T}
= arg max
T
?(T )
Thus, from a PCFG-LA we are able to de-
fine a linear model whose parameters are the log-
probabilities of the rules in distribution q.
2We denote the parse forest of a sentence by F and the char-
acteristic function of a set by 1.
3.3 Products of PCFG-LAs
Although PCFG-LA training is beyond the scope
of this paper, it is worthwhile mentioning that the
most common way to learn their parameters relies
on Expectation-Maximization which is not guaran-
teed to find the optimal estimation. Fortunately, this
can be partly overcome by combining grammars that
only differ on the initial parameterization of the EM
algorithm. The probability of a skeletal tree is the
product of the probabilities assigned by each single
grammar Gi.
T ? = arg max
T
n?
i=1
qGi(T ) (4)
Since grammars only differ by their numerical pa-
rameters (i.e. skeletal rules are the same), inference
can be efficiently implemented using dynamic pro-
gramming (Petrov, 2010).
Scoring with n such grammars now becomes:
T ? = arg max
T
n?
i=1
?
r?T
log qGi(r) (5)
= arg max
T
?
r?T
n?
i=1
log qGi(r) (6)
The distributions qGi still have to be computed in-
dependently ? and possibly in parallel ? but the final
decoding can be performed jointly. This is still a
linear model for PCFG-LA parsing, but restricted to
grammars that share the same skeletal rules.
1161
4 Dual Decomposition
In this section, we show how we derive an algorithm
to work out the best parse according to a set of n
grammars that do not share the exact same skele-
tal rules. As such, the grammars? product cannot
be easily conducted inside the parser to produce and
score a same and unique best tree, and we now con-
sider a c(ompound)-parse as a tuple (T1 . . . Tn) of
n compatible trees. Each grammar Gi is responsi-
ble for scoring tree Ti, and we seek to obtain the
c-parse that maximizes the sum of the scores of its
different trees. For a c-parse to be consistent, we
have to precisely define the parts on which the trees
must agree to be compatible with each other, so that
we can model these as agreement constraints.
4.1 Compound Parse Consistency
Let us suppose we have a set of phrase-structure
parsers trained on different versions of the same
treebank. Hence, some elements in the charts will
either be the same or can be mapped to each other
provided an equivalence relation and we define con-
sensus between parsers on these elements.
When the grammar is not functionally annotated,
phrase-structure trees can be decomposed into a set
of anchored (syntactical) categories Xs, asserting
that a category X is in the tree at position3 s. Thus,
such a tree T can be described by means of a boolean
vector z(T ) indexed by anchored labels Xs, where
z(T )Xs = 1 if Xs is in T and 0 otherwise.
We will differentiate the set of natural non-
terminals that occur in the treebanks from the set
of artificial non-terminals that do not occur in the
treebank and are the results of a binarization with
markovization. As these artificial non-terminals dis-
appear after reversing binarization in solution trees,
they do not play any role in the consensus between
parsers, and we only consider natural non-terminals
in the set of anchored labels.
When the grammar is functionally annotated,
each label X? in a tree is a pair (X,F ), where X
is a syntactical category and F is a function label.
In this case, in order to manage the consensus with
3The anchor s of a label is composed of the span (i, j), de-
noting that the label covers terminals of the input sentence from
index i to index j. In case the grammar contains unary non-
lexical rules, the anchor also discriminates the different posi-
tions in a sequence of unary rules.
non-functional grammars, we decompose such a tree
into two sets: a set of anchored categories Xs and a
set of anchored function labels Fs. Thus, a tree T
can be described by means of two boolean vectors:
? z(T ) indexed by anchored categories Xs,
z(T )Xs = 1 if there exists a function label F
so that (X,F )s is in T , and 0 otherwise;
? ?(T ) indexed by anchored function labels Fs,
?(T )Fs = 1 if there exists a category X so that
(X,F )s is in T , and 0 otherwise.
In the present work, a compound parse (T1 . . . Tn)
is said to be consistent iff every tree shares the same
set of anchored categories, i.e. iff:
?(i, j) ? J1, nK2, z(Ti) = z(Tj)
4.2 Combining Parsers through Dual
Decomposition
Like previous applications, we base our reasoning
on the assumption that computing the optimal score
with each grammar Gi can be efficiently calculated,
which is the case for approximate PCFG-LA pars-
ing. We follow the presentation of the decomposi-
tion from (Martins et al, 2011) to explain how we
can combine several PCFG-LA parsers together.
For a sentence s, we want to obtain the best con-
sistent compound parse from a set of n parsers:
(P ) : find arg max
(T1...Tn)?C
n?
p=1
?p(Tp) (7)
s.t. ?(i, j) ? J1, nK2, z(Ti) = z(Tj) (8)
where C = F1(s) ? ... ? Fn(s) is the product of
parse forests F i(s), and F i(s) is the set of trees in
grammar Gi whose yields are the input sentence s.
Solving this problem with an exact algorithm is
intractable. While artificial nodes could be inferred
using a traditional parsing algorithm based on dy-
namic programming (i.e. CKY), the natural nodes
require a coupling of the parsers? items to enforce
the fact that natural daughter nodes must be identical
(or equivalent) with the same spans for all parsers.
Since the debinarization of markovized rules enables
the creation of arbitrarily long n-ary rules, in the
worst case the number of natural daughters to check
is exponential in the size of the span to infer. Even if
1162
we bound the length of debinarized rules, the prob-
lem is hardly tractable.
As this problem is intractable, even for approxi-
mate PCFG-LA parsing, we apply the iterate method
presented in (Komodakis et al, 2007) for MRFs,
also applied for joint tasks in NLP such as combined
parsing and POS tagging in (Rush et al, 2010).
First, we introduce a witness vector u in order to
simplify constraints in (8). Problem (P ) can then be
written in an equivalent form :
(P ) : find oP = max
(T1...Tn)?C
n?
i=1
?i(Ti) (9)
s.t. ?i ? J1, nK, z(Ti) = u (10)
Next, we proceed to a Lagrangian decomposition.
This decomposition is a two-step process:
Step 1 (Relaxation): the coupling constraints (10)
are removed by introducing a vector of Lagrange
multipliers ?i = (?i,Xs)Xs for each parser i, in-
dexed by anchored categories Xs, and writing the
equivalent problem:
(RP ) : oRP = max
u, T1...n
min
?
f(u, T1...n,?)
where:
f(u, T1...n,?) =
?
i
?i(Ti) +
?
i
(z(Ti)? u) ? ?i
Intuitively, we can see the equivalence of (RP )
and (P ) with the following reasoning:
? whenever all constraints (10) are met, the sec-
ond sum in f is nullified and f(u, T1...n,?) =?
i ?i(Ti), which is a finite value and precisely
the objective function maximized in (P );
? if there is at least one (i,X, s) such that
z(Ti)Xs 6= uXs , then the value of
?
i(z(Ti) ?
u) ? ?i can be made arbitrarily small by
an appropriate choice of ?i,Xs ; in this case,
min? f(u, T1...n,?) = ??. Thus, (RP ) can
not reach its maximum at a point where con-
straints (10) are not satisfied.
Step 2 (dualization): the dual problem (LP ) is ob-
tained by permuting max and min in (RP ):
(LP1) : oLP = min
?
max
u, T1...n
f(u, T1...n,?)
Finally, u can be removed from (LP1) by adding
the constraint:
?
i ?i = 0. As a matter of fact,
one can see that if this constraint is not matched,
maxu,T1...n f(u, T1...n,?) = +? and (LP1) can
not reach its minimum on such a point. We can now
find the maximum of f by maxing each Ti indepen-
dently of each other. The dual problem becomes:
(LP ) : oLP = min
?
n?
i=1
max
Ti?Fi
(?i(Ti) + z(Ti) ? ?i)
s.t.
?
i
?i = 0
Minimization in (LP ) can be solved iteratively
using the projected subgradient method. Finding a
subgradient amounts to computing the optimal so-
lution (Rush and Collins, 2012) for each of the n
subproblems (the slave problems in the terminol-
ogy of (Martins et al, 2011) and (Komodakis et al,
2007)) which can be done efficiently, by incorpo-
rating the calculation of the penalties in the parsing
algorithm, and in parallel. Until the agreement con-
straints are met (or a maximal number of iterations
? ), the Lagrangian multipliers are updated according
to the deviations from the average solutions (i.e. up-
dates are zeros for a natural span if the parsers agree
on it). This leads to Algorithm 1.
It should be noted that the DP charts are built and
pruned during the first iteration only (t = 0); fur-
ther iterations do not require recreating the DP chart,
which is memory intensive and time consuming, nor
recomputing the approximate distribution for varia-
tional inference. As DP on the pruned charts is a fast
process, the bottleneck of the algorithm still is in the
first calculation of slave solutions.
The stepsize sequence (?t)0?t must be diminish-
ing and non-summable, that is to say: ?t, ?t ? 0,
limt?? ?t = 0 and
??
t=0 ?t = ?. In practice, we
set ?t = 11+c(t) where c(t) is the number of times the
objective function oP has increased since iterations
began.
Solving (P): it is easy to see that oLP is an up-
per bound of oP , but we do not necessarily have
1163
Algorithm 1 Find best compound parse with con-
straints on natural spans
Require: n parsers {pi}1?i?n
for all i, syntactical category X , anchor s do
?(0)i,Xs = 0
end for
for t = 0? ? do
for all parsers pi do
T (t)i ? arg maxT?Fi
(
?i(T ) + z(T ) ? ?
(t)
i
)
end for
for all parsers pi do
?(t)i ? ?t
(
z
(
T (t)i
)
?
?
1?j?n z
(
T (t)j
)
n
)
?(t+1)i ? ?
(t)
i + ?
(t)
i
end for
if ?(t)i = 0 for all i then
Exit loop
end if
end for
return (T (?)1 , ? ? ? , T
(?)
n )
strong duality (i.e. oLP = oP ) due to the facts that
parse forests are discrete sets. Furthermore, they get
pruned independently of each other. Thus, the algo-
rithm is not guaranteed to find a t such that z(T (t)i )
is the same for every parser i. However ? see (Koo
et al, 2010) ? if it does reach such a state, then we
have the guarantee of having found an exact solution
of the primal problem (P ). We show in the experi-
ments that this occurs very frequently.
5 Experiments
5.1 Experimental Setup
We perform our experiments on the WSJ sections of
the PTB with the usual split: sections 2 to 21 for
training, section 23 for testing, and we run bench-
marks on section 22. evalb is used for evaluation.
We use the LORG parser modified with Algo-
rithm 1. 4 All grammars are trained using 6
split/merge EM cycles. For the handling of unknown
words, we removed all words occurring once in the
training set and replaced them by their morpholog-
ical signature (Attia et al, 2010). Grammars for
products are obtained by training with 16 random
seeds for each setting. We use the approximate al-
4The LORG parser is available at https://github.
com/CNGLdlab/LORG-Release and the modification at
https://github.com/jihelhere/LORG-Release/
tree/functional_c11.
gorithm MaxRule-Product (Petrov and Klein, 2007).
The basic settings are a combination of the two
following parameters:
left or right binarization: we conjecture that this
affects the quality of the parsers by impacting the
recognition of left and right constituent frontiers.
We set vertical markovization to 1 (no parent anno-
tation) and horizontal markovization to 0 (we drop
all left/right annotations).
with or without functional annotations: in par-
ticular when non-terminals are annotated with mul-
tiple functions, all are kept.
5.2 Products of Grammars
We first evaluate each setting on its own before com-
bining them. We test the 4 different settings on the
development set, using a single grammar or a prod-
uct of n grammars. Results are reported on Figure 3.
We can see that right binarization performs better
than left binarization. Contrary to the results of Gab-
bard et al (2006), function labels are detrimental for
parsing performance for one grammar only. How-
ever, they do not penalize performance when using
the product model with 8 grammars or more.
n
F
1 2 4 8 16
89
90
91
92
93
Func Right
No Func Right
No Func Left
Func Left
Figure 3: F1 for products of n grammars on the dev. set
EM is not guaranteed to find the optimal model
and the problem is made harder by the increased
number of parameters. Product models effectively
alleviate this curse of dimensionality by letting some
models compensate for the errors made by others.
On the other hand, as differences between left
and right binarization settings remain over all prod-
uct sizes, right binarization seems more useful on
its own. The first part of Table 1 gives F-score and
1164
Exact Match results of the product models with 16
grammars on the development set.
5.3 Combinations with Dual Decomposition
We now turn to a series of experiments combining
product models of 16 grammars. In all these experi-
ments, we set the maximum number of iterations in
Algorithm 1 to 1000. The system then returns the
first element of the c-parse. We first try to combine
two settings in four different combinations:
DD Right Bin the two right-binarized systems ?
with and without functions ? the system returns
the function-labeled parse;
DD Left Bin the two left-binarized systems ? with
and without functions ? the system returns the
function-labeled parse;
DD Func the two systems with functions ? left and
right binarization ? the system returns the right-
binarized parse;
DD No Func the two systems without functions ?
left and right binarization ? the system returns
the right-binarized parse;
Results are in the second part of Table 1. Un-
surprisingly, the best configuration is the one com-
bining the two best product systems (with right bi-
narization) but all combined systems perform better
than their single components.
Setting F EX
No Func Right 92.26 42.97
No Func Left 91.92 42.91
Func Right 92.37 43.35
Func Left 91.95 43.15
DD Right Bin 92.71 44.44
DD Left Bin 92.23 43.97
DD Func 92.51 44.79
DD No Func 92.52 44.08
DD3 92.86 45.03
DD4 92.82 45.14
Table 1: Parse evaluation on development set.
We also combine 3 and 4 parsers to see if combin-
ing the above DD Right Bin setting with informa-
tion that could improve the recognition of beginning
of constituents can be helpful. We have 2 settings:
DD3 The 2 right-binarized parsers combined with
the left binarized parser without functions,
DD4 The 4 parsers together.
In both cases the system returns the right-
binarized function annotated parse. The results are
shown in the last part of Table 1. These 2 new con-
figurations give similar F-scores, better than all 2-
parser configurations.
We conclude from these results that left-
binarization and right-binarization capture different
linguistic aspects, even in the case of heavy horizon-
tal markovization, and that the method we propose
enables a practical integration of these models.
Table 2 shows for each setting how often the sys-
tems agree before 1000 iterations of Algorithm 1.
As one might expect, the more diverse the systems
are, the lower the rate of agreement.
Setting Rate
DD Right Bin 99.24
DD Left Bin 99.12
DD Func 98.53
DD No Func 99.12
DD3 96.18
DD4 94.53
Table 2: Rate of certificates of optimality on the dev set.
5.4 Evaluation of Function Labeling
We also evaluate the quality of the function labels.
We compare the results obtained directly from the
parser output with results obtained with Funtag, a
state-of-the-art functional tagger that is applied on
parser output, using a gold model trained on sections
02 to 21 of the WSJ (Chrupala et al, 2007).
Setting SYSTEM FUN FUNTAG
No Func Right ? 90.41
No Func Left ? 90.26
Func Right 89.61 90.37
Func Left 89.29 90.40
DD Right Bin 89.50 90.38
DD Left Bin 89.11 90.31
DD Func 89.54 90.49
DD No Func ? 90.36
DD3 89.48 90.42
DD4 89.57 90.45
Table 3: Function labeling F1 on development set.
The results are shown in Table 3. First, we can
see that the parser output is always outperformed by
Funtag. This is expected from a context-free parser
1165
that has a limited domain of locality with strong in-
dependence constraints, compared to a voted-SVM
classifier that can rely on arbitrarily rich features.
Second, the quality of the Funtag prediction seems
to be influenced by the fact that parser already han-
dle functions and by the accuracy of the parser (Par-
seval F-score). This is because we use a model
trained on the gold reference and so the closer the
parser output is from the reference, the better the
prediction. On the other hand, this is not the case
with parser predicted functions, where the best sys-
tem is the right-binarized product model with func-
tions, with very similar performance obtained by the
combinations consisting of 2 function parsers, set-
tings DD Func and DD4. This tends to indicate
that the constraints we have set to define consisten-
cies in c-parses, focusing on syntactical categories,
do not help in retrieving better function labels. This
suggests some possible further improvements where
parsers with functional annotations should be forced
to agree on these too.
5.5 Evaluation of Dependencies
Setting Stanford LTH p2m
LAS UAS LAS UAS UAS
Func Right 92.18 94.32 89.51 93.92 94.2
No Func Right 92.03 94.47 65.31 92.22 94.2
Func Left 91.86 94.06 89.28 93.75 93.9
No Func Left 91.83 94.29 65.33 92.18 94.1
DD Right Bin 92.56 94.60 89.81 94.17 94.5
DD Left Bin 92.01 94.38 89.62 94.05 94.2
DD Func 92.19 94.36 89.67 94.06 94.2
DD No Func 92.19 94.57 65.44 92.37 94.3
DD3 92.77 94.79 90.04 94.33 94.5
DD4 92.59 94.62 89.95 94.24 94.4
Table 4: Dependency accuracies on the dev set
Dependency-based evaluation of phrase structure
parser output has been used in recent years to pro-
vide a more rounded view on parser performance
and to compare with direct dependency parsers (Cer
et al, 2010; Petrov et al, 2010; Nivre et al, 2010;
Foster et al, 2011; Petrov and McDonald, 2012).
We evaluate our various parsing models on their
ability to recover three types of dependencies: basic
Stanford dependencies (de Marneffe and Manning,
2008)5, LTH dependencies (Johansson and Nugues,
5We used the latest version at the time of writing, i.e. 3.20.
2007)6 and penn2malt dependencies.7 The latter
are a simpler version of the LTH dependencies but
are still used when reporting unlabeled attachment
scores for dependency parsing.
The results, shown in Table 4, mirror the con-
stituency evaluation results in that the dual decom-
position results tend to outperform the basic product
model results, and combining three or four gram-
mars using dual decomposition yields the highest
scores. The differences between the Func and No
Func results highlight an important difference be-
tween the Stanford and LTH dependency schemes.
The tool used to produce Stanford dependencies has
been designed to work with phrase structure trees
that do not contain function labels. In contrast, the
LTH tool makes use of function label information
in phrase structure trees. Thus, their availability re-
sults in only a moderate improvement in LAS for the
Stanford dependencies and a very striking improve-
ment for the LTH dependencies. By retaining func-
tion labels during parsing, we have shown that LTH
dependencies can be recovered with a high level of
accuracy without having to resort to a post-parsing
function labeling step.
5.6 Test Set Results
We now evaluate our various systems on the test set
(the first half of Table 5) and compare these results
with state-of-the-art systems (the second half of Ta-
ble 5). We present parser accuracy results, measured
using Parseval F-score and penn2malt UAS, and, for
our systems, function label accuracy for labels pro-
duced during parsing and after parsing using Funtag.
We also carried out statistical significance testing8
on the F-score differences between our various sys-
tems on the development and test sets. The results
6nlp.cs.lth.se/software/treebank_converter. It
is recommended that LTH is used with the version of the Penn
Treebank which contains the more detailed NP bracketing pro-
vided by Vadas and Curran (2007). However, to facilitate com-
parison with other parsers and dependency schemes, we did not
use it in our experiments. We ran the converter with the right-
Branching=false option to indicate that we are using the version
without extra noun phrase bracketing.
7stp.lingfil.uu.se/?nivre/research/Penn2Malt.
The English head-finding rules of Yamada and Mat-
sumoto (2003), supplied on the website, are employed.
8We used Dan Bikel?s compare.pl script which uses
stratified shuffling to compute significance. We consider a p
value < 0.05 to indicate a statistically significant difference.
1166
Setting F UAS Fun Funtag
Func Right 91.73 93.9 91.02 91.88
No Func Right 91.76 93.8 ? 91.80
Func Left 91.45 93.7 90.41 91.80
No Func Left 91.57 93.7 ? 91.74
DD Right Bin 92.16 94.1 90.85 91.86
DD Left Bin 91.89 93.9 90.10 91.85
DD Func 92.23 94.1 91.02 91.91
DD No Func 92.09 94.0 ? 91.86
DD3 92.45 94.3 90.86 91.98
DD4 92.44 94.3 90.97 92.04
(Shindo et al, 2012) 92.4
(Zhang et al, 2009) 92.3
(Petrov, 2010) 91.8
(Huang, 2008) 91.7
(Bohnet and Nivre, 2012) 93.7
Table 5: Test Set Results: Parseval F-score, penn2malt
UAS, Function Label Accuracy and Funtag Function La-
bel Accuracy
are shown in Table 6.
Comparison Dev Test
Func Right vs. No Func Right 7 7
Func Left vs. No Func Left 7 7
Func Right vs. Func Left X 7
No Func Right vs. No Func Left 7 7
DD Right Bin vs. Func Right X X
DD Right Bin vs. No Func Right X X
DD Left Bin vs. Func Left X X
DD Left Bin vs. No Func Left X X
DD Right Bin vs DD Left Bin X X
DD Func vs. Func Right 7 X
DD Func vs. Func Left X X
DD No Func vs. No Func Right X X
DD No Func vs. No Func Left X X
DD Func vs. DD No Func 7 7
DD3 vs. DD Right Bin 7 X
DD3 vs. No Func Left X X
DD3 vs. DD Func X X
DD4 vs. DD. Right Bin 7 X
DD4 vs. DD. Left Bin X X
DD4 vs. DD Func X X
DD4 vs. DD3 7 7
Table 6: Statistical Significance Testing
We measured the performance of DD4 on the test
set. It is approximately 3 times slower than the
slowest product model (left binarization with func-
tion labels) and 7 slower than the fastest one (right
binarization without function labels). This system
performs on average 85.5 iterations of the DD al-
gorithm. If we exclude the non-converging cases
(5.1% of the cases), this drops to 39.4.
Finally we compare our results with systems
trained and evaluated on the PTB, see the lower half
of Table 5. Our product models are not different
from those presented in (Petrov, 2010) and it is not
surprising to see that the F-scores are similar. More
interestingly our DD4 setting improves on these re-
sults and compares favorably with systems relying
on richer syntactic information, such as the discrim-
inative parser of (Huang, 2008) that makes use of
non-local features to score trees and the TSG parser
of (Shindo et al, 2012) that can take into account
larger tree fragments: this would indicate that by
combining our parsers we extend the domain of lo-
cality, horizontally with binarization schemes and
vertically with function labels. Our system also per-
forms better than the combination system presented
in (Zhang et al, 2009) that only relies on material
from the PTB9 but a more detailed comparison is
difficult: this system does not use products of la-
tent models and more generally their method is or-
thogonal to ours. We also include for comparison
state-of-the-art dependency parsing results (Bohnet
and Nivre, 2012).
6 Conclusion
We presented an algorithm and a set of experiments
showing that grammar extraction strategies can be
combined in an elegant way and give state-of-the-art
results when applied to high-quality phrase-based
parsers. As well as repeating these experiments for
languages which rely more on function annotation,
we also plan to apply our method to other types of
annotations, e.g. more linguistically motivated bina-
rization strategies or ? of particular interest to us ?
annotation of empty elements.
Acknowledgments
We are grateful to the reviewers for their helpful
comments. We also thank Joachim Wagner for pro-
viding feedback on an early version of the paper.
This work has been partially funded by the Labex
EFL (ANR/CGI).
9Their other system relying on the self-trained version of the
BLLIP parser achieves 92.6 F1.
1167
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the First Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010).
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the 1st
Annual Meeting of the North American chapter of the
ACL.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 1455?1465.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
Stanford Dependencies: Trade-offs between speed and
accuracy. In Proceedings of LREC.
Grzegorz Chrupala, Nicolas Stroppa, Josef van Genabith,
and Georgiana Dinu. 2007. Better training for func-
tion labeling. In Proceedings of the 2007 Conference
on Recent Advances in Natural Language Processing
(RANLP).
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable PCFGs. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics (ACL?12).
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011. From news to comment:
Resources and benchmarks for parsing the language
of web 2.0. In Proceedings of IJCNLP.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick. 2006.
Fully parsing the penn treebank. In Proceedings of the
Human Language Technology Conference of the North
American Chapter of the ACL, pages 184?191.
David Hall and Dan Klein. 2012. Training factored
PCFGs with expectation propagation. In Proceedings
of the 2012 Conference on Empirical Methods in Nat-
ural Language Processing, pages 649?652.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings of the 1999 Conference on
Empirical Methods in Natural Language Processing,
pages 187?194.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,
and Mare Koit, editors, Proceedings of NODALIDA
2007, pages 105?112.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. MRF optimization via dual decomposition:
Message-passing revisited. In Computer Vision, 2007.
ICCV 2007. IEEE 11th International Conference on,
pages 1?8. IEEE.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the 1994 ARPA Speech and Natural
Language Workshop, pages 114?119.
Andre? FT Martins, Noah A Smith, Pedro MQ Aguiar,
and Ma?rio AT Figueiredo. 2011. Dual decomposition
with many overlapping components. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 238?249.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
75?82.
Paola Merlo and Gabriele Musillo. 2005. Accu-
rate function parsing. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 620?627.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958.
Joakim Nivre, Laura Rimell, Ryan Mc Donald, and Car-
los Go?mez-Rodr??guez. 2010. Evaluation of depen-
dency parsers on unbounded dependencies. In Pro-
ceedings of COLING.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
the conference on Human Language Technologies and
the conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?07).
1168
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Working
Notes of the SANCL Workshop (NAACL-HLT).
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of EMNLP.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of the conference on Hu-
man Language Technologies and the conference of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL?10), pages 19?27.
Alexander Rush and Michael Collins. 2012. A tutorial
on dual decomposition and lagrangian relaxation for
inference in natural language processing. Journal of
Artificial Intelligence Research, 45:305?362.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL shared task
session of EMNLP-CoNLL, pages 1044?1050.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume
1, pages 440?448.
Noah A. Smith. 2011. Linguistic Structure Predic-
tion. Synthesis Lectures on Human Language Tech-
nologies. Morgan and Claypool, May.
Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Boot-
strapping statistical parsers from small datasets. In
Proceedings of EACL, pages 759?763.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble models for dependency parsing: Cheap and
good? In Proceedings of the conference on Hu-
man Language Technologies and the conference of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL?10), pages 649?
652.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceedings
of ACL, pages 240?247.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552?1560.
1169
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 381?384,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
?cba to check the spelling?
Investigating Parser Performance on Discussion Forum Posts
Jennifer Foster
National Centre for Language Technology
School of Computing
Dublin City University
jfoster@computing.dcu.ie
Abstract
We evaluate the Berkeley parser on text from
an online discussion forum. We evaluate the
parser output with and without gold tokens
and spellings (using Sparseval and Parseval),
and we compile a list of problematic phenom-
ena for this domain. The Parseval f-score for a
small development set is 77.56. This increases
to 80.27 when we apply a set of simple trans-
formations to the input sentences and to the
Wall Street Journal (WSJ) training sections.
1 Introduction
Parsing techniques have recently become efficient
enough for parsers to be used as part of a pipeline in
a variety of tasks. Another recent development is the
rise of user-generated content in the form of blogs,
wikis and discussion forums. Thus, it is both inter-
esting and necessary to investigate the performance
of NLP tools trained on edited text when applied to
unedited Web 2.0 text. McClosky et al (2006) re-
port a Parseval f-score decrease of 5% when a WSJ-
trained parser is applied to Brown corpus sentences.
In this paper, we move even further from the WSJ by
investigating the performance of the Berkeley parser
(Petrov et al, 2006) on user-generated content.
We create gold standard phrase structure trees for
the posts on two threads of the same online dis-
cussion forum. We then parse the sentences in
one thread, our development set, with the Berke-
ley parser under three conditions: 1) when it per-
forms its own tokenisation, 2) when it is provided
with gold tokens and 3) when misspellings in the in-
put have been corrected. A qualitative evaluation is
then carried out on parser output under the third con-
dition. Based on this evaluation, we identify some
?low-hanging fruit? which we attempt to handle ei-
ther by transforming the input sentence or by trans-
forming the WSJ training material. The success of
these transformations is evaluated on our develop-
ment and test sets, with encouraging results.
2 Parser Evaluation Experiments
Data Our data consists of sentences that occur on
the BBC Sport 606 Football discussion forum. The
posts on this forum are quite varied, ranging from
throwaway comments to more considered essay-like
contributions. The development set consists of 42
posts (185 sentences) on a thread discussing a con-
troversial refereeing decision in a soccer match.1
The test set is made up of 40 posts (170 sentences)
on a thread discussing a player?s behaviour in the
same match.2 The average sentence length in the
development set is 18 words and the test set 15
words. Tokenisation and spelling correction were
carried out by hand on the sentences in both sets.3
They were then parsed using Bikel?s parser (Bikel,
2004) and corrected by hand using the Penn Tree-
bank Bracketing Guidelines (Bies et al, 1995).
Parser The Berkeley parser is an unlexicalised
phrase structure parser which learns a latent vari-
able PCFG by iteratively splitting the treebank non-
1http://www.bbc.co.uk/dna/606/F15264075?
thread=7065503&show=50
2http://www.bbc.co.uk/dna/606/F15265997?
thread=7066196&show=50
3Note that abbreviated forms such as cos which are typical
of computer-mediated communication are not corrected.
381
terminals, estimating rule probabilities for the new
grammar using EM and merging the less useful
splits. We train a PCFG from WSJ2-21 by carrying
out five cycles of the split-merge process (SM5).
Tokenisation and Spelling Effects In the first ex-
periment, the parser is given the original devel-
opment set sentences which contain spelling mis-
takes and which have not been tokenised. We ask
the parser to perform its own tokenisation. In the
second experiment, the parser is given the hand-
tokenised sentences which still contain spelling mis-
takes. These are corrected for the third experiment.
Since the yields of the parser output and gold trees
are not guaranteed to match exactly, we cannot use
the evalb implementation of the Parseval evalua-
tion metrics. Instead we use Sparseval (Roark et al,
2006), which was designed to be used to evaluate the
parsing of spoken data and can handle this situation.
An unaligned dependency evaluation is carried out:
head-finding rules are used to convert a phrase struc-
ture tree into a dependency graph. Precision and re-
call are calculated over the dependencies
The Sparseval results are shown in Table 1. For
the purposes of comparison, the WSJ23 perfor-
mance is displayed in the top row. We can see that
performance suffers when the parser performs its
own tokenisation. A reason for this is the under-use
of apostrophes in the forum data, with the result that
words such as didnt and im remain untokenised and
are tagged by the parser as common nouns:
(NP (NP (DT the) (NNS refs)) (SBAR (S (NP (NN didnt))
(VP want to make it to obvious))))
To properly see the effect of the 39 spelling errors
on parsing accuracy, we factor out the mismatches
between the correctly spelled words in the reference
set and their incorrectly spelled equivalents. We do
this by evaluating against a version of the gold stan-
dard which contains the original misspellings (third
row). We can see that the effect of spelling errors
is quite small. The Berkeley parser?s mechanism
for handling unknown words makes use of suffix in-
formation and it is able to ignore many of the con-
tent word spelling errors. It is the errors in function
words that appear to cause a greater problem:
(NP (DT the) (JJ zealous) (NNS fans) (NN whpo) (NN
care) (JJR more) )
Test Set R P F
WSJ 23 88.66 88.66 88.66
Football 68.49 70.74 69.60
Football Gold Tokens 71.54 73.25 72.39
Ft Gold Tok (misspelled gold) 73.49 75.25 74.36
Football Gold Tokens+Spell 73.94 75.59 74.76
Table 1: Sparseval scores for Berkeley SM5
Test Set R P F
WSJ 23 88.88 89.46 89.17
Football Gold Tokens+Spell 78.15 76.97 77.56
Table 2: Parseval scores for Berkeley SM5
Gold Tokens and Spelling Leaving aside the
problems of automatic tokenisation and spelling cor-
rection, we focus on the results of the third experi-
ment. The Parseval results are given in Table 2. Note
that the performance degradation is quite large, more
than has been reported for the Charniak parser on
the Brown corpus. We examine the parser output for
each sentence in the development set. The phenom-
ena which lead the parser astray are listed in Table 3.
One problem is coordination which is difficult for
parsers on in-domain data but which is exacerbated
here by the omission of conjunctions, the use of a
comma as a conjunction and the tendency towards
unlike constituent coordination.
Parser Comparison We test the lexicalised Char-
niak parser plus reranker (Charniak and Johnson,
2005) on the development set sentences. We also
test the Berkeley parser with an SM6 grammar. The
f-scores are shown in Table 4. The parser achiev-
ing the highest score on WSJ23, namely, the C&J
reranking parser, also achieves the highest score on
our development set. The difference between the
two Berkeley grammars supports the claim that an
SM6 grammar overfits to the WSJ (Petrov and Klein,
2007). However, the differences between the four
parser/grammar configurations are small.
Parser WSJ23 Football
Berkeley SM5 89.17 77.56
Berkeley SM6 89.56 77.01
Charniak First-Stage 89.13 77.13
C & J Reranking 91.33 78.33
Table 4: Cross-parser and cross-grammar comparison
382
Problematic Phenomena Examples
Idioms/Fixed Expressions Spot on(S (VP (VB Spot) (PP (IN on))) (. .))
Acronyms
lmao
(S (NP (PRP you))
(VP (VBZ have) (RB n?t) (VP (VBN done)
(NP (ADVP (RB that) (RB once)) (DT this) (NN season))
(NP (NN lmao)))))
Missing subject
Does n?t change the result though
(SQ (VBZ Does) (RB n?t) (NP (NN change))
(NP (DT the) (NN result)) (ADVP (RB though)) (. !))
Lowercase proper nouns paul scholes(NP (JJ paul) (NNS scholes))
Coordination
Very even game and it?s sad that...
(S (ADVP (RB Very))
(NP (NP (JJ even) (NN game)) (CC and) (NP (PRP it)))
(VP (VBZ ?s) (ADJP (JJ sad)) (SBAR (IN that)...
Adverb/Adjective Confusion
when playing bad
(SBAR (WHADVP (WRB when))
(S (VP (VBG playing) (ADJP (JJ bad)))))
CAPS LOCK IS ON
YOU GOT BEATEN BY THE BETTER TEAM
(S (NP (PRP YOU)) (VP (VBP GOT) (NP (NNP BEATEN)
(NNP BY) (NNP THE) (NNP BETTER) (NNP TEAM))))
cos instead of because
or it was cos you lost
(VP (VBD was) (ADJP (NN cos)
(SBAR (S (NP (PRP you)) (VP (VBD lost))))))
Table 3: Phenomena which lead the parser astray. The output of the parser is given for each example.
3 Initial Improvements
Parsing performance on noisy data can be improved
by transforming the input data so that it resembles
the parser?s training data (Aw et al, 2006), trans-
forming the training data so that it resembles the in-
put data (van der Plas et al, 2009), applying semi-
supervised techniques such as the self-training pro-
tocol used by McClosky et al (2006), and changing
the parser internals, e.g. adapting the parser?s un-
known word model to take into account variation in
capitalisation and function word misspelling.4
We focus on the first two approaches and attempt
to transform both the input data and the WSJ training
material. The transformations that we experiment
with are shown in Table 5. The treebank transfor-
mations are performed in such a way that their fre-
quency distribution mirrors their distribution in the
development data. We remove discourse-marking
acronyms such as lol5 from the input sentence, but
4Even when spelling errors have been corrected, unknown
words are still an issue: 8.5% of the words in the football devel-
opment set do not occur in WSJ2-21, compared to 3.6% of the
words in WSJ23.
5In a study of teenage instant messaging, Tagliamonte and
Dennis (2008) found that forms such as lol are not as ubiquitous
as is commonly perceived. Although only occurring a couple of
do not attempt to handle acronyms which are inte-
grated into the sentence.6
We examine the effect of each transformation on
development set parsing performance and discard
those which do not improve performance. We keep
all the input sentence transformations and those tree-
bank transformations which affect lexical rules, i.e.
changing the endings on adverbs and changing the
first character of proper nouns. The treebank trans-
formations which delete subject pronouns and co-
ordinating conjunctions are not as effective. They
work in individual cases, e.g. the original analysis
of the sentence Will be here all day is
(S (NP (NNP Will)) (VP be here all day) (. .))
After applying the treebank transformation, it is
(S (VP (MD Will) (VP be here all day)) (. .))
Their overall effect is, however, negative. It is likely
that, for complex phenomena such as coordination
and subject ellipsis, the development set is still too
small to inform how much of and in what way the
original treebank should be transformed. The results
of applying the effective transformations to the de-
velopment set and the test set are shown in Table 6.
times in our data, they are problematic for the parser.
6An example is: your loss to Wigan would be more scrutu-
nized (cba to check spelling) than it has been this year
383
Input Sentence
cos ? because
Sentences consisting of all uppercase characters converted to standard capitalisation
DEAL WITH IT ? Deal with it
Remove certain acronyms
lol? 
Treebank
Delete subject noun phrases when the subject is a pronoun
(S (NP (PRP It)) (VP (VBD arrived)... ?? (S (VP (VBD arrived)...
Delete or replace conjunctions with a comma (for sentence coordination)
(S ...) (CC and) (S ...) ?? (S ...) (, ,) (S ...) OR (S ...) (CC and) (S ...) ?? (S ...) (S ...)
Delete ly from adverbs
(VP (VBD arrived) (ADVP (RB quickly))) ?? (VP (VBD arrived) (ADVP (RB quick)))
Replace uppercase first character in proper nouns
(NP (NP (NNP Warner) (POS ?s)) (NN price)) ?? (NP (NP (NNP warner) (POS ?s)) (NN price))
Table 5: Input Sentence and Treebank Transformations
Configuration Recall Precision F-Score
Baseline Dev 78.15 76.97 77.56
Transformed Dev 80.83 79.73 80.27
Baseline Test 77.61 79.14 78.37
Transformed Test 80.10 79.77 79.93
Table 6: Effect of transformations on dev and test set
The recall and precision improvements on the devel-
opment set are statistically significant (p < 0.02), as
is the recall improvement on the test set (p < 0.05).
4 Conclusion
Ongoing research on the problem of parsing
unedited informal text has been presented. At the
moment, because of the small size of the data sets
and the variety of writing styles in the development
set, only tentative conclusions can be drawn. How-
ever, even this small data set reveals clear problems
for WSJ-trained parsers: the handling of long co-
ordinated sentences (particularly in the presence of
erratic punctuation usage), domain-specific fixed ex-
pressions and unknown words. We have presented
some preliminary experimental results using simple
transformations to both the input sentence and the
parser?s training material. Treebank transformations
need to be more thoroughly explored with use made
of the Switchboard corpus as well as the WSJ.
Acknowledgments
Thanks to the reviewers and to Emmet ?O Briain,
Deirdre Hogan, Adam Bermingham, Joel Tetreault.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normali-
sation. In Proceedings of the 21st COLING/44th ACL.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
style, Penn Treebank Project. Technical Report Tech
Report MS-CIS-95-06, University of Pennsylvania.
Daniel Bikel. 2004. Intricacies of Collins Parsing Model.
Computational Linguistics, 30(4):479?511.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the 21st COLING/44th ACL.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of HLT
NAACL 2007.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
COLING and the 44th ACL.
Brian Roark, Mary Harper, Eugene Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari
Ostendorf, John Hale, Anna Krasnyanskaya, Matthew
Lease, Izhak Shafran, Matthew Snover, Robin Stewart,
and Lisa Yung. 2006. SParseval: Evaluation metrics
for parsing speech. In Proceedings of LREC.
Sali A. Tagliamonte and Derek Dennis. 2008. Linguis-
tic ruin? LOL! Instant messaging and teen language.
American Speech, 83(1).
Lonneke van der Plas, James Henderson, and Paola
Merlo. 2009. Domain adaptation with artificial data
for semantic parsing of speech. In Proceedings of HLT
NAACL 2009, Companion Volume: Short Papers.
384
Proceedings of the ACL 2010 Conference Short Papers, pages 353?358,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Using Parse Features for Preposition Selection and Error Detection
Joel Tetreault
Educational Testing Service
Princeton
NJ, USA
JTetreault@ets.org
Jennifer Foster
NCLT
Dublin City University
Ireland
jfoster@computing.dcu.ie
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow
@hunter.cuny.edu
Abstract
We evaluate the effect of adding parse fea-
tures to a leading model of preposition us-
age. Results show a significant improve-
ment in the preposition selection task on
native speaker text and a modest increment
in precision and recall in an ESL error de-
tection task. Analysis of the parser output
indicates that it is robust enough in the face
of noisy non-native writing to extract use-
ful information.
1 Introduction
The task of preposition error detection has re-
ceived a considerable amount of attention in re-
cent years because selecting an appropriate prepo-
sition poses a particularly difficult challenge to
learners of English as a second language (ESL).
It is not only ESL learners that struggle with En-
glish preposition usage ? automatically detecting
preposition errors made by ESL speakers is a chal-
lenging task for NLP systems. Recent state-of-the-
art systems have precision ranging from 50% to
80% and recall as low as 10% to 20%.
To date, the conventional wisdom in the error
detection community has been to avoid the use
of statistical parsers under the belief that a WSJ-
trained parser?s performance would degrade too
much on noisy learner texts and that the tradi-
tionally hard problem of prepositional phrase at-
tachment would be even harder when parsing ESL
writing. However, there has been little substantial
research to support or challenge this view. In this
paper, we investigate the following research ques-
tion: Are parser output features helpful in mod-
eling preposition usage in well-formed text and
learner text?
We recreate a state-of-the-art preposition usage
system (Tetreault and Chodorow (2008), hence-
forth T&C08) originally trained with lexical fea-
tures and augment it with parser output features.
We employ the Stanford parser in our experiments
because it consists of a competitive phrase struc-
ture parser and a constituent-to-dependency con-
version tool (Klein and Manning, 2003a; Klein
and Manning, 2003b; de Marneffe et al, 2006;
de Marneffe and Manning, 2008). We com-
pare the original model with the parser-augmented
model on the tasks of preposition selection in well-
formed text (fluent writers) and preposition error
detection in learner texts (ESL writers).
This paper makes the following contributions:
? We demonstrate that parse features have a
significant impact on preposition selection in
well-formed text. We also show which fea-
tures have the greatest effect on performance.
? We show that, despite the noisiness of learner
text, parse features can actually make small,
albeit non-significant, improvements to the
performance of a state-of-the-art preposition
error detection system.
? We evaluate the accuracy of parsing and
especially preposition attachment in learner
texts.
2 Related Work
T&C08, De Felice and Pulman (2008) and Ga-
mon et al (2008) describe very similar preposi-
tion error detection systems in which a model of
correct prepositional usage is trained from well-
formed text and a writer?s preposition is com-
pared with the predictions of this model. It is
difficult to directly compare these systems since
they are trained and tested on different data sets
353
but they achieve accuracy in a similar range. Of
these systems, only the DAPPER system (De Fe-
lice and Pulman, 2008; De Felice and Pulman,
2009; De Felice, 2009) uses a parser, the C&C
parser (Clark and Curran, 2007)), to determine
the head and complement of the preposition. De
Felice and Pulman (2009) remark that the parser
tends to be misled more by spelling errors than
by grammatical errors. The parser is fundamental
to their system and they do not carry out a com-
parison of the use of a parser to determine the
preposition?s attachments versus the use of shal-
lower techniques. T&C08, on the other hand, re-
ject the use of a parser because of the difficulties
they foresee in applying one to learner data. Her-
met et al (2008) make only limited use of the
Xerox Incremental Parser in their preposition er-
ror detection system. They split the input sentence
into the chunks before and after the preposition,
and parse both chunks separately. Only very shal-
low analyses are extracted from the parser output
because they do not trust the full analyses.
Lee and Knutsson (2008) show that knowl-
edge of the PP attachment site helps in the task
of preposition selection by comparing a classifier
trained on lexical features (the verb before the
preposition, the noun between the verb and the
preposition, if any, and the noun after the preposi-
tion) to a classifier trained on attachment features
which explicitly state whether the preposition is
attached to the preceding noun or verb. They also
argue that a parser which is capable of distinguish-
ing between arguments and adjuncts is useful for
generating the correct preposition.
3 Augmenting a Preposition Model with
Parse Features
To test the effects of adding parse features to
a model of preposition usage, we replicated the
lexical and combination feature model used in
T&C08, training on 2M events extracted from a
corpus of news and high school level reading ma-
terials. Next, we added the parse features to this
model to create a new model ?+Parse?. In 3.1 we
describe the T&C08 system and features, and in
3.2 we describe the parser output features used to
augment the model. We illustrate our features us-
ing the example phrase many local groups around
the country. Fig. 1 shows the phrase structure tree
and dependency triples returned by the Stanford
parser for this phrase.
3.1 Baseline System
The work of Chodorow et al (2007) and T&C08
treat the tasks of preposition selection and er-
ror detection as a classification problem. That
is, given the context around a preposition and a
model of correct usage, a classifier determines
which of the 34 prepositions covered by the model
is most appropriate for the context. A model of
correct preposition usage is constructed by train-
ing a Maximum Entropy classifier (Ratnaparkhi,
1998) on millions of preposition contexts from
well-formed text.
A context is represented by 25 lexical features
and 4 combination features:
Lexical Token and POS n-grams in a 2 word
window around the preposition, plus the head verb
in the preceding verb phrase (PV), the head noun
in the preceding noun phrase (PN) and the head
noun in the following noun phrase (FN) when
available (Chodorow et al, 2007). Note that these
are determined not through full syntactic parsing
but rather through the use of a heuristic chun-
ker. So, for the phrase many local groups around
the country, examples of lexical features for the
preposition around include: FN = country, PN =
groups, left-2-word-sequence = local-groups, and
left-2-POS-sequence = JJ-NNS.
Combination T&C08 expand on the lexical fea-
ture set by combining the PV, PN and FN fea-
tures, resulting in features such as PN-FN and
PV-PN-FN. POS and token versions of these fea-
tures are employed. The intuition behind creat-
ing combination features is that the Maximum En-
tropy classifier does not automatically model the
interactions between individual features. An ex-
ample of the PN-FN feature is groups-country.
3.2 Parse Features
To augment the above model we experimented
with 14 features divided among five main classes.
Table 1 shows the features and their values for
our around example. The Preposition Head and
Complement feature represents the two basic at-
tachment relations of the preposition, i.e. its head
(what it is attached to) and its complement (what
is attached to it). Relation specifies the relation
between the head and complement. The Preposi-
tion Head and Complement Combined features
are similar to the T&C08 Combination features
except that they are extracted from parser output.
354
NP
NP
DT
many
JJ
local
NNS
groups
PP
IN
around
NP
DT
the
NN
country
amod(groups-3, many-1)
amod(groups-3, local-2)
prep(groups-3, around-4)
det(country-6, the-5)
pobj(around-4, country-6)
Figure 1: Phrase structure tree and dependency
triples produced by the Stanford parser for the
phrase many local groups around the country
Prep. Head & Complement
1. head of the preposition: groups
2. POS of the head: NNS
3. complement of the preposition: country
4. POS of the complement: NN
Prep. Head & Complement Relation
5. Prep-Head relation name: prep
6. Prep-Comp relation name: pobj
Prep. Head & Complement Combined
7. Head-Complement tokens: groups-country
8. Head-Complement tags: NNS-NN
Prep. Head & Complement Mixed
9. Head Tag and Comp Token: NNS-country
10. Head Token and Comp Tag: groups-NN
Phrase Structure
11. Preposition Parent: PP
12. Preposition Grandparent: NP
13. Left context of preposition parent: NP
14. Right context of preposition parent: -
Table 1: Parse Features
Model Accuracy
combination only 35.2
parse only 60.6
combination+parse 61.9
lexical only 64.4
combination+lexical (T&C08) 65.2
lexical+parse 68.1
all features (+Parse) 68.5
Table 2: Accuracy on preposition selection task
for various feature combinations
The Preposition Head and Complement Mixed
features are created by taking the first feature in
the previous set and backing-off either the head
or the complement to its POS tag. This mix of
tags and tokens in a word-word dependency has
proven to be an effective feature in sentiment anal-
ysis (Joshi and Penstein-Rose?, 2009). All the fea-
tures described so far are extracted from the set of
dependency triples output by the Stanford parser.
The final set of features (Phrase Structure), how-
ever, is extracted directly from the phrase structure
trees themselves.
4 Evaluation
In Section 4.1, we compare the T&C08 and +Parse
models on the task of preposition selection on
well-formed texts written by native speakers. For
every preposition in the test set, we compare the
system?s top preposition for that context to the
writer?s preposition, and report accuracy rates. In
Section 4.2, we evaluate the two models on ESL
data. The task here is slightly different - if the
most likely preposition according to the model dif-
fers from the likelihood of the writer?s preposition
by a certain threshold amount, a preposition error
is flagged.
4.1 Native Speaker Test Data
Our test set consists of 259K preposition events
from the same source as the original training data.
The T&C08 model performs at 65.2% and when
the parse features are added, the +Parse model im-
proves performance by more than 3% to 68.5%.1
The improvement is statistically significant.
1Prior research has shown preposition selection perfor-
mance accuracy ranging from 65% to nearly 80%. The dif-
ferences are largely due to different test sets and also training
sizes. Given the time required to train large models, we report
here experiments with a relatively small model.
355
Model Accuracy
T&C08 65.2
+Phrase Structure Only 67.1
+Dependency Only 68.2
+Parse 68.5
+head-tag+comp-tag 66.9
+left 66.8
+grandparent 66.6
+head-token+comp-tag 66.6
+head-tag 66.5
+head-token 66.4
+head-tag+comp-token 66.1
Table 3: Which parse features are important? Fea-
ture Addition Experiment
Table 2 shows the effect of various feature class
combinations on prediction accuracy. The results
are clear: a significant performance improvement
is obtained on the preposition selection task when
features from parser output are added. The two
best models in Table 2 contain parse features. The
table also shows that the non-parser-based feature
classes are not entirely subsumed by the parse fea-
tures but rather provide, to varying degrees, com-
plementary information.
Having established the effectiveness of parse
features, we investigate which parse feature
classes contribute the most. To test each contri-
bution, we perform a feature addition experiment,
separately adding features to the T&C08 model
(see Table 3). We make three observations. First,
while there is overlapping information between
the dependency features and the phrase structure
features, the phrase structure features are mak-
ing a contribution. This is interesting because
it suggests that a pure dependency parser might
be less useful than a parser which explicitly pro-
duces both constituent and dependency informa-
tion. Second, using a parser to identify the prepo-
sition head seems to be more useful than using it to
identify the preposition complement.2 Finally, as
was the case for the T&C08 features, the combina-
tion parse features are also important (particularly
the tag-tag or tag/token pairs).
4.2 ESL Test Data
Our test data consists of 5,183 preposition events
extracted from a set of essays written by non-
2De Felice (2009) observes the same for the DAPPER sys-
tem.
Method Precision Recall
T&C08 0.461 0.215
+Parse 0.486 0.225
Table 4: ESL Error Detection Results
native speakers for the Test of English as a Foreign
Language (TOEFL R?). The prepositions were
judged by two trained annotators and checked
by the authors using the preposition annotation
scheme described in Tetreault and Chodorow
(2008b). 4,881 of the prepositions were judged to
be correct and the remaining 302 were judged to
be incorrect.
The writer?s preposition is flagged as an error by
the system if its likelihood according to the model
satisfied a set of criteria (e.g., the difference be-
tween the probability of the system?s choice and
the writer?s preposition is 0.8 or higher). Un-
like the selection task where we use accuracy as
the metric, we use precision and recall with re-
spect to error detection. To date, performance
figures that have been reported in the literature
have been quite low, reflecting the difficulty of the
task. Table 4 shows the performance figures for
the T&C08 and +Parse models. Both precision
and recall are higher for the +Parse model, how-
ever, given the low number of errors in our an-
notated test set, the difference is not statistically
significant.
5 Parser Accuracy on ESL Data
To evaluate parser performance on ESL data,
we manually inspected the phrase structure trees
and dependency graphs produced by the Stanford
parser for 210 ESL sentences, split into 3 groups:
the sentences in the first group are fluent and con-
tain no obvious grammatical errors, those in the
second contain at least one preposition error and
the sentences in the third are clearly ungrammati-
cal with a variety of error types. For each preposi-
tion we note whether the parser was successful in
determining its head and complement. The results
for the three groups are shown in Table 5. The
figures in the first row are for correct prepositions
and those in the second are for incorrect ones.
The parser tends to do a better job of de-
termining the preposition?s complement than its
head which is not surprising given the well-known
problem of PP attachment ambiguity. Given the
preposition, the preceding noun, the preceding
356
OK
Head Comp
Prep Correct 86.7% (104/120) 95.0% (114/120)
Prep Incorrect - -
Preposition Error
Head Comp
Prep Correct 89.0% (65/73) 97.3% (71/73)
Prep Incorrect 87.1% (54/62) 96.8% (60/62)
Ungrammatical
Head Comp
Prep Correct 87.8% (115/131) 89.3% (117/131)
Prep Incorrect 70.8% (17/24) 87.5% (21/24)
Table 5: Parser Accuracy on Prepositions in a
Sample of ESL Sentences
verb and the following noun, Collins (1999) re-
ports an accuracy rate of 84.5% for a PP attach-
ment classifier. When confronted with the same
information, the accuracy of three trained annota-
tors is 88.2%. Assuming 88.2% as an approximate
PP-attachment upper bound, the Stanford parser
appears to be doing a good job. Comparing the
results over the three sentence groups, its ability
to identify the preposition?s head is quite robust to
grammatical noise.
Preposition errors in isolation do not tend to
mislead the parser: in the second group which con-
tains sentences which are largely fluent apart from
preposition errors, there is little difference be-
tween the parser?s accuracy on the correctly used
prepositions and the incorrectly used ones. Exam-
ples are
(S (NP I)
(VP had
(NP (NP a trip)
(PP for (NP Italy))
)
)
)
in which the erroneous preposition for is correctly
attached to the noun trip, and
(S (NP A scientist)
(VP devotes
(NP (NP his prime part)
(PP of (NP his life))
)
(PP in (NP research))
)
)
in which the erroneous preposition in is correctly
attached to the verb devotes.
6 Conclusion
We have shown that the use of a parser can boost
the accuracy of a preposition selection model
tested on well-formed text. In the error detection
task, the improvement is less marked. Neverthe-
less, examination of parser output shows the parse
features can be extracted reliably from ESL data.
For our immediate future work, we plan to carry
out the ESL evaluation on a larger test set to bet-
ter gauge the usefulness of a parser in this context,
to carry out a detailed error analysis to understand
why certain parse features are effective and to ex-
plore a larger set of features.
In the longer term, we hope to compare different
types of parsers in both the preposition selection
and error detection tasks, i.e. a task-based parser
evaluation in the spirit of that carried out by Miyao
et al (2008) on the task of protein pair interaction
extraction. We would like to further investigate
the role of parsing in error detection by looking at
other error types and other text types, e.g. machine
translation output.
Acknowledgments
We would like to thank Rachele De Felice and the
reviewers for their very helpful comments.
References
Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, Prague, Czech
Republic, June.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Rachele De Felice and Stephen G. Pulman. 2008. A
classifier-based approach to preposition and deter-
miner error correction in L2 english. In Proceedings
of the 22nd COLING, Manchester, United Kingdom.
Rachele De Felice and Stephen Pulman. 2009. Au-
tomatic detection of preposition errors in learning
writing. CALICO Journal, 26(3):512?528.
Rachele De Felice. 2009. Automatic Error Detection
in Non-native English. Ph.D. thesis, Oxford Univer-
sity.
357
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING08 Work-
shop on Cross-framework and Cross-domain Parser
Evaluation, Manchester, United Kingdom.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, Genoa, Italy.
Michael Gamon, Jianfeng Gao, Chris Brockett,
Alexandre Klementiev, William B. Dolan, Dmitriy
Belenko, and Lucy Vanderwende. 2008. Using con-
textual speller techniques and language modelling
for ESL error correction. In Proceedings of the In-
ternational Joint Conference on Natural Language
Processing, Hyderabad, India.
Matthieu Hermet, Alain De?silets, and Stan Szpakow-
icz. 2008. Using the web as a linguistic resource
to automatically correct lexico-syntactic errors. In
Proceedings of LREC, Marrekech, Morocco.
Mahesh Joshi and Carolyn Penstein-Rose?. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 313?316, Singapore.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the ACL, pages 423?430,
Sapporo, Japan.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for exact pars-
ing. In Advances in Neural Information Processing
Systems, pages 3?10. MIT Press, Cambridge, MA.
John Lee and Ola Knutsson. 2008. The role of PP at-
tachment in preposition generation. In Proceedings
of CICling. Springer-Verlag Berlin Heidelberg.
Yusuke Miyao, Rune Saetre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of the 46th Annual Meeting of
the ACL, pages 46?54, Columbus, Ohio.
Adwait Ratnaparkhi. 1998. Maximum Entropy Mod-
els for natural language ambiguity resolution. Ph.D.
thesis, University of Pennsylvania.
Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL
writing. In Proceedings of the 22nd COLING,
Manchester, United Kingdom.
Joel Tetreault and Martin Chodorow. 2008b. Na-
tive Judgments of non-native usage: Experiments in
preposition error detection. In COLING Workshop
on Human Judgments in Computational Linguistics,
Manchester, United Kingdom.
358
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338?343,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Identifying High-Impact Sub-Structures for Convolution Kernels in
Document-level Sentiment Classification
Zhaopeng Tu? Yifan He?? Jennifer Foster? Josef van Genabith? Qun Liu? Shouxun Lin?
?Key Lab. of Intelligent Info. Processing ?Computer Science Department ?School of Computing
Institute of Computing Technology, CAS New York University Dublin City University
?{tuzhaopeng,liuqun,sxlin}@ict.ac.cn,
?yhe@cs.nyu.edu, ?{jfoster,josef}@computing.dcu.ie
Abstract
Convolution kernels support the modeling of
complex syntactic information in machine-
learning tasks. However, such models are
highly sensitive to the type and size of syntac-
tic structure used. It is therefore an importan-
t challenge to automatically identify high im-
pact sub-structures relevant to a given task. In
this paper we present a systematic study inves-
tigating (combinations of) sequence and con-
volution kernels using different types of sub-
structures in document-level sentiment classi-
fication. We show that minimal sub-structures
extracted from constituency and dependency
trees guided by a polarity lexicon show 1.45
point absolute improvement in accuracy over a
bag-of-words classifier on a widely used sen-
timent corpus.
1 Introduction
An important subtask in sentiment analysis is sen-
timent classification. Sentiment classification in-
volves the identification of positive and negative
opinions from a text segment at various levels of
granularity including document-level, paragraph-
level, sentence-level and phrase-level. This paper
focuses on document-level sentiment classification.
There has been a substantial amount of work
on document-level sentiment classification. In ear-
ly pioneering work, Pang and Lee (2004) use a
flat feature vector (e.g., a bag-of-words) to rep-
resent the documents. A bag-of-words approach,
however, cannot capture important information ob-
tained from structural linguistic analysis of the doc-
uments. More recently, there have been several ap-
proaches which employ features based on deep lin-
guistic analysis with encouraging results including
Joshi and Penstein-Rose (2009) and Liu and Senef-
f (2009). However, as they select features manually,
these methods would require additional labor when
ported to other languages and domains.
In this paper, we study and evaluate diverse lin-
guistic structures encoded as convolution kernels for
the document-level sentiment classification prob-
lem, in order to utilize syntactic structures without
defining explicit linguistic rules. While the applica-
tion of kernel methods could seem intuitive for many
tasks, it is non-trivial to apply convolution kernels
to document-level sentiment classification: previous
work has already shown that categorically using the
entire syntactic structure of a single sentence would
produce too many features for a convolution ker-
nel (Zhang et al, 2006; Moschitti et al, 2008). We
expect the situation to be worse for our task as we
work with documents that tend to comprise dozens
of sentences.
It is therefore necessary to choose appropriate
substructures of a sentence as opposed to using the
whole structure in order to effectively use convolu-
tion kernels in our task. It has been observed that
not every part of a document is equally informa-
tive for identifying the polarity of the whole doc-
ument (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Koppel and Schler, 2005; Ferguson et
al., 2009): a film review often uses lengthy objective
paragraphs to simply describe the plot. Such objec-
tive portions do not contain the author?s opinion and
are irrelevant with respect to the sentiment classifi-
338
cation task. Indeed, separating objective sentences
from subjective sentences in a document produces
encouraging results (Yu and Hatzivassiloglou, 2003;
Pang and Lee, 2004; Koppel and Schler, 2005; Fer-
guson et al, 2009). Our research is inspired by these
observations. Unlike in the previous work, however,
we focus on syntactic substructures (rather than en-
tire paragraphs or sentences) that contain subjective
words.
More specifically, we use the terms in the lexi-
con constructed from (Wilson et al, 2005) as the
indicators to identify the substructures for the con-
volution kernels, and extract different sub-structures
according to these indicators for various types of
parse trees (Section 3). An empirical evaluation on
a widely used sentiment corpus shows an improve-
ment of 1.45 point in accuracy over the baseline
resulting from a combination of bag-of-words and
high-impact parse features (Section 4).
2 Related Work
Our research builds on previous work in the field
of sentiment classification and convolution kernel-
s. For sentiment classification, the design of lexi-
cal and syntactic features is an important first step.
Several approaches propose feature-based learning
algorithms for this problem. Pang and Lee (2004)
and Dave et al (2003) represent a document as a
bag-of-words; Matsumoto et al, (2005) extract fre-
quently occurring connected subtrees from depen-
dency parsing; Joshi and Penstein-Rose (2009) use
a transformation of dependency relation triples; Liu
and Seneff (2009) extract adverb-adjective-noun re-
lations from dependency parser output.
Previous research has convincingly demonstrat-
ed a kernel?s ability to generate large feature set-
s, which is useful to quickly model new and not
well understood linguistic phenomena in machine
learning, and has led to improvements in various
NLP tasks, including relation extraction (Bunescu
and Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhang et al, 2006; Nguyen et al, 2009), question
answering (Moschitti and Quarteroni, 2008), seman-
tic role labeling (Moschitti et al, 2008).
Convolution kernels have been used before in sen-
timent analysis: Wiegand and Klakow (2010) use
convolution kernels for opinion holder extraction,
Johansson and Moschitti (2010) for opinion expres-
sion detection and Agarwal et al (2011) for sen-
timent analysis of Twitter data. Wiegand and K-
lakow (2010) use e.g. noun phrases as possible can-
didate opinion holders, in our work we extract any
minimal syntactic context containing a subjective
word. Johansson and Moschitti (2010) and Agarwal
et al (2011) process sentences and tweets respec-
tively. However, as these are considerably shorter
than documents, their feature space is less complex,
and pruning is not as pertinent.
3 Kernels for Sentiment Classification
3.1 Linguistic Representations
We explore both sequence and convolution kernels
to exploit information on surface and syntactic lev-
els. For sequence kernels, we make use of lexical
words with some syntactic information in the form
of part-of-speech (POS) tags. More specifically, we
define three types of sequences:
? SW, a sequence of lexical words, e.g.: A tragic
waste of talent and incredible visual effects.
? SP, a sequence of POS tags, e.g.: DT JJ NN IN
NN CC JJ JJ NNS.
? SWP, a sequence of words and POS tags,
e.g.: A/DT tragic/JJ waste/NN of/IN talent/NN
and/CC incredible/JJ visual/JJ effects/NNS.
In addition, we experiment with constituency tree
kernels (CON), and dependency tree kernels (D),
which capture hierarchical constituency structure
and labeled dependency relations between words,
respectively. For dependency kernels, we test with
word (DW), POS (DP), and combined word-and-
POS settings (DWP), and similarly for simple se-
quence kernels (SW, SP and SWP). We also use a
vector kernel (VK) in a bag-of-words baseline. Fig-
ure 1 shows the constituent and dependency struc-
ture for the above sentence.
3.2 Settings
As kernel-based algorithms inherently explore the
whole feature space to weight the features, it is im-
portant to choose appropriate substructures to re-
move unnecessary features as much as possible.
339
NP
PP
NP
DT JJ NN
A tragic waste
NP
IN
of
NP NP
NN
talent
CC
and
JJ JJ NNS
incredible visual effect
(a)
waste
det amod prep of
A tragic talent
conj and
effects
amod amod
incredible visual
(b)
waste
det amod prep of
DT JJ NN
conj and
NNS
amod amod
JJ JJ
(c)
waste
det amod prep of
DT
A
JJ
tragic
NN
talent
conj and
NNS
effects
amod amod
JJ
incredible
visual
visual
(d)
Figure 1: Illustration of the different tree structures employed for convolution kernels. (a) Constituent parse tree
(CON); (b) Dependency tree-based words integrated with grammatical relations (DW); (c) Dependency tree in (b)
with words substituted by POS tags (DP); (d) Dependency tree in (b) with POS tags inserted before words (DWP).
NP
DT JJ NN
A tragic waste
(a)
waste
amod
JJ
tragic
(b)
Figure 2: Illustration of the different settings on con-
stituency (CON) and dependency (DWP) parse trees with
tragic as the indicator word.
Unfortunately, in our task there exist several cues
indicating the polarity of the document, which are
distributed in different sentences. To solve this prob-
lem, we define the indicators in this task as subjec-
tive words in a polarity lexicon (Wilson et al, 2005).
For each polarity indicator, we define the ?scope?
(the minimal syntactic structure containing at least
one subjective word) of each indicator for different
representations as follows:
For a constituent tree, a node and its children
correspond to a grammatical production. There-
fore, considering the terminal node tragic in the con-
stituent structure tree in Figure 1(a), we extract the
subtree rooted at the grandparent of the terminal, see
Figure 2(a). We also use the corresponding sequence
Scopes Trees Size
Document 32 24
Subjective Sentences 22 27
Constituent Substructures 30 10
Dependency Substructures 40 3
Table 1: The detail of the corpus. Here Trees denotes the
average number of trees, and Size denotes the averaged
number of words in each tree.
of words in the subtree for the sequential kernel.
For a dependency tree, we only consider the sub-
tree containing the lexical items that are directly
connected to the subjective word. For instance, giv-
en the node tragic in Figure 1(d), we will extract its
direct parent waste integrated with dependency rela-
tions and (possibly) POS, as in Figure 2(b).
We further add two background scopes, one be-
ing subjective sentences (the sentences that contain
subjective words), and the entire document.
4 Experiments
4.1 Setup
We carried out experiments on the movie review
dataset (Pang and Lee, 2004), which consists of
340
1000 positive reviews and 1000 negative reviews.
To obtain constituency trees, we parsed the docu-
ment using the Stanford Parser (Klein and Man-
ning, 2003). To obtain dependency trees, we passed
the Stanford constituency trees through the Stanford
constituency-to-dependency converter (de Marneffe
and Manning, 2008).
We exploited Subset Tree (SST) (Collins and
Duffy, 2001) and Partial Tree (PT) kernels (Mos-
chitti, 2006) for constituent and dependency parse
trees1, respectively. A sequential kernel is applied
for lexical sequences. Kernels were combined using
plain (unweighted) summation. Corpus statistics are
provided in Table 1.
We use a manually constructed polarity lexicon
(Wilson et al, 2005), in which each entry is annotat-
ed with its degree of subjectivity (strong, weak), as
well as its sentiment polarity (positive, negative and
neutral). We only take into account the subjective
terms with the degree of strong subjectivity.
We consider two baselines:
? VK: bag-of-words features using a vector ker-
nel (Pang and Lee, 2004; Ng et al, 2006)
? Rand: a number of randomly selected sub-
structures similar to the number of extracted
substructures defined in Section 3.2
All experiments were carried out using the SVM-
Light-TK toolkit2 with default parameter settings.
All results reported are based on 10-fold cross vali-
dation.
4.2 Results and Discussions
Table 2 lists the results of the different kernel type
combinations. The best performance is obtained by
combining VK and DW kernels, gaining a signifi-
cant improvement of 1.45 point in accuracy. As far
as PT kernels are concerned, we find dependency
trees with simple words (DW) outperform both de-
pendency trees with POS (DP) and those with both
words and POS (DWP). We conjecture that in this
case, as syntactic information is already captured by
1A SubSet Tree is a structure that satisfies the constraint that
grammatical rules cannot be broken, while a Partial Tree is a
more general form of substructures obtained by the application
of partial production rules of the grammar.
2available at http://disi.unitn.it/moschitti/
Kernels Doc Sent Rand Sub
VK 87.05
VK + SW 87.25 86.95 87.25 87.40
VK + SP 87.35 86.95 87.45 87.35
VK + SWP 87.30 87.45 87.30 88.15*
VK + CON 87.45 87.65 87.45 88.30**
VK + DW 87.35 87.50 87.30 88.50**
VK + DP 87.75* 87.20 87.35 87.75
VK + DWP 87.70* 87.30 87.65 87.80*
Table 2: Results of kernels. Here Doc denotes the whole
document of the text, Sent denotes the sentences that con-
tains subjective terms in the lexicon, Rand denotes ran-
domly selected substructures, and Sub denotes the sub-
structures defined in Section 3.2. We use ?*? and ?**? to
denote a result is better than baseline VK significantly at
p < 0.05 and p < 0.01 (sign test), respectively.
the dependency representation, POS tags can intro-
duce little new information, and will add unneces-
sary complexity. For example, given the substruc-
ture (waste (amod (JJ (tragic)))), the PT kernel will
use both (waste (amod (JJ))) and (waste (amod (JJ
(tragic)))). We can see that the former is adding no
value to the model, as the JJ tag could indicate ei-
ther positive words (e.g. good) or negative words
(e.g. tragic). In contrast, words are good indicators
for sentiment polarity.
The results in Table 2 confirm two of our hy-
potheses. Firstly, it clearly demonstrates the val-
ue of incorporating syntactic information into the
document-level sentiment classifier, as the tree k-
ernels (CON and D*) generally outperforms vector
and sequence kernels (VK and S*). More impor-
tantly, it also shows the necessity of extracting ap-
propriate substructures when using convolution ker-
nels in our task: when using the dependency kernel
(VK+DW), the result on lexicon guided substruc-
tures (Sub) outperforms the results on document,
sentence, or randomly selected substructures, with
statistical significance (p<0.05).
5 Conclusion and Future Work
We studied the impact of syntactic information on
document-level sentiment classification using con-
volution kernels, and reduced the complexity of the
kernels by extracting minimal high-impact substruc-
tures, guided by a polarity lexicon. Experiments
341
show that our method outperformed a bag-of-words
baseline with a statistically significant gain of 1.45
absolute point in accuracy.
Our research focuses on identifying and using
high-impact substructures for convolution kernels in
document-level sentiment classification. We expect
our method to be complementary with sophisticated
methods used in state-of-the-art sentiment classifica-
tion systems, which is to be explored in future work.
Acknowledgement
The authors were supported by 863 State Key
Project No. 2006AA010108, the EuroMatrixPlus F-
P7 EU project (grant No 231720) and Science Foun-
dation Ireland (Grant No. 07/CE/I1142). Part of the
research was done while Zhaopeng Tu was visiting,
and Yifan He was at the Centre for Next Generation
Localisation (www.cngl.ie), School of Computing,
Dublin City University. We thank the anonymous
reviewers for their insightful comments. We are al-
so grateful to Junhui Li for his helpful feedback.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, pages 30?38. Association
for Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005a. A
Shortest Path Dependency Kernel for Relation Extrac-
tion. In Proceedings of Human Language Technolo-
gy Conference and Conference on Empirical Methods
in Natural Language Processing, pages 724?731, Van-
couver, British Columbia, Canada, oct. Association for
Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005b. Sub-
sequence Kernels for Relation Extraction. In Y Weis-
s, B Sch o lkopf, and J Platt, editors, Proceedings of
the 19th Conference on Neural Information Processing
Systems, pages 171?178, Cambridge, MA. MIT Press.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems, pages 625?632.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation, Manchester, August.
Paul Ferguson, Neil O?Hare, Michael Davy, Adam
Bermingham, Paraic Sheridan, Cathal Gurrin, and
Alan F. Smeaton. 2009. Exploring the use of
paragraph-level annotations for sentiment analysis of
financial blogs. In Proceedings of the Workshop on
Opinion Mining and Sentiment Analysis.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden, July.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing Dependency Features for Opinion Mining.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 313?316, Suntec, Singapore, jul.
Suntec, Singapore.
Dan Klein and Christopher D Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, jul. As-
sociation for Computational Linguistics.
Moshe Koppel and Jonathan Schler. 2005. Using neutral
examples for learning polarity. In Proceedings of In-
ternational Joint Conferences on Artificial Intelligence
(IJCAI) 2005, pages 1616?1616.
Steve Lawrence Kushal Dave and David Pennock. 2003.
Mining the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. In Proceed-
ings of the 12th International Conference on World
Wide Web, pages 519?528, ACM. ACM.
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Paradigm.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 161?
169, Singapore, aug. Singapore.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. Proceed-
ings of PAKDD?05, the 9th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Mining,
3518/2005:21?32.
Alessandro Moschitti and Silvia Quarteroni. 2008. K-
ernels on Linguistic Structures for Answer Extraction.
In Proceedings of ACL-08: HLT, Short Papers, pages
113?116, Columbus, Ohio, jun. Association for Com-
putational Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning, pages 318?329, Berlin, Germany,
342
sep. Machine Learning: ECML 2006, 17th European
Conference on Machine Learning, Proceedings.
Vincent Ng, Sajib Dasgupta, and S M Niaz Arifin. 2006.
Examining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 611?618,
Sydney, Australia, jul. Sydney, Australia.
Truc-Vien T Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1378?1387.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 271?278, Barcelona, S-
pain, jun. Barcelona, Spain.
Michael Wiegand and Dietrich Klakow. 2010. Convolu-
tion Kernels for Opinion Holder Extraction. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 795?803, Los An-
geles, California, jun. Los Angeles, California.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347?354, Vancouver, British Columbia, Cana-
da, oct. Association for Computational Linguistics.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Toward-
s answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing,
pages 129?136, Association for Computational Lin-
guistics. Association for Computational Linguistics.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Features.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 825?832, Sydney, Australia, jul. Association for
Computational Linguistics.
343
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 87?92,
Dublin, Ireland, August 23-24 2014.
Semantic Role Labelling with minimal resources:
Experiments with French
Rasoul Kaljahi
??
, Jennifer Foster
?
, Johann Roturier
?
?
NCLT, School of Computing, Dublin City University, Ireland
{rkaljahi, jfoster}@computing.dcu.ie
?
Symantec Research Labs, Dublin, Ireland
johann roturier@symantec.com
Abstract
This paper describes a series of French se-
mantic role labelling experiments which
show that a small set of manually anno-
tated training data is superior to a much
larger set containing semantic role labels
which have been projected from a source
language via word alignment. Using uni-
versal part-of-speech tags and dependen-
cies makes little difference over the orig-
inal fine-grained tagset and dependency
scheme. Moreover, there seems to be no
improvement gained from projecting se-
mantic roles between direct translations
than between indirect translations.
1 Introduction
Semantic role labelling (SRL) (Gildea and Juraf-
sky, 2002) is the task of identifying the predicates
in a sentence, their semantic arguments and the
roles these arguments take. The last decade has
seen considerable attention paid to statistical SRL,
thanks to the existence of two major hand-crafted
resources for English, namely, FrameNet (Baker
et al., 1998) and PropBank (Palmer et al., 2005).
Apart from English, only a few languages have
SRL resources and these resources tend to be of
limited size compared to the English datasets.
French is one of those languages which suffer
from a scarcity of hand-crafted SRL resources.
The only available gold-standard resource is a
small set of 1000 sentences taken from Europarl
(Koehn, 2005) and manually annotated with Prop-
bank verb predicates (van der Plas et al., 2010b).
This dataset is then used by van der Plas et al.
(2011) to evaluate their approach to projecting the
SRLs of English sentences to their translations
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
in French. They additionally build a large, ?ar-
tificial? or automatically labelled dataset of ap-
proximately 1M Europarl sentences by projecting
the SRLs from English sentences to their French
translations and use it for training an SRL system.
We build on the work of van der Plas et al.
(2010b) by answering the following questions: 1)
How much artificial data is needed to train an
SRL system? 2) Is it better to use direct trans-
lations than indirect translations, i.e. is it better
to use for projection a source-target pair where
the source represents the original sentence and the
target represents its direct translation as opposed
to a source-target pair where the source and tar-
get are both translations of an original sentence
in a third language? 3) Is it better to use coarse-
grained syntactic information (in the form of uni-
versal part-of-speech tags and universal syntactic
dependencies) than to use fine-grained syntactic
information? We find that SRL performance lev-
els off after only 5K training sentences obtained
via projection and that direct translations are no
more useful than indirect translations. We also
find that it makes very little difference to French
SRL performance whether we use universal part-
of-speech tags and syntactic dependencies or more
fine-grained tags and dependencies.
The surprising result that SRL performance lev-
els off after just 5K training sentences leads us
to directly compare the small hand-crafted set of
1K sentences to the larger artificial training set.
We use 5-fold cross-validation on the small dataset
and find that the SRL performance is substantially
higher (>10 F
1
in identification and classification)
when the hand-crafted annotations are used.
2 Related Work
There has been relatively few works in French
SRL. Lorenzo and Cerisara (2012) propose a clus-
tering approach for verb predicate and argument
labelling (but not identification). They choose
87
VerbNet style roles (Schuler, 2006) and manu-
ally annotate sentences with them for evaluation,
achieving an F
1
of 78.5.
Gardent and Cerisara (2010) propose a method
for semi-automatically annotating the French de-
pendency treebank (Candito et al., 2010) with
Propbank core roles (no adjuncts). They first
manually augment TreeLex (Kup?s?c and Abeill?e,
2008), a syntactic lexicon of French, with seman-
tic roles of syntactic arguments of verbs (i.e. verb
subcategorization). They then project this anno-
tation to verb instances in the dependency trees.
They evaluate their approach by performing error
analysis on a small sample and suggest directions
for improvement. The annotation work is however
at its preliminary stage and no data is published.
As mentioned earlier, van der Plas et al. (2011)
use word alignments to project the SRLs of the
English side of EuroParl to its French side result-
ing in a large artificial dataset. This idea is based
on the Direct Semantic Transfer hypothesis which
assumes that a semantic relationship between two
words in a sentence can be transferred to any
two words in the translation which are aligned to
these source-side words. Evaluation on their 1K
manually-annotated dataset shows that a syntactic-
semantic dependency parser trained on this artifi-
cial data set performs significantly better than di-
rectly projecting the labelling from its English side
? a promising result because, in a real-world sce-
nario, the English translations of the French data
to be annotated do not necessarily exist.
Pad?o and Lapata (2009) also make use of word
alignments to project SRLs from English to Ger-
man. The word alignments are used to compute
the semantic similarity between syntactic con-
stituents. In order to determine the extent of se-
mantic correspondence between English and Ger-
man, they manually annotate a set of parallel sen-
tences and find that about 72% of the frames and
92% of the argument roles exist in both sides, ig-
noring their lexical correspondence.
3 Datasets, SRL System and Evaluation
We use the two datasets described in (van der Plas
et al., 2011) and the delivery report of the Clas-
sic project (van der Plas et al., 2010a). These
are the gold standard set of 1K sentences which
was annotated by manually identifying each verb
predicate, finding its equivalent English frameset
in PropBank and identifying and labelling its ar-
guments based on the description of the frame-
set (henceforth known as Classic1K), and the syn-
thetic dataset consisting of more than 980K sen-
tences (henceforth known as Classic980K), which
was created by word aligning an English-French
parallel corpus (Europarl) using GIZA++ (Och
and Ney, 2003) and projecting the French SRLs
from the English SRLs via the word alignments.
The joint syntactic-semantic parser described in
(Titov et al., 2009) was used to produce the En-
glish SRLs and the dependency parses of the
French side were produced using the ISBN parser
described in (Titov and Henderson, 2007).
We use LTH (Bj?orkelund et al., 2009), a
dependency-based SRL system, in all of our ex-
periments. This system was among the best-
performing systems in the CoNLL 2009 shared
task (Haji?c et al., 2009) and is straightforward to
use. It comes with a set of features tuned for each
shared task language (English, German, Japanese,
Spanish, Catalan, Czech, Chinese). We compared
the performance of the English and Spanish fea-
ture sets on French and chose the former due to its
higher performance (by 1 F
1
point).
To evaluate SRL performance, we use the
CoNLL 2009 shared task scoring script
1
, which
assumes a semantic dependency between the argu-
ment and predicate and the predicate and a dummy
root node and then calculates the precision (P), re-
call (R) and F
1
of identification of these dependen-
cies and classification (labelling) of them.
4 Experiments
4.1 Learning Curve
The ultimate goal of SRL projection is to build a
training set which partially compensates for the
lack of hand-crafted resources. van der Plas et
al. (2011) report encouraging results showing that
training on their projected data is beneficial over
directly obtaining the annotation via projection
which is not always possible. Although the quality
of such automatically-generated training data may
not be comparable to the manual one, the possi-
bility of building much bigger data sets may pro-
vide some advantages. Our first experiment inves-
tigates the extent to which the size of the synthetic
training set can improve performance.
We randomly select 100K sentences from Clas-
sic980K, shuffle them and split them into 20 sub-
1
https://ufal.mff.cuni.cz/
conll2009-st/eval09.pl
88
0
1000
0
2000
0
3000
0
4000
0
5000
0
6000
0
7000
0
8000
0
9000
0
1000
0020
30
40
50
60
70
80
PrecisionRecallF1
Figure 1: Learning curve with 100K training data
of projected annotations
0
1000
0
2000
0
3000
0
4000
0
5000
0
6000
0
7000
0
8000
0
9000
0
1000
0020
30
40
50
60
70
80
PrecisionRecallF1
Figure 2: Learning curve with 100K training data
of projected annotations on only direct translations
sets of 5K sentences. We then split the first 5K into
10 sets of 500 sentences. We train SRL models
on the resulting 29 subsets using LTH. The per-
formance of the models evaluated on Classic1K
is presented in Fig. 1. Surprisingly, the best F
1
(58.7) is achieved by only 4K sentences, and af-
ter that the recall (and consequently F
1
) tends to
drop though precision shows a positive trend, sug-
gesting that the additional sentences bring little in-
formation. The large gap between precision and
recall is also interesting, showing that the projec-
tions do not have wide semantic role coverage.
2
4.2 Direct Translations
Each sentence in Europarl was written in one of
the official languages of the European Parliament
and translated to all of the other languages. There-
fore both sides of a parallel sentence pair can be in-
direct translations of each other. van der Plas et al.
(2011) suggest that translation divergence may af-
2
Note that our results are not directly comparable with
(van der Plas et al., 2011) because they split Classic1K into
development and test sets, while we use the whole set for
testing. We do not have access to their split.
fect automatic projection of semantic roles. They
therefore select for their experiments only those
276K sentences from the 980K which are direct
translations between English and French. Moti-
vated by this idea, we replicate the learning curve
in Fig. 1 with another set of 100K sentences ran-
domly selected from only the direct translations.
The curve is shown in Fig. 2. There is no no-
ticeable difference between this and the graph in
Fig. 1, suggesting that the projections obtained via
direct translations are not of higher quality.
4.3 Impact of Syntactic Annotation
Being a dependency-based semantic role labeller,
LTH employs a large set of features based on syn-
tactic dependency structure. This inspires us to
compare the impact of different types of syntactic
annotations on the performance of this system.
Based on the observations from the previous
sections, we choose two different sizes of training
sets. The first set contains the first 5K sentences
from the original 100K, as we saw that more than
this amount tends to diminish performance. The
second set contains the first 50K from the original
100K, the purpose of which is to check if changing
the parses affects the usefulness of adding more
data. We will call these data sets Classic5K and
Classic50K respectively.
Petrov et al. (2012) create a set of 12 univer-
sal part-of-speech (POS) tags which should in the-
ory be applicable to any natural language. It is
interesting to know whether these POS tags are
more useful for SRL than the original set of the 29
more fine-grained POS tags used in French Tree-
bank which we have used so far. To this end, we
convert the original POS tags of the data to uni-
versal POS tags and retrain and evaluate the SRL
models. The results are given in the second row of
Table 1 (OrgDep+UniPOS). The first row of the
table (Original) shows the performance using
the original annotation. Even though the scores
increase in most cases ? due mostly to a rise in
recall ? the changes are small. It is worth noting
that identification seems to benefit more from the
universal POS tags.
Similar to universal POS tags, McDonald et al.
(2013) introduce a set of 40 universal dependency
types which generalize over the dependency struc-
ture specific to several languages. For French, they
provide a new treebank, called uni-dep-tb,
manually annotating 16,422 sentences from vari-
89
5K 50K
Identification Classification Identification Classification
P R F
1
P R F
1
P R F
1
P R F
1
Original 85.95 59.64 70.42 71.34 49.50 58.45 86.67 58.07 69.54 72.44 48.54 58.13
OrgDep+UniPOS 86.71 60.46 71.24 71.11 49.58 58.43 86.82 58.71 70.05 72.30 48.90 58.34
StdUniDep+UniPOS 86.14 59.76 70.57 70.60 48.98 57.84 86.38 58.90 70.04 71.61 48.83 58.07
CHUniDep+UniPOS 85.98 59.21 70.13 70.66 48.66 57.63 86.47 58.26 69.61 71.74 48.34 57.76
Table 1: SRL performance using different syntactic parses with Classic 5K and 50K training sets
ous domains. We now explore the utility of this
new dependency scheme in SRL.
The French universal dependency treebank
comes in two versions, the first using the stan-
dard dependency structure based on basic Stanford
dependencies (de Marneffe and Manning, 2008)
where content words are the heads except in cop-
ula and adposition constructions, and the second
which treats content words as the heads for all
constructions without exemption. We use both
schemes in order to verify their effect on SRL.
In order to obtain universal dependencies for
our data, we train parsing models with Malt-
Parser (Nivre et al., 2006) using the entire
uni-dep-tb.
3
We then parse our data us-
ing these MaltParser models. The input POS
tags to the parser are the universal POS tags
used in OrgDep+UniPOS. We train and evalu-
ate new SRL models on these data. The results
are shown in the third and fourth rows of Table
1. StdUniDept+UniPOS is the setting using
standard dependencies and CHUDep+UPOS using
content-head dependencies.
According to the third and fourth rows in Table
1, content-head dependencies are slightly less use-
ful than standard dependencies. The general ef-
fect of universal dependencies can be compared to
those of original ones by comparing these results
to OrgDep+UniPOS - the use of universal de-
pendencies appears to have only a modest (nega-
tive) effect. However, we must be careful of draw-
ing too many conclusions because in addition to
the difference in dependency schemes, the training
data used to train the parsers as well as the parsers
themselves are different.
Overall, we observe that the universal annota-
tions can be reliably used when the fine-grained
annotation is not available. This can be especially
3
Based on our preliminary experiments on the pars-
ing performance, we use LIBSVM as learning algorithm,
nivreeager as parsing algorithm for the standard depen-
dency models and stackproj for the content-head ones.
Identification Classification
P R F
1
P R F
1
1K 83.76 83.00 83.37 68.40 67.78 68.09
5K 85.94 59.62 70.39 71.30 49.47 58.40
1K+5K 85.74 66.53 74.92 71.48 55.46 62.46
SelfT 83.82 83.66 83.73 67.91 67.79 67.85
Table 2: Average scores of 5-fold cross-validation
with Classic 1K (1K), 5K (5K), 1K plus 5K
(1K+5K) and self-training with 1K seed and 5K
unlabeled data (SelfT)
useful for languages which lack such resources
and require techniques such as cross-lingual trans-
fer to replace them.
4.4 Quality vs. Quantity
In Section 4.1, we saw that adding more data an-
notated through projection did not elevate SRL
performance. In other words, the same perfor-
mance was achieved using only a small amount
of data. This is contrary to the motivation for cre-
ating synthetic training data, especially when the
hand-annotated data already exist, albeit in a small
size. In this section, we compare the performance
of SRL models trained using manually-annotated
data with SRL models trained using 5K of artifi-
cial or synthetic training data. We use the original
syntactic annotations for both datasets.
To this end, we carry out a 5-fold cross-
validation on Classic1K. We then evaluate the
Classic5K model, on each of the 5 test sets gen-
erated in the cross-validation. The average scores
of the two evaluation setups are compared. The
results are shown in Table 2.
While the 5K model achieves higher precision,
its recall is far lower resulting in dramatically
lower F
1
. This high precision and low recall is due
to the low confidence of the model trained on pro-
jected data suggesting that a considerable amount
of information is not transferred during the projec-
tion. This issue can be attributed to the fact that the
90
Classic projection uses intersection of alignments
in the two translation directions, which is the most
restrictive setting and leaves many source predi-
cates and arguments unaligned.
We next add the Classic5K projected data to
the manually annotated training data in each fold
of another cross-validation setting and evaluate
the resulting models on the same test sets. The
results are reported in the third row of the Ta-
ble 2 (1K+5K). As can be seen, the low qual-
ity of the projected data significantly degrades the
performance compared to when only manually-
annotated data are used for training.
Finally, based on the observation that the qual-
ity of labelling using manually annotated data is
higher than using the automatically projected data,
we replicate 1K+5K with the 5K data labelled us-
ing the model trained on the training subset of 1K
at each cross-validation fold. In other words, we
perform a one-round self-training with this model.
The performance of the resulting model evaluated
in the same cross-validation setting is given in the
last row of Table 2 (SelfT).
As expected, the labelling obtained by mod-
els trained on manual annotation are more useful
than the projected ones when used for training new
models. It is worth noting that, unlike with the
1K+5K setting, the balance between precision and
recall follows that of the 1K model. In addition,
some of the scores are the highest among all re-
sults, although the differences are not significant.
4.5 How little is too little?
In the previous section we saw that using a manu-
ally annotated dataset with as few as 800 sentences
resulted in significantly better SRL performance
than using projected annotation with as many as
5K sentences. This unfortunately indicates the
need for human labour in creating such resources.
It is interesting however to know the lower bound
of this requirement. To this end, we reverse our
cross-validation setting and train on 200 and test
on 800 sentences. We then compare to the 5K
models evaluated on the same 800 sentence sets
at each fold. The results are presented in Table 3.
Even with only 200 manually annotated sentences,
the performance is considerably higher than with
5K sentences of projected annotations. However,
as one might expect, compared to when 800 sen-
tences are used for training, this small model per-
forms significantly worse.
Identification Classification
P R F
1
P R F
1
1K 82.34 79.61 80.95 64.14 62.01 63.06
5K 85.95 59.64 70.42 71.34 49.50 58.45
Table 3: Average scores of 5-fold cross-validation
with Classic 1K (1K) and 5K (5K) using 200 sen-
tences for training and 800 for testing at each fold
5 Conclusion
We have explored the projection-based approach
to SRL by carrying out experiments with a large
set of French semantic role labels which have been
automatically transferred from English. We have
found that increasing the number of these artificial
projections that are used in training an SRL sys-
tem does not improve performance as might have
been expected when creating such a resource. In-
stead it is better to train directly on what little gold
standard data is available, even if this dataset con-
tains only 200 sentences. We suspect that the dis-
appointing performance of the projected dataset
originates in the restrictive way the word align-
ments have been extracted. Only those alignments
that are in the intersection of the English-French
and French-English word alignment sets are re-
tained resulting in low SRL recall. Recent prelim-
inary experiments show that less restrictive align-
ment extraction strategies including extracting the
union of the two sets or source-to-target align-
ments lead to a better recall and consequently F
1
both when used for direct projection to the test
data or for creating the training data and then ap-
plying the resulting model to the test data.
We have compared the use of universal POS
tags and dependency labels to the original, more
fine-grained sets and shown that there is only a
little difference. However, it remains to be seen
whether this finding holds for other languages or
whether it will still hold for French when SRL per-
formance can be improved. It might also be in-
teresting to explore the combination of universal
dependencies with fine-grained POS tags.
Acknowledgments
This research has been supported by the Irish
Research Council Enterprise Partnership Scheme
(EPSPG/2011/102) and the computing infrastruc-
ture of the CNGL at DCU. We thank Lonneke van
der Plas for providing us the Classic data. We also
thank the reviewers for their helpful comments.
91
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th ACL, pages 86?90.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 43?48.
Marie Candito, Benot Crabb?e, and Pascal Denis. 2010.
Statistical french dependency parsing: treebank
conversion and first results. In Proceedings of
LREC?2010.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies
representation. In Proceedings of the COLING
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Claire Gardent and Christophe Cerisara. 2010. Semi-
Automatic Propbanking for French. In TLT9 -
The Ninth International Workshop on Treebanks and
Linguistic Theories.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79?86.
Anna Kup?s?c and Anne Abeill?e. 2008. Growing
treelex. In Proceedings of the 9th International Con-
ference on Computational Linguistics and Intelli-
gent Text Processing, CICLing?08, pages 28?39.
Alejandra Lorenzo and Christophe Cerisara. 2012.
Unsupervised frame based semantic role induction:
application to french and english. In Proceedings of
the ACL 2012 Joint Workshop on Statistical Parsing
and Semantic Processing of Morphologically Rich
Languages, pages 30?35.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92?97.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In In Proceedings of LREC.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Sebastian Pad?o and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles. J.
Artif. Int. Res., 36(1):307?340.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 144?155.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online projectivisation for
synchronous parsing of semantic and syntactic de-
pendencies. In In Proceedings of the Internation
Joint Conference on Artificial Intelligence (IJCAI),
pages 1562?1567.
Lonneke van der Plas, James Henderson, and Paola
Merlo. 2010a. D6. 2: Semantic role annotation of a
french-english corpus.
Lonneke van der Plas, Tanja Samard?zi?c, and Paola
Merlo. 2010b. Cross-lingual validity of propbank
in the manual annotation of french. In Proceedings
of the Fourth Linguistic Annotation Workshop, LAW
IV ?10, pages 113?117.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 299?304.
92
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 223?229,
Dublin, Ireland, August 23-24, 2014.
DCU: Aspect-based Polarity Classification for SemEval Task 4
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman
Dasha Bogdanova, Jennifer Foster and Lamia Tounsi
CNGL Centre for Global Intelligent Content
National Centre for Language Technology
School of Computing
Dublin City University
Dublin, Ireland
{jwagner,parora,scortes,ubarman}@computing.dcu.ie
{dbogdanova,jfoster,ltounsi}@computing.dcu.ie
Abstract
We describe the work carried out by DCU
on the Aspect Based Sentiment Analysis
task at SemEval 2014. Our team submit-
ted one constrained run for the restaurant
domain and one for the laptop domain for
sub-task B (aspect term polarity predic-
tion), ranking highest out of 36 systems on
the restaurant test set and joint highest out
of 32 systems on the laptop test set.
1 Introduction
This paper describes DCU?s participation in the
Aspect Term Polarity sub-task of the Aspect Based
Sentiment Analysis task at SemEval 2014, which
focuses on predicting the sentiment polarity of as-
pect terms for a restaurant and a laptop dataset.
Given, for example, the sentence I have had so
many problems with the computer and the aspect
term the computer, the task is to predict whether
the sentiment expressed towards the aspect term is
positive, negative, neutral or conflict.
Our polarity classification system uses super-
vised machine learning with support vector ma-
chines (SVM) (Boser et al., 1992) to classify an
aspect term into one of the four classes. The fea-
tures we employ are word n-grams (with n rang-
ing from 1 to 5) in a window around the aspect
term, as well as features derived from scores as-
signed by a sentiment lexicon. Furthermore, to
reduce data sparsity, we experiment with replacing
sentiment-bearing words in our n-gram feature set
with their polarity scores according to the lexicon
and/or their part-of-speech tag.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
The paper is organised as follows: in Section 2,
we describe the sentiment lexicons used in this
work and detail the process by which they are
combined, filtered and extended; in Section 3, we
describe our baseline method, a heuristic approach
which makes use of the sentiment lexicon, fol-
lowed by our machine learning method which in-
corporates the rule-based method as features in ad-
dition to word n-gram features; in Section 4, we
present the results of both methods on the training
and test data, and perform an error analysis on the
test set; in Section 5, we compare our approach to
previous research in sentiment classification; Sec-
tion 6 discusses efficiency of our system and on-
going work to improve its speed; finally, in Sec-
tion 7, we conclude and provide suggestions as to
how this research could be fruitfully extended.
2 Sentiment Lexicons
The following four lexicons are employed:
1. MPQA
1
(Wilson et al., 2005) classifies a
word or a stem and its part of speech tag
into positive, negative, both or neutral with
a strong or weak subjectivity.
2. SentiWordNet
2
(Baccianella et al., 2010)
specifies the positive, negative and objective
scores of a synset and its part of speech tag.
3. General Inquirer
3
indicates whether a word
expresses positive or negative sentiment.
4. Bing Liu?s Opinion Lexicon
4
(Hu and Liu,
1
http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
2
http://sentiwordnet.isti.cnr.it/
3
http://www.wjh.harvard.edu/
?
inquirer/
inqtabs.txt
4
http://www.cs.uic.edu/
?
liub/FBS/
sentiment-analysis.html#lexicon
223
2004) indicates whether a word expresses
positive or negative sentiment.
2.1 Lexicon Combination
Since the four lexicons differ in their level of detail
and in how they present information, it is neces-
sary, when combining them, to consolidate the in-
formation and present it in a uniform manner. Our
combination strategy assigns a sentiment score to
a word as follows:
? MPQA: 1 for strong positive subjectivity, -1
for strong negative subjectivity, 0.5 for weak
positive subjectivity, -0.5 for weak negative
subjectivity, and 0 otherwise
? SentiWordNet: The positive score if the pos-
itive score is greater than the negative and ob-
jective scores, the negative score if the nega-
tive score is greater than the positive and the
objective scores, and 0 otherwise
? General Inquirer and Bing Liu?s Opinion
Lexicon: 1 for positive and -1 for negative
The above four scores are summed to arrive at a
final score between -4 and 4 for a word.
5
2.2 Lexicon Filtering
Initial experiments with our sentiment lexicon and
the training data led us to believe that there were
many irrelevant entries that, although capable of
conveying sentiment in some other context, were
not contributing to the sentiment of aspect terms
in the two domains of the task. Therefore, these
words are manually filtered from the lexicon. Ex-
amples of deleted words are just, clearly, indi-
rectly, really and back.
2.3 Adding Domain-Specific Words
A manual inspection of the training data revealed
words missing from the merged sentiment lexicon
but which do express sentiment in these domains.
Examples are mouthwatering, watery and better-
configured. We add these to the lexicon with a
score of either 1 or -1 (depending on their polarity
in the training data). We also add words (e.g. zesty,
acrid) from an online list of culinary terms.
6
5
We also tried to vote over the four lexicon scores but this
did not improve over summing.
6
http://world-food-and-wine.com/
describing-food
2.4 Handling Variation
In order to ensure that all inflected forms of a
word are covered, we lemmatise the words in the
training data using the IMS TreeTagger (Schmid,
1994) and we construct new possibilities using a
suffix list. To correct misspelled words, we con-
sider the corrected form of a misspelled word to be
the form with the highest frequency in a reference
corpus
7
among all the forms within an edit dis-
tance of 1 and 2 from the misspelled word (Norvig,
2012). Multi-word expressions of the form x-y
are added with the polarity of xy or x, as in laid-
back/laidback and well-shaped/well. Expressions
x y, are added with the polarity of x-y, as in so
so/so-so.
3 Methodology
We first build a rule-based system which classi-
fies the polarity of an aspect term based solely on
the scores assigned by the sentiment lexicon. We
then explore different ways of converting the rule-
based system into features which can then be com-
bined with bag-of-n-gram features in a supervised
machine learning set-up.
3.1 Rule-Based Approach
In order to predict the polarity of an aspect term,
we sum the polarity scores of all the words in the
surrounding sentence according to our sentiment
lexicon. Since not all the sentiment words occur-
ring in a sentence influence the polarity of the as-
pect term to the same extent, it is important to
weight the score of each sentiment word by its dis-
tance to the aspect term. Therefore, for each word
in the sentence which is found in our lexicon we
take the score from the lexicon and divide it by its
distance to the aspect term. The distance is calcu-
lated using the sum of the following three distance
functions:
? Token Distance: This function calculates the
difference in the position of the sentiment
word and the aspect term by counting the to-
kens between them.
7
The reference corpus consists of about a million
words retrieved from several public domain books from
Project Gutenberg (http://www.gutenberg.org/),
lists of most frequent words from Wiktionary (http:
//en.wiktionary.org/wiki/Wiktionary:
Frequency_lists) and the British National Corpus
(http://www.kilgarriff.co.uk/bnc-readme.
html) and two thousand laptop reviews crawled from CNET
(http://www.cnet.com/).
224
? Discourse Chunk Distance: This function
counts the discourse chunks that must be
crossed in order to get from the sentiment
word to the aspect term. If the sentiment
word and the aspect term are in the same
discourse chunk, then the distance is zero.
We use the discourse segmenter described in
(Tofiloski et al., 2009).
? Dependency Path Distance: This function
calculates the shortest path between the sen-
timent word and the aspect term in a syntac-
tic dependency graph for the sentence, pro-
duced by parsing the sentence with a PCFG-
LA parser (Attia et al., 2010) trained on con-
sumer review data (Le Roux et al., 2012)
8
,
and converting the resulting phrase-structure
tree into a dependency graph using the Stan-
ford converter (de Marneffe and Manning,
2008) (version 3.3.1).
Since our lexicon also contains multi-word ex-
pressions such as finger licking, we also look up
bigrams and trigrams from the input sentence in
our lexicon. Negation is handled by reversing the
polarity of sentiment words that appear within a
window of three words of the following negators:
not, n?t, no and never.
For each aspect term, we use the distance-
weighted sum of the polarity scores to predict one
of the three classes positive, negative and neutral.
9
After experimenting with various thresholds we
settled on the following simple strategy: if the po-
larity score for an aspect term is greater than zero
then it is classified as positive, if the score is less
than zero, then it is classified as negative, other-
wise it is classified as neutral.
3.2 Machine Learning Approach
We train a four-way SVM classifier for each do-
main (laptop and restaurant), using Weka?s SMO
implementation (Platt, 1998; Hall et al., 2009).
10
8
To facilitate parsing, the data was normalised using the
process described in (Le Roux et al., 2012) with minor mod-
ifications, e. g. treatment of non-breakable space characters,
abbreviations and emoticons. The normalised version of the
data was used for all experiments.
9
We also experimented with classifying aspect terms as
conflict when the individual scores for positive and negative
sentiment were both relatively high. However, this proved
unsuccessful.
10
We also experimented with logistic regression, random
forests, k-nearest neighbour, naive Bayes and multi-layer per-
ceptron in Weka, but did not match performance of an SVM
trained with default parameters.
Transf. n c n-gram Freq.
-L? 2 2 cord with 1
AL? 2 2 <aspect> with 56
ALS? 1 4 <negu080> 595
ALSR- 1 4 <negu080> 502
AL? 2 4 and skip 1
ALSR- 2 4 and <negu080> 25
ALSRP 1 4 <negu080>/vb 308
Table 1: 7 of the 2,640 bag-of-n-gram features
extracted for the aspect term cord from the lap-
top training sentence I charge it at night and skip
taking the cord with me because of the good bat-
tery life. The last column shows the frequency of
the feature in the training data. Transformations:
A=aspect, L=lowercase, S=score, R=restricted to
certain POS, P=POS annotation
Our system submission uses bag-of-n-gram fea-
tures and features derived from the rule-based ap-
proach. Decisions about parameters are made in 5-
fold cross-validation on the training data provided
for the task.
3.2.1 Bag-of-N-gram Features
We extract features encoding the presence of spe-
cific lower-cased n-grams (L) (n = 1, ..., 5) in
the context of the aspect term to be classified (c
words to the left and c words to the right with
c = 1, ..., 5, inf) for 10 combinations of trans-
formations: replacement of the aspect term with
<ASPECT> (A), replacement of sentiment words
with a discretised score (S), restriction (R) of the
sentiment word replacement to certain parts-of-
speech, and annotation of the discretised score
with the POS (P) of the sentiment word. An ex-
ample is shown in Table 1.
3.2.2 Adding Rule-Based Score Features
We explore two approaches for incorporating in-
formation from the rule-based approach (Sec-
tion 3.1) into our SVM classifier. The first ap-
proach is to encode polarity scores directly as the
following four features:
1. distance-weighted sum of scores of positive
words in the sentence
2. distance-weighted sum of scores of negative
words in the sentence
3. number of positive words in the sentence
225
4. number of negative words in the sentence
The second approach is less direct: for each do-
main, we train J48 decision trees with minimum
leaf size 60 using the four rule-based features de-
scribed above. We then use the decision rules
and the conjunctions leading from the root node
to each leaf node to binarise the above four basic
score features, producing 122 features. Further-
more, we add normalised absolute values, rank of
values and interval indicators, producing 48 fea-
tures.
3.2.3 Submitted Runs
We eliminate features that have redundant value
columns for the training data, and we apply fre-
quency thresholds (13, 18, 25 and 35) to further
reduce the number of features. We perform a grid-
search to optimise the parameters C and ? of the
SVM RBF kernel. We choose the system to sub-
mit based on average cross-validation accuracy.
We experiment with combinations of the three fea-
ture sets described above. We choose the bina-
rised features over the raw rule-based scores be-
cause cross-validation results are inferior for the
rule-based scores in initial experiments with fea-
ture frequency threshold 35: 70.26 vs. 71.36 for
laptop and 72.06 vs. 72.15 for restaurant. There-
fore, we decide to focus on systems with binarised
score features for lower feature frequency thresh-
olds, which are more CPU-intensive to train. For
both domains, the system we end up submitting
is a combination of the n-gram features and the
binarised features with parameters C = 3.981,
? = 0.003311 for the laptop data, C = 1.445,
? = 0.003311 for the restaurant data, and a fre-
quency threshold of 13.
4 Results and Analysis
Table 2 shows the training and test accuracy of
the task baseline system (Pontiki et al., 2014), a
majority baseline classifying everything as posi-
tive, our rule-based system and our submitted sys-
tem. The restaurant domain has a higher accuracy
than the laptop domain for all systems, the SVM
system outperforms the rule-based system on both
domains, and the test accuracy is higher than the
training accuracy for all systems in the restaurant
domain.
We observe that the majority of our systems? er-
rors fall into the following categories:
Dataset System Training Test
Laptop Baseline ? 51.1%
Laptop All positive 41.9% 52.1%
Laptop Rule-based 65.4% 67.7%
Laptop SVM 72.3% 70.5%
Restaurant Baseline ? 64.3%
Restaurant All positive 58.6% 64.2%
Restaurant Rule-based 69.5% 77.8%
Restaurant SVM 72.7% 81.0%
Table 2: Accuracy of the task baseline system, a
system classifying everything as positive, our rule-
based system and our submitted SVM-based sys-
tem on train (5-fold cross-validation) and test sets
? Sentiment not expressed explicitly: The
sentiment cannot be inferred from local lexi-
cal and syntactic information, e. g. The sushi
is cut in blocks bigger than my cell phone.
? Non-obvious expression of negation: For
example, The Management was less than ac-
comodating [sic]. The rule-based approach
does not capture such cases and there are
not enough similar training examples for the
SVM to learn to correctly classify them.
? Conflict cases: The training data contains
too few examples of conflict sentences for the
system to learn to detect them.
11
For the restaurant domain, there are more than
fifty cases where the rule-based approach fails to
detect sentiment, but the machine learning ap-
proach classifies it correctly. Most of these cases
contain no sentiment lexicon words, thus the rule-
based system marks them as being neutral. How-
ever, the machine learning system was able to fig-
ure out the correct polarity. Examples of such
cases include Try the rose roll (not on menu) and
The gnocchi literally melts in your mouth!. Fur-
thermore, in the laptop domain, a number of the
errors made by the rule-based system arise from
the ambiguous nature of some lexicon words. For
example, the sentence Only 2 usb ports ... seems
kind of ... limited is misclassified because the
word kind is considered to be positive.
There are a few cases where the rule-based sys-
tem outperforms the machine learning one. It hap-
pens when a sentence contains a rare word with
strong polarity, e. g. the word heavenly in The
11
We only classify one test instance as conflict.
226
chocolate raspberry cake is heavenly - not too
sweet, but full of flavor.
5 Related Work
The use of supervised machine learning with bag-
of-word or bag-of-n-gram feature sets has been
a standard approach to the problem of sentiment
polarity classification since the seminal work by
Pang et al. (2002) on movie review polarity pre-
diction. Heuristic methods which rely on a lexi-
con of sentiment words have also been widespread
and much of the research in this area has been
devoted to the unsupervised induction of good
quality sentiment indicators (see, for example,
Hatzivassiloglou and McKeown (1997) and Tur-
ney (2002), and Liu (2010) for an overview). The
integration of sentiment lexicon scores as fea-
tures in supervised machine learning to supple-
ment standard bag-of-n-gram features has also
been employed before (see, for example, Bak-
liwal et al. (2013)). The replacement of train-
ing/test words with scores/labels from sentiment
lexicons has also been used by Baccianella et
al. (2009), who supplement n-grams such as hor-
rible location with generalised expressions such
as NEGATIVE location. Linguistic features which
capture generalisations at the level of syntax (Mat-
sumoto et al., 2005), semantics (Johansson and
Moschitti, 2010) and discourse (Lazaridou et al.,
2013) have also been widely applied. In using bi-
narised features derived from the nodes of a deci-
sion tree, we are following our recent work which
uses the same technique in a different task: quality
estimation for machine translation (Rubino et al.,
2012; Rubino et al., 2013).
The main novelty in our system lies not in the
individual techniques but rather in they way they
are combined and integrated. For example, our
combination of token/chunk/dependency path dis-
tance used to weight the relationship between a
sentiment word and the aspect term has ? to the
best of our knowledge ? not been applied before.
6 Efficiency
Building a system for a shared task, we focus
solely on the accuracy of the system in all our deci-
sions. For example, we parse all training and test
data multiple times using different grammars to
increase sentence coverage from 99.87% to 100%.
To offer a more practical system, we work on
implementing a simplified, fully automated sys-
tem that is more efficient. So far, we replaced
time-consuming parsing with POS tagging. The
system accepts as input and generates as output
valid SemEval ABSA XML documents.
12
After
extracting the text and the aspect terms from the
input, the text is normalised using the process de-
scribed in Footnote 8. The feature extraction is
performed as described in Section 3 with the fol-
lowing modifications:
? The POS information used by the n-gram
feature extractor is obtained using the IMS
TreeTagger (Schmid, 1994) instead of using
the PCFG-LA parser (Attia et al., 2010).
? The distance used by the rule-based approach
is the token distance only, instead of a com-
bination of three distance functions.
The sentiment lexicon and the classification mod-
els used are described in Sections 2 and 3 respec-
tively.
The test sets containing 800 sentences are POS
tagged in less than half a second each. Surpris-
ingly, accuracy of aspect term polarity prediction
increases to 71.4% (from 70.5% for the submitted
system) on the laptop test set, using the same SVM
parameters as for the submitted system. However,
we see a degradation to 78.8% (from 81.0% for the
submitted system) for the restaurant test set. This
is an encouraging result as the SVM parameters
are not yet fully optimised for the slightly different
information and as the remaining modifications to
be implemented should not change accuracy any
further.
The next bottleneck that needs to be addressed
before the system can be used in applications re-
quiring quick responses is the current implementa-
tion of the n-gram feature extractor: It enumerates
all n-grams (for all context window sizes and n-
gram transformations) only to then intersect these
features with the list of selected features. For the
shared task, this made sense as we initially need
all features to make our selection of features, and
as we only need to run the feature extractor a few
times. For a practical system that has to process
new test sets frequently, however, it will be more
efficient to check for each selected feature whether
the respective event occurs in the input.
12
We validate documents using the XML schema defini-
tion provided on the shared task website.
227
7 Conclusion
We have described our aspect term polarity predic-
tion system, which employs supervised machine
learning using a combination of n-grams and sen-
timent lexicon features. Although our submitted
system performs very well, it is interesting to note
that our rule-based system is not that far behind.
This suggests that a state-of-the-art system can be
build without machine learning and that careful
design of the other system components is impor-
tant. However, the very good performance of our
machine-learning-based system also suggests that
word n-gram features do provide useful informa-
tion that is missed by a sentiment lexicon alone,
and that it is always worthwhile to perform careful
parameter tuning to eke out as much as possible
from such an approach.
Future work should investigate how much each
system component contributes to the overall per-
formance, e. g. lexicon combination, lemmatisa-
tion, spelling correction, other normalisations,
negation handling, distance function and n-gram
feature transformations. There is also room for
improvements in most of these components, e. g.
our handling of complex negations. Detection of
conflicts also needs more attention. Features in-
dicating the presence of trigger words for negation
and conflicts that are currently used only internally
in the rule-based component could be added to the
SVM feature set. It would also be interesting to
see how the compositional approach described by
Socher et al. (2013) handles these difficult cases.
The score features could be easily augmented by
breaking down scores by the four employed lexi-
cons. This way, the SVM can choose to combine
the information from these scores differently than
just summing them, allowing it to learn more com-
plex relations. Lexicon filtering and addition of
domain-specific entries could be automated to re-
duce the time needed to adjust to a new domain.
Finally, machine learning methods that can effi-
ciently handle large feature sets such as logistic
regression should be tried with the full feature set
(not applying frequency thresholds).
Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
CNGL (www.cngl.ie) at Dublin City University.
The authors wish to acknowledge the DJEI/DES/
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational facil-
ities and support. We are grateful to Qun Liu and
Josef van Genabith for their helpful comments.
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statis-
tical latent-variable parsing models for arabic, en-
glish and french. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 67?75.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet rating of product reviews.
In Proceedings of ECIR, pages 461?472.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh Conference
on International Language Resources and Evalua-
tion (LREC?10).
Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil,
Ron O?Brien, Lamia Tounsi, and Mark Hughes.
2013. Sentiment analysis of political tweets: To-
wards an accurate classifier. In Proceedings of the
NAACL Workshop on Language Analysis in Social
Media, pages 49?58.
Bernhard E. Boser, Isabelle M. Guyon, and
Vladimir N. Vapnik. 1992. A training algo-
rithm for optimal margin classifiers. In Proceedings
of the Fifth Annual Workshop on Computational
Learning Theory, pages 144?152.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In COLING 2008 Workshop on Cross-
framework and Cross-domain Parser Evaluation.,
pages 1?8.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting
of the ACL and the 8th Conference of the European
Chapter of the ACL, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177.
228
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 67?76.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of
the 51th Annual Meeting of the Association for
Computational Linguistics, pages 1630?1639.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
sul Samad Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 systems for the SANCL 2012 shared
task. Notes of the First Workshop on Syntactic
Analysis of Non-Canonical Language (SANCL).
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Handbook of Natural Language Processing.
Shotaro Matsumoto, Hiroya Takamura, and Manubu
Okumura, 2005. Advances in Knowledge Discovery
and Data Mining, volume 3518 of Lecture Notes in
Computer Science, chapter Sentiment Classification
Using Word Sub-sequences and Dependency Sub-
trees, pages 301?311.
Peter Norvig. 2012. How to write a spelling corrector.
http://norvig.com/spell-correct.
html. [Online; accessed 2014-03-19].
Po Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
John C. Platt. 1998. Fast training of support vec-
tor machines using sequential minimal optimization.
In B. Schoelkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector
Learning, pages 185?208.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. Dcu-symantec submission for
the wmt 2012 quality estimation task. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 138?144.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, pages 1631?
1642.
Milan Tofiloski, Julian Brooke, and Maite Taboada.
2009. A syntactic and lexical-based discourse seg-
menter. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 77?
80.
Peter Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
cation of reviews. In Proceedings of the ACL, pages
417?424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354.
229
Parser-Based Retraining for Domain Adaptation of Probabilistic Generators
Deirdre Hogan, Jennifer Foster, Joachim Wagner and Josef van Genabith
National Centre for Language Technology
School of Computing
Dublin City University
Ireland
{dhogan, jfoster, jwagner, josef}@computing.dcu.ie
Abstract
While the effect of domain variation on Penn-
treebank-trained probabilistic parsers has been
investigated in previous work, we study its ef-
fect on a Penn-Treebank-trained probabilistic
generator. We show that applying the gener-
ator to data from the British National Corpus
results in a performance drop (from a BLEU
score of 0.66 on the standard WSJ test set to a
BLEU score of 0.54 on our BNC test set). We
develop a generator retraining method where
the domain-specific training data is automat-
ically produced using state-of-the-art parser
output. The retraining method recovers a sub-
stantial portion of the performance drop, re-
sulting in a generator which achieves a BLEU
score of 0.61 on our BNC test data.
1 Introduction
Grammars extracted from the Wall Street Journal
(WSJ) section of the Penn Treebank have been suc-
cessfully applied to natural language parsing, and
more recently, to natural language generation. It is
clear that high-quality grammars can be extracted
for the WSJ domain but it is not so clear how
these grammars scale to other text genres. Gildea
(2001), for example, has shown that WSJ-trained
parsers suffer a drop in performance when applied
to the more varied sentences of the Brown Cor-
pus. We investigate the effect of domain variation in
treebank-grammar-based generation by applying a
WSJ-trained generator to sentences from the British
National Corpus (BNC).
As with probabilistic parsing, probabilistic gener-
ation aims to produce the most likely output(s) given
the input. We can distinguish three types of prob-
abilistic generators, based on the type of probabil-
ity model used to select the most likely sentence.
The first type uses an n-gram language model, e.g.
(Langkilde, 2000), the second type uses a proba-
bility model defined over trees or feature-structure-
annotated trees, e.g. (Cahill and van Genabith,
2006), and the third type is a mixture of the first
and second type, employing n-gram and grammar-
based features, e.g. (Velldal and Oepen, 2005). The
generator used in our experiments is an instance of
the second type, using a probability model defined
over Lexical Functional Grammar c-structure and
f-structure annotations (Cahill and van Genabith,
2006; Hogan et al, 2007).
In an initial evaluation, we apply our probabilistic
WSJ-trained generator to BNC material, and show
that the generator suffers a substantial performance
degradation, with a drop in BLEU score from 0.66
to 0.54. We then turn our attention to the problem
of adapting the generator so that it can more accu-
rately generate the 1,000 sentences in our BNC test
set. The problem of adapting any NLP system to a
domain different from the domain upon which it has
been trained and for which no gold standard train-
ing material is available is a very real one, and one
which has been the focus of much recent research in
parsing. Some success has been achieved by training
a parser, not on gold standard hand-corrected trees,
but on parser output trees. These parser output trees
can by produced by a second parser in a co-training
scenario (Steedman et al, 2003), or by the same
parser with a reranking component in a type of self-
training scenario (McClosky et al, 2006). We tackle
165
the problem of domain adaptation in generation in
a similar way, by training the generator on domain
specific parser output trees instead of manually cor-
rected gold standard trees. This experiment achieves
promising results, with an increase in BLEU score
from 0.54 to 0.61. The method is generic and can be
applied to other probabilistic generators (for which
suitable training material can be automatically pro-
duced).
2 Background
The natural language generator used in our experi-
ments is the WSJ-trained system described in Cahill
and van Genabith (2006) and Hogan et al (2007).
Sentences are generated from Lexical Functional
Grammar (LFG) f-structures (Kaplan and Bresnan,
1982). The f-structures are created automatically
by annotating nodes in the gold standard WSJ trees
with LFG functional equations and then passing
these equations through a constraint solver (Cahill
et al, 2004). The generation algorithm is a chart-
based one which works by finding the most proba-
ble tree associated with the input f-structure. The
yield of the most probable tree is the output sen-
tence. An annotated PCFG, in which the non-
terminal symbols are decorated with functional in-
formation, is used to generate the most probable tree
from an f-structure. Cahill and van Genabith (2006)
attain 98.2% coverage and a BLEU score of 0.6652
on the standard WSJ test set (Section 23). Hogan
et al (2007) describe an extension to the system
which replaces the annotated PCFG selection model
with a more sophisticated history-based probabilis-
tic model. Instead of conditioning the righthand side
of a rule on the lefthand non-terminal and its asso-
ciated functional information alone, the new model
includes non-local conditioning information in the
form of functional information associated with an-
cestor nodes of the lefthand side category. This sys-
tem achieves a BLEU score of 0.6724 and 99.9%
coverage.
Other WSJ-trained generation systems include
Nakanishi et al (2005) and White et al (2007).
Nakanishi et al (2005) describe a generator trained
on a HPSG grammar derived from the WSJ Section
of the Penn Treebank. On sentences of ? 20 words
in length, their system attains coverage of 90.75%
and a BLEU score of 0.7733. White et al (2007)
describe a CCG-based realisation system which has
been trained on logical forms derived from CCG-
Bank (Hockenmaier and Steedman, 2005), achiev-
ing 94.3% coverage and a BLEU score of 0.5768 on
WSJ23 for all sentence lengths. The input structures
upon which these systems are trained vary in form
and specificity, but what the systems have in com-
mon is that their various input structures are derived
from Penn Treebank trees.
3 The BNC Test Data
The new English test set consists of 1,000 sentences
taken from the British National Corpus (Burnard,
2000). The BNC is a one hundred million word bal-
anced corpus of British English from the late twenti-
eth century. Ninety per cent of it is written text, and
the remaining 10% consists of transcribed sponta-
neous and scripted spoken language. The BNC sen-
tences in the test set are not chosen completely at
random. Each sentence in the test set has the prop-
erty of containing a word which appears as a verb
in the BNC but not in the usual training sections of
the Wall Street Journal section of the Penn Treebank
(WSJ02-21). Sentences were chosen in this way so
that the resulting test set would be a difficult one
for WSJ-trained systems. In order to produce in-
put f-structures for the generator, the test sentences
were manually parsed by one annotator, using as
references the Penn Treebank trees themselves and
the Penn Treebank bracketing guidelines (Bies et
al., 1995). When the two references did not agree,
the guidelines took precedence over the Penn Tree-
bank trees. Difficult parsing decisions were docu-
mented. Due to time constraints, the annotator did
not mark functional tags or traces. The context-free
gold standard parse trees were transformed into f-
structures using the automatic procedure of Cahill et
al. (2004).
4 Experiments
Experimental Setup In our first experiment, we
apply the original WSJ-trained generator to our
BNC test set. The gold standard trees for our BNC
test set differ from the gold standard Wall Street
Journal trees, in that they do not contain Penn-II
traces or functional tags. The process which pro-
166
duces f-structures from trees makes use of trace and
functional tag information, if available. Thus, to en-
sure that the training and test input f-structures are
created in the same way, we use a version of the
generator which is trained using gold standard WSJ
trees without functional tag or trace information.
When we test this system on the WSJ23 f-structures
(produced in the same way as the WSJ training ma-
terial), the BLEU score decreases slightly from 0.67
to 0.66. This is our baseline system.
In a further experiment, we attempt to adapt
the generator to BNC data by using BNC trees as
training material. Because we lack gold standard
BNC trees (apart from those in our test set), we
try instead to use parse trees produced by an accu-
rate parser. We choose the Charniak and Johnson
reranking parser because it is freely available and
achieves state-of-the-art accuracy (a Parseval f-score
of 91.3%) on the WSJ domain (Charniak and John-
son, 2005). It is, however, affected by domain vari-
ation ? Foster et al (2007) report that its f-score
drops by approximately 8 percentage points when
applied to the BNC domain. Our training size is
500,000 sentences. We conduct two experiments:
the first, in which 500,000 sentences are extracted
randomly from the BNC (minus the test set sen-
tences), and the second in which only shorter sen-
tences, of length ? 20 words, are chosen as training
material. The rationale behind the second experi-
ment is that shorter sentences are less likely to con-
tain parser errors.
We use the BLEU evaluation metric for our ex-
periments. We measure both coverage and full cov-
erage. Coverage measures the number of cases for
which the generator produced some kind of out-
put. Full coverage measures the number of cases for
which the generator produced a tree spanning all of
the words in the input.
Results The results of our experiments are shown
in Fig. 1. The first row shows the results we ob-
tain when the baseline system is applied to the f-
structures derived from the 1,000 BNC gold stan-
dard parse trees. The second row shows the results
on the same test set for a system trained on Charniak
and Johnson parser output trees for 500,000 BNC
sentences. The results in the final row are obtained
by training the generator on Charniak and Johnson
parser output trees for 500,000 BNC sentences of
length ? 20 words in length.
Discussion As expected, the performance of the
baseline system degrades when faced with out-of-
domain test data. The BLEU score drops from a
0.66 score for WSJ test data to a 0.54 score for
the BNC test data, and full coverage drops from
85.97% to 68.77%. There is a substantial improve-
ment, however, when the generator is trained on
BNC data. The BLEU score jumps from 0.5358
to 0.6135. There are at least two possible reasons
why a BLEU score of 0.66 is not obtained: The first
is that the quality of the f-structure-annotated trees
upon which the generator has been trained has de-
graded. For the baseline system, the generator is
trained on f-structure-annotated trees derived from
gold trees. The new system is trained on f-structure-
annotated parser output trees, and the performance
of Charniak and Johnson?s parser degrades when ap-
plied to BNC data (Foster et al, 2007). The second
reason has been suggested by Gildea (2001): WSJ
data is easier to learn than the more varied data in the
Brown Corpus or BNC. Perhaps even if gold stan-
dard BNC parse trees were available for training, the
system would not behave as well as it does for WSJ
material.
It is interesting to note that training on 500,000
shorter sentences does not appear to help. We hy-
pothesized that it would improve results because
shorter sentences are less likely to contain parser
errors. The drop in full coverage from 86.69% to
79.58% suggests that the number of short sentences
needs to be increased so that the size of the training
material stays constant.
5 Conclusion
We have investigated the effect of domain varia-
tion on a LFG-based WSJ-trained generation sys-
tem by testing the system?s performance on 1,000
sentences from the British National Corpus. Perfor-
mance drops from a BLEU score of 0.66 onWSJ test
data to 0.54 on the BNC test set. Encouragingly, we
have also shown that domain-specific training mate-
rial produced by a parser can be used to claw back
a significant portion of this performance degrada-
tion. Our method is general and could be applied
to other WSJ-trained generators (e.g. (Nakanishi et
167
Train BLEU Coverage Full Coverage
WSJ02-21 0.5358 99.1 68.77
BNC(500k) 0.6135 99.1 86.69
BNC(500k) ? 20 words 0.5834 99.1 79.58
Figure 1: Results for 1,000 BNC Sentences
al., 2005; White et al, 2007)). We intend to con-
tinue this research by training our generator on parse
trees produced by a BNC-self-trained version of the
Charniak and Johnson reranking parser (Foster et al,
2007). We also hope to extend the evaluation beyond
the BLEU metric by carrying out a human judge-
ment evaluation.
Acknowledgments
This research has been supported by the Enterprise
Ireland Commercialisation Fund (CFTD/2007/229),
Science Foundation Ireland (04/IN/I527) and the
IRCSET Embark Initative (P/04/232). We thank the
Irish Centre for High End Computing for providing
computing facilities.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank
II style, Penn Treebank project. Technical Report
Tech Report MS-CIS-95-06, University of Pennsylva-
nia, Philadelphia, PA.
Lou Burnard. 2000. User reference guide for the British
National Corpus. Technical report, Oxford University
Computing Services.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
lfg approximations. In Proceedings of the 21st COL-
ING and the 44th Annual Meeting of the ACL, pages
1033?1040, Sydney.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Meeting of the ACL, pages
320?327, Barcelona.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Ann Arbor.
Jennifer Foster, Joachim Wagner, Djame? Seddah, and
Josef van Genabith. 2007. Adapting WSJ-trained
parsers to the British National Corpus using in-domain
self-training. In Proceedings of the Tenth IWPT, pages
33?35, Prague.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of EMNLP, Pittsburgh.
Julia Hockenmaier and Mark Steedman. 2005. Ccgbank:
Users? manual. Technical report, Computer and Infor-
mation Science, University of Pennsylvania.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units in
history-based probabilistic generation. In Proceedings
of the joint EMNLP/CoNLL, pages 267?276, Prague.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar: a Formal System for Grammatical Repre-
sentation. In Joan Bresnan, editor, The Mental Repre-
sentation of Grammatical Relations, pages 173?281.
MIT Press.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of NAACL, Seattle.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proceedings of the
Ninth IWPT, pages 93?102, Vancouver.
Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Boot-
strapping statistical parsers from small datasets. In
Proceedings of EACL, pages 331?338, Budapest.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proceedings
of the MT-Summit, Phuket.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proceedings of the Workshop on
Using Corpora for NLG: Language Generation and
Machine Translation (UCNLG+MT), pages 267?276,
Copenhagen.
168
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 1?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Statistical Parsing of Morphologically Rich Languages (SPMRL)
What, How and Whither
Reut Tsarfaty
Uppsala Universitet
Djame? Seddah
Alpage (Inria/Univ. Paris-Sorbonne)
Yoav Goldberg
Ben Gurion University
Sandra Ku?bler
Indiana University
Marie Candito
Alpage (Inria/Univ. Paris 7)
Jennifer Foster
NCLT, Dublin City University
Yannick Versley
Universita?t Tu?bingen
Ines Rehbein
Universita?t Saarbru?cken
Lamia Tounsi
NCLT, Dublin City University
Abstract
The term Morphologically Rich Languages
(MRLs) refers to languages in which signif-
icant information concerning syntactic units
and relations is expressed at word-level. There
is ample evidence that the application of read-
ily available statistical parsing models to such
languages is susceptible to serious perfor-
mance degradation. The first workshop on sta-
tistical parsing of MRLs hosts a variety of con-
tributions which show that despite language-
specific idiosyncrasies, the problems associ-
ated with parsing MRLs cut across languages
and parsing frameworks. In this paper we re-
view the current state-of-affairs with respect
to parsing MRLs and point out central chal-
lenges. We synthesize the contributions of re-
searchers working on parsing Arabic, Basque,
French, German, Hebrew, Hindi and Korean
to point out shared solutions across languages.
The overarching analysis suggests itself as a
source of directions for future investigations.
1 Introduction
The availability of large syntactically annotated cor-
pora led to an explosion of interest in automati-
cally inducing models for syntactic analysis and dis-
ambiguation called statistical parsers. The devel-
opment of successful statistical parsing models for
English focused on the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1993)) as the pri-
mary, and sometimes only, resource. Since the ini-
tial release of the Penn Treebank (PTB Marcus et
al. (1993)), many different constituent-based parsing
models have been developed in the context of pars-
ing English (e.g. (Magerman, 1995; Collins, 1997;
Charniak, 2000; Chiang, 2000; Bod, 2003; Char-
niak and Johnson, 2005; Petrov et al, 2006; Huang,
2008; Finkel et al, 2008; Carreras et al, 2008)).
At their time, each of these models improved the
state-of-the-art, bringing parsing performance on the
standard test set of the Wall-Street-Journal to a per-
formance ceiling of 92% F1-score using the PARS-
EVAL evaluation metrics (Black et al, 1991). Some
of these parsers have been adapted to other lan-
guage/treebank pairs, but many of these adaptations
have been shown to be considerably less successful.
Among the arguments that have been proposed
to explain this performance gap are the impact of
small data sets, differences in treebanks? annotation
schemes, and inadequacy of the widely used PARS-
EVAL evaluation metrics. None of these aspects in
isolation can account for the systematic performance
deterioration, but observed from a wider, cross-
linguistic perspective, a picture begins to emerge ?
that the morphologically rich nature of some of the
languages makes them inherently more susceptible
to such performance degradation. Linguistic factors
associated with MRLs, such as a large inventory of
word-forms, higher degrees of word order freedom,
and the use of morphological information in indi-
cating syntactic relations, makes them substantially
harder to parse with models and techniques that have
been developed with English data in mind.
1
In addition to these technical and linguistic fac-
tors, the prominence of English parsing in the litera-
ture reduces the visibility of research aiming to solve
problems particular to MRLs. The lack of stream-
lined communication among researchers working
on different MRLs often leads to a reinventing the
wheel syndrome. To circumvent this, the first work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010) offers a platform for
this growing community to share their views of the
different problems and oftentimes similar solutions.
We identify three main types of challenges, each
of which raises many questions. Many of the ques-
tions are yet to be conclusively answered. The first
type of challenges has to do with the architectural
setup of parsing MRLs: What is the nature of the in-
put? Can words be represented abstractly to reflect
shared morphological aspects? How can we cope
with morphological segmentation errors propagated
through the pipeline? The second type concerns the
representation of morphological information inside
the articulated syntactic model: Should morpholog-
ical information be encoded at the level of PoS tags?
On dependency relations? On top of non-terminals
symbols? How should the integrated representations
be learned and used? A final genuine challenge
has to do with sound estimation for lexical probabil-
ities: Given the finite, and often rather small, set of
data, and the large number of morphological analy-
ses licensed by rich inflectional systems, how can we
analyze words unseen in the training data?
Many of the challenges reported here are mostly
irrelevant when parsing Section 23 of the PTB but
they are of primordial importance in other tasks, in-
cluding out-of-domain parsing, statistical machine
translation, and parsing resource-poor languages.
By synthesizing the contributions to the workshop
and bringing it to the forefront, we hope to advance
the state of the art of statistical parsing in general.
In this paper we therefore take the opportunity
to analyze the knowledge that has been acquired in
the different investigations for the purpose of iden-
tifying main bottlenecks and pointing out promising
research directions. In section 2, we define MRLs
and identify syntactic characteristics associated with
them. We then discuss work on parsing MRLs in
both the dependency-based and constituency-based
setup. In section 3, we review the types of chal-
lenges associated with parsing MRLs across frame-
works. In section 4, we focus on the contributions to
the SPMRL workshop and identify recurring trends
in the empirical results and conceptual solutions. In
section 5, we analyze the emerging picture from a
bird?s eye view, and conclude that many challenges
could be more faithfully addressed in the context of
parsing morphologically ambiguous input.
2 Background
2.1 What are MRLs?
The term Morphologically Rich Languages (MRLs)
is used in the CL/NLP literature to refer to languages
in which substantial grammatical information, i.e.,
information concerning the arrangement of words
into syntactic units or cues to syntactic relations, is
expressed at word level.
The common linguistic and typological wisdom is
that ?morphology competes with syntax? (Bresnan,
2001). In effect, this means that rich morphology
goes hand in hand with a host of nonconfigurational
syntactic phenomena of the kind discussed by Hale
(1983). Because information about the relations be-
tween syntactic elements is indicated in the form of
words, these words can freely change their positions
in the sentence. This is referred to as free word or-
der (Mithun, 1992). Information about the group-
ing of elements together can further be expressed by
reference to their morphological form. Such logical
groupings of disparate elements are often called dis-
continuous constituents. In dependency structures,
such discontinuities impose nonprojectivity. Finally,
rich morphological information is found in abun-
dance in conjunction with so-called pro-drop or zero
anaphora. In such cases, rich morphological infor-
mation in the head (or co-head) of the clause of-
ten makes it possible to omit an overt subject which
would be semantically impoverished.
English, the most heavily studied language within
the CL/NLP community, is not an MRL. Even
though a handful of syntactic features (such as per-
son and number) are reflected in the form of words,
morphological information is often secondary to
other syntactic factors, such as the position of words
and their arrangement into phrases. German, an
Indo-European language closely related to English,
already exhibits some of the properties that make
2
parsing MRLs problematic. The Semitic languages
Arabic and Hebrew show an even more extreme case
in terms of the richness of their morphological forms
and the flexibility in their syntactic ordering.
2.2 Parsing MRLs
Pushing the envelope of constituency parsing:
The Head-Driven models of the type proposed
by Collins (1997) have been ported to parsing
many MRLs, often via the implementation of Bikel
(2002). For Czech, the adaptation by Collins et al
(1999) culminated in an 80 F1-score.
German has become almost an archetype of the
problems caused by MRLs; even though German
has a moderately rich morphology and a moder-
ately free word order, parsing results are far from
those for English (see (Ku?bler, 2008) and references
therein). Dubey (2005) showed that, for German
parsing, adding case and morphology information
together with smoothed markovization and an ade-
quate unknown-word model is more important than
lexicalization (Dubey and Keller, 2003).
For Modern Hebrew, Tsarfaty and Sima?an (2007)
show that a simple treebank PCFG augmented with
parent annotation and morphological information as
state-splits significantly outperforms Head-Driven
markovized models of the kind made popular by
Klein and Manning (2003). Results for parsing
Modern Standard Arabic using Bikel?s implemen-
tation on gold-standard tagging and segmentation
have not improved substantially since the initial re-
lease of the treebank (Maamouri et al, 2004; Kulick
et al, 2006; Maamouri et al, 2008).
For Italian, Corazza et al (2004) used the Stan-
ford parser and Bikel?s parser emulation of Collins?
model 2 (Collins, 1997) on the ISST treebank, and
obtained significantly lower results compared to En-
glish. It is notable that these models were ap-
plied without adding morphological signatures, us-
ing gold lemmas instead. Corazza et al (2004) fur-
ther tried different refinements including parent an-
notation and horizontal markovization, but none of
them obtained the desired improvement.
For French, Crabbe? and Candito (2008) and Sed-
dah et al (2010) show that, given a corpus compara-
ble in size and properties (i.e. the number of tokens
and grammar size), the performance level, both for
Charniak?s parser (Charniak, 2000) and the Berke-
ley parser (Petrov et al, 2006) was higher for pars-
ing the PTB than it was for French. The split-merge-
smooth implementation of (Petrov et al, 2006) con-
sistently outperform various lexicalized and unlexi-
calized models for French (Seddah et al, 2009) and
for many other languages (Petrov and Klein, 2007).
In this respect, (Petrov et al, 2006) is considered
MRL-friendly, due to its language agnostic design.
The rise of dependency parsing: It is commonly
assumed that dependency structures are better suited
for representing the syntactic structures of free word
order, morphologically rich, languages, because this
representation format does not rely crucially on the
position of words and the internal grouping of sur-
face chunks (Mel?c?uk, 1988). It is an entirely differ-
ent question, however, whether dependency parsers
are in fact better suited for parsing such languages.
The CoNLL shared tasks on multilingual depen-
dency parsing in 2006 and 2007 (Buchholz and
Marsi, 2006; Nivre et al, 2007a) demonstrated that
dependency parsing for MRLs is quite challenging.
While dependency parsers are adaptable to many
languages, as reflected in the multiplicity of the lan-
guages covered,1 the analysis by Nivre et al (2007b)
shows that the best result was obtained for English,
followed by Catalan, and that the most difficult lan-
guages to parse were Arabic, Basque, and Greek.
Nivre et al (2007a) drew a somewhat typological
conclusion, that languages with rich morphology
and free word order are the hardest to parse. This
was shown to be the case for both MaltParser (Nivre
et al, 2007c) and MST (McDonald et al, 2005), two
of the best performing parsers on the whole.
Annotation and evaluation matter: An emerg-
ing question is therefore whether models that have
been so successful in parsing English are necessar-
ily appropriate for parsing MRLs ? but associated
with this question are important questions concern-
ing the annotation scheme of the related treebanks.
Obviously, when annotating structures for languages
with characteristics different than English one has to
face different annotation decisions, and it comes as
no surprise that the annotated structures for MRLs
often differ from those employed in the PTB.
1The shared tasks involved 18 languages, including many
MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish.
3
For Spanish and French, it was shown by Cowan
and Collins (2005) and in (Arun and Keller, 2005;
Schluter and van Genabith, 2007), that restructuring
the treebanks? native annotation scheme to match
the PTB annotation style led to a significant gain in
parsing performance of Head-Driven models of the
kind proposed in (Collins, 1997). For German, a
language with four different treebanks and two sub-
stantially different annotation schemes, it has been
shown that a PCFG parser is sensitive to the kind of
representation employed in the treebank.
Dubey and Keller (2003), for example, showed
that a simple PCFG parser outperformed an emula-
tion of Collins? model 1 on NEGRA. They showed
that using sister-head dependencies instead of head-
head dependencies improved parsing performance,
and hypothesized that it is due to the flatness of
phrasal annotation. Ku?bler et al (2006) showed con-
siderably lower PARSEVAL scores on NEGRA (Skut
et al, 1998) relative to the more hierarchically struc-
tured Tu?Ba-D/Z (Hinrichs et al, 2005), again, hy-
pothesizing that this is due to annotation differences.
Related to such comparisons is the question of the
relevance of the PARSEVAL metrics for evaluating
parsing results across languages and treebanks. Re-
hbein and van Genabith (2007) showed that PARS-
EVAL measures are sensitive to annotation scheme
particularities (e.g. the internal node ratio). It was
further shown that different metrics (i.e. the Leaf-
ancestor path (Sampson and Babarczy, 2003) and
dependency based ones in (Lin, 1995)) can lead to
different performance ranking. This was confirmed
also for French by Seddah et al (2009).
The questions of how to annotate treebanks for
MRLs and how to evaluate the performance of the
different parsers on these different treebanks is cru-
cial. For the MRL parsing community to be able to
assess the difficulty of improving parsing results for
French, German, Arabic, Korean, Basque, Hindi or
Hebrew, we ought to first address fundamental ques-
tions including: Is the treebank sufficiently large
to allow for proper grammar induction? Does the
annotation scheme fit the language characteristics?
Does the use of PTB annotation variants for other
languages influence parsing results? Does the space-
delimited tokenization allow for phrase boundary
detection? Do the results for a specific approach
generalize to more than one language?
3 Primary Research Questions
It is firmly established in theoretical linguistics that
morphology and syntax closely interact through pat-
terns of case marking, agreement, clitics and various
types of compounds. Because of such close interac-
tions, we expect morphological cues to help parsing
performance. But in practice, when trying to incor-
porate morphological information into parsing mod-
els, three types of challenges present themselves:
Architecture and Setup: When attempting to
parse complex word-forms that encapsulate both
lexical and functional information, important archi-
tectural questions emerge, namely, what is the na-
ture of the input that is given to the parsing system?
Does the system attempt to parse sequences of words
or does it aim to assign structures to sequences of
morphological segments? If the former is the case,
how can we represent words abstractly so as to re-
flect shared morphological aspects between them?
If the latter is the case, how can we arrive at a good
enough morphological segmentation for the purpose
of statistical parsing, given raw input texts?
When working with morphologically rich lan-
guages such as Hebrew or Arabic, affixes may have
syntactically independent functions. Many parsing
models assume segmentation of the syntactically in-
dependent parts, such as prepositions or pronominal
clitics, prior to parsing. But morphological segmen-
tation requires disambiguation which is non-trivial,
due to case syncretism and high morphological am-
biguity exhibited by rich inflectional systems. The
question is then when should we disambiguate the
morphological analyses of input forms? Should we
do that prior to parsing or perhaps jointly with it?2
Representation and Modeling: Assuming that
the input to our system reflects morphological infor-
mation, one way or another, which types of morpho-
2Most studies on parsing MRLs nowadays assume the gold
standard segmentation and disambiguated morphological infor-
mation as input. This is the case, for instance, for the Arabic
parsing at CoNLL 2007 (Nivre et al, 2007a). This practice de-
ludes the community as to the validity of the parsing results
reported for MRLs in shared tasks. Goldberg et al (2009), for
instance, show a gap of up to 6pt F1-score between performance
on gold standard segmentation vs. raw text. One way to over-
come this is to devise joint morphological and syntactic disam-
biguation frameworks (cf. (Goldberg and Tsarfaty, 2008)).
4
logical information should we include in the parsing
model? Inflectional and/or derivational? Case infor-
mation and/or agreement features? How can valency
requirements reflected in derivational morphology
affect the overall syntactic structure? In tandem with
the decision concerning the morphological informa-
tion to include, we face genuine challenges concern-
ing how to represent such information in the syntac-
tic model, be it constituency-based or dependency-
based. Should we encode morphological informa-
tion at the level of PoS tags and/or on top of syn-
tactic elements? Should we decorate non-terminals
nodes and/or dependency arcs or both?
Incorporating morphology in the statistical model
is often even more challenging than the sum of
these bare decisions, because of the nonconfigu-
rational structures (free word order, discontinuous
constituents) for rich markings are crucial (Hale,
1983). The parsing models designed for English of-
ten focus on learning rigid word order, and they do
not take morphological information into account (cf.
developing parsers for German (Dubey and Keller,
2003; Ku?bler et al, 2006)). The more complex ques-
tion is therefore: what type of parsing model should
we use for parsing MRLs? shall we use a general
purpose implementation and attempt to amend it?
how? or perhaps we should devise a new model from
first principles, to address nonconfigurational phe-
nomena effectively? using what form of representa-
tion? is it possible to find a single model that can
effectively cope with different kinds of languages?
Estimation and Smoothing: Compared to En-
glish, MRLs tend to have a greater number of word
forms and higher out-of-vocabulary (OOV) rates,
due to the many feature combinations licensed by
the inflectional system. A typical problem associ-
ated with parsing MRLs is substantial lexical data
sparseness due to high morphological variation in
surface forms. The question is therefore, given our
finite, and often fairly small, annotated sets of data,
how can we guess the morphological analyses, in-
cluding the PoS tag assignment and various features,
of an OOV word? How can we learn the probabil-
ities of such assignments? In a more general setup,
this problem is akin to handling out-of-vocabulary
or rare words for robust statistical parsing, and tech-
niques for domain adaptation via lexicon enhance-
Constituency-Based Dependency-Based
Arabic (Attia et al, 2010) (Marton et al, 2010)?
Basque - (Bengoetxea and Gojenola, 2010)
English (Attia et al, 2010) -
French (Attia et al, 2010)
(Seddah et al, 2010)
(Candito and Seddah, 2010)? -
German (Maier, 2010) -
Hebrew (Tsarfaty and Sima?an, 2010) (Goldberg and Elhadad, 2010)?
Hindi - (Ambati et al, 2010a)?
(Ambati et al, 2010b)
Korean (Chung et al, 2010) -
Table 1: An overview of SPMRL contributions. (? report
results also for non-gold standard input)
ment (also explored for English and other morpho-
logically impoverished languages).
So, in fact, incorporating morphological informa-
tion inside the syntactic model for the purpose of
statistical parsing is anything but trivial. In the next
section we review the various approaches taken in
the individual contributions of the SPMRL work-
shop for addressing such challenges.
4 Parsing MRLs: Recurring Trends
The first workshop on parsing MRLs features 11
contributions for a variety of languages with a
range of different parsing frameworks. Table 1 lists
the individual contributions within a cross-language
cross-framework grid. In this section, we focus on
trends that occur among the different contributions.
This may be a biased view since some of the prob-
lems that exist for parsing MRLs may have not been
at all present, but it is a synopsis of where we stand
with respect to problems that are being addressed.
4.1 Architecture and Setup: Gold vs. Predicted
Morphological Information
While morphological information can be very infor-
mative for syntactic analysis, morphological anal-
ysis of surface forms is ambiguous in many ways.
In German, for instance, case syncretism (i.e. a sin-
gle surface form corresponding to different cases) is
pervasive, and in Hebrew and Arabic, the lack of vo-
calization patterns in written texts leads to multiple
morphological analyses for each space-delimited to-
ken. In real world situations, gold morphological in-
formation is not available prior to parsing. Can pars-
ing systems make effective use of morphology even
when gold morphological information is absent?
5
Several papers address this challenge by present-
ing results for both the gold and the automatically
predicted PoS and morphological information (Am-
bati et al, 2010a; Marton et al, 2010; Goldberg and
Elhadad, 2010; Seddah et al, 2010). Not very sur-
prisingly, all evaluated systems show a drop in pars-
ing accuracy in the non-gold settings.
An interesting trend is that in many cases, us-
ing noisy morphological information is worse than
not using any at all. For Arabic Dependency pars-
ing, using predicted CASE causes a substantial drop
in accuracy while it greatly improves performance
in the gold setting (Marton et al, 2010). For
Hindi Dependency Parsing, using chunk-internal
cues (i.e. marking non-recursive phrases) is benefi-
cial when gold chunk-boundaries are available, but
suboptimal when they are automatically predicted
(Ambati et al, 2010a). For Hebrew Dependency
Parsing with the MST parser, using gold morpholog-
ical features shows no benefit over not using them,
while using automatically predicted morphological
features causes a big drop in accuracy compared to
not using them (Goldberg and Elhadad, 2010). For
French Constituency Parsing, Seddah et al (2010)
and Candito and Seddah (2010) show that while
gold information for the part-of-speech and lemma
of each word form results in a significant improve-
ment, the gain is low when switching to predicted
information. Reassuringly, Ambati et al (2010a),
Marton et al (2010), and Goldberg and Elhadad
(2010) demonstrate that some morphological infor-
mation can indeed be beneficial for parsing even in
the automatic setting. Ensuring that this is indeed
so, appears to be in turn linked to the question of
how morphology is represented and incorporated in
the parsing model.
The same effect in a different guise appears in
the contribution of Chung et al (2010) concerning
parsing Korean. Chung et al (2010) show a sig-
nificant improvement in parsing accuracy when in-
cluding traces of null anaphors (a.k.a. pro-drop) in
the input to the parser. Just like overt morphology,
traces and null elements encapsulate functional in-
formation about relational entities in the sentence
(the subject, the object, etc.), and including them at
the input level provides helpful disambiguating cues
for the overall structure that represents such rela-
tions. However, assuming that such traces are given
prior to parsing is, for all practical purposes, infeasi-
ble. This leads to an interesting question: will iden-
tifying such functional elements (marked as traces,
overt morphology, etc) during parsing, while com-
plicating that task itself, be on the whole justified?
Closely linked to the inclusion of morphological
information in the input is the choice of PoS tag set
to use. The generally accepted view is that fine-
grained PoS tags are morphologically more informa-
tive but may be harder to statistically learn and parse
with, in particular in the non-gold scenario. Mar-
ton et al (2010) demonstrate that a fine-grained tag
set provides the best results for Arabic dependency
parsing when gold tags are known, while a much
smaller tag set is preferred in the automatic setting.
4.2 Representation and Modeling:
Incorporating Morphological Information
Many of the studies presented here explore the use
of feature representation of morphological informa-
tion for the purpose of syntactic parsing (Ambati et
al., 2010a; Ambati et al, 2010b; Bengoetxea and
Gojenola, 2010; Goldberg and Elhadad, 2010; Mar-
ton et al, 2010; Tsarfaty and Sima?an, 2010). Clear
trends among the contributions emerge concerning
the kind of morphological information that helps sta-
tistical parsing. Morphological CASE is shown to be
beneficial across the board. It is shown to help for
parsing Basque, Hebrew, Hindi and to some extent
Arabic.3 Morphological DEFINITENESS and STATE
are beneficial for Hebrew and Arabic when explic-
itly represented in the model. STATE, ASPECT and
MOOD are beneficial for Hindi, but only marginally
beneficial for Arabic. CASE and SUBORDINATION-
TYPE are the most beneficial features for Basque
transition-based dependency parsing.
A closer view into the results mentioned in the
previous paragraph suggests that, beyond the kind
of information that is being used, the way in which
morphological information is represented and used
by the model has substantial ramification as to
whether or not it leads to performance improve-
ments. The so-called ?agreement features? GEN-
DER, NUMBER, PERSON, provide for an interesting
case study in this respect. When included directly as
3For Arabic, CASE is useful when gold morphology infor-
mation is available, but substantially hurt results when it is not.
6
machine learning features, agreement features ben-
efit dependency parsing for Arabic (Marton et al,
2010), but not Hindi (dependency) (Ambati et al,
2010a; Ambati et al, 2010b) or Hebrew (Goldberg
and Elhadad, 2010). When represented as simple
splits of non-terminal symbols, agreement informa-
tion does not help constituency-based parsing per-
formance for Hebrew (Tsarfaty and Sima?an, 2010).
However, when agreement patterns are directly rep-
resented on dependency arcs, they contribute an im-
provement for Hebrew dependency parsing (Gold-
berg and Elhadad, 2010). When agreement is en-
coded at the realization level inside a Relational-
Realizational model (Tsarfaty and Sima?an, 2008),
agreement features improve the state-of-the-art for
Hebrew parsing (Tsarfaty and Sima?an, 2010).
One of the advantages of the latter study is that
morphological information which is expressed at the
level of words gets interpreted elsewhere, on func-
tional elements higher up the constituency tree. In
dependency parsing, similar cases may arise, that
is, morphological information might not be as use-
ful on the form on which it is expressed, but would
be more useful at a different position where it could
influence the correct attachment of the main verb
to other elements. Interesting patterns of that sort
occur in Basque, where the SUBORDINATIONTYPE
morpheme attaches to the auxiliary verb, though it
mainly influences attachments to the main verb.
Bengoetxea and Gojenola (2010) attempted two
different ways to address this, one using a trans-
formation segmenting the relevant morpheme and
attaching it to the main verb instead, and another
by propagating the morpheme along arcs, through
a ?stacking? process, to where it is relevant. Both
ways led to performance improvements. The idea of
a segmentation transformation imposes non-trivial
pre-processing, but it may be that automatically
learning the propagation of morphological features
is a promising direction for future investigation.
Another, albeit indirect, way to include morpho-
logical information in the parsing model is using
so-called latent information or some mechanism
of clustering. The general idea is the following:
when morphological information is added to stan-
dard terminal or non-terminal symbols, it imposes
restrictions on the distribution of these no-longer-
equivalent elements. Learning latent informa-
tion does not represent morphological information
directly, but presumably, the distributional restric-
tions can be automatically learned along with the
splits of labels symbols in models such as (Petrov
et al, 2006). For Korean (Chung et al, 2010),
latent information contributes significant improve-
ments. One can further do the opposite, namely,
merging terminals symbols for the purpose of ob-
taining an abstraction over morphological features.
When such clustering uses a morphological signa-
ture of some sort, it is shown to significantly im-
prove constituency-based parsing for French (Can-
dito and Seddah, 2010).
4.3 Representation and Modeling: Free Word
Order and Flexible Constituency Structure
Off-the-shelf parsing tools are found in abundance
for English. One problematic aspect of using them
to parse MRLs lies in the fact that these tools fo-
cus on the statistical modeling of configurational
information. These models often condition on the
position of words relative to one another (e.g. in
transition-based dependency parsing) or on the dis-
tance between words inside constituents (e.g. in
Head-Driven parsing). Many of the contributions to
the workshop show that working around existing im-
plementations may be insufficient, and we may have
to come up with more radical solutions.
Several studies present results that support the
conjecture that when free word-order is explicitly
taken into account, morphological information is
more likely to contribute to parsing accuracy. The
Relational-Realizational model used in (Tsarfaty
and Sima?an, 2010) allows for reordering of con-
stituents at a configuration layer, which is indepen-
dent of the realization patterns learned from the data
(vis-a`-vis case marking and agreement). The easy-
first algorithm of (Goldberg and Elhadad, 2010)
which allows for significant flexibility in the order of
attachment, allows the model to benefit from agree-
ment patterns over dependency arcs that are easier
to detect and attach first. The use of larger subtrees
in (Chung et al, 2010) for parsing Korean, within a
Bayesian framework, allows the model to learn dis-
tributions that take more elements into account, and
thus learn the different distributions associated with
morphologically marked elements in constituency
structures, to improve performance.
7
In addition to free word order, MRLs show higher
degree of freedom in extraposition. Both of these
phenomena can result in discontinuous structures.
In constituency-based treebanks, this is either an-
notated as additional information which has to be
recovered somehow (traces in the case of the PTB,
complex edge labels in the German Tu?Ba-D/Z), or
as discontinuous phrase structures, which cannot be
handled with current PCFG models. Maier (2010)
suggests the use of Linear Context-Free Rewriting
Systems (LCFRSs) in order to make discontinuous
structure transparent to the parsing process and yet
preserve familiar notions from constituency.
Dependency representation uses non-projective
dependencies to reflect discontinuities, which is
problematic to parse with models that assume pro-
jectivity. Different ways have been proposed to deal
with non-projectivity (Nivre and Nilsson, 2005; Mc-
Donald et al, 2005; McDonald and Pereira, 2006;
Nivre, 2009). Bengoetxea and Gojenola (2010)
discuss non-projective dependencies in Basque and
show that the pseudo-projective transformation of
(Nivre and Nilsson, 2005) improves accuracy for de-
pendency parsing of Basque. Moreover, they show
that in combination with other transformations, it
improves the utility of these other ones, too.
4.4 Estimation and Smoothing: Coping with
Lexical Sparsity
Morphological word form variation augments the
vocabulary size and thus worsens the problem of lex-
ical data sparseness. Words occurring with medium-
frequency receive less reliable estimates, and the
number of rare/unknown words is increased. One
way to cope with the one of both aspects of this
problem is through clustering, that is, providing an
abstract representation over word forms that reflects
their shared morphological and morphosyntactic as-
pects. This was done, for instance, in previous work
on parsing German. Versley and Rehbein (2009)
cluster words according to linear context features.
These clusters include valency information added to
verbs and morphological features such as case and
number added to pre-terminal nodes. The clusters
are then integrated as features in a discriminative
parsing model to cope with unknown words. Their
discriminative model thus obtains state-of-the-art re-
sults on parsing German.
Several contribution address similar challenges.
For constituency-based generative parsers, the sim-
ple technique of replacing word forms with more
abstract symbols is investigated by (Seddah et al,
2010; Candito and Seddah, 2010). For French, re-
placing each word form by its predicted part-of-
speech and lemma pair results in a slight perfor-
mance improvement (Seddah et al, 2010). When
words are clustered, even according to a very local
linear-context similarity measure, measured over a
large raw corpus, and when word clusters are used in
place of word forms, the gain in performance is even
higher (Candito and Seddah, 2010). In both cases,
the technique provides more reliable estimates for
in-vocabulary words, since a given lemma or cluster
appear more frequently. It also increases the known
vocabulary. For instance, if a plural form is un-
seen in the training set but the corresponding singu-
lar form is known, then in a setting of using lemmas
in terminal symbols, both forms are known.
For dependency parsing, Marton et al (2010) in-
vestigates the use of morphological features that in-
volve some semantic abstraction over Arabic forms.
The use of undiacritized lemmas is shown to im-
prove performance. Attia et al (2010) specifically
address the handling of unknown words in the latent-
variable parsing model. Here again, the technique
that is investigated is to project unknown words to
more general symbols using morphological clues. A
study on three languages, English, French and Ara-
bic, shows that this method helps in all cases, but
that the greatest improvement is obtained for Arabic,
which has the richest morphology among three.
5 Where we?re at
It is clear from the present overview that we are
yet to obtain a complete understanding concerning
which models effectively parse MRLs, how to an-
notate treebanks for MRLs and, importantly, how
to evaluate parsing performance across types of lan-
guages and treebanks. These foundational issues are
crucial for deriving more conclusive recommenda-
tions as to the kind of models and morphological
features that can lead to advancing the state-of-the-
art for parsing MRLs. One way to target such an
understanding would be to encourage the investiga-
tion of particular tasks, individually or in the context
8
of shared tasks, that are tailored to treat those prob-
lematic aspects of MRLs that we surveyed here.
So far, constituency-based parsers have been as-
sessed based on their performance on the PTB (and
to some extent, across German treebanks (Ku?bler,
2008)) whereas comparison across languages was
rendered opaque due to data set differences and
representation idiosyncrasies. It would be interest-
ing to investigate such a cross-linguistic compari-
son of parsers in the context of a shared task on
constituency-based statistical parsing, in additional
to dependency-based ones as reported in (Nivre et
al., 2007a). Standardizing data sets for a large
number of languages with different characteristics,
would require us, as a community, to aim for
constituency-representation guidelines that can rep-
resent the shared aspects of structures in different
languages, while at the same time allowing differ-
ences between them to be reflected in the model.
Furthermore, it would be a good idea to intro-
duce parsing tasks, for either constituent-based or
dependency-based setups, which consider raw text
as input, rather than morphologically segmented
and analyzed text. Addressing the parsing prob-
lem while facing the morphological disambiguation
challenge in its full-blown complexity would be il-
luminating and educating for at least two reasons:
firstly, it would give us a better idea of what is the
state-of-the-art for parsing MRLs in realistic scenar-
ios. Secondly, it might lead to profound insights
about the potentially successful ways to use mor-
phology inside a parser, which may differ from the
insights concerning the use of morphology in the
less realistic parsing scenarios, where gold morpho-
logical information is given.
Finally, to be able to perceive where we stand
with respect to parsing MRLs and how models fare
against one another across languages, it would be
crucial to arrive at evaluation metrics that capture
information that is shared among the different repre-
sentations, for instance, functional information con-
cerning predicate-argument relations. Using the dif-
ferent kinds of measures in the context of cross-
framework tasks will help us understand the util-
ity of the different evaluation metrics that have been
proposed and to arrive at a clearer picture of what it
is that we wish to compare, and how we can faith-
fully do so across models, languages and treebanks.
6 Conclusion
This paper presents the synthesis of 11 contributions
to the first workshop on statistical parsing for mor-
phologically rich languages. We have shown that
architectural, representational, and estimation issues
associated with parsing MRLs are found to be chal-
lenging across languages and parsing frameworks.
The use of morphological information in the non
gold-tagged input scenario is found to cause sub-
stantial differences in parsing performance, and in
the kind of morphological features that lead to per-
formance improvements.
Whether or not morphological features help pars-
ing also depends on the kind of model in which
they are embedded, and the different ways they are
treated within. Furthermore, sound statistical esti-
mation methods for morphologically rich, complex
lexica, turn out to be crucial for obtaining good pars-
ing accuracy when using general-purpose models
and algorithms. In the future we hope to gain better
understanding of the common pitfalls in, and novel
solutions for, parsing morphologically ambiguous
input, and to arrive at principled guidelines for se-
lecting the model and features to include when pars-
ing different kinds of languages. Such insights may
be gained, among other things, in the context of
more morphologically-aware shared parsing tasks.
Acknowledgements
The program committee would like to thank
NAACL for hosting the workshop and SIGPARSE
for their sponsorship. We further thank INRIA Al-
page team for their generous sponsorship. We are
finally grateful to our reviewers and authors for their
dedicated work and individual contributions.
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010a. Two
methods to incorporate local morphosyntactic features
in Hindi dependency parsing. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010b. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
9
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
306?313, Ann Arbor, MI.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Applica-
tion of different techniques to dependency parsing of
Basque. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second International Conference on
Human Language Technology Research, pages 178?
182. Morgan Kaufmann Publishers Inc. San Francisco,
CA, USA.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306?
311, San Mateo (CA). Morgan Kaufman.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 19?26, Budapest, Hungary.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well, Oxford.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Language Learning (CoNLL), pages 149?164,
New York, NY.
Marie Candito and Djame? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 9?16, Manchester,
UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Barcelona, Spain, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Annual Meeting of the
North American Chapter of the ACL (NAACL), Seattle.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar. In
Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 456?463,
Hong Kong. Association for Computational Linguis-
tics Morristown, NJ, USA.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meeting
of the ACL, volume 37, pages 505?512, College Park,
MD.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain.
Anna Corazza, Alberto Lavelli, Giogio Satta, and
Roberto Zanoli. 2004. Analyzing an Italian treebank
with state-of-the-art statistical parsers. In Proceedings
of the Third Third Workshop on Treebanks and Lin-
guistic Theories (TLT 2004), Tu?bingen, Germany.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
in Proceedins of EMNLP.
Benoit Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de la 15e`me Confe?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103,
Ann Arbor, MI.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
10
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Yoav Goldberg and Michael Elhadad. 2010. Easy-
first dependency parsing of Modern Hebrew. In Pro-
ceedings of the NAACL/HLT Workshop on Statistical
Parsing of Morphologically Rich Languages (SPMRL
2010), Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of the 46nd Annual Meet-
ing of the Association for Computational Linguistics.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 327?335.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Erhard W. Hinrichs, Sandra Ku?bler, and Karin Naumann.
2005. A unified representation for morphological,
syntactic, semantic, and referential annotations. In
Proceedings of the ACL Workshop on Frontiers in Cor-
pus Annotation II: Pie in the Sky, pages 13?20, Ann
Arbor, MI.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 111?
119, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63. Association for Com-
putational Linguistics.
Seth Kulick, Ryan Gabbard, and Mitchell Marcus. 2006.
Parsing the Arabic treebank: Analysis and improve-
ments. In Proceedings of TLT.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference on
Arabic Language Resources and Tools.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the Arabic tree-
bank. In Proceedings of INFOS.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 276?283, Cambridge, MA.
Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic dependency parsing with lexical and
inflectional morphological features. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Ryan T. McDonald and Fernando C. N. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proc. of EACL?06.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proc. of ACL?05, Ann Arbor, USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Marianne Mithun. 1992. Is basic word order universal?
In Doris L. Payne, editor, Pragmatics of Word Order
Flexibility. John Benjamins, Amsterdam.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Ann Arbor, MI.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007b. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
11
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007c. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djame? Seddah, Marie Candito, and Benoit Crabbe?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper texts. In ESSLLI
Workshop on Recent Advances in Corpus Annotation,
Saarbru?cken, Germany.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morphologi-
cally rich languages. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies (IWPT),
pages 156?167.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 889?896.
Reut Tsarfaty and Khalil Sima?an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
12
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 67?75,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Handling Unknown Words in Statistical Latent-Variable Parsing Models for
Arabic, English and French
Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi,
Josef van Genabith?
National Centre for Language Technology
School of Computing, Dublin City University
{mattia,jfoster,dhogan,jleroux,ltounsi,josef}@computing.dcu.ie
Abstract
This paper presents a study of the impact
of using simple and complex morphological
clues to improve the classification of rare and
unknown words for parsing. We compare
this approach to a language-independent tech-
nique often used in parsers which is based
solely on word frequencies. This study is ap-
plied to three languages that exhibit different
levels of morphological expressiveness: Ara-
bic, French and English. We integrate infor-
mation about Arabic affixes and morphotac-
tics into a PCFG-LA parser and obtain state-
of-the-art accuracy. We also show that these
morphological clues can be learnt automati-
cally from an annotated corpus.
1 Introduction
For a parser to do a reasonable job of analysing free
text, it must have a strategy for assigning part-of-
speech tags to words which are not in its lexicon.
This problem, also known as the problem of un-
known words, has received relatively little attention
in the vast literature on Wall-Street-Journal (WSJ)
statistical parsing. This is likely due to the fact that
the proportion of unknown words in the standard
English test set, Section 23 of the WSJ section of
Penn Treebank, is quite small. The problem mani-
fests itself when the text to be analysed comes from
a different domain to the text upon which the parser
has been trained, when the treebank upon which the
parser has been trained is limited in size and when
?Author names are listed in alphabetical order. For further
correspondence, contact L. Tounsi, D. Hogan or J. Foster.
the language to be parsed is heavily inflected. We
concentrate on the latter case, and examine the prob-
lem of unknown words for two languages which lie
on opposite ends of the spectrum of morphologi-
cal expressiveness and for one language which lies
somewhere in between: Arabic, English and French.
In our experiments we use a Berkeley-style latent-
variable PCFG parser and we contrast two tech-
niques for handling unknown words within the gen-
erative parsing model: one in which no language-
specific information is employed and one in which
morphological clues (or signatures) are exploited.
We find that the improvement accrued from look-
ing at a word?s morphology is greater for Arabic
and French than for English. The morphological
clues we use for English are taken directly from the
Berkeley parser (Petrov et al, 2006) and those for
French from recent work on French statistical pars-
ing with the Berkeley parser (Crabbe? and Candito,
2008; Candito et al, 2009). For Arabic, we present
our own set of heuristics to extract these signatures
and demonstrate a statistically significant improve-
ment of 3.25% over the baseline model which does
not employ morphological information.
We next try to establish to what extent these clues
can be learnt automatically by extracting affixes
from the words in the training data and ranking these
using information gain. We show that this automatic
method performs quite well for all three languages.
The paper is organised as follows: In Section 2
we describe latent variable PCFG parsing models.
This is followed in Section 3 by a description of our
three datasets, including statistics on the extent of
the unknown word problem in each. In Section 4, we
67
present results on applying a version of the parser
which uses a simple, language-agnostic, unknown-
word handling technique to our three languages. In
Section 5, we show how this technique is extended
to include morphological information and present
parsing results for English and French. In Section 6,
we describe the Arabic morphological system and
explain how we used heuristic rules to cluster words
into word-classes or signatures. We present parsing
results for the version of the parser which uses this
information. In Section 7, we describe our attempts
to automatically determine the signatures for a lan-
guage and present parsing results for the three lan-
guages. Finally, in Section 8, we discuss how this
work might be fruitfully extended.
2 Latent Variable PCFG Parsing
Johnson (1998) showed that refining treebank cate-
gories with parent information leads to more accu-
rate grammars. This was followed by a collection of
linguistically motivated propositions for manual or
semi-automatic modifications of categories in tree-
banks (Klein and Manning, 2003). In PCFG-LAs,
first introduced by Matsuzaki et al (2005), the re-
fined categories are learnt from the treebank us-
ing unsupervised techniques. Each base category
? and this includes part-of-speech tags ? is aug-
mented with an annotation that refines its distribu-
tional properties.
Following Petrov et al (2006) latent annotations
and probabilities for the associated rules are learnt
incrementally following an iterative process consist-
ing of the repetition of three steps.
1. Split each annotation of each symbol into n
(usually 2) new annotations and create rules
with the new annotated symbols. Estimate1 the
probabilities of the newly created rules.
2. Evaluate the impact of the newly created anno-
tations and discard the least useful ones. Re-
estimate probabilities with the new set of anno-
tations.
3. Smooth the probabilities to prevent overfitting.
We use our own parser which trains a PCFG-LA us-
ing the above procedure and parses using the max-
1Estimation of the parameters is performed by running Ex-
pectation/Maximisation on the training corpus.
rule parsing algorithm (Petrov et al, 2006; Petrov
and Klein, 2007). PCFG-LA parsing is relatively
language-independent but has been shown to be very
effective on several languages (Petrov, 2009). For
our experiments, we set the number of iterations to
be 5 and we test on sentences less than or equal to
40 words in length. All our experiments, apart from
the final one, are carried out on the development sets
of our three languages.
3 The Datasets
Arabic We use the the Penn Arabic Treebank
(ATB) (Bies and Maamouri, 2003; Maamouri and
Bies., 2004). The ATB describes written Modern
Standard Arabic newswire and follows the style and
guidelines of the English Penn-II treebank. We use
the part-of-speech tagset defined by Bikel and Bies
(Bikel, 2004). We employ the usual treebank split
(80% training, 10% development and 10% test).
English We use the Wall Street Journal section of
the Penn-II Treebank (Marcus et al, 1994). We train
our parser on sections 2-21 and use section 22 con-
catenated with section 24 as our development set.
Final testing is carried out on Section 23.
French We use the French Treebank (Abeille? et
al., 2003) and divide it into 80% for training, 10%
for development and 10% for final results. We fol-
low the methodology defined by Crabbe? and Can-
dito (2008): compound words are merged and the
tagset consists of base categories augmented with
morphological information in some cases2.
Table 1 gives basic unknown word statistics for
our three datasets. We calculate the proportion of
words in our development sets which are unknown
or rare (specified by the cutoff value) in the corre-
sponding training set. To control for training set
size, we also provide statistics when the English
training set is reduced to the size of the Arabic and
French training sets and when the Arabic training set
is reduced to the size of the French training set. In an
ideal world where training set sizes are the same for
all languages, the problem of unknown words will
be greatest for Arabic and smallest for English. It is
2This is called the CC tagset: base categories with verbal
moods and extraction features
68
language cutoff #train #dev #unk %unk language #train #dev #unk %unk
Arabic 0 594,683 70,188 3794 5.40 Reduced English 597,999 72,970 2627 3.60
- 1 - - 6023 8.58 (Arabic Size) - - 3849 5.27
- 5 - - 11,347 16.17 - - - 6700 9.18
- 10 - - 15,035 21.42 - - - 9083 12.45
English 0 950,028 72,970 2062 2.83 Reduced Arabic 266,132 70,188 7027 10.01
- 1 - - 2983 4.09 (French Size) - - 10,208 14.54
- 5 - - 5306 7.27 - - - 16,977 24.19
- 10 - - 7230 9.91 - - - 21,434 30.54
French 0 268,842 35,374 2116 5.98 Reduced English 265,464 72,970 4188 5.74
- 1 - - 3136 8.89 (French Size) - - 5894 8.08
- 5 - - 5697 16.11 - - - 10,105 13.85
- 10 - - 7584 21.44 - - - 13,053 17.89
Table 1: Basic Unknown Word Statistics for Arabic, French and English
reasonable to assume that the levels of inflectional
richness have a role to play in these differences.
4 A Simple Lexical Probability Model
The simplest method for handling unknown words
within a generative probabilistic parsing/tagging
model is to reserve a proportion of the lexical rule
probability mass for such cases. This is done by
mapping rare words in the training data to a spe-
cial UNKNOWN terminal symbol and estimating rule
probabilities in the usual way. We illustrate the pro-
cess with the toy unannotated PCFG in Figures 1
and 2. The lexical rules in Fig. 1 are the original
rules and the ones in Fig. 2 are the result of apply-
ing the rare-word-to-unknown-symbol transforma-
tion. Given the input sentence The shares recovered,
the word recovered is mapped to the UNKNOWN to-
ken and the three edges corresponding to the rules
NNS ? UNKNOWN, V BD ? UNKNOWN and
JJ ? UNKNOWN are added to the chart at this posi-
tion. The disadvantage of this simple approach is ob-
vious: all unknown words are treated equally and the
tag whose probability distribution is most dominated
by rare words in the training will be deemed the
most likely (JJ for this example), regardless of the
characteristics of the individual word. Apart from
its ease of implementation, its main advantage is its
language-independence - it can be used off-the-shelf
for any language for which a PCFG is available.3
One parameter along which the simple lexical
3Our simple lexical model is equivalent to the Berkeley sim-
pleLexicon option.
probability model can vary is the threshold used to
decide whether a word in the training data is rare or
?unknown?. When the threshold is set to n, a word
in the training data is considered to be unknown if it
occurs n or fewer times. We experiment with three
thresholds: 1, 5 and 10. The result of this experi-
ment for our three languages is shown in Table 2.
The general trend we see in Table 2 is that the
number of training set words considered to be un-
known should be minimized. For all three lan-
guages, the worst performing grammar is the one
obtained when the threshold is increased to 10. This
result is not unexpected. With this simple lexical
probability model, there is a trade-off between ob-
taining good guesses for words which do not occur
in the training data and obtaining reliable statistics
for words which do. The greater the proportion of
the probability mass that we reserve for the unknown
word section of the grammar, the more performance
suffers on the known yet rare words since these are
the words which are mapped to the UNKNOWN sym-
bol. For example, assume the word restructuring oc-
curs 10 times in the training data, always tagged as
a VBG. If the unknown threshold is less than ten and
if the word occurs in the sentence to be parsed, a
VBG edge will be added to the chart at this word?s
position with the probability 10/#VBG. If, however,
the threshold is set to 10, the word (in the training set
and the input sentence) will be mapped to UNKNOWN
and more possibilities will be explored (an edge for
each TAG ? UNKNOWN rule in the grammar). We
can see from Table 1 that at threshold 10, one fifth
69
VBD -> fell 50/153
VBD -> reoriented 2/153
VBD -> went 100/153
VBD -> latched 1/153
NNS -> photofinishers 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> centrist 4/24
DT -> the 170/170
Figure 1: The original toy PCFG
VBD -> fell 50/153
VBD -> UNKNOWN 3/153
VBD -> went 100/153
NNS -> UNKNOWN 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNKNOWN 4/24
DT -> the 170/170
Figure 2: Rare ? UNKNOWN
VBD -> fell 50/153
VBD -> UNK-ed 3/153
VBD -> went 100/153
NNS -> UNK-s 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNK-ist 4/24
DT -> the 170/170
Figure 3: Rare ? UN-
KNOWN+SIGNATURE
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 78.60 80.49 79.53 94.03
5 77.17 79.81 78.47 91.16
10 75.32 78.69 76.97 89.06
English
1 89.20 89.73 89.47 95.60
5 88.91 89.74 89.33 94.66
10 88.00 88.97 88.48 93.61
French
1 83.60 84.17 83.88 94.90
5 82.31 83.10 82.70 92.99
10 80.87 82.05 81.45 91.56
Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model
of the words in the Arabic and French development
sets are unknown, and this is reflected in the drop in
parsing performance at these thresholds.
5 Making use of Morphology
Unknown words are not all the same. We exploit this
fact by examining the effect on parsing accuracy of
clustering rare training set words using cues from
the word?s morphological structure. Affixes have
been shown to be useful in part-of-speech tagging
(Schmid, 1994; Tseng et al, 2005) and have been
used in the Charniak (Charniak, 2000), Stanford
(Klein and Manning, 2003) and Berkeley (Petrov et
al., 2006) parsers. In this section, we contrast the
effect on parsing accuracy of making use of such in-
formation for our three languages of interest.
Returning to our toy English example in Figures 1
and 2, and given the input sentence The shares re-
covered, we would like to use the fact that the un-
known word recovered ends with the past tense
suffix -ed to boost the probability of the lexical
rule V BD ? UNKNOWN. If we specialise the
UNKNOWN terminal using information from English
morphology, we can do just that, resulting in the
grammar in Figure 3. Now the word recovered is
mapped to the symbol UNK-ed and the only edge
which is added to the chart at this position is the one
corresponding to the rule V BD ? UNK-ed.
For our English experiments we use the unknown
word classes (or signatures) which are used in the
Berkeley parser. A signature indicates whether a
words contains a digit or a hyphen, if a word starts
with a capital letter or ends with one of the following
English suffixes (both derivational and inflectional):
-s, -ed, -ing, -ion, -er, -est, -ly, -ity, -y and -al.
For our French experiments we employ the same
signature list as Crabbe? and Candito (2008), which
itself was adapted from Arun and Keller (2005).
This list consists of (a) conjugation suffixes of regu-
70
lar verbs for common tenses (eg. -ons, -ez, -ent. . . )
and (b) derivational suffixes for nouns, adverbs and
adjectives (eg. -tion, -ment, -able. . . ).
The result of employing signature information
for French and English is shown in Table 3. Be-
side each f-score the absolute improvement over the
UNKNOWN baseline (Table 2) is given. For both
languages there is an improvement at all unknown
thresholds. The improvement for English is statis-
tically significant at unknown thresholds 1 and 10.4
The improvement is more marked for French and is
statistically significant at all levels.
In the next section, we experiment with signature
lists for Arabic.5
6 Arabic Signatures
In order to use morphological clues for Arabic we
go further than just looking at suffixes. We exploit
all the richness of the morphology of this language
which can be expressed through morphotactics.
6.1 Handling Arabic Morphotactics
Morphotactics refers to the way morphemes com-
bine together to form words (Beesley, 1998; Beesley
and Karttunen, 2003). Generally speaking, morpho-
tactics can be concatenative, with morphemes either
prefixed or suffixed to stems, or non-concatenative,
with stems undergoing internal alternations to con-
vey morphosyntactic information. Arabic is consid-
ered a typical example of a language that employs
non-concatenative morphotactics.
Arabic words are traditionally classified into three
types: verbs, nouns and particles. Adjectives take
almost all the morphological forms of, and share the
same templatic structures with, nouns. Adjectives,
for example, can be definite, and are inflected for
case, number and gender.
There are a number of indicators that tell us
whether the word is a verb or a noun. Among
4Statistical significance was determined using the strati-
fied shuffling method. The software used to perform the test
was downloaded from http://www.cis.upenn.edu/
?
dbikel/software.html.
5An inspection of the Berkeley Arabic grammar (available
at http://code.google.com/p/berkeleyparser/
downloads/list) shows that no Arabic-specific signatures
were employed. The Stanford parser uses 9 signatures for Ara-
bic, designed for use with unvocalised text. An immediate fu-
ture goal is to test this signature list with our parser.
these indicators are prefixes, suffixes and word tem-
plates. A template (Beesley and Karttunen, 2003) is
a kind of vocalization mould in which a word fits. In
derivational morphology Arabic words are formed
through the amalgamation of two tiers, namely, root
and template. A root is a sequence of three (rarely
two or four) consonants which are called radicals,
and the template is a pattern of vowels, or a com-
bination of consonants and vowels, with slots into
which the radicals of the root are inserted.
For the purpose of detection we use the reverse
of this information. Given that we have a word, we
try to extract the stem, by removing prefixes and suf-
fixes, and match the word against a number of verbal
and nominal templates. We found that most Ara-
bic templatic structures are in complementary dis-
tribution, i.e. they are either restricted to nominal
or verbal usage, and with simple regular expression
matching we can decide whether a word form is a
noun or a verb.
6.2 Noun Indicators
In order to detect that a word form is a noun (or ad-
jective), we employ heuristic rules related to Arabic
prefixes/suffixes and if none of these rules apply we
attempt to match the word against templatic struc-
tures. Using this methodology, we are able to detect
95% of ATB nouns.6
We define a list of 42 noun templates which are
used to indicate active/passive participle nouns, ver-
bal nouns, nouns of instrument and broken plural
nouns (see Table 4 for some examples). Note that
templates ending with taa marboutah ?ap? or start-
ing with meem madmoumah ?mu? are not consid-
ered since they are covered by our suffix/prefix rules,
which are as follows:
1- The definite article prefix ?  or in Buckwalter
transliteration ?Al?.
2- The tanween suffix

, 

,

 or ?N?, ?F?, ?K?, ?AF?.
3- The feminine plural suffix HA, or ?+At?.
4- The taa marboutah ending ? or ?ap? whether as a
6The heuristics we developed are designed to work on dia-
critized texts. Although diacritics are generally ignored in mod-
ern writing, the issue of restoring diacritics has been satisfac-
torily addressed by different researchers. For example, Nelken
and Shieber (2005) presented an algorithm for restoring diacrit-
ics to undiacritized MSA texts with an accuracy of over 90%
and Habasah et al (2009) reported on a freely-available toolkit
(MADA-TOKAN) an accuracy of over 96%.
71
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 80.67 82.19 *81.42 (+ 1.89) 96.32
5 80.66 82.81 *81.72 (+ 3.25) 95.15
10 79.86 82.49 *81.15 (+ 4.18) 94.38
English
1 ***89.64 89.95 89.79 (+ 0.32) 96.44
5 89.16 89.80 89.48 (+ 0.15) 96.32
10 89.14 89.78 **89.46 (+ 0.98) 96.21
French
1 85.15 85.77 *85.46 (+ 1.58) 96.13
5 84.08 84.80 *84.44 (+ 1.74) 95.54
10 84.21 84.78 *84.49 (+ 3.04) 94.68
Table 3: Baseline Signatures for Arabic, French and English
statistically significant with *:p < 10?4, **: p < 10?3, ***: p < 0.004,
Template Name Regular Specification
Arabic Buckwalter Expression
?A

?
	
?

	
K 

{inofiEAl {ino.i.A. verbal noun (masdar)
?A

?
	
??

mifoEAl mi.o.A. noun instrument
??

	
?


J?

? musotafoEil musota.o.i. noun participle
?J


?

A

	
?

? mafAEiyl ma.A.iy. noun plural
?

?
	
?


J?

{isotafoEal {isota.o.a. verb
??

?

	
? fuwEil .uw.i. verb passive
Table 4: Sample Arabic Templatic Structures for Nouns and Verbs
feminine marker suffix or part of the word.
5- The genitive case marking kasrah 

, or ?+i?.
6- Words of length of at least five characters ending
with doubled yaa ?


or ?y??.
7- Words of length of at least six characters ending
with alif mamdoudah and hamzah Z  or ?A??.
8- Words of length of at least seven characters start-
ing with meem madmoumah ? or ?mu?.
6.3 Verb Indicators
In the same way, we define a list of 16 templates and
we combine them with heuristic rules related to Ara-
bic prefixes/suffixes to detect whether a word form
is exclusively a verb. The prefix/suffix heuristics are
as follows:
9-The plural marker suffix  ? or ?uwA? indicates a
verb.
10- The prefixes H , ?


,
	
?
,



,

? or ?sa?, ?>a?,
?>u?, ?na?, ?nu?, ?ya?, ?yu?, ?ta?, ?tu? indicate im-
prefective verb.
The verbal templates are less in number than the
noun templates yet they are no less effective in de-
tecting the word class (see Table 4 for examples).
Using these heuristics we are able to detect 85% of
ATB verbs.
6.4 Arabic Signatures
We map the 72 noun/verb classes that are identi-
fied using our hand-crafted heuristics into sets of
signatures of varying sizes: 4, 6, 14, 21, 25, 28
and 72. The very coarse-grained set considers just
4 signatures UNK-noun, UNK-verb, UNK-num,
and UNK and the most fine-grained set of 72 signa-
tures associates one signature per heuristic. In ad-
dition, we have evaluated the effect of reordering
rules and templates and also the effect of collating
all signatures satisfying an unknown word. The re-
sults of using these various signatures sets in parsing
72
UNK
NUM NOUN VERB
digits (see section 6.2) (see section 6.3)
Al definiteness tashkil At suffix ap suffix imperfect
rule 1 rules 2 and 5 rule 3 rule 4 rule 10
y? suffix A? suffix mu prefix verbal noun templates suffixes
rule 6 rule 7 rule 8 3 groupings dual/plural suffixes
plural templates participle active templates participle passive templates instrument templates passive templates
4 groupings
other templates verbal templates
5 groupings
Table 6: Arabic signatures
Cutoff 1 5 10
4 80.78 80.71 80.09
6 81.14 81.16 81.06
14 80.88 81.45 81.19
14 reorder 81.39 81.01 80.81
21 81.38 81.55 81.35
21 reorder 81.20 81.13 80.58
21 collect 80.94 80.56 79.63
25 81.18 81.25 81.26
28 81.42 81.72 (+ 3.25) 81.15
72 79.64 78.87 77.58
Table 5: Baseline Signatures for Arabic
our Arabic development set are presented in Table 5.
We achieve our best labeled bracketing f-score using
28 signatures with an unknown threshold of five. In
fact we get an improvement of 3.25% over using no
signatures at all (see Table 2). Table 3 describes in
more detail the scores obtained using the 28 signa-
tures present in Table 6. Apart from the set contain-
ing 72 signatures, all of the baseline signature sets in
Table 5 yield a statistically significant improvement
over the generic UNKNOWN results (p < 10?4).
7 Using Information Gain to Determine
Signatures
It is clear that dividing the UNKNOWN terminal into
more fine-grained categories based on morpholog-
ical information helps parsing for our three lan-
guages. In this section we explore whether useful
morphological clues can be learnt automatically. If
they can, it means that a latent-variable PCFG parser
can be adapted to any language without knowledge
of the language in question since the only language-
specific component in such a parser is the unknown-
signature specification.
In a nutshell, we extract affix features from train-
ing set words7 and then use information gain to rank
these features in terms of their predictive power in a
POS-tagging task. The features deemed most dis-
criminative are then used as signatures, replacing
our baseline signatures described in Sections 5 and
6. We are not going as far as actual POS-tagging,
but rather seeing whether the affixes that make good
features for a part-of-speech tagger also make good
unknown word signatures.
We experiment with English and French suffixes
of length 1-3 and Arabic prefixes and suffixes of var-
ious lengths as well as stem prefixes and suffixes of
length 2, 4 and 6. For each of our languages we
experiment with several information gain thresholds
on our development sets and we fix on an English
signature list containing 24 suffixes, a French list
containing 48 suffixes and an Arabic list containing
38 prefixes and suffixes.
Our development set results are presented in Ta-
ble 7. For all three languages, the information gain
signatures perform at a comparable level to the base-
line hand-crafted signatures (Table 3). For each
of the three unknown-word handling techniques, no
signature (UNKNOWN), hand-crafted signatures and
information gain signatures, we select the best un-
known threshold for each language?s development
set and apply these grammars to our test sets. The
f-scores are presented in Table 8, along with the up-
per bounds obtained by parsing with these grammars
in gold-tag mode. For French, the effect of tagging
accuracy on overall parse accuracy is striking. The
improvements that we get from using morphological
signatures are greatest for Arabic8 and smallest for
7We omit all function words and high frequency words be-
cause we are interested in the behaviour of words which are
likely to be similar to rare words.
8Bikel?s parser trained on the same Arabic data and tested
on the same input achieves an f-score of 76.50%. We trained
a 5-split-merge-iteration Berkeley grammar and parsed with the
73
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic IG
1 80.10 82.15 *81.11 (+ 1.58) 96.53
5 80.03 82.49 *81.32 (+ 2.85) 95.30
10 80.17 82.40 *81.27 (+ 4.3) 94.66
English IG
1 89.38 89.87 89.63 (+ 0.16) 96.45
5 89.54 90.22 ***89.88 (+ 0.55) 96.41
10 89.22 90.05 *89.63 (+ 1.15) 96.19
French IG
1 84.78 85.36 *85.07 (+ 1.19) 96.17
5 84.63 85.24 **84.93 (+ 2.23) 95.30
10 84.18 84.80 *84.49 (+ 3.09) 94.68
Table 7: Information Gain Signature Results
statistically significant with *:p < 10?4, **: p < 2 ? 10?4, ***: p < 0.005
Language No Sig Baseline Sig IG Sig
Arabic 78.34 *81.59 *81.33
Arabic Gold Tag 81.46 82.43 81.90
English 89.48 89.65 89.77
English Gold Tag 89.94 90.10 90.23
French 83.74 *85.77 **85.55
French Gold Tag 88.82 88.41 88.86
statistically significant with *: p < 10?4, **: p < 10?3
Table 8: F-Scores on Test Sets
English. The results for the information gain signa-
tures are promising and warrant further exploration.
8 Conclusion
We experiment with two unknown-word-handling
techniques in a statistical generative parsing model,
applying them to Arabic, French and English. One
technique is language-agnostic and the other makes
use of some morphological information (signatures)
in assigning part-of-speech tags to unknown words.
The performance differences from the two tech-
niques are smallest for English, the language with
the sparsest morphology of the three and the small-
est proportion of unknown words in its development
set. As a result of carrying out these experiments,
we have developed a list of Arabic signatures which
can be used with any statistical parser which does
Berkeley parser, achieving an f-score of 75.28%. We trained the
Berkeley parser with the -treebank SINGLEFILE option so that
English signatures were not employed.
its own tagging. We also present results which show
that signatures can be learnt automatically.
Our experiments have been carried out using gold
tokens. Tokenisation is an issue particularly for Ara-
bic, but also for French (since the treebank contains
merged compounds) and to a much lesser extent for
English (unedited text with missing apostrophes). It
is important that the experiments in this paper are re-
peated on untokenised text using automatic tokeni-
sation methods (e.g. MADA-TOKAN).
The performance improvements that we demon-
strate for Arabic unknown-word handling are obvi-
ously just the tip of the iceberg in terms of what can
be done to improve performance on a morpholog-
ically rich language. The simple generative lexical
probability model we use can be improved by adopt-
ing a more sophisticated approach in which known
and unknown word counts are combined when esti-
mating lexical rule probabilities for rare words (see
Huang and Harper (2009) and the Berkeley sophis-
ticatedLexicon training option). Further work will
also include making use of a lexical resource exter-
nal to the treebank (Goldberg et al, 2009; Habash,
2008) and investigating clustering techniques to re-
duce data sparseness (Candito and Crabbe?, 2009).
Acknowledgements
This research is funded by Enterprise Ireland
(CFTD/07/229 and PC/09/037) and the Irish Re-
search Council for Science Engineering and Tech-
nology (IRCSET). We thank Marie Candito and our
three reviewers for their very helpful suggestions.
74
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel,
2003. Treebanks: Building and Using Parsed
Corpora, chapter Building a Treebank for French.
Kluwer, Dordrecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In ACL. The Association for Computer Lin-
guistics.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI studies in computational lin-
guistics.
Kenneth R. Beesley. 1998. Arabic morphology using
only finite-state operations. In The Workshop on Com-
putational Approaches to Semitic Languages.
Ann Bies and Mohammed Maamouri. 2003. Penn Ara-
bic Treebank guidelines. Technical Report TB-1-28-
03.
Dan Bikel. 2004. On the Parameter Space of Generative
Lexicalized Parsing Models. Ph.D. thesis, University
of Pennslyvania.
Marie Candito and Benoit Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. In Proceedings of IWPT?09.
Marie Candito, Beno??t Crabbe?, and Djame? Seddah. 2009.
On statistical parsing of French with supervised and
semi-supervised strategies. In Proceedings of the
EACL 2009 Workshop on Computational Linguis-
tic Aspects of Grammatical Inference, pages 49?57,
Athens, Greece, March.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the Annual Meeting of the
North American Association for Computational Lin-
guistics (NAACL-00), pages 132?139, Seattle, Wash-
ington.
Beno??t Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de TALN.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In EACL, pages 327?335. The Association for Com-
puter Linguistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization, di-
acritization, morphological disambiguation, pos tag-
ging, stemming and lemmatization. In Proceedings of
the 2nd International Conference on Arabic Language
Resources and Tools (MEDAR).
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In Proceedings of Association for
Computational Linguistics, pages 57?60.
Zhongqiang Huang and Mary Harper. 2009. Self-
training pcfg grammars with latent annotations across
languages. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
Singapore, August.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003. Accurate unlex-
icalised parsing. In Proceedings of the 41st Annual
Meeting of the ACL.
Mohammed Maamouri and Ann Bies. 2004. Developing
an Arabic Treebank: Methods, guidelines, procedures,
and tools. In Workshop on Computational Approaches
to Arabic Script-based Languages, COLING.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of the 1994 ARPA Speech and Natural
Language Workshop, pages 114?119, Princeton, New
Jersey.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 75?82, Ann Arbor, June.
Rani Nelken and Stuart M. Shieber. 2005. Arabic dia-
critization using weighted finite-state transducers. In
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, Rochester, NY, April.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, Sydney,
Australia, July.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Berkeley, Berkeley, CA, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing (NeMLaP-1), pages 44?49.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help POS tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
75
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 14?19,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Decreasing lexical data sparsity in statistical syntactic parsing - experiments
with named entities
Deirdre Hogan, Jennifer Foster and Josef van Genabith
National Centre for Language Technology
School of Computing
Dublin City University
Dublin 9, Ireland
dhogan,jfoster,josef@computing.dcu.ie
Abstract
In this paper we present preliminary exper-
iments that aim to reduce lexical data spar-
sity in statistical parsing by exploiting infor-
mation about named entities. Words in the
WSJ corpus are mapped to named entity clus-
ters and a latent variable constituency parser
is trained and tested on the transformed cor-
pus. We explore two different methods for
mapping words to entities, and look at the ef-
fect of mapping various subsets of named en-
tity types. Thus far, results show no improve-
ment in parsing accuracy over the best base-
line score; we identify possible problems and
outline suggestions for future directions.
1 Introduction
Techniques for handling lexical data sparsity in
parsers have been important ever since the lexical-
isation of parsers led to significant improvements
in parser performance (Collins, 1999; Charniak,
2000). The original treebank set of non-terminal la-
bels is too general to give good parsing results. To
overcome this problem, in lexicalised constituency
parsers, non-terminals are enriched with lexical in-
formation. Lexicalisation of the grammar vastly
increases the number of parameters in the model,
spreading the data over more specific events. Statis-
tics based on low frequency events are not as reliable
as statistics on phenomena which occur regularly in
the data; frequency counts involving words are typi-
cally sparse.
Word statistics are also important in more re-
cent unlexicalised approaches to constituency pars-
ing such as latent variable parsing (Matsuzaki et al,
2005; Petrov et al, 2006). The basic idea of latent
variable parsing is that rather than enrich the non-
terminal labels by augmenting them with words, a
set of enriched labels which can encapsulate the syn-
tactic behaviour of words is automatically learned
via an EM training mechanism.
Parsers need to be able to handle both low fre-
quency words and words occurring in the test set
which were unseen in the training set (unknown
words). The problem of rare and unknown words is
particularly significant for languages where the size
of the treebank is small. Lexical sparseness is also
critical when running a parser on data that is in a dif-
ferent domain to the domain upon which the parser
was trained. As interest in parsing real world data
increases, a parsers ability to adequately handle out-
of-domain data is critical.
In this paper we examine whether clustering
words based on their named entity category can be
useful for reducing lexical sparsity in parsing. In-
tuitively word tokens in the corpus such as, say,
?Dublin? and ?New York? should play similar syn-
tactic roles in sentences. Likewise, it is difficult to
see how different people names could have differ-
ent discriminatory influences on the syntax of sen-
tences. This paper describes experiments at replac-
ing word tokens with special named entity tokens
(person names are mapped to PERSON tokens and
so on). Words in the original WSJ treebank are
mapped to entity types extracted from the BBN cor-
pus (Weischedel and Brunstein, 2005) and a latent
variable parser is trained and tested on the mapped
corpus. Ultimately, the motivation behind grouping
words together in this fashion is to make it easier for
14
the parser to recognise regularities in the data.1
The structure of paper is as follows: A brief sum-
mary of related work is given in Section 2. This
includes an outline of a common treatment of low
frequency and rare words in constituency parsing,
involving a mapping process that is similar to the
named entity mappings. Section 3 presents the ex-
periments carried out, starting with a short introduc-
tion of the named entity resource used in our exper-
iments and a description of the types of basic entity
mappings we examine. In ?3.1 and ?3.2 we describe
the two different types of mapping technique. Re-
sults are presented in Section 4, followed by a brief
discussion in Section 5 indicating possible problems
and avenues worth pursuing. Finally, we conclude.
2 Related Work
Much previous work on parsing and multiword units
(MWUs) adopts the words-with-spaces approach
which treats MWUs as one token (by concatenat-
ing the words together) (Nivre and Nilsson, 2004;
Cafferkey et al, 2007; Korkontzelos and Manand-
har, 2010). Alternative approaches are that of Finkel
and Manning (2009) on joint parsing and named en-
tity recognition and the work of (Wehrli et al, 2010)
which uses collocation information to rank compet-
ing hypotheses in a symbolic parser. Also related
is work on MWUs and grammar engineering, such
as (Zhang et al, 2006; Villavicencio et al, 2007)
where automatically detected MWUs are added to
the lexicon of a HPSG grammar to improve cover-
age.
Our work is most similar to the words-with-
spaces approach. Our many-to-one experiments
(see ?3.1) in particular are similar to previous
work on parsing words-with-spaces, except that we
map words to entity types rather than concatenated
words. Results are difficult to compare however, due
to different parsing methodologies, different types
of MWUs, as well as different evaluation methods.
Other relevant work is the integration of named
1It is true that latent variable parsers automatically induce
categories for similar words, and thus might be expected to
induce a category for say names of people if examples of
such words occurred in similar syntactic patterns in the data.
Nonetheless, the problem of data sparsity remains - it is diffi-
cult even for latent variable parsers to learn accurate patterns
based on words which only occur say once in the training set.
entity types in a surface realisation task by Rajku-
mar et al (2009) and the French parsing experiments
of (Candito and Crabbe?, 2009; Candito and Sed-
dah, 2010) which involve mapping words to clusters
based on morphology as well as clusters automati-
cally induced via unsupervised learning on a large
corpus.
2.1 Parsing unknown words
Most state-of-the-art constituency parsers (e.g.
(Petrov et al, 2006; Klein and Manning, 2003))
take a similar approach to rare and unknown words.
At the beginning of the training process very low
frequency words in the training set are mapped to
special UNKNOWN tokens. In this way, some
probability mass is reserved for occurrences of UN-
KNOWN tokens and the lexicon contains produc-
tions for such tokens (X ? UNKNOWN), with as-
sociated probabilities. When faced with a word in
the test set that the parser has not seen in its train-
ing set - the unknown word is mapped to the special
UNKNOWN token.
In syntactic parsing, rather than map all low fre-
quency words to one generic UNKNOWN type, it
is useful to have several different clusters of un-
known words, grouped according to morphologi-
cal and other ?surfacey? clues in the original word.
For example, certain suffixes in English are strong
predictors for the part-of-speech tag of the word
(e.g. ?ly?) and so all low frequency words end-
ing in ?ly? are mapped to ?UNKNOWN-ly?. As
well as suffix information, UNKNOWN words are
commonly grouped based on information on capi-
talisation and hyphenation. Similar techniques for
handling unknown words have been used for POS
tagging (e.g. (Weischedel et al, 1993; Tseng et
al., 2005)) and are used in the Charniak (Char-
niak, 2000), Berkeley (Petrov et al, 2006) and Stan-
ford (Klein and Manning, 2003) parsers, as well as
in the parser used for the experiments in this paper,
an in-house implementation of the Berkeley parser.
3 Experiments
The BBN Entity Type Corpus (Weischedel and
Brunstein, 2005) consists of sentences from the
Penn WSJ corpus, manually annotated with named
entities. The Entity Type corpus includes annota-
15
type count examples
PERSON 11254 Kim Cattrall
PER DESC 21451 president,chief executive officer,
FAC 383 office, Rockefeller Center
FAC DESC 2193 chateau ,stadiums, golf course
ORGANIZATION 24239 Securities and Exchange Commission
ORG DESC 15765 auto maker, college
GPE 10323 Los Angeles,South Africa
GPE DESC 1479 center, nation, country
LOCATION 907 North America,Europe, Hudson River
NORP 3269 Far Eastern
PRODUCT 667 Maxima, 300ZX
PRODUCT DESC 1156 cars
EVENT 296 Vietnam war,HUGO ,World War II
WORK OF ART 561 Revitalized Classics Take..
LAW 300 Catastrophic Care Act,Bill of Rights
LANGUAGE 62 Latin
CONTACT INFO 30 555 W. 57th St.
PLANT 172 crops, tree
ANIMAL 355 hawks
SUBSTANCE 2205 gold,drugs, oil
DISEASE 254 schizophrenia,alcoholism
GAME 74 football senior tennis and golf tours
Table 1: Name expression entity types (sections 02-21)
tion for three classes of named entity: name expres-
sions, time expressions and numeric expressions (in
this paper we focus on name expressions). These
are further broken down into types. Table 1 displays
name expression entity types, their frequency in the
training set (sections 02-21), as well as some illus-
trative examples from the training set data.
We carried out experiments with different subsets
of entity types. In one set of experiments, all name
expression entities were mapped, with no restriction
on the types (ALL NAMED). We also carried
out experiments on a reduced set of named entities
- where only entities marked as PERSON, ORGA-
NIZATION, or GPE and LOCATION were mapped
(REDUCED). Finally, we ran experiments where
only one type of named entity was mapped at a time.
In all cases the words in the named entities were re-
placed by their entity type.
3.1 Many-to-one Mapping
In the many-to-one mapping all words in a named
entity were replaced with one named entity type
token. This approach is distinct from the words-
with-spaces approach previously pursued in parsing
where, for example, ?New York? would be replaced
with ?New York?. Instead, in our experiments ?New
York? is replaced with ?GPE? (geo-political entity).
In both approaches, the parser is forced to respect
unk map NE map #unks f-score POS
generic
none (baseline 1) 2966 (4.08%) 88.69 95.57
ALL NAMED 1908 (2.73%) 89.21 95.49
REDUCED 2122 (3.02%) 89.43 96.08
Person 2671 (3.68%) 88.98 95.55
Organisation 2521 (3.55%) 89.38 95.92
Location 2945 (4.05%) 89.00 95.62
sigs
none (baseline 2) 2966 (4.08%) 89.72 96.51
ALL NAMED 1908 (2.73%) 89.67 95.99
REDUCED 2122 (3.02%) 89.53 96.65
Person 2671 (3.68%) 89.32 96.47
Organisation 2521 (3.55%) 89.53 96.64
Location 2945 (4.05%) 89.20 96.52
Table 2: Many-to-One Parsing Results.
the multiword unit boundary (and analyses which
contain constituents that cross the MWU boundary
will not be considered by the parser). Intuitively,
this should help parser accuracy and speed. The ad-
vantage of mapping the word tokens to their entity
type rather than to a words-with-spaces token is that
in addition we will be reducing data sparsity.
One issue with the many-to-one mapping is that
in evaluation exact comparison with a baseline re-
sult is difficult because the tokenisation of test and
gold sets is different. When named entities span
more than one word, we are reducing the number
of words in the sentences. As parsers tend to do bet-
ter on short sentences than on long sentences, this
could make parsing somewhat easier. However, we
found that the average number of words in a sen-
tence before and after this mapping does not change
by much. The average number of words in the devel-
opment set is 23.9. When we map words to named
entity tokens (ALL NAMED), the average drops
by just one word to 22.9.2
3.2 One-to-one Mapping
In the one-to-one experiments we replaced each
word in named entity with a named entity type to-
ken (e.g. Ada Lovelace ? pperson pperson).3 The
motivation was to measure the effect of reducing
word sparsity using named entities without altering
the original tokenisation of the data.4
2A related issue is that the resulting parse tree will lack an
analysis for the named entity.
3The entity type was given an extra letter where needed (e.g.
?pperson?) to avoid the conflation of a mapped entity token with
an original word (e.g. ?person?) in the corpus.
4Note, where there is punctuation as part of a named entity
we do not map the punctuation.
16
unk map NE map #unks f-score POS
generic
none (baseline 1) 2966 (4.08%) 88.69 95.57
ALL NAMED 1923 (2.64%) 89.28 94.99
REDUCED 2122 (2.90%) 88.76 95.76
Person 2654(3.65%) 88.95 95.57
Organisation 2521 (3.45%) 88.80 95.59
Location 2945 (4.04%) 88.88 95.66
sigs
none (baseline 2) 2966 (4.08%) 89.72 96.51
ALL NAMED 1923 (2.64%) 89.36 95.64
REDUCED 2122 (2.90%) 89.01 96.32
Person 2654(3.65%) 89.30 96.52
Organisation 2521 (3.45%) 89.29 96.30
Location 2945 (4.04%) 89.55 96.54
Table 3: One-to-One Parsing Results
In an initial experiment, where the mapping was
simply the word to the named entity type, many sen-
tences received no parse. This happened often when
a named entity consisted of three or more words and
resulted in a sentence such as ?But while the Oor-
ganization Oorganization Oorganization Oorganiza-
tion did n?t fall apart Friday?. We found that refining
the named entity by adding the number of the word
in the entity to the mapping resolved the coverage
problem. The example sentence is now: ?But while
the Oorganization1 Oorganization2 Oorganization3
Oorganization4 did n?t fall apart Friday?. See ?5 for
a possible explanation for the parser?s difficulty with
one-to-one mappings to coarse grained entity types.
4 Results
Table 2 and Table 3 give the results for the many-to-
one and one-to-one experiments respectively. Re-
sults are given against a baseline where unknowns
are given a ?generic? treatment (baseline 1) - i.e.
they are not clustered according to morphological
and surface information - and for the second baseline
(baseline 2), where morphological or surface feature
markers (sigs) are affixed to the unknowns.5
The results indicate that though lexical spar-
sity is decreasing, insofar as the number of un-
known words (#unks column) in the development
set decreases with all named entity mappings, the
named entity clusters are not informative enough
and parser accuracy falls short of the previous best
result. For all experiments, a pattern that emerges
5For all experiments, a split-merge cycle of 5 was used. Fol-
lowing convention, sections 02-21 were used for training. Sec-
tions 22 and 24 (sentences less than or equal to 100 words) were
used for the development set. As experiments are ongoing we
do not report results on a test set.
is that mapping words to named entities improves
results when low frequency words are mapped to
a generic UNKNOWN token. However, when low
frequency words are mapped to more fine-grained
UNKNOWN tokens, mapping words to named enti-
ties decreases accuracy marginally.
If a particular named entity occurs often in the text
then data sparsity is possibly not a problem for this
word. Rather than map all occurrences of a named
entity to its entity type, we experimented with map-
ping only low frequency entities. These named en-
tity mapping experiments now mirror more closely
the unknown words mappings - low frequency en-
tities are mapped to special entity types, then the
parser maps all remaining low frequency words to
UNKNOWN types. Table 4 shows the effect of map-
ping only entities that occur less than 10 times in the
training set, to the person type and the reduced set
of entity types. Results somewhat improve for all
but one of the one-to-one experiments, but nonethe-
less remain below the best baseline result. There is
still no advantage in mapping low frequency person
name words to, say, the person cluster, rather than to
an UNKNOWN-plus-signature cluster.
5 Discussion
Our results thus far suggest that clusters based on
morphology or surface clues are more informative
than the named entity clusters.
For the one-to-one mappings one obvious prob-
lem that emerged is that all words in entities (in-
cluding function words for example) get mapped to
a generic named entity token. A multi-word named
entity has its own internal syntactic structure, re-
flected for example in its sequence of part-of-speech
tags. By replacing each word in the entity with
the generic entity token we end up loosing informa-
tion about words, conflating words that take differ-
ent part-of-speech categories, and in fact make pars-
ing more difficult. The named entity clusters in this
case are too coarse-grained and words with different
syntactic properties are merged into the one cluster,
something we would like to avoid.
In future work, as well as avoiding mapping more
complex named entities, we will refine the named
entity clusters by attaching to the entity type signa-
tures similar to those attached to the UNKNOWN
17
unk map NE map one2one f-score many2one f-score
generic
Person 88.95 88.98
Person < 10 88.97 89.05
Reduced 88.76 89.43
Reduced < 10 89.51 88.85
sigs
Person 89.30 89.32
Person < 10 89.49 89.33
Reduced 89.01 89.53
Reduced < 10 89.42 89.15
Table 4: Measuring the effect of mapping only low fre-
quency named entities.
types. It would also be interesting to examine the ef-
fect of mapping other types of named entities, such
as dates and numeric expressions. Finally, we intend
trying similar experiments on out-of-domain data,
such as social media text where unknown words are
more problematic.
6 Conclusion
We have presented preliminary experiments which
test the novel technique of mapping word tokens to
named entity clusters, with the aim of improving
parser accuracy by reducing data sparsity. While our
results so far are disappointing, we have identified
possible problems and outlined future experiments,
including suggestions for refining the named entity
clusters so that they become more syntactically ho-
mogenous.
References
Conor Cafferkey, Deirdre Hogan, and Josef van Gen-
abith. 2007. Multi-word units in treebank-based prob-
abilistic parsing and generation. In Proceedings of the
10th International Conference on Recent Advances in
Natural Language Processing (RANLP-07), Borovets,
Bulgaria.
Marie Candito and Benoit Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. In Proceedings of the International Work-
shop on Parsing Technologies (IWPT-09).
Marie Candito and Djame? Seddah. 2010. Lemmatization
and statistical lexicalized parsing of morphologically-
rich languages. In Proceedings of the NAACL/HLT
Workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL).
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In Proceedings of the 1st North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-2009).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association of Computational
Linguistics (ACL).
Ioannis Korkontzelos and Suresh Manandhar. 2010. Can
recognising multiword expressions improve shallow
parsing? In Proceedings of the Conference of the
North American Chapter of the ACL (NAACL-10), Los
Angeles, California.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 75?82, Ann Arbor, June.
Joakim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Workshop on Methodologies
and Evaluation of Multiword Units in Real-World Ap-
plications.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, Sydney,
Australia, July.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
ccg surface realisation. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-09).
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morpholgical features help pos tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Eric Wehrli, Violeta Seretan, and Luke Nerima. 2010.
Sentence analysis and collocation identification. In
Proceedings of the Workshop on Multiword Expres-
sion: From Theory to Applications (MWE).
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. In Tehcnical
Report.
18
Ralph Weischedel, Richard Schwartz, Jeff Palmucci,
Marie Meteer, and Lance Ramshaw. 1993. Coping
with ambiguity and unknown words through proba-
bilistic models. Computational Linguistics, 19(2).
Yi Zhang, Valia Kordoni, Aline Villavicencio, and Marco
Idiart. 2006. Automated multiword expression pre-
diction for grammar engineering. In Proceedings of
the Workshop on Multiword Expressions: Identifying
and Exploiting Underlying Properties.
19
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 138?144,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DCU-Symantec Submission for the WMT 2012 Quality Estimation Task
Raphael Rubino??, Jennifer Foster?, Joachim Wagner?,
Johann Roturier?, Rasul Samad Zadeh Kaljahi??, Fred Hollowood?
?Dublin City University, ?Symantec, Ireland
?firstname.lastname@computing.dcu.ie
?firstname lastname@symantec.com
Abstract
This paper describes the features and the ma-
chine learning methods used by Dublin City
University (DCU) and SYMANTEC for the
WMT 2012 quality estimation task. Two sets
of features are proposed: one constrained, i.e.
respecting the data limitation suggested by the
workshop organisers, and one unconstrained,
i.e. using data or tools trained on data that was
not provided by the workshop organisers. In
total, more than 300 features were extracted
and used to train classifiers in order to predict
the translation quality of unseen data. In this
paper, we focus on a subset of our feature set
that we consider to be relatively novel: fea-
tures based on a topic model built using the
Latent Dirichlet Allocation approach, and fea-
tures based on source and target language syn-
tax extracted using part-of-speech (POS) tag-
gers and parsers. We evaluate nine feature
combinations using four classification-based
and four regression-based machine learning
techniques.
1 Introduction
For the first time, the WMT organisers this year pro-
pose a Quality Estimation (QE) shared task, which
is divided into two sub-tasks: scoring and ranking
automatic translations. The aim of this workshop is
to define useful sets of features and machine learn-
ing techniques in order to predict the quality of a
machine translation (MT) output T (Spanish) given
a source segment S (English). Quality is measured
using a 5-point likert scale which is based on post-
editing effort, following the scoring scheme:
1. The MT output is incomprehensible
2. About 50-70% of the MT output needs to be
edited
3. About 25-50% of the MT output needs to be
edited
4. About 10-25% of the MT output needs to be
edited
5. The MT output is perfectly clear and intelligi-
ble
The final score is a combination of the scores as-
signed by three evaluators. The use of a 5-point scale
makes the scoring task more difficult than a binary
classification task where a translation is considered
to be either good or bad. However, if the task is
successfully carried out, the score produced is more
useful.
Dublin City University and Symantec jointly ad-
dress the scoring task. For each pair (S, T ) of source
segment S and machine translation T , we train three
classifiers and one classifier combination using the
training data provided by the organisers to predict
5-point Likert scores. In this paper, we present the
classification results on the test set alng with addi-
tional results obtained using regression techniques.
We evaluate the usefulness of two new sets of fea-
tures:
1. topic-based features using Latent Dirichlet Al-
location (LDA (Blei et al, 2003)),
2. syntax-based features using POS taggers and
parsers (Wagner et al, 2009)
The remainder of this paper is organised as fol-
lows. In Section 2, we give an overview of all the
138
features employed in our QE system. Then, in Sec-
tion 3, we describe the topic and syntax-based fea-
tures in more detail. Section 4 presents the vari-
ous classification and regression techniques we ex-
plored. Our results are presented and discussed in
Section 5. Finally, we summarise and outline our
plans in Section 6.
2 Features Overview
In this section, we describe the features used in our
QE system. In the first subsection, the features in-
cluded in our constrained system are presented. In
the second subsection, we detail the features in-
cluded in our unconstrained system. Both of these
systems include the 17 baseline features provided
for the shared task.
2.1 Constrained System
The constrained system is based only on the data
provided by the organisers. We extracted 70 fea-
tures in total (including the baseline features) and
we present them here according to the type of infor-
mation they capture.
Word and Phrase-Level Features
? Ratio of source and target segment length:
the number of source words divided by the
number of target words
? Ratio of source and target number of punc-
tuation marks: the number of source punctua-
tion marks divided by the number of target ones
? Number of phrases comprising the MT out-
put: given a phrase-table, we assume that a
sentence composed of several phrases indicates
uncertainty on the part of the MT system.
? Average length of source and target phrases:
concatenating short phrases may result in lower
fluency compared to the use of longer ones.
? Ratio of source and target averaged phrase
length
? Number of source prepositions and conjunc-
tions word: our assumption here is that seg-
ments containing a relatively high number of
prepositions and conjunctions may be more
complex and difficult to translate.
? Number of source out-of-vocabulary words
Language Model Features
All the language models (LMs) used in our work
are n-gram LMs with Kneser-Ney smoothing built
with the SRI Toolkit (Stolcke, 2002).
? Backward 2-gram and 3-gram source and
target log probabilities: as proposed by
Duchateau et al (2002)
? Log probability of target segments on
5-gram MT-output-based LM: using
MOSES (Koehn et al, 2007) trained on the
provided parallel corpus, we translated the En-
glish side of this corpus into Spanish, assuming
that the MT output contains mistakes. This
MT output is used to build a LM that models
the behavior of the MT system. We assume
that for a given MT output, a high n-gram
probability (or a low perplexity) of the LM
indicates that the MT output contains mistakes.
MT-system Features
? 15 scores provided by Moses: phrase-table,
language model, reordering model and word
penalty (weighted and unweighted)
? Number of n-bests for each source segment
? MT output back-translation: from Spanish to
English using MOSES trained on the provided
parallel corpus, scored with TER (Snover et
al., 2006), BLEU (Papineni et al, 2002) and
the Levenshtein distance (Levenshtein, 1966),
based on the source segments as a translation
reference
Topic Model Features
? Probability distribution over topics: Source
and target segment probability distribution over
topics for a 10-dimension topic model
? Cosine distance between source and target
topic vectors
More details about these two features are provided
in Section 3.1.
2.2 Unconstrained System
In addition to the features used for the constrained
system, a further 238 unconstrained features were
included in our unconstrained system.
139
MT System Features
As for our constrained system, we use MT output
back-translation from Spanish to English, but this
time using Bing Translator1 in addition to Moses.
Each back-translated segment is scored with TER,
BLEU and the Levenshtein distance, based on the
source segments as a translation reference.
Source Syntax Features
Wagner et al (2007; 2009) propose a series of
features to measure sentence grammaticality. These
features rely on a part-of-speech tagger, a probabilis-
tic parser and a precision grammar/parser. We have
at our disposal these tools for English and so we ap-
ply them to the source data. The features themselves
are described in more detail in Section 3.2.
Target Syntax Features
We use a part-of-speech tagger trained on Spanish
to extract from the target data the subset of grammat-
icality features proposed by Wagner et al (2007;
2009) that are based on POS n-grams. In addition
we extract features which reflect the prevalence of
particular POS tags in each target segment. These
are explained in more detail in Section 3.2 below.
Grammar Checker Features
LANGUAGETOOL (based on (Naber, 2003)) is an
open-source grammar and style proofreading tool
that finds errors based on pre-defined, language-
specific rules. The latest version of the tool can
be run in server mode, so individual sentences can
be checked and assigned a total number of errors
(which may or may not be true positives).2 This
number is used as a feature for each source segment
and its corresponding MT output.
3 Topic and Syntax-based Features
In this section, we focus on the set of features
that aim to capture adequacy using topic modelling
and grammaticality using POS tagging and syntactic
parsing.
1http://www.microsofttranslator.com/
2The list of English and Spanish rules is available at:
http://languagetool.org/languages.
3.1 Topic-based Features
We extract source and target features based on a
topic model built using LDA. The main idea in topic
modelling is to produce a set of thematic word clus-
ters from a collection of documents. Using the par-
allel corpus provided for the task, a bilingual corpus
is built where each line is composed of a source seg-
ment and its translation separated by a space. Each
pair of segments is considered as a bilingual docu-
ment. This corpus is used to train a bilingual topic
model after stopwords removal. The resulting model
is one set of bilingual topics z containing words w
with a probability p(wn|zn, ?) (with n equal to the
vocabulary size in the whole parallel corpus). This
model can be used to infer the probability distri-
bution of unseen source and target segments over
bilingual topics. During the test step, each source
segment and its translation are considered individu-
ally, as two monolingual documents. This method
allows us to compare the source and target topic dis-
tributions. We assume that a source segment and its
translation share topic similarities.
We propose two ways of using topic-based fea-
tures for quality estimation: keeping source and tar-
get topic vectors as two sets of k features, or com-
puting a vector distance between these two vectors
and using one feature only. To measure the prox-
imity of two vectors, we decided to used the Co-
sine distance, as it leads to the best results in terms
of classification accuracy. However, we plan to
study different metrics in further experiments, like
the Manhattan or the Euclidean distances. Some
parameters related to LDA have to be studied more
carefully too, such as the number of topics (dimen-
sions in the topic space), the number of words per
topic, the Dirichlet hyperparameter ?, etc. In our
experiments, we built a topic model composed of 10
dimensions using Gibbs sampling with 1000 itera-
tions. We assume that a higher dimensionality can
lead to a better repartitioning of the vocabulary over
the topics.
Multilingual LDA has been used before in nat-
ural language processing, e.g. polylingual topic
models (Mimno et al, 2009) or multilingual topic
models for unaligned text (Boyd-Graber and Blei,
2009). In the field of machine translation, Tam et
al. (2007) propose to adapt a translation and a lan-
140
guage model to a specific topic using Latent Se-
mantic Analysis (LSA, or Latent Semantic Index-
ing, LSI (Deerwester et al, 1990)). More recently,
some studies were conducted on the use of LDA to
adapt SMT systems to specific domains (Gong et al,
2010; Gong et al, 2011) or to extract bilingual lexi-
con from comparable corpora (Rubino and Linare`s,
2011). Extracting features from a topic model is, to
the best of our knowledge, the first attempt in ma-
chine translation quality estimation.
3.2 Syntax-based Features
Syntactic features have previously been used in MT
for confidence estimation and for building automatic
evaluation measures. Corston-Oliver et al (2001)
build a classifier using 46 parse tree features to pre-
dict whether a sentence is a human translation or MT
output. Quirk (2004) uses a single parse tree feature
in the quality estimation task with a 4-point scale,
namely whether a spanning parse can be found, in
addition to LM perplexity and sentence length. Liu
and Gildea (2005) measure the syntactic similarity
between MT output and reference translation. Al-
brecht and Hwa (2007) measure the syntactic simi-
larity between MT output and reference translation
and between MT output and a large monolingual
corpus. Gimenez and Marquez (2007) explore lexi-
cal, syntactic and shallow semantic features and fo-
cus on measuring the similarity of MT output to ref-
erence translation. Owczarzak et al (2007) use la-
belled dependencies together with WordNet to avoid
penalising valid syntactic and lexical variations in
MT evaluation. In what follows, we describe how
we make use of syntactic information in the QE task,
i.e. evaluating MT output without a reference trans-
lation.
Wagner et al (2007; 2009) use three sources
of linguistic information in order to extract features
which they use to judge the grammaticality of En-
glish sentences:
1. For each POS n-gram (with n ranging from 2 to
7), a feature is extracted which represents the
frequency of the least frequent n-gram in the
sentence according to some reference corpus.
TreeTagger (Schmidt, 1994) is used to produce
POS tags.
2. Features provided by a hand-crafted, broad-
coverage precision grammar of English (Butt
et al, 2002) and a Lexical Functional Grammar
parser (Maxwell and Kaplan, 1996). These in-
clude whether or not a sentence could be parsed
without resorting to robustness measures, the
number of analyses found and the parsing time.
3. Features extracted from the output of three
probabilistic parsers of English (Charniak and
Johnson, 2005), one trained on Wall Street
Journal trees (Marcus et al, 1993), one trained
on a distorted version of the treebank obtained
by automatically creating grammatical error
and adjusting the parse trees, and the third
trained on the union of the original and dis-
torted versions.
These features were originally designed to distin-
guish grammatical sentences from ungrammatical
ones and were tested on sentences from learner cor-
pora by Wagner et al (2009) and Wagner (2012).
In this work we extract all three sets of features
from the source side of our data and the POS-based
subset from the target side.3 We use the publicly
available pre-trained TreeTagger models for English
and Spanish4. The reference corpus used to obtain
POS n-gram frequences is the MT translation model
training data.5
In addition to the POS-based features described in
Wagner et al (2007; 2009), we also extract the fol-
lowing features from the Spanish POS-tagged data:
for each POS tag P and target segment T , we ex-
tract a feature which is the proportion of words in
T that are tagged as P . Two additional features are
extracted to represent the proportion of words in T
that are assigned more than one tag by the tagger,
3Unfortunately, due to time constraints, we were unable to
source a suitable probabilistic phrase-structure parser and a pre-
cision grammar for Spanish and were thus unable to extract
parser-based features for Spanish. We expect that these features
would be more useful on the target side than the source side.
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
5To aid machine learning methods that linearly combine fea-
ture values, we add binarised features derived from the raw XLE
and POS n-gram features described above, for example we add
a feature indicating whether the frequency of the least frequent
POS 5-gram is below 10. We base the choice of binary fea-
tures on (a) decision rules observed in decision trees trained for
a binary scoring task and (b) decision rules of simple classifiers
(decision trees with just one decision node and 2 leaf nodes)
that form a convex hull of optimal classifiers in ROC space.
141
and the proportion of words in T that are unknown
to the tagger.
4 Machine Learning
In this section, we describe the machine learning
methods that we experimented with. Our final sys-
tems submitted for the shared task are based on clas-
sification methods. However, we also performed
some experiments with regression methods.
We evaluate the systems on the test set using the
official evaluation script and the reference scores.
We report the evaluation results as Mean Aver-
age Error (MAE) and Root Mean Squared Error
(RMSE).
4.1 Classification
In order to apply classification algorithms to the
set of features associated with each source and tar-
get segment, we rounded the training data scores
to the closest integer. We tested several classifiers
and empirically chose three algorithms: Support
Vector Machine using sequential minimal optimiza-
tion and RBF kernel (parameters optimized by grid-
search) (Platt, 1999), Naive Bayes (John and Lang-
ley, 1995) and Random Forest (Breiman, 2001) (the
latter two techniques were applied with default pa-
rameters). We use the Weka toolkit (Hall et al,
2009) to train the classifiers and predict the scores
on the test set. Each method is evaluated individu-
ally and then combined by averaging the predicted
scores.
4.2 Regression
We applied three different regression techniques:
SVM epsilon-SVR with RBF kernel, Linear Regres-
sion and M5P (Quinlan, 1992; Wang and Witten,
1997). The two latter algorithms were used with
default parameters, whereas SVM parameters (?, c
and ) were optimized by grid-search. We also per-
formed a combination of the three algorithms by av-
eraging the predicted scores. We apply a linear func-
tion on the predicted scores S in order to keep them
in the correct range (from 1 to 5) as detailed in (1),
where S? is the rescaled sentence score, Smin is the
lowest predicted score and Smax is the highest pre-
dicted score.
S? = 1 + 4?
S ? Smin
Smax ? Smin
(1)
5 Evaluation
Table 1 shows the results obtained by our classifi-
cation approach on various feature subsets. Note
that the two submitted systems used the combined
classifier approach with the constrained and uncon-
strained feature sets. Table 2 shows the results for
the same feature combinations, this time using re-
gression rather than classification.
The results of quality estimation using classifica-
tion methods show that the baseline and the syntax-
based features with the classifier combination leads
to the best results with an MAE of 0.71 and an
RMSE of 0.87. However, these scores are substan-
tially lower than the ones obtained using regression,
where the unconstrained set of features with SVM
leads to an MAE of 0.62 and an RMSE of 0.78.
It seems that the classification methods are not
suitable for this task according to the different sets
of features studied. Furthermore, the topic-distance
feature is not correlated with the quality scores, ac-
cording to the regression results. On the other hand,
the syntax-based features appear to be the most in-
formative and lead to an MAE of 0.70.
6 Conclusion
We presented in this paper our submission for the
WMT12 Quality Estimation shared task. We also
presented further experiments using different ma-
chine learning techniques and we evaluated the im-
pact of two sets of features - one set which is based
on linguistic features extracted using POS tagging
and parsing, and a second set which is based on topic
modelling. The best results are obtained by our un-
constrained system containing all features and us-
ing an -SVR regression method with a Radial Basis
Function kernel. This setup leads to a Mean Aver-
age Error of 0.62 and a Root Mean Squared Error
of 0.78. Unfortunately, we did not submit our best
configuration for the shared task.
We plan to continue working on the task of ma-
chine translation quality estimation. Our immediate
next steps are to continue to investigate the contribu-
tion of individual features, to explore feature selec-
tion in a more detailed fashion and to apply our best
system to other types of data including sentences
taken from an online discussion forum.
142
SMO NAIVE BAYES RANDOM FOREST Combination
Features MAE RMSE MAE RMSE MAE RMSE MAE RMSE
baseline 0.74 0.89 0.85 1.10 0.84 1.06 0.71 0.88
topic distribution 0.84 1.02 1.09 1.38 0.91 1.15 0.78 0.98
topic distance 0.88 1.11 0.93 1.17 1.04 1.23 0.84 1.04
syntax 0.78 0.97 1.01 1.27 0.83 1.05 0.72 0.90
baseline + topic 0.82 1.01 1.00 1.31 0.84 1.05 0.75 0.95
baseline + syntax 0.76 0.94 1.01 1.25 0.79 0.98 0.71 0.87
baseline + topic + syntax 0.82 1.04 1.03 1.29 0.79 0.98 0.74 0.93
all constrained 0.99 1.26 1.12 1.46 0.71 0.88 0.86 ? 1.12 ?
all unconstrained 0.97 1.25 0.80 1.02 0.79 0.99 0.75 ? 0.97 ?
Table 1: MAE and RMSE results for different sets of features using three classification methods. The results with ?
and ? correspond to the DCU-SYMC constrained and the DCU-SYMC unconstrained systems respectively, submitted
for the shared task.
SVM LINEAR REG. M5P Combination
Features MAE RMSE MAE RMSE MAE RMSE MAE RMSE
baseline 0.78 0.93 0.80 0.99 0.73 0.91 0.72 0.88
topic distribution 0.78 0.95 0.79 0.96 0.80 0.96 0.79 0.95
topic distance 1.38 1.67 1.31 1.62 1.85 2.09 1.00 1.24
syntax 0.70 0.88 0.97 1.22 1.41 1.65 0.76 0.92
baseline + topic 0.78 0.96 1.06 1.31 1.16 1.42 0.88 1.10
baseline + syntax 0.67 0.82 0.90 1.12 2.17 2.38 0.98 1.22
baseline + topic + syntax 0.68 0.84 0.93 1.16 2.12 2.33 0.97 1.21
all constrained 0.83 1.02 0.94 1.18 0.78 0.99 0.71 0.88
all unconstrained 0.62 0.78 1.33 1.60 0.71 0.89 0.73 0.91
Table 2: MAE and RMSE results for different sets of features using three regression methods.
References
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level MT eval-
uation. In Proceedings of the 45th Annual Meeting of
the ACL, pages 880?887.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet Allocation. The Journal of Machine Learn-
ing Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of the 25th Conference on Uncertainty in Artificial In-
telligence, pages 75?82.
L. Breiman. 2001. Random forests. Machine learning,
45(1):5?32.
M. Butt, H. Dyvik, T. Holloway King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project. In
Proceedings of the Coling Workshop on Grammar En-
gineering and Evaluation.
E. Charniak and M. Johnson. 2005. Course-to-fine n-
best-parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 173?180, Ann Arbor.
S. Corston-Oliver, M. Gamon, and C. Brockett. 2001.
A machine learning approach to the automatic evalu-
ation of machine translation. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 148?155, Toulouse, France, July.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society for Infor-
mation Science, 41(6):391?407.
J. Duchateau, K. Demuynck, and P. Wambacq. 2002.
Confidence scoring based on backward language mod-
els. In Proceedings IEEE international confer-
ence on acoustics, speech, and signal processing,
ICASSP?2002, volume 1, pages 221?224.
J. Gime?nez and L. Ma`rquez. 2007. Linguistic features
for automatic evaluation of heterogenous MT systems.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 256?264, Prague, Czech
Republic, June.
143
Z. Gong, Y. Zhang, and G. Zhou. 2010. Statistical ma-
chine translation based on lda. In Universal Commu-
nication Symposium (IUCS), 2010 4th International,
pages 286?290.
Z. Gong, G. Zhou, and L. Li. 2011. Improve smt with
source-side ?topic-document? distributions. In MT
Summit, pages 496?501.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
G.H. John and P. Langley. 1995. Estimating continuous
distributions in bayesian classifiers. In Eleventh con-
ference on uncertainty in artificial intelligence, pages
338?345. Morgan Kaufmann Publishers Inc.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. In Soviet
Physics Doklady, volume 10-8, pages 707?710.
D. Liu and D. Gildea. 2005. Syntactic features for eval-
uation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 25?32, Ann Arbor, Michigan.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the penn treebank. Computational Linguistics,
19(2):313?330.
John Maxwell and Ron Kaplan. 1996. An Efficient
Parser for LFG. In Proceedings of LFG-96, Grenoble.
D. Mimno, H.M. Wallach, J. Naradowsky, D.A. Smith,
and A. McCallum. 2009. Polylingual topic models.
In Proceedings of EMNLP: Volume 2-Volume 2, pages
880?889. Association for Computational Linguistics.
D. Naber. 2003. A rule-based style and grammar
checker. Technical report, Bielefeld University Biele-
feld, Germany.
K. Owczarzak, J. van Genabith, and A. Way. 2007. La-
belled dependencies in machine translation evaluation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 104?111, Prague, Czech
Republic, June.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
J.C. Platt. 1999. Fast training of support vector machines
using sequential minimal optimization. In Advances in
kernel methods, pages 185?208. MIT Press.
R. J. Quinlan. 1992. Learning with continuous classes.
In 5th Australian Joint Conference on Artificial Intel-
ligence, pages 343?348, Singapore. World Scientific.
C. Quirk. 2004. Training a sentence-level machine trans-
lation confidence measure. In Proceedings of LREC,
Lisbon, June.
R. Rubino and G. Linare`s. 2011. A multi-view approach
for term translation spotting. Computational Linguis-
tics and Intelligent Text Processing, 6609:29?40.
H. Schmidt. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Natural Lan-
guage Processing.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas, pages
223?231.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In InterSpeech, volume 2, pages 901?
904.
Y.C. Tam, I. Lane, and T. Schultz. 2007. Bilingual
lsa-based adaptation for statistical machine translation.
Machine Translation, 21(4):187?207.
J. Wagner, J. Foster, and J. van Genabith. 2007. A com-
parative evaluation of deep and shallow approaches to
the automatic detection of common grammatical er-
rors. In Proceedings of EMNLP-CoNLL, pages 112?
121, Prague, Czech Republic, June.
J. Wagner, J. Foster, and J. van Genabith. 2009. Judg-
ing grammaticality: Experiments in sentence classifi-
cation. CALICO Journal, 26(3):474?490.
J. Wagner. 2012. Detecting grammatical errors with
treebank-induced probabilistic parsers. Ph.D. thesis,
Dublin City University.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
144
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 49?58,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Sentiment Analysis of Political Tweets: Towards an Accurate Classifier
Akshat Bakliwal1, Jennifer Foster2, Jennifer van der Puil3?,
Ron O?Brien4, Lamia Tounsi2 and Mark Hughes5
1Search and Information Extraction Lab, IIIT-Hyderabad, India
2NCLT/CNGL, School of Computing, Dublin City University, Ireland
3Department of Computer Science and Statistics, Trinity College, Ireland
4Quiddity, Dublin, Ireland
5CLARITY, School of Computing, Dublin City University, Ireland
1akshat.bakliwal@research.iiit.ac.in
2,5{jfoster,ltounsi,mhughes}@computing.dcu.ie
3jvanderp@tcd.ie
4ron@quiddity.ie
Abstract
We perform a series of 3-class sentiment clas-
sification experiments on a set of 2,624 tweets
produced during the run-up to the Irish Gen-
eral Elections in February 2011. Even though
tweets that have been labelled as sarcastic
have been omitted from this set, it still rep-
resents a difficult test set and the highest
accuracy we achieve is 61.6% using super-
vised learning and a feature set consisting
of subjectivity-lexicon-based scores, Twitter-
specific features and the top 1,000 most dis-
criminative words. This is superior to various
naive unsupervised approaches which use sub-
jectivity lexicons to compute an overall senti-
ment score for a <tweet,political party> pair.
1 Introduction
Supervised machine learning using minimal feature
engineering has been shown to work well in binary
positive/negative sentiment classification tasks on
well-behaved datasets such as movie reviews (Pang
et al, 2002). In this paper we describe sentiment
analysis experiments in a more complicated setup:
the task is three-class positive/negative/neutral clas-
sification, the sentiment being classified is not at the
general document level but rather directed towards a
topic, the documents are tweets, and the topic is poli-
tics, specifically the Irish General Election of Febru-
ary 2011.
?Akshat Bakliwal and Jennifer van der Puil carried out their
part of this work while employed as summer interns at the Cen-
tre for Next Generation Localisation(CNGL) in the School of
Computing, DCU.
The dataset used in the experiments contains
tweets which were collected in the run up to the elec-
tion and which were subsequently doubly annotated
as positive, negative or neutral towards a particular
political party or party leader. The annotators also
marked a tweet as sarcastic if its literal sentiment
was different to its actual sentiment. Before explor-
ing the thorny issue of sentiment classification in the
face of sarcasm, we simplify the problem by first try-
ing to establish some sentiment analysis baselines
for those tweets which were not deemed to be sar-
castic.
We first explore a naive approach in which a sub-
jectivity lexicon is used as the primary source of in-
formation in determining whether sentiment towards
a political party or party leader is positive, negative
or neutral. The best version of this method achieves
an accuracy of 58.9, an absolute improvement of 4.9
points over the majority baseline (54%) in which all
tweets are classified as neutral. When these lexi-
con scores are combined with bag-of-word features
and some Twitter-specific features in a supervised
machine learning setup, this accuracy increases to
61.6%.
The paper is organised as follows: related work
is described in Section 2, followed by a brief dis-
cussion of the 2011 Irish General Election in Sec-
tion 3, a description of the dataset in Section 4
and a description of the natural language processing
tools and resources employed in Section 5. In Sec-
tion 6, the unsupervised lexicon-based approach is
presented and its limitations discussed. Section 7 de-
scribes the machine-learning-based experiments and
Section 8 concludes and provides hints towards fu-
49
ture work with this new dataset.
2 Previous Work
The related work can be divided into two groups,
general sentiment analysis research and research
which is devoted specifically to the political domain.
2.1 General Sentiment Analysis
Research in the area of sentiment mining started
with product (Turney, 2002) and movie (Pang et al,
2002) reviews. Turney (2002) used Pointwise Mu-
tual Information (PMI) to estimate the sentiment ori-
entation of phrases. Pang et al (2002) employed
supervised learning with various set of n-gram fea-
tures, achieving an accuracy of almost 83% with un-
igram presence features on the task of document-
level binary sentiment classification. Research on
other domains and genres including blogs (Chesley,
2006) and news (Godbole et al, 2007) followed.
Early sentiment analysis research focused on
longer documents such as movie reviews and blogs.
Microtext on the other hand restricts the writer to a
more concise expression of opinion. Smeaton and
Bermingham (2010) tested the hypothesis that it is
easier to classify sentiment in microtext as compared
to longer documents. They experimented with mi-
crotext from Twitter, microreviews from blippr, blog
posts and movie reviews and concluded that it is eas-
ier to identify sentiment from microtext. However,
as they move from contextually sparse unigrams to
higher n-grams, it becomes more difficult to improve
the performance of microtext sentiment classifica-
tion, whereas higher-order information makes it eas-
ier to perform classification of longer documents.
There has been some research on the use of pos-
itive and negative emoticons and hashtags in tweets
as a proxy for sentiment labels (Go et al, 2009; Pak
and Paroubek, 2010; Davidov et al, 2010; Bora,
2012). Bakliwal et al (2012) emphasized the im-
portance of preprocessing and proposed a set of
features to extract maximum sentiment information
from tweets. They used unigram and bigram fea-
tures along with features which are more associated
with tweets such as emoticons, hashtags, URLs, etc.
and showed that combining linguistic and Twitter-
specific features can boost the classification accu-
racy.
2.2 Political Sentiment Analysis
In recent years, there has been growing interest in
mining online political sentiment in order to pre-
dict the outcome of elections. One of the most in-
fluential papers is that of Tumasjan et al (2010)
who focused on the 2009 German federal election
and investigated whether Twitter can be used to pre-
dict election outcomes. Over one hundred thousand
tweets dating from August 13 to September 19, 2009
containing the names of the six parties represented
in the German parliament were collected. LIWC
2007 (Pennebaker et al, 2007) was then used to ex-
tract sentiment from the tweets. LIWC is a text anal-
ysis software developed to assess emotional, cog-
nitive and structural components of text samples
using a psychometrically validated internal dictio-
nary. Tumasjan et al concluded that the number of
tweets/mentions of a party is directly proportional to
the probability of winning the elections.
O?Connor et al (2010) investigated the extent to
which public opinion polls were correlated with po-
litical sentiment expressed in tweets. Using the Sub-
jectivity Lexicon (Wilson et al, 2005), they estimate
the daily sentiment scores for each entity. A tweet is
defined as positive if it contains a positive word and
vice versa. A sentiment score for that day is calcu-
lated as the ratio of the positive count over the neg-
ative count. They find that their sentiment scores
were correlated with opinion polls on presidential
job approval but less strongly with polls on electoral
outcome.
Choy et al (2011) discuss the application of on-
line sentiment detection to predict the vote percent-
age for each of the candidates in the Singapore pres-
idential election of 2011. They devise a formula to
calculate the percentage vote each candidate will re-
ceive using census information on variables such as
age group, sex, location, etc. They combine this
with a sentiment-lexicon-based sentiment analysis
engine which calculates the sentiment in each tweet
and aggregates the positive and negative sentiment
for each candidate. Their model was able to predict
the narrow margin between the top two candidates
but failed to predict the correct winner.
Wang et al (2012) proposed a real-time sentiment
analysis system for political tweets which was based
on the U.S. presidential election of 2012. They col-
50
lected over 36 million tweets and collected the sen-
timent annotations using Amazon Mechanical Turk.
Using a Naive Bayes model with unigram features,
their system achieved 59% accuracy on the four-
category classification.
Bermingham and Smeaton (2011) are also con-
cerned with predicting electoral outcome, in partic-
ular, the outcome of the Irish General Election of
2011 (the same election that we focused on). They
analyse political sentiment in tweets by means of su-
pervised classification with unigram features and an
annotated dataset different to and larger than the one
we present, achieving 65% accuracy on the task of
positive/negative/neutral classification. They con-
clude that volume is a stronger indicator of election
outcome than sentiment, but that sentiment still has
a role to play.
Gayo-Avello (2012) calls into question the use of
Twitter for election outcome prediction. Previous
works which report positive results on this task using
data from Twitter are surveyed and shortcomings in
their methodology and/or assumptions noted. In this
paper, our focus is not the (non-) predictive nature of
political tweets but rather the accurate identification
of any sentiment expressed in the tweets. If the ac-
curacy of sentiment analysis of political tweets can
be improved (or its limitations at least better under-
stood) then this will likely have a positive effect on
its usefulness as an alternative or complement to tra-
ditional opinion polling.
3 #ge11: The Irish General Election 2011
The Irish general elections were held on February
25, 2011. 165 representatives were elected across 43
constituencies for the Da?il, the main house of parlia-
ment. Eight parties nominated their candidates for
election and a coalition (Fine Gael and Labour) gov-
ernment was formed. The parties in the outgoing
coalition government, Fianna Fa?il and the Greens,
suffered disastrous defeats, the worst defeat of a sit-
ting government since the foundatation of the State
in 1922.
Gallagher and Marsh (2011, chapter 5) discuss the
use of social media by parties, candidates and vot-
ers in the 2011 election and conclude that it had a
much more important role to play in this election
than in the previous one in 2007. On the role of Twit-
ter in particular, they report that ?Twitter was less
widespread among candidates [than Facebook], but
it offered the most diverse source of citizen coverage
during the election, and it has been integrated into
several mainstream media?. They estimated that 7%
of the Irish population had a Twitter account at the
time of the election.
4 Dataset
We compiled a corpus of tweets using the Twitter
search API between 20th and the 25th of January
2011 (one month before the election). We selected
the main political entities (the five biggest politi-
cal parties ? Fianna Fa?il, Fine Gael, Labour, Sinn
Fe?in and the Greens ? and their leaders) and per-
form query-based search to collect the tweets relat-
ing to these entities. The resulting dataset contains
7,916 tweets of which 4,710 are retweets or dupli-
cates, leaving a total of 3,206 tweets.
The tweets were annotated by two Irish annota-
tors with a knowledge of the Irish political land-
scape. Disagreements between the two annotators
were studied and resolved by a third annotator. The
annotators were asked to identify the sentiment as-
sociated with the topic (or entity) of the tweet. An-
notation was performed using the following 6 labels:
? pos: Tweets which carry positive sentiment to-
wards the topic
? neg: Tweets which carry negative sentiment to-
wards the topic
? mix: Tweets which carry both positive and neg-
ative sentiment towards the topic
? neu: Tweets which do not carry any sentiment
towards the topic
? nen: Tweets which were written in languages
other than English.
? non: Tweets which do not have any mention
or relation to the topic. These represent search
errors.
In addition to the above six classes, annotators were
asked to flag whether a tweet was sarcastic.
The dataset which we use for the experiments
described in this paper contains only those tweets
51
Positive Tweets 256 9.75%
Negative Tweets 950 36.22%
Neutral Tweets 1418 54.03%
Total Tweets 2624
Table 1: Class Distribution
that have been labelled as either positive, negative
or neutral, i.e. non-relevant, mixed-sentiment and
non-English tweets are discarded. We also simplify
our task by omitting those tweets which have been
flagged as sarcastic by one or both of the annotators,
leaving a set of 2,624 tweets with a class distribution
as shown in Table 1.
5 Tools and Resources
In the course of our experiments, we use two differ-
ent subjectivity lexicons, one part-of-speech tagger
and one parser. For part-of-speech tagging we use
a tagger (Gimpel et al, 2011) designed specifically
for tweets. For parsing, we use the Stanford parser
(Klein and Manning, 2003). To identify the senti-
ment polarity of a word we use:
1. Subjectivity Lexicon (SL) (Wilson et al,
2005): This lexicon contains 8,221 words
(6,878 unique forms) of which 3,249 are adjec-
tives, 330 are adverbs, 1,325 are verbs, 2,170
are nouns and remaining (1,147) words are
marked as anypos. There are many words
which occur with two or more different part-of-
speech tags. We extend SL with 341 domain-
specific words to produce an extended SL.
2. SentiWordNet 3.0 (SWN) (Baccianella et al,
2010): With over 100+ thousand words, SWN
is far larger than SL but is likely to be noisier
since it has been built semi-automatically. Each
word in the lexicon is associated with both a
positive and negative score, and an objective
score given by (1), i.e. the positive, negative
and objective score sum to 1.
ObjScore = 1?PosScore?NegScore (1)
6 Naive Lexicon-based Classification
In this section we describe a naive approach to sen-
timent classification which does not make use of la-
belled training data but rather uses the information
in a sentiment lexicon to deduce the sentiment ori-
entation towards a political party in a tweet (see
Liu (2010) for an overview of this unsupervised
lexicon-based approach). In Section 6.1, we present
the basic method along with some variants which
improve on the basic method by making use of infor-
mation about part-of-speech, negation and distance
from the topic. In Section 6.2, we examine some
of the cases which remain misclassified by our best
lexicon-based method. In Section 6.3, we discuss
briefly those tweets that have been labelled as sar-
castic.
6.1 Method and Results
Our baseline lexicon-based approach is as follows:
we look up each word in our sentiment lexicon and
sum up the scores to corresponding scalars. The re-
sults are shown in Table 2. Note that the most likely
estimated class prediction is neutral with a probabil-
ity of .5403 (1418/2624).
6.1.1 Which Subjectivity Lexicon?
The first column shows the results that we obtain
when the lexicon we use is our extended version of
the SL lexicon. The results in the second column
are those that result from using SWN. In the third
column, we combine the two lexicons. We define
a combination pattern of Extended-SL and SWN in
which we prioritize Extended-SL because it is man-
ually checked and some domain-specific words are
added. For the words which were missing from
Extended-SL (SWN), we assign them the polarity of
SWN (Extended-SL). Table 3 explains exactly how
the scores from the two lexicons are combined. Al-
though SWN slightly outperforms Extended-SL for
the baseline lexicon-based approach (first row of Ta-
ble 2), it is outperformed by Extended-SL and the
combinaton of the two lexicons for all the variants.
We can conclude from the full set of results in Ta-
ble 2 that SWN is less useful than Extended-SL or
the combination of SWN and Extended-SL.
6.1.2 Filtering by Part-of-Speech
The results in the first row of Table 2 represent
our baseline experiment in which each word in the
tweet is looked up in the sentiment lexicon and
its sentiment score added to a running total. We
achieve a classification accuracy of 52.44% with the
52
Method Extended-SL SWN Combined
3-Class Classification (Pos vs
Neg vs Neu)
Correct Accuracy Correct Accuracy Correct Accuracy
Baseline 1376 52.44% 1379 52.55% 1288 49.09%
Baseline + Adj 1457 55.53% 1449 55.22% 1445 55.07%
Baseline + Adj + S 1480 56.40% 1459 55.60% 1481 56.44%
Baseline + Adj + S + Neg 1495 56.97% 1462 55.72% 1496 57.01%
Baseline + Adj + S + Neg +
Phrases
1511 57.58% 1479 56.36% 1509 57.51%
Baseline + Adj + S + Neg +
Phrases + Than
1533 58.42% 1502 57.24% 1533 58.42%
Distance Based Scoring:
Baseline + Adj + S + Neg +
Phrases + Than
1545 58.88% 1506 57.39% 1547 58.96%
Sarcastic Tweets 87/344 25.29% 81/344 23.55% 87/344 25.29%
Table 2: 3-class classification using the naive lexicon-based approach. The majority baseline is 54.03%.
Extended-
SL
polarity
SWN
Polarity
Combination
Polarity
-1 -1 -2
-1 0 -1
-1 1 -1
0 -1 -0.5
0 0 0
0 1 0.5
1 -1 1
1 0 1
1 1 2
Table 3: Combination Scheme of extended-SL and SWN.
Here 0 represents either a neutral word or a word missing
from the lexicon.
Extended-SL lexicon. We speculate that this low
accuracy is occurring because too many words that
appear in the sentiment lexicon are included in the
overall sentiment score without actually contribut-
ing to the sentiment towards the topic. To refine our
approach one step further, we use part-of-speech in-
formation and consider only adjectives for the clas-
sification of tweets since adjectives are strong in-
dicators of sentiment (Hatzivassiloglou and Wiebe,
2000). We achieve an accuracy improvement of ap-
proximately three absolute points, and this improve-
ment holds true for both sentiment lexicons. This
supports our hypothesis that we are using irrelevant
information for classification in the baseline system.
Our next improvement (third row of Table 2)
comes from mapping all inflected forms to their
stems (using the Porter stemmer). Examples of in-
flected forms that are reduced to their stems are de-
lighted or delightful. Using stemming with adjec-
tives over the baseline, we achieve an accuracy of
56.40% with Extended-SL.
6.1.3 Negation
?Negation is a very common linguistic construc-
tion that affects polarity and, therefore, needs to
be taken into consideration in sentiment analysis?
(Councill et al, 2010). We perform negation han-
dling in tweets using two different approaches. In
the first approach, we first identify negation words
53
and reverse the polarity of sentiment-bearing words
within a window of three words. In the second ap-
proach, we try to resolve the scope of the negation
using syntactic parsing. The Stanford dependency
scheme (de Marneffe and Manning, 2008) has a spe-
cial relation (neg) to indicate negation. We reverse
the sentiment polarity of a word marked via the neg
relation as being in the scope of a negation. Using
the first approach, we see an improvement of 0.6%
in the classification accuracy with the Extended-SL
lexicon. Using the second approach, we see an
improvement of 0.5%. Since there appears to be
very little difference between the two approaches to
negation-handling and in order to reduce the compu-
tational burden of running the Stanford parser each
time to obtain the dependencies, we continue further
experiments with the first method only. Using base-
line + stemming + adjectives + neg we achieve an
accuracy of 56.97% with the Extended-SL lexicon.
6.1.4 Domain-specific idioms
In the context of political tweets we see many
sentiment-bearing idioms and fixed expressions, e.g.
god save us, X for Taoiseach1, wolf in sheep?s cloth-
ing, etc. In our study, we had a total of 89 phrases.
When we directly account for these phrases, we
achieve an accuracy of 57.58% (an absolute im-
provement of 0.6 points over the last step).
6.1.5 Comparative Expressions
Another form of expressing an opinion towards
an entity is by comparing the entity with some other
entity. For example consider the tweet:
Fast Food sounds like a better vote than Fianna Fail.
(2)
In this tweet, an indirect negative sentiment is ex-
pressed towards the political party Fianna Fa?il. In
order to take into account such constructions, the
following procedure is applied: we divide the tweet
into two parts, left and right. The left part contains
the text which comes before the than and the right
part contains the text which comes after than, e.g.
Tweet: ?X is better than Y?
Left: ?X is better?
Right: ?Y?.
1The term Taoiseach refers to the Irish Prime Minister.
We then use the following strategy to calculate the
polarity of the tweet oriented towards the entity:
S left = sentiment score of Left.
S right = sentiment score of Right.
Ent pos left = if entity is left of
?than?, then 1, otherwise ? 1.
Ent pos right = if entity is right of
?than?, then 1, otherwise ? 1.
S(tweet) = Ent pos left ? S left +
Ent pos right ? S right. (3)
So in (2) above the entity, Fianna Fa?il, is to the
right of than meaning that its Ent pos right value
is 1 and its Ent pos left value is -1. This has the
effect of flipping the polarity of the positive word
better. By including the ?than? comparison, we see
an improvement of absolute 0.8% (third last row of
Table 2).
6.1.6 Distance Scoring
To emphasize the topic-oriented nature of our sen-
timent classification, we also define a distance-based
scoring function where we define the overall score
of the tweet as given in (4). Here dis(word) is de-
fined as number of words between the topic (i.e. the
political entity) and the sentiment word.
S(tweet) =
n?
i=1
S(wordi)/dis(wordi). (4)
The addition of the distance information further en-
hanced our system accuracy by 0.45%, taking it to
58.88% (second last row of Table 2). Our highest
overall accuracy (58.96) is achieved in this setting
using the combined lexicon.
It should be noted that this lexicon-based ap-
proach is overfitting to our dataset since the list of
domain-specific phrases and the form of the com-
parative constructions have been obtained from the
dataset itself. This means that we are making a
strong assumption about the representativeness of
this dataset and accuracy on a held-out test set is
likely to be lower.
6.2 Error Analysis
In this section we discuss pitfalls of the naive
lexicon-based approach with the help of some exam-
ples (see Table 4). Consider the first example from
54
the table, @username and u believe people in fianna
fail . What are you a numbskull or a journalist ?
In this tweet, we see that negative sentiment is im-
parted by the question part of the tweet, but actually
there are no sentiment adjectives. The word numb-
skull is contributing to the sentiment but is tagged as
a noun and not as an adjective. This tweet is tagged
as negative by our annotators and as neutral by our
lexicon-based classifier.
Consider the second example from Table 4,
@username LOL . A guy called to our house tonight
selling GAA tickets . His first words were : I?m
not from Fianna Fail . This is misclassified because
there are no sentiment bearing words according to
the sentiment lexicon. The last tweet in the table rep-
resents another example of the same problem. Note
however that the emoticon :/ in the last tweet and the
web acronym LOL in the second tweet are providing
hints which our system is not making use of.
In the third example from Table 4, @username
Such scary words .? Sinn Fein could top the poll ?
in certain constituencies . I feel sick at the thought
of it . ? In this example, we have three sentiment
bearing words: scary, top and sick. Two of the three
words are negative and one word is positive. The
word scary is stemmed incorrectly as scari which
means that it is out of the scope of our lexicons.
If we just count the number of sentiment words re-
maining, then this tweet is labelled as neutral but ac-
tually is negative with respect to the party Sinn Fe?in.
We proposed the use of distance as a measure of re-
latedness to the topic and we observed a minor im-
provement in classification accuracy. However, for
this example, the distance-based approach does not
work. The word top is just two words away from the
topic and thus contributes the maximum, resulting in
the whole tweet being misclassified as positive.
6.3 Sarcastic Tweets
?Political discouse is plagued with humor, double
entendres, and sarcasm; this makes determining po-
litical preference of users hard and inferring voting
intention even harder.?(Gayo-Avello, 2012)
As part of the annotation process, annotators were
asked to indicate whether they thought a tweet ex-
hibited sarcasm. Some examples of tweets that were
annotated as sarcastic are shown in Table 5.
We made the decision to omit these tweets from
the main sentiment classification experiments under
the assumption that they constituted a special case
which would be better handled by a different clas-
sifier. This decision is vindicated by the results in
the last row of Table 2 which show what happens
when we apply our best classifier (Distance-based
Scoring: Baseline+Adj+S+Neg+Phrases+Than) to
the sarcastic tweets ? only a quarter of them are cor-
rectly classified. Even with a very large and highly
domain-tuned lexicon, the lexicon-based approach
on its own will struggle to be of use for cases such
as these, but the situation might be improved were
the lexicon to be used in conjunction with possible
sarcasm indicators such as exclamation marks.
7 Supervised Machine Learning
Although our dataset is small, we investigate
whether we can improve over the lexicon-based ap-
proach by using supervised machine learning. As
our learning algorithm, we employ support vector
machines in a 5-fold cross validation setup. The tool
we use is SVMLight (Joachims, 1999).
We explore two sets of features. The first are the
tried-and-tested unigram presence features which
have been used extensively not only in sentiment
analysis but in other text classification tasks. As we
have only 2,624 training samples, we performed fea-
ture selection by ranking the features using the Chi-
squared metric.
The second feature set consists of 25 features
which are inspired by the work on lexicon-based
classification described in the previous section.
These are the counts of positive, negative, objec-
tive words according to each of the three lexicons
and the corresponding sentiment scores for the over-
all tweets. In total there are 19 such features. We
also employ six Twitter-related presence features:
positive emoticons, negative emoticons, URLs, pos-
itive hashtags, negative hashtags and neutral hash-
tags. For further reference we call this second set of
features our ?hand-crafted? features.
The results are shown in Table 6. We can see
that using the hand-crafted features alone barely im-
proves over the majority baseline of 54.03 but it does
improve over our baseline lexicon-based approach
(see first row of Table 2). Encouragingly, we see
some benefit from using these features in conjunc-
55
Tweet Topic
Manual
Polar-
ity
Calculated
Polarity
Reason for
misclassifica-
tion
@username and u believe people in fianna fail .
What are you a numbskull or a journalist ?
Fianna
Fa?il
neg neu
Focus only on
adjectives
@username LOL . A guy called to our house
tonight selling GAA tickets . His first words were :
I?m not from Fianna Fail .
Fianna
Fa?il
neg neu
No sentiment
words
@username Such scary words .? Sinn Fein could
top the poll ? in certain constituencies . I feel sick
at the thought of it .
Sinn
Fe?in
neg pos
Stemming
and word
distance order
@username more RTE censorship . Why are they
so afraid to let Sinn Fein put their position across .
Certainly couldn?t be worse than ff
Sinn
Fe?in
pos neg
contribution
of afraid
Based on this programme the winners will be Sinn
Fein & Gilmore for not being there #rtefl
Sinn
Fe?in
pos neu
Focus only on
adjectives
#thefrontline pearce Doherty is a spoofer ! Vote
sinn fein and we loose more jobs
Sinn
Fe?in
neg pos
Focus only on
adjectives &
contribution
of phrase Vote
X
@username Tread carefully Conor . BNP
endorsing Sinn Fin etc . etc .
Sinn
Fe?in
neg neu
No sentiment
words
@username ah dude . You made me go to the fine
gael web site ! :/
Fine
Gael
neg neu
No sentiment
words
Table 4: Misclassification Examples
Feature Set # Features Accuracy
# samples = 2624 SVM Light
Hand-crafted 25 54.76
Unigram
7418 55.22
Top 1000 58.92
Top 100 56.86
Unigram + Hand-crafted
7444 54.73
Top 1000 61.62
Top 100 59.53
Table 6: Results of 3-Class Classification using Super-
vised Machine Learning
tion with the unigram features. Our best overall re-
sult of 61.62% is achieved by using the Top 1000 un-
igram features together with these hand-crafted fea-
tures. This result seems to suggest that, even with
only a few thousand training instances, employing
supervised machine learning is still worthwhile.
8 Conclusion
We have introduced a new dataset of political tweets
which will be made available for use by other re-
searchers. Each tweet in this set has been annotated
for sentiment towards a political entity, as well as
for the presence of sarcasm. Omitting the sarcastic
tweets from our experiments, we show that we can
classify a tweet as being positive, negative or neutral
towards a particular political party or party leader
with an accuracy of almost 59% using a simple ap-
proach based on lexicon lookup. This improves over
the majority baseline by almost 5 absolute percent-
age points but as the classifier uses information from
the test set itself, the result is likely to be lower on
a held-out test set. The accuracy increases slightly
when the lexicon-based information is encoded as
features and employed together with bag-of-word
features in a supervised machine learning setup.
Future work involves carrying out further exper-
56
Sarcastic Tweets
Ah bless Brian Cowen?s little cotton socks! He?s staying on as leader of FF because its better for the
country. How selfless!
So now Brian Cowen is now Minister for foreign affairs and Taoiseach? Thats exactly what he needs
more responsibilities http://bbc.in/hJI0hb
Mary Harney is going. Surprise surprise! Brian Cowen is going to be extremely busy with all these
portfolios to administer. Super hero!
Now in its darkest hour Fianna Fail needs. . . Ivor!
Labour and Fine Gael have brought the election forward by 16 days Crisis over Ireland is SAVED!! #vinb
@username Maybe one of those nice Sinn Fein issue boiler suits? #rtefl
I WILL vote for Fine Gael if they pledge to dress James O?Reilly as a leprechaun and send him
to the White House for Paddy?s Day.
Table 5: Examples of tweets which have been flagged as sarcastic
iments on those tweets that have been annotated as
sarcastic, exploring the use of syntactic dependency
paths in the computation of distance between a word
and the topic, examining the role of training set class
bias on the supervised machine learning results and
exploring the use of distant supervision to obtain
more training data for this domain.
Acknowledgements
Thanks to Emmet O Briain, Lesley Ni Bhriain and
the anonymous reviewers for their helpful com-
ments. This research has been supported by En-
terprise Ireland (CFTD/2007/229) and by Science
Foundation Ireland (Grant 07/CE/ I1142) as part of
the CNGL (www.cngl.ie) at the School of Comput-
ing, DCU.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10).
Akshat Bakliwal, Piyush Arora, Senthil Madhappan,
Nikhil Kapre, Mukesh Singh, and Vasudeva Varma.
2012. Mining sentiments from tweets. In Proceedings
of the WASSA?12 in conjunction with ACL?12.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Proceedings of the 19th ACM international
conference on Information and Knowledge Manage-
ment.
Adam Bermingham and Alan Smeaton. 2011. On using
Twitter to monitor political sentiment and predict elec-
tion results. In Proceedings of the Workshop on Sen-
timent Analysis where AI meets Psychology (SAAIP
2011).
Nibir Nayan Bora. 2012. Summarizing public opinions
in tweets. In Journal Proceedings of CICLing 2012.
Paula Chesley. 2006. Using verbs and adjectives to au-
tomatically classify blog sentiment. In Proceedings
of AAAI-CAAW-06, the Spring Symposia on Computa-
tional Approaches.
Murphy Choy, Michelle L. F. Cheong, Ma Nang Laik,
and Koo Ping Shung. 2011. A sentiment analysis
of Singapore Presidential Election 2011 using Twitter
data with census correction. CoRR, abs/1108.5520.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, NeSp-NLP ?10.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation.
Michael Gallagher and Michael Marsh. 2011. How Ire-
land Voted 2011: The Full Story of Ireland?s Earth-
quake Election. Palgrave Macmillan.
Daniel Gayo-Avello. 2012. ?I wanted to predict elec-
tions with Twitter and all I got was this lousy paper?.
57
A balanced survey on election prediction using Twitter
data. CoRR, abs/1204.6441.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter
sentiment classification using distant supervision. In
CS224N Project Report, Stanford University.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM).
Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000.
Effects of adjective orientation and gradability on sen-
tence subjectivity. In Proceedings of COLING.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making large-scale support vector ma-
chine learning practical, pages 169?184. MIT Press,
Cambridge, MA, USA.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1.
Bing Liu. 2010. Handbook of natural language pro-
cessing. chapter Sentiment Analysis and Subjectivity.
Chapman and Hall.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
tweets to polls: Linking text sentiment to public opin-
ion time series. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM).
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the con-
ference on Empirical Methods in Natural Language
Processing - Volume 10.
James W. Pennebaker, Cindy K. Chung, Molly Ireland,
Amy Gonzales, and Roger J. Booth. 2007. The de-
velopment and psychometric properties of liwc2007.
Technical report, Austin,Texas.
Andranik Tumasjan, Timm Oliver Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting elec-
tions with Twitter: What 140 characters reveal about
political sentiment. In Proceedings of the Interna-
tional Conference on Weblogs and Social Media.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics.
Hao Wang, Dogan Can, Abe Kazemzadeh, Franc?ois Bar,
and Shrikanth Narayanan. 2012. A system for real-
time Twitter sentiment analysis of 2012 U.S. presiden-
tial election cycle. In ACL (System Demonstrations).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing.
58
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392?397,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
DCU-Symantec at the WMT 2013 Quality Estimation Shared Task
Raphael Rubino??, Joachim Wagner??, Jennifer Foster?,
Johann Roturier? Rasoul Samad Zadeh Kaljahi?? and Fred Hollowood?
?NCLT, School of Computing, Dublin City University, Ireland
?Center for Next Generation Localisation, Dublin, Ireland
?Symantec Research Labs, Dublin, Ireland
?{rrubino, jwagner, jfoster}@computing.dcu.ie
?{johann roturier, fhollowood}@symantec.com
Abstract
We describe the two systems submit-
ted by the DCU-Symantec team to Task
1.1. of the WMT 2013 Shared Task on
Quality Estimation for Machine Transla-
tion. Task 1.1 involve estimating post-
editing effort for English-Spanish trans-
lation pairs in the news domain. The
two systems use a wide variety of fea-
tures, of which the most effective are the
word-alignment, n-gram frequency, lan-
guage model, POS-tag-based and pseudo-
references ones. Both systems perform at
a similarly high level in the two tasks of
scoring and ranking translations, although
there is some evidence that the systems are
over-fitting to the training data.
1 Introduction
The WMT 2013 Quality Estimation Shared Task
involve both sentence-level and word-level qual-
ity estimation (QE). The sentence-level task con-
sist of three subtasks: scoring and ranking transla-
tions with regard to post-editing effort (Task 1.1),
selecting among several translations produced by
multiple MT systems for the same source sentence
(Task 1.2), and predicting post-editing time (Task
1.3). The DCU-Symantec team enter two systems
to Task 1.1. Given a set of source English news
sentences and their Spanish translations, the goals
are to predict the HTER score of each translation
and to produce a ranking based on HTER for the
set of translations. A set of 2,254 sentence pairs
are provided for training.
On the ranking task, our system DCU-SYMC
alltypes is second placed out of thirteen sys-
tems and our system DCU-SYMC combine is
ranked fifth, according to the Delta Average met-
ric. According to the Spearman rank correlation,
our systems are the joint-highest systems. In the
scoring task, the DCU-SYMC alltypes system
is placed sixth out of seventeen systems accord-
ing to Mean Absolute Error (MAE) and third ac-
cording to Root Mean Squared Error (RMSE). The
DCU-SYMC combine system is placed fifth ac-
cording to MAE and second according to RMSE.
In this system description paper, we describe the
features, the learning methods used, the results for
the two submitted systems and some other systems
we experiment with.
2 Features
Our starting point for the WMT13 QE shared task
was the feature set used in the system we submit-
ted to the WMT12 QE task (Rubino et al, 2012).
This feature set, comprising 308 features in to-
tal, extended the 17 baseline features provided by
the task organisers to include 6 additional sur-
face features, 6 additional language model fea-
tures, 17 additional features derived from the
MT system components and the n-best lists, 138
features obtained by part-of-speech tagging and
parsing the source sentences and 95 obtained by
part-of-speech tagging the target sentences, 21
topic model features, 2 features produced by a
grammar checker1 and 6 pseudo-source (or back-
translation) features.
We made the following modifications to this
2012 feature set:
? The pseudo-source (or back-translation) fea-
tures were removed, as they did not con-
tribute useful information to our system last
year.
? The language model and n-gram frequency
feature sets were extended in order to cover
1 to 5 gram sequences, as well as source and
target ratios for these feature values.
? The word-alignment feature set was also
extended by considering several thresholds
1http://www.languagetool.org/
392
when counting the number of target words
aligned with source words.
? We extracted 8 additional features from the
decoder log file, including the number of dis-
carded hypotheses, the total number of trans-
lation options and the number of nodes in the
decoding graph.
? The set of topic model features was reduced
in order to keep only those that were shown
to be effective on three quality estimation
datasets (the details can be found in (Rubino
et al (to appear), 2013)). These features en-
code the difference between source and target
topic distributions according to several dis-
tance/divergence metrics.
? Following Soricut et al (2012), we employed
pseudo-reference features. The source sen-
tences were translated with three different
MT systems: an in-house phrase-based SMT
system built using Moses (Koehn et al,
2007) and trained on the parallel data pro-
vided by the organisers, the rule-based sys-
tem Systran2 and the online, publicly avail-
able, Bing Translator3. The obtained trans-
lations are compared to the target sentences
using sentence-level BLEU (Papineni et al,
2002), TER (Snover et al, 2006) and the Lev-
enshtein distance (Levenshtein, 1966).
? Also following Soricut et al (2012), one-
to-one word-alignments, with and without
Part-Of-Speech (POS) agreement, were in-
cluded as features. Using the alignment in-
formation provided by the decoder, we POS
tagged the source and target sentences with
TreeTagger (Schmidt, 1994) and the publicly
available pre-trained models for English and
Spanish. We mapped the tagsets of both lan-
guages by simplifying the initial tags and ob-
tain a reduced set of 8 tags. We applied that
simplification on the tagged sentences before
checking for POS agreement.
3 Machine Learning
In this section, we describe the learning algo-
rithms and feature selection used in our experi-
ments, leading to the two submitted systems for
the shared task.
2Systran Enterprise Server version 6
3http://www.bing.com/translator
3.1 Primary Learning Method
To estimate the post-editing effort of translated
sentences, we rely on regression models built us-
ing the Support Vector Machine (SVM) algorithm
for regression -SVR, implemented in the LIB-
SVM toolkit (Chang and Lin, 2011). To build
our final regression models, we optimise SVM
hyper-parameters (C, ? and ) using a grid-search
method with 5-fold cross-validation for each pa-
rameter triplet. The parameters leading to the best
MAE, RMSE and Pearson?s correlation coefficient
(r) are kept to build the model.
3.2 Feature Selection on Feature Types
In order to reduce the feature and obtain more
compact models, we apply feature selection on
each of our 15 feature types. Examples of feature
types are language model features or topic model
features. For each feature type, we apply a feature
subset evaluation method based on the wrapper
paradigm and using the best-first search algorithm
to explore the feature space. The M5P (Wang
and Witten, 1997) regression tree algorithm im-
plemented in the Weka toolkit (Hall et al, 2009)
is used with default parameters to train and eval-
uate a regression model for each feature subset
obtained with best-first search. A 10-fold cross-
validation is performed for each subset and we
keep the features leading to the best RMSE. We
use M5P regression trees instead of -SVR be-
cause grid-search with the latter is too computa-
tionally expensive to be applied so many times.
Using feature selection in this way, we obtain 15
reduced feature sets that we combine to form the
DCU-SYMC alltypes system, containing 102
features detailed in Table 1.
3.3 Feature Binarisation
In order to aid the SVM learner, we also experi-
ment with binarising our feature set, i.e. convert-
ing our features with various feature value ranges
into features whose values are either 1 or 0. Again,
we employ regression tree learning. We train
regression trees with M5P and M5P-R4 (imple-
mented in the Weka toolkit) and create a binary
feature for each regression rule found in the trees
(ignoring the leaf nodes). For example, a binary
feature indicating whether the Bing TER score is
less than or equal to 55.685 is derived from the
4We experiment with J48 decision trees as well, but this
method did not outperform regression tree methods.
393
Backward LM
Source 1-gram perplexity.
Source & target 1-grams perplexity ratio.
Source & target 3-grams and 4-gram perplexity ratio.
Target Syntax
Frequency of tags: ADV, FS, DM, VLinf, VMinf, semicolon, VLger, NC, PDEL, VEfin, CC, CCNEG, PPx, ART, SYM,
CODE, PREP, SE and number of ambiguous tags
Frequency of least frequent POS 3-gram observed in a corpus.
Frequency of least frequent POS 4-gram and 6-gram with sentence padding (start and end of sentence tags) observed in a
corpus.
Source Syntax
Features from three probabilistic parsers. (Rubino et al, 2012).
Frequency of least frequent POS 2-gram, 4-gram and 9-gram with sentence padding observed in a corpus.
Number of analyses found and number of words, using a Lexical Functional Grammar of English as described in Rubino
et al (2012).
LM
Source unigram perplexity.
Target 3-gram and 4-gram perplexity with sentence padding.
Source & target 1-gram and 5-gram perplexity ratio.
Source & target unigram log-probability.
Decoder
Component scores during decoding.
Number of phrases in the best translation.
Number of translation options.
N -gram Frequency
Target 2-gram in second and third frequency quartiles.
Target 3-gram and 5-gram in low frequency quartiles.
Number of target 1-gram seen in a corpus.
Source & target 1-grams in highest and second highest frequency quartile.
One-to-One Word-Alignment
Count of O2O word alignment, weighted by target sentence length.
Count of O2O word alignment with POS agreement, weighted by count of O2O, by source length, by target length.
Pseudo-Reference
Moses translation TER score.
Bing translation number of words and TER score.
Systran sBLEU, number of substitutions and TER score.
Surface
Source number of punctuation marks and average words occurrence in source sentence.
Target number of punctuation marks, uppercased letters and binary value if the last character of the sentence is a punctuation
mark.
Ratio of source and target sentence lengths, average word length and number of punctuation marks over sentence lengths.
Topic Model
Cosine distance between source and target topic distributions.
Jensen-Shannon divergence between source and target topic distributions.
Word Alignment
Averaged number of source words aligned per target words with p(s|t) thresholds: 1.0, 0.75, 0.5, 0.25, 0.01
Averaged number of source words aligned per target words with p(s|t) = 0.01 weighted by target words frequency
Averaged number of target words aligned per source word with p(t|s) = 0.01 weighted by source words frequency
Ratio of source and target averaged aligned words with thresholds: 1.0 and 0.1, and with threshold: 0.75, 0.5, 0.25 weighted
by words frequency
Table 1: Features selected with the wrapper approach using best-first search and M5P. These features are
included in the submitted system alltypes.
394
Feature to which threshold t is applied t (?)
Target 1-gram backward LM log-prob. ?35.973
Target 3-gram backward LM perplexity 7144.99
Probabilistic parsing feature 3.756
Probabilistic parsing feature 57.5
Frequency of least frequent POS 6-gram 0.5
Source 3-gram LM log-prob. 65.286
Source 4-gram LM perplexity with padding 306.362
Target 2-gram LM perplexity 176.431
Target 4-gram LM perplexity 426.023
Target 4-gram LM perplexity with padding 341.801
Target 5-gram LM perplexity 112.908
Ratio src&trg 5-gram LM log-prob. 1.186
MT system component score ?50
MT system component score ?0.801
Source 2-gram frequency in low quartile 0.146
Ratio src&trg 2-gram in high freq. quartile 0.818
Ratio src&trg 3-gram in high freq. quartile 0.482
O2O word alignment 15.5
Pseudo-ref. Moses Levenshtein 19
Pseudo-ref. Moses TER 21.286
Pseudo-ref. Bing TER 16.905
Pseudo-ref. Bing TER 23.431
Pseudo-ref. Bing TER 37.394
Pseudo-ref. Bing TER 55.685
Pseudo-ref. Systran sBLEU 0.334
Pseudo-ref. Systran TER 36.399
Source average word length 4.298
Target uppercased/lowercased letters ratio 0.011
Ratio src&trg average word length 1.051
Source word align., p(s|t) > 0.75 11.374
Source word align., p(s|t) > 0.1 485.062
Source word align., p(s|t) > 0.75 weighted 0.002
Target word align., p(t|s) > 0.01 weighted 0.019
Word align. ratio p > 0.25 weighted 1.32
Table 2: Features selected with the M5P-R M50
binarisation approach. For each feature, the cor-
responding rule indicates the binary feature value.
These features are included in the submitted sys-
tem combine in addition to the features presented
in Table 1.
regression rule Bing TER score ? 55.685.
The primary motivation for using regression
tree learning in this way was to provide a quick
and convenient method for binarising our feature
set. However, we can also perform feature selec-
tion using this method by experimenting with vari-
ous minimum leaf sizes (Weka parameter M ). We
plot the performance of the M5P and M5P-R (opti-
mising towards correlation) over the parameter M
and select the best three values of M . To experi-
ment with the effect of smaller and larger feature
sets, we further include parameters of M that (a)
lead to an approximately 50% bigger feature set
and (b) to an approximately 50% smaller feature
set.
Our DCU-SYMC combine system was built
by combining the DCU-SYMC alltypes fea-
ture set, reduced using the best-first M5P wrap-
per approach as described in subsection 3.2, with
a binarised set produced using an M5P regres-
sion tree with a minimum of 50 nodes per leaf.
This latter configuration, containing 34 features
detailed in Table 2, was selected according to the
evaluation scores obtained during cross-validation
on the training set using -SVR, as described in
the next section. Finally, we run a greedy back-
ward feature selection algorithm wrapping -SVR
on both DCU-SYMC alltypes and DCU-SYMC
combine in order to optimise our feature sets for
the SVR learning algorithm, removing 6 and 2 fea-
tures respectively.
4 System Evaluation and Results
In this section, we present the results obtained with
-SVR during 5-fold cross-validation on the train-
ing set and the final results obtained on the test
set. We selected two systems to submit amongst
the different configurations based on MAE, RMSE
and r. As several systems reach the same perfor-
mance according to these metrics, we use the num-
ber of support vectors (noted SV) as an indicator
of training data over-fitting. We report the results
obtained with some of our systems in Table 3.
The results show that the submitted sys-
tems DCU-SYMC alltypes and DCU-SYMC
combine lead to the best scores on cross-
validation, but they do not outperform the system
combining the 15 feature types without feature se-
lection (15 types). This system reaches the best
scores on the test set compared to all our systems
built on reduced feature sets. This indicates that
we over-fit and fail to generalise from the training
data.
Amongst the systems built using reduced fea-
ture sets, the M5P-R M80 system, based on the
tree binarisation approach using M5P-R, yields
the best results on the test set on 3 out of 4 offi-
cial metrics. These results indicate that this sys-
tem, trained on 16 features only, tends to estimate
HTER scores more accurately on the unseen test
data. The results of the two systems based on
the M5P-R binarisation method are the best com-
pared to all the other systems presented in this
Section. This feature binarisation and selection
method leads to robust systems with few features:
31 and 16 for M5P-R M50 and M5P-R M80 re-
spectively. Even though these systems do not lead
to the best results, they outperform the two sub-
mitted systems on one metric used to evaluate the
395
Cross-Validation Test
System nb feat MAE RMSE r SV MAE RMSE DeltaAvg Spearman
15 types 442 0.106 0.138 0.604 1194.6 0.126 0.156 0.108 0.625
M5P M50 34 0.106 0.138 0.600 1417.8 0.135 0.167 0.102 0.586
M5P M130 4 0.114 0.145 0.544 750.6 0.142 0.173 0.079 0.517
M5P-R M50 31 0.106 0.137 0.610 655.4 0.135 0.166 0.100 0.591
M5P-R M80 16 0.107 0.139 0.597 570.6 0.134 0.165 0.106 0.597
alltypes? 96 0.104 0.135 0.624 1130.6 0.135 0.171 0.101 0.589
combine? 134 0.104 0.134 0.629 689.8 0.134 0.166 0.098 0.588
Table 3: Results obtained with different regression models, during cross-validation on the training set
and on the test set, depending on the feature selection method. Systems marked with ? were submitted
for the shared task.
scoring task and two metrics to evaluate the rank-
ing task.
On the systems built using reduced feature sets,
we observe a difference of approximately 0.03pt
absolute between the MAE and RMSE scores ob-
tained during cross-validation and those on the test
set. Such a difference can be related to train-
ing data over-fitting, even though the feature sets
obtained with the tree binarisation methods are
small. For instance, the system M5P M130 is
trained on 4 features only, but the difference be-
tween cross-validation and test MAE scores is
similar to the other systems. We see on the fi-
nal results that our feature selection methods is an
over-fitting factor: by selecting the features which
explain well the training set, the final model tends
to generalise less. The selected features are suited
for the specificities of the training data, but are less
accurate at predicting values on the unseen test set.
5 Discussion
Training data over-fitting is clearly shown by the
results presented in Table 3, indicated by the per-
formance drop between results obtained during
cross-validation and the ones obtained on the test
set. While this drop may be related to data over-
fitting, it may also be related to the use of differ-
ent machine learning methods for feature selec-
tion (M5P and M5P-R) and for building the fi-
nal regression models (-SVR). In order to ver-
ify this aspect, we build two regression models
using M5P, based on the feature sets alltypes
and combine. Results are presented in Table 4
and show that, for the alltypes feature set, the
RMSE, DeltaAvg and Spearman scores are im-
proved using M5P compared to SVM. For the
combine feature set, the scoring results (MAE
and RMSE) are better using SVM, while the rank-
ing results are similar for both machine learning
methods.
The performance drop between the results on
the training data (or a development set) and the
test data was also observed by the highest ranked
participants in the WMT12 QE shared task. To
compare our system without feature selection to
the winner of the previous shared task, we eval-
uate the 15 types system in Table 3 using the
WMT12 QE dataset. The results are presented in
Table 5. We can see that similar MAEs are ob-
tained with our feature set and the WMT12 QE
winner, whereas our system gets a higher RMSE
(+0.01). For the ranking scores, our system is
worse using the DeltaAvg metric while it is bet-
ter on Spearman coefficient.
6 Conclusion
We presented in this paper our experiments for the
WMT13 Quality Estimation shared task. Our ap-
proach is based on the extraction of a large ini-
tial feature set, followed by two feature selection
methods. The first one is a wrapper approach us-
ing M5P and a best-first search algorithm, while
the second one is a feature binarisation approach
using M5P and M5P-R. The final regression mod-
els were built using -SVR and we selected two
systems to submit based on cross-validation re-
sults.
We observed that our system reaching the best
scores on the test set was not a system trained on
a reduced feature set and it did not yield the best
cross-validation results. This system was trained
using 442 features, which are the combination of
15 different feature types. Amongst the systems
built on reduced sets, the best results are obtained
396
System nb feat MAE RMSE DeltaAvg Spearman
alltypes 96 0.135 0.165 0.104 0.604
combine 134 0.139 0.169 0.098 0.587
Table 4: Results obtained with the two feature sets contained in our submitted systems using M5P to
build the regression models instead of -SVR.
System nb feat MAE RMSE DeltaAvg Spearman
WMT12 winner 15 0.61 0.75 0.63 0.64
15 types 442 0.61 0.76 0.60 0.65
Table 5: Results obtained on WMT12 QE dataset with our best system (15 types) compared to WMT12
QE highest ranked team, in the Likert score prediction task.
using the feature binarisation approach M5P-R
80, which contains 16 features selected from our
initial set of features. The tree-based feature bina-
risation is a fast and flexible method which allows
us to vary the number of features by optimising the
leaf size and leads to acceptable results with a few
selected features.
Future work involves a deeper analysis of the
over-fitting effect and an investigation of other
methods in order to outperform the non-reduced
feature set. We are also interested in finding a ro-
bust way to optimise the leaf size parameter for
our tree-based feature binarisation method, with-
out using cross-validation on the training set with
an SVM algorithm.
Acknowledgements
The research reported in this paper has been
supported by the Research Ireland Enterprise
Partnership Scheme (EPSPG/2011/102 and EP-
SPD/2011/135) and Science Foundation Ireland
(Grant 12/CE/I2267) as part of the Centre for
Next Generation Localisation (www.cngl.ie)
at Dublin City University.
References
Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A Library for Support Vector Ma-
chines. ACM Transactions on Intelligent Sys-
tems and Technology, 2:27:1?27:27. Soft-
ware available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA Data Mining Software: an
Update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. DCU-Symantec Submission for
the WMT 2012 Quality Estimation Task. In Pro-
ceedings of the Seventh WMT, pages 138?144.
Raphael Rubino et al (to appear). 2013. Topic Models
for Translation Quality Estimation for Gisting Pur-
poses. In Proceeding of MT Summit XIV.
Helmut Schmidt. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Natu-
ral Language Processing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA, pages 223?231.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the Seventh WMT, pages 145?151.
Yong Wang and Ian H Witten. 1997. Inducing Model
Trees for Continuous Classes. In Proceedings of
ECML, pages 128?137. Prague, Czech Republic.
397
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 1?11,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Working with a small dataset - semi-supervised dependency parsing for Irish
Teresa Lynn1,2, Jennifer Foster1, Mark Dras2 and Josef van Genabith1
1NCLT/CNGL, Dublin City University, Ireland
2Department of Computing, Macquarie University, Sydney, Australia
1{tlynn,jfoster,josef}@computing.dcu.ie
2{teresa.lynn,mark.dras}@mq.edu.au,
Abstract
We present a number of semi-supervised pars-
ing experiments on the Irish language carried
out using a small seed set of manually parsed
trees and a larger, yet still relatively small, set
of unlabelled sentences. We take two pop-
ular dependency parsers ? one graph-based
and one transition-based ? and compare re-
sults for both. Results show that using semi-
supervised learning in the form of self-training
and co-training yields only very modest im-
provements in parsing accuracy. We also try
to use morphological information in a targeted
way and fail to see any improvements.
1 Introduction
Developing a data-driven statistical parser relies on
the availability of a parsed corpus for the language
in question. In the case of Irish, the only parsed
corpus available to date is a dependency treebank,
which is currently under development and still rel-
atively small, with only 803 gold-annotated trees
(Lynn et al, 2012a). As treebank development is
a labour- and time-intensive process, in this study
we evaluate various approaches to bootstrapping a
statistical parser with a set of unlabelled sentences
to ascertain how accurate parsing output can be
at this time. We carry out a number of differ-
ent semi-supervised bootstrapping experiments us-
ing self-training, co-training and sample-selection-
based co-training. Our studies differ from previous
similar experiments as our data is taken from a work-
in-progress treebank. Thus, aside from the current
small treebank which is used for training the initial
seed model and for testing, there is no additional
gold-labelled data available to us to directly com-
pare supervised and semi-supervised approaches us-
ing training sets of comparable sizes.
In the last decade, data-driven dependency pars-
ing has come to fore, with two main approaches
dominating ? transition-based and graph-based. In
classic transition-based dependency parsing, the
training phase consists of learning the correct parser
action to take given the input string and the parse
history, and the parsing phase consists of the greedy
application of parser actions as dictated by the
learned model. In contrast, graph-based depen-
dency parsing involves the non-deterministic con-
struction of a parse tree by predicting the maximum-
spanning-tree in the digraph for the input sentence.
In our study, we employ Malt (Nivre et al, 2006),
a transition-based dependency parsing system, and
Mate (Bohnet, 2010), a graph-based parser.
In line with similar experiments carried out on
English (Steedman et al, 2003), we find that co-
training is more effective than self-training. Co-
training Malt on the output of Mate proves to be the
most effective method for improving Malt?s perfor-
mance on the limited data available for Irish. Yet, the
improvement is relatively small (0.6% over the base-
line for LAS, 0.3% for UAS) for the best co-trained
model. The best Mate results are achieved through a
non-iterative agreement-based co-training approach,
in which Mate is trained on trees produced by Malt
which exhibit a minimum agreement of 85% with
Mate (LAS increase of 1.2% and UAS of 1.4%).
The semi-supervised parsing experiments do not
explicitly take into account the morphosyntactic
properties of the Irish language. In order to examine
the effect of this type of information during parsing,
we carry out some orthogonal experiments where we
1
reduce word forms to lemmas and introduce mor-
phological features in certain cases. These changes
do not bring about an increase in parsing accuracy.
The paper is organised as follows. Section 2 is
an overview of Irish morphology. In Section 3 our
previous work carried out on the development of an
Irish dependency treebank is discussed followed in
Section 4 by a description of some of our prior pars-
ing results. Section 5 describes the self-training, co-
training and sample-selection-based co-training ex-
periments, Section 6 presents the preliminary pars-
ing experiments involving morphological features,
and, finally, Section 7 discusses our future work.
2 Irish as a morphologically rich language
Irish is a Celtic language of the Indo-European lan-
guage family. It has a VSO word order and is rich in
morphology. The following provides an overview of
the type of morphology present in the Irish language.
It is not a comprehensive summary as the rules gov-
erning morphological changes are too extensive and
at times too complex to document here.
Inflection in Irish mainly occurs through suffixa-
tion, but initial mutation through lenition and eclip-
sis is also common (Christian-Brothers, 1988). A
prominent feature of Irish (also of Scottish and
Manx), which influences inflection, is the existence
of two sets of consonants, referred to as ?broad? and
?slender? consonants (O? Siadhail, 1989). Conso-
nants can be slenderised by accompanying the con-
sonant with a slender vowel, either e or i. Broaden-
ing occurs through the use of broad vowels; a, o or
u. For example, buail ?to hit? becomes ag bualadh
?hitting? in the verbal noun form. In general, there
needs to be vowel harmony (slender or broad) be-
tween stem endings and the initial vowel in a suffix.
A process known as syncopation also occurs
when words with more than one syllable have a
vowel-initial suffix added. For example imir ?to
play? inflects as imr??m ?I play?.
Nouns While Old Irish employed several gram-
matical cases, Modern Irish uses only three: Nomi-
native, Genitive and Vocative. The nominative form
is sometimes regarded as the ?common form? as it is
now also used to account for accusative and dative
forms. Nouns in Irish are divided into five classes, or
declensions, depending on the manner in which the
genitive case is formed. In addition, there are two
grammatical genders in Irish - masculine and fem-
inine. Case, declension and gender are expressed
through noun inflection. For example, pa?ipe?ar ?pa-
per? is a masculine noun in the first declension. Both
lenition and slenderisation are used to form the geni-
tive singular form: pha?ipe?ir. In addition, possessive
adjectives cause noun inflection through lenition,
eclipsis and prefixation. For example, teach ?house?,
mo theach ?my house?, a?r dteach ?our house?; ainm
?name?, a hainm ?her name?.
Verbs Verbs can incorporate their subject, inflect-
ing for person and number through suffixation. Such
forms are referred to as synthetic verb forms. In
addition, verb tense is often indicated through var-
ious combinations of initial mutation, syncopation
and suffixation. For example, scr??obh ?write? can in-
flect as scr??obhaim ?I write?. The past tense of the
verb tug ?give? is thug ?gave?. Lenition occurs af-
ter the negative particle n??. For example, tugaim ?I
give?; n?? thugaim ?I do not give?; n??or thug me? ?I
did not give?. Eclipsis occurs following clitics such
as interrogative particles (an, nach); complementis-
ers (go, nach); and relativisers (a, nach) (Stenson,
1981). For example, an dtugann se?? ?does he give??;
nach dtugann se?? ?does he not give??.
Adjectives In general, adjectives follow nouns and
agree in number, gender and case. Depending on
the noun they modify, adjectives can also inflect.
Christian-Brothers (1988) note eight declensions of
adjectives. They can decline for genitive singular
masculine, genitive singular feminine and nomina-
tive plural. For example, bacach ?lame? inflects as
bacaigh (Gen.Sg.Masc), baca?? (Gen.Fem.Sg) and
bacacha (Nom.PL). Comparative adjectives are also
formed through inflection. For example, la?idir
?strong?, n??os la?idre ?stronger?; de?anach ?late?, is
de?ana?? ?latest?.
Prepositions Irish has simple and compound
prepositions. Most of the simple prepositions can
inflect for person and number (known as preposi-
tional pronouns or pronominal prepositions), thus
including a nominal element. For example, com-
pare bh?? se? ag labhairt le fear ?he was speaking
with a man? with bh?? se? ag labhairt leis ?he was
speaking with him?. These forms are used quite fre-
2
quently, not only with regular prepositional attach-
ment where pronominal prepositions operate as ar-
guments of verbs or modifiers of nouns and verbs,
but also in idiomatic use where they express emo-
tions and states, e.g. ta? bro?n orm (lit. ?be-worry-
on me?) ?I am worried? or ta? su?il agam (lit. ?be-
expectation-with me?) ?I hope?. Noted by Greene
(1966) as a noun-centered language, nouns are of-
ten used to convey the meaning that verbs often
would. Pronominal prepositions are often used in
these types of structures. For example, bhain me?
geit aisti (lit. extracted-I-shock-from her) ?I fright-
ened her?; bhain me? mo cho?ta d??om (lit. extracted-I-
coat-from me) ?I took off my coat?; bhain me? u?sa?id
as (lit. extracted-I-use-from it) ?I used it?; bhain
me? triail astu (lit. extracted-I-attempt-from them)?I
tried them?.
Derivational morphology There are also some
instances of derivational morphology in Irish. U??
Dhonnchadha (2009) notes that all verb stems and
agentive nouns can inflect to become verbal nouns.
Verbal adjectives are also derived from verb stems
through suffixation. For example, the verb du?n
?close? undergoes suffixation to become du?nadh
?closing? (verbal noun) and du?nta ?closed? (verbal
adjective). An emphatic suffix -sa/-se (both broad
and slender form) can attach to nouns or pronouns.
It can also be attached to any verb that has been in-
flected for person and number and also to pronom-
inal prepositions. For example mo thuairim ?my
opinion??mo thuairimse ?my opinion; tu? ?you?(sg)
? tusa ?you?; cloisim ?I hear?? cloisimse ?I hear?;
liom ?with me?? liomsa ?with me?. In addition, the
diminutive suffix -??n can attach to all nouns to form
a derived diminutive form. The rules of slenderisa-
tion apply here also. For example, buachaill ?boy?
becomes buachaill??n ?little boy?, and tamall ?while?
becomes tamaill??n ?short while?.
3 The Irish Dependency Treebank
Irish is the official language of Ireland, yet English
is the primary language for everyday use. Irish is
therefore considered an EU minority language and
is lacking in linguistic resources that can be used to
develop NLP applications (Judge et al, 2012).
Recently, in efforts to address this issue, we have
begun work on the development of a dependency
treebank for Irish (Lynn et al, 2012a). The treebank
has been built upon a gold standard 3,000 sentence
POS-tagged corpus1 developed by U?? Dhonnchadha
(2009). Our labelling scheme is based on an ?LFG-
inspired? dependency scheme developed for English
by C?etinog?lu et al (2010). This scheme was adopted
with the aim of identifying functional roles while
at the same time circumventing outstanding, unre-
solved issues in Irish theoretical syntax.2 The Irish
labelling scheme has 47 dependency labels in the la-
bel set. The treebank is in the CoNLL format with
the following fields: ID, FORM, LEMMA, CPOSTAG,
POSTAG, HEAD and DEPREL. The coarse-grained
part of speech of a word is marked by the la-
bel CPOSTAG, and POSTAG marks the fine-grained
part of speech for that word. For example, prepo-
sitions are tagged with the CPOSTAG Prep and
one of the following POSTAGs: Simple: ar ?on?,
Compound: i ndiaidh ?after?, Possessive: ina
?in its?, Article: sa ?in the?.
At an earlier stage of the treebank?s develop-
ment, we carried out on an inter-annotator agree-
ment (IAA) study. The study involved four stages.
(i) The first experiment (IAA-1) involved the as-
sessment of annotator agreement following the in-
troduction of a second annotator. The results re-
ported a Kappa score of 0.79, LAS of 74.4% and
UAS of 85.2% (Lynn et al, 2012a). (ii) We then
held three workshops that involved thorough anal-
ysis of the output of IAA-1, highlighting disagree-
ments between annotators, gaps in the annotation
guide, shortcomings of the labelling scheme and lin-
guistic issues not yet addressed. (iii) The annotation
guide, labelling scheme and treebank were updated
accordingly, addressing the highlighted issues. (iv)
Finally, a second inter-annotator agreement exper-
iment (IAA-2) was carried out presenting a Kappa
score of 0.85, LAS of 79.2% and UAS of 87.8%
(Lynn et al, 2012b).
We found that the IAA study was valuable in the
development of the treebank, as it resulted in im-
1A tagged, randomised subset of the NCII, (New Corpus for
Ireland - Irish http://corpas.focloir.ie/), comprised of text from
books, news data, websites, periodicals, official and government
documents.
2For example there are disagreements over the existence of
a VP in Irish and whether the language has a VSO or an under-
lying SVO structure.
3
provement of the quality of the labelling scheme,
the annotation guide and the linguistic analysis of
the Irish language. Our updated labelling scheme
is now hierarchical, allowing for a choice between
working with fine-grained or coarse-grained labels.
The scheme has now been finalised. A full list of
the labels can be found in Lynn et al (2012b). The
treebank currently contains 803 gold-standard trees.
4 Preliminary Parsing Experiments
In our previous work (Lynn et al, 2012a), we car-
ried out some preliminary parsing experiments with
MaltParser and 10-fold cross-validation using 300
gold-standard trees. We started out with the fea-
ture template used by C?etinog?lu et al (2010) and ex-
amined the effect of omitting LEMMA, WORDFORM,
POSTAG and CPOSTAG features and combinations
of these, concluding that it was best to include all
four types of information. Our final LAS and UAS
scores were 63.3% and 73.1% respectively. Follow-
ing the changes we made to the labelling scheme
as a result of the second IAA study (described
above), we re-ran the same parsing experiments on
the newly updated seed set of 300 sentences - the
LAS increased to 66.5% and the UAS to 76.3%
(Lynn et al, 2012b).
In order to speed up the treebank creation, we also
applied an active learning approach to bootstrapping
the annotation process. This work is also reported in
Lynn et al (2012b). The process involved training a
MaltParser model on a small subset of the treebank
data, and iteratively, parsing a new set of sentences,
selecting a 50-sentence subset to hand-correct, and
adding these new gold sentences to the training set.
We compared a passive setup, in which the parses
that were selected for correction were chosen at ran-
dom, to an active setup, in which the parses that
were selected for correction were chosen based on
the level of disagreement between two parsers (Malt
and Mate). The active approach to annotation re-
sulted in superior parsing results to the passive ap-
proach (67.2% versus 68.1% LAS) but the differ-
ence was not statistically significant.
5 Semi-Supervised Parsing Experiments
In order to alleviate data sparsity issues brought
about by our lack of training material, we experi-
ment with automatically expanding our training set
using well known semi-supervised techniques.
5.1 Self-Training
5.1.1 Related Work
Self-training, the process of training a system on
its own output, has a long and chequered history in
parsing. Early experiments by Charniak (1997) con-
cluded that self-training is ineffective because mis-
takes made by the parser are magnified rather than
smoothed during the self-training process. The self-
training experiments of Steedman et al (2003) also
yielded disappointing results. Reichart and Rap-
paport (2007) found, on the other hand, that self-
training could be effective if the seed training set
was very small. McClosky et al (2006) also re-
port positive results from self-training, but the self-
training protocol that they use cannot be considered
to be pure self-training as the first-stage Charniak
parser (Charniak, 2000) is retrained on the output of
the two-stage parser (Charniak and Johnson, 2005)
They later show that the extra information brought
by the discriminative reranking phase is a factor
in the success of their procedure (McClosky et al,
2008). Sagae (2010) reports positive self-training re-
sults even without the reranking phase in a domain
adaptation scenario, as do Huang and Harper (2009)
who employ self-training with a PCFG-LA parser.
5.1.2 Experimental Setup
The labelled data available to us for this experi-
ment comprises the 803 gold standard trees referred
to in Section 3. This small treebank includes the
150-tree development set and 150-tree test set used
in experiments by Lynn et al (2012b). We use the
same development and test sets for this study. As
for the remaining 503 trees, we remove any trees
that have more than 200 tokens. The motivation for
this is two-fold: (i) we had difficulties training Mate
parser with long sentences due to memory resource
issues, and (ii) in keeping with the findings of Lynn
et al (2012b), the large trees were sentences from
legislative text that were difficult to analyse for au-
tomatic parsers and human annotators. This leaves
us with 500 gold-standard trees as our seed training
data set.
For our unlabelled data, we take the next 1945
sentences from the gold standard 3,000-sentence
4
A is a parser.
M iA is a model of A at step i.
P iA is a set of trees produced using M
i
A.
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA is labelled training data for A at step i.
Initialise:
L0A ? L.
M0A ? Train(A,L
0
A)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
Li+1A ? L
i
A + P
i
A
M i+1A ? Train(A,L
i+1
A )
end for
Figure 1: Self-training algorithm
POS-tagged corpus referred to in Section 3. When
we remove sentences with more than 200 tokens, we
are left with 1938 sentences in our unlabelled set.
The main algorithm for self-training is given in
Figure 1. We carry out two separate experiments
using this algorithm. In the first experiment we use
Malt. In the second experiment, we substitute Mate
for Malt.3
The steps are as follows: Initialisation involves
training the parser on a labelled seed set of 500 gold
standard trees (L0A), resulting in a baseline parsing
model: M iA. We divide the set of gold POS-tagged
sentences (U ) into 6 sets, each containing 323 sen-
tences U i. For each of the six iterations in this ex-
periment i = [1?6], we parse U i. Each time, the set
of newly parsed sentences (PA) is added to the train-
ing set LiA to make a larger training set of L
i+1
A . A
new parsing model (M i+1A ) is then induced by train-
ing with the new training set.
5.1.3 Results
The results of our self-training experiments are
presented in Figure 2. The best Malt model was
trained on 2115 trees, at the 5th iteration (70.2%
LAS). UAS scores did not increase over the baseline
(79.1%). The improvement in LAS over the baseline
is not statistically significant. The best Mate model
was trained on 1792 trees, at the 4th iteration (71.2%
3Versions used: Maltparser v1.7 (stacklazy parsing algo-
rithm); Mate tools v3.3 (graph-based parser)
Figure 2: Self-Training Results on the Development Set
LAS, 79.2% UAS). The improvement over the base-
line is not statistically significant.
5.2 Co-Training
5.2.1 Related Work
Co-training involves training a system on the out-
put of a different system. Co-training has found
more success in parsing than self-training, and it
is not difficult to see why this might be the case
as it can be viewed as a method for combining the
benefits of individual parsing systems. Steedman
et al (2003) directly compare co-training and self-
training and find that co-training outperforms self-
training. Sagae and Tsujii (2007) successfully em-
ploy co-training in the domain adaption track of the
CoNLL 2007 shared task on dependency parsing.
5.2.2 Experimental Setup
In this and all subsequent experiments, we use
both the same training data and unlabelled data that
we refer to in Section 5.1.2.
Our co-training algorithm is given in Figure 3 and
it is the same as the algorithm provided by Steedman
et al (2003). Again, our experiments are carried out
using Malt and Mate. This time, the experiments are
run concurrently as each parser is bootstrapped from
the other parser?s output.
5
A and B are two different parsers.
M iA and M
i
B are models of A and B at step i.
P iA and P
i
B are a sets of trees produced using M
i
A and M
i
B .
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA and L
i
B are labelled training data for A and B at step i.
Initialise:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
P iB ? Parse(U
i , M iB)
Li+1A ? L
i
A + P
i
B
Li+1B ? L
i
B + P
i
A
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
end for
Figure 3: Co-training algorithm
The steps are as follows: Initialisation involves
training both parsers on a labelled seed set of 500
gold standard trees (L0A and L
0
B), resulting in two
separate baseline parsing models: M iA (Malt) and
M iB (Mate). We divide the set of gold POS-tagged
sentences (U ) into 6 sets, each containing 323 sen-
tences U i. For each of the six iterations in this ex-
periment i = [1? 6], we use Malt and Mate to parse
U i. This time, the set of newly parsed sentences P iB
(Mate output) is added to the training set LiA to make
a larger training set of Li+1A (Malt training set). Con-
versely, the set of newly parsed sentences P iA (Malt
output) is added to the training set LiB to make a
larger training set of Li+1B (Mate training set). Two
new parsing models (M i+1A and M
i+1
B ) are then in-
duced by training Malt and Mate respectively with
their new training sets.
5.2.3 Results
The results of our co-training experiment are pre-
sented in Figure 4. The best Malt model was trained
on 2438 trees, at the final iteration (71.0% LAS
and 79.8% UAS). The improvement in UAS over
the baseline is statistically significant. Mate?s best
model was trained on 823 trees on the second iter-
ation (71.4% LAS and 79.9% UAS). The improve-
ment over the baseline is not statistically significant.
Figure 4: Co-Training Results on the Development Set
5.3 Sample-Selection-Based Co-Training
5.3.1 Related Work
Sample selection involves choosing training items
for use in a particular task based on some criteria
which approximates their accuracy in the absence of
a label or reference. In the context of parsing, Re-
hbein (2011) chooses additional sentences to add to
the parser?s training set based on their similarity to
the existing training set ? the idea here is that sen-
tences that are similar to training data are likely to
have been parsed properly and so are ?safe? to add
to the training set. In their parser co-training experi-
ments, Steedman et al (2003) sample training items
based on the confidence of the individual parsers (as
approximated by parse probability).
In Active Learning research, the Query By Com-
mittee selection method (Seung et al, 1992) is used
to choose items for annotation ? if a committee of
two or more systems disagrees on an item, this is ev-
idence that the item needs to be prioritised for man-
ual correction (see for example Lynn et al (2012b)).
Steedman et al (2003) discuss a sample selection
approach based on differences between parsers ? if
parser A and parser B disagree on an analysis, parser
A can be improved by being retrained on parser B?s
analysis, and vice versa. In contrast, Ravi et al
(2008) show that parser agreement is a strong in-
6
dicator of parse quality, and in parser domain adap-
tation, Sagae and Tsujii (2007) and Le Roux et al
(2012) use agreement between parsers to choose
which automatically parsed target domain items to
add to the training set.
Sample selection can be used with both self-
training and co-training. We restrict our attention
to co-training since our previous experiments have
demonstrated that it has more potential than self-
training. In the following set of experiments, we ex-
plore the role of both parser agreement and parser
disagreement in sample selection in co-training.
5.3.2 Agreement-Based Co-Training
Experimental Setup The main algorithm for
agreement-based co-training is given in Figure 5.
Again, Malt and Mate are used. However, this algo-
rithm differs from the co-training algorithm in Fig-
ure 3 in that rather than adding the full set of 323
newly parsed trees (P iA and P
i
B) to the training set
at each iteration, selected subsets of these trees (P iA?
and P iB?) are added instead. To define these subsets,
we identify the trees that have 85% or higher agree-
ment between the two parser output sets. As a re-
sult, the number of trees in the subsets differ at each
iteration. For iteration 1, 89 trees reach the agree-
ment threshold; iteration 2, 93 trees; iteration 3, 117
trees; iteration 4, 122 trees; iteration 5, 131 trees;
iteration 6, 114 trees. The number of trees in the
training sets is much smaller compared with those
in the experiments of Section 5.2.
Results The results for agreement-based co-
training are presented in Figure 6. Malt?s best
model was trained on 1166 trees at the final iteration
(71.0% LAS and 79.8% UAS). Mate?s best model
was trained on 1052 trees at the 5th iteration (71.5%
LAS and 79.7% UAS). Neither result represents a
statistically significant improvement over the base-
line.
5.3.3 Disagreement-based Co-Training
Experimental Setup This experiment uses the
same sample selection algorithm we used for
agreement-based co-training (Figure 5). For this ex-
periment, however, the way in which the subsets
of trees (P iA? and P
i
B?) are selected differs. This
time we choose the trees that have 70% or higher
disagreement between the two parser output sets.
A and B are two different parsers.
M iA and M
i
B are models of A and B at step i.
P iA and P
i
B are a sets of trees produced using M
i
A and M
i
B .
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA and L
i
B are labelled training data for A and B at step i.
Initialise:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
P iB ? Parse(U
i , M iB)
P iA? ? a subset of X trees from P
i
A
P iB ? ? a subset of X trees from P
i
B
Li+1A ? L
i
A + P
i
B ?
Li+1B ? L
i
B + P
i
A?
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
end for
Figure 5: Sample selection Co-training algorithm
Again, the number of trees in the subsets differ at
each iteration. For iteration 1, 91 trees reach the dis-
agreement threshold; iteration 2, 93 trees; iteration
3, 73 trees; iteration 4, 74 trees; iteration 5, 68 trees;
iteration 6, 71 trees.
Results The results for our disagreement-based
co-training experiment are shown in Figure 7. The
best Malt model was trained with 831 trees at the
4th iteration (70.8% LAS and 79.8% UAS). Mate?s
best models were trained on (i) 684 trees on the 2nd
iteration (71.0% LAS) and (ii) 899 trees on the 5th
iteration (79.4% UAS). Neither improvement over
the baseline is statistically significant.
5.3.4 Non-Iterative Agreement-based
Co-Training
In this section, we explore what happens when
we add the additional training data at once rather
than over several iterations. Rather than testing this
idea with all our previous setups, we choose sample-
selection-based co-training where agreement be-
tween parsers is the criterion for selecting additional
training data.
Experimental Setup Again, we also follow the
algorithm for agreement-based co-training as pre-
sented in Figure 5. However, two different ap-
7
Figure 6: Agreement-based Co-Training Results on the
Development Set
proaches are taken this time, involving only one it-
eration in each. For the first experiment (ACT1a),
the subsets of trees (P iA? and P
i
B?) that are added to
the training data are chosen based on an agreement
threshold of 85% between parsers, and are taken
from the full set of unlabelled data (where U i = U ),
comprising 1938 trees. In this instance, the subset
consists of 603 trees, making a final training set of
1103 trees.
For the second experiment (ACT1b), only trees
meeting a parser agreement threshold of 100% are
added to the training data. 253 trees (P iA? and P
i
B?)
out of 1938 trees (U i = U ) meet this threshold. The
final training set consists of 753 trees.
Results ACT1a proved to be the most accurate
parsing model for Mate overall. The addition of
603 trees that met the agreement threshold of 85%
increased the LAS and UAS scores over the base-
line by 1.0% and 1.3% to 71.8 and 80.4 respec-
tively. This improvement is statistically significant.
Malt showed a LAS improvement of 0.93% and
a UAS improvement of 0.42% (71.0% LAS and
79.6% UAS). The LAS improvement over the base-
line is statistically significant.
The increases for ACT1b, where 100% agreement
trees are added, are less pronounced and are not sta-
Figure 7: Disagreement-based Co-Training Results on
the Development Set
tistically significant. Results showed a 0.5% LAS
and 0.2% UAS increase over the baseline with Malt,
based on the 100% agreement threshold (adding 235
trees). Mate performs at 0.5% above the LAS base-
line and 0.1% above the UAS baseline.
5.4 Analysis
We perform an error analysis for the Malt and Mate
baseline, self-trained and co-trained models on the
development set. We observe the following trends:
? All Malt and Mate parsing models confuse the
subj and obj labels. A few possible rea-
sons for this stand out: (i) It is difficult for
the parser to discriminate between analytic verb
forms and synthetic verb forms. For example,
in the phrase pho?sfainn thusa ?I would marry
you?, pho?sfainn is a synthetic form of the verb
po?s ?marry? that has been inflected with the in-
corporated pronoun ?I?. Not recognising this,
the parser decided that it is an intransitive verb,
taking ?thusa?, the emphatic form of the pro-
noun tu? ?you?, as its subject instead of object.
(ii) Possibly due to a VSO word order, when
the parser is dealing with relative phrases, it
can be difficult to ascertain whether the follow-
ing noun is the subject or object. For example,
an chail??n a chonaic me? inne? ?the girl whom
8
I saw yesterday/ the girl who saw me yester-
day?.4 (iii) There is no passive verb form in
Irish. The autonomous form is most closely
linked with passive use and is used when the
agent is not known or mentioned. A ?hidden?
or understood subject is incorporated into the
verbform. Casadh eochair i nglas ?a key was
turned in a lock? (lit. somebody turned a key
in a lock). In this sentence, eochair ?key? is the
object.
? For both parsers, there is some confusion be-
tween the labelling of obl and padjunct,
both of which mark the attachment between
verbs and prepositions. Overall, Malt?s con-
fusion decreases over the 6 iterations of self-
training, but Mate begins to incorrectly choose
padjunct over obl instead. Mixed results
are obtained using the various variants of co-
training.
? Mate handles coordination better than Malt.5 It
is not surprising then that co-training Malt us-
ing Mate parses improves Malt?s coordination
handling whereas the opposite is the case when
co-training Mate on Malt parses, demonstrat-
ing that co-training can both eliminate and in-
troduce errors.
? Other examples of how Mate helps Malt during
co-training is in the distinction between top
and comp relations, between vparticle
and relparticle, and in the analysis of
xcomps.
? Distinguishing between relative and cleft par-
ticles is a frequent error for Mate, and there-
fore Malt also begins to make this kind of error
when co-trained using Mate. Mate improves
using sample-selection-based co-training with
Malt.
? The sample-selection-based co-training vari-
ants show broadly similar trends to the basic
co-training.
4Naturally ambiguous Irish sentences like this require con-
text for disambiguation.
5Nivre and McDonald (2007) make a similar observation
when they compare the errors made by graph and transition
based dependency parsers.
Parsing Models LAS UAS
Development Set
Malt Baseline: 70.0 79.1
Malt Best (co-train) : 71.0 80.2
Mate Baseline: 70.8 79.1
Mate Best (85% threshold ACT1a): 71.8 80.4
Test Set
Malt Baseline: 70.2 79.5
Malt Best (co-train) : 70.8 79.8
Mate Baseline: 71.9 80.1
Mate Best (85% threshold ACT1a): 73.1 81.5
Table 1: Results for best performing models
5.5 Test Set Results
The best performing parsing model for Malt on
the development set is in the final iteration of the
basic co-training approach in Section 5.2. The
best performing parsing model for Mate on the de-
velopment set is the non-iterative 85% threshold
agreement-based co-training approach described in
Section 5.3.4. The test set results for these opti-
mal development set configurations are also shown
in Table 1. The baseline model for Malt obtains
a LAS of 70.2%, the final co-training iteration a
LAS of 70.8%. The baseline model for Mate ob-
tains a LAS of 71.9%, and the non-iterative 85%
agreement-based co-trained model obtains a LAS of
73.1%.
6 Parsing Experiments Using
Morphological Features
As well as the size of the dataset, data sparsity is
also confounded by the number of possible inflected
forms for a given root form. With this in mind,
and following on from the discussion in Section 5.4,
we carry out further parsing experiments in an at-
tempt to make better use of morphological informa-
tion during parsing. We attack this in two ways: by
reducing certain words to their lemmas and by in-
cluding morphological information in the optional
FEATS (features) field. The reasoning behind re-
ducing certain word forms to lemmas is to further
reduce the differences between inflected forms of
the same word, and the reasoning behind including
morphological information is to make more explicit
the similarity between two different word forms in-
flected in the same way. All experiments are car-
9
Parsing Models (Malt) LAS UAS
Baseline: 70.0 79.1
Lemma (Pron Prep): 69.7 78.9
Lemma + Pron Prep Morph Features: 69.6 78.9
Form + Pron Prep Morph Features: 69.8 79.1
Verb Morph Features: 70.0 79.1
Table 2: Results with morphological features on the de-
velopment set
ried out with MaltParser and our seed training set
of 500 gold trees. We focus on two phenomena:
prepositional pronouns or pronominal prepositions
(see Section 2) and verbs with incorporated subjects
(see Section 2 and Section 5.4).
In the first experiment, we include extra mor-
phological information for pronominal prepositions.
We ran three parsing experiments: (i) replacing the
value of the surface form (FORM) of pronominal
prepositions with their lemma form (LEMMA), for
example agam?ag, (ii) including morphological in-
formation for pronominal prepositions in the FEATS
column. For example, in the case of agam ?at me?,
we include Per=1P|Num=Sg, (iii) we combine
both approaches of reverting to lemma form and also
including the morphological features. The results
are given in Table 2.
In the second experiment, we include morpholog-
ical features for verbs with incorporated subjects:
imperative verb forms, synthetic verb forms and au-
tonomous verb forms such as those outlined in Sec-
tion 5.4. For each instance of these verb types, we
included incorpSubj=true in the FEATS col-
umn. The results are also given in Table 2.
The experiments on the pronominal prepositions
show a drop in parsing accuracy while the experi-
ments carried out using verb morphological infor-
mation showed no change in parsing accuracy.6 In
the case of inflected prepositions, perhaps we have
not seen any improvement because we have not fo-
cused on a phenomenon which is critical for parsing.
More experimentation is necessary.
7 Concluding Remarks
We have presented two sets of experiments which
aim to improve dependency parsing performance for
6Although the total number of correct attachments are the
same, the parser output is different.
a minority language with a very small treebank. In
the first set of experiments, the main focus of the pa-
per, we tried to overcome the limited treebank size
by increasing the parsers? training sets using auto-
matically parsed sentences. While we do manage
to achieve statistically significant improvements in
some settings, it is clear from the results that the
gains in parser accuracy through semi-supervised
bootstrapping methods are fairly modest. Yet, in the
absence of more gold labelled data, it is difficult to
know now whether we would achieve similar or im-
proved results by adding the same amount of gold
training data. This type of analysis will be interest-
ing at a later date when the unlabelled trees used in
these experiments are eventually annotated and cor-
rected manually.
The second set of experiments tries to mitigate
some of the data sparseness issues by exploiting
morphological characteristics of the language. Un-
fortunately, we do not see any improvements but we
may get different results if we repeat these experi-
ments using the larger semi-supervised training sets
from the first set of experiments.
There are many directions this parsing research
could take us in the future. Our unlabelled data con-
sisted of sentences annotated with gold POS tags.
In the future we would like to take advantage of
the fully unlabelled, untagged data in the New Cor-
pus for Ireland ? Irish, which consists of 30 million
words. We would also like to experiment with a fully
unsupervised parser using this dataset. Our Malt fea-
ture models are manually optimised ? it would be in-
teresting to experiment with optimising them using
MaltOptimizer (Ballesteros, 2012). An additional
avenue of research would be to exploit the hierar-
chical nature of the dependency scheme to arrive at
more flexible way of measuring agreement or dis-
agreement in sample selection.
Acknowledgements
We thank the three anonymous reviewers for their
helpful feedback. This work is supported by Sci-
ence Foundation Ireland (Grant No. 07/CE/I1142)
as part of the Centre for Next Generation Localisa-
tion (www.cngl.ie) at Dublin City University.
10
References
Miguel Ballesteros. 2012. Maltoptimizer: A sys-
tem for maltparser optimization. In Proceedings of
the Eighth International Conference on Linguistic Re-
sources and Evaluation (LREC), pages 2757?2763, Is-
tanbul, Turkey.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING.
O?zlem C?etinog?lu, Jennifer Foster, Joakim Nivre, Deirdre
Hogan, Aoife Cahill, and Josef van Genabith. 2010.
LFG without c-structures. In Proceedings of the 9th
International Workshop on Treebanks and Linguistic
Theories.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd ACL.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of AAAI.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics (NAACL-00).
Christian-Brothers. 1988. New Irish Grammar. Dublin:
C J Fallon.
David Greene. 1966. The Irish Language. Dublin: The
Three Candles.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of EMNLP.
John Judge, Ailbhe N?? Chasaide, Rose N?? Dhubhda,
Kevin P. Scannell, and Elaine U?? Dhonnchadha. 2012.
The Irish Language in the Digital Age. Springer Pub-
lishing Company, Incorporated.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
soul Samed Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 systems for the sancl 2012 shared task.
In Working Notes of SANCL.
Teresa Lynn, O?zlem C?etinog?lu, Jennifer Foster, Elaine U??
Dhonnchadha, Mark Dras, and Josef van Genabith.
2012a. Irish treebanking and parsing. In Proceedings
of the Eight International Conference on Language
Resources and Evaluation, pages 1939?1946.
Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U??
Dhonnchadha. 2012b. Active learning and the Irish
treebank. In Proceeedings of the Australasian Lan-
guage Technology Workshop (ALTA), pages 23?32.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City, USA, June. Association for Computational
Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. When is self-training effective for parsing? In
Proceedings of COLING.
Joakim Nivre and Ryan McDonald. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, Prague, Czech
Republic.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC2006).
M??chea?l O? Siadhail. 1989. Modern Irish: Grammatical
structure and dialectal variation. Cambridge: Cam-
bridge University Press.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic prediction of parser accuracy. In Proceedings
of EMNLP, Hawaii.
Ines Rehbein. 2011. Data point selection for self-
training. In Proceedings of the Second Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2011), Dublin, Ireland.
Roi Reichart and Ari Rappaport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
ACL.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL shared task
session of EMNLP-CoNLL.
Kenji Sagae. 2010. Self-training without reranking for
parser domain adapation and its impact on semantic
role labelling. In Proceedings of the ACL Workshop
on Domain Adaptation for NLP.
Sebastian Seung, Manfred Opper, and Haim Sompolin-
sky. 1992. Query by committee. In Proceedings
of the Fifth Annual ACM Workshop on Computational
Learning Theory.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the tenth conference on European chapter
of the Association for Computational Linguistics - Vol-
ume 1, EACL ?03, pages 331?338, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nancy Stenson. 1981. Studies in Irish Syntax. Tu?bingen:
Gunter Narr Verlag.
Elaine U?? Dhonnchadha. 2009. Part-of-Speech Tagging
and Partial Parsing for Irish using Finite-State Trans-
ducers and Constraint Grammar. Ph.D. thesis, Dublin
City University.
11
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13?23,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Code Mixing: A Challenge for Language Identification in the Language of
Social Media
Utsab Barman, Amitava Das
?
, Joachim Wagner and Jennifer Foster
CNGL Centre for Global Intelligent Content, National Centre for Language Technology
School of Computing, Dublin City University, Dublin, Ireland
?
Department of Computer Science and Engineering
University of North Texas, Denton, Texas, USA
{ubarman,jwagner,jfoster}@computing.dcu.ie
amitava.das@unt.edu
Abstract
In social media communication, multilin-
gual speakers often switch between lan-
guages, and, in such an environment, au-
tomatic language identification becomes
both a necessary and challenging task.
In this paper, we describe our work in
progress on the problem of automatic
language identification for the language
of social media. We describe a new
dataset that we are in the process of cre-
ating, which contains Facebook posts and
comments that exhibit code mixing be-
tween Bengali, English and Hindi. We
also present some preliminary word-level
language identification experiments using
this dataset. Different techniques are
employed, including a simple unsuper-
vised dictionary-based approach, super-
vised word-level classification with and
without contextual clues, and sequence la-
belling using Conditional Random Fields.
We find that the dictionary-based approach
is surpassed by supervised classification
and sequence labelling, and that it is im-
portant to take contextual clues into con-
sideration.
1 Introduction
Automatic processing and understanding of Social
Media Content (SMC) is currently attracting much
attention from the Natural Language Processing
research community. Although English is still by
far the most popular language in SMC, its domi-
nance is receding. Hong et al. (2011), for exam-
ple, applied an automatic language detection algo-
rithm to over 62 million tweets to identify the top
10 most popular languages on Twitter. They found
that only half of the tweets were in English. More-
over, mixing multiple languages together (code
mixing) is a popular trend in social media users
from language-dense areas (C?ardenas-Claros and
Isharyanti, 2009; Shafie and Nayan, 2013). In
a scenario where speakers switch between lan-
guages within a conversation, sentence or even
word, the task of automatic language identifica-
tion becomes increasingly important to facilitate
further processing.
Speakers whose first language uses a non-
Roman alphabet write using the Roman alphabet
for convenience (phonetic typing) which increases
the likelihood of code mixing with a Roman-
alphabet language. This can be especially ob-
served in South-East Asia and in the Indian sub-
continent. The following is a code mixing com-
ment taken from a Facebook group of Indian uni-
versity students:
Original: Yaar tu to, GOD hain. tui JU
te ki korchis? Hail u man!
Translation: Buddy you are GOD. What
are you doing in JU? Hail u man!
This comment is written in three languages: En-
glish, Hindi (italics), and Bengali (boldface). For
Bengali and Hindi, phonetic typing has been used.
We follow in the footsteps of recent work on
language identification for SMC (Hughes et al.,
2006; Baldwin and Lui, 2010; Bergsma et al.,
2012), focusing specifically on the problem of
word-level language identification for code mixing
SMC. Our corpus for this task is collected from
Facebook and contains instances of Bengali(BN)-
English(EN)-Hindi(HI) code mixing.
The paper is organized as follows: in Section 2,
we review related research in the area of code
mixing and language identification; in Section 3,
we describe our code mixing corpus, the data it-
13
self and the annotation process; in Section 4, we
list the tools and resources which we use in our
language identification experiments, described in
Section 5. Finally, in Section 6, we conclude
and provide suggestions for future research on this
topic.
2 Background and Related Work
The problem of language identification has been
investigated for half a century (Gold, 1967) and
that of computational analysis of code switching
for several decades (Joshi, 1982), but there has
been less work on automatic language identifi-
cation for multilingual code-mixed texts. Before
turning to that topic, we first briefly survey studies
on the general characteristics of code mixing.
Code mixing is a normal, natural product of
bilingual and multilingual language use. Signif-
icant studies of the phenomenon can be found
in the linguistics literature (Milroy and Muysken,
1995; Alex, 2008; Auer, 2013). These works
mainly discuss the sociological and conversational
necessities behind code mixing as well as its lin-
guistic nature. Scholars distinguish between inter-
sentence, intra-sentence and intra-word code mix-
ing.
Several researchers have investigated the rea-
sons for and the types of code mixing. Initial stud-
ies on Chinese-English code mixing in Hong Kong
(Li, 2000) and Macao (San, 2009) indicated that
mainly linguistic motivations were triggering the
code mixing in those highly bilingual societies.
Hidayat (2012) showed that Facebook users tend
to mainly use inter-sentential switching over intra-
sentential, and report that 45% of the switching
was instigated by real lexical needs, 40% was used
for talking about a particular topic, and 5% for
content clarification. The predominance of inter-
sentential code mixing in social media text was
also noted in the study by San (2009), which com-
pared the mixing in blog posts to that in the spoken
language in Macao. Dewaele (2010) claims that
?strong emotional arousal? increases the frequency
of code mixing. Dey and Fung (2014) present
a speech corpus of English-Hindi code mixing in
student interviews and analyse the motivations for
code mixing and in what grammatical contexts
code mixing occurs.
Turning to the work on automatic analysis of
code mixing, there have been some studies on de-
tecting code mixing in speech (Solorio and Liu,
2008a; Weiner et al., 2012). Solorio and Liu
(2008b) try to predict the points inside a set of spo-
ken Spanish-English sentences where the speak-
ers switch between the two languages. Other
studies have looked at code mixing in differ-
ent types of short texts, such as information re-
trieval queries (Gottron and Lipka, 2010) and SMS
messages (Farrugia, 2004; Rosner and Farrugia,
2007). Yamaguchi and Tanaka-Ishii (2012) per-
form language identification using artificial mul-
tilingual data, created by randomly sampling text
segments from monolingual documents. King
and Abney (2013) used weakly semi-supervised
methods to perform word-level language identifi-
cation. A dataset of 30 languages has been used
in their work. They explore several language
identification approaches, including a Naive Bayes
classifier for individual word-level classification
and sequence labelling with Conditional Random
Fields trained with Generalized Expectation crite-
ria (Mann and McCallum, 2008; Mann and Mc-
Callum, 2010), which achieved the highest scores.
Another very recent work on this topic is (Nguyen
and Do?gru?oz, 2013). They report on language
identification experiments performed on Turkish
and Dutch forum data. Experiments have been
carried out using language models, dictionaries,
logistic regression classification and Conditional
Random Fields. They find that language models
are more robust than dictionaries and that contex-
tual information is helpful for the task.
3 Corpus Acquisition
Taking into account the claim that code mixing is
frequent among speakers who are multilingual and
younger in age (C?ardenas-Claros and Isharyanti,
2009), we choose an Indian student community
between the 20-30 year age group as our data
source. India is a country with 30 spoken lan-
guages, among which 22 are official. code mix-
ing is very frequent in the Indian sub-continent
because languages change within very short geo-
distances and people generally have a basic knowl-
edge of their neighboring languages.
A Facebook group
1
and 11 Facebook users
(known to the authors) were selected to obtain
publicly available posts and comments. The Face-
book graph API explorer was used for data collec-
tion. Since these Facebook users are from West
Bengal, the most dominant language is Bengali
1
https://www.facebook.com/jumatrimonial
14
(Native Language), followed by English and then
Hindi (National Language of India). The posts
and comments in Bengali and Hindi script were
discarded during data collection, resulting in 2335
posts and 9813 comments.
3.1 Annotation
Four annotators took part in the annotation task.
Three were computer science students and the
other was one of the authors. The annotators are
proficient in all three languages of our corpus. A
simple annotation tool was developed which en-
abled these annotators to identify and distinguish
the different languages present in the content by
tagging them. Annotators were supplied with 4
basic tags (viz. sentence, fragment, inclusion and
wlcm (word-level code mixing)) to annotate differ-
ent levels of code mixing. Under each tag, six at-
tributes were provided, viz. English (en), Bengali
(bn), Hindi (hi), Mixed (mixd), Universal (univ)
and Undefined (undef). The attribute univ is as-
sociated with symbols, numbers, emoticons and
universal expressions (e.g. hahaha, lol). The at-
tribute undef is specified for a sentence or a word
for which no language tags can be attributed or
cannot be categorized as univ. In addition, anno-
tators were instructed to annotate named entities
separately. What follows are descriptions of each
of the annotation tags.
Sentence (sent): This tag refers to a sentence
and can be used to mark inter-sentential code mix-
ing. Annotators were instructed to identify a sen-
tence with its base language (e.g. en, bn, hi and
mixd) or with other types (e.g. univ, undef ) as the
first task of annotation. Only the attribute mixd is
used to refer to a sentence which contains multi-
ple languages in the same proportion. A sentence
may contain any number of inclusions, fragments
and word-level code mixing. A sentence can be at-
tributed as univ if and only if it contains symbols,
numbers, emoticons, chat acronyms and no other
words (Hindi, English or Bengali). A sentence can
be attributed as undef if it is not a sentence marked
as univ and has words/tokens that can not be cate-
gorized as Hindi, English or Bengali. Some exam-
ples of sentence-level annotations are the follow-
ing:
1. English-Sentence:
[sent-lang=?en?] what a.....6 hrs long...but re-
ally nice tennis.... [/sent]
2. Bengali-Sentence:
[sent-lang=?bn?] shubho nabo borsho.. :)
[/sent]
3. Hindi Sentence:
[sent-lang=?hi?] karwa sachh ..... :( [/sent]
4. Mixed-Sentence:
[sent-lang=?mixd?] [frag-lang=?hi?] oye
hoye ..... angreji me kahte hai ke [/frag]
[frag-lang=?en?] I love u.. !!! [/frag] [/sent]
5. Univ-Sentence:
[sent-lang=?univ?] hahahahahahah....!!!!!
[/sent]
6. Undef-Sentence:
[sent-lang=?undef?] Hablando de una triple
amenaza. [/sent]
Fragment (frag): This refers to a group of for-
eign words, grammatically related, in a sentence.
The presence of this tag in a sentence conveys that
intra-sentential code mixing has occurred within
the sentence boundary. Identification of fragments
(if present) in a sentence was the second task of
annotation. A sentence (sent) with attribute mixd
must contain multiple fragments (frag) with a spe-
cific language attribute. In the fourth example
above, the sentence contains a Hindi fragment oye
hoye ..... angreji me kahte hai ke and an English
fragment I love u.. !!!, hence it is considered as a
mixd sentence. A fragment can have any number
of inclusions and word-level code mixing. In the
first example below, Jio is a popular Bengali word
appearing in the English fragment Jio.. good joke,
hence tagged as a Bengali inclusion. One can ar-
gue that the word Jio could be a separate Bengali
inclusion (i.e. can be tagged as a Bengali inclu-
sion outside the English fragment). But looking
at the syntactic pattern and the sense expressed by
the comment, the annotator kept it as a single unit.
In the second example below, an instance of word-
level code mixing, typer, has been found in an En-
glish fragment (where the root English word type
has the Bengali suffix r).
1. Fragment with Inclusion:
[sent-lang=?mixd?] [frag-lang=?en?] [incl-
lang=?bn?] Jio.. [/incl] good joke [/frag] [frag
lang=?bn?] ?amar Babin? [/frag] [/sent]
2. Fragment with Word-Level code mixing:
[sent-lang=?mixd?] [frag-lang=?en?] ? I will
find u and marry you ? [/frag] [frag-
lang=?bn?] [wlcm-type=?en-and-bn-suffix?]
typer [/wlcm] hoe glo to! :D [/frag] [/sent]
15
Inclusion (incl): An inclusion is a foreign word
or phrase in a sentence or in a fragment which
is assimilated or used very frequently in native
language. Identification of inclusions can be per-
formed after annotating a sentence and fragment
(if present in that sentence). An inclusion within a
sentence or fragment also denotes intra-sentential
code mixing. In the example below, seriously is an
English inclusion which is assimilated in today?s
colloquial Bengali and Hindi. The only tag that an
inclusion may contain is word-level code mixing.
1. Sentence with Inclusion:
[sent-lang=?bn?] Na re [incl-lang=?en?] seri-
ously [/incl] ami khub kharap achi. [/sent]
Word-Level code mixing (wlcm): This is the
smallest unit of code mixing. This tag was in-
troduced to capture intra-word code mixing and
denotes cases where code mixing has occurred
within a single word. Identifying word-level code
mixing is the last task of annotation. Annotators
were told to mention the type of word-level code
mixing in the form of an attribute (Base Language
+ Second Language) format. Some examples are
provided below. In the first example below, the
root word class is English and e is an Bengali suf-
fix that has been added. In the third example be-
low, the opposite can be observed ? the root word
Kando is Bengali, and an English suffix z has been
added. In the second example below, a named en-
tity suman is present with a Bengali suffix er.
1. Word-Level code mixing (EN-BN):
[wlcm-type=?en-and-bn-suffix?] classe
[/wlcm]
2. Word-Level code mixing (NE-BN):
[wlcm-type=?NE-and-bn-suffix?] sumaner
[/wlcm]
3. Word-Level code mixing (BN-EN):
[wlcm-type=?bn-and-en-suffix?] kandoz
[/wlcm]
3.1.1 Inter Annotator Agreement
We calculate word-level inter annotator agreement
(Cohen?s Kappa) on a subset of 100 comments
(randomly selected) between two annotators. Two
annotators are in agreement about a word if they
both annotate the word with the same attribute
(en, bn, hi, univ, undef ), regardless of whether
the word is inside an inclusion, fragment or sen-
tence. Our observations that the word-level anno-
tation process is not a very ambiguous task and
that annotation instruction is also straightforward
are confirmed in a high inter-annotator agreement
(IAA) with a Kappa value of 0.884.
3.2 Data Characteristics
Tag-level and word-level statistics of annotated
data that reveal the characteristics of our data set
are described in Table 1 and in Table 2 respec-
tively. More than 56% of total sentences and al-
most 40% of total tokens are in Bengali, which is
the dominant language of this corpus. English is
the second most dominant language covering al-
most 33% of total tokens and 35% of total sen-
tences. The amount of Hindi data is substantially
lower ? nearly 1.75% of total tokens and 2% of to-
tal sentences. However, English inclusions (84%
of total inclusions) are more prominent than Hindi
or Bengali inclusions and there are a substantial
number of English fragments (almost 52% of total
fragments) present in our corpus. This means that
English is the main language involved in the code
mixing.
Statistics of Different Tags
Tags En Bn Hi Mixd Univ Undef
sent 5,370 8,523 354 204 746 15
frag 288 213 40 0 6 0
incl 7,377 262 94 0 1,032 1
wlcm 477
Name Entity 3,602
Acronym 691
Table 1: Tag-level statistics
Word-Level Tag Count
EN 66,298
BN 79,899
HI 3,440
WLCM 633
NE 5,233
ACRO 715
UNIV 39,291
UNDEF 61
Table 2: Word-level statistics
3.2.1 Code Mixing Types
In our corpus, inter- and intra-sentential code mix-
ing are more prominent than word-level code mix-
ing, which is similar to the findings of (Hidayat,
2012) . Our corpus contains every type of code
mixing in English, Hindi and Bengali viz. in-
ter/intra sentential and word-level as described in
the previous section. Some examples of different
types of code mixing in our corpus are presented
below.
16
1. Inter-Sentential:
[sent-lang=?hi?] Itna izzat diye aapne mujhe
!!! [/sent]
[sent-lang=?en?] Tears of joy. :?( :?( [/sent]
2. Intra-Sentential:
[sent-lang=?bn?] [incl-lang=?en?] by d way
[/incl] ei [frag-lang=?en?] my craving arms
shall forever remain empty .. never hold u
close .. [/frag] line ta baddo [incl-lang=?en?]
cheezy [/incl] :P ;) [/sent]
3. Word-Level:
[sent-lang=?bn?] [incl-lang=?en?] 1st yr
[/incl] eo to ei [wlcm-type=?en+bnSuffix?]
tymer [/wlcm] modhye sobar jute jay ..
[/sent]
3.2.2 Ambiguous Words
Annotators were instructed to tag an English word
as English irrespective of any influence of word
borrowing or foreign inclusion but an inspection of
the annotations revealed that English words were
sometimes annotated as Bengali or Hindi. To un-
derstand this phenomenon we processed the list
of language (EN,BN and HI) word types (total
26,475) and observed the percentage of types that
were not always annotated with the one language
throughout the corpus. The results are presented in
Table 3. Almost 7% of total types are ambiguous
(i.e. tagged in different languages during annota-
tion). Among them, a substantial amount (5.58%)
are English/Bengali.
Label(s) Count Percentage
EN 9,109 34.40
BN 14,345 54.18
HI 1,039 3.92
EN or BN 1,479 5.58
EN or HI 61 0.23
BN or HI 277 1.04
EN or BN or HI 165 0.62
Table 3: Statistics of ambiguous and monolingual
word types
There are two reasons why this is happening:
Same Words Across Languages Some words
are the same (e.g. baba, maa, na, khali) in Hindi
and Bengali because both of the languages orig-
inated from a single language Sanskrit and share
a good amount of common vocabulary. It also
occurred in English-Hindi and English-Bengali as
a result of word borrowing. Most of these are
commonly used inclusions like clg, dept, ques-
tion, cigarette, and topic. Sometimes the anno-
tators were careful enough to tag such words as
English and sometimes these words were tagged
in the annotators? native languages. During cross
checking of the annotated data the same error pat-
terns were observed for multiple annotators, i.e.
tagging commonly used foreign words into native
language. It only demonstrates that these English
words are highly assimilated in the conversational
vocabulary of Bengali and Hindi.
Phonetic Similarity of Spellings Due to pho-
netic typing some words share the same surface
form across two and sometimes across three lan-
guages. As an example, to is a word in the three
languages: it has occurred 1209 times as English,
715 times as Bengali and 55 times as Hindi in our
data. The meaning of these words (e.g. to, bolo,
die) are different in different languages. This phe-
nomenon is perhaps exacerbated by the trend to-
wards short and noisy spelling in SMC.
4 Tools and Resources
We have used the following resources and tools in
our experiment.
Dictionaries
1. British National Corpus (BNC): We com-
pile a word frequency list from the BNC (As-
ton and Burnard, 1998).
2. SEMEVAL 2013 Twitter Corpus (Se-
mevalTwitter): To cope with the language
of social media we use the SEMEVAL 2013
(Nakov et al., 2013) training data for the
Twitter sentiment analysis task. This data
comes from a popular social media site and
hence is likely to reflect the linguistic proper-
ties of SMC.
3. Lexical Normalization List (LexNorm-
List): Spelling variation is a well-known
phenomenon in SMC. We use a lexical nor-
malization dictionary created by Han et al.
(2012) to handle the different spelling vari-
ations in our data.
Machine Learning Toolkits
1. WEKA: We use the Weka toolkit (Hall et
al., 2009) for our experiments in decision tree
training.
2. MALLET: CRF learning is applied using the
MALLET toolkit (McCallum, 2002).
17
3. Liblinear: We apply Support Vector Ma-
chine (SVM) learning with a linear kernel us-
ing the Liblinear package (Fan et al., 2008).
NLP Tools For data tokenization we used the
CMU Tweet-Tokenizer (Owoputi et al., 2013).
5 Experiments
Since our training data is entirely labelled at the
word-level by human annotators, we address the
word-level language identification task in a fully
supervised way.
Out of the total data, 15% is set aside as a
blind test set, while the rest is employed in our ex-
periments through a 5-fold cross-validation setup.
There is a substantial amount of token overlap be-
tween the cross-validation data and the test set ?
88% of total EN tokens, 86% of total Bengali to-
kens and 57% of total Hindi tokens of the test set
are present in the cross-validation data.
2
We address the problem of word-level in three
different ways:
1. A simple heuristic-based approach which
uses a combination of our dictionaries to clas-
sify the language of a word
2. Word-level classification using supervised
machine learning with SVMs but no contex-
tual information
3. Word-level classification using supervised
machine learning with SVMs and sequence
labelling using CRFs, both employing con-
textual information
Named entities and instances of word-level
code mixing are excluded from evaluation. For
systems which do not take the context of a word
into account, i.e. the dictionary-based approach
(Section 5.1) and the SVM approach without con-
textual clues (Section 5.2), named entities and in-
stances of word-level code mixing can be safely
excluded from training. For systems which do
take context into account, the CRF system (Sec-
tion 5.3.1) and the SVM system with contextual
clues (Section 5.3.2), these are included in train-
ing, because to exclude them would result in un-
realistic contexts. This means that these systems
2
We found 25 comments and 17 posts common between
the cross-validation data and the test set. The reason for this
is that users of social media often express themselves in a
concise way. Almost all of these common data consisted of 1
to 3 token(s). In most of the cases these tokens were emoti-
cons, symbols or universal expressions such as wow and lol.
As the percentage of these comments is low, we keep these
comments as they are.
can classify a word to be a named entity or an in-
stance of word-level code mixing. To avoid this,
we implement a post-processor which backs off in
these cases to a system which hasn?t seen named
entities or word-level code mixing in training (see
Section 5.3).
5.1 Dictionary-Based Detection
We start with dictionary-based language detec-
tion. Generally a dictionary-based language de-
tector predicts the language of a word based on
its frequency in multiple language dictionaries. In
our data the Bengali and Hindi tokens are phoneti-
cally typed. As no such transliterated dictionary is,
to our knowledge, available for Bengali and Hindi,
we use the training set words as dictionaries. For
words that have multiple annotations in training
data (ambiguous words), we select the majority
tag based on frequency, e.g. the word to will al-
ways be tagged as English.
Our English dictionaries are those described
in Section 4 (BNC, LexNormList, SemEvalTwit-
ter) and the training set words. For LexNorm-
List, we have no frequency information, and so
we consider it as a simple word list. To pre-
dict the language of a word, dictionaries with nor-
malized frequency were considered first (BNC,
SemEvalTwitter, Training Data), if not found,
word list look-up was performed. The predicted
language is chosen based on the dominant lan-
guage(s) of the corpus if the word appears in mul-
tiple dictionaries with same frequency or if the
word does not appear in any dictionary or list.
A simple rule-based method is applied to pre-
dict universal expressions. A token is considered
as univ if any of the following conditions satisfies:
? All characters of the token are symbols or
numbers.
? The token contains certain repetitions identi-
fied by regular expressions.(e.g. hahaha).
? The token is a hash-tag or an URL or
mention-tags (e.g. @Sumit).
? Tokens (e.g. lol) identified by a word list
compiled from the relevant 4/5th of the train-
ing data.
Table 4 shows the results of dictionary-based
detection obtained from 5-fold cross-validation
averaging. We try different combinations and fre-
quency thresholds of the above dictionaries. We
find that using a normalized frequency is helpful
18
and that a combination of LexNormList and Train-
ing Data dictionaries is suited best for our data.
Hence, we consider this as our baseline language
identification system.
Dictionary Accuracy(%)
BNC 80.09
SemevalTwitter 77.61
LexNormList 79.86
Training Data 90.21
LexNormList+TrainingData (Baseline) 93.12
Table 4: Average cross-validation accuracy of
dictionary-based detection
5.2 Word-Level Classification without
Contextual Clues
The following feature types are employed:
1. Char-n-grams (G): We start with a character
n-gram-based approach (Cavnar and Tren-
kle, 1994), which is most common and fol-
lowed by many language identification re-
searchers. Following the work of King and
Abney (2013), we select character n-grams
(n=1 to 5) and the word as the features in our
experiments.
2. Presence in Dictionaries (D): We use pres-
ence in a dictionary as a features for all avail-
able dictionaries in previous experiments.
3. Length of words (L): Instead of using the
raw length value as a feature, we follow our
previous work (Rubino et al., 2013; Wagner
et al., 2014) and create multiple features for
length using a decision tree (J48). We use
length as the only feature to train a decision
tree for each fold and use the nodes obtained
from the tree to create boolean features.
4. Capitalization (C): We use 3 boolean fea-
tures to encode capitalization information:
whether any letter in the word is capitalized,
whether all letters in the word are capitalized
and whether the first letter is capitalized.
We perform experiments with an SVM classifier
(linear kernel) for different combination of these
features.
3
Parameter optimizations (C range 2
-15
to 2
10
) for SVM are performed for each feature
3
According to (Hsu et al., 2010) the SVM linear kernel
with parameter C optimization is good enough when dealing
with a large number of features. Though an RBF kernel can
be more effective than a linear one, it is possible only after
proper optimization of C and ? parameters, which is compu-
tational expensive for such a large feature set.
Features Accuracy Features Accuracy
G 94.62 GD 94.67
GL 94.62 GDL 94.73
GC 94.64 GDC 94.72
GLC 94.64 GDLC 94.75
Table 5: Average cross-validation accuracy for
SVM word-level classification (without context),
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features
GDLC: 94.75%
GLC: 94.64% GDL: 94.73% GDC: 94.72%
GL: 94.62% GC: 94.64% GD: 94.67%
G: 94.62%
Figure 1: Average cross-validation accuracy for
SVM word-level classification (without context),
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features: cube visualization
set and best cross-validation accuracy is found for
the GDLC-based run (94.75%) at C=1 (see Table 5
and Fig. 1).
We also investigate the use of a dictionary-to-
char-n-gram back-off model ? the idea is to ap-
ply the char-n-gram model SVM-GDLC for those
words for which a majority-based decision is taken
during dictionary-based detection. However, it
does not outperform the SVM. Hence, we select
SVM-GDLC for the next steps of our experiments
as the best exemplar of our individual word-level
classifier (without contextual clues).
5.3 Language Identification with Contextual
Clues
Contextual clues can play a very important role in
word-level language identification. As an exam-
ple, a part of a comment is presented from cross-
validation fold 1 that contains the word die which
is wrongly classified by the SVM classifier. The
frequency of die in the training set of fold 1 is 6
for English, 31 for Bengali and 0 for Hindi.
Gold Data: ..../univ the/en movie/en
for/en which/en i/en can/en die/en for/en
19
Features Order-0 Order-1 Order-2
G 92.80 95.16 95.36
GD 93.42 95.59 95.98
GL 92.82 95.14 95.41
GDL 93.47 95.60 95.94
GC 92.07 94.60 95.05
GDC 93.47 95.62 95.98
GLC 92.36 94.53 95.02
GDLC 93.47 95.58 95.98
Table 6: Average cross-validation accuracy of
CRF-based predictions where G = char-n-gram, L
= length feature, D = single dictionary-based la-
bels (baseline system) and C = capitalization fea-
tures
...../univ
SVM Output: ..../univ the/en
movie/en for/en which/en i/en can/en
die/bn for/en ...../univ
We now investigate whether contextual informa-
tion can correct the mis-classified tags.
Although named entities and word-level code
mixing are excluded from evaluation, when deal-
ing with context it is important to consider named
entity and word-level code mixing during training
because these may contain some important infor-
mation. We include these tokens in the training
data for our context-based experiments, labelling
them as other. The presence of this new label may
affect the prediction for a language token during
classification and sequence labelling. To avoid this
situation, a 4-way (bn, hi, en, univ) backoff classi-
fier is trained separately on English, Hindi, Ben-
gali and universal tokens. During evaluation of
any context-based system we discard named en-
tity and word-level code mixing from the predic-
tion of that system. If any of the remaining tokens
is predicted as other we back off to the decision
of the 4-way classifier for that token. For the CRF
experiments (Section 5.3.1), the backoff classifier
is a CRF system, and, for the SVM experiments
(Section 5.3.2), the backoff classifier is an SVM
system.
5.3.1 Conditional Random Fields (CRF)
As our goal is to apply contextual clues, we first
employ Conditional Random Fields (CRF), an ap-
proach which takes history into account in pre-
dicting the optimal sequence of labels. We em-
ploy a linear chain CRF with an increasing or-
der (Order-0, Order-1 and Order-2) with 200 it-
erations for different feature combinations (used
GDLC: 95.98%
GLC: 95.02% GDL: 95.94% GDC: 95.98%
GL: 95.41% GC: 95.05% GD: 95.98%
G: 95.36%
Figure 2: CRF Order-2 results: cube visualisation
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features
Context Accuracy (%)
GDLC + P
1
94.66
GDLC + P
2
94.55
GDLC + N
1
94.53
GDLC + N
2
94.37
GDLC + P
1
N
1
95.14
GDLC + P
2
N
2
94.55
Table 7: Average cross-validation accuracy of
SVM (GDLC) context-based runs, where P-i =
previous i word(s) , N-i = next i word(s)
in SVM-based runs). However, we observe that
accuracy of CRF based runs decreases when bi-
narized length features (see Section 5.2 and dic-
tionary features (a feature for each dictionary) are
involved. Hence, we use the dictionary-based pre-
dictions of the baseline system to generate a single
dictionary feature for each token and only the raw
length value of a token instead of binarized length
features. The results are presented in Table 6 and
the second order results are visualized in Fig. 2.
As expected, the performance increases as the
order increases from zero to one and two. The use
of a single dictionary feature is also helpful. The
results for GDC, GDLC, and GD based runs are
almost similar (95.98%). However, we choose the
GDC system because it performed slightly better
(95.989%) than the GDLC (95.983%) and the GD
(95.983%) systems.
5.3.2 SVM with Context
We also add contextual clues to our SVM classi-
fier. To obtain contextual information we include
the previous and next two words as features in
the SVM-GDLC-based run.
4
All possible com-
4
We also experimented with extracting all GDLC features
for the context words but this did not help.
20
binations are considered during experiments (Ta-
ble 7). After C parameter optimization, the best
cross-validation accuracy is found for the P
1
N
1
(one word previous and one word next) run with
C=0.125 (95.14%).
5.4 Test Set Results
We apply our best dictionary-based system, our
best SVM system (with and without context) and
our best CRF system to the held-out test set. The
results are shown in Table 8. Our best result is
achieved using the CRF model (95.76%).
5.5 Error Analysis
Manual error analysis shows the limitations of
these systems. The word-level classifier without
contextual clues does not perform well with Hindi
data. The number of Hindi tokens is quite low.
Only 2.4% (4,658) of total tokens of the training
data are Hindi, out of which 55.36% are bilin-
gually ambiguous and 29.51% are tri-lingually
ambiguous tokens. Individual word-level systems
often fail to assign proper labels to ambiguous
words, but adding context information helps to
overcome this problem. Considering the previ-
ous example of die, both context-based SVM and
CRF systems classify it properly. Though the final
system CRF-GDC performs well, it also has some
limitations, failing to identify the language for the
tokens which appear very frequently in three lan-
guages (e.g. are, na, pic).
6 Conclusion
We have presented an initial study on automatic
language identification with Indian language code
mixing from social media communication. We
described our dataset of Bengali-Hindi-English
Facebook comments and we presented the results
of our word-level classification experiments on
this dataset. Our experimental results lead us to
conclude that character n-gram features are useful
for this task, contextual information is also impor-
tant and that information from dictionaries can be
effectively incorporated as features.
In the future we plan to apply the techniques
and feature sets that we used in these experiments
to other datasets. We have already started this by
applying variants of the systems presented here to
the Nepali-English and Spanish-English datasets
which were introduced as part of the 2014 code
mixing shared task (Solorio et al., 2014; Barman
et al., 2014).
We did not include word-level code mixing in
our experiments ? in our future experiments we
will explore ways to identify and segment this type
of code mixing. It will be also important to find the
best way to handle inclusions since there is a fine
line between word borrowing and code mixing.
Acknowledgements
This research is supported by the Science Founda-
tion Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University. The
authors wish to acknowledge the DJEI/DES/SFI/
HEA for the provision of computational facili-
ties and support. Our special thanks to Soumik
Mandal from Jadavpur University, India for co-
ordinating the annotation task. We also thank the
administrator of JUMatrimonial and the 11 Face-
book users who agreed that we can use their posts
for their support and permission.
References
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, School of Informatics, The
University of Edinburgh, Edinburgh, UK.
Guy Aston and Lou Burnard. 1998. The BNC hand-
book: exploring the British National Corpus with
SARA. Capstone.
Peter Auer. 2013. Code-Switching in Conversation:
Language, Interaction and Identity. Routledge.
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237. Association for Computational Lin-
guistics.
Utsab Barman, Joachim Wagner, Grzegorz Chrupa?a,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching. EMNLP
2014, Conference on Empirical Methods in Natural
Language Processing, Doha, Qatar. Association for
Computational Linguistics.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74. Associ-
ation for Computational Linguistics.
21
System
Precision (%) Recall (%) Accuracy
(%)EN BN HI UNIV EN BN HI UNIV
Baseline (Dictionary) 92.67 90.73 80.64 99.67 92.28 94.63 43.47 94.99 93.64
SVM-GDLC 92.49 94.89 80.31 99.34 96.23 94.28 44.92 97.07 95.21
SVM-P
1
N
1
93.51 95.56 83.18 99.42 96.63 95.23 55.94 96.95 95.52
CRF-GDC 94.77 94.88 91.86 99.34 95.65 96.22 55.65 97.73 95.76
Table 8: Test set results for Baseline (Dictionary), SVM-GDLC, SVM-P1N1 and CRF-GDC
MS C?ardenas-Claros and N Isharyanti. 2009. Code-
switching and code-mixing in internet chatting:
Between?yes,?ya,?and?si?-a case study. The Jalt Call
Journal, 5(3):67?78.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Theo Pavlidis,
editor, Proceedings of SDAIR-94, Third Annual
Symposium on Document Analysis and Information
Retrieval, pages 161?175.
Jean-Marc Dewaele. 2010. Emotions in Multiple Lan-
guages. Palgrave Macmillan.
Anik Dey and Pascale Fung. 2014. A Hindi-
English code-switching corpus. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), pages 2410?
2413, Reykjavik, Iceland. European Language Re-
sources Association (ELRA).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Paulseph-John Farrugia. 2004. TTS pre-processing is-
sues for mixed language support. In Proceedings
of CSAW?04, the second Computer Science Annual
Workshop, pages 36?41. Department of Computer
Science & A.I., University of Malta.
E Mark Gold. 1967. Language identification in the
limit. Information and control, 10(5):447?474.
Thomas Gottron and Nedim Lipka. 2010. A compar-
ison of language identification approaches on short,
query-style texts. In Advances in Information Re-
trieval, pages 611?614. Springer.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explor. Newsl., 11(1):10?18.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421?432. Association
for Computational Linguistics.
Taofik Hidayat. 2012. An analysis of code switch-
ing used by facebookers: a case study in a
social network site. Student essay for the
study programme ?Pendidikan Bahasa Ing-
gris? (English Education) at STKIP Siliwangi
Bandung, Indonesia, http://publikasi.
stkipsiliwangi.ac.id/files/2012/
10/08220227-taofik-hidayat.pdf.
Lichan Hong, Gregorio Convertino, and Ed H. Chi.
2011. Language matters in twitter: A large scale
study. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media
(ICWSM-11), pages 518?521, Barcelona, Spain. As-
sociation for the Advancement of Artificial Intelli-
gence.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-
Jen Lin. 2010. A practical guide to sup-
port vector classification. Technical re-
port. Department of Computer Science, Na-
tional Taiwan University, Taiwan, https:
//www.cs.sfu.ca/people/Faculty/
teaching/726/spring11/svmguide.pdf.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proc. of the 5th edition of the Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2006), pages 485?488, Genoa, Italy.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In J. Horeck?y, ed-
itor, Proceedings of the 9th conference on Compu-
tational linguistics - Volume 1 (COLING?82), pages
145?150. Academia Praha, North-Holland Publish-
ing Company.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia. Association for Computa-
tional Linguistics.
David C. S. Li. 2000. Cantonese-English code-
switching research in Hong Kong: a Y2K review.
World Englishes, 19(3):305?322.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL-08: HLT, pages 870?878, Columbus,
Ohio. Association for Computational Linguistics.
22
Gideon S. Mann and Andrew McCallum. 2010.
Generalized expectation criteria for semi-supervised
learning with weakly labeled data. The Journal of
Machine Learning Research, 11:955?984.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Lesley Milroy and Pieter Muysken, editors. 1995. One
speaker, two languages: Cross-disciplinary per-
spectives on code-switching. Cambridge University
Press.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA. Association for Com-
putational Linguistics.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 857?862, Seattle,
Washington, USA. Association for Computational
Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2013), pages 380?390, Atlanta, Geor-
gia. Association for Computational Linguistics.
Mike Rosner and Paulseph-John Farrugia. 2007. A
tagging algorithm for mixed language identifica-
tion in a noisy domain. In INTERSPEECH-2007,
8th Annual Conference of the International Speech
Communication Association, pages 190?193. ISCA
Archive.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397, Sofia, Bulgaria.
Association for Computational Linguistics.
Hong Ka San. 2009. Chinese-English code-switching
in blogs by Macao young people. Master?s the-
sis, The University of Edinburgh, Edinburgh, UK.
http://hdl.handle.net/1842/3626.
Latisha Asmaak Shafie and Surina Nayan. 2013.
Languages, code-switching practice and primary
functions of facebook among university students.
Study in English Language Teaching, 1(1):187?
199. http://www.scholink.org/ojs/
index.php/selt.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051?
1060. Association for Computational Linguistics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar. Associ-
ation for Computational Linguistics.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings
of the International Workshop on Semantic Evalu-
ation (SemEval-2014), pages 392?397, Dublin, Ire-
land. Association for Computational Linguistics.
Jochen Weiner, Ngoc Thang Vu, Dominic Telaar, Flo-
rian Metze, Tanja Schultz, Dau-Cheng Lyu, Eng-
Siong Chng, and Haizhou Li. 2012. Integration
of language identification into a recognition system
for spoken conversations containing code-switches.
In Proceedings of the 3rd Workshop on Spoken Lan-
guage Technologies for Under-resourced Languages
(SLTU?12), Cape Town, South Africa. International
Research Center MICA.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012.
Text segmentation by language using minimum de-
scription length. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 969?978.
Association for Computational Linguistics.
23
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127?132,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
DCU-UVT: Word-Level Language Classification with Code-Mixed Data
Utsab Barman, Joachim Wagner, Grzegorz Chrupa?a
?
and Jennifer Foster
CNGL Centre for Global Intelligent Content, National Centre for Language Technology
School of Computing, Dublin City University, Dublin, Ireland
?
Tilburg School of Humanities, Department of Communication and Information Sciences
Tilburg University, Tilburg, The Netherlands
{ubarman,jwagner,jfoster}@computing.dcu.ie
G.A.Chrupala@uvt.nl
Abstract
This paper describes the DCU-UVT
team?s participation in the Language Iden-
tification in Code-Switched Data shared
task in the Workshop on Computational
Approaches to Code Switching. Word-
level classification experiments were car-
ried out using a simple dictionary-based
method, linear kernel support vector ma-
chines (SVMs) with and without con-
textual clues, and a k-nearest neighbour
approach. Based on these experiments,
we select our SVM-based system with
contextual clues as our final system and
present results for the Nepali-English and
Spanish-English datasets.
1 Introduction
This paper describes DCU-UVT?s participation
in the shared task Language Identification in
Code-Switched Data (Solorio et al., 2014) at
the Workshop on Computational Approaches to
Code Switching, EMNLP, 2014. The task is to
make word-level predictions (six labels: lang1,
lang2, ne, mixed, ambiguous and other) for mixed-
language user generated content. We submit pre-
dictions for Nepali-English and Spanish-English
data and perform experiments using dictionaries, a
k-nearest neighbour (k-NN) classifier and a linear-
kernel SVM classifier.
In our dictionary-based approach, we investi-
gate the use of different English dictionaries as
well as the training data. In the k-NN based
approach, we use string edit distance, character-
n-gram overlap and context similarity to make
predictions. For the SVM approach, we experi-
ment with context-independent (word, character-
n-grams, length of a word and capitalisation in-
formation) and context-sensitive (adding the pre-
vious and next word as bigrams) features in differ-
ent combinations. We also experiment with adding
features from the k-NN approach and another set
of features from a neural network. Based on per-
formance in cross-validation, we select the SVM
classifier with basic features (word, character-n-
grams, length of a word, capitalisation information
and context) as our final system.
2 Background
While the problem of automatically identify-
ing and analysing code-mixing has been iden-
tified over 30 years ago (Joshi, 1982), it has
only recently drawn wider attention. Specific
problems addressed include language identifica-
tion in multilingual documents, identification of
code-switching points and POS tagging (Solorio
and Liu, 2008b) of code-mixing data. Ap-
proaches taken to the problem of identifying code-
mixing include the use of dictionaries (Nguyen
and Do?gru?oz, 2013; Barman et al., 2014; El-
fardy et al., 2013; Solorio and Liu, 2008b), lan-
guage models (Alex, 2008; Nguyen and Do?gru?oz,
2013; Elfardy et al., 2013), morphological and
phonological analysis (Elfardy et al., 2013; El-
fardy and Diab, 2012) and various machine learn-
ing algorithms such as sequence labelling with
Hidden Markov Models (Farrugia, 2004; Ros-
ner and Farrugia, 2007) and Conditional Random
Fields (Nguyen and Do?gru?oz, 2013; King and
Abney, 2013), as well as word-level classifica-
tion using Naive Bayes (Solorio and Liu, 2008a),
logistic regression (Nguyen and Do?gru?oz, 2013)
and SVMs (Barman et al., 2014), using features
such as word, POS, lemma and character-n-grams.
Language pairs that have been explored include
English-Maltese (Farrugia, 2004; Rosner and Far-
rugia, 2007), English-Spanish (Solorio and Liu,
2008b), Turkish-Dutch (Nguyen and Do?gru?oz,
127
2013), modern standard Arabic-Egyptian di-
alect (Elfardy et al., 2013), Mandarin-English (Li
et al., 2012; Lyu et al., 2010), and English-Hindi-
Bengali (Barman et al., 2014).
3 Data Statistics
The training data provided for this task consists of
tweets. Unfortunately, because of deleted tweets,
the full training set could not be downloaded. Out
of 9,993 Nepali-English training tweets, we were
able to download 9,668 and out of 11,400 Spanish-
English training tweets, we were able to download
11,353. Table 1 shows the token-level statistics of
the two datasets.
Label Nepali-English Spanish-English
lang1 (en) 43,185 76,204
lang2 (ne/es) 59,579 32,477
ne 3,821 2,814
ambiguous 125 341
mixed 112 51
other 34,566 21,813
Table 1: Number of tokens in the Nepali-English
and Spanish-English training data for each label
Nepali (lang2) is the dominant language in
the Nepali-English training data but for Spanish-
English, English (lang1) is dominant. The third
largest group contains tokens with the label other.
These are mentions (@username), punctuation
symbols, emoticons, numbers (except numbers
that represent words such as 2 for to), words in a
language other than lang1 and lang2 and unintel-
ligible words. Named entities (ne) are much less
frequent and mixed language words (e.g. ramri-
ness) and words for which there is not enough con-
text to disambiguate them are rare. Hash tags are
annotated as if the hash symbol was not there, e.g.
#truestory is labelled lang1.
4 Experiments
All experiments are carried out for Nepali-English
data. Later we apply the best approach to Spanish-
English. We train our systems in a five-fold cross-
validation and obtain best parameters based on
average cross-validation results. Cross-validation
splits are made based on users, i.e. we avoid the
occurrence of a user?s tweets both in training and
test splits for each cross-validation run. We ad-
dress the task with the following approaches:
1. a simple dictionary-based classifier,
Resource Accuracy
BNC 43.61
LexNorm 54.60
TrainingData 89.53
TrainingData+BNC+LexNorm 90.71
Table 2: Average cross-validation accuracy of
dictionary-based prediction for Nepali-English
2. classification using supervised machine
learning with k-nearest neighbour, and
3. classification using supervised machine
learning with SVMs.
4.1 Dictionary-Based Detection
We start with a simple dictionary-based approach
using as dictionaries (a) the British National Cor-
pus (BNC) (Aston and Burnard, 1998), (b) Han
et al.?s lexical normalisation dictionary (LexNorm)
(Han et al., 2012) and (c) the training data.
The BNC and LexNorm dictionaries are built by
recording all words occurring in the respective
corpus or word list as English. For the BNC, we
also collect word frequency information. For the
training data, we obtain dictionaries for each of the
six labels and each of the five cross-validation runs
(using the relevant 4/5 of training data).
To make a prediction, we consult all dictionar-
ies. If there are more than one candidate label,
we choose the label for which the frequency for
the query token is highest. To account for the fact
that the BNC is much larger than the training data,
we normalise all frequencies before comparison.
LexNorm has no frequency information, hence it
is added to our system as a simple word list (we
consider the language of a word to be English if it
appears in LexNorm). If a word appears in multi-
ple dictionaries with the same frequency or if the
word does not appear in any dictionary or list, the
predicted language is chosen based on the domi-
nant language(s)/label(s) of the corpus.
We experiment with the individual dictionar-
ies and the combination of all three dictionaries,
among which the combination achieves the high-
est cross-validation accuracy (90.71%). Table 2
shows the results of dictionary-based detection ob-
tained in five-fold cross-validation.
4.2 Classification with k-NN
For Nepali-English, we also experiment with a
simple k-nearest neighbour (k-NN) approach. For
each test item, we select a subset of the training
data using string edit distance and n-gram overlap
128
and choose the majority label of the subset as our
prediction. For efficiency, we first select k
1
items
that share an n-gram with the token to be classi-
fied.
1
The set of k
1
items is then re-ranked ac-
cording to string edit distance to the test item and
the best k
2
matches are used to make a prediction.
Apart from varying k
1
and k
2
, we experiment
with (a) lowercasing strings, (b) including context
by concatenating the previous, current and next
token, and (c) weighting context by first calcu-
lating edit distances for the previous, current and
next token separately and using a weighted aver-
age. The best configuration we found in cross-
validation uses lowercasing with k
1
= 800 and
k
2
= 16 but no context information. It achieves
an accuracy of 94.97%.
4.3 SVM Classification
We experiment with linear kernel SVM classifiers
using Liblinear (Fan et al., 2008). Parameter opti-
misation
2
is performed for each feature set combi-
nation to obtain best cross-validation accuracy.
4.3.1 Basic Features
Following Barman et al. (2014), our basic features
are:
Char-N-Grams (G): We start with a charac-
ter n-gram-based approach (Cavnar and Trenkle,
1994). Following King and Abney (2013), we se-
lect lowercased character n-grams (n=1 to 5) and
the word as the features in our experiments.
Dictionary-Based Labels (D): We use presence
in the dictionary of the 5,000 most frequent words
in the BNC and presence in the LexNorm dictio-
nary as binary features.
3
Length of words (L): We create multiple fea-
tures for token length using a decision tree (J48).
We use length as the only feature to train a deci-
sion tree for each fold and use the nodes obtained
from the tree to create boolean features (Rubino et
al., 2013; Wagner et al., 2014).
1
Starting with n = 5, we decrease n until there are at
least k
1
items and then we randomly remove items added in
the last augmentation step to arrive at exactly k
1
items. (For
n = 0, we randomly sample from the full training data.)
2
C = 2
i
with i = ?15,?14, ..., 10
3
We chose these parameters based on experiments with
each dictionary, combinations of dictionaries and various fre-
quency thresholds. We apply a frequency threshold to the
BNC to increase precision. We rank the words according to
frequency and used the rank as a threshold (e.g. top-5K, top-
10K etc.). With the top 5,000 ranked words and C = 0.25,
we obtained best accuracy (96.40%).
Features Accuracy Features Accuracy
G 96.02 GD 96.27
GL 96.11 GDL 96.32
GC 96.15 GDC 96.20
GLC 96.21 GDLC 96.40
Table 3: Average cross-validation accuracy of 6-
way SVMs on the Nepali-English data set; G =
char-n-gram, L = binary length features, D = dict.-
based labels and C = capitalisation features
Context Accuracy(%)
GDLC + P
1
96.41
GDLC + P
2
96.38
GDLC + N
1
96.41
GDLC + N
2
96.41
GDLC + P
1
+ N
1
96.42
GDLC + P
2
+ N
2
96.41
Table 4: Average cross-validation accuracy of 6-
way SVMs using contextual features for Nepali-
English
Capitalisation (C): We choose 3 boolean
features to encode capitalisation information:
whether any letter in the word is capitalised,
whether all letters in the word are capitalised and
whether the first letter is capitalised.
Context (P
i
and N
j
): We consider the previous
i and next j token to be combined with the current
token, forming an (i+1)-gram and a (j+1)-gram,
which we add as features. Six settings are tested.
Table 4 shows that using the bigrams formed with
the previous and next word are the best combina-
tion for the task (among those tested).
Among the eight combinations of the first four
feature sets that contain the first set (G), Table 3
shows that the 6-way SVM classifier
4
performs
best with all features sets (GDLC), achieving
96.40% accuracy. Adding contextual information
P
i
N
j
to GDLC, Table 4 shows best results for
i=j=1, achieving 96.42% accuracy, only slightly
ahead of the context-independent system.
4.3.2 Neural Network (Elman) and k-NN
Features
We experiment with two additional features sets
not covered by Barman et al. (2014):
Neural Network (Elman): We extract features
from the hidden layer of a recurrent neural net-
4
We also test 3-way SVM classification (lang1, lang2 and
other) and heuristic post-processing, but it does not outper-
form our 6-way classification runs.
129
Systems Accuracy
GDLC 96.40
k-NN 95.10
Elman 89.96
GDLC+k-NN 96.31
GDLC+Elman 96.46
GDLC+k-NN+Elman 96.40
GDLC+P
1
N
1
96.42
k-NN+P
1
N
1
95.11
Elman+P
1
N
1
91.53
GDLC+P
1
N
1
+k-NN 96.33
GDLC+P
1
N
1
+Elman 96.45
GDLC+P
1
N
1
+k-NN+Elman 96.40
Table 5: Average cross-validation accuracy of 6-
way SVMs of combinations of GDLC, k-NN, El-
man and P
1
N
1
features for Nepali-English
work that has been trained to predict the next char-
acter in a string (Chrupa?a, 2014). The 10 most ac-
tive units of the hidden layer for each of the initial
4 bytes and final 4 bytes of each token are bina-
rised by using a threshold of 0.5.
k-Nearest Neighbour (kNN): We obtain fea-
tures from our basic k-NN approach (Section 4.2),
encoding the prediction of the k-NN model with
six binary features (one for each label) and a nu-
meric feature for each label stating the relative
number of votes for the label, e.g. if k
2
= 16
and 12 votes are for lang1 the value of the fea-
ture votes4lang1 will be 0.75. Furthermore, we
add two features stating the minimum and maxi-
mum edit distance between the test token and the
k
2
selected training tokens.
Table 5 shows cross-validation results for these
new feature sets with and without the P
1
N
1
con-
text features. Excluding the GDLC features, we
can see that best accuracy is with k-NN and P
1
N
1
features (95.11%). For Elman features, the accu-
racy is lower (91.53% with context). In combina-
tion with the GDLC features, however, the Elman
features can achieve a small improvement over
the GDLC+P
1
N
1
combination (+0.04 percentage
points): 96.46% accuracy for the GDLC+Elman
setting (without P
1
N
1
features). Furthermore, the
k-NN features do not combine well.
5
4.3.3 Final System and Test Results
At the time of submission of predictions, we had
an error in our GDLC+Elman feature combiner re-
5
A possible explanation may be that the k-NN features
are based on only 3 of 5 folds for the training data (3 folds
are used to make predictions for the 4th set) but 4 of 5 folds
are used for test data predictions in each cross-validation run.
Tweets
Token-Level Tweet-Level
Nepali-English 96.3 95.8
Spanish-English 84.4 80.4
Surprise Genre
Token-Level Post-Level
Nepali-English 85.6 77.5
Spanish-English 94.4 80.0
Table 6: Test set results (overall accuracy) for
Nepali-English and Spanish-English tweet data
and surprise genre
sulting in slightly lower performance. Therefore,
we selected SVM-GDLC-P
1
N
1
as our final ap-
proach and trained the final two systems using the
full training data for Nepali-English and Spanish-
English respectively. While we knew that C =
0.125 is best for Nepali-English from our experi-
ments, we had to re-tune parameter C for Spanish-
English using cross-validation on the training data.
We found best accuracy of 94.16% for Spanish-
English with C = 128. Final predictions for the
test sets are made using these systems.
Table 6 shows the test set results. The test
set for this task is divided into tweets and a sur-
prise genre. For the tweets, we achieve 96.3%
and 84.4% accuracy (overall token-level accuracy)
in Nepali-English and in Spanish-English respec-
tively. For this surprise genre (a collection of posts
from Facebook and blogs), we achieve 85.6% for
Nepali-English and 94.4% for Spanish-English.
5 Conclusion
To summarise, we achieved reasonable accuracy
with a 6-way SVM classifier by employing basic
features only. We found that using dictionaries
is helpful, as are contextual features. The perfor-
mance of the k-NN classifier is also notable: it is
only 1.45 percentage points behind the final SVM-
based system (in terms of cross-validation accu-
racy). Adding neural network features can further
increase the accuracy of systems.
Briefly opening the test files to check for for-
matting issues, we notice that the surprise genre
data contains language-specific scripts that could
easily be addressed in an English vs. non-English
scenario.
Acknowledgments
This research is supported by the Science Founda-
tion Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University.
130
References
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, School of Informatics, The
University of Edinburgh, Edinburgh, UK.
Guy Aston and Lou Burnard. 1998. The BNC hand-
book: exploring the British National Corpus with
SARA. Capstone.
Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code-mixing: A challenge
for language identification in the language of so-
cial media. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Theo Pavlidis,
editor, Proceedings of SDAIR-94, Third Annual
Symposium on Document Analysis and Information
Retrieval, pages 161?175.
Grzegorz Chrupa?a. 2014. Normalizing tweets with
edit scripts and recurrent neural embeddings. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 680?686, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.
Heba Elfardy and Mona Diab. 2012. Token level
identification of linguistic code switching. In Pro-
ceedings of Proceedings of COLING 2012: Posters
(the 24th International Conference on Computa-
tional Linguistics), pages 287?296, Mumbai, India.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in Ara-
bic. In Natural Language Processing and Informa-
tion Systems, pages 412?416. Springer.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Paulseph-John Farrugia. 2004. TTS pre-processing is-
sues for mixed language support. In Proceedings
of CSAW?04, the second Computer Science Annual
Workshop, pages 36?41. Department of Computer
Science & A.I., University of Malta.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421?432. Association
for Computational Linguistics.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In J. Horeck?y, ed-
itor, Proceedings of the 9th conference on Compu-
tational linguistics - Volume 1 (COLING?82), pages
145?150. Academia Praha, North-Holland Publish-
ing Company.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
mandarin-english code-switching corpus. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet Uur Doan, Bente Mae-
gaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
Dau-Cheng Lyu, Tien Ping Tan, Engsiong Chng, and
Haizhou Li. 2010. SEAME: A Mandarin-English
code-switching speech corpus in South-East Asia.
In INTERSPEECH 2010, 11th Annual Conference
of the International Speech Communication Asso-
ciation, volume 10, pages 1986?1989, Makuhari,
Chiba, Japan. ISCA Archive.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 857?862, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
Mike Rosner and Paulseph-John Farrugia. 2007. A
tagging algorithm for mixed language identifica-
tion in a noisy domain. In INTERSPEECH-2007,
8th Annual Conference of the International Speech
Communication Association, pages 190?193. ISCA
Archive.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397, Sofia, Bulgaria.
Association for Computational Linguistics.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
131
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051?
1060. Association for Computational Linguistics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2014), pages 392?397, Dublin, Ireland,
August. Association for Computational Linguistics.
132
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67?77,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Syntax and Semantics in Quality Estimation of Machine Translation
Rasoul Kaljahi
??
, Jennifer Foster
?
, Johann Roturier
?
?
NCLT, School of Computing, Dublin City University, Ireland
{rkaljahi, jfoster}@computing.dcu.ie
?
Symantec Research Labs, Dublin, Ireland
{johann roturier}@symantec.com
Abstract
We employ syntactic and semantic infor-
mation in estimating the quality of ma-
chine translation from a new data set
which contains source text from English
customer support forums and target text
consisting of its machine translation into
French. These translations have been both
post-edited and evaluated by professional
translators. We find that quality estima-
tion using syntactic and semantic informa-
tion on this data set can hardly improve
over a baseline which uses only surface
features. However, the performance can
be improved when they are combined with
such surface features. We also introduce
a novel metric to measure translation ade-
quacy based on predicate-argument struc-
ture match using word alignments. While
word alignments can be reliably used,
the two main factors affecting the per-
formance of all semantic-based methods
seems to be the low quality of seman-
tic role labelling (especially on ill-formed
text) and the lack of nominal predicate an-
notation.
1 Introduction
The problem of evaluating machine translation
output without reference translations is called
quality estimation (QE) and has recently been the
centre of attention (Bojar et al., 2014) following
the seminal work of Blatz et al. (2003). Most
QE studies have focused on surface and language-
model-based features of the source and target. The
quality of translation is however closely related to
the syntax and semantics of the languages, the for-
mer concerning fluency and the latter adequacy.
While there have been some attempts to utilize
syntax in this task, semantics has been paid less
attention. In this work, we aim to exploit both
syntax and semantics in QE, with a particular fo-
cus on the latter. We use shallow semantic analy-
sis obtained via semantic role labelling (SRL) and
employ this information in QE in various ways in-
cluding statistical learning using both tree kernels
and hand-crafted features. We also design a QE
metric which is based on the Predicate-Argument
structure Match (PAM ) between the source and its
translation. The semantic-based system is then
combined with the syntax-based system to evalu-
ate the full power of structural linguistic informa-
tion. We also combine this system with a baseline
system consisting of effective surface features.
A second contribution of the paper is the release
of a new data set for QE.
1
This data set comprises
a set of 4.5K sentences chosen from customer sup-
port forum text. The machine translation of the
sentences are not only evaluated in terms of ade-
quacy and fluency, but also manually post-edited
allowing various metrics of interest to be applied
to measure different aspects of quality. All exper-
iments are carried out on this data set.
The rest of the paper is organized as follows:
after reviewing the related work, the data is de-
scribed and the semantic role labelling approach
is explained. The baseline is then introduced, fol-
lowed by the experiments with tree kernels, hand-
crafted features, the PAM metric and finally the
combination of all methods. The paper ends with
a summary and suggestions for future work.
2 Related Work
Syntax has been exploited in QE in various ways
including tree kernels (Hardmeier et al., 2012;
Kaljahi et al., 2013; Kaljahi et al., 2014b),
parse probabilities and syntactic label frequency
(Avramidis, 2012), parseability (Quirk, 2004) and
POS n-gram scores (Specia and Gim?enez, 2010).
1
The data will be made publicly available - see http://
www.computing.dcu.ie/mt/confidentmt.html
67
Turning to the role of semantic knowledge in
QE and MT evaluation in general, Pighin and
M`arquez (2011) propose a method for ranking two
translation hypotheses that exploits the projection
of SRL from a sentence to its translation using
word alignments. They first project the SRL of a
source corpus to its parallel corpus and then build
two translation models: 1) translations of proposi-
tion labelling sequences in the source to its projec-
tion in the target and 2) translations of argument
role fillers in the source to their counterparts in
the target. The source SRL is then projected to
its machine translation and the above models are
forced to translate source proposition labelling se-
quences to the projected ones. Finally the confi-
dence scores of these translations and their reach-
ability are used to train a classifier which selects
the better of the two translation hypotheses with
an accuracy of 64%. Factors hindering their clas-
sifier are word alignment limitations and low SRL
recall due to the lack of a verb or the loss of a
predicate during translation.
In MT evaluation, where reference translations
are available, Gim?enez and M`arquez (2007) use
semantic roles in building several MT evaluation
metrics which measure the full or partial lexical
match between the fillers of same semantic roles in
the hypothesis and translation, or simply the role
label matches between them. They conclude that
these features can only be useful in combination
with other features and metrics reflecting different
aspects of the quality.
Lo and Wu (2011) introduce HMEANT, a man-
ual MT evaluation metric based on predicate-
argument structure matching which involves two
steps of human engagement: 1) semantic role an-
notation of the reference and machine translation,
2) evaluating the translation of predicates and ar-
guments. The metric calculates the F
1
score of
the semantic frame match between the reference
and machine translation based on this evaluation.
To keep the costs reasonable, the first step is car-
ried out by amateur annotators who were mini-
mally trained with a simplified list of 10 thematic
roles. On a set of 40 examples, the metric is
meta-evaluated in terms of correlation with human
judgements of translation adequacy ranking, and a
correlation as high as that of HTER is reported.
Lo et al. (2012) propose MEANT, a variant of
HMEANT, which automatizes its manual steps
using 1) automatic SRL systems for (only) verb
predicates, 2) automatic alignment of predicates
and their arguments in the reference and ma-
chine translation based on their lexical similarity.
Once the predicates and arguments are aligned,
their similarities are measured using a variety of
methods such as cosine distance and even Me-
teor and BLEU. In computation of the final score,
the similarity scores replace the counts of correct
and partial translations used in HMEANT. This
metric outperforms several automatic metrics in-
cluding BLEU, Meteor and TER, but it signifi-
cantly under-performs HMEANT and HTER. Fur-
ther analysis shows that automatizing the second
step does not affect the performance of MEANT.
Therefore, it seems to be the lower accuracy of the
semantic role labelling that is responsible.
Bojar and Wu (2012) identify a set of flaws
with HMEANT and propose solutions for them.
The most important problems stem from the su-
perficial SRL annotation guidelines. These prob-
lems are exacerbated in MEANT due to the auto-
matic nature of the two steps. More recently, Lo
et al. (2014) extend MEANT to ranking transla-
tions without a reference by using phrase transla-
tion probabilities for aligning semantic role fillers
of the source and its translation.
3 Data
We randomly select 4500 segments from a large
collection of Symantec English Norton forum
text.
2
In order to be independent of any one MT
system, we translate these segments into French
with the following three systems and randomly
choose 1500 distinct segments from each.
? ACCEPT
3
: a phrase-based Moses system
trained on training sets of WMT12 releases
of Europarl and News Commentary plus
Symantec translation memories
? SYSTRAN: a proprietary rule-based system
augmented with domain-specific dictionaries
? Bing
4
: an online translation system
These translations are evaluated in two ways.
The first method involves light post-editing by
a professional human translator who is a native
2
http://community.norton.com
3
http://www.accept.unige.ch/Products/
D_4_1_Baseline_MT_systems.pdf
4
http://www.bing.com/translator(on24-
Feb-2014)
68
Adequacy Fluency
5 All meaning Flawless Language
4 Most of meaning Good Language
3 Much of meaning Non-native Language
2 Little meaning Disfluent Language
1 None of meaning Incomprehensible
Table 2: Adequacy/fluency score interpretation
French speaker.
5
Each sentence translation is then
scored against its post-edit using BLEU
6
(Papineni
et al., 2002), TER (Snover et al., 2006) and
METEOR (Denkowski and Lavie, 2011), which are
the most widely used MT evaluation metrics. Fol-
lowing Snover et al. (2006), we consider this way
of scoring MT output to be a variation of human-
targeted scoring, where no reference translation
is provided to the post-editor, so we call them
HBLEU, HTER and HMETEOR. The average scores
for the entire data set together with their standard
deviations are presented in Table 1.
7
In the second method, we asked three profes-
sional translators, who are again native French
speakers, to assess the quality of MT output in
terms of adequacy and fluency in a 5-grade scale
(LDC, 2002). The interpretation of the scores is
given in Table 2. Each evaluator was given the
entire data set for evaluation. We therefore col-
lected three sets of scores and averaged them to
obtain the final scores. The averages of these
scores for the entire data set together with their
standard deviations are presented in Table 1. To
be easily comparable to human-targeted scores,
we scale these scores to the [0,1] range, i.e. ad-
equacy/fluency scores of 1 and 5 are mapped to 0
and 1 respectively and all the scores in between
are accordingly scaled.
The average Kappa inter-annotator agreement
for adequacy scores is 0.25 and for fluency scores
0.19. However, this measurement does not dif-
ferentiate between small and large differences in
agreement. In other words, the difference between
5
The post-editing guidelines are based on the
TAUS/CNGL guidelines for achieving ?good enough?
quality downloaded from https://evaluation.
taus.net/images/stories/guidelines/taus-
cngl-machine-translation-postediting-
guidelines.pdf.
6
Version 13a of MTEval script was used at the segment
level which performs smoothing.
7
Note that HTER scores have no upper limit and can be
higher than 1 when the number of errors is higher than the
segment length. In addition, the higher HTER indicates lower
translation quality. To be comparable to the other scores, we
cut-off them at 1 and convert to 1-HTER.
1-HTER HBLEU HMeteor Adq Flu
1-HTER - - - - -
HBLEU 0.9111 - - - -
HMeteor 0.9207 0.9314 - - -
Adq 0.6632 0.7049 0.6843 - -
Flu 0.6447 0.7213 0.6652 0.8824 -
Table 3: Pearson r between pairs of metrics on the
entire 4.5K data set
scores of 5 and 4 is the same as the difference
between 5 and 2. To account for this, we use
weighted Kappa instead. Specifically, we consider
two scores of difference 1 to represent 75% agree-
ment instead of 100%. All the other differences
are considered to be a disagreement. The aver-
age weighted Kappa computed in this way is 0.65
for adequacy and 0.63 for fluency. Though the
weighting used is quite strict, the weighted Kappa
values are in the substantial agreement range.
Once we have both human-targeted and manual
evaluation scores together, it is interesting to know
how they are correlated. We calculate the Pearson
correlation coefficient r between each pair of the
five scores and present them in Table 3. HBLEU
has the highest correlation with both adequacy and
fluency scores among the human-targeted metrics.
HTER on the other hand has the lowest correla-
tion. Moreover, HBLEU is more correlated with
fluency than with adequacy which is the opposite
to HMeteor. This is expected according to the
definition of BLEU and Meteor. There is also
a high correlation between adequacy and fluency
scores. Although this could be related to the fact
that both scores are from the same evaluators, it
indicates that if either the fluency and adequacy of
the MT output is low or high, the other tends to be
the same.
The data is split into train, development and test
sets of 3000, 500 and 1000 sentences respectively.
4 Semantic Role Labelling
The type of semantic information we use in this
work is the predicate-argument structure or se-
mantic role labelling of the sentence. This infor-
mation needs to be extracted from both sides of the
translation, i.e. English and French. Though the
SRL of English has been well-studied (M`arquez
et al., 2008) thanks to the existence of two major
hand-crafted resources, namely FrameNet (Baker
et al., 1998) and PropBank (Palmer et al., 2005),
French is one of the under-studied languages in
69
1-HTER HBLEU HMeteor Adequacy Fluency
Average 0.6976 0.5517 0.7221 0.6230 0.4096
Standard Deviation 0.2446 0.2927 0.2129 0.2488 0.2780
Table 1: Average and standard deviation of the evaluation scores for the entire data set
this respect mainly due to a lack of such resources.
The only available gold standard resource is a
small set of 1000 sentences taken from Europarl
(Koehn, 2005) and manually annotated with Prop-
bank verb predicates (van der Plas et al., 2010).
van der Plas et al. (2011) attempt to tackle this
scarcity by automatically projecting SRL from the
English side of a large parallel corpus to its French
side. Our preliminary experiments (Kaljahi et al.,
2014a), however, show that SRL models trained
on the small manually annotated corpus have a
higher quality than ones trained on the much larger
projected corpus. We therefore use the 1K gold
standard set to train a French SRL model. For En-
glish, we use all the data provided in the CoNLL
2009 shared task (Haji?c et al., 2009).
We use LTH (Bj?orkelund et al., 2009), a
dependency-based SRL system, for both the En-
glish and French data. This system was among
the best performing systems in the CoNLL 2009
shared task and is straightforward to use. It comes
with a set of features tuned for each shared task
language (English, German, Japanese, Spanish,
Catalan, Czech, Chinese). We compared the per-
formance of the English and Spanish feature sets
on French and chose the former due to its higher
performance (by 1 F
1
point).
It should be noted that the English SRL data
come with gold standard syntactic annotation. On
the other hand, for our QE data set, such anno-
tation is not available. Our preliminary experi-
ments show that, since the SRL system heavily
relies on syntactic features, the performance con-
siderably drops when the syntactic annotation of
the test data is obtained using a different parser
than that of the training data. We therefore re-
place the parses of the training data with those ob-
tained automatically by first parsing the data us-
ing the Lorg PCFG-LA parser
8
(Attia et al., 2010)
and then converting them to dependencies using
Stanford converter (de Marneffe and Manning,
2008). The POS tags are also replaced with those
output by the parser. For the same reason, we re-
8
https://github.com/CNGLdlab/LORG-
Release.
place the original POS tagging of the French 1K
data with those obtained by the MElt tagger (De-
nis and Sagot, 2012).
The English SRL achieves 77.77 and 67.02 la-
belled F
1
points when trained only on the training
section of PropBank and tested on the WSJ and
Brown test sets respectively.
9
The French SRL is
evaluated using 5-fold cross-validation on the 1K
data set and obtains an F
1
average of 67.66. When
applied to the QE data set, these models identify
9133, 8875 and 8795 propositions on its source
side, post-edits and MT output respectively.
5 Baseline
We compare the results of our experiments to a
baseline built using the 17 baseline features of the
WMT QE shared task (Bojar et al., 2014). These
features provide a strong baseline and have been
used in all three years of the shared task. We
use support vector regression implemented in the
SVMLight toolkit
10
with Radial Basis Function
(RBF) kernel to build this baseline. To extract
these features, a parallel English-French corpus
is required to build a lexical translation table us-
ing GIZA++ (Och and Ney, 2003). We use the
Europarl English-French parallel corpus (Koehn,
2005) plus around 1M segments of Symantec
translation memory.
Table 4 shows the performance of this system
(WMT17) on the test set measured by Root Mean
Square Error (RMSE) and Pearson correlation co-
efficient (r). We only report the results on predict-
ing four of the metrics introduced above, omitting
HMeteor due to space constraints. C and ? pa-
rameters are tuned on the development set with re-
spect to r. The results show a significant differ-
ence between manual and human-targeted metric
prediction. The higher r for the former suggests
that the patterns of these scores are easier to learn.
The RMSE seems to follow the standard deviation
9
Although the English SRL data are annotated for noun
predicates as well as verb predicates, since the French data
has only verb predicate annotations, we only consider verb
predicates for English.
10
http://svmlight.joachims.org/
70
of the scores as the same ranking is seen in both.
6 Tree Kernels
Tree kernels (Moschitti, 2006) have been success-
fully used in QE by Hardmeier et al. (2012) and
in our previous work (Kaljahi et al., 2013; Kal-
jahi et al., 2014b), where syntactic trees are em-
ployed. Tree kernels eliminate the burden of man-
ual feature engineering by efficiently utilizing all
subtrees of a tree. We employ both syntactic and
semantic information in learning quality scores,
using the SVMLight-TK
11
, a support vector ma-
chine (SVM) implementation of tree kernels.
We implement a syntactic tree kernel QE sys-
tem with constituency and dependency trees of
the source and target side, following our previous
work (Kaljahi et al., 2013; Kaljahi et al., 2014b).
The performance of this system (TKSyQE) is
shown in Table 4. Unlike our previous results,
where the syntax-based system significantly out-
performed the WMT17 baseline, TKSyQE can only
beat the baseline in HTER and fluency prediction,
with neither difference being statistically signifi-
cant and it is below the baseline for HBLEU and
adequacy prediction.
12
It should be noted that in
our previous work, a WMT News data set was
used as the QE data set which, unlike our new data
set, is well-formed and in the same domain as the
parsers? training data. The discrepancy between
our new and old results suggests that the perfor-
mance is strongly dependent on the data set.
Unlike syntactic parsing, semantic role la-
belling does not produce a tree to be directly used
in the tree kernel framework. There can be var-
ious ways to accomplish this goal. We first try
a method inspired by the PAS format introduced
by Moschitti et al. (2006). In this format, a fixed
number of nodes are gathered under a dummy root
node as slots of one predicate and 6 arguments of
a proposition (one tree per predicate). Each node
dominates an argument label or a dummy label for
the predicate, which in turn dominates the POS
tag of the argument or the predicate lemma. If a
proposition has more than 6 arguments they are
ignored, if it has fewer than 6 arguments, the extra
slots are attached to a dummy null label. Note that
these trees are derived from the dependency-based
SRL of both the source and target side (Figure
11
http://disi.unitn.it/moschitti/Tree-
Kernel.htm
12
We use paired bootstrap resampling Koehn (2004) for
statistical significance testing.
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
TKSyQE 0.2267 0.2721 0.2258 0.2431
D-PAS 0.2489 0.2856 0.2423 0.2652
D-PST 0.2409 0.2815 0.2383 0.2606
C-PST 0.2400 0.2809 0.2410 0.2615
CD-PST 0.2394 0.2795 0.2373 0.2578
TKSSQE 0.2269 0.2722 0.2253 0.2425
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
TKSyQE 0.3693 0.3559 0.4306 0.5013
D-PAS 0.1774 0.1843 0.2770 0.3252
D-PST 0.2136 0.2450 0.3169 0.3670
C-PST 0.2319 0.2541 0.2966 0.3616
CD-PST 0.2311 0.2714 0.3303 0.3923
TKSSQE 0.3682 0.3537 0.4351 0.5046
Table 4: RMSE and Pearson r of the 17 base-
line features (WMT17) and tree kernel systems;
TKSyQE: syntax-based tree kernels, D-PAS:
dependency-based PAS tree kernels of Moschitti
et al. (2006), D-PST, C-PST and CD-PST:
dependency-based, constituency-based proposi-
tion subtree kernels and their combination,
TKSSQE: syntactic-semantic tree kernels
1(a)). The results are shown in Table 4 (D-PAS).
The performance is statistically significantly lower
than the baseline.
13
In order to encode more information in the trees,
we propose another format in which proposition
subtrees (PST) of the sentence are gathered un-
der a dummy root node. A dependency PST (Fig-
ure 1(b)) is formed by the predicate label under
the root dominating its lemma and all its argu-
ments roles. Each of these nodes in turn dominates
three nodes: the argument word form (the predi-
cate word form for the case of a predicate lemma),
its syntactic dependency relation to its head and its
POS tag. We preserve the order of arguments and
predicate in the sentence.
14
This system is named
D-PST in Table 4. Tree kernels in this format sig-
nificantly outperform D-PAS. However, the per-
formance is still far lower than the baseline.
The above formats are based on dependency
trees. We try another PST format derived from
constituency trees. These PSTs (Figure 1(c)) are
the lowest common subtrees spanning the predi-
cate node and its argument nodes and are gath-
ered under a dummy root node. The argument role
13
Note that the only lexical information in this format is
the predicate lemma. We tried replacing the POS tags with
argument word forms, which led to a slight degradation.
14
This format is chosen among several other variations due
to its higher performance.
71
(a) D-PAS (b) D-PST (c) C-PST (d) D-TKSSQE (e) C-TKSSQE
Figure 1: Semantic tree kernel formats for the sentence: Can anyone help?
labels are concatenated with the syntactic non-
terminal category of the argument node. Predi-
cates are not marked. However, our dependency-
based SRL is required to be converted into a
constituency-based format. While constituency-
to-dependency conversion is straightforward us-
ing head-finding rules (Surdeanu et al., 2008),
the other way around is not. We therefore ap-
proximate the conversion using a heuristic we call
(D2C).
15
As shown in Table 4, the system built us-
ing these PSTs C-PST improves over D-PST for
human-targeted metric prediction, but not man-
ual metric prediction. However, when they are
combined in CD-PST, we can see improvement
over the highest scores of both systems, except
for HTER prediction for Pearson r. The fluency
prediction improvement is statistically significant.
The other changes are not statistically significant.
An alternative approach to formulating seman-
tic tree kernels is to augment syntactic trees with
semantic information. We augment the trees in
TKSyQE with semantic role labels. We attach se-
mantic roles to dependency labels of the argument
nodes in the dependency trees as in Figure 1(d).
For constituency trees, we use the D2C heuristic
to elevate roles up the terminal nodes and attach
the labels to the syntactic non-terminal category
of the node as in Figure 1(e). The performance
of the resulting system, TKSSQE, is shown in Ta-
ble 4. It substantially outperforms its counterpart,
CD-PST, all differences being statistically signif-
icant. However, compared to the plain syntactic
tree kernels (TKSyQE), the changes are slight and
inconsistent, rendering the augmentation not use-
ful. We consider this system to be our syntactic-
15
This heuristic (D2C) recursively elevates the argument
role already assigned to a terminal node (based on the
dependency-based argument position) to the parent node as
long as 1) the argument node is not a root node or is not
tagged as a POS (possessive), 2) the role is not an AM-NEG,
AM-MOD or AM-DIS adjunct, and 3) the argument does not
dominate its predicate?s node or another argument node of the
same proposition.
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
HCSyQE 0.2435 0.2797 0.2334 0.2479
HCSeQE 0.2482 0.2868 0.2416 0.2612
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
HCSyQE 0.2572 0.3080 0.3961 0.4696
HCSeQE 0.1794 0.1636 0.2972 0.3577
Table 5: RMSE and Pearson r of the 17 baseline
features (WMT17) and hand-crafted features
semantic tree kernel system.
7 Hand-crafted Features
In our previous work (Kaljahi et al., 2014b), we
experiment with a set of hand-crafted syntactic
features extracted from both constituency and de-
pendency trees on a different data set. We apply
the same feature set on the new data set here. The
results are reported in Table 5. The performance of
this system (HCSyQE) is significantly lower than
the baseline. This is opposite to what we ob-
serve with the same feature set on a different data
set, again showing that the role of data is funda-
mental in understanding system performance. The
main difference between these two data sets is that
the former is extracted from a well-formed text in
the news domain, the same domain on which our
parsers and SRL system have been trained, while
the new data set does not necessarily contain well-
formed text nor is it from the same domain.
We design another set of feature types aiming
at capturing the semantics of the source and trans-
lation via predicate-argument structure. The fea-
ture types are listed in Table 6. Feature types
1 to 8 each contain two features, one extracted
from the source and the other from the transla-
tion. To compute argument span sizes (feature
types 4 and 5), we use the constituency conver-
sion of SRL obtained using the D2C heuristic in-
troduced in Section 6. The proposition label se-
72
1 Number of propositions
2 Number of arguments
3 Average number of arguments per proposition
4 Sum of span sizes of arguments
5 Ratio of sum of span sizes of arguments to sentence
length
6 Proposition label sequences
7 Constituency label sequences of proposition elements
8 Dependency label sequences of proposition elements
9 Percentage of predicate/argument word alignment
mapping types
Table 6: Semantic feature types
quence (feature type 6) is the concatenation of ar-
gument roles and predicate labels of the propo-
sition with their preserved order (e.g. A0-go.01-
A4). Similarly, constituency and dependency la-
bel sequences (feature types 4 and 5) are extracted
by replacing argument and predicate labels with
their constituency and dependency labels respec-
tively. Feature type 9 consists of three features
based on word alignment of source and target
sentences: number of non-aligned, one-to-many-
aligned and many-to-one-aligned predicates and
arguments. The word alignments are obtained us-
ing the grow-diag-final-and heuristic as
they performed slightly better than other types.
16
As in the baseline system, we use SVMs to build
the QE systems using these hand-crafted features.
The nominal features are binarized to be usable by
SVM. However, the set of possible feature values
can be large, leading to a large number of binary
features. For example, there are more than 5000
unique proposition label sequences in our data.
Not only does this high dimensionality reduce the
efficiency of the system, it can also affect its per-
formance as these features are sparse. To tackle
this issue, we impose a frequency cutoff on these
features: we keep only frequent features using a
threshold set empirically on the development set.
Table 5 shows the performance of the system
(HCSeQE) built with these features. The semantic
features perform substantially lower than the syn-
tactic features and thus the baseline, especially in
predicting human-targeted scores. Since these fea-
tures are chosen from a comprehensive set of se-
mantic features, and as they should ideally capture
adequacy better than general features, a probable
reason for their low performance is the quality of
16
It should be noted that a number of features in addition
to those presented here have been tried, e.g. the ratio and dif-
ference of the source and target values of numerical features.
However, through manual feature selection, we have removed
features which do not appear to contribute much.
the underlying syntactic and semantic analysis.
8 Predicate-Argument Match (PAM)
Translation adequacy measures how much of the
source meaning is preserved in the translated text.
Predicate-argument structure or semantic role la-
belling expresses a substantial part of the meaning.
Therefore, the matching between the predicate-
argument structure of the source and its transla-
tion could be an important clue to the translation
adequacy, independent of the language pair used.
We attempt to exploit predicate-argument match
(PAM) to create a metric that measures the trans-
lation adequacy.
The algorithm to compute PAM score starts
by aligning the predicates and arguments of the
source side to its target side using word align-
ments.
17
It then treats the problem as one of SRL
scoring, similar to the scoring scheme used in the
CoNLL 2009 shared task (Haji?c et al., 2009). As-
suming the source side SRL as a reference, it com-
putes unlabelled precision and recall of the target
side SRL with respect to it:
UPrec =
# aligned preds and their args
# target side preds and args
URec =
# aligned preds and their args
# source side preds and args
Labelled precision and recall are calculated in
the same way except that they also require argu-
ment label agreement. UF
1
and LF
1
are the har-
monic means of unlabelled and labelled scores re-
spectively. Inspired by the observation that most
source sentences with no identified proposition are
short and can be assumed to be easier to translate,
and based on experiments on the dev set, we assign
a score of 1 to such sentences. When no proposi-
tion is identified in the target side while there is a
proposition in the source, we assign a score of 0.5.
We obtain word alignments using the Moses
toolkit (Hoang et al., 2009), which can gener-
ate alignments in both directions and combine
them using a number of heuristics. We try in-
tersection, union, source-to-target only, as well
as the grow-diag-final-and heuristic, but
only the source-to-target results are reported here
as they slightly outperform the others.
Table 7 shows the RMSE and Pearson r for
each of the unlabelled and labelled F
1
against ade-
17
We also tried lexical and phrase translation tables for this
purpose in addition to word alignments but they do not out-
perform word alignments.
73
1-HTER HBLEU Adq Flu
RMSE
1 UF
1
0.3175 0.3607 0.3108 0.4033
LF
1
0.4247 0.3903 0.3839 0.3586
Pearson r
UF
1
0.2328 0.2179 0.2698 0.2865
LF
1
0.1784 0.1835 0.2225 0.2688
Table 7: RMSE and Pearson r of PAM unlabelled
and labelled F
1
scores as estimation of the MT
evaluation metrics
1-HTER HBLEU Adq Flu
RMSE
PAM 0.2414 0.2833 0.2414 0.2661
HCSeQE 0.2482 0.2868 0.2416 0.2612
HCSeQE
pam
0.2445 0.2822 0.2370 0.2575
Pearson r
PAM 0.2292 0.2195 0.2787 0.3210
HCSeQE 0.1794 0.1636 0.2972 0.3577
HCSeQE
pam
0.2387 0.2368 0.3571 0.3908
Table 8: RMSE and Pearson r of PAM scores as
features, alone and combined (PAM)
quacy and also fluency scores on the test data set.
18
According to the results, the unlabelled F
1
(UF
1
)
is a closer estimation than the labelled one. Its
Pearson correlation scores are overall competitive
to the hand-crafted semantic features (HCSeQE in
Table 5): they are better for the automatic metric
cases but lower for manual ones. However, the
RMSE scores are considerably larger. Overall, the
performance is not comparable to the baseline and
other well performing systems. We investigate the
reasons behind this result in the next section.
Another way to employ the PAM scores in QE
is to use them in a statistical framework. We build
a SVM model using all 6 PAM scores The per-
formance of this system (PAM) on the test set is
shown in Table 8. The performance is consider-
ably higher than when the PAM scores are used
directly as estimations. Interestingly, compared to
the 47 semantic hand-crafted features (HCSeQE),
this small feature set performs better in predicting
human-targeted metrics.
We add these features to our set of hand-
crafted features in Section 7 to yield a new sys-
tem (HCSeQE
pam
in Table 8). All scores improve
compared to the stronger of the two components.
However, only the manual metric prediction im-
provements are statistically significant. The per-
formance is still not close to the baseline.
18
Precision and recall scores were also tried. Precision
proved to be the weakest estimator, whereas recall scores
were highest for some settings.
8.1 Analyzing PAM
Ideally, PAM scores should capture the adequacy
of translation with a high accuracy. The results
are however far from ideal. There are two fac-
tors involved in the PAM scoring procedure, the
quality of which can affect its performance: 1)
predicate-argument structure of the source and
target side of the translation, 2) alignment of
predicate-argument structures of source and target.
The SRL systems for both English and French
are trained on edited newswire. On the other
hand, our data is neither from the same domain nor
edited. The problem is exacerbated on the trans-
lation target side, where our French SRL system
is trained on only a small data set and applied to
machine translation output. To discover the con-
tribution of each of these factors in the accuracy
of PAM, we carry out a manual analysis. We ran-
domly select 10% of the development set (50 sen-
tences) and count the number of problems of each
of these two categories.
We find only 8 cases in which a wrong word
alignment misleads PAM scoring. On the other
hand, there are 219 cases of SRL problems, in-
cluding predicate and argument identification and
labelling: 82 cases (37%) in the source and 138
cases (63%) in the target.
We additionally look for the cases where a
translation divergence causes predicate-argument
mismatch in the source and translation. For ex-
ample, without sacrificing is translated into sans
impact sur (without impact on), a case of transpo-
sition, where the source side verb predicate is left
unaligned thus affecting the PAM score. We find
only 9 such cases in the sample, which is similar
to the proportion of word alignment problems.
As mentioned in the previous section, PAM
scoring has to assign default values for cases in
which there is no predicate in the source or tar-
get. This can be another source of estimation error.
In order to verify its effect, we find such cases in
the development set and manually categorize them
based on the reason causing the sentence to be left
without predicates. There are 79 (16%) source and
96 (19%) target sentences for which the SRL sys-
tems do not identify any predicate, out of which
64 cases have both sides without any predicate.
Among such source sentences, 20 (25%) have no
predicate due to a predicate identification error of
the SRL system, 57 (72%) because of the sentence
structure (e.g. copula verbs which are not labelled
74
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
SyQE 0.2255 0.2711 0.2248 0.2419
SeQE 0.2249 0.2710 0.2242 0.2404
SSQE 0.2246 0.2696 0.2230 0.2402
SSQE+WMT17 0.2225 0.2673 0.2202 0.2379
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
SyQE 0.3824 0.3650 0.4393 0.5087
SeQE 0.3884 0.3648 0.4447 0.5182
SSQE 0.3920 0.3768 0.4538 0.5196
SSQE+WMT17 0.4144 0.3953 0.4771 0.5331
Table 9: RMSE and Pearson r of the 17 baseline
features (WMT17) and system combinations
as predicates in the SRL training data, titles, etc.),
and the remaining 2 due to spelling errors mislead-
ing the SRL system. Among the target side sen-
tences, most of the cases are due to the sentence
structure (65 or 68%) and only 14 (15%) cases are
caused by an SRL error. In 13 cases, no verb pred-
icate in the source is translated correctly. Among
the remaining cases, two are due to untranslated
spelling errors in the source and the other two due
to tokenization errors misleading the SRL system.
These numbers show that the main reason lead-
ing to the sentences without verbal predicates is
the sentence structure. This problem can be al-
leviated by employing nominal predicates in both
sides. While this is possible for the English side,
there is currently no French resource where nomi-
nal predicates have been annotated.
9 Combining Systems
We now combine the systems we have built so
far (Table 9). We first combine syntax-based
and semantic-based systems individually. SyQE
is the combination of the syntactic tree kernel
system (TKSyQE) and the hand-crafted features
(HCSyQE). Likewise, SeQE is the combination
of the semantic tree kernel system (TKSSQE) and
the semantic hand-crafted features including PAM
features (HCSeQE
pam
). These two systems are
combined in SSQE but without syntactic tree ker-
nels (TKSyQE) to avoid redundancy with TKSSQE
as these are the augmented syntactic tree kernels.
We finally combine SSQE with the baseline.
SyQE significantly improves over its tree ker-
nel and hand-crafted components. It also outper-
forms the baseline in HTER and fluency predic-
tion, but is beaten by it in HBLEU and adequacy
prediction. None of these differences are statis-
tically significant however. SeQE also performs
better than the stronger of its components. Except
for adequacy prediction, the other improvements
are statistically significant. This system performs
slightly better than SyQE. Its comparison to the
baseline is the same as that of SyQE, except that
its superiority to the baseline in fluency prediction
is statistically significant.
The full syntactic-semantic system (SSQE) also
improves over its syntactic and semantic compo-
nents. However, the improvements are not statisti-
cally significant. Compared to the baseline, HTER
and fluency prediction perform better, the latter
being statistically significant. HBLEU prediction
is around the same as the baseline, but adequacy
prediction performance is lower, though not statis-
tically significantly.
Finally, when we combine the syntactic-
semantic system with the baseline system, the
combination continues to improve further. Com-
pared to the stronger component however, only the
HTER and fluency prediction improvements are
statistically significant.
10 Conclusion
We introduced a new QE data set drawn from cus-
tomer support forum text, machine translated and
both post-edited and manually evaluated for ad-
equacy and fluency. We used syntactic and se-
mantic QE systems via both tree kernels and hand-
crafted features. We found it hard to improve over
a baseline, albeit strong, using such information
which is extracted by applying parsers and seman-
tic role labellers on out-of-domain and unedited
text. We also defined a metric for estimating the
translation adequacy based on predicate-argument
structure match between source and target. This
metric relies on automatic word alignments and
semantic role labelling. We find that word align-
ment and translation divergence only have minor
effects on the performance of this metric, whereas
the quality of semantic role labelling is the main
hindering factor. Another major issue affecting the
performance of PAM is the unavailability of nom-
inal predicate annotation.
Our PAM scoring method is based on only word
matches as there are no constituent SRL resources
available for French ? perhaps constituent-based
arguments can make a more accurate comparison
between the source and target predicate-argument
structure possible.
75
Acknowledgments
This research has been supported by the Irish
Research Council Enterprise Partnership Scheme
(EPSPG/2011/102) and the computing infrastruc-
ture of the CNGL at DCU. We thank the reviewers
for their helpful comments.
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English
and French. In Proceedings of the 1st Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages.
Eleftherios Avramidis. 2012. Quality estimation for
Machine Translation output using linguistic analysis
and decoding features. In Proceedings of the 7th
WMT.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley Framenet project. In Proceed-
ings of the 36th ACL.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning: Shared Task.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence
estimation for Machine Translation. In JHU/CLSP
Summer Workshop Final Report.
Ond?rej Bojar and Dekai Wu. 2012. Towards a
predicate-argument evaluation for MT. In Proceed-
ings of the Sixth Workshop on Syntax, Semantics and
Structure in Statistical Translation.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Ale?s
Tamchyna. 2014. Findings of the 2014 workshop
on Statistical Machine Translation. In Proceedings
of the 9th WMT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies
representation. In Proceedings of the COLING
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Pascal Denis and Beno??t Sagot. 2012. Coupling an
annotated corpus and a lexicon for state-of-the-art
pos tagging. Lang. Resour. Eval., 46(4):721?736.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the 6th WMT.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguistic
features for automatic evaluation of heterogenous mt
systems. In Proceedings of the Second Workshop on
Statistical Machine Translation.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Tree kernels for machine translation
quality estimation. In Proceedings of the Seventh
WMT.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT).
Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, Jo-
hann Roturier, and Fred Hollowood. 2013. Parser
accuracy in quality estimation of machine transla-
tion: a tree kernel approach. In International Joint
Conference on Natural Language Processing (IJC-
NLP).
Rasoul Kaljahi, Jennifer Foster, and Johann Roturier.
2014a. Semantic role labelling with minimal re-
sources: Experiments with french. In Third Joint
Conference on Lexical and Computational Seman-
tics (*SEM).
Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, and
Johann Roturier. 2014b. Quality estimation of
english-french machine translation: A detailed study
of the role of syntax. In International Conference on
Computational Linguistics (COLING).
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit.
LDC. 2002. Linguistic data annotation specification:
Assessment of fluency and adequacy in chinese-
english translations. Technical report.
Chi-kiu Lo and Dekai Wu. 2011. Meant: An inex-
pensive, high-accuracy, semi-automatic metric for
evaluating translation utility via semantic frames. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic mt evaluation. In
Proceedings of the Seventh WMT.
76
Chi-kiu Lo, Meriem Beloucif, Markus Saers, and
Dekai Wu. 2014. Xmeant: Better semantic mt eval-
uation without reference translations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), June.
Llu??s M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: An introduction to the special
issue. Comput. Linguist., 34(2):145?159, June.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Tree kernel engineering for propo-
sition re-ranking. In Proceedings of Mining and
Learning with Graphs (MLG).
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proceedings
of EACL.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311?318.
Daniele Pighin and Llu??s M`arquez. 2011. Automatic
projection of semantic structures: An application to
pairwise translation ranking. In Proceedings of the
Fifth Workshop on Syntax, Semantics and Structure
in Statistical Translation, pages 1?9.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of
LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA.
Lucia Specia and Jes?us Gim?enez. 2010. Combining
confidence estimation and reference-based metrics
for segment level MT evaluation. In Proceedings of
AMTA.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
conll-2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning.
Lonneke van der Plas, Tanja Samard?zi?c, and Paola
Merlo. 2010. Cross-lingual validity of propbank
in the manual annotation of french. In Proceedings
of the Fourth Linguistic Annotation Workshop, LAW
IV ?10.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.
77
Proceedings of the First Celtic Language Technology Workshop, pages 41?49,
Dublin, Ireland, August 23 2014.
Cross-lingual Transfer Parsing for Low-Resourced Languages: An Irish
Case Study
Teresa Lynn
1,2
, Jennifer Foster
1
, Mark Dras
2
and Lamia Tounsi
1
1
CNGL, School of Computing, Dublin City University, Ireland
2
Department of Computing, Macquarie University, Sydney, Australia
1
{tlynn,jfoster,ltounsi}@computing.dcu.ie
2
{teresa.lynn,mark.dras}@mq.edu.au
Abstract
We present a study of cross-lingual direct transfer parsing for the Irish language. Firstly we
discuss mapping of the annotation scheme of the Irish Dependency Treebank to a universal de-
pendency scheme. We explain our dependency label mapping choices and the structural changes
required in the Irish Dependency Treebank. We then experiment with the universally annotated
treebanks of ten languages from four language family groups to assess which languages are the
most useful for cross-lingual parsing of Irish by using these treebanks to train delexicalised pars-
ing models which are then applied to sentences from the Irish Dependency Treebank. The best
results are achieved when using Indonesian, a language from the Austronesian language family.
1 Introduction
Considerable efforts have been made over the past decade to develop natural language processing re-
sources for the Irish language (U?? Dhonnchadha et al., 2003; U?? Dhonnchadha and van Genabith, 2006;
U?? Dhonnchadha, 2009; Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013). One such resource
is the Irish Dependency Treebank (Lynn et al., 2012a) which contains just over 1000 gold standard de-
pendency parse trees. These trees are labelled with deep syntactic information, marking grammatical
roles such as subject, object, modifier, and coordinator. While a valuable resource, the treebank does not
compare in size to similar resources of other languages.
1
The small size of the treebank affects the accu-
racy of any statistical parsing models learned from this treebank. Therefore, we would like to investigate
whether training data from other languages can be successfully utilised to improve Irish parsing.
Cross-lingual transfer parsing involves training a parser on one language, and parsing data of another
language. McDonald et al. (2011) describe two types of cross-lingual parsing, direct transfer parsing in
which a delexicalised version of the source language treebank is used to train a parsing model which
is then used to parse the target language, and a more complicated projected transfer approach in which
the direct transfer approach is used to seed a parsing model which is then trained to obey source-target
constraints learned from a parallel corpus. These experiments revealed that languages that were typo-
logically similar were not necessarily the best source-target pairs, sometimes due to variations between
their language-specific annotation schemes. In more recent work, however, McDonald et al. (2013) re-
ported improved results on cross-lingual direct transfer parsing using a universal annotation scheme, to
which six chosen treebanks are mapped for uniformity purposes. Underlying the experiments with this
new annotation scheme is the universal part-of-speech (POS) tagset designed by Petrov et al. (2012).
While their results confirm that parsers trained on data from languages in the same language group (e.g.
Romance and Germanic) show the most accurate results, they also show that training data taken across
language-groups also produces promising results. We attempt to apply the direct transfer approach with
Irish as the target language.
The Irish language belongs to the Celtic branch of the Indo-European language family. The natural
first step in cross-lingual parsing for Irish would be to look to those languages of the Celtic language
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
For example, the Danish dependency treebank has 5,540 trees (Kromann, 2003); the Finnish dependency treebank has
15,126 trees (Haverinen et al., 2013)
41
group, i.e. Welsh, Scots Gaelic, Manx, Breton and Cornish, as a source of training data. However,
these languages are just as, if not further, under-resourced. Thus, we attempt to use the languages of the
universal dependency treebanks (McDonald et al., 2013).
The paper is organised as follows. In Section 2, we give an overview of the status of the Irish lan-
guage and the Irish Dependency Treebank. Section 3 describes the mapping of the Irish Dependency
Treebank?s POS tagset (U?? Dhonnchadha and van Genabith, 2006) to that of Petrov et al. (2012), and
the Irish Dependency Treebank annotation scheme (Lynn et al. (2012b)) to the Universal Dependency
Scheme. Following that, in Section 4 we carry out cross-lingual direct transfer parsing experiments with
ten harmonised treebanks to assess whether any of these languages are suitable for such parsing transfer
for Irish. Section 5 summarises our work.
2 Irish Language and Treebank
Irish, a minority EU language, is the national and official language of Ireland. Despite this status, Irish
is only spoken on a daily basis by a minority. As a Celtic language, Irish shares specific linguistic
features with other Celtic languages, such as a VSO (verb-subject-object) word order and interesting
morphological features such as inflected prepositions and initial mutations, for example.
Compared to other EU-official languages, Irish language technology is under-resourced, as highlighted
by a recent study (Judge et al., 2012). In the area of morpho-syntactic processing, recent years have seen
the development of a part-of-speech tagger (U?? Dhonnchadha and van Genabith, 2006), a morphological
analyser (U?? Dhonnchadha et al., 2003), a shallow chunker (U?? Dhonnchadha, 2009), a dependency tree-
bank (Lynn et al., 2012a; Lynn et al., 2012b) and statistical dependency parsing models for MaltParser
(Nivre et al., 2006) and Mate parser (Bohnet, 2010) trained on this treebank (Lynn et al., 2013).
The annotation scheme for the Irish Dependency Treebank (Lynn et al., 2012b) was inspired by Lexical
Functional Grammar (Bresnan, 2001) and has its roots in the dependency annotation scheme described
by C?etino?glu et al. (2010). It was extended and adapted to suit the linguistic characterisics of the Irish
language. The final label set consists of 47 dependency labels, defining grammatical and functional
relations between the words in a sentence. The label set is hierarchical in nature with labels such as
vparticle (verb particle) and vocparticle (vocative particle), for example, representing more
fine-grained versions of the particle label.
3 A universal dependency scheme for the Irish Dependency Treebank
In this section, we describe how a ?universal? version of the Irish Dependency Treebank was created by
mapping the original POS tags to universal POS tags and mapping the original dependency scheme to the
universal dependency scheme. The result of this effort is an alternative version of the Irish Dependency
Treebank which will be made available to the research community along with the original.
3.1 Mapping the Irish POS tagset to the Universal POS tagset
The Universal POS tagset (Petrov et al., 2012) has been designed to facilitate unsupervised and cross-
lingual part-of-speech tagging and parsing research, by simplifying POS tagsets and unifying them across
languages. The Irish Dependency Treebank was built upon a POS-tagged corpus developed by U?? Dhon-
nchadha and van Genabith (2006). The treebank?s tagset contains both coarse- and fine-grained POS tags
which we map to the Universal POS tags (e.g. Prop Noun? NOUN). Table 1 shows the mappings.
Most of the POS mappings made from the Irish POS tagset to the universal tagset are intuitive. How-
ever, some decisions require explanation.
Cop ? VERB There are two verbs ?to be? in Irish: the substantive verb b?? and the copula is. For
that reason, the Irish POS tagset differentiates the copula by using the POS tag Cop. In Irish syntax
literature, there is some discussion over its syntactic role, whether it is a verb or a linking particle. The
role normally played is that of a linking element between a subject and a predicate. However, Lynn et al.
(2012a)?s syntactic analysis of the copula is in line with that of Stenson (1981), regarding it as a verb. In
addition, because the copula is often labelled in the Irish annotation scheme as the syntactic head of the
matrix clause, we have chosen VERB as the most suitable mapping for this part of speech.
42
Part-of-speech (POS) mappings
Universal Irish Universal Irish
NOUN
Noun Noun, Pron Ref,
Subst Subst, Verbal Noun,
Prop Noun
ADP
Prep Deg, Prep Det, Prep Pron,
Prep Simp, Prep Poss,
Prep CmpdNoGen, Prep Cmpd,
Prep Art, Pron Prep
PRON
Pron Pers, Pron Idf, Pron Q,
Pron Dem
ADV
Adv Temp, Adv Loc, Adv Dir,
Adv Q, Adv Its, Adv Gn
VERB
Cop Cop, Verb PastInd, Verb PresInd,
Verb PresImp, Verb VI, Verb VT,
Verb VTI, Verb PastImp, Verb Cond,
Verb FutInd, Verb VD, Verb Imper
PRT
Part Vb, Part Sup, Part Inf, Part Pat,
Part Voc, Part Ad, Part Deg, Part Comp,
Part Rel, Part Num, Part Cp,
DET Art Art, Det Det NUM Num Num
ADJ Prop Adj, Verbal Adj, Adj Adj X
Item Item, Abr Abr, CM CM, CU CU,
CC CC, Unknown Unknown,
Guess Abr, Itj Itj, Foreign Foreign,
CONJ Conj Coord, Conj Subord . . . ... ... ? ? ! ! : : ? . Punct Punct
Table 1: Mapping of Irish Coarse and Fine-grained POS pairs (coarse fine) to Universal POS tagset.
Pron Prep?ADP Pron Prep is the Irish POS tag for pronominal prepositions, which are also referred
to as prepositional pronouns. Characteristic of Celtic languages, they are prepositions inflected with their
pronominal objects ? compare, for example, le mo chara ?with my friend? with leis ?with him?. While
the Irish POS labelling scheme labels them as pronouns in the first instance, our dependency labelling
scheme treats the relationship between them and their syntactic heads as obl (obliques) or padjunct
(prepositional adjuncts). Therefore, we map them to ADP (adpositions).
3.2 Mapping the Irish Dependency Scheme to the Universal Dependency Scheme
The departure point for the design of the Universal Dependency Annotation Scheme (McDonald et
al., 2013) was the Stanford typed dependency scheme (de Marneffe and Manning, 2008), which was
adapted based on a cross-lingual analysis of six languages: English, French, German, Korean, Spanish
and Swedish. Existing English and Swedish treebanks were automatically mapped to the new universal
scheme. The rest of the treebanks were developed manually to ensure consistency in annotation. The
study also reports some structural changes (e.g. Swedish treebank coordination structures).
2
There are 41 dependency relation labels to choose from in the universal annotation scheme
3
. McDon-
ald et al. (2013) use all labels in the annotation of the German and English treebanks. The remaining
languages use varying subsets of the label set. In our study we map the Irish dependency annotation
scheme to 30 of the universal labels. The mappings are given in Table 2.
As with the POS mapping discussed in Section 3.1, mapping the Irish dependency scheme to the
universal scheme was relatively straightforward, due in part, perhaps, to a similar level of granularity
suggested by the similar label set sizes (Irish 47; standard universal 41). That said, there were significant
considerations made in the mapping process, which involved some structural change in the treebank and
the introduction of more specific analyses in the labelling scheme. These are discussed below.
3.2.1 Structural Differences
The following structural changes were made manually before the dependency labels were mapped to the
universal scheme.
coordination The most significant structural change made to the treebank was an adjustment to the
analysis of coordination. The original Irish Dependency Treebank subscribes to the LFG coordination
analysis, where the coordinating conjunction (e.g. agus ?and?) is the head, with the coordinates as its
dependents, labelled coord (see Figure 1). The Universal Dependency Annotation scheme, on the
2
There are two versions of the annotation scheme: the standard version (where copulas and adpositions are syntactic heads),
and the content-head version which treats content words as syntactic heads. We are using the standard version for our study.
3
The vmod label is used only in the content-head version.
43
Dependency Label Mappings
Universal Irish Universal Irish
root top csubj csubj
acomp adjpred, advpred, ppred dep for
adpcomp N/A det det, det2, dem
adpmod padjunct, obl, obl2, obl ag dobj obj, vnobj, obj q
adpobj pobj mark subadjunct
advcl N/A nmod addr, nadjunct
advmod
adjunct, advadjunct, quant,
advadjunct q
nsubj subj, subj q
amod adjadjunct num N/A
appos app p punctuation
attr npred parataxis N/A
aux toinfinitive poss poss
cc N/A prt
particle, vparticle, nparticle, advparticle,
vocparticle, particlehead, cleftparticle,
qparticle, aug
ccomp comp rcmod relmod
compmod nadjunct rel relparticle
conj coord xcomp xcomp
Table 2: Mapping of Irish Dependency Annotation Scheme to Universal Dependency Annotation Scheme
other hand, uses right-adjunction, where the first coordinate is the head of the coordination, and the
rest of the phrase is adjoined to the right, labelling coordinating conjunctions as cc and the following
coordinates as conj (Figure 2).
coord det subj advpred top coord det subj advpred obl det pobj
Bh?? an l?a an-te agus bh?? gach duine sti?ugtha leis an tart
Be-PAST the day very-hot and be-PAST every person parched with the thirst
?The day was very hot and everyone was parched with the thirst?
Figure 1: LFG-style coordination of original Irish Dependency Treebank
top det subj advpred cc conj det subj advpred obl det pobj
Bh?? an l?a an-te agus bh?? gach duine sti?ugtha leis an tart
Be-PAST the day very-hot and be-PAST every person parched with the thirst
?The day was very hot and everyone was parched with the thirst?
Figure 2: Stanford-style coordination changes to original Irish Dependency Treebank
subordinate clauses In the original Irish Dependency Treebank, the link between a matrix clause and
its subordinate clause is similar to that of LFG: the subordinating conjunction (e.g. mar ?because?, nuair
?when?) is a subadjunct dependent of the matrix verb, and the head of the subordinate clause is a
comp dependent of the subordinating conjunction (Figure 3). In contrast, the universal scheme is in
line with the Stanford analysis of subordinate clauses, where the head of the clause is dependent on the
matrix verb, and the subordinating conjunction is a dependent of the clause head (Figure 4).
3.2.2 Differences between dependency types
We found that the original Irish scheme makes distinctions that the universal scheme does not ? this
finer-grained information takes the form of the following Irish-specific dependency types: advpred,
44
top subj xcomp obl det pobj adjadjunct subadjunct comp subj ppred pobj num
Caithfidh t?u brath ar na himreoir?? ?aiti?ula nuair at?a t?u i Roinn 1
Have-to-FUT you rely on the players local when REL-be-PRES you in Division 1
?You have to rely on the local players when you are in Division 1?
Figure 3: LFG-style subordinate clause analysis (with original Irish Dependency labels)
top subj xcomp obl det pobj adjadjunct subadjunct comp subj ppred pobj num
Caithfidh t?u brath ar na himreoir?? ?aiti?ula nuair at?a t?u i Roinn 1
Have-to-FUT you rely on the players local when REL-be-PRES you in Division 1
?You have to rely on the local players when you are in Division 1?
Figure 4: Stanford-style subordinate clause analysis (with original Irish Dependency labels)
ppred, subj q, obj q, advadjunct q, obl, obl2. In producing the universal version of the tree-
bank, these Irish-specific dependency types are mapped to less informative universal ones (see Table 2).
Conversely, we found that the universal scheme makes distinctions that the Irish scheme does not. Some
of these dependency types are not needed for Irish. For example, there is no indirect object iobj in Irish,
nor is there a passive construction that would require nsubjpass, csubjpass or auxpass. Also, in
the Irish Dependency Treebank, the copula is usually the root (top) or the head of a subordinate clause
(e.g. comp) which renders the universal type cop redundant. Others that are not used are adp, expl,
infmod, mwe, neg, partmod. However, we did identify some dependency relationships in the univer-
sal scheme that we introduce to the universal Irish Dependency Treebank (adpcomp, adposition,
advcl, num, parataxis). These are explained below.
comp? adpcomp, advcl, parataxis, ccomp The following new mappings were previously subsumed
by the Irish dependency label comp (complement clause). The mapping for comp has thus been split
between adpcomp, advcl, parataxis and ccomp.
? adpcomp is a clausal complement of an adposition. An example from the English data is ?some
understanding of what the company?s long-term horizon should begin to look like?, where ?begin?,
as the head of the clause, is a dependent of the preposition ?of?. An example of how we use this
label in Irish is: an l??ne l?antosach is m?o cl?u a th?ainig as Ciarra?? ?o bh?? aimsir Sheehy ann ?the most
renowned forward line to come out of Kerry since Sheehy?s time? (lit. ?from it was Sheehy?s time?).
The verb bh?? ?was?, head of the dependent clause, is an adcomp dependent of the preposition ?o.
? advcl is used to identify adverbial clause modifiers. In the English data, they are often introduced
by subordinating conjunctions such as ?when?, ?because?, ?although?, ?after?, ?however?, etc. An
example is ?However, because the guaranteed circulation base is being lowered, ad rates will be
higher?. Here, ?lowered? is a advcl dependent of ?will?. An example of usage is: T?a truailli?u m?or
san ?ait mar nach bhfuil c?oras s?earachais ann ?There is a lot of pollution in the area because there
is no sewerage system?, where bhfuil ?is? is an advcl dependent of T?a ?is?.
45
? parataxis labels clausal structures that are separated from the previous clause with punctuation
such as ? ... : () ; and so on. Examples in Irish Is l?eir go bhfuil ag ?eir?? le feachtas an IDA ?
meastar gur in
?
Eirinn a lonna??tear timpeall 30% de na hionaid ?It is clear that the IDA campaign is
succeeding ? it is believed that 30% of the centres are based in Ireland?. Here, meastar ?is believed?
is a parataxis dependent of Is ?is?.
? ccomp covers all other types of clausal complements. For example, in English, ?Mr. Amos says the
Show-Crier team will probably do two live interviews a day?. The head of the complement clause
here is ?do?, which is a comp dependent of the matrix verb ?says?. A similar Irish example is: D?uirt
siad nach bhfeiceann siad an cine?al seo chomh minic ?They said that they don?t see this type as
often?. Here, bhfeiceann ?see? is the head of the complement clause, which is a comp dependent of
the verb D?uirt ?Said?.
quant? num, advmod The Irish Dependency Scheme uses one dependency label (quant) to cover
all types of numerals and quantifiers. We now use the universal scheme to differentiate between quanti-
fiers such as m?or?an ?many? and numerals such as fiche ?twenty?.
nadjunct? nmod, compmod The Irish dependency label nadjunct accounts for all nominal mod-
ifiers. However, in order to map to the universal scheme, we discriminate two kinds: (i) nouns that mod-
ify nouns (usually genitive case in Irish) are mapped to compmod (e.g. plean marga??ochta ?marketing
plan?) and (ii) nouns that modify clauses are mapped to nmod (e.g. bliain ?o shin ?a year ago?).
4 Parsing Experiments
We now describe how we extend the direct transfer experiments described in McDonald et al. (2013)
to Irish. In Section 4.1, we describe the datasets used in our experiments and explain the experimental
design. In Section 4.2, we present the results, which we then discuss in Section 4.3.
4.1 Data and Experimental Setup
We present the datasets used in our experiments and explain how they are used. Irish is the target
language for all our parsing experiments.
Universal Irish Dependency Treebank This is the universal version of the Irish Dependency Treebank
which contains 1020 gold-standard trees, which have been mapped to the Universal POS tagset and
Universal Dependency Annotation Scheme, as described in Section 3. In order to establish a monolingual
baseline against which to compare our cross-lingual results, we perform a five-fold cross-validation by
dividing the full data set into five non-overlapping training/test sets. We also test our cross-lingual models
on an delexicalised version of this treebank.
Transfer source training data For our direct transfer cross-lingual parsing experiments, we use 10 of
the standard version harmonised training data sets
4
made available by McDonald et al. (2013): Brazilian
Portuguese (PT-BR), English (EN), French (FR), German (DE), Indonesian (ID), Italian (IT), Japanese
(JA), Korean (KO), Spanish (ES) and Swedish (SV). For the purposes of uniformity, we select the first
4447 trees from each treebank ? to match the number of trees in the smallest data set (Swedish). We
delexicalise all treebanks and use the universal POS tags as both the coarse- and fine-grained values.
5
We train a parser on all 10 source data sets outlined and use each induced parsing model to parse and test
on a delexicalised version of the Universal Irish Dependency Treebank.
Largest transfer source training data - Universal English Dependency Treebank English has the
largest source training data set (sections 2-21 of the Wall Street Journal data in the Penn Treebank (Mar-
cus et al., 1993) contains 39, 832 trees). As with the smaller transfer datasets, we delexicalise this dataset
and use the universal POS tag values only. We experiment with this larger training set in order to establish
whether more training data helps in a cross-lingual setting.
4
Version 2 data sets downloaded from https://code.google.com/p/uni-dep-tb/
5
Note that the downloaded treebanks had some fine-grained POS tags that were not used across all languages: e.g. VERB-
VPRT (Spanish), CD (English).
46
Parser and Evaluation Metrics We use a transition-based dependency parsing system, MaltParser
(Nivre et al., 2006) for all of our experiments. All our models are trained using the stacklazy algorithm,
which can handle the non-projective trees present in the Irish data. In each case we report Labelled
Attachment Score (LAS) and Unlabelled Attachment Score (UAS).
6
4.2 Results
All cross-lingual results are presented in Table 3. Note that when we train and test on Irish (our mono-
lingual baseline), we achieve an average accuracy of 78.54% (UAS) and 71.59% (LAS) over the five
cross-validation runs. The cross-lingual results are substantially lower than this baseline. The LAS
results range from 0.84 (JA) to 43.88 (ID) and the UAS from 16.74 (JA) to 61.69 (ID).
SingleT MultiT LargestT
Training EN FR DE ID IT JA KO PT-BR ES SV All EN
UAS 51.72 56.84 49.21 61.69 50.98 16.74 18.02 57.31 57.00 49.95 57.69 51.59
LAS 35.03 37.91 33.04 43.88 37.98 0.84 9.35 42.13 41.94 34.02 41.38 33.97
Experiment SingleT-30 MultiT-30 LargestT-30
Training EN FR DE ID IT JA KO PT-BR ES SV All EN
Avg sent len 23 24 16 21 21 9 11 24 26 14 19 23
UAS 55.97 60.98 53.42 64.86 54.47 16.88 19.27 60.47 60.53 54.40 61.40 55.54
LAS 38.42 41.44 36.24 46.45 40.56 1.19 10.08 45.04 45.23 37.76 44.63 37.08
Table 3: Multi-lingual transfer parsing results
A closer look at the single-source transfer parsing evaluation results (SingleT) shows that some lan-
guage sources are particularly strong for parsing accuracy of certain labels. For example, ROOT (for
Indonesian), adpobj (for French) and amod (for Spanish). In response to these varied results, we ex-
plore the possibility of combining the strengths of all the source languages (multi-source direct transfer
(MultiT) ? also implemented by McDonald et al. (2011)). A parser is trained on a concatenation of
all the delexicalised source data described in Section 4.1 and tested on the full delexicalised Universal
Irish Dependency Treebank. Combining all source data produces parsing results of 57.69% (UAS) and
41.38% (LAS), which is outperformed by the best individual source language model.
Parsing with the large English training set (LargestT) yielded results of 51.59 (UAS) and 33.97 (LAS)
compared to a UAS/LAS of 51.72/35.05 for the smaller English training set. We investigated more
closely why the larger training set did not improve performance by incrementally adding training sen-
tences to the smaller set ? none of these increments reveal any higher scores, suggesting that English is
not a suitable source training language for Irish.
It is well known that sentence length has a negative effect on parsing accuracy. As noted in earlier
experiments (Lynn et al., 2012b), the Irish Dependency Treebank contains some very long difficult-to-
parse sentences (some legal text exceeds 300 tokens in length). The average sentence length is 27 tokens.
By placing a 30-token limit on the Universal Irish Dependency Treebank we are left with 778 sentences,
with an average sentence length of 14. We use this new 30-token-limit version of the Irish Dependency
Treebank data to test our parsing models. The results are shown in the lower half of Table 3. Not
surprisingly, the results rise substantially for all models.
4.3 Discussion
McDonald et al. (2013)?s single-source transfer parsing results show that languages within the same
language groups make good source-target pairs. They also show reasonable accuracy of source-target
pairing across language groups. For instance, the baseline when parsing French is 81.44 (UAS) and 73.37
(LAS), while the transfer results obtained using an English treebank are 70.14 (UAS) and 58.20(LAS).
Our baseline parser for Irish yields results of 78.54 (UAS) and 71.59 (LAS), while Indonesian-Irish
transfer results are 61.69 (UAS) and 43.88 (LAS).
The lowest scoring source language is Japanese. This parsing model?s output shows less than 3%
accuracy when identifying the ROOT label. This suggests the effect that the divergent word orders have
6
All scores are micro-averaged.
47
on this type of cross-lingual parsing ? VSO (Irish) vs SOV (Japanese). Another factor that is likely to be
playing a role is the size of the Japanese sentences. The average sentence length in the Japanese training
data is only 9 words, which means that this dataset is comparatively smaller than the others. It is also
worth noting that the universal Japanese treebank uses only 15 of the 41 universal labels (the universal
Irish treebank uses 30 of these labels).
As our best performing model (Indonesian) is an Austronesian language, we investigate why this
language does better when compared to Indo-European languages. We compare the results obtained by
the Indonesian parser with those of the English parser (SingleT). Firstly, we note that the Indonesian
parser captures nominal modification much better than English, resulting in an increased precision-recall
score of 60/67 on compmod. This highlights that the similarities in noun-noun modification between
Irish and Indonesian helps cross-lingual parsing. In both languages the modifying noun directly follows
the head noun, e.g. ?the statue of the hero? translates in Irish as dealbh an laoich (lit. statue the hero);
in Indonesian as patung palawan (lit. statue hero). Secondly, our analysis shows that the English parser
does not capture long-distance dependencies as well as the Indonesian parser. For example, we have
observed an increased difference in precision-recall of 44%-44% on mark, 12%-17.88% on cc and
4%-23.17% on rcmod when training on Indonesian. Similar differences have also been observed when
we compare with the French and English (LargestT) parsers. The Irish language allows for the use
of multiple conjoined structures within a sentence and it appears that long-distance dependencies can
affect cross-lingual parsing. Indeed, excluding very long sentences from the test set reveals substantial
increases in precision-recall scores for labels such as advcl, cc, conj and ccomp ? all of which are
labels associated with long-distance dependencies.
With this study, we had hoped that we would be able to identify a way to bootstrap the development
of the Irish Dependency Treebank and parser through the use of delexicalised treebanks annotated with
the Universal Annotation Scheme. While the current treebank data might capture certain linguistic phe-
nomena well, we expected that some cross-linguistic regularities could be taken advantage of. Although
the best cross-lingual model failed to outperform the monolingual model, perhaps it might be possible to
combine the strengths of the Indonesian and Irish treebanks? We performed 5-fold cross-validation on
the combined Indonesian and Irish data sets. The results did not improve over the Irish model. We then
analysed the extent of their complementarity by counting the number of sentences where the Indonesian
model outperformed the Irish model. This happened in only 20 cases, suggesting that there is no benefit
in using the Indonesian data over the Irish data nor in combining them at the sentence-level.
5 Conclusion and Future Work
In this paper, we have reported an implementation of cross-lingual direct transfer parsing of the Irish
language. We have also presented and explained our mapping of the Irish Dependency Treebank to the
Universal POS tagset and Universal Annotation Scheme. Our parsing results show that an Austronesian
language surpasses Indo-European languages as source data for cross-lingual Irish parsing.
In extending this research, there are many interesting avenues which could be explored including
the use of Irish as a source language for another Celtic language and experimenting with the projected
transfer approach of McDonald et al. (2011).
Acknowledgements
This research is supported by the Science Foundation Ireland (Grant 12/CE/I2267) as part of the CNGL
(www.cngl.ie) at Dublin City University. We thank the three anonymous reviewers for their helpful
feedback. We also thank Elaine U?? Dhonnchadha (Trinity College Dublin) and Brian
?
O Raghallaigh
(Fiontar, Dublin City University) for their linguistic advice.
References
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of COL-
ING?10.
48
Joan Bresnan. 2001. Lexical Functional Syntax. Oxford: Blackwell.
?
Ozlem C?etino?glu, Jennifer Foster, Joakim Nivre, Deirdre Hogan, Aoife Cahill, and Josef van Genabith. 2010. LFG
without C-structures. In Proceedings of the 9th International Workshop on Treebanks and Linguistic Theories.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Workshop on Crossframework and Cross-domain Parser Evaluation (COLING2008).
Katri Haverinen, Jenna Nyblom, Timo Viljanen, Veronika Laippala, Samuel Kohonen, Anna Missil?a, Stina Ojala,
Tapio Salakoski, and Filip Ginter. 2013. Building the essential resources for Finnish: the Turku dependency
treebank. Language Resources and Evaluation, pages 1?39.
John Judge, Ailbhe N?? Chasaide, Rose N?? Dhubhda, Kevin P. Scannell, and Elaine U?? Dhonnchadha. 2012. The
Irish Language in the Digital Age. Springer Publishing Company, Incorporated.
Matthias Kromann. 2003. The Danish Dependency Treebank and the DTAG Treebank Tool. In Proceedings of
the Second Workshop on Treebanks and Linguistic Theories (TLT2003).
Teresa Lynn,
?
Ozlem C?etino?glu, Jennifer Foster, Elaine U?? Dhonnchadha, Mark Dras, and Josef van Genabith.
2012a. Irish treebanking and parsing: A preliminary evaluation. In Proceedings of the Eight International
Conference on Language Resources and Evaluation (LREC?12), pages 1939?1946.
Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U?? Dhonnchadha. 2012b. Active learning and the Irish
treebank. In Proceedings of the Australasian Language Technology Workshop (ALTA), pages 23?32.
Teresa Lynn, Jennifer Foster, and Mark Dras. 2013. Working with a small dataset ? semi-supervised depen-
dency parsing for Irish. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich
Languages, pages 1?11, Seattle, Washington, USA, October. Association for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of
english: The Penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
62?72, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,
Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu, and Castell?o Jungmee
Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of ACL ?13.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for depen-
dency parsing. In Proceedings of the Fifth International Conference on Language Resources and Evaluation
(LREC2006).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the
Eight International Conference on Language Resources and Evaluation (LREC?12).
Nancy Stenson. 1981. Studies in Irish Syntax. T?ubingen: Gunter Narr Verlag.
Elaine U?? Dhonnchadha and Josef van Genabith. 2006. A part-of-speech tagger for Irish using finite-state morphol-
ogy and constraint grammar disambiguation. In Proceedings of the 5th International Conference on Language
Resources and Evaluation (LREC 2006).
Elaine U?? Dhonnchadha, Caoilfhionn Nic Ph?aid??n, and Josef van Genabith. 2003. Design, implementation and
evaluation of an inflectional morphology finite state transducer for Irish. Machine Translation, 18:173?193.
Elaine U?? Dhonnchadha. 2009. Part-of-Speech Tagging and Partial Parsing for Irish using Finite-State Transduc-
ers and Constraint Grammar. Ph.D. thesis, Dublin City University.
49
