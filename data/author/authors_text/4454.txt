Japanese Case Structure Analysis 
by Unsupervised Construction of a Case Frame Dictionary 
Daisuke Kawahara, Nobuhiro Kaji and Sadao Kurohashi 
Graduate  School of Intbrm~tics, Kyoto  University 
Yoshida-Honmachi ,  S~kyo-ku, Kyoto,  606-8501, Japan  
{kawahara, kaj i ,  kuro }@p inc. kuee. kyoto-u, ac. jp 
Abstract 
In Japanese, case structure analysis is very im- 
t)ortant to handle several troublesome charac- 
teristics of Japanese snch as scrambling, onfis- 
sion of ease components, mid disappearance of
case markers. However, fi)r lack of a wide- 
coverage ase frame dictionary, it has been dif- 
ficult to perfornl case structure analysis accu- 
rat;ely. Although several methods to construct 
a ease fl'mne dictionary from analyzed corpora 
have been proposed, they cannot avoid data 
sparseness 1)rol)lem. This paper proposes an un- 
supervised method of constructing a case frame 
dictionary from an enormous raw corpus by us- 
ing a robust and accurate parser. It also pro- 
rides a case structure analysis method based on 
the constructed ictionary. 
1 I n t roduct ion  
Syntactic analysis, or parsing has been a main 
objective in Natural Language Processing. In 
case of Jat)anese , however, syntactic analysis 
cannot clarify relations between words ill sen- 
tences because of several troublesome character- 
istics of Japanese such as scrambling, omission 
of case components, and disappearance of case 
markers. Therefore, in Japanese sentence analy- 
sis, case structure analysis is an important issue, 
and a case frame dictionary is necessary for the 
analysis. 
Some research institutes have constructed 
Japanese case frmne dictiouaries manually (Ike- 
hara et al, 1997; Infbrmation-Technology Pro- 
motion Agency, Japan, 1987). However, it is 
quite expensive, or almost impossible to con- 
struct a wide-coverage ease fl'anm dictionary by 
hand. 
Others have tried to construct a case fl'mne 
dictionary automatically from analyzed corpora 
(Utsuro et al, 1998). However, existing syntac- 
tically analyzed corpora are too small to learn a 
dictionary, since case fl'ame iuformation consists 
of relations between ouns and verbs, which rnul- 
tiplies to millions of combinations. 
Based on such a consideration, we took the 
fbllowing unsupervised learning strategy to the 
.Japanese case structure analysis: 
1. At first, a robust and accurate parser is de- 
veloped, which does not utilize a case fl'mne 
dictionary, 
2. a very large corI)us is parsed by the parser, 
3. reliable noun-verb relations are extracted 
from the parse results, and a case frmne dic- 
tionary is constructed from them, and 
4. the dictionary is utilized for case structure 
analysis. 
2 Characteristics of Japanese 
language and necessity of case 
s t ructure  ana lys i s  
In Japanese, postpositions function as case 
markers ( (Ms)  mid a verb is final in a sentence. 
The basic structure of a Japanese sentence is as 
fbllows: 
(1) kate  9a coat wo ki~'u. 
he nominative-CM coat accusative-CM wear 
(lie wears a coat) 
A clause modifier is left to the modified noun 
as follows: 
(2) kate  9 a k i te - i ru  coat 
lie nom-CM wear coat 
(the coat he wears) 
The modified noun followed by a postposition 
then becomes a case component of a matrix verb. 
The typical structure of a Japanese complex sen- 
tence is as fbllows: 
432 
(3) boush, i no irv wa kitc-ir'u 
hat of color tol)ic-marker wear 
coat ni a'wa~'cr"~,. 
coal; dative-CM harmonize 
(c/) harmonizes the color of his/her hat with 
the coat he/she wears) 
In terms of autolnatic analysis, the problen> 
atic characteristics of Japanese sentences can be 
summarized as follows: 
1. Case componenl;s are often scrambled or 
omitted. 
2. Case-marking postpositions disappear when 
case components are accompanied by topic- 
markers or other special 1)ostpositions 
meaning 'just', 'also' and others. 
cx) karv 'wa coat me ti:itc-iv'u. 
he tol}iC-lna.rker coat also wear 
(Ile wears a coat a.lso) 
3. A noun modified 1)y a clause is usually a case 
component for the verb of the mo(litlying 
clause. However, there is no case-marker for 
their relation. In case of sentence 3, there is 
no case-marker for coat in relation to kite- 
ir'u 'wear'. Note that 'hi (dative-CM) of coat: 
ni does not show the case to kitc-ivu 'wear', 
lint to awascr'v, 'harmonize'. 
4. Sentence 3 exhibits a typical structural am- 
1)iguity in a ,lalmnese sentence. That is, 
ir'o "~va 'color topic-marker' possit)ly modi- 
ties kit, c- iru 'wear' or awa.scv'u qmrmonize'. 
In English, sentence structure is rather rigid, 
and word order (the position in relation to the 
verb) clearly defines cases. In Japanese, how- 
ew% the l)roblem 1 above makes word order use- 
less, and CMs constitute the only int'ormation for 
detecting cases. 
Nevertheless, CMs often disapl)ear because of 
the problems 2 and 3, whidl means that sim- 
ple syntactic analysis cmmot clari(5~ cases sui\[i- 
cientl> For eXalnple, given an inlmt sentence: 
(4) har'c wa Dcv, tsch,-go me hano, sv,. 
he topic-marker Germall also sl)eak 
(he speaks German also) 
a simple syntactic analysis just detects both kar'c 
'he' and Dcutsch-go 'German' modifies \]ta'aas~t 
'speak', but tells nothing about which is subject 
and object. This analysis result is not sufficient 
for subsequent NLP applieations like Japanese 
to English machine l;rmmlation. 
Then, what we need to do is a case structure. 
analysis based on a case fl'ame dictiolmry, or a 
subcat, of each verb as follows: 
hanasu 'speak': 
ga (nora) ks're 'he', hire 'person' 
'we (ace) cigo 'English', kotoba 'language' 
~;i?'U, ~%vear': 
ga (nora) kavc qm', hil, o 'person' 
we (ace) fuhu 'cloth', coat 'coat' 
a'tl Jasel'~t 'har l l lonize' :  
ga (nora) kar'c 'he', hito 'person' 
'we (ace) ire ~color' 
ni (dat) fltku 'cloth' 
Consultation of such a dictionary can easily find 
that kar'c 'he' is a nomilmtive case and Dev, tsch,- 
90 'German' is an accusative (:as(', in the sentence 
4. 
Furthermore, a (:ase frame dictionary Call so lve  
the problem 4 above, that is, some part of struc- 
tural ambiguity in sentences. In case of sentence 
3, a t)r()l)er head for 'ir'o wa 'color topic-marker' 
(:all })e selected by consulting case slots of kir'u 
~wear' and those of a'wascru 'harmonize'. 
3 Unsupe,  rv i sed  const ruct ion  o f  a 
case  f ra lne  d ic t ionary  
This s(x:tion explains how to construct a case 
fralll(*, dictionary fl'om corl)ora autonmtica.lly. 
As mentioned in the introduction section, it; 
is quite expensive, or ahnost ilnl)ossible to con- 
struct a wide-coverage case frame dictionary by 
lmnd. In Japanese,, some noun q- copula works 
like an adjective. For example, sa~tsei da 'posi- 
tiveness + Colmla' can take 9a case and 'hi case. 
However, such case frames are rarely covered t)y 
the existing handmade dictionaries 1.
Fm'thermore, existing halldmade dictionaries 
cover typical obligatory cases like ga (nomina- 
tive), wo (accusative), ni (dative), but do not 
cover compound case markers uch as ni-kandz.itc 
'in terms of', 'wo-rncqutte 'concerning' and oth- 
ers. 
Then, we tried to construct an example-based 
case frmne dictionary from corpora, which de- 
lOut method collects case frames not only tbr verbs, 
but also tbr adjectives mM nouns-kcopula. In this paper, 
we use 'verb' instead of 'w;rb/adjective. orllOllll -{- copula.' 
for simplicity. 
433 
Table 1: The accuracy of KNP. 
'wa~ 7tto c lause  clause 
9 a 'we n i  ka'r'a rr~.ade ?lori topic- modif~ying modifying 
noln. ace. dative from to from marker verbs nouIIS 
'lbtal 
91.2% 97.7% 94.2% 83.8% 85.3% 82.8% 88.0% 84.3% 95.5% 91.3% 
scribes what kind of cases each verb has and 
what kind of nouns can fill a case slot. Very large 
syntactically analyzed corpora could be useful to 
construct such a dictionary. However, corpus an- 
notation costs very much and existing analyzed 
corpora are too small from the view point of case 
frame learning. For exmnple, in Kyoto Univer- 
sity Corpus which consists of about 40,000 ana- 
lyzed sentences of newspaper articles, very basic 
verbs like te tsudau 'help' or v, ketsv ,  kcr 'u 'accept' 
appear only 10 times or 15 times respectively. It
is obvious that such small data are insufficient 
for automatic ase frmne learning. That is, case 
frame learning must be done from enormous un- 
analyzed corpora, in unsupervised way 2. 
3.1 Good parser 
NLP research group at Kyoto University has 
been developing a robust and accurate parsing 
system, KNP, over the last ten yem's (Kurohashi 
and Nagao, 1994; Kurohashi and Nagao, 1998). 
This parser has the following advantages: 
? .Japanese is an agglutinative language, and 
several Nnction words (auxiliary verbs, suf- 
fixes, and postpositions) often appear to- 
gether and in many cases compositionality 
does not hold among them. KNP treats 
such function words careflflly and precisely. 
? KNP detects scopes of coordination struc- 
tures well based on their parallelism. 
? KNP employs everal heuristic rules to pro- 
duce mfique parses for the input sentences. 
The accuracy of KNP is shown in Table 1, 
which counted whether each phrase modifies a 
proper head or not. The overall accuracy was 
around 90%, and the accuracy concerning case 
components varies from 82% to 98%. 
21n English, several unsupervised methods have been 
proposed (Manning, 1993; Briscoe and Carroll, 19!)7). 
However, as mentioned in Section 3, automatic Japanese 
case analysis is much harder than English. 
We can collect pairs of verbs and case compo- 
nents from the automatic analyses of large cor- 
pora by KNP. 
3.2 Cop ing wi th  two problems 
The quality of automatic ase frame learning 
could be negatively influenced by the %llowing 
two problems: 
Word sense ambiguity:  A verb sometimes 
has w~rious usages and possibly has several 
case frames depending on its usages. 
Structura l  ambiguity:  KNP performs fairly 
well, but automatic parse results inevitably 
contt~in errors. 
The tbllowing sections explain how to solve 
these problems. 
3.2.1 Word sense ambigu i ty  
If a verb has two or more meanings and their 
case fl'ame patterns differ, we htwe to disam- 
biguate the sense of each occurrence of the verb 
in a corpus first, and collect case components for 
each sense respectively. However, unsupervised 
word sense disambiguation f fl'ee texts is one of 
the most ditficult problems in NLP. At the very 
begimfing, even the definition of word senses is 
open to question. 
To cope with this problem, we made a very 
simple but usefltl assumption: a light verb has 
diffbrent case frames det)ending on its main case 
component; an ordinary verb has a unique case 
frmne even if it has two or more meanings. For 
example, the case frmne of the verb narn  'be- 
come' differs depending on its ni (dative) case 
as %llows: 
. . .  ga  b?}ouki n i  na ' ru  
nora. become ill 
? . .  ga  . . .  to  to rnodach i  n i  na'r"u 
nora. with become a fliend 
In most cases, the main case components are 
placed just in front of the light verbs so that 
the automatic parser can detect their relations 
434 
Tal/le 2: EXmnl)les of the constructed ease frames. 
verl)s 
t, aS?t\]gCl"~l, 
'help' 
yomu 
l'ead' 
case lnarkel"s 
(1,o111) 
,,,,o (it(:(:) 
'r~,i (dat) 
ae (op) 
.qa (no\]n) 
'wo (at(;) 
hi, (dat) 
& (o10 
example  nomls  
husband, person, child, staff, I, SUSl)eet, faculty, ... 
.jol), shol) , farmwork, preparation, election, move, ... 
son, friend, ambassador, meml~er, thank, holid~\y, ... 
volunte(,r, aft'air, otfice, rewar(l, house, headquarters, ...
lX;rson, \], chihl, adult, parent, teacher, ... 
newspaper, book, magazine, article, nov(J, letter, ... 
chiht, person, daughter, teacher, student, reader, ... 
newspaper, book, magazine, library, classroom, bathroom, ... 
reliably. Therefore, as for five major and trou- 
t)lesome light verbs (.~'.,r'u 'do', 'nwr'u, q)ceomo?, 
ar'u 'is . . . ' ,  iu ~s~w', nai 'not'), their case fl'mnes 
are distinguished epending (m their left neigh- 
bouring case components. For other verbs, we 
aSStlllle a \] lnique ease f rame.  
3.2 .2  St ructura l  ambigu i ty  
As shown in '_\['~dfle 1, KNP detects heads of case 
conlt~onents in faMy high accuracy. However, 
in order to collect nmch reliable data, we dis- 
carded moditier-hcad relations in the aul;onmti- 
tal ly Imrsed corpora in the following cases: 
? When CMs of ease conqxments disappear 
because oi" topic markers or others. 
? When the verb is followed 1)y a causative 
auxil iary o1' a passive auxiliary, l;he case tm.t- 
t(:rn is e\]mnged and the 1;race in KNI '  is not 
so rclial)le. 
Based on the conditions al)ove, case compo- 
nents of each verb are collected froln the 1)arscd 
corpora, and the collected ata arc considered as 
case frames of verbs. However, if the f lcquency 
of a CM is very low compared to other CMs, it 
might t)e collected because of parse errors. So, 
we set the threshold for the CM flequency as 
2~,  where m.f means the frequency of the 
1nest folln(t ChJ. i f  the fl'equeney of ~t CM is less 
t lmn the threshold, it is discarded, l.~br exalnple, 
suppose the most frequent CM fin' a verb is we, 
100 times, and the frequency of ni CM tbr the 
verb is 1.6, ni CM is discarded (since it is less 
than the threshold, 20). 
a.3  Const ructed  case  f rmne d ic t ionary  
We applied the al)ow', procedure to Mainichi 
Newst)al)er Corpus (7 years, 3,600,00(} sen- 
tences). Fronl the cortms , case franws of 23,497 
verbs are constructed; the average number of 
ease slots of a verb is 2.8; the average munber 
of cxanqflc nouns in a (:as(: slot is 33.6. Table 2 
shows exmnlfles of constructed ease Dames. 
Although the constructed ata look apl)ropri- 
ate in most cases, it is hard to evaluate a (lictio- 
nary statica.ll> In the next section, we use the 
dictiomu'y in case structure analysis and eval- 
uate the analysis result, wlfich also im\])lies an 
cvahu~.ti(m of the dictionary itself. 
4 Case  s t ructure  ana lys i s  us ing  the  
const ructed  case  f rame d ic t ionary  
4.1. Match ing  o f  an  input  sentence  and  
a case  f ra l l le  
'Jl~e basic 1)ro(:cdure in ('ase strucl;ul"e analysis is 
lo match an inlml sentence with a case frame, 
aS show11 ill lqgUl'C, 1. 
The matching of case conq)onenl:s in an input 
and case slots in a case  fl'alllO is (\[Olle Oll the 
following conditions: 
I. When a ease component has a CM, it must 
be assigned to 1;11o case slot with the same 
CM.  
. When a case COml)Onent does nol: have a 
CM, it can 1)e assigned to the 9a, we, or ni 
CM slot. 
. ()nly one case component can be assigned 
to a case slot (unique case assiglmmnt con- 
straint). 
The conditions above may produce nmltil)le 
matching patterns, and to select the proper one 
alllOng {,llclll, 11Oll118 of case COlllpon(',lltS al'o COlll- 
pared with examph',s in case slots of the (tictio- 
nary. 
435 
syorui wa . 
(5) document topic-marker / 
ka,'e .i .___1 
! 
,,e 1 
~" walashila 
" hand 
\[ (1 handed the document to him.) 
WaRlSU 'hand' 
ga defendam, president .... 
we money, nlelllO, bribe .... 
ni person, suspect, ... 
de affair, office, room .... 
(6) Deutsch-go me 
/ Gcl'nlan also q 
hHI I ( IS I I  
speak " -7  
{';cq/;i'i;ir 
a teacher who speaks also German) 
~ professor, president 1
ni person, friend .... 
- -  we reason English Japanese 
to (sentence) 
Figure 1: Matching of an inl)ut sentence and a case fl:ame. 
Even though a 3,600,000 sentences corpus was 
used for learning, examples in case slots are still 
sparse, and an input noun mostly does not match 
exactly an example in the dictionary. Then, a 
thesaurus i employed to solve this problem. 
In our experiments, NTT Semantic Feature 
Dictionary (Ikehara et al, 1997) is employed as 
a thesaurus. Suppose we calculate the silnilar- 
ity between Wl and w2, their depth is dl and d2 
in the thesaurus, and the depth of their lowest 
(most specitic) common ode is de, the similarity 
score between them is calculated as follows: 
= (4  ? + 
If W 1 and w2 are in the same node of the the- 
saurus, the similarity is 1.0, the maximum score 
based on this criteria. If Wl and w2 are identical, 
the similarity is 1.0, of course. 
The score of case assigmnent is the best sim- 
ilarity between the input noun and examples in 
the case slots. The score of a matching pattern 
is the sum of scores of case assignments in it. If 
two or more patterns meet the above conditions, 
one which has the best score is selected as a final 
result. 
In the case of sentence 5 in Figure 1, karc 7ti 
'he dativc-CM' is assigned to the ni case slot. 
Then, syorui wa 'document topic-marker' can be 
assigned to the ga or wo case slot. By calculating 
similarity between syorui and 9a-slot examples 
and wo-slot exmnples, it; is considered to be as- 
signed to the wo slot. 
In case of sentence 6, none of the case compo- 
nents has a CM. Based on similarity calculation, 
Deutsch,-go is assigned to 'wo, sensei is assigned 
to ga. 
4.2 Pars ing with case structure analysis 
A complex sentence which contains a clausal 
modifier exhit)its a typical structural ambiguity 
of Japanese; case components left to a verb of 
a clausal modifier, Vc, possibly modify V~: or a 
matrix verb Vm. 
For example, in sentence 3, ir'o 'w~L 'color 
topic-inarker' possibly modifies kite-iru 'wear' or 
(l,~l)(\],Sel'~l, q lar i i l on ize ' .  
KNP, a rule-based parser, handles this type of 
ambiguity ~s follows. If a case component is fol- 
lowed by a comma, it is treated as modif\[ying Vm ; 
if not, it is treated as modif\[ying 1~:. Although 
this heuristic rule usually explains real data very 
well, sentence 3 will be analyzed incorrectly. 
Parsing which utilizes a case frame dictionary 
can consider which is a proper head, V~ or Vm, 
tbr an ambiguous case compolmnt by comparing 
examples in the case slots of V~ and 14~. Such 
a consideration nmst be done considering wlmt 
other case components modifly Vc and Vm, since 
the assigned case slot of a case component might 
differ depending on the candidate structure of 
the sentence due to the unique case assignment 
constraint. 
Therefore, it is necessary to expand the struc- 
tural ambiguity and consider all the possible 
structures fbr an input. So, we calculate the 
matching score of all pairs of case components 
and verbs in all possible structures of the sen- 
tence, and select the best structure based on the 
436 
boushi tic; ire wa 
hat color 
bott.~\]ti no
hal ~,~ 
~-- - __  ire wa 
co lo r - -  I 
C(;?lt It\[ 
COat ~5"?rll 
harlllOlliZe 
We ClOth, tllli\]'/)l'lll, CI)la .... ~ WO \] pOWCI', face, }l}ind 
~ . I  
I de party, oily, home .... ni I prcl)lcnce, cItlth .... 
kite-it'lt Co(It I1i HWCLTCI'll 
weal" coat hllrnlolliZC 
hat 
i re wa -2 (distance penalty) 
c? l ? r  ~ \] 
kite-iru \[ 
co.t ,,i ~ 
~ COat IdilA't!rll 
k i ru  'wear' ~. \  awa,~emt 'harllloilizc' 
2 ,:2,,,,,,6 ....
Figure 2: Parsing with east structure analysis. 
sum of the matching scores in it. 
Since the heuristic rule employed ill KNP is 
actually very useful, we in(:orporate it, that is, 
l)enalty score is imposed to the modifier-hea(l re- 
la.tion depending on the distraint between ~t mod- 
ifi(;l" and a head. If a moditier is not followed by 
a comma, the penalty score, 0 , -2 , -4 , -6 ,  ... is 
imposed when a moditler modifie.s the first (nea.r- 
est), second, third, tburth, ... verbs ill a sentence 
respectively; if with a comma, the tmnalty score, 
-2 ,  0, -2,  -4,  ... is impose& 
For example, sentence 3 was analyzed t)y our 
method as shown ill Figure 2. Since the simi- 
lm'ity score between fro ~color' a.nd the 'we-slot 
of uwa.s'cr'u hmunonize is nmch larger t;\]iall theft 
l)etween ire 'eoloff and the ga-slot of lci'r'u. 'wear', 
the correct structure of the selltellee was de- 
tected (the right-lmnd parse of Figure 2). Note 
that, furthermore, both the ease of ire ill reb> 
tion to awascru  'harmonize', and the case of coal, 
in relation to kite-iru 'wear' were dete(:ted cor- 
rectly. 
Structm'al ambiguities often cause a combina- 
torial explosion when a sentence is long. How- 
ever, by detecting the SeOl)eS of coordinate struc- 
tures 1)e%rehand, which off;ell aPl)ear in long 
'l'td)le 3: The at:curacy of case detection. 
(;orre(:I; ill(:orl'e(;t 
\])arsing ease case 
er ror  (lel;e(:l;ion (tete(:tion 
topic-marleer 82 13 5 
clausal modifier 73 18 9 
senl;ences, we can reasonably limit the possil)le 
sl;ructures of the sentence. 
The ~werage analysis peed of tile ext)criments 
described in the next section was about 50 sen- 
tenets/aria. 'File tinm-oul, of one rain. was only 
employed to 7 out of 4,272 test Selltellces. 
4.3 Exper i lnents  and discuss ion 
We used 4,272 sentences of Kyoto University co l  
pus as a test set. We parsed them by our new 
lnethod (Figure 3 shows several examt)les) and 
cheekc, d two 1)oints: case detection of mnbiguous 
case (-omponents and syntactic analysis. 
First, we randomly selected ambiguous ease 
components: 100 l,ol)ic-markcA case components 
all(t 100 (:ase coral)orients moditied by clausal 
437 
ookllrasyo ha 
the Treasury 
3gatsuki kes:~an de 
settlenlellt ill March ,, 
.l'hintakuginkotl kakukmt ga I impr<n'ed bycase iajbmtatiml 
each trust bank 
tsttmitareteirtt 
save lip 
tokubetsu t3,uhokil~ ,1o 
specially reserved money 
mrikuzushi wo 
collstllllptioll 
gai,,'yoltha 
the Foreign Minislcr 
m 
tnitonwru 
allow 
hm~shhula. 
policy 
mikka ni 
on tile third \] 
4 Mexico ga Mexico 
\[Iglppyolt shiRl imln'ored hy cave i@~tmltli?~*i 
illltl(RlnCC ~ ~ \ ]  
i@lre Ixmshi mulo ? 
prevention o1" inl\]alion \[ 
I 
L'eizai misaku ni 
fimmclal pllllcy 
t,wtite 
al~(Rl\[ 
st'Lvlltttl'i ~hifa 
cxphdn 
Figure 3: Exmnt)les of the mmlysis results. 
modifiers, and checked whether their cases were 
correctly detected or not. As shown in Table 3, 
the accuracy of the analysis was fairly good: that 
tbr topic-markers was 82% and that tbr clausal 
modifiers was 73%. 
Then, we compared the parse results of our 
method with those of the original KNP. As a re- 
sult, 565 modifier-head relations differed; in 260 
cases, our method was correct and the original 
KNP was incorrect (by considering the struc- 
tures in the Kyoto University Corpus as a golden 
standard); in 224 cases, vice versa. That is, 
our method was superior to KNP by 36 cases, 
and increased the overall accuracy from 89.8% 
to 89.9%. Since the heuristic rule used in KNP 
is very strong, the improvement was not big. 
The improvement of the accuracy, though small, 
is valuable, because the accuracy around 90% 
seems close to the ceiling of this task. 
5 Conc lus ion  
We proposed an unsupervised construction 
method of a case frame dictionary. We obtained 
a large case fl'alne dictionary, which consists 
of 23,497 verbs. Using this dictionary, we can 
detect ambiguous case components accurately. 
Also since our method employs unsupervised dic- 
tionary learning, it can be easily scaled up. 
Re ferences  
Ted Briscoe and John Carroll. 1997. Automatic 
extraction of subcategorization from corpora. 
In Prvccedings of ANLP-97. 
Satoru Ikehara, Masahiro Miyazaki, Satoshi 
Shirai, Akio Yokoo, Hiromi Nakaiwa, Ken- 
tarou Ogura, and Yoshiflmfi Oyama Yoshi- 
hiko Hayashi, editors. 1997. Japanese Lexi- 
con. Iwananfi Publishing. 
Information-q~chnology Promotion Agency, 
,Japan. 1987. Japanese Verbs : A Guide to 
the H~A Lea:icon of Basic ,Japa~tcsc Verbs. 
S. Kurohashi and M. Nagao. 1994. A syntac- 
tic analysis method of long japanese sentences 
based on the detection of conjunctive struc- 
tures. Computational Linguistics, 20(4). 
S. Kurohashi and M. Nagao. 1998. Build- 
ing a jal)anese parsed corpus while improv- 
ing the t)arsing system. In Prvcccdin.qs of" Th.c 
Fir;st h~,tcr'national Co't@r~ncc on Lwnguage 
R.csources 64 Evaluation, pages 719 724. 
Christopher D. Maturing. 1993. Automatic ac- 
quisition of a large snbcategorization dictio- 
nary froln corpora. In Pr'occcding s of A CL-93. 
Takehito Utsuro, Takashi Miyata, and Yuji Mat- 
sumoto. 1998. General-to-simeific model se- 
lection tbr subcategorization preference. In 
Proceedings of th.c 17th International ConJ'cr- 
cncc on Computational Li'n.quistics and the 
36th Annual Mectin.q of the Association for 
Computational Lin.quistics. 
438 
Finding Structural Correspondences from Bilingual Parsed Corpus 
for Corpus-b sed Translation 
Hideo Watanabe*, Sadao Kurohashi** and Eiji Aramaki** 
* IBM Researdt, Tokyo Research Laboratory 
1623-14 Shimotsuruma, Yamato, 
Kanagawa 242-8502, Japan 
watanabe@trl.ibm.co.jp 
** Graduate School of Inforlnatics, Kyoto University 
Yoshida-homnachi, Sakyo, 
Kyoto 606-8501, .JaI)an 
kuro@i.kyoto-u.ac.jp, 
aramaki@pine.kuee.kyoto-u.ac.jp 
Abstract 
In this paper, we describe a system and meth- 
ods for finding structural correspondences from the 
paired dependency structures of a source sentence 
and its translation in a target language. The sys- 
tem we have developed finds word correspondences 
first, then finds phrasal correspon(tences based on 
word correspondences. We have also developed a
GUI system with which a user can check and cor- 
rect tile correspondences retrieved by the system. 
These structural correspondences will be used as 
raw translation I)atterns in a corpus-based transla- 
tion system. 
1 Introduction 
So far, a number of methodologies and systelns 
for machine trauslation using large corpora exist. 
They include example-based at)proaches \[7, 8, 9, 
12\], pattern-based approaches \[10, 11, 14\], and sta- 
tistical approaches. For instance, example-based 
approaches use a large set of translation patterns 
each of which is a pair of parsed structures of a 
source-language fragment and its target-language 
translation fragment. Figure 1 shows an exanl- 
ple of translation by an example-based method, ill 
which translation patterns (pl) and (p2) are se- 
lected as similar to a (left hand) Japanese depen- 
dency structure, and an (right hand) English de- 
pendency structure is constructed by merging the 
target parts of these translation patterns 1.
In this kind of system, it is very important o 
collect a large set of translatiou patterns easily and 
efficiently. Previous systems, however, collect such 
translation patterns mostly manually. Therefore, 
they have problems in terms of the development 
cost. 
1Words in parenthesis at the nodes of the Japanese de- 
pendency structure are representative English translations, 
and are for explanation. 
This paper tries to provide solutions for this is- 
sue by proposing methods for finding structural 
correspondences of parsed trees of a translation 
pair. These structural correspondences are used as 
bases of translation patterns in corpus-based ap- 
proaches. 
Figure 2 shows an example of extracting struc- 
tural correspondences. In this figure, tile left tree 
is a Japanese dependency tree, the right tree is a 
dependency tree of its English translation, dotted 
arrows represent word correspondence, and a pair 
of boxes connected by a solid line represent phrasal 
correspondence. We would like to extract these 
 ,ook \ 
"4" - . .~  ...," ~.. a movie ~ 
Figure 2: An Example of Finding Structural Cor- 
respoudences 
word and phrasal correspondeuces automatically. 
In what follows, we will describe details of proce- 
dures for finding these structural correspondences. 
2 Finding Structural Correspondences 
This sectiou describes methods for finding struc- 
tural correspondences for a paired parsed trees. 
2.1 Data  St ructure  
Before going into the details of finding structural 
correspondences, we describe the data format of a 
906 
verb - -  
9a 
noun- -noun , 
n0mu--drink 
l l 0un  - -  n0un  
verb ! 
i 
,, 
t .  
"', dl lk 
 he  .__medicine 
l 
1 ~ # 
\[.-- 
I 
(p2) 
Figure 1: Translation Example by Examt)le-based ~li'anslation 
dependency structure. A det)endeney stru('ture as 
used in this pat)er is a tree consisting of nodes and 
links (or m:cs), wh('.re a node represents a content 
word, while a link rel)resents a fllnctional word or 
a relation between content words. For instance, as 
shown in Figure 2, a t)reposition "at;" is represented 
as a l ink  in l~,nglish. 
2.2 F ind ing  Word  Cor respondences  
The  tirst task for finding stru('tm:al corresI)On- 
den(:c's is to lind word (:orro, sl)ondenccs t)et;ween (;he 
nodes of a sour(:e parsed tree and the nodes of a 
t;wget parsed tree. 
Word correspondences are tkmn(1 by eonsull;ing a
source-to-target translation dictionary. Most words 
can find a unique 1;ranslation candidate in a target 
tree, but there are cases such that there are many 
translation candidates in a target parsed tree for 
a source word. Theretbre, the main task of tind- 
ing word correspondences is to determine the most 
plausible l;ranslation word mnong can(tidates. We 
call a pair of a source word and its translation 
candidate word in a target tree a word correspon- 
dence candidate denoted by WC(s,/,), where s is a 
source word and t is a target word. If 17\[TC(s,/,) ix a 
word correspondence andida.te such that there is 
rto other WC originating h'om s, then it is called 
WA word correspondence. 
The basic idea to select the most plausil)le word 
correspondence candidate ix to select a candidate 
which is near to another word correspondence whose 
source is also near to a sour(:e word in question. 
Suppose a source word s has multiple candidate 
translation target words t~ (i = 1,...,7~,), that  is, 
there are multiple 17FCs originating h'om .s'. We, 
denote these multiple word corresl)ondence candi- 
dates by WC(s, tl). For each I'VC of s, this proce- 
dure finds the neighbor WA correspondence whose 
distance to WC ix below a threshold. The distance 
between WC(sl,/,~) and WA(s.2,/,2) is defined as 
the distance between sl and .s2 plus the distmme 
between s2 and 1,2 where a distance between two 
nodes is defined as the number of nodes in the t)ath 
whoso, ends are the two nodes. Among I~VCs of 
.s for which neighbor H/A ix tound, the one with 
the smallest (listan(:(~ is chosen as the word corre- 
Sl)ondenee of s, and I/VCs whMl are not chosen 
are invalidated (or deleted). We call a word corre- 
spondence found t)y this procedure WX.  We use 
3 as t;he distance threshold of the above procedure 
currently. This procedure ix applied to all source 
nodes which have multii)le WCs. Figure 3 shows 
an example of WX word correspondence. In this 
examt)le, since the Japanese word "ki" has two En- 
glish l;ranslation word candidates "time" and "pe- 
riod," there are two WCs  (~7C 1 and WC2). The 
direct parent node "ymlryo" of "ki" has a WA cor- 
respondence (I/VA1) to "concern," and the direct 
child node "ikou" has also a WA correspondc'nee 
(WA2) to "transition." In this ease, since the dis- 
tance between I'VC2 and WA2 is smaller than the 
distan(:e between I.VC1 and WA1, I'VC~ in clmnged 
to a 1/l/X, and I~ITC1 is adandoned. 
In addition to WX correspondences, weconsider 
a special case such that given a word correspon- 
dence l'lZ(s,/,), if s has only one child node which ix 
907 
. . ,  ........ be ....... ",,, 
.. -~%omp t 
. . .WAI at / /  concern 
." time 
yuuryo ,.." 
(concern) -" 
ni ..,Wc1 same 
.... accompany 
ki..,*\[__ 
(time) .............. W_..G2 ............ 
'"-- period 
ikou ......... VVA2 of 
transition (transition) 
Figure 3: An Exmnt)le of WX Word Correst)on- 
(lence 
a leaf and t has also only one child node which ix a 
leaf, th(;n we COllStrllet a lleW word correspondence 
called 1US from these two leaf nodes. This WE 
procedure is al)plied to all word correspondences. 
Note tlmt this word correst)ondence is not to se.le, ct 
one of candidates, rather it is a new finding of word 
corre, spondence by utilizing a special structm:e. For 
instance, in Figure 3, if there is a word eorrespol> 
dence 1)etween "ki" and "period" and there is no 
word correst)ondence between "ikou" and "transi- 
tion," then I<V,g(iko'u~ transition) will be found 1)3' 
this 1)roeedure. 
These WX and WS t)rocedures are continuously 
al)plied until no new word correspondences arc t'(mnd. 
Aft;er al)l)lying the above WX and I'VS pro(:e- 
dures, there are some target words t such that t is a 
destination of a l,l/C(.s ", t) and there ix no other 1,176 , 
whose destination ix t:. In this case, the lUG(s,t)  
correspondence andidate is chosen as a valid word 
correspondence b tween s and/,,  and it; is called a 
HzZ word eorrest)ondence. 
We call a source node or a target node of a word 
correspondence an anchor node in what tbllows. 
The above t)rocedures for finding word corre- 
sI)ondences are summarized as follows: 
Find WCs by consulting translation dictionary; 
Find WAs; 
whi le  (true) { 
find WXs; 
find WSs; 
i f  no new word corresp, is found, then  break; 
} 
find WZs;  
2.3 F ind ing  Phrasa l  Co l ' res l )ondences  
The next step is to tind phrasal correspondences 
based on word eorl'eSl)ondences t'(mnd t) 3, 1)roce.- 
dures described in tim previous section. What  we 
would like to retrieve here, is a set of phrasal cor- 
respondences which (:overs all elements of a paired 
dependency trees. 
In what follows, we (:all a portion of a tree which 
consists of nodes in a 1)att~ from a node ?t I (;o all- 
oth(;r node nu which is a descen(lanl; of n:l a lin-. 
ear tree denoted by LT(v,1, n~), and we denote a 
minimal sul)tree including st)coiffed nodes hi,  ..., n.~, 
l)y T (n l , . . . ,n , ) .  For instan(:(,~ in the English tree 
structure (the right tree) in Figure 4, LT(tcch, nology ,
science) is a rectangular area covering %eclmol- 
? tg "e e ~ ogy," and SOl ,no ,, anti .T(J'acl;or, cou'ntrjl ) is a 
1)olygonal area covering "factor,""atDcl,, . . . .  t)ol- 
icy," and "country." 
The tirst step is to find a 1)air of word correst)on- 
dences W, (.~'~, t ) and ~4q(.,.~, t ~) such that .,, a.,t  
s2 constructs a linear tree LT(si ,  s2) and there is no 
anchor node in th(' 1)al;h from s~ to s2 other than .s'~ 
and .s2, where 1UI and H~ denote any tyi)e of word 
('orrest)on(lences 2 and we assmne there is a word 
corresI)ondence t)etwee, n roots of source and (;arget 
trees by defmflt. We construct a t)hrasal correspon- 
dence fi'om source nodes in LT(s , , s2)  and target 
l/o(les itl r \ ] ' ( t : l , / '2 ) ,  (l()llote(t by \];'(l~,~F'(.q'l, .";2), 5\].n(tl, t2)). 
For illstall('e~ ill F ig l l re 41~ \]"11~ \]~12~ 1)'2~ 1)3 and 
\])4 tu.'e source portions of phrasal et)rrespondences 
found in this step. 
The next stel) checks, for ea(:h 1', if all anchor 
l lo(les of wor(1 eorres1)Oll(leile(?s wllose SOUlT(;e o1 ~;al- 
get node is included in P are al,eo included in P. 
If a t)hrasal correst)ondenee satisiies this condition, 
then it is called closed, otherwise it ix called open. 
Further, nodes which are not included in the I ) in 
question are called open nodes. If a l ) ix ot)en, then 
it ix merged with other 1)hrasal correspondences 
having ol)en nodes of P so that the merged 1)hrasal 
correspondence b comes (-losed. 
Next, each P~,, is checked if there is another l)q 
which shares any nodes ottmr than anchor nodes 
with P.,,. If this is the case, these P:., and 1~ are 
lnerged into one phrasal correspondence. In Figure 
4, t)hrasal correspondences i 11 and P12 are merged 
into P1, since their source I)ortions LT (haikei, koku) 
and LT (haikci, seisaku) share "doukou" which is 
not an anchor node. 
Finally, any path whose nodes other than the 
root are not included in any 1)s but the root node 
ix included in a 1 ) is searched for. This procedure 
2Since WC is not a word correspondence (it is a candi- 
date, of word corresi)ondence), it is llOi; conside, red here. 
908 
is apl)lied I;o 1)oth source a.nd (;arget trees. A im.th 
found 1)y this 1)ro(:(xlur(~ is called an open pal, h,, m~(t 
its root no(le is called a pivot. If such an Ol)en path 
is found, it is t)rocessed as follows: l, br each 1)ivot 
node, (a) if the t)ivot is not an mmhor nod(;, then 
open lmths originating fl:om the pivot is merged 
into a 1 ) having I;he pivot, (b) if the pivot is an 
~LIlChOf l lo(lo~ {;hOll 3_ llOW t)hl'~lS~L1 c()rFos1)oII(|(~IlC( ~, iS 
created from Ol)(m 1)ai;hs originating from the m> 
thor nodes of the word (:orrcsl)on(l(:ncc. 
In Figure 4, w(: get tinally four phrasal (:orr(:- 
Sl)on(lences l~, f~, l~, an(l l~t. 
! haikei.!,,: I - ................... { -~ factor',,,l ', 
i /~0 :: a ect " 
, ,  ',,(tre, nd) i f \  ~ ~-"  l i ;  
k . '/ ;~oy, v __: 
~, koku ( seisak~'~{t - - - - .~-  - -~  
~' (C0UrlttV)_l , (p0%,~ :t' .. technology .lrltly 
~< :::>i--- -::-:~ 
= 7 :: TLI io. ; I 
(major) ~,_/_ 
- X-~ / 
' giutu"\]\] 
(technolo~ly)l' .? science 
kagaku " 
(scie, nce) . -  
P4 
/ 
t 
t /  ff 
i 
Figm:e d: An l~;xaml)le of Finding Phrasal  Corr(> 
S\])Olld(~,IIC(',S 
The above 1)ro(:edures fl)r finding l)hrasal (:orr(> 
Ht)oIIdoIICOS ~-LF(~ SlllIllIl?~riz(Kl gtS fo l lows :  
Find initial Ps; 
Mea'ge a.n Ol)Cn 1>~ with other i ' s  having 
open nodes of 1}; 
Create new Ps 1)y merging \])s 
which have more tlmn 2 (:ommon nodes; 
Find ot)en path, alld 
if the t)ivot is ml mmhor, | ;hen 
merge the path to P having the anchor, 
o therwise  create new l ) by merging 
all open t)ai,hs having l;lm pivot; 
3 Exper iments  
3.1 C, o r lms  and  D ic t ionary  
We used (l()(;lllil(~'ll|;s t'rolil White Papers on S(:i- 
en(-e and Technology (1.994 to \ ]996) pul)lished by 
the S(:ience mid Technology Agency (STA) of tim 
.\]al)mmse govcrlim(~nl;. STA lmblished th(;se White 
PaI)ers in both Jat)mmse and English. The Com- 
mmfications l{esea.rch Laboratory of" the Ministry 
of Posts and Telecommuni(:a.tion of the .\]al)mmse 
goverlmmnt supl)lied us with the l)ilingual corpus 
wtfich is already roughly aligned. We made a bilin- 
gual cortms consisting of pa.rs(;d dependency struc- 
tures by using the KNP\[2\] .\]al)mmso, 1)arser ((l(wel- 
Ol)ed by Kyoto (hfive)sity) for .Jal)anes(~ sentences 
and the ESG\[5\] English 1)arser (developed by IBM 
Watson i{e, sear(:h Center) for English s(~nl;(!nces. 
We mad(} al)oul; 500 senl;(m(:e l)airs, each of whi(:h 
11~1,'4 ;I, OIlC-I;O-OII(', 80,11|;(',11(;0 (-orresl)onden(:(~,, fl'OI\[l (,\]lO 
raw (t~tta of l;he, White l)al)crs, mid s(',l(;(;i;(xl rm> 
domly aboul; 130 s('aH;en(:c pairs for (',Xl)(Mm(;nts. 
ilow(wer, since a 1)nrser does not always \])ro(hwe 
(;orl'c(;\[; 1);~l"s(t t;re(}s~ wo (~x(:lude(1 some, ~(~ii|;(Hic(~ p;Lil's 
wlfich have severe 1)arse errors, and tinally got i\[15 
S(~,II\[;OIlC(; pairs as a, to, st s(%. 
As a trm~slation wor(1 dictionary/)etw(',(m .l at)ml(',s(; 
and English, we, tirsl; used ,l-to-l~; trmlslati()n (li(:- 
l, ionary which has mot(,' t lmn 100,000 (,ifl;l'i(;~, but 
we, fi)un(l l;}l~/{; l ller(? are som(~ word ('orr(~sl)Oll(l(~,llt;(~s 
not (:()v(ued in this di(:ti()nary. Tlmref()rG we merged 
(retries fi:om \]';-t;o-.I translatioll dictionary in order 
to get; much broad (:ov(wag(,'. The l;oDd nulnl)(}r ()f 
entries a.re now more I;ha.n \[50,000. 
3.2 Exper inmnta l  Resu l ts  
Td)le i shows l;he result of (~Xl)c, rimeni; fl)r tind- 
ing word correspond(nm(~s. A row with ALL in th(', 
l:yl)e cohmm shows Llle total  ~CClll'~lcy of WOI'(1 cor- 
r(Lqpolld('31c(~s and ol;\]l{~r rows sh()\v Llle .~iCClll'ktcy of 
each t, yt)e. It is clear that WA (:orr(~sl)Olld(~ll(;(',s 
have a very high a('cura(:y. Other word (:orresl)On-- 
do, nc(,,s also ha.ve a roJatively high ac(:ura(:y. 
Table 2 shows tim remflt of exl)erimenl,s for find~ 
ing 1)hrasal correspondences. The row with ALL in 
I;he l;yt)c cohlmn shows l;he l;ol;al accuracy of phrasal 
(:ol'r(~sl)ondo, n(:(~s found by the 1)rol)osed 1)rocedure. 
This ac(:macy level is not I)romising and it is not; 
useful for later 1)ro(:e, sses since it needs human (:he(:k- 
ing ml(l (:orrec?ion. Therefore, we sul)categoriz(~ 
each phrasal corl'eSpond(m('es, and check l;he a('- 
(:uracy for each subca.tegory. 
We consider the following sut)catcgories for 1)hrasal 
('x)rl'(}Sl)olidell(-(~s: 
? MIN ... The minimal  t)hrasal correst)ondence, 
that is, I'(1Zl'(.s'l, .s2), LT(t l ,  t2)) such that  (;herc 
909 
type 
ALL 
WA 
WX 
WS 
WZ 
1111111. nunl .  of SUCCESS 
of correct ratio found corresp. (%) corresp. 
771 745 96.63 
612 600 98.03 
131 118 90.07 
13 12 92.3 
15 15 100 
Table h Experimental Result of Word Correspon- 
dences 
are word correspondences W(s1,  t l )  and W 
(s2,t2), s2 is a direct child of St and t2 is a 
direct child of tl. 
? LTX ... P(LT(.s'I,S2),LT(tl,t2)) such that 
all nodes other titan s2 and t2 have only one 
child node. 
? LTY ... P(LT(sl,.S2), LT(tl, t2)) such that 
all nodes other than Sl, s2,1':1 and t.2 have only 
one child node. 
LTX is a special case of LTY, since Sl and tl of 
LTX must have only one child node, on the other 
hand, ones of LTY may have more than two child 
nodes. A subcategory test tbr a phrasal correspon- 
dence is done in the above order. Exmnples of these 
subcategories are shown in Fig 5. 
Tlm result of these subcategories are also shown 
in Table 2. Subcategories MIN and LTX have very 
high accuracy and this result is very promising, 
since we can avoid nmnual checking for ttmse phrasal 
correst)ondences , or we would check only these types 
of t)hrasal correspondences mmmally and discard 
other types. 
As stated earlier, since we removed only sen- 
tences with severe parsing errors from the test set, 
please note that the above mtmbers of experimental 
results are calculated for a bilingual parsed corpus 
including parsing errors. 
4 D iscuss ion  
There have been some studies on structural align- 
Inent of bilingual texts such as \[1, 4, 13, 3, 6\]. Our 
work is similar to these previous tudies at the con- 
ceptual level, but different in some aspects. \[1\] 
reported a method for extracting translation tem- 
plates by CKY parsing of bilingual sentences. This 
work is to get phrase-structure level phrasal cor- 
respondences, but our work is to get dependency- 
structure level phrasal correspondences. \[4\] pro- 
posed a method for extracting structural matclfing 
(pairs of dependency trees) by calculating matching 
similarities of two dependency structures. Their 
work focuses on tile parsing ambiguity resolution 
by calculating structural matching. Further, \[3, 6\] 
proposed structural alignnmnt of dependency struc- 
tures. Their work assuined tha.t least common an- 
cestors of each fragment of a structural correspon- 
dence are preserved, but our work does not have 
such structural restriction. \[13\] is different o oth- 
ers in that it tries to find phrasal correspondences 
by comt)aring a MT result and its manual correc- 
tion. 
In addition to these differences, the main differ- 
ence is to find classes (or categories) of phrasal cor- 
respondences which have high accuracy. In general, 
since bilingual structural alignment is very compli- 
cated and difficult task, it; is very hard to get more 
than 90% accuracy in total. If we get only such 
an accuracy rate, the result is not useful, since we 
need manual clmcks tbr the all correspondences re-
trieved. But, if we can get some classes of phrasal 
correspondence with, for instance, more than 90% 
accuracy rate, then we can reduce manual clmck- 
ing for phrasal correspondences in such classes, and 
this reduces the development cost of translation 
patterns used in later corpus-based translation pro- 
tess. As shown in the previous section, we could 
find ttmt all (:lasses of word correspondences and 
two subclasses of phrasal correspondences are more 
than 90% accurate. 
When actually using this automatically retrieved 
structural correspondence data, we must consider 
how to manually correct the incomplete parts and 
how to reuse mamlal correction data if the parser 
results are ctmnged. 
As for the tbrlner issue, we need an easy-to-use 
tool to modify correspondences to reduce the cost 
of mmmal operation. We have developed a GUI 
tool as shown in Figure 6. In this figure, the bot- 
tom half presents a pair of source and target depen- 
dency structures with word correspondences (solid 
lines) and phrasal correspondences ( equences of 
slmded circles). You can easily correct correspon- 
dences by looking at this graplfical presentation. 
As for tlm latter issue, we must develop meth- 
ods for reusing the manual correction data as much 
as possible even if tim parser outputs are changed. 
We have developed a tool for attaching phrasal 
correspondences by using existing phrasal corm- 
spondence data. This is implemented as follows: 
Each phrasal correspondence is assigned a signa- 
ture which is a pair of source and target, sentences, 
each of which tins bracketed segments which are in- 
cluded in the phrasal correspondence. For instance, 
910 
I 
tmihatu 
((~uebiomeR) 
,~10 
gijutu -,,- 
(tedr, do?.t?) 
I 
--,<Jeveloprned 
0f 
--,'.-tect'lrlOlogy 
ILl Zl.l~l.l ,~ 
\[<dime} 
go 
seityou 
(i, otldl} 
I 
ya t~.a d 0ute ki 
. . . . . .  ,.corlti nue 
*o 
- -  shob'o 
-. ~obj 
" "k 9 Kl t;dh 
/ ' / \ 
economic 
f 
kaga t,lJ 
"~. 
% 
unparallelled 
. ?tij d u - \ [ - -~e  c;h n 010 ?tl 
~<~o~l I "-)/. ' ,  ,,o  ol oy- .  x ,,0,,, 
ka nte n 
{a~laedl 
korera .,- 
science. 
# 
g 
z 
ee 
(a) MIN (b) LTX (c) LTY 
p Urp ose 
sPec I 
-,qhis 
Figure 5: Examples of Categories of Phrasal Correst)ondences 
A: 
5115511. of 
type found 
COl; l 'es i ) .  
ALL 678 
MIN 223 
LTX 17 
LTY 27 
B:  
I515151. of 
(:orrect 
co5-5"(~Sl). 
431 
215 
(~: 
SllC(;(~SS 
ratio 
~/A (%) 
63.56 
96A1 
D:  
nunL of nodes 
covered t)y A 
7248 
1234 
E: 
nunl. of nodes 
covered by B 
4278 
1194 
F: 
Sl lCCeSS 
ratio 
E/D (%) 
59.02 
96.76 
17 100 153 153 100 
20 I 74.07 253 191 75A9 
I 
Tal)le 2: lgxperinmntal Fh',sult ot' Phrasal Correst)on(len :es 
the following signature is made h)r a i)hrasal corre- 
Sl)on(lence (c) in Figure 5: 
(.~i:j) 
... \[korer~ no kanten karmlo\] kagaku \[gi- 
j u tu \ ]  ... 
... science and \[technology fl:om this 
lmrl)ose\] ... 
(/.~io) 
In the above e, xample, segments betwee, n '\[' and '\]' 
represent a phrasal correspondence. 
If new parsed dqmndency structures for a sen- 
tence pair is given, for each phrasal correspondence 
signature of the sentence pair, nodes in the struc- 
tures wtfich are inside 1)rackets of the signature are 
marked, mid if there is a minimal sul)tree consist- 
ing of only marked nodes, then a phrasal corre- 
Sl)ondence is reconstructed from the phrasal corre- 
spondence signature. By using this tool, we can 
efficiently reuse the manual efforts as much as pos- 
sible even if parsers are updated. 
5 Conc lus ion  
Ill this I)al)er, we have t)rol)osed methods for 
finding structural correspondences (word correst)on- 
dences and i)hrasal corr(;spondences) of bilingual 
parsed corpus. Further, we showed that the t)reci- 
sion of word correst)ond(mces and some catc'gories 
of t)hrasal corresl)ondences found 1)y our methods 
are highly accurate, and these correst)ondences can 
reduce the cost of trm~slation pattern accumula- 
tion. 
In addition to these results, we showed a GUI 
tool for mmmal correction and a tool for reusing 
previous correspondence data. 
As fld;ure directions, we will find more subclasses 
with high accuracy to reduce the cost for transla- 
tion pattern preparation. 
We believe that these methods and tools can ac- 
celerate the collection of a large set of translation 
patterns and the developlnent of a corlms-based 
translation system. 
911 
~rel id="28" type="P4" src="3.4,9,10,11,12.13" tgt="1,2.3,4,8,9,12" eval="T~R UE" score="O" geoeratlon='' subtype="orff' org=" con'lment='"'= 
~rel id="29" type="P5" src="1,2,3" tg~"l 0,11,12" BvaI="TRUE" score="0" generation="" subtype="org" org=" comment:="'> 
~rel ld="3O" type="P5" src="5.6.7" \[g1="5.6,7" eva~"TRUE" score="0" generation=" sublype="org" org="" cornmen~''~ 
<tel id="31" type="P5" src="7.8,9" tg~"7.F' evaI="TRUE" ecore="O" generation=" subtype="org" erg=" cerumen .t=""~. 
'~rel id="32" type="P5" src="3,4.9.10,11.12.13" tgt="1,2,3,4.e,g,12" evaI="TRUE" score="0" generation="' subtype="org" org='' comment=-"'-'. 
! 
\ 
L 
4\ 
h6 ac len ;~aad tactiilC:;Id6i' g,0tiCid;~ ; f  rn~\[,~r.c~un~les 
.. . . . . .  , . . . .  ? ~- .  
,. ? L:i : :  ? : i % 
Figure 6: An GUI tool for presenting/manipulating structural correspondences 
References  
\[1\] Kaji, H., Kids, Y., and Morimoto, Y., "Learning Trans- 
lation Templates from Bilingual Texts," Proc. of Coling 
92, pp. 672-678, I992. 
\[2\] Kurohashi, S., and Nagao, M., "A Syntactic Analy~ 
sis Method of Long Japanese Sentences based on the 
Detection of Conjunctive Structures," Computational 
Linguisties~ Voh 20, No. 4, 1994. 
\[3\] Grishman, R., "Iterative Alignment of Syntactic Struc- 
tures for a Bilingual Corpus," Proe. of 2nd Workshop 
for Very Large Corpora, pp. 57-68, 1994. 
\[4\] Matsumoto, Y., Ishimoto, H., and Utsuro, T., "Struc- 
tural Matching of Parallel Texts," Proc. of the 31st of 
ACL,  pp. 23-30, 1993. 
\[5\] MeCord, C. M., "Slot Grammars," Computational Lin- 
guistics, Voh 6, pp. 31-43, 1980. 
\[6\] Meyers, A., Yanharber, R., and Grishman, R., "Align- 
ment of Shared Forests for Bilingual Corpora," Proc. of 
the 16th of COLING, pp. 460-465, June 1996. 
\[7\] Nagao, M., "A Framework of a Mechanical Translation 
between Japanese and English by Analogy Principle," 
Elithorn, A. and Banerji, R. (eds.) : Artificial and Hu- 
man Intelligence , NATO 1984. 
\[8\] Sato, S., and Nagao, M. "Toward Memory-based Trans- 
lation," Proc. of 13th COLING, August 1990. 
\[19\] Sumita, E., Iida, II., and Kohyama, H. "'Translating 
with Examples: A New Approach to Machine 3Yanslao 
tion," Proc. of" Info Japan 90, 1990. 
\[10\] Takeda, K., "Pattern-Based Context-Free Grammars 
for Machine ~l~anslation," Proc. of 34th ACL, pp. 144-- 
15I, June 1996. 
\[11\] Takeda, K., "Pattern-Based Machine ~lYanslation," 
Proc. of 16th COLING, Vol. 2, pp. 1155-1158, August 
1996. 
\[12\] Watanabe, H. "A Similarity-Driven Transfer System," 
Proc. of the 14th COLING, Vol. 2, pp. 770.-776, 1992. 
\[13\] ~Vatanabe, H. "A Method for Extracting ~IYanslation 
Patterns from ~lS'anstation Examples," Proc. of the 5th 
Int. Conf. on Theoretical and Methodological Issues in 
Machine Translation, pp. 292-301, 1993. 
\[14\] Watanabe, H., and Takeda, K., "A Pattern-based Ma.- 
chine Translation System Extended by Example-based 
Processing," Proc. of the 36th ACL & 17th COLING, 
Vol. 2, pp. 1369o1373, 1998. 
912 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 425?432
Manchester, August 2008
Coordination Disambiguation without Any Similarities
Daisuke Kawahara
National Institute of Information and
Communications Technology,
3-5 Hikaridai Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University,
Yoshida-Honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
The use of similarities has been one of the
main approaches to resolve the ambigui-
ties of coordinate structures. In this pa-
per, we present an alternative method for
coordination disambiguation, which does
not use similarities. Our hypothesis is
that coordinate structures are supported
by surrounding dependency relations, and
that such dependency relations rather yield
similarity between conjuncts, which hu-
mans feel. Based on this hypothesis, we
built a Japanese fully-lexicalized genera-
tive parser that includes coordination dis-
ambiguation. Experimental results on web
sentences indicated the effectiveness of our
approach, and endorsed our hypothesis.
1 Introduction
The interpretation of coordinate structures directly
affects the meaning of the text. Addressing co-
ordination ambiguities is fundamental to natu-
ral language understanding. Previous studies on
coordination disambiguation suggested that con-
juncts in coordinate structures have syntactic or
semantic similarities, and dealt with coordination
ambiguities using (sub-)string matching, part-of-
speech matching, semantic similarities, and so
forth (Agarwal and Boggess, 1992). Semantic sim-
ilarities are acquired from thesauri (Kurohashi and
Nagao, 1994; Resnik, 1999) or distributional simi-
larity (Chantree et al, 2005).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
For instance, consider the following example:
(1) eat Caesar salad and Italian pasta
The above methods detect the similarity between
salad and pasta using a thesaurus or distributional
similarity, and identify the coordinate structure
that conjoins salad and pasta. They do not use the
information of the word eat.
On the other hand, this coordinate structure can
be analyzed by using selectional preference of eat.
Since eat is likely to have salad and pasta as its ob-
jects, it is plausible that salad and pasta are coor-
dinated. Such selectional preferences are thought
to support the construction of coordinate structures
and to yield similarity between conjuncts on the
contrary.
We present a method of coordination disam-
biguation without using similarities. Coordinate
structures are supported by their surrounding de-
pendency relations that provide selectional prefer-
ences. These relations implicitly work as similari-
ties, and thus it is not necessary to use similarities
explicitly.
In this paper, we focus on Japanese. Coor-
dination disambiguation is integrated in a fully-
lexicalized generative dependency parser (Kawa-
hara and Kurohashi, 2007). For the selectional
preferences, we use lexical knowledge, such as
case frames, which is extracted from a large raw
corpus.
The remainder of this paper is organized as fol-
lows. Section 2 summarizes previous work related
to coordination disambiguation and its integration
into parsing. Section 3 briefly describes the back-
ground of this study. Section 4 overviews our idea,
and section 5 describes our model in detail. Sec-
tion 6 is devoted to our experiments. Finally, sec-
tion 7 gives the conclusions.
425
2 Related Work
Previous work on coordination disambiguation has
focused mainly on finding the scope of coordinate
structures.
There are several methods that use similari-
ties between the heads of conjuncts. Similari-
ties are obtained from manually assigned seman-
tic tags (Agarwal and Boggess, 1992), a the-
saurus (Resnik, 1999) and a distributional the-
saurus (Chantree et al, 2005). Other approaches
used cooccurrence statistics. To determine the at-
tachments of ambiguous coordinate noun phrases,
Goldberg (1999) applied a cooccurrence-based
probabilistic model, and Nakov and Hearst (2005)
used web-based frequencies. The performance of
these methods ranges from 50% to 80%.
Of the above approaches, Resnik (1999) and
Nakov and Hearst (2005) considered the statistics
of noun-noun modification. For example, the co-
ordinate structure ?((mail and securities) fraud)? is
guided by the estimation that mail fraud is a salient
compound nominal phrase. On the other hand, the
coordinate structure ?(corn and (peanut butter))? is
led because corn butter is not a familiar concept.
They did not use the selectional preferences of the
predicates that the conjuncts depend on. There-
fore, this idea is subsumed into ours.
The previously described methods focused on
coordination disambiguation. Some research has
been undertaken that integrated coordination dis-
ambiguation into parsing.
Several techniques have considered the charac-
teristics of coordinate structures in a generative or
reranking parser. Dubey et al (2006) proposed
an unlexicalized PCFG parser that modified PCFG
probabilities to condition the existence of syntactic
parallelism. Hogan (2007) improved a generative
lexicalized parser by considering the symmetry be-
tween words in each conjunct. As for a reranking
parser, Charniak and Johnson (2005) incorporated
some features of syntactic parallelism in coordi-
nate structures into their MaxEnt reranking parser.
Nilsson et al tried to transform the tree rep-
resentation of a treebank into a more suitable
representation for data-driven dependency parsers
(Nilsson et al, 2006; Nilsson et al, 2007). One
of their targets is the representation of coordinate
structures. They succeeded in improving a deter-
ministic parser, but failed for a globally optimized
discriminative parser.
Kurohashi and Nagao proposed a Japanese pars-
ing method that included coordinate structure de-
tection (Kurohashi and Nagao, 1994). Their
method first detects coordinate structures in a sen-
tence, and then determines the dependency struc-
ture of the sentence under the constraints of the
detected coordinate structures. Their method cor-
rectly analyzed 97 out of 150 Japanese sentences.
Kawahara and Kurohashi (2007) integrated this
method into a generative parsing model. Shimbo
and Hara (2007) considered many features for co-
ordination disambiguation and automatically opti-
mized their weights, which were heuristically de-
termined in Kurohashi and Nagao (1994), using a
discriminative learning model.
A number of machine learning-based ap-
proaches to Japanese parsing have been developed.
Among them, the best parsers are the SVM-based
dependency analyzers (Kudo and Matsumoto,
2002; Sassano, 2004). In particular, Sassano added
some features to improve his parser by enabling
it to detect coordinate structures (Sassano, 2004).
However, the added features did not contribute to
improving the parsing accuracy. Tamura et al
(2007) learned not only standard modifier-head
relations but also ancestor-descendant relations.
With this treatment, their method can indirectly
improve the handling of coordinate structures in
limited cases.
3 Background
3.1 Japanese Grammar
Let us first briefly introduce Japanese grammar.
The structure of a Japanese sentence can be de-
scribed well by the dependency relation between
bunsetsus. A bunsetsu is a basic unit of depen-
dency, consisting of one or more content words and
the following zero or more function words. A bun-
setsu corresponds to a base phrase in English and
eojeol in Korean. The Japanese language is head-
final, that is, a bunsetsu depends on another bun-
setsu to its right (but not necessarily the adjacent
bunsetsu).
For example, consider the following sentence
1
:
(2) ane-to
sister-CMI
gakkou-ni
school-ALL
itta
went
(went to school with (my) sister)
1
In this paper, we use the following abbreviations:
NOM (nominative), ACC (accusative), ABL (ablative),
ALL (allative), CMI (comitative), CNJ (conjunction) and
TM (topic marker).
426
This sentence consists of three bunsetsus. The fi-
nal bunsetsu, itta, is a predicate, and the other bun-
setsus, ane-to and gakkou-ni, are its arguments.
Their endings, to and ni, are postpositions that
function as case markers.
3.2 Treebank
To evaluate our method, we use a web corpus that
is manually annotated using the criteria of the Ky-
oto Text Corpus (Kurohashi and Nagao, 1998).
The Kyoto Text Corpus is syntactically annotated
in dependency formalism, and consists of 40K
Japanese newspaper sentences. The web corpus,
which is used in our evaluation, consists of 759
sentences extracted from the web.
Under the annotation criteria of the Kyoto Text
Corpus, the last bunsetsu in a pre-conjunct depends
on the last bunsetsu in a post-conjunct, as shown in
the dependency trees of Figure 1.
4 Our Idea of Addressing Coordination
Ambiguities
The target of our approach is nominal coordinate
structures. Consider, for example, the follow-
ing sentence, which contains a nominal coordinate
structure.
(3) jinkou-no
population-GEN
zouka-to
increase-CNJ
taiki-no
air-GEN
osen-ga
pollution-NOM
sokushin-sareta
stimulated
(increase of population and pollution of air
were stimulated)
In this sentence, the postposition to is a coordinate
conjunction
2
. In Japanese, a coordinate conjunc-
tion is attached to a verb or noun, forming a bun-
setsu, like case-marking postpositions. We call a
bunsetsu that contains a coordinate conjunction co-
ordination key bunsetsu.
The coordinate structure in example (3) has four
possible scopes as depicted in Figure 1. In this
figure, our parser generates the constituent words
according to the arrows in the reverse direction.
Note that the words that have ?1/2? marks are gen-
erated from multiple words, because they depend
2
Note that the postposition to can be used as a coordinate
conjunction and also a comitative case marker as in exam-
ple (2). The detection of coordinate conjunctions is a task of
coordination disambiguation as well as the identification of
coordination scopes. Both of these tasks are simultaneously
handled in our method.
on a coordinate structure. In this case, their gen-
erative probabilities, which are described later, are
averaged.
The scope patterns in Figure 1 can be written in
English as follows:
a. (population increase) and (air pollution)
b. population (increase and (air pollution))
c. ((population increase) and air) pollution
d. population (increase and air) pollution
In (a) and (b), two arguments, zouka (increase)
and osen (pollution), are generated from the verb
sokushin-sareta (stimulated), and are eligible for
the ga (NOM) words of the verb sokushin-sareta
(stimulated). However, (b) is not appropriate,
because we cannot say the nominal compound
?jinkou-no osen? (pollution of population). In (c)
and (d), the heads of conjuncts, zouka (increase)
and taiki (air), are generated from osen (pollu-
tion). These cases are also inappropriate, because
we cannot say the nominal compound ?zouka-no
osen? (pollution of increase). Accordingly, in this
case, the correct scope, (a), is derived based on the
selectional preferences of predicates and nouns.
In this framework, we require selectional prefer-
ences. We use case frames for predicates (Kawa-
hara and Kurohashi, 2006) and occurrences of
noun-noun modifications for nouns. Both of them
are extracted from a large amount of raw text.
5 Our Model of Coordination
Disambiguation
This section describes an integrated model of co-
ordination disambiguation in a generative parsing
framework. First, we describe resources for selec-
tional preferences, and then illustrate our model of
coordination disambiguation.
5.1 Resources for Selectional Preferences
As the resources of selectional preferences to
support coordinate structures, we use automati-
cally constructed case frames and cooccurrences
of noun-noun modifications.
5.1.1 Automatically Constructed Case
Frames
We employ automatically constructed case
frames (Kawahara and Kurohashi, 2006). This
section outlines the method for constructing the
case frames.
427
zouka-to
increase-CNJ
jinkou-no
population-GEN
zouka-to
increase-CNJ
taiki-no
air-GEN
osen-ga
pollution-NOM
sokushin-sareta
stimulated
C
(a) jinkou-no
populuation-GEN
zouka-to
increase-CNJ
taiki-no
air-GEN
osen-ga
pollution-NOM
sokushin-sareta
stimulated
(b)
jinkou-no
population-GEN
zouka-to
increase-CNJ
taiki-no
air-GEN
osen-ga
pollution-NOM
sokushin-sareta
stimulated
C
(c) jinkou-no
population-GEN
taiki-no
air-GEN
osen-ga
pollution-NOM
sokushin-sareta
stimulated
C
(d)
C
1/2
1/2
Figure 1: Four possible coordination scopes for example (3). Rounded rectangles represent conjuncts.
The solid arrows represent dependency trees. The dotted arrows represent the additional processes of
generation for coordinate structures. Note that the arrows with coordinate relation (?C? mark) do not
participate in generation instead.
Table 1: Acquired case frames of yaku. Example
words are expressed only in English due to space
limitation. The number following each word de-
notes its frequency.
CS examples
ga I:18, person:15, craftsman:10, ? ? ?
yaku (1)
wo bread:2484, meat:1521, cake:1283, ? ? ?
(bake)
de oven:1630, frying pan:1311, ? ? ?
yaku (2) ga teacher:3, government:3, person:3, ? ? ?
(have wo fingers:2950
difficulty) ni attack:18, action:15, son:15, ? ? ?
ga maker:1, distributor:1
yaku (3)
wo data:178, file:107, copy:9, ? ? ?
(burn)
ni R:1583, CD:664, CDR:3, ? ? ?
.
.
.
.
.
.
.
.
.
A large corpus is automatically parsed, and case
frames are constructed from modifier-head exam-
ples in the resulting parses. The problems of auto-
matic case frame construction are syntactic and se-
mantic ambiguities. That is to say, the parsing re-
sults inevitably contain errors, and verb senses are
intrinsically ambiguous. To cope with these prob-
lems, case frames are gradually constructed from
reliable modifier-head examples.
First, modifier-head examples that have no syn-
tactic ambiguity are extracted, and they are disam-
biguated by a pair consisting of a verb and its clos-
est case component. Such pairs are explicitly ex-
pressed on the surface of text, and are thought to
play an important role in sentence meanings. For
instance, examples are distinguished not by verbs
(e.g., ?yaku? (bake/broil/have difficulty)), but by
pairs (e.g., ?pan-wo yaku? (bake bread), ?niku-wo
yaku? (broil meat), and ?te-wo yaku? (have diffi-
culty)). Modifier-head examples are aggregated in
this way, and yield basic case frames.
Thereafter, the basic case frames are clustered
to merge similar case frames. For example, since
?pan-wo yaku? (bake bread) and ?niku-wo yaku?
(broil meat) are similar, they are clustered. The
similarity is measured using a thesaurus (The Na-
tional Institute for Japanese Language, 2004).
Using this gradual procedure, we constructed
case frames from a web corpus (Kawahara and
Kurohashi, 2006). The case frames were ob-
tained from approximately 500M sentences ex-
tracted from the web corpus. They consisted of
90,000 verbs, and the average number of case
frames for a verb was 34.3.
In Table 1, some examples of the resulting case
frames of the verb yaku are listed. In this table,
?CS? indicates a case slot.
428
ane-to
sister-CNJ
otouto-wo
brother-ACC
yondainvited
C(b)ane-tosister-CMI
otouto-wo
brother-ACC
yondainvited
(a) to
wo wo
wo
Figure 2: Dependency trees and generation pro-
cesses for example (4). This example sentence has
two possible dependency structures according to
the interpretation of to: comitative in (a) and co-
ordinate conjunction in (b).
5.1.2 Cooccurrences of Noun-noun
Modifications
Adnominal nouns have selectional preferences
to nouns, and thus this characteristic is useful for
coordination disambiguation (Resnik, 1999). We
collect dependency relations between nouns from
automatic parses of the web corpus. As a re-
sult, 10.7M unique dependency relations were ob-
tained.
5.2 Our Model
We employ a probabilistic generative dependency
parser (Kawahara and Kurohashi, 2007) as a base
model. This base model measures similarities
between conjuncts in the same way as (Kuro-
hashi and Nagao, 1994), and calculates probabil-
ities of generating these similarities. Our proposed
model, however, does not do both of them. Our
model purely depends on selectional preferences
provided by automatically acquired lexical knowl-
edge.
Our model gives probabilities to all the possible
dependency structures for an input sentence, and
selects the structure that has the highest probabil-
ity. For example, consider the following sentence:
(4) ane-to
sister-CNJ
otouto-wo
brother-ACC
yonda
invited
(invited (my) sister and brother)
For this sentence, our model assesses the two de-
pendency structures (a) and (b) in Figure 2. In our
model, both of the pre-conjunct and post-conjunct
are generated from the predicate. That is, in (b),
both ane (sister) and otouto (brother) with wo
(ACC) are generated from yonda (invited). To
identify the correct structure, (b), it is essential
that both ane (sister) and otouto (brother) are el-
igible for the accusative words of yonda (invited).
Therefore, selectional preferences play an impor-
tant role in coordination disambiguation. On the
other hand, in (a), ane (sister) with to (CMI) is
generated from yonda (invited), and also otouto
(brother) with wo (ACC) is generated from yonda.
However, yonda is not likely to have the to case
slot, so the probability of (a) is lower than that of
(b). Our model can finally select the correct struc-
ture, (b), which has the highest probability. This
kind of assessment is also performed to resolve
the scope ambiguities of coordinate structures as
shown in Figure 1.
This model gives a probability to each possible
dependency structure, T , and case structure, L, of
the input sentence, S, and outputs the dependency
and case structure that have the highest probability.
That is to say, the model selects the dependency
structure, T
best
, and the case structure, L
best
, that
maximize the probability, P (T,L|S):
(T
best
, L
best
) = argmax
(T,L)
P (T,L|S)
= argmax
(T,L)
P (T,L, S)
P (S)
= argmax
(T,L)
P (T,L, S) (1)
The last equation is derived because P (S) is con-
stant.
The model considers a clause as a generation
unit and generates the input sentence from the end
of the sentence in turn. The probability P (T,L, S)
is defined as the product of probabilities for gener-
ating clause C
i
as follows:
P (T,L, S) =
?
C
i
?S
P (C
i
, rel
ih
i
|C
h
i
) (2)
C
h
i
is C
i
?s modifying clause, and rel
ih
i
is the de-
pendency relation between C
i
and C
h
i
. The main
clause, C
n
, at the end of a sentence does not have
a modifying head, but a virtual clause C
h
n
= EOS
(End Of Sentence) is added. Dependency relation
rel
ih
i
is classified into two types, C (coordinate)
and D (normal dependency).
Clause C
i
is decomposed into its clause type,
f
i
, (including the predicate?s inflection and func-
tion words) and its remaining content part C
i
?
.
Clause C
h
i
is also decomposed into its content
part, C
h
i
?
, and its clause type, f
h
i
.
P (C
i
, rel
ih
i
|C
h
i
) = P (C
i
?
, f
i
, rel
ih
i
|C
h
i
?
, f
h
i
)
? P (C
i
?
, rel
ih
i
|f
i
, C
h
i
?
)? P (f
i
|f
h
i
)
? P (C
i
?
|rel
ih
i
, f
i
, C
h
i
?
)? P (rel
ih
i
|f
i
)
? P (f
i
|f
h
i
) (3)
429
Equation (3) is derived using appropriate approx-
imations described in Kawahara and Kurohashi
(2007).
We call P (C
i
?
|rel
ih
i
, f
i
, C
h
i
?
) generative prob-
ability of a content part, and P (rel
ih
i
|f
i
) gener-
ative probability of a dependency relation. The
following two subsections describe these probabil-
ities.
5.2.1 Generative Probability of Dependency
Relation
The most important feature to determine
whether two clauses are coordinate is a coordina-
tion key. Therefore, we consider a coordination
key, k
i
, as clause type f
i
. The generative prob-
ability of a dependency relation, P (rel
ih
i
|f
i
), is
defined as follows:
P (rel
ih
i
|f
i
) = P (rel
ih
i
|k
i
) (4)
We classified coordination keys into 52 classes ac-
cording to the classification described in (Kuro-
hashi and Nagao, 1994). If type f
i
does not
contain a coordination key, the relation is always
D (normal dependency), that is, P (rel
ih
i
|f
i
) =
P (D|?) = 1.
The generative probability of a dependency re-
lation was estimated from the Kyoto Text Corpus
using maximum likelihood.
5.2.2 Generative Probability of Content Part
The generative probability of a content part
changes according to the class of a content part,
C
i
?
. We classify C
i
?
into two classes: predicate
clause and nominal phrase.
If C
i
?
is a predicate clause, C
i
?
represents a case
structure. We consider that a case structure con-
sists of a predicate, v
i
, a case frame, CF
l
, and
a case assignment, CA
k
. Case assignment CA
k
represents correspondences between the input case
components and the case slots shown in Figure 3.
Thus, the generative probability of a content part
is decomposed as follows:
P
v
(C
i
?
|rel
ih
i
, f
i
, C
h
i
?
)
= P (v
i
, CF
l
, CA
k
|rel
ih
i
, f
i
, C
h
i
?
)
? P (v
i
|rel
ih
i
, f
i
, w
h
i
)
? P (CF
l
|v
i
)
? P (CA
k
|CF
l
, f
i
) (5)
These generative probabilities are estimated from
case frames themselves and parsing results of a
large web corpus.
bentou-wa
tabete
(lunchbox)
(eat)
?
lunchbox, bread, ?wo
man, student, ?ga
taberu1 (eat)
Case Frame CF
l
Case 
Assignment
CA
k
(no correspondence)
Dependency Structure of S
Figure 3: Example of case assignment.
If C
i
?
is a nominal phrase and consists of a noun
n
i
, we consider the following probability instead
of equation (5):
P
n
(C
i
?
|rel
ih
i
, f
i
, C
h
i
?
) ? P (n
i
|rel
ih
i
, f
i
, w
h
i
)
This is because a noun does not have a case frame
or any case components in the current framework.
Since we do not use cooccurrences of coordinate
phrases as used in the base model, rel
ih
i
is always
D (normal dependency). This probability is esti-
mated from the cooccurrences of noun-noun mod-
ifications using maximum likelihood.
6 Experiments
We evaluated the dependency structures that were
output by our model. The case frames used in this
paper were automatically constructed from 500M
Japanese sentences obtained from the web.
In this work, the parameters related to unlexical
types were calculated from the Kyoto Text Corpus,
which is a small tagged corpus of newspaper ar-
ticles, and lexical parameters were obtained from
a huge web corpus. To evaluate the effectiveness
of our model, our experiments were conducted us-
ing web sentences. As the test corpus, we used
759 web sentences
3
, which are described in sec-
tion 3.2. We also used the Kyoto Text Corpus as
a development corpus to optimize the smoothing
parameters. The system input was automatically
tagged using the JUMAN morphological analyzer
4
.
We used two baseline systems for compara-
tive purposes: a rule-based dependency parser
(Kurohashi and Nagao, 1994) and the probabilistic
generative model of dependency, coordinate and
case structure analysis (Kawahara and Kurohashi,
2007)
5
.
6.1 Evaluation of Dependency Structures
We evaluated the dependency structures that were
analyzed by the proposed model. Evaluating the
3
The test set was not used to construct case frames or es-
timate probabilities.
4
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
5
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
430
Table 2: Experimental results of dependency structures. ?all? represents the accuracy of all the depen-
dencies, and ?coordination key? represents the accuracy of only the coordination key bunsetsus.
rule-coord-w/sim prob-coord-w/sim prob-coord-wo/sim
all 3,821/4,389 (87.1%) 3,852/4,389 (87.8%) 3,877/4,389 (88.3%)
coordination key 878/1,106 (79.4%) 881/1,106 (79.7%) 897/1,106 (81.1%)
scope ambiguity of coordinate structures is sub-
sumed within this dependency evaluation. The de-
pendency structures obtained were evaluated with
regard to dependency accuracy ? the proportion
of correct dependencies out of all dependencies
except for the last one in the sentence end
6
. Ta-
ble 2 lists the dependency accuracy. In this table,
?rule-coord-w/sim? represents a rule-based depen-
dency parser; ?prob-coord-w/sim? represents the
probabilistic parser of dependency, coordinate and
case structure (Kawahara and Kurohashi, 2007);
and ?prob-coord-wo/sim? represents our proposed
model. ?all? represents the overall accuracy, and
?coordination key? represents the accuracy of only
the coordination key bunsetsus. The proposed
model, ?prob-coord-wo/sim?, significantly outper-
formed both ?rule-coord-w/sim? and ?prob-coord-
w/sim? (McNemar?s test; p < 0.05) for ?all?.
Figure 4 shows some analyses that are cor-
rectly analyzed by the proposed method. For
example, in sentence (1), our model can rec-
ognize the correct coordinate structure that con-
joins ?densya-no hassyaaizu? (departure signals
of trains) and ?keitaidenwa-no tyakushinon? (ring
tones of cell phones). This is because the case
frame of ?ongaku-ni naru? (become music) is
likely to generate ?hassyaaizu? (departure signal)
and ?tyakushinon? (ring tone).
To compare our results with a state-of-the-art
discriminative dependency parser, we input the
same test corpus into an SVM-based Japanese
dependency parser, CaboCha
7
(Kudo and Mat-
sumoto, 2002). Its dependency accuracy was
86.7% (3,807/4,389), which is close to that of
?rule-coord-w/sim?. This low accuracy is at-
tributed to the lack of the consideration of coor-
dinate structures. Though dependency structures
are closely related to coordinate structures, the
CaboCha parser failed to incorporate coordination
features. Another cause of the low accuracy is
the out-of-domain training corpus. That is, the
parser is trained on a newspaper corpus, whereas
6
Since Japanese is head-final, the second to last bunsetsu
unambiguously depends on the last bunsetsu, and the last bun-
setsu has no dependency.
7
http://chasen.org/?taku/software/cabocha/
the test corpus is obtained from the web, because
of the non-availability of a tagged web corpus that
is large enough to train a supervised parser.
6.2 Discussion
We presented a method for coordination dis-
ambiguation without using similarities, and this
method achieved better performance than the
conventional approaches based on similarities.
Though we do not use similarities, we implicitly
consider similarities between conjuncts. This is
because the heads of pre- and post-conjuncts share
a case marker and a predicate, and thus they are es-
sentially similar. Our idea is related to the notion
of distributional similarity. Chantree et al (2005)
applied the distributional similarity proposed by
Lin (1998) to coordination disambiguation. Lin
extracted from a corpus dependency triples of two
words and the grammatical relationship between
them, and considered that similar words are likely
to have similar dependency relations. The differ-
ence between Chantree et al (2005) and ours is
that their method does not use the information of
verbs in the sentence under consideration, but use
only the cooccurrence information extracted from
a corpus.
On the other hand, the disadvantage of our
model is that it cannot consider the parallelism of
conjuncts, which still seems to exist in especially
strong coordinate structures. Handling of such par-
allelism is an open question of our model.
The generation process adopted in this work
is similar to the design of dependency structure
described in Hudson (1990), which lets the con-
juncts have a dependency relation to the predi-
cate. Nilsson et al (2006) mentioned this notion,
but did not consider this idea in their experiments
of tree transformations for data-driven dependency
parsers. In addition, it is not necessary for our
method to transform dependency trees in pre- and
post-processes, because we just changed the pro-
cess of generation in the generative parser.
7 Conclusion
In this paper, we first came up with a hypoth-
esis that coordinate structures are supported by
431
? ?
(1) densya-no hassyaaizu-ya, keitaidenwa-no tyakushinon-madega ongaku-ni naru-hodoni, ...
train-GEN departure signal cell phone-GEN ring tone-also music-ACC become
(departure signals of trains and ring tones of cell phones become music, ...)
? ?
(2) nabe-ni dashijiru 3 kappu-to, nokori-no syouyu, mirin, sake-wo irete, ...
pot-DAT stock three cups-and remainder-GEN soy mirin sake-ACC pour
(pour three cups of stock and remaining soy, mirin and sake to the pot, ...)
Figure 4: Examples of correct analyses. The dotted lines represent the analysis by the baseline, ?prob-
coord-w/sim?, and the solid lines represent the analysis by the proposed method, ?prob-coord-wo/sim?.
surrounding dependency relations. Based on this
hypothesis, we built an integrated probabilistic
model for coordination disambiguation and depen-
dency/case structure analysis. This model does
not make use of similarities to analyze coordinate
structures, but takes advantage of selectional pref-
erences from a huge raw corpus and large-scale
case frames. The experimental results indicate
the effectiveness of our model, and thus support
our hypothesis. Our future work involves incorpo-
rating ellipsis resolution to develop an integrated
model for syntactic, case, and ellipsis analysis.
References
Agarwal, Rajeev and Lois Boggess. 1992. A simple but use-
ful approach to conjunct identification. In Proceedings of
ACL1992, pages 15?21.
Chantree, Francis, Adam Kilgarriff, Anne de Roeck, and Al-
istair Wills. 2005. Disambiguating coordinations us-
ing word distribution information. In Proceedings of
RANLP2005.
Charniak, Eugene and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking. In
Proceedings of ACL2005, pages 173?180.
Dubey, Amit, Frank Keller, and Patrick Sturt. 2006. Inte-
grating syntactic priming into an incremental probabilistic
parser, with an application to psycholinguistic modeling.
In Proceedings of COLING-ACL2006, pages 417?424.
Goldberg, Miriam. 1999. An unsupervised model for statis-
tically determining coordinate phrase attachment. In Pro-
ceedings of ACL1999, pages 610?614.
Hogan, Deirdre. 2007. Coordinate noun phrase disambigua-
tion in a generative parsing model. In Proceedings of
ACL2007, pages 680?687.
Hudson, Richard. 1990. English Word Grammar. Blackwell.
Kawahara, Daisuke and Sadao Kurohashi. 2006. Case frame
compilation from the web using high-performance com-
puting. In Proceedings of LREC2006.
Kawahara, Daisuke and Sadao Kurohashi. 2007. Proba-
bilistic coordination disambiguation in a fully-lexicalized
Japanese parser. In Proceedings of EMNLP-CoNLL2007,
pages 306?314.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceedings
of CoNLL2002, pages 29?35.
Kurohashi, Sadao and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on the
detection of conjunctive structures. Computational Lin-
guistics, 20(4):507?534.
Kurohashi, Sadao and Makoto Nagao. 1998. Building a
Japanese parsed corpus while improving the parsing sys-
tem. In Proceedings of LREC1998, pages 719?724.
Lin, Dekang. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL98, pages
768?774.
Nakov, Preslav and Marti Hearst. 2005. Using the web as
an implicit training set: Application to structural ambigu-
ity resolution. In Proceedings of HLT-EMNLP2005, pages
835?842.
Nilsson, Jens, Joakim Nivre, and Johan Hall. 2006. Graph
transformations in data-driven dependency parsing. In
Proceedings of COLING-ACL2006, pages 257?264.
Nilsson, Jens, Joakim Nivre, and Johan Hall. 2007. General-
izing tree transformations for inductive dependency pars-
ing. In Proceedings of ACL2007, pages 968?975.
Resnik, Philip. 1999. Semantic similarity in a taxonomy: An
information-based measure and its application to problems
of ambiguity in natural language. Journal of Artificial In-
telligence Research, 11:95?130.
Sassano, Manabu. 2004. Linear-time dependency analysis
for Japanese. In Proceedings of COLING2004, pages 8?
14.
Shimbo, Masashi and Kazuo Hara. 2007. A discriminative
learning model for coordinate conjunctions. In Proceed-
ings of EMNLP-CoNLL2007, pages 610?619.
Tamura, Akihiro, Hiroya Takamura, and Manabu Oku-
mura. 2007. Japanese dependency analysis using the
ancestor-descendant relation. In Proceedings of EMNLP-
CoNLL2007, pages 600?609.
The National Institute for Japanese Language. 2004. Bun-
ruigoihyo. Dainippon Tosho, (In Japanese).
432
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 769?776
Manchester, August 2008
A Fully-Lexicalized Probabilistic Model
for Japanese Zero Anaphora Resolution
Ryohei Sasano
?
Graduate School of Information Science
and Technology, University of Tokyo
ryohei@nlp.kuee.kyoto-u.ac.jp
Daisuke Kawahara
National Institute of Information
and Communication Technology
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Infomatics,
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
This paper presents a probabilistic model
for Japanese zero anaphora resolution.
First, this model recognizes discourse en-
tities and links all mentions to them. Zero
pronouns are then detected by case struc-
ture analysis based on automatically con-
structed case frames. Their appropriate
antecedents are selected from the entities
with high salience scores, based on the
case frames and several preferences on
the relation between a zero pronoun and
an antecedent. Case structure and zero
anaphora relation are simultaneously de-
termined based on probabilistic evaluation
metrics.
1 Introduction
Anaphora resolution is one of the most important
techniques in discourse analysis. In English, def-
inite noun phrases such as the company and overt
pronouns such as he are anaphors that refer to pre-
ceding entities (antecedents). On the other hand,
in Japanese, anaphors are often omitted and these
omissions are called zero pronouns. We focus
on zero anaphora resolution of Japanese web cor-
pus, in which anaphors are often omitted and zero
anaphora resolution plays an important role in dis-
course analysis.
Zero anaphora resolution can be divided into
two phases. The first phase is zero pronoun detec-
tion and the second phase is zero pronoun resolu-
tion. Zero pronoun resolution is similar to coref-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
* Research Fellow of the Japan Society for the Promotion of
Science (JSPS)
erence resolution and pronoun resolution, which
have been studied for many years (e.g. Soon et
al. (2001); Mitkov (2002); Ng (2005)). Isozaki and
Hirao (2003) and Iida et al (2006) focused on zero
pronoun resolution assuming perfect pre-detection
of zero pronouns. However, we consider that zero
pronoun detection and resolution have a tight rela-
tion and should not be handled independently. Our
proposed model aims not only to resolve zero pro-
nouns but to detect zero pronouns.
Zero pronouns are not expressed in a text and
have to be detected prior to identifying their an-
tecedents. Seki et al (2002) proposed a proba-
bilistic model for zero pronoun detection and res-
olution that uses hand-crafted case frames. In
order to alleviate the sparseness of hand-crafted
case frames, Kawahara and Kurohashi (2004) in-
troduced wide-coverage case frames to zero pro-
noun detection that are automatically constructed
from a large corpus. They use the case frames as
selectional restriction for zero pronoun resolution,
but do not utilize the frequency of each example of
case slots. However, since the frequency is shown
to be a good clue for syntactic and case structure
analysis (Kawahara and Kurohashi, 2006), we con-
sider the frequency also can benefit zero pronoun
detection. Therefore we propose a probabilistic
model for zero anaphora resolution that fully uti-
lizes case frames. This model directly consid-
ers the frequency and estimates case assignments
for overt case components and antecedents of zero
pronoun simultaneously.
In addition, our model directly links each zero
pronoun to an entity, while most existing mod-
els link it to a certain mention of an entity. In
our model, mentions and zero pronouns are treated
similarly and all of them are linked to correspond-
ing entities. In this point, our model is similar to
769
Table 1: Examples of Constructed Case Frames.
case slot examples generalized examples with rate
ga (subjective) he, driver, friend, ? ? ? [CT:PERSON]:0.45, [NE:PERSON]:0.08, ? ? ?
tsumu (1)
wo (objective) baggage, luggage, hay, ? ? ? [CT:ARTIFACT]:0.31, ? ? ?
(load)
ni (dative) car, truck, vessel, seat, ? ? ? [CT:VEHICLE]:0.32, ? ? ?
tsumu (2)
ga (subjective) player, children, party, ? ? ? [CT:PERSON]:0.40, [NE:PERSON]:0.12, ? ? ?
(accumulate)
wo (objective) experience, knowledge, ? ? ? [CT:ABSTRACT]:0.47, ? ? ?
.
.
.
.
.
.
.
.
.
ga (subjective) company, Microsoft, firm, ? ? ? [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, ? ? ?
hanbai (1) wo (objective) goods, product, ticket, ? ? ? [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, ? ? ?
(sell) ni (dative) customer, company, user, ? ? ? [CT:PERSON]:0.28, ? ? ?
de (locative) shop, bookstore, site ? ? ? [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, ? ? ?
.
.
.
.
.
.
.
.
.
the coreference model proposed by Luo (2007) and
that proposed by Yang et al (2008). Due to this
characteristic, our model can utilize information
beyond a mention and easily consider salience (the
importance of an entity).
2 Construction of Case Frames
Case frames describe what kinds of cases each
predicate has and what kinds of nouns can fill
these case slots. We construct case frames from
a large raw corpus by using the method proposed
by Kawahara and Kurohashi (2002), and use them
for case structure analysis and zero anaphora res-
olution. This section shows how to construct the
case frames.
2.1 Basic Method
After a large corpus is parsed by a Japanese parser,
case frames are constructed from modifier-head
examples in the resulting parses. The problems of
case frame construction are syntactic and seman-
tic ambiguities. That is to say, the parsing results
inevitably contain errors and predicate senses are
intrinsically ambiguous. To cope with these prob-
lems, case frames are gradually constructed from
reliable modifier-head examples.
First, modifier-head examples that have no syn-
tactic ambiguity are extracted, and they are disam-
biguated by coupling a predicate and its closest
case component. Such couples are explicitly ex-
pressed on the surface of text, and can be consid-
ered to play an important role in sentence mean-
ings. For instance, examples are distinguished not
by predicates (e.g., ?tsumu (load/accumulate))?,
but by couples (e.g., ?nimotsu-wo tsumu (load bag-
gage)? and ?keiken-wo tsumu (accumulate experi-
ence))?. Modifier-head examples are aggregated in
this way, and yield basic case frames.
Thereafter, the basic case frames are clustered
to merge similar case frames. For example, since
?nimotsu-wo tsumu (load baggage)? and ?busshi-
wo tsumu (load supplies)? are similar, they are
clustered. The similarity is measured using a
thesaurus (The National Language Institute for
Japanese Language, 2004). Using this gradual pro-
cedure, we constructed case frames from approx-
imately 1.6 billion sentences extracted from the
web. In Table 1, some examples of the resulting
case frames are shown.
2.2 Generalization of Examples
By using case frames that are automatically con-
structed from a large corpus, sparseness problem
is alleviated to some extent, but still remains. For
instance, there are thousands of named entities
(NEs), which cannot be covered intrinsically. To
deal with this sparseness problem, we general-
ize the examples of case slots. Kawahara and
Kurohashi also give generalized examples such
as ?agent? but only a few types. We generalize
case slot examples based on categories of common
nouns and NE classes.
First, we use the categories that Japanese mor-
phological analyzer JUMAN
1
adds to common
nouns. In JUMAN, about twenty categories are de-
fined and tagged to common nouns. For example,
?ringo (apple),? ?inu (dog)? and ?byoin (hospi-
tal)? are tagged as ?FOOD,? ?ANIMAL? and ?FA-
CILITY,? respectively. For each category, we cal-
culate the rate of categorized example among all
case slot examples, and add it to the case slot as
?[CT:FOOD]:0.07.?
We also generalize NEs. We use a common
standard NE definition for Japanese provided by
IREX workshop (1999). IREX defined eight NE
classes as shown in Table 2. We first recognize
NEs in the source corpus by using an NE recog-
nizer (Sasano and Kurohashi, 2008), and then con-
struct case frames from the NE-recognized corpus.
1
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
770
Table 2: Definition of NE in IREX.
NE class Examples
ORGANIZATION NHK Symphony Orchestra
PERSON Kawasaki Kenjiro
LOCATION Rome, Sinuiju
ARTIFACT Nobel Prize
DATE July 17, April this year
TIME twelve o?clock noon
MONEY sixty thousand dollars
PERCENT 20%, thirty percents
As well as categories, for each NE class, we calcu-
late the NE rate among all case slot examples, and
add it to the case slot as ?[NE:PERSON]:0.12.?
The generalized examples are also included in
Table 1. This information is utilized to estimate the
case assignment probability, which will be men-
tioned in Section 3.2.3.
3 Zero Anaphora Resolution Model
In this section, we propose a probabilistic model
for Japanese zero anaphora resolution.
3.1 Overview
The outline of our model is as follows:
1. Parse an input text using the Japanese parser
KNP
2
and recognize NEs.
2. Conduct coreference resolution and link each
mention to an entity or create new entity.
3. For each sentence, from the end of the sen-
tence, analyze each predicate by the follow-
ing steps:
(a) Select a case frame temporarily.
(b) Consider all possible correspondence
between each input case component and
an case slot of the selected case frame.
(c) Regard case slots that have no corre-
spondence as zero pronoun candidates.
(d) Consider all possible correspondence
between each zero pronoun candidate
and an existing entity.
(e) For each possible case frame, estimate
each correspondence probabilistically,
and select the most likely case frame and
correspondence.
In this paper, we concentrate on three case slots
for zero anaphora resolution: ?ga (subjective),?
?wo (objective)? and ?ni (dative),? which cover
about 90% of zero anaphora.
2
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
Morphological analysis, NE recognition, syn-
tactic analysis and coreference resolution are con-
ducted as pre-processes for zero anaphora resolu-
tion. Therefore, the model has already recognized
existing entities before zero anaphora resolution.
For example, let us consider the following text:
(i) Toyota-wa 1997-nen hybrid car Prius-wo
hatsubai(launch). 2000-nen-karaha kaigai
(overseas)-demo hanbai(sell)-shiteiru.
(Toyota launched the hybrid car Prius in 1997. ?
1
started selling ?
2
overseas in 2000.)
Figure 1 shows the analysis process for this text.
There are three mentions
3
in the first sentence, and
the two mentions, hybrid car and Prius, appear in
apposition. Thus, after the pre-processes, two enti-
ties, {Toyota} and {hybrid-car, Prius}, are created.
Then, case structure analysis for the predicate
hatsubai (launch) is conducted. First, one of the
case frames of hatsubai (launch) is temporarily se-
lected and each input case component is assigned
to an appropriate case slot. For instance, case com-
ponent Toyota is assigned to ga case slot and Prius
is assigned to wo case slot
4
. In this case, though
there is a mention hybrid-car that is not a case
component of hatsubai (launch) by itself, it refers
to the same entity as Prius refers. Thus, there is no
entity that is not linked to hatsubai (launch), and
no further analysis is conducted.
Now, let us consider the second sentence. A
mention kaigai (overseas) appears and a new entity
{kaigai} is created. Then, case structure analysis
for the predicate hanbai (sell) is conducted. There
is only one overt case component kaigai (over-
seas), and it is assigned to a case slot of the se-
lected case frame of hanbai (sell). For instance,
the case frame hanbai(1) in Table 1 is selected and
kaigai (overseas) is assigned to de (locative) case
slot. In this case, the remaining case slots ga, wo
and ni are considered as zero pronouns, and all
possible correspondences between zero pronouns
and remaining entities are considered. As a result
of probabilistic estimation, the entity {Toyota} is
assigned to ga case, the entity {hybrid-car, Prius}
is assigned to wo case and no entity is assigned to
ni case.
Now, we show how to estimate the correspon-
dence probabilistically in the next subsection.
3
In this paper, we do not consider time expressions, such
as 1997, as mentions.
4
Note that since there are some non case-making postposi-
tions in Japanese, such as ?wa? and ?mo,? several correspon-
dences can be considered.
771
Toyota-wa
Prius-wo
hybrid car
hatsubai.
kaigai-demo
hanbai-shiteiru.
1997-nen
2000-nen-karawa
{Toyota, ?
?
}
{hybrid car, 
Prius, ?2 }
{kaigai}
Entities
(overseas)
(launch)
(sell)
hatsubai (launch)
ga
subjective
company, SONY, firm, ? 
[NE:ORGANIZATION] 0.15, ?
wo
objective
product, CD, model, car,  ?
[CT:ARTIFACT] 0.40, ?
de      
locative
area, shop, world, Japan, ?
[CT:FACILITY] 0.13, ?
hanbai (sell)
ga
subjective
company, Microsoft, ? 
[NE:ORGANIZATION] 0.16, ?
wo     
objective
goods, product, ticket, ? 
[CT:ARTIFACT] 0.40, ?
ni
dative
customer, company, user, ? 
[CT:PERSON] 0.28, ?
de      
locative
shop, bookstore, site, ? 
[CT:FACILITY] 0.40, ?
:direct case assignment
:indirect case assignment (zero anaphora)
Case framesInput sentences
Toyota launched the hybrid car Prius in 1997. ?
?
started selling ?2 overseas in 2000.
Figure 1: An Example of Case Assignment CA
k
.
3.2 Probabilistic Model
The proposed model gives a probability to each
possible case frame CF and case assignment CA
when target predicate v, input case components
ICC and existing entities ENT are given. It also
outputs the case frame and case assignment that
have the highest probability. That is to say, our
model selects the case frame CF
best
and the case
assignment CA
best
that maximize the probability
P (CF,CA|v, ICC,ENT ):
(CF
best
, CA
best
)
= argmax
CF,CA
P (CF,CA|v, ICC,ENT ) (1)
Though case assignment CA usually represents
correspondences between input case components
and case slots, in our model it also represents
correspondences between antecedents of zero pro-
nouns and case slots. Hereafter, we call the former
direct case assignment (DCA) and the latter indi-
rect case assignment (ICA). Then, we transform
P (CF
l
, CA
k
|v, ICC,ENT ) as follows:
P (CF
l
, CA
k
|v, ICC,ENT )
=P (CF
l
|v, ICC,ENT )
? P (DCA
k
|v, ICC,ENT,CF
l
)
? P (ICA
k
|v, ICC,ENT,CF
l
, DCA
k
)
?P (CF
l
|v, ICC) ? P (DCA
k
|ICC,CF
l
)
? P (ICA
k
|ENT,CF
l
, DCA
k
) (2)
=P (CF
l
|v)?P (DCA
k
, ICC|CF
l
)/P (ICC|v)
? P (ICA
k
|ENT,CF
l
, DCA
k
) (3)
(
? P (CF
l
|v, ICC) =
P (CF
l
, ICC|v)
P (ICC|v)
=
P (ICC|CF
l
, v) ? P (CF
l
|v)
P (ICC|v)
=
P (ICC|CF
l
) ? P (CF
l
|v)
P (ICC|v)
,
(? CF
l
contains the information about v.)
P (DCA
k
|ICC,CF
l
)
=
P (DCA
k
, ICC|CF
l
)
P (ICC|CF
l
)
)
Equation (2) is derived because we assume that
the case frame CF
l
and direct case assignment
DCA
k
are independent of existing entities ENT ,
and indirect case assignment ICA
k
is independent
of input case components ICC.
Because P (ICC|v) is constant, we can say that
our model selects the case frame CF
best
and the
direct case assignment DCA
best
and indirect case
assignment ICA
best
that maximize the probability
P (CF,DCA, ICA|v, ICC,ENT ):
(CF
best
, DCA
best
, ICA
best
) =
argmax
CF,DCA,ICA
(
P (CF |v) ? P (DCA, ICC|CF )
?P (ICA|ENT,CF,DCA)
)
(4)
The probability P (CF
l
|v), called generative
probability of a case frame, is estimated from
case structure analysis of a large raw corpus. The
following subsections illustrate how to calculate
P (DCA
k
, ICC|CF
l
) and P (ICA
k
|ENT,CF
l
,
DCA
k
).
772
3.2.1 Generative Probability of Direct Case
Assignment
For estimation of generative probability of di-
rect case assignment P (DCA
k
, ICC|CF
l
), we
follow Kawahara and Kurohashi?s (2006) method.
They decompose P (DCA
k
, ICC|CF
l
) into the
following product depending on whether a case
slot s
j
is filled with an input case component or
vacant:
P (DCA
k
, ICC|CF
l
) =
?
s
j
:A(s
j
)=1
P (A(s
j
) = 1, n
j
, c
j
|CF
l
, s
j
)
?
?
s
j
:A(s
j
)=0
P (A(s
j
) = 0|CF
l
, s
j
)
=
?
s
j
:A(s
j
)=1
{
P (A(s
j
) = 1|CF
l
, s
j
)
? P (n
j
, c
j
|CF
l
, s
j
, A(s
j
) = 1)
}
?
?
s
j
:A(s
j
)=0
P (A(s
j
) = 0|CF
l
, s
j
) (5)
where the function A(s
j
) returns 1 if a case slot s
j
is filled with an input case component; otherwise
0, n
j
denotes the content part of the case compo-
nent, and c
j
denotes the surface case of the case
component.
The probabilities P (A(s
j
) = 1|CF
l
, s
j
) and
P (A(s
j
) = 0|CF
l
, s
j
) are called generative prob-
ability of a case slot, and estimated from case
structure analysis of a large raw corpus as well as
generative probability of a case frame.
The probability P (n
j
, c
j
|CF
l
, s
j
, A(s
j
) = 1) is
called generative probability of a case component
and estimated as follows:
P (n
j
, c
j
|CF
l
, s
j
, A(s
j
) = 1)
?P (n
j
|CF
l
, s
j
, A(s
j
)=1)?P (c
j
|s
j
, A(s
j
)=1) (6)
P (n
j
|CF
l
, s
j
, A(s
j
) = 1) means the gener-
ative probability of a content part n
j
from a
case slot s
j
in a case frame CF
l
, and esti-
mated by using the frequency of a case slot
example in the automatically constructed case
frames. P (c
j
|s
j
, A(s
j
) = 1) is approximated by
P (c
j
|case type of(s
j
), A(s
j
)=1) and estimated
from the web corpus in which the relationship be-
tween a surface case marker and a case slot is an-
notated by hand.
3.2.2 Probability of Indirect Case Assignment
To estimate probability of indirect case assign-
ment P (ICA
k
|ENT,CF
l
, DCA
k
) we also de-
compose it into the following product depending
Table 3: Location Classes of Antecedents.
intra-sentence: case components of
L
1
: parent predicate of V
z
L
2
: parent predicate of V
z
? (parallel)
L
3
: child predicate of V
z
L
4
: child predicate of V
z
(parallel)
L
5
: parent predicate of parent noun phrase of V
z
L
6
: parent predicate of parent predicate of V
z
(parallel)
L
7
: other noun phrases following V
z
L
8
: other noun phrases preceding V
z
inter-sentence: noun phrases in
L
9
: 1 sentence before
L
10
: 2 sentences before
L
11
: 3 sentences before
L
12
: more than 3 sentences before
on whether a case slot s
j
is filled with an entity
ent
j
or vacant:
P (ICA
k
|ENT,CF
l
, DCA
k
) =
?
s
j
:A
?
(s
j
)=1
P (A
?
(s
j
) = 1, ent
j
|ENT,CF
l
, s
j
)
?
?
s
j
:A
?
(s
j
)=0
P (A
?
(s
j
) = 0|ENT,CF
l
, s
j
) (7)
where the function A
?
(s
j
) returns 1 if a case slot
s
j
is filled with an entity ent
j
; otherwise 0. Note
that we only consider case slots ga, wo and ni that
is not filled with an input case component. We
approximate P (A
?
(s
j
) = 1, ent
j
|ENT,CF
l
, s
j
)
and P (A
?
(s
j
) = 0|ENT,CF
l
, s
j
) as follows:
P (A
?
(s
j
) = 1, ent
j
|ENT,CF
l
, s
j
)
? P (A
?
(s
j
) = 1, ent
j
|ent
j
, CF
l
, s
j
)
= P (A
?
(s
j
) = 1|ent
j
, CF
l
, s
j
) (8)
P (A
?
(s
j
) = 0|ENT,CF
l
, s
j
)
? P (A
?
(s
j
) = 0|case type of(s
j
)) (9)
Equation (8) is derived because we assume
P (A
?
(s
j
) = 1|CF
l
, s
j
) is independent of exist-
ing entities that are not assigned to s
j
. Equation
(9) is derived because we assume P (A
?
(s
j
) = 0)
is independent of ENT and CF
l
, and only de-
pends on the case type of s
j
, such as ga, wo and ni.
P (A
?
(s
j
)=0|case type of(s
j
)) is the probability
that a case slot has no correspondence after zero
anaphora resolution and estimated from anaphoric
relation tagged corpus.
Let us consider the probability P (A
?
(s
j
) =
1|ent
j
, CF
l
, s
j
). We decompose ent
j
into content
part n
j
m
, surface case c
j
n
and location class l
j
n
.
Here, location classes denote the locational rela-
tions between zero pronouns and their antecedents.
We defined twelve location classes as described in
Table 3. In Table 3, V
z
means a predicate that has
a zero pronoun. Note that we also consider the
773
locations of zero pronouns that are linked to the
target entity as location class candidates. Now we
roughly approximate P (A
?
(s
j
)=1|ent
j
, CF
l
, s
j
)
as follows:
P (A
?
= 1|ent
j
, CF
l
, s
j
)
=P (A
?
= 1|n
j
m
, c
j
n
, l
j
n
, CF
l
, s
j
)
=
P (n
j
m
, c
j
n
, l
j
n
|CF
l
, s
j
,A
?
=1)?P (A
?
=1|CF
l
, s
j
)
P (n
j
m
, c
j
n
, l
j
n
|CF
l
, s
j
)
?
P (n
j
m
|CF
l
, s
j
, A
?
=1)
P (n
j
m
|CF
l
, s
j
)
?
P (c
j
n
|CF
l
, s
j
, A
?
=1)
P (c
j
n
|CF
l
, s
j
)
?
P (l
j
n
|CF
l
, s
j
, A
?
=1)
P (l
j
n
|CF
l
, s
j
)
?P (A
?
=1|CF
l
, s
j
) (10)
?
P (n
j
m
|CF
l
, s
j
, A
?
=1)
P (n
j
m
)
?
P (c
j
n
|case type of(s
j
), A
?
=1)
P (c
j
n
)
? P (A
?
=1|l
j
n
, case type of(s
j
)) (11)
(
?
P (l
j
n
|CF
l
, s
j
, A
?
=1)
P (l
j
n
|CF
l
, s
j
)
?P (A
?
=1|CF
l
, s
j
)
=
P (A
?
=1, l
j
n
|CF
l
, s
j
)
P (l
j
n
|CF
l
, s
j
)
=P (A
?
=1|CF
l
, l
j
n
, s
j
)
)
Note that because ent
j
is often mentioned more
than one time, there are several combinations of
content part n
j
m
, surface case c
j
n
and location
class l
j
n
candidates. We select the pair of m and n
with the highest probability.
Equation (10) is derived because we as-
sume n
j
m
, c
j
n
and l
j
n
are independent of each
other. Equation (11) is derived because we ap-
proximate P (A
?
= 1|CF
l
, l
j
n
, s
j
) as P (A
?
=
1|l
j
n
, case type of(s
j
)), and assume P (n
j
m
) and
P (c
j
n
) are independent of CF
l
and s
j
. Since these
approximation is too rough, specifically, P (n
j
m
)
and P (c
j
n
) tend to be somewhat smaller than
P (n
j
m
|CF
l
, s
j
) and P (c
j
n
|CF
l
, s
j
) and equation
(11) often becomes too large, we introduce a
parameter ?(? 1) and use the ?-times value as
P (A
?
= 1|ent
j
, CF
l
, s
j
).
The first term of equation (11) represents how
likely an entity that contains n
j
m
as a content part
is considered to be an antecedent, the second term
represents how likely an entity that contains c
j
n
as
a surface case is considered to be an antecedent,
and the third term gives the probability that an
entity that appears in location class l
j
n
is an an-
tecedent.
The probabilities P (n
j
m
) and P (c
j
n
) are esti-
mated from a large raw corpus. The probabili-
ties P (c
j
n
|case type of(s
j
)) and P (A
?
= 1|l
j
n
,
case type of(s
j
)) are estimated from the web
corpus in which the relationship between an an-
tecedent of a zero pronoun and a case slot, and the
relationship between its surface case marker and a
case slot are annotated by hand. Then, let us con-
sider the probability P (n
j
m
|CF
l
, s
j
, A
?
(s
j
) = 1)
in the next subsection.
3.2.3 Probability of Component Part of Zero
Pronoun
P (n
j
m
|CF
l
, s
j
, A
?
=1) is similar to P (n
j
|CF
l
,
s
j
, A=1) and can be estimated approximately from
case frames using the frequencies of case slot ex-
amples. However, while A
?
(s
j
) = 1 means s
j
is
not filled with input case component but filled with
an entity as the result of zero anaphora resolution,
case frames are constructed by extracting only the
input case component. Therefore, the content part
of a zero anaphora antecedent n
j
m
is often not in-
cluded in the case slot examples. To cope with this
problem, we utilize generalized examples.
When one mention of an entity is tagged any
category or recognized as an NE, we also use the
category or the NE class as the content part of the
entity. For examples, if an entity {Prius} is recog-
nized as an artifact name and assigned to wo case
of the case frame hanbai(1) in Table 1, the system
also calculates:
P (NE :ARTIFACT |hanbai(1),wo, A
?
(wo)=1)
P (NE :ARTIFACT )
besides:
P (Prius|hanbai(1),wo, A
?
(wo) = 1)
P (Prius)
and uses the higher value.
3.3 Salience Score
Previous works reported the usefulness of salience
for anaphora resolution (Lappin and Leass, 1994;
Mitkov et al, 2002). In order to consider salience
of an entity, we introduce salience score, which is
calculated by the following set of simple rules:
? +2 : mentioned with topical marker ?wa?.
? +1 : mentioned without topical marker ?wa?.
? +0.5 : assigned to a zero pronoun.
? ?0.7 : beginning of each sentence.
For examples, we consider the salience score of
the entity {Toyota} in (i) in Section 3.1. In the
first sentence, since {Toyota} is mentioned with
topical marker ?wa?, the salience score is 2. At the
beginning of the second sentence it becomes 1.4,
774
Table 4: Data for Parameter Estimation.
probability data
P (n
j
) raw corpus
P (c
j
) raw corpus
P (c
j
|case type of(s
j
), A(s
j
)=1) tagged corpus
P (c
j
|case type of(s
j
), A
?
(s
j
)=1) tagged corpus
P (n
j
|CF
l
, s
j
, A(s
j
)=1) case frames
P (n
j
|CF
l
, s
j
, A
?
(s
j
)=1) case frames
P (CF
l
|v
i
) case structure analysis
P (A(s
j
)={0, 1} |CF
l
, s
j
) case structure analysis
P (A
?
(s
j
)=0|case type of(s
j
)) tagged corpus
P (A
?
(s
j
)=1|l
j
, case type of(s
j
)) tagged corpus
Table 5: Experimental Results.
R P F
Kawahara & Kurohashi .230 (28/122) .173 (28/162) .197
Proposed (? = 1) .426 (52/122) .271 (52/192) .331
(? = 1/2) .410 (50/122) .373 (50/134) .391
(? = 1/4) .295 (36/122) .419 (36/86) .346
and after assigned to the zero pronoun of ?hanbai?
it becomes 1.9. Note that we use the salience score
not as a probabilistic clue but as a filter to consider
the target entity as a possible antecedent. When we
use the salience score, we only consider the entities
that have the salience score no less than 1.
4 Experiments
4.1 Setting
We created an anaphoric relation-tagged corpus
consisting of 186 web documents (979 sentences).
We selected 20 documents for test and used the
other 166 documents for calculating several proba-
bilities. Since the anaphoric relations in some web
documents were not so clear and too difficult to
recognize, we did not select such documents for
test. In the 20 test documents, 122 zero anaphora
relations were tagged between one of the mentions
of the antecedent and the target predicate that had
the zero pronoun.
Each parameter for proposed model was esti-
mated using maximum likelihood from the data
described in Table 4. The case frames were auto-
matically constructed from web corpus comprising
1.6 billion sentences. The case structure analysis
was conducted on 80 million sentences in the web
corpus, and P (n
j
) and P (c
j
)were calculated from
the same 80 million sentences.
In order to concentrate on zero anaphora resolu-
tion, we used the correct morphemes, named enti-
ties, syntactic structures and coreferential relations
that were annotated by hand. Since correct corefer-
ential relations were given, the number of created
entities was same between the gold standard and
the system output because zero anaphora resolu-
tion did not create new entities.
4.2 Experimental Results
We conducted experiments of zero anaphora reso-
lution. As the parameter ? introduced in Section
3.2.2., we tested 3 values 1, 1/2, and 1/4. For
comparison, we also tested Kawahara and Kuro-
hashi?s (2004) model. The experimental results are
shown in Table 5, in which recall R, precision P
and F-measure F were calculated by:
R =
# of correctly recognized zero anaphora
# of zero anaphora tagged in corpus
,
P =
# of correctly recognized zero anaphora
# of system outputted zero anaphora
,
F =
2
1/R + 1/P
.
Kawahara and Kurohashi?s model achieved al-
most 50% as F-measure against newspaper arti-
cles. However, as a result of our experiment
against web documents, it achieved only about
20% as F-measure. This may be because anaphoric
relations in web documents were not so clear as
those in newspaper articles and more difficult to
recognize. As to the parameter ?, the larger ?
tended to output more zero anaphora, and the high-
est F-measure was achieved against ? = 1/2.
When using ? = 1/2, there were 72 (=122?50)
zero pronouns that were tagged in the corpus and
not resolved correctly. Only 12 of them were cor-
rectly detected and assigned to a wrong entity, that
is, 60 of them were not even detected. Therefore,
we can say our recall errors were mainly caused by
the low recall of zero pronoun detection.
In order to confirm the effectiveness of gener-
alized examples of case slots and salience score,
we also conducted experiments under several con-
ditions. We set ? = 1/2 in these experiments. The
results are shown in Table 6, in which CT means
generalized categories, NE means generalized NEs
and SS means salience score.
Without using any generalized examples, the F-
measure is less than Kawahara and Kurohashi?s
method, which use similarity to deal with sparse-
ness of case slot examples, and we can con-
firm the effectiveness of the generalized examples.
While generalized categories much improved the
F-measure, generalized NEs contribute little. This
may be because the NE rate is smaller than com-
mon noun rate, and so the effect is limited.
We also confirmed that the salience score filter
improved F-measure. Moreover, by using salience
score filter, the zero anaphora resolution becomes
about ten times faster. This is because the system
775
Table 6: Experiments under Several Conditions.
CT NE SS R P F
?
.131 (16/122) .205 (16/78) .160
? ?
.164 (20/122) .247 (20/81) .197
? ?
.402 (49/122) .368 (49/133) .384
? ?
.385 (47/122) .196 (47/240) .260
? ? ?
.410 (50/122) .373 (50/134) .391
can avoid checking entities with low salience as
antecedent candidates.
4.3 Comparison with Previous Works
We compare our accuracies with (Seki et al,
2002). They achieved 48.9% in precision, 88.2%
in recall, and 62.9% in F-measure for zero pro-
noun detection, and 54.0% accuracy for antecedent
estimation on 30 newspaper articles, that is, they
achieved about 34% in F-measure for whole zero
pronoun resolution. It is difficult to directly com-
pare their results with ours due to the difference
of the corpus, but our method achieved 39% in
F-measure and we can confirm that our model
achieves reasonable performance considering the
task difficulty.
5 Conclusion
In this paper, we proposed a probabilistic model
for Japanese zero anaphora resolution. By us-
ing automatically constructed wide-coverage case
frames that include generalized examples and in-
troducing salience score filter, our model achieves
reasonable performance against web corpus. As
future work, we plan to conduct large-scale ex-
periments and integrate this model to a fully-
lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis (Kawahara and
Kurohashi, 2006).
References
Iida, Ryu, Kentaro Inui, and Yuji Matsumoto. 2006.
Exploiting syntactic patterns as clues in zero-
anaphora resolution. In Proceedings of COL-
ING/ACL 2006, pages 625?632.
IREX Committee, editor. 1999. Proc. of the IREX
Workshop.
Isozaki, Hideki and Tsutomu Hirao. 2003. Japanese
zero pronoun resolution based on ranking rules and
machine learning. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 184?191.
Kawahara, Daisuke and Sadao Kurohashi. 2002.
Fertilization of Case Frame Dictionary for Robust
Japanese Case Analysis. In Proceedings of the 19th
International Conference on Computational Linguis-
tics, pages 425?431.
Kawahara, Daisuke and Sadao Kurohashi. 2004.
Zero pronoun resolution based on automatically con-
structed case frames and structural preference of an-
tecedents. In Proceedings of the 1st International
Joint Conference on Natural Language Processing
(IJCNLP-04), pages 334?341.
Kawahara, Daisuke and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, Main Conference, pages 176?183.
Lappin, Shalom and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?562.
Luo, Xiaoqiang. 2007. Coreference or not: A
twin model for coreference resolution. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 73?80.
Mitkov, Ruslan, Richard Evans, and Constantin Or?asan.
2002. A new, fully automatic version of mitkov?s
knowledge-poor pronoun resolution method. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLing-2002).
Ng, Vincent. 2005. Machine learning for coreference
resolution: From local classification to global rank-
ing. In Proceedings of the 43rd Annual Meeting
of the Asssociation for Computational Linguistics,
pages 157?164.
Sasano, Ryohei and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural lan-
guage processing. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP-08), pages 607?612.
Seki, Kazuhiro, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing Japanese
anaphora integrating zero pronoun detection and res-
olution. In Proceedings of the 19th International
Conference on Computational Linguistics, pages
911?917.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
The National Language Institute for Japanese Lan-
guage. 2004. Bunruigoihyo. Dainippon Tosho, (In
Japanese).
Yang, Xiaofeng, Jian Su, Jun Lang, Ghew Lim Tan,
Ting Liu, and Sheng Li. 2008. An entity-mention
model for coreference resolution with inductive logic
programming. In Proceedings of ACL-08: HLT,
pages 843?851.
776
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1049?1056
Manchester, August 2008
Chinese Dependency Parsing with Large Scale Automatically 
Constructed Case Structures 
Kun Yu 
Graduate School of Infor-
matics, 
Kyoto University, Japan 
kunyu@nlp.kuee.kyo
to-u.ac.jp 
Daisuke Kawahara 
National Institute of Informa-
tion and Communications 
Technology, Japan 
dk@nict.go.jp 
Sadao Kurohashi 
Graduate School of Infor-
matics, 
Kyoto University, Japan 
kuro@i.kyoto-
u.ac.jp 
 
Abstract 
This paper proposes an approach using 
large scale case structures, which are 
automatically constructed from both a 
small tagged corpus and a large raw cor-
pus, to improve Chinese dependency 
parsing. The case structure proposed in 
this paper has two characteristics: (1) it 
relaxes the predicate of a case structure to 
be all types of words which behaves as a 
head; (2) it is not categorized by semantic 
roles but marked by the neighboring 
modifiers attached to a head. Experimen-
tal results based on Penn Chinese Tree-
bank show the proposed approach 
achieved 87.26% on unlabeled attach-
ment score, which significantly outper-
formed the baseline parser without using 
case structures. 
1 Introduction 
Case structures (i.e. predicate-argument struc-
tures) represent what arguments can be attached 
to a predicate, which are very useful to recognize 
the meaning of natural language text. Research-
ers have applied case structures to Japanese syn-
tactic analysis and improved parsing accuracy 
successfully (Kawahara and Kurohashi, 2006(a); 
Abekawa and Okumura, 2006). However, few 
works focused on using case structures in Chi-
nese parsing. Wu (2003) proposed an approach 
to learn the relations between verbs and nouns 
and applied these relations to a Chinese parser. 
Han et al (2004) presented a method to acquire 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
the sub-categorization of Chinese verbs and used 
them in a PCFG parser. 
Normally, case structures are categorized by 
semantic roles for verbs. For example, Kawahara 
and Kurohashi (2006(b)) constructed Japanese 
case structures which were marked by post posi-
tions. Wu (2003) classified the Chinese verb-
noun relations as ?verb-object? and ?modifier-
head?. In this paper, we propose a new type of 
Chinese case structure, which is different from 
those presented in previous work (Wu, 2003; 
Han et al, 2004; Kawahara and Kurohashi, 
2006(a); Abekawa and Okumura, 2006) in two 
aspects:  
(1) It relaxes the predicate of a case structure 
to be all types of words which behaves as a head; 
(2) It is not categorized by semantic roles 
but marked by the neighboring modifiers at-
tached to a head. The sibling modification infor-
mation remembers the parsing history of a head 
node, which is useful to correct the parsing error 
such as a verb ? (see) is modified by two nouns 
?? (film) and ?? (introduction) as objects 
(see Figure 1). 
 
Figure 1. Dependency trees of an example sen-
tence (I see the introduction of a film). 
We automatically construct large scale case 
structures from both a small tagged corpus and a 
large raw corpus. Then, we apply the large scale 
case structures to a Chinese dependency parser to 
improve parsing accuracy.  
The Chinese dependency parser using case 
structures is evaluated by Penn Chinese Tree-
bank 5.1 (Xue et al, 2002). Results show that the 
1049
automatically constructed case structures helped 
increase parsing accuracy by 2.13% significantly.  
The rest of this paper is organized as follows: 
Section 2 describes the proposed Chinese case 
structure and the construction method in detail; 
Section 3 describes a Chinese dependency parser 
using constructed case structures; Section 4 lists 
the experimental results with a discussion in sec-
tion 5; Related work is introduced in Section 6; 
Finally, Section 7 gives a brief conclusion and 
the direction of future work. 
2 Chinese Case Structure and its Con-
struction 
2.1 A New Type of Chinese Case Structure 
We propose a new type of Chinese case structure 
in this paper, which is represented as the combi-
nation of a case pattern and a case element (see 
Figure 2). Case element remembers the bi-lexical 
dependency relation between all types of head-
modifier pairs, which is also recognized in previ-
ous work (Wu, 2003; Han et al, 2004; Kawahara 
and Kurohashi, 2006(a); Abekawa and Okumura, 
2006). Case pattern keeps the pos-tag sequence 
of all the modifiers attached to a head to remem-
ber the parsing history of a head node.  
 
Figure 2. An example of constructed case 
structure. 
2.2 Construction Corpus 
We use 9,684 sentences from Penn Chinese 
Treebank 5.1 as the tagged corpus, and 7,338,028 
sentences written in simplified Chinese from 
Chinese Gigaword (Graff et al, 2005) as the raw 
corpus for Chinese case structure construction.  
Before constructing case structures from the 
raw corpus, we need to get the syntactic analysis 
of it. First, we do word segmentation and pos-
tagging for the sentences in Chinese Gigaword 
by a Chinese morphological  analyzer 
(Nakagawa and Uchimoto, 2007). Then a 
Chinese deterministic syntactic analyzer (Yu et 
al., 2007) is used to parse the whole corpus. 
To guarantee the accuracy of constructed case 
structures, we only use the sentences with less 
than k words from Chinese Gigaword. It is based 
on the assumption that parsing short sentences is 
more accurate than parsing long sentences. The 
performance of the deterministic parser used for 
analyzing Chinese Gigaword (see Figure 3) 
shows smaller k ensures better parsing quality 
but suffers from lower sentence coverage. 
Referring to Figure 2, we set k as 30. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Performance of the deterministic parser 
with different k on 1,800 sentences2. 
2.3 Case Pattern Construction 
A case pattern consists of a sequence of pos-tags 
indicating the order of all the modifiers attached 
to a head (see Figure 1), which can be repre-
sented as following.  
>=< ?? rnnlmmi posposposposposposcp ],,...,[,],...,,[ 1111
Here, 
lmm pospospos ],...,,[ 11? means the pos-tag 
sequence of the modifiers attached to a head 
from the left side, and 
rnn pospospos ],,...,[ 11 ? means 
the pos-tag sequence of the modifiers attached to 
a head from the right side. 
We use the 33 pos-tags defined in Penn Chi-
nese Treebank (Xue et al, 2002) to describe a 
case pattern, and make following modifications: 
? group common noun, proper noun and 
pronoun together and mark them as ?noun?; 
? group predicative adjective and all the 
other verbs together and mark them as ?verb?; 
? only regard comma, pause, colon and 
semi-colon as punctuations and mark them as 
?punc?, and neglect other punctuations. 
                                                 
2 UAS means unlabeled attachment score (Buchholz and 
Marsi, 2006). The sentences used for this evaluation are 
from Penn Chinese Treebank with gold word segmentation 
and pos-tag. 
0 10 20 30 40 50 60 70 80 90 100 110
20
30
40
50
60
70
80
90
100
%
k
 UAS
 Sentence Coverage
1050
? group cardinal number and ordinal number 
together and mark them as ?num?; 
? keep the original definition for other pos-
tags but label them by new tags, such as labeling 
?P? as ?prep? and labeling ?AD? as ?adv?. 
The task of case pattern construction is to ex-
tract cpi for each head from both the tagged cor-
pus and the raw corpus. As we will introduce 
later, the Chinese dependency parser using case 
structures applies CKY algorithm for decoding. 
Thus the following substrings of cpi are also ex-
tracted for each head as horizontal Markoviza-
tion during case pattern construction. 
],1[],,1[                                                
],,...,[,],...,,[ 1111
njmk
pospospospospospos rjjlkk
???
>< ??  
],1[  ,],...,,[ 11 mkpospospos lkk ??>< ?  
],1[  ,],,...,[ 11 njpospospos rjj ??>< ?  
2.4 Case Element Construction 
As introduced in Section 2.1, a case element 
keeps the lexical preference between a head and 
its modifier. Therefore, the task of case element 
construction is to extract head-modifier pairs 
from both the tagged corpus and the raw corpus.  
Although only the sentences with less than k 
(k=30) words from Chinese Gigaword are used 
as raw corpus to guarantee the accuracy, there 
still exist some dependency relations with low 
accuracy in these short sentences because of the 
non-perfect parsing quality. Therefore, we apply 
a head-modifier (HM) classifier to the parsed 
sentences from Chinese Gigaword to further ex-
tract head-modifier pairs with high quality. This 
HM classifier is based on SVM classification. 
Table 1 lists the features used in this classifier.  
Feature Description 
Poshead/ 
Posmod 
Pos-tag pair of head and modifier 
Distance Distance between head and modifier 
HasComma If there exists comma between head and  modifier, set as 1; otherwise as 0 
HasColon If there exists colon between head and modifier, set as 1; otherwise as 0 
HasSemi If there exists semi-colon between head and modifier, set as 1; otherwise as 0 
Table 1. Features for HM classifier. 
The HM classifier is trained on 3500 sentences 
from Penn Chinese Treebank with gold-standard 
word segmentation and pos-tag. All the sentences 
are parsed by the same Chinese deterministic 
parser used for Chinese Gigaword analysis. The 
correct dependency relations created by the 
parser are looked as positive examples and the 
left dependency relations are used as negative 
examples. TinySVM 3  is selected as the SVM 
toolkit. A polynomial kernel is used and degree 
is set as 2. Tested on 346 sentences, which are 
from Penn Chinese Treebank and parsed by the 
same deterministic parser with gold standard 
word segmentation and pos-tag, this HM classi-
fier achieved 96.77% on precision and 46.35% 
on recall. 
3 A Chinese Dependency Parser Using 
Case Structures 
3.1 Parsing Model 
We develop a lexicalized Chinese dependency 
parser to use constructed case structures. This 
parser gives a probability P(T|S) to each possible 
dependency tree T of an input sentence 
S=w1,w2,?,wn (wi is a node representing a word 
with its pos-tag), and outputs the dependency 
tree T* that maximizes P(T|S) (see equation 1). 
CKY algorithm is used to decode the dependency 
tree from bottom to up. 
)|(maxarg STPT
T
=?  (1)
To use case structures, P(T|S) is divided into 
two parts (see equation 2): the probability of a 
sentence S generating a root node wROOT, and the 
product of the probabilities of a node wi generat-
ing a case structure CSi. 
?
=
?=
m
i
iiROOT wCSPSwPSTP
1
)|()|()|(  (2)
As introduced in Section 2, a case structure 
CSi is composed of a case pattern cpi and a case 
element cmi. Thus 
),|()|(                  
)|,()|(
iiiii
iiiii
cpwcmPwcpP
wcmcpPwCSP
?=
=  
(3)
A case element cmi consists of a set of de-
pendencies {Dj}, in which each Dj is a tuple <wj, 
disj, commaj>. Here wj means a modifier node, 
disj means the distance between wj and its head, 
and commaj means the number of commas be-
tween wj and its head. Assuming any Dj and Dk 
are independent of each other when they belong 
to the same case element, P(cmi|wi,cpi) can be 
written as  
),|,,(                        
),|(),|(
iij
j
jj
ii
j
jiii
cpwcommadiswP
cpwDPcpwcmP
?
?
=
=
(4)
                                                 
3 http://chasen.org/~taku/software/TinySVM/ 
1051
Finally, P(wj,disj,commaj | wi,cpi) is divided as 
),,|,(),|(
),|,,(
ijijjiij
iijjj
cpwwcommadisPcpwwP
cpwcommadiswP
?=  (5) 
Maximum likelihood estimation is used to es-
timate P(wROOT|S) on training data set with the 
smoothing method used in (Collins, 1996). The 
estimation of P(cpi|wi), P(wj|wi,cpi), and P(disj, 
commaj |wi,wj,cpi) will be introduced in the fol-
lowing subsections. 
3.2 Estimating P(cpi|wi) by Case Patterns 
Three steps are used to estimate P(cpi|wi) by 
maximum likelihood estimation using the con-
structed case patterns: 
? Estimate P(cpi|wi) only by the case pat-
terns from the tagged corpus and represent it as 
)|(? iitagged wcpP ; 
? Estimate P(cpi|wi) only by the case pat-
terns from the raw corpus and represent it as 
)|(? iiraw wcpP ; 
? Estimate P(cpi|wi) by equation 6, in which 
?pattern is calculated by equation 7 to set proper 
ratio for the probabilities estimated by the case 
patterns from different corpora.  
)|(?)1()|(?
)|(?
iirawpatterniitaggedpattern
ii
wcpPwcpP
wcpP
??+?= ?? (6)
1++++
+++=
rawrawtaggedtagged
rawrawtaggedtagged
pattern ????
?????  
(7)
In equation 7, ?tagged and ?raw mean the occur-
rence of a lexicalized node wi=<lexi, posi> gener-
ating cpi in the tagged or raw corpus, ?tagged and 
?raw mean the occurrence of a back-off node 
wi=<posi> generating cpi in the tagged or raw 
corpus. To overcome the data sparseness prob-
lem, we not only apply the smoothing method 
used in (Collins, 1996) for a lexicalized head to 
back off it to its part-of-speech, but also assign a 
very small value to P(cpi|wi) when there is no cpi 
modifying wi in the constructed case patterns. 
3.3 Estimating P(wj|wi,cpi) and P(disj, com-
maj |wi,wj,cpi) by Case Elements 
To estimate P(wj|wi,cpi) and P(disj, commaj 
|wi,wj,cpi) by maximum likelihood estimation, we 
also use three steps: 
? Estimate the two probabilities only by the 
case elements from the tagged corpus and repre-
sent them as ),|(? iijtagged cpwwP  and 
),,|,(? ijijjtagged cpwwcommadisP ; 
? Estimate the two probabilities only by the 
case elements from the raw corpus, and represent 
them as ),|(? iijraw cpwwP  and 
),,|,(? ijijjraw cpwwcommadisP ; 
? Estimate P(wj|wi,cpi) and P(disj, commaj 
|wi,wj,cpi) by equation 8 and equation 9. 
),|(?)1(   
 ),|(?
),|(?
iijrawelement
iijtaggedelement
iij
cpwwP
cpwwP
cpwwP
??
+?=
?
?  
(8)
),,|,(?)1(   
),,|,(?
),,|,(?
ijijjrawelement
ijijjtaggedelement
ijijj
cpwwcommadisP
cpwwcommadisP
cpwwcommadisP
??
+?=
?
?  
(9)
The smoothing method used in (Collins, 1996) 
is applied during estimation.  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Parsing accuracy with different ?element 
on development data set. 
In order to set proper ratio for the probabilities 
estimated by the case elements from different 
corpora, we use a parameter ?element in equation 8 
and 9. The appropriate setting (?element =0.4) is 
learned by a development data set (see Figure 4). 
4 Evaluation Results 
4.1 Experimental Setting 
We use Penn Chinese Treebank 5.1 as data set to 
evaluate the proposed approach. 9,684 sentences 
from Section 001-270 and 400-931, which are 
also used for constructing case structures, are 
used as training data. 346 sentences from Section 
271-300 are used as testing data. 334 sentences 
from Section 301-325 are used as development 
data. Penn2Malt4 is used to transfer the phrase 
structure of Penn Chinese Treebank to depend-
ency structure. Gold-standard word segmentation 
and pos-tag are applied in all the experiments. 
                                                 
4 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
86.0
86.5
87.0
87.5
88.0
88.5
 
 
U
A
S 
(%
)
?
element
1052
Unlabeled attachment score (UAS) (Buchholz 
and Marsi, 2006) is used as evaluation metric. 
Because of the difficulty of assigning correct 
head to Chinese punctuation, we calculate UAS 
only on the dependency relations in which the 
modifier is not punctuation. 
4.2 Results 
Three parsers were evaluated in this experiment: 
? ?baseline?: a parser not using case struc-
tures, where P(T|S) is calculated by equation 11 
and P(wj|wi) and P(disj, commaj |wi,wj) are esti-
mated by training data set only.  
?
?
??
??
??=
?=
jinji
jijjijROOT
jinji
ijjjROOT
wwcommadisPwwPSwP
wcommadiswPSwP
STP
],,1[,
],,1[,
)),|,()|(()|(
)|,,()|(
)|(
(11) 
? ?w/ case elem?: a parser only using case 
element, which also calculates P(T|S) by equa-
tion 11 but estimates P(wj|wi) and P(disj, commaj 
|wi,wj) by constructed case elements.  
? ?proposed?: the parser introduced in Sec-
tion 3, which uses both case elements and case 
patterns. 
The evaluation results on testing data set (see 
Table 2) shows the proposed parser achieved 
87.26% on UAS, which was 2.13% higher than 
that of the baseline parser. This improvement is 
regarded as statistically significant (McNemar?s 
test: p<0.0005). Besides, Table 2 shows only us-
ing case elements increased parsing accuracy by 
1.30%. It means both case elements and case pat-
terns gave help to parsing accuracy, and case 
elements contributed more in the proposed ap-
proach. 
Parsing 
model baseline w/ case elem proposed 
UAS (%) 85.13 86.43 (+1.30) 87.26 (+2.13)
Table 2. Parsing accuracy of different parsing 
models. 
Figure 5 and Figure 6 show the dependency 
trees of two example sentences created by both 
the baseline parser and the proposed parser. In 
Figure 5, the baseline parser incorrectly assigned 
??/NN (signing) as the head of ??/NN (co-
operation). However, after using the case ele-
ment ??/NN (project) ? ??/NN, the cor-
rect head of ??/NN was found by the pro-
posed parser. Figure 6 shows the baseline parser 
recognized ??/VV (opening) as the head of ?
/P (as) incorrectly. But in the proposed parser, 
the probability of ??/VV generating the case 
pattern ?[prep, punc, prep]l? was much lower 
than that of ??/VV  generating the case pattern 
?[prep]l?. Therefore, the proposed parser rejected 
the incorrect dependency that?/P modified?
?/VV  and got the correct head of ?/P as ??
/VV (show) successfully. 
5 Discussion 
5.1 Influence of the Number of Case Struc-
tures on Parsing Accuracy 
During case structure construction, we only used 
the sentences with less than k (k=30) words from 
Chinese Gigaword as the raw corpus. Enlarging k 
will introduce more sentences from Chinese Gi-
gaword and increase the number of case struc-
tures. Table 4 lists the number of case structures 
and parsing accuracy of the proposed parser on 
testing data set with different k5. It shows enlarg-
ing the number of case structures is a possible 
way to increase parsing accuracy. But simply 
setting larger k did not help parsing, because it 
decreased the parsing accuracy of Chinese Gi-
gaword and consequently decreased the accuracy 
of constructed case structures. Using good parse 
selection (Reichart and Rappoport, 2007; Yates 
et al, 2006) on the syntactic analysis of Chinese 
Gigaword is a probable way to construct more 
case structures without decreasing their accuracy. 
We will consider about it in the future. 
k 10 20 30 40 
# of Case Ele-
ment (M) 0.66 1.14 1.81 2.75
# of Case Pat-
tern (M) 0.57 1.55 3.91 8.48
UAS (%) 85.16 86.42 87.26 87.07
Table 4. Case structure number and parsing 
accuracy with different k. 
5.2 Influence of the Case Structure Con-
struction Corpus on Parsing Accuracy 
We also evaluated the proposed parser on testing 
data set using case structures constructed from 
different corpora.  
Results (see Table 5) show that parsing accu-
racy was improved greatly only when using case 
structures constructed from both the two corpora. 
The case structures constructed from either of a 
                                                 
5 Considering about the time expense of case structure con-
struction, we only did test for k ?40. 
1053
single corpus only gave a little help to parsing. It 
is because among all the case structures used 
during testing (see Table 6), 19.57% case ele-
ments were constructed from the tagged corpus 
only and 54.18% case patterns were constructed 
from the raw corpus only. The incorrect head-
modifier pairs extracted from Chinese Gigaword 
is a possible reason for the fact that some case 
elements only existing in the tagged corpus. En-
hancing good parse selection on Chinese Giga-
word could improve the quality of extracted 
head-modifier pairs and solve this problem. In 
addition, the strict definition of case pattern is a 
probable reason that makes more than half of the 
case patterns only exist in the raw corpus and 
18.18% case patterns exist in neither of the two 
corpora. We will modify the representation of 
case pattern to make it more flexible to the num-
ber of modifiers to resolve this issue in the future. 
Corpus Tagged Raw Tagged+Raw
UAS (%) 85.25 85.90 87.26 
Table 5. Parsing accuracy with case structures 
constructed from different corpora. 
Corpus Tagged Raw Tagged+Raw None
% of case 
element  19.57 8.95 68.07 3.41
% of case 
pattern 0.03 54.18 27.61 18.18
Table 6. Ratio of case structures constructed 
from different corpora. 
 
Figure 5. Dependency trees of an example sentence (The signing of China-US cooperation high tech 
project ?). 
?/P ??/NN  ?/PU  ?/P  ???/NN  ??/VV ?/DEC  ???/NN  ?/AD  ??/VV  ... 
(a) dependency tree created by the baseline parser
(b) dependency tree created by the proposed parser
?/P ??/NN  ?/PU  ?/P  ???/NN  ??/VV  ?/DEC  ???/NN  ?/AD  ??/VV ... 
 
Figure 6. Dependency trees of an example sentence (As introduced, the exhibition opening in the 
stadium will show?). 
5.3 Parsing Performance with Real Pos-tag 
Gold standard word segmentation and pos-tag 
are applied in previous experiments. However, 
parsing accuracy will be affected by the incorrect 
word segmentation and pos-tag in the real appli-
cations. Currently, the best performance of Chi-
nese word segmentation has achieved 99.20% on 
F-score, but the best accuracy of Chinese pos-
tagging was 96.89% (Jin and Chen, 2008). 
Therefore, we think pos-tagging is more crucial 
for applying parser in real task compared with 
word segmentation. Considering about this, we 
evaluated the parsing models introduced in Sec-
tion 4 with real pos-tag in this experiment. 
 
 
 
Parsing model baseline proposed 
UAS (%) 80.91 82.90 (+1.99) 
Table 7. Parsing accuracy of different parsing 
models with real pos-tag. 
An HMM-based pos-tagger is used to get pos-
tag for testing sentences with gold word segmen-
tation. The pos-tagger was trained on the same 
training data set described in Section 4.1 and 
achieved 93.70% F-score on testing data set. Re-
sults (see Table 7) show that even if with real 
pos-tags, the proposed parser still outperformed 
the baseline parser significantly. However, the 
results in Table 7 indicate that incorrect pos-tag 
affected the parsing accuracy of the proposed 
parser greatly. Some researchers integrated pos-
1054
tagging into parsing and kept n-best pos-tags to 
reduce the effect of pos-tagging errors on parsing 
accuracy (Cao et al, 2007). We will also con-
sider about this in our future work. 
6 Related Work 
To our current knowledge, there were few works 
about using case structures in Chinese parsing, 
except for the work of Wu (2003) and Han et al 
(2004). Compared with them, our proposed ap-
proach presents a new type of case structures for 
all kinds of head-modifier pairs, which not only 
recognizes bi-lexical dependency but also re-
members the parsing history of a head node. 
Parsing history has been used to improve pars-
ing accuracy by many researchers (Yamada and 
Matsumoto, 2003; McDonald and Pereira, 2006). 
Yamada and Matsumoto (2003) showed that 
keeping a small amount of parsing history was 
useful to improve parsing performance in a shift-
reduce parser. McDonald and Pereira (2006) ex-
panded their first-order spanning tree model to be 
second-order by factoring the score of the tree 
into the sum of adjacent edge pair scores. In our 
proposed approach, the case patterns remember 
the neighboring modifiers for a head node like 
McDonald and Pereira?s work. But it keeps all 
the parsing histories of a head, which is different 
from only keeping adjacent two modifiers in 
(McDonald and Pereira, 2006). Besides, to use 
the parsing histories in CKY decoding, our ap-
proach applies horizontal Markovization during 
case pattern construction. In general, the success 
of using case patterns in Chinese parsing in his 
paper proves again that keeping parsing history is 
crucial to improve parsing performance, no mat-
ter in which way and to which parsing model it is 
applied. 
There were also some works that handled lexi-
cal preference for Chinese parsing in other ways. 
For example, Cheng et al (2006) and Hall et al 
(2007) applied shift-reduce deterministic parsing 
to Chinese. Sagae and Tsujii (2007) generalized 
the standard deterministic framework to prob-
abilistic parsing by using a best-first search strat-
egy. In these works, lexical preferences were 
introduced as features for predicting parsing ac-
tion. Besides, Bikel and Chiang (2000) applied 
two lexicalized parsing models developed for 
English to Penn Chinese Treebank. Wang et al 
(2005) proposed a completely lexicalized bot-
tom-up generative parsing model to parse Chi-
nese, in which a word-similarity-based smooth-
ing was introduced to replace part-of-speech 
smoothing. 
7 Conclusion and Future Work 
This paper proposes an approach to use large 
scale case structures, which are automatically 
constructed from both a small tagged corpus and 
the syntactic analysis of a large raw corpus, to 
improve Chinese dependency parsing. The pro-
posed case structures not only recognize the lexi-
cal preference between all types of head-modifier 
pairs, but also keep the parsing history of a head 
word. Experimental results show the proposed 
approach improved parsing accuracy signifi-
cantly. Besides, although we only apply the pro-
posed approach to Chinese dependency parsing 
currently, the same idea could be adapted to 
other languages easily because it doesn?t use any 
language specific knowledge. 
There are several future works under consid-
eration, such as modifying the representation of 
case patterns to make it more robust, enhancing 
good parse selection on the analysis of raw cor-
pus, and integrating pos-tagging into parsing 
model.  
References 
T.Abekawa and M.Okumura. 2006. Japanese De-
pendency Parsing Using Co-occurrence Informa-
tion and a Combination of Case Elements. In Pro-
ceedings of   the joint conference of the 
International Committee on Computational Lin-
guistics and the Association for Computational 
Linguistics 2006. pp. 833-840. 
D.Bikel. 2004. Intricacies of Collins? Parsing Model. 
Computational Linguistics, 30(4): 479-511. 
D.Bikel and D.Chiang. 2000. Two Statistical Parsing 
Models Applied to the Chinese Treebank. In Pro-
ceedings of the 2nd Chinese Language Processing 
Workshop. pp. 1-6. 
S.Buchholz and E.Marsi. 2006. CoNLL-X Shared 
Task on Multilingual Dependency Parsing. In Pro-
ceedings of the 10th Conference on Computational 
Natural Language Learning.  
H.Cao et al. 2007. Empirical Study on Parsing Chi-
nese Based on Collins? Model. In Proceedings of 
the 10th Conference of the Pacific Association for 
Computational Linguisitcs. pp. 113-119. 
Y.Cheng, M.Asahara and Y.Matsumoto. 2006. Multi-
lingual Dependency Parsing at NAIST. In Proceed-
ings of the 10th Conference on Computational 
Natural Language Learning. pp. 191-195. 
1055
M.Collins. 1996. A New Statistical Parser Based on 
Bigram Lexical Dependencies. In Proceedings of 
the 34th Annual Meeting of the Association for 
Computational Linguistics. pp. 184-191. 
M.Collins. 1999. Head-Driven Statistical Models for 
Natural Language Parsing. Ph.D Thesis. University 
of Pennsylvania. 
D.Graff et al. 2005. Chinese Gigaword Second Edi-
tion. Linguistic Data Consortium, Philadelphia. 
J.Hall et al 2007. Single Malt or Blended? A Study in 
Multilingual Parser Optimization. In Proceedings 
of the shared task at the Conference on Computa-
tional Natural Language Learning 2007. pp. 933-
939. 
X.Han et al. 2004. Subcategorization Acquisition and 
Evaluation for Chinese Verbs. In Proceedings of 
the 20th International Conference on Computa-
tional Linguistics.  
G.Jin and X.Chen. 2008. The Fourth International 
Chinese Language Processing Bakeoff: Chinese 
Word Segmentation, Named Entity Recognition 
and Chinese Pos Tagging. In Proceedings of the 6th 
SIGHAN Workshop on Chinese Language Process-
ing. 
D.Kawahara and S.Kurohashi. 2006 (a). A Fully-
lexicalized Probabilistic Model for Japanese Syn-
tactic and Case frame Analysis. In Proceedings of 
the Human Language Technology conference - 
North American chapter of the Association for 
Computational Linguistics annual meeting 2006. 
pp. 176-183. 
D.Kawahara and S.Kurohashi. 2006 (b). Case Frame 
Compilation from the Web Using High-
performance Computing. In Proceedings of the 5th 
International Conference on Language Resources 
and Evaluation. 
T.Kudo and Y.Matsumoto. 2002. Japanese Depend-
ency Analysis Using Cascaded Chunking. In Pro-
ceedings of the Conference on Natural Language 
Learning. pp. 29-35. 
R.McDonald and F.Pereira. 2006. Online Learning of 
Approximate Dependency Parsing Algorithm. In 
Proceedings of the 11th Conference of the Euro-
pean Chapter of the Association for Computational 
Linguistics. 
R.McDonald and J.Nivre. 2007. Characterizing the 
Errors of Data-driven Dependency Parsing Models. 
In Proceedings of the Conference on Empirical 
Methods in Natural Language Processing Confer-
ence on Computational Natural Language Learn-
ing 2007. 
T.Nakagawa and K.Uchimoto. 2007. A Hybrid Ap-
proach to Word Segmentation and POS Tagging. 
In Proceedings of the 45th Annual Meeting of the 
Association for Computational Linguistics. pp. 
217-220. 
R.Reichart and A.Rappoport. 2007. An Ensemble 
Method for Selection of High Quality Parses. In 
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics. pp. 408-
415. 
K.Sagae and J.Tsujii. 2007. Dependency Parsing and 
Domain Adaptation with LR Models and Parser 
Ensembles. In Proceedings of the shared task at 
the Conference on Computational Natural Lan-
guage Learning 2007. pp. 1044-1050. 
Q.Wang, D.Schuurmans, and D.Lin. 2005. Strictly 
Lexical Dependency Parsing. In Proceedings of the 
9th International Workshop on Parsing Technolo-
gies. pp. 152-159.  
A.Wu. 2003. Learning Verb-Noun Relations to Im-
prove Parsing. In Proceedings of the 2nd SIGHAN 
Workshop on Chinese Language Processing. pp. 
119-124. 
N.Xue, F.Chiou and M.Palmer. 2002. Building a 
Large-Scale Annotated Chinese Corpus. In Pro-
ceedings of the 18th International Conference on 
Computational Linguistics. 
N.Xue and M.Palmer. 2003. Annotating the Proposi-
tions in the Penn Chinese Treebank. In Proceed-
ings of the 2nd SIGHAN Workshop on Chinese Lan-
guage Processing. 
N.Xue and M.Palmer. 2005. Automatic Semantic 
Rule Labeling for Chinese Verbs. In Proceedings 
of the 19th International Joint Conference on Artifi-
cial Intelligence.  
H.Yamada and Y.Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. 
In Proceedings of the 7th International Workshop 
on Parsing Technologies. 
A.Yates, S.Schoenmackers, and O.Etzioni. 2006. De-
tecting Parser Errors Using Web-based Semantic 
Filters. In Proceedings of the 2006 Conference on 
Empirical Methods in Natural Language Process-
ing. pp. 27-34. 
J.You and K.Chen. 2004. Automatic Semantic Role 
Assignment for a Tree Structure. In Proceedings of 
the 3rd SIGHAN Workshop on Chinese Language 
Processing. 
K.Yu, S.Kurohashi, and H.Liu. 2007. A Three-step 
Deterministic Parser for Chinese Dependency Pars-
ing. In Proceedings of the Human Language Tech-
nologies: the Annual Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics 2007. pp. 201-204. 
 
1056
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 306?314, Prague, June 2007. c?2007 Association for Computational Linguistics
Probabilistic Coordination Disambiguation
in a Fully-lexicalized Japanese Parser
Daisuke Kawahara
National Institute of Information and
Communications Technology,
3-5 Hikaridai Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University,
Yoshida-Honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
This paper describes a probabilistic model
for coordination disambiguation integrated
into syntactic and case structure analy-
sis. Our model probabilistically assesses
the parallelism of a candidate coordinate
structure using syntactic/semantic similari-
ties and cooccurrence statistics. We inte-
grate these probabilities into the framework
of fully-lexicalized parsing based on large-
scale case frames. This approach simulta-
neously addresses two tasks of coordination
disambiguation: the detection of coordinate
conjunctions and the scope disambiguation
of coordinate structures. Experimental re-
sults on web sentences indicate the effective-
ness of our approach.
1 Introduction
Coordinate structures are a potential source of syn-
tactic ambiguity in natural language. Since their in-
terpretation directly affects the meaning of the text,
their disambiguation is important for natural lan-
guage understanding.
Coordination disambiguation consists of the fol-
lowing two tasks:
? the detection of coordinate conjunctions,
? and finding the scope of coordinate structures.
In English, for example, coordinate structures are
triggered by coordinate conjunctions, such as and
and or. In a coordinate structure that consists of
more than two conjuncts, commas, which have var-
ious usages, also function like coordinate conjunc-
tions. Recognizing true coordinate conjunctions
from such possible coordinate conjunctions is a task
of coordination disambiguation (Kurohashi, 1995).
The other is the task of identifying the range of co-
ordinate phrases or clauses.
Previous work on coordination disambiguation
has focused on the task of addressing the scope am-
biguity (e.g., (Agarwal and Boggess, 1992; Gold-
berg, 1999; Resnik, 1999; Chantree et al, 2005)).
Kurohashi and Nagao proposed a similarity-based
method to resolve both of the two tasks for Japanese
(Kurohashi and Nagao, 1994). Their method, how-
ever, heuristically detects coordinate conjunctions
by considering only similarities between possible
conjuncts, and thus cannot disambiguate the follow-
ing cases1:
(1) a. kanojo-to
she-cmi
gakkou-ni
school-acc
itta
went
(? went to school with her)
b. kanojo-to
she-cnj
watashi-ga
I-nom
goukaku-shita
passed an exam
(she and I passed an exam)
In sentence (1a), postposition ?to? is used as a comi-
tative case marker, but in sentence (1b), postposition
?to? is used as a coordinate conjunction.
To resolve this ambiguity, predicative case frames
are required. Case frames describe what kinds of
1In this paper, we use the following abbreviations:
nom (nominative), acc (accusative), abl (ablative), cmi (comi-
tative), cnj (conjunction) and TM (topic marker).
306
Table 1: Case frame examples (Examples are writ-
ten in English. Numbers following each example
represent its frequency.).
CS Examples
ga I:18, person:15, craftsman:10, ? ? ?
yaku (1) wo bread:2484, meat:1521, cake:1283, ? ? ?
(broil) de oven:1630, frying pan:1311, ? ? ?
yaku (2) ga teacher:3, government:3, person:3, ? ? ?
(have wo fingers:2950
difficulty) ni attack:18, action:15, son:15, ? ? ?
ga maker:1, distributor:1
yaku (3) wo data:178, file:107, copy:9, ? ? ?
(burn) ni R:1583, CD:664, CDR:3, ? ? ?
...
...
...
ga dolphin:142, student:50, fish:28, ? ? ?
oyogu (1) wo sea:1188, underwater:281, ? ? ?
(swim) de crawl:86, breaststroke:49, stroke:24, ? ? ?
...
...
...
ga I:4, man:4, person:4, ? ? ?
migaku (1) wo tooth:5959, molar:27, foretooth:12
(brush) de brush:38, salt:13, powder:12, ? ? ?
...
...
...
nouns are related to each predicate. For example, a
case frame of ?iku? (go) has a ?to? case slot filled
with the examples such as ?kanojo? (she) or human.
On the other hand, ?goukaku-suru? (pass an exam)
does not have a ?to? case slot but does have a ?ga?
case slot filled with ?kanojo? (she) and ?watashi?
(I). These case frames provide the information for
disambiguating the postpositions ?to? in sentences
(1a) and (1b): (1a) is not coordinate and (1b) is co-
ordinate.
This paper proposes a method for integrating co-
ordination disambiguation into probabilistic syntac-
tic and case structure analysis. This method simul-
taneously addresses the two tasks of coordination
disambiguation by utilizing syntactic/semantic par-
allelism in possible coordinate structures and lexi-
cal preferences in large-scale case frames. We use
the case frames that were automatically constructed
from the web (Table 1). In addition, cooccurrence
statistics of coordinate conjuncts are incorporated
into this model.
2 Related Work
Previous work on coordination disambiguation has
focused mainly on finding the scope of coordinate
structures.
Agarwal and Boggess proposed a method for
identifying coordinate conjuncts (Agarwal and
Boggess, 1992). Their method simply matches parts
of speech and hand-crafted semantic tags of the
head words of the coordinate conjuncts. They tested
their method using the Merck Veterinary Manual
and found their method had an accuracy of 81.6%.
Resnik described a similarity-based approach for
coordination disambiguation of nominal compounds
(Resnik, 1999). He proposed a similarity measure
based on the notion of shared information content.
He conducted several experiments using the Penn
Treebank and reported an F-measure of approxi-
mately 70%.
Goldberg applied a cooccurrence-based proba-
bilistic model to determine the attachments of am-
biguous coordinate phrases with the form ?n1 p n2
cc n3? (Goldberg, 1999). She collected approxi-
mately 120K unambiguous pairs of two coordinate
words from a raw newspaper corpus for a one-year
period and estimated parameters from these statis-
tics. Her method achieved an accuracy of 72% using
the Penn Treebank.
Chantree et al presented a binary classifier for co-
ordination ambiguity (Chantree et al, 2005). Their
model is based on word distribution information
obtained from the British National Corpus. They
achieved an F-measure (? = 0.25) of 47.4% using
their own test set.
The previously described methods focused on co-
ordination disambiguation. Some research has been
undertaken that integrated coordination disambigua-
tion into parsing.
Kurohashi and Nagao proposed a Japanese pars-
ing method that included coordinate structure detec-
tion (Kurohashi and Nagao, 1994). Their method
first detects coordinate structures in a sentence, and
then heuristically determines the dependency struc-
ture of the sentence under the constraints of the de-
tected coordinate structures. Their method correctly
analyzed 97 Japanese sentences out of 150.
Charniak and Johnson used some features of syn-
tactic parallelism in coordinate structures for their
MaxEnt reranking parser (Charniak and Johnson,
2005). The reranker achieved an F-measure of
91.0%, which is higher than that of their genera-
tive parser (89.7%). However, they used a numer-
ous number of features, and the contribution of the
307
Table 2: Expressions that indicate coordinate struc-
tures.
(a) coordinate noun phrase:
,(comma) to ya toka katsu oyobi ka aruiwa ...
(b) coordinate predicative clause:
-shi ga oyobi ka aruiwa matawa ...
(c) incomplete coordinate structure:
,(comma) oyobi narabini aruiwa ...
parallelism features is unknown.
Dubey et al proposed an unlexicalized PCFG
parser that modified PCFG probabilities to condi-
tion the existence of syntactic parallelism (Dubey
et al, 2006). They obtained an F-measure increase
of 0.4% over their baseline parser (73.0%). Experi-
ments with a lexicalized parser were not conducted
in their work.
A number of machine learning-based approaches
to Japanese parsing have been developed. Among
them, the best parsers are the SVM-based depen-
dency analyzers (Kudo and Matsumoto, 2002; Sas-
sano, 2004). In particular, Sassano added some fea-
tures to improve his parser by enabling it to detect
coordinate structures (Sassano, 2004). However, the
added features did not contribute to improving the
parsing accuracy. This failure can be attributed to
the inability to consider global parallelism.
3 Coordination Ambiguity in Japanese
In Japanese, the bunsetsu is a basic unit of depen-
dency that consists of one or more content words and
the following zero or more function words. A bun-
setsu corresponds to a base phrase in English and
?eojeol? in Korean.
Coordinate structures in Japanese are classified
into three types. The first type is the coordinate noun
phrase.
(2) nagai
long
enpitsu-to
pencil-cnj
keshigomu-wo
eraser-acc
katta
bought
(bought a long pencil and an eraser)
We can find these phrases by referring to the words
listed in Table 2-a.
The second type is the coordinate predicative
clause, in which two or more predicates form a co-
ordinate structure.
bn
An: Partial matrix
A = (a(i, j))
Coordination key bunsetsu
a(n, m)
a(pm-n, n+1)
a path
Similarity betweenbn and bm
Figure 1: Method using triangular matrix.
(3) kanojo-to
she-cmi
kekkon-shi
married
ie-wo
house-acc
katta
bought
(married her and bought a house)
We can find these clauses by referring to the words
and ending forms listed in Table 2-b.
The third type is the incomplete coordinate struc-
ture, in which some parts of coordinate predicative
clauses are present.
(4) Tom-wa
Tom-TM
inu-wo,
dog-acc
Jim-wa
Jim-TM
neko-wo
cat-acc
kau
buys
(Tom (buys) a dog, and Jim buys a cat)
We can find these structures by referring to the
words listed in Table 2-c and also the correspon-
dence of case-marking postpositions.
For all of these types, we can detect the possibility
of a coordinate structure by looking for a coordina-
tion key bunsetsu that accompanies one of the words
listed in Table 2 (in total, we have 52 coordination
expressions). That is to say, the left and right sides of
a coordination key bunsetsu constitute possible pre-
and post-conjuncts, and the key bunsetsu is located
at the end of the pre-conjunct. The size of the con-
juncts corresponds to the scope of the coordination.
4 Calculating Similarity between Possible
Coordinate Conjuncts
We assess the parallelism of potential coordinate
structures in a probabilistic parsing model. In this
308
puroguramingu gengo-wa 2 2 0 2 2 2 0 0 2 0 (prog. language)
mondai kaiketsu-no 2 0 2 4 2 0 0 2 0 (problem solution)
arugorizumu-wo 0 2 2 4 0 0 2 0 (algorithm)hyogen dekiru 0 0 0 2 4 0 2 (can express)kijutsuryoku-to 2 2 0 0 2 0 (descriptive power)keisanki-no 2 0 0 2 0 (computer)kinou-wo 0 0 2 0 (function)jubun-ni 2 0 2 (sufficiently)kudou dekiru 0 2 (can drive)
wakugumi-ga 0 (framework)hitsuyou-dearu. (require)
(Programming language requires descriptive power to express an algorithm for 
solving problems and a framework to sufficiently drive functions of a computer.)
post-conjunct
pre-conjunct
Figure 2: Example of calculating path scores.
section, we describe a method for calculating simi-
larities between potential coordinate conjuncts.
To measure the similarity between potential pre-
and post-conjuncts, a lot of work on the coordi-
nation disambiguation used the similarity between
conjoined heads. However, not only the conjoined
heads but also other components in conjuncts have
some similarity and furthermore structural paral-
lelism. Therefore, we use a method to calculate the
similarity between two whole coordinate conjuncts
(Kurohashi and Nagao, 1994). The remainder of this
section contains a brief description of this method.
To calculate similarity between two series of bun-
setsus, a triangular matrix, A, is used (illustrated in
Figure 1).
A = (a(i, j)) (0 ? i ? l; i ? j ? l) (1)
where l is the number of bunsetsus in a sentence,
diagonal element a(i, j) is the i-th bunsetsu, and el-
ement a(i, j) (i < j) is the similarity value between
bunsetsus bi and bj . A similarity value between
two bunsetsus is calculated on the basis of POS
matching, exact word matching, and their semantic
closeness in a thesaurus tree (Kurohashi and Nagao,
1994). We use the Bunruigoihyo thesaurus, which
contains 96,000 Japanese words (The National In-
stitute for Japanese Language, 2004).
To detect a coordinate structure involving a key
bunsetsu, bn, we consider only a partial matrix (de-
noted An), that is, the upper right part of bn (Figure
1).
An = (a(i, j)) (0 ? i ? n;n + 1 ? j ? l) (2)
To specify correspondences between bunsetsus in
potential pre- and post-conjuncts, a path is defined
as follows:
path ::= (a(p1,m), a(p2,m? 1), . . . ,
a(pm?n, n + 1)) (3)
where n+1 ? m ? l, a(p1,m) 6= 0, p1 = n, pi ?
pi+1, (1 ? i ? m? n? 1).
That is, a path represents a series of elements from
a non-zero element in the lowest row in An to an
element in the leftmost column in An. The path has
an only element in each column and extends toward
the upper left. The series of bunsetsus on the left side
of the path and the series under the path are potential
conjuncts for key bn. Figure 2 shows an example of
a path.
A path score is defined based on the following cri-
teria:
? the sum of each element?s points on the path
? penalty points when the path extends non-
diagonally (which causes conjuncts of unbal-
anced lengths)
? bonus points on expressions signaling the be-
ginning or ending of a coordinate structure,
such as ?kaku? (each) and nado? (and so on)
? the total score of the above criteria is divided
by the square root of the number of bunsetsus
covered by the path for normalization
The score of each path is calculated using a dy-
namic programming method. We consider each path
as a candidate of pre- and post-conjuncts.
309
5 Integrated Probabilistic Model for
Syntactic, Coordinate and Case
Structure Analysis
This section describes a method of integrating coor-
dination disambiguation into a probabilistic parsing
model. The integrated model is based on a fully-
lexicalized probabilistic model for Japanese syntac-
tic and case structure analysis (Kawahara and Kuro-
hashi, 2006b).
5.1 Outline of the Model
This model gives a probability to each possible de-
pendency structure, T , and case structure, L, of the
input sentence, S, and outputs the syntactic, coordi-
nate and case structure that have the highest proba-
bility. That is to say, the model selects the syntactic
structure, T best, and the case structure, Lbest, that
maximize the probability, P (T,L|S):
(T best, Lbest) = argmax (T,L)P (T,L|S)
= argmax (T,L)
P (T,L, S)
P (S)
= argmax (T,L)P (T,L, S) (4)
The last equation is derived because P (S) is con-
stant.
The model considers a clause as a generation unit
and generates the input sentence from the end of the
sentence in turn. The probability P (T,L, S) is de-
fined as the product of probabilities for generating
clause Ci as follows:
P (T,L, S) =
?
i=1..nP (Ci, relihi |Chi) (5)
where n is the number of clauses in S, Chi is Ci?s
modifying clause, and relihi is the dependency re-
lation between Ci and Chi . The main clause, Cn,
at the end of a sentence does not have a modify-
ing head, but a virtual clause Chn = EOS (End Of
Sentence) is inserted. Dependency relation relihi is
first classified into two types C (coordinate) and D
(normal dependency), and C is further divided into
five classes according to the binned similarity (path
score) of conjuncts. Therefore, relihi can be one of
the following six classes.
relihi = {D,C0, C1, C2, C3, C4} (6)
For instance, C0 represents a coordinate relation
with a similarity of less than 1, and C4 represents
a coordinate relation with a similarity of 4 or more.
bentou-wa
tabete-te
kaet-ta(go home)
bentou-wa
tabete-te
kaet-ta(go home) EOSEOS)|,( EOSDtakaetP ? )|,( EOSDtakaetwabentouP ??)|,( takaetDtetabewabentouP ??? )|,( takaetwabentouDtetabeP ???
(eat)
(lunchbox)
(eat)
(lunchbox)
)|,( EOSDtakaetP ? )|,( EOSDtakaetwabentouP ??)|0,( takaetCtetabewabentouP ??? )|0,( takaetwabentouCtetabeP ???
(1) (3)
(4)(2)
Dependency structure Dependency structure21,TT 43 ,TT
DT :1 0:2 CT DT :3 0:4 CT
Figure 3: Example of probability calculation.
For example, consider the sentence shown in Fig-
ure 3. There are four possible dependency structures
in this figure, and the product of the probabilities
for each structure indicated below the tree is calcu-
lated. Finally, the model chooses the structure with
the highest probability (in this case T 1 is chosen).
Clause Ci is decomposed into its clause type,
f i, (including the predicate?s inflection and function
words) and its remaining content part Ci?. Clause
Chi is also decomposed into its content part, Chi ?,
and its clause type, fhi .
P (Ci, relihi |Chi) = P (Ci
?, f i, relihi |Chi
?, fhi)
= P (Ci?, relihi |f i, Chi
?, fhi)? P (f i|Chi
?, fhi)
? P (Ci?, relihi |f i, Chi
?)? P (f i|fhi) (7)
Equation (7) is derived because the content part, Ci?,
is usually independent of its modifying head type,
fhi , and in most cases, the type, f i, is independent
of the content part of its modifying head, Chi .
We call P (Ci?, relihi |f i, Chi ?) generative prob-
ability of a case and coordinate structure, and
P (f i|fhi) generative probability of a clause type.
The latter is the probability of generating func-
tion words including topic markers and punctuation
marks, and is estimated using a syntactically an-
notated corpus in the same way as (Kawahara and
Kurohashi, 2006b).
The generative probability of a case and coordi-
nate structure can be rewritten as follows:
P (Ci?, relihi |f i, Chi
?)
= P (Ci?|relihi , f i, Chi
?)? P (relihi |f i, Chi
?)
? P (Ci?|relihi , f i, Chi
?)? P (relihi |f i) (8)
310
Equation (8) is derived because dependency rela-
tions (coordinate or not) heavily depend on mod-
ifier?s types including coordination keys. We call
P (Ci?|relihi , f i, Chi ?) generative probability of a
case structure, and P (relihi |f i) generative proba-
bility of a coordinate structure. The following two
subsections describe these probabilities.
5.2 Generative Probability of Coordinate
Structure
The most important feature to decide whether two
clauses are coordinate is coordination keys. There-
fore, we consider a coordination key, ki, as clause
type f i. The generative probability of a coordinate
structure, P (relihi |f i), is defined as follows:
P (relihi |f i) = P (relihi |ki) (9)
We classified coordination keys into 52 classes ac-
cording to the classification proposed by (Kurohashi
and Nagao, 1994). If type f i does not contain a co-
ordination key, the relation is always D (normal de-
pendency), that is P (relihi |f i) = P (D|?) = 1.
The generative probability of a coordinate struc-
ture was estimated from a syntactically annotated
corpus using maximum likelihood. We used the
Kyoto Text Corpus (Kurohashi and Nagao, 1998),
which consists of 40K Japanese newspaper sen-
tences.
5.3 Generative Probability of Case Structure
We consider that a case structure consists of a pred-
icate, vi, a case frame, CF l, and a case assignment,
CAk. Case assignment CAk represents correspon-
dences between the input case components and the
case slots shown in Figure 4. Thus, the generative
probability of a case structure is decomposed as fol-
lows:
P (Ci?|relihi , f i, Chi
?)
= P (vi, CF l, CAk|relihi , f i, Chi
?)
= P (vi|relihi , f i, Chi
?)
? P (CF l|relihi , f i, Chi
?, vi)
? P (CAk|relihi , f i, Chi
?, vi, CF l)
? P (vi|relihi , f i, whi)
? P (CF l|vi)
? P (CAk|CF l, f i) (10)
bentou-wa
tabete
(lunchbox)
(eat)
?
lunchbox, bread, ?wo
man, student, ?ga
taberu1 (eat)
Case Frame CF
l
Case 
Assignment
CA
k
(no correspondence)
Dependency Structure of S
Figure 4: Example of case assignment.
The above approximation is given because it is nat-
ural to consider that the predicate vi depends on its
modifying head whi instead of the whole modifying
clause, that the case frame CF l only depends on the
predicate vi, and that the case assignment CAk de-
pends on the case frame CF l and the clause type f i.
The generative probabilities of case frames and
case assignments are estimated from case frames
themselves in the same way as (Kawahara and Kuro-
hashi, 2006b). The remainder of this section de-
scribes the generative probability of a predicate,
P (vi|relihi , f i, whi).
The generative probability of a predicate cap-
tures cooccurrences of coordinate or non-coordinate
phrases. This kind of information is not handled
in case frames, which aggregate only predicate-
argument relations.
The generative probability of a predicate mainly
depends on a coordination key in the clause type, f i,
as well as the generative probability of a coordinate
structure. We define this probability as follows:
P (vi|relihi , f i, whi) = P (vi|relihi , ki, whi)
If Ci? is a nominal clause and consists of a noun
ni, we consider the following probability in stead of
equation (10):
Pn(Ci?|relihi , f i, Chi
?) ? P (ni|relihi , f i, whi)
This is because a noun does not have a case frame
and any case components in the current framework.
To estimate these probabilities, we first applied a
conventional parsing system with coordination dis-
ambiguation to a huge corpus, and collected coor-
dinate bunsetsus from the parses. We used KNP2
(Kurohashi and Nagao, 1994) as the parser and a
web corpus consisting of 470M Japanese sentences
(Kawahara and Kurohashi, 2006a). The generative
probability of a predicate was estimated from the
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
311
collected coordinate bunsetsus using maximum like-
lihood.
5.4 Practical Issue
The proposed model considers all the possible de-
pendency structures including coordination ambigu-
ities. To reduce this high computational cost, we in-
troduced the CKY framework to the search.
Each parameter in the model is smoothed by using
several back-off levels in the same way as (Collins,
1999). Smoothing parameters are optimized using a
development corpus.
6 Experiments
We evaluated the coordinate structures and depen-
dency structures that were outputted by our model.
The case frames used in this paper were automati-
cally constructed from 470M Japanese sentences ob-
tained from the web. Some examples of the case
frames are listed in Table 1 (Kawahara and Kuro-
hashi, 2006a).
In this work, the parameters related to unlexical
types are calculated from a small tagged corpus of
newspaper articles, and lexical parameters are ob-
tained from a huge web corpus. To evaluate the ef-
fectiveness of our fully-lexicalized model, our ex-
periments are conducted using web sentences. As
the test corpus, we prepared 759 web sentences 3.
The web sentences were manually annotated using
the same criteria as the Kyoto Text Corpus. We also
used the Kyoto Text Corpus as a development corpus
to optimize the smoothing parameters. The system
input was automatically tagged using the JUMAN
morphological analyzer 4.
We used two baseline systems for comparative
purposes: the rule-based dependency parser, KNP
(Kurohashi and Nagao, 1994), and the probabilis-
tic model of syntactic and case structure analysis
(Kawahara and Kurohashi, 2006b), in which coor-
dination disambiguation is the same as that of KNP.
6.1 Evaluation of Detection of Coordinate
Structures
First, we evaluated detecting coordinate structures,
namely whether a coordination key bunsetsu triggers
3The test set was not used to construct case frames and esti-
mate probabilities.
4http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
Table 3: Experimental results of detection of coor-
dinate structures.
baseline proposed
precision 366/460 (79.6%) 361/435 (83.0%)
recall 366/447 (81.9%) 361/447 (80.8%)
F-measure ? (80.7%) ? (81.9%)
a coordinate structure. Table 3 lists the experimen-
tal results. The F-measure of our method is slightly
higher than that of the baseline method (KNP). In
particular, our method achieved good precision.
6.2 Evaluation of Dependency Parsing
Secondly, we evaluated the dependency structures
analyzed by the proposed model. Evaluating the
scope ambiguity of coordinate structures is sub-
sumed within this dependency evaluation. The de-
pendency structures obtained were evaluated with
regard to dependency accuracy ? the proportion of
correct dependencies out of all dependencies except
for the last dependency in the sentence end 5. Ta-
ble 4 lists the dependency accuracy. In this table,
?syn? represents the rule-based dependency parser,
KNP, ?syn+case? represents the probabilistic parser
of syntactic and case structure (Kawahara and Kuro-
hashi, 2006b), and ?syn+case+coord? represents our
proposed model. The proposed model significantly
outperformed both of the baseline systems (McNe-
mar?s test; p < 0.01).
In the table, the dependency accuracies are clas-
sified into four types on the basis of the bunsetsu
classes (PB: predicate bunsetsu and NB: noun bun-
setsu) of a dependent and its head. ?syn+case?
outperformed ?syn?. In particular, the accuracy
of predicate-argument relations (?NB?PB?) was
improved, but the accuracies of ?NB?NB? and
?PB?PB? decreased. ?syn+case+coord? outper-
formed the two baselines for all of the types. Not
only the accuracy of predicate-argument relations
(?NB?PB?) but also the accuracies of coordinate
noun/predicate bunsetsus (related to ?NB?NB? and
?PB?PB?) were improved. These improvements
are conduced by the integration of coordination dis-
ambiguation and syntactic/case structure analysis.
5Since Japanese is head-final, the second last bunsetsu un-
ambiguously depends on the last bunsetsu, and the last bunsetsu
has no dependency.
312
Table 4: Experimental results of dependency parsing.
syn syn+case syn+case+coord
all 3,833/4,436 (86.4%) 3,852/4,436 (86.8%) 3,893/4,436 (87.8%)
NB?PB 1,637/1,926 (85.0%) 1,664/1,926 (86.4%) 1,684/1,926 (87.4%)
NB?NB 1,032/1,136 (90.8%) 1,029/1,136 (90.6%) 1,037/1,136 (91.3%)
PB?PB 654/817 (80.0%) 647/817 (79.2%) 659/817 (80.7%)
PB?NB 510/557 (91.6%) 512/557 (91.9%) 513/557 (92.1%)
To compare our results with a state-of-the-art dis-
criminative dependency parser, we input the same
test corpus into an SVM-based Japanese dependency
parser, CaboCha6(Kudo and Matsumoto, 2002).
Its dependency accuracy was 86.3% (3,829/4,436),
which is equivalent to that of ?syn? (KNP). This low
accuracy is attributed to the out-of-domain training
corpus. That is, the parser is trained on a newspa-
per corpus, whereas the test corpus is obtained from
the web, because of the non-availability of a tagged
web corpus that is large enough to train a supervised
parser.
6.3 Discussion
Figure 5 shows some analysis results, where the
dotted lines represent the analysis by the baseline,
?syn+case?, and the solid lines represent the analysis
by the proposed method, ?syn+case+coord?. These
sentences are incorrectly analyzed by the baseline
but correctly analyzed by the proposed method. For
instance, in sentence (1), the noun phrase coordina-
tion of ?apurikeesyon? (application) and ?doraiba?
(driver) can be correctly analyzed. This is because
the case frame of ?insutooru-sareru? (installed) is
likely to generate ?doraiba?, and ?apurikeesyon?
and ?doraiba? are likely to be coordinated.
One of the causes of errors in dependency parsing
is the mismatch between analysis results and anno-
tation criteria. As per the annotation criteria, each
bunsetsu has only one modifying head. Therefore, in
some cases, even if analysis results are semantically
correct, they are judged as incorrect from the view-
point of the annotation. For example, in sentence
(4) in Figure 6, the baseline method, ?syn?, correctly
recognized the head of ?iin-wa? (commissioner-TM)
as ?hirakimasu? (open). However, the proposed
method incorrectly judged it as ?oujite-imasuga?
(offer). Both analysis results can be considered to
be semantically correct, but from the viewpoint of
6http://chasen.org/?taku/software/cabocha/
our annotation criteria, the latter is not a syntactic
relation (i.e., incorrect), but an ellipsis relation. This
kind of error is caused by the strong lexical prefer-
ence considered in our method.
To address this problem, it is necessary to simul-
taneously evaluate not only syntactic relations but
also indirect relations, such as ellipses and anaphora.
This kind of mismatch also occurred for the detec-
tion of coordinate structures.
Another errors were caused by an inherent char-
acteristic of generative models. Generative models
have some advantages, such as their application to
language models. However, it is difficult to incor-
porate various features that seem to be useful for
addressing syntactic and coordinate ambiguity. We
plan to apply discriminative reranking to the n-best
parses produced by our generative model in the same
way as (Charniak and Johnson, 2005).
7 Conclusion
This paper has described an integrated probabilistic
model for coordination disambiguation and syntac-
tic/case structure analysis. This model takes advan-
tage of lexical preference of a huge raw corpus and
large-scale case frames and performs coordination
disambiguation and syntactic/case analysis simulta-
neously. The experiments indicated the effective-
ness of our model. Our future work involves incor-
porating ellipsis resolution to develop an integrated
model for syntactic, case, and ellipsis analysis.
Acknowledgment
This research is partially supported by special coor-
dination funds for promoting science and technol-
ogy.
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Proceed-
ings of ACL1992, pages 15?21.
313
??
(1) insutooru-sareteiru apurikeesyon-oyobi doraiba-tono kyougou-niyori dousa-shinai baai-ga arimasu.
installed application driver conflict not work case-nom exist
(due to the conflict between installed application and driver, there is a case that (it) does not work.)
? ?
(2) ... kuroji-wa 41oku-doru-to, zennen-yori 10oku-doru gensyou-shita.
surplus-TM 4.1 billion dollars preceding year-abl 1 billion dollars reduced
(... surplus was 4.1 billion dollars and was reduced by 1 billion dollars from the preceding year.)
??
(3) ... gurupu-wa sugu ugokidasu-node wakaru-nodaga, ugokidasa-nai gurupu-mo aru.
group-TM soon start to work see not start to work group also be
(... can see the groups that start to work soon, but there are groups that do not start to work.)
Figure 5: Examples of correct analysis results. The dotted lines represent the analysis by the baseline,
?syn+case?, and the solid lines represent the analysis by the proposed method, ?syn+case+coord?.
??
(4) iin-wa, jitaku-de minasan-karano gosoudan-ni oujite-imasuga, ... soudansyo-wo hirakimasu
commissioner-TM at home all of you consultation-acc offer window open
(the commissioner offers consultation to all of you at home, but opens a window ...)
Figure 6: An example of incorrect analysis results caused by the mismatch between analysis results and
annotation criteria.
Francis Chantree, Adam Kilgarriff, Anne de Roeck, and
Alistair Wills. 2005. Disambiguating coordinations
using word distribution information. In Proceedings
of RANLP2005.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL2005, pages 173?180.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Amit Dubey, Frank Keller, and Patrick Sturt. 2006. In-
tegrating syntactic priming into an incremental prob-
abilistic parser, with an application to psycholinguis-
tic modeling. In Proceedings of COLING-ACL2006,
pages 417?424.
Miriam Goldberg. 1999. An unsupervised model for
statistically determining coordinate phrase attachment.
In Proceedings of ACL1999, pages 610?614.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using
high-performance computing. In Proceedings of
LREC2006.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis. In Proceedings of
HLT-NAACL2006, pages 176?183.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of CoNLL2002, pages 29?35.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
Sadao Kurohashi and Makoto Nagao. 1998. Building
a Japanese parsed corpus while improving the parsing
system. In Proceedings of LREC1998, pages 719?724.
Sadao Kurohashi. 1995. Analyzing coordinate structures
including punctuation in English. In Proceedings of
IWPT1995, pages 136?147.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95?130.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proceedings of COLING2004,
pages 8?14.
The National Institute for Japanese Language. 2004.
Bunruigoihyo. Dainippon Tosho, (In Japanese).
314
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 429?437,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Online Acquisition of Japanese Unknown Morphemes
using Morphological Constraints
Yugo Murawaki Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan
murawaki@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
Abstract
We propose a novel lexicon acquirer that
works in concert with the morphological ana-
lyzer and has the ability to run in online mode.
Every time a sentence is analyzed, it detects
unknown morphemes, enumerates candidates
and selects the best candidates by comparing
multiple examples kept in the storage. When
a morpheme is unambiguously selected, the
lexicon acquirer updates the dictionary of the
analyzer, and it will be used in subsequent
analysis. We use the constraints of Japanese
morphology and effectively reduce the num-
ber of examples required to acquire a mor-
pheme. Experiments show that unknown mor-
phemes were acquired with high accuracy and
improved the quality of morphological analy-
sis.
1 Introduction
Morphological analysis is the first step for most nat-
ural language processing applications. In Japanese
morphological analysis, segmentation is processed
simultaneously with the assignment of a part of
speech (POS) tag to each morpheme. Segmentation
is a nontrivial task in Japanese because it does not
delimit words by white-space.
Japanese morphological analysis has successfully
adopted dictionary-based approaches (Kurohashi et
al., 1994; Asahara and Matsumoto, 2000; Kudo et
al., 2004). In these approaches, a sentence is trans-
formed into a lattice of morphemes by searching a
pre-defined dictionary, and an optimal path in the
lattice is selected.
This area of research may be considered almost
completed, as previous studies reported the F-score
of nearly 99% (Kudo et al, 2004). When applied
to web texts, however, more errors are made due to
unknown morphemes. In previous studies, exper-
iments were performed on newspaper articles, but
web texts include slang words, informal spelling al-
ternates (Nishimura, 2003) and technical terms. For
example, the verb ????? (gugu-ru, to google) is
erroneously segmented into ???? (gugu) and ???
(ru).
One solution to this problem is to augment the
lexicon of the morphological analyzer by extracting
unknown morphemes from texts (Mori and Nagao,
1996). In the previous method, a morpheme extrac-
tion module worked independently of the morpho-
logical analyzer and ran in off-line (batch) mode.
It is inefficient because almost all high-frequency
morphemes have already been registered to the pre-
defined dictionary. Moreover, it is inconvenient
when applied to web texts because the web corpus
is huge and diverse compared to newspaper corpora.
It is not necessarily easy to build subcorpora before
lexicon acquisition. Suppose that we want to ana-
lyze whaling-related documents. It is unnecessary
and probably harmful to acquire morphemes that are
irrelevant to the topic. A whaling-related subcorpus
should be extracted from the whole corpus but it is
not clear how large it must be.
We propose a novel lexicon acquirer that works
in concert with the morphological analyzer and has
the ability to run in online mode. As shown in Fig-
ure 1, every time a sentence is analyzed, the lexicon
acquirer detects unknown morphemes, enumerates
429
text
Analyzer
JUMAN
(morph.
analyzer)
KNP
(parser)
analysis
DetectorEnumeratorSelector
accumulated
examples
hand-crafted
dictionary
automatically constructed
dictionary
update
lookup
Lexicon Acquirer
analysis
Figure 1: System architecture
candidates and selects the best candidates by com-
paring multiple examples kept in the storage. When
a morpheme is unambiguously selected, the lexicon
acquirer updates the automatically constructed dic-
tionary, and it will be used in subsequent analysis.
The proposed method is flexible and gives the sys-
tem more control over the process. We do not have
to limit the target corpus beforehand and the system
can stop whenever appropriate.
We use the constraints of Japanese morphology
that have already been coded in the morphological
analyzer. These constraints effectively reduce the
number of examples required to acquire an unknown
morpheme. Experiments show that unknown mor-
phemes were acquired with high accuracy and im-
proved the quality of morphological analysis.
2 Japanese Morphology
In order to understand the task of lexicon acquisi-
tion, we briefly describe the Japanese morpholog-
ical analyzer JUMAN.1 We explain Japanese mor-
phemes in Section 2.1, morphological constraints in
Section 2.2, and unknown morpheme processing in
Section 2.3.
2.1 Morpheme
In JUMAN, the POS tagset consists of four ele-
ments: class, subclass, conjugation type and con-
jugation form. The classes are noun, verb, adjec-
tive and others. Noun has subclasses such as com-
mon noun, sa-group noun, proper noun, organiza-
1
http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman.html
tion, place, personal name. Verb and adjective have
no subclasses.
Verbs and adjectives among others change their
form according to the morphemes that occur after
them, which is called conjugation. Conjugable mor-
phemes are grouped by conjugation types such as
vowel verb, ra-row verb, i-type adjective and na-
type adjective. Each conjugable morpheme takes
one of conjugation forms in texts. It has an invari-
ant stem and an ending which changes according to
conjugation type and conjugation form.
In this paper, the tuple of class, subclass and con-
jugation type is referred to as a POS tag. For sim-
plicity, POS tags for nouns are called by their sub-
classes and those for verbs and adjectives by their
conjugation types.
There are two types of morphemes: abstract dic-
tionary entries, and examples or actual occurrences
in texts. An entry consists of a stem and a POS tag
while an example consists of a stem, a POS tag and
a conjugation form. For example, the entry of the
ra-row verb ???? (hashi-ru, to run) can be repre-
sented as
(??? (hashi), ra-row verb),
and their examples ???? (hashi-ra) and ????
(hashi-ri) as
(??? (hashi), ra-row verb, imperfective),
and
(??? (hashi), ra-row verb, plain continu-
ative)
respectively. As nouns do not conjugate, the entry
of the sa-group noun ???? (kibou, hope) can be
represented as
(???? (kibou), sa-group noun)
and its sole example form is
(???? (kibou), sa-group noun, NIL).
2.2 Morphological Constraints
Japanese is an agglutinative language. Depending
on its grammatical roles, a morpheme is followed by
a sequence of grammatical suffixes, auxiliary verbs
and particles, and the connectivity of these elements
is bound by morphological constraints. For exam-
ple, the particle ??? (wo, accusative case) can fol-
low a verb with the conjugation form of plain contin-
uative, as in ????? (hashi-ri-wo, running-ACC),
430
but it cannot follow an imperfective verb (?*????
(*hashi-ra-wo)).
These constraints are used by JUMAN to reduce
the ambiguity. They can be also used in lexicon ac-
quisition.
2.3 Unknown Morpheme Processing
Given a sentence, JUMAN builds a lattice of mor-
phemes by searching a pre-defined dictionary, and
then selects an optimal path in the lattice. To han-
dle morphemes that cannot be found in the dictio-
nary, JUMAN enumerates unknown morpheme can-
didates using character type-based heuristics, and
adds them to the morpheme lattice. Unknown mor-
phemes are given the special POS tag ?undefined,?
which is treated as noun.
Character type-based heuristics are based on the
fact that Japanese is written with several different
character types such as kanji, hiragana and katakana,
and that the choice of character types gives some
clues on morpheme boundaries. For example, a se-
quence of katakana characters are considered as an
unknown morpheme candidate, as in ??????
(gu?guru, Google) out of ??????? (gu?guru-ga,
Google-NOM). Kanji characters are segmented per
character, which is sometimes wrong but prevents
error propagation.
These heuristics are simple and effective, but far
from perfect. They cannot identify mixed-character
morphemes, verbs and adjectives correctly. For ex-
ample, the verb ????? (gugu-ru, to google) is
wrongly divided into the katakana unknown mor-
pheme ???? (gugu) and the hiragana suffix ???
(ru).
3 Lexicon Acquisition
3.1 Task
The task of lexicon acquisition is to generate dictio-
nary entries inductively from their examples in texts.
Since the morphological analyzer provides a basic
lexicon, the morphemes to be acquired are limited
to those unknown to the analyzer.
In order to generate an entry, its stem and POS
tag need to be identified. Determining the stem of
an example is to draw the front and rear boundaries
in a character sequence in texts which corresponds
to the stem. The POS tag is selected from the tagset
given by the morphological analyzer.
3.2 System Architecture
Figure 1 shows the system architecture. Each sen-
tence in texts is processed by the morphological an-
alyzer JUMAN and the dependency parser KNP.2
JUMAN consults a hand-crafted dictionary and an
automatically constructed dictionary. KNP is used
to form a phrasal unit called bunsetsu by chunking
morphemes.
Every time a sentence is analyzed, the lexicon
acquirer receives the analysis. It detects examples
of unknown morphemes and keeps them in storage.
When an entry is unambiguously selected, the lex-
icon acquirer updates the automatically constructed
dictionary, and it will be used in subsequent analy-
sis.
3.3 Algorithm Overview
The process of lexicon acquisition has four phases:
detection, candidate enumeration, aggregation and
selection. First the analysis is scanned to detect ex-
amples of unknown morphemes. For each exam-
ple, one or more candidates for dictionary entries are
enumerated. It is added to the storage, and multiple
examples in the storage that share the candidates are
aggregated. They are compared and the best candi-
date is selected from it.
Take the ra-row verb ????? (gugu-ru) for ex-
ample. Its example ????????? (gugu-tte-
mi-ta, to have tried to google) can be interpreted in
many ways as shown in Figure 2. Similarly, multi-
ple candidates are enumerated for another example
??????? (gugu-ru-no-ha, to google-TOPIC). If
these examples are compared, we can see that the
ra-row verb ????? (gugu-ru) can explain them.
3.4 Suffixes
Morphological constraints are used for candidate
enumeration. Since they are coded in JUMAN, we
first transform them into a set of strings called suf-
fixes. A suffix is created by concatenating the end-
ing of a morpheme (if any) and subsequent ancillary
morphemes. Each POS tag is associated with a set
of suffixes, as shown in Table 1. This means that a
stem can be followed by one of the suffixes specified
2
http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp.html
431
Table 1: Examples of suffixes
POS tag base form stem ending conjugation form1 suffixes
ra-row verb hashi-ru hashi
ra imperfective razu, ranaide
ri plain continuative riwo, riwomo
ru plain ru, rukawo
vowel verb akogare-ru akogare
? imperfective zu, naide
? plain continuative wo, womo
ru plain ru, rukawo
sa-group noun kibou kibou NIL wo wo, womoNIL suru suru, shitara
1 The conjugation form of a noun is substituted with the base form of its immediate
ancillary morpheme because nouns do not conjugate.
suffix
???????
google -CONT try-PAST
stem
stem
stem
suffix
suffix
[POS tags]
? ra-row verb
? wa-row verb
? ta-row verb
? ma-row verb
? vowel verb
? ta-row verb
? (EOB)
stem
Figure 2: Candidate enumeration
by its POS tag and cannot be followed by any other
suffix.
In preparation for lexicon acquisition, suffixes are
acquired from a corpus. We used a web corpus that
was compiled through the procedures proposed by
Kawahara and Kurohashi (2006). Suffixes were ex-
tracted from examples of registered morphemes and
were aggregated per POS tag.
We found that the number of suffixes did not con-
verge even in this large-scale corpus. It was because
ancillary morphemes included the wide variety of
auxiliary verbs and formal nouns. Alternatively, we
used the first five characters as a suffix. In the exper-
iments, we obtained 500 thousand unique suffixes
from 100 million pages. The number of POS tags
that corresponded to a suffix was 1.33 on average.
3.5 Unknown Morpheme Detection
The first step of lexicon acquisition is unknown mor-
pheme detection. Every time the analysis of a sen-
tence was given, the sequence of morphemes are
scanned, and suspicious points that probably repre-
sent unknown morphemes are detected.
Currently, we use the POS tag ?undefined? to de-
tect unknown morphemes. For example, the exam-
ple ????????? is detected because ????
is given ?undefined.? This simple method cannot
detect unknown morphemes if they are falsely seg-
mented into combinations of registered morphemes.
We leave the comprehensive detection of unknown
morphemes to future work.
3.6 Candidate Enumeration
For each example, one or more candidates for the
dictionary entry are enumerated. Each candidate is
represented by a combination of a front boundary
and the pair of a rear boundary and a POS tag.
The search range for enumeration is based on bun-
setsu phrases, which is created by chunking mor-
phemes. The range is at most the corresponding
bunsetsu and the two immediately preceding and
succeeding bunsetsu, which we found wide enough
to contain correct candidates.
The candidates for the rear boundary and the POS
tag are enumerated by string matching of suffixes as
shown in Figure 2. If a suffix matches, the start-
ing position of the suffix becomes a candidate for
the rear boundary and the suffix is mapped to one or
more corresponding POS tags.
In addition, the candidates for the front and
rear boundaries are enumerated by scanning the se-
quence of morphemes. The boundary markers we
use are
? punctuations,
? grammatical prefixes such as ??? (go-, hon-
orific prefix), for front boundaries,
432
? grammatical suffixes such as ??? (-sama, hon-
orific title), for rear boundaries, and
? bunsetsu boundaries given by KNP.
Each rear boundary candidate whose correspond-
ing POS tag is not decided is given the special tag
?EOB? (end-of-bunsetsu). This means that no suf-
fix is attached to the candidate. Since nouns, vowel
verbs and na-type adjectives can appear in isolation,
it will be expanded to these POS tags when selecting
the best POS tag.
3.7 Aggregation of Examples
Selection of the best candidate is done by compar-
ing multiple examples. Each example is added to
the storage, and then examples that possibly repre-
sent the same entry with it are extracted from the
storage. Examples aggregated at this phase share the
front boundary but may be unrelated to the example
in question. They are pruned in the next phase.
In order to manage examples efficiently, we im-
plement a trie. The example is added to the trie for
each front boundary candidate. The key is the char-
acter sequence determined by the front boundary
and the leftmost rear boundary. To retrieve examples
that share the front boundary with it, we check every
node in the path from the root to the node where it is
stored, and collect examples stored in each node.
3.8 Selection
The best candidate is selected by identifying the
front boundary, the rear boundary and the POS tag
in this order. Starting from the rightmost front
boundary candidate, multiple rear boundary candi-
dates that share the front boundary are compared and
some are dropped. Then starting from the leftmost
surviving rear boundary candidate, the best POS tag
is selected from the examples that share the stem.
If the selected candidate satisfies simple termination
conditions, it is added to the dictionary and the ex-
amples are removed from the storage.
For each front boundary candidate, some inappro-
priate rear boundary candidates are dropped by ex-
amining the inclusion relation between the examples
of a pair of candidates. The assumption behind this
is that an appropriate candidate can interpret more
examples than incorrect ones. Let p and q be a pair
of the candidates for the rear boundary, and R
p
and
R
q
be the sets of examples for which p and q are
enumerated. If p is a prefix of q and p is the correct
stem, then R
q
must be contained in R
p
. In practice
we loosen this condition, considering possible errors
in candidate enumeration
For each stem candidate, the appropriate POS tag
is identified. Similarly to rear boundary identifica-
tion, POS identification is done by checking inclu-
sion relation.
If the POS tag is successfully disambiguated, sim-
ple termination conditions is checked to prevent the
accidental acquisition of erroneous candidates. The
first condition is that the number of unique conjuga-
tion forms that appear in the examples should be 3 or
more. If the candidate is a noun, it is substituted with
the number of the unique base forms of their imme-
diate ancillary morphemes. The second condition is
that the front boundaries of some examples are de-
cided by clear boundary markers such as punctua-
tions and the beginning of sentence. This prevents
oversegmentation. For example, the stem candidate
?*??? (*sengumi) is always enumerated for exam-
ples of ????? (Shingengumi, a historical organi-
zation) since ??? (shin-, new) is a prefix. This can-
didate is not acquired because ?*??? (*sengumi)
does not occur alone and is always accompanied by
??? (shin-). Thresholds are chosen empirically.
3.9 Decompositionality
Since a morpheme is extracted from a small num-
ber of examples, it is inherently possible that the ac-
quired morpheme actually consists of two or more
morphemes. For example, the noun phrase ???
???? (karyuu-taipu, granular type) may be ac-
quired as a morpheme before ???? (karyuu, gran-
ule) is extracted. To handle this phenomenon, it
is checked at the time of acquisition whether the
new morpheme (kairyuu) can decompose registered
morphemes (kairyuu-taipu). If found, a composite
?morpheme? is removed from the dictionary.
Currently we leave the decompositionality check
to the morphological analyzer. Possible compounds
are enumerated by string matching and temporar-
ily removed from the dictionary. Each candidate
is analyzed by the morphological analyzer and it is
checked whether the candidate is divided into a com-
bination of registered morphemes. If not, the candi-
date is restored to the dictionary.
433
Table 2: Statistical information per query
query
number of number of number of number of number of
sentences affected acquired correct examples1
sentences morphs morphs
(ratio) (precision)
???? 135,379 2,444 293 290 4
(whaling issue) (1.81%) (99.0%)
??????? 74,572 775 107 105 4
(baby hatch) (1.04%) (98.1%)
?????? 195,928 6,259 913 907 4
(JASRAC) (3.19%) (99.3%)
???? 77,962 12,012 243 238 5
(tsundere) (15.4%) (97.4%)
????? 78,922 3,037 114 107 9
(agaricus) (3.85%) (93.9%)
1 The median number of examples used for acquisition.
4 Experiments
4.1 Experimental Design
We used the default dictionary of the morphological
analyzer JUMAN as the initial lexicon. It contained
30 thousand basic morphemes. If spelling variants
were expanded and proper nouns were counted, the
total number of morphemes was 120 thousands.
We used domain-specific corpora as target texts
because efficient acquisition was expected. If target
texts shared a topic, relevant unknown morphemes
were used frequently. In the experiments, we used
search engine TSUBAKI (Shinzato et al, 2008) and
casted the search results as domain-specific corpora.
For each query, our system sequentially read pages
from the top of the result and acquired morphemes.
We terminated the acquisition at the 1000th page
and analyzed the same 1000 pages with the aug-
mented lexicon. The queries used were ????
?? (whaling issue), ????????? (baby hatch),
???????? (JASRAC, a copyright collective),
?????? (tsundere, a slang word) and ?????
?? (agaricus).
4.2 Evaluation Measures
The proposed method is evaluated by measuring the
accuracy of acquired morphemes and their contri-
bution to the improvement of morphological analy-
sis. A morpheme is considered accurate if both seg-
mentation and the POS tag are correct. Note that
segmentation is a nontrivial problem for evaluation.
In fact, the disagreement over segmentation criteria
was considered one of the main reasons for reported
errors by Nagata (1999) and Uchimoto et al (2001).
It is difficult to judge whether a compound term
should be divided because there is no definite stan-
dard for morpheme boundaries in Japanese. For ex-
ample, ?????? (minku-kujira, minke whale) can
be extracted as a single morpheme or decomposed
into ????? and ??.? While segmentation is an
open question in Japanese morphological analysis,
?correct? segmentation is not necessarily important
for applications using morphological analysis. Even
if a noun is split into two or more morphemes in
morphological analysis, they are chunked to form
a phrasal unit called bunsetsu in dependency pars-
ing, and to extract a keyword (Nakagawa and Mori,
2002).
To avoid the decompositionality problem, we
adopted manual evaluation. We analyzed the tar-
get texts with both the initial lexicon and the aug-
mented lexicon. Then we checked differences be-
tween the two analyses and extracted sentences that
were affected by the augmentation. Among these
sentences, we evaluated randomly selected 50 sen-
tences per query. We checked the accuracy of seg-
mentation and POS tagging of each ?diff? block,
which is illustrated in Figure 3. The segmentation of
a block was judged correct unless morpheme bound-
aries were clearly wrong.
In the evaluation of POS tagging, we did not dis-
tinguish subclasses of noun3 such as common noun
3In the experiments, we regarded demonstrative pronouns as
434
Table 3: Examples of acquired morphemes
query examples
whaling issue ?????? (moratorium),????? (giant beaked whale),?? (bycatch)
baby hatch ??? (husband),??? (midwife),??? (to abandon),?? (to inquire)
JASRAC ??? (an organization),??? Q (a pop-rock band),?? (geek)
tsundere ??? (abbr. of Akihabara),??? (fujoshi, a slang word),??? (to be popular)
agaricus ??? (abbr. of suppliment),??? (aroma),?? (enhanced nutritional function)
Table 4: Evaluation of ?diff? blocks
segmentation POS tagging
query E ? C C ? C E ? E C ? E E ? C C ? C E ? E C ? E total
whaling issue 11 45 0 2 11 45 0 2 58
baby hatch 37 12 0 3 37 12 0 3 52
JASRAC 16 23 1 12 16 23 1 12 52
tsundere 17 39 0 1 17 39 0 1 57
agaricus 22 31 0 0 22 31 0 0 53
(Legend ? C: correct; E: erroneous)
???????????
)QQING KVCPFYGYKNNHKPFCNQV
?? WPFGHKPGF MCVCMCPC
? UWHHKZ? XGTDCNUWHHKZ
 ??? XGTD TCTQYXGTD
Figure 3: A ?diff? block in a sentence
and proper noun. The special POS tag ?undefined?
given by JUMAN was treated as noun.
4.3 Results
Table 2 summarizes statistical information per
query. The number of sentences affected by the
augmentation varied considerably (1.04%?15.4%).
The initial lexicon of the morphological analyzer
lacked morphemes that appeared frequently in some
corpora because morphological analysis had been
tested mainly with newspaper articles.
The precision of acquired morphemes was high
(97.4%?99.3%), and the number of examples used
for acquisition was as little as 4?9. These results are
astonishing considering that Mori and Nagao (1996)
ignored candidates that appeared less than 10 times
(because they were unreliable).
nouns because their morphological behaviors were the same as
those of nouns. Although demonstrative nouns are closed class
morphemes, their katakana forms such as ???? (this) were
acquired as nouns. The morphological analyzer assumed that
demonstrative pronouns were written in hiragana, e.g., ???,?
as they always are in a newspaper.
Table 3 shows some acquired morphemes. As
expected, the overwhelming majority were nouns
(93.0%?100%) and katakana morphemes (80.7%?
91.6%). Some were mixed-character morphemes
(????? and ????Q?), which cannot be recog-
nized by character-type based heuristics, and slang
words (????,? ???,? etc.) which did not ap-
pear in newspaper articles. Some morphemes were
spelling variants of those in the pre-defined dictio-
nary. Uncommon kanji characters were used in ba-
sic words (????? for ????? and ???? for
????) and katakana was used to change nuances
(????? for ????? and ????? for ????).
Table 4 shows the results of manual evaluation of
?diff? blocks. The overwhelming majority of blocks
were correctly analyzed with the augmented lexicon
(E ? C and C ? C). On the other hand, adverse
effects were observed only in a few blocks (C ?
E). In conclusion, acquired morphemes improve the
quality of morphological analysis.
4.4 Error Analysis
Some short katakana morphemes oversegmented
other katakana nouns. For example, ??????
(sa?ba?, server) was wrongly segmented by newly-
acquired ???? (sa?, sir) and preregistered ????
(ba?, bar). Neither the morphological analyzer and
the lexicon acquirer could detect this semantic mis-
match. Curiously, one example of ???? (sa?) was
actuallly part of ?????? (sa?ba?), which was erro-
435
 0
 200
 400
 600
 800
 1000
 0  100000  200000
 0
 8000
 16000
 24000
 32000
 40000
nu
m
. o
f a
cq
ui
re
d 
m
or
ph
em
es
nu
m
. o
f e
xa
m
pl
es
num. of sentences
acquired morphemes
stored examples
acquired morphemes in re-analysis
Figure 4: Process of online acquisition
neously segmented when extracting sentences from
HTML.
The katakana adjective ???? (i-i, good), a
spelling variant of the basic morpheme ???,? was
falsely identified as a noun because its ending ???
was written in katakana. The morphological ana-
lyzer, and hence the lexicon acquirer, assume that
the ending of a verb or adjective is written in hi-
ragana. This assumption is reasonable for stan-
dard Japanese, but does not always hold when we
analyze web texts. In order to recognize uncon-
ventional spellings that are widely used in web
texts (Nishimura, 2003), more flexible analysis is
needed.
4.5 Discussion
It is too costly or impractical to calculate the re-
call of acquisition, or the ratio of the number of ac-
quired morphemes against the total number of un-
known morphemes because it requires human judges
to find undetected unknown morphemes from a large
amount of raw texts.
Alternatively, we examined the ratio against the
number of detected unknown morphemes. Figure 4
shows the process of online acquisition for the query
?JASRAC.? The monotonic increase of the num-
bers of acquired morphemes and stored examples
suggests that the vocabulary size did not converge.
The number of occurrences of acquired morphemes
in re-analysis was approximately the same with the
number of examples kept in the storage during ac-
quisition. This means that, in terms of frequency of
occurrence, about half of unknown morphemes were
acquired. Most unknown morphemes belong to the
?long tail? and the proposed method seems to have
seized a ?head? of the long tail.
Although some previous studies emphasized cor-
rect identification of low frequency terms (Nagata,
1999; Asahara and Matsumoto, 2004), it is no longer
necessary because very large scale web texts are
available today. If a small set of texts needs to
be analyzed with high accuracy, we can incorporate
similar texts retrieved from the web, to increase the
number of examples of unknown morphemes. The
proposed method can be modified to check if un-
known morphemes detected in the initial set are ac-
quired and to terminate whenever sufficient acquisi-
tion coverage is achieved.
5 Related Work
Since most languages delimit words by white-space,
morphological analysis in these languages is to seg-
ment words into morphemes. For example, Mor-
pho Challenge 2007 (Kurimo et al, 2007) was eval-
uations of unsupervised segmentation for English,
Finnish, German and Turkish.
While Japanese is an agglutinative language,
other non-segmented languages such as Chinese and
Thai are analytic languages. Among them, Chinese
has been a subject of intensive research. Peng et
al. (2004) integrated new word detection into word
segmentation. They detected new words by comput-
ing segment confidence and re-analyzed the inputs
with detected words as features.
The Japanese language is unique in that it is writ-
ten with several different character types. Heuris-
tics widely used in unknown morpheme process-
ing are based on character types. They were also
used as important clues in statistical methods. Na-
gata (1999) integrated a probabilistic unknown word
models into the word segmentation model. Uchi-
moto et al (2001) incorporated them as feature func-
tions of a Maximum Entropy-based morphological
analyzer. Asahara and Matsumoto (2004) used them
as a feature of character-based chunking of unknown
words using Support Vector Machines.
Mori (1996) extracted words from texts and esti-
mated their POSs using distributional analysis. The
appropriateness of a word candidate was measured
436
by the distance between probability distributions of
the candidate and a model. In this method, mor-
phological constraints were indirectly represented
by distributions.
Nakagawa and Matsumoto (2006) presented a
method for guessing POS tags of pre-segmented un-
known words that took into consideration all the oc-
currences of each unknown word in a document.
This setting is impractical in Japanese because POS
tagging is inseparable from segmentation.
6 Conclusion
We propose a novel method that augments the lexi-
con of a Japanese morphological analyzer by acquir-
ing unknown morphemes from texts in online mode.
Unknown morphemes are acquired with high accu-
racy and improve the quality of morphological anal-
ysis.
Unknown morphemes are one of the main sources
of error in morphological analysis when we analyze
web texts. The proposed method has the potential
to overcome the unknown morpheme problem, but
it cannot be achieved without recognizing or being
robust over various phenomena such as unconven-
tional spellings and typos. These phenomena are not
observed in newspaper articles but cannot be ignored
in web texts. In the future, we will work on these
phenomena.
Morphological analysis is now very mature. It
is widely applied as preprocessing for NLP appli-
cations such as parsing and information retrieval.
Hence in the future, we aim to use the proposed
method to improve the quality of these applications.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
tagger. In Procs. of COLING 2000, pages 21?27.
Masayuki Asahara and Yuji Matsumoto. 2004.
Japanese unknown word identification by character-
based chunking. In Procs. of COLING 2004, pages
459?465.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Procs. of LREC-06, pages
1344?1347.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Procs. of EMNLP 2004,
pages 230?237.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of Morpho Challenge in CLEF 2007.
In Working Notes of the CLEF 2007 Workshop, pages
19?21.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Procs. of The In-
ternational Workshop on Sharable Natural Language
Resources, pages 22?38.
Shinsuke Mori and Makoto Nagao. 1996. Word extrac-
tion from corpora and its part-of-speech estimation us-
ing distributional analysis. In Procs. of COLING 1996,
pages 1119?1122.
Masaaki Nagata. 1999. A part of speech estimation
method for Japanese unknown words using a statistical
model of morphology and context. In Procs. of ACL
1999, pages 277?284.
Tetsuji Nakagawa and Yuji Matsumoto. 2006. Guessing
parts-of-speech of unknown words using global infor-
mation. In Procs. of COLING-ACL 2006, pages 705?
712.
Hiroshi Nakagawa and Tatsunori Mori. 2002. A sim-
ple but powerful automatic term extraction method. In
COLING-02 on COMPUTERM 2002, pages 29?35.
Yukiko Nishimura. 2003. Linguistic innovations and in-
teractional features of casual online communication in
Japanese. Journal of Computer-Mediated Communi-
cation, 9(1).
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Procs. of COLING
?04, pages 562?568.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: An open search engine infrastructure for
developing new information access methodology. In
Procs. of IJCNLP-08, pages 189?196.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
2001. The unknown word problem: a morphological
analysis of Japanese using maximum entropy aided by
a dictionary. In Procs. of EMNLP 2001, pages 91?99.
437
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455?1464,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
A Probabilistic Model for Associative Anaphora Resolution
Ryohei Sasano and Sadao Kurohashi
Graduate School of Informatics, Kyoto University,
Yoshida-honmachi, Sakyo-ku, Kyoto
{sasano,kuro}@i.kyoto-u.ac.jp
Abstract
This paper proposes a probabilistic model
for associative anaphora resolution in
Japanese. Associative anaphora is a
type of bridging anaphora, in which the
anaphor and its antecedent are not coref-
erent. Our model regards associative
anaphora as a kind of zero anaphora and
resolves it in the same manner as zero
anaphora resolution using automatically
acquired lexical knowledge. Experimen-
tal results show that our model resolves
associative anaphora with good perfor-
mance and the performance is improved
by resolving it simultaneously with zero
anaphora.
1 Introduction
The correct interpretation of anaphora is vital
for natural language understanding. Bridging
anaphora (Clark, 1975) represents a special part of
the general problem of anaphora resolution, which
has been studied and discussed for various lan-
guages and domains (Hahn et al, 1996; Murata et
al., 1999; Poesio et al, 2004; Gasperin and Vieira,
2004; Gasperin and Briscoe, 2008).
Usually bridging anaphora considers two
types:
1
associative anaphors are noun phrases
(NPs) that have an antecedent that is necessary
to their interpretation but the relation between the
anaphor and its antecedent is different from iden-
tity; and indirect anaphors are those that have
an identity relation with their antecedents but the
anaphor and its antecedent have different head
1
The terminology that we use here is introduced by
Hawkins (1978), which is also used in (Vieira et al, 2006).
nouns. In this paper, we focus on associative
anaphora in Japanese.
Associative anaphora resolution is decomposed
into two steps: acquiring lexical knowledge for as-
sociative anaphora resolution, and resolving asso-
ciative anaphora using the acquired knowledge.
Grammatical salience plays a lesser role for
resolving anaphors with full lexical heads, than
for pronominal anaphora (Strube and Hahn, 1999;
Modjeska, 2002). Furthermore, since associative
anaphors and their antecedents usually have differ-
ent head nouns, string matching technique cannot
be applied. Therefore, a large and diverse amount
of lexical knowledge is essential to understand as-
sociative anaphora. For example, to recognize the
meronymic relation between ?a house? and ?the
roof? in (1), such knowledge as ?a roof? is a part
of a building or vehicle is required. To recognize
the attributive relation between ?Prius? and ?the
price? in (2), such knowledge as ?price? is a price
of some goods or service is required.
(1) There was a house. The roof was white.
(2) Toyota launched the hybrid car Prius in
1997. The price was 21.5 million yen.
To acquire such lexical knowledge, various
studies have been carried out. Early studies used
hand-crafted lexical knowledge such as Word-
Net (Strube and Hahn, 1999; Vieira and Poe-
sio, 2000; Meyer and Dale, 2002), but obtained
poor or mediocre results. Hence, Poesio et al
(2002) proposed to exploit ?N
h
of N
m
? phrases
in large corpora to resolve associative anaphora
in English; Murata et al (1999) proposed to ex-
ploit ?N
m
no N
h
? phrases to resolve associative
anaphora in Japanese. Here, the Japanese postpo-
sition ?no? roughly corresponds to ?of,? but it has
1455
much broader usage. These studies obtained rea-
sonable results, but the coverage of the acquired
knowledge was not sufficient. Recently, a num-
ber of researchers argued for using the Web as a
source of lexical knowledge, and theWeb has been
shown to be a useful resource for anaphora resolu-
tion (Bunescu, 2003; Markert et al, 2003; Poesio
et al, 2004).
Hence, in this study, we acquire the lexi-
cal knowledge for associative anaphora resolution
from ?N
m
noN
h
? phrases in the Web by using the
method described in (Sasano et al, 2004). We pro-
posed a method for acquiring such lexical knowl-
edge, called nominal case frames (NCFs), using
an ordinary language dictionary and ?N
m
no N
h
?
phrases, and constructed NCFs from newspaper
corpora. In this study, we aim to acquire a suffi-
cient amount of lexical knowledge by constructing
NCFs from the Web.
As for associative anaphora resolution itself, we
propose an integrated probabilistic model for zero
anaphora and associative anaphora resolution, in
which associative anaphora is regarded as a kind
of zero anaphora and resolved by using the same
model as zero anaphora. Our model assumes zero
pronouns that represent indispensable entities of
target noun phrases, which are called zero adnom-
inal in (Yamura-Takei, 2003), and conducts zero
pronoun resolution.
Let us consider the associative anaphoric re-
lation between ?Prius? and ?kakaku? (price).
Although ?kakaku? itself is considered as the
anaphor from a conventional point of view (3a),
our model assumes a zero pronoun ? and consid-
ers it as the anaphor (3b).
(3) a. Prius - kakaku (price)
[antecedent: Prius, anaphor: kakaku (price)]
b. Prius - (?-no) kakaku (price (of ?))
[antecedent: Prius, anaphor: ?]
The point of this study is three-fold: the ac-
quisition of the lexical knowledge for associative
anaphora resolution from the Web, the application
of zero anaphora resolution model to associative
anaphora resolution, and the integrated resolution
of zero anaphora and associative anaphora.
2 Construction of Nominal Case Frames
Most nouns have their indispensable entities:
?price? is a price of some goods or service, ?roof?
is a roof of some building, and ?coach? is a coach
of some sports. The relation between a noun and
its indispensable entities is parallel to that between
a verb and its arguments or obligatory cases. In
this paper, we call indispensable entities of nouns
obligatory cases. Note that, obligatory does not
mean grammatically obligatory but obligatory to
interpret the meaning of the noun. Associative
anaphora resolution needs comprehensive infor-
mation of obligatory cases of nouns. Nominal case
frames (NCFs) describe such information, and we
construct them from the Web.
2.1 Automatic Construction of NCFs
First, we briefly introduce our method for con-
structing NCFs from raw corpora proposed in
(Sasano et al, 2004).
Whereas verbal case frame construction uses ar-
guments of each verb (Kawahara and Kurohashi,
2002), nominal case frame construction basically
uses adnominal constituents of each noun. How-
ever, while the meaning of a verbal argument can
be distinguished by the postposition, such as ?ga?
(nominative), ?wo? (accusative), and ?ni? (dative),
the meaning of an adnominal constituent can not
be distinguished easily, because most adnominal
constituents appear with the same postposition
?no? (of). Thus, we first conduct a semantic anal-
ysis of adnominal constituents, and then construct
NCFs using the results as follows:
1. Collect syntactically unambiguous noun
phrases ?N
m
no N
h
? from the automatic re-
sulting parses of large corpora.
2. Analyze the relation between N
m
and N
h
by Kurohashi and Sakai?s method (1999) that
exploits an ordinary language dictionary.
3. Depending on the results, classify N
m
, and
obtain preliminary case slots for N
h
.
4. Merge case slots if two preliminary case slots
of N
h
are similar.
5. Consider frequent case slots as obligatory
cases of N
h
. The frequency thresholds are
varied according to semantic analyses.
6. For each meaning of N
h
, collect case slots
and construct case frames.
The point of this method is the integrated use of
an ordinary dictionary and example phrases from
1456
Table 1: Examples of constructed nominal case frames.
Case slot Examples with freq Generalized examples with rate
Definition: the amount of money you have to pay for something.
kakaku (1) [something] sh?ohin(goods):9289, seihin(product):2520, [CT:ARTIFACT]:0.93, ? ? ?
(price) buhin(part):341, yunyuhin(importation):232, ? ? ?
Definition: the structure that covers or forms the top of a building etc.
yane (1) [building] ie(house):2505, kuruma(car):1565, koya(hut):895, [CT:FACILITY]:0.44,
(roof) tatemono(building):883,minka(private house):679, ? ? ? [CT:VEHICLE]:0.13,? ? ?
Definition: the elected leader of the government in a country that has a parliament.
shusho (1) [country] nihon(Japan):2355, kuni(country):272, [NE:LOCATION]:0.82,
(prime minister) doitsu(Germany):157, ch?ugoku(China):130, ? ? ? [CT:VEHICLE]:0.13,? ? ?
Definition: a girl or woman who has the same parents as you.
imouto (1) <relationship> watashi(me):3385, ore(me):1188, boku(me):898, [CT:PERSON]:0.74,
(sister) jibun(oneself):341, tomodachi(friend):537, ? ? ? [NE:PERSON]:0.22, ? ? ?
Definition: a stick or handle on a machine.
reb?a(1) [machine] bu?reki(brake):122, sokketo(sochet):67, [CT:ARTIFACT]:0.61,
(lever) waip?a(wiper):54, souchi(device):52,? ? ? [CT:VEHICLE]:0.04, ? ? ?
Definition: the liver of an animal, used as food.
reb?a(2) [animal] niwatori(chicken):153, buta(pig):153, [CT:ANIMAL]:0.98, ? ? ?
(liver) ushi(cattle):62, doubutsu(animal):25,? ? ?
Definition: someone who takes part in a sport.
senshu(1) [sport] yaky?u(baseball):1252, rir?e(relay):736, [CT:ABSTRACTION]:0.56, ? ? ?
(player) ky?ogi(competition):430, sakk?a(soccer):394, ? ? ?
<affiliation> ch??mu(team):4409, nihon(Japan):3222, [NE:LOCATION]:0.33,
reddu(Reds):771, kankoku(Korea):644,r??gu(league) ? ? ? [CT:ORGANIZATION]:0.30, ? ? ?
* ?[]? and ?<>? denote dictionary-based and semantic feature-based analysis respectively. For details see (Sasano et al, 2004).
large corpora. Dictionary definition sentences are
an informative resource to recognize obligatory
cases of nouns. However, it is difficult to resolve
associative anaphora by using a dictionary as it is,
because all nouns in a definition sentence are not
an obligatory case, and only the frequency infor-
mation of noun phrases tells us which is the oblig-
atory case. On the other hand, a simple method
that just collects and clusters ?N
m
no N
h
? phrases
based on some similarity measure of nouns cannot
construct comprehensive nominal case frames, be-
cause of polysemy and multiple obligatory cases.
For details see (Sasano et al, 2004).
It is desirable to use a probability distribution
for deciding whether a case slot is obligatory or
not. However, it is difficult to estimate a prob-
ability distribution, since we construct nominal
case frames not by using the examples of associa-
tive anaphora itself but by using the examples of
noun phrases ?N
m
no N
h
? (N
h
of N
m
). We use
such noun phrases because indispensable entities
of noun ?N
h
? often appear as ?N
m
.? However, we
can say neither frequently appeared ?N
m
? is an in-
dispensable entity of ?N
h
.? nor an indispensable
entity frequently appears as ?N
m
.? For example,
the name of a country is considered as an indis-
pensable entity of ?shusho? (prime minister), but
does not frequently appear as ?N
m
.?
2
Thus, it is
difficult to estimate a probability distribution and
we use a hard decision.
2.2 NCF Construction from the Web
We constructed nominal case frames from theWeb
Corpus (Kawahara and Kurohashi, 2006), which
comprises 1.6 billion unique Japanese sentences.
In this corpus, there were about 390 million noun
phrases ?N
m
no N
h
,? about 100 million unique
noun phrases, and about 17 million unique head
nouns ?N
h
.? There were about 4.07 million head
nouns that appeared more than 10 times in the cor-
pus, and we used only such head nouns.
The resultant nominal case frames consisted of
about 564,000 nouns including compound nouns.
We show examples of constructed nominal case
frames in Table 1. The average number of case
frames for a noun that has case frames was 1.0031,
and the average number of case slots for a case
frame was 1.0101. However, these statistics dif-
fered with the frequency of the noun. Therefore,
we investigated the statistics of constructed nom-
inal case frames for each group classified by the
frequency of the nouns. Table 2 shows the re-
2
It is because ?the prime minister of Japan? is often men-
tioned by simply ?the prime minister? in Japanese.
1457
Table 2: The statistics of constructed NCFs.
Frequency Proportion # of NCFs # of CSs Coverage
ranking of nouns per noun per NCF
with NCF with NCF
-100 56.0% 1.34 1.07 17.3%
-1000 68.8% 1.17 1.16 25.6%
-10000 51.7% 1.11 1.17 27.0%
-100000 14.8% 1.05 1.13 17.6%
100001- 13.7% 1.0009 1.0053 12.5%
all 13.9% 1.0031 1.0101 100%
Table 3: Evaluation of constructed NCFs.
Precision Recall F-measure
62/70 (0.89) 62/84 (0.74) 0.81
sult. As for the 10,000 most frequently appeared
nouns, which occupied about 70% of all noun ap-
pearances, the average number of case frames for
a noun was 1.11, and the average number of case
slots for a case frame was 1.17.
For evaluating the resultant case frames, we ran-
domly selected 100 nouns from the 10,000 most
frequent nouns, and created gold standard case
frames for these nouns by hand. For each noun,
case frames were given if the noun was considered
to have any indispensable entity, and for each case
frame, obligatory case slots were given manually:
70 case frames were created that had 84 case slots;
56 case frames had only one case slot, the other 14
case frames had two case slots. 30 nouns had no
case frames.
We then evaluated the automatically con-
structed case slots for these selected nouns. The
evaluation result is shown in Table 3: the sys-
tem output 70 case slots, and out of them, 62 case
frames were judged as correct. The F-measure was
0.81. Since the boundary between indispensable
cases and optional cases of a noun is not always
obvious, this score is considered to be reasonable.
2.3 Generalization of Examples
By using nominal case frames constructed from
the Web, sparseness problem was alleviated to
some extent, but still remained. For instance, there
were thousands of named entities (NEs), which
could not be covered intrinsically. To deal with
this sparseness problem, we generalized the exam-
ples of case slots.
First, we used the categories that Japanese mor-
phological analyzer JUMAN
3
adds to common
nouns. In JUMAN, about twenty categories are
defined and tagged to common nouns. For ex-
ample, ?kuruma (car),? ?niwatori (chicken),? and
?tatemono (building)? are tagged as ?VEHICLE,?
?ANIMAL? and ?FACILITY,? respectively. For
each category, we calculated the rate of catego-
rized examples among all case slot examples, and
added it to the case slot as ?[CT:VEHICLE]:0.13.?
We also generalized NEs. We used a com-
mon standard NE definition for Japanese pro-
vided by IREX workshop (1999). We first rec-
ognized NEs in the source corpus by using an
NE recognizer (Sasano and Kurohashi, 2008), and
then constructed NCFs from the NE-recognized
corpus. As well as categories, for each NE
class, we calculated the NE rate among all case
slot examples, and added it to the case slot as
?[NE:PERSON]:0.22.? The generalized examples
are also included in Table 1.
3 Probabilistic Model
In this study, we apply a lexicalized probabilis-
tic model for zero anaphora resolution proposed in
(Sasano et al, 2008) to associative anaphora reso-
lution.
3.1 A Lexicalized Probabilistic Model for
Zero Anaphora Resolution
In English, overt pronouns such as ?she? and
definite noun phrases such as ?the company?
are anaphors that refer to preceding entities (an-
tecedents). On the other hand, in Japanese,
anaphors are often omitted, which are called zero
pronouns, and zero anaphora resolution is one of
the most important techniques for semantic analy-
sis in Japanese.
Here, we introduce our model for zero anaphora
resolution (Sasano et al, 2008). This model first
resolves coreference and identifies discourse enti-
ties; then from the end of each sentence, analyzes
each predicate by the following steps:
1. Select a case frame temporarily.
2. Consider all possible correspondences be-
tween each input argument and a case slot of
the selected case frame.
3. Regard case slots that have no correspon-
dence as zero pronoun candidates.
3
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
1458
4. Consider all possible correspondences be-
tween zero pronoun candidates and existing
entities.
5. For each possible case frame, estimate each
correspondence probabilistically, and select
the most likely case frame and correspon-
dence.
Figure 1 shows an example of correspondences
between case frames and discourse entities.
The probabilistic model gives a probability to
each possible case frame CF and case assignment
CA when target predicate v, input arguments IA
and existing discourse entities ENT are given,
and outputs the case frame and case assignment
that have the highest probability. That is to say,
their model selects the case frame CF
best
and the
case assignment CA
best
that maximize the proba-
bility P (CF,CA|v, IA,ENT ):
(CF
best
, CA
best
)
= argmax
CF,CA
P (CF,CA|v, IA,ENT ) (i)
By decomposing case assignment (CA) into
direct case assignment (DCA) and the indirect
case assignment (ICA) and using several inde-
pendence assumptions, Equation (i) is transformed
into the following equation:
4
(CF
best
, DCA
best
,ICA
best
) =
argmax
CF,DCA,ICA
(
P (CF |v)? P (DCA, IA|CF )
?P (ICA|ENT,CF,DCA)
)
(ii)
Here, P (CF
l
|v) denotes the probability to se-
lect CF
l
when target predicate v is given, and es-
timated by using case structure analysis of large
raw corpora.
P (DCA
k
, IA|CF
l
) denotes the probability to
generate direct case assignment and input argu-
ments when a case frame is given, and estimated
by using case structure analysis of large raw cor-
pora, the frequency of a case slot example in the
automatically constructed verbal case frames, and
the web corpus in which the relation between a
surface case marker and a case slot is manually
annotated.
P (ICA
k
|ENT,CF
l
, DCA
k
) denotes the
probability to generate indirect case assignment
when existing discourse entities, a case frame and
4
For details see (Sasano et al, 2008).
Toyota-wa
Prius-wo
hybrid car
hatsubai.
kaigai-demo
hanbai-shiteiru.
1997-nen
2000-nen-karawa
{Toyota, ?
?
}
{hybrid car, Prius, ?2 }
{kaigai}
Entities
(overseas)
(launch)
(sell)
hatsubai (launch)ganominative company, SONY, firm, ? [NE:ORGANIZATION] 0.15, ?
woaccusative product, CD, model, car,  ?[CT:ARTIFACT] 0.40, ?de      locative area, shop, world, Japan, ?[CT:FACILITY] 0.13, ?
hanbai (sell)ganominative company, Microsoft, ? [NE:ORGANIZATION] 0.16, ?
woaccusative goods, product, ticket, ? [CT:ARTIFACT] 0.40, ?
nidative customer, company, user, ? [CT:PERSON] 0.28, ?de      locative shop, bookstore, site, ? [CT:FACILITY] 0.40, ?:direct case assignment:indirect case assignment (zero anaphora)
Verbal case framesInput sentences
Toyota launched the hybrid car Prius in 1997. ?
?
started selling ?2 overseas in 2000.
{1997-nen}
{2000-nen}
Figure 1: An example of correspondences be-
tween verbal case frames and discourse entities.
direct case assignments are given, and estimated
by using several preferences on the relation
between a zero pronoun and an antecedent, such
as a lexical preference, a surface case preferences,
and a locational preference.
For example, the lexical preference represents
how likely an entity that contains n
j
m
as a con-
tent part is considered to be an antecedent and is
estimated by the following equation.
P (n
j
m
|CF
l
, s
j
, A
?
(s
j
)=1)
P (n
j
m
)
(iii)
where, the function A
?
(s
j
) returns 1 if a case slot
s
j
is filled with an antecedent of a zero pronoun;
otherwise 0. P (n
j
|CF
l
, s
j
, A
?
(s
j
) = 1) is calcu-
lated by using case frames and denotes the proba-
bility of generating a content part n
j
of a zero pro-
noun, when a case frame and a case slot are given
and the case slot is filled with an antecedent of a
zero pronoun.
3.2 Extension to Associative Anaphora
Resolution
We then extend this probabilistic model to associa-
tive anaphora resolution. In this model, associative
anaphora is regarded as a kind of zero anaphora,
that is, the relation between a noun and its oblig-
atory cases is considered to be parallel to that be-
tween a verb and its arguments. Omitted obliga-
tory cases are considered to be zero pronouns and
resolved by the same process as zero anaphora res-
olution.
We conduct associative anaphora resolution for
only non-coreferent noun phrases. This is because
most of the relationships between coreferent noun
1459
Toyota-wa
Prius-wohybrid car
kakaku-wa
215-man-yen-datta.
1997-nen
Hatsubai-tosho
{Toyota, ?
?
}
{hybrid car, Prius, ?2 }
Entities
(price)
(ten thousands)
kakaku (price)
something goods, product, part, importation, ? [CT:ARTIFCAT] 0.40, ?
Nominal case framesInput sentences
{1997-nen}
{215-man-yen}
{kaigai}
Toyota launched the hybrid car Prius ???. The initial price of ?2 was 21.5 million yen.
:indirect case assignment (associative anaphora)(initial)
Figure 2: An example of correspondences be-
tween a nominal case frame and discourse entities.
phrases and its obligatory entities are easy to rec-
ognize by following up the coreference chains.
For example, the second appearance of ?the roof?
in (4) means ?the roof of the house,? and it is
easy to recognize by looking the first appearance
of ?the roof.?
(4) I saw the roof of the house. The roof was
painted dark green.
While verbal case frames describe both obliga-
tory and optional cases, nominal case frames de-
scribe only obligatory cases. Therefore, we con-
sider all case slots of nominal case frames as the
target of associative anaphora resolution.
Let us consider following example:
(5) Toyota-wa 1997-nen hybrid car Prius-wo
year
hatsubai. 2000-nen-kara-wa kaigai-demo
launched year overseas
hanbai-shiteiru. Hatsubai tosho,
selling initial
(?-no) kakaku-wa 215-man yen-datta.
price ten thousands
(Toyota
1
launched the hybrid car Prius
2
in 1997. ?
1
started selling ?
2
overseas in 2000. The initial price
of ?
2
was 21.5 million yen.)
?Kakaku? (price) in this example has an omitted
obligatory case ?[something]? as shown in Table
1. Therefore, our model assumes a zero pronoun
and identifies the antecedent from the existing dis-
course entities, such as {Toyota}, {hybrid-car,
Prius},
5
and {kaigai}. Figure 2 shows an exam-
ple of correspondences between the nominal case
frame of ?kakaku? (price) and discourse entities.
In addition, as well as zero anaphora resolution,
we exploit generalized examples to estimate lexi-
cal preference. When one mention of an entity is
5
?Hybrid car? and ?Prius? are in apposition and these two
phrases are considered to refer to the same discourse entity.
tagged any category or recognized as an NE, our
model also uses the category or the NE class as the
content part of the entity. Specifically, for estimat-
ing Equation (iii), our model also calculates:
P (NE :ARTIFACT |kakaku(1), no, A
?
(no)=1)
P (NE :ARTIFACT )
besides:
P (Prius|kakaku(1), no, A
?
(no) = 1)
P (Prius)
and uses the geometric mean of them.
3.3 Salience Score Filtering
Previous work has reported the usefulness of
salience for anaphora resolution (Lappin and Le-
ass, 1994; Mitkov et al, 2002). In order to con-
sider the salience of a discourse entity, we intro-
duce the concept of salience score, which is calcu-
lated by the following set of simple rules, and only
consider the entities that have the salience score no
less than 1 as candidate antecedents of an associa-
tive anaphor.
? +2 : mentioned with topical marker ?wa,? or
at the end of a sentence.
? +1 : mentioned without topical marker ?wa.?
? +1 : assigned to a zero pronoun.
? ?? : beginning of each sentence.
We call ? a decay rate. If ? ? 1, we do not
filter out any entities. If ? = 0, we only consider
the entities that appears in the same sentence as
candidate antecedents. For example, we consider
the salience score of the discourse entity {hybrid-
car, Prius} in the example (5) when using ?=0.6.
In the first sentence, since {hybrid-car, Prius} is
mentioned twice, the salience score is 2.0. At the
beginning of the second sentence it becomes 1.2,
and after the zero anaphora resolution of ?hanbai?
it becomes 2.2. At the beginning of the third sen-
tence it becomes 1.32.
Note that, this is an ideal case. Practically, some
zero pronouns are not detected and some pronouns
are assigned wrong antecedent; thus the salience
score varies according to the preceding analysis.
In addition, the salience score also depends on
whether we resolve only associative anaphora or
resolve associative anaphora simultaneously with
zero anaphora. If zero pronoun resolution is not
1460
conducted, zero pronouns that represent omitted
cases of verbs are not considered.
For example, in case of {hybrid-car, Prius}
with ? = 0.6, if zero anaphora resolution is not
conducted, the salience score at the beginning of
the third sentence becomes 0.72, because the zero
anaphora resolution of ?hanbai? is not considered;
and thus {hybrid-car, Prius} is not considered as
an antecedent candidate.
In order to recognize discourse structure more
properly, our model basically resolves associa-
tive anaphora simultaneously with zero anaphora,
and aims to consider zero pronouns that represent
omitted cases of verbs.
3.4 Summary of Our model
Our model is summarized as follows:
1. Parse an input text using the Japanese parser
KNP
6
and recognize NEs.
2. Resolve coreference, and link each mention
to an entity or create a new entity.
3. From the end of each sentence, zero anaphora
and associative anaphora resolution is con-
ducted for each verb and non-coreferent noun
by the following steps:
(a) Select a case frame temporarily.
(b) Consider all possible correspondences
between each input argument and a case
slot of the selected case frame.
(c) Regard case slots that have no corre-
spondence as zero pronoun candidates.
(d) Consider all possible correspondences
between zero pronoun candidates and
existing entities that has a salience score
no less than 1.0.
(e) Estimate each correspondence proba-
bilistically, and select the most likely
case frame and a correspondence.
4 Experiments
4.1 Setting
We created an anaphoric relation-tagged corpus
consisting of 186 web documents (979 sentences),
in which all predicate-argument relations and re-
lations between nouns were manually tagged. We
show some examples:
6
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
(6) Toyota-wa 1997-nen Prius-wo hatsubai.
year launch
2000-nen-kara-wa kaigai-demo hanbai.
year overseas sell
(Toyota launched Prius in 1997.
?
1
started selling ?
2
overseas in 2000.)
TAG: hatsubai ? ga:Toyota, wo:Prius,
(NOM) (ACC)
hanbai ? ga:Toyota, wo:Prius
(NOM) (ACC)
For the predicate ?hatsubai? (launch), ?Toyota?
is tagged as ga (nominative) case and ?Prius? is
tagged as wo (accusative) case. For the predicate
?hanbai? (sell), ?Toyota? is tagged as omitted ga
(nominative) case and ?Prius? is tagged as omit-
ted wo (accusative) case, which are indicated in
bold, and such omitted cases are the target of zero
anaphora resolution.
As for relations between nouns, both overt and
implicit relations are tagged with the Japanese
case marker ?no? (adnominal). In addition, rela-
tions between nouns are classified into three cate-
gories: indispensable, possible, and adjunct. Since
it is not always obvious whether the relations are
indispensable or not, borderline relations between
indispensable and adjunct are tagged possible. We
consider only the implicit relations that are tagged
indispensable as the target of associative anaphora
resolution.
(7) Ken-wa imouto-to yatte-kita.
sister came.
(Ken came with ??s sister.)
TAG: imouto ? no:Ken (indispensable)
(ADN)
(8) K?oen-ni ikuto benchi-ga atta.
park went bench was
(I went to the park. There was a bench in ?.)
TAG: benchi ? no:k?oen (possible)
(ADN)
We used 62 documents for testing and used the
other 124 documents for calculating several prob-
abilities. In the 62 test documents, 110 associa-
tive anaphoric relations were tagged. Each param-
eter for the proposed model was estimated using
maximum likelihood from raw corpora, the tagged
corpus, and case frames. As verbal case frames,
we used the case frames constructed from the Web
corpus comprising 1.6 billion sentences (Sasano et
al., 2009).
In order to concentrate on associative anaphora
resolution, we used the correct morphemes, named
entities, syntactic structures, and coreference re-
1461
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Decay Rate ?
Recall
F-measure
.427
Precision
Figure 3: Experimental results of associative
anaphora resolution on several salience decay
rates ?.
lations that were annotated by hand. Since cor-
rect coreference relations were given, the number
of created entities was the same between the gold
standard data and the system output because zero
anaphora and associative anaphora resolution did
not create new entities.
4.2 Results
Figure 3 shows the experimental results of asso-
ciative anaphora resolution, in which we used gen-
eralized examples, resolved zero anaphora auto-
matically, and varied the decay rate ? introduced
in Section 3.3 from 0 to 1. When we used the de-
cay rates smaller than 0.5, the recall score wors-
ened clearly. On the other hand, although we ex-
pected to obtain higher precision with small decay
rate, the highest precision was achieved by the de-
cay rate 0.5. Consequently, we obtained the high-
est F-measure of 0.427 with the decay rate 0.5. In
the following experiments, we fixed the decay rate
0.5.
We utilized two baseline models for demon-
strating the effectiveness of our approach: a ran-
dom model and a salience-based model. The ran-
dom model selects a case frame and its correspon-
dence randomly from all possible case frames and
correspondences. The salience-based model se-
lects a case frame and its correspondence that as-
sign a zero pronoun candidate the existing entity
that have highest salience score. In addition, in or-
der to confirm the effectiveness of generalized ex-
amples of NCFs, we conducted experiments with-
out using generalized examples. Table 4 shows
the experimental results. We can confirm that our
proposed model outperforms two baseline mod-
els. Without using any generalized examples, the
Table 4: Experimental results of associative
anaphora resolution with two baseline models and
our model with/without generalized examples.
Model Recall Precision F-measure
Random* 0.148 0.035 0.056
(16.3/110) (16.3/467.5)
Salience- 0.400 0.135 0.202
based (44/110) (44/325)
Proposed
CT NE
0.318 0.257 0.285
(35/110) (35/136)
?
0.345 0.268 0.302
(38/110) (38/142)
?
0.464 0.333 0.388
(51/110) (51/153)
? ?
0.518 0.363 0.427
(57/110) (57/157)
CT: Using examples generalized by categories.
NE: Using examples generalized by named entities.
* The average of 10 trials is shown.
F-measure was about 0.14 lower than the method
using generalized examples, and we can also con-
firm the effectiveness of the generalized examples.
While generalization of categories much improved
the F-measure, generalization of NEs contributed
little. This is because the NE rate was smaller than
the common noun rate, and so the effect was lim-
ited. This tendency was also seen in zero anaphora
resolution (Sasano et al, 2008).
In order to investigate the effects of zero
anaphora resolution, we tested our model under
three conditions: without zero anaphora resolu-
tion (no resolution), with zero anaphora resolution
(automatically resolved), and with using correct
zero anaphora relations that are manually tagged
(manually identified). The performance of auto-
matic zero anaphora resolution resulted in a recall
of 0.353, a precision of 0.375, and an F-measure of
0.364. Table 5 shows the experimental results. To
resolve associative anaphora simultaneously with
zero anaphora improved F-measure by 0.072; us-
ing correct zero anaphora relations improved F-
measure by 0.103. We can confirm that the per-
formance of associative anaphora resolution is im-
proved by considering zero anaphora.
Note that, strictly speaking, these comparisons
are not fair because we set the decay rate ? to max-
imize the performance when using generalized ex-
amples and resolving zero anaphora automatically.
However, these tendencies described above were
also seen with other decay rates.
1462
Table 5: The effects of zero anaphora resolution.
Zero anaphora Recall Precision F-measure
No resolution 0.373 0.339 0.355
(41/110) (41/121)
Automatically 0.518 0.363 0.427
resolved (57/110) (57/157)
Manually 0.573 0.382 0.458
identified (63/110) (63/165)
4.3 Discussion
By using generalized examples and resolving
simultaneously with zero anaphora, our model
achieved a recall of 0.518 (57/110), but there were
still 53 associative anaphoric relations that were
not recognized. Table 6 shows the causes of them.
22 false negatives were caused by salience score
filtering. Note that, it does not mean that these 22
associative anaphoric relations were always recog-
nized correctly if the correct antecedents were not
filtered by salience score.
Case frame sparseness caused only 5 false neg-
atives. Considering that the recall of nominal case
frames was 74% as shown in Table 3, this seems to
be too few. This is because we do not considered
the relations that tagged possible, and only con-
sidered obviously indispensable relations. From
this result, we can say that coverage of nominal
case frames for nouns that have obviously indis-
pensable entities is much higher than 74%, which
is considered to achieve a coverage of about 95%
(105/110).
4.4 Comparison with previous work
Murata et al (1999) proposed a method of utiliz-
ing ?N
m
no N
h
? phrases for associative anaphora
resolution.
7
They basically used all ?N
m
no N
h
?
phrases from corpora as a lexical knowledge, and
used rule-based approach. They obtained a recall
of 0.63 and a precision of 0.68 by using exam-
ples of ?X no Y? (Y of X), a recall of 0.71 and a
precision of 0.82 by assuming ideal nominal case
frames. One reason of such high performance may
be that they considered referential properties of
noun phrases, such as generic, indefinite, and defi-
nite, while our model does not. We can also say
that their experiments were conducted on small
and supposedly easy corpora. Half of their corpora
7
Murata et al (1999) and we (Sasano et al, 2004) used
the terminology indirect anaphora, but concerned with the
same phenomena as we concerned with in this paper.
Table 6: Causes of false negatives.
Causes Num
Filtered by salience score 22 (15)
Judge as non-anaphoric 13 (14)
Select false antecedents 13 (13)
Case frame sparseness 5 (5)
Total 53 (47)
*?()? denotes the number of causes when
using correct zero anaphora tags.
were occupied by fairy tale, against which domain
specific rules are considered to be effective.
We proposed a rule-based approach for asso-
ciative anaphora resolution based on automati-
cally acquired nominal case frames (Sasano et al,
2004).
7
We obtained a recall of 0.633 and a pre-
cision of 0.508 against news paper articles. How-
ever, we regarded some additional relations that
can be interpreted by considering coreference re-
lations as associative anaphoric relations.
(9) Chechen Ky?owakoku-no shuto-ni ...
Chechen Republic capital
... shuto seiatsu-no saishu dankai-ni ...
capital conquer last stage
(... to the capital of Chechen Republic ... in the last
stage to conquer the capital ...)
For example, although the second mention of
?shuto? (capital) in example (9) means ?Chechen
Ky?owakoku-no shuto? (the capital of Chechen Re-
public), it can be interpreted by recognizing the
coreference relation between the first and second
mentions of ?shuto? (capital). Therefore, as men-
tioned in Section 3.2, we do not consider such re-
lations as associative anaphora in this study; we
included such relations as associative anaphora in
(Sasano et al, 2004). The relatively high score is
caused by this criterion.
5 Conclusion
In this paper, we proposed a probabilistic model
for associative anaphora resolution. Our model
regards associative anaphora as a kind of zero
anaphora and resolves it in the same manner as
zero anaphora resolution that uses automatically
acquired case frames. We also showed that the
performance of associative anaphora resolution
can be improved by resolving it simultaneously
with zero anaphora. As future work, we plan to
consider referential properties of noun phrases in
associative anaphora resolution.
1463
References
Razvan Bunescu. 2003. Associative anaphora res-
olution: A web-based approach. In Proc. of
EACL?03: Workshop on The Computational Treat-
ment of Anaphora, pages 47?52.
Herbert H Clark. 1975. Bridging. In Proc. of the Con-
ference on Theoretical Issues in Natural Language
Processing, pages 169?174.
Caroline Gasperin and Ted Briscoe. 2008. Statistical
anaphora resolution in biomedical texts. In Proc. of
COLING?08, pages 257?264.
Caroline Gasperin and Renata Vieira. 2004. Using
word similarity lists for resolving indirect anaphora.
In Proc. of ACL?04: Workshop on Reference Resolu-
tion and its Applications, pages 40?46.
Udo Hahn, Michael Strube, and Katja Markert. 1996.
Bridging textual ellipsis. In Proc. of COLING?96,
pages 496?501.
John A. Hawkins. 1978. Definiteness and indefinite-
ness: a study in reference and grammaticality pre-
diction. Croom Helm Ltd.
IREX Committee, editor. 1999. Proc. of the IREX
Workshop.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertil-
ization of case frame dictionary for robust Japanese
case analysis. In Proc. of COLING?02, pages 425?
431.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC?06,
pages 1344?1347.
Sadao Kurohashi and Yasuyuki Sakai. 1999. Seman-
tic analysis of Japanese noun phrases: A new ap-
proach to dictionary-based understanding. In Proc.
of ACL?99, pages 481?488.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?562.
Katja Markert, Malvina Nissim, and Natalia N Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proc. of EACL?03: Workshop on the
Computational Treatment of Anaphora, pages 39?
46.
Josef Meyer and Robert Dale. 2002. Using the Word-
Net hierarchy for associative anaphora resolution. In
Proc. of SemaNet?02: Building and Using Semantic
Networks.
Ruslan Mitkov, Richard Evans, and Constantin Or?asan.
2002. A new, fully automatic version of Mitkov?s
knowledge-poor pronoun resolution method. In
Proc. of CICLing?02.
Natalia N Modjeska. 2002. Lexical and grammati-
cal role constraints in resolution other-anaphora. In
Proc. of DAARC?02.
Masaki Murata, Hitoshi Isahara, and Makoto Nagao.
1999. Resolution of indirect anaphora in Japanese
sentences using examples ?X no Y?(Y of X). In
Proc. of ACL?99: Workshop on Coreference and Its
Applications.
Massimo Poesio, Tomonori Ishikawa, Sabine Schulte
im Walde, and Renata Vieira. 2002. Acquiring lex-
ical knowledge for anaphora resolution. In Proc. of
LREC?02, pages 1220?1224.
Massimo Poesio, Pahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to Resolve Bridg-
ing References. In Proc. of ACL?04, pages 143?150.
Ryohei Sasano and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural
language processing. In Proc. of IJCNLP?08, pages
607?612.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proc. of COLING?04, pages 1201?
1207.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proc. of
COLING?08, pages 769?776.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2009. The effect of corpus size on case
frame acquisition for discourse analysis. In Proc.
of NAACL-HLT?09, pages 521?529.
Michael Strube and Udo Hahn. 1999. Functional
centering ? grounding referential coherence in in-
formation structure. Computational Linguistics,
25(3):309?344.
Renata Vieira and Massimo Poesio. 2000. An empir-
ically based system for processing definite descrip-
tions. Computational Linguistics, 26(4):539?592.
Renata Vieira, Eckhard Bick, Jorge Coelho, Vinicius
Muller, Sandra Collovini, Jose Souza, and Lucia
Rino. 2006. Semantic tagging for resolution of indi-
rect anaphora. In Proc. of the 7th SIGdial Workshop
on Discourse and Dialogue, pages 76?79.
Mitsuko Yamura-Takei. 2003. Approaches to zero ad-
nominal recognition. In Proc. of ACL?03: Student
Research Workshop, pages 87?94.
1464
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 166?174,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
An Alignment Algorithm using Belief Propagation and a Structure-Based
Distortion Model
Fabien Cromie`res
Graduate school of informatics
Kyoto University
Kyoto, Japan
fabien@nlp.kuee.kyoto-u.ac.jp
Sadao Kurohashi
Graduate school of informatics
Kyoto University
Kyoto, Japan
kuro@i.kyoto-u.ac.jp
Abstract
In this paper, we first demonstrate the in-
terest of the Loopy Belief Propagation al-
gorithm to train and use a simple align-
ment model where the expected marginal
values needed for an efficient EM-training
are not easily computable. We then im-
prove this model with a distortion model
based on structure conservation.
1 Introduction and Related Work
Automatic word alignment of parallel corpora is
an important step for data-oriented Machine trans-
lation (whether Statistical or Example-Based) as
well as for automatic lexicon acquisition. Many
algorithms have been proposed in the last twenty
years to tackle this problem. One of the most suc-
cessfull alignment procedure so far seems to be
the so-called ?IBM model 4? described in (Brown
et al, 1993). It involves a very complex distor-
tion model (here and in subsequent usages ?dis-
tortion? will be a generic term for the reordering
of the words occurring in the translation process)
with many parameters that make it very complex
to train.
By contrast, the first alignment model we are
going to propose is fairly simple. But this sim-
plicity will allow us to try and experiment differ-
ent ideas for making a better use of the sentence
structures in the alignment process. This model
(and even more so its subsequents variations), al-
though simple, do not have a computationally ef-
ficient procedure for an exact EM-based training.
However, we will give some theoretical and empir-
ical evidences that Loopy Belief Propagation can
give us a good approximation procedure.
Although we do not have the space to review the
many alignment systems that have already been
proposed, we will shortly refer to works that share
some similarities with our approach. In particu-
lar, the first alignment model we will present has
already been described in (Melamed, 2000). We
differ however in the training and decoding pro-
cedure we propose. The problem of making use
of syntactic trees for alignment (and translation),
which is the object of our second alignment model
has already received some attention, notably by
(Yamada and Knight, 2001) and (Gildea, 2003) .
2 Factor Graphs and Belief Propagation
In this paper, we will make several use of Fac-
tor Graphs. A Factor Graph is a graphical
model, much like a Bayesian Network. The three
most common types of graphical models (Factor
Graphs, Bayesian Network and Markov Network)
share the same purpose: intuitively, they allow to
represent the dependencies among random vari-
ables; mathematically, they represent a factoriza-
tion of the joint probability of these variables.
Formally, a factor graph is a bipartite graph with
2 kinds of nodes. On one side, the Variable Nodes
(abbreviated as V-Node from here on), and on the
other side, the Factor Nodes (abbreviated as F-
Node). If a Factor Graph represents a given joint
distribution, there will be one V-Node for every
random variable in this joint distribution. Each F-
Node is associated with a function of the V-Nodes
to which it is connected (more precisely, a func-
tion of the values of the random variables associ-
ated with the V-Nodes, but for brevity, we will fre-
quently mix the notions of V-Node, Random Vari-
ables and their values). The joint distribution is
then the product of these functions (and of a nor-
malizing constant). Therefore, each F-Node actu-
ally represent a factor in the factorization of the
joint distribution.
As a short example, let us consider a prob-
lem classically used to introduce Bayesian Net-
work. We want to model the joint probability of
the Weather(W) being sunny or rainy, the Sprin-
kle(S) being on or off, and the Lawn(L) being
wet or dry. Figure 1 show the dependencies of
166
Figure 1: A classical example
the variables represented with a Factor Graph and
with a Bayesian Network. Mathematically, the
Bayesian Network imply that the joint probabil-
ity has the following factorization: P (W,L, S) =
P (W ) ? P (S|W ) ? P (L|W,S). The Factor Graph
imply there exist two functions ?1 and ?2 as well
as a normalization constant C such that we have
the factorization: P (W,L, S) = C ? ?2(W,S) ?
?1(L,W,S). If we set C = 1, ?2(W,S) =
P (W ) ? P (S|W ) and ?1(L,W,S) = P (L|W,S),
the Factor Graph express exactly the same factor-
ization as the Bayesian Network.
A reason to use Graphical Models is that we can
use with them an algorithm called Belief Propa-
gation (abbreviated as BP from here on) (Pearl,
1988). The BP algorithm comes in two flavors:
sum-product BP and max-product BP. Each one
respectively solve two problems that arise often
(and are often intractable) in the use of a proba-
bilistic model: ?what are the marginal probabili-
ties of each individual variable?? and ?what is the
set of values with the highest probability??. More
precisely, the BP algorithm will give the correct
answer to these questions if the graph represent-
ing the distribution is a forest. If it is not the case,
the BP algorithm is not even guaranteed to con-
verge. It has been shown, however, that the BP al-
gorithm do converge in many practical cases, and
that the results it produces are often surprisingly
good approximations (see, for example, (Murphy
et al, 1999) or (Weiss and Freeman, 2001) ).
(Yedidia et al, 2003) gives a very good presen-
tation of the sum-product BP algorithm, as well as
some theoretical justifications for its success. We
will just give an outline of the algorithm. The BP
algorithm is a message-passing algorithm. Mes-
sages are sent during several iterations until con-
vergence. At each iteration, each V-Node sends
to its neighboring F-Nodes a message represent-
ing an estimation of its own marginal values. The
message sent by the V-Node Vi to the F-Node Fj
estimating the marginal probability of Vi to take
the value x is :
mV i?Fj(x) =
?
Fk?N(V i)\Fj
mFk?V i(x)
(N(Vi) represent the set of the neighbours of Vi)
Also, every F-Node send a message to its neigh-
boring V-Nodes that represent its estimates of the
marginal values of the V-Node:
mFj?V i(x) =
?
v1,...,vn
?j(v1, .., x, .., vn)?
?
?
V k?N(Fj)\V i
mV k?Fj(vk)
At any point, the belief of a V-Node V i is given
by
bi(x) =
?
Fk?N(V i)
mFk?V i(x)
, bi being normalized so that
?
x bi(x) = 1. The
belief bi(x) is expected to converge to the marginal
probability (or an approximation of it) of Vi taking
the value x .
An interesting point to note is that each message
can be ?scaled? (that is, multiplied by a constant)
by any factor at any point without changing the re-
sult of the algorithm. This is very useful both for
preventing overflow and underflow during compu-
tation, and also sometimes for simplifying the al-
gorithm (we will use this in section 3.2). Also,
damping schemes such as the ones proposed in
(Murphy et al, 1999) or (Heskes, 2003) are use-
ful for decreasing the cases of non-convergence.
As for the max-product BP, it is best explained
as ?sum-product BP where each sum is replaced
by a maximization?.
3 The monolink model
We are now going to present a simple alignment
model that will serve both to illustrate the effi-
ciency of the BP algorithm and as basis for fur-
ther improvement. As previously mentioned, this
model is mostly identical to one already proposed
in (Melamed, 2000). The training and decoding
procedures we propose are however different.
3.1 Description
Following the usual convention, we will designate
the two sides of a sentence pair as French and En-
glish. A sentence pair will be noted (e, f). ei rep-
resents the word at position i in e.
167
In this first simple model, we will pay little at-
tention to the structure of the sentence pair we
want to align. Actually, each sentence will be re-
duced to a bag of words.
Intuitively, the two sides of a sentence pair ex-
press the same set of meanings. What we want to
do in the alignment process is find the parts of the
sentences that originate from the same meaning.
We will suppose here that each meaning generate
at most one word on each side, and we will name
concept the pair of words generated by a mean-
ing. It is possible for a meaning to be expressed
in only one side of the sentence pair. In that case,
we will have a ?one-sided? concept consisting of
only one word. In this view, a sentence pair ap-
pears ?superficially? as a pair of bag of words, but
the bag of words are themselves the visible part of
an underlying bag of concepts.
We propose a simple generative model to de-
scribe the generation of a sentence pair (or rather,
its underlying bag of concepts):
? First, an integer n, representing the number
of concepts of the sentence is drawn from a
distribution Psize
? Then, n concepts are drawn independently
from a distribution Pconcept
The probability of a bag of concepts C is then:
P (C) = Psize(|C|)
?
(w1,w2)?C
Pconcept((w1, w2))
We can alternatively represent a bag of concepts
as a pair of sentence (e, f), plus an alignment a.
a is a set of links, a link being represented as a
pair of positions in each side of the sentence pair
(the special position -1 indicating the empty side
of a one-sided concept). This alternative represen-
tation has the advantage of better separating what
is observed (the sentence pair) and what is hidden
(the alignment). It is not a strictly equivalent rep-
resentation (it also contains information about the
word positions) but this will not be relevant here.
The joint distribution of e,f and a is then:
P (e, f, a) = Psize(|a|)
?
(i,j)?a
Pconcept(ei, fj)
(1)
This model only take into consideration one-
to-one alignments. Therefore, from now on, we
will call this model ?monolink?. Considering
only one-to-one alignments can be seen as a lim-
itation compared to others models that can of-
ten produce at least one-to-many alignments, but
on the good side, this allow the monolink model
to be nicely symmetric. Additionally, as already
argued in (Melamed, 2000), there are ways to
determine the boundaries of some multi-words
phrases (Melamed, 2002), allowing to treat sev-
eral words as a single token. Alternatively, a pro-
cedure similar to the one described in (Cromieres,
2006), where substrings instead of single words
are aligned (thus considering every segmentation
possible) could be used.
With the monolink model, we want to do two
things: first, we want to find out good values for
the distributions Psize and Pconcept. Then we want
to be able to find the most likely alignment a given
the sentence pair (e, f).
We will consider Psize to be a uniform distribu-
tion over the integers up to a sufficiently big value
(since it is not possible to have a uniform distri-
bution over an infinite discrete set). We will not
need to determine the exact value of Psize . The
assumption that it is uniform is actually enough to
?remove? it of the computations that follow.
In order to determine the Pconcept distribution,
we can use an EM procedure. It is easy to
show that, at every iteration, the EM procedure
will require to set Pconcept(we, wf ) proportional
to the sum of the expected counts of the concept
(we, wf ) over the training corpus. This, in turn,
mean we have to compute the conditional expec-
tation:
E((i, j) ? a|e, f) =
?
a|(i,j)?a
P (a|e, f)
for every sentence pair (e, f). This computation
require a sum over all the possible alignments,
whose numbers grow exponentially with the size
of the sentences. As noted in (Melamed, 2000),
it does not seem possible to compute this expecta-
tion efficiently with dynamic programming tricks
like the one used in the IBM models 1 and 2 (as a
passing remark, these ?tricks? can actually be seen
as instances of the BP algorithm).
We propose to solve this problem by applying
the BP algorithm to a Factor Graph representing
the conditional distribution P (a|e, f). Given a
sentence pair (e, f), we build this graph as fol-
lows.
We create a V-node V ei for every position i in
the English sentence. This V-Node can take for
168
Figure 2: A Factor Graph for the monolink model
in the case of a 2-words English sentence and a 3-
words french sentence (F recij nodes are noted Fri-j)
value any position in the french sentence, or the
special position ?1 (meaning this position is not
aligned, corresponding to a one-sided concept).
We create symmetrically a V-node V fj for every
position in the french sentence.
We have to enforce a ?reciprocal love? condi-
tion: if a V-Node at position i choose a position j
on the opposite side, the opposite V-Node at po-
sition j must choose the position i. This is done
by adding a F-Node F reci,j between every opposite
node V ei and V
f
j , associated with the function:
?reci,j (k, l) =
?
??
??
1 if (i = l and j = k)
or (i 6= l and j 6= k)
0 else
We then connect a ?translation probability? F-
Node F tp.ei to every V-Node V
e
i associated with
the function:
?tp.ei (j) =
{?
Pconcept(ei, fj) if j 6= ?1
Pconcept(ei, ?) if j = ?1
We add symmetrically on the French side F-Nodes
F tp.fj to the V-Nodes V
f
j .
It should be fairly easy to see that such a Factor
Graph represents P (a|e, f). See figure 2 for an
example.
Using the sum-product BP, the beliefs of ev-
ery V-Node V ei to take the value j and of every
node V fj to take the value i should converge to the
marginal expectation E((i, j) ? a|e, f) (or rather,
a hopefully good approximation of it).
We can also use max-product BP on the same
graph to decode the most likely alignment. In the
monolink case, decoding is actually an instance of
the ?assignment problem?, for which efficient al-
gorithms are known. However this will not be the
case for the more complex model of the next sec-
tion. Actually, (Bayati et al, 2005) has recently
proved that max-product BP always give the opti-
mal solution to the assignment problem.
3.2 Efficient BP iterations
Applying naively the BP algorithm would lead us
to a complexity of O(|e|2 ? |f |2) per BP iteration.
While this is not intractable, it could turn out to be
a bit slow. Fortunately, we found it is possible to
reduce this complexity to O(|e| ? |f |) by making
two useful observations.
Let us note meij the resulting message from V
e
i
to V fj (that is the message sent by F
rec
i,j to V
f
j af-
ter it received its own message from V ei ). m
e
ij(x)
has the same value for every x different from i:
meij(x 6= i) =
?
k 6=j
bei (k)
mfji(k)
. We can divide all the
messages meij by m
e
ij(x 6= i), so that m
e
ij(x) = 1
except if x = i; and the same can be done for the
messages coming from the French side mfij . It fol-
lows that meij(x 6= i) =
?
k 6=j b
e
i (k) = 1 ? b
e
i (j)
if the bei are kept normalized. Therefore, at ev-
ery step, we only need to compute meij(j), not
meij(x 6= j).
Hence the following algorithm (meij(j) will be
here abbreviated to meij since it is the only value
of the message we need to compute). We describe
the process for computing the English-side mes-
sages and beliefs (meij and b
e
i ) , but the process
must also be done symmetrically for the French-
side messages and beliefs (mfij and b
f
i ) at every
iteration.
0- Initialize all messages and beliefs with:
me(0)ij = 1 and b
e(0)
i (j) = ?
tp.e
i (j)
Until convergence (or for a set number of itera-
tion):
1- Compute the messages meij : m
e(t+1)
ij =
be(t)i (j)/((1 ? b
e(t)
i (j)) ? m
f(t)
ji )
2- Compute the beliefs bei (j):bi(j)
e(t+1) =
?tp.ei (j) ? m
f(t+1)
ji
3- And then normalize the bi(j)e(t+1) so that?
j bi(j)
e(t+1) = 1.
A similar algorithm can be found for the max-
product BP.
3.3 Experimental Results
We evaluated the monolink algorithm with two
languages pairs: French-English and Japanese-
English.
169
For the English-French Pair, we used 200,000
sentence pairs extracted from the Hansard cor-
pus (Germann, 2001). Evaluation was done with
the scripts and gold standard provided during
the workshop HLT-NAACL 20031 (Mihalcea and
Pedersen, 2003). Null links are not considered for
the evaluation.
For the English-Japanese evaluation, we used
100,000 sentence pairs extracted from a corpus of
English/Japanese news. We used 1000 sentence
pairs extracted from pre-aligned data(Utiyama and
Isahara, 2003) as a gold standard. We segmented
all the Japanese data with the automatic segmenter
Juman (Kurohashi and Nagao, 1994). There is
a caveat to this evaluation, though. The reason
is that the segmentation and alignment scheme
used in our gold standard is not very fine-grained:
mostly, big chunks of the Japanese sentence cover-
ing several words are aligned to big chunks of the
English sentence. For the evaluation, we had to
consider that when two chunks are aligned, there
is a link between every pair of words belonging to
each chunk. A consequence is that our gold stan-
dard will contain a lot more links than it should,
some of them not relevants. This means that the
recall will be largely underestimated and the pre-
cision will be overestimated.
For the BP/EM training, we used 10 BP iter-
ations for each sentences, and 5 global EM iter-
ations. By using a damping scheme for the BP
algorithm, we never observed a problem of non-
convergence (such problems do commonly ap-
pears without damping). With our python/C im-
plementation, training time approximated 1 hour.
But with a better implementation, it should be pos-
sible to reduce this time to something comparable
to the model 1 training time with Giza++.
For the decoding, although the max-product BP
should be the algorithm of choice, we found we
could obtain slightly better results (by between 1
and 2 AER points) by using the sum-product BP,
choosing links with high beliefs, and cutting-off
links with very small beliefs (the cut-off was cho-
sen roughly by manually looking at a few aligned
sentences not used in the evaluation, so as not to
create too much bias).
Due to space constraints, all of the results of this
section and the next one are summarized in two
tables (tables 1 and 2) at the end of this paper.
In order to compare the efficiency of the BP
1http://www.cs.unt.edu/ rada/wpt/
training procedure to a more simple one, we reim-
plemented the Competitive Link Algorithm (ab-
breviated as CLA from here on) that is used in
(Melamed, 2000) to train an identical model. This
algorithm starts with some relatively good esti-
mates found by computing correlation score (we
used the G-test score) between words based on
their number of co-occurrences. A greedy Viterbi
training is then applied to improve this initial
guess. In contrast, our BP/EM training do not need
to compute correlation scores and start the training
with uniform parameters.
We only evaluated the CLA on the
French/English pair. The first iteration of
CLA did improve alignment quality, but subse-
quent ones decreased it. The reported score for
CLA is therefore the one obtained during the best
iteration. The BP/EM training demonstrate a clear
superiority over the CLA here, since it produce
almost 7 points of AER improvement over CLA.
In order to have a comparison with a well-
known and state-of-the-art system, we also used
the GIZA++ program (Och and Ney, 1999) to
align the same data. We tried alignments in both
direction and provide the results for the direction
that gave the best results. The settings used were
the ones used by the training scripts of the Moses
system2, which we assumed to be fairly optimal.
We tried alignment with the default Moses settings
(5 iterations of model 1, 5 of Hmm, 3 of model 3,
3 of model 4) and also tried with increased number
of iterations for each model (up to 10 per model).
We are aware that the score we obtained for
model 4 in English-French is slightly worse than
what is usually reported for a similar size of train-
ing data. At the time of this paper, we did not
have the time to investigate if it is a problem of
non-optimal settings in GIZA++, or if the train-
ing data we used was ?difficult to learn from? (it
is common to extract sentences of moderate length
for the training data but we didn?t, and some sen-
tences of our training corpus do have more than
200 words; also, we did not use any kind of pre-
processing). In any case, Giza++ is compared here
with an algorithm trained on the same data and
with no possibilities for fine-tuning; therefore the
comparison should be fair.
The comparison show that performance-wise,
the monolink algorithm is between the model 2
and the model 3 for English/French. Considering
2http://www.statmt.org/moses/
170
our model has the same number of parameters as
the model 1 (namely, the word translation prob-
abilities, or concept probabilities in our model),
these are pretty good results. Overall, the mono-
link model tend to give better precision and worse
recall than the Giza++ models, which was to be
expected given the different type of alignments
produced (1-to-1 and 1-to-many).
For English/Japanese, monolink is at just about
the level of model 1, but model 1,2 and 3 have very
close performances for this language pair (inter-
estingly, this is different from the English/French
pair). Incidentally, these performances are very
poor. Recall was expected to be low, due to the
previously mentioned problem with the gold stan-
dard. But precision was expected to be better. It
could be the algorithms are confused by the very
fine-grained segmentation produced by Juman.
4 Adding distortion through structure
4.1 Description
While the simple monolink model gives interest-
ing results, it is somehow limited in that it do not
use any model of distortion. We will now try to
add a distortion model; however, rather than di-
rectly modeling the movement of the positions of
the words, as is the case in the IBM models, we
will try to design a distortion model based on the
structures of the sentences. In particular, we are
interested in using the trees produced by syntactic
parsers.
The intuition we want to use is that, much like
there is a kind of ?lexical conservation? in the
translation process, meaning that a word on one
side has usually an equivalent on the other side,
there should also be a kind of ?structure conserva-
tion?, with most structures on one side having an
equivalent on the other.
Before going further, we should precise the idea
of ?structure? we are going to use. As we said, our
prime (but not only) interest will be to make use of
the syntactic trees of the sentences to be aligned.
However these kind of trees come in very different
shapes depending on the language and the type of
parser used (dependency, constituents,. . . ). This is
why we decided the only information we would
keep from a syntactic tree is the set of its sub-
nodes. More specifically, for every sub-node, we
will only consider the set of positions it cover in
the underlying sentence. We will call such a set
of positions a P-set. This simplification will allow
Figure 3: A small syntactic tree and the 3 P-Sets it
generates
us to process dependency trees, constituents trees
and other structures in a uniformized way. Fig-
ure 3 gives an example of a constituents tree and
the P-sets it generates.
According to our intuition about the ?conserva-
tion of structure?, some (not all) of the P-sets on
one side should have an equivalent on the other
side. We can model this in a way similar to how
we represented equivalence between words with
concepts. We postulate that, in addition to a bag of
concepts, sentence pairs are underlaid by a set of
P-concepts. P-concepts being actually pairs of P-
sets (a P-set for each side of the sentence pair). We
also allow the existence of one-sided P-concepts.
In the previous model, sentence pairs where
just bag of words underlaid by a or bag of con-
cepts, and there was no modeling of the position
of the words. P-concepts bring a notion of word
position to the model. Intuitively, there should
be coherency between P-concepts and concepts.
This coherence will come from a compatibility
constraint: if a sentence contains a two-sided P-
concept (PSe, PSf ), and if a word we covered
by PSe come from a two-sided concept (we, wf ),
then wf must be covered by PSf .
Let us describe the model more formally. In
the view of this model, a sentence pair is fully de-
scribed by: e and f (the sentences themselves), a
(the word alignment giving us the underlying bag
of concept), se and sf (the sets of P-sets on each
side of the sentence) and as (the P-set algnment
that give us the underlying set of P-concepts).
e,f ,se,sf are considered to be observed (even if
we will need parsing tools to observe se and sf );
a and as are hidden. The probability of a sentence
pair is given by the joint probability of these vari-
ables :P (e, f, se, sf , a, as). By making some sim-
ple independence assumptions, we can write:
P (a, as, e, f,s
e, sf ) = Pml(a, e, f)?
? P (se, sf |e, f) ? P (as|a, s
e, sf )
171
Pml(a, e, f) is taken to be identical to the mono-
link model (see equation (1)). We are not inter-
ested in P (se, sf |e, f) (parsers will deal with it for
us). In our model, P (as|a, se, sf ) will be equal to:
P(as|a, s
e, sf ) = C ?
?
(i,j)?as
Ppc(s
e
i , s
f
j )?
? comp(a, as, s
e, sf )
where comp(a, as, se, sf ) is equal to 1 if the com-
patibility constraint is verified, and 0 else. C is a
normalizing constant. Ppc describe the probability
of each P-concept.
Although it would be possible to learn parame-
ters for the distribution Ppc depending on the char-
acteristics of each P-concepts, we want to keep
our model simple. Therefore, Ppc will have only
two different values. One for the one-sided P-
concepts, and one for the two-sided ones. Con-
sidering the constraint of normalization, we then
have actually one parameter: ? = Ppc(1?sided)Ppc(2?sided) .
Although it would be possible to learn the param-
eter ? during the EM-training, we choose to set
it at a preset value. Intuitively, we should have
0 < ? < 1, because if ? is greater than 1, then
the one-sided P-concepts will be favored by the
model, which is not what we want. Some empiri-
cal experiments showed that all values of ? in the
range [0.5,0.9] were giving good results, which
lead to think that ? can be set mostly indepen-
dently from the training corpus.
We still need to train the concepts probabilities
(used in Pml(a, e, f)), and to be able to decode
the most probable alignments. This is why we are
again going to represent P (a, as|e, f, se, sf ) as a
Factor Graph.
This Factor Graph will contain two instances of
the monolink Factor Graph as subgraph: one for
a, the other for as (see figure 4). More precisely,
we create again a V-Node for every position on
each side of the sentence pair. We will call these
V-Nodes ?Word V-Nodes?, to differentiate them
from the new ?P-set V-Nodes?. We will create a
?P-set V-Node? V ps.ei for every P-set in se, and a
?P-set V-Node? V ps.fj for every P-set in sj . We
inter-connect all of the Word V-Nodes so that we
have a subgraph identical to the Factor Graph used
in the monolink case. We also create a ?monolink
subgraph? for the P-set V-Nodes.
We now have 2 disconnected subgraphs. How-
ever, we need to add F-Nodes between them to en-
force the compatibility constraint between as and
Figure 4: A part of a Factor Graph showing the
connections between P-set V-Nodes and Word V-
Nodes on the English side.The V-Nodes are con-
nected to the French side through the 2 monolink
subgraphs
a. On the English side, for every P-set V-Node
V psek , and for every position i that the correspond-
ing P-set cover, we add a F-Node F comp.ek,i between
V psek and V
e
i , associated with the function:
?comp.ek,i (l, j) =
?
??
??
1 if j ? sfl or
j = ?1 or l = ?1
0 else
We proceed symmetrically on the French side.
Messages inside each monolink subgraph can
still be computed with the efficient procedure de-
scribed in section 3.2. We do not have the space to
describe in details the messages sent between P-set
V-Nodes and Word V-Nodes, but they are easily
computed from the principles of the BP algorithm.
Let NE =
?
ps?se |ps| and NF =
?
ps?sf |ps|.
Then the complexity of one BP iteration will be
O(NG ? ND + |e| ? |f |).
An interesting aspect of this model is that it
is flexible towards enforcing the respect of the
structures by the alignment, since not every P-set
need to have an equivalent in the opposite sen-
tence. (Gildea, 2003) has shown that too strict an
enforcement can easily degrade alignment quality
and that good balance was difficult to find.
Another interesting aspect is the fact that
we have a somehow ?parameterless? distortion
model. There is only one real-valued parameter to
control the distortion: ?. And even this parameter
is actually pre-set before any training on real data.
The distortion is therefore totally controlled by the
two sets of P-sets on each side of the sentence.
Finally, although we introduced the P-sets as
being generated from a syntactic tree, they do
not need to. In particular, we found interest-
ing to use P-sets consisting of every pair of adja-
172
cent positions in a sentence. For example, with
a sentence of length 5, we generate the P-sets
{1,2},{2,3},{3,4} and {4,5}. The underlying in-
tuition is that ?adjacency? is often preserved in
translation (we can see this as another case of
?conservation of structure?). Practically, using P-
sets of adjacent positions create a distortion model
where permutation of words are not penalized, but
gaps are penalized.
4.2 Experimental Results
The evaluation setting is the same as in the previ-
ous section. We created syntactic trees for every
sentences. For English,we used the Dan Bikel im-
plementation of the Collins parser (Collins, 2003).
For French, the SYGMART parser (Chauche?,
1984) and for Japanese, the KNP parser (Kuro-
hashi and Nagao, 1994).
The line SDM:Parsing (SDM standing for
?Structure-based Distortion Monolink?) shows the
results obtained by using P-sets from the trees pro-
duced by these parsers. The line SDM:Adjacency
shows results obtained by using adjacent positions
P-sets ,as described at the end of the previous sec-
tion (therefore, SDM:Adjacency do not use any
parser).
Several interesting observations can be made
from the results. First, our structure-based distor-
tion model did improve the results of the mono-
link model. There are however some surprising
results. In particular, SDM:Adjacency produced
surprisingly good results. It comes close to the
results of the IBM model 4 in both language pairs,
while it actually uses exactly the same parameters
as model 1. The fact that an assumption as simple
as ?allow permutations, penalize gaps? can pro-
duce results almost on par with the complicated
distortion model of model 4 might be an indica-
tion that this model is unnecessarily complex for
languages with similar structure.Another surpris-
ing result is the fact that SDM:Adjacency gives
better results for the English-French language pair
than SDM:Parsing, while we expected that infor-
mation provided by parsers would have been more
relevant for the distortion model. It might be an
indication that the structure of English and French
is so close that knowing it provide only moder-
ate information for word reordering. The con-
trast with the English-Japanese pair is, in this re-
spect, very interesting. For this language pair,
SDM:Adjacency did provide a strong improve-
Algorithm AER P R
Monolink 0.197 0.881 0.731
SDM:Parsing 0.166 0.882 0.813
SDM:Adjacency 0.135 0.887 0.851
CLA 0.26 0.819 0.665
GIZA++ /Model 1 0.281 0.667 0.805
GIZA++ /Model 2 0.205 0.754 0.863
GIZA++ /Model 3 0.162 0.806 0.890
GIZA++ /Model 4 0.121 0.849 0.927
Table 1: Results for English/French
Algorithm F P R
Monolink 0.263 0.594 0.169
SDM:Parsing 0.291 0.662 0.186
SDM:Adjacency 0.279 0.636 0.179
GIZA++ /Model 1 0.263 0.555 0.172
GIZA++ /Model 2 0.268 0.566 0.176
GIZA++ /Model 3 0.267 0.589 0.173
GIZA++ /Model 4 0.299 0.658 0.193
Table 2: Results for Japanese/English.
ment, but significantly less so than SDM:Parsing.
This tend to show that for language pairs that have
very different structures, the information provided
by syntactic tree is much more relevant.
5 Conclusion and Future Work
We will summarize what we think are the 4 more
interesti ng contributions of this paper. BP al-
gorithm has been shown to be useful and flexi-
ble for training and decoding complex alignment
models. An original mostly non-parametrical dis-
tortion model based on a simplified structure of
the sentences has been described. Adjacence con-
straints have been shown to produce very efficient
distortion model. Empirical performances differ-
ences in the task of aligning Japanese and English
to French hint that considering different paradigms
depending on language pairs could be an improve-
ment on the ?one-size-fits-all? approach generally
used in Statistical alignment and translation.
Several interesting improvement could also be
made on the model we presented. Especially,
a more elaborated Ppc, that would take into ac-
count the nature of the nodes (NP, VP, head,..) to
parametrize the P-set algnment probability, and
would use the EM-algorithm to learn those param-
eters.
173
References
M. Bayati, D. Shah, and M. Sharma. 2005. Maxi-
mum weight matching via max-product belief prop-
agation. Information Theory, 2005. ISIT 2005. Pro-
ceedings. International Symposium on, pages 1763?
1767.
Peter E Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer, 1993. The
mathematics of statistical machine translation: pa-
rameter estimation, volume 19, pages 263?311.
J. Chauche?. 1984. Un outil multidimensionnel de
lanalyse du discours. Coling84. Stanford Univer-
sity, California.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics.
Fabien Cromieres. 2006. Sub-sentential alignment us-
ing substring co-occurrence counts. In Proceedings
of ACL. The Association for Computer Linguistics.
U. Germann. 2001. Aligned hansards
of the 36th parliament of canada.
http://www.isi.edu/naturallanguage/download/hansard/.
D. Gildea. 2003. Loosely tree-based alignment for
machine translation. Proceedings of ACL, 3.
T. Heskes. 2003. Stable fixed points of loopy be-
lief propagation are minima of the bethe free energy.
Advances in Neural Information Processing Systems
15: Proceedings of the 2002 Conference.
S. Kurohashi and M. Nagao. 1994. A syntactic analy-
sis method of long japanese sentences based on the
detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
I. Melamed. 2002. Empirical Methods for Exploiting
Parallel Texts. The MIT Press.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Rada Mihalcea and
Ted Pedersen, editors, HLT-NAACL 2003 Workshop:
Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 1?10, Edmon-
ton, Alberta, Canada, May 31. Association for Com-
putational Linguistics.
Kevin P Murphy, Yair Weiss, and Michael I Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proceedings of Un-
certainty in AI, pages 467?475.
Franz Josef Och and Hermann Ney. 1999. Improved
alignment models for statistical machine translation.
University of Maryland, College Park, MD, pages
20?28.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers.
M. Utiyama and H. Isahara. 2003. Reliable measures
for aligning japanese-english news articles and sen-
tences. Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume
1, pages 72?79.
Y. Weiss and W. T. Freeman. 2001. On the optimality
of solutions of the max-product belief propagation
algorithm in arbitrary graphs. IEEE Trans. on Infor-
mation Theory, 47(2):736?744.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. Proceedings of ACL.
Jonathan S. Yedidia, William T. Freeman, and Yair
Weiss, 2003. Understanding belief propagation and
its generalizations, pages 239?269. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
174
Japanese Case Frame Construction by Coupling the Verb
and its Closest Case Component
Daisuke Kawahara
Graduate School of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501,
Japan
kawahara@pine.kuee.kyoto-u.ac.jp
Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501,
Japan
kuro@pine.kuee.kyoto-u.ac.jp
ABSTRACT
This paper describes a method to construct a case frame dic-
tionary automatically from a raw corpus. The main prob-
lem is how to handle the diversity of verb usages. We col-
lect predicate-argument examples, which are distinguished
by the verb and its closest case component in order to deal
with verb usages, from parsed results of a corpus. Since
these couples multiply to millions of combinations, it is dif-
ficult to make a wide-coverage case frame dictionary from a
small corpus like an analyzed corpus. We, however, use a
raw corpus, so that this problem can be addressed. Further-
more, we cluster and merge predicate-argument examples
which does not have different usages but belong to different
case frames because of different closest case components.
We also report on an experimental result of case structure
analysis using the constructed case frame dictionary.
1. INTRODUCTION
Syntactic analysis or parsing has been a main objective in
Natural Language Processing. In case of Japanese, however,
syntactic analysis cannot clarify relations between words in
sentences because of several troublesome characteristics of
Japanese such as scrambling, omission of case components,
and disappearance of case markers. Therefore, in Japanese
sentence analysis, case structure analysis is an important
issue, and a case frame dictionary is necessary for the anal-
ysis.
Some research institutes have constructed Japanese case
frame dictionaries manually [2, 3]. However, it is quite ex-
pensive, or almost impossible to construct a wide-coverage
case frame dictionary by hand.
Others have tried to construct a case frame dictionary
automatically from analyzed corpora. However, existing
syntactically analyzed corpora are too small to learn a dic-
tionary, since case frame information consists of relations
between nouns and verbs, which multiplies to millions of
combinations. Based on such a consideration, we took the
unsupervised learning strategy to Japanese case frame con-
struction1.
To construct a case frame dictionary from a raw corpus,
we parse a raw corpus first, but parse errors are problematic
in this case. However, if we use only reliable modifier-head
relations to construct a case frame dictionary, this problem
can be addressed. Verb sense ambiguity is rather problem-
atic. Since verbs can have different cases and case compo-
nents depending on their meanings, verbs which have dif-
ferent meanings should have different case frames. To deal
with this problem, we collect predicate-argument examples,
which are distinguished by the verb and its closest case com-
ponent, and cluster them. That is, examples are not distin-
guished by verbs such as naru ?make, become? and tsumu
?load, accumulate?, but by couples such as tomodachi ni
naru ?make a friend?, byouki ni naru ?become sick?,nimotsu
wo tsumu ?load baggage?, and keiken wo tsumu ?accumulate
experience?. Since these couples multiply to millions of com-
binations, it is difficult to make a wide-coverage case frame
dictionary from a small corpus like an analyzed corpus. We,
however, use a raw corpus, so that this problem can be ad-
dressed. The clustering process is to merge examples which
does not have different usages but belong to different case
frames because of different closest case components.
2. VARIOUS METHODS FOR CASE FRAME
CONSTRUCTION
We employ the following procedure of case frame construc-
tion from raw corpus (Figure 1):
1. A large raw corpus is parsed by KNP [5], and reliable
modifier-head relations are extracted from the parse
results. We call these modifier-head relations exam-
ples.
2. The extracted examples are distinguished by the verb
and its closest case component. We call these data
example patterns.
3. The example patterns are clustered based on a the-
saurus. We call the output of this process example
case frames, which is the final result of the system.
We call words which compose case components case
examples, and a group of case examples case exam-
ple group. In Figure 1, nimotsu ?baggage?, busshi
1In English, several unsupervised methods have been pro-
posed[7, 1]. However, it is different from those that combi-
nations of nouns and verbs must be collected in Japanese.
example patterns
raw corpus
tagging or
analysis+extraction
of reliable relations
thesaurus
by hand or learning
I. examples example case frames
III. merged frame
II. co-occurrences
IV. semantic case frames
wo
wo
niga
ga
tsumuwonimotsu
busshi
keiken
nikuruma
truck
hikoki
gajugyoin
sensyu
wo
ni
ga
wo
ni
ga
wo
tsumu
tsumu
tsumu
tsumu
tsumu
tsumu
tsumu
tsumu
nimotsu
kuruma
jugyoin
busshi
truck
keiken
sensyu
nihikoki
woga
ga woni tsumu
tsumusensyu keiken
jugyoin kuruma
truck
hikoki
nimotsu
busshi
ni
niga
wo
wo
wo tsumu
tsumu
tsumu
nimotsu
busshi
keiken
kurumajugyoin
truck
hikoki
gasensyu
wo
wo
wo
wo
wo
ni
ni
ga
ga
ni
kuruma
car
tsumu
load
tsumu
load
tsumu
load
tsumu
load
tsumu
nimotsu
baggage
nimotsu
baggage
jugyoin
worker
busshi
supply
busshi
supply
experience
keikensensyu
player
truck
hikoki
truck
airplane
tsumu
tsumu<mind>
<thing><vehicle><person>
<person>
accumulate
Figure 1: Several methods for case frame construction.
?supply?, and keiken ?experience? are case examples,
and {nimotsu ?baggage?, busshi ?supply?} (of wo case
marker in the first example case frame of tsumu ?load,
accumulate?) is a case example group. A case com-
ponent therefore consists of a case example and a case
marker (CM).
Let us now discuss several methods of case frame construc-
tion as shown in Figure 1.
First, examples (I of Figure 1) can be used individually,
but this method cannot solve the sparse data problem. For
example,
(1) kuruma ni nimotsu wo tsumu
car dat-CM baggage acc-CM load
(load baggage onto the car)
(2) truck ni busshi wo tsumu
truck dat-CM supply acc-CM load
(load supply onto the truck)
even if these two examples occur in a corpus, it cannot
be judged whether the expression ?kuruma ni busshi wo
tsumu? (load supply onto the car) is allowed or not.
Secondly, examples can be decomposed into binomial re-
lations (II of Figure 1). These co-occurrences are utilized
by statistical parsers, and can address the sparse data prob-
lem. In this case, however, verb sense ambiguity becomes a
serious problem. For example,
(3) kuruma ni nimotsu wo tsumu
car dat-CM baggage acc-CM load
(load baggage onto the car)
(4) keiken wo tsumu
experience acc-CM accumulate
(accumulate experience)
from these two examples, three co-occurrences (?kuruma ni
tsumu?, ?nimotsu wo tsumu?, and ?keiken wo tsumu?) are
extracted. They, however, allow the incorrect expression
?kuruma ni keiken wo tsumu? (load experience onto the
car, accumulate experience onto the car).
Thirdly, examples can be simply merged into one frame
(III of Figure 1). However, information quantity of this is
equivalent to that of the co-occurrences (II of Figure 1), so
verb sense ambiguity becomes a problem as well.
We distinguish examples by the verb and its closest case
component. Our method can address the two problems
above: verb sense ambiguity and sparse data.
On the other hand, semantic markers can be used as case
components instead of case examples. These we call seman-
tic case frames (IV of Figure 1). Constructing semantic
case frames by hand leads to the problem mentioned in Sec-
tion 1. Utsuro et al constructed semantic case frames from
a corpus [8]. There are three main differences to our ap-
proach: they use an annotated corpus, depend deeply on a
thesaurus, and did not resolve verb sense ambiguity.
3. COLLECTING EXAMPLES
This section explains how to collect examples shown in
Figure 1. In order to improve the quality of collected exam-
ples, reliable modifier-head relations are extracted from the
parsed corpus.
3.1 Conditions of case components
When examples are collected, case markers, case exam-
ples, and case components must satisfy the following condi-
tions.
Conditions of case markers
Case components which have the following case markers
(CMs) are collected: ga (nominative), wo (accusative), ni
(dative), to (with, that), de (optional), kara (from), yori
(from), he (to), and made (to). We also handle compound
case markers such as ni-tsuite ?in terms of?, wo-megutte
?concerning?, and others.
In addition to these cases, we introduce time case marker.
Case components which belong to the class <time>(see be-
low) and contain a ni, kara, or made CM are merged into
time CM. This is because it is important whether a verb
deeply relates to time or not, but not to distinguish between
surface CMs.
Generalization of case examples
Case examples which have definite meanings are general-
ized. We introduce the following three classes, and use these
classes instead of words as case examples.
<time>
? nouns which mean time
e.g. asa ?morning?, haru ?spring?,
rainen ?next year?
? case examples which contain a unit of time
e.g. 1999nen ?year?, 12gatsu ?month?,
9ji ?o?clock?
? words which are followed by the suffix mae ?before?,
tyu ?during?, or go ?after? and do not have the semantic
marker <place> on the thesaurus
e.g. kaku mae ?before ? ? ? write?,
kaigi go ?after the meeting?
<quantity>
? numerals
e.g. ichi ?one?, ni ?two?, juu ?ten?
? numerals followed by a numeral classifier2 such as tsu,
ko, and nin.
They are expressed with pairs of the class <quan-
tity> and a numeral classifier: <quantity>tsu, <quan-
tity>ko, and <quantity>nin.
e.g. 1tsu ? <quantity>tsu
2ko ? <quantity>ko
<clause>
? quotations (?? ? ? to? ?that ? ? ? ?) and expressions which
function as quotations (?? ? ? koto wo? ?that ? ? ? ?).
e.g. kaku to ?that ? ? ? write?,
kaita koto wo ?that ? ? ? wrote?
Exclusion of ambiguous case components
We do not use the following case components:
? Since case components which contain topic markers
(TMs) and clausal modifiers do not have surface case
markers, we do not use them. For example,
sono giin wa ? ? ? wo teian-shita.
the assemblyman TM acc-CM proposed
wa is a topic marker and giin wa ?assemblyman TM?
depends on teian-shita ?proposed?, but there is no case
marker for giin ?assemblyman? in relation to teian-
shita ?proposed?.
? ? ? wo teian-shiteiru giin ga ? ? ?
acc-CM proposing assemblyman
?? ? ? wo teian-shiteiru? is a clausal modifier and teian-
shiteiru ?proposing? depends on giin ?assemblyman?,
but there is no case marker for giin ?assemblyman? in
relation to teian-shiteiru ?proposing?.
? Case components which contain a ni or de case marker
are sometimes used adverbially. Since they have the
optional relation to their verbs, we do not use them.
e.g. tame ni ?because of?,
mujouken ni ?unconditionally?,
ue de ?in addition to?
For example,
30nichi ni souri daijin ga
30th on prime minister nom-CM
sono 2nin ni
those two people dat-CM
syou wo okutta
award acc-CM gave
2Most nouns must take a numeral classifier when they are
quantified in Japanese. An English equivalent to it is ?piece?.
(On 30th the prime minister gave awards to those two peo-
ple.)
from this sentence, the following example is acquired.
<time>:time-CM daijin:ga
minister:nom-CM
<quantity>nin:ni syou:wo okuru
people:dat-CM award acc-CM give
3.2 Conditions of verbs
We collect examples not only for verbs, but also for adjec-
tives and noun+copulas3 . However, when a verb is followed
by a causative auxiliary or a passive auxiliary, we do not
collect examples, since the case pattern is changed.
3.3 Extraction of reliable examples
When examples are extracted from automatically parsed
results, the problem is that the parsed results inevitably
contain errors. Then, to decrease influences of such errors,
we discard modifier-head relations whose parse accuracies
are low and use only reliable relations.
KNP employs the following heuristic rules to determine a
head of a modifier:
HR1 KNP narrows the scope of a head by finding a clear
boundary of clauses in a sentence. When there is only
one candidate verb in the scope, KNP determines this
verb as the head of the modifier.
HR2 Among the candidate verbs, verbs which rarely take
case components are excluded.
HR3 KNP determines the head according to the preference:
a modifier which is not followed by a comma depends
on the nearest candidate, and a modifier with a comma
depends on the second nearest candidate.
Our approach trusts HR1 but not HR2 and HR3. That is,
modifier-head relations which are decided in HR1 (there is
only one candidate of the head in the scope) are extracted
as examples, but relations which HR2 and HR3 are applied
to are not extracted. The following examples illustrate the
application of these rules.
(5) kare wa kai-tai hon wo
he TM want to buy book acc-CM
takusan mitsuketa node,
a lot found because
Tokyo he okutta.
Tokyo to sent
(Because he found a lot of books which he wants to buy, he
sent them to Tokyo.)
In this example, an example which can be extracted without
ambiguity is ?Tokyo he okutta? ?sent ? to Tokyo? at the end
of the sentence. In addition, since node ?because? is analyzed
as a clear boundary of clauses, the head candidate of hon
wo ?book acc-CM? is only mitsuketa ?find?, and this is also
extracted.
Verbs excluded from head candidates by HR2 possibly
become heads, so we do not use the examples which HR2 is
applied to. For example, when there is a strong verb right
3In this paper, we use ?verb? instead of ?verb/adjective or
noun+copula? for simplicity.
after an adjective, this adjective tends not to be a head of a
case component, so it is excluded from head candidates.
(6) Hi no mawari ga hayaku
fire of spread nom-CM rapidly
sukuidase-nakatta.
could not save
(The fire spread rapidly, so ?
1
could not save ?
2
.)
In this example, the correct head of mawari ga ?spread? is
hayaku ?rapidly?. However, since hayaku ?rapidly? is ex-
cluded from the head candidates, the head of mawari ga
?spread? is analyzed incorrectly.
We show an example of the process HR3:
(7) kare ga shitsumon ni
he nom-CM question acc-CM
sentou wo kitte kotaeta.
lead acc-CM take answered
(He took the lead to answer the question.)
In this example, head candidates of shitsumon ni ?question
acc-CM? are kitte ?take? and kotaeta ?answered?. According
to the preference ?modify the nearer head?, KNP incorrectly
decides the head is kitte ?take?. Like this example, when
there are many head candidates, the decided head is not
reliable, so we do not use examples in this case.
We extracted reliable examples from Kyoto University
Corpus[6], that is a syntactically analyzed corpus, and eval-
uated the accuracy of them. The accuracy of all the case
examples which have the target cases was 90.9%, and the
accuracy of the reliable examples was 97.2%. Accordingly,
this process is very effective.
4. CONSTRUCTION OF EXAMPLE CASE
FRAMES
As shown in Section 2, when examples whose verbs have
different meanings are merged, a case frame which allows an
incorrect expression is created. So, for verbs with different
meanings, different case frames should be acquired.
In most cases, an important case component which decides
the sense of a verb is the closest one to the verb, that is, the
verb sense ambiguity can be resolved by coupling the verb
and its closest case component. Accordingly, we distinguish
examples by the verb and its closest case component. We
call the case marker of the closest case component closest
case marker.
The number of example patterns which one verb has is
equal to that of the closest case components. That is, ex-
ample patterns which have almost the same meaning are
individually handled as follows:
(8) jugyoin:ga kuruma:ni
worker:nom-CM car:dat-CM
nimotsu:wo tsumu
baggage:acc-CM load
(9) {truck,hikoki}:ni
{truck,airplane}:dat-CM
busshi :wo tsumu
supply:acc-CM load
In order to merge example patterns that have almost the
same meaning, we cluster example patterns. The final ex-
( 5 + 8 ) + ( 3 + 2 + 10 )
( 3 + 5 + 8 ) + ( 3 + 2 + 10 )( )1/2 = 0.90ratio of common cases :
0.911.0 0.86
5
10
8
3 2
0.91= 0.941.0 ?  (5? 3) + 0.86 ?  (5? 2)
(5? 3) + (5? 2)
1/2 1/2
1/2 1/2
3 tsumu
tsumu
wo
wo
nimotsu
 busshini
kuruma
{truck  , hikoki  }
jugyoin ga ni
0.92 ?  0.90 = 0.83similarity between
example patterns :
= 0.920.94 ?  ( (5? 3) + (5? 2)  )   + 0.91 ?  (8? 10)
1/2 1/2 1/2 1/4
( (5? 3) + (5? 2)  )  + (8? 10)1/2 1/2 1/2 1/4
similarity between
case example groups :
load
load
baggageworker car
supplyairplanetruck
Figure 2: Example of calculating the similarity be-
tween example patterns (Numerals in the lower
right of examples represent their frequencies.)
ample case frames consist of the example pattern clusters.
The detail of the clustering is described in the following sec-
tion.
4.1 Similarity between example patterns
The clustering of example patterns is performed by using
the similarity between example patterns. This similarity
is based on the similarities between case examples and the
ratio of common cases. Figure 2 shows an example of cal-
culating the similarity between example patterns.
First, the similarity between two examples e
1
, e
2
is calcu-
lated using the NTT thesaurus as follows:
sime(e1, e2) = maxx?s
1
,y?s
2
sim(x,y)
sim(x,y) =
2L
lx + ly
where x, y are semantic markers, and s
1
, s
2
are sets of se-
mantic markers of e
1
, e
2
respectively4. lx, ly are the depths
of x, y in the thesaurus, and the depth of their lowest (most
specific) common node is L. If x and y are in the same node
of the thesaurus, the similarity is 1.0, the maximum score
based on this criterion.
Next, the similarity between the two case example groups
E
1
, E
2
is the normalized sum of the similarities of case ex-
amples as follows:
simE(E1, E2)
=
P
e
1
?E
1
P
e
2
?E
2
?
|e
1
||e
2
| sim
e
(e
1
,e
2
)
P
e
1
?E
1
P
e
2
?E
2
?
|e
1
||e
2
|
where |e
1
| , |e
2
| represent the frequencies of e
1
, e
2
respec-
tively.
The ratio of common cases of example patterns F
1
, F
2
is
4In many cases, nouns have many semantic markers in NTT
thesaurus.
calculated as follows:
cs =
s
P
n
i=1
|E
1cc
i
|+
P
n
i=1
|E
2cc
i
|
Pl
i=1
|E
1c1
i
|+
Pm
i=1
|E
2c2
i
|
where the cases of example pattern F
1
are c1
1
, c1
2
, ? ? ? , c1l,
the cases of example pattern F
2
are c2
1
, c2
2
, ? ? ? , c2m, and
the common cases of F
1
and F
2
is cc
1
, cc
2
, ? ? ? , ccn. E1cc
i
is the case example group of cci in F1. E2cc
i
, E
1c1
i
, and
E
2c2
i
are defined in the same way. The square root in this
equation decreases influences of the frequencies.
The similarity between F
1
and F
2
is the product of the
ratio of common cases and the similarities between case ex-
ample groups of common cases of F
1
and F
2
as follows:
score = cs ?
Pn
i=1
?
wi simE(E1cc
i
, E
2cc
i
)
P
n
i=1
?
wi
wi =
X
e
1
?E
1cc
i
X
e
2
?E
2cc
i
p
|e
1
| |e
2
|
where wi is the weight of the similarities between case ex-
ample groups.
4.2 Selection of semantic markers of example
patterns
The similarities between example patterns are deeply in-
fluenced by semantic markers of the closest case compo-
nents. So, when the closest case components have semantic
ambiguities, a problem arises. For example, when cluster-
ing example patterns of awaseru ?join, adjust?, the pair of
example patterns (te ?hand?, kao, ?face?)5 is created with
the common semantic marker <part of an animal>, and (te
?method?, syouten ?focus?) is created with the common se-
mantic marker <logic, meaning>. From these two pairs, the
pair (te ?hand?, kao ?face?, syouten ?focus?) is created, though
<part of an animal> is not similar to <logic, meaning> at
all.
To address this problem, we select one semantic marker of
the closest case component of each example pattern in order
of the similarity between example patterns as follows:
1. In order of the similarity of a pair, (p, q), of two exam-
ple patterns, we select semantic markers of the closest
case components, np, nq of p, q. The selected semantic
markers sp, sq maximize the similarity between np and
nq .
2. The similarities of example patterns related to p, q are
recalculated.
3. These two processes are iterated while there are pairs
of two example patterns, of which the similarity is
higher than a threshold.
4.3 Clustering procedure
The following is the clustering procedure:
1. Elimination of example patterns which occur infre-
quently
Target example patterns of the clustering are those
whose closest case components occur more frequently
than a threshold. We set this threshold to 5.
5Example patterns are represented by the closest case com-
ponents.
2. Clustering of example patterns which have the same
closest CM
(a) Similarities between pairs of two example pat-
terns which have the same closest CM are calcu-
lated, and semantic markers of closest case com-
ponents are selected. These two processes are it-
erated as mentioned in 4.2.
(b) Each example pattern pair whose similarity is higher
than some threshold is merged.
3. Clustering of all the example patterns
The example patterns which are output by 2 are clus-
tered. In this phase, it is not considered whether the
closest CMs are the same or not. The following exam-
ple patterns have almost the same meaning, but they
are not merged by 2 because of the different closest
CM. This clustering can merge these example patterns.
(10) {busshi,kamotsu}:wo
{supply,cargo}:acc-CM
truck :ni tsumu
truck:dat-CM load
(11) {truck,hikoki}:ni
{truck,airplane}:dat-CM
{nimotsu,busshi}:wo tsumu
{baggage,supply}:acc-CM load
5. SELECTION OF OBLIGATORY CASE
MARKERS
If a CM whose frequency is lower than other CMs, it might
be collected because of parsing errors, or has little relation
to its verb. So, we set the threshold for the CM frequency
as 2
?
mf, where mf means the frequency of the most found
CM. If the frequency of a CM is less than the threshold, it
is discarded. For example, suppose the most frequent CM
for a verb is wo, 100 times, and the frequency of ni CM for
the verb is 16, ni CM is discarded (since it is less than the
threshold, 20).
However, since we can say that all the verbs have ga (nom-
inative) CMs, ga CMs are not discarded. Furthermore, if an
example case frame do not have a ga CM, we supplement
its ga case with semantic marker <person>.
6. CONSTRUCTED CASE FRAME DICTIO-
NARY
We applied the above procedure to Mainichi Newspaper
Corpus (9 years, 4,600,000 sentences). We set the threshold
of the clustering 0.80. The criterion for setting this threshold
is that case frames which have different case patterns or
different meanings should not be merged into one case frame.
Table1 shows examples of constructed example case frames.
From the corpus, example case frames of 71,000 verbs are
constructed; the average number of example case frames of
a verb is 1.9; the average number of case slots of a verb is
1.7; the average number of example nouns in a case slot is
4.3. The clustering led a decrease in the number of example
case frames of 47%.
Table 1: Examples of the constructed case frames(*
means the closest CM).
verb CM case examples
kau1 ga person, passenger
?buy? wo* stock, land, dollar, ticket
de shop, station, yen
kau2 ga treatment, welfare, postcard
wo* anger, disgust, antipathy
...
...
...
yomu1 ga student, prime minister
?read? wo* book, article, news paper
yomu2 ga <person>
wo talk, opinion, brutality
de* news paper, book, textbook
yomu3 ga <person>
wo* future
...
...
...
tadasu1 ga member, assemblyman
?examine? wo* opinion, intention, policy
ni tsuite problem, <clause>, bill
tadasu2 ga chairman, oneself
?improve? wo* position, form
...
...
...
kokuchi1 ga doctor
?inform? ni* the said person
kokuchi2 ga colleague
wo* infection, cancer
ni* patient, family
sanseida1 ga <person>
?agree? ni* opinion, idea, argument
sanseida2 ga <person>
ni* <clause>
As shown in Table1, example case frames of noun+copulas
such as sanseida ?positiveness+copula (agree)?, and com-
pound case markers such as ni-tsuite ?in terms of? of tadasu
?examine? are acquired.
7. EXPERIMENTS AND DISCUSSION
Since it is hard to evaluate the dictionary statically, we
use the dictionary in case structure analysis and evaluate the
analysis result. We used 200 sentences of Mainichi Newspa-
per Corpus as a test set. We analyzed case structures of the
sentences using the method proposed by [4]. As the evalua-
tion of the case structure analysis, we checked whether cases
of ambiguous case components (topic markers and clausal
modifiers) are correctly detected or not. The evaluation re-
sult is presented in Table 2. The baseline is the result by
assigning a vacant case in order of ?ga?, ?wo?, and ?ni?. When
we do not consider parsing errors to evaluate the case de-
tection, the accuracy of our method for topic markers was
96% and that for clausal modifiers was 76%. The baseline
accuracy for topic markers was 91% and that for clausal
modifiers was 62%. Thus we see our method is superior to
the baseline.
Table 2: The accuracy of case detection.
correct case
detection
incorrect case
detection
parsing error
our method
topic marker 85 4 13
clausal modifier 48 15 2
baseline
topic marker 81 8 13
clausal modifier 39 24 2
The following are examples of analysis results6:
(1) 1
ookurasyo
?ga wa ginko ga
the Ministry of Finance TM bank nom-CM
2
tsumitate-teiru
2
ryuhokin
?wo no
deposit reserve fund of
torikuzushi wo
3
mitomeru
consume acc-CM consent
3
houshin
?ni? wo
1
kimeta .
policy acc-CM decide
(The Ministry of Finance decided the policy of con-
senting to consume the reserve fund which the banks
have deposited.)
(2)
korera no
1
gyokai
?wo? wa seijiteki
these industry TM political
hatsugenryoku ga tsuyoi toiu
voice nom-CM strong
tokutyo ga 1 aru .
characteristic nom-CM have
(These industries have the characteristic of
strong political voice.)
Analysis errors are mainly caused by two phenomena. The
first is clausal modifiers which have no case relation to the
modifees such as ?? ? ? wo mitomeru houshin? ?policy of con-
senting ? ? ? ? (? above). The Second is verbs which take two
ga ?nominative? case markers (one is wa superficially) such
as ?gyokai wa ? ? ? toiu tokutyo ga aru? ?industries have the
characteristic of ? ? ? ? (? above). Handling these phenomena
is an area of future work.
8. CONCLUSION
We proposed an unsupervised method to construct a case
frame dictionary by coupling the verb and its closest case
component. We obtained a large case frame dictionary,
which consists of 71,000 verbs. Using this dictionary, we
can detect ambiguous case components accurately. We plan
to exploit this dictionary in anaphora resolution in the fu-
ture.
9. ACKNOWLEDGMENTS
The research described in this paper was supported in
part by JSPS-RFTF96P00502 (The Japan Society for the
Promotion of Science, Research for the Future Program).
6The underlined words with ? are correctly analyzed, but
ones with ? are not. The detected CMs are shown after the
underlines.
10. REFERENCES
[1] T. Briscoe and J. Carroll. Automatic extraction of
subcategorization from corpora. In Proceedings of the
5th Conference on Applied Natural Language
Processing, pages 356?363, 1997.
[2] S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo,
H. Nakaiwa, K. Ogura, and Y. O. Y. Hayashi, editors.
Japanese Lexicon. Iwanami Publishing, 1997.
[3] Information-Technology Promotion Agency, Japan.
Japanese Verbs : A Guide to the IPA Lexicon of Basic
Japanese Verbs. 1987.
[4] S. Kurohashi and M. Nagao. A method of case
structure analysis for japanese sentences based on
examples in case frame dictionary. In IEICE
Transactions on Information and Systems, volume
E77-D No.2, 1994.
[5] S. Kurohashi and M. Nagao. A syntactic analysis
method of long japanese sentences based on the
detection of conjunctive structures. Computational
Linguistics, 20(4), 1994.
[6] S. Kurohashi and M. Nagao. Building a japanese
parsed corpus while improving the parsing system. In
Proceedings of The First International Conference on
Language Resources & Evaluation, pages 719?724, 1998.
[7] C. D. Manning. Automatic acquisition of a large
subcategorization dictionary from corpora. In
Proceedings of the 31th Annual Meeting of ACL, pages
235?242, 1993.
[8] T. Utsuro, T. Miyata, and Y. Matsumoto. Maximum
entropy model learning of subcategorization preference.
In Proceedings of the 5th Workshop on Very Large
Corpora, pages 246?260, 1997.
TSUBAKI: An Open Search Engine Infrastructure for
Developing New Information Access Methodology
Keiji Shinzato?, Tomohide Shibata?, Daisuke Kawahara?,
Chikara Hashimoto?? and Sadao Kurohashi?
?Graduate School of Informatics, Kyoto University
?National Institute of Information and Communications Technology
??Department of Informatics, Yamagata University
{shinzato, shibata, kuro}@nlp.kuee.kyoto-u.ac.jp
dk@nict.go.jp ch@yz.yamagata-u.ac.jp
Abstract
As the amount of information created by
human beings is explosively grown in the
last decade, it is getting extremely harder
to obtain necessary information by conven-
tional information access methods. Hence,
creation of drastically new technology is
needed. For developing such new technol-
ogy, search engine infrastructures are re-
quired. Although the existing search engine
APIs can be regarded as such infrastructures,
these APIs have several restrictions such as a
limit on the number of API calls. To help the
development of new technology, we are run-
ning an open search engine infrastructure,
TSUBAKI, on a high-performance comput-
ing environment. In this paper, we describe
TSUBAKI infrastructure.
1 Introduction
As the amount of information created by human be-
ings is explosively grown in the last decade (Uni-
versity of California, 2003), it is getting extremely
harder to obtain necessary information by con-
ventional information access methods, i.e., Web
search engines. This is obvious from the fact that
knowledge workers now spend about 30% of their
day on only searching for information (The Del-
phi Group White Paper, 2001). Hence, creation of
drastically new technology is needed by integrating
several disciplines such as natural language process-
ing (NLP), information retrieval (IR) and others.
Conventional search engines such as Google and
Yahoo! are insufficient to search necessary informa-
tion from the current Web. The problems of the con-
ventional search engines are summarized as follows:
Cannot accept queries by natural language sen-
tences: Search engine users have to represent their
needs by a list of words. This means that search
engine users cannot obtain necessary information if
they fail to represent their needs into a proper word
list. This is a serious problem for users who do not
utilize a search engine frequently.
Cannot provide organized search results: A
search result is a simple list consisting of URLs,
titles and snippets of web pages. This type of re-
sult presentation is obviously insufficient consider-
ing explosive growth and diversity of web pages.
Cannot handle synonymous expressions: Exist-
ing search engines ignore a synonymous expression
problem. Especially, since Japanese uses three kinds
of alphabets, Hiragana, Katakana and Kanji, this
problem is more serious. For instance, although both
Japanese words ????? and ???? mean child,
the search engines provide quite different search re-
sults for each word.
We believe that new IR systems that overcome the
above problems give us more flexible and com-
fortable information access and that development
of such systems is an important and interesting re-
search topic.
To develop such IR systems, a search engine in-
frastructure that plays a low-level layer role (i.e., re-
trieving web pages according to a user?s query from
a huge web page collection) is required. The Appli-
cation Programming Interfaces (APIs) provided by
189
commercial search engines can be regarded as such
search engine infrastructures. The APIs, however,
have the following problems:
1. The number of API calls a day and the num-
ber of web pages included in a search result are
limited.
2. The API users cannot know how the acquired
web pages are ranked because the ranking mea-
sure of web pages has not been made public.
3. It is difficult to reproduce previously-obtained
search results via the APIs because search en-
gine?s indices are updated frequently.
These problems are an obstacle to develop new IR
systems using existing search engine APIs.
The research project ?Cyber Infrastructure for the
Information-explosion Era1? gives researchers sev-
eral kinds of shared platforms and sophisticated
tools, such as an open search engine infrastructure,
considerable computational environment and a grid
shell software (Kaneda et al, 2002), for creation of
drastically new IR technology. In this paper, we de-
scribe an open search engine infrastructure TSUB-
AKI, which is one of the shared platforms devel-
oped in the Cyber Infrastructure for the Information-
explosion Era project. The overview of TSUBAKI is
depicted in Figure 1. TSUBAKI is built on a high-
performance computing environment consisting of
128 CPU cores and 100 tera-byte storages, and it
can provide users with search results retrieved from
approximately 100 million Japanese web pages.
The mission of TSUBAKI is to help the develop-
ment of new information access methodology which
solves the problems of conventional information ac-
cess methods. This is achieved by the following
TSUBAKI?s characteristics:
API without any restriction: TSUBAKI pro-
vides its API without any restrictions such as the
limited number of API calls a day and the number
of results returned from an API per query, which are
the typical restrictions of the existing search engine
APIs. Consequently, TSUBAKI API users can de-
velop systems that handle a large number of web
pages. This feature is important for dealing with the
Web that has the long tail aspect.
1http://i-explosion.ex.nii.ac.jp/i-explosion/ctr.php/m/Inde-
xEng/a/Index/
	







 	
Japanese Named Entity Recognition
Using Structural Natural Language Processing
Ryohei Sasano?
Graduate School of Information Science
and Technology, University of Tokyo
ryohei@nlp.kuee.kyoto-u.ac.jp
Sadao Kurohashi
Graduate School of Infomatics,
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
This paper presents an approach that uses
structural information for Japanese named
entity recognition (NER). Our NER system
is based on Support Vector Machine (SVM),
and utilizes four types of structural informa-
tion: cache features, coreference relations,
syntactic features and caseframe features,
which are obtained from structural analyses.
We evaluated our approach on CRL NE data
and obtained a higher F-measure than exist-
ing approaches that do not use structural in-
formation. We also conducted experiments
on IREX NE data and an NE-annotated web
corpus and confirmed that structural infor-
mation improves the performance of NER.
1 Introduction
Named entity recognition (NER) is the task of iden-
tifying and classifying phrases into certain classes
of named entities (NEs), such as names of persons,
organizations and locations.
Japanese texts, which we focus on, are written
without using blank spaces. Therefore, Japanese
NER has tight relation with morphological analy-
sis, and thus it is often performed immediately after
morphological analysis (Masayuki and Matsumoto,
2003; Yamada, 2007). However, such approaches
rely only on local context. The Japanese NER sys-
tem proposed in (Nakano and Hirai, 2004), which
achieved the highest F-measure among conventional
systems, introduced the bunsetsu1 feature in order to
consider wider context, but considers only adjacent
bunsetsus.
*Research Fellow of the Japan Society for the Promotion of Science (JSPS)
1Bunsetsu is a commonly used linguistic unit in Japanese,
consisting of one or more adjacent content words and zero or
more following functional words.
On the other hand, as for English or Chinese, var-
ious NER systems have explored global information
and reported their effectiveness. In (Malouf, 2002;
Chieu and Ng, 2002), information about features as-
signed to other instances of the same token is uti-
lized. (Ji and Grishman, 2005) uses the information
obtained from coreference analysis for NER. (Mohit
and Hwa, 2005) uses syntactic features in building a
semi-supervised NE tagger.
In this paper, we present a Japanese NER system
that uses global information obtained from several
structural analyses. To be more specific, our system
is based on SVM, recognizes NEs after syntactic,
case and coreference analyses and uses information
obtained from these analyses and the NER results
for the previous context, integrally. At this point,
it is true that NER results are useful for syntactic,
case and coreference analyses, and thus these analy-
ses and NER should be performed in a complemen-
tary way. However, since we focus on NER, we rec-
ognize NE after these structural analyses.
2 Japanese NER Task
A common standard definition for Japanese NER
task is provided by IREXworkshop (IREX Commit-
tee, 1999). IREX defined eight NE classes as shown
in Table 1. Compared with the MUC-6 NE task def-
inition (MUC, 1995), the NE class ?ARTIFACT,?
which contains book titles, laws, brand names and
so on, is added.
NER task can be defined as a chunking problem
to identify token sequences that compose NEs. The
chunking problem is solved by annotating chunk
tags to tokens. Five chunk tag sets, IOB1, IOB2,
IOE1, IOE2 and IOBES are commonly used. In this
paper, we use the IOBES model, in which ?S? de-
notes a chunk itself, and ?B,? ?I? and ?E? denote the
607
Table 1: Definition of NE in IREX.
NE class Examples
ORGANIZATION NHK Symphony Orchestra
PERSON Kawasaki Kenjiro
LOCATION Rome, Sinuiju
ARTIFACT Nobel Prize
DATE July 17, April this year
TIME twelve o?clock noon
MONEY sixty thousand dollars
PERCENT 20%, thirty percents
beginning, intermediate and end parts of a chunk.
If a token does not belong to any named entity, it is
tagged as ?O.? Since IREX defined eight NE classes,
tokens are classified into 33 (= 8 ? 4 + 1) NE tags.
For example, NE tags are assigned as following:
(1) Kotoshi 4 gatsu Roma ni itta.
this year April Rome to went
B-DATE I-DATE E-DATE S-LOCATION O O
(? went to Rome on April this year.)
3 Motivation for Our Approach
Our NER system utilizes structural information. In
this section, we describe the motivation for our ap-
proach.
High-performance Japanese NER systems are of-
ten based on supervised learning, and most of them
use only local features, such as features obtained
from the target token, two preceding tokens and two
succeeding tokens. However, in some cases, NEs
cannot be recognized by using only local features.
For example, while ?Kawasaki? in the second
sentence of (2) is the name of a person, ?Kawasaki?
in the second sentence of (3) is the name of a soc-
cer team. However, the second sentences of (2) and
(3) are exactly the same, and thus it is impossible to
correctly distinguish these NE classes by only using
information obtained from the second sentences.
(2) Kachi-ha senpatsu-no Kawasaki Kenjiro.
winner starter
Kawasaki-ha genzai 4 shou 3 pai.
now won lost
(The winning pitcher is the starter Kenjiro Kawasaki.
Kawasaki has won 4 and lost 3.)
(3) Dai 10 setsu-wa Kawasaki Frontale-to taisen.
the round against
Kawasaki-ha genzai 4 shou 3 pai.
now won lost
(The 10th round is against Kawasaki Frontale.
Kawasaki has won 4 and lost 3.)
In order to recognize these NE classes, it is essential
to use the information obtained from the previous
context. Therefore, we utilize information obtained
from the NER for the previous context: cache fea-
ture and coreference relation.
For another example, ?Shingishu? in (4) is the
name of city in North Korea. The most important
clue for recognizing ?Shingishu? as ?LOCATION?
may be the information obtained from the head verb,
?wataru (get across).?
(4) Shingishu-kara Ouryokko-wo wataru.
Sinuiju from Amnokkang get across
(? gets across the Amnokkang River from Sinuiju.)
However, when using only local features, the word
?wataru? is not taken into consideration because
there are more than two morphemes between ?shu2?
and ?wataru.? In order to deal with such problem,
we use the information obtained from the head verb:
syntactic feature and caseframe feature.
4 NER Using Structural Information
4.1 Outline of Our NER System
Our NER system performs the chunking process
based on morpheme units because character-based
methods do not outperform morpheme-based meth-
ods (Masayuki and Matsumoto, 2003) and are not
suitable for considering wider context.
A wide variety of trainable models have been ap-
plied to Japanese NER task, including maximum en-
tropy models (Utsuro et al, 2002), support vector
machines (Nakano and Hirai, 2004; Yamada, 2007)
and conditional random fields (Fukuoka, 2006). Our
system applies SVMs because, for Japanese NER,
SVM-based systems achieved higher F-measure
than the other systems. (Isozaki and Kazawa, 2003)
proposed an SVM-based NER system with Viterbi
search, which outperforms an SVM-based NER sys-
tem with sequential determination, and our system
basically follows this system. Our NER system con-
sists of the following four steps:
1. Morphological analysis
2. Syntactic, case and coreference analyses
3. Feature extraction for chunking
4. SVM and Viterbi search based chunking
The following sections describe each of these steps
in detail.
2Since the dictionary for morphological analysis has no en-
try ?Shingishu,? ?Shingishu? is analyzed as consisting of three
morphemes: ?shin,? ?gi? and ?shu.?
608
Input sentence:
Gai mu sho no shin Bei ha .
foreign affairs ministry in pro America group
(Pro-America group in the Ministry of Foreign Affairs.)
Output of JUMAN:
Gaimu sho no shin Bei ha .
noun noun particle noun noun noun
Output of ChaSen:
Gaimusho no shin-Bei ha .
noun particle noun noun
Figure 1: Example of morphological analyses.
4.2 Morphological Analysis
While most existing Japanese NER systems use
ChaSen (Matsumoto et al, 2003) as a morphological
analyzer, our NER system uses a Japanese morpho-
logical analyzer JUMAN (Kurohashi and Kawahara,
2005) because of the following two reasons.
First, JUMAN tends to segment a sentence into
smaller morphemes than ChaSen, and this is a good
tendency for morpheme-based NER systems be-
cause the boundary contradictions between morpho-
logical analysis and NEs are considered to be re-
duced. Figure 1 shows an example of the outputs
of JUMAN and ChaSen. Although both analyses
are reasonable, JUMAN divided ?Gaimusho? and
?shin-Bei? into two morphemes, while ChaSen left
them as a single morpheme. Second, JUMAN adds
categories to some morphemes, which can be uti-
lized for NER. In JUMAN, about thirty categories
are defined and tagged to about one fifth of mor-
phemes. For example, ?ringo (apple),? ?inu (dog)?
and ?byoin (hospital)? are tagged as ?FOOD,? ?AN-
IMAL? and ?FACILITY,? respectively.
4.3 Syntactic, Case and Coreference Analyses
syntactic analysis Syntactic analysis is performed
by using the Japanese parser KNP (Kurohashi and
Nagao, 1994). KNP employs some heuristic rules to
determine the head of a modifier.
case analysis Case analysis is performed by using
the system proposed in (Kawahara and Kurohashi,
2002). This system uses Japanese case frames that
are automatically constructed from a large corpus.
To utilize case analysis for NER, we constructed
case frames that include NE labels in advance. We
explain details in Section 4.4.2. The case analysis is
applied to each predicate in an input sentence. For
details see (Kawahara and Kurohashi, 2002).
coreference analysis Coreference analysis is per-
formed by using the coreference analyzer proposed
by (Sasano et al, 2007). As will be mentioned in
Section 4.4.2, our NER system uses coreference re-
lations only when coreferential expressions do not
share same morphemes. Basically, such coreference
relations are recognized by using automatically ac-
quired synonym knowledge.
4.4 Feature Extraction
4.4.1 Basic Features
As basic features for chunking, our NER system
uses the morpheme itself, character type, POS tag
and category if it exists.
As character types, we defined seven types:
?kanji,? ?hiragana,? ?katakana,? ?kanji with hira-
gana,? ?punctuation mark,? ?alphabet? and ?digit.?
As for POS tag, more than one POS feature are
extracted if the target morpheme has POS ambigu-
ity. In addition, besides POS tag obtained by JU-
MAN, our system also uses POS tag obtained from
Japanese morphological analyzer MeCab3 that uses
IPADIC as a word dictionary (Asahara and Mat-
sumoto, 2002). The JUMAN dictionary has few
named entity entries; thus our system supplements
the lack of lexical knowledge by using MeCab.
4.4.2 Structural Features
Our NER system uses three types of global fea-
tures: cache features, syntactic features and case-
frame features, and a rule that reflects coreference
relations. Although the coreference relations are not
used as features, we describe how to use them in this
section.
cache feature If the same morpheme appears mul-
tiple times in a single document, in most cases the
NE tags of these morphemes have some relation to
each other, and the NER results for previous parts
of the document can be a clue for the analysis for
following parts.
We consider the examples (2) and (3) again. Al-
though the second sentences of (2) and (3) are ex-
actly the same, we can recognize ?Kawasaki? in
the second sentence of (2) is ?S-PERSON? and
?Kawasaki? in the second sentence of (3) is ?S-
ORGANIZATION? by reading the first sentences.
To utilize the information obtained from previous
parts of the document, our system uses the NER
results for previous parts of the document as fea-
tures, called cache features. When analyzing (2),
our system uses the outputs of NE recognizer for
3http://mecab.sourceforge.jp/
609
?Kawasaki? in the first sentence as a feature for
?Kawasaki? in the second sentence. For simplicity,
our system uses correct NE tags when training. That
is, as a feature for ?Kawasaki? in the second sen-
tence of (2), the correct feature ?B-PERSON? is al-
ways added when training, not always added when
analyzing.
coreference rule Coreference relation can be a
clue for NER. This clue is considered by using cache
features to a certain extent. However, if the same
morpheme is not used, cache features cannot work.
For example, ?NHK kokyo gakudan? and ?N-kyo?
in (5) have coreference relation, but they do not
share the same morpheme.
(5) NHK kokyo gakudan-no ongaku kantoku-ni
symphony orchestra musical director
shuunin. N-kyo-to kyoen-shite irai ... .
became perform together since
(He became musical director of the NHK Symphony
Orchestra. Since performing together with N-kyo ... .)
In this case, ?NHK kokyo gakudan? can easily be
recognized as ?ORGANIZATION,? because it ends
with ?kokyo gakudan (symphony orchestra).? Mean-
while, ?N-kyo,? the abbreviation of ?NHK kokyo
gakudan,? cannot easily be recognized as ?ORGA-
NIZATION.?
Therefore, our system uses a heuristic rule that if
a morpheme sequence is analyzed to be coreferential
to a previous morpheme sequence that is recognized
as an NE class, the latter morpheme sequence is rec-
ognized as the same NE class. Since this heuristic
rule is introduced in order to utilize the coreference
relation that is not reflected by cache features, our
system applies this rule only when coreferential ex-
pressions do not have any morphemes in common.
syntactic feature As mentioned in Section 3, our
system utilizes the information obtained from the
head verb. As syntactic features, our system uses the
head verb itself and the surface case of the bunsetsu
that includes the target morpheme.
For the morpheme ?shin? in example (4), the
head verb ?wataru (get across)? and the surface case
?kara (from)? are added as syntactic features.
caseframe feature Syntactic features cannot work
if the head verb does not appear in the training data.
To overcome this data sparseness problem, case-
frame features are introduced.
Table 2: Case frame of ?haken (dispatch).?
case examples
ga Japan:23,party:13,country:12,government:7,
(nominative) company6,ward:6,corps:5,UN:4,US:4,Korea:4,
team:4,. . . (ORGANIZATION,LOCATION)
wo party:1249,him:1017,soldier:932,official:906,
(objective) company6:214,instructor:823,expert:799,
helper:694,staff:398,army:347,. . .
ni Iraq:700,on-the-scene:576,abroad:335,
(locative) home:172,Japan:171,Indirect Ocean:142,
scene:141,China:125,. . . (LOCATION)
For example, although the head verb ?haken (dis-
patch)? can be a clue for recognizing ?ICAO? in
(6) as ?ORGANIZATION,? syntactic features can-
not work if ?haken (dispatch)? did not appear in the
training data.
(6) ICAO-ha genchi-ni senmonka-wo haken-shita.
scene to expert dispatched
(ICAO dispatched experts to the scene)
However, this clue can be utilized if there is knowl-
edge that the ?ga (nominative)? case of ?haken (dis-
patch)? is often assigned by ?ORGANIZATION.?
Therefore, we construct case frames that include
NE labels in advance. Case frames describe what
kinds of cases each verb has and what kinds of nouns
can fill a case slot. We construct them from about
five hundred million sentences. We first recognize
NEs appearing in the sentences by using a primitive
NER system that uses only local features, and then
construct the case frames from the NE-recognized
sentences. To be more specific, if one tenth of the
examples of a case are classified as a certain NE
class, the corresponding label is attached to the case.
Table 2 shows the constructed case frame of ?haken
(dispatch).? In the ?ga (nominative)? case, the NE
labels, ?ORGANIZATION? and ?LOCATION? are
attached.
We then explain how to utilize these case frames.
Our system first performs case analysis, and uses as
caseframe features the NE labels attached in the case
to which the target morpheme is assigned. For in-
stance, by the case analyzer, the postpositional par-
ticle ?-ha? in (6) is recognized as meaning nom-
inative and ?ICAO? is assigned to the ?ga (nom-
inative)? case of the case frame of ?haken (dis-
patch).?Therefore, the caseframe features, ?ORGA-
NIZATION? and ?LOCATION? are added to the
features for the morpheme ?ICAO.?
4.5 SVM and Viterbi Search Based Chunking
To utilize cache features obtained from the previous
parts of the same sentence, our system determines
610
Table 3: Experimental results (F-measure).
CRL IREX WEB
baseline 88.63 85.47 68.98
+ cache 88.81 +0.18* 85.94 +0.47 69.67 +0.69*
+ coreference 88.68 +0.05 86.52 +1.05*** 69.17 +0.19
+ syntactic 88.80 +0.17* 85.77 +0.30 70.25 +1.27**
+ caseframe 88.57?0.06 85.51 +0.04 70.12 +1.14*
+ thesaurus 88.77 +0.14 86.36 +0.89* 68.63?0.35
use all 89.40 +0.77*** 87.72 +2.25*** 71.03 +2.05***
significant at the .1 level:*, .01 level:**, .001 level:***
NE tags clause by clause. The features extracted
from two preceding morphemes and two succeed-
ing morphemes are also used for chunking a target
morpheme. Since SVM can solve only a two-class
problem, we have to extend a binary classifier SVM
to n-class classifier. Here, we employ the one versus
rest method, in which we prepared n binary classi-
fiers and each classifier is trained to distinguish a
class from the rest of the classes.
To consider consistency of NE tags in a clause,
our system uses Viterbi search with some constraints
such as a ?B-DATE? must be followed by ?I-DATE?
or ?E-DATE.? Since SVMs do not output proba-
bilities, our system uses the SVM+sigmoid method
(Platt et al, 2000). That is, a sigmoid function
s(x) = 1/(1+exp(??x)) is applied to map the out-
put of SVM to a probability-like value. Our system
determines NE tags by using these probability-like
values. Our system is trained by TinySVM-0.094
with C = 0.1 and uses a fixed value ? = 10. This
process is almost the same as the process proposed
by Isozaki and Kazawa and for details see (Isozaki
and Kazawa, 2003).
5 Experiments
5.1 Data
For training, we use CRL NE data, which was pre-
pared for IREX. CRL NE data has 18,677 NEs on
1,174 articles in Mainichi Newspaper.
For evaluation, we use three data: CRL NE data,
IREX?s formal test data called GENERAL andWEB
NE data. When using CRL NE data for evalua-
tion, we perform five-fold cross-validation. IREX
test data has 1,510 NEs in 71 articles from Mainichi
Newspaper. Although both CRL NE data and IREX
test data use Mainichi Newspaper, these formats are
not the same. For example, CRL NE data removes
parenthesis expressions, but IREX test data does not.
WEB NE data, which we annotated NEs on corpus
collected from the Web, has 1,686 NEs in 354 arti-
4http://chasen.org/ taku/software/TinySVM/
cles. Although the domain of the web corpus differs
from that of CRL NE data, the format of the web
corpus is the same as CRL NE data format.
5.2 Experiments and Discussion
To confirm the effect of each feature, we conducted
experiments on seven conditions as follows:
1. Use only basic features (baseline)
2. Add cache features to baseline
3. Add the coreference rule to baseline
4. Add parent features to baseline
5. Add caseframe features to baseline
6. Add thesaurus features to baseline
7. Use all structural information and thesaurus
Since (Masayuki andMatsumoto, 2003; Nakano and
Hirai, 2004) reported the performance of NER sys-
tem was improved by using a thesaurus, we also
conducted experiment in which semantic classes ob-
tained from a Japanese thesaurus ?Bunrui Goi Hyo?
(NLRI, 1993) were added to the SVM features. Ta-
ble 3 shows the experimental results.
To judge the statistical significance of the dif-
ferences between the performance of the baseline
system and that of the others, we conducted a
McNemar-like test. First, we extract the outputs that
differ between the baseline method and the target
method. Then, we count the number of the outputs
that only baseline method is correct and that only
target method is correct. Here, we assume that these
outputs have the binomial distribution and apply bi-
nomial test. As significance level, we use .1 level,
.01 level and .001 level. The results of the signifi-
cance tests are also shown in Table 3.
When comparing the performance between data
sets, we can say that the performance for WEB NE
data is much worse than the others. This may be
because the domain of the WEB corpus differs from
that of CRL NE data.
As for the differences in the same data set, cache
features and syntactic features improve the perfor-
mance not dramatically but consistently and inde-
pendently from the data set. The coreference rule
also improves the performance for all data sets, but
especially for IREX test data. This may be because
IREX test data does not remove parenthesis expres-
sions, and thus there are a many coreferential ex-
pressions in the data. Caseframe features improve
the performance for WEB NE data, but do not con-
tribute to the performance for CRL NE data and
611
Table 4: Comparison with previous work.
CRL cross IREX Learning Analysis Features
validation test data Method Units
(Isozaki and Kazawa, 2003) 86.77 85.10 SVM + Viterbi morpheme basic features
(Masayuki and Matsumoto, 2003) 87.21 SVM character +thesaurus
(Fukuoka, 2006) 87.71 Semi-Markov CRF character basic features
(Yamada, 2007) 88.33 SVM + Shift-Reduce morpheme +bunsetsu features
(Nakano and Hirai, 2004) 89.03 SVM character +bunsetsu features & thesaurus
Our system 89.40 87.72 SVM + Viterbi morpheme +structural information & thesaurus
IREX test data. This result shows that caseframe
features are very generalized features and effective
for data of different domain. On the other hand, the-
saurus features improve the performance for CRL
NE data and IREX test data, but worsen the perfor-
mance for WEB NE data. The main cause for this
may be overfitting to the domain of the training data.
By using all structural information, the perfor-
mance is significantly improved for all data sets, and
thus we can say that the structural information im-
proves the performance of NER.
5.3 Comparison with Previous Work
Table 4 shows the comparison with previous work
for CRL NE data and IREX test data. Our system
outperforms all other systems, and thus we can con-
firm the effectiveness of our approach.
6 Conclusion
In this paper, we presented an approach that uses
structural information for Japanese NER. We in-
troduced four types of structural information to an
SVM-based NER system: cache features, coref-
erence relations, syntactic features and caseframe
features, and conducted NER experiments on three
data. As a consequence, the performance of NER
was improved by using structural information and
our approach achieved a higher F-measure than ex-
isting approaches.
References
Masayuki Asahara and Yuji Matsumoto, 2002. IPADIC User
Manual. Nara Institute of Science and Technology, Japan.
Hai Leong Chieu and Hwee Tou Ng. 2002. Named entity
recognition: A maximum entropy approach using global in-
formation. In Proc. of COLING 2002, pages 1?7.
Kenta Fukuoka. 2006. Named entity extraction with semi-
markov conditional random fields (in Japanese). Master?s
thesis, Nara Institute of Science and Technology.
IREX Committee, editor. 1999. Proc. of the IREX Workshop.
Hideki Isozaki and Hideto Kazawa. 2003. Speeding up
support vector machines for named entity recognition (in
japanese). Trans. of Information Processing Society of
Japan, 44(3):970?979.
Heng Ji and Ralph Grishman. 2005. Improving name tagging
by reference resolution and relation detection. In Proc. of
ACL-2005, pages 411?418.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of
Case Frame Dictionary for Robust Japanese Case Analysis.
In Proc. of COLING-2002, pages 425?431.
Sadao Kurohashi and Daisuke Kawahara. 2005. Japanese mor-
phological analysis system JUMAN version 5.1 manual.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic anal-
ysis method of long Japanese sentences based on the detec-
tion of conjunctive structures. Computational Linguistics,
20(4):507?534.
R. Malouf. 2002. Markov models for language-independent
named entity recognition. In Proc. of CoNLL-2002, pages
187?190.
Asahara Masayuki and Yuji Matsumoto. 2003. Japanese
named entity extraction with redundant morphological anal-
ysis. In Proc. of HLT-NAACL 2003, pages 8?15.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka
Hirano, Hiroshi Matsuda, Kazuma Takaoka, and Masayuki
Asahara. 2003. Morphological analysis System chasen
2.3.3 users manual.
Behrang Mohit and Rebecca Hwa. 2005. Syntax-based semi-
supervised named entity tagging. In Proc. of ACL Interactive
Poster and Demonstration Sessoins, pages 57?60.
MUC-6. 1995. Proc. of the Sixth Message Understanding Con-
ference. Morgan Kaufmann Publishers, INC.
Keigo Nakano and Yuzo Hirai. 2004. Japanese named entity
extraction with bunsetsu features (in Japanese). Trans. of
Information Processing Society of Japan, 45(3):934?941.
The National Language Institute for Japanese Language, NLRI,
editor. 1993. Bunrui Goi Hyo (in Japanese). Shuuei Pub-
lishing.
John C. Platt, Nello Cristiani, and John ShaweTaylor. 2000.
Lage margin DAGs for multiclas classification. In Advances
in Neural Information Processing System 12.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi.
2007. Improving coreference resolution using bridging ref-
erence resolution and automatically acquired synonyms. In
Proc. of DAARC-2007.
Takehito Utsuro, Manabu Sassano, and Kiyotaka Uchimoto.
2002. Combing outputs of multiple named entity chunkers
by stacking. In Proc. of EMNLP-2002.
Hiroyasu Yamada. 2007. Shift reduce chunking for Japanese
named entity extraction (in Japanese). In IPSJ SIG Notes
NL-179-3, pages 13?18.
612
SYNGRAPH: A Flexible Matching Method based on Synonymous
Expression Extraction from an Ordinary Dictionary and a Web Corpus
Tomohide Shibata?, Michitaka Odani?, Jun Harashima?,
Takashi Oonishi??, and Sadao Kurohashi?
?Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan
??NEC Corporation, 1753, Shimonumabe, Nakahara-Ku, Kawasaki, Kanagawa 211-8666, Japan
{shibata,odani,harashima,kuro}@nlp.kuee.kyoto-u.ac.jp
t-onishi@bq.jp.nec.com
Abstract
This paper proposes a flexible matching
method that can assimilate the expressive
divergence. First, broad-coverage syn-
onymous expressions are automatically ex-
tracted from an ordinary dictionary, and
among them, those whose distributional
similarity in a Web corpus is high are used
for the flexible matching. Then, to overcome
the combinatorial explosion problem in the
combination of expressive divergence, an ID
is assigned to each synonymous group, and
SYNGRAPH data structure is introduced to
pack the expressive divergence. We con-
firmed the effectiveness of our method on
experiments of machine translation and in-
formation retrieval.
1 Introduction
In natural language, many expressions have almost
the same meaning, which brings great difficulty to
many NLP tasks, such as machine translation (MT),
information retrieval (IR), and question answering
(QA). For example, suppose an input sentence (1) is
given to a Japanese-English example-based machine
translation system.
(1) hotel ni
hotel
ichiban
best
chikai
near
eki wa
station
doko-desuka
where is
Even if a very similar translation example (TE)
?(2-a) ? (2-b)? exists in the TEs, a simple exact
matching method cannot utilize this example for the
translation.
(2) a. ryokan no
Japanese hotel
moyori no
nearest
eki wa
station
doko-desuka
where is
b. Where?s the nearest station to the hotel?
How to handle these synonymous expressions has
become one of the important research topics in NLP.
This paper presents a flexible matching method,
which can assimilate the expressive divergence, to
solve this problem. This method has the following
two features:
1. Synonymy relations and hypernym-hyponym
relations are automatically extracted from an
ordinary dictionary and a Web corpus.
2. Extracted synonymous expressions are effec-
tively handled by SYNGRAPH data structure,
which can pack the expressive divergence.
An ordinary dictionary is a knowledge source
to provide synonym and hypernym-hyponym rela-
tions (Nakamura and Nagao, 1988; Tsurumaru et al,
1986). A problem in using synonymous expressions
extracted from a dictionary is that some of them are
not appropriate since they are rarely used. For exam-
ple, a synonym pair ?suidou?1 = ?kaikyou(strait)? is
extracted.
Recently, some work has been done on corpus-
based paraphrase extraction (Lin and Pantel, 2001;
Barzilay and Lee, 2003). The basic idea of their
methods is that two words with similar meanings
are used in similar contexts. Although their methods
can obtain broad-coverage paraphrases, the obtained
paraphrases are not accurate enough to be utilized
1This word usually means ?water supply?.
787
for achieving precise matching since they contain
synonyms, near-synonyms, coordinate terms, hyper-
nyms, and inappropriate synonymous expressions.
Our approach makes the best use of an ordi-
nary dictionary and a Web corpus to extract broad-
coverage and precise synonym and hypernym-
hyponym expressions. First, synonymous expres-
sions are extracted from a dictionary. Then, the
distributional similarity of a pair of them is calcu-
lated using a Web corpus. Among extracted syn-
onymous expressions, those whose similarity is high
are used for the flexible matching. By utilizing only
synonymous expressions extracted from a dictionary
whose distributional similarity is high, we can ex-
clude synonymous expressions extracted from a dic-
tionary that are rarely used, and the pair of words
whose distributional similarity is high that is not ac-
tually a synonymous expression (is not listed in a
dictionary).
Another point of our method is to introduce SYN-
GRAPH data structure. So far, the effectiveness
of handling expressive divergence has been shown
for IR using a thesaurus-based query expansion
(Voorhees, 1994; Jacquemin et al, 1997). However,
their methods are based on a bag-of-words approach
and thus does not pay attention to sentence-level
synonymy with syntactic structure. MT requires
such precise handling of synonymy, and advanced
IR and QA also need it. To handle sentence-level
synonymy precisely, we have to consider the combi-
nation of expressive divergence, which may cause
combinatorial explosion. To overcome this prob-
lem, an ID is assigned to each synonymous group,
and then SYNGRAPH data structure is introduced
to pack expressive divergence.
2 Synonymy Database
This section describes a method for constructing a
synonymy database. First, synonym/hypernym re-
lations are automatically extracted from an ordinary
dictionary, and the distributional similarity of a pair
of synonymous expressions is calculated using a
Web corpus. Then, the extracted synonymous ex-
pressions whose similarity is high are used for the
flexible matching.
2.1 Synonym/hypernym Extraction from an
Ordinary Dictionary
Although there were some attempts to extract syn-
onymous expressions from a dictionary (Nakamura
and Nagao, 1988; Tsurumaru et al, 1986), they ex-
tracted only hypernym-hyponym relations from the
limited entries. In contrast, our method extracts not
only hypernym-hyponym relations, but also basic
synonym relations, predicate synonyms, adverbial
synonyms and synonym relations between a word
and a phrase.
The last word of the first definition sentence is
usually the hypernym of an entry word. Some defi-
nition sentences in a Japanese dictionary are shown
below (the left word of ?:? is an entry word, the right
sentence is a definition, and words in bold font is the
extracted words):
yushoku (dinner) : yugata (evening) no (of)
shokuji (meal).
jushin (barycenter) : omosa (weight) ga (is)
tsuriatte (balance) tyushin (center) tonaru
(become) ten (spot).
For example, the last word shokuji (meal) can be
extracted as the hypernym of yushoku (dinner). In
some cases, however, a word other than the last word
can be a hypernym or synonym. These cases can be
detected by sentence-final patterns as follows (the
underlined expressions represent the patterns):
Hypernyms
dosei (Saturn) : wakusei (planet) no (of) hitotsu
(one).
tobi (kite) : taka (hawk) no (of) issyu (kind).
Synonyms / Synonymous Phrases
ice : ice cream no (of) ryaku (abbreviation).
mottomo (most) : ichiban (best). (? one word defi-
nition)
moyori (nearest) : ichiban (best) chikai (near)
tokoro (place)2. (? less than three phrases)
2.2 Calculating the Distributional Similarity
using a Web Corpus
The similarity between a pair of synonymous ex-
pressions is calculated based on distributional sim-
ilarity (J.R.Firth, 1957; Harris, 1968) using the
Web corpus collected by (Kawahara and Kurohashi,
2006). The similarity between two predicates is de-
fined to be one between the patterns of case exam-
ples of each predicate (Kawahara and Kurohashi,
2001). The similarity between two nouns are defined
2If the last word of a sentence is a highly general term such
as koto (thing) and tokoro (place), it is removed from the syn-
onymous expression.
788
gakkou (school)
gakue n (a ca d e m y )
<school>
s h ogakkou (p r i m a r y  school)
s h ogaku (e le m e n t a r y  school)
<p r i m a r y  school>
koukou (hi g h school)
kout ougakkou (se n i or  hi g h)
<hi g h school>
t okor o (p la ce )
<p la ce >
h an t e n (b lob )
m ad ar a (m ot t le )
b uc h i (m a cu la )
<b lob >
t e n  (sp ot )
<sp ot >
j us h i n (b a r y ce n t e r )
<b a r y ce n t e r >
 m oy or i (n e a r e st )
 i c h i b an (b e st )  c h i kaku(n e a r )
<n e a r e st >
m ot t om o (m ost )
i c h i b an (b e st )
<m ost >
p oly se m i c w or dhy p e r n y m -hy p on y m  r e la t i on
 t e n  (sp ot )
<sp ot >
t e n  (sp ot )
p oc h i (d ot )
c h i s an a (sm a ll)   s h i r us h i(m a r k )
<sp ot >
 t e n  (sp ot )
b as h o (a r e a )
i c h i  (loca t i on )
<sp ot >
s h i r us h i  (m a r k )
<m a r k >
sy n on y m ou s g r ou p
Figure 1: An example of synonymy database.
as the ratio of the overlapped co-occurrence words
using the Simpson coefficient. The Simpson coeffi-
cient is computed as |T (w1)?T (w2)|min(|T (w1)|,|T (w2)|) , where T (w) is
the set of co-occurrence words of word w.
2.3 Integrating the Distributional Similarity
into the Synonymous Expressions
Synonymous expressions can be extracted from a
dictionary as described in Section 2.1. However,
some extracted synonyms/hypernyms are not appro-
priate since they are rarely used. Especially, in the
case of that a word has multiple senses, the syn-
onym/hypernym extracted from the second or later
definition might cause the inappropriate matching.
For example, since ?suidou? has two senses, the
two synonym pairs, ?suidou? = ?jyosuidou(water
supply)? and ?suidou? = ?kaikyou(strait)?, are ex-
tracted. The second sense is rarely used, and thus if
the synonymy pair extracted from the second defi-
nition is used as a synonym relation, an inappropri-
ate matching through this synonymmight be caused.
Therefore, only the pairs of synonyms/hypernyms
whose distributional similarity calculated in Section
2.2 is high are utilized for the flexible matching.
The similarity threshold is set to 0.4 for synonyms
and to 0.3 for hypernyms. For example, since the
similarity between ?suidou? and ?kaikyou? is 0.298,
this synonym is not utilized.
2.4 Synonymy Database Construction
With the extracted binomial relations, a synonymy
database can be constructed. Here, polysemic words
should be treated carefully3. When the relations
A=B and B=C are extracted, and B is not polysemic,
3If a word has two or more definition items in the dictionary,
the word can be regarded as polysemic.
they can be merged into A=B=C. However, if B is
polysemic, the synonym relations are not merged
through a polysemic word. In the same way, as for
hypernym-hyponym relations, A ? B and B ? C,
and A ? B and C ? B are not merged if B is pol-
ysemic. By merging binomial synonym relations
with the exception of polysemic words, synony-
mous groups are constructed first. They are given
IDs, hereafter called SYNID4. Then, hypernym-
hyponym relations are established between synony-
mous groups. We call this resulting data as syn-
onymy database. Figure 1 shows examples of syn-
onymous groups in the synonymy database. In this
paper, SYNID is denoted by using English gloss
word, surrounded by ? ? ? ?.
3 SYNGRAPH
3.1 SYNGRAPH Data Structure
SYNGRAPH data structure is an acyclic directed
graph, and the basis of SYNGRAPH is the depen-
dency structure of an original sentence (in this paper,
a robust parser (Kurohashi and Nagao, 1994) is al-
ways employed). In the dependency structure, each
node consists of one content word and zero or more
function words, which is called a basic node here-
after. If the content word of a basic node belongs to
a synonymous group, a new node with the SYNID is
attached to it, and it is called a SYN node hereafter.
For example, in Figure 2, the shaded nodes are basic
nodes and the other nodes are SYN nodes5.
Then, if the expression conjoining two or more
4Spelling variations such as use of Hiragana, Katakana
or Kanji are handled by the morphological analyzer JUMAN
(Kurohashi et al, 1994).
5The reason why we distinguish basic nodes from SYN
nodes is to give priority to exact matching over synonymous
matching.
789
hotel ni
<hotel> ni i c hi b a n( b es t)
<m os t> c hi k a i( n ea r )
<n ea r es t>
0.99
1 .0
0.99
0.99
1 .0
1 .0
m oy or i( n ea r es t)
0.99
1 .0
<n ea r es t>
N M S = 0 . 9 8
N M S = 0 . 9
ek i( s ta ti on ) w a N M S = 1 . 01 .0
ek i( s ta ti on ) w a1 .0
hotel no
<hotel> no0.99
1 .0
Figure 2: SYNGRAPH matching.
nodes corresponds to one synonymous group, a
SYN node is added there. In Figure 2, ?nearest? is
such a SYN node. Furthermore, if one SYN node
has a hyper synonymous group in the synonymy
database, the SYN node with the hyper SYNID is
also added.
In this SYNGRAPH data structure, each node has
a score, NS (Node Score), which reflects how much
the expression of the node is shifted from the orig-
inal expression. We explain how to calculate NSs
later.
3.2 SYNGRAPH Matching
Two SYNGRAPHs match if and only if
? all the nodes in one SYNGRAPH can be
matched to the nodes in the other one,
? the matched nodes in two SYNGRAPHs have
the same dependency structure, and
? the nodes can cover the original sentences.
An example of SYNGRAPH matching is illustrated
in Figure 2. When two SYNGRAPHs match each
other, their matching score is calculated as follows.
First, the matching score of the matching two nodes,
NMS (Node Match Score) is calculated with their
node scores, NS1 and NS2,
NMS = NS 1 ? NS 2 ? FI Penalty,
where we define FI Penalty (Function word Incon-
sistency Penalty) is 0.9 when their function words
are not the same, and 1.0 otherwise.
Then, the matching score of two SYNGRAPHs,
SMS (SYNGRAPH Match Score) is defined as the
average of NMSs weighted by the number of basic
nodes,
SMS =
?
(# of basic nodes ? NMS)?
# of basic nodes
.
In an example shown in Figure 2, the NMS of the
left-hand side hotel node and the right-hand side ho-
tel node is 0.9 (= 1.0 ? 1.0 ? 0.9). The NMS of the
left-hand side ?nearest? node and the right-hand side
?nearest? node is 0.98 (= 0.99 ? 0.99 ? 1.0). Then,
the SMS becomes 0.9?2+0.98?3+1.0?22+3+2 = 0.96.
3.3 SYNGRAPH Transformation of Synonymy
Database
The synonymy database is transformed into SYN-
GRAPHs, where SYNGRAPH matching is itera-
tively applied to interpret the mutual relationships
in the synonymy database, as follows:
Step 1: Each expression in each synonymous group
is parsed and transformed into a fundamental SYN-
GRAPH.
Step 2: SYNGRAPH matching is applied to check
whether a sub-tree of one expression is matched with
any other whole expressions. If there is a match, a
new node with the SYNID of the whole matched ex-
pression is assigned to the partially matched nodes
group. Furthermore, if the SYNID has a hyper syn-
onymous group, another new node with the hyper-
nym SYNID is also assigned. This checking process
starts from small parts to larger parts.
We define the NS of the newly assigned SYN
node as the SMS multiplied by a relation penalty.
Here, we define the synonymy relation penalty as
0.99 and the hypernym relation penalty as 0.7. For
instance, the NS of ?underwater? node is 0.99 and
that of ?inside? node is 0.7.
Step 3: Repeat Step 2, until no more new SYN node
can be assigned to any expressions. In the case of
Figure 3 example, the new SYN node, ?diving? is
given to ?suityu (underwater) ni (to) moguru (dive)?
of ?diving(sport)? at the second iteration.
4 Flexible Matching using SYNGRAPH
We use example-based machine translation (EBMT)
as an example to explain how our flexible matching
method works (Figure 4). EBMT generates a trans-
lation by combining partially matching TEs with an
input6. We use flexible matching to fully exploit the
TEs.
6How to select the best TEs and combine the selected TEs
for generating a translation is omitted in this paper.
790
ni
ni
ni
ni
sport
ni
ni
moguru(dive)
ni
<underwater>
<inside>
<diving>
0.99
0.7
0.99
1.0
1.0
diving1.0
<diving(sport)>
mizu(water)
suityu(underwater)
naka(inside)
1.0
1.0
1.0
<underwater>
<inside>
<inside>0.99
naka(inside)
<inside>
moguru(dive)
0.99
1.0
1.0
<inside>0.7
mizu(water)1.0
sensui(diving)1.0
<diving>Synonymy databaseTranslation example
naka(inside)
suityu(underwater) no
no
1.0
1.0
suru
sport
suru
<diving>
<diving(sport)>
 
 
sensui(diving)
0.99
1.0
0.93
1.0 <underwater> 0.99
Figure 3: SYNGRAPH transformation of synonymy database.input sentence translation examplestransform into a SYNGRAPH
Japanese English
Figure 4: Flexible matching using SYNGRAPH in
EBMT.
First, TEs are transformed into SYNGRAPHs by
SYNGRAPH matching with SYNGRAPHs of the
synonymy database. Since the synonymy database
has been transformed into SYNGRAPHs, we do not
need to care the combinations of synonymous ex-
pressions any more. In the example shown in Fig-
ure 3, ?sensui (diving) suru (do) sport? in the TE is
given ?diving(sport)? node just by looking at SYN-
GRAPHs in ?diving(sport)? synonymous group.
Then, an input sentence is transformed into a
SYNGRAPH by SYNGRAPH matching, and then
the SYNGRAPH matching is applied between all
the sub trees of the input SYNGRAPH and SYN-
GRAPHs of TEs to retrieve the partially matching
TEs.
5 Experiments and Discussion
5.1 Evaluation on Machine Translation Task
To see the effectiveness of the our proposed method,
we conducted our evaluations on a MT task us-
ing Japanese-English translation training corpus
(20,000 sentence pairs) and 506 test sentences of
IWSLT?057. As an evaluation measure, NIST and
BLEU were used based on 16 reference English sen-
tences for each test sentence.
7http://www.is.cs.cmu.edu/iwslt2005/.
Table 1: Size of synonymy database.
# of synonymous group 5,046
# of hypernym-hyponym relation 18,590
The synonymy database used in the experiments
was automatically extracted from the REIKAI-
SHOGAKU dictionary (a dictionary for children),
which consists of about 30,000 entries. Table
1 shows the size of the constructed synonymy
database.
As a base translation system, we used an EBMT
system developed by (Kurohashi et al, 2005). Ta-
ble 2 shows the experimental results. ?None? means
the baseline system without using the synonymy
database. ?Synonym? is the system using only
synonymous relations, and it performed best and
achieved 1.2% improvement for NIST and 0.8%
improvement for BLEU over the baseline. These
differences are statistically significant (p < 0.05).
Some TEs that can be retrieved by our flexible
matching are shown below:
? input: fujin (lady) you (for) toile (toilet) ?
TE: josei (woman) you (for) toile (toilet)
? input: kantan-ni ieba (in short)?TE: tsumari
(in other words)
On the other hand, if the system also uses
hypernym-hyponym relation (?Synonym Hyper-
nym?), the score goes down. It proves that hyper-
nym examples are not necessarily good for trans-
lation. For example, for a translation of depato
(department store), its hypernym ?mise(store)? was
used, and it lowered the score.
Major errors are caused by the deficiency of word
sense disambiguation. When a polysemic word oc-
curs in a sentence, multiple SYNIDs are attached
to the word, and thus, the incorrect matching might
be occurred. Incorporation of unsupervised word-
791
Table 2: Evaluation results on MT task.
Synonymy DB NIST BLEU
None 8.023 0.375
Synonym 8.121 0.378
Synonym Hypernym 8.010 0.374
Table 3: Evaluation results on IR task.
Method Synonymy DB R-prec
Best IREX system ? 0.493
BM25 ? 0.474
None 0.492
Our method Synonym 0.509
Synonym Hypernym 0.514
sense-disambiguation of words in dictionary defini-
tions and matching sentences is one of our future
research targets.
5.2 Evaluation on Information Retrieval Task
To demonstrate the effectiveness of our method
in other NLP tasks, we also evaluated it in IR.
More concretely, we extended word-based impor-
tance weighting of Okapi BM25 (Robertson et al,
1994) to SYN node-based weighting. We used the
data set of IR evaluation workshop IREX, which
contains 30 queries and their corresponding relevant
documents in 2-year volume of newspaper articles8.
Table 3 shows the experimental results, which are
evaluated with R-precision. The baseline system is
our implementation of OKAPI BM25. Differently
from the MT task, the system using both synonym
and hypernym-hyponym relations performed best,
and its improvement over the baseline was 7.8%
relative. This difference is statistically significant
(p < 0.05). This result shows the wide applicabil-
ity of our flexible matching method for NLP tasks.
Some examples that can be retrieved by our flexible
matching are shown below:
? query: gakkou-ni (school) computer-wo
(computer) dounyuu (introduce) ? docu-
ment: shou-gakkou-ni (elementary school)
pasokon-wo (personal computer) dounyuu
(introduce)
6 Conclusion
This paper proposed a flexible matching method by
extracting synonymous expressions from an ordi-
nary dictionary and a Web corpus, and introducing
SYNGRAPH data structure. We confirmed the ef-
fectiveness of our method on experiments of ma-
chine translation and information retrieval.
8http://nlp.cs.nyu.edu/irex/.
Our future research targets are to incorporate
word sense disambiguation to our framework, and
to extend SYNGRAPH matching to more structural
paraphrases.
References
Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence align-
ment. In HLT-NAACL 2003, pages 16?23.
Zellig Harris. 1968. Mathematical Structures of Language.
Wiley.
Christian Jacquemin, Judith L. Klavans, and Evelyne Tzouker-
mann. 1997. Expansion of multi-word terms for indexing
and retrieval using morphology and syntax. In 35th Annual
Meeting of the Association for Computational Linguistics,
pages 24?31.
J.R.Firth. 1957. A synopsis of linguistic theory, 1933-1957. In
Studies in Linguistic Analysis, pages 1?32. Blackwell.
Daisuke Kawahara and Sadao Kurohashi. 2001. Japanese case
frame construction by coupling the verb and its closest case
component. In Proc. of HLT 2001, pages 204?210.
Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame
compilation from the web using high-performance comput-
ing. In Proc. of LREC-06.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic anal-
ysis method of long japanese sentences based on the detec-
tion of conjunctive structures. Computational Linguistics,
20(4):507?534.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and
Makoto Nagao. 1994. Improvements of Japanese mor-
phological analyzer JUMAN. In Proc. of the International
Workshop on Sharable Natural Language, pages 22?28.
Sadao Kurohashi, Toshiaki Nakazawa, Kauffmann Alexis, and
Daisuke Kawahara. 2005. Example-based machine transla-
tion pursuing fully structural NLP. In Proc. of IWSLT?05,
pages 207?212.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question answering. Natural Language Engineer-
ing, 7(4):343?360.
Junichi Nakamura and Makoto Nagao. 1988. Extraction of se-
mantic information from an ordinary english dictionary and
its evaluation. In Proc. of the 12th COLING, pages 459?464.
S. E. Robertson, S. Walker, S. Jones, M.M. Hancock-Beaulieu,
and M. Gatford. 1994. Okapi at TREC-3. In the third Text
REtrieval Conference (TREC-3).
Hiroaki Tsurumaru, Toru Hitaka, and Sho Yoshida. 1986. An
attempt to automatic thesaurus construction from an ordinary
japanese language dictionary. In Proc. of the 11th COLING,
pages 445?447.
Ellen M. Voorhees. 1994. Query expansion using lexical-
semantic relations. In SIGIR, pages 61?69.
792
Proceedings of NAACL HLT 2007, Companion Volume, pages 201?204,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Three-step Deterministic Parser for Chinese Dependency Parsing 
 
Kun Yu Sadao Kurohashi Hao Liu 
Graduate School of Informatics Graduate School of Informatics Graduate School of Information 
Science and Technology 
Kyoto University Kyoto University The University of Tokyo 
kunyu@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp liuhao@kc.t.u-tokyo.ac.jp 
 
Abstract 
  This paper presents a three-step dependency 
parser to parse Chinese deterministically. By divid-
ing a sentence into several parts and parsing them 
separately, it aims to reduce the error propagation 
coming from the greedy characteristic of determi-
nistic parsing. Experimental results showed that 
compared with the deterministic parser which 
parsed a sentence in sequence, the proposed parser 
achieved extremely significant improvement on 
dependency accuracy.  
1 Introduction 
Recently, as an attractive alternative to probabilistic 
parsing, deterministic parsing (Yamada and Matsumoto, 
2003; Nivre and Scholz, 2004) has drawn great attention 
with its high efficiency, simplicity and good accuracy 
comparable to the state-of-the-art generative probabilis-
tic models. The basic idea of deterministic parsing is 
using a greedy parsing algorithm that approximates a 
globally optimal solution by making a sequence of lo-
cally optimal choices (Hall et al, 2006). This greedy 
idea guarantees the simplicity and efficiency, but at the 
same time it also suffers from the error propagation 
from the previous parsing choices to the left decisions.  
For example, given a Chinese sentence, which means 
Paternity test is a test that gets personal identity 
through DNA analysis, and it brings proof for finding 
lost children, the correct dependency tree is shown by 
solid line  (see Figure 1). But, if word ??(through) is 
incorrectly parsed as depending on word ?(is) (shown 
by dotted line), this error will result in the incorrect 
parse of word??(a test) as depending on word ??
(brings) (shown by dotted line).  
This problem exists not only in Chinese, but also in 
other languages. Some efforts have been done to solve 
this problem. Cheng et al (2005) used a root finder to 
divide one sentence into two parts by the root word and 
parsed them separately. But the two-part division is not 
enough when a sentence is composed of several coordi-
nating sub-sentences. Chang et al (2006) applied a 
pipeline framework in their dependency parser to make 
the local predictions more robust. While it did not show 
great help for stopping the error propagation between 
different parsing stages.  
 
Figure 1. Dependency tree of a sentence  (word sequence is top-down) 
This paper focuses on resolving this issue for Chi-
nese. After analyzing the dependency structure of sen-
tences in Penn Chinese Treebank 5.1 (Xue et al, 2002), 
we found an interesting phenomenon: if we define a 
main-root as the head of a sentence, and define a sub-
sentence as a sequence of words separated by punctua-
tions, and the head1 of these words is the child of main-
root or main-root itself, then the punctuations that de-
pend on main-root can be a separator of sub-sentences.  
For example, in the example sentence there are three 
punctuations marked as PU_A, PU_B and PU_C, in 
which PU_B and PU_C depends on main-root but 
PU_A depends on word ??(gets). According to our 
observation, PU_B and PU_C can be used for segment-
ing this sentence into two sub-sentences A and B (cir-
cled by dotted line in Figure 2), where the sub-root of A 
is main-root and the sub-root of B depends on main-root.  
This phenomenon gives us a useful clue: if we divide 
a sentence by the punctuations whose head is main-root, 
then the divided sub-sentences are basically independ-
ent of each other, which means we can parse them sepa-
rately. The shortening of sentence length and the recog-
nition of sentence structure guarantee the robustness of 
deterministic parsing. The independent parsing of each 
sub-sentence also prevents the error-propagation. In 
                                                 
1
 The head of sub-sentence is defined as a sub-root. 
201
addition, because the sub-root depends on main-root or 
is main-root itself, it is easy to combine the dependency 
structure of each sub-sentence to create the final de-
pendency tree. 
 
Figure 2. A segmentation of the sentence in Figure 1 
Based on above analyses, this paper proposes a three-
step deterministic dependency parser for Chinese, which 
works as: 
Step1(Sentence Segmentation): Segmenting a sen-
tence into sub-sentences by punctuations (sub-sentences 
do not contain the punctuations for segmentation); 
Step2(Sub-sentence Parsing): Parsing each sub-
sentence deterministically; 
Step3(Parsing Combination): Finding main-root 
among all the sub-roots, then combining the dependency 
structure of sub-sentences by making main-root as the 
head of both the left sub-roots and the punctuations for 
sentence segmentation. 
2 Sentence Segmentation 
As mentioned in section 1, the punctuations depending 
on main-root can be used to segment a sentence into 
several sub-sentences, whose sub-root depends on main-
root or is main-root. But by analysis, we found only 
several punctuations were used as separator commonly. 
To ensure the accuracy of sentence segmentation, we 
first define the punctuations which are possible for seg-
mentation as valid punctuation, which includes comma, 
period, colon, semicolon, question mark, exclamatory 
mark and ellipsis. Then the task in step 1 is to find 
punctuations which are able to segment a sentence from 
all the valid punctuations in a sentence, and use them to 
divide the sentence into two or more sub-sentences. 
We define a classifier (called as sentence seg-
menter) to classify the valid punctuations in a sentence 
to be good or bad for sentence segmentation. SVM (Se-
bastiani, 2002) is selected as classification model for its 
robustness to over-fitting and high performance.  
Table 1 shows the binary features defined for sen-
tence segmentation. We use a lexicon consisting of all 
the words in Penn Chinese Treebank 5.1 to lexicalize 
word features. For example, if word ? (for) is the 
27150th word in the lexicon, then feature Word1 of 
PU_B (see Figure 2) is ?27150:1?. The pos-tag features 
are got in the same way by a pos-tag list containing 33 
pos-tags, which follow the definition in Penn Chinese 
Treebank. Such method is also used to get word and 
pos-tag features in other modules. 
Table 1. Features for sentence segmenter 
Feature Description 
Wordn/Posn word/pos-tag in different position, n=-2,-1,0,1,2 
Word_left/ 
Pos_left 
word/pos-tag between the first left valid punctua-
tion and current punctuation 
Word_right/ 
Pos_right 
word/pos-tag between current punctuation and 
the first right valid punctuation 
#Word_left/ 
#Word_right 
if the number of words between the first left/right 
valid punctuation and current punctuation is 
higher than 2, set as 1; otherwise set as 0 
V_left/ 
V_right 
if there is a verb between the first left/right valid 
punctuation and current punctuation, set as 1; 
otherwise set as 0 
N_leftFirst/ 
N_rightFirst 
if the left/right neighbor word is a noun, set as 1; 
otherwise set as 0 
P_rightFirst/ 
CS_rightFirst 
if the right neighbor word is a preposi-
tion/subordinating conjunction, set as 1; other-
wise set as 0 
3 Sub-sentence Parsing  
3.1 Parsing Algorithm 
The parsing algorithm in step 2 is a shift-reduce parser 
based on (Yamada and Matsumoto, 2003). We call it as 
sub-sentence parser. 
Two stacks P and U are defined, where stack P keeps 
the words under consideration and stack U remains all 
the unparsed words. All the dependency relations cre-
ated by the parser are stored in queue A.  
At start, stack P and queue A are empty and stack U 
contains all the words. Then word on the top of stack U 
is pushed into stack P, and a trained classifier finds 
probable action for word pair <p,u> on the top of the 
two stacks. After that, according to different actions, 
dependency relations are created and pushed into queue 
A, and the elements in the two stacks move at the same 
time. Parser stops when stack U is empty and the de-
pendency tree can be drawn according to the relations 
stored in queue A.  
Four actions are defined for word pair <p, u>: 
LEFT: if word p modifies word u, then push pu 
into A and push u into P. 
RIGHT: if word u modifies word p, then push up 
into A and pop p. 
REDUCE: if there is no word u? (u??U and u??u) 
which modifies p, and word next to p in stack P is p?s 
head, then pop p. 
SHIFT: if there is no dependency relation between p 
and u, and word next to p in stack P is not p?s head, then 
push u into stack P. 
202
We construct a classifier for each action separately, 
and classify each word pair by all the classifiers. Then 
the action with the highest classification score is se-
lected. SVM is used as the classifier, and One vs. All 
strategy (Berger, 1999) is applied for its good efficiency 
to extend binary classifier to multi-class classifier. 
3.2 Features 
Features are crucial to this step. First, we define some 
features based on local context (see Flocal in Table 2), 
which are often used in other deterministic parsers 
(Yamada and Matsumoto, 2003; Nivre et al, 2006). 
Then, to get top-down information, we add some global 
features (see Fglobal in Table 2). All the features are bi-
nary features, except that Distance is normalized be-
tween 0-1 by the length of sub-sentence.  
Before parsing, we use a root finder (i.e. the sub-
sentence root finder introduced in Section 4) to get 
Rootn feature, and develop a baseNP chunker to get 
BaseNPn feature. In the baseNP chunker, IOB represen-
tation is applied for each word, where B means the word 
is the beginning of a baseNP, I means the word is inside 
of a baseNP, and O means the word is outside of a 
baseNP. Tagging is performed by SVM with One vs. All 
strategy. Features used in baseNP chunking are current 
word, surrounding words and their corresponding pos-
tags. Window size is 5. 
Table 2. Features for sub-sentence parser 
Feature Description 
Wordn/ 
Posn 
word/pos-tag in different position, 
n= P0, P1, P2, U0, U1, U2 (Pi/Ui mean 
the ith position from top in stack P/U) 
Word_childn/ 
Pos_childn 
the word/pos-tag of the children of 
Wordn, n= P0, P1, P2, U0, U1, U2 
Local 
Feature 
(Flocal) 
Distance distance between p and u in sentence 
Rootn 
if Wordn is the sub-root of this sub-
sentence, set as 1; otherwise set as 0 
Global 
Feature 
(Fglobal) BaseNPn baseNP tag of Wordn 
Table 3. Features for sentence/sub-sentence root finder 
Feature Description 
Wordn/Posn words in different position, n=-2,-1,0,1,2 
Word_left/Pos_left wordn/posn where n<-2 
Word_right/Pos_right wordn/posn where n>2 
#Word_left/ 
#Word_right 
if the number of words between the 
start/end of sentence and current word is 
higher than 2, set as 1; otherwise set as 0 
V_left/V_right 
if there is a verb between the start/end of 
sentence and current word, set as 1; oth-
erwise set as 0 
Nounn/Verbn/Adjn 
if the word in different position is a 
noun/verb/adjective, set as 1; otherwise 
set as 0. n=-2,-1,1,2 
Dec_right if the word next to current word in right 
side is ?(of), set as 1; otherwise set as 0 
CC_left 
if there is a coordinating conjunction 
between the start of sentence and current 
word, set as 1; otherwise set as 0 
BaseNPn baseNP tag of Wordn 
4 Parsing Combination 
A root finder is developed to find main-root for parsing 
combination. We call it as sentence root finder. We 
also retrain the same module to find the sub-root in step 
2, and call it as sub-sentence root finder. 
We define the root finding problem as a classification 
problem. A classifier, where we still select SVM, is 
trained to classify each word to be root or not. Then the 
word with the highest classification score is chosen as 
root. All the binary features for root finding are listed in 
Table 3. Here the baseNP chunker introduced in section 
3.2 is used to get the BaseNPn feature. 
5 Experimental Results 
5.1 Data Set and Experimental Setting 
We use Penn Chinese Treebank 5.1 as data set. To 
transfer the phrase structure into dependency structure, 
head rules are defined based on Xia?s head percolation 
table (Xia and Palmer, 2001). 16,984 sentences and 
1,292 sentences are used for training and testing. The 
same training data is also used to train the sentence 
segmenter, the baseNP chunker, the sub-sentence root 
finder, and the sentence root finder. During both train-
ing and testing, the gold-standard word segmentation 
and pos-tag are applied. 
TinySVM is selected as a SVM toolkit. We use a 
polynomial kernel and set the degree as 2 in all the ex-
periments.  
5.2 Three-step Parsing vs. One-step Parsing 
First, we evaluated the dependency accuracy and root 
accuracy of both three-step parsing and one-step parsing. 
Three-step parsing is the proposed parser and one-step 
parsing means parsing a sentence in sequence (i.e. only 
using step 2). Local and global features are used in both 
of them. 
Results (see Table 4) showed that because of the 
shortening of sentence length and the prevention of er-
ror propagation three-step parsing got 2.14% increase 
on dependency accuracy compared with one-step pars-
ing. Based on McNemar?s test (Gillick and Cox, 1989), 
this improvement was considered extremely statistically 
significant (p<0.0001).  In addition, the proposed parser 
got 1.01% increase on root accuracy.  
Table 4. Parsing result of three-step and one-step parsing 
Parsing Strategy Dep.Accu. (%) 
Root Accu. 
(%) 
Avg. Parsing 
Time (sec.) 
One-step Parsing 82.12 74.92 22.13 
Three-step Parsing 84.26 (+2.14) 
75.93 
(+1.01) 
24.27 
(+2.14) 
Then we tested the average parsing time for each sen-
tence to verify the efficiency of proposed parser. The 
average sentence length is 21.68 words. Results (see 
Table 4) showed that compared with one-step parsing, 
the proposed parser only used 2.14 more seconds aver-
203
agely when parsing one sentence, which did not affect 
efficiency greatly. 
To verify the effectiveness of proposed parser on 
complex sentences, which contain two or more sub-
sentences according to our definition, we selected 665 
such sentences from testing data set and did evaluation 
again. Results (see Table 5) proved that our parser 
outperformed one-step parsing successfully.  
Table 5. Parsing result of complex sentence 
Parsing Strategy Dep.Accu. (%) Root Accu. (%) 
One-step Parsing 82.56 78.95 
Three-step Parsing 84.94 (+2.38) 79.25 (+0.30) 
5.3 Comparison with Others? Work 
At last, we compare the proposed parser with Nivre?s 
parser (Hall et al, 2006). We use the same head rules 
for dependency transformation as what were used in 
Nivre?s work. We also used the same training (section 
1-9) and testing (section 0) data and retrained all the 
modules. Results showed that the proposed parser 
achieved 84.50% dependency accuracy, which was 
0.20% higher than Nivre?s parser (84.30%).  
6 Discussion 
In the proposed parser, we used five modules: sentence 
segmenter (step1); sub-sentence root finder (step2); 
baseNP chunker (step2&3); sub-sentence parser (step2); 
and sentence root finder (step3).  
The robustness of the modules will affect parsing ac-
curacy. Thus we evaluated each module separately. Re-
sults (see Table 6) showed that all the modules got rea-
sonable accuracy except for the sentence root finder. 
Considering about this, in step 3 we found main-root 
only from the sub-roots created by step 2. Because the 
sub-sentence parser used in step 2 had good accuracy, it 
could provide relatively correct candidates for main-root 
finding. Therefore it helped decrease the influence of 
the poor sentence root finding to the proposed parser. 
Table 6. Evaluation result of each module 
Module F-score(%) Dep.Accu(%) 
Sentence Segmenter (M1) 88.04 --- 
Sub-sentence Root Finder (M2) 88.73 --- 
BaseNP Chunker (M3) 89.25 --- 
Sub-sentence Parser (M4) --- 85.56 
Sentence Root Finder (M5) 78.01 --- 
Then we evaluated the proposed parser assuming us-
ing gold-standard modules (except for sub-sentence 
parser) to check the contribution of each module to 
parsing. Results (see Table 7) showed that (1) the accu-
racy of current sentence segmenter was acceptable be-
cause only small increase on dependency accuracy and 
root accuracy was got by using gold-standard sentence 
segmentation; (2) the correct recognition of baseNP 
could help improve dependency accuracy but gave a 
little contribution to root accuracy; (3) the accuracy of 
both sub-sentence root finder and sentence root finder 
was most crucial to parsing. Therefore improving the 
two root finders is an important task in our future work. 
Table 7. Parsing result with gold-standard modules 
Gold-standard Module Dep.Accu(%) Root.Accu(%) 
w/o 84.26 75.93 
M1 84.51 76.24 
M1+M2 86.57 80.34 
M1+M2+M3 88.63 80.57 
M1+M2+M3+M5 91.25 91.02 
7 Conclusion and Future Work 
We propose a three-step deterministic dependency 
parser for parsing Chinese. It aims to solve the error 
propagation problem by dividing a sentence into inde-
pendent parts and parsing them separately. Results 
based on Penn Chinese Treebank 5.1 showed that com-
pared with the deterministic parser which parsed a sen-
tence in sequence, the proposed parser achieved ex-
tremely significant increase on dependency accuracy. 
Currently, the proposed parser is designed only for 
Chinese. But we believe it can be easily adapted to other 
languages because no language-limited information is 
used. We will try this work in the future. In addition, 
improving sub-sentence root finder and sentence root 
finder will also be considered in the future. 
Acknowledgement 
We would like to thank Dr. Daisuke Kawahara and Dr. Eiji Aramaki 
for their helpful discussions. We also thank the three anonymous 
reviewers for their valuable comments. 
Reference 
A.Berger. Error-correcting output coding for text classification. 1999. 
In Proceedings of the IJCAI-99 Workshop on Machine Learning 
for Information Filtering. 
M.Chang, Q.Do and D.Roth. 2006. A Pipeline Framework for De-
pendency Parsing. In Proceedings of Coling-ACL 2006. 
Y.Cheng, M.Asahara and Y.Matsumoto. 2005. Chinese Deterministic 
Dependency Analyzer: Examining Effects of Global Features and 
Root Node Finder. In Proceedings of IJCNLP 2005.  
L.Gillick and S.J.Cox. 1989. Some Statistical Issues in the Compari-
son of Speech Recognition Algorithms. In Proceedings of ICASSP.  
J.Hall, J.Nivre and J.Nilsson. 2006. Discriminative Classifiers for 
Deterministic Dependency Parsing. In Proceedings of Coling-ACL 
2006. pp. 316-323. 
J.Nivre and M.Scholz. 2004. Deterministic Dependency Parsing of 
English Text. In Proceedings of Coling 2004. pp. 64-70. 
F.Sebastiani. 2002. Machine learning in automated text categorization. 
ACM Computing Surveys, 34(1): 1-47. 
F.Xia and M.Palmer. 2001. Converting Dependency Structures to 
Phrase Structures. In HLT-2001. 
N.Xue, F.Chiou and M.Palmer. 2002. Building a Large-Scale Anno-
tated Chinese Corpus. In Proceedings of COLING 2002. 
H.Yamada and Y.Matsumoto. 2003. Statistical Dependency Analysis 
with Support Vector Machines. In Proceedings of IWPT. 2003. 
204
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 521?529,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The Effect of Corpus Size on Case Frame Acquisition
for Discourse Analysis
Ryohei Sasano
Graduate School of Informatics,
Kyoto University
sasano@i.kyoto-u.ac.jp
Daisuke Kawahara
National Institute of Information
and Communications Technology
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
This paper reports the effect of corpus size on
case frame acquisition for discourse analysis
in Japanese. For this study, we collected a
Japanese corpus consisting of up to 100 bil-
lion words, and constructed case frames from
corpora of six different sizes. Then, we ap-
plied these case frames to syntactic and case
structure analysis, and zero anaphora resolu-
tion. We obtained better results by using case
frames constructed from larger corpora; the
performance was not saturated even with a
corpus size of 100 billion words.
1 Introduction
Very large corpora obtained from the Web have
been successfully utilized for many natural lan-
guage processing (NLP) applications, such as prepo-
sitional phrase (PP) attachment, other-anaphora res-
olution, spelling correction, confusable word set dis-
ambiguation and machine translation (Volk, 2001;
Modjeska et al, 2003; Lapata and Keller, 2005; At-
terer and Schu?tze, 2006; Brants et al, 2007).
Most of the previous work utilized only the sur-
face information of the corpora, such as n-grams,
co-occurrence counts, and simple surface syntax.
This may be because these studies did not require
structured knowledge, and for such studies, the size
of currently available corpora is considered to have
been almost enough. For instance, while Brants et
al. (2007) reported that translation quality continued
to improve with increasing corpus size for training
language models at even size of 2 trillion tokens, the
increase became small at the corpus size of larger
than 30 billion tokens.
However, for more complex NLP tasks, such as
case structure analysis and zero anaphora resolution,
it is necessary to obtain more structured knowledge,
such as semantic case frames, which describe the
cases each predicate has and the types of nouns that
can fill a case slot. Note that case frames offer not
only the knowledge of the relationships between a
predicate and its particular case slot, but also the
knowledge of the relationships among a predicate
and its multiple case slots. To obtain such knowl-
edge, very large corpora seem to be necessary; how-
ever it is still unknown how much corpora would be
required to obtain good coverage.
For examples, Kawahara and Kurohashi pro-
posed a method for constructing wide-coverage case
frames from large corpora (Kawahara and Kuro-
hashi, 2006b), and a model for syntactic and case
structure analysis of Japanese that based upon case
frames (Kawahara and Kurohashi, 2006a). How-
ever, they did not demonstrate whether the coverage
of case frames was wide enough for these tasks and
how dependent the performance of the model was on
the corpus size for case frame construction.
This paper aims to address these questions. We
collect a very large Japanese corpus consisting of
about 100 billion words, or 1.6 billion unique sen-
tences from the Web. Subsets of the corpus are ran-
domly selected to obtain corpora of different sizes
ranging from 1.6 million to 1.6 billion sentences.
We construct case frames from each corpus and ap-
ply them to syntactic and case structure analysis, and
zero anaphora resolution, in order to investigate the
521
relationships between the corpus size and the perfor-
mance of these analyses.
2 Related Work
Many NLP tasks have successfully utilized very
large corpora, most of which were acquired from
the Web (Kilgarriff and Grefenstette, 2003). Volk
(2001) proposed a method for resolving PP attach-
ment ambiguities based upon Web data. Modjeska
et al (2003) used the Web for resolving nominal
anaphora. Lapata and Keller (2005) investigated the
performance of web-based models for a wide range
of NLP tasks, such as MT candidate selection, ar-
ticle generation, and countability detection. Nakov
and Hearst (2008) solved relational similarity prob-
lems using the Web as a corpus.
With respect to the effect of corpus size on NLP
tasks, Banko and Brill (2001a) showed that for
content sensitive spelling correction, increasing the
training data size improved the accuracy. Atterer
and Schu?tze (2006) investigated the effect of cor-
pus size in combining supervised and unsupervised
learning for two types of attachment decision; they
found that the combined system only improved the
performance of the parser for small training sets.
Brants et al (2007) varied the amount of language
model training data from 13 million to 2 trillion to-
kens and applied these models to machine transla-
tion systems. They reported that translation qual-
ity continued to improve with increasing corpus size
for training language models at even size of 2 tril-
lion tokens. Suzuki and Isozaki (2008) provided ev-
idence that the use of more unlabeled data in semi-
supervised learning could improve the performance
of NLP tasks, such as POS tagging, syntactic chunk-
ing, and named entities recognition.
There are several methods to extract useful infor-
mation from very large corpora. Search engines,
such as Google and Altavista, are often used to ob-
tain Web counts (e.g. (Nakov and Hearst, 2005;
Gledson and Keane, 2008)). However, search en-
gines are not designed for NLP research and the re-
ported hit counts are subject to uncontrolled vari-
ations and approximations. Therefore, several re-
searchers have collected corpora from the Web by
themselves. For English, Banko and Brill (2001b)
collected a corpus with 1 billion words from vari-
ety of English texts. Liu and Curran (2006) created
a Web corpus for English that contained 10 billion
words and showed that for content-sensitive spelling
correction the Web corpus results were better than
using a search engine. Halacsy et al (2004) created
a corpus with 1 billion words for Hungarian from
the Web by downloading 18 million pages. Others
utilize publicly available corpus such as the North
American News Corpus (NANC) and the Gigaword
Corpus (Graff, 2003). For instance, McClosky et al
(2006) proposed a simple method of self-training a
two phase parser-reranker system using NANC.
As for Japanese, Kawahara and Kurohashi
(2006b) collected 23 million pages and created a
corpus with approximately 20 billion words. Google
released Japanese n-gram constructed from 20 bil-
lion Japanese sentences (Kudo and Kazawa, 2007).
Several news wires are publicly available consisting
of tens of million sentences. Kotonoha project is
now constructing a balanced corpus of the present-
day written Japanese consisting of 50 million words
(Maekawa, 2006).
3 Construction of Case Frames
Case frames describe the cases each predicate has
and what nouns can fill the case slots. In this study,
case frames we construct case frames from raw cor-
pora by using the method described in (Kawahara
and Kurohashi, 2006b). This section illustrates the
methodology for constructing case frames.
3.1 Basic Method
After parsing a large corpus by a Japanese parser
KNP1, we construct case frames from modifier-head
examples in the resulting parses. The problems for
case frame construction are syntactic and semantic
ambiguities. In other words, the resulting parses in-
evitably contain errors and predicate senses are in-
trinsically ambiguous. To cope with these problems,
we construct case frames from reliable modifier-
head examples.
First, we extract modifier-head examples that had
no syntactic ambiguity, and assemble them by cou-
pling a predicate and its closest case component.
That is, we assemble the examples not by predi-
cates, such as tsumu (load/accumulate), but by cou-
1http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
522
Table 1: Examples of Constructed Case Frames.
Case slot Examples Generalized examples with rate
ga (nominative) he, driver, friend, ? ? ? [CT:PERSON]:0.45, [NE:PERSON]:0.08, ? ? ?tsumu (1) wo (accusative) baggage, luggage, hay, ? ? ? [CT:ARTIFACT]:0.31, ? ? ?(load) ni (dative) car, truck, vessel, seat, ? ? ? [CT:VEHICLE]:0.32, ? ? ?
tsumu (2) ga (nominative) player, children, party, ? ? ? [CT:PERSON]:0.40, [NE:PERSON]:0.12, ? ? ?
(accumulate) wo (accusative) experience, knowledge, ? ? ? [CT:ABSTRACT]:0.47, ? ? ?
... ... ...
ga (nominative) company, Microsoft, firm, ? ? ? [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, ? ? ?
hanbai (1) wo (accusative) goods, product, ticket, ? ? ? [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, ? ? ?
(sell) ni (dative) customer, company, user, ? ? ? [CT:PERSON]:0.28, ? ? ?
de (locative) shop, bookstore, site ? ? ? [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, ? ? ?
... ... ...
ples, such as nimotsu-wo tsumu (load baggage) and
keiken-wo tsumu (accumulate experience). Such
couples are considered to play an important role
for constituting sentence meanings. We call the as-
sembled examples as basic case frames. In order
to remove inappropriate examples, we introduce a
threshold ? and use only examples that appeared no
less than ? times in the corpora.
Then, we cluster the basic case frames to merge
similar case frames. For example, since nimotsu-
wo tsumu (load baggage) and busshi-wo tsumu (load
supplies) are similar, they are merged. The similar-
ity is measured by using a Japanese thesaurus (The
National Language Institute for Japanese Language,
2004). Table 1 shows examples of constructed case
frames.
3.2 Generalization of Examples
When we use hand-crafted case frames, the data
sparseness problem is serious; by using case frames
automatically constructed from a large corpus, it was
alleviated to some extent but not eliminated. For in-
stance, there are thousands of named entities (NEs)
that cannot be covered intrinsically. To deal with
this problem, we generalize the examples of the case
slots. Kawahara and Kurohashi also generalized ex-
amples but only for a few types. In this study, we
generalize case slot examples based upon common
noun categories and NE classes.
First, we generalize the examples based upon the
categories that tagged by the Japanese morpholog-
ical analyzer JUMAN2. In JUMAN, about 20 cat-
egories are defined and tagged to common nouns.
For example, ringo (apple), inu (dog) and byoin
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
Table 2: Definition of NE in IREX.
NE class Examples
ORGANIZATION NHK Symphony Orchestra
PERSON Kawasaki Kenjiro
LOCATION Rome, Sinuiju
ARTIFACT Nobel Prize
DATE July 17, April this year
TIME twelve o?clock noon
MONEY sixty thousand dollars
PERCENT 20%, thirty percents
(hospital) are tagged as FOOD, ANIMAL and FA-
CILITY, respectively. For each category, we calcu-
late the ratio of the categorized example among all
case slot examples, and add it to the case slot (e.g.
[CT:FOOD]:0.07).
We also generalize the examples based upon NE
classes. We use a common standard NE defini-
tion for Japanese provided by the IREX (1999).
We first recognize NEs in the source corpus by
using an NE recognizer (Sasano and Kurohashi,
2008); and then construct case frames from the NE-
recognized corpus. Similar to the categories, for
each NE class, we calculate the NE ratio among all
the case slot examples, and add it to the case slot
(e.g. [NE:PERSON]:0.12). The generalized exam-
ples are also included in Table 1.
4 Discourse Analysis with Case Frames
In order to investigate the effect of corpus size
on complex NLP tasks, we apply the constructed
cases frames to an integrated probabilistic model
for Japanese syntactic and case structure analysis
(Kawahara and Kurohashi, 2006a) and a probabilis-
tic model for Japanese zero anaphora resolution
(Sasano et al, 2008). In this section, we briefly de-
scribe these models.
523
4.1 Model for Syntactic and Case Structure
Analysis
Kawahara and Kurohashi (2006a) proposed an in-
tegrated probabilistic model for Japanese syntactic
and case structure analysis based upon case frames.
Case structure analysis recognizes predicate argu-
ment structures. Their model gives a probability to
each possible syntactic structure T and case struc-
ture L of the input sentence S, and outputs the syn-
tactic and case structure that have the highest proba-
bility. That is to say, the system selects the syntactic
structure Tbest and the case structure Lbest that max-
imize the probability P (T,L|S):
(Tbest, Lbest) = argmax
(T,L)
P (T,L|S)
= argmax
(T,L)
P (T,L, S) (1)
The last equation is derived because P (S) is con-
stant. P (T,L, S) is defined as the product of a prob-
ability for generating a clause Ci as follows:
P (T,L, S) = ?
i=1..n
P (Ci|bhi) (2)
where n is the number of clauses in S, and bhi is
Ci?s modifying bunsetsu3. P (Ci|bhi) is approxi-
mately decomposed into the product of several gen-
erative probabilities such as P (A(sj) = 1|CFl, sj)
and P (nj |CFl, sj , A(sj) = 1), where the function
A(sj) returns 1 if a case slot sj is filled with an input
case component; otherwise 0. P (A(sj)=1|CFl, sj)
denotes the probability that the case slot sj is filled
with an input case component, and is estimated from
resultant case structure analysis of a large raw cor-
pus. P (nj |CFl, sj , A(sj) = 1) denotes the proba-
bility of generating a content part nj from a filled
case slot sj in a case frame CFl, and is calculated
by using case frames. For details see (Kawahara and
Kurohashi, 2006a).
4.2 Model for Zero Anaphora Resolution
Anaphora resolution is one of the most important
techniques for discourse analysis. In English, overt
pronouns such as she and definite noun phrases such
as the company are anaphors that refer to preced-
ing entities (antecedents). On the other hand, in
3In Japanese, bunsetsu is a basic unit of dependency, con-
sisting of one or more content words and the following zero or
more function words. It corresponds to a base phrase in English.
Japanese, anaphors are often omitted; these omis-
sions are called zero pronouns. Zero anaphora res-
olution is the integrated task of zero pronoun detec-
tion and zero pronoun resolution.
We proposed a probabilistic model for Japanese
zero anaphora resolution based upon case frames
(Sasano et al, 2008). This model first resolves
coreference and identifies discourse entities; then
gives a probability to each possible case frame CF
and case assignment CA when target predicate v,
input case components ICC and existing discourse
entities ENT are given, and outputs the case frame
and case assignment that have the highest probabil-
ity. That is to say, this model selects the case frame
CFbest and the case assignment CAbest that maxi-
mize the probability P (CF,CA|v, ICC,ENT ):
(CF best, CAbest)
= argmax
(CF,CA)
P (CF,CA|v, ICC,ENT ) (3)
P (CF,CA|v, ICC,ENT ) is approximately de-
composed into the product of several probabilities.
Case frames are used for calculating P (nj |CFl,
sj , A(sj) = 1), the probability of generating a con-
tent part nj from a case slot sj in a case frame
CFl, and P (nj |CFl, sj , A?(sj)=1), the probability
of generating a content part nj of a zero pronoun,
where the function A?(sj) returns 1 if a case slot sj
is filled with an antecedent of a zero pronoun; other-
wise 0.
P (nj |CFl, sj , A?(sj)=1) is similar to P (nj |CFl,
sj , A(sj)=1) and estimated from the frequencies of
case slot examples in case frames. However, while
A?(sj)=1 means sj is not filled with an overt argu-
ment but filled with an antecedent of zero pronoun,
case frames are constructed from overt predicate ar-
gument pairs. Therefore, the content part nj is often
not included in the case slot examples. To cope with
this problem, this model also utilizes generalized ex-
amples to estimate P (nj |CFl, sj , A(sj) = 1). For
details see (Sasano et al, 2008).
5 Experiments
5.1 Construction of Case Frames
In order to investigate the effect of corpus size,
we constructed case frames from corpora of dif-
ferent sizes. We first collected Japanese sentences
524
Table 4: Statistics of the Constructed Case Frames.
Corpus size (sentences) 1.6M 6.3M 25M 100M 400M 1.6G
# of predicate 2460 6134 13532 27226 42739 65679
(type) verb 2039 4895 10183 19191 28523 41732
adjective 154 326 617 1120 1641 2318
noun with copula 267 913 2732 6915 12575 21629
average # of case frames for a predicate 15.9 12.2 13.3 16.1 20.5 25.3
average # of case slots for a case frame 2.95 3.44 3.88 4.21 4.69 5.08
average # of examples for a case slot 4.89 10.2 19.5 34.0 67.2 137.6
average # of unique examples for a case slot 1.19 1.85 3.06 4.42 6.81 9.64
average # of generalized examples for a case slot 0.14 0.24 0.37 0.49 0.67 0.84
File size(byte) 8.9M 20M 56M 147M 369M 928M
Table 3: Corpus Sizes and Thresholds.
Corpus size for caseframe construction 1.6M 6.3M 25M 100M 400M 1.6G(sentences)
Threshold ?introduced in Sec. 3.1 2 3 4 5 7 10
Corpus size toestimate generative 1.6M 3.2M 6.3M 13M 25M 50Mprobability (sentences)
from the Web using the method proposed by Kawa-
hara and Kurohashi (2006b). We acquired approx-
imately 6 billion Japanese sentences consisting of
approximately 100 billion words from 100 million
Japanese web pages. After discarding duplicate sen-
tences, which may have been extracted from mirror
sites, we acquired a corpus comprising of 1.6 bil-
lion (1.6G) unique Japanese sentences consisting of
approximately 25 billion words. The average num-
ber of characters and words in each sentence was
28.3, 15.6, respectively. Then we randomly selected
subsets of the corpus for five different sizes; 1.6M,
6.3M, 25M, 100M, and 400M sentences to obtain
corpora of different sizes.
We constructed case frames from each corpus. We
employed JUMAN and KNP to parse each corpus.
We changed the threshold ? introduced in Section
3.1 depending upon the size of the corpus as shown
in Table 3. Completing the case frame construc-
tion took about two weeks using 600 CPUs. Ta-
ble 4 shows the statistics for the constructed case
frames. The number of predicates, the average num-
ber of examples and unique examples for a case slot,
and whole file size were confirmed to be heavily de-
pendent upon the corpus size. However, the average
number of case frames for a predicate and case slots
for a case frame did not.
5.2 Coverage of Constructed Case Frames
5.2.1 Setting
In order to investigate the coverage of the resul-
tant case frames, we used a syntactic relation, case
structure, and anaphoric relation annotated corpus
consisting of 186 web documents (979 sentences).
This corpus was manually annotated using the same
criteria as Kawahara et al (2004). There were 2,390
annotated relationships between predicates and their
direct (not omitted) case components and 837 zero
anaphoric relations in the corpus.
We used two evaluation metrics depending upon
whether the target case component was omitted or
not. For the overt case component of a predicate, we
judged the target component was covered by case
frames if the target component itself was included in
the examples for one of the corresponding case slots
of the case frame. For the omitted case component,
we checked not only the target component itself but
also all mentions that refer to the same entity as the
target component.
5.2.2 Coverage of Case Frames
Figure 1 shows the coverage of case frames for
the overt argument, which would have tight relations
with case structure analysis. The lower line shows
the coverage without considering generalized exam-
ples, the middle line shows the coverage considering
generalized NE examples, and the upper line shows
the coverage considering all generalized examples.
Figure 2 shows the coverage of case frames for
the omitted argument, which would have tight rela-
tions with zero anaphora resolution. The upper line
shows the coverage considering all generalized ex-
amples, which is considered to be the upper bound
of performance for the zero anaphora resolution sys-
525
0.0
0.2
0.4
0.6
0.8
1.0
1M 10M 100M 1000M
C
ov
er
ag
e
Corpus Size (Number of Sentences)
0.897
0.683
0.649
+NE,CT match
+ NE match
exact match
Figure 1: Coverage of CF (overt argument).
0.0
0.2
0.4
0.6
0.8
1.0
1M 10M 100M 1000M
C
ov
er
ag
e
Corpus Size (Number of Sentences)
0.892
0.608
0.472
+NE,CT match
+ NE match
exact match
Figure 2: Coverage of CF (omitted argument).
tem described in Section 4.2. Comparing with Fig-
ure 1, we found two characteristics. First, the lower
and middle lines of Figure 2 were located lower than
the corresponding lines in Figure 1. This would re-
flect that some frequently omitted case components
are not described in the case frames because the case
frames were constructed from only overt predicate
argument pairs. Secondly, the effect of generalized
NE examples was more evident for the omitted ar-
gument reflecting the important role of NEs in zero
anaphora resolution.
Both figures show that the coverage was improved
by using larger corpora and there was no saturation
even when the largest corpus of 1.6 billion sentences
was used. When the largest corpus and all general-
ized examples were used, the case frames achieved a
coverage of almost 90% for both the overt and omit-
ted argument.
Figure 3 shows the coverage of case frames for
each predicate type, which was calculated for both
overt and omitted argument considering all general-
ized examples. The case frames for verbs achieved
a coverage of 93.0%. There were 189 predicate-
argument pairs that were not included case frames;
0.0
0.2
0.4
0.6
0.8
1.0
1M 10M 100M 1000M
C
ov
er
ag
e
Corpus Size (Number of Sentences)
verb
adjective
noun with copula
0.930
0.788
0.545
Figure 3: Coverage of CF for Each Predicate Type.
11 pairs of them were due to lack of the case frame
of target predicate itself, and the others were due
to lack of the corresponding example. For adjec-
tive, the coverage was 78.8%. The main cause of
the lower coverage would be that the predicate argu-
ment relations concerning adjectives that were used
in restrictive manner, such as ?oishii sushi? (deli-
cious sushi), were not used for case frame construc-
tion, although such relations were also the target of
the coverage evaluation. For noun with copula, the
coverage was only 54.5%. However, most predicate
argument relations concerning nouns with copula
were easily recognized from syntactic preference,
and thus the low coverage would not quite affect the
performance of discourse analysis.
5.3 Syntactic and Case Structure Analysis
5.3.1 Accuracy of Syntactic Analysis
We investigated the effect of corpus size for syn-
tactic analysis described in Section 4.1. We used
hand-annotated 759 web sentences, which was used
by Kawahara and Kurohashi (2007). We evaluated
the resultant syntactic structures with regard to de-
pendency accuracy, the proportion of correct depen-
dencies out of all dependencies4.
Figure 4 shows the accuracy of syntactic struc-
tures. We conducted these experiments with case
frames constructed from corpora of different sizes.
We also changed the corpus size to estimate gen-
erative probability of a case slot in Section 4.1 de-
pending upon the size of the corpus for case frame
construction as shown in Table 3. Figure 4 also in-
4Note that Kawahara and Kurohashi (2007) exclude the de-
pendency between the last two bunsetsu, since Japanese is head-
final and thus the second last bunsetsu unambiguously depends
on the last bunsetsu.
526
0.886
0.888
0.890
0.892
0.894
0.896
1M 10M 100M 1000M
A
cc
ur
ac
y
Corpus Size (Number of Sentences)
0.894
1.6G
p < 0.1
100M
25M
p < 0.01
6.3M
1.6M
400M
p < 0.1
25M
p < 0.01
6.3M
1.6M
100M
p < 0.1
6.3M
1.6M
25M
p < 0.1
6.3M
1.6M6.3M1.6M
with case frames
w/o case frames
Figure 4: Accuracy of Syntactic Analysis. (McNemar?s
test results are also shown under each data point.)
cludes McNemar?s test results. For instance, the dif-
ference between the corpus size of 1.6G and 100M
sentences is significant at the 90% level (p = 0.1),
but not significant at the 99% level (p = 0.01).
In Figure 4, ?w/o case frames? shows the accu-
racy of the rule-based syntactic parser KNP that does
not use case frames. Since the model described
in Section 4.1 assumes the existence of reasonable
case frames, when we used case frames constructed
from very small corpus, such as 1.6M and 6.3M sen-
tences, the accuracy was lower than that of the rule-
based syntactic parser. Moreover, when we tested
the model described in Section 4.1 without any case
frames, the accuracy was 0.885.
We confirmed that better performance was ob-
tained by using case frames constructed from larger
corpora, and the accuracy of 0.8945 was achieved
by using the case frames constructed from 1.6G sen-
tences. However the effect of the corpus size was
limited. This is because there are various causes
of dependency error and the case frame sparseness
problem is not serious for syntactic analysis.
We considered that generalized examples can
benefit for the accuracy of syntactic analysis, and
tried several models that utilize these examples.
However, we cannot confirm any improvement.
5.3.2 Accuracy of Case Structure Analysis
We conducted case structure analysis on 215 web
sentences in order to investigate the effect of cor-
pus size for case structure analysis. The case mark-
ers of topic marking phrases and clausal modifiers
5It corresponds to 0.877 in Kawahara and Kurohashi?s
(2007) evaluation metrics.
0.400
0.500
0.600
0.700
0.800
0.900
1M 10M 100M 1000M
A
cc
ur
ac
y
Corpus Size (Number of Sentences)
0.784
Figure 5: Accuracy of Case Structure Analysis.
Table 5: Corpus Sizes for Case Frame Construction and
Time for Syntactic and Case Structure Analysis.
Corpus size 1.6M 6.3M 25M 100M 400M 1.6G
Time (sec.) 850 1244 1833 2696 3783 5553
were evaluated by comparing them with the gold
standard in the corpus. Figure 5 shows the experi-
mental results. We confirmed that the accuracy of
case structure analysis strongly depends on corpus
size for case frame construction.
5.3.3 Analysis Speed
Table 5 shows the time for analyzing syntactic
and case structure of 759 web sentences. Although
the time for analysis became longer by using case
frames constructed from a larger corpus, the growth
rate was smaller than the growth rate of the size for
case frames described in Table 4.
Since there is enough increase in accuracy of case
structure analysis, we can say that case frames con-
structed larger corpora are desirable for case struc-
ture analysis.
5.4 Zero Anaphora Resolution
5.4.1 Accuracy of Zero Anaphora Resolution
We used an anaphoric relation annotated corpus
consisting of 186 web documents (979 sentences)
to evaluate zero anaphora resolution. We used first
51 documents for test and used the other 135 doc-
uments for calculating several probabilities. In the
51 test documents, 233 zero anaphora relations were
annotated between one of the mentions of the an-
tecedent and corresponding predicate that had zero
pronoun.
In order to concentrate on evaluation for zero
anaphora resolution, we used the correct mor-
527
0.00
0.10
0.20
0.30
0.40
0.50
1M 10M 100M 1000M
F
-m
ea
su
re
Corpus Size (Number of Sentences)
0.417
0.330
0.313
+NE,CT match
+ NE match
exact match
Figure 6: F-measure of Zero Anaphora Resolution.
phemes, named entities, syntactic structures and
coreference relations that were manually annotated.
Since correct coreference relations were given, the
number of created entities was the same between the
gold standard and the system output because zero
anaphora resolution did not create new entities.
The experimental results are shown in Figure 6, in
which F-measure was calculated by:
R = # of correctly recognized zero anaphora# of zero anaphora annotated in corpus ,
P = # of correctly recognized zero anaphora# of system outputted zero anaphora ,
F = 21/R + 1/P .
The upper line shows the performance using all
generalized examples, the middle line shows the
performance using only generalized NEs, and the
lower line shows the performance without using
any generalized examples. While generalized cat-
egories much improved the F-measure, generalized
NEs contributed little. This tendency is similar to
that of coverage of case frames for omitted argument
shown in Figure 2. Unlike syntactic and case struc-
ture analysis, the performance for the zero anaphora
resolution is quite low when using case frames con-
structed from small corpora, and we can say case
frames constructed from larger corpora are essential
for zero anaphora resolution.
5.4.2 Analysis Speed
Table 6 shows the time for resolving zero
anaphora in 51 web documents consisting of 278
sentences. The time for analysis became longer by
using case frames constructed from larger corpora,
Table 6: Corpus Sizes for Case Frame Construction and
Time for Zero Anaphora Resolution.
Corpus size 1.6M 6.3M 25M 100M 400M 1.6G
Time (sec.) 538 545 835 1040 1646 2219
which tendency is similar to the growth of the time
for analyzing syntactic and case structure.
5.5 Discussion
Experimental results of both case structure analy-
sis and zero anaphora resolution show the effective-
ness of a larger corpus in case frame acquisition for
Japanese discourse analysis. Up to the corpus size
of 1.6 billion sentences, or 100 billion words, these
experimental results still show a steady increase in
performance. That is, we can say that the corpus
size of 1.6 billion sentences is not enough to obtain
case frames of sufficient coverage.
These results suggest that increasing corpus size
is more essential for acquiring structured knowledge
than for acquiring unstructured statistics of a corpus,
such as n-grams, and co-occurrence counts; and for
complex NLP tasks such as case structure analysis
and zero anaphora resolution, the currently available
corpus size is not sufficient.
Therefore, to construct more wide-coverage case
frames by using a larger corpus and reveal howmuch
corpora would be required to obtain sufficient cov-
erage is considered as future work.
6 Conclusion
This paper has reported the effect of corpus size
on case frame acquisition for syntactic and case
structure analysis, and zero anaphora resolution in
Japanese. We constructed case frames from cor-
pora of six different sizes ranging from 1.6 million
to 1.6 billion sentences; and then applied these case
frames to Japanese syntactic and case structure anal-
ysis, and zero anaphora resolution. Experimental re-
sults showed better results were obtained using case
frames constructed from larger corpora, and the per-
formance showed no saturation even when the cor-
pus size was 1.6 billion sentences.
The findings suggest that increasing corpus size
is more essential for acquiring structured knowledge
than for acquiring surface statistics of a corpus; and
for complex NLP tasks the currently available cor-
pus size is not sufficient.
528
References
Michaela Atterer and Hinrich Schu?tze. 2006. The ef-
fect of corpus size in combining supervised and un-
supervised training for disambiguation. In Proc. of
COLING-ACL?06, pages 25?32.
Michele Banko and Eric Brill. 2001a. Mitigating the
paucity-of-data problem: Exploring the effect of train-
ing corpus size on classifier performance for natural
language processing. In Proc. of HLT?01.
Michele Banko and Eric Brill. 2001b. Scaling to very
very large corpora for natural language disambigua-
tion. In Proc. of ACL?01, pages 26?33.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proc. of EMNLP-CoNLL?07,
pages 858?867.
Ann Gledson and John Keane. 2008. Using web-search
results to measure word-group similarity. In Proc. of
COLING?08, pages 281?288.
David Graff. 2003. English Gigaword. Technical Report
LDC2003T05, Linguistic Data Consortium, Philadel-
phia, PA USA.
Peter Halacsy, Andras Kornai, Laszlo Nemeth, Andras
Rung, Istvan Szakadat, and Vikto Tron. 2004. Creat-
ing open language resources for Hungarian. In Proc.
of LREC?04, pages 203?210.
IREX Committee, editor. 1999. Proc. of the IREX Work-
shop.
Daisuke Kawahara and Sadao Kurohashi. 2006a. A
fully-lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis. In Proc. of HLT-
NAACL?06, pages 176?183.
Daisuke Kawahara and Sadao Kurohashi. 2006b.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC?06, pages
1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2007.
Probabilistic coordination disambiguation in a fully-
lexicalized Japanese parser. In Proc. of EMNLP-
CoNLL?07, pages 306?314.
Daisuke Kawahara, Ryohei Sasano, and Sadao Kuro-
hashi. 2004. Toward text understanding: Integrat-
ing relevance-tagged corpora and automatically con-
structed case frames. In Proc. of LREC?04, pages
1833?1836.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistic, 29(3):333?347.
Taku Kudo and Hideto Kazawa. 2007. Web Japanese N-
gram version 1, published by Gengo Shigen Kyokai.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2:1:1?31.
Vinci Liu and James R. Curran. 2006. Web text corpus
for natural language processing. In Proc. of EACL?06,
pages 233?240.
Kikuo Maekawa. 2006. Kotonoha, the corpus develop-
ment project of the National Institute for Japanese lan-
guage. In Proc. of the 13th NIJL International Sympo-
sium, pages 55?62.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proc. of
HLT-NAACL?06, pages 152?159.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proc. of EMNLP-2003,
pages 176?183.
Preslav Nakov and Marti Hearst. 2005. A study of using
search engine page hits as a proxy for n-gram frequen-
cies. In Proc. of RANLP?05.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. of ACL-HLT?08, pages 452?460.
Ryohei Sasano and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural lan-
guage processing. In Proc. of IJCNLP?08, pages 607?
612.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proc. of
COLING?08, pages 769?776.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL-HLT?08,
pages 665?673.
The National Language Institute for Japanese Language.
2004. Bunruigoihyo. Dainippon Tosho, (In Japanese).
Martin Volk. 2001. Exploiting the WWW as a corpus
to resolve PP attachment ambiguities. In Proc. of the
Corpus Linguistics, pages 601?606.
529
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 137?140,
Prague, June 2007. c?2007 Association for Computational Linguistics
Construction of Domain Dictionary for Fundamental Vocabulary
Chikara Hashimoto
Faculty of Engineering,
Yamagata University
4-3-16 Jonan, Yonezawa-shi, Yamagata,
992-8510 Japan
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
36-1 Yoshida-Honmachi, Sakyo-ku, Kyoto,
606-8501 Japan
Abstract
For natural language understanding, it is es-
sential to reveal semantic relations between
words. To date, only the IS-A relation
has been publicly available. Toward deeper
natural language understanding, we semi-
automatically constructed the domain dic-
tionary that represents the domain relation
between Japanese fundamental words. This
is the first Japanese domain resource that is
fully available. Besides, our method does
not require a document collection, which is
indispensable for keyword extraction tech-
niques but is hard to obtain. As a task-based
evaluation, we performed blog categoriza-
tion. Also, we developed a technique for es-
timating the domain of unknown words.
1 Introduction
We constructed a lexical resource that represents the
domain relation among Japanese fundamental words
(JFWs), and we call it the domain dictionary.1 It
associates JFWs with domains in which they are typ-
ically used. For example,
 
home run is
associated with the domain SPORTS2 . That is, we
aim to make explicit the horizontal relation between
words, the domain relation, while thesauri indicate
the vertical relation called IS-A.3
1In fact, there have been a few domain resources in Japanese
like Yoshimoto et al (1997). But they are not publicly available.
2Domains are CAPITALIZED in this paper.
3The lack of the horizontal relationship is also known as the
?tennis problem? (Fellbaum, 1998, p.10).
2 Two Issues
You have to address two issues. One is what do-
mains to assume, and the other is how to associate
words with domains without document collections.
The former is paraphrased as how people cate-
gorize the real world, which is really a hard prob-
lem. In this study, we avoid being too involved in
the problem and adopt a simple domain system that
most people can agree on, which is as follows:
CULTURE
RECREATION
SPORTS
HEALTH
LIVING
DIET
TRANSPORTATION
EDUCATION
SCIENCE
BUSINESS
MEDIA
GOVERNMENT
It has been created based on web directories such
as Open Directory Project with some adjustments.
In addition, NODOMAIN was prepared for those
words that do not belong to any particular domain.
As for the latter issue, you might use keyword ex-
traction techniques; identifying words that represent
a domain from the document collection using statis-
tical measures like TF*IDF and matching between
extracted words and JFWs. However, you will find
that document collections of common domains such
as those assumed here are hard to obtain.4 Hence,
we had to develop a method that does not require
document collections. The next section details it.
4Initially, we tried collecting web pages in Yahoo! JAPAN.
However, we found that most of them were index pages with a
few text contents, from which you cannot extract reliable key-
words. Though we further tried following links in those index
pages to acquire enough texts, extracted words turned out to be
site-specific rather than domain-specific since many pages were
collected from a particular web site.
137
Table 1: Examples of Keywords for each Domain
Domain Examples of Keywords
CULTURE   movie,  music
RECREATION  tourism, 
	 firework
SPORTS  player,  baseball
HEALTH  surgery,  diagnosis
LIVING  childcare, 
 furniture
DIET  chopsticks, ffProceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 69?72,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Blog Categorization Exploiting Domain Dictionary and
Dynamically Estimated Domains of Unknown Words
Chikara Hashimoto
Graduate School of Science and Engineering
Yamagata University
Yonezawa-shi, Yamagata, 992-8510, Japan
ch@yz.yamagata-u.ac.jp
Sadao Kurohashi
Graduate School of Informatics
Kyoto University
Sakyo-ku, Kyoto, 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
This paper presents an approach to text cate-
gorization that i) uses no machine learning and
ii) reacts on-the-fly to unknown words. These
features are important for categorizing Blog
articles, which are updated on a daily basis
and filled with newly coined words. We cat-
egorize 600 Blog articles into 12 domains. As
a result, our categorization method achieved
an accuracy of 94.0% (564/600).
1 Introduction
This paper presents a simple but high-performance
method for text categorization. The method assigns
domain tags to words in an article, and categorizes
the article as the most dominant domain. In this
study, the 12 domains in Table 1 are used follow-
ing (Hashimoto and Kurohashi, 2007) (H&K here-
after)1. Fundamental words are assigned with a do-
Table 1: Domains Assumed in H&K
CULTURE LIVING SCIENCE
RECREATION DIET BUSINESS
SPORTS TRANSPORTATION MEDIA
HEALTH EDUCATION GOVERNMENT
main tag by H&K?s domain dictionary, while the
domains of non-fundamental words (i.e. unknown
words) are dynamically estimated, which makes the
method different from previous ones. Another hall-
mark of the method is that it requires no machine
1In addition, NODOMAIN is prepared for words belonging to
no particular domain like blue or people.
learning. All you need is the domain dictionary and
the access to the Web.
2 The Domain Dictionary
H&K constructed a domain dictionary, where about
30,000 Japanese fundamental content words (JFWs)
are associated with appropriate domains. For exam-
ple, homer is associated with SPORTS.
2.1 Construction Process
1 Preparing Keywords for each Domain About
20 keywords for each domain were collected manu-
ally from words that appear frequently in the Web.
They represent the contents of domains.
2 Associating JFWs with Domains A JFW is
associated with a domain of the highest Ad score.
An Ad score of domain is calculated by summing
up the top five Ak scores of the domain. Then,
an Ak score, which is defined between a JFW and
a keyword of a domain, is a measure that shows
how strongly the JFW and the keyword are related.
H&K adopt the ?2 statistics to calculate an Ak score
and use web pages as a corpus. The number of
co-occurrences is approximated by the number of
search engine hits when the two words are used as
queries. Ak score between a JFW (jw) and a key-
word (kw) is given as below.
Ak(jw, kw) =
n(ad ? bc)2
(a + b)(c + d)(a + c)(b + d) (1)
where n is the total number of Japanese web pages,
a = hits(jw & kw), b = hits(jw) ? a,
c = hits(kw) ? a, d = n ? (a + b + c).
69
Note that hits(q) represents the number of search
engine hits when q is used as a query.
3 Manual Correction Manual correction of the
automatic association2 is done to complete the dic-
tionary. Since the accuracy of 2 is 81.3%, manual
correction is not time-consuming.
2.2 Distinctive Features
H&K?s method is independent of what domains to
assume. You can create your own dictionary. All
you need is prepare keywords of your own domains.
After that, the same construction process is applied.
Also note that H&K?s method requires no text col-
lection that is typically used for machine learning
techniques. All you need is the access to the Web.
3 Blog Categorization
The categorization proceeds as follows: 1 Extract
words from an article, 2 Assign domains and IDFs
to the words, 3 Sum up IDFs for each domain, 4
Categorize the article as the domain of the highest
IDF.3 As for 2 , the IDF is calculated as follows:4
IDF(w) = log Total # of Japanese web pages
# of hits of w (2)
Fundamental words are assigned with their do-
mains and IDFs by the domain dictionary, while
those for unknown words are dynamically estimated
by the method described in ?4.
4 Domain Estimation of Unknown Words
The domain (and IDF) of unknown word is dynam-
ically estimated exploiting the Web. More specifi-
cally, we use Wikipedia and Snippets of Web search,
in addition to the domain dictionary. The estimation
proceeds as follows (Figure 1): 1 Search the Web
with an unknown word, acquire the top 100 records,
and calculate the IDF. 2 Get the Wikipedia article
about the word from the search result if any, estimate
the domain of the word with the Wikipedia-strict
module (?4.1), and exit. 3 When no Wikipedia arti-
cle about the word is found, then get any Wikipedia
2In H&K?s method, reassociating JFWs with NODOMAIN is
required before 3 . We omit that due to the space limitation.
3If the domain of the highest IDF is NODOMAIN, the article
is categorized as the second highest domain.
4We used 10,000,000,000 as the total number.
Unknown Word
Search Result: 100 records
Is There the Wikipedia
Article about the Word in
the Search Result?
Is There Any Wikipedia
Article in the Top 30 in
the Search Result?
Is There Any Snippet Left
in the Search Result?
Does the Input Contain
Fundamental Words?
Failure
Wikipedia
-strict
Wikipedia
-loose
Snippets
Components
Domain and IDF
No
No
No
No
Yes
Yes
Yes
Yes
Remove Corporate Snippets in the Result
Web Search & IDF Calculation
Figure 1: Domain Estimation Process
article in the top 30 of the search result if any, es-
timate the domain with the Wikipedia-loose module
(?4.1), and exit. 4 If no Wikipedia article is found
in the top 30 of the search result, then remove all
corporate snippets. 5 Estimate the domain with the
Snippets module (?4.2) if any snippet is left in the
search result, and exit. 6 If no snippet is left but the
unknown word is a compound word containing fun-
damental words, then estimate the domain with the
Components module (?4.3), and exit. 7 If no snip-
pet is left and the word does not contain fundamental
words, then the estimation is a failure.
4.1 Wikipedia(-strict|-loose) Module
The two Wikipedia modules take the following pro-
cedure: 1 Extract only fundamental words from the
Wikipedia article. 2 Assign domains and IDFs to
the words using the domain dictionary. 3 Sum up
IDFs for each domain. 4 Assign the domain of the
highest IDF to the unknown word. If the domain
is NODOMAIN, the second highest domain is chosen
for the unknown word under the condition below:
70
Second-highest-IDF/ NODOMAIN?s-IDF>0.15
4.2 Snippets Module
The Snippets module takes as input the snippets that
are left in the search result after removing those
of corporate web sites. We remove snippets in
which corporate keywords like sales appear more
than once. The keywords were collected from the
analysis of our preliminary experiments. Remov-
ing corporate snippets is indispensable because they
bias the estimation toward BUSINESS. This module
is the same as the Wikipedia modules except that it
extracts fundamental words from residual snippets.
4.3 Components Module
This is basically the same as the others except that it
extracts fundamental words from the unknown word
itself. For example, the domain of finance market is
estimated from the domains of finance and market.
5 Evaluation
5.1 Experimental Condition
Data We categorized 600 Blog articles from Ya-
hoo! Blog (blogs.yahoo.co.jp) into the 12 do-
mains (50 articles for each domain). In Yahoo! Blog,
articles are manually classified into Yahoo! Blog cat-
egories (' domains) by authors of the articles.
Evaluation Method We measured the accuracy of
categorization and the domain estimation. In cate-
gorization, we tried three kinds of words to be ex-
tracted from articles: fundamental words (F only in
Table 3), fundamental and simplex unknown words
(i.e. no compound word) (F+SU), and fundamen-
tal and all unknown words (both simplex and com-
pound, F+AU). Also, we measured the accuracy of
N best outputs (Top N). During the categorization,
about 12,000 unknown words were found in the 600
articles. Then, we sampled 500 estimation results
from them. Table 2 shows the breakdown of the 500
unknown words in terms of their correct domains.
The other 167 words belong to NODOMAIN.
5.2 Result of Blog Categorization
Table 3 shows the accuracy of categorization. The
F only column indicates that a rather simple method
like the one in ?3 works well, if fundamental words
are given good clues for categorization: the domain
Table 2: Breakdown of Unknown Words
CULT 42 LIVI 19 SCIE 38
RECR 15 DIET 19 BUSI 32
SPOR 27 TRAN 28 MEDI 23
HEAL 22 EDUC 24 GOVE 44
Table 3: Accuracy of Blog Categorization
Top N F only F+SU F+AU
1. 0.89 0.91 0.94
2. 0.96 0.97 0.98
3. 0.98 0.98 0.99
in our case. This is consistent with Kornai et al
(2003), who claim that only positive evidence mat-
ter in categorization. Also, F+SU slightly outper-
formed F only, and F+AU outperformed the others.
This shows that the domain estimation of unknown
words moderately improves Blog categorization.
Errors are mostly due to the system?s incorrect fo-
cus on topics of secondary importance. For exam-
ple, in an article on a sightseeing trip, which should
be RECREATION, the author frequently mentions the
means of transportation. As a result, the article was
wrongly categorized as TRAFFIC.
5.3 Result of Domain Estimation
The accuracy of the domain estimation of unknown
words was 77.2% (386/500). Table 4 shows the fre-
quency in use and accuracy for each domain esti-
mation module.5 The Snippets module was used
Table 4: Frequency and Accuracy for each Module
Frequency Accuracy
Wiki-s 0.146 (73/500) 0.85 (62/73)
Wiki-l 0.208 (104/500) 0.70 (73/104)
Snippt 0.614 (307/500) 0.76 (238/307)
Cmpnt 0.028 (14/500) 0.64 (9/14)
Failure 0.004 (2/500) ??
most frequently and achieved the reasonably good
accuracy of 76%. Though the Wikipedia-strict mod-
ule showed the best performance, it was used not
5Wiki-s, Wiki-l, Snippt and Cmpnt stand for Wikipedia-
strict, Wikipedia-loose, Snippets and Components, respectively.
71
so often. However, we expect that as the number
of Wikipedia articles increases, the best performing
module will be used more frequently.
An example of newly coined words whose do-
mains were estimated correctly is
 
, which
is the abbreviation of
 	

day-trade.
It was correctly assigned with BUSINESS by the
Wikipedia-loose module.
Errors were mostly due to the subtle boundary be-
tween NODOMAIN and the other particular domains.
For instance, person?s names that are common and
popular should be NODOMAIN. But in most cases
they were associated with some particular domain.
This is due to the fact that virtually any person?s
name is linked to some particular domain in the Web.
6 Related Work
Previous text categorization methods like Joachims
(1999) and Schapire and Singer (2000) are mostly
based on machine learning. Those methods need
huge quantities of training data, which is hard to ob-
tain. Though there has been a growing interest in
semi-supervised learning (Abney, 2007), it is in an
early phase of development.
In contrast, our method requires no training data.
All you need is a manageable amount of fundamen-
tal words with domains. Also note that our method
is NOT tailored to the 12 domains. If you want
your own domains to categorize, it is only neces-
sary to construct your own dictionary, which is also
domain-independent and not time-consuming.
In fact, there have been other proposals without
the burden of preparing training data. Liu et al
(2004) prepare representative words for each class,
by which they collect initial training data to build
classifier. Ko and Seo (2004) automatically collect
training data using a large amount of unlabeled data
and a small amount of seed information. However,
the novelty of this study is the on-the-fly estimation
of unknown words? domains. This feature is very
useful for categorizing Blog articles that are updated
on a daily basis and filled with newly coined words.
Domain information has been used for many NLP
tasks. Magnini et al (2002) show the effectiveness
of domain information for WSD. Piao et al (2003)
use domain tags to extract MWEs.
Previous domain resources include WordNet
(Fellbaum, 1998) and HowNet (Dong and Dong,
2006), among others. H&K?s dictionary is the first
fully available domain resource for Japanese.
7 Conclusion
This paper presented a text categorization method
that exploits H&K?s domain dictionary and the dy-
namic domain estimation of unknown words. In the
Blog categorization, the method achieved the accu-
racy of 94%, and the domain estimation of unknown
words achieved the accuracy of 77%.
References
Steven Abney. 2007. Semisupervised Learning for Com-
putational Linguistics. Chapman & Hall.
Zhendong Dong and Qiang Dong. 2006. HowNet and
the Computation of Meaning. World Scientific Pub Co
Inc.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Chikara Hashimoto and Sadao Kurohashi. 2007. Con-
struction of Domain Dictionary for Fundamental Vo-
cabulary. In ACL ?07 Poster, pages 137?140.
Thorsten Joachims. 1999. Transductive Inference for
Text Classification using Support Vector Machines. In
Proceedings of the Sixteenth International Conference
on Machine Learning, pages 200?209.
Youngjoong Ko and Jungyun Seo. 2004. Learning with
Unlabeled Data for Text Categorization Using Boot-
strapping and Feature Projection Techniques. In ACL
?04, pages 255?262.
Andra?s Kornai, Marc Krellenstein, Michael Mulligan,
David Twomey, Fruzsina Veress, and Alec Wysoker.
2003. Classifying the Hungarian web. In EACL ?03,
pages 203?210.
Bing Liu, Xiaoli Li, Wee Sun Lee, , and Philip Yu. 2004.
Text Classification by Labeling Words. In AAAI-2004,
pages 425?430.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,
and Alfio Gliozzo. 2002. The Role of Domain Infor-
mation in Word Sense Disambiguation. Natural Lan-
guage Engineering, special issue on Word Sense Dis-
ambiguation, 8(3):359?373.
Scott S. L. Piao, Paul Rayson, Dawn Archer, Andrew
Wilson, and Tony McEnery. 2003. Extracting multi-
word expressions with a semantic tagger. In Proceed-
ings of the ACL 2003 workshop on Multiword expres-
sions, pages 49?56.
Robert E. Schapire and Yoram Singer. 2000. BoosTex-
ter: A Boosting-based System for Text Categorization.
Machine Learning, 39(2/3):135?168.
72
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49?52,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Unified Single Scan Algorithm
for Japanese Base Phrase Chunking and Dependency Parsing
Manabu Sassano
Yahoo Japan Corporation
Midtown Tower,
9-7-1 Akasaka, Minato-ku,
Tokyo 107-6211, Japan
msassano@yahoo-corp.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
We describe an algorithm for Japanese
analysis that does both base phrase chunk-
ing and dependency parsing simultane-
ously in linear-time with a single scan of a
sentence. In this paper, we show a pseudo
code of the algorithm and evaluate its per-
formance empirically on the Kyoto Uni-
versity Corpus. Experimental results show
that the proposed algorithm with the voted
perceptron yields reasonably good accu-
racy.
1 Introduction
Single scan algorithms of parsing are important for
interactive applications of NLP. For instance, such
algorithms would be more suitable for robots ac-
cepting speech inputs or chatbots handling natural
language inputs which should respond quickly in
some situations even when human inputs are not
clearly ended.
Japanese sentence analysis typically consists of
three major steps, namely morphological analysis,
bunsetsu (base phrase) chunking, and dependency
parsing. In this paper, we describe a novel algo-
rithm that combines the last two steps into a sin-
gle scan process. The algorithm, which is an ex-
tension of Sassano?s (2004), allows us to chunk
morphemes into base phrases and decide depen-
dency relations of the phrases in a strict left-to-
right manner. We show a pseudo code of the al-
gorithm and evaluate its performance empirically
with the voted perceptron on the Kyoto University
Corpus (Kurohashi and Nagao, 1998).
2 Japanese Sentence Structure
In Japanese NLP, it is often assumed that the struc-
ture of a sentence is given by dependency relations
Meg-ga kare-ni ano pen-wo age-ta.
Meg-subj to him that pen-acc give-past.
ID 0 1 2 3 4
Head 4 4 3 4 -
Figure 1: Sample sentence (bunsetsu-based)
among bunsetsus. A bunsetsu is a base phrasal
unit and consists of one or more content words fol-
lowed by zero or more function words.
In addition, most of algorithms of Japanese de-
pendency parsing, e.g., (Sekine et al, 2000; Sas-
sano, 2004), assume the three constraints below.
(1) Each bunsetsu has only one head except the
rightmost one. (2) Dependency links between bun-
setsus go from left to right. (3) Dependency links
do not cross one another. In other words, depen-
dencies are projective.
A sample sentence in Japanese is shown in Fig-
ure 1. We can see all the constraints are satisfied.
3 Previous Work
As far as we know, there is no dependency parser
that does simultaneously both bunsetsu chunking
and dependency parsing and, in addition, does
them with a single scan. Most of the modern
dependency parsers for Japanese require bunsetsu
chunking (base phrase chunking) before depen-
dency parsing (Sekine et al, 2000; Kudo and Mat-
sumoto, 2002; Sassano, 2004). Although word-
based parsers are proposed in (Mori et al, 2000;
Mori, 2002), they do not build bunsetsus and are
not compatible with other Japanese dependency
parsers. Multilingual parsers of participants in the
CoNLL 2006 shared task (Buchholz and Marsi,
2006) can handle Japanese sentences. But they are
basically word-based.
49
Meg ga kare ni ano pen wo age-ta.
Meg subj him to that pen acc give-past.
ID 0 1 2 3 4 5 6 7
Head 1 7 3 7 6 6 7 -
Type B D B D D B D -
Figure 2: Sample sentence (morpheme-based).
?Type? represents the type of dependency relation.
4 Algorithm
4.1 Dependency Representation
In our proposed algorithm, we use a morpheme-
based dependency structure instead of a bunsetsu-
based one. The morpheme-based representation
is carefully designed to convey the same informa-
tion on dependency structure of a sentence without
the loss from the bunsetsu-based one. The right-
most morpheme of the bunsetsu t should modify
the rightmost morpheme of the bunsetsu u when
the bunsetsu t modifies the bunsetsu u. Every
morpheme except the rightmost one in a bunsetsu
should modify its following one. The sample sen-
tence in Figure 1 is converted to the sentence with
our proposed morpheme-based representation in
Figure 2.
Take for instance, the head of the 0-th bunsetsu
?Meg-ga? is the 4-th bunsetsu ?age-ta.? in Fig-
ure 1. This dependency relation is represented by
that the head of the morpheme ?ga? is ?age-ta.? in
Figure 2.
The morpheme-based representation above can-
not explicitly state the boundaries of bunsetsus.
Thus we add the type to every dependency rela-
tion. A bunsetsu boundary is represented by the
type associated with every dependency relation.
The type ?D? represents that this relation is a de-
pendency of two bunsetsus, while the type ?B?
represents a sequence of morphemes inside of a
given bunsetsu. In addition, the type ?O?, which
represents that two morphemes do not have a de-
pendency relation, is used in implementations of
our algorithm with a trainable classifier. Following
this encoding scheme of the type of dependency
relations bunsetsu boundaries exist just after the
morphemes that have the type ?D?. Inserting ?|?
after every morpheme with ?D? of the sentence in
Figure 2 results in Meg-ga | kare-ni | ano | pen-wo
| age-ta. This is identical to the sentence with the
bunsetsu-based representation in Figure 1.
Input: w
i
: morphemes in a given sentence.
N : the number of morphemes.
Output: h
j
: the head IDs of morphemes w
j
.
t
j
: the type of dependency relation. A possible
value is either ?B?, ?D?, or ?O?.
Functions: Push(i, s): pushes i on the stack s.
Pop(s): pops a value off the stack s.
Dep(j, i, w, t): returns true when w
j
should
modify w
i
. Otherwise returns false. Sets
always t
j
.
procedure Analyze(w, N , h, t)
var s: a stack for IDs of modifier morphemes
begin
Push(?1, s); { ?1 for end-of-sentence }
Push(0, s);
for i ? 1 to N ? 1 do begin
j ? Pop(s);
while (j 6= ?1
and (Dep(j, i, w, t) or (i = N ? 1)) ) do
begin
h
j
? i; j ? Pop(s)
end
Push(j, s); Push(i, s)
end
end
Figure 3: Pseudo code for base phrase chunking
and dependency parsing.
4.2 Pseudo Code for the Proposed Algorithm
The algorithm that we propose is based on (Sas-
sano, 2004), which is considered to be a simple
form of shift-reduce parsing. The pseudo code of
our algorithm is presented in Figure 3. Important
variables here are h
j
and t
j
where j is an index
of morphemes. The variable h
j
holds the head ID
and the variable t
j
has the type of dependency re-
lation. For example, the head and the dependency
relation type of ?Meg? in Figure 2 are represented
as h
0
= 1 and t
0
= ?B? respectively. The flow
of the algorithm, which has the same structure as
Sassano?s (2004), is controlled with a stack that
holds IDs for modifier morphemes. Decision of
the relation between two morphemes is made in
Dep(), which uses a machine learning-based clas-
sifier that supports multiclass prediction.
The presented algorithm runs in a left-to-right
manner and its upper bound of the time complex-
ity is O(n). Due to space limitation, we do not
discuss its complexity here. See (Sassano, 2004)
50
for further details.
5 Experiments and Discussion
5.1 Experimental Set-up
Corpus For evaluation, we used the Kyoto Uni-
versity Corpus Version 2 (Kurohashi and Nagao,
1998). The split for training/test/development is
the same as in other papers, e.g., (Uchimoto et al,
1999).
Selection of a Classifier and its Setting We im-
plemented a parser with the voted perceptron (VP)
(Freund and Schapire, 1999). We used a poly-
nomial kernel and set its degree to 3 because cu-
bic kernels proved to be effective empirically for
Japanese parsing (Kudo and Matsumoto, 2002).
The number of epoch T of VP was selected using
the development test set. For multiclass predic-
tion, we used the pairwise method (Kre?el, 1999).
Features We have designed rather simple fea-
tures based on the common feature set (Uchimoto
et al, 1999; Kudo and Matsumoto, 2002; Sassano,
2004) for bunsetsu-based parsers. We use the fol-
lowing features for each morpheme:
1. major POS, minor POS, conjugation type,
conjugation form, surface form (lexicalized
form)
2. Content word or function word
3. Punctuation (periods and commas)
4. Open parentheses and close parentheses
5. Location (at the beginning or end of the sen-
tence)
Gap features between two morphemes are also
used since they have proven to be very useful and
contribute to the accuracy (Uchimoto et al, 1999;
Kudo and Matsumoto, 2002). They are repre-
sented as a binary feature and include distance (1,
2, 3, 4 ? 10, or 11 ?), particles, parentheses, and
punctuation.
In our proposed algorithm basically two mor-
phemes are examined to estimate their dependency
relation. Context information about the current
morphemes to be estimated would be very use-
ful and we can incorporate such information into
our model. We assume that we have the j-th mor-
pheme and the i-th one in Figure 3. We also use
the j?n, ..., j?1, j+1, ..., j+n morphemes and
the i ? n, ..., i ? 1, i + 1, ..., i + n ones, where n
Measure Accuracy (%)
Dependency Acc. 93.96
Dep. Type Acc. 99.49
Both 93.92
Table 1: Performance on the test set. This result is
achieved by the following parameters: The size of
context window is 2 and epoch T is 4.
Bunsetsu-based Morpheme-based
Previous 88.48 95.09
Ours NA 93.96
Table 2: Dependency accuracy. The system with
the previous method employs the algorithm (Sas-
sano, 2004) with the voted perceptron.
is the size of the context window. We examined 0,
1, 2 and 3 for n.
5.2 Results and Discussion
Accuracy Performances of our parser on the test
set is shown in Table 1. The dependency accuracy
is the percentage of the morphemes that have a
correct head. The dependency type accuracy is the
percentage of the morphemes that have a correct
dependency type, i.e., ?B? or ?D?. The bottom line
of Table 1 shows the percentage of the morphemes
that have both a correct head and a correct depen-
dency type. In all these measures we excluded the
last morpheme in a sentence, which does not have
a head and its associated dependency type.
The accuracy of dependency type in Table 1
is interpreted to be accuracy of base phrase
(bunsetsu) chunking. Very accurate chunking is
achieved.
Next we examine the dependency accuracy. In
order to recognize how accurate it is, we com-
pared the performance of our parser with that of
the parser that uses one of previous methods. We
implemented a parser that employs the algorithm
of (Sassano, 2004) with the commonly used fea-
tures and runs with VP instead of SVM, which
Sassano (2004) originally used. His parser, which
cannot do bunsetsu chunking, accepts only a chun-
ked sentence and then produces a bunsetsu-based
dependency structure. Thus we cannot directly
compare results with ours. To enable us to com-
pare them we gave bunsetsu chunked sentences by
our parser to the parser of (Sassano, 2004) instead
of giving directly the correct chunked sentences
51
Window Size Dep. Acc. Dep. Type Acc.
0 (T = 1) 82.71 99.29
1 (T = 2) 93.57 99.49
2 (T = 4) 93.96 99.49
3 (T = 3) 93.79 99.42
Table 3: Performance change depending on the
context window size
 0
 0.5
 1
 1.5
 2
 2.5
 3
 0  10  20  30  40  50  60  70  80  90  100
Se
co
nd
s
Sentence Length (Number of Morphemes)
Figure 4: Running time on the test set. We used
a PC (Intel Xeon 2.33 GHz with 8GB memory on
FreeBSD 6.3).
in the Kyoto University Corpus. And then we re-
ceived results from the parser of (Sassano, 2004),
which are bunsetsu-based dependency structures,
and converted them to morpheme-based structures
that follow the scheme we propose in this paper.
Finally we have got results that have the compat-
ible format and show a comparison with them in
Table 2.
Although the bunsetsu-based parser outper-
formed slightly our morpheme-based parser in this
experiment, it is still notable that our method
yields comparable performance with even a sin-
gle scan of a sentence for dependency parsing in
addition to bunsetsu chunking. According to the
results in Table 2, we suppose that performance of
our parser roughly corresponds to about 86?87%
in terms of bunsetsu-based accuracy.
Context Window Size Performance change de-
pending on the size of context window is shown
in Table 3. Among them the best size is 2. In
this case, we use ten morphemes to determine
whether or not given two morphemes have a de-
pendency relation. That is, to decide the relation
of morphemes j and i (j < i), we use morphemes
j?2, j?1, j, j+1, j+2 and i?2, i?1, i, i+1, i+2.
Running Time and Asymptotic Time Complex-
ity We have observed that the running time is
proportional to the sentence length (Figure 4). The
theoretical time complexity of the proposed algo-
rithm is confirmed with this observation.
6 Conclusion and Future Work
We have described a novel algorithm that com-
bines Japanese base phrase chunking and depen-
dency parsing into a single scan process. The pro-
posed algorithm runs in linear-time with a single
scan of a sentence.
In future work we plan to combine morpholog-
ical analysis or word segmentation into our pro-
posed algorithm. We also expect that structure
analysis of compound nouns can be incorporated
by extending the dependency relation types. Fur-
thermore, we believe it would be interesting to
discuss linguistically and psycholinguistically the
differences between Japanese and other European
languages such as English. We would like to know
what differences lead to easiness of analyzing a
Japanese sentence.
References
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. of CoNLL
2006, pages 149?164.
Y. Freund and R. E. Schapire. 1999. Large margin classifi-
cation using the perceptron algorithm. Machine Learning,
37(3):277?296.
U. Kre?el. 1999. Pairwise classification and support vec-
tor machines. In B. Scho?lkopf, C. J. Burges, and A. J.
Smola, editors, Advances in Kernel Methods: Support
Vector Learning, pages 255?268. MIT Press.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proc. of CoNLL-
2002, pages 63?69.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proc. of LREC-1998, pages 719?724.
S. Mori, M. Nishimura, N. Itoh, S. Ogino, and H. Watanabe.
2000. A stochastic parser based on a structural word pre-
diction model. In Proc. of COLING 2000, pages 558?564.
S. Mori. 2002. A stochastic parser based on an SLM with
arboreal context trees. In Proc. of COLING 2002.
M. Sassano. 2004. Linear-time dependency analysis for
Japanese. In Proc. of COLING 2004, pages 8?14.
S. Sekine, K. Uchimoto, and H. Isahara. 2000. Back-
ward beam search algorithm for dependency analysis of
Japanese. In Proc. of COLING-00, pages 754?760.
K. Uchimoto, S. Sekine, and H. Isahara. 1999. Japanese
dependency structure analysis based on maximum entropy
models. In Proc. of EACL-99, pages 196?203.
52
Dia logue  He lpsystem based  on  F lex ib le  Match ing  o f  
User  Query  w i th  Natura l  Language Knowledge Base  
Sadao  Ku_rohashi and  Wataru  H igasa  
Graduate School of Informatics 
Kyoto University 
Yoshida-honmachi, Sakyo, Kyoto, 606-8501 Japan 
kuro@i, kyoto-u, ac. jp, higasa@pine, kuee. kyoto-u, ac. jp 
Abst ract  
This paper describes a dialog help- 
system which advises users in us- 
ing computer facilities and software 
applications provided by the Cen- 
ter for Information and Multime- 
dia Studies, Kyoto University. The 
system employs a knowledge base 
written in natural language and re- 
trieves a proper knowledge unit by 
flexible matching of user query with 
the knowledge base. The  system 
is running since July 1999, received 
about 2,000 queries for the first 
seven months, and answered about 
40~ of them satisfactory. 
1 In t roduct ion  
One of the ultimate goals of Natural Lan- 
guage Processing is to realize a dialogue sys- 
tem which can communicate with human be- 
ings in a natural way (Wilks, 1999). How- 
ever, no effective real world dialogue applica- 
tion exists so far, not only in spoken language 
for which speech recognition is still a big ob- 
stacle, but also in typing language. 
The most serious problem is knowledge. It 
is obvious that without sufficient knowledge a 
dialogue system cannot alk with people sat- 
isfactorily. Classical dialogue systems like UC 
(Wilensky et al, 1984) utilized a formal lan- 
guage to represent knowledge, which requires 
the heavy cost of construction and mainte- 
nance and makes the scaling up quite difficult. 
In contrast, along with the improvement 
of NLP, research activities which utilize nat- 
ural language text as a knowledge base be- 
come popular, such as START (Katz, 1990), 
FAQ Finder (Cooper, 1996), and QA Track 
in TREC 8 (NIST and DARPA, 2000). These 
systems, however, basically produce one time 
response, and do not have a conversation with 
users. 
This paper proposes a dialogue helpsystem 
in which natural anguage knowledge base is 
not only used for one time response, but also 
for conducting a conversation. To put it con- 
cretely, the system can 
? ask the user back if there is an unknown 
word in the user utterance, 
? interpret the user utterance contextually, 
and 
? ask the user back when the user utter- 
ance is not clear. 
In order to achieve these functions, a mild 
restriction is imposed on the knowledge base; 
it is compiled in a dictionary-like structure 
(entry and its description) and the descrip- 
tion is written in an if-then format (though 
several expressions can be used to show if- 
then relation). 
The dialogue helpsystem reported in this 
paper advises faculties and students in us- 
ing the computer facilities and software ap- 
plications of a university. The computer 
facilities of Kyoto University, about 1,200 
computers (PCs and WSs) in several rooms 
and modem connections are administrated 
by the Center for Information and Multime- 
dia Studies (CIMS; http://www.media.kyoto- 
u.ac.jp/). The situation at CIMS is that 
many people are using computers, teaching 
141 
assistants (TAs) help the users, but are not al- 
ways available, and certain fundamental ques- 
tions are asked frequently. Considering this, 
we assumed that it is a very promising domain 
for an experimental dialogue system. 
The system started its service on July 1999, 
received about 2,000 queries for the first seven 
months, and answered about 40% of them sat- 
isfactorily. 
2 Examples  
The following examples (real logs of the ser- 
vice) are illustrative of the breadth of dia- 
logues in which the system aims to partic- 
ipate. The system is capable of generating 
the answers hown below in response to most 
forms of the request users might supply 1. 
<1999 Jul 08 19:31:49> 
Ui: Method to invoke a screen save in 
UNIX? 
Si: What is "screen save"? 
U2: screen saver. 
$2: Type "xlock" in the command line. 
<1999 Aug 19 21:20:52> 
UI: Please tell me how can \] read emails in 
Emacs. 
S l: Select mail software from "mh-e", 
, ,mew ,~" 
U2: I try mh-e. 
$2: Type "M-x mh-rmail" on Emacs. Type 
'T' to get unread emails and type "" 
after you move the cursor on the line 
you want to read. 
<1999 Sep 16 22:32:20> 
UI: How can I use Mew in UNIX? 
SI: After invoking Emacs, type "M-x 
mew". See http://www.mew.org/ for
det~.~l.~. 
U2: How can I delete emails? 
$2: Type "d" in the Summary Mode and 
then type "x" for execution. 
1Although our system is a Japanese dialogue sys- 
tem, in this paper we use their English translations 
for the explanation. 
<1999 Oct 25 12:42:28> 
UI: Tell me how to use Netscape. 
SI: On the \[Start Menu\], click \[program\] 
and then click \[Netscape Navigator\]. 
U2: What's start menu? 
$2: \[Start Menu\] is a pull-down menu sum- 
marizing Windows applications, which 
appears when you chck \[Start\] icon on 
the left bottom of the desktop screen. 
U3: What is click? 
$3: I cannot explain "click" in detail. 
3 Types  o f  user  u t terances  
We observed the conversations of users and 
TAs in the CIMS computer rooms by record- 
ing and transcription (20 hours observation; 
1.5 hours recording). 
From this study, we discovered that user 
utterances can be classified as follows: 
What  type:  The user asks some fact. 
ex. What is Emacs? 
Explain Emacs. 
How type:  The user asks how to do some- 
thing. 
ex. How can I input Japanese characters 
in Emacs? 
I want to input Japanese characters 
in Emacs. 
Symptom type:  The user shows some 
symptom since he/she want to know 
how to cope with it. 
ex. I cannot access my private folder. 
The screen is white. 
Request  ype:  The user requests something 
to C1MS. 
ex. Please install Visual-C. 
Please make my disk quota bigger. 
Add i t ion  type:  The user adds or modifies 
the previous question. 
ex. How about WindowsNT? 
In the case of reply? 
142 
ut : How can I send emails by Mew? 
User 
User Interface 
(WWW browser) 
(KNP) 
Utterance content 
Dialogue 
Manager 
Knowledge 
Base 
Flexible Mathing 
1 <TITLE>Mcw</TITLE> 
b <SYN>mew, MEW, MIYU</SYN> 
<BT>Mail sofiwarc<\]BT> 
<DEF>A kind of mail sofficwam working on 
"~cbest <DESCRIPTION> .......... ? <KU>In order to invoke Mew, 
i Woe "M-x mew" on Emacs.</KU> :::::::::::::::::::::::::: ? "-<KU>Ifyou want o receive mails by Mew on Emacs, 
type "i'.</KU> 
? .-<KU>If you want o reply emails by Mew on Emacs, 
........ </KU> 
</DESCRIPTION> 
<TITLE>Mh-e</TITLE> 
<SYN>mh-e</SYN> 
<BT>Mail software</BT> 
Figure 1: The outline of the helpsystem. 
Answer  type:  The user answers the system 
question. 
ex. WINDOWS. 
The compression type is zip. 
The helpsystem reported in the paper an- 
swers what, how and symptom questions. In 
addition, it can interpret addition and answer 
utterances contextually. The request utter- 
ances are out of the system scope currently. 
4 Out l ine  o f  the  he lpsystem 
The system is comprised of the following com- 
ponents (Figure 1): 
User Interface:  Users access to the helpsys- 
tern via a WWW browser by using CGI 
based HTML forms. The helpsystem is 
actually running on a workstation i  our 
lab. 
Input  Analyzer:  The user utterance is 
transformed into a dependency structure 
by a robust parser, KNP (Kurohashi 
and Nagao, 1994; Kurohashi and Na- 
gao, 1998), and utterance-pattern rules 
are applied to extract he utterance type 
and the utterance content. 
Japanese is head-final and the final ex- 
pression shows an utterance type. There- 
fore, the longest matching of utterance- 
pattern rules form the end of the utter? 
ance can detect the utterance type in 
most cases. For example, if the final ex- 
pression is "niha doushitara ii desu ka 
(How can I -- -)", how type is assigned; if
"no baai ha (In case...)", addition type. 
143 
Knowledge  base: The knowledge base is 
written in a natural language, in a 
dictionary-like structure. 
D ia logue Manager :  The core process of 
the dialogue manager is to match the 
user utterance with the hmowledge base 
in order to find the most appropriate de- 
scription. It also handles contextual in- 
terpretation of the user utterance and 
question to the user. 
Eraal l  Sender :  The user can send his/her 
input-log to the CIMS staff via email if 
the automatic response is not satisfac- 
tory. So, the user does not have to in- 
put his questions a second time. This 
option surely contributes to the popular- 
ity of the system. 
In the following sections, we discuss the 
knowledge base and the dialogue manager, 
since we consider these components as the 
core of the system. 
5 Knowledge  base  
5.1 The  out l ine  
The knowledge base has a dictionary-like 
structure, in which each entry describes a 
concept/issue in the domain. It was com- 
piled manually by referring to the real QAs in 
CIMS rooms, the FAQ page of CIMS (about 
100 items), and question emails sent to CIMS 
(about 150 emails). Currently, it contains 
about 250 entries. 
Each entry consists of a headword 
(<TITLE> tag), synonyms(<SYN> tag), 
an upper word (<BT> tag), a definition 
of the headword (<DEF> tag) and sev- 
eral descriptions concerning the headword 
(<DESCRIPTION> tag; see Figure 1). All 
content words in the knowledge base were 
registered to the system database, which is 
used to see whether a user input word is 
known or unknown to the system (Section 
6.1). 
In addition, by collecting the headword 
and its upper word pairs from the knowl- 
edge base, the domain ontology (concept tax- 
onomy) is constructed automatically. The 
top categories of the current ontology are 
software, hardware, computer term (different 
to soft/hardware), action term, and general 
term. The domain ontology is used by the di- 
alogue manager in several ways (Section 6.2, 
6.3). 
5.2 Natura l  language representat ion  
In the knowledge base, the definition and sev- 
eral descriptions for the headword axe written 
in natural anguage, which provides both high 
power of expression and high extensibility. 
The definition of the headword is used for 
what questions; the descriptions are used for 
how and symptom questions. Each descrip- 
tion, called knowledge unit (abbreviated to 
KU), is written in the following style: 
<KU> if a case, then what/how to do. </KU> 
In Japanese, there are many sentential pat- 
terns to express if-then relation. Authors of 
the knowledge base can use several expres- 
sions like "--- deareba... ( i f - . . , - . - )" ,  "..- no 
baai ha. . .  (in case that .-., ..-)". 
The basic form of the how and symp- 
tom question is "in some case, what/how 
can I do?". Therefore, the system can an- 
swer the question by finding the most similar 
KU case part and showing the corresponding 
what/how to do part to the user (see Figure 
1). 
5.3 Match ing  of  user  quest ion  and  
knowledge  un i t  
Matching of the user question and a knowl- 
edge unit (KU) is done by comparing their 
dependency trees whose nodes are phrases. 
Their similarity is calculated as follows (Fig- 
ure 2): 
. For each phrase in the user question, the 
most similar phrase in the KU case part 
is looked for based on the following cri- 
teria: 
? Matching of content words : 3 points 
? The second or more matching 
of content words (when the 
phrase contains two or more 
content words) : 1 point 
144 
3+0+1+1=--5 
3+0+1+1=5 
The user question 
(The maximum atching score : 15) 
A knowledge uait 
(The maximum atching score : 20) 
The c~tainty score (5+5+5: 15 x 20 x 100 = 75 (%) 
Figure 2: Matching of the user question and a knowledge unit. 
? Matching of the depth of the 
phrases in parse trees : 1 point 
? Matching of the type of the 
phrases (phrase types differ 
depending on surface cases 
and verb conjugations, etc) : 1 point 
2. The similarity scores of phrases in the 
user question are summed up and nor- 
malized by the maximum matching score 
(MMS) as follows (the MMS is the simi- 
larity score with the same sentence): 
The sum of scores of~ 2 
phrase similarities \] 
The MMS of ~ (The MMS of~ 
the user question\] ? \ the  KU case\] 
The above score is given to the KU as its 
certainty score. 
The above algorithm cares for the struc- 
tures of sentences to some extent by giving 
phrase depth scores and phrase type scores, 
but not in a strict way. This leads to a flex- 
ible matching for handling a variety of natu- 
ral language sentences and some parse errors. 
For the present, the parameters were given 
empirically. 
6 Dialogue manager  
Figure 1 showed the simplest case of a QA. In 
some cases, however, the user and the system 
have to take more turns until the user obtains 
a satisfactory answer. Such a turn-taking is 
an essential point of a conversation. 
To conduct a conversation, that is, to per- 
form a proper turn-taking, the participant 
have to be able to do the following functions 
at least: 
* ask the opponent back if there is an un- 
known word in the opponent's utterance, 
? interpret the opponent's utterance con- 
textually, and 
? ask the opponent ba~k when the oppo- 
nent's utterance is not clear. 
Our dialogue helpsystem can perform the 
basic level of the above functions by referring 
to natural language knowledge base. In the 
following subsections, we explain each of these 
functions in detail. 
6.1 Ask ing  back  of  an unknown word  
Given the user utterance, the system first 
checks whether each content word in it is reg- 
istered in the system database or not. If the 
word is not registered, it means that the word 
is an unknown word to the system. An un- 
known word appears in the following cases: 
. Technical term not covered by the knowl- 
edge base. 
ex. shell script~ clone 
. Technical term whose synonym or related 
term is covered by the knowledge base. 
ex. Mozaic, Mozilla 
3. Misspell of the user. 
ex. Internetmai, Windo 
145 
Table 1: Patterns of the system responses. 
The best ~ of the candidate KUs 
cert~.~nty score one many 
100--60% 
60-30% 
30--0% 
<what /how to do> 
(of the KU) 
(one difference) 
Select <upper  concept> from <list 
of the difference>. 
(two or more differences) 
Your question is not clear. Select 
<list of the candidate KU cases>. 
Your question is not clear. Select 
<list of the candidate KU cases> 
I cannot answer your question. 
4. General term. 
ex. name, summer vacation 
The system decision, whether the unknown 
word is general term or not, is taken accord- 
ing to whether it is an entry of a children's 
dictionary or not (Tadika, 199'7). 
If the unknown word is not a general term, 
the system asks the user back in the form 
"what is 'unknown word'?". If the system 
asks "what is Internetmai?' ,  the user prob- 
ably notices his/her misspell and re-input 
it correctly. If the system asks "what is 
Mozilla?", the user might paraphrase it like 
"It means Netscape' .  
If the unknown word is a general word, it 
does not make sense to ask the user back, like 
''what is name?". Therefore, the system just 
overlooks it. 
6.2 Contextua l  in terpretat ion  of  user  
quest ions  
The user question may be related to its pre- 
ceding utterances, modifying or supplement- 
ing them. As an example, consider the fol- 
lowing dialogue: 
UI: How can I send emails by Mew? 
S1: Type "C-c C-c" when you finish writing 
a mail. 
U2: In case to reply. 
$2: Move the cursor on the message to 
which you want to reply and type "A". 
In this dialogue, the user utterance U2 is a 
modification of U1, indicating "How can I re- 
ply emails by Mew?". 
In order to interpret such a context depen- 
dent utterance properly, the dialogue man- 
ager attempt to merge the user's new utter- 
mace onto the previous one. For each word of 
the user's new utterance, Wnew, if the previ- 
ous utterance contains the word of the same 
category, Wold, w,~ew is overwritten on wol d. 
If not, Wne w is added to the previous utter- 
ance. Two words are considered to be in the 
same category if they belong to the same top 
category of the domain ontology described in 
Section 4.1. Then, the system looks up the 
knowledge base by the merged utterance. 
In the above example, "reply" of U2 is over- 
written on "send" of U1, since they belong 
to the same category, act ion  te rm.  Then the 
system attempts to match the combined ut- 
terance "How can I reply emails by Mew?" 
with the knowledge base, and as a final re- 
sult, it can response as $2 2 
In the above example, since U2 is an addi- 
t ion  utterance, the system does not need to 
interpret U2 as a new, context independent 
question. However, if the user utterance has 
a different type, it is not possible to decide 
whether it is context dependent or indepen- 
2Note that the system keeps the resultant interpre- 
tation of the user query, which means that the system 
can keep more than one user utterances practically. 
For example, if the user asks "In case to forward" af- 
ter $2 in the above xample, the system can interpret 
it as "How can I forward emails by Mew". 
146 
liiiiiiii~ii~!~iiiiiii~t -\] 
UI :  I would  l ike to uncompress  a file. ? . . . . . . . . . . . . . . .  "~ 
? 
S1 : Select compressed f i le f rom "lzh file" 66?/?\[  ~ 
and  "zip file". "~ ~, ~ 
Iby LHA U~iity ~ |by'LHA U~I,~ 
U2:  lzh me 
$2 : Invoke  LHA Ut i l i ty  32 and  select \ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
uncompress wizard from \[File\] menu. I u oo p o , I I I 
? Candidate KUs 
J 
. . . . . . .  Difference : Izh file, zip file 
Upper concept : compressed file 
J 
Figure 3: An example of handling many candidate KUs. 
dent by seeing the new utterance alone. 
To decide this, the dialogue manager e- 
gard s the certainty score between the utter- 
ance and the most similar KU as an appropri- 
ateness measure of the interpretation. That 
is, we calculate both the certainty score be- 
tween the merged utterance (contextual inter- 
pretation) and the knowledge base, and that 
between the new utterance as it is and the 
knowledge base. Then; we choose the inter- 
pretation which obtained the KU with the 
bigger certainty score. 
For example, in the following dialogue, U2 
is interpreted as "How to invoke Mew?", since 
this combined interpretation has the bigger 
certainty score rather than U2 itself. 
Ui: What is Mew? 
SI: A kind of mail software working on 
Emacs. 
U2: How to invoke? 
$2: Type 'M-x mew" on Emacs. 
On the other hand, in the next example, U2 
is interpreted as it is, since U2 has the big- 
ger certainty score rather than the combined, 
somehow strange interpretation of "How to 
uncompress zip files by Outlook on Unix?". 
UI: I want to send an email by Outlook. 
$1: Invoke OutlookExpress, write your 
email, and select "Send/Get" button. 
U2: How to uncompress zip files on UNIX. 
$2: Type "unzip \[zip filename\]" in the com- 
mand line. 
6.3 Asking back for an unc lear  
quest ion 
As mentioned so far, the system retrieves 
proper KUs which are similar to the user 
question. The KU with the best certainty 
score and the KUs with the 90% or larger cer- 
tainty score are called candidate KUs. The di- 
alogue manager performs differently depend- 
ing on the best certainty score and the num- 
ber of candidate KUs, as shown in Table 1. 
If the certainty score is 60% or higher and 
the number of the candidate KUs are two or 
more, the dialogue manager detects the dif- 
ference between their cases and ask the user 
to make his/her question more specific. 
Figure 3 shows an example of such a dia- 
logue. The two candidate KUs are detected 
for U1 and their cases have only one differ- 
ence: "lzh file" and "zip file". Then, the sys- 
tem detects their common upper concept in 
the domain ontology and ask the user in the 
form "Select <upper concept> from <list of 
the difference >" as shown in Figure 3. 
If the candidate KUs contain two or more 
differences, it is hard to edit them in a neat 
way. Therefore, the system shows the candi- 
date KUs' cases as they are. If the certainty 
score is less than 60% and larger than 30%, 
the system responses in the same way. 
147 
o o  ~ " ~  = . . . . . . .  , , , ,  ~ 7 ~  ~o 7 
Figure 4: Evaluation of the helpsystem. 
7 Eva luat ion  
The helpsystem started its service on July 
1999 as part of the CIMS web service. All 
conversation logs between users and the sys- 
tem have been stored as a dialogue database. 
In the dialogue database, ach dialogue be- 
tween a user and the system is segmented into 
task units manually. We call this unit a ses- 
sion. Figure 4 shows the number of sessions 
and their evaluation of each week from July 
5th to January 30th. On average, there are 
70 sessions in a week; a dialogue with a user 
and the system consists of 2.1 sessions, which 
means a user asks 2.1 topics in one dialogue; 
one session consists of 3.2 turns. 
The evaluation of sessions is based on the 
following criteria. 
Success: The system could return a satisfac- 
tory answer. 
Fa i lure : Input  Analyzer:  'The system 
could not response properly because of 
the input analysis error, mostly the lack 
of utterance-pattern rules. Utterance~ 
pattern rules are added whenever the 
lack is found. 
Fai lure:Dialog Manager :  The system 
could not response properly because 
of the dialogue manager error. Dia- 
logue manager error comes both from 
simple bugs of the system and from 
unnoticed patterns of the user response. 
For example, when the system asks 
"select from A and B" expecting the 
answer "A" or "B", a user might answer 
"the latter". The system is modified 
whenever necessary. 
Fai lure:Knowledge: The system could not 
answer the question because of the lack 
of knowledge. This is the major rea- 
son of the failure as shown in Figure 4. 
Though the knowledge base is being ex- 
tended step by step, the range of the user 
148 
query is unlimited, including troubles in 
using PCs and advanced settings of soft- 
ware/hardware. 
Failure:Difi lcult: Current system architec- 
ture could not handle the question. For 
example, a user sometimes asks "what is 
the difference between A and B", or when 
the system asks "select from A and B", a 
user answers "I don't know". In order to 
handle such utterances, we are planning 
to improve the system to exploit defini- 
tions of "A" and "B". 
Out of scope: Out of the system domain, 
such as questions about telephone 
charges in using PPP or the Y2K prob- 
lem. 
Miscel laneous: Such as "hello", "this is a 
test" or just a simple typo like "a". 
The success ratio, that is, the ratio of Suc- 
cess over Success plus Failure, of the whole 
period is 37%. The system became stable 
around October 1999, and the success ratio 
after that (14 weeks) is 39%. Considering rel- 
atively wide domain the system have to cover, 
we feel the success ratio is reasonable, and the 
system is contributing to CIMS to some ex- 
tent by handling simple FAQs like "how to 
change my password". 
8 Conc lus ion  
This paper described the dialogue helpsys- 
tern, which has been working in practice with 
real users. 
Construction of natural language knowl- 
edge base needs ome cost, though it is much 
easier than that of formal language knowl- 
edge base. However, providing a high-quality 
service needs cost; good manuals and FAQs 
are important for any products, and a large 
amount of materials are prepared for cus- 
tomer service operators. With that in mind, 
we can say preparing a good document is a 
universal problem, not just to a dialogue sys- 
tem. 
By rlmning the system, the real dialogue 
database can be accumulated. Based on this 
database, we would like to study the phenom- 
ena of man-machine conversation and to ex- 
tend our work to user modeling, user intention 
estimation, and other interesting dialogue re- 
search areas. 
The system is designed to be domain- 
independent and can be ported to a new do- 
main by preparing a domain knowledge base. 
Exploiting this merit, we are planning to con- 
struct the automatic reference service system 
of Kyoto University Library, which certainly 
provides us with a wider breadth of dialogue 
data. 
Re ferences  
Edwin Cooper. 1996. Improving FAQ Finder's 
performance: Setting parameters by getetic 
programming. In Working Notes of the AAAI 
Spring Symposium on Machine Learning in In- 
formation Access. 
B Katz. 1990. Using english for indexing and 
retrieving. In Artificial Intelligence at MIT. 
Vol.1, MIT Press, pages 134-165. 
Sadao Kurohashi and Makoto Nagao. 1994. A 
syntactic analysis method of long Japanese sen- 
tences based on the detection of conjunctive 
structures. Computational Linguistics, 20(4). 
Sadao Kurohashi and Makoto Nagao. 1998. 
Building a Japanese parsed corpus while im- 
proving the parsing system. In Proceedings of 
the First International Conference on Language 
Resources ~ Evaluation, pages 719-724. 
NIST and DARPA. 2000. The Eighth Text RE- 
trieval Conference (TREC-8). NIST Special 
Publication. 
Jyunichi Tadika, editor. 1997. Reika Shougaku 
Kokugojiten (Japanese dictionary for children). 
Sanseido. 
Robert Wilensky, Yigal Arens, and David Chin. 
1984. Talking to unix in English: An oveview of 
UC. Communications of the ACM, 27(6):574- 
593. 
Yorick Wilks, editor. 1999. Machine Conversa- 
tions. Kluwer Academic Publishers. 
149 
Nonlocal Language Modeling 
based on Context Co-occurrence Vectors 
Sadao Kurohash i  and  Manabu Or i  
Graduate School of Informatics 
Kyoto University 
Yoshida-honmachi, Sakyo; Kyoto, 606-8501 Japan 
kuro@?, ky0to -u ,  ac. j p, or iOpine,  kuee. kyoto -u ,  ac.  j p 
Abstract  
This paper presents a novel nonlocal lmlguage 
model which utilizes contextual information. 
A reduced vector space model calculated from 
co-occurrences of word pairs provides word 
co-occurrence v ctors. The sum of word co- 
occurrence vectors represents ile context of a 
document, and the cosine similarity between 
the context vector and the word co-occurrence 
vectors represents he \]ong-distmlce exical de- 
pendencies. Experiments on the Mainichi 
Newspaper corpus show significant improve- 
ment in perplexity (5.070 overall and 27.2% 
on target vocabulary) 
1 I n t roduct ion  
Human pattern recognition rarely handles iso- 
lated or independent objects. We recog- 
nize objects in various patiotemporal circum- 
stances uch as an object in a scene, a word 
in an uttermlce. These circumstances work 
as conditions, eliminating ambiguities and en- 
abling robust recognition. The most challeng- 
ing topics in machine pattern recognition are 
in what representation a d to what extent 
those circumstances are utilized. 
In laalguage processing, a context--that is;
a portion of the utterance or the text before 
the object--is ml important circumstmlce. 
One way of representing a context is statis- 
tical language nmdels which provide a word 
sequence probability, P(w~), where w~ de- 
notes the sequence wi . . .w j .  In other words, 
they provide the conditional probability of a 
word given with the previous word sequence, 
P( wilw~-l ), which shows the prediction of a 
word in a given context. 
The most conmmn laalguage models used 
nowadays are N-granl models based on a 
(N-  1)-th order Markov process: event pre- 
dictions depend on at most (N-  1) previous 
events. Therefore, they offer the following ap- 
proximation: 
P(w.ilw  -1)   wiJwi_N+l) (I) 
A common value for N is 2 (bigram language 
model) or 3 (trigram language model); only 
a short local context of one or two words is 
considered. 
Even such a local context is effective in 
some cases. For example, in Japanese, after 
the word kokumu 'state affairs', words such as 
daijin 'minister' mad shou 'department' likely 
follow; kaijin 'monster' and shou 'priZe' do 
not. After dake de 'only at', you cml often 
find wa (topic-marker), but you hardly find 
ga (nominative-marker) or wo (accusative- 
marker). These examples how behaviors of 
compound nouns and function word sequences 
are well handled by bigram mad trigraan mod- 
els. These models are exploited in several ap- 
plications uch as speech recognition, optical 
character recognition and nmrphological nal- 
ysis. 
Local language models, however, cannot 
predict nmch in some cases. For instance, the 
word probability distribution after de wa 'at 
(topic-marker)' is very flat. However, even if 
the probability distribution isflat in local lan- 
guage models, the probability of daijin 'min- 
ister' and kaijin 'monster' must be very differ- 
ent in documents concenfing politics. Bigram 
and trigram models are obviously powerless 
to such kind of nonlocal, long-distmlce l xical 
dependencies. 
This paper presents a nonlocal language 
model. The important information concern- 
ing long-distance l xical dependencies is the 
word co-occurrence information. For example, 
words such as politics, govermnent, admin- 
istration, department, tend to co-occur with 
daijin 'minister'. It is easy to measure co- 
occurrences ofword pairs from a training cor- 
pus, but utilizing them as a representation f 
context is the problem. We present a vector 
80 
Wl 
W2 
w3 
w4 
w5 
w6 
D1 D2 D3 D4 D~ D6 D7 Ds 
1 0 1 0 1 0 1 0 
1 O 1 1 0 0 0 0 
0 1 0 0 1 1 0 1 
1 1 1 0 0 0 0 0 
0 0 0 O 1 0 1 0 
0 0 0 0 1 0 0 1 
Wl 
w2 
w3 
w4 
w5 
w6 
Wl W2 w3 W4 w5 w 6 
4 2 1 2 2 1 
3 0 2 0 0 
4 1 1 2 
3 0 0 
2 1 
2 
Figure 1: V~rord-document co-occurrence ma- 
trix. 
representation f word co-occurrence informa- 
tion; and show that the context can be repre- 
sented as a sum of word co-occurrence v ctors 
in a docmnent and it is incorporated in a non- 
local language model. 
2 Word  Co-occur rence  Vector  
2.1 Word-Document  Co-occur rence  
Mat r ix  
Word co-occurrences are directly represented 
in a matrix whose rows correspond to words 
and whose columns correspond to documents 
(e.g. a newspaper article). The element of 
the matrix is 1 if the word of the row ap- 
pears in the document of the colunm (Figure 
1). Wre call such a matrix a word-document 
co-occurrence matrix. 
The row-vectors of a word-document co- 
occurrence matrix represent the co-occurrence 
information of words. If two words tend to ap- 
pear in the same documents, that is: tend to 
co-occur, their row-vectors are similar, that is, 
they point in sinfilar directions. 
The more document is considered, the more 
reliable and realistic the co-occurrence infor- 
mation will be. Then, the row size of a word- 
document co-occurrence matrix may become 
very large. Since enormous amounts of online 
text are available these days, row size can be- 
come more than a million documents. Then, 
it is not practical to use a word-docmnent co- 
occurrence matrix as it is. It is necessary to 
reduce row size and to simulate the tendency 
in the original matrix by a reduced matrix. 
2.2 Reduct ion  o f  Word-Document  
Co-occur rence  Matr ix  
The aim of a word-document co-occurrence 
matrix is to measure co-occurrence of two 
words by the angle of the two row-vectors. 
In the reduction of a matrix, angles of two 
row-vectors in the original matrLx should be 
maintained in the reduced matrLx. 
Figure 2: ~Vord-word co-occurrence matrix. 
As such a matrix reduction, we utilized a 
learning method developed by HNC Software 
(Ilgen and Rushall, 1996). 1 
1. Not the word-docmnent co-occurrence 
matrix is constructed from tile learning 
corpus, but a word-word co-occurrence 
matrix. In this matrix: the rows and 
colunms correspond to words and the i- 
th diagonal element denotes the number 
of documents in which the word wl ap- 
pears, F(wi). The i:j-th element denotes 
the number of documents in which both 
words w,: and wj appear, F(wi, wj) (Fig- 
ure 2). 
The importmlt information in a word- 
document co-occurrence matrix is the co- 
sine of the angle of the row-vector of wi 
and that of wj, which can be calculated 
by the word-word co-occurrence matrix 
as follows: 
F(w,:, wj) (2) 
This is because x/F(wi) corresponds to 
the magnitude of the row-vector of wl, 
and F(wl, wi) corresponds to the dot 
product of the row-vector of wl and 
that of wj in the word-docmnent co- 
occurrence matrix. 
2. Given a reduced row size, a matrix is ini- 
tialized as follows: matrix elements are 
chosen from a normal distribution ran- 
domly, then each row-vector is normal- 
ized to magnitude 1.0. The random refit 
row-vector of the word wl is denoted as 
,WCi Rand. 
Random unit row-vectors in high di- 
mensional floating point spaces have a 
1The goal of HNC was the enhancement of text 
retrieval. The reduced word vectors were regarded as 
semantic representation f words and used to represent 
documents and queries. 
81 
sori wa kakugi de' kankyo mondai 
(Prime Minister) (Cabinet meeting) (environment) (issue) 
I wc I \] wc I I  wc 
\ 
ni tuite 
w (cc ? wc) 2 Pc  
kaigi (conference) 0.237962 0.002702 
senkyo (election) 0.150773 0.001712 
yosan (budget) 0.128907 0.001463 
daijin (minister) 0.018549 0.000211 
yakyu (baseball) 0.004556 0.000052 
kaijin (monster) 0.000002 0.000000 
sugaku (mathematics) 0.000001 0.000000 
TOTAL 88.079230 1.000000 
Figure 3: An example of context co-occurrence probabilities. 
property that is referred to a "qnasi- 
orthogonality'. That is; the expected 
~?alue of the dot product between an3" 
pair of random row-vectors, wci  Rand and 
wet  and, is approximately equal to zero 
(i.e. all vectors are approximately or- 
thogonal). 
3. The trained row-vector, wai is calculated 
as follows: 
WCi -~ ~13C~ and + "q ~ O'ij'T.ll4 and 
J (3) 
wc - (4) 
The procedure iterates the following calcu- 
lation: 
OJ 
wen e~' = wc l  - q Owe/  
= + rl (a j - we~.  wcj)wc  
(6) 
new -- W C7 e~: 
ilwcF wl I (7) 
The learning method by HNC is a rather 
simple approximation of the procedure, doing 
just one step of it. Note that wci .wc j  is 
approximately zero for the initialized random 
vectors. 
ai j  corresponds to the degree of the co- 
occurrence of two words. By adding 
wc~ and to wet  a'd depending on ai j ,  th.e 
learning formula (3) achieves that two 
words that, tend to co-occur will have 
trained vectors that point in shnilar di- 
rections, r/is a design parameter chosen 
to optimize performance. The formula 
(4) is to normalize vectors to magnitude 
1.0. 
We call the trained row-vector we/o f  the 
word wi a word co-occurrence vector. 
The background of the above method is a 
stochastic gradient descent procedure for min- 
imizing the cost function: 
1 J = ~ .~(a i j  -- we/"  wcj )  2 (5) 
%3 
subject to the constraints \[\[we/I\[ = 1. 
3 Context  Co-occur rence  Vector  
The next question is how to represent he 
context of a document based on word co- 
occurrence vectors. We propose a simple 
model which represents the context as the sum 
of the word co-occurrence vectors associated 
with content words ill a document so far. It 
should be noted that the vector is normalized 
to unit length. V~re call the resulting vector a 
context co-occurrence vector. 
W'ord co-occurrence vectors have the prop- 
erty that words which tend to co-occur have 
vectors that. point in similar directions. Con- 
text co-occurrence vectors are expected to 
have the sinfilar property. That is, if a word 
tends to appear in a given context, the word 
co-occurrence vector of the word and the con- 
text co-occurrence vector of the context will 
point in similar directions . . . . . .  
Such a context co-occurrence vector can be 
seen to predict the occurrence of words in a 
82 
where 
p(.wdwi_,) = ( P(C~lwi-' ) x P(wdw~-'Cc) 
P(Cflwj-') x P(wdw~-lc/) ( 
if wl E C~ 
if wi E C/ 
P(C~Iw~ -1) 
P(wilw~-:C~) 
P(wi\[w -lc/) 
= A:P(Cc) + A2P(C~lwi_l ) + A3P(C~\[wi-2wi-1) 
= AclP(wiICc) + A~2P(wi\[wi-lC~) + A~3P(wi\[wi-2Wi-lCc) 
= 1-  P(C~lwj - : )  
= a/ :P (wdc / )  + a/2P(wd,  -:ci) +
with 
Figure 4: Context language model. 
given context, mad is utilized as a component 
of statistical language modeling, as shown in 
the next section. 
4 Language Model ing using 
Context  Co-occur rence  Vector  
4.1 Context Co-occurrence 
Probab i l i ty  
The dot product of a context co-occurrence 
vector and a word co-occurrence vector shows 
the degree of affinity of the context m:d the 
word. The probability of a content word based 
on such dot products, called a context co- 
occurrence probability, can be calculated as 
follows: 
Pc(wilw~_lcc) = f(cc~ -1 "~cl) 
~wjEcc f(cc~ -1" ~vcj) 
(S) 
where cc~ -1 denotes the context co-occurrence 
vector of the left context, Wl . . .  wi-1, and Cc 
denotes a content word class. Pc(wilw~-lcc) 
metals the conditional probability of wi given 
that a content word follows wj- : .  
One choice for the function .f(x) is the iden- 
tity. However, a linear contribution of dot 
products to the probability results in poorer 
estimates, since the differences of dot prod- 
ucts of related words (tend to co-occur) and 
unrelated words are not so large. Experiments 
showed that x 2 or x 3 is a better estimate. 
An example of context co-occurrence prob- 
abilities is shown in Figure 3. 
4.2 Language Modeling using Context 
Co-occurrence Probab i l i ty  
Context co-occurrence probabilities can ham 
dle long-distance l xical dependencies while a 
standard trigram model can handle local con- 
texts more clearly: in this way they comple- 
ment each other. Therefore, language model- 
ing of their linear interpolation is employed. 
Note that tile linear interpolation of unigram, 
bigram and trigram models is simply referred 
to 'trigxan: model' in this paper. 
The proposed language model, called a con- 
text language model, computes probabilities 
as shown in Figure 4. Since context co- 
occurrence probabilities are considered only 
for content words (Cc), probabilities are cal- 
culated separately for content words (Co) and 
function words (C/). 
P(Cc\[w~ -1) denotes the probability that a 
content word follows w~-:, which is approx- 
imated by a trigrmn nmdel. P(.wi\[w~-lcc) 
denotes the probability that wi follows w~-: 
given that a content word follows w~-:, which 
is a linear interpolation of a standard trigram 
model and the context co-occurrence proba- 
bilities. 
In the case of a function word, since the 
context co-occurrence probability is not con- 
sidered, P(wdw~-lCi) is just a standard tri- 
granl model. 
X's adapt using an EM re-estimation proce- 
dure on the held-out data. 
83 
Table 1: Perplexity results for the stmldard trigrazn model and the context language nmdel. 
Perplexity on Perplexity on 
Language Model the entire the target 
vocabulary vocabulary 
Standard Trigram Model 107.7 1930.2 
Context Language Model 
Vector size 0 f (x)  
500 0.5 x ~ 
1000 0.3 x ~ 
1000 0.5 x 
* 1000 0.5 x 2 
1000 0.5 x 3 
1000 1.0 x 2 
2000 0.5 x 2 
106.3 (-1.3%) 
~o 102.7 (-4., %) 
103.6 (-3.9%) 
102.4 (-5.0%) 
102.4 (-5.0%) 
102.5 (-4.8%) 
102.4 (-5.0%) 
1663.8 (-13.8%) 
1495.9 (-22.5%) 
1496.1 (-22.5%) 
1406.2 (-27.2%) 
1416.8 (-26.9%) 
1430.3 (-25.9%) 
1408.1 (-27.1%) 
Standard Bigram Model 130.28 2719.67 
Context Language Model 
125.06 (-4.0%) 
122.85 (-5.7%) 
1000 0.5 x 
1000 0.5 x 2 
2075.10 (-23.7%) 
1933.68 (-28.9%) 
shijyo no ~ wo ~ ni Wall-gai ga kakkyou wo teishi, bei kabushiki 
'US' 'stock' 'market' 'sudden rise' 'background' %Vall Street' 'activity' 'show' 
wagayonoharu wo ~a~ shire iru. \[shoukenl kaisha, ~h~ ginkou wa 1996 nen ni 
'prosperity' 'enjoy' 'do' 'stock' 'company' 'investment' 'bank' 'year' 
halite ka o saiko  l ko shi  \] '96 ne,  I k b shiki l so.ha '95 
'enter' 'past' maximum' 'profit' 'renew' 'year' 
ni I .tsuzuki\] kyushin . mata \] kab.uka\] kyushin wo 
'continue' 'rapid increase' 'stock price' 'rapidly increase' 
I shinkabul hakkou ga ~ saikou to natta. 
'new stock' 'issue' 'past' 'maximum' 'become' 
'stock' 'market' 'year' 
ni ~u~ no 
'background' 'corporation' 
Figure 5: Comparison of probabilities of content words by the trigraan model and the context 
model. (Note that wa, ga, wo, ni; to and no are Japanese postpositions.) 
4.3 Test Set Perp lex i ty  
By using the Mainichi Newspaper corpus 
(from 1991 to 1997, 440,000 articles), test 
set perplexities of a standard trigrmn/bigram 
model and the proposed context language 
model are compared. The articles of six 
years were used for the leanfing of word co- 
occurrence vectors, unigrams, bigrmns and 
trigrams; the articles of half a year were used 
as a held-out data for EM re-estimation f A's; 
the remaining articles (half a year) for com- 
puting test set perplexities. 
Word co-occurrence v ctors were computed 
for the top 50,000 frequent content words (ex- 
cluding pronouns, numerals, temporal nouns, 
mad light verbs) in the corpus, and unigrmn: 
bigrmn and trigrmn were computed for the top 
60,000 frequent words. 
The upper part of Table 1 shows thecom- 
parison results of the stmldard trigram model 
and the context language model. For the best 
parameters (marked by *), the overall per- 
plexity decreased 5.0% and the perplexity on 
target vocabulary (50,000 content words) de- 
creased 27.270 relative to the standard trigram 
model. For the best parameters, A's were 
adapted as follows: 
A1 = 0.08, A2 = 0.50, A3 = 0.42 
Acl = 0.03, ~c2 = 0.50, Xc3 = 0.30, Xcc = 0.17 
Afl = 0.06, ~f2 = 0.57, A f3 = 0.37 
As for parazneter settings, note that per- 
formance is decreased by using shorter word 
co-occurrence vector size. The vaxiation of 
~/does not change the performance so much. 
84 
f (x )  = x 2 and f (x )  = x 3 are alnmst the same; 
better thaaa f (x )  = x. 
The lower part of Table 1 shows the compar- 
ison results of the standard bigram model and 
the context language model. Here, the context 
language model is based on the bigrana model, 
that is, the terms concerning trigrmn in Fig- 
ure 4 were eliminated. The result was similar, 
but the perplexity decreased a bit more; 5.7% 
overall and 28.9% on target vocabulary. 
Figure 5 shows a test article in which the 
probabilities of content words by the trigram 
lnodel aald the context model are compared. If 
that by the context model is bigger (i.e. the 
context model predicts better), the word is 
boxed; if not, the word is underlined. 
The figure shows that the context model 
usually performs better after a function word, 
where the trigram model usually has little pre- 
diction. On the other hand, the trigram model 
performs better after a content word (i.e. in 
a compound noun) because a clear prediction 
by the trigram model is reduced by paying 
attention to the relatively vague context co- 
occurrence probability (Acc is 0.17). 
The proposed model is a constant interpo- 
lation of a trigram model and the context co- 
.0ccurrence probabilities. More adaptive inter- 
polation depending on the N-gram probabil- 
ity distribution may improve the performance. 
5 Re la ted  Work  
Cache language models (Kuhn mad de Mori, 
1990) boost the probability of the words al- 
ready seen in the history. 
Trigger models (Lau et al, 1993), even more 
general, try to capture the co-occurrences be- 
tween words. While the basic idea of our 
model is similar to trigger models, they handle 
co-occurrences of word pairs independently 
and do not use a representation of the whole 
context. This omission is also done in ap- 
plications such as word sense dismnbiguation 
(Yarowsky: 1994; FUNG et al, 1999). 
Our model is the most related to Coccaro 
mad Jurafsky (1998), in that a reduced vec- 
tor space approach was taken and context is 
represented by the accumulation of word co- 
occurrence vectors. Their model was reported 
to decrease the test set perplexity by 12%, 
compared to the bigram nmdel. The major 
differences are: 
1. SVD (Singular Value Decomposition) 
was used to reduce the matrix which is 
common in the Latent Semaaltic Analysis 
(Deerwester et ai.; 1990), and 
2. context co-occurrence probabilities were 
computed for all words, and the degree 
of combination of context co-occurrence 
probabilities and N-gram probabilities 
was computed for each word, depending 
on its distribution over the set of docu- 
l nents .  
As for the first point, we utilized the 
computationally-light, i eration-based proce- 
dure. One reason for this is that the com- 
putational cost of SVD is very high when 
millions or more documents are processed. 
Furthermore, considering an extension of our 
nmdel with a cognitive viewpoint, we believe 
an iteration-based model seems more reason- 
able than an algebraic model such as SVD. 
As for the second point, we doubt the ap- 
propriateness to use the word's distribution 
as a measure of combination of two models. 
What we need to do is to distinguish words 
to which semantics hould be considered and 
other words. We judged the distinction of con- 
tent words and function words is good enough 
for that purpose, and developed their trigram- 
based distinction as shown in Figure 4. 
Several topic-based models have been pro- 
posed based on the observation that certain 
words tend to have different probability dis- 
tributions in different topics. For example, 
Florian and Yarowsky (1999) proposed the fol- 
lowing model: 
t 
(9) 
where t denotes a topic id. Topics are 
obtained by hierarchical clustering from a 
training corpus, and a topic-specific language 
model, Pt, is learned from the clustered ocu- 
ments. Reductions in perplexity relative to a 
bigrmn model were 10.5% for the entire text 
and 33.5% for the target vocabulary. 
Topic-based models capture long-distance 
lexical dependencies via intermediate topics. 
In other words, the estimated istribution of 
topics, P(t\]w~), is the representation f a con- 
text. Our model does not use such interme- 
diate topics, but accesses word cg-occurrence 
information directly aald represents a context 
as the accumulation of this information. 
85 
6 Conclusion 
In this paper we described a novel language 
model of incorporating long-distance lexical 
dependencies based on context co-occurrence 
vectors. Reduced vector representation of 
word co-occurrences nables rather simple but 
effective representation of the context. Sig- 
nificant reductions in perplexity are obtained 
relative to a staaldard trigram model: both on 
the entire text. (5.0~) and on the target vo- 
cabulary (27.2%). 
Acknowledgments  
The research described in this paper was sup- 
ported in part. by JSPS-RFTF96P00502 (The 
Japan Society for the Promotion of Science, 
Research for the Future Program). 
References 
Noah Coccaxo and Daniel Jurafsky. 1998. To- 
wards better integration of semantic predictors 
in statistical language modeling. In Proceedings 
of ICSLP-98, volume 6, pages 2403-2406. 
Scott Deem, ester, Susan T. Dumais, George W. 
Furnas, Thomas K. Landauer, and Richard 
Harshmaa~. 1990. Indexing by latent semantic 
analysis. Journal of the American Society for 
Information Science, 41(6):391-407. 
Radu Florian and David Yarowsky. 1999. Dy- 
namic nonlocal anguage modefing via hierar- 
chical topic-based adaptation. In Proceedings off 
the 37rd Annual Meeting of ACL, pages 167- 
174. 
Pascale FUNG, LIU Xiaohu, mad CHEUNG Chi 
Shun. 1999. Mixed language query disambigua- 
tion. In Proceedings of the 37rd Annual Meeting 
of A CL, pages 333-340. 
Maa'd R. Ilgen and David A. Rushall. 1996. Re- 
cent advances in HNC's context vector informa- 
tion retrieval technology. In TIPSTER PRO- 
GRAM PHASE II, pages 149--158. 
R. Kuhn and IL de Mori. 1990. A cache-based 
natural anguage model for speech recognition. 
IEEE Transactions on Pattern Analysis and 
Machine Intelligence, 12(6):570-583. 
R. Lau, Ronald Rosenfeld, and Safim Roukos. 
1993. Trigger based language models: a max- 
imum entropy approach. In Proceedings of 
ICASSP, pages 45-48. 
David Yarowsky. 1994. Decision fists for lexical 
ambiguity resolution : Application to accent 
restoration in Spanish and French. In Proceed- 
ings o/the 32nd Annual Meeting of A CL, pages 
88-995. 
86 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 146?149,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation and Named Entity Recognition by 
Character Tagging 
 
Kun Yu1      Sadao Kurohashi2     Hao Liu1     Toshiaki Nakazawa1 
 Graduate School of Information Science and Technology, The University of 
Tokyo, Tokyo, Japan, 113-86561 
Graduate School of Informatics, Kyoto University, Kyoto, Japan, 606-85012 
 {kunyu, liuhao, nakazawa}@kc.t.u-tokyo.ac.jp1 
kuro@i.kyoto-u.ac.jp2 
  
Abstract 
This paper describes our word segmenta-
tion system and named entity recognition 
(NER) system for participating in the 
third SIGHAN Bakeoff. Both of them are 
based on character tagging, but use dif-
ferent tag sets and different features. 
Evaluation results show that our word 
segmentation system achieved 93.3% and 
94.7% F-score in UPUC and MSRA open 
tests, and our NER system got 70.84% 
and 81.32% F-score in LDC and MSRA 
open tests. 
1 Introduction 
Dealing with word segmentation as character 
tagging showed good results in last SIGHAN 
Bakeoff (J.K.Low et al,2005). It is good at un-
known word identification, but only using char-
acter-level features sometimes makes mistakes 
when identifying known words (T.Nakagawa, 
2004). Researchers use word-level features 
(J.K.Low et al,2005) to solve this problem. 
Based on this idea, we develop a word segmenta-
tion system based on character-tagging, which 
also combine character-level and word-level fea-
tures. In addition, a character-based NER module 
and a rule-based factoid identification module 
are developed for post-processing.  
Named entity recognition based on character-
tagging has shown better accuracy than word-
based methods (H.Jing et al,2003). But the small 
window of text makes it difficult to recognize the 
named entities with many characters, such as 
organization names (H.Jing et al,2003). Consid-
ering about this, we developed a NER system 
based on character-tagging, which combines 
word-level and character-level features together. 
In addition, in-NE probability is defined in this 
system to remove incorrect named entities and 
create new named entities as post-processing. 
2 Character Tagging for Word 
Segmentation and NER 
2.1 Basic Model 
We look both word segmentation and NER as 
character tagging, which is to find the tag se-
quence T* with the highest probability given a 
sequence of characters S=c1c2?cn.  
)|(maxarg* STPT
T
=  (1) 
Then we assume that the tagging of one char-
acter is independent of each other, and modify 
formula 1 as 
?
=
=
=
=
=
n
i
ii
tttT
nn
tttT
ctP
ccctttPT
n
n
1...
2121
...
*
)|(maxarg     
)...|...(maxarg
21
21
 (2) 
 Beam search (n=3) (Ratnaparkhi,1996) is ap-
plied for tag sequence searching, but we only 
search the valid sequences to ensure the validity 
of searching result. SVM is selected as the basic 
classification model for tagging because of its 
robustness to over-fitting and high performance 
(Sebastiani, 2002). To simplify the calculation, 
the output of SVM is regarded as P(ti|ci). 
2.2 Tag Definition 
Four tags ?B, I, E, S? are defined for the word 
segmentation system, in which ?B? means the 
character is the beginning of one word, ?I? means 
the character is inside one word, ?E? means the 
character is at the end of one word and ?S? means 
the character is one word by itself. 
For the NER system, different tag sets are de-
fined for different corpuses. Table 1 shows the 
146
tag set defined for MSRA corpus. It is the prod-
uct of Segment-Tag set and NE-Tag set, because 
not only named entities but also words are seg-
mented in this corpus. Here NE-Tag ?O? means 
the character does not belong to any named enti-
ties. For LDC corpus, because there is no seg-
mentation information, we delete NE-Tag ?O? 
but add tag ?NONE? to indicate the character 
does not belong to any named entities (Table 2). 
Table 1 Tags of NER for MSRA corpus 
Segment-Tag NE-Tag 
B, I, E, S ? PER, LOC, ORG, O 
Table 2 Tags of NER for LDC corpus 
Segment Tag NE Tag 
B, I, E, S ? PER, LOC, ORG, GPE + NONE 
2.3 Feature Definition 
First, some features based on characters are 
defined for the two tasks, which are: 
(a) Cn (n=-2,-1,0,1,2) 
(b) Pu(C0) 
Feature Cn (n=-2,-1,0,1,2) mean the Chinese 
characters appearing in different positions (the 
current character and two characters to its left 
and right), and they are binary features. A char-
acter list, which contains all the characters in the 
lexicon introduced later, is used to identify them.
 
Besides of that, feature Pu(C0) means whether C0 
is in a punctuation character list. It is also binary 
feature and all the punctuations in the punctua-
tion character list come from Penn Chinese Tree-
bank 5.1 (N.Xue et al,2002). 
In addition, we define some word-level fea-
tures based on a lexicon to enlarge the window 
size of text in the two tasks, which are:  
(c) Wn (n=-1,0,1) 
Feature Wn (n=-1,0,1) mean the lexicon words 
in different positions (the word containing C0 
and one word to its left and right) and they are 
also binary features. We select all the possible 
words in the lexicon that satisfy the requirements, 
not like only selecting the longest one in 
(J.K.Low et al,2005). To create the lexicon, we 
use following steps. First, a lexicon from NICT 
(National Institute of Information and Communi-
cations Technology, Japan) is used as the basic 
lexicon, which is extracted from Peking Univer-
sity Corpus of the second SIGHAN Bakeoff 
(T.Emerson, 2005), Penn Chinese Treebank 4.0 
(N.Xue et al,2002), a Chinese-to-English Word-
list1  and part of NICT corpus (K.Uchimoto et 
al.,2004; Y.J.Zhang et al,2005). Then, all the 
words containing digits and letters are removed 
                                                 
1
 http://projects.ldc.upenn.edu/Chinese/  
from this lexicon. At last, all the punctuations in 
Penn Chinese Treebank 5.1 (N.Xue et al,2002) 
and all the words in the training data of UPUC 
and MSRA corpuses are added into the lexicon.  
Besides of above features, some extra features 
are defined only for NER task. 
First, we add some character-based features to 
improve the accuracy of person name recognition, 
which are CNn (n=-2,-1,0,1,2). They mean 
whether C
 n (n=-2,-1,0,1,2) belong to a Chinese 
surname list. All of them are binary features. The 
Chinese surname list contains the most famous 
100 Chinese surnames, such as ?, ?, ?, ? 
(Zhao, Qian, Sun, Li). 
Then, we add some word-based features to 
help identify the organization name, which are 
WORGn (n=-1,0,1). They mean whether W n (n= 
-1,0,1) belong to an organization suffix list. All 
of them are also binary features. The organiza-
tion suffix list is created by extracting the last 
word from all the organization names in the 
training data of both MSRA and LDC corpuses. 
3 Post-processing 
Besides of the basic model, a NER module 
and a factoid identification module are developed 
in our word segmentation system for post-
processing. In addition, we define in-NE prob-
ability to delete the incorrect named entities and 
identify new named entities in the post-
processing phrase of our NER system. 
3.1 Named Entity Recognition for Word 
Segmentation 
In this module, if two or more segments in the 
outputs of basic model are recognized as one 
named entity, we combine them as one segment.  
This module uses the same basic NER model 
as what we introduced in the previous section. 
But it only identifies person and location names, 
because organization names often contain more 
than one word. In addition, to keep the high ac-
curacy of person name recognition, the features 
about organization suffixes are not used here.  
3.2 Factoid Identification for Word Seg-
mentation 
Rules are used to identify the following fac-
toids among the segments from the basic word 
segmentation model:  
NUMBER: Integer, decimal, Chinese number 
PERCENT: Percentage and fraction 
DATE: Date 
FOREIGN: English words 
147
Table 3 shows some rules defined here. 
Table 3 Some Rules for Factoid Identification 
Factoid Rule 
NUMBER If previous segment ends with DIGIT and current 
segment starts with DIGIT, then combine them. 
PERCENT If previous segment is composed of DIGIT and 
current segment equals ?%?, then combine them. 
DATE 
If previous segment is composed of DIGIT and 
current segment is in the list of ??, ?, ?, ? 
(Year, Month, Day, Day)?, then combine them. 
FOREIGN Combine the consequent letters as one segment. 
(DIGIT means both Arabic and Chinese numerals) 
3.3 NER Deletion and Creation 
In-word probability has been used in unknown 
word identification successfully (H.Q.Li et al, 
2004). Accordingly, we define in-NE probability 
to help delete and create named entities (NE). 
Formula 3 shows the definition of in-NE prob-
ability for character sequence cici+1?ci+n. Here ?# 
of cici+1?ci+n as NE? is defined as TimeInNE and 
the occurrence of cici+1?ci+n in different type of 
NE is treated differently. 
data in testing ... of #
NE as ... of #)...(
1
1
1
niii
niii
niiiInNE
ccc
ccc
cccP
++
++
++ =
 (3) 
Then, we use some criteria to delete the incor-
rect NE and create new possible NE, in which 
different thresholds are set for different tasks. 
Criterion 1: If PInNE(cici+1?ci+n) of one NE 
type is lower than TDel, and TimeInNE(cici+1?ci+n) 
of the same NE type is also lower than TTime, then 
delete this type of NE composed of cici+1?ci+n.  
Criterion 2: If PInNE(cici+1?ci+n) of one NE 
type is higher than TCre, and in other places the 
character sequence cici+1?ci+n does not belong to 
any NE, then create a new NE containing 
cici+1?ci+n with this NE type.  
4 Evaluation Results and Discussion 
4.1 Evaluation Setting 
SVMlight (T.Joachims, 1999) was used as 
SVM tool. In addition, we used the MSRA train-
ing corpus of NER task in this Bakeoff to train 
our NER post-processing module. 
4.2 Results of Word Segmentation 
We attended the open track of word segmenta-
tion task for two corpuses: UPUC and MSRA. 
Table 4 shows the evaluation results. 
Table 4 Results of Word Segmentation Task (in percentage %) 
Corpus Pre. Rec. F-score Roov Riv 
UPUC 94.4 92.2 93.3 68.0 97.0 
MSRA 94.0 95.3 94.7 50.3 96.9 
The F-score of our word segmentation system 
in UPUC corpus ranked 4th (same as that of the 
3rd group) among all the 8 participants. And it 
was only 1.1% lower than the highest one and 
0.2% lower than the second one. It showed that 
our character-tagging approach was feasible. But 
the F-score of MSRA corpus was only higher 
than one participant in all the 10 groups (the 
highest one was 97.9%). Error analysis shows 
that there are two main reasons.  
First, in MSRA corpus, they tend to segment 
one organization name as one word, such as ?
?????(China Chamber of Commerce in 
USA). But our basic segmentation model seg-
mented such word into several words, e.g. ??/
??/??(USA/China/Chamber of Commerce), 
and our post-processing NER module does not 
consider about organization names.  
Second, our factoid identification rule did not 
combine the consequent DATE factoids into one 
word, but they are combined in MSRA corpus. 
For example, our system segmented the word?
? 9?? (9 o?clock in the evening) into three 
parts ??/9 ?/? (Evening/9 o?clock/Exact). 
This error can be solved by revising the rules for 
factoid identification. 
Besides of that, we also found although our 
large lexicon helped identify the known word 
successfully, it also decreased the recall of OOV 
words (our Riv of UPUC corpus ranked 2nd, with 
only 0.6% decrease than the highest one, but 
Roov ranked 4th, with 8.8% decrease than the 
highest one). The large size of this lexicon is 
looked as the main reason.  
Our lexicon contains 221,407 words, in which 
6,400 words are single-character words. It made 
our system easy to segment one word into sev-
eral words, for example word ??? (Economy 
Group) in UPUC corpus was segmented into?
?  (Economy) and ? (Group). Moreover, the 
large size of this lexicon also brought errors of 
combining two words into one word if the word 
was in the lexicon. For example, words ? (Only) 
and ? (Have) in MSRA corpus were identified 
as one word because there existed the word?? 
(Only) in our lexicon. We will reduce our lexi-
con to a reasonable size to solve these problems. 
4.3 Results of NER 
We also attended the open track of NER task 
for both LDC corpus and MSRA corpus. Table 5 
and Table 6 give the evaluation results.  
There were only 3 participants in the open 
track of LDC corpus and our group got the best 
F-score. In addition, among all the 11 partici-
pants for MSRA corpus, our system ranked 6th 
148
by F-score. It showed the validity of our charac-
ter-tagging method for NER. But for location 
name (LOC) in LDC corpus, both the precision 
and recall of our NER system were very low. It 
was because there were too few location names 
in the training data (there were only 476 LOC in 
the training data, but 5648 PER, 5190 ORG and 
9545 GPE in the same data set). 
Table 5 Results of NER Task for LDC corpus (in percentage %) 
 PER LOC ORG GPE Overall 
Pre. 83.29 58.52 61.48 78.66 76.16 
Rec. 66.93 18.87 45.19 79.94 66.21 
F-score 74.22 28.57 52.09 79.30 70.84 
Table 6 Results of NER Task for MSRA corpus (in percentage %) 
 PER LOC ORG Overall 
Pre. 90.76 85.62 73.90 84.68 
Rec. 76.13 85.41 65.74 78.22 
F-score 82.80 85.52 69.58 81.32 
Besides of that, error analysis shows there are 
four types of main errors in the NER results. 
First, some organization names were very long 
and can be divided into several words, in which 
parts of them can also be looked as named enti-
ties. In such case, our system only recognized the 
small parts as named entities. For example,  ??
???????????  (Fei Zhengqing 
Eastern Asia Research Center of Harvard Univ.) 
was an organization name. But our system rec-
ognized it as????(Harvard Univ.)/ORG+?
? ? (Fei Zheng Qing)/PER+ ? ? (Eastern 
Asia)/LOC+ ????(Research Center)/ORG. 
Adding more context features may be useful to 
resolve this issue. 
In addition, our system was not good at recog-
nizing foreign person names, such as ??? 
(Riordan), and abbreviations, such as ?? (Los 
Angeles), if they seldom or never appeared in 
training corpus. It is because the use of the large 
lexicon decreased the unknown word identifica-
tion ability of our NER system simultaneously. 
Third, the in-NE probability used in post-
processing is helpful to identify named entities 
which cannot be recognized by the basic model. 
But it also recognized some words which can 
only be regarded as named entities in the local 
context incorrectly. For example, our system 
recognized?? (Najing) as GPE in ?????
? (Send to Najing for remedy) in LDC corpus. 
We will consider about adding the in-NE prob-
ability as one feature into the basic model to 
solve this problem. 
At last, in LDC corpus, they combine the at-
tributive of one named entity (especially person 
and organization names) with the named entity 
together. But our system only recognized the 
named entity by itself. For example, our system 
only recognized ??? (Liu Gui Fang) as PER 
in the reference person name ???????? 
(Liu Gui Fang who does not know the inside). 
5 Conclusion and Future Work 
Through the participation of the third 
SIGHAN Bakeoff, we found that tagging charac-
ters with both character-level and word-level fea-
tures was effective for both word segmentation 
and NER. While, this work is only our 
preliminary attempt and there are still many 
works needed to do in the future, such as the 
control of lexicon size, the use of extra 
knowledge (e.g. pos-tag), the feature definition, 
and so on. In addition, our word segmentation 
system only combined the NER module as post-
processing, which resulted in that lots of infor-
mation from NER module cannot be used by the 
basic model. We will consider about combining 
the NER and factoid identification modules into 
the basic word segmentation model by defining 
new tag sets in our future work. 
Acknowledgement 
We would like to thank Dr. Kiyotaka Uchi-
moto for providing the NICT lexicon. 
Reference 
T.Emerson. 2005. The Second International Chinese Word Seg-
mentation Bakeoff. In the 4th SIGHAN Workshop. pp. 123-133. 
H.Jing et al 2003. HowtogetaChineseName(Entity): Segmentation 
and Combination Issues. In EMNLP 2003. pp. 200-207. 
T.Joachims. 1999. Making large-scale SVM learning practical. 
Advances in Kernel Methods - Support Vector Learning. MIT-
Press. 
H.Q.Li et al 2004. The Use of SVM for Chinese New Word Identi-
fication. In IJCNLP 2004. pp. 723-732. 
J.K.Low et al 2005. A Maximum Entropy Approach to Chinese 
Word Segmentation. In the 4th SIGHAN Workshop. pp. 161-164. 
T.Nakagawa. 2004. Chinese and Japanese Word Segmentation 
Using Word-level and Character-level Information. In COLING 
2004. pp. 466-472. 
A.Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-
Speech Tagging. In EMNLP 1996. 
F.Sebastiani. 2002. Machine learning in automated text categoriza-
tion. ACM Computing Surveys. 34(1): 1-47. 
K.Uchimoto et al 2004. Multilingual Aligned Parallel Treebank 
Corpus Reflecting Contextual Information and its Applications. 
In Proceedings of the MLR 2004. pp. 63-70. 
N.Xue et al 2002. Building a Large-Scale Annotated Chinese Cor-
pus. In COLING 2002. 
Y.J.Zhang et al 2005. Building an Annotated Japanese-Chinese 
Parallel Corpus ? A part of NICT Multilingual Corpora. In Pro-
ceedings of the MT SummitX. pp. 71-78. 
149
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Statistical Phrase Alignment Model
Using Dependency Relation Probability
Toshiaki Nakazawa Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan
nakazawa@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
Abstract
When aligning very different language pairs,
the most important needs are the use of struc-
tural information and the capability of gen-
erating one-to-many or many-to-many corre-
spondences. In this paper, we propose a
novel phrase alignment method which models
word or phrase dependency relations in depen-
dency tree structures of source and target lan-
guages. The dependency relation model is a
kind of tree-based reordering model, and can
handle non-local reorderings which sequen-
tial word-based models often cannot handle
properly. The model is also capable of esti-
mating phrase correspondences automatically
without any heuristic rules. Experimental re-
sults of alignment show that our model could
achieve F-measure 1.7 points higher than the
conventional word alignment model with sym-
metrization algorithms.
1 Introduction
We consider that there are two important needs in
aligning parallel sentences written in very differ-
ent languages such as Japanese and English. One
is to adopt structural or dependency analysis into
the alignment process to overcome the difference in
word order. The other is that the method needs to
have the capability of generating phrase correspon-
dences, that is, one-to-many or many-to-many word
correspondences. Most existing alignment methods
simply consider a sentence as a sequence of words
(Brown et al, 1993), and generate phrase correspon-
dences using heuristic rules (Koehn et al, 2003).
Some studies incorporate structural information into
the alignment process after this simple word align-
ment (Quirk et al, 2005; Cowan et al, 2006). How-
ever, this is not sufficient because the basic word
alignment itself is not good.
On the other hand, a few models have been pro-
posed which use structural information from the be-
ginning of the alignment process. Watanabe et al
(2000) and Menezes and Richardson (2001) pro-
posed a structural alignment methods. These meth-
ods use heuristic rules when resolving correspon-
dence ambiguities. Yamada and Knight (2001) and
Gildea (2003) proposed a tree-based probabilistic
alignment methods. These methods reorder, insert
or delete sub-trees on one side to reproduce the other
side, but the constraints of using syntactic informa-
tion is often too rigid. Yamada and Knight flat-
tened the trees by collapsing nodes. Gildea cloned
sub-trees to deal with the problem. Cherry and Lin
(2003) proposed a model which uses a source side
dependency tree structure and constructs a discrim-
inative model. However, there is the defect that its
alignment unit is a word, so it can only find one-
to-one alignments. Nakazawa and Kurohashi (2008)
also proposed a model focusing on the dependency
relations. Their model has the constraint that content
words can only correspond to content words on the
other side, and the same applies for function words.
This sometimes leads to an incorrect alignment. We
have removed this constraint to make more flexi-
ble alignments possible. Moreover, in their model,
some function words are brought together, and thus
they cannot handle the situation where each func-
tion word corresponds to a different part. The small-
est unit of our model is a single word, which should
solve this problem.
10
In this paper, we propose a novel phrase align-
ment method which models word or phrase de-
pendency relations in dependency tree structures of
source and target languages. For a pair of correspon-
dences which has a parent-child relation on one side,
the dependency relation on the other side is defined
as the relation between the two correspondences.
It is a kind of tree-based reordering model, and
can capture non-local reorderings which sequential
word-based models often cannot handle properly.
The model is also capable of estimating phrase cor-
respondences automatically without heuristic rules.
The model is trained in two steps: Step 1 estimates
word translation probabilities, and Step 2 estimates
phrase translation probabilities and dependency re-
lation probabilities. Both Step 1 and Step 2 are per-
formed iteratively by the EM algorithm. During the
Step 2 iterations, word correspondences are grown
into phrase correspondences.
2 Proposed Model
We suppose that Japanese is the source language and
English is the target language in the description of
our model. Note that the model is not specialized
for this language pair, and it can be applied to any
language pair.
Because our model uses dependency tree struc-
tures, both source and target sentences are parsed
beforehand. Japanese sentences are converted into
dependency structures using the morphological ana-
lyzer JUMAN (Kurohashi et al, 1994), and the de-
pendency analyzer KNP (Kawahara and Kurohashi,
2006). MSTparser (McDonald et al, 2005) is used
to convert English sentences. Figure 1 shows an ex-
ample of dependency structures. The root of a tree is
placed at the extreme left and words are placed from
top to bottom.
2.1 Overview
This section outlines our proposed model in compar-
ison to the IBM models, which are the conventional
statistical alignment models.
In the IBM models (Brown et al, 1993), the best
alignment a? between a given source sentence f and
its target sentence e is acquired by the following
equation:
a? = argmax
a
p(f ,a|e)
= argmax
a
p(f |e,a) ? p(a|e) (1)
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
(accept)
(light)
(device)
(photo)
(gate)
(used)
(ni)
(ha)
(wo)
Figure 1: An example of a dependency tree and its align-
ment.
where p(f |e,a) is called lexicon probability and
p(a|e) is called alignment probability.
Suppose f consists of nwords f1, f2, ..., fn, and e
consists ofmwords e1, e2, ..., em and a NULL word
(e0). The alignment mapping a consists of associa-
tions j ? i = aj from source position j to target
position i = aj . The two probabilities above are
broken down as:
p(f |e,a) =
J?
j=1
p(fj |eaj ) (2)
p(a|e) =
I?
i=1
p(?j|ei) (3)
where ?j is a relative position of words in the
source side which corresponds to ei. Equation 2 is
the product of the word translation probabilities, and
Equation 3 is the product of relative position proba-
bilities.
In the proposed model, we refine the IBM models
in three ways. First, as for Equation 2, we consider
phrases instead of words. Second, as for Equation 3,
we consider dependencies of words instead of their
positions in a sentence.
Finally, the proposed model can find the best
alignment a? by not using f -to-e alone, but simulta-
neously with e-to-f . That is, Equation 1 is modified
as follows:
a? = argmax
a
p(f |e,a) ? p(a|e) ?
p(e|f ,a) ? p(a|f) (4)
Since our model regards a phrase as a basic unit,
the above formula is calculated in a straightforward
way. In contrast, the IBM models can consider
a many-to-one alignment by combining one-to-one
11
alignments, but they cannot consider a one-to-many
or many-to-many alignment.
The models are estimated by EM-like algorithm
which is very similar to (Liang et al, 2006). The
important difference is that we are using tree struc-
tures.
We maximize the data likelihood:
max
?ef ,?fe
?
f ,e
(log pef (f , e; ?ef ) + log pfe(f , e; ?fe))
(5)
In the E-step, we compute the posterior distribution
of the alignments with the current parameter ?:
q(a; f , e) := pef (a|f , e; ?ef ) ? pfe(a|f , e; ?fe) (6)
In the M-step, we update the parameter ?:
?? := argmax
?
?
a,f ,e
q(a; f , e) log pef (a, f , e; ?ef )
+?
a,f ,e
q(a; f , e) log pfe(a, f , e; ?fe)
= argmax
?
?
a,f ,e
q(a; f , e) log p(e) ? pef (a, f |e; ?ef )
+?
a,f ,e
q(a; f , e) log p(f) ? pfe(a, e|f ; ?fe)
(7)
Note that p(e) and p(f) have no effect on maxi-
mization, and pef (a, f |e; ?ef ) and pfe(a, e|f ; ?fe)
appeared in Equation 1 or Equation 4.
In the following sections, we decompose the lexi-
con probability and alignment probability.
2.2 Phrase Translation Probability
Suppose f consists of N phrases F1, F2, ..., FN , and
e consists of M phrases E1, E2, ..., EM . The align-
ment mapping a consists of associations j ? i =
Aj from source phrase j to target phrase i = Aj .
We consider phrase translation probability
p(Fj |Ei) instead of word translation probability.
There is one restriction: that phrases composed of
more than one word cannot be aligned to NULL.
Only a single word can be aligned to NULL.
We denote a phrase which the word fj belongs to
as Fs(j), and a phrase which the word ei belongs to
as Et(i). With these notations, we refine Equation 2
as follows:
p(f |e,a) =
J?
j=1
p(Fs(j)|EAs(j)) (8)
Suppose phrase Fj and Ei are aligned where the
number of words in Fj is denoted by |Fj | and that
number in Ei is |Ei|, the probability mass related to
this alignment in Equation 8 is as follows:
p(Fj |Ei)|Fj | ? p(Ei|Fj)|Ei| (9)
We call this probability for the link between Fj and
Ei phrase alignment probability. The upper part of
Table 1 shows phrase alignment probabilities for the
alignment in Figure 1.
2.3 Dependency Relation Probability
The reordering model in the IBM Models is defined
on the relative position between an alignment and
its previous alignment, as shown in Equation 3. Our
model, on the other hand, considers dependencies of
words instead of positional relations.
We start with a dependency relation where fc de-
pends on fp in the source sentence. In a possible
alignment, fc belongs to Fs(c), fp belongs to Fs(p),
and Fs(c) depends on Fs(p). In this situation, we con-
sider the relation between EAs(p) and EAs(c) . Even
if two languages have different word order, their de-
pendency structures are similar in many cases, and
EAs(c) tends to depend on EAs(p) . Our model takes
this tendency into consideration. In order to de-
note the relationship between phrases, we introduce
rel(EAs(p) , EAs(c)). This is defined as the path from
EAs(p) to EAs(c) . It is represented by applying the
notations below:
? ?c? if going down to the child node
? ?p? if going down to the parent node
For example, in Figure 1, the path from ?for? to
?photodetector? is ?c?, from ?the? to ?for? is ?p;p?
because it travels across two nodes. All the phrases
are considered as a single node, so the path from
?photogate? to ?the? is ?p;c;c;c? with the alignment
in Figure 1.
We refine Equation 3 using rel as follows:
p(a|e) = ?
(p,c)?Ds-pc
pt(rel(EAs(p) , EAs(c))|pc)
(10)
where Ds-pc denotes a set of parent-child
word pairs in the source sentence. We call
pt(rel(EAs(p) , EAs(c))|pc) target side dependency
relation probability. pt is a kind of tree-based
reordering model.
12
Table 1: A probability calculation example.
Source Target Phrase alignment probability
???? photodetector p(???? |photodetector)3 ? p(photodetector|????)
?? for p(?? |for)2 ? p(for|??)
?????? photogate p(?????? |a photogate)2 ? p(a photogate|??????)2
???? is used p(???? |is used)2 ? p(is used|????)2
NULL the p(the|NULL)
Source Target dependency Target Source dependency
c p relation probability c p relation probability
? ? pt(SAME|pc) A photogate ps(SAME|pc)
? ?? pt(SAME|pc) photogate is ps(c|pc)
?? ? pt(c|pc) used is ps(SAME|pc)
? ? pt(SAME|pc) for used ps(c|pc)
? ??? pt(c|pc) the photodetector ps(NULL c|pc)
??? ??? pt(SAME|pc) photodetector for ps(c|pc)
??? ? pt(c|pc)
? ??? pt(SAME|pc)
There are some special cases for rel. When Fs(c)
and Fs(p) are the same, that is, fc and fp belong
to the same phrase, rel is represented as ?SAME?.
When fp is aligned to NULL, fc is aligned to NULL,
and both of them are aligned to NULL, rel is repre-
sented as ?NULL p?, ?NULL c?, and ?NULL b?, re-
spectively. The lower part of Table 1 shows depen-
dency relation probabilities corresponding to Figure
1.
Actually, we extend the dependency relation
probability to consider a wider relation, i.e, the
grandparent-child relation, as follows:
p(a|e) = ?
(p,c)?Ds-pc
pt(rel(EAs(p) , EAs(c))|pc) ?
?
(g,c)?Ds-gc
pt(rel(EAs(g) , EAs(c))|gc)
(11)
where Ds-gc denotes a set of grandparent-child word
pairs in the source sentence.
3 Model Training
Our model is trained in two steps. In Step 1, word
translation probability is estimated. Then, in Step 2,
possible phrases are acquired, and both phrase trans-
lation probability and dependency relation probabil-
ity are estimated. In both steps, parameter estima-
tion is done with the EM algorithm.
3.1 Step 1
In Step 1, word translation probability in each di-
rection is estimated independently. This is done in
exactly the same way as in IBM Model 1.
In this process, the alignment unit is a word.
When we consider f -to-e alignment, each word on
the source side fj can correspond to a word on the
target side ei or a NULL word, independently of
other source words. The probability of one possible
alignment a is calculated as follows:
p(a, f |e) =
J?
j=1
p(fj |eaj ) (12)
By considering all possible alignments, p(f |e) is
calculated as:
p(f |e) =?
a
p(a, f |e) (13)
As initial parameters of p(f |e), we use uniform
probabilities. Then, after calculating Equation 12
and 13, we give the fractional count p(a,f |e)p(f |e) to all
word alignments in a, and we estimate p(f |e) by
MLE. We perform this estimation iteratively.
The inverse model e-to-f can be calculated in the
same manner.
3.2 Step 2
Both phrase translation probability and dependency
relation probability are estimated, and one undi-
rected alignment is found using the e-to-f and f -to-e
probabilities simultaneously in this step. In contrast
to Step 1, it is impossible to enumerate all the possi-
ble alignments. To find the best alignment, we first
create an initial alignment based on phrase trans-
lation probability only, and then gradually revise it
13
by considering the dependency relation probability
with a hill-climbing algorithm.
The initial parameters of Step 2 are calculated
as follows. The dependency relation probability is
calculated using the final alignment result of Step
1, and we use the word translation probability esti-
mated in Step 1 as the initial phrase translation prob-
ability.
3.2.1 Initial Alignment
We first create an initial alignment based on the
phrase translation probability without considering
the dependency relation probabilities.
For all the combinations of possible phrases
(including NULL), phrase alignment probabilities
are calculated (equation 9). Correspondences are
adopted one by one in descending order of geomet-
ric mean of the phrase alignment probabilities. All
the words should be aligned only once, that is, the
correspondences are adopted exclusively. Genera-
tion of possible phrases is explained in Section 3.2.3.
3.2.2 Hill-climbing
To find better alignments, the initial alignment is
gradually revised with a hill-climbing algorithm. We
use four kinds of revising operations:
Swap: Focusing on any two correspondences, the
partners are swapped. In the first step in
Figure2, the correspondences ?? ? photo-
gate? and ???????? photodetector? are
swapped to ??? photodetector? and ????
??? ? photogate?.
Extend: Focusing on one correspondence, the
source or target phrase is extended to include
its neighboring (parent or child) NULL-aligned
word.
Add: A new correspondence is added between a
source word and a target word both of which
are aligned to NULL.
Reject: A correspondence is rejected and the source
and target phrase are aligned to NULL.
Figure 2 shows an illustrative example of hill
climbing. The alignment is revised only if the align-
ment probability gets increased. It is repeated un-
til no operation can improve the alignment probabil-
ity, and the final state is the best approximate align-
ment. As a by-product of hill-climbing, pseudo n-
best alignment can be acquired. It is used in collect-
ing fractional counts.
3.2.3 Phrase Generation
If there is a word which is aligned to NULL in the
best approximate alignment, a new possible phrase
is generated by merging the word into a neighbor-
ing phrase which is not aligned to NULL. In the last
alignment result in Figure 2, for example, ????
is treated as being included in the correspondence
between ?? ?? and ?photodetector? and the cor-
respondence between ??? and ?for?. As a result,
we consider the correspondence between ?? ? ?
?? and ?photodetector? and the correspondence be-
tween ????? and ?for? existing in parallel sen-
tences. The new possible phrase is taken into con-
sideration from the next iteration.
3.2.4 Model Estimation
Collecting all the alignment results, we estimate
phrase alignment probabilities and dependency rela-
tion probabilities.
One way of estimating parameters of phrase
alignment probabilities is using the following equa-
tions:
p(Fj |Ei) = C(Fj , Ei)?
k C(Fk, Ei)
p(Ei|Fj) = C(Fj , Ei)?
k C(Ek, Fj)
(14)
where C(Fj , Ei) is a frequency of Fj and Ei is
aligned.
However, if we use this in our model, the phrase
translation probability of the new possible phrase
can become extremely high (often it becomes 1).
To avoid this problem, we use the equations below
for the estimation of phrase translation probability
in place of Equation 14:
p(Fj |Ei) = C(Fj , Ei)C(Ei) , p(Ei|Fj) =
C(Fj , Ei)
C(Fj) (15)
C(Ei) is the frequency of the phrase Ei in the train-
ing corpus which can be pre-counted. This definition
can resolve the problem where the phrase translation
probability of the new possible phrase becomes too
high.
As for the NULL, we use Equation 14 because we
cannot pre-count the frequency of NULL.
Using the estimated phrase alignment probabil-
ities and dependency relation probabilities, we go
back to the initial alignment described in Section
3.2.1 iteratively.
14
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
Ini al alignment
Swap Reject
Add Extend
(accept)
(light)
(device)
(ni)
(ha)
(photo)
(gate)
(used)
(wo)
Figure 2: An example of hill-climbing.
4 Experimental Results
We conducted alignment experiments. A JST1
Japanese-English paper abstract corpus consisting
of 1M parallel sentences was used for the model
training. This corpus was constructed from a 2M
Japanese-English paper abstract corpus by NICT2
using the method of Uchiyama and Isahara (2007).
As gold-standard data, we used 475 sentence pairs
which were annotated by hand. The annotations
were only sure (S) alignments (there were no possi-
ble (P ) alignments) (Och and Ney, 2003). The unit
of evaluation was word-base for both Japanese and
English. We used precision, recall, and F-measure
as evaluation criteria.
We conducted two experiments to reveal 1) the
contribution of our proposed model compared to the
existing models, and 2) the effectiveness of using
dependency tree structure and phrases, which are
larger alignment units than words. Trainings were
run on the original forms of words for both the pro-
posed model and the models used for comparison.
4.1 Comparison with Word Sequential Model
For comparison, we used GIZA++ (Och and Ney,
2003) which implements the prominent sequential
word-base statistical alignment model of IBM Mod-
els. We conducted word alignment bidirectionally
with its default parameters and merged them using
three types of symmetrization heuristics (Koehn et
al., 2003). The results are shown in Table 2.
1http://www.jst.go.jp/
2http://www.nict.go.jp/
The result of ?Step 1? uses parameters estimated
after 5 iterations of Step 1. The alignment is ob-
tained by the method of initial alignment shown in
Section 3.2.1. In ?Step 2-1?, the phrase translation
probabilities are the same as those in ?Step 1?. In ad-
dition, dependency relation probabilities estimated
from the ?Step 1? alignment result are used. By com-
paring ?Step 1? and ?Step 2-1?, we can see the ef-
fectiveness of dependency relation probability. We
performed 5 iterations for Step 2 and calculated the
alignment accuracy each time. As a result, the pro-
posed model could achieve a higher F-measure by
1.7 points compared to the sequential model. ?In-
tersection? achieved best Precision, but its Recall is
quite low. ?grow-diag-final-and? achieved best Re-
call, but its Precision is lower than our best result
where the Recall is almost same. Thus, we can say
our result is better than sequential word alignment
models.
4.2 Effectiveness of Dependency Trees and
Phrases
To confirm the effectiveness of dependency trees and
phrases, we conducted alignment experiments on the
following four conditions:
? Using both dependency trees and phrases (re-
ferred to as ?proposed?).
? Using dependency trees only.
? Using phrases only.
? Not using dependency trees or phrases (referred
to as ?none?)
For the conditions which do not use dependency
trees, we used positional relations of a sentence as
15
Table 2: Results of alignment experiment.
Precision Recall F
Step 1 77.55 33.92 47.20
Step 2-1 83.46 40.03 54.11
Step 2-2 87.74 45.37 59.81
Step 2-3 87.62 48.92 62.79
Step 2-4 86.87 50.42 63.81
Step 2-5 85.90 50.75 63.80
Step 2-6 85.54 51.00 63.90
Step 2-7 85.18 50.87 63.70
Step 2-8 84.66 50.75 63.46
intersection 90.34 34.28 49.71
grow-final-and 81.32 48.85 61.04
grow-diag-final-and 79.39 51.15 62.22
Table 3: Effectiveness of dependency trees and phrases
(results after 5 iterations in Step 2.)
Precision Recall F
proposed 85.54 51.00 63.90
dependency tree only 89.77 39.47 54.83
phrase only 84.41 47.33 60.65
none 85.07 38.06 52.59
a sequence of words instead of dependency tree re-
lations. The results are shown in Table 3. All the
results are the alignment accuracy after 5 iterations
of Step 2.
5 Discussion
Table 2 shows that our proposed model could
achieve reasonably high accuracy of alignment, and
is better than sequential word-base models. As
an example, alignment results of a word sequen-
tial model are shown in Figure 3. The gray col-
ored cells are the gold-standard alignments, and the
black boxes are the outputs of the sequential model.
The model failed to resolve the correspondence am-
biguities between ?? (not) ?? (castrated) ??
? (mice)?, and ??? ????; and ?non-castrated
mice?, and ?castrated mice? respectively. This is
because these words are placed close to each other
and are also close to the correspondence ????
? as? which can be a clue to the word order. Us-
ing the tree structure in Figure 4, these words were
correctly aligned. This is because in the English
tree, the phrase ?castrated mice? does not depend
on ?as?, and ?non-castrated mice? does. Similarly
in the Japanese tree, ???????? depends on ?
???? and ??????? does not.
As mentioned in Section 1, sequential statistical
???exhibited ? ?astrong ?inhibitory ?effect ?on ? ?tumor ?growth ?in ?the ?castrated ?mice ?as ? ?in ?thenon-castrated ?mice ????? ?? ???
? ??
?
?? ???
? ???
? ?? ? ???? ?? ?? ? ???
Figure 3: An alignment example of the word sequential
model (grow-diag-final-and).
???exhibited ? ?? ??a? ??strong ?? ??inhibitory ???effect ???on? ? ??tumor ?? ??growth ???in ?? ? ??the? ? ??castrated ?? ??mice ???as ???in ?? ??the? ??non-castrated ? ???mice ?????
??
?
?
??
??
?
??
???
?
??
?
??
??
?
?
??
??
?
??
???
?
??
?
?
??
???
?
??
?
?
??
??
?
??
?
??
??
?
??
??
?
??
??
?
??
??
??
?
???
Figure 4: An alignment example of the proposed model.
methods, which regard a sentence as a sequence of
words, work well for language pairs that are not too
different in their language structure. Japanese and
English have significantly different structures. One
of the issues is that Japanese sentences have a SOV
word order, but in English, the word order is SVO, so
the dependency relations are often turned over. For
language pairs such as Japanese and English, deeper
sentence analysis using NLP resources is necessary
and useful. Our method is therefore suitable for such
language pairs.
As another example of an alignment failure by
the sequential model, Figure 5 shows the phrase cor-
respondence ?? ? ?? ? photodetector?, which
was correctly found as shown in Figure 6. The pro-
16
Aphotogate ? ?is ?used ?for ?the ?photodetector ?
?? ?? ? ? ???
???
? ???
Figure 5: An unsuccessful example of phrase detection in
the sequential model (grow-diag-final-and).
??A ????photogate ? ?
is ? ???used ? ???for ? ?? ??the??photodetector ? ? ?
??
?
??
?
??
??
??
?
??
?
?
??
???
?
??
???
??
?
???
Figure 6: An example of phrase detection in the proposed
model.
posed method of generating possible phrases during
iterations works well and improves alignment.
From the result of our second experiment, we can
see the following points:
1. Phrasal alignment improves the recall, but low-
ers the precision.
2. By using dependency trees, precision can be
improved.
3. We can find a balance point by using both
phrasal alignment and dependency trees.
The causes of alignment errors in our model can
be summarized into categories. The biggest one is
parsing errors. Since our model is highly dependent
on the parsing result, the alignments would easily
turn out wrong if the parsing result was incorrect.
Sometimes the hill-climbing algorithm could not
revise the initial alignment. Most of these cases
would happen when one word occurred several
times on one side, but some of those occurrences
were omitted on the other side. Let?s suppose there
are two identical words on the source side, but the
target side has only one corresponding word. Initial
alignment is created without considering the depen-
dencies at all, so it cannot judge which source word
should be aligned to the corresponding target word.
In this case, the best alignment searching sometimes
gets the local solution. This problem could be re-
solved by considering local dependencies for am-
biguous words.
One difficulty is how to handle function words.
Function words often do not have exactly corre-
sponding words in the opposite language. Japanese
case markers such as ?? (ha)?, ?? (ga)? (subjec-
tive case), ?? (wo)? (objective case) and so on, and
English articles are typical examples of words, that
do not have corresponding parts. There is a differ-
ence between alignment criteria for function words
of gold-standard and our outputs, and it is somewhat
difficult to improve alignment accuracy.
6 Conclusion
In this paper, we have proposed a linguistically-
motivated probabilistic phrase alignment model
based on dependency tree structures. The model in-
corporates the tree-based reordering model. Experi-
mental results show that the word sequential model
does not work well for linguistically different lan-
guage pairs, and this can be resolved by using syn-
tactic information. We have conducted the experi-
ments only on Japanese-English corpora. To firmly
support our claim that syntactic information is im-
portant, it is necessary to do more investigation on
other language pairs.
Most frequent alignment errors are derived from
parsing errors. Because our method depends heavily
on structural information, parsing errors easily make
the alignment accuracy worse. Although the parsing
accuracy is high in general for both Japanese and
English, it sometimes outputs incorrect dependency
structures because technical or unknown words of-
ten appears in scientific papers. This problem could
be resolved by introducing parsing probabilities into
our model using parsing tools which can output n-
best parsing with their parsing probabilities. This
will not only improve the alignment accuracy, it will
allow revision of the parsing result. Moreover, we
need to investigate the contribution of our alignment
result to the translation quality.
17
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Association for Computational Linguistics,
19(2):263?312.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
the 41st Annual Meeting of the Association of Compu-
tational Linguistics, pages 88?95.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the 2006 Conference on
EMNLP, pages 232?241, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting on ACL, pages 80?87.
Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-
lexicalized probabilistic model for japanese syntactic
and case structure analysis. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 176?183, New York City,
USA, June. Association for Computational Linguis-
tics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL
2003: Main Proceedings, pages 127?133.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Proceedings of
The International Workshop on Sharable Natural Lan-
guage, pages 22?28.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL) Workshop on Data-
Driven Machine Translation, pages 39?46.
Toshiaki Nakazawa and Sadao Kurohashi. 2008.
Linguistically-motivated tree-based probabilistic
phrase alignment. In In Proceedings of the Eighth
Conference of the Association for Machine Translation
in the Americas (AMTA2008).
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Association for Computational Linguistics, 29(1):19?
51.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 271?279.
Masao Utiyama and Hitoshi Isahara. 2007. A japanese-
english patent parallel corpus. In MT summit XI, pages
475?482.
Hideo Watanabe, Sadao Kurohashi, and Eiji Aramaki.
2000. Finding structural correspondences from bilin-
gual parsed corpus for corpus-based translation. In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 906?912.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the ACL, pages 523?530.
18
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 55?62,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Bottom-up Named Entity Recognition
using a Two-stage Machine Learning Method
Hirotaka Funayama Tomohide Shibata Sadao Kurohashi
Kyoto University, Yoshida-honmachi,
Sakyo-ku, Kyoto, 606-8501, Japan
{funayama, shibata, kuro}@nlp.kuee.kyoto-u.ac.jp
Abstract
This paper proposes Japanese bottom-up
named entity recognition using a two-
stage machine learning method. Most
work has formalized Named Entity Recog-
nition as a sequential labeling problem, in
which only local information is utilized
for the label estimation, and thus a long
named entity consisting of several mor-
phemes tends to be wrongly recognized.
Our proposed method regards a compound
noun (chunk) as a labeling unit, and first
estimates the labels of all the chunks in
a phrasal unit (bunsetsu) using a machine
learning method. Then, the best label as-
signment in the bunsetsu is determined
from bottom up as the CKY parsing al-
gorithm using a machine learning method.
We conducted an experimental on CRL
NE data, and achieved an F measure of
89.79, which is higher than previous work.
1 Introduction
Named Entity Recognition (NER) is a task of rec-
ognizing named entities such as person names,
organization names, and location. It is used for
several NLP applications such as Information Ex-
traction (IE) and Question Answering (QA). Most
work uses machine learning methods such as Sup-
port Vector Machines (SVMs) (Vapnik, 1995) and
Conditional Random Field (CRF) (Lafferty et al,
2001) using a hand-annotated corpus (Krishnan
and D.Manning, 2006; Kazama and Torisawa,
2008; Sasano and Kurohashi, 2008; Fukushima et
al., 2008; Nakano and Hirai, 2004; Masayuki and
Matsumoto, 2003).
In general, NER is formalized as a sequential
labeling problem. For example, regarding a mor-
pheme as a basic unit, it is first labeled as S-
PERSON, B-PERSON, I-PERSON, E-PERSON,
S-ORGANIZATION, etc. Then, considering the
labeling results of morphemes, the best NE label
sequence is recognized.
When the label of each morpheme is estimated,
only local information around the morpheme (e.g.,
the morpheme, the two preceding morphemes, and
the two following morphemes) is utilized. There-
fore, a long named entity consisting of several
morphemes tends to be wrongly recognized. Let
us consider the example sentences shown in Fig-
ure 1.
In sentence (1), the label of ?Kazama? can be
recognized to be S-PERSON (PERSON consist-
ing of one morpheme) by utilizing the surrounding
information such as the suffix ?san? (Mr.) and the
verb ?kikoku shita? (return home).
On the other hand, in sentence (2), when the
label of ?shinyou? (credit) is recognized to be
B-ORGANIZATION (the beginning of ORGA-
NIZATION), only information from ?hatsudou?
(invoke) to ?kyusai? (relief) can be utilized, and
thus the information of the morpheme ?ginkou?
(bank) that is apart from ?shinyou? by three mor-
phemes cannot be utilized. To cope with this prob-
lem, Nakano et al (Nakano and Hirai, 2004) and
Sasano et al (Sasano and Kurohashi, 2008) uti-
lized information of the head of bunsetsu1. In their
methods, when the label of ?shinyou? is recog-
nized, the information of the morpheme ?ginkou?
can be utilized.
However, these methods do not work when the
morpheme that we want to refer to is not a head
of bunsetsu as in sentence (3). In this example,
when ?gaikoku? (foreign) is recognized to be B-
ARTIFACT (the beginning of ARTIFACT), we
want to refer to ?hou? (law), not ?ihan? (viola-
tion), which is the head of the bunsetsu.
This paper proposes Japanese bottom-up named
1Bunsetsu is the smallest coherent phrasal unit in
Japanese. It consists of one or more content words followed
by zero or more function words.
55
(1) kikoku-shita
return home
Kazama-san-wa
Mr.Kazama TOP
. . .
?Mr. Kazama who returned home?
(2) hatsudou-shita
invoke
shinyou-kumiai-kyusai-ginkou-no
credit union relief bank GEN
setsuritsu-mo. . .
establishment
?the establishment of the invoking credit union relief bank?
(3) shibunsyo-gizou-to
private document falsification and
gaikoku-jin-touroku-hou-ihan-no
foreigner registration law violation GEN
utagai-de
suspicion INS
?on suspicion of the private document falsification and the violation of the foreigner registra-
tion law?
Figure 1: Example sentences.
entity recognition using a two-stage machine
learning method. Different from previous work,
this method regards a compound noun as a la-
beling unit (we call it chunk, hereafter), and es-
timates the labels of all the chunks in the bun-
setsu using a machine learning method. In sen-
tence (3), all the chunks in the second bunsetsu
(i.e., ?gaikoku?, ?gaikoku-jin?, ? ? ?, ?gaikoku-jin-
touroku-hou-ihan ?, ? ? ?, ?ihan?) are labeled, and
in the case that the chunk ?gaikoku-jin-touroku-
hou? is labeled, the information about ?hou? (law)
is utilized in a natural manner. Then, in the bun-
setsu, the best label assignment is determined. For
example, among the combination of ?gaikoku-jin-
touroku-hou? (ARTIFACT) and ?ihan? (OTHER),
the combination of ?gaikoku-jin? (PERSON) and
?touroku-hou-ihan? (OTHER), etc., the best la-
bel assignment, ?gaikoku-jin-touroku-hou? (AR-
TIFACT) and ?ihan? (OTHER), is chosen based
on a machine learning method. In this determi-
nation of the best label assignment, as the CKY
parsing algorithm, the label assignment is deter-
mined by bottom-up dynamic programming. We
conducted an experimental on CRL NE data, and
achieved an F measure of 89.79, which is higher
than previous work.
This paper is organized as follows. Section 2 re-
views related work of NER, especially focusing on
sequential labeling based method. Section 3 de-
scribes an overview of our proposed method. Sec-
tion 4 presents two machine learning models, and
Section 5 describes an analysis algorithm. Section
6 gives an experimental result.
2 Related Work
In Japanese Named Entity Recognition, the defi-
nition of Named Entity in IREX Workshop (IREX
class example
PERSON Kimura Syonosuke
LOCATION Taiheiyou (Pacific Ocean)
ORGANIZATION Jimin-tou (Liberal Democratic
Party)
ARTIFACT PL-houan (PL bill)
DATE 21-seiki (21 century)
TIME gozen-7-ji (7 a.m.)
MONEY 500-oku-en (50 billions yen)
PERCENT 20 percent
Table 1: NE classes and their examples.
Committee, 1999) is usually used. In this def-
inition, NEs are classified into eight classes:
PERSON, LOCATION, ORGANIZATION, AR-
TIFACT, DATE, TIME, MONEY, and PERCENT.
Table 1 shows example instances of each class.
NER methods are divided into two approaches:
rule-based approach and machine learning ap-
proach. According to previous work, machine
learning approach achieved better performance
than rule-based approach.
In general, a machine learning method is for-
malized as a sequential labeling problem. This
problem is first assigning each token (character or
morpheme) to several labels. In an SE-algorithm
(Sekine et al, 1998), S is assigned to NE com-
posed of one morpheme, B, I, E is assigned to the
beginning, middle, end of NE, respectively, and O
is assigned to the morpheme that is not an NE2.
The labels S, B, I, and E are prepared for each NE
classes, and thus the total number of labels is 33
(= 8 * 4 + 1).
The model for the label estimation is learned
based on machine learning. The following fea-
tures are generally utilized: characters, type of
2Besides, there are IOB1, IOB2 algorithm using only
I,O,B and IOE1, IOE2 algorithm using only I,O,E (Kim and
Veenstra, 1999).
56
HabuPERSON 0.111
Habu-YoshiharuPERSON 0.438
Habu-Yoshiharu-MeijinORGANIZATION0.083
Yoshiharu Yoshiharu-MeijinMONEY0.075 OTHERe0.092
MeijinOTHERe0.245
(a):initial state
HabuPERSON 0.111
Habu-YoshiharuPERSON 0.438
Habu-Yoshiharu + MeijinPSN+OTHERe0.438+0.245
Yoshiharu Yoshiharu + Meijin
final outputanalysis direction
MONEY0.075 MNY+OTHERe0.075+0.245
MeijinOTHERe0.245
(b):final output
Figure 2: An overview of our proposed method. (the bunsetsu ?Habu-Yoshiharu-Meijin?)
character, POS, etc. about the morpheme and the
surrounding two morphemes. The methods utiliz-
ing SVM or CRF are proposed.
Most of NER methods based on sequential la-
beling use only local information. Therefore,
methods utilizing global information are pro-
posed. Nakano et al utilized as a feature the word
sub class of NE on the analyzing direction in the
bunsetsu, the noun in the end of the bunsetsu ad-
jacent to the analyzing direction, and the head of
each bunsetsu (Nakano and Hirai, 2004). Sasano
et al utilized cache feature, coreference result,
syntactic feature, and caseframe feature as struc-
tural features (Sasano and Kurohashi, 2008).
Some work acquired knowledge from unan-
notated large corpus, and applied it to NER.
Kazama et al utilized a Named Entity dic-
tionary constructed from Wikipedia and a noun
clustering result obtained using huge amount of
pairs of dependency relations (Kazama and Tori-
sawa, 2008). Fukushima et al acquired huge
amount of category-instance pairs (e.g., ?po-
litical party - New party DAICHI?,?company-
TOYOTA?) by some patterns from a large Web
corpus (Fukushima et al, 2008).
In Japanese NER researches, CRL NE data are
usually utilized for the evaluation. This data in-
cludes approximately 10 thousands sentences in
news paper articles, in which approximately 20
thousands NEs are annotated. Previous work
achieved an F measure of about 0.89 using this
data.
3 Overview of Proposed Method
Our proposed method first estimates the label of
all the compound nouns (chunk) in a bunsetsu.
Then, the best label assignment is determined
by bottom-up dynamic programming as the CKY
parsing algorithm. Figure 2 illustrates an overview
of our proposed method. In this example, the
bunsetsu ?Habu-Yoshiharu-Meijin? (Grand Mas-
ter Yoshiharu Habu) is analyzed. First, the labels
of all the chunks (?Habu?, ?Habu-Yoshiharu?,
?Habu-Yoshiharu-Meijin?, ? ? ?, ?Meijin?, etc.) in
the bunsetsu are analyzed using a machine learn-
ing method as shown in Figure 2 (a).
We call the state in Figure 2 (a) initial state,
where the labels of all the chunks have been es-
timated. From this state, the best label assign-
ment in the bunsetsu is determined. This pro-
cedure is performed from the lower left (corre-
sponds to each morpheme) to the upper right like
the CKY parsing algorithm as shown in Figure 2
(b). For example, when the label assignment for
?Habu-Yoshiharu? is determined, the label assign-
ment ?Habu-Yoshiharu? (PERSON) and the label
assignment ?Habu? (PERSON) and ?Yoshiharu?
(OTHER) are compared, and the better one is cho-
sen. While grammatical rules are utilized in a
general CKY algorithm, this method chooses bet-
ter label assignment for each cell using a machine
learning method.
The learned models are the followings:
? the model that estimates the label of a chunk
(label estimation model)
? the model that compares two label assign-
ments (label comparison model)
The two models are described in detail in the
next section.
57
Habu Yoshiharu Meijin ga
PERSON OTHERe
invalid invalid
invalid
invalid
Figure 3: Label assignment for all the chunks in
the bunsetsu ?Habu-Yoshiharu-Meijin.?
4 Model Learning
4.1 Label Estimation Model
This model estimates the label for each chunk. An
analysis unit is basically bunsetsu. This is because
93.5% of named entities is located in a bunsetsu
in CRL NE data. Exceptionally, the following ex-
pressions located in multiple bunsetsus tend to be
an NE:
? expressions enclosed in parentheses (e.g., ?
?Himeyuri-no tou? ? (The tower of Himeyuri)
(ARTIFACT))
? expressions that have an entry in Wikipedia
(e.g., ?Nihon-yatyou-no kai? (Wild Bird So-
ciety of Japan) (ORGANIZATION))
Hereafter, bunsetsu is expanded when one of the
above conditions meet. By this expansion, 98.6%
of named entities is located in a bunsetsu3.
For each bunsetsu, the head or tail function
words are deleted. For example, in the bun-
setsu ?Habu-Yoshiharu-Meijin-wa?, the tail func-
tion word ?wa? (TOP) is deleted. In the bunsetsu
?yaku-san-bai? (about three times), the head func-
tion word ?yaku? (about) is deleted.
Next, for learning the label estimation model,
all the chunks in a bunsetsu are attached to the cor-
rect label from a hand-annotated corpus. The la-
bel set is 13 classes, which includes eight NE class
(as shown in Table 1), and five classes: OTHERs,
OTHERb, OTHERi, OTHERe, and invalid.
The chunk that corresponds to a whole bun-
setsu and does not contain any NEs is labeled
as OTHERs, and the head, middle, tail chunk
that does not correspond to an NE is labeled as
OTHERb, OTHERi, OTHERe, respectively4.
3As an example in which an NE is not included by an
expanded bunsetsu, there are ?Toru-no Kimi? (PERSON)
and ?Osaka-fu midori-no kankyo-seibi-shitsu? (ORGANI-
ZATION).
4Each OTHER is assigned to the longest chunk that satis-
fies its condition in a chunk.
1. # of morphemes in the chunk
2. the position of the chunk in its bunsetsu
3. character type5
4. the combination of the character type of adjoining
morphemes
- For the chunk ?Russian Army?, this feature is
?Katakana,Kanji?
5. word class, word sub class, and several features pro-
vided by a morphological analyzer JUMAN
6. several features6 provided by a parser KNP
7. string of the morpheme in the chunk
8. IPADIC7 feature
- If the string of the chunk are registered in the fol-
lowing categories of IPADIC: ?person?, ?lo-
cation?, ?organization?, and ?general?, this
feature fires.
9. Wikipedia feature
- If the string of the chunk has an entry in
Wikipedia, this feature fires.
- the hypernym extracted from its definition sen-
tence using some patterns (e.g., The hyper-
nym of ?the Liberal Democratic Party? is a
political party.)
10. cache feature
- When the same string of the chunk appears in the
preceding context, the label of the preceding
chunk is used for the feature.
11. particles that the bunsetsu includes
12. the morphemes, particles, and head morpheme in the
parent bunsetsu
13. the NE/category ratio in a case slot of predicate/noun
case frame(Sasano and Kurohashi, 2008)
- For example, in the case ga (NOM) of the pred-
icate case frame ?kaiken? (interview), the NE
ratio ?PERSON:0.245? is assigned to the case
slot. Hence, in the sentence ?Habu-ga kaiken-
shita? (Mr. Habu interviewed), the feature
?PERSON:0.245? is utilized for the chunk
?Habu.?
14. parenthesis feature
- When the chunk in a parenthesis, this feature
fires.
Table 2: Features for the label estimation model.
The chunk that is neither any eight NE class nor
the above four OTHER is labeled as invalid.
In an example as shown in Figure 3, ?Habu-
Yoshiharu? is labeled as PERSON, ?Meijin? is la-
beled as OTHERe, and the other chunks are la-
beled as invalid.
Next, the label estimation model is learned from
the data in which the above label set is assigned
5The following five character types are considered: Kanji,
Hiragana, Katakana, Number, and Alphabet.
6When a morpheme has an ambiguity, all the correspond-
ing features fire.
7http://chasen.aist-nara.ac.jp/chasen/distribution.html.ja
58
to all the chunks. The features for the label esti-
mation model are shown in Table 2. Among the
features, as for feature (3), (5)?(8), three cate-
gories according to the position of a morpheme
in the chunk are prepared: ?head?, ?tail?, and
?anywhere.? For example, in the chunk ?Habu-
Yoshiharu-Meijin,? as for the morpheme ?Habu?,
feature (7) is set to be ?Habu? in ?head? and as for
the morpheme ?Yoshiharu?, feature (7) is set to be
?Yoshiharu? in ?anywhere.?
The label estimation model is learned from pairs
of label and feature in each chunk. To classify the
multi classes, the one-vs-rest method is adopted
(consequently, 13 models are learned). The SVM
output is transformed by using the sigmoid func-
tion 11+exp(??x) , and the transformed value is nor-
malized so that the sum of the value of 13 labels
in a chunk is one.
The purpose for setting up the label ?invalid? is
as follows. In the chunk ?Habu? and ?Yoshiharu?
in Figure 3, since the label ?invalid? has a rela-
tively higher score, the score of the label PERSON
is relatively low. Therefore, when the label com-
parison described in Section 4.2 is performed, the
label assignment ?Habu-Yoshiharu? (PERSON) is
likely to be chosen. In the chunk where the score
of the label invalid has the highest score, the label
that has the second highest score is adopted.
4.2 Label Comparison Model
This model compares the two label assignments
for a certain string. For example, in the string
?Habu-Yoshiharu?, the model compares the fol-
lowing two label assignments:
? ?Habu-Yoshiharu? is labeled as PERSON
? ?Habu? is labeled as PERSON and ?Yoshi-
haru? is labeled as MONEY
First, as shown in Figure 4, the two compared
sets of chunks are lined up by sandwiching ?vs.?
(The left one, right one is called the first set, the
second set, respectively.) When the first set is cor-
rect, this example is positive: otherwise, this ex-
ample is negative. The max number of chunks for
each set is five, and thus examples in which the
first or second set has more than five chunks are
not utilized for the model learning.
Then, the feature is assigned to each example.
The feature (13 dimensions) for each chunk is de-
fined as follows: the first 12 dimensions are used
positive:
+1 Habu-Yoshiharu vs Habu + Yoshiharu
PSN PSN + MNY
+1 Habu-Yoshiharu + Meijin vs Habu + Yoshiharu + Meijin
PSN + OTHERe PSN + MONEY + OTHERe
...
negative:
- 1 Habu-Yoshiharu-Meijin vs Habu-Yoshiharu + Meijin
ORG PSN + OTHERe
...
Figure 4: Assignment of positive/negative exam-
ples.
for each label, which is estimated by the label esti-
mation model, and the last 13th dimension is used
for the score of an SVM output. Then, for the first
and second set, the features for each chunk are ar-
ranged from the left, and zero vectors are placed
in the remainder part.
Figure 5 illustrates the feature for ?Habu-
Yoshiharu? vs ?Habu + Yoshiharu.? The label
comparison model is learned from such data us-
ing SVM. Note that only the fact that ?Habu-
Yoshiharu? is PERSON can be found from the
hand-annotated corpus, and thus in the example
?Habu-Yoshiharu-Meijin? vs ?Habu + Yoshiharu-
Meijin?, we cannot determine which one is cor-
rect. Therefore, such example cannot be used for
the model learning.
5 Analysis
First, the label of all the chunks in a bunsetsu is
estimated by using the label estimation model de-
scribed in Section 4.1. Then, the best label assign-
ment in the bunsetsu is determined by applying the
label comparison model described in Section 4.2
iteratively as shown in Figure 2 (b). In this step,
the better label assignment is determined from bot-
tom up as the CKY parsing algorithm.
For example, the initial state shown in Figure
2(a) is obtained using the label estimation model.
Then, the label assignment is determined using the
label comparison model from the lower left (cor-
responds to each morpheme) to the upper right.
In determining the label assignment for the cell
of ?Habu-Yoshiharu? as shown in 6(a), the model
compares the label assignment ?B? with the la-
bel assignment ?A+D.? In this case, the model
chooses the label assignment ?B?, that is, ?Habu
- Yoshiharu? is labeled as PERSON. Similarly,
in determining the label assignment for the cell
of ?Yoshiharu-Meijin?, the model compares the
59
chunk Habu-Yoshiharu Habu Yoshiharu
label PERSON PERSON MONEY
vector V11 0 0 0 0 V21 V22 0 0 0
Figure 5: An example of the feature for the label comparison model. (The example is ?Habu-Yoshiharu
vs Habu + Yoshiharu?, and V11, V21, V22, and 0 is a vector whose dimension is 13.)
HabuPERSON 0.111
Habu-YoshiharuPERSON 0.438
Habu-Yoshiharu-MeijinORGANIZATION0.083Yoshiharu Yoshiharu-Meijin
label assighment ?Habu-Yoshiharu?
label assignment
?Habu? + ?Yoshiharu?
A B C
EDMONEY0.075 OTHERe0.092
MeijinOTHERe0.245
F
(a): label assignment for the cell ?Habu-Yoshiharu?.
HabuPERSON 0.111
Habu-YoshiharuPERSON 0.438
Habu-Yoshiharu-MeijinORGANIZATION0.083Yoshiharu Yoshiharu + Meijin
label assignment
?Habu-Yoshiharu-Meijin?
A B C
EDMONEY0.075 MNY+OTHERe0.075+0.245
MeijinOTHERe0.245
label assignment
?Habu? + ?Yoshiharu? + ?Meijin?
label assignment
?Habu-Yoshiharu? + ?Meijin?
F
(b): label assignment for the cell ?Habu-Yoshiharu-Meijin?.
Figure 6: The label comparison model.
label assignment ?E? with the label assignment
?D+F.? In this case, the model chooses the label
assignment ?D+F?, that is, ?Yoshiharu? is labeled
as MONEY and ?Meijin? is labeled as OTHERe.
When the label assignment consists of multiple
chunks, the content of the cell is updated. In
this case, the cell ?E? is changed from ?Yoshi-
haru-Meijin? (OTHERe) to ?Yoshiharu + Meijin?
(MONEY + OTHERe).
As shown in Figure 6(b), in determining the best
label assignment for the upper right cell, that is,
the final output is determined, the model compares
the label assignment ?A+D+F?, ?B+F?, and ?C?.
When there are more than two candidates of label
assignments for a cell, all the label assignments are
compared in a pairwise, and the label assignment
that obtains the highest score is adopted.
In the label comparing step, the label as-
signment in which OTHER? follows OTHER?
(OTHER? - OTHER?) is not allowed since each
OTHER is assigned to the longest chunk as de-
scribed in Section 4.1. When the first combina-
tion of chunks equals to the second combination
of chunks, the comparison is not performed.
6 Experiment
To demonstrate the effectiveness of our proposed
method, we conducted an experiment on CRL NE
data. In this data, 10,718 sentences in 1,174 news
articles are annotated with eight NEs. The expres-
sion to which it is difficult to annotate manually is
labeled as OPTIONAL, and was not used for both
a b c d e
learn the label estimation model 1
Corpus: CRL NE data
learn the label estimation model 2bobtain features for the label apply
b c d e
b c d e
b c d e
comparison model
learn the label estimation model 2c
learn the label estimation model 2d
learn the label estimation model 2e
learn the label comparison model
Figure 7: 5-fold cross validation.
the model learning8 and the evaluation.
We performed 5-fold cross validation following
previous work. Different from previous work, our
work has to learn the SVM models twice. There-
fore, the corpus was divided as shown in Figure 7.
Let us consider the analysis in the part (a). First,
the label estimation model 1 is learned from the
part (b)-(e). Then, the label estimation model 2b
is learned from the part (c)-(e), and applying the
learned model to the part (b), features for learning
the label comparison model are obtained. Simi-
larly, the label estimation model 2c is learned from
the part (b),(d),(e), and applying it to the part (c),
features are obtained. It is the same with the part
8Exceptionally, ?OPTIONAL? is used when the label es-
timation model for OTHER? and invalid is learned.
60
Recall Precision
ORGANIZATION 81.83 (3008/3676) 88.37 (3008/3404)
PERSON 90.05 (3458/3840) 93.87 (3458/3684)
LOCATION 91.38 (4992/5463) 92.44 (4992/5400)
ARTIFACT 46.72 ( 349/ 747) 74.89 ( 349/ 466)
DATE 93.27 (3327/3567) 93.12 (3327/3573)
TIME 88.25 ( 443/ 502) 90.59 ( 443/ 489)
MONEY 93.85 ( 366/ 390) 97.60 ( 366/ 375)
PERCENT 95.33 ( 469/ 492) 95.91 ( 469/ 489)
ALL-SLOT 87.87 91.79
F-measure 89.79
Table 3: Experimental result.
(d) and (e). Then, the label comparison model is
learned from the obtained features. After that, the
analysis in the part (a) is performed by using both
the label estimation model 1 and the label compar-
ison model.
In this experiment, a Japanese morphological
analyzer, JUMAN9, and a Japanese parser, KNP10
were adopted. The two SVM models were learned
with polynomial kernel of degree 2, and ? in the
sigmoid function was set to be 1.
Table 6 shows an experimental result. An F-
measure in all NE classes is 89.79.
7 Discussion
7.1 Comparison with Previous Work
Table 7 presents the comparison with previ-
ous work, and our method outperformed previ-
ous work. Among previous work, Fukushima
et al acquired huge amount of category-
instance pairs (e.g., ?political party - New party
DAICHI?,?company-TOYOTA?) by some patterns
from a large Web corpus, and Sasano et al uti-
lized the analysis result of corefer resolution as a
feature for the model learning. Therefore, in our
method, by incorporating these knowledge and/or
such analysis result, the performance would be im-
proved.
Compared with Sasano et al, our method
achieved the better performance in analyzing
a long compound noun. For example, in the
bunsetsu ?Oushu-tsuujyou-senryoku-sakugen-
jyouyaku? (Treaty on Conventional Armed
Forces in Europe), while Sasano et al labeled
?Oushu? (Europe) as LOCATION, our method
correctly labeled ?Oushu-tsuujyou-senryoku-
sakugen-jyouyaku? as ARTIFACT. Sasano et
al. incorrectly labeled ?Oushu? as LOCATION
although they utilized the information about
9http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
10http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
the head of bunsetsu ?jyouyaku? (treaty). In
our method, for the cell ?Oushu?, invalid has
the highest score, and thus the score of LOCA-
TION relatively drops. Similarly, for the cell
?senryoku-sakugen-jyouyaku?, invalid has the
highest score. Consequently, ?Oushu-tsuujyou-
senryoku-sakugen-jyouyaku? is correctly labeled
as ARTIFACT.
In the bunsetsu ?gaikoku-jin-touroku-hou-ihan?
(the violation of the foreigner registration law),
while Sasano et al labeled ?touroku-hou? as AR-
TIFACT, our method correctly labeled ?gaikoku-
jin-touroku-hou? as ARTIFACT. Sasano et al can-
not utilize the information about ?hou? that is use-
ful for the label estimation since the head of this
bunsetsu is ?ihan.? In contrast, in estimating the
label of the chunk ?gaikoku-jin-touroku-hou?, the
information of ?hou? can be utilized.
7.2 Error Analysis
There were some errors in analyzing a Katakana
alphabet word. In the following example, although
the correct is that ?Batistuta? is labeled as PER-
SON, the system labeled it as OTHERs.
(4) Italy-de
Italy LOC
katsuyaku-suru
active
Batistuta-wo
Batistuta ACC
kuwaeta
call
Argentine
Argentine
?Argentine called Batistuta who was active in
Italy.?
There is not an entry of ?Batistuta? in the dictio-
nary of JUMAN nor Wikipedia, and thus only the
surrounding information is utilized. However, the
case analysis of ?katsuyaku? (active) is incorrect,
which leads to the error of ?Batistuta?.
There were some errors in applying the la-
bel comparison model although the analysis of
each chunk is correct. For example, in the
bunsetsu ?HongKong-seityou? (Government of
HongKong), the correct is that ?HongKong-
seityou? is labeled as ORGANIZATION. As
shown in Figure 8 (b), the system incorrectly
labeled ?HongKong? as LOCATION. As shown
in Figure 8(a), although in the initial state,
?HongKong-seityou? was correctly labeled as OR-
GANIZATION, the label assignment ?HongKong
+ seityou? was incorrectly chosen by the label
comparison model. To cope with this problem,
we are planning to the adjustment of the value ?
in the sigmoid function and the refinement of the
61
F1 analysis unit distinctive features
(Fukushima et al, 2008) 89.29 character Web
(Kazama and Torisawa, 2008) 88.93 character Wikipedia,Web
(Sasano and Kurohashi, 2008) 89.40 morpheme structural information
(Nakano and Hirai, 2004) 89.03 character bunsetsu feature
(Masayuki and Matsumoto, 2003) 87.21 character
(Isozaki and Kazawa, 2003) 86.77 morpheme
proposed method 89.79 compound noun Wikipedia,structural information
Table 4: Comparison with previous work. (All work was evaluated on CRL NE data using cross valida-
tion.)
HongKongLOCATION HongKong-seityouORGANIZATION0.266 0.205
seityouOTHERe0.184
(a):initial state
HongKongLOCATION HongKong + seityouLOC+OTHERe0.266 0.266+0.184
seityouOTHERe0.184
(b):the final output
Figure 8: An example of the error in the label com-
parison model.
features for the label comparison model.
8 Conclusion
This paper proposed bottom-up Named Entity
Recognition using a two-stage machine learning
method. This method first estimates the label of
all the chunks in a bunsetsu using a machine learn-
ing, and then the best label assignment is deter-
mined by bottom-up dynamic programming. We
conducted an experiment on CRL NE data, and
achieved an F-measure of 89.79.
We are planning to integrate this method with
the syntactic and case analysis method (Kawa-
hara and Kurohashi, 2007), and perform syntactic,
case, and Named Entity analysis simultaneously to
improve the overall accuracy.
References
Ken?ichi Fukushima, Nobuhiro Kaji, and Masaru
Kitsuregawa. 2008. Use of massive amounts
of web text in Japanese named entity recogni-
tion. In Proceedings of Data Engineering Workshop
(DEWS2008). A3-3 (in Japanese).
IREX Committee, editor. 1999. Proceedings of the
IREX Workshop.
Hideki Isozaki and Hideto Kazawa. 2003. Speeding up
support vector machines for named entity recogni-
tion. Transaction of Information Processing Society
of Japan, 44(3):970?979. (in Japanese).
Daisuke Kawahara and Sadao Kurohashi. 2007. Prob-
abilistic coordination disambiguation in a fully-
lexicalized Japanese parser. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL2007),
pages 304?311.
Jun?ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT, pages 407?415.
Erik F. Tjong Kim and Jorn Veenstra. 1999. Repre-
senting text chunks. In Proceedings of EACL ?99,
pages 173?179.
Vajay Krishnan and Christopher D.Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition.
pages 1121?1128.
John Lafferty, Andrew McCallun, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference (ICML?01), pages 282?289.
Asahara Masayuki and Yuji Matsumoto. 2003.
Japanese named entity extraction with redundant
morphological analysis. In Proceeding of HLT-
NAACL 2003, pages 8?15.
Keigo Nakano and Yuzo Hirai. 2004. Japanese named
entity extraction with bunsetsu features. Transac-
tion of Information Processing Society of Japan,
45(3):934?941. (in Japanese).
Ryohei Sasano and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural
language processing. In Proceeding of Third In-
ternational Joint Conference on Natural Language
Processing, pages 607?612.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-
nou. 1998. A decision tree method for finding and
classifying names in japanese texts. In Proceed-
ings of the Sixth Workshop on Very Large Corpora
(WVLC-6), pages 171?178.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
62
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108?116,
Paris, October 2009. c?2009 Association for Computational Linguistics
Capturing Consistency between Intra-clause and Inter-clause Relations
in Knowledge-rich Dependency and Case Structure Analysis
Daisuke Kawahara
National Institute of Information and
Communications Technology,
3-5 Hikaridai Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University,
Yoshida-Honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
We present a method for dependency and
case structure analysis that captures the
consistency between intra-clause relations
(i.e., case structures or predicate-argument
structures) and inter-clause relations. We
assess intra-clause relations on the basis
of case frames and inter-clause relations
on the basis of transition knowledge be-
tween case frames. Both knowledge bases
are automatically acquired from a mas-
sive amount of parses of a Web corpus.
The significance of this study is that the
proposed method selects the best depen-
dency and case structure that are con-
sistent within each clause and between
clauses. We confirm that this method con-
tributes to the improvement of dependency
parsing of Japanese.
1 Introduction
The approaches of dependency parsing basically
assess the likelihood of a dependency relation be-
tween two words or phrases and subsequently
collect all the assessments for these pairs as the
dependency parse of the sentence. To improve
dependency parsing, it is important to consider
as broad a context as possible, rather than a
word/phrase pair.
In the recent evaluation workshops (shared
tasks) of multilingual dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al, 2007),
transition-based and graph-based methods
achieved good performance by incorporating rich
context. Transition-based dependency parsers
consider the words following the word under
consideration as features of machine learning
(Kudo and Matsumoto, 2002; Nivre and Scholz,
2004; Sassano, 2004). Graph-based dependency
parsers consider sibling and grandparent nodes,
i.e., second-order and higher-order features
(McDonald and Pereira, 2006; Carreras, 2007;
Nakagawa, 2007).
It is desirable to consider a wider-range phrase,
clause, or a whole sentence, but it is difficult to
judge whether the structure of such a wide-range
expression is linguistically correct. One of the rea-
sons for this is the scarcity of the knowledge re-
quired to make such a judgment. When we use
the Penn Treebank (Marcus et al, 1993), which is
one of the largest corpora among the available ana-
lyzed corpora, as training data, even bi-lexical de-
pendencies cannot be learned sufficiently (Bikel,
2004). To circumvent such scarcity, for instance,
Koo et al (2008) proposed the use of word classes
induced by clustering words in a large raw cor-
pus. They succeeded in improving the accuracy of
a higher-order dependency parser.
On the other hand, some researchers have pro-
posed other approaches where linguistic units such
as predicate-argument structures (also known as
case structures and logical forms) are considered
instead of arbitrary nodes such as sibling nodes.
To solve the problem of knowledge scarcity, they
learned knowledge of such predicate-argument
structures from a very large number of automat-
ically analyzed corpora (Abekawa and Okumura,
2006; Kawahara and Kurohashi, 2006b). While
Abekawa and Okumura (2006) used only co-
occurrence statistics of verbal arguments, Kawa-
hara and Kurohashi (2006b) assessed predicate-
argument structures by checking case frames,
which are semantic frames that are automatically
compiled for each predicate sense from a large raw
corpus. These methods outperformed the accuracy
of supervised dependency parsers.
In such linguistically-motivated approaches,
well-formedness within a clause was considered,
but coherence between clauses was not con-
sidered. Even if intra-clause relations (i.e., a
predicate-argument structure within a clause) are
108
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
o r g a n i z e
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
( b 1 )
( a 1 )
( c 1 )
( a 2 )
( b 2 )
( c 2 )
( a 3 )
( b 3 )
( c 3 )
3 3 3
3 3 3
3 3 3
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
p a c k
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
o r g a n i z e
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
p a c k
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
o r g a n i z e
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
p a c k
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
p a c k
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
o r g a n i z e
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
p o i n t o  w a ,
p o i n t
 T O P
h
i t o t s u  n i
o n e
 D A T
m a t o m e t e
p a c k
t a k u
h
a i b i n  d e
c o u r i e r
 C M I
o k u r u
s e n d
k o t o  d e s u
b
e t h a t
Figure 1: Possible dependency and case structures of sentence (1).
optimized, they might not be optimum when look-
ing at clause pairs or sequences. To improve the
accuracy of dependency parsing, we propose a
method for dependency and case structure analy-
sis that considers the consistency between intra-
clause and inter-clause relations. This method an-
alyzes intra-clause relations on the basis of case
frames and inter-clause relations on the basis of
transition knowledge between case frames. These
two knowledge sources are automatically acquired
from a massive amount of parses of a Web corpus.
The contributions of this paper are two-fold.
First, we acquire transition knowledge not be-
tween verbs or verb phrases but between case
frames, which are semantically disambiguated
representations. Second, we incorporate the tran-
sition knowledge into dependency and case struc-
ture analysis to capture the consistency between
intra-clause and inter-clause relations.
The remainder of this paper is organized as
follows. Section 2 illustrates our idea. Section
3 describes a method for acquiring the transi-
tion knowledge. Section 4 explains the proposed
method of incorporating the acquired transition
knowledge into a probabilistic model of depen-
dency and case structure analysis. Section 5 re-
ports experimental results. Section 6 gives the
conclusions.
109
2 Idea of Capturing Consistency between
Intra-clause and Inter-clause Relations
We propose a method for generative dependency
parsing that captures the consistency between
intra-clause and inter-clause relations.
Figure 1 shows the ambiguities of dependency
and case structure of pointo-wa (point-TOP) in the
following sentence:
(1) pointo-wa,
point-TOP
hitotsu-ni
one-DAT
matomete
pack
takuhaibin-de
courier-CMI
okuru
send
koto-desu
be that
(The point is that (we) pack (one?s bag-
gage) and send (it) using courier service.)
The correct structure is (c1), which is surrounded
by the dotted rectangle. Structures (c2), (c3)
and so on have the same dependency structure as
(c1), but have incorrect case structures, in which
incorrect case frames are selected. Note that
matomeru:5, okuru:6 and so on in the figure rep-
resent the IDs of the case frames.
The parser of Kawahara and Kurohashi (2006b)
(and also conventional Japanese parsers) erro-
neously analyzes the head of pointo-wa (point-
TOP)1 as matomete (organize), whereas the cor-
rect head is koto-desu (be that), as shown in struc-
ture (a1) in Figure 1.
This error is caused by the incorrect selection
of the case frame matomeru:6 (organize), which is
shown in Table 1. This case frame locally matches
the input predicate-argument structure ?pointo-wa
hitotsu-ni matomeru? (organize points). There-
fore, this method considers only intra-clause re-
lations, and falls into local optimum.
If we consider the wide range of two clauses,
this error can be corrected. In structure (a1)
in Figure 1, the generative probability of case
frame transition, P (matomeru:6|okuru:6), is con-
sidered. This probability value is very low, be-
cause there are few relations between the case
frame matomeru:6 (organize) and the case frame
okuru:6 (send baggage) in corpora.
Consequently, structure (c1) is chosen as the
correct one, where both intra-clause and inter-
clause relations can be interpreted by the case
1In this paper, we use the following abbreviations:
NOM (nominative), ACC (accusative), ABL (ablative),
CMI (comitative) and TOP (topic marker).
Table 1: Case frame examples for matomeru and
okuru. ?CS? represents case slot. Argument words
are written only in English. ?<num>? represents
the class of numerals.
case frame ID CS example words
... ... ...
matomeru:5
(pack)
ga I, person, ...
wo baggage, luggage, variables, ...
ni <num>, pieces, compact, ...
matomeru:6
(organize)
ga doctor, ...
wo point, singularity, ...
ni <num>, pieces, below, ...
... ... ...
okuru:1
(send)
ga person, I, ...
wo mail, message, information, ...
ni friend, address, direction, ...
de mail, post, postage, ...
... ... ...
okuru:6
(send)
ga woman, ...
wo baggage, supply, goods, ...
ni person, Japan, parental house, ...
de mail, post, courier, ...
... ... ...
frames and the transition knowledge between case
frames.
3 Acquiring Transition Knowledge
between Case Frames
We automatically acquire large-scale transition
knowledge of inter-clause relations from a raw
corpus. The following two points are different
from previous studies on the acquisition of inter-
clause knowledge such as entailment/synonym
knowledge (Lin and Pantel, 2001; Torisawa, 2006;
Pekar, 2006; Zanzotto et al, 2006), verb relation
knowledge (Chklovski and Pantel, 2004), causal
knowledge (Inui et al, 2005) and event relation
knowledge (Abe et al, 2008):
? the unit of knowledge is disambiguated and
generalized
The unit in previous studies was a verb or a
verb phrase, in which verb sense ambiguities
still remain. Our unit is case frames that are
semantically disambiguated.
? the variation of relations is not limited
Although previous studies focused on lim-
ited kinds of semantic relations, we compre-
hensively collect generic relations between
clauses.
110
In this section, we first describe our unit of
transition knowledge, case frames, briefly. We
then detail the acquisition method of the transition
knowledge, and report experimental results. Fi-
nally, we refer to related work to the acquisition of
such knowledge.
3.1 The Unit of Transition Knowledge: Case
Frames
In this paper, we regard case frames as the unit of
transition knowledge. Case frames are constructed
from unambiguous structures and are semantically
clustered according to their meanings and usages.
Therefore, case frames can be a less ambiguous
and more generalized unit than a verb and a verb
phrase. Due to these characteristics, case frames
are a suitable unit for acquiring transition knowl-
edge and weaken the influence of data sparseness.
3.1.1 Automatic Construction of Case
Frames
We employ the method of Kawahara and Kuro-
hashi (2006a) to automatically construct case
frames. In this section, we outline the method for
constructing the case frames.
In this method, a large raw corpus is auto-
matically parsed, and the case frames are con-
structed from argument-head examples in the re-
sulting parses. The problems in automatic case
frame construction are syntactic and semantic am-
biguities. In other words, the parsing results in-
evitably contain errors, and verb senses are intrin-
sically ambiguous. To cope with these problems,
case frames are gradually constructed from reli-
able argument-head examples.
First, argument-head examples that have no
syntactic ambiguity are extracted, and they are dis-
ambiguated by a pair comprising a verb and its
closest case component. Such pairs are explic-
itly expressed on the surface of the text and can be
considered to play an important role in conveying
the meaning of a sentence. For instance, exam-
ples are distinguished not by verbs (e.g., ?tsumu?
(load/accumulate)), but by pairs (e.g., ?nimotsu-
wo tsumu? (load baggage) and ?keiken-wo tsumu?
(accumulate experience)). argument-head exam-
ples are aggregated in this manner, and they yield
basic case frames.
Thereafter, the basic case frames are clustered
in order to merge similar case frames, including
similar case frames that are made from scram-
bled sentences. For example, since ?nimotsu-
wo tsumu? (load baggage) and ?busshi-wo tsumu?
(load supply) are similar, they are clustered to-
gether. The similarity is measured by using a dis-
tributional thesaurus based on the study described
in Lin (1998).
3.2 Acquisition of Transition Knowledge
from Large Corpus
To acquire the transition knowledge, we collect the
clause pairs in a large raw corpus that have a de-
pendency relation and represent them as pairs of
case frames. For example, from the following sen-
tence, a case frame pair, (matomeru:5, okuru:6), is
extracted.
(2) nimotsu-wo
baggage-ACC
matomete,
pack
takuhaibin-de
courier-CMI
okutta
sent
(packed one?s baggage and sent (it) using
courier service)
These case frames are determined by applying
a conventional case structure analyzer (Kawa-
hara and Kurohashi, 2006b), which selects the
case frames most similar to the input expres-
sions ?nimotu-wo matomeru? (pack baggage) and
?takuhaibin-de okuru? (send with courier service)
from among the case frames of matomeru (or-
ganize/settle/pack/...) and okuru (send/remit/see
off/...); some of the case frames of matomeru and
okuru are listed in Table 1.
We adopt the following steps to acquire the tran-
sition knowledge between case frames:
1. Apply dependency and case structure analy-
sis to assign case frame IDs to each clause in
a large raw corpus.
2. Collect clause pairs that have a dependency
relation, and represent them as pairs of case
frame IDs.
3. Count the frequency of each pair of case
frame IDs; these statistics are used in the
analysis described in Section 4.
At step 2, we collect both syntactically ambigu-
ous and unambiguous relations in order to allevi-
ate data sparseness. The influence of a small num-
ber of dependency parsing errors would be hidden
by a large number of correct (unambiguous) rela-
tions.
111
Table 2: Examples of automatically acquired transition knowledge between case frames.
pairs of case frame IDs meaning freq.
(okuru:1, okuru:6) (send mails, send baggage) 186
(aru:1, okuru:6) (have, send baggage) 150
(suru:1, okuru:6) (do, send baggage) 134
(issyoda:10, okuru:6) (get together, send baggage) 118
(kaku:1, okuru:6) (write, send baggage) 115
... ... ...
(matomeru:5, okuru:6) (pack, send baggage) 12
(dasu:3, okuru:6) (indicate, send baggage) 12
... ... ...
3.3 Experiments of Acquiring Transition
Knowledge between Case Frames
To obtain the case frames and the transition knowl-
edge between case frames, we first built a Japanese
Web corpus by using the method of Kawahara and
Kurohashi (2006a). We first crawled 100 million
Japanese Web pages, and then, we extracted and
unduplicated Japanese sentences from the Web
pages. Consequently, we developed a Web corpus
consisting of 1.6 billion Japanese sentences.
Using the procedure of case frame construction
presented in Section 3.1.1, we constructed case
frames from the whole Web corpus. They con-
sisted of 43,000 predicates, and the average num-
ber of case frames for a predicate was 22.2.
Then, we acquired the transition knowledge be-
tween case frames using 500 million sentences of
the Web corpus. The resulting knowledge con-
sisted of 108 million unique case frame pairs. Ta-
ble 2 lists some examples of the acquired transition
knowledge. In the acquired transition knowledge,
we can find various kinds of relation such as en-
tailment, cause-effect and temporal relations.
Let us compare this result with the results of
previous studies. For example, Chklovski and
Pantel (2004) obtained 29,165 verb pairs for sev-
eral semantic relations in VerbOcean. The tran-
sition knowledge acquired in this study is several
thousand times larger than that in VerbOcean. It
is very difficult to make a meaningful compari-
son, but it can be seen that we have succeeded in
acquiring generic transition knowledge on a large
scale.
3.4 Related Work
In order to realize practical natural language pro-
cessing (NLP) systems such as intelligent dialog
systems, a lot of effort has been made to develop
world knowledge or inference knowledge. For ex-
ample, in the CYC (Lenat, 1995) and Open Mind
(Stork, 1999) projects, such knowledge has been
obtained manually, but it is difficult to manually
develop broad-coverage knowledge that is suffi-
cient for practical use in NLP applications.
On the other hand, the automatic acquisition of
such inference knowledge from corpora has at-
tracted much attention in recent years. First, se-
mantic knowledge between entities has been au-
tomatically obtained (Girju and Moldovan, 2002;
Ravichandran and Hovy, 2002; Pantel and Pennac-
chiotti, 2006). For example, Pantel and Pennac-
chiotti (2006) proposed the Espresso algorithm,
which iteratively acquires entity pairs and extrac-
tion patterns using reciprocal relationship between
entities and patterns.
As for the acquisition of the knowledge be-
tween events or clauses, which is most relevant
to this study, many approaches have been adopted
to acquire entailment knowledge. Lin and Pan-
tel (2001) and Szpektor and Dagan (2008) learned
entailment rules based on distributional similar-
ity between instances that have a relation to a
rule. Torisawa (2006) extracted entailment knowl-
edge using coordinated verb pairs and noun-verb
co-occurrences. Pekar (2006) also collected en-
tailment knowledge with discourse structure con-
straints. Zanzotto et al (2006) obtained entailment
knowledge using nominalized verbs.
There have been some studies on relations other
than entailment relations. Chklovski and Pan-
tel (2004) obtained verb pairs that have one of
five semantic relations by using a search engine.
Inui et al (2005) classified the occurrences of
the Japanese connective marker tame. Abe et al
112
(2008) learned event relation knowledge for two
semantic relations. They first gave seed pairs of
verbs or verb phrases and extracted the patterns
that matched these seed pairs. Subsequently, by
using the Espresso algorithm (Pantel and Pennac-
chiotti, 2006), this process was iterated to augment
both instances and patterns. The acquisition unit
in these studies was a verb or a verb phrase.
In contrast to these studies, we obtained generic
transition knowledge between case frames without
limiting target semantic relations.
4 Incorporating Transition Knowledge
into Dependency and Case Structure
Analysis
We employ the probabilistic generative model of
dependency and case structure analysis (Kawahara
and Kurohashi, 2006b) as a base model. We incor-
porate the obtained transition knowledge into this
base parser.
Our model assigns a probability to each possi-
ble dependency structure, T , and case structure,
L, of the input sentence, S, and outputs the de-
pendency and case structure that have the highest
probability. In other words, the model selects the
dependency structure T best and the case structure
Lbest that maximize the probability P (T,L|S) or
its equivalent, P (T,L, S), as follows:
(T best, Lbest) = argmax (T,L)P (T,L|S)
= argmax (T,L)P (T,L, S)P (S)
= argmax (T,L)P (T,L, S). (1)
The last equation follows from the fact that P (S)
is constant.
In the model, a clause (or predicate-argument
structure) is considered as a generation unit and
the input sentence is generated from the end of the
sentence. The probability P (T,L, S) is defined
as the product of the probabilities of generating
clauses Ci as follows:
P (T,L, S) = ? Ci?SP (Ci|Ch), (2)
where Ch is the modifying clause of Ci. Since the
Japanese language is head final, the main clause at
the end of a sentence does not have a modifying
head; we account for this by assuming Ch = EOS
(End Of Sentence).
The probability P (Ci|Ch) is defined in a man-
ner similar to that in Kawahara and Kurohashi
(2006b). However, the difference between the
probability in the above-mentioned study and that
in our study is the generative probability of the
case frames, i.e., the probability of generating a
case frame CF i from its modifying case frame
CF h. The base model approximated this proba-
bility as the product of the probability of gener-
ating a predicate vi from its modifying predicate
vh and the probability of generating a case frame
CF i from the predicate vi as follows:
P (CF i|CF h) ?
P (vi|vh)? P (CF i|vi). (3)
Our proposed model directly estimates the proba-
bility P (CF i|CF h) and considers the transition
likelihood between case frames. This probabil-
ity is calculated from the transition knowledge be-
tween case frames using maximum likelihood.
In practice, to avoid the data sparseness prob-
lem, we interpolate the probability P (CF i|CF h)
with the probability of generating predicates,
P (vi|vh), as follows:
P ?(CF i|CF h) ?
?P (CF i|CF h) + (1? ?)P (vi|vh), (4)
where ? is determined using the frequencies of the
case frame pairs, (CF i, CF h), in the same man-
ner as in Collins (1999).
5 Experiments
We evaluated the dependency structures that were
output by our new dependency parser. The case
frames used in these experiments are the same as
those described in Section 3.3, which were au-
tomatically constructed from 1.6 billion Japanese
sentences obtained from the Web.
In this study, the parameters related to unlexi-
cal types were calculated from the Kyoto Univer-
sity Text Corpus, which is a small tagged corpus
of newspaper articles, and lexical parameters were
obtained from a large Web corpus. To evaluate the
effectiveness of our model, our experiments were
conducted using sentences obtained from the Web.
As a test corpus, we used 759 Web sentences2,
which were manually annotated using the same
criteria as those in the case of the Kyoto Univer-
sity Text Corpus. We also used the Kyoto Univer-
sity Text Corpus as a development corpus to op-
timize some smoothing parameters. The system
2The test set was not used to construct case frames and
estimate probabilities.
113
Table 3: The dependency accuracies in our experiments.
syn syn+case syn+case+cons
all 4,555/5,122 (88.9%) 4,581/5,122 (89.4%) 4,599/5,122 (89.8%)
NP?VP 2,115/2,383 (88.8%) 2,142/2,383 (89.9%) 2,151/2,383 (90.3%)
NP?NP 1,068/1,168 (91.4%) 1,068/1,168 (91.4%) 1,068/1,168 (91.4%)
VP?VP 779/928 (83.9%) 777/928 (83.7%) 783/928 (84.4%)
VP?NP 579/623 (92.9%) 579/623 (92.9%) 582/623 (93.4%)
input was automatically tagged using the JUMAN
morphological analyzer 3.
We used two baseline systems for the purposes
of comparison: a rule-based dependency parser
(Kurohashi and Nagao, 1994) and the probabilistic
generative model of dependency and case struc-
ture analysis (Kawahara and Kurohashi, 2006b)4.
We use the above-mentioned case frames also in
the latter baseline parser, which also requires au-
tomatically constructed case frames.
5.1 Evaluation of Dependency Structures
We evaluated the obtained dependency structures
in terms of phrase-based dependency accuracy ?
the proportion of correct dependencies out of all
dependencies5.
Table 3 lists the dependency accuracies. In this
table, ?syn? represents the rule-based dependency
parser, ?syn+case? represents the probabilistic
parser of syntactic and case structure (Kawahara
and Kurohashi, 2006b)6, and ?syn+case+cons?
represents our proposed model. In the table, the
dependency accuracies are classified into four cat-
egories on the basis of the phrase classes (VP:
verb phrase7 and NP: noun phrase) of a dependent
and its head. The parser ?syn+case+cons? signif-
icantly outperformed the two baselines for ?all?
(McNemar?s test; p < 0.05). In particular, the ac-
curacy of the intra-clause (predicate-argument) re-
lations (?NP?VP?) was improved by 1.5% from
?syn? and by 0.4% from ?syn+case.? These im-
3http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
4http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
5Since Japanese is head-final, the second to last phrase
unambiguously depends on the last phrase. However, we in-
clude such dependencies into our evaluation as in most of
previous studies.
6The accuracy described in Kawahara and Kurohashi
(2006b) is different from that of this paper due to the different
evaluation measure excluding the unambiguous dependencies
of the second last phrases.
7VP includes not only verbs but also adjectives and nouns
with copula.
provements are due to the incorporation of the
transition knowledge into syntactic/case structure
analysis.
In order to compare our results with a state-of-
the-art discriminative dependency parser, we in-
put the test corpus into an SVM-based Japanese
dependency parser, CaboCha8(Kudo and Mat-
sumoto, 2002), which was trained using the Kyoto
University Text Corpus. Its dependency accuracy
was 88.6% (4,540/5,122), which is close to that of
?syn.? This low accuracy is attributed to the lack
of knowledge of both intra-clause and inter-clause
relations. Another cause of the low accuracy is the
out-of-domain training corpus. In other words, the
parser was trained on a newspaper corpus, while
the test corpus was obtained from theWeb because
a tagged Web corpus that is large enough to train
a supervised parser is not available.
5.2 Discussions
Figure 2 shows some improved analyses; here, the
dotted lines represent the results of the analysis
performed using the baseline ?syn + case,? and
the solid lines represent the analysis performed
using the proposed method, ?syn+case+cons.?
These sentences are incorrectly analyzed by the
baseline but correctly analyzed by the proposed
method. For example, in sentence (a), the head of
gunegunemichi-wo (winding road-ACC) was cor-
rectly analyzed as yurareru (be jolted). This is
because the case frame of ?basu-ni yurareru? (be
jolted by bus) is likely to generate tatsu (stand)
that does not take the wo (ACC) slot. In this man-
ner, by considering the transition knowledge be-
tween case frames, the selection of case frames
became accurate, and thus, the accuracy of the
dependencies within clauses (predicate-argument
structures) was improved.
In the case of the dependencies between pred-
icates (VP?VP), however, only small improve-
8http://chasen.org/?taku/software/
cabocha/
114
? ?(a) gunegunemichi-wo tattamama basu-ni yurareru toko-wo kakugoshimashita.
winding road-ACC stand bus-DAT be jolted (that)-ACC be resolved
(be resolved to be jolted standing on the bus by the winding road.)
??(b) nanika-wo eru tame-ni suteta mono-nimo miren-wo nokoshiteiru.
something-ACC get for discarded thing-also lingering desire-ACC retain
(retain a lingering desire also for the thing that was discarded to get something.)
??(c) senbei-no hako-wa, kankaku-wo akete chinretsusareteiruno-ga mata yoi.
rice cracker-GEN box-TOP interval-ACC place be displayed-NOM also good
(It is also good that boxes of rice cracker are displayed placing an interval.)
Figure 2: Improved examples.
? ?(d) ketsuron-kara itteshimaeba, kaitearukoto-wa machigattenaishi, juyouna kotodato-wa wakaru.
conclusion-ABL say content-TOP not wrong important (that)-TOP understand
(Saying from conclusions, the content is not wrong and (I) understand that (it) is important)
Figure 3: An erroneous example.
ments were achieved by using the transition
knowledge between case frames. This is mainly
because the heads of the predicates are intrinsi-
cally ambiguous in many cases.
For example, in sentence (d) in Figure 3, the
correct head of itteshimaeba (say) is wakaru (un-
derstand) as designated by the solid line, but our
model incorrectly judged the head to be machigat-
teinaishi, (not wrong) as designated by the dotted
line. However, in this case, both the phrases that
are being modified are semantically related to the
modifier. To solve this problem, it is necessary to
re-consider the evaluation metrics of dependency
parsing.
6 Conclusion
In this paper, we have described a method for ac-
quiring the transition knowledge of inter-clause re-
lations and a method for incorporating this knowl-
edge into dependency and case structure analy-
sis. The significance of this study is that the pro-
posed parsing method selects the best dependency
and case structures that are consistent within each
clause and between clauses. We confirmed that
this method contributed to the improvement of the
dependency parsing of Japanese.
The case frames that are acquired from 1.6 bil-
lion Japanese sentences have been made freely
available to the public9. In addition, we are prepar-
ing to make the acquired transition knowledge ac-
cessible on the Web.
In future, we will investigate the iteration of
knowledge acquisition and parsing based on the
acquired knowledge. Since our parser is a gener-
ative model, we are expecting a performance gain
by the iteration. Furthermore, we would like to ex-
plore the use of the transition knowledge between
case frames to improve NLP applications such as
recognizing textual entailment (RTE) and sentence
generation.
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning
cooccurrence patterns and fertilizing cooccurrence
samples with verbal nouns. In Proceedings of IJC-
NLP2008, pages 497?504.
Takeshi Abekawa and Manabu Okumura. 2006.
Japanese dependency parsing using co-occurrence
information and a combination of case elements. In
Proceedings of COLING-ACL2006, pages 833?840.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
9http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/caseframe-e.html
115
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings
of EMNLP-CoNLL2007 Shared Task, pages 957?
961.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP2004, pages
33?40.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Roxana Girju and Dan Moldovan. 2002. Mining an-
swers for causation questions. In Proceedings of
AAAI Spring Symposium.
Takashi Inui, Kentaro Inui, and Yuji Matsumoto.
2005. Acquiring causal knowledge from text us-
ing the connective marker tame. ACM Transactions
on Asian Language Information Processing (ACM-
TALIP), 4(4):435?474.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using
high-performance computing. In Proceedings of
LREC2006.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proceed-
ings of HLT-NAACL2006, pages 176?183.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08:HLT, pages 595?603.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of CoNLL2002, pages 29?35.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistics, 20(4):507?534.
Douglas B. Lenat. 1995. CYC: A large-scale invest-
ment in knowledge infrastructure. Communications
of the ACM, 38(11):32?38.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDDConference on Knowledge Discovery
and Data Mining, pages 323?328.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL98, pages 768?774.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL2006, pages 81?88.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In Proceedings of
EMNLP-CoNLL2007 Shared Task, pages 952?956.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING2004, pages 64?70.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL2007, pages 915?932.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of COLING-ACL2006, pages 113?120.
Viktor Pekar. 2006. Acquisition of verb entailment
from text. In Proceedings of HLT-NAACL2006,
pages 49?56.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL2002, pages 41?47.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proceedings of COLING2004,
pages 8?14.
David G. Stork. 1999. Character and document re-
search in the open mind initiative. In Proceedings
of International Conference on Document Analysis
and Recognition, pages 1?12.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING2008, pages 849?856.
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using Japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of HLT-NAACL2006, pages 57?64.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of COLING-
ACL2006, pages 849?856.
116
Improving Japanese Zero Pronoun Resolution
by Global Word Sense Disambiguation
Daisuke Kawahara and Sadao Kurohashi
Graduate School of Information Science and Technology, University of Tokyo
{kawahara,kuro}@kc.t.u-tokyo.ac.jp
Abstract
This paper proposes unsupervised word
sense disambiguation based on automati-
cally constructed case frames and its in-
corporation into our zero pronoun resolu-
tion system. The word sense disambigua-
tion is applied to verbs and nouns. We
consider that case frames define verb senses
and semantic features in a thesaurus define
noun senses, respectively, and perform sense
disambiguation by selecting them based on
case analysis. In addition, according to the
one sense per discourse heuristic, the word
sense disambiguation results are cached and
applied globally to the subsequent words.
We integrated this global word sense disam-
biguation into our zero pronoun resolution
system, and conducted experiments of zero
pronoun resolution on two different domain
corpora. Both of the experimental results
indicated the effectiveness of our approach.
1 Introduction
For a long time, parsing has been a central is-
sue for the area of natural language analyses.
In recent years, its accuracy has improved to
over 90%, and it became the fundamental tech-
nology that is applied to a lot of NLP applica-
tions, such as question answering, text summa-
rization, and machine translation. Accordingly,
anaphora resolution, which is positioned as the
next step of parsing, has been studied actively
(Ng and Cardie, 2002; Yang et al, 2003; Iida
et al, 2003; Isozaki and Hirao, 2003; Kawa-
hara and Kurohashi, 2004). Its performance,
however, is not satisfactory enough to benefit
the NLP applications. We investigated errors
of our Japanese zero pronoun resolution system
(Kawahara and Kurohashi, 2004), and found
that word sense ambiguity causes a major part
of errors.
Our zero pronoun resolution system uti-
lizes the general-purpose thesaurusNihongo Goi
Taikei (Ikehara et al, 1997) (hereafter, NTT
thesaurus) to do matching of example words.
In this thesaurus, one or more semantic features
are given to each word, and similarity between
words can be calculated by comparing closeness
of their semantic features in the thesaurus tree
(Appendix A). Multiple semantic features for
a word, i.e. word sense ambiguity, cause in-
correct matching, and furthermore deteriorate
accuracy of the zero pronoun resolution sys-
tem. For instance, in the thesaurus, ?gobou?
(burdock/priest/temple) has four semantic fea-
tures: <crop>, <vegetable>, <priest> and
<temple>?. If <priest> is used for ?gobou? in
a cooking domain text, though ?gobou? means
?burdock? (a genus of coarse biennial herbs) in
its context, ?gobou? is identified as an agent.
That is, ?gobou? is incorrectly analyzed as an-
tecedents of the following nominative zero pro-
nouns.
If such word sense ambiguity can be resolved,
incorrect matching decreases, and the anaphora
resolution system will improve. Word sense dis-
ambiguation is a basic issue of NLP. Recently,
the evaluation exercises for word sense disam-
biguation such as SENSEVAL-1 (Kilgarriff and
Palmer, 2000) and SENSEVAL-2 (Yarowsky,
2001) have been held, but word sense disam-
biguation has rarely been incorporated into
deep analyses like anaphora resolution.
This paper proposes unsupervised word sense
disambiguation based on automatically con-
structed case frames and its incorporation into
the zero pronoun resolution system. The word
sense disambiguation is applied to verbs? and
nouns. We consider that case frames define
verb senses and semantic features in the the-
saurus define noun senses, respectively. A verb
is disambiguated by selecting a corresponding
case frame to its context, and a noun is disam-
biguated by selecting an appropriate semantic
?In this paper, <> means a semantic feature.
?In this paper, we use ?verb? instead of ?verb, adjective
and noun+copula? for simplicity.
NOUN
CONCRETE ABSTRACT
AGENT PLACE CONCRETE
HUMAN ORGANIZATION
ABSTRACT EVENT ABSTRACT RELATION
TIME POSITION QUANTITY . . . .
Figure 1: The upper levels of the NTT thesaurus.
feature from the ones defined for the noun in
the thesaurus. In addition, according to the one
sense per discourse heuristic, the disambigua-
tion results are cached and applied globally to
the following words in the same text.
The remainder of this paper is organized as
follows. Section 2 briefly describes the NTT the-
saurus and the automatic construction method
of case frames. Section 3 outlines our zero pro-
noun resolution system. Section 4 describes the
method of word sense disambiguation and its
integration into the zero pronoun resolution sys-
tem. Section 5 presents the experiments of the
integrated system. Section 6 summarizes the
conclusions.
2 Resources
We consider that verb and noun senses corre-
spond to case frames and semantic features de-
fined in the NTT thesaurus, respectively. This
section describes the NTT thesaurus and the
case frames briefly.
2.1 NTT thesaurus
NTT Communication Science Laboratories con-
structed a semantic feature tree, whose 3,000
nodes are semantic features, and a nominal dic-
tionary containing about 300,000 nouns, each of
which is given one or more appropriate seman-
tic features. Figure 1 shows the upper levels of
the semantic feature tree.
The similarity between two words is defined
by formula (1) in Appendix A.
2.2 Automatically constructed case
frames
We employ the automatically constructed case
frames (Kawahara and Kurohashi, 2002) as
the basic resource for zero pronoun resolution
and word sense disambiguation. This section
outlines the method of constructing the case
frames.
The biggest problem in automatic case frame
construction is verb sense ambiguity. Verbs
which have different meanings should have dif-
ferent case frames, but it is hard to dis-
ambiguate verb senses precisely. To deal
with this problem, predicate-argument exam-
ples which are collected from a large cor-
pus are distinguished by coupling a verb and
its closest case component. That is, ex-
amples are not distinguished by verbs (e.g.
?tsumu? (load/accumulate)), but by couples
(e.g. ?nimotsu-wo tsumu? (load baggage) and
?keiken-wo tsumu? (accumulate experience)).
This process makes separate case frames
which have almost the same meaning or usage.
For example, ?nimotsu-wo tsumu? (load bag-
gage) and ?busshi-wo tsumu? (load supply) are
similar, but have separate case frames. To cope
with this problem, the case frames are clustered.
Example words are collected for each case
marker, such as ?ga?, ?wo?, ?ni? and
?kara?. They are case-marking postpositions
in Japanese, and usually mean nominative, ac-
cusative, dative and ablative, respectively. We
call such a case marker ?case slot? and example
words in a case slot ?case examples?.
Case examples in a case slot are similar, but
have some incorrect semantic features because
of word sense ambiguity. For instance, ?ni-
motsu? (baggage), ?busshi? (supply) and ?nise-
mono? (imitation) are gathered in a case slot,
and all of them are below the semantic feature
<goods>. On the other hand, ?nisemono? be-
longs to <lie>. <lie> is incorrect for this case
slot, and possibly causes errors in case analysis.
We delete a semantic feature that is not similar
to the other semantic features of its case slot.
To sum up, the procedure for the automatic
case frame construction is as follows.
1. A large raw corpus is parsed by the
Japanese parser, KNP (Kurohashi and
Nagao, 1994b), and reliable predicate-
argument examples are extracted from the
parse results.
2. The extracted examples are bundled ac-
cording to the verb and its closest case com-
ponent, making initial case frames.
3. The initial case frames are clustered using a
similarity measure function. This similar-
ity is calculated by formula (5) in Appendix
B.
4. For each case slot of clustered case frames,
an inappropriate semantic feature that is
not similar to the other semantic features
is discarded.
We constructed two sets of case frames: for
newspaper and cooking domain.
The newspaper case frames are constructed
from about 21,000,000 sentences of newspaper
articles in 20 years (9 years of Mainichi news-
paper and 11 years of Nihonkeizai newspaper).
They consist of 23,000 verbs, and the average
number of case frames for a verb is 14.5.
The cooking case frames are constructed from
about 5,000,000 sentences of cooking domain
that are collected from WWW. They consist
of 5,600 verbs, and the average number of case
frames for a verb is 6.8.
In Figure 1, some examples of the resulting
case frames are shown. In this table, ?CS? means
a case slot. <agent> in the table is a general-
ized case example, which is given to the case
slot where half of the case examples belong to
<agent>. <agent> is also given to ?ga? case
slot that has no case examples, because ?ga?
case components are often omitted, but ?ga?
case slots usually mean nominative.
3 The Outline of the Zero Pronoun
Resolution System
We have proposed a Japanese zero pronoun
resolution system using the case frames, an-
tecedent preference orders, and a machine learn-
ing technique (Kawahara and Kurohashi, 2004).
Its procedure is as follows.
1. Parse an input sentence using the Japanese
parser, KNP.
2. Process each verb in the sentence from left
to right by the following steps.
Table 1: Case frame examples.
CS case examples?
ga <agent>, group, party, ? ? ?youritsu (1) wo <agent>, candidate, applicant(support) ni <agent>, district, election, ? ? ?
ga <agent>youritsu (2) wo <agent>, member, minister, ? ? ?(support) ni <agent>, candidate, successor
... ... ...
orosu (1) ga <agent>
(grate) wo radish
ga <agent>orosu (2) wo money(withdraw) kara bank, post
... ... ...
itadaku (1) ga <agent>
(have) wo soup
ga <agent>itadaku (2) wo advice, instruction, address(be given) kara <agent>, president, circle, ? ? ?
... ... ...
?case examples are expressed only in English for
space limitation.
2.1. Narrow case frames down to corre-
sponding ones to the verb and its clos-
est case component.
2.2. Perform the following processes for
each case frame of the target verb.
i. Match each input case component
with an appropriate case slot of
the case frame. Regard case slots
that have no correspondence as
zero pronouns.
ii. Estimate an antecedent of each
zero pronoun.
2.3. Select a case frame which has the high-
est total score, and output the analysis
result for the case frame.
The rest of this section describes the above
steps (2.1), (2.2.i) and (2.2.ii) in detail.
3.1 Narrowing down case frames
The closest case component plays an important
role to determine the usage of a verb. In par-
ticular, when the closest case is ?wo? or ?ni?,
this trend is clear-cut. In addition, an expres-
sion whose nominative belongs to <agent> (e.g.
?<agent> has accomplished?), does not have
enough clue to decide its usage, namely a case
frame. By considering these aspects, we impose
the following conditions on narrowing down case
frames.
? The closest case component exists, and
must immediately precede its verb.
? The closest case component and the closest
case meet one of the following conditions:
? The closest case is ?wo? or ?ni?.
? The closest case component does
not belong to the semantic marker
<agent>.
? A case frame with the closest case exists,
and the similarity between the closest case
component and examples in the closest case
exceeds a threshold.
We choose the case frames whose similarity
is the highest. If the above conditions are not
satisfied, case frames are not narrowed down,
and the subsequent processes are performed for
each case frame of the target verb. The simi-
larity used here is defined as the best similarity
between the closest case component and exam-
ples in the case slot. The similarity between two
examples is defined as formula (1) in Appendix
A.
Let us consider ?youritsu? (support) in the
second sentence of Figure 2. ?youritsu? has the
case frames shown in Table 1. The input expres-
sion ?kouho-wo youritsu? (support a candidate)
satisfies the above two conditions, and the case
frame ?youritsu (1)? meets the last condition.
Accordingly, this case frame is selected.
3.2 Matching input case components
with case slots in the case frame
We match case components of the target verb
with case slots in the case frame (Kurohashi and
Nagao, 1994a). When a case component has a
case marker, it must be assigned to the case
slot with the same case marker. When a case
component is a topic marked phrase or a clausal
modifiee, which does not have a case marker, it
can be assigned to one of the case slots in the
following table.
topic marked phrases : ga, wo, ga2
clausal modifiees : ga, wo, non-gapping
The conditions above may produce multiple
matching patterns. In this case, one which has
the best score is selected. The score of a match-
ing pattern is defined as the sum of similarities
of case assignments. This similarity is calcu-
lated as the same way described in Section 3.1.
 	
  

 
 





 Automatic Construction of Nominal Case Frames and
its Application to Indirect Anaphora Resolution
Ryohei Sasano, Daisuke Kawahara and Sadao Kurohashi
Graduate School of Information Science and Technology, University of Tokyo
{ryohei,kawahara,kuro}@kc.t.u-tokyo.ac.jp
Abstract
This paper proposes a method to auto-
matically construct Japanese nominal case
frames. The point of our method is the in-
tegrated use of a dictionary and example
phrases from large corpora. To examine the
practical usefulness of the constructed nom-
inal case frames, we also built a system of
indirect anaphora resolution based on the
case frames. The constructed case frames
were evaluated by hand, and were confirmed
to be good quality. Experimental results of
indirect anaphora resolution also indicated
the effectiveness of our approach.
1 Introduction
What is represented in a text has originally
a network structure, in which several concepts
have tight relations with each other. However,
because of the linear constraint of texts, most
of them disappear in the normal form of texts.
Automatic reproduction of such relations can be
regarded as the first step of ?text understand-
ing?, and surely benefits NLP applications such
as machine translation, automatic abstraction,
and question answering.
One of such latent relationship is indirect
anaphora, functional anaphora, or bridging ref-
erence, such as the following examples.
(1) I bought a ticket. The price was 20 dollars.
(2) There was a house. The roof was white.
Here, ?the price? means ?the price of a ticket?
and ?the roof? means ?the roof of a house.?
Most nouns have their indispensable or req-
uisite entities: ?price? is a price of some goods
or service, ?roof? is a roof of some building,
?coach? is a coach of some sport, and ?virus?
is a virus causing some disease. The relation
between a noun and its indispensable entity is
parallel to that between a verb and its argu-
ments or obligatory cases. In this paper, we call
indispensable entities of nouns obligatory cases.
Indirect anaphora resolution needs a compre-
hensive information or dictionary of obligatory
cases of nouns.
In case of verbs, syntactic structures such as
subject/object/PP in English or case markers
such as ga, wo, ni in Japanese can be utilized
as a strong clue to distinguish several obliga-
tory cases and adjuncts (and adverbs), which
makes it feasible to construct case frames from
large corpora automatically (Briscoe and Car-
roll, 1997; Kawahara and Kurohashi, 2002).
(Kawahara and Kurohashi, 2004) then utilized
the automatically constructed case frames to
Japanese zero pronoun resolution.
On the other hand, in case of nouns, obliga-
tory cases of noun Nh appear, in most cases, in
the single form of noun phrase ?Nh of Nm? in
English, or ?Nm no Nh? in Japanese. This sin-
gle form can express several obligatory cases,
and furthermore optional cases, for example,
?rugby no coach? (obligatory case concerning
what sport), ?club no coach? (obligatory case
concerning which institution), and ?kyonen ?last
year? no coach? (optional case). Therefore, the
key issue to construct nominal case frames is to
analyze ?Nh of Nm? or ?Nm no Nh? phrases to
distinguish obligatory case examples and others.
Work which addressed indirect anaphora in
English texts so far restricts relationships to a
small, relatively well-defined set, mainly part-of
relation like the above example (2), and utilized
hand-crafted heuristic rules or hand-crafted lex-
ical knowledge such as WordNet (Hahn et al,
1996; Vieira and Poesio, 2000; Strube and
Hahn, 1999). (Poesio et al, 2002) proposed
a method of acquiring lexical knowledge from
?Nh of Nm? phrases, but again concentrated on
part-of relation.
In case of Japanese text analysis, (Murata et
al., 1999) proposed a method of utilizing ?Nm
no Nh? phrases for indirect anaphora resolution
of diverse relationships. However, they basically
used all ?Nm no Nh? phrases from corpora, just
excluding some pre-fixed stop words. They con-
fessed that an accurate analysis of ?Nm no Nh?
phrases is necessary for the further improve-
ment of indirect anaphora resolution.
As a response to these problems and follow-
ing the work in (Kurohashi and Sakai, 1999), we
propose a method to construct Japanese nom-
inal case frames from large corpora, based on
an accurate analysis of ?Nm no Nh? phrases
using an ordinary dictionary and a thesaurus.
To examine the practical usefulness of the con-
structed nominal case frames, we also built a
system of indirect anaphora resolution based on
the case frames.
2 Semantic Feature Dictionary
First of all, we briefly introduce NTT Seman-
tic Feature Dictionary employed in this paper.
NTT Semantic Feature Dictionary consists of a
semantic feature tree, whose 3,000 nodes are se-
mantic features, and a nominal dictionary con-
taining about 300,000 nouns, each of which is
given one or more appropriate semantic fea-
tures.
The main purpose of using this dictionary is
to calculate the similarity between two words.
Suppose the word x and y have a semantic fea-
ture sx and sy, respectively, their depth is dx
and dy in the semantic tree, and the depth of
their lowest (most specific) common node is dc,
the similarity between x and y, sim(x, y), is cal-
culated as follows:
sim(x, y) = (dc ? 2)/(dx + dy).
If sx and sy are the same, the similarity is 1.0,
the maximum score based on this criteria.
We also use this dictionary to specify seman-
tic category of words, such as human, time and
place.
3 Semantic Analysis of Japanese
Noun Phrases Nm no Nh
In many cases, obligatory cases of nouns are
described in an ordinary dictionary for human
being. For example, a Japanese dictionary for
children, Reikai Shougaku Kokugojiten, or RSK
(Tajika, 1997), gives the definitions of the word
coach and virus as follows1:
coach a person who teaches technique in some
sport
virus a living thing even smaller than bacte-
ria which causes infectious disease like in-
fluenza
1Although our method handles Japanese noun
phrases by using Japanese definition sentences, in this
paper we use their English translations for the explana-
tion. In some sense, the essential point of our method is
language-independent.
Based on such an observation, (Kurohashi
and Sakai, 1999) proposed a semantic analy-
sis method of ?Nm no Nh?, consisting of the
two modules: dictionary-based analysis (abbre-
viated to DBA hereafter) and semantic feature-
based analysis (abbreviated to SBA hereafter).
This section briefly introduces their method.
3.1 Dictionary-based analysis
Obligatory case information of nouns in an ordi-
nary dictionary can be utilized to solve the dif-
ficult problem in the semantic analysis of ?Nm
no Nh? phrases. In other words, we can say the
problem disappears.
For example, ?rugby no coach? can be inter-
preted by the definition of coach as follows: the
dictionary describes that the noun coach has an
obligatory case sport, and the phrase ?rugby no
coach? specifies that the sport is rugby. That is,
the interpretation of the phrase can be regarded
as matching rugby in the phrase to some sport
in the coach definition. ?Kaze ?cold? no virus?
is also easily interpreted based on the definition
of virus, linking kaze ?cold? to infectious disease.
Dictionary-based analysis (DBA) tries to find
a correspondence between Nm and an obliga-
tory case of Nh by utilizing RSK and NTT Se-
mantic Feature Dictionary, by the following pro-
cess:
1. Look up Nh in RSK and obtain the defini-
tion sentences of Nh.
2. For each word w in the definition sentences
other than the genus words, do the follow-
ing steps:
2.1. When w is a noun which shows an
obligatory case explicitly, like kotog-
ara ?thing?, monogoto ?matter?, nanika
?something?, and Nm does not have a
semantic feature of human or time,
give 0.8 to their correspondence2.
2.2. When w is other noun, calculate the
similarity between Nm and w by us-
ing NTT Semantic Feature Dictionary,
and give the similarity score to their
correspondence.
3. Finally, if the best correspondence score is
0.75 or more, DBA outputs the best corre-
spondence, which can be an obligatory case
of the input; if not, DBA outputs nothing.
2For the present, parameters in the algorithm were
given empirically, not optimized by a learning method.
Table 1: Examples of rules for semantic feature-based analysis.
1. Nm:human, Nh:relative ? <obligatory case(relative)> e.g. kare ?he? no oba ?aunt?
2. Nm:human, Nh:human ? <modification(apposition)> e.g. gakusei ?student? no kare ?he?
3. Nm:organization, Nh:human ? <belonging> e.g. gakkou ?school? no seito ?student?
4. Nm:agent, Nh:event ? <agent> e.g. watashi ?I? no chousa ?study?
5. Nm:material, Nh:concrete ? <modification(material)> e.g. ki ?wood? no hako ?box?
6. Nm:time, Nh:? ? <time> e.g. aki ?autumn? no hatake ?field?
7. Nm:color, quantity, or figure, Nh:? ? <modification> e.g. gray no seihuku ?uniform?
8. Nm:?, Nh:quantity ? <obligatory case(attribute)> e.g. hei ?wall? no takasa ?height?
9. Nm:?, Nh:position ? <obligatory case(position)> e.g. tsukue ?desk? no migi ?right?
10. Nm:agent, Nh:? ? <possession> e.g. watashi ?I? no kuruma ?car?
11. Nm:place or position, Nh:? ? <place> e.g. Kyoto no mise ?store?
??? meets any noun.
In case of the phrase ?rugby no coach?, ?tech-
nique? and ?sport? in the definition sentences
are checked: the similarity between ?technique?
and ?rugby? is calculated to be 0.21, and the
similarity between ?sport? and ?rugby? is cal-
culated to be 1.0. Therefore, DBA outputs
?sport?.
3.2 Semantic feature-based analysis
Since diverse relations in ?Nm no Nh? are han-
dled by DBA, the remaining relations can be
detected by simple rules checking the semantic
features of Nm and/or Nh.
Table 1 shows examples of the rules. For ex-
ample, the rule 1 means that if Nm has a seman-
tic feature human and Nh relative, <obliga-
tory case> relation is assigned to the phrase.
The rules 1, 2, 8 and 9 are for certain oblig-
atory cases. We use these rules because these
relations can be analyzed more accurately by us-
ing explicit semantic features, rather than based
on a dictionary.
3.3 Integration of two analyses
Usually, either DBA or SBA outputs some re-
lation. When both DBA and SBA output some
relations, the results are integrated (basically, if
DBA correspondence score is higher than 0.8,
DBA result is selected; if not, SBA result is se-
lected). In rare cases, neither analysis outputs
any relations, which means analysis failure.
4 Automatic Construction of
Nominal Case Frames
4.1 Collection and analysis of Nm no Nh
Syntactically unambiguous noun phrases ?Nm
no Nh? are collected from the automatic parse
results of large corpora, and they are analyzed
using the method described in the previous sec-
tion.
Table 2: Preliminary case frames for hisashi
?eaves/visor?.
DBA result
1. a roof that stick out above the window of
a house.
[house] hall:2, balcony:1, building:1, ? ? ?
[window] window:2, ceiling:1, counter:1, ? ? ?
2. the fore piece of a cap.
[cap] cap:8, helmet:1, ? ? ?
SBA result
<place> parking:3, store:3, shop:2, ? ? ?
<mod.> concrete:1, metal:1, silver:1, ? ? ?
No semantic analysis result
<other> part:1, light:1, phone:1, ? ? ?
By just collecting the analysis results of each
head word Nh, we can obtain its preliminary
case frames. Table 2 shows preliminary case
frames for hisashi ?eaves/visor?. The upper part
of the table shows the results by DBA. The line
starting with ?[house]? denotes a group of anal-
ysis results corresponding to the word ?house?
in the first definition sentence. For example,
?hall no hisashi? occurs twice in the corpora,
and they were analyzed by DBA to correspond
to ?house.?
The middle part of the table shows the results
by SBA. Noun phrases that have no semantic
analysis result (analysis failure) are bundled and
named <other>, as shown in the last part of the
table.
A case frame should be constructed for each
meaning (definition) of Nh, and groups start-
ing with ?[...]? or ?<...>? in Table 2 are possi-
ble case slots. The problem is how to arrange
the analysis results of DBA and SBA and how
to distinguish obligatory cases and others. The
following sections explain how to handle these
problems.
Table 3: Threshold to select obligatory slots.
type of case slots threshold of probability
analyzed by DBA 0.5% (1/200)
<obligatory case> 2.5% (1/40)
<belonging> 2.5% (1/40)
<possessive> 5% (1/20)
<agent> 5% (1/20)
<place> 5% (1/20)
<other> 10% (1/10)
<modification> not used
<time> not used
Probability = (# of Nm no Nh) / (# of Nh)
4.2 Case slot clustering
One obligatory case might be separated in pre-
liminary case frames, since the definition sen-
tence is sometimes too specific or too detailed.
For example, in the case of hisashi ?eaves/visor?
in Table 2, [house], [window], and <place>
have very similar examples that mean building
or part of building. Therefore, case slots are
merged if similarity of two case slots is more
than 0.5 (case slots in different definition sen-
tences are not merged in any case). Similarity
of two case slots is the average of top 25% sim-
ilarities of all possible pairs of examples.
In the case of Table 2, the similarity between
[house] and [window] is 0.80, and that between
[house] and <place> is 0.67, so that these three
case slots are merged into one case slot.
4.3 Obligatory case selection
Preliminary case frames contain both obliga-
tory cases and optional cases for the head word.
Since we can expect that an obligatory case
co-occurs with the head word in the form of
noun phrase frequently, we can take frequent
case slots as obligatory case of the head word.
However, we have to be careful to set up
the frequency thresholds, because case slots de-
tected by DBA or <obligatory case> by SBA
are more likely to be obligatory; on the other
hand case slots of <modification> or <time>
should be always optional. Considering these
tendencies, we set thresholds for obligatory
cases as shown in Table 3.
In the case of hisashi ?eaves/visor? in Table 2,
[house-window]-<place> slot and [cap] slot are
chosen as the obligatory cases.
4.4 Case frame construction for each
meaning
Case slots that are derived from each definition
sentence constitute a case frame.
If a case slot of <obligatory case> by SBA
or <other> is not merged into case slots in def-
inition sentences, it can be considered that it
indicates a meaning of Nh which is not covered
in the dictionary. Therefore, such a case slot
constitutes an independent case frame.
On the other hand, when other case slots by
SBA such as <belonging> and <possessive>
are remaining, we have to treat them differently.
The reason why they are remaining is that they
are not always described in the definition sen-
tences, but their frequent occurrences indicate
they are obligatory cases. Therefore, we add
these case slots to the case frames derived from
definition sentences.
Table 4 shows several examples of resul-
tant case frames. Hyoujou ?expression? has a
case frame containing two case slots. Hisashi
?eaves/visor? has two case frames according to
the two definition sentences. In case of hiki-
dashi ?drawer?, the first case frame corresponds
to the definition given in the dictionary, and
the second case frame was constructed from the
<other> case slot, which is actually another
sense of hikidashi, missed in the dictionary. In
case of coach, <possessive> is added to the case
frame which was made from the definition, pro-
ducing a reasonable case frame for the word.
4.5 Point of nominal case frame
construction
The point of our method is the integrated
use of a dictionary and example phrases from
large corpora. Although dictionary definition
sentences are informative resource to indicate
obligatory cases of nouns, it is difficult to do
indirect anaphora resolution by using a dictio-
nary as it is, because all nouns in a definition
sentence are not an obligatory case, and only
the frequency information of noun phrases tells
us which is the obligatory case. Furthermore,
sometimes a definition is too specific or detailed,
and the example phrases can adjust it properly,
as in the example of hisashi in Table 2.
On the other hand, a simple method that
just collects and clusters ?Nm no Nh? phrases
(based on some similarity measure of nouns)
can not construct comprehensive nominal case
frames, because of polysemy and multiple oblig-
atory cases. We can see that dictionary defini-
tion can guide the clustering properly even for
such difficult cases.
Table 4: Examples of nominal case frames.
case slot examples
hisashi :1 ?eaves/visor? (the edges of a roof that stick out above the window of a house etc.)
[house, window] parking, store, hall, ? ? ?
hisashi :2 ?eaves/visor? (the fore piece of a cap.)
[cap] cap, helmet, ? ? ?
hyoujou ?expression? (to express one?s feelings on the face or by gestures.)
[one] people, person, citizen, ? ? ?
[feelings] relief, margin, ? ? ?
hikidashi :1 ?drawer? (a boxlike container in a desk or a chest.)
[desk, chest] desk, chest, dresser, ? ? ?
hikidashi :2 ?drawer? <other> credit, fund, saving, ? ? ?
coach (a person who teaches technique in some sport.)
[sport] baseball, swimming, ? ? ?
<belonging> team, club, ? ? ?
kabushiki ?stock? (the total value of a company?s shares.)
[company] company, corporation, ? ? ?
5 Indirect Anaphora Resolution
To examine the practical usefulness of the con-
structed nominal case frames, we built a pre-
liminary system of indirect anaphora resolution
based on the case frames.
An input sentence is parsed using the
Japanese parser, KNP (Kurohashi and Nagao,
1994). Then, from the beginning of the sen-
tence, each noun x is analyzed. When x has
more than one case frame, the process of an-
tecedent estimation (stated in the next para-
graph) is performed for each case frame, and the
case frame with the highest similarity score (de-
scribed below) and assignments of antecedents
to the case frame are selected as a final result.
For each case slot of the target case frame of
x, its antecedent is estimated. A possible an-
tecedent y in the target sentence and the previ-
ous two sentences is checked. This is done one
by one, from the syntactically closer y. If the
similarity of y to the case slot is equal to or
greater than a threshold ? (currently 0.95), it
is assigned to the case slot.
The similarity between y and a case slot is
defined as the highest similarity between y and
an example in the case slot.
For instance, let us consider the sentence
shown in Figure 1. soccer, at the beginning of
the sentence, has no case frame, and is consid-
ered to have no obligatory case.
For the second noun ticket, soccer, which is
a nominal modifier of ticket, is examined first.
The similarity between soccer and the examples
of the case slot [theater, transport] exceeds the
soccer-no
ticket-ga
takai
nedan-de
urareteita.
expensive
price
be sold
case slot examples result
ticket [theater, transport] stage, game,? ? ? soccer
nedan [things] thing, ticket,? ? ? ticket
ticket a printed piece of paper which shows that you have
paid to enter a theater or use a transport
nedan the amount of money for which things are sold or
bought
Figure 1: Indirect anaphora resolution example.
threshold ?, and soccer is assigned to [theater,
transport].
Lastly, for nedan ?price?, its possible an-
tecedents are ticket and soccer. ticket, which
is the closest from nedan, is checked first. The
similarity between ticket and the examples of
the case slot [things] exceeds the threshold ?,
and ticket is judged as the antecedent of nedan.
6 Experiments
We evaluated the automatically constructed
nominal case frames, and conducted an experi-
ment of indirect anaphora resolution.
6.1 Evaluation of case frames
We constructed nominal case frames from news-
paper articles in 25 years (12 years of Mainichi
newspaper and 13 years of Nihonkeizai newspa-
per). These newspaper corpora consist of about
Table 5: Evaluation result of case frames.
precision recall F
58/70 (0.829) 58/68 (0.853) 0.841
25,000,000 sentences, and 10,000,000 ?Nm no
Nh? noun phrases were extracted from them.
The result consists of 17,000 nouns, the average
number of case frames for a noun is 1.06, and
the average number of case slots for a case frame
is 1.09.
We randomly selected 100 nouns that occur
more than 10,000 times in the corpora, and cre-
ated gold standard case frames by hand. For
each test noun, possible case frames were con-
sidered, and for each case frame, obligatory case
slots were given manually. As a result, 68 case
frames for 65 test nouns were created, and 35
test nouns have no case frames.
We evaluated automatically constructed case
frames for these test nouns against the gold
standard case frames. A case frame which has
the same case slots with the gold standard is
judged as correct. The evaluation result is
shown in Table 5: the system output 70 case
frames, and out of them, 58 case frames were
judged as correct.
The recall was deteriorated by the highly re-
stricted conditions in the example collection.
For instance, maker does not have obligatory
case slot for its products. This is because maker
is usually used in the form of compound noun
phrase, ?products maker?, and there are few
occurrences of ?products no maker?. To ad-
dress this problem, not only ?Nm no Nh? but
also ?Nm Nh? (compound noun phrase) and
?Nm ni-kansuru ?in terms of? Nh? should be
collected.
6.2 Experimental results of indirect
anaphora resolution
We conducted a preliminary experiment of
our indirect anaphora resolution system using
?Relevance-tagged corpus? (Kawahara et al,
2002). This corpus consists of Japanese news-
paper articles, and has relevance tags, including
antecedents of indirect anaphors.
We prepared a small test corpus that con-
sists of randomly selected 10 articles. The test
corpus contains 217 nouns. Out of them, 106
nouns are indirect anaphors, and have 108 an-
tecedents, which is because two nouns have dou-
ble antecedents. 49 antecedents directly depend
on their anaphors, and 59 do not. For 91 an-
tecedents out of 108, a case frame of its anaphor
Table 6: Experimental results of indirect
anaphora resolution.
precision recall F
w dep. 40/46 (0.870) 40/59 (0.678) 0.762
w/o dep. 31/61 (0.508) 31/49 (0.633) 0.564
total 71/107 (0.664) 71/108 (0.657) 0.660
includes the antecedent itself or its similar word
(the similarity exceeds the threshold, 0.95). Ac-
cordingly, the upper bound of the recall of our
case-frame-based anaphora resolution is 84.3%
(91/108).
We ran the system on the test corpus, and
compared the system output and the corpus an-
notation. Table 6 shows the experimental re-
sults. In this table, ?w dep.? (with dependency)
is the evaluation of the antecedents that directly
depend on their anaphors. ?w/o dep.? (with-
out dependency) is the case of the antecedents
that do not directly depend on their anaphors.
Although the analysis of ?w dep.? is intrinsi-
cally easier than that of ?w/o dep.?, the recall
of ?w dep.? was not much higher than that
of ?w/o dep.?. The low recall score of ?w dep.?
was caused by nonexistence of case frames which
include the antecedent itself or its similar word.
The antecedents that directly depend on their
anaphors were often a part of compound noun
phrases, such as ?products maker?, which are
not covered by our examples collection.
Major errors in the analyses of the an-
tecedents that do not directly depend on their
anaphors were caused by the following reasons.
Specific/generic usages of nouns
Some erroneous system outputs were caused by
nouns that have both specific and generic us-
ages.
(3) kogaisya-no
subsidiary
kabushiki-wo
stock
baikyaku-shita.
sell
(? sold the stock of the subsidiary.)
In this case, kogaisya ?subsidiary? is an oblig-
atory information for kabushiki ?stock?, which is
specifically used. kogaisya matches the [kaisya
?company?] case slot in Table 4.
However, kabushiki ?stock? in the following ex-
ample is used generically, and does not need spe-
cific company information.
(4)kabushiki
stock
souba-no
price
oshiage
rise
youin-to naru.
factor become
(? become the rise factor of the stock prices.)
Since the current system cannot judge generic
or specific nouns, an antecedent which corre-
sponds to [kaisha ?company?] is incorrectly esti-
mated.
Beyond selectional restriction of case
frames
Selectional restriction based on the case frames
usually worked well, but did not work to distin-
guish candidates both of which belong to Hu-
man or Organization.
(5) Bush bei
American
seiken-wa
administration
Russia-tono
... Bush daitouryou-ga
president
shutyou-shita.
claim
(Bush American administration ... with
Russia ... President Bush claimed ...)
In this example, daitouryou ?president? re-
quires an obligatory case kuni ?nation?. The sys-
tem estimates its antecedent as Russia, though
the correct answer is bei ?America?. This is be-
cause Russia is closer than beikoku. This prob-
lem is somehow related to world knowledge, but
if the system can carefully exploit the context,
it might be able to find the correct answer from
?Bush bei seiken? ?Bush American administra-
tion?.
7 Conclusion
This paper has first proposed an automatic
construction method of Japanese nominal case
frames. This method is based on semantic anal-
ysis of noun phrases ?Nm no Nh? ?Nh of Nm?.
To examine the practical usefulness of the con-
structed nominal case frames, we built a pre-
liminary system of indirect anaphora resolution
based on the case frames. The evaluation indi-
cated the good quality of the constructed case
frames. On the other hand, the accuracy of our
indirect anaphora resolution system is not satis-
factory. In the future, we are planning to make
the case frames more wide-coverage, and im-
prove the indirect anaphora resolution by con-
sidering larger context and more various factors.
References
Ted Briscoe and John Carroll. 1997. Auto-
matic extraction of subcategorization from
corpora. In Proceedings of the 5th Confer-
ence on Applied Natural Language Process-
ing, pages 356?363.
Udo Hahn, Michael Strube, and Katja Markert.
1996. Bridging textual ellipses. In Proceed-
ings of the 16th International Conference on
Computational Linguistics, pages 496?501.
Daisuke Kawahara and Sadao Kurohashi. 2002.
Fertilization of case frame dictionary for ro-
bust Japanese case analysis. In Proceedings of
the 19th International Conference on Compu-
tational Linguistics, pages 425?431.
Daisuke Kawahara and Sadao Kurohashi. 2004.
Zero pronoun resolution based on automati-
cally constructed case frames and structural
preference of antecedents. In Proceedings of
the 1st International Joint Conference on
Natural Language Processing.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of
the 3rd International Conference on Lan-
guage Resources and Evaluation, pages 2008?
2013.
Sadao Kurohashi and Makoto Nagao. 1994. A
syntactic analysis method of long Japanese
sentences based on the detection of conjunc-
tive structures. Computational Linguistics,
20(4):507?534.
Sadao Kurohashi and Yasuyuki Sakai. 1999.
Semantic analysis of Japanese noun phrases:
A new approach to dictionary-based under-
standing. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, pages 481?488.
Masaki Murata, Hitoshi Isahara, and Makoto
Nagao. 1999. Pronoun resolution in Japanese
sentences using surface expressions and exam-
ples. In Proceedings of the ACL?99 Workshop
on Coreference and Its Applications, pages
39?46.
Massimo Poesio, Tomonori Ishikawa,
Sabine Schulte im Walde, and Renata
Vieira. 2002. Acquiring lexical knowledge for
anaphora resolution. In Proceedings of the
3rd International Conference on Language
Resources and Evaluation, pages 1220?1224.
Michael Strube and Udo Hahn. 1999. Func-
tional centering ? grounding referential coher-
ence in information structure. Computational
Linguistics, 25(3):309?344.
Jun-ichi Tajika, editor. 1997. Reikai Syogaku
Kokugojiten. Sanseido.
Renata Vieira and Massimo Poesio. 2000. An
empirically based system for processing defi-
nite descriptions. Computational Linguistics,
26(4):539?592.
PP-Attachment Disambiguation Boosted by
a Gigantic Volume of Unambiguous Examples
Daisuke Kawahara and Sadao Kurohashi
Graduate School of Information Science and Technology, University of Tokyo,
7-3-1 Hongo Bunkyo-ku, Tokyo, 113-8656, Japan
{kawahara, kuro}@kc.t.u-tokyo.ac.jp
Abstract. We present a PP-attachment disambiguation method based
on a gigantic volume of unambiguous examples extracted from raw cor-
pus. The unambiguous examples are utilized to acquire precise lexical
preferences for PP-attachment disambiguation. Attachment decisions are
made by a machine learning method that optimizes the use of the lexical
preferences. Our experiments indicate that the precise lexical preferences
work effectively.
1 Introduction
For natural language processing (NLP), resolving various ambiguities is a fun-
damental and important issue. Prepositional phrase (PP) attachment ambigu-
ity is one of the structural ambiguities. Consider, for example, the following
sentences [1]:
(1) a. Mary ate the salad with a fork.
b. Mary ate the salad with croutons.
The prepositional phrase in (1a) ?with a fork? modifies the verb ?ate?, because
?with a fork? describes how the salad is eaten. The prepositional phrase in (1b)
?with croutons? modifies the noun ?the salad?, because ?with croutons? de-
scribes the salad. To disambiguate such PP-attachment ambiguity, some kind of
world knowledge is required. However, it is currently difficult to give such world
knowledge to computers, and this situation makes PP-attachment disambigua-
tion difficult. Recent state-of-the-art parsers perform with the practical accuracy,
but seem to suffer from the PP-attachment ambiguity [2, 3].
For NLP tasks including PP-attachment disambiguation, corpus-based ap-
proaches have been the dominant paradigm in recent years. They can be divided
into two classes: supervised and unsupervised. Supervised methods automati-
cally learn rules from tagged data, and achieve good performance for many NLP
tasks, especially when lexical information, such as words, is given. Such methods,
however, cannot avoid the sparse data problem. This is because tagged data are
not sufficient enough to discriminate a large variety of lexical information. To
deal with this problem, many smoothing techniques have been proposed.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 188?198, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
PP-Attachment Disambiguation Boosted by Unambiguous Examples 189
The other class for corpus-based approaches is unsupervised learning. Unsu-
pervised methods take advantage of a large number of data that are extracted
from large raw corpora, and thus can alleviate the sparse data problem. How-
ever, the problem is their low performance compared with supervised methods,
because of the use of unreliable information.
For PP-attachment disambiguation, both supervised and unsupervised meth-
ods have been proposed, and supervised methods have achieved better perfor-
mance (e.g., 86.5% accuracy by [1]). Previous unsupervised methods tried to ex-
tract reliable information from large raw corpora, but the extraction heuristics
seem to be inaccurate [4, 5]. For example, Ratnaparkhi extracted unambiguous
word triples of (verb, preposition, noun) or (noun, preposition, noun), and re-
ported that their accuracy was 69% [4]. This means that the extracted triples
are not truly unambiguous, and this inaccurate treatment may have led to low
PP-attachment performance (81.9%).
This paper proposes a PP-attachment disambiguation method based on an
enormous amount of truly unambiguous examples. The unambiguous examples
are extracted from raw corpus using some heuristics inspired by the following
example sentences in [6]:
(2) a. She sent him into the nursery to gather up his toys.
b. The road to London is long and winding.
In these sentences, the underlined PPs are unambiguously attached to the
double-underlined verb or noun. The extracted unambiguous examples are uti-
lized to acquire precise lexical preferences for PP-attachment disambiguation.
Attachment decisions are made by a machine learning technique that optimizes
the use of the lexical preferences. The point of our work is to use a ?gigantic?
volume of ?truly? unambiguous examples. The use of only truly unambiguous
examples leads to statistics of high-quality and good performance of disambigua-
tion in spite of the learning from raw corpus. Furthermore, by using a gigantic
volume of data, we can alleviate the influence of the sparse data problem.
The remainder of this paper is organized as follows. Section 2 briefly describes
the globally used training and test set of PP-attachment. Section 3 summarizes
previous work for PP-attachment. Section 4 describes a method of calculating
lexical preference statistics from a gigantic volume of unambiguous examples.
Section 5 is devoted to our PP-attachment disambiguation algorithm. Section
6 presents the experiments of our disambiguation method. Section 7 gives the
conclusions.
2 Tagged Data for PP-Attachment
The PP-attachment data with correct attachment site are available 1. These data
were extracted from Penn Treebank [7] by the IBM research group [8]. Hereafter,
we call these data ?IBM data?. Some examples in the IBM data are shown
in Table 1.
1 Available at ftp://ftp.cis.upenn.edu/pub/adwait/PPattachData/
190 D. Kawahara and S. Kurohashi
Table 1. Some Examples of the IBM data
v n1 p n2 attach
join board as director V
is chairman of N.V. N
using crocidolite in filters V
bring attention to problem V
is asbestos in product N
making paper for filters N
including three with cancer N
Table 2. Various Baselines and Upper Bounds of PP-Attachment Disambiguation
method accuracy
always N 59.0%
N if p is ?of?; otherwise V 70.4%
most likely for each preposition 72.2%
average human (only quadruple) 88.2%
average human (whole sentence) 93.2%
The data consist of 20,801 training and 3,097 test tuples. In addition, a de-
velopment set of 4,039 tuples is provided. Various baselines and upper bounds of
PP-Attachment disambiguation are shown in Table 2. All the accuracies except
the human performances are on the IBM data. The human performances were
reported by [8].
3 Related Work
There have been lots of supervised approaches for PP-attachment disambigua-
tion. Most of them used the IBM data for their training and test data.
Ratnaphakhi et al proposed a maximum entropy model considering words
and semantic classes of quadruples, and performed with 81.6% accuracy [8].
Brill and Resnik presented a transformation-based learning method [9]. They
reported 81.8% accuracy, but they did not use the IBM data 2. Collins and
Brooks used a probabilistic model with backing-off to smooth the probabili-
ties of unseen events, and its accuracy was 84.5% [10]. Stetina and Nagao used
decision trees combined with a semantic dictionary [11]. They achieved 88.1%
accuracy, which is approaching the human accuracy of 88.2%. This great per-
formance is presumably indebted to the manually constructed semantic dictio-
nary, which can be regarded as a part of world knowledge. Zavrel et al em-
ployed a nearest-neighbor method, and its accuracy was 84.4% [12]. Abney et
al. proposed a boosting approach, and yielded 84.6% accuracy [13]. Vanschoen-
winkel and Manderick introduced a kernel method into PP-attachment disam-
2 The accuracy on the IBM data was 81.9% [10].
PP-Attachment Disambiguation Boosted by Unambiguous Examples 191
biguation, and attained 84.8% accuracy [14]. Zhao and Lin proposed a nearest-
neighbor method with contextually similar words learned from large raw corpus
[1]. They achieved 86.5% accuracy, which is the best performance among previ-
ous methods for PP-attachment disambiguation without manually constructed
knowledge bases.
There have been several unsupervised methods for PP-attachment disam-
biguation. Hindle and Rooth extracted over 200K (v, n1, p) triples with ambigu-
ous attachment sites from 13M words of AP news stories [15]. Their disambigua-
tion method used lexical association score, and performed at 75.8% accuracy on
their own data set. Ratnaparkhi collected 910K unique unambiguous triples (v,
p, n2) or (n1, p, n2) from 970K sentences of Wall Street Journal, and pro-
posed a probabilistic model based on cooccurrence values calculated from the
collected data [4]. He reported 81.9% accuracy. As previously mentioned, the
accuracy was possibly lowered by the inaccurate (69% accuracy) extracted ex-
amples. Pantel and Lin extracted ambiguous 8,900K quadruples and unambigu-
ous 4,400K triples from 125M word newspaper corpus [5]. They utilized scores
based on cooccurrence values, and resulted in 84.3% accuracy. The accuracy of
the extracted unambiguous triples are unknown, but depends on the accuracy of
their parser.
There is a combined method of supervised and unsupervised approaches.
Volk combined supervised and unsupervised methods for PP-attachment disam-
biguation for German [16]. He extracted triples that are possibly unambiguous
from 5.5M words of a science magazine corpus, but these triples were not truly
unambiguous. His unsupervised method is based on cooccurrence probabilities
learned from the extracted triples. His supervised method adopted the backed-
off model by Collins and Brooks. This model is learned the model from 5,803
quadruples. Its accuracy on a test set of 4,469 quadruples was 73.98%, and was
boosted to 80.98% by the unsupervised cooccurrence scores. However, his work
was constrained by the availability of only a small tagged corpus, and thus it
is unknown whether such an improvement can be achieved if a larger size of a
tagged set like the IBM data is available.
4 Acquiring Precise Lexical Preferences from Raw
Corpus
We acquire lexical preferences that are useful for PP-attachment disambiguation
from a raw corpus. As such lexical preferences, cooccurrence statistics between
the verb and the prepositional phrase or the noun and the prepositional phrase
are used. These cooccurrence statistics can be obtained from a large raw corpus,
but the simple use of such a raw corpus possibly produces unreliable statistics.
We extract only truly unambiguous examples from a huge raw corpus to acquire
precise preference statistics.
This section first mentions the raw corpus, and then describes how to extract
truly unambiguous examples. Finally, we explain our calculation method of the
lexical preferences.
192 D. Kawahara and S. Kurohashi
4.1 Raw Corpus
In our approach, a large volume of raw corpus is required. We extracted raw
corpus from 200M Web pages that had been collected by a Web crawler for
a month [17]. To obtain the raw corpus, each Web page is processed by the
following tools:
1. sentence extracting
Sentences are extracted from each Web page by a simple HTML parser.
2. tokenizing
Sentences are tokenized by a simple tokenizer.
3. part-of-speech tagging
Tokenized sentences are given part-of-speech tags by Brill tagger [18].
4. chunking
Tagged sentences are chunked by YamCha chunker [19].
By the above procedure, we acquired 1,300M chunked sentences, which con-
sist of 21G words, from the 200M Web pages.
4.2 Extraction of Unambiguous Examples
Unambiguous examples are extracted from the chunked sentences. Our heuristics
to extract truly unambiguous examples were decided in the light of the following
two types of unambiguous examples in [6].
(3) a. She sent him into the nursery to gather up his toys.
b. The road to London is long and winding.
The prepositional phrase ?into the nursery? in (3a) must attach to the verb
?sent?, because attachment to a pronoun like ?him? is not possible. The prepo-
sitional phrase ?to London? in (3b) must attach to the noun ?road?, because
there are no preceding possible heads.
We use the following two heuristics to extract unambiguous examples like
the above.
? To extract an unambiguous triple (v, p, n2) like (3a), a verb followed by a
pronoun and a prepositional phrase is extracted.
? To extract an unambiguous triple (n1, p, n2) like (3b), a noun phrase followed
by a prepositional phrase at the beginning of a sentence is extracted.
4.3 Post-processing of Extracted Examples
The extracted examples are processed in the following way:
? For verbs (v):
? Verbs are reduced to their lemma.
? For nouns (n1, n2):
? 4-digit numbers are replaced with <year>.
PP-Attachment Disambiguation Boosted by Unambiguous Examples 193
? All other strings of numbers were replaced with <num>.
? All words at the beginning of a sentence are converted into lower case.
? All words starting with a capital letter followed by one or more lower
case letters were replaced with <name>.
? All other words are reduced to their singular form.
? For prepositions (p):
? Prepositions are converted into lower case.
As a result, 21M (v, p, n2) triples and 147M (n, p, n2) triples,in total 168M
triples, were acquired.
4.4 Calculation of Lexical Preferences for PP-Attachment
From the extracted truly unambiguous examples, lexical preferences for PP-
attachment are calculated. As the lexical preferences, pointwise mutual informa-
tion between v and ?p n2? is calculated from cooccurrence counts of v and ?p
n2? as follows3:
I(v, pn2) = log
f(v,pn2)
N
f(v)
N
f(pn2)
N
(1)
where N denotes the total number of the extracted examples (168M), f(v) and
f(pn2) is the frequency of v and ?p n2?, respectively, and f(v, pn2) is the cooc-
currence frequency of v and pn2.
Similarly, pointwise mutual information between n1 and ?p n2? is calculated
as follows:
I(n1, pn2) = log
f(n1,pn2)
N
f(n1)
N
f(pn2)
N
(2)
The preference scores ignoring n2 are also calculated:
I(v, p) = log
f(v,p)
N
f(v)
N
f(p)
N
(3)
I(n1, p) = log
f(n1,p)
N
f(n1)
N
f(p)
N
(4)
5 PP-Attachment Disambiguation Method
Our method for resolving PP-attachment ambiguity takes a quadruple (v, n1, p,
n2) as input, and classifies it as V or N. The class V means that the prepositional
3 As in previous work, simple probability ratios can be used, but a preliminary ex-
periment on the development set shows their accuracy is worse than the mutual
information by approximately 1%.
194 D. Kawahara and S. Kurohashi
phrase ?p n2? modifies the verb v. The class N means that the prepositional
phrase modifies the noun n1.
To solve this binary classification task, we employ Support Vector Machines
(SVMs), which have been well-known for their good generalization
performance [20].
We consider the following features:
? LEX: word of each quadruple
To reduce sparse data problems, all verbs and nouns are pre-processed using
the method stated in Section 4.3.
? POS: part-of-speech information of v, n1 and n2
POSs of v, n1 and n2 provide richer information than just verb or noun,
such as inflectional information.
The IBM data, which we use for our experiments, do not contain POS in-
formation. To obtain POS tags of a quadruple, we extracted the original
sentence of each quadruple from Penn Treebank, and applied the Brill tag-
ger to it. Instead of using the correct POS information in Penn Treebank,
we use the POS information automatically generated by the Brill tagger to
keep the experimental environment realistic.
? LP: lexical preferences
Given a quadruple (v, n1, p, n2), four statistics calculated in Section4.4,
I(v, pn2), I(n1, pn2), I(v, p) and I(n1, p), are given to SVMs as features.
6 Experiments and Discussions
We conducted experiments on the IBM data. As an SVM implementation, we em-
ployed SVMlight [21]. To determine parameters of SVMlight, we run our method
on the development data set of the IBM data. As the result, parameter j, which
is used to make much account of training errors on either class [22], is set to
0.65, and 3-degree polynomial kernel is chosen. Table 3 shows the experimen-
tal results for PP-attachment disambiguation. For comparison, we conducted
several experiments with different feature combinations in addition to our pro-
posed method ?LEX+POS+LP?, which uses all of the three types of features.
The proposed method ?LEX+POS+LP? surpassed ?LEX?, which is the stan-
dard supervised model, and furthermore, significantly outperformed all other
Table 3. PP-Attachment Accuracies
LEX POS LP accuracy?
85.34? ?
85.05?
83.73? ?
84.66? ?
86.44? ? ?
87.25
PP-Attachment Disambiguation Boosted by Unambiguous Examples 195
Table 4. Precision and Recall for Each Attachment Site (?LEX+POS+LP? model)
class precision recall
V 1067/1258 (84.82%) 1067/1271 (83.95%)
N 1635/1839 (88.91%) 1635/1826 (89.54%)
Table 5. PP-Attachment Accuracies of Previous Work
method accuracy
our method SVM 87.25%
supervised
Ratnaphakhi et al, 1994 ME 81.6%
Brill and Resnik, 1994 TBL 81.9%
Collins and Brooks, 1995 back-off 84.5%
Zavrel et al, 1997 NN 84.4%
Stetina and Nagao, 1997 DT 88.1%
Abney et al, 1999 boosting 84.6%
Vanschoenwinkel and Manderick, 2003 SVM 84.8%
Zhao and Lin, 2004 NN 86.5%
unsupervised
Ratnaparkhi, 1998 - 81.9%
Pantel and Lin, 2000 - 84.3%
ME: Maximum Entropy, TBL: Transformation-Based Learning,
DT: Decision Tree, NN: Nearest Neighbor
configurations (McNemar?s test; p < 0.05). ?LEX+POS? model was a little
worse than ?LEX?, but ?LEX+POS+LP? was better than ?LEX+LP? (and
also ?POS+LP? was better than ?LP?). From these results, we can see that
?LP? worked effectively, and the combination of ?LEX+POS+LP? was very ef-
fective. Table 4 shows the precision and recall of ?LEX+POS+LP? model for
each class (N and V).
Table 5 shows the accuracies achieved by previous methods. Our performance
is higher than any other previous methods except [11]. The method of Stetina
and Nagao employed a manually constructed sense dictionary, and this conduces
to good performance.
Figure 1 shows the learning curve of ?LEX? and ?LEX+POS+LP? models
while changing the number of tagged data. When using all the training data,
?LEX+POS+LP? was better than ?LEX?by approximately 2%. Under the con-
dition of small data set, ?LEX+POS+LP? was better than ?LEX?by approxi-
mately 5%. In this situation, in particular, the lexical preferences worked more
effectively.
Figure 2 shows the learning curve of ?LEX+POS+LP? model while changing
the number of used unambiguous examples. The accuracy rises rapidly by 10M
unambiguous examples, and then drops once, but after that rises slightly. The
best score 87.28% was achieved when using 77M unambiguous examples.
196 D. Kawahara and S. Kurohashi
 76
 78
 80
 82
 84
 86
 88
 0  5000  10000  15000  20000  25000
Ac
c u
ra
c y
Number of Tagged Data
"LEX+POS+LP"
"LEX"
Fig. 1. Learning Curve of PP-Attachment Disambiguation
 85.2
 85.4
 85.6
 85.8
 86
 86.2
 86.4
 86.6
 86.8
 87
 87.2
 87.4
 0  2e+07  4e+07  6e+07  8e+07  1e+08  1.2e+08  1.4e+08  1.6e+08  1.8e+08
Ac
c u
ra
c y
Number of Used Unambiguous Examples
Fig. 2. Learning Curve of PP-Attachment Disambiguation while changing the number
of used unambiguous examples
7 Conclusions
This paper has presented a corpus-based method for PP-attachment disam-
biguation. Our approach utilizes precise lexical preferences learned from a gi-
gantic volume of truly unambiguous examples in raw corpus. Attachment deci-
sions are made using a machine learning method that incorporates these lexi-
cal preferences. Our experiments indicated that the precise lexical preferences
worked effectively.
PP-Attachment Disambiguation Boosted by Unambiguous Examples 197
In the future, we will investigate useful contextual features for PP-
attachment, because human accuracy improves by around 5% when they see
more than just a quadruple.
Acknowledgements
We would like to thank Prof. Kenjiro Taura for allowing us to use an enormous
volume of Web corpus. We also would like to express our thanks to Tomohide
Shibata for his constructive and fruitful discussions.
References
1. Zhao, S., Lin, D.: A nearest-neighbor method for resolving pp-attachment ambigu-
ity. In: Proceedings of the 1st International Joint Conference on Natural Language
Processing. (2004) 428?434
2. Collins, M.: Head-Driven Statistical Models for Natural Language Parsing. PhD
thesis, University of Pennsylvania (1999)
3. Charniak, E.: A maximum-entropy-inspired parser. In: Proceedings of the 1st
Meeting of the North American Chapter of the Association for Computational
Linguistics. (2000) 132?139
4. Ratnaparkhi, A.: Statistical models for unsupervised prepositional phrase attach-
ment. In: Proceedings of the 17th International Conference on Computational
Linguistics. (1998) 1079?1085
5. Pantel, P., Lin, D.: An unsupervised approach to prepositional phrase attachment
using contextually similar words. In: Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics. (2000) 101?108
6. Manning, C., Schu?tze, H.: Foundations of Statistical Natural Language Processing.
MIT Press (1999)
7. Marcus, M., Santorini, B., Marcinkiewicz, M.: Building a large annotated corpus
of English: the Penn Treebank. Computational Linguistics 19 (1994) 313?330
8. Ratnaparkhi, A., Reynar, J., Roukos, S.: A maximum entropy model for preposi-
tional phrase attachment. In: Proceedings of the ARPA Human Language Tech-
nology Workshop. (1994) 250?255
9. Brill, E., Resnik, P.: A rule-based approach to prepositional phrase attachment
disambiguation. In: Proceedings of the 15th International Conference on Compu-
tational Linguistics. (1994) 1198?1204
10. Collins, M., Brooks, J.: Prepositional phrase attachment through a backed-off
model. In: Proceedings of the 3rd Workhop on Very Large Corpora. (1995) 27?38
11. Stetina, J., Nagao, M.: Corpus based pp attachment ambiguity resolution with a
semantic dictionary. In: Proceedings of the 5th Workhop on Very Large Corpora.
(1997) 66?80
12. Zavrel, J., Daelemans, W., Veenstra, J.: Resolving pp attachment ambiguities
with memory-based learning. In: Proceedings of the Workshop on Computational
Natural Language Learning. (1997) 136?144
13. Abney, S., Schapire, R., Singer, Y.: Boosting applied to tagging and pp attach-
ment. In: Proceedings of 1999 Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Corpora. (1999) 38?45
198 D. Kawahara and S. Kurohashi
14. Vanschoenwinkel, B., Manderick, B.: A weighted polynomial information gain
kernel for resolving pp attachment ambiguities with support vector machines. In:
Proceedings of the 18th International Joint Conference on Artificial Intelligence.
(2003) 133?138
15. Hindle, D., Rooth, M.: Structural ambiguity and lexical relations. Computational
Linguistics 19 (1993) 103?120
16. Volk, M.: Combining unsupervised and supervised methods for pp attachment
disambiguation. In: Proceedings of the 19th International Conference on Compu-
tational Linguistics. (2002) 1065?1071
17. Takahashi, T., Soonsang, H., Taura, K., Yonezawa, A.: World wide web crawler. In:
Poster Proceedings of the 11th International World Wide Web Conference. (2002)
18. Brill, E.: Transformation-based error-driven learning and natural language process-
ing: A case study in part-of-speech tagging. Computational Linguistics 21 (1995)
543?565
19. Kudo, T., Matsumoto, Y.: Chunking with support vector machines. In: Proceed-
ings of the 2nd Meeting of the North American Chapter of the Association for
Computational Linguistics. (2001) 192?199
20. Vapnik, V.: The Nature of Statistical Learning Theory. Springer (1995)
21. Joachims, T.: 11. In: Making Large-Scale Support Vector Machine Learning Prac-
tical, in Advances in Kernel Methods - Support Vector Learning. MIT Press (1999)
169?184
22. Morik, K., Brockhausen, P., Joachims, T.: Combining statistical learning with a
knowledge-based approach ? a case study in intensive care monitoring. In: Proceed-
ings of the 16th International Conference on Machine Learning. (1999) 268?277
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 682 ? 693, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Automatic Acquisition of Basic Katakana Lexicon  
from a Given Corpus 
Toshiaki Nakazawa, Daisuke Kawahara, and Sadao Kurohashi 
University of Tokyo, 7-3-1 Hongo Bunkyo-ku, Tokyo, 113-8656, Japan 
{nakazawa, kawahara, kuro}@kc.t.u-tokyo.ac.jp 
Abstract. Katakana, Japanese phonogram mainly used for loan words, is a 
trou-blemaker in Japanese word segmentation. Since Katakana words are heavily 
domain-dependent and there are many Katakana neologisms, it is almost 
impossible to construct and maintain Katakana word dictionary by hand. This 
paper proposes an automatic segmentation method of Japanese Katakana 
compounds, which makes it possible to construct precise and concise Katakana 
word dictionary automati-cally, given only a medium or large size of Japanese 
corpus of some domain. 
1   Introduction
Handling words properly is very important for Natural Language Processing. Words 
are basic unit to assign syntactic/semantic information manually, basic unit to acquire 
knowledge based on frequencies and co-occurrences, and basic unit to access texts in 
Information Retrieval.  
Languages with explicit word boundaries, like white spaces in English, do not suffer 
from this issue so severely, though it is a bit troublesome to handle compounds and 
hyphenation appropriately. On the other hand, languages without explicit boundaries 
such as Japanese always suffer from this issue.  
Japanese character set and their usage. Here, we briefly explain Japanese character set 
and their usage. Japanese uses about 6,000 ideogram, Kanji characters, 83 phonogram, 
Hiragana, and another 86 phonogram, Katakana.  
Kanji is used for Japanese time-honored nouns (including words imported from 
China ancient times) and stems of verbs and adjectives; Hiragana is used for function 
words such as postpositions and auxiliary verbs, and endings of verbs and adjectives; 
Katakana is used for loan words, mostly from the West, as transliterations.  
Japanese is very active to naturalize loan words. Neologisms in special/technical 
domains are often transliterated into Katakana words without translations, or even if 
there are translations, Katakana transliterations are more commonly used in many 
cases. For example,  , transliteration of ?computer? is more commonly 
used than the translation, (keisanki).  
Even for some time-honored Japanese nouns, both Japanese nouns and 
translitera-tions of their English translations are used together these days, and the use of 
        Automatic Acquisition of Basic Katakana Lexicon from a Given Corpus 683 
translit-erations is increasing, such as  , transliteration of ?desk work? vs. 
 (tsukue shigoto). Furthermore, some Japanese nouns, typically the names of 
animals, plants, and food, which can be written in Kanji or Hiragana, are also written in 
Katakana sometimes [4, 6].  
Word segmentation and Katakana words. Let us go back to the word segmentation 
issue. Japanese word segmentation is performed like this: Japanese words are registered 
into the dictionary; given an input sentence, all possible words embedded in the sentence 
and their connections are checked by looking up the dictionary and some connectivity 
grammar; then the most plausible word sequence is selected. The criteria of selecting the 
best word sequence were simple heuristic rules preferring longer words in earlier times, 
and some cost calculation based on manual rules or using some training data, these days.  
Such a segmentation process is in practice not so difficult for Kanji-Hiragana string. 
First of all, since Kanji words and Hiragana words are fairly stable excepting proper 
nouns, they are most perfectly registered in the dictionary. Then, the orthogonal usage of 
Kanji and Hiragana mentioned above makes the segmentation rather simple, as follows:  
                   
(Kare      wa  daigaku      ni  kayou) 
he     postp.   Univ.    postp.     go 
Kanji compound words can cause a segmentation problem. However, since large 
num-ber of Kanji characters lead fairly sparse space of Kanji words, most Kanji 
compounds can be segmented unambiguously.  
A real troublemaker is Katakana words, which are sometimes very long compounds 
such as 	
?extra vergin olive oil? and 
Automatic Slide Generation Based on Discourse
Structure Analysis
Tomohide Shibata and Sadao Kurohashi
Graduate School of Information Science and Technology, University of Tokyo,
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
Abstract. In this paper, we describe a method of automatically gener-
ating summary slides from a text. The slides are generated by itemizing
topic/non-topic parts that are extracted from the text based on syntac-
tic/case analysis. The indentations of the items are controlled according
to the discourse structure, which is detected by cue phrases, identifica-
tion of word chain and similarity between two sentences. Our experiments
demonstrates generated slides are far easier to read in comparison with
original texts.
1 Introduction
A presentation with slides is so effective to pass information to people in many
situations, such as an academic conference or business. Although some softwares,
such as PowerPoint and Keynote, help us with making presentation slides, it is
still cumbersome to make them from scratch.
Some researchers have developed a method of (semi-)automatically making
presentation slides from a technical paper or a news article [1, 2]. However, input
texts of their systems were supposed to be documents whose structure is anno-
tated: in [1], TEX source and in [2], semantically annotated documents by GDA
tag.
In this paper, we propose a method of automatically generating summary
slides from a raw text. An example of a text is shown in Figure 1 and an example
slide that is generated from the text is shown in Figure 2 (the translated slide
is shown in Figure 3). In a slide, topic/non-topic parts that are extracted from
the original text are itemized and each item is indented based on the discourse
structure of the text. In particular, a big contrast/list structure in the text is an
important clue for producing an easy-to-read slide.
The outline of our procedure is as follows:
1. Inputsentences are processedbyJapanese morphologicalanalyzer,JUMAN[3],
and are parsed by Japanese syntactic analyzer, KNP [4].
2. Each sentence is segmented into clauses and the discourse structure of the
text is analyzed.
3. Topic/non-topic parts are extracted from the text.
4. Summary slides are generated by displaying the topic/non-topic parts based
on the discourse structure.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 754?766, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
{shibata, kuro}@kc.t.u-tokyo.ac.jp
Fig. 1. An example of a text
Fig. 2. An example of a slide
Our method not only helps us with making presentation slides but also creates
a full-automatic presentation. That is to say, input texts are spoken via text-
to-speech engine while presenting automatically generated summary slides. We
call this system ?text-to-presentation?, as illustrated in Figure 4. For the input
of text-to-speech engine, written texts are not appropriate, because unnatural
speech might be produced due to difficult words or long compound nouns, which
are unsuitable for speech synthesis. Therefore, written texts are automatically
converted into spoken texts based on paraphrasing technique [5, 6] and then are
inputted into speech synthesis.
The rest of this paper is organized as follows. Section 2 introduces how to
analyze discourse structure. Section 3 explains how to extract topic/non-topic
parts and Section 4 introduces how to generate a slide. Next, in Section 5, we
describe our implemented system and report the evaluation. Finally, in Section
6, we discuss the related work, and in Section 7, we conclude this paper.
Automatic Slide Generation Based on Discourse Structure Analysis 755
Railway Recovery (1)
? Interruption of the three train services, JR Kobe-line, Hankyu Express Kobe-
line and Hanshin Electric Railway
? 450,000 people per day, 120,000 people per hour at the peak of rush,
had no transportation
? Interruption sections in West Japan Railway Toukaidou Line, Sannyou Line,
Hankyu Takarazuka, Imazu and Itami Line and Kobe-Electric Arima-line
? after the earthquake occurred
? transportation by alternate-bus was provided
? from January 23th, when National Route 2 was opened
? transportation by alternate-bus between Osaka and Kobe was pro-
vided
? from January 28th
? the alternate-bus priority lane was set up and smooth transporta-
tion was maintained.
Fig. 3. An example of a slide (in English)
	
	   
      	   	 		    	 
	


Fig. 4. Overview of text-to-presentation system
2 Discourse Structure Analysis
2.1 Model of Discourse Structure
We consider a clause and a sentence as a discourse unit and take into account
the following two types of coherence relations:
1. coherence relations between two clauses in a sentence (four types)
list, contrast, additive, adversative
2. coherence relations between two sentences (eight types)
list, contrast, topic-chaining, topic-dominant chaining, elaboration,
reason, cause, example
An example of discourse structure is shown in Figure 5. The arrows mean
connection of clauses/sentences and the labels on the arrows mean coherence
relation.
In our model, as an initial state, the discourse structure has a starting node.
A sentence connecting to the starting node has the coherence relation ?start?,
which means this sentence is the beginning of a new topic.
756 T. Shibata and S. Kurohashi
 
     	 







      	   

        	    	   
   
 	 
  	        
 	 
  	
 	 
  	
   	      	  
  	          Lexical Choice via Topic Adaptation for
Paraphrasing Written Language to Spoken
Language
Nobuhiro Kaji1 and Sadao Kurohashi2
1 Institute of Industrial Science, The University of Tokyo,
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan
kaji@tkl.iis.u-tokyo.ac.jp
2 Graduate School of Information Science and Technology,
The University of Tokyo, 7-3-1 Hongo,
Bunkyo-ku, Tokyo 113-8656, Japan
kuro@kc.t.u-tokyo.ac.jp
Abstract. Our research aims at developing a system that paraphrases
written language text to spoken language style. In such a system, it is
important to distinguish between appropriate and inappropriate words
in an input text for spoken language. We call this task lexical choice
for paraphrasing. In this paper, we describe a method of lexical choice
that considers the topic. Basically, our method is based on the word
probabilities in written and spoken language corpora. The novelty of our
method is topic adaptation. In our framework, the corpora are classified
into topic categories, and the probability is estimated using such corpora
that have the same topic as input text. The result of evaluation showed
the effectiveness of topic adaptation.
1 Introduction
Written language is different from spoken language. That difference has various
aspects. For example, spoken language is often ungrammatical, or uses simplified
words rather than difficult ones etc. Among these aspects this paper examines
difficulty. Difficult words are characteristic of written language and are not ap-
propriate for spoken language.
Our research aims at developing a system that paraphrases written language
text into spoken language style. It helps text-to-speech generating natural voice
when the input is in written language. In order to create such a system, the
following procedure is required: (1) the system has to detect inappropriate words
in the input text for spoken language, (2) generate paraphrases of inappropriate
words, and (3) confirm that the generated paraphrases are appropriate. This
paper examines step (1) and (3), which we call lexical choice for paraphrasing
written language to spoken language.
Broadly speaking, lexical choice can be defined as binary classification task:
the input is a word and a system outputs whether it is appropriate for spoken
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 981?992, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
982 N. Kaji and S. Kurohashi
language or not. This definition is valid if we can assume that the word diffi-
culty is independent of such factors as context or listeners. However, we think
such assumption is not always true. One example is business jargon (or technical
term). Generally speaking, business jargon is difficult and inappropriate for spo-
ken language. Notwithstanding, it is often used in business talk. This example
implies that the word difficulty is dependent on the topic of text/talk.
In this paper, we define the input of lexical choice as a word and text where it
occurs (= the topic). Such definition makes it possible for a system to consider
the topic. We think the topic plays an important role in lexical choice, when
dealing with such words that are specific to a certain topic, e.g., business jargon.
Hereafter, those words are called topical words, and others are called non-topical
words. Of course, in addition to the topic, we have to consider other factors such
as listeners and so on. But, the study of such factors lies outside the scope of
this paper.
Based on the above discussion, we describe a method of lexical choice that
considers the topic. Basically, our method is based on the word probabilities in
written and spoken language corpora. It is reasonable to assume that these two
probabilities reflect whether the word is appropriate or not. The novelty of the
method is topic adaptation. In order to adapt to the topic of the input text,
the corpora are classified into topic categories, and the probability is estimated
using such corpora that have the same topic category as the input text. This
process enables us to estimate topic-adapted probability. Our method was evalu-
ated by human judges. Experimental results demonstrated that our method can
accurately deal with topical words.
This paper is organized as follows. Section 2 represents method overview.
Section 3 and Section 4 describe the corpora construction. Section 5 represents
learning lexical choice. Section 6 reports experimental results. Section 7 describes
related works. We conclude this paper in Section 8.
2 Method Overview
Our method uses written and spoken language corpora classified into topic cat-
egories. They are automatically constructed from the WWW. The construction
procedure consists of the following two processes (Figure 1).
1. Style Classification
Web pages are downloaded from the WWW, and are classified into written
and spoken language style. Those pages classified as written/spoken language
are referred as written/spoken language corpus. In this process, we discarded
ambiguous pages that are difficult to classify.
2. Topic Classification
The written and spoken language corpora are classified into 14 topic cate-
gories, such as arts, computers and so on.
Both classification methods are represented in Section 3 and Section 4.
Given an input word and a text where it occurs, it is decided as follows
whether the input word is appropriate or inappropriate for spoken language.
Lexical Choice via Topic Adaptation for Paraphrasing Written Language 983
1. The topic category of the input text is decided by the same method as the
one used to classify Web pages into topic categories.
2. We estimate the probabilities of the input word in the written and spoken
language corpora. We use such corpora that have the same topic as the input
text.
3. Using the two probabilities, we decide whether the input word is appropriate
or not. Section 5 describes this method.
Web Pages
Spoken AmbiguousWritten
Topic classification
Style classification
(Section 3)
(Section 4)
Written
Arts category
Spoken Written
Recreation category
Spoken
.........
Written
Science category
Spoken
Fig. 1. Written and spoken language corpora construction
3 Style Classification
In order to construct written and spoken language corpora classified into topic
categories, first of all, Web pages are classified into written and spoken language
pages (Figure 1). Note that what is called spoken language here is not real
utterance but chat like texts. Although it is not real spoken language, it works
as a good substitute, as some researchers pointed out [2,11].
We follow a method proposed by Kaji et al(2004). Their method classifies
Web pages into three types: (1) written language page, (2) spoken language page,
and (3) ambiguous page. Then, Web pages classified into type (1) or (2) are
used. Ambiguous pages are discarded because classification precision decreases
if such pages are used. This Section summarizes their method. See [11] for detail.
Note that for this method the target language is Japanese, and its procedure is
dependent on Japanese characteristics.
3.1 Basic Idea
Web pages are classified based on interpersonal expressions, which imply an at-
titude of a speaker toward listeners, such as familiarity, politeness, honor or con-
tempt etc. Interpersonal expressions are often used in spoken language, although
984 N. Kaji and S. Kurohashi
not frequently used in written language. For example, when spoken language is
used, one of the most basic situations is face-to-face communication. On the
other hand, such situation hardly happens when written language is used.
Therefore, Web pages containing many interpersonal expressions are classi-
fied as spoken language, and vice versa. Among interpersonal expressions, such
expressions that represent familiarity or politeness are used, because:
? Those two kinds of interpersonal expressions frequently appear in spoken
language,
? They are represented by postpositional particle in Japanese and, therefore,
are easily recognized as such.
Hereafter, interpersonal expression that represents familiarity/politeness is
called familiarity/politeness expression.
3.2 Style Classification Procedure
Web pages are classified into the three types based on the following two ratios:
? Familiarity ratio (F-ratio): ?# of sentences including familiarity expressions?
divided by ?# of all the sentences in the page?.
? Politeness ratio (P-ratio): ?# of sentences including politeness expressions?
divided by ?# of all the sentences in the page?.
The procedure is as follows. First, Web pages are processed by Japanese
morphological analyzer JUMAN3. And then, in order to calculate F-ratio and P-
ratio, sentences which include familiarity or politeness expressions are recognized
in the following manner. A sentence is considered to include the familiarity
expression, if it has one of the following six postpositional particles: ne, yo,
wa, sa, ze, na. A sentence is considered to include the politeness expression, if
it has one of the following four postpositional particles: desu, masu, kudasai,
gozaimasu.
After calculating the two ratios, the page is classified according to the rules
illustrated in Figure 2. If F-ratio and P-ratio are equal to 0, the page is classified
as written language page. If F-ratio is more than 0.2, or if F-ratio is more than
0.1 and P-ratio is more than 0.2, the page is classified as spoken language page.
The other pages are regarded as ambiguous and are discarded.
3.3 The Result
Table 1 shows the number of pages and words (noun, verb, and adjective) in the
corpora constructed from the WWW. About 8,680k pages were downloaded from
the WWW, and 994k/1,338k were classified as written/spoken language. The
rest were classified as ambiguous page and they were discarded. The precision of
this method was reported by Kaji et al(2004). According to their experiment,
the precision was 94%.
3 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman-e.html
Lexical Choice via Topic Adaptation for Paraphrasing Written Language 985
0
0.2
F-ratio
P-ratio0.2
0.1
Written language
Ambiguous
Spoken language
Fig. 2. Style classification rule
Table 1. The size of the written and spoken
language corpora
# of pages # of words
Written language 989k 432M
Spoken language 1,337k 907M
Table 2. The size of the training and test
data
Topic category Training Test
Arts 2,834 150
Business & Economy 5,475 289
Computers & Internet 6,156 325
Education 2,943 155
Entertainment 6,221 328
Government 3,131 165
Health 1,800 95
News 2,888 152
Recreation 4,352 230
Reference 1,099 58
Regional 4,423 233
Science 3,868 204
Social Science 5,410 285
Society & Culture 5,208 275
4 Topic Classification
The written and spoken language corpora are classified into 14 topic categories
(Figure 1). This task is what is called text categorization. We used Support Vec-
tor Machine because it is reported to achieve high performance in this task.The
training data was automatically built from Yahoo! Japan4.
The category provided by Yahoo! Japan have hierarchy structure. For exam-
ple, there are Arts and Music categories, and Music is one of the subcategories
of Arts. We used 14 categories located at the top level of the hierarchy. We
downloaded Web pages categorized in one of the 14 categories. Note that we
did not use Web pages assigned more than one categories. And then, the Web
pages were divided them into 20 segments. One of them was used as the test
data, and the others were used as the training data (Table 2). In the Table, the
4 http://www.yahoo.co.jp/
986 N. Kaji and S. Kurohashi
0
50000
100000
150000
200000
250000
300000
350000
400000
A
r
t
s
B
u
s
i
n
e
s
s
&
E
c
o
n
o
m
y
C
o
m
p
u
t
e
r
s
&
I
n
t
e
r
n
e
t
E
d
u
c
a
t
i
o
n
E
n
t
e
r
t
a
i
n
m
e
n
t
G
o
v
e
r
n
m
e
n
t
H
e
a
l
t
h
N
e
w
s
R
e
c
r
e
a
t
i
o
n
R
e
f
e
r
e
n
c
e
R
e
g
i
o
n
a
l
S
c
i
e
n
c
e
S
o
c
i
a
l
 
S
c
i
e
n
c
e
S
o
c
i
e
t
y
&
C
u
l
t
u
r
e
Topic category
#
 
o
f
 
p
a
g
e
s
Written language page Spoken language page
Fig. 3. The size of written and spoken language corpora in each topic category
first column shows the name of the 14 topic categories. The second/third column
shows the number of pages in the training/test data.
SVM was trained using the training data. In order to build multi-class clas-
sifier, we used One-VS-Rest method. Features of SVM are probabilities of nouns
in a page. Kernel function was linear. After the training, it was applied to the
test data. The macro-averaged accuracy was 86%.
The written and spoken language corpora constructed from the WWW were
classified into 14 categories by SVM. Figure 3 depicts the number of pages in
each category.
5 Learning Lexical Choice
We can now construct the written and spoken language corpora classified into
topic categories. The next step is discrimination between inappropriate and ap-
propriate words for spoken language using the probabilities in written and spoken
language corpora (Section 2). This paper proposes two methods: one is based
on Decision Tree (DT), and the other is based on SVM. This Section first de-
scribes the creation of gold standard data, which is used for both training and
evaluation. Then, we describe the features given to DT and SVM.
5.1 Creation of Gold Standard Data
We prepared data consisting of pairs of a word and binary tag. The tag represents
whether that word is inappropriate or appropriate for spoken language. This data
is referred as gold standard data. Note that the gold standard is created for each
topic category.
Gold standard data of topic T is created as follows.
1. Web pages in topic T are downloaded from Yahoo! Japan, and we sampled
words (verbs, nouns, and adjectives) from those pages at random.
2. Three human judges individually mark each word as INAPPROPRIATE,
APPROPRIATE or NEUTRAL. NEUTRAL tag is used when a judge cannot
mark a word as INAPPROPRIATE or APPROPRIATE with certainty.
Lexical Choice via Topic Adaptation for Paraphrasing Written Language 987
3. The three annotations are combined, and single gold standard data is cre-
ated. A word is marked as INAPPROPRIATE/APPROPRIATE in the gold
standard, if
? All judges agree that it is INAPPROPRIATE/APPROPRIATE, or
? Two judges agree that it is INAPPROPRIATE/APPROPRIATE and
the other marked it as NEUTRAL.
The other words are not used in the gold standard data.
5.2 The Features
Both DT and SVM use the same three features: the word probability in written
language corpus, the word probability in spoken language corpus, and the ratio
of the word probability in spoken language corpus to that in written language.
Note that when DT and SVM are trained on the gold standard of topic T, the
probability is estimated using the corpus in topic T.
6 Evaluation
This Section first reports the gold standard creation. Then, we show that DT and
SVM can successfully classify INAPPROPRIATE and APPROPRIATE words
in the gold standard. Finally, the effect of topic adaptation is represented.
6.1 The Gold Standard Data
The annotation was performed by three human judges (Judge1, Judge2 and
Judge3) on 410 words sampled from Business category, and 445 words sampled
from Health category. Then, we created the gold standard data in each category
(Table 3). The average Kappa value [3] between the judges was 0.60, which
corresponds to substantial agreement.
Table 3. Gold standard data
Business Health
INAPPROPRIATE 49 38
APPROPRIATE 267 340
Total 316 378
Table 4. # of words in Business and Health
categories corpora
Business Health
Written language 29,891k 30,778k
Spoken language 9,018k 32,235k
6.2 Lexical Choice Evaluation
DT and SVM were trained and tested on the gold standard data using Leave-
One-Out (LOO) cross validation. DT and SVM were implemented using C4.55
and TinySVM6 packages. The kernel function of SVM was Gaussian RBF. Table
5 http://www.rulequest.com/Personal/
6 http://chasen.org/ taku/software/TinySVM/
988 N. Kaji and S. Kurohashi
Table 5. The result of LOO cross validation
Topic Method Accuracy # of correct answers Precision Recall
Business DT .915 (289/316) 31 + 258 = 289 .775 .660
SVM .889 (281/316) 21 + 260 = 281 .750 .429
MCB .845 (267/316) 0 + 267 = 267 ? .000
Health DT .918 (347/378) 21 + 326 = 347 .600 .552
SVM .918 (347/378) 13 + 334 = 347 .684 .342
MCB .899 (340/378) 0 + 340 = 340 ? .000
4 shows the number of words in Business and Health categories corpora. Three
features described in Section 5 were used.
The result is summarized in Table 5. For example, in Business category, the
accuracy of DT was 91.5%. 289 out of 316 words were classified successfully, and
the 289 consists of 31 INAPPROPRIATE and 258 APPROPRIATE words. The
last two columns show the precision and recall of INAPPROPRIATE words.
MCB is Majority Class Baseline, which marks every word as APPROPRIATE.
Judging from the accuracy in Health category, one may think that our
method shows only a little improvement over MCB. However, considering other
evaluation measures such as recall of INAPPROPRIATE words, it is obvious
that the proposed method overwhelms MCB. We would like to emphasize the
fact that MCB is not at all practical lexical choice method. If MCB is used, all
words in the input text are regarded as appropriate for spoken language and the
input is never paraphrased.
One problem of our method is that the recall of INAPPROPRIATE words is
low. We think that the reason is as follows. The number of INAPPROPRIATE
words in the gold standard is much smaller than that of APPROPRIATE words.
Hence, we think a system that is biased to classify words as APPROPRIATE
often achieves high accuracy. It is one of future works to improve the recall while
keeping high accuracy.
We examined discrimination rules learned by DT. Figure 4 depicts the rules
learned by DT when the whole gold standard data of Business category is used
as a training data. In the Figure, the horizontal/vertical axis corresponds to the
probability in the written/spoken language corpus. Words in the gold standard
can be mapped into this two dimension space. INAPPROPRIATE/ APPROPRI-
ATE words are represented by a cross/square. The line represents discrimination
rules. Words below the line are classified as INAPPROPRIATE, and the others
are classified as APPROPRIATE.
6.3 Effect of Topic Adaptation
Finally, we investigated the effect of topic adaptation by comparing our method
to a baseline method that does not consider topic.
Our method consists of two steps: (1) mapping from a word to features, and
(2) applying discrimination rules to the features. In the step (1), the probability
is estimated using the written and spoken language corpora in a certain topic
Lexical Choice via Topic Adaptation for Paraphrasing Written Language 989
0
0.0001
0 0.0001 0.0002
Probability in written language
P
r
o
b
a
b
i
l
i
t
y
 
i
n
 
s
p
o
k
e
n
 
l
a
n
g
u
a
g
e
Fig. 4. Decision tree rules in Business cat-
egory
customer
customer
management
management
0
0.0002
0.0004
0 0.0002 0.0004 0.0006 0.0008
Probability in written language
P
r
o
b
a
b
i
l
i
t
y
 
i
n
 
s
p
o
k
e
n
 
l
a
n
g
u
a
g
e
Fig. 5. Examples in Business category
T. In the step (2), discrimination rules are learned by DT using the whole gold
standard data of topic T. We used DT rather than SVM because rules are easy
for humans to understand. On the other hand, the baseline uses the same dis-
crimination rules as our method, but uses the whole written and spoken language
corpora to map a word to features. Hereafter, the two methods are referred as
Proposed and Baseline. Both methods use the same discrimination rules but
map a word to features in a different way. Therefore, there are such words that
are classified as INAPPROPRIATE by Proposed and are classified as AP-
PROPRIATE by Baseline, and vice versa. In the evaluation, we compared the
classification results of such words.
We evaluated the results of topical words and non-topical words separately.
This is because we think Proposed is good at dealing with topical words and
hence we can clearly confirm the effectiveness of topic adaptation. Here, a word is
regarded as topical word in topic T, if its probabilities in the written and spoken
language corpora assigned topic category T are larger than those in the whole
corpora with statistical significance (the 5% level). Otherwise it is regarded as
non-topical word in topic T. As a statistical test log-likelihood ratio [4] was used.
The evaluation procedure is as follows.
1. Web pages in Business category were downloaded from Yahoo! Japan, and
words in those pages were classified by the two methods. If the results of the
two methods disagree, such words were stocked.
2. From the stocked words, we randomly sampled 50 topical words in Business
and 50 non-topical words. Note that we did not use such words that are
contained in the gold standard.
3. Using Web pages in Health category, we also sampled 50 topical words in
Health and 50 non-topical words in the same manner.
4. As a result, 100 topical words and 100 non-topical words were prepared. For
each word, two judges (Judge-A and Judge-B) individually assessed which
method successfully classified the word. Some classification results were dif-
ficult even for human judges to assess. In such cases, the results of the both
methods were regarded as correct.
990 N. Kaji and S. Kurohashi
Table 6 represents the classification accuracy of the 100 topical words. For
example, according to assessment by Judge-A, 75 out of 100 words were classified
successfully by Proposed. Similarly, Table 7 represents the accuracy of the 100
non-topical words. The overall agreement between the two judges according to
the Kappa value was 0.56. We compared the result of the two methods using
McNemar?s test [8], and we found statistically significant difference (the 5%
level) in the results. There was no significant difference in the result of non-
topical words assessed by the Judge-A.
Table 6. Accuracy of topical words classi-
fication
Judge Method Accuracy
Judge-A Proposed 75% (75/100)
Baseline 52% (52/100)
Judge-B Proposed 72% (72/100)
Baseline 53% (53/100)
Table 7. Accuracy of non-topical words
classification
Judge Method Accuracy
Judge-A Proposed 48% (48/100)
Baseline 66% (66/100)
Judge-B Proposed 38% (38/100)
Baseline 78% (78/100)
6.4 Discussion and Future Work
Proposed outperformed Baseline in topical words classification. This result
indicates that the difficulty of topical words depends on the topic and we have
to consider the topic. On the other hand, the result of Proposed was not good
when applied to non-topical words. We think this result is caused by two reasons:
(1) the difficulty of non-topical words is independent of the topic, and (2) Base-
line uses larger corpora than Proposed (see Table 1 and Table 4). Therefore,
we think this result does not deny the effectiveness of topic adaptation. These
results mean that Proposed and Baseline are complementary to each other,
and it is effective to combine the two methods: Proposed/Baseline is applied
to topical/non-topical words. It is obvious from the experimental results that
such combination is effective.
We found that Baseline is prone to classify topical words as inappropriate
and such bias decreases the accuracy. Figure 5 depicts typical examples sam-
pled from topical words in Business. Both judges regarded ?management? and
?customer?7 as appropriate for spoken language in Business topic. The white tri-
angle and diamond in the Figure represent their features when the probability
is estimated using the corpora in Business category. They are located above the
line, which corresponds to discrimination rules, and are successfully classified as
appropriate by Proposed. However, if the probability is estimated using the
whole corpora, the features shift to the black triangle and diamond, and Base-
line wrongly classified the two as inappropriate. In Health category, we could
observe similar examples such as ?lung cancer? or ?metastasis?.
7 Our target language is Japanese. Examples illustrated here are translation of the
original Japanese words.
Lexical Choice via Topic Adaptation for Paraphrasing Written Language 991
These examples can be explained in the following way. Consider topical words
in Business. When the probability is estimated using the whole corpora, it is
influenced by the topic but Business, where topical words in Business are often
inappropriate for spoken language. Therefore, we think that Baseline is biased
to classify topical words as inappropriate.
Besides the lexical choice method addressed in this paper, we proposed lex-
ical paraphrase generation method [10]. Our future direction is to apply these
methods to written language texts and evaluate the output of text-to-speech. So
far, the methods were tested on a small set of reports.
Although the main focus of this paper is lexical paraphrases, we think that
it is also important to deal with structural paraphrases. So far, we implemented
a system that paraphrases compound nouns into nominal phrases. It is our
future work to build a system that generates other kinds of structural para-
phrases.
7 Related Work
Lexical choice has been widely discussed in both paraphrasing and natural lan-
guage generation (NLG). However, to the best of our knowledge, no researches
address topic adaptation. Previous approaches are topic-independent or specific
to only certain topic.
Lexical choice has been one of the central issues in NLG. However, the main
focus is mapping from concepts to words, (e.g., [1]). In NLG, a work by Edmonds
and Hirst is related to our research [5]. They proposed a computational model
that represents the connotation of words.
Some paraphrasing researches focus on lexical choice. Murata and Isahara
addressed paraphrasing written language to spoken language. They used only
probability in spoken language corpus [12]. Kaji et al also discussed paraphras-
ing written language to spoken language, and they used the probabilities in
written and spoken language corpora [11]. On the other hand, Inkpen et al ex-
amined paraphrasing positive and negative text [9]. They used the computational
model proposed by Edmonds and Hirst [5].
The proposed method is based on the probability, which can be considered
as a simple language model. In language model works, many researchers have
discussed topic adaptation in order to precisely estimate the probability of topical
words [6,7,13]. Our work can be regarded as one application of such language
model technique.
8 Conclusion
This paper proposed lexical choice method that considers the topic. The method
utilizes written and spoken language corpora classified into topic categories, and
estimate the word probability that is adapted to the topic of the input text.
From the experimental result we could confirm the effectiveness of topic adap-
tation.
992 N. Kaji and S. Kurohashi
References
1. Berzilay, R., Lee, L.: Bootstrapping Lexical Choice via Multiple-Sequence Align-
ment. Proceedings of EMNLP. (2002) 50?57
2. Bulyko, I., Ostendorf, M., and Stolcke, A.: Getting More Mileage from Web Text
Sources for Conversational Speech Language Modeling using Class-Dependent Mix-
tures. Proceedings of HLT-NAACL (2003) 7?9
3. Carletta, J.: Assessing Agreement on Classification Tasks: The Kappa Statistic.
Computational Linguistics. 22 (2). (1996) 249?255
4. Dunning, T.: Accurate Methods for the Statistics of Surprise and Coincidence.
Computational Linguistics. 19 (1). (1993) 61?74
5. Edmonds, P., Hirst, G.: Near-Synonymy and Lexical Choice. Computational Lin-
guistics. 28 (2). (2002) 105?144
6. Florian, R., Yarowsky, D.: Dynamic Nonlocal Language Modeling via Hierarchical
Topic-Based Adaptation: Proceedings of ACL. (1999) 167?174
7. Gildea, D., Hofmann, T.; TOPIC-BASED LANGUAGE MODELS USING EM.
Proceedings of EUROSPEECH. (1999) 2167?2170
8. Gillick, L., Cox, S.: Some Statistical Issues in the Comparison of Speech Recogni-
tion Algorithms. Proceedings of ICASSP. (1989) 532?535
9. Inkpen, D., Feiguina, O., and Hirst, G.: Generating more-positive and more-
negative text. Proceedings of AAAI Spring Symposium on Exploring Attitude and
Affect in Text. (2004)
10. Kaji, N., Kawahara, D., Kurohashi, S., and Satoshi, S. : Verb Paraphrase based
on Case Frame Alignment. Proceedings of ACL. (2002) 215?222
11. Kaji, N., Okamoto, M., and Kurohasih, S.: Paraphrasing Predicates from Written
Language to Spoken Language Using the Web. Proceedings of HLT-NAACL. (2004)
241?248
12. Murata, M., Isahara, H.: Automatic Extraction of Differences Between Spoken and
Written Languages, and Automatic Translation from the Written to the Spoken
Language. Proceedings of LREC. (2002)
13. Wu, J., Khudanpur, S.: BUILDING A TOPIC-DEPENDENT MAXIMUM EN-
TROPY MODEL FOR VERY LARGE CORPORA. Proceedings of ICASSP.
(2002) 777?780
Paraphrasing Predicates from Written Language
to Spoken Language Using the Web
Nobuhiro Kaji and Masashi Okamoto and Sadao Kurohashi
Graduate School of Information Science and Technology, the University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
 kaji,okamoto,kuro@kc.t.u-tokyo.ac.jp
Abstract
There are a lot of differences between expres-
sions used in written language and spoken lan-
guage. It is one of the reasons why speech syn-
thesis applications are prone to produce unnat-
ural speech. This paper represents a method
of paraphrasing unsuitable expressions for spo-
ken language into suitable ones. Those two
expressions can be distinguished based on the
occurrence probability in written and spoken
language corpora which are automatically col-
lected from the Web. Experimental results indi-
cated the effectiveness of our method. The pre-
cision of the collected corpora was 94%, and
the accuracy of learning paraphrases was 76 %.
1 Introduction
Information can be provided in various forms, and one of
them is speech form. Speech form is familiar to humans,
and can convey information effectively (Nadamoto et al,
2001; Hayashi et al, 1999). However, little electronic
information is provided in speech form so far. On the
other hand, there is a lot of information in text form, and
it can be transformed into speech by a speech synthesis.
Therefore, a lot of attention has been given to applications
which uses speech synthesis, for example (Fukuhara et
al., 2001).
In order to enhance such applications, two problems
need to be resolved. The first is that current speech syn-
thesis technology is still insufficient and many applica-
tions often produce speech with unnatural accents and in-
tonations. The second one is that there are a lot of differ-
ences between expressions used in written language and
spoken language. For example, Ohishi indicated that dif-
ficult words and compound nouns are more often used in
written language than in spoken language (Ohishi, 1970).
Therefore, the applications are prone to produce unnatu-
ral speech, if their input is in written language.
Although the first problem is well-known, little atten-
tion has been given to the second one. The reason why the
second problem arises is that the input text contains Un-
suitable Expressions for Spoken language (UES). There-
fore, the problem can be resolved by paraphrasing UES
into Suitable Expression for Spoken language (SES).
This is a new application of paraphrasing. There are no
similar attempts, although a variety of applications have
been discussed so far, for example question-answering
(Lin and Pantel, 2001; Hermjakob et al, 2002; Duclaye
and Yvon, 2003) or text-simplification (Inui et al, 2003).
(1) Written (2) Spoken
(3) Unnatural
Figure 1: Paraphrasing UES into SES
Figure 1 illustrates paraphrasing UES into SES. In the
figure, three types of expressions are shown: (1) expres-
sions used in written language, (2) expressions used in
spoken language, and (3) unnatural expressions. The
overlap between two circles represents expressions used
both in written language and spoken language. UES is
the shaded portion: unnatural expressions, and expres-
sions used only in written language. SES is the non-
shaded portion. The arrows represent paraphrasing UES
into SES, and other paraphrasing is represented by broken
arrows. Paraphrasing unnatural expressions is not consid-
ered, since such expressions are not included in the input
text. The reason why unnatural expressions are taken into
consideration is that paraphrasing into such expressions
should be avoided.
In order to paraphrase UES into SES, this paper pro-
poses a method of learning paraphrase pairs in the form
of ?UES   SES?. The key notion of the method is to
distinguish UES and SES based on the occurrence prob-
ability in written and spoken language corpora which are
automatically collected from the Web. The procedure of
the method is as follows:1
(step 1) Paraphrase pairs of predicates2 are learned from
a dictionary using a method proposed by (Kaji et al,
2002).
(step 2) Written and spoken language corpora are auto-
matically collected from the Web.
(step 3) From the paraphrase pairs learned in step 1,
those in the form of ?UES  SES? are selected using
the corpora.
This paper deals with only paraphrase pairs of predicates,
although UES includes not only predicates but also other
categories such as nouns.
This paper is organized as follows. In Section 2 related
works are illustrated. Section 3 summarizes the method
of Kaji et al In Section 4, we describe the method of
collecting corpora form the Web and report the experi-
mental result. In Section 5, we describe the method of
selecting suitable paraphrases pairs and the experimental
result. Our future work is described in Section 6, and we
conclude in Section 7.
2 Related Work
Paraphrases are different expressions which convey the
same or almost the same meaning. However, there are
few paraphrases that have exactly the same meaning, and
almost all have subtle differences such as style or formal-
ity etc. Such a difference is called a connotational dif-
ference. This paper addresses one of the connotational
differences, that is, the difference of whether an expres-
sion is suitable or unsuitable for spoken language.
Although a large number of studies have been made
on learning paraphrases, for example (Barzilay and Lee,
2003), there are only a few studies which address the con-
notational difference of paraphrases. One of the studies
is a series of works by Edmonds et al and Inkpen et
al (Edmonds and Hirst, 2002; Inkpen and Hirst, 2001).
Edmonds et al proposed a computational model which
represents the connotational difference, and Inkpen et
al. showed that the parameters of the model can be
learned from a synonym dictionary. However, it is doubt-
ful whether the connotational difference between para-
phrases is sufficiently described in such a lexical re-
source. On the other hand, Inui et al discussed read-
1Note that this paper deals with Japanese.
2A predicate is a verb or an adjective.
ability, which is one of the connotational differences,
and proposed a method of learning readability ranking
model of paraphrases from a tagged corpus (Inui and Ya-
mamoto, 2001). The tagged corpus was built as follows:
a large amount of paraphrase pairs were prepared and an-
notators tagged them according to their readability. How-
ever, they focused only on syntactic paraphrases. This
paper deals with lexical paraphrases.
There are several works that try to learn paraphrase
pairs from parallel or comparable corpora (Barzilay and
McKeown, 2001; Shinyama et al, 2002; Barzilay and
Lee, 2003; Pang et al, 2003). In our work, paraphrase
pairs are not learned from corpora but learned from a dic-
tionary. Our corpora are neither parallel nor comparable,
and are used to distinguish UES and SES.
There are several studies that compare two corpora
which have different styles, for example, written and spo-
ken corpora or British and American English corpora,
and try to find expressions unique to either of the styles
(Kilgarriff, 2001). However, those studies did not deal
with paraphrases.
Bulyko et al also collected spoken language corpora
from the Web (Bulyko et al, 2003). The method of Bu-
lyko et al used N-grams in a training corpus and is dif-
ferent from ours (the detail of our method is described in
Section 4).
In respect of automatically collecting corpora which
have a desired style, Tambouratzis et al proposed a
method of dividing Modern Greek corpus into Demokiti
and Katharevoua, which are variations of Modern Greek
(Tambouratzis et al, 2000).
3 Learning Predicate Paraphrase Pairs
Kaji et al proposed a method of paraphrasing predi-
cates using a dictionary (Kaji et al, 2002). For example,
when a definition sentence of ?chiratsuku (to shimmer)?
is ?yowaku hikaru (to shine faintly)?, his method para-
phrases (1a) into (1b).
(1) a. ranpu-ga chiratsuku
a lamp to shimmer
b. ranpu-ga yowaku hikaru
a lamp faintly to shine
As Kaji et al discussed, this dictionary-based paraphras-
ing involves three difficulties: word sense ambiguity, ex-
traction of the appropriate paraphrase from a definition
sentence, transformation of postposition3. In order to
solve those difficulties, he proposed a method based on
case frame alignment.
If paraphrases can be extracted from the definition sen-
tences appropriately, paraphrase pairs can be learned. We
extracted paraphrases from definition sentences using the
3Japanese noun is attached with a postposition.
method of Kaji et al However, it is beyond the scope of
this paper to describe his method as a whole. Instead, we
represent an overview and show examples.
(predicate) (definition sentence)
(2) a. chiratsuku [ kasukani hikaru ]
to shimmer faintly to shine
to shine faintly
b. chokinsuru [ okane-wo tameru ]
to save money money to save
to save money
c. kansensuru byouki-ga [ utsuru ]
to be infected disease to be infected
to be infected with a disease
In almost all cases, a headword of a definition sentence of
a predicate is also a predicate, and the definition sentence
sometimes has adverbs and nouns which modify the head
word. In the examples, headwords are ?hikaru (to shine)?,
?tameru (to save)?, and ?utsuru (to be infected)?. The ad-
verbs are underlined, the nouns are underlined doubly,
paraphrases of the predicates are in brackets. The head-
word and the adverbs can be considered to be always in-
cluded in the paraphrase. On the other hand, the nouns
are not, for example ?money? in (2b) is included but ?dis-
ease? in (2c) is not. It is decided by the method of Kaji et
al. whether they are included or not.
The paraphrase includes one noun at most, and is in
the form of ?adverb noun+ predicate? 4. Hereafter, it
is assumed that a paraphrase pair which is learned is in
the form of ?predicate   adverb noun+ predicate?. The
predicate is called source, the ?adverb noun+ predicate?
is called target.
We used reikai-shougaku-dictionary (Tadika, 1997),
and 5,836 paraphrase pairs were learned. The main prob-
lem dealt with in this paper is to select paraphrase pairs
in the form of ?UES   SES? from those 5,836 ones.
4 Collecting Written and Spoken
Language Corpora from the Web
We distinguish UES and SES (see Figure 1) using the oc-
currence probability in written and spoken language cor-
pora. Therefore, large written and spoken corpora are
necessary. We cannot use existing Japanese spoken lan-
guage corpora, such as (Maekawa et al, 2000; Takezawa
et al, 2002), because they are small.
Our solution is to automatically collect written and
spoken language corpora from the Web. The Web con-
tains various texts in different styles. Such texts as news
articles can be regarded as written language corpora, and
such texts as chat logs can be regarded as spoken lan-
guage corpora. Since we do not need information such as
4
  means zero or more, and + means one or more.
accents or intonations, speech data of real conversations
is not always required.
This papepr proposes a method of collecting written
and spoken language corpora from the Web using inter-
personal expressions (Figure 2). Our method is as fol-
lows. First, a corpus is created by removing useless parts
such as html tags from the Web. It is called Web corpus.
Note that the Web corpus consist of Web pages (hereafter
page). Secondly, the pages are classified into three types
(written language corpus, spoken language corpus, and
ambiguous corpus) based on interpersonal expressions.
And then, only written and spoken language copora are
used, and the ambiguous corpus is abandoned. This is
because:
 Texts in the same page tend to be described in the
same style.
 The boundary between written and spoken language
is not clear even for humans, and it is almost im-
possible to precisely classify all pages into written
language or spoken language.
written 
language corpus
spoken 
language corpus
The Web corpus
.
.
.
.
.
.
.
.
.
ambiguous corpus
pages
Figure 2: Collecting written and spoken language corpora
4.1 Interpersonal expressions
Each page in the Web corpus is classified based on inter-
personal expressions.
Spoken language is often used as a medium of informa-
tion which is directed to a specific listener. For example,
face-to-face communication is one of the typical situa-
tions in which spoken language is used. Due to this fact,
spoken language tends to contain expressions which im-
ply an certain attitude of a speaker toward listeners, such
as familiarity, politeness, honor or contempt etc. Such
an expression is called interpersonal expression. On the
other hand, written language is mostly directed to unspe-
cific readers. For example, written language is often used
in news articles or books or papers etc. Therefore, inter-
personal expressions are not used so frequently in written
language as in spoken language.
Among interpersonal expressions, we utilized familiar-
ity and politeness expressions. The familiarity expression
is one kind of interpersonal expressions, which implies
the speaker?s familiarity toward the listener. It is repre-
sented by a postpositional particle such as ?ne? or ?yo?
etc. The following is an example:
(3) watashi-wa ureshikatta yo
I was happy (familiarity)
I was happy
(3) implies familiarity using the postpositional particle
?yo?.
The politeness expression is also one kind of inter-
personal expressions, which implies politeness to the lis-
tener. It is represented by a postpositional particle. For
example:
(4) watashi-wa eiga-wo mi masu
I a movie to watch (politeness)
I watch a movie
(4) implies politeness using the postpositional particle
?masu?.
Those two interpersonal expressions often appear in
spoken language, and are easily recognized as such by
a morphological analyzer and simple rules. Therefore, a
page in the Web corpus can be classified into the three
types based the following two ratios.
 Familiarity ratio (F-ratio):
# of sentences which include familiarity expressions
# of all the sentences in the page
 Politeness ratio (P-ratio):
# of sentences which include politeness expressions
# of all the sentences in the page.
4.2 Algorithm
After the Web corpus is processed by a Japanese mor-
phological analyzer (JUMAN)5, sentences which include
familiarity or politeness expressions are recognized in the
following manner in order to calculate F-ratio and P-ratio.
If a sentence has one of the following six postpositional
particles, it is considered to include the familiarly expres-
sion.
ne, yo, wa, sa, ze, na
A sentence is considered to include the politeness expres-
sion, if it has one of the following four postpositional par-
ticles.
desu, masu, kudasai, gozaimasu
5http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman-e.html
If F-ratio and P-ratio of a page are very low, the page
is in written language, and vice versa. We observed a
part of the Web corpus, and empirically decided the rules
illustrated in Table 1. If F-ratio and P-ratio are equal to
0, the page is classified as written language. If F-ratio is
more than 0.2, or if F-ratio is more than 0.1 and P-ratio is
more than 0.2, the page is classified as spoken language.
The other pages are regarded as ambiguous.
Table 1: Page classification rules
F-ratio   
  Written languageP-ratio   
F-ratio   
  Spoken languageorF-ratio   
P-ratio   
Otherwise   Ambiguous
4.3 Evaluation
The Web corpus we prepared consists of 660,062 pages
and contains 733M words. Table 2 shows the size of
the written and spoken language corpora which were col-
lected from the Web corpus.
Table 2: The size of the corpora
# of pages # of words
The Web corpus 660,062 733M
Written language corpus 80,685 77M
Spoken language corpus 73,977 113M
Size comparison The reason why written and spo-
ken language corpora were collected from the Web is
that Japanese spoken language corpora available are too
small. As far as we know, the biggest Japanese one
is Spontaneous Speech Corpus of Japanese, which con-
tains 7M words (Maekawa et al, 2000). Our corpus is
about ten times as big as Spontaneous Speech Corpus of
Japanese.
Precision of our method What is important for our
method is not recall but precision. Even if the recall is
not high we can collect large corpora, because the Web
corpus is very huge. However, if the precision is low, it is
impossible to collect corpora with high quality.
240 pages of the written and spoken language cor-
pora were extracted at random, and the precision of our
method was evaluated. The 240 pages consist of 125
pages collected as written language corpus and 115 pages
collected as spoken language corpus. Two judges (here-
after judge 1 and 2) respectively assessed how many of
the 240 pages were classified properly.
The result is shown in Table 3. The judge 1 identified
228 pages as properly classified ones; the judge 2 iden-
tified 221 pages as properly classified ones. The average
precision of the total was 94% (=228+221/240+240) and
we can say that our corpora have sufficient quality.
Table 3: # of pages properly collected
Judge 1 Judge 2
Written language corpus 119/125 110/125
Spoken language corpus 109/115 111/115
Total 228/240 221/240
Discussion Pages which were inappropriately collected
were examined, and it was found that lexical information
is useful in order to properly classify them. (5) is an ex-
ample which means ?A new software is exciting?.
(5) atarashii
new
sohuto-ha
software
wakuwakusuru
exiting
(5) is in spoken language, although it does not include any
familiarity and politeness expressions. This is because of
the word ?wakuwakusuru?, which is informal and means
?exiting?.
On way to deal with such pages is to use words charac-
teristic of written or spoken language. Such words will be
able to be gathered form our written and spoken language
corpora. It is our future work to improve the quality of
our corpora in an iterative way.
5 Paraphrase Pair Selection
A paraphrase pair we want is one in which the source
is UES and the target is SES. From the paraphrase pairs
learned in Section 3, such paraphrase pairs are selected
using the written and spoken language corpora.
Occurrence probabilities (OPs) of expressions in the
written and spoken language corpora can be used to dis-
tinguish UES and SES. This is because:
 An expression is likely to be UES if its OP in spoken
language corpora is very low.
 An expression is likely to be UES, if its OP in writ-
ten language corpora is much higher than that in spo-
ken language corpora.
For example, Table 4 shows OP of ?jikaisuru?. It is a
difficult verb which means ?to admonish oneself?, and
rarely used in a conversation. The verb ?jikaisuru? ap-
peared 14 times in the written language corpus, which
contains 6.1M predicates, and 7 times in the spoken lan-
guage corpus, which contains 11.7M predicates. The OP
of jikaisuru in spoken language corpus is low, compared
Table 4: Occurrence probability of ?jikaisuru?
written language spoken language
corpus corpus
# of jikaisuru 14 7
# of predicates 6.1M 11.7M
OP of jikaisuru 14 6.1M 7 11.7M
with that in written language corpus. Therefore, we can
say that ?jikaisuru? is UES.
The paraphrase pair we want can be selected based on
the following four OPs.
(1) OP of source in the written language corpus
(2) OP of source in the spoken language corpus
(3) OP of target in the written language corpus
(4) OP of target in the spoken language corpus
The selection can be considered as a binary classification
task: paraphrase pairs in which source is UES and target
is SES are treated as positive, and others are negative.
We propose a method based on Support Vector Machine
(Vapnik, 1995). The four OPs above are used as features.
5.1 Feature calculation
The method of calculating OP of an expression  ( 
 ) in a corpus is described. According to the
method, those four features can be calculated. The
method is broken down into two steps: counting the fre-
quency of , and calculation of   using the fre-
quency.
Frequency After a corpus is processed by the Japanese
morphological analyzer (JUMAN) and the parser
(KNP)6, the frequency of e ( ) is counted. Although
the frequency is often obvious from the analysis result,
there are several issues to be discussed.
The frequency of a predicate is sometimes quite differ-
ent from that of the same predicate in the different voice.
Therefore, the same predicates which have different voice
should be treated as different predicates.
As already mentioned in Section 3, the form of source
is ?predicate? and that of target is ?adjective noun+ pred-
icate?. If e is target and contains adverbs and nouns, it is
difficult to count the frequency because of the sparse data
problem. In order to avoid the problem, an approximation
that the adverbs are ignored is used. For example, the fre-
quency of ?run fast? is approximated by that of ?run?. We
did not ignore the noun because of the following reason.
As a noun and a predicate forms an idiomatic phrase more
often than an adverb and a predicate, the meaning of such
idiomatic phrase completely changes without the noun.
6http://www.kc.t.u-tokyo.ac.jp/nl-resource/knp-e.html
If the form of target is ?adverb noun predicate?, the
frequency is approximated by that of ?noun predicate?,
which is counted based on the parse result. However,
generally speaking, the accuracy of Japanese parser is
low compared with that of Japanese morphological an-
alyzer; the former is about 90% while the latter about
99%. Therefore, only reliable part of the parse result is
used in the same way as Kawahara et al did. See (Kawa-
hara and Kurohashi, 2001) for the details. Kawahara et
al. reported that 97% accuracy is achieved in the reliable
part.
Occurrence probability In general,   is defined
as:
      # of expressions in a corpus.
  tends to be small when  contains a noun, because
only a reliable part of the parsed corpus is used to count
 . Therefore, the value of the denominator ?# of ex-
pressions in a corpus? should be changed depending on
whether  contains a noun or not. The occurrence proba-
bility is defined as follows:
if  does not contain any nouns
      # of predicates in a corpus.
otherwise
      # of noun-predicates in a
corpus.
Table 5 illustrates # of predicates and # of noun-
predicates in our corpora.
Table 5: # of predicates, and # of noun-predicates
# of predicates # of noun-predicates
written language corpus 6.1M 1.5M
spoken language corpus 11.7M 1.9M
5.2 Evaluation
The two judges built a data set, and 20-hold cross valida-
tion was used.
Data set 267 paraphrase pairs were extracted at random
form the 5,836 paraphrase pairs learned in section 3. Two
judges independently tagged each of the 267 paraphrase
pairs as positive or negative. Then, only such paraphrase
pairs that were agreed upon by both of them were used as
data set. The data set consists of 200 paraphrase pairs (70
positive pairs and 130 negative pairs).
Experimental result We implemented the system us-
ing Tiny SVM package7.The Kernel function explored
was the polynomial function of degree 2.
Using 20-hold cross validation, two types of feature
sets (F-set1 and F-set2) were evaluated. F-set1 is a fea-
ture set of all the four features, and F-set2 is that of only
two features: OP of source in the spoken language cor-
pus, and OP of target in the spoken language corpus.
The results were evaluated through three measures: ac-
curacy of the classification (positive or negative), preci-
sion of positive paraphrase pairs, and recall of positive
paraphrase pairs. Table 6 shows the result. The accuracy,
precision and recall of F-set1 were 76 %, 70 % and 73 %
respectively. Those of F-set2 were 75 %, 67 %, and 69
%.
Table 6: Accuracy, precision and recall
F-set1 F-set2
Accuracy 76% 75%
Precision 70% 67%
Recall 73% 69%
Table 7 shows examples of classification. The para-
phrase pair (1) is positive example and the paraphrase
pair (2) is negative, and both of them were successfully
classified. The source of (1) appears only 10 times in the
spoken language corpus, on the other hand, the source of
(2) does 67 times.
Discussion It is challenging to detect the connotational
difference between lexical paraphrases, and all the fea-
tures were not explicitly given but estimated using the
corpora which were prepared in the unsupervised man-
ner. Therefore, we think that the accuracy of 76 % is very
high.
The result of F-set1 exceeds that of F-set2. This in-
dicates that comparing   in the written and spoken
language corpus is effective.
Calculated   was occasionally quite far from our
intuition. One example is that of ?kangekisuru?, which is
a very difficult verb that means ?to watch a drama?. Al-
though the verb is rarely used in real spoken language,
its occurrence probability in the spoken language corpus
was very high: the verb appeared 9 times in the writ-
ten language corpus and 69 times in the spoken language
corpus. We examined those corpora, and found that the
spoken language corpus happens to contain a lot of texts
about dramas. Such problems caused by biased topics
will be resolved by collecting corpora form larger Web
corpus.
7http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM/
Table 7: Successfully classified paraphrase pairs
Occurrence probabilities
Paraphrase pair source target
written language spoken language written language spoken language
(1) denraisuru  tsutawaru 43/6.1M 10/11.7M 1,927/6.1M 4,213/11.7Mto descend to be transmitted
(2) hebaru  hetohetoni tsukareru 18/6.1M 67/11.7M 1,026/6.1M 7,829/11.7Mto be tired out to be exhausted
6 Future Work
In order to estimate more reliable features, we are going
to increase the size of our corpora by preparing larger
Web corpus.
Although the paper has discussed paraphrasing from
the point of view that an expression is UES or SES, there
are a variety of SESs such as slang or male/female speech
etc. One of our future work is to examine what kind of
spoken language is suitable for such a kind of application
that was illustrated in the introduction.
This paper has focused only on paraphrasing predi-
cates. However, there are other kinds of paraphrasing
which are necessary in order to paraphrase written lan-
guage text into spoken language. For example, para-
phrasing compound nouns or complex syntactic structure
is the task to be tackled.
7 Conclusion
This paper represented the method of learning paraphrase
pairs in which source is UES and target is SES. The key
notion of the method is to identify UES and SES based
on the occurrence probability in the written and spoken
language corpora which are automatically collected from
the Web. The experimental result indicated that reliable
corpora can be collected sufficiently, and the occurrence
probability calculated from the corpora is useful to iden-
tify UES and SES.
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics, pages 50?57.
Ivan Bulyko, Mari Ostenforf, and Andreas Stolcke.
2003. Getting more mileage from web text sources
for conversational speech language modeling using
class-dependent mixtures. In Proceedings of HLT-
NAACL2003, pages 7?9.
Florence Duclaye and FranC?ois Yvon. 2003. Learn-
ing paraphrases to improve a question-answering sys-
tem. In Proceedings of the 10th Conference of EACL
Workshop Natural Language Processing for Question-
Answering.
Philip Edmonds and Graeme Hirst. 2002. Near-
synonymy and lexical choice. Computational Linguis-
tics, 28(2):105?144.
Tomohiro Fukuhara, Toyoaki Nishida, and Shunsuke Ue-
mura. 2001. Public opinion channel: A system for
augmenting social intelligence of a community. In
Workshop notes of the JSAI-Synsophy International
Conference on Social Intelligence Design, pages 22?
25.
Masaki Hayashi, Hirotada Ueda, Tsuneya Kurihara,
Michiaki Yasumura, Mamoru Douke, and Kyoko
Ariyasu. 1999. Tvml (tv program making language) -
automatic tv program generation from text-based script
-. In ABU Technical Review.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and web exploitation for question answering.
In Proceedings of TREC 2002 Conference.
Diana Zaiu Inkpen and Graeme Hirst. 2001. Building a
lexical knowledge-base of near-synonym differences.
In Proceedings of Workshop on WordNet and Other
Lexical Sources, pages 47?52.
Kentaro Inui and Satomi Yamamoto. 2001. Corpus-
based acquisition of sentence readability ranking mod-
els for deaf people. In Proceedings of NLPRS 2001.
Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, Ryu
Iida, and Tomoya Iwakura. 2003. Text simplification
for reading assistance: A project note. In Proceedings
of the Second International Workshop on Paraphras-
ing, pages 9?16.
Nobuhiro Kaji, Daisuke Kawahara, Sadao Kurohashi,
and Satoshi Sato. 2002. Verb paraphrase based on
case frame alignment. In Proceedings of ACL 2002,
pages 215?222.
Daisuke Kawahara and Sadao Kurohashi. 2001.
Japanese case frame construction by coupling the verb
and its closest case component. In Proceedings of HLT
2001, pages 204?210.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Journal of Natural
Language Engneering, 7(4):343?360.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In Proceedings of LREC 2000, pages 947?
952.
Akiyo Nadamoto, Hiroyuki Kondo, and Katsumi Tanaka.
2001. Webcarousel: Restructuring web search results
for passive viewing in mobile environments. In 7th
International ?Conference on Database Systems for
Advanced Applications, pages 164?165.
Hatsutaroh Ohishi, editor. 1970. Hanashi Ko-
toba(Spoken Language). Bunkacho.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating sentences. In Pro-
ceedings of HLT-NAACL 2003.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of HLT 2002.
Jyunichi Tadika, editor. 1997. Reikai Shougaku Kokugo-
jiten (Japanese dictionary for children). Sanseido.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world. In
Proceedings of LREC 2002, pages 147?152.
George Tambouratzis, Stella Markantonatou, Nikolaos
Hairetakis, Marina Vassiliou, Dimitrios Tambouratzis,
and George Carayannis. 2000. Discriminating the reg-
isters and styles in the modern greek language. In Pro-
ceedings of Workshop on Comparing Corpora 2000.
Vladimir Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 176?183,
New York, June 2006. c?2006 Association for Computational Linguistics
A Fully-Lexicalized Probabilistic Model
for Japanese Syntactic and Case Structure Analysis
Daisuke Kawahara? and Sadao Kurohashi?
Graduate School of Information Science and Technology, University of Tokyo
7-3-1 Hongo Bunkyo-ku, Tokyo, 113-8656, Japan
{kawahara,kuro}@kc.t.u-tokyo.ac.jp
Abstract
We present an integrated probabilistic
model for Japanese syntactic and case
structure analysis. Syntactic and case
structure are simultaneously analyzed
based on wide-coverage case frames that
are constructed from a huge raw corpus in
an unsupervised manner. This model se-
lects the syntactic and case structure that
has the highest generative probability. We
evaluate both syntactic structure and case
structure. In particular, the experimen-
tal results for syntactic analysis on web
sentences show that the proposed model
significantly outperforms known syntactic
analyzers.
1 Introduction
Case structure (predicate-argument structure or log-
ical form) represents what arguments are related to
a predicate, and forms a basic unit for conveying the
meaning of natural language text. Identifying such
case structure plays an important role in natural lan-
guage understanding.
In English, syntactic case structure can be mostly
derived from word order. For example, the left ar-
gument of the predicate is the subject, and the right
argument of the predicate is the object in most cases.
Blaheta and Charniak proposed a statistical method
?Currently, National Institute of Information and Communi-
cations Technology, JAPAN, dk@nict.go.jp
?Currently, Graduate School of Informatics, Kyoto Univer-
sity, kuro@i.kyoto-u.ac.jp
for analyzing function tags in Penn Treebank, and
achieved a really high accuracy of 95.7% for syn-
tactic roles, such as SBJ (subject) and DTV (da-
tive) (Blaheta and Charniak, 2000). In recent years,
there have been many studies on semantic structure
analysis (semantic role labeling) based on PropBank
(Kingsbury et al, 2002) and FrameNet (Baker et al,
1998). These studies classify syntactic roles into se-
mantic ones such as agent, experiencer and instru-
ment.
Case structure analysis of Japanese is very differ-
ent from that of English. In Japanese, postpositions
are used to mark cases. Frequently used postposi-
tions are ?ga?, ?wo? and ?ni?, which usually mean
nominative, accusative and dative. However, when
an argument is followed by the topic-marking post-
position ?wa?, its case marker is hidden. In addi-
tion, case-marking postpositions are often omitted in
Japanese. These troublesome characteristics make
Japanese case structure analysis very difficult.
To address these problems and realize Japanese
case structure analysis, wide-coverage case frames
are required. For example, let us describe how to
apply case structure analysis to the following sen-
tence:
bentou-wa taberu
lunchbox-TM eat
(eat lunchbox)
In this sentence, taberu (eat) is a verb, and bentou-
wa (lunchbox-TM) is a case component (i.e. argu-
ment) of taberu. The case marker of ?bentou-wa?
is hidden by the topic marker (TM) ?wa?. The an-
alyzer matches ?bentou? (lunchbox) with the most
176
suitable case slot (CS) in the following case frame
of ?taberu? (eat).
CS examples
taberu ga person, child, boy, ? ? ?wo lunch, lunchbox, dinner, ? ? ?
Since ?bentou? (lunchbox) is included in ?wo? ex-
amples, its case is analyzed as ?wo?. As a result, we
obtain the case structure ??:ga bentou:wo taberu?,
which means that ?ga? (nominative) argument is
omitted, and ?wo? (accusative) argument is ?bentou?
(lunchbox). In this paper, we run such case structure
analysis based on example-based case frames that
are constructed from a huge raw corpus in an unsu-
pervised manner.
Let us consider syntactic analysis, into which our
method of case structure analysis is integrated. Re-
cently, many accurate statistical parsers have been
proposed (e.g., (Collins, 1999; Charniak, 2000) for
English, (Uchimoto et al, 2000; Kudo and Mat-
sumoto, 2002) for Japanese). Since they somehow
use lexical information in the tagged corpus, they are
called ?lexicalized parsers?. On the other hand, un-
lexicalized parsers achieved an almost equivalent ac-
curacy to such lexicalized parsers (Klein and Man-
ning, 2003; Kurohashi and Nagao, 1994). Accord-
ingly, we can say that the state-of-the-art lexicalized
parsers are mainly based on unlexical (grammatical)
information due to the sparse data problem. Bikel
also indicated that Collins? parser can use bilexical
dependencies only 1.49% of the time; the rest of
the time, it backs off to condition one word on just
phrasal and part-of-speech categories (Bikel, 2004).
This paper aims at exploiting much more lexical
information, and proposes a fully-lexicalized proba-
bilistic model for Japanese syntactic and case struc-
ture analysis. Lexical information is extracted not
from a small tagged corpus, but from a huge raw cor-
pus as case frames. This model performs case struc-
ture analysis by a generative probabilistic model
based on the case frames, and selects the syntactic
structure that has the highest case structure proba-
bility.
2 Automatically Constructed Case Frames
We employ automatically constructed case frames
(Kawahara and Kurohashi, 2002) for our model of
Table 1: Case frame examples (examples are ex-
pressed only in English for space limitation.).
CS examples
ga <agent>, group, party, ? ? ?
youritsu (1) wo <agent>, candidate, applicant
(support) ni <agent>, district, election, ? ? ?
ga <agent>
youritsu (2) wo <agent>, member, minister, ? ? ?
(support) ni <agent>, candidate, successor
...
...
...
itadaku (1) ga <agent>
(have) wo soup
ga <agent>
itadaku (2) wo advice, instruction, address
(be given) kara <agent>, president, circle, ? ? ?
...
...
...
case structure analysis. This section outlines the
method for constructing the case frames.
A large corpus is automatically parsed, and case
frames are constructed from modifier-head exam-
ples in the resulting parses. The problems of auto-
matic case frame construction are syntactic and se-
mantic ambiguities. That is to say, the parsing re-
sults inevitably contain errors, and verb senses are
intrinsically ambiguous. To cope with these prob-
lems, case frames are gradually constructed from re-
liable modifier-head examples.
First, modifier-head examples that have no syn-
tactic ambiguity are extracted, and they are dis-
ambiguated by a couple of a verb and its closest
case component. Such couples are explicitly ex-
pressed on the surface of text, and can be consid-
ered to play an important role in sentence mean-
ings. For instance, examples are distinguished not
by verbs (e.g., ?tsumu? (load/accumulate)), but by
couples (e.g., ?nimotsu-wo tsumu? (load baggage)
and ?keiken-wo tsumu? (accumulate experience)).
Modifier-head examples are aggregated in this way,
and yield basic case frames.
Thereafter, the basic case frames are clustered
to merge similar case frames. For example, since
?nimotsu-wo tsumu? (load baggage) and ?busshi-wo
tsumu? (load supply) are similar, they are clustered.
The similarity is measured using a thesaurus (Ike-
hara et al, 1997).
Using this gradual procedure, we constructed case
frames from the web corpus (Kawahara and Kuro-
177
hashi, 2006). The case frames were obtained from
approximately 470M sentences extracted from the
web. They consisted of 90,000 verbs, and the aver-
age number of case frames for a verb was 34.3.
In Figure 1, some examples of the resulting case
frames are shown. In this table, ?CS? means a case
slot. <agent> in the table is a generalized example,
which is given to the case slot where half of the ex-
amples belong to <agent> in a thesaurus (Ikehara
et al, 1997). <agent> is also given to ?ga? case
slot that has no examples, because ?ga? case com-
ponents are usually agentive and often omitted.
3 Integrated Probabilistic Model for
Syntactic and Case Structure Analysis
The proposed method gives a probability to each
possible syntactic structure T and case structure L
of the input sentence S, and outputs the syntactic
and case structure that have the highest probability.
That is to say, the system selects the syntactic struc-
ture Tbest and the case structure Lbest that maximize
the probability P (T,L|S):
(Tbest, Lbest) = argmax
(T,L)
P (T,L|S)
= argmax
(T,L)
P (T,L, S)
P (S)
= argmax
(T,L)
P (T,L, S) (1)
The last equation is derived because P (S) is con-
stant.
3.1 Generative Model for Syntactic and Case
Structure Analysis
We propose a generative probabilistic model based
on the dependency formalism. This model considers
a clause as a unit of generation, and generates the
input sentence from the end of the sentence in turn.
P (T,L, S) is defined as the product of a probability
for generating a clause Ci as follows:
P (T,L, S) =
?
i=1..n
P (Ci|bhi) (2)
where n is the number of clauses in S, and bhi isCi?s
modifying bunsetsu1. The main clause Cn at the end
1In Japanese, bunsetsu is a basic unit of dependency, con-
sisting of one or more content words and the following zero or
more function words. It corresponds to a base phrase in English,
and ?eojeol? in Korean.
Figure 1: An Example of Probability Calculation.
of a sentence does not have a modifying head, but
we handle it by assuming bhn = EOS (End Of Sen-
tence).
For example, consider the sentence in Figure 1.
There are two possible dependency structures, and
for each structure the product of probabilities indi-
cated below of the tree is calculated. Finally, the
model chooses the highest-probability structure (in
this case the left one).
Ci is decomposed into its predicate type fi (in-
cluding the predicate?s inflection) and the rest case
structure CSi. This means that the predicate in-
cluded in CSi is lemmatized. Bunsetsu bhi is also
decomposed into the content part whi and the type
fhi .
P (Ci|bhi) = P (CSi, fi|whi , fhi)
= P (CSi|fi, whi , fhi)P (fi|whi , fhi)
? P (CSi|fi, whi)P (fi|fhi) (3)
The last equation is derived because the content part
in CSi is independent of the type of its modifying
head (fhi), and in most cases, the type fi is indepen-
dent of the content part of its modifying head (whi).
For example, P (bentou-wa tabete|syuppatsu-shita)
is calculated as follows:
P (CS(bentou-wa taberu)|te, syuppatsu-suru)P (te|ta.)
We call P (CSi|fi, whi) generative model for case
structure and P (fi|fhi) generative model for predi-
cate type. The following two sections describe these
models.
3.2 Generative Model for Case Structure
We propose a generative probabilistic model of case
structure. This model selects a case frame that
178
Figure 2: An example of case assignment CAk.
matches the input case components, and makes cor-
respondences between input case components and
case slots.
A case structure CSi consists of a predicate vi,
a case frame CFl and a case assignment CAk.
Case assignment CAk represents correspondences
between input case components and case slots as
shown in Figure 2. Note that there are various pos-
sibilities of case assignment in addition to that of
Figure 2, such as corresponding ?bentou? (lunch-
box) with ?ga? case. Accordingly, the index k of
CAk ranges up to the number of possible case as-
signments. By splitting CSi into vi, CFl and CAk,
P (CSi|fi, whi) is rewritten as follows:
P (CSi|fi, whi) = P (vi, CFl, CAk|fi, whi)
= P (vi|fi, whi)
? P (CFl|fi, whi , vi)
? P (CAk|fi, whi , vi, CFl)
? P (vi|whi)
? P (CFl|vi)
? P (CAk|CFl, fi) (4)
The above approximation is given because it is
natural to consider that the predicate vi depends on
its modifying headwhi , that the case frameCFl only
depends on the predicate vi, and that the case assign-
ment CAk depends on the case frame CFl and the
predicate type fi.
The probabilities P (vi|whi) and P (CFl|vi) are
estimated from case structure analysis results of a
large raw corpus. The remainder of this section il-
lustrates P (CAk|CFl, fi) in detail.
3.2.1 Generative Probability of Case
Assignment
Let us consider case assignment CAk for each
case slot sj in case frame CFl. P (CAk|CFl, fi)
can be decomposed into the following product de-
pending on whether a case slot sj is filled with an
input case component (content part nj and type fj)
or vacant:
P (CAk|CFl, fi) =
?
sj :A(sj)=1
P (A(sj) = 1, nj , fj |CFl, fi, sj)
?
?
sj :A(sj)=0
P (A(sj) = 0|CFl, fi, sj)
=
?
sj :A(sj)=1
{
P (A(sj) = 1|CFl, fi, sj)
?P (nj , fj |CFl, fi, A(sj) = 1, sj)
}
?
?
sj :A(sj)=0
P (A(sj) = 0|CFl, fi, sj) (5)
where the function A(sj) returns 1 if a case slot sj
is filled with an input case component; otherwise 0.
P (A(sj) = 1|CFl, fi, sj) and P (A(sj) =
0|CFl, fi, sj) in equation (5) can be rewritten as
P (A(sj) = 1|CFl, sj) and P (A(sj) = 0|CFl, sj),
because the evaluation of case slot assignment de-
pends only on the case frame. We call these proba-
bilities generative probability of a case slot, and they
are estimated from case structure analysis results of
a large corpus.
Let us calculate P (CSi|fi, whi) using the ex-
ample in Figure 1. In the sentence, ?wa? is
a topic marking (TM) postposition, and hides
the case marker. The generative probability of
case structure varies depending on the case slot
to which the topic marked phrase is assigned.
For example, when a case frame of ?taberu?
(eat) CFtaberu1 with ?ga? and ?wo? case slots is
used, P (CS(bentou-wa taberu)|te, syuppatsu-suru)
is calculated as follows:
P1(CS(bentou-wa taberu)|te, syuppatsu-suru) =
P (taberu|syuppatsu-suru)
? P (CFtaberu1|taberu)
? P (bentou,wa|CFtaberu1, te, A(wo) = 1,wo)
? P (A(wo) = 1|CFtaberu1,wo)
? P (A(ga) = 0|CFtaberu1, ga) (6)
179
P2(CS(bentou-wa taberu)|te, syupatsu-suru) =
P (taberu|syuppatsu-suru)
? P (CFtaberu1|taberu)
? P (bentou,wa|CFtaberu1, te, A(ga) = 1, ga)
? P (A(ga) = 1|CFtaberu1, ga)
? P (A(wo) = 0|CFtaberu1,wo) (7)
Such probabilities are computed for each case frame
of ?taberu? (eat), and the case frame and its cor-
responding case assignment that have the highest
probability are selected.
We describe the generative probability of a case
component P (nj , fj |CFl, fi, A(sj) = 1, sj) below.
3.2.2 Generative Probability of Case
Component
We approximate the generative probability of a
case component, assuming that:
? a generative probability of content part nj is in-
dependent of that of type fj ,
? and the interpretation of the surface case in-
cluded in fj does not depend on case frames.
Taking into account these assumptions, the genera-
tive probability of a case component is approximated
as follows:
P (nj , fj |CFl, fi, A(sj) = 1, sj) ?
P (nj |CFl, A(sj) = 1, sj) P (fj |sj , fi) (8)
P (nj |CFl, A(sj) = 1, sj) is the probability of
generating a content part nj from a case slot sj in a
case frame CFl. This probability is estimated from
case frames.
Let us consider P (fj |sj , fi) in equation (8). This
is the probability of generating the type fj of a case
component that has a correspondence with the case
slot sj . Since the type fj consists of a surface case
cj2, a punctuation mark (comma) pj and a topic
marker ?wa? tj , P (fj |sj , fi) is rewritten as follows
2A surface case means a postposition sequence at the end of
bunsetsu, such as ?ga?, ?wo?, ?koso? and ?demo?.
(using the chain rule):
P (fj |sj , fi) = P (cj , tj , pj |sj , fi)
= P (cj |sj , fi)
? P (pj |sj , fi, cj)
? P (tj |sj , fi, cj , pj)
? P (cj |sj)
? P (pj |fi)
? P (tj |fi, pj) (9)
This approximation is given by assuming that cj
only depends on sj , pj only depends on fj , and tj
depends on fj and pj . P (cj |sj) is estimated from the
Kyoto Text Corpus (Kawahara et al, 2002), in which
the relationship between a surface case marker and
a case slot is annotated by hand.
In Japanese, a punctuation mark and a topic
marker are likely to be used when their belong-
ing bunsetsu has a long distance dependency. By
considering such tendency, fi can be regarded as
(oi, ui), where oi means whether a dependent bun-
setsu gets over another head candidate before its
modifying head vi, and ui means a clause type of
vi. The value of oi is binary, and ui is one of the
clause types described in (Kawahara and Kurohashi,
1999).
P (pj |fi) = P (pj |oi, ui) (10)
P (tj |fi, pj) = P (tj |oi, ui, pj) (11)
3.3 Generative Model for Predicate Type
Now, consider P (fi|fhi) in the equation (3). This is
the probability of generating the predicate type of a
clause Ci that modifies bhi . This probability varies
depending on the type of bhi .
When bhi is a predicate bunsetsu, Ci is a subor-
dinate clause embedded in the clause of bhi . As for
the types fi and fhi , it is necessary to consider punc-
tuation marks (pi, phi) and clause types (ui, uhi).
To capture a long distance dependency indicated by
punctuation marks, ohi (whether Ci has a possible
head candidate before bhi) is also considered.
PV Bmod(fi|fhi) = PV Bmod(pi, ui|phi , uhi , ohi)
(12)
When bhi is a noun bunsetsu, Ci is an embedded
clause in bhi . In this case, clause types and a punc-
tuation mark of the modifiee do not affect the prob-
ability.
PNBmod(fi|fhi) = PNBmod(pi|ohi) (13)
180
Table 2: Data for parameter estimation.
probability what is generated data
P (pj |oi, uj) punctuation mark Kyoto Text Corpus
P (tj |oi, ui, pj) topic marker Kyoto Text Corpus
P (pi, ui|phi , uhi , ohi) predicate type Kyoto Text Corpus
P (cj |sj) surface case Kyoto Text Corpus
P (vi|whi) predicate parsing results
P (nj |CFl, A(sj) = 1, sj) words case frames
P (CFl|vi) case frame case structure analysis results
P (A(sj) = {0, 1} |CFl, sj) case slot case structure analysis results
Table 3: Experimental results for syntactic analysis.
baseline proposed
all 3,447/3,976 (86.7%) 3,477/3,976 (87.4%)
NB?VB 1,310/1,547 (84.7%) 1,328/1,547 (85.8%)
TM 244/298 (81.9%) 242/298 (81.2%)
others 1,066/1,249 (85.3%) 1,086/1,249 (86.9%)
NB?NB 525/556 (94.4%) 526/556 (94.6%)
VB?VB 593/760 (78.0%) 601/760 (79.1%)
VB?NB 453/497 (91.1%) 457/497 (92.0%)
4 Experiments
We evaluated the syntactic structure and case struc-
ture outputted by our model. Each parameter is es-
timated using maximum likelihood from the data
described in Table 2. All of these data are not
existing or obtainable by a single process, but ac-
quired by applying syntactic analysis, case frame
construction and case structure analysis in turn. The
process of case structure analysis in this table is a
similarity-based method (Kawahara and Kurohashi,
2002). The case frames were automatically con-
structed from the web corpus comprising 470M sen-
tences, and the case structure analysis results were
obtained from 6M sentences in the web corpus.
The rest of this section first describes the exper-
iments for syntactic structure, and then reports the
experiments for case structure.
4.1 Experiments for Syntactic Structure
We evaluated syntactic structures analyzed by the
proposed model. Our experiments were run on
hand-annotated 675 web sentences 3. The web sen-
tences were manually annotated using the same cri-
teria as the Kyoto Text Corpus. The system input
was tagged automatically using the JUMAN mor-
phological analyzer (Kurohashi et al, 1994). The
syntactic structures obtained were evaluated with re-
3The test set is not used for case frame construction and
probability estimation.
gard to dependency accuracy ? the proportion of
correct dependencies out of all dependencies except
for the last dependency in the sentence end 4.
Table 3 shows the dependency accuracy. In
the table, ?baseline? means the rule-based syn-
tactic parser, KNP (Kurohashi and Nagao, 1994),
and ?proposed? represents the proposed method.
The proposed method significantly outperformed the
baseline method (McNemar?s test; p < 0.05). The
dependency accuracies are classified into four types
according to the bunsetsu classes (VB: verb bun-
setsu, NB: noun bunsetsu) of a dependent and its
head. The ?NB?VB? type is further divided into
two types: ?TM? and ?others?. The type that is most
related to case structure is ?others? in ?NB?VB?.
Its accuracy was improved by 1.6%, and the error
rate was reduced by 10.9%. This result indicated
that the proposed method is effective in analyzing
dependencies related to case structure.
Figure 3 shows some analysis results, where the
dotted lines represent the analysis by the baseline
method, and the solid lines represent the analysis by
the proposed method. Sentence (1) and (2) are in-
correctly analyzed by the baseline but correctly ana-
lyzed by the proposed method.
There are two major causes that led to analysis
errors.
Mismatch between analysis results and annota-
tion criteria
In sentence (3) in Figure 3, the baseline
method correctly recognized the head of ?iin-wa?
(commissioner-TM) as ?hirakimasu? (open). How-
ever, the proposed method incorrectly judged it as
?oujite-imasuga? (offer). Both analysis results can
be considered to be correct semantically, but from
4Since Japanese is head-final, the second last bunsetsu un-
ambiguously depends on the last bunsetsu, and the last bunsetsu
has no dependency.
181
? ?
(1) mizu-ga takai tokoro-kara hikui tokoro-he nagareru.
water-nom high ground-abl low ground-all flow
(Water flows from high ground to low ground.)
? ?
(2) ... Kobe shi-ga senmonchishiki-wo motsu volunteer-wo bosyushita ...
Kobe city-nom expert knowledge-acc have volunteer-acc recruited
(Kobe city recruited a volunteer who has expert knowledge, ...)
??
(3) iin-wa, jitaku-de minasan-karano gosoudan-ni oujite-imasuga, ... soudansyo-wo hirakimasu
commissioner-TM at home all of you consultation-dat offer window open
(the commissioner offers consultation to all of you at home, but opens a window ...)
Figure 3: Examples of analysis results.
Table 4: Experimental results for case structure anal-
ysis.
baseline proposed
TM 72/105 (68.6%) 82/105 (78.1%)
clause 107/155 (69.0%) 121/155 (78.1%)
the viewpoint of our annotation criteria, the latter is
not a syntactic relation, but an ellipsis relation. To
address this problem, it is necessary to simultane-
ously evaluate not only syntactic relations but also
indirect relations, such as ellipses and anaphora.
Linear weighting on each probability
We proposed a generative probabilistic model,
and thus cannot optimize the weight of each proba-
bility. Such optimization could be a way to improve
the system performance. In the future, we plan to
employ a machine learning technique for the opti-
mization.
4.2 Experiments for Case Structure
We applied case structure analysis to 215 web sen-
tences which are manually annotated with case
structure, and evaluated case markers of TM phrases
and clausal modifiees by comparing them with the
gold standard in the corpus. The experimental re-
sults are shown in table 4, in which the baseline
refers to a similarity-based method (Kawahara and
Kurohashi, 2002). The experimental results were re-
ally good compared to the baseline. It is difficult to
compare the results with the previous work stated in
the next section, because of different experimental
settings (e.g., our evaluation includes parse errors in
incorrect cases).
5 Related Work
There have been several approaches for syntactic
analysis handling lexical preference on a large scale.
Shirai et al proposed a PGLR-based syntactic
analysis method using large-scale lexical preference
(Shirai et al, 1998). Their system learned lexical
preference from a large newspaper corpus (articles
of five years), such as P (pie|wo, taberu), but did
not deal with verb sense ambiguity. They reported
84.34% accuracy on 500 relatively short sentences
from the Kyoto Text Corpus.
Fujio and Matsumoto presented a syntactic anal-
ysis method based on lexical statistics (Fujio and
Matsumoto, 1998). They made use of a probabilistic
model defined by the product of a probability of hav-
ing a dependency between two cooccurring words
and a distance probability. The model was trained
on the EDR corpus, and performed with 86.89% ac-
curacy on 10,000 sentences from the EDR corpus 5.
On the other hand, there have been a number
of machine learning-based approaches using lexical
preference as their features. Among these, Kudo
and Matsumoto yielded the best performance (Kudo
and Matsumoto, 2002). They proposed a chunking-
based dependency analysis method using Support
Vector Machines. They used two-fold cross valida-
tion on the Kyoto Text Corpus, and achieved 90.46%
5The evaluation includes the last dependencies in the sen-
tence end, which are always correct.
182
accuracy 5. However, it is very hard to learn suffi-
cient lexical preference from several tens of thou-
sands sentences of a hand-tagged corpus.
There has been some related work analyzing
clausal modifiees and TM phrases. For exam-
ple, Torisawa analyzed TM phrases using predicate-
argument cooccurences and word classifications in-
duced by the EM algorithm (Torisawa, 2001). Its
accuracy was approximately 88% for ?wa? and 84%
for ?mo?. It is difficult to compare the accuracy
of their system to ours, because the range of tar-
get expressions is different. Unlike related work,
it is promising to utilize the resultant case frames
for subsequent analyzes such as ellipsis or discourse
analysis.
6 Conclusion
We have described an integrated probabilistic model
for syntactic and case structure analysis. This model
takes advantage of lexical selectional preference of
large-scale case frames, and performs syntactic and
case analysis simultaneously. The experiments indi-
cated the effectiveness of our model. In the future,
by incorporating ellipsis resolution, we will develop
an integrated model of syntactic, case and ellipsis
analysis.
References
Collin Baker, Charles Fillmore, and John Lowe. 1998. The
Berkeley FrameNet Project. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics and
the 36th Annual Meeting of the Association for Computa-
tional Linguistics, pages 86?90.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Don Blaheta and Eugene Charniak. 2000. Assigning function
tags to parsed text. In Proceedings of the 1st Meeting of
the North American Chapter of the Association for Compu-
tational Linguistics, pages 234?240.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Meeting of the North American
Chapter of the Association for Computational Linguistics,
pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Masakazu Fujio and Yuji Matsumoto. 1998. Japanese depen-
dency structure analysis based on lexicalized statistics. In
Proceedings of the 3rd Conference on Empirical Methods in
Natural Language Processing, pages 88?96.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentarou Ogura, Yoshifumi
Oyama, and Yoshihiko Hayashi, editors. 1997. Japanese
Lexicon. Iwanami Publishing.
Daisuke Kawahara and Sadao Kurohashi. 1999. Corpus-based
dependency analysis of Japanese sentences using verb bun-
setsu transitivity. In Proceedings of the 5th Natural Lan-
guage Processing Pacific Rim Symposium, pages 387?391.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of
case frame dictionary for robust Japanese case analysis. In
Proceedings of the 19th International Conference on Com-
putational Linguistics, pages 425?431.
Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame
compilation from the web using high-performance comput-
ing. In Proceedings of the 5th International Conference on
Language Resources and Evaluation.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida. 2002.
Construction of a Japanese relevance-tagged corpus. In Pro-
ceedings of the 3rd International Conference on Language
Resources and Evaluation, pages 2008?2013.
Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the Penn TreeBank. In Pro-
ceedings of the Human Language Technology Conference.
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics, pages
423?430.
Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proceedings of the
Conference on Natural Language Learning, pages 29?35.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic anal-
ysis method of long Japanese sentences based on the detec-
tion of conjunctive structures. Computational Linguistics,
20(4):507?534.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and
Makoto Nagao. 1994. Improvements of Japanese morpho-
logical analyzer JUMAN. In Proceedings of the Interna-
tional Workshop on Sharable Natural Language, pages 22?
28.
Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga, and Hozumi
Tanaka. 1998. An empirical evaluation on statistical parsing
of Japanese sentences using lexical association statistics. In
Proceedings of the 3rd Conference on Empirical Methods in
Natural Language Processing, pages 80?87.
Kentaro Torisawa. 2001. An unsupervised method for canon-
icalization of Japanese postpositions. In Proceedings of the
6th Natural Language Processing Pacific Rim Simposium,
pages 211?218.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and Hi-
toshi Isahara. 2000. Dependency model using posterior
context. In Proceedings of the 6th International Workshop
on Parsing Technology, pages 321?322.
183
Dialog Navigator : A Spoken Dialog Q-A System
based on Large Text Knowledge Base
Yoji Kiyota, Sadao Kurohashi (The University of Tokyo)
kiyota,kuro@kc.t.u-tokyo.ac.jp
Teruhisa Misu, Kazunori Komatani, Tatsuya Kawahara (Kyoto University)
misu,komatani,kawahara@kuis.kyoto-u.ac.jp
Fuyuko Kido (Microsoft Co., Ltd.)
fkido@microsoft.com
Abstract
This paper describes a spoken dialog Q-
A system as a substitution for call centers.
The system is capable of making dialogs
for both fixing speech recognition errors
and for clarifying vague questions, based
on only large text knowledge base. We in-
troduce two measures to make dialogs for
fixing recognition errors. An experimental
evaluation shows the advantages of these
measures.
1 Introduction
When we use personal computers, we often en-
counter troubles. We usually consult large manu-
als, experts, or call centers to solve such troubles.
However, these solutions have problems: it is diffi-
cult for beginners to retrieve a proper item in large
manuals; experts are not always near us; and call
centers are not always available. Furthermore, op-
eration cost of call centers is a big problem for en-
terprises. Therefore, we proposed a spoken dialog
Q-A system which substitute for call centers, based
on only large text knowledge base.
If we consult a call center, an operator will help
us through a dialog. The substitutable system also
needs to make a dialog. First, asking backs for fixing
speech recognition errors are needed. Note that too
many asking backs make the dialog inefficient. Sec-
ondly, asking backs for clarifying users? problems
are also needed, because they often do not know
their own problems so clearly.
To realize such asking backs, we developed a sys-
tem as shown in Figure 1. The features of our system
are as follows:
 Precise text retrieval.
The system precisely retrieves texts from large
confirmation using
confidence in recognition
confirmation using
significance for retrieval
automatic speech recognizer
(Julius)
speech input
confirmation for
significant parts
user?s selection
N-best candidates
(or reject all)
user?s selection
N-best candidates of speech recognition
asking back(s)
with dialog cards
description extraction
choices in
dialog cards
user?s selection
final result
retrieval result text
retrieval
systemuser
text
knowledge
base
dialog for clarifying
vague questions
dialog
cards
dialog for fixing
speech recognition
errors
Figure 1: Architecture.
text knowledge base provided by Microsoft
Corporation (Table 1), using question types,
products, synonymous expressions, and syntac-
tic information. Dialog cards which can cope
with very vague questions are also retrieved.
 Dialog for fixing speech recognition errors.
When accepting speech input, recognition er-
rors are inevitable. However, it is not obvi-
ous which portions of the utterance the sys-
tem should confirm by asking back to the user.
A great number of spoken dialog systems for
particular task domains, such as (Levin et al,
2000), solved this problem by defining slots,
but it is not applicable to large text knowledge
base. Therefore, we introduce two measures
of confidence in recognition and significance
for retrieval to make dialogs for fixing speech
recognition errors.
 Dialog for clarifying vague questions.
When a user asks a vague question such as
?An error has occurred?, the system navigates
him/her to the desired answer, asking him/her
back using both dialog cards and extraction of
Table 1: Text collections.
# of # of matching
text collection texts characters target
Glossary 4,707 700,000 entries
Help texts 11,306 6,000,000 titles
Support KB 23,323 22,000,000 entire texts
summaries that makes differences between re-
trieved texts more clear.
Our system makes asking backs by showing them
on a display, and users respond them by selecting
the displayed buttons by mouses.
Initially, we developed the system as a keyboard
based Q-A system, and started its service in April
2002 at the web site of Microsoft Corporation. The
extension for speech input was done based on the
one-year operation. Our system uses Julius (Lee et
al., 2001) as a Japanese speech recognizer, and it
uses language model acquired from the text knowl-
edge base of Microsoft Corporation.
In this paper, we describe the above three features
in Section 2, 3, and 4. After that, we show experi-
mental evaluation, and then conclude this paper.
2 Precise Text Retrieval
It is critical for a Q-A system to retrieve relevant
texts for a question precisely. In this section, we
describe the score calculation method, giving large
points to modifier-head relations between bunsetsu1
based on the parse results of KNP (Kurohashi and
Nagao, 1994), to improve precision of text retrieval.
Our system also uses question types, product names,
and synonymous expression dictionary as described
in (Kiyota et al, 2002).
First, scores of all sentences in each text are calcu-
lated as shown in Figure 2. Sentence score is the to-
tal points of matching keywords and modifier-head
relations. We give 1 point to a matching of a key-
word, and 2 points to a matching of a modifier-head
relation (these parameters were set experimentally).
Then sentence score is normalized by the maximum
matching score (MMS) of both sentences as follows
(the MMS is the sentence score with itself):
sentence score

the MMS of a
user question



the MMS of a
text sentence

1Bunsetsu is a commonly used linguistic unit in Japanese,
consisting of one or more adjoining content words and zero or
more following functional words.
Outlook
?Outlook?
tsukau
?use?
meru
?mail?
jushin
?receive?
Outlook wo tsukatte
meru wo jushin dekinai.
?I cannot receive mails using Outlook.?
Outlook
?Outlook?
?mail?
jushin
?receive?
error
?error?
Outlook de meru wo jushin
suru sai no error.
?An error while receiving mails
 using Outlook.?
+1
+1
+1
MMS8 10
user question text sentence
meru
+2
sentence score
= 5
	
  



 
Figure 2: Score calculation.
vague
concrete
Error ga hassei shita.
?An error has occurred.?
Komatte imasu.
?I have a problem.?
Windows 98 de kidouji ni
error ga hassei shita.
?An error has occurred
while booting Windows 98.?
text knowledge base
clarifying questions
using dialog cards
text retrieval &
description extraction
user
questions
Figure 3: User navigation.
Finally, the sentence that has the largest score in
each text is selected as the representative sentence of
the text. Then, the score of the sentence is regarded
as the score of the text.
3 Dialog Strategy for Clarifying Questions
In most cases, users? questions are vague. To cope
with such vagueness, our system uses the following
two methods: asking backs using dialog cards and
extraction of summaries that makes difference be-
tween retrieved texts more clear (Figure 3).
3.1 Dialog cards
If a question is very vague, it matches many texts,
so users have to pay their labor on finding a rele-
vant one. Our system navigates users to the desired
answer using dialog cards as shown in Figure 3.
We made about three hundred of dialog cards
to throw questions back to users. Figure 4 shows
two dialog cards. UQ (User Question) is fol-
lowed by a typical vague user question. If a user
question matches it, the dialog manager asks the
back question after SYS, showing choices be-
[Error]
UQ Error ga hassei suru
?An error occurs?
SYSError wa itsu hassei shimasuka?
?When does the error occurs??
SELECT
Windows kidou ji goto [Error/Booting Windows]
?while booting Windows?
in?satsu ji goto [Error/Printing Out]
?while printing out?
application kidou ji goto [Error/Launching Applications]
?while launching applications?
/SELECT
[Error/Booting Windows]
UQ Windows wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows?
SYSAnata ga otsukai no Windows wo erande kudasai.
?Choose your Windows.?
SELECT
Windows 95 retrieve Windows 95 wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows 95?
Windows 98 retrieve Windows 98 wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows 98?
Windows ME retrieve Windows ME wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows ME?
/SELECT
Figure 4: Dialog cards.
tween SELECT and /SELECT. Every choice is
followed by goto or retrieve. goto means that the
system follow the another dialog cards if this choice
is selected. retrieve means that the system retrieve
texts using the query specified there.
3.2 Description extraction from retrieved texts
In most cases, the neighborhood of the part that
matches the user question describes specific symp-
toms and conditions of the problem users encounter.
Our system extracts such descriptions from the re-
trieved texts as the summaries of them. The algo-
rithm is described in (Kiyota et al, 2002).
4 Dialog Strategy for Speech Input
It is necessary for a spoken dialog system to deter-
mine which portions of the speech input should be
confirmed. Moreover, criteria for judging whether
it should make confirmation or not are needed, be-
cause too many confirmations make the dialog inef-
ficient. Therefore, we introduce two criteria of con-
fidence in recognition and significance for retrieval.
Our system makes two types of asking backs for
fixing recognition errors (Figure 1). First, Julius out-
puts  -best candidates of speech recognition. Then,
the system makes confirmation for significant parts
based on confidence in recognition. After that, the
system retrieves relevant texts in the text knowledge
base using each candidate, and makes confirmation
based on significance for retrieval.
4.1 Confidence in recognition
We define the confidence in recognition for each
phrase in order to reject partial recognition errors. It
is calculated based on word perplexity, which is of-
ten used in order to evaluate suitability of language
models for test-set sentences. We adopt word per-
plexity because of the following reasons: incorrectly
recognized parts are often unnatural in context, and
words that are unnatural in context have high per-
plexity values.
As Julius uses trigram as its language model, the
word perplexity  is calculated as follows:
  




 



 


 s are summed up in each bunsetsu (phrases).
As a result, the system assigned the sum of  s
to each bunsetsu as the criterion for confidence in
recognition.
We preliminarily defined the set of product names
as significant phrases2. If the sums of  s for any
significant phrases are beyond the threshold (now,
we set it 50), the system makes confirmation for
these phrases.
4.2 Significance for retrieval
The system calculates significance for retrieval us-
ing  -best candidates of speech recognition. Be-
cause slight speech recognition errors are not harm-
ful for retrieval results, we regard a difference that
affects its retrieval result as significant. Namely,
when the difference between retrieval results for
each recognition candidate is large, we regard that
the difference is significant.
Significance for retrieval is defined as a rate
of disagreement of five high-scored retrieved texts
among  recognition candidates. For example, if
there is a substituted part in two recognition candi-
dates, and only one text is commonly retrieved out
of five high-scored texts by both candidates, the sig-
nificance for retrieval for the substituted part is 0.8
(   ).
The system makes confirmation which candidate
should be used, if significance for retrieval is beyond
the threshold (now, we set it 0.5).
2We are now developing a method to define the set of sig-
nificant phrases semi-automatically.
Table 2: Number of successful retrieval for each speaker.
speaker # of ASR transcription speech recognition with confidence with significance with both
ID utterances corr. (1) results (2) in recognition (3) for retrieval (4) measures (5)
A 13 87.8% 10/13 8/13 8/13 10/13 10/13
B 14 55.0% 6/14 3/14 5/14 3/14 5/14
C 13 61.7% 5/13 5/13 5/13 5/13 5/13
D 13 83.1% 11/13 9/13 9/13 9/13 9/13
total 53 68.5% 32/53 25/53 27/53 27/53 29/53
U: OS ? ME ???????????????????????????
????????????????????????????????
?????????????
?I am using (Windows) ME as OS(operating system), then ...?
ASR: OS ? IME ???????????????????????????
???????????????????????????
(?ME? was incorrectly recognized as ?IME?.)
S: The underlined product name seems to be uncorrectly recognized. May I
remove it, or leave it?
U: (selected ?remove?)
S: The following texts are retrieved.
1. [AC97]????????????? [???] ??????????
2. [??????]? [???]??????????
3. [WinMe] TrueType??????????????? (a correct answer)
U: Word 2002 ? ?? ???????????????
?Please tell me the way to input formulas in Word 2002.?
ASR: 1. Word 2002 ? ?? (numbers) ???????????????
2. Word 2002 ? ?? (formulas) ???????????????
3. Word 2002 ? ?? (values) ???????????????
S: Please select the most correct recognition result from the above candidates.
U: (selected No. 2)
S: The following texts are retrieved.
1. Word ???????? (a correct answer)
2. Word ??????????????
3. ????????????????
Figure 5: Dialogs for fixing speech recognition er-
rors.
(U: user, S: system, ASR: automatic speech recognition)
5 Experimental Evaluation
We evaluated the system performance experimen-
tally. For the experiments, we had 4 subjects, who
were accustomed to using computers. They made
utterances by following given 10 scenarios and also
made several utterances freely. In total, 53 utter-
ances were recorded. Figure 5 shows two successful
dialogs by confirmation using confidence in recog-
nition and by that using significance for retrieval.
We experimented on the system using the 53
recorded utterances by the following methods:
(1) Using correct transcription of recorded utter-
ance, including fillers.
(2) Using speech recognition results from which
only fillers were removed.
(3) Using speech recognition results and making
confirmation by confidence in recognition.
(4) Using  -best candidates of speech recognition
and making confirmation by significance for re-
trieval. Here,   .
(5) Using  -best candidates of speech recognition
and both measures in (3) and (4).
In these experiments, we assumed that users al-
ways correctly answer system?s asking backs. We
regarded a retrieval as a successful one if a relevant
text was contained in ten high-scored retrieval texts.
Table 2 shows the result. It indicates that our
confirmation methods for fixing speech recognition
errors improve the success rate. Furthermore, the
success rate with both measures gets close to that
with the transcriptions. Considering that the speech
recognition correctness is about 70%, the proposed
dialog strategy is effective.
6 Conclusion
We proposed a spoken dialog Q-A system in which
asking backs for fixing speech recognition errors and
those for clarifying vague questions are integrated.
To realize dialog for fixing recognition errors based
on large text knowledge base, we introduced two
measures of confidence in recognition and signif-
icance for retrieval. The experimental evaluation
shows the advantages of these measures.
References
Yoji Kiyota, Sadao Kurohashi, and Fuyuko Kido. 2002.
?Dialog Navigator? : A Question Answering System
based on Large Text Knowledge Base. In Proceedings
of COLING 2002, pages 460?466.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4).
A. Lee, T. Kawahara, and K. Shikano. 2001. Julius ? an
open source real-time large vocabulary recognition en-
gine. In Proceedings of European Conf. Speech Com-
mun. & Tech. (EUROSPEECH), pages 1691?1694.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker.
2000. The AT&T-DARPA communicator mixed-
initiative spoken dialogue system. In Proceedings of
Int?l Conf. Spoken Language Processing (ICSLP).
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 755?762,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Topic Identification by Integrating Linguistic and
Visual Information Based on Hidden Markov Models
Tomohide Shibata
Graduate School of Information Science
and Technology, University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo, 113-8656, Japan
shibata@kc.t.u-tokyo.ac.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
This paper presents an unsupervised topic
identification method integrating linguis-
tic and visual information based on Hid-
den Markov Models (HMMs). We employ
HMMs for topic identification, wherein a
state corresponds to a topic and various
features including linguistic, visual and
audio information are observed. Our ex-
periments on two kinds of cooking TV
programs show the effectiveness of our
proposed method.
1 Introduction
Recent years have seen the rapid increase of mul-
timedia contents with the continuing advance of
information technology. To make the best use
of multimedia contents, it is necessary to seg-
ment them into meaningful segments and annotate
them. Because manual annotation is extremely ex-
pensive and time consuming, automatic annotation
technique is required.
In the field of video analysis, there have been
a number of studies on shot analysis for video
retrieval or summarization (highlight extraction)
using Hidden Markov Models (HMMs) (e.g.,
(Chang et al, 2002; Nguyen et al, 2005; Q.Phung
et al, 2005)). These studies first segmented videos
into shots, within which the camera motion is con-
tinuous, and extracted features such as color his-
tograms and motion vectors. Then, they classi-
fied the shots based on HMMs into several classes
(for baseball sports video, for example, pitch view,
running overview or audience view). In these
studies, to achieve high accuracy, they relied on
handmade domain-specific knowledge or trained
HMMs with manually labeled data. Therefore,
they cannot be easily extended to new domains
on a large scale. In addition, although linguistic
information, such as narration, speech of charac-
ters, and commentary, is intuitively useful for shot
analysis, it is not utilized by many of the previous
studies. Although some studies attempted to uti-
lize linguistic information (Jasinschi et al, 2001;
Babaguchi and Nitta, 2003), it was just keywords.
In the field of Natural Language Processing,
Barzilay and Lee have recently proposed a prob-
abilistic content model for representing topics and
topic shifts (Barzilay and Lee, 2004). This content
model is based on HMMs wherein a state corre-
sponds to a topic and generates sentences relevant
to that topic according to a state-specific language
model, which are learned from raw texts via anal-
ysis of word distribution patterns.
In this paper, we describe an unsupervised topic
identification method integrating linguistic and vi-
sual information using HMMs. Among several
types of videos, in which instruction videos (how-
to videos) about sports, cooking, D.I.Y., and oth-
ers are the most valuable, we focus on cooking
TV programs. In an example shown in Figure 1,
preparation, sauteing, and dishing up are automat-
ically labeled in sequence. Identified topics lead to
video segmentation and can be utilized for video
summarization.
Inspired by Barzilay?s work, we employ HMMs
for topic identification, wherein a state corre-
sponds to a topic, like preparation and frying, and
various features, which include visual and audio
information as well as linguistic information (in-
structor?s utterances), are observed. This study
considers a clause as an unit of analysis and the
following eight topics as a set of states: prepara-
tion, sauteing, frying, baking, simmering, boiling,
dishing up, steaming.
In Barzilay?s model, although domain-specific
755
cut:1 saute:1 add:3 put:2
preparation sauteing dishing up
?
?
?
?
?
?
?
?
preparation
sauteing
dishing up
silencecue phrase
?then?
t
Cut an avocado. We?ll saute. Add spices.
identified
topic:
hidden
states
observed
data
utterance
case frame
image
Put cheese between 
slices of bread.
Figure 1: Topic identification with Hidden Markov Models.
word distribution can be learned from raw texts,
their model cannot utilize discourse features, such
as cue phrases and lexical chains. We incorpo-
rate domain-independent discourse features such
as cue phrases, noun/verb chaining, which indicate
topic change/persistence, into the domain-specific
word distribution.
Our main claim is that we utilize visual and au-
dio information to achieve robust topic identifi-
cation. As for visual information, we can utilize
background color distribution of the image. For
example, frying and boiling are usually performed
on a gas range and preparation and dishing up are
usually performed on a cutting board. This infor-
mation can be an aid to topic identification. As for
audio information, silence can be utilized as a clue
to a topic shift.
2 Related Work
In Natural Language Processing, text segmenta-
tion tasks have been actively studied for infor-
mation retrieval and summarization. Hearst pro-
posed a technique called TextTiling for subdivid-
ing texts into sub-topics (Hearst.M, 1997). This
method is based on lexical co-occurrence. Galley
et al presented a domain-independent topic seg-
mentation algorithm for multi-party speech (Gal-
ley et al, 2003). This segmentation algorithm
uses automatically induced decision rules to com-
bine linguistic features (lexical cohesion and cue
phrases) and speech features (silences, overlaps
and speaker change). These studies aim just at
segmenting a given text, not at identifying topics
of segmented texts.
Marcu performed rhetorical parsing in the
framework of Rhetorical Structure Theory (RST)
based on a discourse-annotated corpus (Marcu,
2000). Although this model is suitable for ana-
lyzing local modification in a text, it is difficult for
this model to capture the structure of topic transi-
tion in the whole text.
In contrast, Barzilay and Lee modeled a con-
tent structure of texts within specific domains,
such as earthquake and finance (Barzilay and Lee,
2004). They used HMMs wherein each state cor-
responds to a distinct topic (e.g., in earthquake
domain, earthquake magnitude or previous earth-
quake occurrences) and generates sentences rel-
evant to that topic according to a state-specific
language model. Their method first create clus-
ters via complete-link clustering, measuring sen-
tence similarity by the cosine metric using word
bigrams as features. They calculate initial proba-
bilities: state si specific language model ps
i
(w?|w)
756
????????? (Cut a Chinese cabbage.)
???????????????? (Cut off its root and wash it.)
???????????????(A Japanese radish would taste delicious.)
??3???????? (Divide it into three equal parts.)
?????????? (Now, we'll  saute.)
??
[individual action]
[individual action] [individual action]
[substitution]
[individual action]
[action declaration]
???????????????????? (Just a little more and go for it!)
[small talk][small talk]
cut:1
cut off:1 wash:1
divide:3
saute:1
Figure 2: An example of closed captions. (The phrase sandwiched by a square bracket means an utterance
type and the word surrounded by a rectangle means an extracted utterance referring to an action. The
bold word means a case frame assigned to the verb.)
and state-transition probability p(sj|si) from state
si to state sj . Then, they continue to estimate
HMM parameters with the Viterbi algorithm un-
til the clustering stabilizes. They applied the con-
structed content model to two tasks: information
ordering and summarization. We differ from this
study in that we utilize multimodal features and
domain-independent discourse features to achieve
robust topic identification.
In the field of video analysis, there have been
a number of studies on shot analysis with HMMs.
Chang et al described a method for classifying
shots into several classes for highlight extraction
in baseball games (Chang et al, 2002). Nguyen
et al proposed a robust statistical framework to
extract highlights from a baseball video (Nguyen
et al, 2005). They applied multi-stream HMMs
to control the weight among different features,
such as principal component features capturing
color information and frame-difference features
for moving objects. Phung et al proposed a prob-
abilistic framework to exploit hierarchy structure
for topic transition detection in educational videos
(Q.Phung et al, 2005).
Some studies attempted to utilize linguistic
information in shot analysis (Jasinschi et al,
2001; Babaguchi and Nitta, 2003). For exam-
ple, Babaguchi and Nitta segmented closed cap-
tion text into meaningful units and linked them to
video streams in sports video. However, linguistic
information they utilized was just keywords.
3 Features for Topic Identification
First, we?ll describe the features that we use for
topic identification, which are listed in Table 1.
They consist of three modalities: linguistic, visual
and audio modality.
We utilize as linguistic information the instruc-
tor?s utterances in video, which can be divided into
various types such as actions, tips, and even small
talk. Among them, actions, such as cut, peel and
grease a pan, are dominant and supposed to be use-
ful for topic identification and others can be noise.
In the case of analyzing utterances in video, it
is natural to utilize visual information as well as
linguistic information for robust analysis. We uti-
lize background image as visual information. For
example, frying and boiling are usually performed
on a gas range and preparation and dishing up are
usually performed on a cutting board.
Furthermore, we utilize cue phrases and silence
as a clue to a topic shift, and noun/verb chaining
as a clue to a topic persistence.
We describe these features in detail in the fol-
lowing sections.
3.1 Linguistic Features
Closed captions of Japanese cooking TV programs
are used as a source for extracting linguistic fea-
757
Table 1: Features for topic identification.
Modality Feature Domain dependent Domain independent
linguistic case frame utterance generalization
cue phrases topic change
noun chaining topic persistence
verb chaining topic persistence
visual background image bottom of image
audio silence topic change
Table 2: Utterance-type classification. (An underlined phrase means a pattern for recognizing utterance
type.)
[action declaration]
ex. ???????????????? (Then, we ?ll cook a steak)
????????????? (OK, we?ll fry.)
[individual action]
ex. ??????????? (Cut off a step of this eggplant.)
??????????? (Pour water into a pan.)
[food state]
ex. ???????????????? (There is no water in the carrot.)
[note]
ex. ??????????? (Don?t cut this core off.)
[substitution]
ex. ?????????? (You may use a leek.)
[food/tool presentation]
ex. ?????????????????? Today, we use this handy mixer.)
[small talk]
ex. ?????? (Hello.)
tures. An example of closed captions is shown in
Figure 2. We first process them with the Japanese
morphological analyzer, JUMAN (Kurohashi et
al., 1994), and make syntactic/case analysis and
anaphora resolution with the Japanese analyzer,
KNP (Kurohashi and Nagao, 1994). Then, we
perform the following process to extract linguis-
tic features.
3.1.1 Extracting Utterances Referring to
Actions
Considering a clause as a basic unit, utterances
referring to an action are extracted in the form
of case frame, which is assigned by case analy-
sis. This procedure is performed for generaliza-
tion and word sense disambiguation. For exam-
ple, ?????? (add salt)? and ???????
?? (add sugar into a pan)? are assigned to case
frame ireru:1 (add) and ??????? (carve with
a knife)? is assigned to case frame ireru:2 (carve).
We describe this procedure in detail below.
Utterance-type recognition
To extract utterances referring to actions, we
classify utterances into several types listed in Ta-
ble 21. Note that actions are supposed to have two
levels: [action declaration] means a declaration of
beginning a series of actions and [individual ac-
tion] means an action that is the finest one.
1In this paper, [ ] means an utterance type.
Input sentences are first segmented into
clauses and their utterance type is recognized.
Among several utterance types, [individual ac-
tion], [food/tool presentation], [substitution],
[note], and [small talk] can be recognized by
clause-end patterns. We prepare approximately
500 patterns for recognizing the utterance type. As
for [individual action] and [food state], consider-
ing the portability of our system, we use general
rules regarding intransitive verbs or adjective + ?
?? (become)? as [food state], and others as [in-
dividual action].
Action extraction
We extract utterances whose utterance type is
recognized as action ([action declaration] or [indi-
vidual action]). For example, ??? (peel)? and ?
?? (cut)? are extracted from the following sen-
tence.
(1) ????????? [individual action]??
???????? [individual action]?(We
peel this carrot and cut it in half.)
We make two exceptions to reduce noises. One
is that clauses are not extracted from the sen-
tence in which sentence-end clause?s utterance-
type is not recognized as an action. In the fol-
lowing example, ??? (simmer)? and ??? (cut)?
are not extracted because the utterance type of
758
Table 3: An example of the automatically con-
structed case frame.
Verb Case
marker Examples
kiru:1 ga <agent>
(cut) wo pork, carrot, vegetable, ? ? ?
ni rectangle, diamonds, ? ? ?
kiru:2 ga <agent>
(drain) wo damp ? ? ?
no eggplant, bean curd, ? ? ?
ireru:1 ga <agent>
(add) wo salt, oil, vegetable, ? ? ?
ni pan, bowl, ? ? ?
ireru:2 ga <agent>
(carve) wo knife ? ? ?
ni fish ? ? ?
the sentence-end clause is recognized as [substi-
tution].
(2) ???? [individual action]???? [indi-
vidual action]????? [substitution]?(It
doesn?t matter if you cut it after simmering.)
The other is that conditional/causal clauses are
not extracted because they sometimes refer to the
previous/next topic.
(3) ?????? ????????(After we
finish cutting it, we?ll fry.)
(4) ???????? ??????????
??????(We cut in this cherry tomato,
because we?ll fry it in oil.)
Note that relations between clauses are recognized
by clause-end patterns.
Verb sense disambiguation by assigning to a
case frame
In general, a verb has multiple mean-
ings/usages. For example, ????? has multiple
usages, ?????? (add salt)? and ????
??? (carve with a knife)? , which appear in
different topics. We do not extract a surface form
of verb but a case frame, which is assigned by
case analysis. Case frames are automatically
constructed from Web cooking texts (12 million
sentences) by clustering similar verb usages
(Kawahara and Kurohashi, 2002). An example of
the automatically constructed case frame is shown
in Table 3. For example, ?????? (add salt)?
is assigned to ireru:1 (add) and ???????
(carve with a knife)? is assigned to case frame
ireru:2 (carve).
3.1.2 Cue phrases
As Grosz and Sidner (Grosz and Sidner, 1986)
pointed out, cue phrases such as now and well
serve to indicate a topic change. We use approx-
imately 20 domain-independent cue phrases, such
as ??? (then)?, ??? (next)? and ??????
?? (then)?.
3.1.3 Noun Chaining
In text segmentation algorithms such as Text-
Tiling (Hearst.M, 1997), lexical chains are widely
utilized for detecting a topic shift. We utilize such
a feature as a clue to topic persistence.
When two continuous actions are performed to
the same ingredient, their topics are often identi-
cal. For example, because ???? (grate)? and ?
??? (raise)? are performed to the same ingredi-
ent ???? (turnip)? , the topics (in this instance,
preparation) in the two utterances are identical.
(5) a. ??????????????????
(We?ll grate a turnip.)
b. ????????????????
(Raise this turnip on this basket.)
However, in the case of spoken language, be-
cause there exist many omissions, it is often the
case that noun chaining cannot be detected with
surface word matching. Therefore, we detect
noun chaining by using the anaphora resolution
result2 of verbs (ex.(6)) and nouns (ex.(7)). The
verb, noun anaphora resolution is conducted by
the method proposed by (Kawahara and Kuro-
hashi, 2004), (Sasano et al, 2004), respectively.
(6) a. ?????????? (Cut a cabbage.)
b. ?? [?????] ????? (Wash it
once.)
(7) a. ??????????????????
(Slice a carrot into 4-cm pieces.)
b. [?????] ???????????
(Peel its skin.)
3.1.4 Verb Chaining
When a verb of a clause is identical with that
of the previous clause, they are likely to have the
same topic. We utilize the fact that the adjoining
two clauses contain an identical verbs or not as an
observed feature.
(8) a. ?????????????(Add some
red peppers.)
2[ ] indicates an element complemented with anaphora
resolution.
759
b. ????????? (Add chicken
wings.)
3.2 Image Features
It is difficult for the current image processing tech-
nique to extract what object appears or what ac-
tion is performing in video unless a detailed ob-
ject/action model for a specific domain is con-
structed by hand. Therefore, referring to (Hamada
et al, 2000), we focus our attention on color dis-
tribution at the bottom of the image, which is com-
paratively easy to exploit. As shown in Figure 1,
we utilize the mass point of RGB in the bottom of
the image at each clause.
3.3 Audio Features
A cooking video contains various types of audio
information, such as instructor?s speech, cutting
sounds and frizzling sound. If cutting sound or
frizzling sound could be distinguished from other
sounds, they could be an aid to topic identification,
but it is difficult to recognize them.
As Galley et al (Galley et al, 2003) pointed
out, a longer silence often appears when topic
changes, and we can utilize it as a clue to topic
change. In this study, silence is automatically ex-
tracted by finding duration below a certain ampli-
tude level which lasts more than one second.
4 Topic Identification based on HMMs
We employ HMMs for topic identification, where
a hidden state corresponds to a topic and vari-
ous features described in Section 3 are observed.
In our model, considering the case frame as a
basic unit, the case frame and background im-
age are observed from the state, and discourse
features indicating to topic shift/persistence (cue
phrases, noun/verb chaining and silence) are ob-
served when the state transits.
4.1 Parameters
HMM parameters are as follows:
? initial state distribution ?i : the probability
that state si is a start state.
? state transition probability aij : the probabil-
ity that state si transits to state sj .
? observation probability bij(ot) : the proba-
bility that symbol ot is emitted when state si
transits to state sj . This probability is given
by the following equation:
bij(ot) = bj(cfk) ? bj(R,G,B)
? bij(discourse features) (1)
- case frame bj(cfk): the probability that
case frame cfk is emitted by state sj .
- background image bj(R,G,B): the prob-
ability that background image bj(R,G,B) is
emitted by state sj . The emission probability
is modeled by a single Gaussian distribution
with mean (Rj ,Gj ,Bj) and variance ?j .
- discourse features : the probability that
discourse features are emitted when state si
transits to state sj . This probability is defined
as multiplication of the observation probabil-
ity of each feature (cue phrase, noun chain-
ing, verb chaining, silence). The observation
probability of each feature does not depend
on state si and sj , but on whether si and sj
are the same or different. For example, in the
case of cue phrase (c), the probability is given
by the following equation:
bij(c) =
{
psame(c)(i = j)
pdiff (c)(i 6= j)
(2)
4.2 Parameters Estimation
We apply the Baum-Welch algorithm for esti-
mating these parameters. To achieve high accu-
racy with the Baum-Welch algorithm, which is
an unsupervised learning method, some labeled
data have been required or proper initial param-
eters have been set depending on domain-specific
knowledge. These requirements, however, make
it difficult to extend to other domains. We auto-
matically extract ?pseudo-labeled? data focusing
on the following linguistic expressions: if a clause
has the utterance-type [action declaration] and an
original form of its verb corresponds to a topic, its
topic is set to that topic. Remind that [action dec-
laration] is a kind of declaration of starting a series
of actions. For example, in Figure 1, the topic of
the clause ?We?ll saute.? is set to sauteing because
its utterance-type is recognized as [action decla-
ration] and the original form of its verb is topic
sauteing.
By using a small amounts of ?pseudo-labeled?
data as well as unlabeled data, we train the
HMM parameters. Once the HMM parameters are
trained, the topic identification is performed using
the standard Viterbi algorithm.
5 Experiments and Discussion
5.1 Data
To demonstrate the effectiveness of our proposed
method, we made experiments on two kinds of
cooking TV programs: NHK ?Today?s Cooking?
760
Table 5: Experimental result of topic identification.
Features Accuracy
case frame background image discourse features silence ?Today?s Cooking? ?Kewpie 3-Min Cooking?
?
61.7% 66.4%
?
56.8% 72.9%
? ?
69.9% 77.1%
? ? ?
70.5% 82.9%
? ? ? ?
70.5% 82.9%
Table 4: Characteristics of the two cooking pro-
grams we used for our experiments.
Program Today?s Cooking Kewpie 3-Min Cooking
Videos 200 70
Duration 25min 10min
# of utterances
per video 249.4 183.4
and NTV ?Kewpie 3-Min Cooking?. Table 4
presents the characteristics of the two programs.
Note that time stamps of closed captions syn-
chronize themselves with the video stream. Ex-
tracted ?pseudo-labeled? data by the expression
mentioned in Section 4.2 are 525 clauses out of
13564 (3.87%) in ?Today?s Cooking?, and 107
clauses out of 1865 (5.74%) in ?Kewpie 3-Min
Cooking?.
5.2 Experiments and Discussion
We conducted the experiment of the topic iden-
tification. We first trained HMM parameters for
each program, and then applied the trained model
to five videos each, in which, we manually as-
signed appropriate topics to clauses. Table 5
gives the evaluation results. The unit of evalua-
tion was a clause. The accuracy was improved
by integrating linguistic and visual information
compared to using linguistic / visual informa-
tion alone. (Note that ?visual information? uses
pseudo-labeled data.) In addition, the accuracy
was improved by using various discourse features.
The reason why silence did not contribute to ac-
curacy improvement is supposed to be that closed
captions and video streams were not synchronized
precisely due to time lagging of closed captions.
To deal with this problem, an automatic closed
caption alignment technique (Huang et al, 2003)
will be applied or automatic speech recognition
will be used as texts instead of closed captions
with the advance of speech recognition technol-
ogy.
Figure 3 illustrates an improved example by
adding visual information. In the case of using
only linguistic information, this topic was rec-
First, saute and 
body.
Chop a garlic 
noisely.
Let?s start cooked 
vegitable.
preparation sauteing
sauteing
linguistic
linguistic
+ visual
Figure 3: An improved example by adding visual
information.
ognized as sauteing, but this topic was actually
preparation, which referred to the next topic. By
using the visual information that background color
was white, this topic was correctly recognized as
preparation.
We conducted another experiment to demon-
strate the validity of several linguistic processes,
such as utterance-type recognition and word sense
disambiguation with case frames, for extracting
linguistic information from closed captions de-
scribed in Section 3.1.1. We compared our method
to three methods: a method that does not per-
form word sense disambiguation with case frames
(w/o cf), a method that does not perform utterance-
type recognition for extracting actions (uses all
utterance-type texts) (w/o utype), a method, in
which a sentence is emitted according to a state-
specific language model (bigram) as Barzilay and
Lee adopted (bigram). Figure 6 gives the exper-
imental result, which demonstrates our method is
appropriate.
One cause of errors in topic identification is that
some case frames are incorrectly constructed. For
example, kiru:1 (cut) contains ?????? (cut
a vegetable)? and ????? (drain oil)?. This
leads to incorrect parameter training. Other cause
is that some verbs are assigned to an inaccurate
case frame by the failure of case analysis.
6 Conclusions
This paper has described an unsupervised topic
identification method integrating linguistic and vi-
sual information based on Hidden Markov Mod-
761
Table 6: Results of the experiment that compares our method to three methods.
Method Accuracy
?Today?s Cooking? ?Kewpie 3-Min Cooking?
proposed method 61.7% 66.4%
w/o cf 57.1% 60.0%
w/o utype 61.7% 62.1%
bigram 54.7% 59.3%
els. Our experiments on the two kinds of cooking
TV programs showed the effectiveness of integra-
tion of linguistic and visual information and in-
corporation of domain-independent discourse fea-
tures to domain-dependent features (case frame
and background image).
We are planning to perform object recognition
using the automatically-constructed object model
and utilize the object recognition results as a fea-
ture for HMM-based topic identification.
References
Noboru Babaguchi and Naoko Nitta. 2003. Intermodal
collaboration: A strategy for semantic content anal-
ysis for broadcasted sports video. In Proceedings of
IEEE International Conference on Image Process-
ing(ICIP2003), pages 13?16.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the NAACL/HLT, pages 113?120.
Peng Chang, Mei Han, and Yihong Gong. 2002.
Extract highlights from baseball game video with
hidden markov models. In Proceedings of the
International Conference on Image Processing
2002(ICIP2002), pages 609?612.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 562?569, 7.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistic, 12:175?204.
Reiko Hamada, Ichiro Ide, Shuichi Sakai, and Hide-
hiko Tanaka. 2000. Associating cooking video with
related textbook. In Proceedings of ACM Multime-
dia 2000 workshops, pages 237?241.
Hearst.M. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64, March.
Chih-Wei Huang, Winston Hsu, and Shin-Fu Chang.
2003. Automatic closed caption alignment based
on speech recognition transcripts. Technical report,
Columbia ADVENT.
Radu Jasinschi, Nevenka Dimitrova, Thomas McGee,
Lalitha Agnihotri, John Zimmerman, and Dongge.
2001. Integrated multimedia processing for topic
segmentation and classification. In Proceedings of
IEEE International Conference on Image Process-
ing(ICIP2003), pages 366?369.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertil-
ization of case frame dictionary for robust japanese
case analysis. In Proceedings of 19th COLING
(COLING02), pages 425?431.
Daisuke Kawahara and Sadao Kurohashi. 2004. Zero
pronoun resolution based on automatically con-
structed case frames and structural preference of an-
tecedents. In Proceedings of The 1st International
Joint Conference on Natural Language Processing,
pages 334?341.
Sadao Kurohashi and Makoto Nagao. 1994. A syntac-
tic analysis method of long japanese sentences based
on the detection of conjunctive structures. Compu-
tational Linguistics, 20(4).
Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improve-
ments of Japanese morphological analyzer JUMAN.
In Proceedings of the International Workshop on
Sharable Natural Language, pages 22?28.
Daniel Marcu. 2000. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395?448.
Huu Bach Nguyen, Koichi Shinoda, and Sadaoki Fu-
rui. 2005. Robust highlight extraction using multi-
stream hidden markov models for baseball video. In
Proceedings of the International Conference on Im-
age Processing 2005(ICIP2005), pages 173?176.
Dinh Q.Phung, Thi V.T Duong, Hung H.Bui, and
S.Venkatesh. 2005. Topic transition detection using
hierarchical hidden markov and semi-markov mod-
els. In Proceedings of ACM International Confer-
ence on Multimedia(ACM-MM05), pages 6?11.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proceedings of the 20th International
Conference on Computational Linguistics, number
1201?1207, 8.
762
Word Selection for EBMT based on Monolingual Similarity and Translation
Confidence
Eiji Aramaki, Sadao Kurohashi, Hideki Kashioka and Hideki Tanaka
 Graduate School of Information Science and Tech. University of Tokyo
Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
aramaki, kuro@kc.t.u-tokyo.ac.jp
 ATR Spoken Language Translation Research Laboratories
2-2 Hikaridai, Seika, Soraku, Kyoto 619-0288, Japan
hideki.kashioka, hideki.tanaka@atr.co.jp
Abstract
We propose a method of constructing an
example-based machine translation (EBMT)
system that exploits a content-aligned bilingual
corpus. First, the sentences and phrases in the
corpus are aligned across the two languages,
and the pairs with high translation confidence
are selected and stored in the translation mem-
ory. Then, for a given input sentences, the
system searches for fitting examples based on
both the monolingual similarity and the transla-
tion confidence of the pair, and the obtained re-
sults are then combined to generate the transla-
tion. Our experiments on translation selection
showed the accuracy of 85% demonstrating the
basic feasibility of our approach.
1 Introduction
The basic idea of example-based machine translation, or
EBMT, is that translation examples similar to a part of
an input sentence are retrieved and combined to produce
a translation(Nagao, 1984). In order to make a practi-
cal MT system based on this approach, a large number
of translation examples with structural correspondences
are required. This naturally presupposes high-accuracy
parsers and well-aligned large bilingual corpora.
Over the last decade, the accuracy of the parsers im-
proved significantly. The availability of well-aligned
bilingual corpora, however, has not increased despite our
expectations. In reality, the number of bilingual cor-
pora that share the same content, such as newspapers and
broadcast news, has increased steadily. We call this type
of corpus a content-aligned corpus. With these observa-
tions, we started a research project that covered all as-
pects of constructing EBMT systems starting from using
Figure 1: Translation Example (TE).
a content-aligned corpus, i.e., a bilingual broadcast news
corpus.
First, the sentences and phrases in the corpus are
aligned across the two languages, and the pairs with high
translation confidence are selected and stored in the trans-
lation memory. Then, translation examples are retrieved
based on both the monolingual similarity and the trans-
lation confidence of the pair. Finally, these examples are
combined to generate the translation.
This paper is organized as follows. The next sec-
tion presents how to build the translation memory from
a content-aligned corpus. Section 3 describes our EBMT
system, paying special attention to the selection of trans-
lation examples. Section 4 reports experimental results
of word selection, Section 5 describes related works, and
Section 6 gives our conclusions.
* Underlined phrases and sentences have no parallel expressions in the other language.
Figure 2: NHK News Corpus.
2 Building Translation Memory
In EBMT, an input sentence can hardly be translated by
a single translation example, except when an input is ex-
tremely short or is a typical domain-dependent sentence.
Therefore, two or more translation examples are used to
translate parts of the input and are then combined to gen-
erate a whole translation. Syntactic information is useful
for composing example fragments.
In this paper, we call a structurally aligned bilingual
sentence pair a translation example or TE (Figure 1).
This section presents our method for building TEs from a
content-aligned corpus.
Since the bilingual corpus used in our project does not
contain literal translations, automatic parsing and align-
ment inevitably contain errors. Therefore, we selected
highly likely TEs to make a translation memory.
2.1 NHK News Corpus
We used a bilingual news corpus compiled by the NHK
broadcasting service (NHK News Corpus), which con-
sists of about 40,000 Japanese-English article pairs cov-
ering a five-year period. The average number of Japanese
sentences in an article is 5.2, and that of English sentence
is 7.4. Table 2 shows an example of an article pair.
As shown in Table 2, an English article is not a literal
translation of a Japanese article, although their contents
are almost parallel.
2.2 Sentence Alignment
We used a DP matching for bilingual sentence alignment,
where we allow the matching of 1-to-1, 1-to-2, 1-to-3, 2-
to-1 and 2-to-2 Japanese and English sentence pairs. This
matching covered 84% of the following evaluation set.
We selected 96 article pairs for the evaluation of sentence
and phrase alignment, and we call this the evaluation set.
We use the following score for matching, which is based
on a ratio of corresponding content words (WCR: content
Word Corresponding Ratio).
WCR  




 (1)
where 

is the number of Japanese content words in a
unit, 

is the number of English content words, and 

is the number of content words whose translation is also
in the unit, which is found by translation dictionaries?
We used the EDR electronic dictionary, EDICT,
ENAMDICT, the ANCHOR translation dictionary, and
Figure 3: Handling of Remaining Phrases.
Figure 4: WCR and Precision.
the EIJIRO translation dictionary. These dictionaries
have about two million entries in total.
On the evaluation data, the precision of the sentence
alignment (defined as follows) was 60.7%.
precision  # of correct system outputs
# of system outputs
(2)
Among types of a corresponding unit, the precision of
1-to-1 correspondence was the best, at 77.5%. Since a 1-
to-1 correspondence is suitable for the following phrase
alignment, we decided to use only the 1-to-1 correspon-
dence results.
2.3 Phrase Alignment
The 1-to-1 sentence pairs obtained in the previous sec-
tion are then aligned at phrase level by the method based
on (Aramaki et al, 2001). The method consists of the
following pre-process and two aligning steps.
Pre-process: Conversion to phrasal dependency struc-
tures.
First, the phrasal dependency structures of the sen-
tence pair are estimated. The English parser returns
a word-based phrase structure, which is merged into
a phrase sequence by the following rules and con-
verted into a dependency structure by lifting up head
phrases.
Table 1: Number of TEs.
Corpus WCR # of TEs
0.3?0.4 18290
NHK News 0.4?0.5 6975
0.5? 2314
White Paper ? 2225
SENSEVAL ? 6920
1. Function words are grouped with the following
content word.
2. Adjoining nouns are grouped into one phrase.
3. Auxiliary verbs are grouped with the following
verb.
The Japanese parser outputs the phrasal dependency
structure of an input, and that is used as is. We used
The Japanese parser KNP (Kurohashi and Nagao,
1994) and The English nl-parser (Charniak, 2000).
Step 1: Estimation of basic phrasal correspondences.
We started with the word-level alignment to get the
basic phrasal alignment. We used translation dictio-
naries for this process. The word sense ambiguity
in the dictionaries is resolved with a heuristics that
the most plausible correspondence is near other cor-
respondences.
Step 2: Expansion of phrasal correspondences.
Finally, the remaining phrases, which were not han-
dled in the step 1, are merged into a neighboring
phrase correspondence or are used to establish a new
correspondence, depending on the surrounding ex-
isting correspondences. Figure 3 shows an example
of a new correspondence established by a structural
pattern.
These procedures can detect the phrasal alignments in
a pair of sentences as shown in Figure 1.
For phrase alignment evaluation, we selected all of the
145 sentence pairs that had 1-to-1 correspondences form
the evaluation set and gave correct content word corre-
spondences to these pairs. The phrase correspondences
detected by the system were judged correct when the cor-
respondences include the manually given content word
correspondences.
Based on this criterion, the precision of phrase align-
ment was 50%. Then, we found a correlation between
the phrase alignment precision and WCR of parallel sen-
tences as shown in Figure 4. Furthermore, the precision
of sentence alignment and WCR also have a correlation.
Since their performances nearly reaches their limits when
WCR is 0.3, we decided to use parallel sentences whose
WCR is 0.3 or greater as TEs.
Figure 5: Example of Translation.
2.4 Building Translation Memory
As explained in the preceding sections, among sentence-
aligned and phrase-aligned NHK News articles, TEs with
a 1-to-1 sentence correspondence and whose WCR is 0.3
or greater are registered in the translation memory. Table
1 shows the number of TEs for each WCR range.
In addition, the Bilingual White Paper and Translation
Memory of SENSEVAL2 (Kurohashi, 2001) were also
phrase-aligned and registered in the translation memory.
Sentence alignments are already given for these corpora.
Since their parallelism are fairly high and the accuracies
of their phrase alignments are more than 70%, we utilized
all phrase-aligned sentence pairs as TEs (Table 1).
3 EBMT System
Our EBMT system translates a Japanese sentence into
English. A Japanese input sentence is parsed and trans-
formed into a phrase-based dependency structure. Then,
for each phrase, an appropriate TE is retrieved from the
translation memory that is most suitable for translating
Figure 6: Selection of a TE.
the phrase (and its neighboring phrases). Finally, the En-
glish expressions of the TEs are combined to produce the
final English translation (Figure 5).
This section describes our EBMT system, mainly the
TE selection part.
3.1 Basic Idea of TE Selection
The basic idea of TE selection is shown in Figure 6.
When a part of the input sentence and a part of the TE
source language sentence have an equal expression, the
part of the input sentence is called I and the part of the
TE source language sentence is called S. A part of the TE
target language corresponding to S is called T. The pair S
and T is called fragment of TE (FTE).
I, S and T have to meet the following conditions, as a
natural consequence of the fact that S-T is used for trans-
lating I.
1. I, S and T are each structurally connected phrases.
2. I is equal to S except for function words at the
boundaries.
3. S corresponds to T completely, that is, all phrases in
S and T are aligned.
It might be the case that for an I, two or more FTEs that
meet the above conditions exist in the translation mem-
ory. Our method takes into account the following rela-
tions among I-S-T to select the best FTE:
1. The largest pair of I and S.
2. The similarity between the surroundings of I and
these of S.
3. The confidence of alignment between S and T.
The following sections concretely present how to cal-
culate these criteria. For simplicity of explanation, we
call a set of phrasal correspondences between S and T,
EQ; that neighboring EQ, CONTEXT; that between S and
T, ALIGN (Figure 6).
3.2 Monolingual Similarity between Japanese
Expressions
The equality between I and S is a sum of the equality
score of each phrase correspondence in EQ, which is cal-
culated as follows:
EQUAL 



 





	
 

	
 (3)
where

is the number of content words in the phrase
correspondence, 
	
is the number of function words,


is the equality between content words, and 
	
is the equality between function words. 

and 
	
are given in Table 2.1
Usually, the equality score between I and S is equal to
the number of phrases in I (the number of phrase corre-
spondences in EQ), but sometimes these are slightly dif-
ferent, depending on the conjugation type and function
words.
1All constant values in Table 2 and formulas were decided
based on preliminary experiments.
On the other hand, the similarity between the surround-
ings of I and those of S is a sum of the similarity score of
each phrase correspondence in CONTEXT, which is cal-
culated as follows:
SIM 




 






 






(4)
Basically the calculation of SIM and EQUAL is the
same, except that SIM considers the relation type between
the phrase in I and its outer phrase by 

. When
the relation is the same, the influence of the surrounding
phrases must be large, so 

is set to 1.0; when the
relation is not the same, 

is set to 0.5. The rela-
tions between phases are estimated by the function word
or conjugation type of the dependent phrase.
The monolingual similarity between Japanese expres-
sions I and S is calculated as follows:



EQUAL 



SIM (5)
3.3 Translation Confidence of Japanese-to-English
Alignment
The translation confidence of phrase alignment between
S and T is the sum of the confidence score of each phrase
correspondence in ALIGN, CONF() in Table 2, and it is
weighted by the WCR of the parallel sentences.
As a final measure, the score of I-S-T is calculated as
follows:




EQUAL 



SIM






CONF

WCR (6)
3.4 Search Algorithm of FTE
For each phrase (P) in an input sentence, the most plausi-
ble FTE is retrieved by the following algorithm:
1. FTEs are retrieved from the translation memory, in
which a Japanese phrase matches P, and it is aligned
to an English phrase. (that is, these are FTEs that
meet the basic conditions for translation in Section
3.1).
2. For each FTE obtained in the previous step, it is
checked whether the surrounding phrase of P and
that of FTE are the same or similar, phrase by
phrase, and the largest I-S-T that meets the basic
conditions is detected.
Table 2: Parameters for Similarity and Confidence Calculation.
1.1 exact match
1.0 stem match


0.5  

+ 0.3 thesaurus match
0.3 POS match
0 otherwise
* 

is a similarity calculated based on NTT thesaurus(Ikehara et al, 1997) (max = 1).
1.1 exact match


1.0 stem match
0 otherwise
1.0 all content words in alignment  correspond to each other in dic
CONF() 0.8 some content words in alignment  correspond to each other in dic
0.5 otherwise
3. The score of each I-S-T is calculated, and the best
I-S-T (S-T is the FTE) is selected as the FTE for P.
As a result of detecting FTEs for phrases in the input,
two FTEs starting from the different phrase might over-
lap each other. In such a case, we employed a greedy
search algorithm that adopts the higher score FTE one
by one; therefore, each previously adopted FTE is only
partly used for translation.
On the other hand, when no FTE is obtained for an in-
put phrase, a translation dictionary is utilized (when the
phrase contains two or more content words, the longest
matching strategy is used for dictionary look-up). When
two or more possible translations are given from the dic-
tionary, the most frequent phrase/word in the NHK News
Corpus is adopted.
Figure 5 shows examples of FTEs detected by our
method.2
3.5 Generating a Target Sentence
The English expressions in the selected FTEs are com-
bined, and the English dependency structure is con-
structed. The dependency relations in FTEs are pre-
served, and the relation between the two FTEs is esti-
mated based on the relation of the input sentences. Figure
5 shows an example of a combined English dependency
structure.
When a surface expression is generated from its depen-
dency structure, its word order must be selected properly.
This can be done by preserving the word order in FTEs
and by ordering FTEs by a set of rules governing both the
dependency relation and the word-order.
The module for controlling conjugation, determiner,
and singular/plural is not yet implemented in our current
MT system.
2As the bottom example in Figure 5 shows, EBMT can eas-
ily handle head-switching translation by using an FTE that con-
tains all of the head-switching phenomena in it.
4 Experiments
For evaluation, we selected 50 sentence pairs from the
NHK News Corpus that were not used for the translation
memory. Their source (Japanese) sentences were trans-
lated by our EBMT system, and the selected FTEs were
evaluated by hand, referring to the target (English) sen-
tences.
A phrase by phrase evaluation was done to judge
whether the English expression of the selected FTE was
good or bad. The accuracy was 85.0%.
In order to investigate the effectiveness of each com-
ponent of FTE selection, we compared the following four
methods:
1. EQCONTEXTALIGN: The proposed method.
2. EQALIGN: FTE score is calculated as follows, with-
out the CONTEXT similarity:



EQUAL()



CONF()WCR (7)
3. EQCONTEXT: FTE score is calculated as follows,
without the ALIGN confidence:



EQUAL() 



SIM() (8)
4. DICONLY: Word selection is based only on dictio-
naries and frequency in the corpus.
The accuracy of each method is shown in Table 3,
and the results indicate that the proposed method, EQ-
CONTEXTALIGN, is the best, that is, using context sim-
ilarity and align confidence works effectively. Figure 7
Figure 7: Word Selection by EQCONTEXTALIGN and DICONLY.
Table 3: Experimental Results.
Good Bad Accuracy
EQCONTEXTALIGN 268 47 85.0%
(246) (35) (87.5%)
EQALIGN 254 61 80.6 %
(233) (48) (82.9%)
EQCONTEXT 234 80 74.2%
(213) (68) (75.8%)
DICONLY 232 83 73.6%
* Values in brackets indicate the accuracy only for FTEs,
excluding cases in which the dictionary was used as a
backup.
shows examples of EQCONTEXTALIGN and DICONLY.
EQCONTEXTALIGN usually selects appropriate words,
compared to DICONLY.
When there are no plausible translation examples in the
translationmemory, the system selects a low-similarity or
low-confidence FTE. However we believe this problem
will be resolved as the number of translation examples
increases, since the News Corpus is increasing day by
day.
5 Related Work
The idea of example based machine translation systems
was first proposed by (Nagao, 1984), and preliminary
systems that appeared about ten years (Sato and Na-
gao, 1990; Sadler and Vendelmans, 1990; Maruyama and
Watanabe, 1992; Furuse and Iida, 1994) showed the basic
feasibility of the idea.
Recent studies have focused on the practical aspects
of EBMT, and this technology has even been applied
to some restricted domains. The work in (Richardson
et al, 2001; Menezes and Richardson, 2001) addressed
the problem of technical manual translation in several
languages, and the work of (Imamura, 2002) dealt with
dialogues translation in the travel arrangement domain.
These works select the translation example pairs based
solely on the source language similarity. We believe this
is partly due to the high parallelism found in their cor-
pora.
Our work targets a more general corpus of wider cover-
age, i.e., the broadcast news collection. Generally avail-
able corpora like the one we use tend to be more freely
translated and suffer from lower parallelism. This com-
pelled us to use the criterion of translation confidence,
together with the criterion of monolingual similarity used
in the previous works. As we showed in this paper, this
metric succeeded in meeting our expectations.
6 Conclusion
In this paper, we described operations of the entire EBMT
process while using a content-aligned corpus, i.e., the
NHK Broadcast Corpus. In this process, one of the key
problems is how to select plausible translation examples.
We proposed a new method to select translation exam-
ples based on source language similarity and translation
confidence. In the word selection task, the performance
is highly accurate.
Acknowledgements
This work was supported in part by the 21st Century COE
program ?Information Science and Technology Strate-
gic Core? at University of Tokyo and by a contract with
the Telecommunications Advancement Organization of
Japan, entitled ?A study of speech dialogue translation
technology based on a large corpus?.
References
Eiji Aramaki, Sadao Kurohashi, Satoshi Sato, and Hideo
Watanabe. 2001. Finding translation correspondences
from parallel parsed corpus for example-based transla-
tion. In Proceedings of MT Summit VIII, pages 27?32.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In In Proceedings of NAACL 2000, pages 132?
139.
Osamu Furuse and Hitoshi Iida. 1994. Constituent
boundary parsing for example-based machine transla-
tion. In Proceedings of the 15th COLING, pages 105?
111.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentarou Ogura, and Yoshi-
fumi Oyama Yoshihiko Hayashi, editors. 1997.
Japanese Lexicon. Iwanami Publishing.
Kenji Imamura. 2002. Application of translation knowl-
edgeacquired by hierarchical phrase alignment for
pattern-based mt. In Proceedings of TMI-2002, pages
74?84.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4).
Sadao Kurohashi. 2001. Senseval2 Japanese translation
task. In Proceedings of SENSEVAL2, pages 37?40.
Hiroshi Maruyama and Hideo Watanabe. 1992. The
cover search algorithm for example-based translation.
In Proceedings of TMI-1992, pages 173?184.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the ACL 2001 Workshop on Data-Driven Meth-
ods in Machine Translation, pages 39?46.
Makoto Nagao. 1984. A framework of a mechanical
translation between Japanese and english by analogy
principle. In In Artificial and Human Intelligence,
pages 173?180.
Stephen D. Richardson, William B. Dolan, Arul
Menezes, and Monica Corston-Oliver. 2001. Over-
coming the customization bottleneck using example-
based mt. In Proceedings of the ACL 2001 Work-
shop onData-DrivenMethods in Machine Translation,
pages 9?16.
V. Sadler and R. Vendelmans. 1990. Pilot implementa-
tion of a bilingual knowledge bank. In Proeedings of
the 13th COLING, pages 449?451.
Satoshi Sato andMakoto Nagao. 1990. Toward memory-
based translation. InProceedings of the 13th COLING,
pages 247?252.
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 1?4,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
WISDOM: A Web Information Credibility Analysis System 
Susumu Akamine?  Daisuke Kawahara?  Yoshikiyo Kato? 
Tetsuji Nakagawa?  Kentaro Inui?  Sadao Kurohashi??  Yutaka Kidawara? 
?National Institute of Information and Communications Technology 
? Graduate School of Informatics, Kyoto University 
{akamine, dk, ykato, tnaka, inui, kidawara}@nict.go.jp, kuro@i.kyoto-u.ac.jp 
 
 
 
 
Abstract 
We demonstrate an information credibility 
analysis system called WISDOM. The purpose 
of WISDOM is to evaluate the credibility of in-
formation available on the Web from multiple 
viewpoints. WISDOM considers the following 
to be the source of information credibility: in-
formation contents, information senders, and 
information appearances. We aim at analyzing 
and organizing these measures on the basis of 
semantics-oriented natural language processing 
(NLP) techniques. 
1. Introduction 
As computers and computer networks become 
increasingly sophisticated, a vast amount of in-
formation and knowledge has been accumulated 
and circulated on the Web. They provide people 
with options regarding their daily lives and are 
starting to have a strong influence on govern-
mental policies and business management. How-
ever, a crucial problem is that the information 
available on the Web is not necessarily credible. 
It is actually very difficult for human beings to 
judge the credibility of the information and even 
more difficult for computers. However, comput-
ers can be used to develop a system that collects, 
organizes, and relativises information and helps 
human beings view information from several 
viewpoints and judge the credibility of the in-
formation. 
Information organization is a promising en-
deavor in the area of next-generation Web search. 
The search engine Clusty provides a search result 
clustering1, and Cuil classifies a search result on 
the basis of query-related terms2. The persuasive 
technology research project at Stanford Universi-
ty discussed how websites can be designed to 
influence people?s perceptions (B. J. Fogg, 2003). 
However, as per our knowledge, no research has 
been carried out for supporting the human judg-
ment on information credibility and information 
organization systems for this purpose. 
In order to support the judgment of informa-
tion credibility, it is necessary to extract the 
background, facts, and various opinions and their 
                                                 
1 http://clusty.com/, http://clusty.jp/  
distribution for a given topic. For this purpose, 
syntactic and discourse structures must be ana-
lyzed, their types and relations must be extracted, 
and synonymous and ambiguous expressions 
should be handled properly.  
Furthermore, it is important to determine the 
identity of the information sender and his/her 
specialty as criteria for credibility, which require 
named entity recognition and total analysis of 
documents. 
In this paper, we describe an information cre-
dibility analysis system called WISDOM, which 
automatically analyzes and organizes the above 
aspects on the basis of semantically oriented 
NLP techniques. WISDOM currently operates 
over 100 million Japanese Web pages. 
2. Overview of WISDOM 
We consider the following three criteria for the 
judgment of information credibility.  
(1) Credibility of information contents,  
(2) Credibility of the information sender, and  
(3) Credibility estimated from the document 
style and superficial characteristics. 
In order to help people judge the credibility of 
information from these viewpoints, we have been 
developing an information analysis system called 
WISDOM. Figure 1 shows the analysis result of 
WISDOM on the analysis topic ?Is bio-ethanol 
good for the environment?? Figure 2 shows the 
system architecture of WISDOM. 
Given an analysis topic (query), WISDOM 
sends the query to the search engine TSUBAKI 
(Shinzato et al, 2008), and TSUBAKI returns a 
list of the top N relevant Web pages (N is usually 
set to 1000). 
Then, those pages are automatically analyzed, 
and major and contradictory expressions and eva-
luative expressions are extracted. Furthermore, 
the information senders of the Web pages, which 
were analyzed beforehand, are collected and the 
distribution is calculated. 
The WISDOM analysis results can be viewed 
from several viewpoints by changing the tabs 
using a Web browser. The leftmost tab, ?Sum-
mary,? shows the summary of the analysis, with 
major phrases and major/contradictory state-
ments first.  
1
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Query: ?Is bio-ethanol good for the environment?? Summary 
Figure 1. An analysis example of the information credibility analysis system WISDOM. 
 
 
Figure 2. System architecture of WISDOM. 
 
By referring to these phrases and statements, 
a user can grasp the important issues related to 
the topic at a glance. The pie diagram indicates 
the distribution of the information sender class 
spread over 1000 pages, such as company, indus-
try group, and government. The names of the 
information senders of the class can be viewed 
by placing the cursor over a class region. The last 
bar chart shows the distribution of positive and 
negative opinions related to the topic spread over 
1000 pages, for all and for each sender class. For 
example, with regard to ?Bio-ethanol,? we can 
see that the number of positive opinions is more 
than that of negative opinions, but it is the oppo-
site in the case of some sender classes. Several 
display units in the Summary tab are cursor sen-
sitive, providing links to more detailed informa-
tion (e.g., the page list including a major state-
Sender 
Opinion 
Search Result Major/Contradictory Expressions
2
ment, the page list of a sender class, and the page 
list containing negative opinions). 
The ?Search Result? tab shows the search re-
sult by TSUBAKI, i.e., ranking the relevant pag-
es according to the TSUBAKI criteria. The ?Ma-
jor/Contradictory Expressions? tab shows the list 
of major phrases and major/contradictory state-
ments about the given topic and the list of pages 
containing the specified phrase or statement. The 
?Opinion? tab shows the analysis result of the 
evaluative expressions, classified according to 
for/against, like/dislike, merit/demerit, and others, 
and it also shows the list of pages containing the 
specified type of evaluative expressions. The 
?Sender? tab classifies the pages according to the 
class of the information sender, for example, a 
user can view the pages created only by the gov-
ernment.  
Furthermore, the superficial characteristics of 
pages called as information appearance are ana-
lyzed beforehand and can be viewed in WIS-
DOM, such as whether or not the contact address 
is shown in the page and the privacy policy is on 
the page, the volume of advertisements on the 
page, the number of images, and the number of 
in/out links. 
As shown thus far, given an analysis topic, 
WISDOM collects and organizes the relevant 
information available on the Web and provides 
users with multi-faceted views. We believe that 
such a system can considerably support the hu-
man judgment of information credibility. 
3. Data Infrastructure  
We usually utilize 100 million Japanese Web 
pages as the analysis target. The Web pages have 
been converted into the standard formatted Web 
data, an XML format. The format includes sever-
al metadata such as URLs, crawl dates, titles, and 
in/out links. A text in a page is automatically 
segmented into sentences (note that the sentence 
boundary is not clear in the original HTML file), 
and the analysis results obtained by a morpholog-
ical analyzer, parser, and synonym analyzer are 
also stored in the standard format. Furthermore, 
the site operator, the page author, and informa-
tion appearance (e.g., contact address, privacy 
policy, volume of advertisements, and images) 
are automatically analyzed and stored in the 
standard format. 
4. Extraction of Major Expressions and 
Their Contradictions 
For the organization of information contents, 
WISDOM extracts and presents the major ex-
pressions and their contradictions on a given 
analysis topic (Kawahara et al, 2008). Major 
expressions are defined as expressions occurring 
at a high frequency in the set of Web pages on 
the analysis topic. They are classified into two: 
noun phrases and predicate-argument structures 
(statements). Contradictions are the predicate-
argument structures that contradict the major ex-
pressions. For the Japanese phrase yutori kyouiku 
(cram-free education), for example, tsumekomi 
kyouiku (cramming education) and ikiru chikara 
(life skills) are extracted as the major noun 
phrases; yutori kyouiku-wo minaosu (reexamine 
cram-free education) and gakuryokuga teika-suru 
(scholastic ability deteriorates), as the major pre-
dicate-argument structures; and gakuryoku-ga 
koujousuru (scholastic ability ameliorates), as its 
contradiction. This kind of summarized informa-
tion enables a user to grasp the facts and argu-
ments on the analysis topic available on the Web. 
We use 1000 Web pages for a topic retrieved 
from the search engine TSUBAKI. Our method 
of extracting major expressions and their contra-
dictions consists of the following steps: 
1. Extracting candidates of major expressions: 
The candidates of major expressions are ex-
tracted from each Web page in the search result. 
From the relevant sentences to the analysis topic 
that consist of approximately 15 sentences se-
lected from each Web page, compound nouns, 
parenthetical expressions, and predicate-
argument structures are extracted as the candi-
dates of the major expressions. 
2. Distilling major expressions: 
Simply presenting expressions at a high fre-
quency is not always information of high quality. 
This is because scattering synonymous expres-
sions such as karikyuramu (curriculum) and 
kyouiku katei (course of study) and entailing ex-
pressions such as IWC and IWC soukai (IWC 
plenary session), all of which occur frequently, 
hamper the understanding process of users. Fur-
ther, synonymous predicate-argument structures 
such as gakuryoku-ga teika-suru (scholastic 
ability deteriorates) and gakuryoku-ga sagaru 
(scholastic ability lowers) have the same problem. 
To overcome this problem, we distill major ex-
pressions by merging spelling variations with 
morphological analysis, merging synonymous 
expressions automatically acquired from an ordi-
nary dictionary and the Web, and merging ex-
pressions that can be entailed by another expres-
sion. 
3. Extracting contradictory expressions: 
Predicate-argument structures that negate the 
predicate of major ones and that replace the pre-
dicate of major ones with its antonym are ex-
tracted as contradictions. For example, gakuryo-
ku-ga teika-shi-nai (scholastic ability does not 
deteriorate) and gakuryokuga koujou-suru (scho-
lastic ability ameliorates) are extracted as the 
contradictions to gakuryoku-ga teikasuru (scho-
lastic ability deteriorates). This process is per-
formed using an antonym lexicon, which consists 
of approximately 2000 pairs; these pairs are ex-
tracted from an ordinary dictionary. 
5. Extraction of Evaluative Information 
The extraction and classification of evaluative 
information from texts are important tasks with 
3
many applications and they have been actively 
studied recently (Pang and Lee, 2008). Most pre-
vious studies on opinion extraction or sentiment 
analysis deal with only subjective and explicit 
expressions. For example, Japanese sentences 
such as watashi-wa apple-ga sukida (I like ap-
ples) and kono seido-ni hantaida (I oppose the 
system) contain evaluative expressions that are 
directly expressed with subjective expressions. 
However, sentences such as kono shokuhin-wa 
kou-gan-kouka-ga aru (this food has an anti-
cancer effect) and kono camera-wa katte 3-ka-de 
kowareta (this camera was broken 3 days after I 
bought it) do not contain subjective expressions 
but contain negative evaluative expressions. 
From the viewpoint of information credibility, it 
appears important to deal with a wide variety of 
evaluative information including such implicit 
evaluative expressions (Nakagawa et al, 2008). 
A corpus annotated with evaluative informa-
tion was developed for evaluative information 
analysis studies. Fifty topics such as ?Bio-
ethanol? and ?Pension plan? were chosen. For 
each topic, 200 sentences containing the topic 
word were collected from the Web to construct 
the corpus totaling 10,000 sentences. For each 
sentence, annotators judged whether or not the 
sentence contained evaluative expressions. When 
evaluative expressions were identified, the evalu-
ative expressions, their holders, their sentiment 
polarities (positive or negative), and their relev-
ance to the topic were annotated. 
We developed an automatic analyzer of evalu-
ative information using the corpus. We per-
formed experiments of sentiment polarity classi-
fication using Support Vector Machines. Word 
forms, POS tags, and sentiment polarities from 
an evaluative word dictionary of all the words in 
evaluative expressions were used as features, and 
an accuracy of 83% was obtained. From the error 
analysis, we found that it was difficult to classify 
domain-specific evaluative expressions; we are 
now planning the automatic acquisition of evalu-
ative word dictionaries. 
6. Information Sender Analysis 
The source of information (or information sender) 
is one of the important elements when judging the 
credibility of information. It is rather easy for human 
beings to identify the information sender of a Web 
page. When reading a Web page, whether it is deli-
berate or not, we attribute some characteristics to the 
information sender and accordingly form our atti-
tudes toward the information. However, the state-of-
the-art search engines do not provide facilities to 
organize a vast amount of information on the basis 
of the information sender. If we can organize the 
information on a topic on the basis of who or what 
type the information sender is, it would enable the 
user to grasp an overview of the topic or to judge the 
credibility of relevant information. 
WISDOM automatically identifies the site op-
erators of Web pages and classifies them into 
predefined categories of information sender 
called information sender class. A site operator 
of a Web page is the governing body of a website 
on which the page is published. The information 
sender class categorizes the information sender 
on the basis of axes such as individuals vs. or-
ganizations and profit vs. nonprofit organizations. 
The list below shows the categories of informa-
tion sender class. 
 
 
 
1. Organization (cont?d) 
  (c) Press 
    i. Broadcasting Station 
    ii. Newspaper 
    iii. Publisher 
2. Individual 
  (a) Real Name 
  (b) Anonymous,  
Screen Name 
 
1. Organization 
  (a) Profit Organization 
    i. Company 
    ii. Industry Group 
  (b) Nonprofit Organization 
    i. Academic Society 
    ii. Government 
    iii. Political Organization 
    iv. Public Service Corp., 
         Nonprofit Organization 
    v. University 
    vi. Voluntary Association 
   vii. Education Institution
WISDOM allows the user to organize the in-
formation on the basis of the information sender 
class assigned to each Web page. Technical de-
tails of the information sender analysis employed 
in WISDOM can be found in (Kato et al, 2008). 
7. Conclusions 
This paper has described an information analy-
sis system called WISDOM. As shown in this pa-
per, WISDOM already provides a reasonably nice 
organized view for a given topic and can serve as a 
useful tool for handling informational queries and 
for supporting human judgment of information 
credibility. WISDOM is freely available at 
http://wisdom-nict.jp/.  
References 
B. J. Fogg. 2003. Persuasive Technology: Using Com-
puters to Change What We Think and Do (The Mor-
gan Kaufmann Series in Interactive Technologies). 
Morgan Kaufmann. 
K. Shinzato, T. Shibata, D. Kawahara, C. Hashimoto, 
and S. Kurohashi 2008. TSUBAKI: An open search 
engine infrastructure for developing new information 
access methodology. In Proceedings of IJCNLP2008. 
D. Kawahara, S. Kurohashi, and K. Inui 2008. Grasping 
major statements and their contradictions toward in-
formation credibility analysis of web contents. In 
Proceedings of  WI?08. 
B. Pang and L. Lee 2008. Opinion mining and senti-
ment analysis, Foundations and Trends in Informa-
tion Retrieval, Volume 2, Issue 1-2, 2008. 
T. Nakagawa, T. Kawada, K. Inui, and S. Kurohashi 
2008. Extracting subjective and objective evaluative 
expressions from the web. In Proceedings of 
ISUC2008. 
Y. Kato, D. Kawahara, K. Inui, S. Kurohashi, and T. 
Shibata 2008. Extracting the author of web pages. In 
Proceedings of WICOW2008. 
4
 	
   
  

  Fertilization of Case Frame Dictionary
for Robust Japanese Case Analysis
Daisuke Kawahara? and Sadao Kurohashi??
?Graduate School of Information Science and Technology, University of Tokyo
?PRESTO, Japan Science and Technology Corporation (JST)
{kawahara,kuro}@kc.t.u-tokyo.ac.jp
Abstract
This paper proposes a method of fertilizing a
Japanese case frame dictionary to handle com-
plicated expressions: double nominative sen-
tences, non-gapping relation of relative clauses,
and case change. Our method is divided into
two stages. In the first stage, we parse a large
corpus and construct a Japanese case frame dic-
tionary automatically from the parse results. In
the second stage, we apply case analysis to the
large corpus utilizing the constructed case frame
dictionary, and upgrade the case frame dictio-
nary by incorporating newly acquired informa-
tion.
1 Introduction
To understand a text, it is necessary to find out
relations between words in the text. What is
required to do so is a case frame dictionary. It
describes what kinds of cases each verb has and
what kinds of nouns can fill a case slot. Since
these relations have millions of combinations,
it is difficult to construct a case frame dictio-
nary by hand. We proposed a method to con-
struct a Japanese case frame dictionary auto-
matically by arranging large volumes of parse
results by coupling a verb and its closest case
component (Kawahara and Kurohashi, 2001).
This case frame dictionary, however, could not
handle complicated expressions: double nomi-
native sentences, non-gapping relation of rela-
tive clauses, and case change.
This paper proposes a method of fertiliz-
ing the case frame dictionary to handle these
complicated expressions. We take an iterative
method which consists of two stages. This
means gradual learning of what is understood
by an analyzer in each stage. In the first stage,
we parse a large raw corpus and construct a
Japanese case frame dictionary automatically
from the parse results. This is the method pro-
posed by (Kawahara and Kurohashi, 2001). In
the second stage, we apply case analysis to the
large corpus utilizing the constructed case frame
dictionary, and upgrade the case frame dictio-
nary by incorporating newly acquired informa-
tion.
We conducted a case analysis experiment
with the upgraded case frame dictionary, and
its evaluation showed effectiveness of the fertil-
ization process.
2 Japanese Grammar
We introduce Japanese grammar briefly in this
section.
Japanese is a head-final language. Word or-
der does not play a case-marking role. Instead,
postpositions function as case markers (CMs).
The basic structure of a Japanese sentence is as
follows:
(1) kare
he
ga
nom-CM
hon
book
wo
acc-CM
kaku
write
(he writes a book)
ga and wo are postpositions which mean nom-
inative and accusative, respectively. kare ga
and hon wo are case components, and kaku is a
verb1.
There are two phenomena that case markers
are hidden.
A modifying clause is left to the modified
noun in Japanese. In this paper, we call a
noun modified by a clause clausal modifiee.
A clausal modifiee is usually a case component
for the verb of the modifying clause. There is,
however, no case marker for their relation.
1In this paper, we call verbs, adjectives, and
noun+copulas as verbs for convenience.
(2) hon
book
wo
acc-CM
kaita
write
hito
person
(the person who wrote the book)
(3) kare
he
ga
nom-CM
kaita
write
hon
book
(a book which he wrote)
In (2), hito ?person? has ga ?nominative? rela-
tion to kaita ?write?. In (3), hon ?book? has wo
?accusative? relation to kaita ?write?.
There are some non case-marking postposi-
tions, such as wa and mo. They topicalize or
emphasize noun phrases. We call them topic
markers (TMs) and a phrase followed by one
of them TM phrase.
(4) kare
he
wa
TM
hon
book
wo
acc-CM
kaita
write
(he wrote a book)
(5) kare
he
ga
nom-CM
hon
book
mo
TM
kaita
write
(he wrote a book also)
In (4), wa is interpreted as ga ?nominative?. In
(5), mo is interpreted as wo ?accusative?.
3 Construction of the initial case
frame dictionary
This section describes how to construct the ini-
tial case frame dictionary. This is the first stage
of our two-stage approach, and is performed by
the method proposed by (Kawahara and Kuro-
hashi, 2001). In the rest of this section, we de-
scribe this approach in detail.
The biggest problem in automatic case frame
construction is verb sense ambiguity. Verbs
which have different meanings should have dif-
ferent case frames, but it is hard to disam-
biguate verb senses very precisely. To deal
with this problem, we distinguish predicate-
argument examples, which are collected from
a large corpus, by coupling a verb and its
closest case component. That is, examples
are not distinguished by verbs such as naru
?make/become? and tsumu ?load/accumulate?,
but by couples such as ?tomodachi ni naru?
?make a friend?, ?byouki ni naru? ?become
sick?, ?nimotsu wo tsumu? ?load baggage?, and
?keiken wo tsumu? ?accumulate experience?.
This process makes separate case frames
which have almost the same meaning or usage.
For example, ?nimotsu wo tsumu? ?load bag-
gage? and ?busshi wo tsumu? ?load supply? are
separate case frames. To merge these similar
case frames and increase coverage of the case
frame, we cluster the case frames.
We employ the following procedure for the
automatic case frame construction:
1. A large raw corpus is parsed by a Japanese
parser, and reliable predicate-argument ex-
amples are extracted from the parse re-
sults. Nouns with a TM such as wa or
mo and clausal modifiees are discarded, be-
cause their case markers cannot be under-
stood by syntactic analysis.
2. The extracted examples are bundled ac-
cording to the verb and its closest case com-
ponent, making initial case frames.
3. The initial case frames are clustered using
a similarity measure, resulting in the final
case frames. The similarity is calculated by
using NTT thesaurus.
We constructed a case frame dictionary from
newspaper articles of 20 years (about 20,000,000
sentences).
4 Target expressions
The following expressions could not be handled
with the initial case frame dictionary shown in
section 3, because of lack of information in the
case frame.
Non-gapping relation
This is the case in which the clausal modifiee
is not a case component of the verb in the modi-
fying clause, but is semantically associated with
the clause.
(6) kare ga
he
syudoken wo
initiative
nigiru
have
kaigi
meeting
(the meeting in which he has the initiative)
In this example, kaigi ?meeting? is not a case
component of nigiru ?have?, and there is no case
relation between kaigi and nigiru. We call this
relation non-gapping relation.
Double nominative sentence
This is the case in which the verb has two
nominatives in sentences such as the following.
(7) kuruma
car
wa
TM
engine ga
engine
yoi
good
(the engine of the car is good)
In this example, wa plays a role of nominative,
so yoi ?good? subcategorizes two nominatives:
kuruma ?car? and engine. We call this outer
nominative outer ga and this sentence double
nominative sentence.
Case change
In Japanese, to express the same meaning,
we can use different case markers. We call this
phenomenon case change.
(8) Tom ga
Tom
Mary
Mary
no
of
shiji wo
support
eta
derive
(Tom derived his support from Mary)
In this example, Mary has kara ?from? relation
to eta ?derive?. In this paper, we handle case
change related to no ?of?, such as (no, kara).
The following is an example that outer nom-
inative is related to no case.
(9) kuruma no
car
engine ga
engine
yoi
good
(the engine of the car is good)
The outer nominative of (7) can be nominal
modifier of the inner nominative like this ex-
ample. This is case change of (no, outer ga).
There is a different case from the above that
an NP with no modifying a case component
does not have a case relation to the verb.
(10) kare ga
he
kaigi no
meeting
syudoken wo
initiative
nigiru
have
(he has the initiative in the meeting)
In this example, kaigi ?meeting? has a no rela-
tion to syudoken ?initiative?, but does not have a
case relation to nigiru ?have?. This example is a
transformation of (6), and includes case change
of (no, non-gapping).
5 Fertilization of case frame
dictionary
We construct a fertilized case frame dictionary
from the initial case frame dictionary shown in
section 3, to handle the complicated expressions
described in section 4.
We apply case analysis to a large corpus using
the dictionary, collect information which could
not be acquired by a mere parsing, and upgrade
the case frame dictionary.
The procedure is as follows (figure 1):
1. The initial case frames are acquired by the
method shown in section 3.
2. Case analysis utilizing the case frames ac-
quired in phase 1 is applied to a large cor-
pus, and examples of outer nominative are
collected from case analysis results.
3. Case analysis utilizing the case frames ac-
quired in phase 2 is applied to the large
corpus, and examples of non-gapping rela-
tion are collected similarly.
4. Case similarities are judged to handle case
change.
5.1 Case analysis based on the initial
case frame dictionary
Case analysis of TM phrases and clausal modi-
fiees is indebted to a case frame dictionary. This
section describes an example of case analysis
utilizing the initial case frame dictionary.
(11) sono
that
hon
book
wa
TM
kare ga
he
tosyokan
library
de
in
yonda
read
(he read that book in the library)
Case analysis of this example chooses the fol-
lowing case frame ?tosyokan de yonda? ?read in
the library? (?*? in the case frame means the
closest CM.).
CM examples input
read
nom person, child, ? ? ? he
acc book, paper, ? ? ? book
loc* library, house, ? ? ? library
kare ?he? and tosyokan ?library? correspond to
nominative and locative, respectively, according
to the surface cases. The case marker of TM
phrase ?hon wa? ?book (TM)? cannot be under-
stood by the surface case, but it is interpreted
as wo ?accusative? because of the matching be-
tween ?hon wa? ?book (TM)? and the accusative
case slot of the case frame (underlined in the
case frame).
gano fueru
consumer
bank
company{ }outerga{ }corporationbankJapan { }effectresultprofit}{
(1) case frames based on
    parsing
(2) collection of
    outer nominative
(3) collection of
    non-gapping relation
(4) case similarity judgement
Figure 1: Outline of our method
5.2 Collecting examples of outer
nominative
In the initial case frame construction described
in section 3, the TM phrase was discarded, be-
cause its case marker could not be understood
by parsing. In the example (7), ?engine ga yoi?
?the engine is good? is used to build the initial
case frame, but the TM phrase ?kuruma wa?
?the car? is not used.
Case analysis based on the initial case frame
dictionary tells a case of a TM phrase. Corre-
spondence to outer nominative cannot be under-
stood by the case slot matching, but indirectly.
If the TM cannot correspond to any case slots of
the initial case frame, the TM can be regarded
as outer nominative. For example, in the case
of (7), since the case frame of ?engine ga yoi?
?the engine is good? has only nominative which
corresponds to ?engine?, the TM of ?kuruma
wa? cannot correspond to any case slots and is
recognized as outer nominative. On the other
hand, in the case of (11), the TM of hon wa
is recognized as accusative, because hon ?book?
is similar to the examples of the accusative slot.
We can distinguish and collect outer nominative
examples in this way.
We apply the following procedure to each sen-
tence which has both a TM and ga. To reduce
the influence of parsing errors, the collection
process of these sentences is done under the con-
dition that a TM phrase has no candidates of
its modifying head without its verb.
1. We apply case analysis to a verb which is a
head of a TM phrase. If the verb does not
have the closest case component and can-
not select a case frame, we quit processing
this sentence and proceed to the next sen-
tence. In this phase, the TM phrase is not
made correspondence with a case of the se-
lected case frame.
2. If the case frame does not have any cases
which have no correspondence with the
case components in the input, the TM can-
not correspond to any case slots and is
regarded as outer nominative. This TM
phrase is added to outer nominative exam-
ples of the case frame.
The following is an example of this process.
(12) nagai
long
sumo
sumo
wa
TM
ashi-koshi ni
legs and loins
futan ga
burden
kakaru
impose
(long sumo imposes a burden on legs and
loins)
Case analysis of this example chooses the fol-
lowing case frame ?futan ga kakaru? ?impose a
burden?.
CM examples input
impose nom* burden burdendat heart, legs, loins, ? ? ? legs and loins
futan ?burden? and ashi-koshi ?legs and loins?
correspond to nominative and dative of the case
frame, respectively, and sumo corresponds to no
case marker. Accordingly, the TM of ?sumo
wa? is recognized as outer nominative, and
sumo is added to outer nominative examples of
the case frame ?futan ga kakaru?.
This process made outer nominative of 15,302
case frames (of 597 verbs).
5.3 Collecting examples of non-gapping
relation
Examples of non-gapping relation can be col-
lected in a similar way to outer nominative.
When a clausal modifiee has non-gapping re-
lation, it should not be similar to any exam-
ples of any cases in the case frame, because the
constructed case frames have examples of only
cases except for non-gapping relation. From this
point of view, we apply the following procedure
to each example sentence which contains a mod-
ifying clause. To reduce the influence of pars-
ing errors, the collection process of example sen-
tences is done under the condition that a verb
in a clause has no candidates of its modifying
head without its clausal modifiee (?? ? ? [modify-
ing verb] N1 no N2? is not collected).
1. We apply case analysis to a verb which is
contained by a modifying clause. If the
verb does not have the closest case compo-
nent and cannot select a case frame, we quit
processing this sentence and proceed to the
next sentence. In this phase, the clausal
modifiee is not made correspondence with
a case of the selected case frame.
2. If the similarity between the clausal modi-
fiee and examples of any cases which have
no correspondence with input case com-
ponents does not exceed a threshold, this
clausal modifiee is added to examples of
non-gapping relation in the case frame. We
set the threshold 0.3 empirically.
The following is an example of this process.
(13) gyomu
business
wo itonamu
carry on
menkyo
license
wo syutoku-shita
get
(? got a license to carry on business)
Case analysis of this example chooses the follow-
ing case frame ?{gyomu, business} wo itonamu?
?carry on { work, business }?.
CM examples input
carry on nom bank, company, ? ? ? -acc* work, business business
Nominative of this case frame has no corre-
spondence with a case component of the in-
put, so the clausal modifiee, menkyo ?license?,
is checked whether it can correspond to nom-
inative case examples. In this case, the sim-
ilarity between menkyo ?license? and examples
of nominative is not so high. Consequently, the
relation of menkyo ?license? is recognized as non-
gapping relation, and menkyo is added to exam-
ples of non-gapping relation in the case frame
?{gyomu, business} wo itonamu?.
(14) ihouni
illegally
denwa
telephone
gyomu
business
wo
itonande-ita
carry on
utagai
suspect
(suspect that ? carried on telephone busi-
ness illegally)
In this case, the above case frame is also se-
lected. Since utagai ?suspect? is not similar to
the nominative case examples, it is added to
case examples of non-gapping relation in the
case frame.
This process made non-gapping relation of
23,094 case frames (of 637 verbs).
Collecting examples of non-gapping rela-
tion for all the case frames
Non-gapping relation words which have wide
distribution over verbs can be considered to
have non-gapping relation for all the verbs or
case frames. We add these words to examples
of non-gapping relation of all the case frames.
For example, 5 verbs have menkyo ?license? (ex-
ample (13)) in their non-gapping relation, and
381 verbs have utagai ?suspect? (example (14)).
We, consequently, judge utagai has non-gapping
relation for all the case frames. We call such a
word global non-gapping word.
We treated words which have non-gapping re-
lation for more than 100 verbs as global non-
gapping words. We acquired 128 global non-
gapping words, and the following is the exam-
ples of them (in English).
possibility, necessity, result, course, case,
thought, schedule, outlook, plan, chance,
? ? ?
5.4 Case similarity judgement
To deal with case change, we applied the fol-
lowing process to every case frame with outer
nominative and non-gapping relation.
1. A similarity of every two cases is calculated.
It is the average of similarities between all
the combinations of case examples. But
similarities of couples of basic cases are not
handled, such as (ga, wo), (ga, ni), (wo,
ni), and so on.
2. A couple whose similarity exceeds a thresh-
old is judged to be similar, and is merged
into one case. We set the threshold 0.8 em-
pirically.
The following example is the case when this
process is applied to ?{setsumei, syakumei} wo
motomeru? ?demand {explanation, excuse}?.
CM examples
demand
nom committee, group,
acc* explanation, excuse
dat government, president,
about progress, condition, state,
no progress, reason, content,
In this case frame, the examples of no ?of?2 are
similar to those of ni-tsuite ?about?, and the sim-
ilarity between them is very high, 0.94, so these
case examples are merged into a new case no/ni-
tsuite ?of/about?.
By this process, 6,461 couples of similar cases
are merged. An NP with no modifying a case
component can be analyzed by this merging.
6 Case Analysis
To perform case analysis, we basically employ
the algorithm proposed by (Kurohashi and Na-
gao, 1994). In this section, our case analysis
method of the complicated expressions shown
in section 4 is described.
6.1 Analysis of clausal modifiees
If an clausal modifiee is a function word such
as koto ?(that clause)? or tame ?due?, or a time
expression such as 3 ji ?three o?clock? or saikin
?recently?, it is analyzed as non-gapping rela-
tion.
2In no case in case frames, every noun which modifies
the closest case component of the verb is collected.
The other clausal modifiee can correspond
to ga ?nominative?, wo ?accusative?, ni ?dative?,
outer ga ?outer nominative?, non-gapping rela-
tion, or no ?of?. We decide a corresponding case
which maximizes the score3 of the verb in the
clause. If a clausal modifiee corresponds to ga,
wo, ni, or outer ga, the relation is decided as it
is. If it corresponds to non-gapping relation or
no, the relation is decided as non-gapping re-
lation. In the case of corresponding to no, the
clausal modifiee has no relation to the closest
case component of the verb.
A clausal modifiee can correspond to non-
gapping relation or no under the condition that
similarity between the clausal modifiee and case
examples of non-gapping relation or no is the
maximum value (which means two nouns locate
in the same node in a thesaurus). This is be-
cause a noun which is a little similar to case
examples of non-gapping relation may not have
non-gapping relation.
6.2 Analysis of TM phrases
If a TM phrase is a time expression, it is ana-
lyzed as time case. The other TM phrase can
correspond to ga ?nominative?, wo ?accusative?,
or outer ga ?outer nominative?. We decide a
corresponding case which maximizes the score
of the verb modified by the TM phrase. When
the verb has both a case component with ga and
a TM phrase, the case component with ga cor-
responds to ga in the selected case frame, and
its TM phrase corresponds to wo or outer ga. If
the correspondence between the TM phrase and
outer ga case components gets the best similar-
ity, the input sentence is recognized as a double
nominative sentence.
6.3 Analysis of case change
If the selected case frame of the input verb has
merged cases which include no ?of?, no case in
the input sentence is interpreted as the counter-
part of no between the merged cases. If not, the
no case is considered not to have a case relation
to the verb and has no corresponding case in
the case frame.
3This score is the sum of each similarity between an
input case component and examples of the corresponding
case in the case frame.
Table 1: Case analysis accuracy
clausal
modifiee TM
our method 301/358 307/345
84.0% 88.9%
baseline 287/358 305/345
80.1% 88.4%
Table 2: Non-gapping relation accuracy
precision recall F
our method 82/116 82/92
70.7% 89.1% 78.8%
baseline 88/148 88/92
59.5% 95.7% 73.3%
7 Experiment
We made a case analysis experiment on
Japanese relevance-tagged corpus (Kawahara et
al., 2002). This corpus has correct tags of
predicate-argument relations. We conducted
case analysis on an open test set which consists
of 500 sentences, and evaluated clausal modi-
fiees and TM phrases in these sentences. To
evaluate the real case analysis without influence
of parsing errors, we input the correct structure
of the corpus sentences to the analyzer.
The accuracy of clausal modifiees and TM
phrases is shown in table 1, and the accuracy of
non-gapping relation is shown in table 2. The
baseline of these tables is that if a clausal mod-
ifiee belongs to a non-gapping noun dictionary
in which nouns always having non-gapping re-
lation as clausal modifiees are written, it is an-
alyzed as non-gapping relation.
The accuracy of clausal modifiees increased
by 4%. This shows effectiveness of our fertil-
ization process. However, the accuracy of TM
phrases did not increase. This is because the ac-
curacy of TM phrases which were analyzed us-
ing added outer nominative examples was 4/6,
and its frequency was too low. The accuracy of
case change was 2/4.
8 Related work
There has been some related work analyzing
clausal modifiees and TM phrases. Baldwin et
al. analyzed clausal modifiees with heuristic
rules or decision trees considering various lin-
guistic features (Baldwin et al, 1999). Its ac-
curacy was about 89%. Torisawa analyzed TM
phrases using predicate-argument cooccurences
and word classifications induced by the EM al-
gorithm (Torisawa, 2001). Its accuracy was
about 88% for wa and 84% for mo.
It is difficult to compare the accuracy because
the range of target expressions is different. Un-
like related work, it is promising to utilize our
resultant case frame dictionary for subsequent
analyzes such as ellipsis or discourse analysis.
9 Conclusion
This paper proposed a method of fertilizing
the case frame dictionary to realize an analy-
sis of the complicated expressions, such as dou-
ble nominative sentences, non-gapping relation,
and case change. We can analyze these expres-
sions accurately using the fertilized case frame
dictionary. So far, accuracy of subsequent an-
alyzes such as ellipsis or discourse analysis has
not been so high, because double nominative
sentences and non-gapping relation cannot be
analyzed accurately. It is promising to improve
the accuracy of these analyzes utilizing the fer-
tilized case frame dictionary.
References
Timothy Baldwin, Takenobu Tokunaga, and
Hozumi Tanaka. 1999. The parameter-based
analysis of Japanese relative clause construc-
tions. In IPSJ SIJ Notes 1999-NL-134, pages
55?62.
Daisuke Kawahara and Sadao Kurohashi. 2001.
Japanese case frame construction by coupling
the verb and its closest case component. In
Proceedings of the Human Language Technol-
ogy Conference, pages 204?210.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of
the 3rd International Conference on Lan-
guage Resources and Evaluation, pages 2008?
2013.
Sadao Kurohashi and Makoto Nagao. 1994.
A method of case structure analysis for
Japanese sentences based on examples in
case frame dictionary. In IEICE Transactions
on Information and Systems, volume E77-D
No.2.
Kentaro Torisawa. 2001. An unsupervised
method for canonicalization of Japanese post-
positions. In Proceedings of the 6th Natural
Language Processing Pacific Rim Simposium,
pages 211?218.
 
				Coling 2010: Poster Volume, pages 534?542,
Beijing, August 2010
Identifying Contradictory and Contrastive Relations between Statements
to Outline Web Information on a Given Topic
Daisuke Kawahara? Kentaro Inui?? Sadao Kurohashi??
?National Institute of Information and Communications Technology
?Graduate School of Information Sciences, Tohoku University
?Graduate School of Informatics, Kyoto University
dk@nict.go.jp, inui@ecei.tohoku.ac.jp, kuro@i.kyoto-u.ac.jp
Abstract
We present a method for producing a
bird?s-eye view of statements that are ex-
pressed on Web pages on a given topic.
This method aggregates statements that
are relevant to the topic, and shows con-
tradictory and contrastive relations among
them. This view of contradictions and
contrasts helps users acquire a top-down
understanding of the topic. To realize
this, we extract such statements and re-
lations, including cross-document implicit
contrastive relations between statements,
in an unsupervised manner. Our experi-
mental results indicate the effectiveness of
our approach.
1 Introduction
The quantity of information on theWeb is increas-
ing explosively. Online information includes news
reports, arguments, opinions, and other coverage
of innumerable topics. To find useful information
from such a mass of information, people gener-
ally use conventional search engines such as Ya-
hoo! and Google. They input keywords to a search
engine as a query and obtain a list of Web pages
that are relevant to the keywords. They then use
the list to check several dozen top-ranked Web
pages one by one.
This method of information access does not
provide a bird?s-eye view of the queried topic;
therefore it can be highly time-consuming and dif-
ficult for a user to gain an overall understanding of
what is written on the topic. Also, browsing only
top-ranked Web pages may provide the user with
biased information. For example, when a user
direct contrastive statement ?A is more P than B?
contrastive keyword pair (A, B)
contradictory relation ?A is P? ? ?A is not P?
contrastive relation ?A is P? ? ?B is P (not P)?
Table 1: Overview of direct contrastive state-
ments, contrastive keyword pairs and contradic-
tory/contrastive relations. Note that ?P? is a pred-
icate.
searches for information on ?agaricus,? claimed
to be a health food, using a conventional search
engine, many commercial pages touting its health
benefits appear at the top of the ranks, while other
pages remain low-ranked. The user may miss an
existing Web page that indicates its unsubstanti-
ated health benefits, and could be unintentionally
satisfied by biased or one-sided information.
This paper proposes a method for produc-
ing a bird?s-eye view of statements that are ex-
pressed on Web pages on a given query (topic).
In particular, we focus on presenting contradic-
tory/contrastive relations and statements on the
topic. This presentation enables users to grasp
what arguing points exist and furthermore to see
contradictory/contrastive relations between them
at a glance. Presenting these relations and state-
ments is thought to facilitate users? understanding
of the topic. This is because people typically think
about contradictory and contrastive entities and is-
sues for decision-making in their daily lives.
Our system presents statements and relations
that are important and relevant to a given topic,
including the statements and relations listed in Ta-
ble 1. Direct contrastive statements compare two
entities or issues in a single sentence. The con-
trasted entities or issues are also extracted as con-
trastive keyword pairs. In addition to them, our
534
sekken-wa gosei senzai-to chigai, kankyo-ni yoi. 
!"#$%&'%("")%*"+%,-.%./0&+"/1./,%#'%2"1$#+.)%,"%'3/,-.42%).,.+(./,5!
gosei senzai-de yogore-ga ochiru (15) 
6#'-%',#&/'%6&,-%'3/,-.42%).,.+(./,!
gosei senzai-ni dokusei-ga aru (9) 
'3/,-.42%).,.+(./,%-#'%,"7&2&,3! gosei senzai-ni dokusei-ga nai (2) '3/,-.42%).,.+(./,%&'%/",%,"7&2!
sekken-de yogore-ga ochi-nai (6) 
/",%+.1"0.%',#&/%6&,-%'"#$! sekken-de yogore-ga ochiru (4) +.1"0.%',#&/'%6&,-%'"#$!
goseisenzai-de te-ga areru (7) 
13%-#/)%(.,'%+"8(-%6&,-%'3/).,!
gosei senzai-wa kaimen kasseizai-wo fukumu (5) 
'3/,-.42%).,.+(./,%2"/,#&/'%'8+*#2,#/,!
[direct contrastive statement]!
contrastive relation!
contradictory relation!
Legend:!
Figure 1: Examples of statements on ?gosei senzai? (synthetic detergent), which are represented by
rounded rectangles. Each statement is linked with the pages from which it is extracted. The number in
a parenthesis represents the number of pages.
system shows contradictory and contrastive rela-
tions between statements. Contradictory relations
are the relations between statements that are con-
tradictory about an entity or issue. Contrastive
relations are the relations between statements in
which two entities or issues are contrasted.
In particular, we have the following two novel
contributions.
? We identify contrastive relations between
statements, which consist of in-document
and cross-document implicit relations.
These relations complement direct con-
trastive statements, which are explicitly
mentioned in a single sentence.
? We precisely extract direct contrastive state-
ments and contrastive keyword pairs in an
unsupervised manner, whereas most previ-
ous studies used supervised methods (Jindal
and Liu, 2006b; Yang and Ko, 2009).
Our system focuses on the Japanese language.
For example, Figure 1 shows examples of ex-
tracted statements on the topic ?gosei senzai?
(synthetic detergent). Rounded rectangles repre-
sent statements relevant to this topic. The first
statement is a direct contrastive statement, which
refers to a contrastive keyword pair, ?gosei sen-
zai? (synthetic detergent) and ?sekken? (soap).
The pairs of statements connected with a broad
arrow have contradictory relations. The pairs of
statements connected with a thin arrow have con-
trastive relations. Users not only can see what is
written on this topic at a glance, but also can check
out the details of a statement by following its links
to the original pages.
2 Related Work
Studies have been conducted on automatic extrac-
tion of direct contrastive sentences (comparative
sentences) for English (Jindal and Liu, 2006b) and
for Korean (Yang and Ko, 2009). They prepared a
set of keywords that serve as clues to direct con-
trastive sentences and proposed supervised tech-
niques on the basis of tagged corpora. We pro-
pose an unsupervised method for extracting direct
contrastive sentences without constructing tagged
corpora.
From direct contrastive sentences, Jindal and
Liu (2006a) and Satou and Okumura (2007) pro-
posed methods for extracting quadruples of (tar-
get, basis, attribute, evaluation). Jindal and Liu
(2006a) extracted these quadruples and obtained
an F-measure of 70%-80% for the extraction of
?target? and ?basis.? Since this extraction was
535
not their main target, they did not perform er-
ror analysis on the extracted results. Satou and
Okumura (2007) extracted quadruples from blog
posts. They provided a pair of named entities
for ?target? and ?basis,? whereas we automati-
cally identify such pairs. Ganapathibhotla and Liu
(2008) proposed a method for detecting which en-
tities (?target? and ?basis?) in a direct contrastive
statement are preferred by its author.
There is also related work that focuses on non-
contrastive sentences. Ohshima et al (2006) ex-
tracted coordinated terms, which are semantically
broader than our contrastive keyword pairs, using
hit counts from a search engine. They made use
of syntactic parallelism among coordinated terms.
Their task was to input one of coordinated terms
as a query, which is different from ours. Soma-
sundaran and Wiebe (2009) presented a method
for recognizing a stance in online debates. They
formulated this task as debate-side classification
and solved it by using automatically learned prob-
abilities of polarity.
To aggregate statements and detect relations be-
tween them, one of important modules is recogni-
tion of synonymous, entailed, contradictory and
contrastive statements. Studies on rhetorical
structure theory (Mann and Thompson, 1988) and
recognizing textual entailment (RTE) deal with
these relations. In particular, evaluative work-
shops on RTE have been held and this kind of re-
search has been actively studied (Bentivogli et al,
2009). The recent workshops of this series set up
a task that recognizes contradictions. Harabagiu
et al (2006), de Marneffe et al (2008), Voorhees
(2008), and Ritter et al (2008) focused on rec-
ognizing contradictions. For example, Harabagiu
et al (2006) used negative expressions, antonyms
and contrast discourse relations to recognize con-
tradictions. These methods only detect relations
between given sentences, and do not create a
bird?s-eye view.
To create a kind of bird?s-eye view, Kawahara et
al. (2008), Statement Map (Murakami et al, 2009)
and Dispute Finder (Ennals et al, 2010) identi-
fied various relations between statements includ-
ing contradictory relations, but do not handle con-
trastive relations, which are one of the important
relations for taking a bird?s-eye view on a topic.
Lerman and McDonald (2009) proposed a method
for generating contrastive summaries about given
two entities on the basis of KL-divergence. This
study is related to ours in the aspect of extracting
implicit contrasts, but contrastive summaries are
different from contrastive relations between state-
ments in our study.
3 Our Method
We propose a method for grasping overall infor-
mation on the Web on a given query (topic). This
method extracts and presents statements that are
relevant to a given topic, including direct con-
trastive statements and contradictory/contrastive
relations between these statements.
As a unit for statements, we use a predicate-
argument structure (also known as a case structure
and logical form). A predicate-argument struc-
ture represents a ?who does what? event. Pro-
cesses such as clustering, summarization, compar-
ison with other knowledge and logical consistency
verification, which are required for this study and
further analysis, are accurately performed on the
basis of predicate-argument structures. The ex-
traction of our target relations and statements is
performed via identification and aggregation of
synonymous, contrastive, and contradictory rela-
tions between predicate-argument structures.
As stated in section 1, we extract direct con-
trastive statements, contrastive keyword pairs, rel-
evant statements, contrastive relations and contra-
dictory relations. We do this with the following
steps:
1. Extraction and aggregation of predicate-
argument structures
2. Extraction of contrastive keyword pairs and
direct contrastive statements
3. Identification of contradictory relations
4. Identification of contrastive relations
Below, we first describe our method of extract-
ing and aggregating predicate-argument struc-
tures. Then, we explain our method of extract-
ing direct contrastive statements with contrastive
keyword pairs, and identifying contradictory and
contrastive relations in detail.
536
3.1 Extraction and Aggregation of
Predicate-argument Structures
A predicate-argument structure consists of a pred-
icate and one or more arguments that have a de-
pendency relation to the predicate.
We extract predicate-argument structures from
automatic parses of Web pages on a given topic
by using the method of Kawahara et al (2008).
We apply the following procedure to Web pages
that are retrieved from the TSUBAKI (Shinzato
et al, 2008) open search engine infrastructure, by
inputting the topic as a query.
1. Extract important sentences from each Web
page. Important sentences are defined as sen-
tences neighboring the topic word(s).
2. Obtain results of morphological analysis
(JUMAN1) and dependency parsing (KNP2)
of the important sentences, and extract
predicate-argument structures from them.
3. Filter out functional and meaningless
predicate-argument structures, which are
not relevant to the topic. Pointwise mutual
information between the entire Web and the
target Web pages for a topic is used.
Note that the analyses in step 2 are performed be-
forehand and stored in an XML format (Shinzato
et al, 2008).
Acquired predicate-argument structures vary
widely in their representations of predicates and
arguments. In particular, many separate predicate-
argument structures have the same meaning due to
spelling variations, transliterations, synonymous
expressions and so forth. To cope with this prob-
lem, we apply ?keyword distillation? (Shibata
et al, 2009), which is a process of absorbing
spelling variations, synonymous expressions and
keywords with part-of relations on a set of Web
pages about a given topic. As a knowledge source
to merge these expressions, this process uses a
knowledge base that is automatically extracted
from an ordinary dictionary and the Web. For
instance, the following predicate-argument struc-
tures are judged to be synonymous3.
1http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
3In this paper, we use the following abbreviations:
(1) a. sekken-wo
soap-ACC
tsukau
use
b. sopu-wo
soap-ACC
tsukau
use
c. sekken-wo
soap-ACC
shiyou-suru
utilize
We call the predicate-argument structures that
are obtained as the result of the above proce-
dure statement candidates. The final output of
our system consists of direct contrastive state-
ments (with contrastive keyword pairs), top-N
statements (major statements) in order of fre-
quency of statement candidates, and statements
with contradictory/contrastive relations. Contra-
dictory/contrastive relations are identified against
major statements by searching statement candi-
dates.
Another outcome of keyword distillation is a re-
sultant set of keywords that are important and rel-
evant to the topic. We call this set of keywords
relevant keywords, which also include words or
phrases in the query. Relevant keywords are used
to extract contrastive keyword pairs.
3.2 Extraction of Contrastive Keyword Pairs
and Direct Contrastive Statements
We extract contrastive keyword pairs from con-
trastive constructs, which are manually speci-
fied as patterns of predicate-argument structures.
Statements that contain contrastive constructs are
defined as direct contrastive statements.
For example, the following sentence is a typi-
cal direct contrastive statement, which contains a
contrastive verb ?chigau? (differ).
(2) sekken-wa
soap-TOP
gosei senzai-to
synthetic detergent-ABL
chigai,
differ
? ? ?
(soap differs from synthetic detergent, ? ? ?)
From this sentence, a contrastive keyword pair,
?sekken? (soap) and ?gosei senzai? (synthetic de-
tergent), is extracted. The above sentence is ex-
tracted as a direct contrastive statement.
We preliminarily evaluated this simple pattern-
based method and found that it has the following
three problems.
NOM (nominative), ACC (accusative), DAT (dative),
ABL (ablative), CMI (comitative), GEN (genitive) and
TOP (topic marker).
537
? Keyword pairs that are mentioned in a con-
trastive construct are occasionally not rele-
vant to the given topic.
? Non-contrastive keyword pairs are erro-
neously extracted due to omissions of at-
tributes and targets of comparisons.
? Non-contrastive keyword pairs that have an
is-a relation are erroneously extracted.
To deal with the first problem, we filter out key-
word pairs that are contrastive but that are not rel-
evant to the topic. For this purpose, we apply fil-
tering by using relevant keywords, which are de-
scribed in section 3.1.
As an example of non-contrastive keyword
pairs (the second problem), from the following
sentence, a keyword pair, ?tokkyo seido? (patent
system) and ?nihon? (Japan), is incorrectly ex-
tracted by the pattern-based method.
(3) amerika-no
America-GEN
tokkyo seido-wa
patent system-TOP
nihon-to
Japan-ABL
kotonari,
different
? ? ?
(patent system of America is different from ? of
Japan ? ? ?)
In this sentence, ?nihon? (Japan) has a meaning of
?nihon-no tokkyo seido? (patent system of Japan).
That is to say, ?tokkyo seido? (patent system),
which is the attribute of comparison, is omitted.
In this study, in addition to patterns of con-
trastive constructs, we use checking and filtering
on the basis of similarity. The use of similarity
is inspired by the semantic parallelism between
contrasted keywords. As this similarity, we em-
ploy distributional similarity (Lin, 1998), which
is calculated using automatic dependency parses
of 100 million Japanese Web pages. By search-
ing similar keywords from the above sentence, we
successfully extract a contrastive keyword pair,
?amerika? (America) and ?nihon? (Japan), and
the above sentence as a direct contrastive state-
ment.
Similarly, a target of comparison can be omitted
as in the following sentence.
(4) nedan-wa
price-TOP
gosei senzai-yori
synthetic detergent-ABL
takaidesu
high
(price of ? is higher than synthetic detergent)
In this example, the similarity between ?nedan?
(price) and ?gosei senzai? (synthetic detergent) is
lower than a threshold, and this sentence and the
extracts from it are filtered out.
As for the third problem, we may extract non-
contrastive keyword pairs that have an is-a rela-
tion. From the following sentence, we incorrectly
extract a contrastive keyword pair, ?konbini? (con-
venience store) and ?7-Eleven,? which cannot be
filtered out due to its high similarity.
(5) 7-Eleven-wa
7-Eleven-TOP
hokano
other
konbini-to
convenience store-ABL
kurabete,
compare
? ? ?
(7-Eleven is ? ? ? compared to other convenience
stores)
To deal with this problem, we use a filter on the
basis of a set of words that indicate the existence
of hypernyms, such as ?hokano? (other) and ip-
panno (general). We prepare six words for this
purpose.
To sum up, we use the following procedure to
identify contrast keyword pairs.
1. Extract predicate-argument structures that do
not match the above is-a patterns and match
one of the following patterns. They are ex-
tracted from the statement candidates.
? X-wa Y-to {chigau | kotonaru | kuraberu}
(X {differ | vary | compare} from/with Y)
? X-wa Y-yori [adjective]
(X is more ? ? ? than Y)
Note that each of X and Y is a noun phrase
in the argument position.
2. Extract (x, y) that satisfies both the follow-
ing conditions as a contrastive keyword pair.
Note that (x, y) is part of a word sequence in
(X, Y), respectively.
? Both x and y are included in a set of rel-
evant keywords.
? (x, y) has the highest similarity among
any other candidates of (x, y), and this
similarity is higher than a threshold.
Note that the threshold is determined based on a
preliminary experiment using a set of synonyms
(Aizawa, 2007). We extract the sentence that con-
tains the predicate-argument structure used in step
1 as a direct contrastive statement.
538
3.3 Identification of Contradictory Relations
We identify contradictory relations between state-
ment candidates. In this paper, contradictory re-
lations are defined as the following two types
(Kawahara et al, 2008).
negation of predicate
If the predicate of a candidate statement is
negated, its contradiction has the same or synony-
mous predicate without negation. If not, its con-
tradiction has the same or synonymous predicate
with negation.
(6) a. sekken-ga
soap-NOM
kankyou-ni
environment-DAT
yoi
good
b. sekken-ga
soap-NOM
kankyou-ni
environment-DAT
yoku-nai
not good
antonym of predicate
The predicate of a contradiction is an antonym
of that of a candidate statement. To judge antony-
mous relations, we use an antonym lexicon ex-
tracted from a Japanese dictionary (Shibata et al,
2008). This lexicon consists of approximately
2,000 entries.
(7) a. gosei senzai-ga
synthetic detergent-NOM
anzen-da
safe
b. gosei senzai-ga
synthetic detergent-NOM
kiken-da
dangerous
To identify contradictory relations between
statements in practice, we search statement can-
didates that satisfy one of the above conditions
against major statements.
3.4 Identification of Contrastive Relations
We identify contrastive relations between state-
ment candidates. In this paper, we define a con-
trastive relation as being between a pair of state-
ment candidates whose arguments are contrastive
keyword pairs and whose predicates have synony-
mous or contradictory relations. Contradictory re-
lations of predicates are defined in the same way
as section 3.3.
In the following example, (a, b) and (a, c) have
a contrastive relation. Also, (b, c) has a contradic-
tory relation.
(8) a. gosei senzai-de
synthetic detergent-CMI
yogore-ga
stain-NOM
ochiru
wash
Topic: bio-ethanol
? (bio-ethanol fuel, gasoline)
(bio-ethanol car, electric car)
Topic: citizen judgment system
? (citizen judgment system, jury system)
(citizen judgment system, lay judge system)
Topic: patent system
? (patent system, utility model system)
(large enterprise, small enterprise)
Topic: Windows Vista
? (Vista, XP)
Table 2: Examples of extracted contrastive key-
word pairs (translated into English).
b. sekken-de
soap-CMI
yogore-ga
stain-NOM
ochiru
wash
c. sekken-de
soap-CMI
yogore-ga
stain-NOM
ochi-nai
not wash
The process of identifying contrastive relations
between statements is performed in the same way
as the identification of contradictory relations.
That is to say, we search statement candidates
that satisfy the definition of contrastive relations
against major statements.
4 Experiments
We conducted experiments for extracting con-
trastive keyword pairs, direct contrastive state-
ments and contradictory/contrastive relations on
50 topics, such as age of adulthood, anticancer
drug, bio-ethanol, citizen judgment system, patent
system and Windows Vista.
We retrieve at most 1,000 Web pages for a topic
from the search engine infrastructure, TSUBAKI.
As major statements, we extract 10 statement can-
didates in order of frequency.
Below, we first evaluate the extracted con-
trastive keyword pairs and direct contrastive state-
ments, and then evaluate the identified contradic-
tory and contrastive relations between statements.
4.1 Evaluation of Contrastive Keyword Pairs
and Direct Contrastive Statements
Contrastive keyword pairs and direct contrastive
statements were extracted on 30 of 50 topics. 99
direct contrastive statements and 73 unique con-
trastive keyword pairs were obtained on 30 topics.
The average number of obtained contrastive key-
word pairs for a topic was approximately 2.4. Ta-
539
Topic: ?tyosakuken hou? (copyright law)
?syouhyouken-wa tyosakuken-yori zaisantekina kachi-wo motsu.?
The trademark right has more financial value than the copyright.
?tyosakuken hou-de hogo-sareru? ? ?tyosakuken hou-de hogo-sare-nai?protected by the copyright law not protected by the copyright law
?tyosakuken-wo shingai-suru? ? ?tyosakuken-wo shingai-shi-nai?infringe the copyright not infringe the copyright
? ?syouhyouken-wo shingai-shi-nai?not infringe the trademark right
Topic: ?genshiryoku hatsuden syo? (nuclear power plant)
?genshiryoku hatsuden syo-wa karyoku hatsuden syo-to chigau.?
Nuclear power plants are different from thermoelectric power plants.
?CO2-wo hassei-shi-nai? ? ?CO2-wo hassei-suru?not emit carbon dioxide emit carbon dioxide
?genpatsu-wo tsukuru? ? ?genshiryoku hatsuden syo-wo tsukura-nai?construct a nuclear power plant not construct a nuclear power plant
? ?karyoku hatsuden syo-wo tsukuru?construct a thermoelectric power plant
Table 3: Examples of identified direct contrastive statements, contradictory relations and contrastive
relations. The sentences with two underlined parts are direct contrastive statements. The arrows ???
and ??? represent a contradictory relation and a contrastive relation, respectively.
ble 2 lists examples of obtained contrastive key-
word pairs. We successfully extracted not only
contrastive keyword pairs including topic words,
but also those without them.
Our manual evaluation of the extracted con-
trastive keyword pairs found that 89% (65/73) of
the contrastive keyword pairs are actually con-
trasted in direct contrastive statements. Correct
contrastive keyword pairs were extracted on 28 of
30 topics. We also evaluated the contrastive key-
word pairs extracted without similarity filtering.
In this case, 190 contrastive keyword pairs on 41
topics were extracted and 44% (84/190) of them
were correct. Correct contrastive keyword pairs
were extracted on 31 of 41 topics. Therefore, sim-
ilarity filtering did not largely decrease the recall,
but significantly increased the precision.
We have eight contrastive keyword pairs that
were incorrectly extracted by our proposed
method. These contrastive keyword pairs acciden-
tally have similarity that is higher than the thresh-
old. Major errors were caused by the ambiguity of
Japanese ablative keyword ?yori.?
(9) heisya-wa
our company-TOP
bitWallet sya-yori
bitWallet, Inc.-ABL
Edy gifuto-no
Edy gift-GEN
gyomu itaku-wo
entrustment-ACC
ukete-imasu
have
(Our company is entrusted with Edy gift by bitWal-
let, Inc.)
In this example, ?yori? means not the basis of
contrast but the source of action. The similar-
ity filtering usually prevents incorrect extraction
from such a non-contrastive sentence. However,
in this case, the pair of ?heisya? (our company)
and ?bitWallet sya? (bitWallet, Inc.) was not fil-
tered due to the high similarity between them. To
cope with this problem, it is necessary to use lin-
guistic knowledge such as case frames.
4.2 Evaluation of Contradictory and
Contrastive Relations
Contradictory relations were identified on 49 of
50 topics. For 49 topics, 268 contradictory re-
lations were identified. The average number of
identified contradictory relations for a topic was
5.5. Contrastive relations were identified on 18
of 30 topics, on which contrastive keyword pairs
were extracted. For the 18 topics, 60 contrastive
relations were identified. The average number of
identified contrastive relations for a topic was 3.3.
Table 3 lists examples of the identified contra-
dictory and contrastive relations as well as direct
contrastive statements. We manually evaluated
the identified contradictory relations and the con-
trastive relations that were identified for correct
contrastive keyword pairs. As a result, we con-
cluded that they completely obey our definitions.
We also classified each of the obtained contra-
dictory and contrastive relations into two classes:
?cross-document? and ?in-document.? ?Cross-
540
Topic: age of adulthood
lower the age of adulthood to 18
? lower the voting age to 18
Topic: anticancer drug
anticancer drugs have side effects
? anticancer drugs have effects
Table 4: Examples of unidentified contrastive re-
lations (translated into English).
document? means that a contradictory/contrastive
relation is obtained not from a single page but
across multiple pages. If a relation can be
obtained from both, we classified it into ?in-
document.? As a result, 67% (179/268) of contra-
dictory relations and 70% (42/60) of contrastive
relations were ?cross-document.? We can see that
many cross-document implicit relations that can-
not be retrieved from a single page were success-
fully identified.
4.3 Discussions
We successfully identified contradictory relations
on almost all the topics. However, out of 50 top-
ics, we extracted contrastive keyword pairs on 30
topics and contrastive relations on 18 topics. To
investigate the resultant contrastive relations from
the viewpoint of recall, we manually checked
whether there were unidentified contrastive rela-
tions among 100 statement candidates for each
topic. We actually checked 20 topics and found
six unidentified contrastive relations in total. Ta-
ble 4 lists examples of the unidentified contrastive
relations. Out of 20 topics, in total, 44 contrastive
relations are manually discovered on 13 topics,
but out of 13 topics, 38 contrastive relations are
identified on eight topics by our method. There-
fore, we achieved a recall of 86% (38/44) at rela-
tion level and 62% (8/13) at topic level. We can
see that our method was able to cover a relatively
wide range of contrastive relations on the topics
on which our method successfully extracted con-
trastive keyword pairs.
To detect such unidentified contrastive rela-
tions, it is necessary to robustly extract contrastive
keyword pairs. In the future, we will employ a
bootstrapping approach to identify patterns of di-
rect contrastive statements and contrastive key-
!"#$"%&'(#)*$+#'#,-#&'!
+.$&.'$!"#$"%&'(#)*$+#'#,-#&'!
!"#$"./0!
1/"($12'($"%&'(#)*$+#'#,-#&'!
1/"($12'($"./0!
3!#,%4$"%&'(#)*$+#'#,-#&'!
5678$92#1$.:$;!"#$"%&'(#)*$+#'#,-#&'<!
5678$92#1$.:$;+.$&.'$!"#$"%&'(#)*$+#'#,-#&'<!
Figure 2: A view of major, contradictory and con-
trastive statements in WISDOM.
word pairs. We will also use patterns of con-
trastive discourse structures as well as those of
predicate-argument structures.
5 Conclusion
This paper has described a method for producing a
bird?s-eye view of statements that are expressed in
Web pages on a given topic. This method aggre-
gates statements relevant to the topic and shows
the contradictory/contrastive relations and state-
ments among them.
In particular, we successfully extracted direct
contrastive statements in an unsupervised man-
ner. We specified only several words for the
extraction patterns and the filtering. Therefore,
our method for Japanese is thought to be easily
adapted to other languages. We also proposed
a novel method for identifying contrastive rela-
tions between statements, which included cross-
document implicit relations. These relations com-
plemented direct contrastive statements.
We have incorporated our proposed method
into an information analysis system, WISDOM4
(Akamine et al, 2009), which can show multi-
faceted information on a given topic. Now, this
system can show contradictory/contrastive rela-
tions and statements as well as their contexts as
a view of KWIC (keyword in context) (Figure 2).
This kind of presentation facilitates users? under-
standing of an input topic.
4http://wisdom-nict.jp/
541
References
Aizawa, Akiko. 2007. On calculating word similarity
using web as corpus. In Proceedings of IEICE Tech-
nical Report, SIG-ICS, pages 45?52 (in Japanese).
Akamine, Susumu, Daisuke Kawahara, Yoshikiyo
Kato, Tetsuji Nakagawa, Kentaro Inui, Sadao Kuro-
hashi, and Yutaka Kidawara. 2009. WISDOM:
A web information credibility analysis system. In
Proceedings of the ACL-IJCNLP 2009 Software
Demonstrations, pages 1?4.
Bentivogli, Luisa, Ido Dagan, Hoa Dang, Danilo Gi-
ampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge.
In Proceedings of TAC 2009 Workshop.
de Marneffe, Marie-Catherine, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08: HLT, pages
1039?1047.
Ennals, Rob, Beth Trushkowsky, and John Mark
Agosta. 2010. Highlighting disputed claims on the
web. In Proceedings of WWW 2010.
Ganapathibhotla, Murthy and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings
of COLING 2008, pages 241?248.
Harabagiu, Sanda, Andrew Hickl, and Finley Laca-
tusu. 2006. Negation, contrast and contradiction
in text processing. In Proceedings of AAAI-06.
Jindal, Nitin and Bing Liu. 2006a. Identifying com-
parative sentences in text documents. In Proceed-
ings of SIGIR 2006.
Jindal, Nitin and Bing Liu. 2006b. Mining compar-
ative sentences and relations. In Proceedings of
AAAI-06.
Kawahara, Daisuke, Sadao Kurohashi, and Kentaro
Inui. 2008. Grasping major statements and their
contradictions toward information credibility analy-
sis of web contents. In Proceedings of WI?08, short
paper, pages 393?397.
Lerman, Kevin and Ryan McDonald. 2009. Con-
trastive summarization: An experiment with con-
sumer reviews. In Proceedings of NAACL-HLT
2009, Companion Volume: Short Papers, pages
113?116.
Lin, Dekang. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL98, pages 768?774.
Mann, William and Sandra Thompson. 1988. Rhetor-
ical structure theory: toward a functional theory of
text organization. Text, 8(3):243?281.
Murakami, Koji, Eric Nichols, Suguru Matsuyoshi,
Asuka Sumida, Shouko Masuda, Kentaro Inui, and
Yuji Matsumoto. 2009. Statement map: Assisting
information credibility analysis by visualizing argu-
ments. In Proceedings of WICOW 2009.
Ohshima, Hiroaki, Satoshi Oyama, and Katsumi
Tanaka. 2006. Searching coordinate terms with
their context from the web. In Proceedings of WISE
2006, pages 40?47.
Ritter, Alan, Stephen Soderland, Doug Downey, and
Oren Etzioni. 2008. It?s a contradiction ? no, it?s
not: A case study using functional relations. In Pro-
ceedings of EMNLP 2008, pages 11?20.
Satou, Toshinori and Manabu Okumura. 2007. Ex-
traction of comparative relations from Japanese we-
blog. In IPSJ SIG Technical Report 2007-NL-181,
pages 7?14 (in Japanese).
Shibata, Tomohide, Michitaka Odani, Jun Harashima,
Takashi Oonishi, and Sadao Kurohashi. 2008.
SYNGRAPH: A flexible matching method based on
synonymous expression extraction from an ordinary
dictionary and a web corpus. In Proceedings of IJC-
NLP 2008, pages 787?792.
Shibata, Tomohide, Yasuo Banba, Keiji Shinzato, and
Sadao Kurohashi. 2009. Web information organi-
zation using keyword distillation based clustering.
In Proceedings of WI?09, short paper, pages 325?
330.
Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-
hara, Chikara Hashimoto, and Sadao Kurohashi.
2008. TSUBAKI: An open search engine in-
frastructure for developing new information access
methodology. In Proceedings of IJCNLP 2008,
pages 189?196.
Somasundaran, Swapna and Janyce Wiebe. 2009.
Recognizing stances in online debates. In Proceed-
ings of ACL-IJCNLP 2009, pages 226?234.
Voorhees, Ellen M. 2008. Contradictions and justi-
fications: Extensions to the textual entailment task.
In Proceedings of ACL-08: HLT, pages 63?71.
Yang, Seon and Youngjoong Ko. 2009. Extract-
ing comparative sentences from korean text docu-
ments using comparative lexical patterns and ma-
chine learning techniques. In Proceedings of ACL-
IJCNLP 2009 Conference Short Papers, pages 153?
156.
542
Coling 2010: Poster Volume, pages 876?884,
Beijing, August 2010
Semantic Classification of Automatically Acquired Nouns
using Lexico-Syntactic Clues
Yugo Murawaki
Graduate School of Informatics
Kyoto University
murawaki@nlp.kuee.kyoto-u.ac.jp
Sadao Kurohashi
Graduate School of Informatics
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
In this paper, we present a two-stage ap-
proach to acquire Japanese unknown mor-
phemes from text with full POS tags as-
signed to them. We first acquire unknown
morphemes only making a morphology-
level distinction, and then apply semantic
classification to acquired nouns. One ad-
vantage of this approach is that, at the sec-
ond stage, we can exploit syntactic clues
in addition to morphological ones because
as a result of the first stage acquisition, we
can rely on automatic parsing. Japanese
semantic classification poses an interest-
ing challenge: proper nouns need to be
distinguished from common nouns. It
is because Japanese has no orthographic
distinction between common and proper
nouns and no apparent morphosyntactic
distinction between them. We explore
lexico-syntactic clues that are extracted
from automatically parsed text and inves-
tigate their effects.
1 Introduction
A dictionary plays an important role in Japanese
morphological analysis, or the joint task of
segmentation and part-of-speech (POS) tag-
ging (Kurohashi et al, 1994; Asahara and Mat-
sumoto, 2000; Kudo et al, 2004). Like Chi-
nese and Thai, Japanese does not delimit words
by white-space. This makes the first step of nat-
ural language processing more ambiguous than
simple POS tagging. Accordingly, morphemes in
a pre-defined dictionary compactly represent our
knowledge about both segmentation and POS.
One obvious problem with the dictionary-based
approach is caused by unknown morphemes,
or morphemes not defined in the dictionary.
Even though, historically, extensive human re-
sources were used to build high-coverage dictio-
naries (Yokoi, 1995), texts other than newspa-
per articles, in particular web pages, contain a
large number of unknown morphemes. These un-
known morphemes often cause segmentation er-
rors. For example, morphological analyzer JU-
MAN 6.01 wrongly segments the phrase ????
??? (saQporo eki, ?Sapporo Station?), where ?
????? (saQporo) is an unknown morpheme,
as follows:
??? (sa, noun-common, ?difference?),
??? (Q, UNK), ??? (po, UNK),
??? (ro, noun-common, ?sumac?) and
??? (eki, noun-common, ?station?),
where UNK refers to unknown morphemes auto-
matically identified by the analyzer. Such an er-
roneous sequence has disastrous effects on appli-
cations of morphological analysis. For example, it
can hardly be identified as a LOCATION in named
entity recognition.
One solution to the unknown morpheme prob-
lem is unknown morpheme acquisition (Mori and
Nagao, 1996; Murawaki and Kurohashi, 2008). It
is the task of automatically augmenting the dictio-
nary by acquiring unknown morphemes from text.
In the above example, the goal is to acquire the
morpheme ?????? (saQporo) with the POS
tag ?noun-location name.? However, unknown
morpheme acquisition usually adopts a coarser
POS tagset that only represents the morphology
level distinction among noun, verb and adjective.
This means that ?????? (saQporo) is acquired
as just a noun and that the semantic label ?loca-
tion name? remains to be assigned. The reason
only the morphology level distinction is made is
1http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
876
that the semantic level distinction cannot easily
be captured with morphological clues that are ex-
ploited in unknown morpheme acquisition.
In this paper, we investigate the remaining
problem and introduce the new task of seman-
tic classification that is to be applied to automat-
ically acquired nouns. In this task, we can ex-
ploit syntactic clues in addition to morphologi-
cal ones because, as a result of acquisition, we
can now rely on automatic parsing. For exam-
ple, since text containing ?????? (saQporo,
noun-unclassified) is correctly segmented, we can
extract not only the phrase ?saQporo station,? but
the tree fragment ?? go to saQporo,? and we can
determine its semantic label.
Japanese semantic classification poses an inter-
esting challenge: proper nouns need to be distin-
guished from common nouns. Like Chinese and
Thai, Japanese has no orthographic distinction be-
tween common and proper nouns as there is no
such thing as capitalization. In addition, there
seems no morphosyntactic (i.e. grammatical) dis-
tinction between them.
In this paper, we explore lexico-syntactic clues
that can be extracted from automatically parsed
text. We train a classification model on manually
registered nouns and apply it to automatically ac-
quired nouns. We then investigate the effects of
lexico-syntactic clues.
2 Semantic Classification Task
2.1 Two-Stage Approach to Unknown
Morpheme Acquisition
Our goal is to identify unknownmorphemes in un-
segmented text and assign POS tags to them. In
this section, we omit the details of boundary iden-
tification (segmentation) and review the Japanese
POS tagset to see why we propose a two-stage ap-
proach to assign full POS tags.
The Japanese POS tagset derives from tradi-
tional grammar. It is a mixture of several linguis-
tic levels: morphology, syntax and semantics. In
other words, information encoded in a POS tag
is more than how the morpheme behaves in a se-
quence of morphemes. In fact, POS tags given to
pre-defined morphemes are useful for applications
of morphological analysis, such as dependency
parsing (Kudo and Matsumoto, 2002), named en-
tity recognition (Asahara and Matsumoto, 2003;
Sasano and Kurohashi, 2008) and anaphora res-
olution (Iida et al, 2009; Sasano and Kurohashi,
2009). In these applications, POS tags are incor-
porated as features for models.
On the other hand, the mixed nature of the POS
tagset poses a challenge to unknown morpheme
acquisition. Previous approaches (Mori and Na-
gao, 1996; Murawaki and Kurohashi, 2008) di-
rectly or indirectly reply on morphology, or our
knowledge on how a morpheme behaves in a se-
quence of morphemes. This means that semantic
level distinction is difficult to make in these ap-
proaches, and in fact, is left unresolved. To be
specific, nouns are only distinguished from verbs
and adjectives but they have subcategories in the
original tagset. These are what we try to classify
acquired nouns into in this paper.
2.2 Semantic Labels
The Japanese noun subcategories may require an
explanation since they are different from the En-
glish ones (Marcus et al, 1993) in many re-
spects. Singular and mass nouns are not distin-
guished from plural nouns because Japanese has
no grammatical distinction between them. More
importantly for this paper, proper nouns have sub-
categories such as person name, location name
and organization name in addition to the distinc-
tion from common nouns. These subcategories
provide important information to named entity
recognition among other applications. For proper
nouns, we adopt these subcategories as semantic
labels in our task.
In contrast to proper nouns, common nouns
have only one subcategory ?common.? How-
ever, we consider that subcategories of common
nouns similar to those of proper nouns are use-
ful for, for example, anaphora resolution (Sasano
and Kurohashi, 2009). We adopt the ?categories?
of morphological analyzer JUMAN, with which
common nouns in its dictionary are annotated.
There are 22 ?categories? including PERSON,
ORGANIZATION and CONCEPT. We collapse
these ?categories? into coarser semantic labels
that roughly correspond to those for proper nouns.
To sum up, we define 9 semantic labels as shown
877
Table 1: List of semantic labels.
labels P/C sources1 manually registered nouns automatically acquired nouns
PSN-P
proper
subPOS:person name ?? (matsui, a surname) ??? (sayuri, a given name)???? (jo?ji, ?George?) ??? (kyoN, a nickname)
LOC-P subPOS:place name ?? (kyouto, ?Kyoto?) ??? (akiba, ?Akihabara?)??? (doitsu, ?Germany?) ???? (waikiki, ?Waikiki?)
ORG-P subPOS:organization name ?? (nichigin, a bank) ??? (matsuda, ?Mazda?)NHK (a broadcaster) ??? (yahu?, ?Yahoo?)
OTH-P subPOS:proper noun ?? (heisei, an era name) ???? (jipush??, ?Gypsy?)??? (surabu, ?Slav?)
PSN-C
common
category:PERSON ?? (seNsei, ?teacher?) ??? (merutomo, ?keypal?)???? (sutaQfu, ?staff?) ??? (n??to, ?NEET?)
LOC-C category:PLACE-?2 ?? (shokuba, ?office?) ??? (irori, ?hearth?)??? (kafe, ?cafe?) ?? (hojou, ?farm field?)
ORG-C category:ORGANIZATION ?? (seifu, ?government?) ??? (me?ka, ?manufacturer?)??? (ch??mu, ?team?) ?? (heisho, ?our office?)
ANI-C category:ANIMAL and ? (inu, ?dog?) ??? (chiwawa, ?Chihuahua?)category:ANIMAL-PART ? (kao, ?face?) ??? (maNta, ?manta?)
OTH-C other categories ?? (shuchou, ?argument?) ?? (jiNbei, a kind of clothing)? (makura, ?pillow?) ??? (chakumero, ?ringtone?)
1 A subPOS refers to a subcategory of noun. For example, PSN-P corresponds to the POS tag ?noun-person name?.
2 category:PLACE-INSTITUTION, category:PLACE-INSTITUION PART and others.
in Table 1.
2.3 Related Tasks
A line of research is dedicated to identify un-
known morphemes with varying degrees of identi-
fication. Asahara and Matsumoto (2004) only fo-
cus on boundary identification (segmentation) of
unknown morphemes. Mori and Nagao (1996),
Nagata (1999) and Murawaki and Kurohashi
(2008) assign POS tags at the morphology level.
Uchimoto et al (2001) assign full POS tags but
unsurprisingly the accuracy is low. Nakagawa
and Matsumoto (2006) also assign full POS tags.
They address the fact that local information used
in previous studies is inherently insufficient and
present a method that uses global information,
in other words, takes into consideration all oc-
currences of each unknown word in a document.
They report an improvement in tagging proper
nouns in Japanese.
A related task is named entity recognition
(NER). It can handle a named entity longer than
a single morpheme and is usually formalized as a
chunking problem. Since Japanese does not de-
limit words by white-space, the unit of chunk-
ing can be a character (Asahara and Matsumoto,
2003; Kazama and Torisawa, 2008) or a mor-
pheme (Sasano and Kurohashi, 2008). In either
case, NER models encode the output of morpho-
logical analysis and therefore are affected by its
errors. In fact, Saito et al (2007) report that a ma-
jority of unknown named entities (those never ap-
pear in a training corpus) contain unknown mor-
phemes as their constituents and that NER models
perform poorly on them. A straightforward solu-
tion to this problem would be to acquire unknown
morphemes and to assign semantic labels to them.
Another related task is supersense tagging (Cia-
ramita and Johnson, 2003; Curran, 2005; Cia-
ramita and Altun, 2006). A supersense corre-
sponds to one of the 26 broad categories defined
by WordNet (Fellbaum, 1998). Each noun synset
is associated with a supersense. For example,
?chair? has supersenses PERSON, ARTIFACT
and ACT because it belongs to several synsets.
Since supersense tagging is studied in English,
it differs from our task in several respects. In En-
glish, the distinction between common and proper
nouns is clear. In fact, the tagging models can use
POS features even for unknown nouns. In addi-
tion, the syntactic behavior of English nouns is
different from that of Japanese nouns (Gil, 1987).
Definiteness is not marked in Japanese as it lacks
determiners (e.g. ?the? and ?a?), and Japanese has
no obligatory plural marking. On the other hand,
Japanese obligatorily uses numeral classifiers to
indicate the count of nouns, as in
(1) saN
three
satsu
CL
no
GEN
hoN
book
three volumes of books, or three books,
878
where ?satsu? is a numeral classifier for books. A
number together with its numeral classifier forms
a numeral quantifier. Numeral quantifiers would
be informative about the semantic categories of
nouns. Note that Japanese shares the above fea-
tures with Chinese and Thai. Our findings in this
paper may hold for these languages.
3 Proposed Method
3.1 Lexico-Syntactic Clues
In the task of semantic classification, we can ex-
ploit syntactic clues in addition to morpholog-
ical ones. As a result of unknown morpheme
acquisition, text containing acquired morphemes,
or former unknown morphemes, is correctly seg-
mented. Now we can treat automatic parsing as
(at least partly) reliable with regard to acquired
morphemes.
For noun X , we use the following sets of fea-
tures for classification.
call: noun phrase Y that appears in a pat-
tern like ?Y called X? and ?Y such as X ,? e.g.
?call:kuni? from
X
X
to
QT
iu
call
kuni
country
a country called X .
cf: predicate with a case marker with which it
takes X as an argument, e.g. ?cf:tooru:wo? from
X
X
wo
ACC
tooru
pass
? pass through X .
demo: demonstrative that modifies X , e.g.
?demo:kono? from ?kono X? (this X) and
?demo:doNna? from ?doNna X? (what kind of
X).
ncf1: noun phrase which X modifies with the
genitive case marker ?no,? e.g. ?ncf1:heya? from
X
X
no
GEN
heya
room
X?s room.
ncf2: noun phrase that modifies X with the
genitive case marker ?no,? e.g. ?ncf2:subete?
from
subete
all
no
GEN
X
X
all X .
suf: suffix or suffix-like noun that follows X ,
e.g. ?suf:saN? from ?X saN? (Mr./Ms. X) and
?suf:eki? from ?X eki? (X station).
Using automatically parsed text to extract syn-
tactic features has an advantage. Since no manual
annotation is necessary, we can utilize a huge raw
corpus. On the other hand, parsing errors are in-
evitable. However, we can circumvent this prob-
lem by using the constraints of Japanese depen-
dency structures: head-final and projective. The
simplest example is the second last element of a
sentence, which always depends on the last ele-
ment. With these constraints, we can focus on
syntactically unambiguous dependency pairs and
extract syntactic features accurately. We follow
Kawahara and Kurohashi (2001) to extract a pair
of an argument noun and a predicate (cf), and
Sasano et al (2004) to extract a pair of nouns con-
nected with the genitive case marker ?no? (ncf1
and ncf2).
Noun X can be part of a compound noun. We
leave it for named entity recognition. Except for
suf, we extract features only when X alone forms
a word. Similarly, we extract suf features only
when X and a suffix alone form a noun phrase.
For call, ncf1, and ncf2, we generalize
numerals within noun phrases. For ?hoN?
(book) in example 1, we extract the feature
?ncf2:<NUM>satsu.?
3.2 Instances for Classification
Now that features are extracted for each noun, the
question is how to combine them together to make
an instance for classification. One factor we need
to consider is polysemy: a noun can be a person
name in one context and a location name in an-
other. If we combine features extracted from the
whole corpus, they may represent several seman-
tic labels.
Modeling a mixture of semantic labels might
be a solution, but we do not take this approach on
the grounds that each occurrence of a noun corre-
sponds to a single semantic label.
In our strategy, we perform classification mul-
tiple times for each noun and aggregate the results
at the end. The features for each classification are
extracted from a relatively small subset of a cor-
pus where the noun is supposedly consistent in
879
terms of semantic labels. In the field of named
entity recognition, it is known that label consis-
tency holds strongly at the level of a document
and less strongly across different documents (Kr-
ishnan and Manning, 2006). Thus we start with a
document and gradually cluster related documents
until a sufficient number of features are obtained.
For the specific procedures we took in the experi-
ments, see Section 4.1.
3.3 Training Data
Following unknown morpheme acquisition (Mu-
rawaki and Kurohashi, 2008), we create training
data using manually registered nouns, for which
we can obtain correct semantic labels. We per-
form the same procedure as above to make in-
stances of registered nouns.
Some registered nouns are tagged with more
than one semantic label, which we call ?explicit
polysemy.? We drop them from the training data.
The remaining problem is ?implicit polysemy.?
Nouns are sometimes used with an uncovered
sense. In preliminary experiments, we found that
a typical case of implicit polysemy was that a
proper noun derived from a basic noun. To al-
leviate this problem, we use an NE tagger for fil-
tering. We run an NE tagger over a small portion
of the corpus and extract common nouns that are
frequently tagged as named entities. Then we re-
move these nouns from the training data.
We also drop nouns that appear extremely fre-
quently such as ??? (hito, ?person?), ??? (koto,
?thing?) and ??? (watashi, ?I?2). Since acquired
nouns to be classified are typically low frequency
morphemes, they would not behave similarly to
these basic nouns.
3.4 Classifier
To assign a semantic label to each instance, we use
a multiclass discriminative classifier. The input it
takes is an instance that is represented by a feature
vector x ? Rd. The output is one semantic label
y ? Y , where Y is the set of semantic labels.
We use a linear classifier. It has a weight vector
wy ? Rd for each y and outputs y that maximizes
2Japanese personal pronouns are treated as common
nouns because they show no special morphosyntactic behav-
ior.
the inner product of wy and x.
y = argmax
y
?wy, x?.
Several methods have been proposed to esti-
mate weight vector wy from training data. We use
online algorithms because they are easy to imple-
ment and scale to huge instances. We try the Per-
ceptron family of algorithms.
4 Experiments
4.1 Settings
We used JUMAN for morphological analysis and
KNP3 for dependency parsing. The dictionary
of JUMAN was augmented with automatically
acquired morphemes (Murawaki and Kurohashi,
2008). The number of manually registered mor-
phemes was 120 thousands while there were
13,071 acquired morphemes, of which 12,615
morphemes were nouns.
We used a web corpus that was compiled
through the procedures proposed by Kawahara
and Kurohashi (2006). It consisted of 100 million
pages.
We first extracted features from the web cor-
pus. To keep the model size manageable, we
used 447,082 features that appeared more than
100 times in the corpus.
We constructed training data from manually
registered nouns and test data from automatically
acquired nouns. For each noun, we combined text
together until the number of features grew to more
than 100. We started with a single web page, then
merge pages that share a domain name and fi-
nally clustered texts across different domains. We
split the web corpus into 40 subcorpora and ap-
plied this procedure in parallel. We used Bayon4
for clustering domain texts. We sequentially read
texts and applied the repeated bisections cluster-
ing every time some 5,000 pages were appended.
The vectors for clustering were nouns, both regis-
tered and acquired, with their tf-idf scores. We ob-
tained 4,843,085 instances for 10,613 registered
nouns and 196,098 instances for 2,556 acquired
nouns.
3http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
4http://code.google.com/p/bayon/
880
Table 2: Results of semantic classification.
learning algorithms acquired nouns registered nouns
Averaged Perceptron 86.40% (432 / 500) 88.59% (123,113 / 138,971)
Passive-Aggressive 87.00% (435 / 500) 91.68% (127,407 / 138,971)
Confidence-Weighted 85.20% (426 / 500) 89.66% (124,604 / 138,971)
baseline1 69.60% (348 / 500) 79.14% (109,980 / 138,971)
1 assign OTH-C to all instances.
Table 3: Examples of aggregated instances.
acquired nouns instances labels
??? (hikaru, a person name) 84 PSN-P:58.33%, PSN-C:41.67%
??? (chiwawa, ?Chihuahua?) 128 ANI-C:54.69%, OTH-C:45.31%
???? (kamisaN, colloq. ?wife?) 131 PSN-C:100%
????? (rasubegasu, ?Las Vegas?) 136 LOC-P:97.06%, LOC-C:2.94%
???? (aQpuru, ?Apple/apple?) 187 ORG-P:63.10%, PSN-C:34.76%, OTH-C:2.14%
???? (merumaga, abbr. of ?mail magazine?) 1,622 OTH-C:99.32%, LOC-C:0.55%, PSN-C:0.06%
In order to handle polysemy, we evaluated se-
mantic classification on an instance-by-instance
basis. We randomly selected 500 instances from
the test data and manually assigned the correct la-
bels to them. For comparison purposes, we also
classified registered nouns. We split the training
data: 829 nouns or 138,971 instances for testing
and the rest for training.
We trained the model with three online learn-
ing algorithms, (1) the averaged version (Collins,
2002) of Perceptron (Crammer and Singer, 2003),
(2) the Passive-Aggressive algorithm (Crammer
et al, 2006), and (3) the Confidence-Weighted
algorithm (Crammer et al, 2009). For Passive-
Aggressive algorithm, we used PA-I and set pa-
rameter C to 1. For Confidence-Weighted, we
used the single-constraint updates. All algorithms
iterated five times through the training data.
4.2 Results
Table 2 shows the results of semantic classifica-
tion. All algorithms significantly improved over
the baseline. As suggested by the gap in accu-
racy between acquired and registered nouns in the
baseline method, the label distribution of the train-
ing data differed from that of the test data, but the
decrease in accuracy was smaller than expected.
The Passive-Aggressive algorithm performed
best on both acquired and registered nouns. For
the rest of this paper, we report the results of the
Passive-Aggressive algorithm.
Table 3 shows aggregated instances of some ac-
quired nouns. Although classification sometimes
failed, correct labels took the majority. How-
ever, it is noticeable that PSN-P was frequently
misidentified as PSN-C while PSN-C was cor-
rectly identified. This phenomenon is clearly seen
in the confusion matrix (Table 4). Half of PSN-P
instances were misidentified as PSN-C but the
percentage of errors in the opposite direction was
just above 9%. We will investigate this in the next
section.
4.3 Discussion
Our interest is in determining what kinds of fea-
tures are effective in semantic classification. We
first performed standard ablation experiments. We
trained a series of models on the training data af-
ter removing each feature set. The training and
test data were the same with those in Section 4.1.
Table 5 shows the results of ablation experi-
ments. Significant decreases in accuracy are ob-
served in the cf dataset. This is easily explained by
the fact that more than half of features belonged
to cf. The ratio of ncf1 was much the same with
that of ncf2, but the removal of ncf1 resulted in a
worse performance in classifying registered nouns
than that of ncf2. This means that a modifiee of a
noun explains more about the noun than its modi-
fier.
The ablation experiments cannot capture inter-
esting properties of features because each feature
set has a great diversity within it. Next, we di-
rectly examine features instead. Since we use a
simple linear classifier, a feature has |Y | corre-
sponding weights, each of which represents how
likely a noun belongs to label y. For example,
features whose weights for PSN-C are the largest
881
Table 4: Confusion matrix of acquired nouns.
Actual
PSN-P LOC-P ORG-P OTH-P PSN-C LOC-C ORG-C ANI-C OTH-C
Pre
dic
ted
PSN-P 16 1 4 1
LOC-P 1
ORG-P 4
OTH-P
PSN-C 16 39 1 2
LOC-C 2 2 1 10 4
ORG-C 2
ANI-C 28
OTH-C 3 1 1 1 13 9 338
Table 5: Results of ablation experiments.
feature set ratio1 acquired nouns registered nouns
-call 0.23% 87.60% (438 / 500) 91.58% (127,276 / 138,971)
-cf 54.84% 84.80% (424 / 500) 88.96% (123,630 / 138,971)
-demo 2.40% 88.00% (440 / 500) 91.38% (126,996 / 138,971)
-ncf1 19.03% 87.20% (436 / 500) 89.23% (124,008 / 138,971)
-ncf2 18.40% 85.60% (428 / 500) 91.54% (127,220 / 138,971)
-suf 5.10% 87.40% (437 / 500) 91.30% (126,889 / 138,971)
all 87.00% (435 / 500) 91.68% (127,407 / 138,971)
1 The proportion of each feature set that appears in the instances of the test
data.
include:
? cf:nakusu:wo (?? lose X to the disease?),
? cf:oshieru:ni (??1 teach X ?2?),
? ncf2:ooku (?many/much X?), and
? ncf2:<NUM>niN (X is modified by
<NUM> plus a numeral classifier for
persons).
As briefly mentioned in Section 2.3, Japanese
numeral quantifiers received scholarly attention
in the fields of linguistic philosophy and lin-
guistics in relation to the count/mass distinc-
tion (Quine, 1969; Gil, 1987). In our feature
sets, numeral quantifiers typically appear as ncf2,
e.g. ?ncf2:<NUM>niN.? The weights given to
them demonstrate their effectiveness in semantic
classification. They discriminate common nouns
from proper nouns as the weights given to com-
mon nouns are larger with wide margins. It is not
surprising because, say, the phrase ?two Johns? is
semantically acceptable but extremely rare in re-
ality. They are also informative about the distinc-
tion among PSN, LOC and others. For example,
the classifier ?niN? for persons suggest the noun in
question is a person while ?keN? for houses would
modify a location-like noun. However, we found
quite a few ?noises? about these features in data.
The modifiee of a numeral expression is not al-
ways the noun to be counted, as demonstrated by
the following example:
(2) saN
three
niN
CL
no
GEN
moNdai
problem
matters among the three persons.
From the above, the feature ?ncf2:<NUM>niN?
is extracted although ?moNdai? is OTH-C. Theis
?noise? is attributed to the genitive case marker
?no? because it can denote a wide range of rela-
tions between two nouns. We might be able to
avoid this problem if we focus on ?floating? nu-
meral quantifiers. A floating numeral quantifier
has no direct dependency relation to the noun to
be counted, as in
(3) seito
student
ga
NOM
saN
three
niN
CL
keQseki
absence
shita
do
three students were absent,
where the numeral quantifier modifies the verb
phrase instead of the noun. Further work is
needed to anchor floating numeral quantifiers
since they bring a different kind of ambiguity
themselves (Bond et al, 1998).
Closely related to numeral quantifiers are quan-
tificational nouns that appear as ?ncf2:ooku?
(?many/much?), ?ncf2:subete? (?all?) and oth-
ers. They distinguish common nouns from proper
882
nouns but does not make a further classifica-
tion. The same is true of other numeral expres-
sions such as ?cf:hueru:ga? (?X increase in num-
ber?) and ?cf:nai:ga? (?there is no X? or ?X
do not exist?). We found that, other than nu-
meral expressions, some features distinguished
common nouns from proper nouns because they
indicated the noun denoted an attribute. Such fea-
tures include ?cf:naru:ni? (?? become X?) and
?cf:kaneru:wo? (?? double as X?).
We expected that demonstratives (demo)
served similar functions to quantificational ex-
pressions, but it turned out to be more com-
plex. The distal demonstrative ?ano? (?that?) of-
ten modifies proper nouns to give emphasis. In
fact, the model gave larger weights to proper
nouns. On the other hand, interrogative demon-
stratives such as ?dono? (?which?) and ?doNna?
(?what kind of?) are rarely used with proper nouns
although semantically acceptable.
As seen above, there is an abundant variety
of features that distinguish common nouns from
proper nouns. Also, it is not difficult to make a
distinction among PSN, LOC and others although
the far largest cluster OTH-C sometimes absorbs
other instances. The remaining question is how to
distinguish proper nouns from common nouns, or
specifically PSN-P from PSN-C. We examined
features that gave larger weights to PSN-P than
to PSN-C. They generally had smaller margins
in weights than those which distinguish PSN-C
from PSN-P. Among them, features such as
?cf:utau:ga? (?X sing?) and ?cf:hanasu:ni? (??
talk to X?) have no problem with being used for
common nouns in terms of both semantics and
pragmatics. They seem to have resulted from
over-training. There were seemingly appropriate
features such as ?suf:saNchi? (?X?s house?) and
?suf:seNshu? (honorific suffix for players), but
they were not ubiquitous in the corpus. PSN-P in-
stances suffered from lack of distinctive features.
One solution to this problem is to combine ad-
ditional knowledge about person names. For ex-
ample, a Japanese family name is followed by a
given name, and most Chinese names consist of
three Chinese characters. However, quite a few
person names in the web corpus do not follow
the usual patterns of person names because they
are handles (or nicknames) and names for fic-
tional characters. Thus it would be desirable to be
able to classify person names without additional
knowledge.
5 Conclusion
In this paper, we presented the new task of seman-
tic classification of Japanese nouns and applied it
to nouns automatically acquired from text. Unlike
in unknown morpheme identification in previous
studies, we can exploit automatically parsed text.
We explored lexico-syntactic clues and investi-
gated their effects. We found plenty of features
that distinguished common nouns from proper
nouns, but few features worked in the opposite di-
rection. Further work is needed to overcome this
bias.
References
Asahara, Masayuki and Yuji Matsumoto. 2000. Ex-
tended models and tools for high-performance part-
of-speech tagger. In Proc. of COLING 2000, pages
21?27.
Asahara, Masayuki and Yuji Matsumoto. 2003.
Japanese named entity extraction with redundant
morphological analysis. In Proc. of HLT/NAACL
2003, pages 8?15.
Asahara, Masayuki and Yuji Matsumoto. 2004.
Japanese unknown word identification by character-
based chunking. In Proc. COLING 2004, pages
459?465.
Bond, Francis, Daniela Kurz, and Satoshi Shirai.
1998. Anchoring floating quantifiers in Japanese-
to-English machine translation. In Proc. of COL-
ING 1998, pages 152?159.
Ciaramita, Massimiliano and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proc. of EMNLP 2006, pages 594?602.
Ciaramita, Massimiliano and Mark Johnson. 2003.
Supersense tagging of unknown nouns in WordNet.
In Proc. of EMNLP 2003, pages 168?175.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP 2002, pages 1?8.
883
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Crammer, Koby, Mark Dredze, and Alex Kulesza.
2009. Multi-class confidence weighted algorithms.
In Proc. of EMNLP 2009, pages 496?504.
Curran, James R. 2005. Supersense tagging of un-
known nouns using semantic similarity. In Proc. of
ACL 2005, pages 26?33.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database. The MIT Press, Cam-
bridge, MA.
Gil, David. 1987. Definiteness, NP configurationality
and the count-mass distinction. In Reuland, Eric J.
and Alice G. B. ter Meulen, editors, The Representa-
tion of (In)definiteness, pages 254?269. MIT Press.
Iida, Ryu, Kentaro Inui, and Yuji Matsumoto. 2009.
Capturing salience with a trainable cache model for
zero-anaphora resolution. In Proc. of ACL/IJCNLP
2009, pages 647?655.
Kawahara, Daisuke and Sadao Kurohashi. 2001.
Japanese case frame construction by coupling the
verb and its closest case component. In Proc. of
HLT 2001, pages 204?210.
Kawahara, Daisuke and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC-06,
pages 1344?1347.
Kazama, Jun?ichi and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Proc. of
ACL 2008, pages 407?415, June.
Krishnan, Vijay and Christopher D. Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition. In
Proc. of COLING-ACL 2006, pages 1121?1128.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of CONLL 2002, pages 1?7.
Kudo, Taku, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proc. of
EMNLP 2004, pages 230?237.
Kurohashi, Sadao, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improvements
of Japanese morphological analyzer JUMAN. In
Proc. of The International Workshop on Sharable
Natural Language Resources, pages 22?38.
Marcus, Mitchell P., Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Com-
putational Linguistics, 19(2):313?330.
Mori, Shinsuke and Makoto Nagao. 1996. Word ex-
traction from corpora and its part-of-speech estima-
tion using distributional analysis. In Proc. of COL-
ING 1996, volume 2, pages 1119?1122.
Murawaki, Yugo and Sadao Kurohashi. 2008. Online
acquisition of Japanese unknown morphemes us-
ing morphological constraints. In Proc. of EMNLP
2008, pages 429?437.
Nagata, Masaaki. 1999. A part of speech estimation
method for Japanese unknown words using a statis-
tical model of morphology and context. In Proc. of
ACL 1999, pages 277?284.
Nakagawa, Tetsuji and Yuji Matsumoto. 2006. Guess-
ing parts-of-speech of unknown words using global
information. In Proc. of COLING-ACL 2006, pages
705?712.
Quine, Willard Van. 1969. Ontological Relativity and
Other Essays. Columbia University Press.
Saito, Kuniko, Jun Suzuki, and Kenji Imamura. 2007.
Extraction of named entities from blogs using CRF.
In Proc. of The 13th Annual Meeting of The Associ-
ation for Natural Language Processing, pages 107?
110. (in Japanese).
Sasano, Ryohei and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural
language processing. In Proc. of IJCNLP 2008,
pages 607?612.
Sasano, Ryohei and Sadao Kurohashi. 2009. A prob-
abilistic model for associative anaphora resolution.
In Proc. of EMNLP 2009, pages 1455?1464.
Sasano, Ryohei, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proc. of COLING 2004, pages 1201?
1207.
Uchimoto, Kiyotaka, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The unknown word problem: a mor-
phological analysis of Japanese using maximum en-
tropy aided by a dictionary. In Proc. of EMNLP
2001, pages 91?99.
Yokoi, Toshio. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42?44.
884
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 269?278, Dublin, Ireland, August 23-29 2014.
Rapid Development of a Corpus with Discourse Annotations
using Two-stage Crowdsourcing
Daisuke Kawahara
??
Yuichiro Machida
?
Tomohide Shibata
??
Sadao Kurohashi
??
Hayato Kobayashi
?
Manabu Sassano
?
?
Graduate School of Informatics, Kyoto University
?
CREST, Japan Science and Technology Agency
?
Yahoo Japan Corporation
{dk, shibata, kuro}@i.kyoto-u.ac.jp, machida@nlp.ist.i.kyoto-u.ac.jp,
{hakobaya, msassano}@yahoo-corp.jp
Abstract
We present a novel approach for rapidly developing a corpus with discourse annotations using
crowdsourcing. Although discourse annotations typically require much time and cost owing to
their complex nature, we realize discourse annotations in an extremely short time while retaining
good quality of the annotations by crowdsourcing two annotation subtasks. In fact, our experi-
ment to create a corpus comprising 30,000 Japanese sentences took less than eight hours to run.
Based on this corpus, we also develop a supervised discourse parser and evaluate its performance
to verify the usefulness of the acquired corpus.
1 Introduction
Humans understand text not by individually interpreting clauses or sentences, but by linking such a text
fragment with another in a particular context. To allow computers to understand text, it is essential to
capture the precise relations between these text fragments. This kind of analysis is called discourse
parsing or discourse structure analysis, and is an important and fundamental task in natural language
processing (NLP). Systems for discourse parsing are, however, available only for major languages, such
as English, owing to the lack of corpora with discourse annotations.
For English, several corpora with discourse annotations have been developed manually, consuming a
great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al.,
2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson,
2005). Discourse parsers trained on these corpora have also been developed and practically used. To
create the same resource-rich environment for another language, a quicker method than the conventional
time-consuming framework should be sought. One possible approach is to use crowdsourcing, which
has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008;
Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource
the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of
spans with a certain relation and identifying the relation between the pair.
In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the proce-
dure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for
crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a
discourse unit for the span is a costly process, and thus we adopt a clause as the discourse unit, since this
is reliable enough to be automatically detected. We also limit the length of each target document to three
sentences and at most five clauses to facilitate the annotation task. Secondly, we detect and annotate
clause pairs in a document that hold logical discourse relations. However, since this is too complicated
to assign as one task using crowdsourcing, we divide the task into two steps: determining the existence
of logical discourse relations and annotating the type of relation. Our two-stage approach is a robust
method in that it confirms the existence of the discourse relations twice. We also designed the tagset
of discourse relations for crowdsourcing, which consists of two layers, where the upper layer contains
the following three classes: ?CONTINGENCY,? ?COMPARISON? and ?OTHER.? Although the task
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
269
settings are simplified for crowdsourcing, the obtained corpus and knowledge of discourse parsing could
be still useful in general discourse parsing.
In our experiments, we crowdsourced discourse annotations for Japanese, for which there are no pub-
licly available corpora with discourse annotations. The resulting corpus consists of 10,000 documents,
each of which comprises three sentences extracted from the web. Carrying out this two-stage crowd-
sourcing task took less than eight hours. The time elapsed was significantly shorter than the conventional
corpus building method.
We also developed a discourse parser by exploiting the acquired corpus with discourse annotations.
We learned a machine learning-based model for discourse parsing based on this corpus and evaluated its
performance. An F1 value of 37.9% was achieved for contingency relations, which would be roughly
comparable with state-of-the-art discourse parsers on English. This result indicates the usefulness of the
acquired corpus. The resulting discourse parser would be effectively exploited in NLP applications, such
as sentiment analysis (Zirn et al., 2011) and contradiction detection (Murakami et al., 2009; Ennals et
al., 2010).
The novel contributions of this study are summarized below:
? We propose a framework for developing a corpus with discourse annotations using two-stage crowd-
sourcing, which is both cheap and quick to execute, but still retains good quality of the annotations.
? We construct a Japanese discourse corpus in an extremely short time.
? We develop a discourse parser based on the acquired corpus.
The remainder of this paper is organized as follows. Section 2 introduces related work, while Section
3 describes our proposed framework and reports the experimental results for the creation of a corpus with
discourse annotations. Section 4 presents a method for discourse parsing based on the corpus as well as
some experimental results. Section 5 concludes the paper.
2 Related Work
Snow et al. (2008) applied crowdsourcing to five NLP annotation tasks, but the settings of these tasks
are very simple. There have also been several attempts to construct language resources with complex
annotations using crowdsourcing. Negri et al. (2011) proposed a method for developing a cross-lingual
textual entailment (CLTE) corpus using crowdsourcing. They tackled this complex data creation task by
dividing it into several simple subtasks: sentence modification, type annotation and sentence translation.
The creative CLTE task and subtasks are quite different from our non-creative task and subtasks of
discourse annotations. Fossati et al. (2013) proposed FrameNet annotations using crowdsourcing. Their
method is a single-step approach to only detect frame elements. They verified the usefulness of their
approach through an experiment on a small set of verbs with only two frame ambiguities per verb.
Although they seem to be running a larger-scale experiment, its result has not been revealed yet. Hong
and Baker (2011) presented a crowdsourcing method for selecting FrameNet frames, which is a part of
the FrameNet annotation process. Since their task is equivalent to word sense disambiguation, it is not
very complex compared to the whole FrameNet annotation process. These FrameNet annotations are
still different from discourse annotations, which are our target. To the best of our knowledge, there have
been no attempts to crowdsource discourse annotations.
There are several manually-crafted corpora with discourse annotation for English, such as the Penn
Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse
Graphbank (Wolf and Gibson, 2005). These corpora were developed from English newspaper articles.
Several attempts have been made to manually create corpora with discourse annotations for languages
other than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (news-
paper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents;
1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres;
267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences com-
pared with the English corpora containing several tens of thousands sentences.
270
In recent years, there have been many studies on discourse parsing on the basis of the above hand-
annotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009;
Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty
et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing
can be attributed to the existence of corpora with discourse annotations. However, the target language is
mostly English since English is the only language that has large-scale discourse corpora. To develop and
improve discourse parsers for languages other than English, it is necessary to build large-scale annotated
corpora, especially in a short period if possible.
3 Development of Corpus with Discourse Annotations using Crowdsourcing
3.1 Corpus Specifications
We develop a tagged corpus in which pairs of discourse units are annotated with discourse relations.
To achieve this, it is necessary to determine target documents, discourse units, and a discourse relation
tagset. The following subsections explain the details of these three aspects.
3.1.1 Target Text and Discourse Unit
In previous studies on constructing discourse corpora, the target documents were mainly newspaper
texts, such as the Wall Street Journal for English. However, discourse parsers trained on such newspaper
corpora usually have a problem of domain adaptation. That is to say, while discourse parsers trained on
newspaper corpora are good at analyzing newspaper texts, they generally cannot perform well on texts
of other domains.
To address this problem, we set out to create an annotated corpus covering a variety of domains.
Since the web contains many documents across a variety of domains, we use the Diverse Document
Leads Corpus (Hangyo et al., 2012), which was extracted from the web. Each document in this corpus
consists of the first three sentences of a Japanese web page, making these short documents suitable for
our discourse annotation method based on crowdsourcing.
We adopt the clause as a discourse unit, since spans are too fine-grained to annotate using crowdsourc-
ing and sentences are too coarse-grained to capture discourse relations. Clauses, which are automatically
identified, do not need to be manually modified since they are thought to be reliable enough. Clause
identification is performed using the rules of Shibata and Kurohashi (2005). For example, the following
rules are used to identify clauses as our discourse units:
? clauses that function as a relatively strong boundary in a sentence are adopted,
? relative clauses are excluded.
Since workers involved in our crowdsourcing task need to judge whether clause pairs have discourse
relations, the load of these workers increases combinatorially as the number of clauses in a sentence
increases. To alleviate this problem, we limit the number of clauses in a document to five. This limitation
excludes only about 5% of the documents in the original corpus.
Our corpus consists of 10,000 documents corresponding to 30,000 sentences. The total number of
clauses in this corpus is 39,032, and thus the average number of clauses in a document is 3.9. The total
number of clause pairs is 59,426.
3.1.2 Discourse Relation Tagset
One of our supposed applications of discourse parsing is to automatically generate a bird?s eye view of a
controversial topic as in Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010),
which identify various relations between statements, including contradictory relations. We assume that
expansion relations, such as elaboration and restatement, and temporal relations are not important for this
purpose. This setting is similar to the work of Bethard et al. (2008), which annotated temporal relations
independently of causal relations. We also suppose that temporal relations can be annotated separately
for NLP applications that require temporal information. We determined the tagset of discourse relations
271
Upper type Lower type Example
CONTINGENCY
Cause/Reason ???????????????????
[since (I) pushed the button] [hot water was turned on]
Purpose ?????????????????????
[to pass the exam] [(I) studied a lot]
Condition ?????????????????
[if (you) push the button] [hot water will be turned on]
Ground ??????????????????????????
[here is his/her bag] [he/she would be still in the company]
COMPARISON
Contrast ?????????????????????????????
[at that restaurant, sushi is good] [ramen is so-so]
Concession ??????????????????????????
[that restaurant is surely good] [the price is high]
OTHER (Other) ???????????????????
[After being back home] [it began to rain]
Table 1: Discourse relation tagset with examples.
by referring to the Penn Discourse Treebank. This tagset consists of two layers, where the upper layer
contains three classes and the lower layer seven classes as follows:
? CONTINGENCY
? Cause/Reason (causal relations and not conditional relations)
? Purpose (purpose-action relations where the purpose is not necessarily accomplished)
? Condition (conditional relations)
? Ground (other contingency relations including pragmatic cause/condition)
? COMPARISON (same as the Penn Discourse Treebank)
? Contrast
? Concession
? OTHER (other weak relation or no relation)
Note that we do not consider the direction of relations to simplify the annotation task for crowdsourcing.
Table 1 shows examples of our tagset.
Therefore, our task is to annotate clause pairs in a document with one of the discourse relations given
above. Sample annotations of a document are shown below. Here, clause boundaries are shown by ?::?
and clause pairs that are not explicitly marked are allocated the ?OTHER? relation.
Cause/Reason ?????::??????????::????????????::???????
???????::??????????????
... [the surgery of my father ended safely] [(I) am relieved a little bit]
Contrast ???????????????????????::????????????
????????????????????????::???????????
??????::????????
... [There is tailwind to live,] [there is also headwind.]
3.2 Two-stage Crowdsourcing for Discourse Annotations
We create a corpus with discourse annotations using two-stage crowdsourcing. We divide the annotation
task into the following two subtasks: determining whether a clause pair has a discourse relation excluding
?OTHER,? and then, ascertaining the type of discourse relation for a clause pair that passes the first stage.
272
Probability Number
= 1.0 64
> 0.99 554
> 0.9 1,065
> 0.8 1,379
> 0.5 2,655
> 0.2 4,827
> 0.1 5,895
> 0.01 9,068
> 0.001 12,277
> 0.0001 15,554
Table 2: Number of clause pairs resulting from the judgments of discourse relation existence.
3.2.1 Stage 1: Judgment of Discourse Relation Existence
This subtask determines whether each clause pair in a document has one of the following discourse
relations: Cause/Reason, Purpose, Condition, Ground, Contrast, and Concession (that is, all the relations
except ?OTHER?). Workers are shown examples of these relations and asked to determine only the
existence thereof.
In this subtask, an item presented to a worker at a particular time consists of all the judgments of
clause pairs in a document. By adopting this approach, each worker considers the entire document when
making his/her judgments.
3.2.2 Stage 2: Judgment of Discourse Relation Type
This subtask involves ascertaining the discourse relation type for a clause pair that passes the first stage.
The result of this subtask is one of the seven lower types in our discourse relation tagset. Workers
are shown examples of these types and then asked to select one of the relations. If a worker chooses
?OTHER,? this corresponds to canceling the positive determination of the existence of the discourse
relation in stage one.
In this subtask, an item is the judgment of a clause pair. That is, if a document contains more than
one clause pair that must be judged, the judgments for this document are divided into multiple items,
although this is rare.
3.3 Experiment and Discussion
We conducted an experiment of the two-stage crowdsourcing approach using Yahoo! Crowdsourcing.
1
To increase the reliability of the produced corpus, we set the number of workers for each item for each
task to 10. The reason why we chose this value is as follows. While Snow et al. (2008) claimed that an
average of 4 non-expert labels per item in order to emulate expert-level label quality, the quality of some
tasks increased by increasing the number of workers to 10. We also tested hidden gold-standard items
once every 10 items to examine worker?s quality. If a worker failed these items in serial, he/she would
have to take a test to continue the task.
We obtained judgments for the 59,426 clause pairs in the 10,000 documents of our corpus in the
first stage of crowdsourcing, i.e., the subtask of determining the existence of discourse relations. We
calculated the probability of each label using GLAD
2
(Whitehill et al., 2009), which was proved to
be more reliable than the majority voting. This probability corresponds to the probability of discourse
relation existence of each clause pair. Table 2 lists the results. We set a probability threshold to select
those clause pairs whose types were to be judged in the second stage of crowdsourcing. With this
threshold set to 0.01, 9,068 clause pairs (15.3% of all the clause pairs) were selected. The threshold was
set fairly low to allow low-probability judgments to be re-examined in the second stage.
1
http://crowdsourcing.yahoo.co.jp/
2
http://mplab.ucsd.edu/?jake/OptimalLabelingRelease1.0.3.tar.gz
273
Lower type All prob > 0.8
Cause/Reason 2,104 1,839 (87.4%)
Purpose 755 584 (77.4%)
Condition 1,109 925 (83.4%)
Ground 442 273 (61.8%)
Contrast 437 354 (81.0%)
Concession 80 49 (61.3%)
Sum of the above discourse relations 4,927 4,024 (81.7%)
Other 4,141 3,753 (90.6%)
Total 9,068 7,777 (85.8%)
Table 3: Results of the judgments of lower discourse relation types.
Upper type All prob > 0.8
CONTINGENCY 4,439 3,993 (90.0%)
COMPARISON 516 417 (80.8%)
Sum of the above discourse relations 4,955 4,410 (89.0%)
OTHER 4,113 3,753 (91.2%)
Total 9,068 8,163 (90.0%)
Table 4: Results of the judgments of upper discourse relation types.
The discourse relation types of the 9,068 clause pairs were determined in the second stage of crowd-
sourcing. We extended GLAD (Whitehill et al., 2009) for application to multi-class tasks, and calculated
the probability of the labels of each clause pair. We assigned the label (discourse relation type) with the
highest probability to each clause pair. Table 3 gives some statistics of the results. The second column in
this table denotes the numbers of each discourse relation type, while the third column gives the numbers
of each type of clause pair with a probability higher than 0.80. Table 4 gives statistics of the results when
the lower discourse relation types are merged into the upper types. Table 5 shows some examples of the
resulting annotations.
Carrying out the two separate subtasks using crowdsourcing took approximately three hours and five
hours with 1,458 and 1,100 workers, respectively. If we conduct this task at a single stage, it would take
approximately 33 (5 hours / 0.153) hours. It would be four times longer than our two-stage approach.
Such single-stage approach is also not robust since it does not have a double check mechanism, with
which the two-stage approach is equipped. We spent 111 thousand yen and 113 thousand yen (approx-
imately 1,100 USD, respectively) for these subtasks, which would be extremely less expensive than the
projects of conventional discourse annotations.
For the examples in Table 5, we confirmed that the discourse relation types of the top four examples
were surely correct. However, we judged the type (Contrast) of the bottom example as incorrect. Since
the second clause is an instantiation of the first clause, the correct type should be ?Other.? We found such
errors especially in the clause pairs with a probability lower than 0.80.
4 Development of Discourse Parser based on Acquired Discourse Corpus
To verify the usefulness of the acquired corpus with discourse annotations, we developed a supervised
discourse parser based on the corpus, and evaluated its performance. We built two discourse parsers using
the annotations of the lower and upper discourse relation types, respectively. From the annotations in the
first stage of crowdsourcing (i.e., judging the existence of discourse relations), we assigned annotations
with a probability less than 0.01 as ?OTHER.? Of the annotations acquired in the second stage (i.e.,
judging discourse relation types), we adopted those with a probability greater than 0.80 and discarded
the rest. After this preprocessing, we obtained 58,135 (50,358 + 7,777) instances of clause pairs for
the lower-type discourse parser and 58,521 (50,358 + 8,163) instances of clause pairs for the upper-type
274
Prob # W Type Document
1.00 6/10 Cause/Reason ???????????????????????????????
??????????????????????????????
????
... [Since the flower blooms in the fifth lunar month] [it is called ?Sat-
suki.?] ...
0.99 4/10 Condition ??????????????????????????????
???????????????????????????????
?????????????????????????????
??????
[If you click the balloon on the map] [you can see the recommended
route] ...
0.81 3/10 Purpose ?????????????????????????????
?????????????????????????????
??????????????????????????????
?????????????????????????????
... [And seeking ?Great harvest?] [each country is engaged in a war]
0.61 2/10 Cause/Reason ??????????????????????????????
?????????????????????????????
??????????????????????????????
??????????????????????????????
?????
... [by transmitting power to the front and rear axle with the combina-
tion of gears and shafts] [(it) drives the four wheels.]
0.54 3/10 Contrast ?????????????????????????????
??????????????????????????????
???????????????????????????
... [a scramble for customers by department stores would be severe.]
[What comes out is the possibility of the closure of Fukuoka Mit-
sukoshi.]
Table 5: Examples of Annotations. The first column denotes the estimated label probability and the
second column denotes the number of workers that assigned the designated type. In the fourth column,
the clause pair annotated with the type is marked with?? ([ ] in English translations).
discourse parser. Of these, 4,024 (6.9%) and 4,410 (7.5%) instances, respectively, had one of the types
besides ?OTHER.? We conducted experiments using five-fold cross validation on these instances.
To extract features of machine learning, we applied the Japanese morphological analyzer, JUMAN,
3
and the Japanese dependency parser, KNP,
4
to the corpus. We used the features listed in Table 6, which
are usually used for discourse parsing.
We adopted Opal (Yoshinaga and Kitsuregawa, 2010)
5
for the machine learning implementation. This
tool enables online learning using a polynomial kernel. As parameters for Opal, we used the passive-
aggressive algorithm (PA-I) with a polynomial kernel of degree two as a learner and the extension to
multi-class classification (Matsushima et al., 2010). The numbers of classes were seven and three for the
lower- and upper-type discourse parsers, respectively. We set the aggressiveness parameter C to 0.001,
which generally achieves good performance for many classification tasks. Other parameters were set to
the default values of Opal.
To measure the performance of the discourse parsers, we adopted precision, recall and their harmonic
mean (F1). These metrics were calculated as the proportion of the number of correct clause pairs to the
3
http://nlp.ist.i.kyoto-u.ac.jp/EN/?JUMAN
4
http://nlp.ist.i.kyoto-u.ac.jp/EN/?KNP
5
http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/opal/
275
Name Description
clause distance clause distance between two clauses
sentence distance sentence distance between two clauses
bag of words bag of words (lemmas) for each clause
predicate a content word (lemma) of the predicate of each clause
conjugation form of predicate a conjugation form of the predicate of each clause
conjunction a conjunction if it is located at the beginning of a clause
word overlapping ratio an overlapping ratio of words between the two clauses
clause type a lexical type output by KNP for each clause (about 100 types)
topic marker existence existence of a topic marker in each clause
topic marker cooccurrence existence of a topic marker in both clauses
Table 6: Features for our discourse parsers.
Type Precision Recall F1
Cause/Reason 0.623 (441/708) 0.240 (441/1,839) 0.346
Purpose 0.489 (44/90) 0.075 (44/584) 0.131
Condition 0.581 (256/441) 0.277 (256/925) 0.375
Ground 0.000 (0/12) 0.000 (0/273) 0.000
Contrast 0.857 (6/7) 0.017 (6/354) 0.033
Concession 0.000 (0/0) 0.000 (0/49) 0.000
Other 0.944 (53,702/56,877) 0.992 (53,702/54,111) 0.968
Table 7: Performance of our lower-type discourse parser.
Type Precision Recall F1
CONTINGENCY 0.625 (1,084/1,735) 0.272 (1,084/3,993) 0.379
COMPARISON 0.412 (7/17) 0.017 (7/417) 0.032
OTHER 0.942 (53,454/56,769) 0.988 (53,454/54,111) 0.964
Table 8: Performance of our upper-type discourse parser.
number of all recognized or gold-standard ones for each discourse relation type. Tables 7 and 8 give the
accuracies for the lower- and upper-type discourse parsers, respectively.
From Table 8, we can see that our upper-type discourse parser achieved an F1 of 37.9% for contingency
relations. It is difficult to compare our results with those in previous work due to the use of different data
set and different languages. We, however, anticipate that our results would be comparable with those
of state-of-the-art English discourse parsers. For example, the end-to-end discourse parser of Lin et al.
(2012) achieved an F1 of 20.6% ? 46.8% on the Penn Discourse Treebank.
We also obtained a low F1 for comparison relations. This tendency is similar to the previous results
on the Penn Discourse Treebank. The biggest cause of this low F1 is the lack of unambiguous explicit
discourse connectives for these relations. Although there are explicit discourse connectives in Japanese,
many of them have multiple meanings and cannot be used as a direct clue for discourse relation detection
(e.g., as described in Kaneko and Bekki (2014)). As reported in Pitler et al. (2009) and other studies,
the identification of implicit discourse relations are notoriously difficult. To improve its performance, we
need to incorporate external knowledge sources other than the training data into the discourse parsers.
A promising way is to use large-scale knowledge resources that are automatically acquired from raw
corpora.
276
5 Conclusion
We presented a rapid approach for building a corpus with discourse annotations and a discourse parser
using two-stage crowdsourcing. The acquired corpus is made publicly available and can be used for
research purposes.
6
This corpus can be used not only to build a discourse parser but also to evaluate
its performance. The availability of the corpus with discourse annotations will accelerate the develop-
ment and improvement of discourse parsing. In the future, we intend integrating automatically acquired
knowledge from corpora into the discourse parsers to further enhance their performance. We also aim to
apply our framework to other languages without available corpora with discourse annotations.
References
Steven Bethard, William Corvey, Sara Klingenstein, and James H. Martin. 2008. Building a corpus of temporal-
causal structure. In Proceedings of the 6th International Conference on Language Resources and Evaluation,
pages 908?915.
Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disam-
biguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume
2: Short Papers), pages 69?73.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and
Dialogue.
Iria da Cunha, Juan-Manuel Torres-Moreno, and Gerardo Sierra. 2011. On the development of the RST Spanish
treebank. In Proceedings of the 5th Linguistic Annotation Workshop (LAW V), pages 1?10.
Rob Ennals, Beth Trushkowsky, and John Mark Agosta. 2010. Highlighting disputed claims on the web. In
Proceedings of the 19th international conference on World Wide Web, pages 341?350.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 60?68. Association for Computational Linguistics.
Marco Fossati, Claudio Giuliano, and Sara Tonelli. 2013. Outsourcing FrameNet to the crowd. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics, pages 742?747.
Sucheta Ghosh, Sara Tonelli, Giuseppe Riccardi, and Richard Johansson. 2011. End-to-end discourse parser
evaluation. In Fifth IEEE International Conference on Semantic Computing (ICSC), pages 169?172.
Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kurohashi. 2012. Building a diverse document leads corpus
annotated with semantic relations. In Proceedings of 26th Pacific Asia Conference on Language Information
and Computing, pages 535?544.
Hugo Hernault, Helmut Prendinger, David duVerle, and Mitsuru Ishizuka. 2010. HILDA: A discourse parser
using support vector machine classification. Dialogue & Discourse, 1(3):1?33.
Jisup Hong and Collin F. Baker. 2011. How good is the crowd at ?real? WSD? In Proceedings of the 5th Linguistic
Annotation Workshop, pages 30?37.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level
discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 904?915.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Yashar Mehdad. 2013. Combining intra- and multi-sentential
rhetorical parsing for document-level discourse analysis. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, pages 486?496.
Kimi Kaneko and Daisuke Bekki. 2014. Building a Japanese corpus of temporal-causal-discourse structures
based on SDRT for extracting causal relations. In Proceedings of the EACL 2014 Workshop on Computational
Approaches to Causality in Language (CAtoCL), pages 33?39.
6
http://nlp.ist.i.kyoto-u.ac.jp/EN/?DDLC
277
Man Lan, Yu Xu, and Zhengyu Niu. 2013. Leveraging synthetic discourse data via multi-task learning for implicit
discourse relation recognition. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 476?485.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, pages 1?34.
Shin Matsushima, Nobuyuki Shimizu, Kazuhiro Yoshida, Takashi Ninomiya, and Hiroshi Nakagawa. 2010. Exact
passive-aggressive algorithm for multiclass classification using support class. In Proceedings of 2010 SIAM
International Conference on Data Mining (SDM2010), pages 303?314.
Koji Murakami, Eric Nichols, Suguru Matsuyoshi, Asuka Sumida, Shouko Masuda, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Statement map: Assisting information credibility analysis by visualizing arguments. In Pro-
ceedings of the 3rd Workshop on Information Credibility on the Web, pages 43?50.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide
and conquer: Crowdsourcing the creation of cross-lingual textual entailment corpora. In Proceedings of the
2011 Conference on Empirical Methods in Natural Language Processing, pages 670?679.
Thiago Alexandre Salgueiro Pardo, Maria das Grac?as Volpe Nunes, and Lucia Helena Machado Rino. 2004.
Dizer: An automatic discourse analyzer for Brazilian Portuguese. In Advances in Artificial Intelligence?SBIA
2004, pages 224?234. Springer.
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13?16.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in
text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP, pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn discourse treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation, pages 2961?2968.
Tomohide Shibata and Sadao Kurohashi. 2005. Automatic slide generation based on discourse structure analysis.
In Proceedings of Second International Joint Conference on Natural Language Processing, pages 754?766.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and AndrewNg. 2008. Cheap and fast ? but is it good? evaluating
non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 254?263.
Manfred Stede. 2004. The Potsdam commentary corpus. In Proceedings of the 2004 ACL Workshop on Discourse
Annotation, pages 96?102.
Rajen Subba and Barbara Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 566?574.
Jacob Whitehill, Paul Ruvolo, Ting fan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should
count more: Optimal integration of labels from labelers of unknown expertise. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22,
pages 2035?2043.
FlorianWolf and Edward Gibson. 2005. Representing discourse coherence: A corpus-based study. Computational
Linguistics, 31(2):249?287.
Naoki Yoshinaga and Masaru Kitsuregawa. 2010. Kernel slicing: Scalable online training with conjunctive fea-
tures. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING2010), pages
1245?1253.
C?acilia Zirn, Mathias Niepert, Heiner Stuckenschmidt, and Michael Strube. 2011. Fine-grained sentiment analysis
with structural features. In Proceedings of 5th International Joint Conference on Natural Language Processing,
pages 336?344.
278
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 508?518,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Efficient retrieval of tree translation examples for
Syntax-Based Machine Translation
Fabien Cromieres
Graduate School of Informatics
Kyoto University
Kyoto, Japan
fabien@nlp.kuee.kyoto-u.ac.jp
Sadao Kurohashi
Graduate School of Informatics
Kyoto University
Kyoto, Japan
kuro@i.kyoto-u.ac.jp
Abstract
We propose an algorithm allowing to effi-
ciently retrieve example treelets in a parsed
tree database in order to allow on-the-fly ex-
traction of syntactic translation rules. We also
propose improvements of this algorithm al-
lowing several kinds of flexible matchings.
1 Introduction
The popular Example-Based (EBMT) and Statistical
Machine Translation (SMT) paradigms make use of
the translation examples provided by a parallel bilin-
gual corpus to produce new translations. Most of
these translation systems process the example data
in a similar way: The parallel sentences are first
word-aligned. Then, translation rules are extracted
from these aligned sentences. Finally, the transla-
tion rules are used in a decoding step to translate
sentences. We use the term translation rule in a very
broad sense here, as it may refer to substring pairs as
in (Koehn et al, 2003), synchronous grammar rules
as in (Chiang, 2007) or treelet pairs as in (Quirk et
al., 2005; Nakazawa and Kurohashi, 2008).
As the size of bilingual corpus grow larger, the
number of translation rules to be stored can easily
become unmanageable. As a solution to this prob-
lem in the context of phrase-based Machine Transla-
tion, (Callison-Burch et al, 2005) proposed to pre-
align the example corpora, but delay the rule extrac-
tion to the decoding stage. They showed that using
Suffix Arrays, it was possible to efficiently retrieve
all sentences containing substrings of the sentence
to be translated, and thus extract the needed trans-
lation rules on-the-fly. (Lopez, 2007) proposed an
extension of this method for retrieving discontinu-
ous substrings, making it suitable for systems such
as (Chiang, 2007).
In this paper, we propose a method to apply the
same idea to Syntax-Based SMT and EBMT (Quirk
et al, 2005; Mi et al, 2008; Nakazawa and Kuro-
hashi, 2008). Since Syntax-Based systems usually
work with the parse trees of the source-side sen-
tences, we will need to be able to retrieve effi-
ciently examples trees from fragments (treelets) of
the parse tree of the sentence we want to translate.
We will also propose extensions of this method al-
lowing more flexible matchings.
2 Overview of the method
2.1 Treelet retrieval
We first formalize the setting of this chapter by pro-
viding some definitions.
Definition 2.1 (Treelets). A treelet is a connected
subgraph of a tree. A treelet T1 is a subtreelet of an-
other treelet T2 if T1 is itself a connected subgraph
of T2. We note |T| the number of nodes in a treelet.
If |T| = 1, T is called an elementary treelet. A lin-
ear treelet is a treelet whose nodes have at most 1
child. A subtree rooted at node n of a tree T is a
treelet containing all nodes descendants of n.
Definition 2.2 (Sub- and Supertreelets). If T1 is a
subtreelet of T2 and |T1| = |T2| ? 1, we call T1
an immediate subtreelet of T2. Reciprocally, T2 is
an (immediate) supertreelet of T1. Furthermore, if
T2 and T1 are rooted at the same node in the original
tree, we say that T2 is a descending supertreelet of
T1. Otherwise it is an ascending supertreelet of T1.
508
In treelet retrieval, we are given a certain treelet
type and want to find all of the tokens of this type in
the database DB. Each token of a given treelet type
will be identified by a mapping from the node of the
treelet type to the nodes of the treelet token in the
database.
Definition 2.3 (Matching). Given a treelet T and a
tree database DB, a matching of T in DB is a func-
tion M that associate the treelet T to a tree T in
DB and every node of T to nodes of T in such a
way that: ?n ? T, label(M(n)) = label(n) and
?(n1, n2) ? T s.t n2 is a child of n1, M(n2) is a
child of M(n1).
In the common case where the siblings of a tree
are ordered, a matching must satisfy the additional
restriction: ?n1, n2 ? T, n1 <s n1 ? M(n1) <s
M(n1), where <s is the partial order relation be-
tween nodes meaning ?is a sibling and to the left of?
We note occ(T) (for ?occurrences of T ?) the set
of all possible matchings from T to DB. We will
call computing T the task of finding occ(T). If
|occ(T)| = 0, we call T an empty treelet. Computing
a query tree TQ means computing all of its treelets.
Definition 2.4 (Notations). Although treelets are
themselves trees, we will use the word treelet to
emphasize they are a subpart of a bigger tree. We
will note T a treelet, and T a tree. TQ is the query
tree we want to compute. DB will refer to the set of
trees in our database. We will use a bracket notation
to describe trees or treelets. Thus ?a(b c d(e))? is
the tree at the bottom of figure 2.
2.2 General approach
There exists already a large body of research about
tree pattern matching (Dubiner et al, 1994; Bruno
et al, 2002). However, our problem is quite differ-
ent from finding the tokens of a given treelet in a
database. We actually want to find all the tokens of
all of the treelets of a given query tree. The query
tree itself is unlikely to appear in full even once in
the database. In this respect, our approach will have
many similarities with (Callison-Burch et al, 2005)
and (Lopez, 2007), and can be seen as an extension
of these works.
The basis of the method in (Lopez, 2007) is to
look for the occurrences of continuous substrings us-
ing a Suffix Array, and then intersect them to find the
occurrences of discontinuous substrings. We will
have a similar approach with two variants. The first
variant consists in using an adaptation of the con-
cept of suffix arrays to trees, which we will call Path-
To-Root Arrays (section 3.4), that allows us to find
efficiently the set of occurrences of a linear treelet.
Occurrences of non-linear treelets can then be com-
puted by intersection. The second variant is to use an
inverted index (section 3.5). Then the occurrences of
all treelets, even the linear treelets, are computed by
intersection.
The main additional difficulty in considering trees
instead of strings is that while a string has a
quadratic number of continuous substrings, a tree
has in general an exponential number of treelets (eg.
several trillion for the dependency tree of a 70 words
sentence). There is also an exponential number of
discontinuous substrings, but (Lopez, 2007) only
consider substrings of bounded size, limiting this
problem. We will not try to bound the size of treelets
retrieved. It is therefore crucial to avoid computing
the occurrences of treelets that have no occurrences
in the database, and also to eliminate as much redun-
dant calculation as is possible.
Lopez proposes to use Prefix Trees for avoiding
any redundant or useless computation. We will use
a similar idea but with an hypergraph that we will
call ?computation hypergraph? (section 3.2). This
hypergraph will not only fit the same role as the Pre-
fix Tree of (Lopez, 2007), but also will allow us to
easily implement different search strategies for flex-
ible search (section 6).
2.3 Representing positions
Whether we use a Path-to-Root Array or an inverted
index, we will need a compact way to represent the
position of a node in a tree. It is straightforward to
define such a position for strings, but slightly less for
trees. Especially, if we consider ordered trees, we
will want to be able to compare the relative location
of the nodes by comparing their positions.
The simplest possibility is to use an integer corre-
sponding to the rank of the node in a in-order depth-
first traversal of the tree. It is then easy, for two
nodes b and c, children of a parent node a, to check
if b is on the left of c, or on the left of a, for example.
A more advanced possibility is to use a represen-
tation inspired from (Zhang et al, 2001), in which
509
the position of a node is a tuple consisting of its rank
in a preorder (ie. children last) and a postorder (chil-
dren first) depth-first traversal, and of its distance to
the root. This allows to test easily whether a node
is an ancestor of another, and their distance to each
other. This allows in turn to compute by intersec-
tion the occurrences of discontinuous treelets, much
like what is done in (Lopez, 2007) for discontinuous
strings. This is discussed in section 7.2.
3 Computing treelets incrementally
We describe here in more details how the treelets can
be efficiently computed incrementally.
3.1 Dependence of treelet computation
Let us first define how it is possible to compute a
treelet from two of its subtreelets. Let us consider
a treelet T and two treelets T1 and T2 such that
T = T1 ? T2, where, in the equality and the union,
the treelet are seen as the set of their nodes. There
are two possibilities. If T1?T2 = ?, then the root of
T1 is a child of a node of T2 or vice-versa. We then
say that T1 and T2 form a disjoint coverage (abbrevi-
ated as D-coverage) of T. If T1?T2 6= ?, we will say
that T1 and T2 form an overlapping coverage (abbre-
viated as O-coverage) of T.
Given two treelets T1 and T2 forming a cover-
age of T, we can compute occ(T) from occ(T1) and
occ(T2) by combining their matchings.
Definition 3.1 (compatibility for O-coverage). Let
T be a treelet of TQ. Let T1 and T2 be 2 treelets
forming a O-coverage of T. Let M1 ? occ(T1)
and M2 ? occ(T2). M1 and M2 are compat-
ible if and only if M1|T1?T2 = M2|T1?T2 and
I(M1|T1\T2) ? I(M2|T2\T1) = ?.
In the definition above, |S is the restriction of a func-
tion to a set S and I is the image set of a function.
If the children of a tree are ordered, we must add
the additional restriction: ?(n1, n2) ? (T1 \ T2) ?
(T2 \ T1), n1 <s n2 ?M1(n1) <s M2(n2).
Definition 3.2 (compatibility for D-coverage). Let
T1 and T2 be 2 treelets forming a D-coverage
of T. Let?s suppose that the root n2 of T2 is a
child of node n1 of T1. Let M1 ? occ(T1) and
M2 ? occ(T2). M1 and M2 are compatible if and
only if M2(n2) is a child of M1(n1).
Figure 1: A computing hypergraph for ?a(b c)?.
Definition 3.3 (intersection (?) operation). If two
matchings are compatible, we can form their union,
which is defined as (M1 ? M2)(n) = M1(n)
if n ? T1 and M2(n) else. We note
occ(T1) ? occ(T2) = {M1 ? M2 | M1 ?
occ(T1),M2 ? occ(T2) and M1 is compati-
ble with M2 }. Then,we have the property:
occ(T ) = occ(T1)? occ(T2)
In practice, the intersection operation will be im-
plemented using merge and binary merge algorithms
(Baeza-Yates and Salinger, 2005), following (Lopez,
2007).
3.2 The computation hypergraph
We have seen that it is possible to compute occ(T)
from two subtreelets forming a coverage of T. This
can be represented by a hypergraph in which nodes
are all the treelets of a given query tree, and every
pair of overlapping or adjacent treelet is linked by
an hyperedge to their union treelet. Whenever we
have computed two starting points of an hyper-edge,
we can compute its destination treelet. An example
of a small computation hypergraph is described in
figure 1.
It is very convenient to represent the incremen-
tal computation of the treelets as a traversal of this
hypergraph. First because it contributes to avoid
redundant computations: each treelet is computed
only once, even if it is used to compute several other
treelets. Also, if a query tree contains two distinct
but identical treelets, only one computation will be
done, provided the two treelets are represented by
the same node in the hypergraph. The hypergraph
also allows us to avoid computing empty treelets, as
we describe in next section. This hypergraph there-
fore has the same role for us as the prefix tree used
510
Figure 2: Inclusion DAG for the tree a(bcd(e))
in (Lopez, 2007). Of course, the hypergraph is gen-
erated on-the-fly during the traversal.
Furthermore, different traversals will define dif-
ferent computation strategies, and we will be able to
use some more advanced graph exploration methods
in section 6.
3.3 The Immediate Inclusion DAG
In many cases (but not always: see section 4.3),
the most optimal computation strategy should be
to always compute a treelet from two of its imme-
diate subtreelets. This is because the computation
time will be proportional to the size of the small-
est occurrence set of the two treelets, and thus the
?cheapest? subtreelet is always one of the immedi-
ate subtreelets. With this computation strategy, we
can replace the general computation hypergraph by
a DAG (Directed Acyclic Graph) in which every
treelet point to its immediate supertreelets. An ex-
ample is given on figure 2. We will call this DAG
the (Immediate) Inclusion DAG.
Traversals of the Inclusion DAG should be pruned
when an empty treelet is found, since all of its su-
pertreelets will also be empty. The algorithm 1 pro-
vide a general traversal of the DAG avoiding to com-
pute as many empty treelets as possible. It uses a
queue D of discovered treelets, and a data-structure
C that associate a treelet to those of its subtreelets
that have been already computed. Once a treelet T
has been computed and is found to be non empty, we
discover its immediate supertreelets TS1, TS2, . . . (if
they have not been discovered already) and add T to
C (TS1), C (TS2), . . . . The operation min(C (T)) re-
Algorithm 1: Generic DAG traversal
Add the set of precomputed treelets to D;1
while ?T ? D s.t T ? precomputed or |C(T )| > 22
do
pop T from D;3
if T in precomputed then4
occ(T )? precomputed[T ];5
else6
T1,T2=min(C (T));7
if |occ(T1)| = 0 then8
occ(T )? ?;9
else10
occ(T )? occ(T1)? occ(T2);11
for TS ? supertree(T ) do12
if occ(TS) = undef then13
Add T to C(TS);14
if |occ(T )| > 0 and TS /? D then15
Add TS to D;16
trieve the 2 subtreelets from C (T) that have the least
occurrences. If one of them is empty, we can di-
rectly conclude that T is empty. No treelet whose all
immediate subtreelets are empty is ever put in the
discovered queue, which allows us to prune most of
the empty treelets of the Inclusion DAG.
A treelet in the inclusion DAG can be computed
as soon as two of its antecedents have been com-
puted. To start the computation (or rather, ?seed?
it), it is necessary to know the occurrences of treelet
of smaller size. In the following sections 3.4 and
3.5, we describe two methods for efficiently obtain-
ing the set of occurrences of some initial treelets.
3.4 Path-to-Root Array
We present here a method to compute very effi-
ciently occ(T) when T is linear. This method is sim-
ilar to the use of Suffix Arrays (Manber and My-
ers, 1990) to find the occurrences of continuous sub-
strings in a text.
Definition 3.4 (Paths-to-Root Array). Given a la-
beled tree T and a node n ? T , the path-to-root
of n is the sequence of labels from n to the root.
The Paths-to-Root Array of a set of trees DB is the
lexicographically sorted list of the Path-to-Roots of
every node in DB.
Just as with suffixes, a path-to-root can be rep-
resented compactly by a pointer to its starting node
in DB. We then need to keep the database DB in
511
Pos PtR Pos PtR Pos PtR
1 3:4 a 8 2:2 bf 15 1:3 fba
2 1:7 a 9 1:6 ca 16 3:3 fga
3 2:6 af 10 1:8 da 17 3:2 ga
4 1:4 afba 11 2:7 daf 18 2:1 gbf
5 3:7 ba 12 3:1 ega 19 1:5 gca
6 1:2 ba 13 2:3 f 20 1:1 hba
7 2:5 baf 14 3:8 fba 21 3:5 heba
Figure 3: Path To Root Array for a set of three trees.
?Pos.? is the position of the starting point of a given path-
to-root (noted as indexOfTree:positionInTree), and PtR is
the sequence of labels on this path-to-root. The path-to-
root are sorted in lexicographic order. We can find the set
of occurrences of any linear treelet with a binary search.
For example, the treelet a(b) corresponds to the label se-
quence ?ba?. With a binary search, we find that the path-
to-root starting with ?ba? are between indexes 5 and 7.
The corresponding occurrences are then 3:7, 1:2 and 2:5.
memory to retrieve efficiently the pointed-to path-
to-root. Once the Path-to-Root Array is built, for a
linear treelet T, we can find its occurrences by a bi-
nary search of the first and last path-to-root starting
with the labels of T. See figure 3 for an example.
Memory cost is quite manageable, since we only
need 10 bytes per nodes in total. 5 bytes per pointer
in the array (tree id: 4 bytes, start position: 1 byte),
and 5 bytes per nodes to store the database in mem-
ory (label id:4 bytes, parent position: 1 byte).
All the optimization tricks proposed in (Lopez,
2007) for Suffix Arrays can be used here, espe-
cially the optimization proposed in (Zhang and Vo-
gel, 2005).
3.5 Inverted Index and Precomputation
Instead of a Path-to-Root array, one can simply use
an inverted index. The inverted index associates
with every label the set of its occurrences, each oc-
currences being represented by a tuple containing
the index of the tree, the position of the label in the
tree, and the position of the parent of the label in
the tree. Knowing the position of the parent will
allow to compute treelets of size 2 by intersection
(D-coverage). This is less effective than the Path-
To-Root Array approach, but open the possibilities
for the flexible search discussed in section 6.
Taking the idea further, we can actually con-
sider the possibility of precomputing treelets of size
greater than 1, especially if they appear frequently
in the corpus.
4 Practical implementation of the traversal
4.1 Postorder traversal
The way we choose the treelet to be popped out on
line 3 of algorithm 1 will define different computa-
tion strategies. For concreteness, we describe now a
more specific traversal. We will process treelets in
an order depending on their root node. More pre-
cisely, we consider the nodes of the query tree in the
order given by a depth-first postorder traversal of the
query tree. This way, when a treelet rooted at n is
processed, all of the treelets rooted at a descendant
of n have already been processed.
We can suppose that every processed treelet is as-
signed an index that we note #T. This allows a con-
venient recursive representation of treelets.
Definition 4.1 (Recursive representation). Let T be
a treelet rooted at node n of TQ. We note ni the
ith child of n in TQ. For all i, ti is the subtree of
T rooted at ni. We note ti = ? and #ti = 0 if T
does not contain ni. The recursive representation of
T is then: [n, (#t1,#t2, . . . ,#tm)]. We note T i the
value #ti.
For example, if TQ =?a(b c d(e))? and the treelets
?b? and ?d(e)? have been assigned the indexes 2
and 4, the recursive representation of the treelet ?a(b
d(e))? would be [a,(2,0,4)].
Algorithm 2 describes this ?postorder traversal?.
DNode is a priority queue containing the treelets
rooted atNode discovered so far. The priority queue
pop out the smallest treelets first. Line 14 maintain a
list L of processed treelets and assign the index of T
in L to #T. Line 22 keeps track of the non-empty
immediate supertreelets of every treelet through a
dictionary S. This is used in the procedure compute-
supertreelets (algorithm 3) to generate the immedi-
ate supertreelets of a treelet T given its recursive rep-
resentation. In this procedure, line 6 produces the
512
Algorithm 2: DAG traversal by query-tree pos-
torder
for Node in postorder-traversal(query-tree) do1
Telem = [Node, (0, 0, .., 0)];2
DNode ? Telem;3
while |DNode| > 0 do4
T=pop-first(DNode);5
if T in precomputed then6
occ(T )? precomputed[Node.label];7
else8
T1, T2=min(C (t));9
if |occ(T1)| = 0 then10
occ(T )? ?;11
else12
occ(T )? occ(T1)? occ(T2);13
Append T to L;14
#T ? |L|;15
for TS in compute-supertree(T,#T ) do16
Add T to C(TS);17
if |occ(T )| > 0 then18
if TS /? DNode and19
root(TS)=Node then
Add TS to D;20
for #t in C (T ) do21
Add #T to S(#t);22
descending supertreelets, and line 8 produces the as-
cending supertreelet. Figure 4 describes the content
of all these data structures for a simple run of the
algorithm.
This postorder traversal has several advantages.
A treelet is only processed once all of its immedi-
ate supertreelets have been computed, which is op-
timal to reduce the cost of the ? operation. The
way the procedure compute-supertreelets discover
supertreelets from the info in S has also several
benefit. One is that, by not adding empty treelets
(line 18) to S , we naturally prevent the discovery
of larger empty treelets. Similarly, in the next sec-
tion, we will be able to prevent the discovery of non-
maximal treelets by modifying S . Modifications of
compute-supertreelets will also allow different kind
of retrieval in section 6.
4.2 Pruning non-maximal treelets
We now try to address another aspect of the over-
whelming number of potential treelets in a query
tree. As we said, in most practical cases, most of the
larger treelets in a query tree will be empty. Still, it is
Algorithm 3: compute-supertrees
Input: T ,#T
Output: lst: list of immediate supertreelets of T
m? |root(T)|;1
for i in 1 . . .m do2
for #TS in S(#T i) do3
if root(#TS) 6= root(T) then4
Tnew ? [root(T), T 0, ..#T ?, . . . , Tm];5
Append Tnew to lst;6
Tnew ? [parent(root(T)), (0, . . . ,#T, . . . , 0)];7
Append Tnew to lst;8
possible that some tree exactly identical to the query
tree (or some tree having a very large treelet in com-
mon with the query tree) do exist in the database.
This case is obviously a best case for translation,
but unfortunately could be a worst-case for our al-
gorithm, as it means that all of the (possibly trillions
of) treelets of the query tree will be computed.
To solve this issue, we try to consider a concept
analogous to that of maximal substring, or substring
class, found in Suffix Trees and Suffix Arrays (Ya-
mamoto and Church, 2001). The idea is that in most
cases where a query tree is ?full? (that is all of its
treelets are not empty), most of the larger treelets
will share the same occurrences (in the database
trees that are very similar to the query tree). We for-
malize this as follow:
Definition 4.2 (domination and maximal treelets).
Let T1 be a subtreelet of T2. If for every matching
M1 of occ(T1), there exist a matching M2 of
occ(T2) such that M2|T1 = M1, we say that T1 is
dominated by T2. A treelet is maximal if it is not
dominated by any other treelet.
If T1 is dominated by T2, it means that all occur-
rences of T1 are actually part of an occurrence of
T2. We will therefore be, in general, more interested
by the larger treelet T2 and can prune as many non-
maximal treelets as we want in the traversal. The key
point is that the algorithm has to avoid discovering
most non maximal treelets.The algorithm 2 can eas-
ily be modified to do this. We will use the following
property.
Property 4.1. Given k treelets T1 . . . Tk with k dis-
tinct roots, all the roots being children of a same
node n. We note n(T1 . . . Tk) the treelet whose root
is n, and for which the k subtrees rooted at the k
513
T d e b b(d) [Empty] b(e) b(d e) [Empty] c a a(b) a(b(e)) a(c) a(b c) a(b(e) c)
# 1 2 3 4 5 6 7 8 9 10 11 12 13
R d e b(..) b(1.) b(.2) b(1 2) c a(..) a(3.) a(5.) a(.7) a(3 7) a(5 7)
C - - - 1,3 2,3 4,5 - - 8,3 5,9 7,8 9,11 10,12
S - - 5 - - - - 9,11 10,12 13 12 13 -
Figure 4: A run of the algorithm 2, for the query tree a(b(d e) c). The row ?T? represents the treelets in the order
they are discovered. The row ?#? is the index #T, and the row ?R? is the recursive representation of the treelet. Also
represented are the content of C and S at the end of the computation. When a treelet is poped out of DNode, occ(T) is
computed from the treelets listed in C (T ). If occ(T) is not empty, the entries of the immediate subtreelets of T in S
are updated with #T. We suppose here that |occ( b(d))|=0. Then, b(d e) is marked as empty and neither b(d) nor b(d e)
are added to the entries of their subtreelets in S. This way, when considering treelets rooted at the upper node ?a?, the
algorithm will not discover any of the treelets containing b(d).
children of n are T1 . . . Tk. Let us further suppose
that for all i, Ti is dominated by a descending su-
pertreelet T di (with the possibility that Ti = T di ).
Then n(T1 . . . Tk) is dominated by n(T d1 . . . T dk ).
For example, if b(c) is dominated by b(c d), then
a(b(c) e) will be dominated by a(b(c d) e).
In algorithm 2, after processing each node, we
proceed to a cleaning of the S dictionary in the fol-
lowing way: for every treelet T (considering the
treelets by increasing size) that is dominated by
one of its supertreelets TS ? S(T) and for every
subtreelet T ? of T such that T ? S(T ?), we re-
place T by TS in S(T ?). The procedure compute-
supertreelets, when called during the processing of
the parent node, will thus skip all of the treelets that
are ?trivially? dominated according to property 4.1.
Let?s note that testing for the domination of a
treelet T by one of its supertrelets TS is not a matter
of just testing if |occ(T)| = |occ(TS)|, as would be
the case with substring: a treelet can have less oc-
currences than one of its supertreelets (eg. b(a) has
more occurrences than b in b(a a) ). An efficient way
is to first check that the two treelets occurs in the
same number of sentences, then confirm this with a
systematic check of the definition.
4.3 The case of constituent trees
We have focused our experiments on dependency
trees, but the method can be applied to any tree.
However, the computations strategies we have used
might not be optimal for all kind of trees. In a de-
pendency tree, nodes are labeled by words and most
non-elementary treelets have a small number of oc-
currences. In a constituent tree, many treelets con-
taining only internal nodes have a high frequency
and will be expensive to compute.
If we have enough memory, we can solve this by
precomputing the most common (and therefore ex-
pensive) treelets.
However, it is usually not very interesting to re-
trieve all the occurrences of treelets such as ?NP(Det
NN)? in the context of aMT system. Such very com-
mon pattern are best treated by some pre-computed
rules. What is interesting is the retrieval of lexical-
ized rules. More precisely, we want to retrieve ef-
ficiently treelets containing at least one leaf of the
query tree. Therefore, an alternative computation
strategy would only explore treelets containing at
least one terminal node. We would thus compute
successively ?dog?, ?NN(dog)? ?NP(NN(dog))?,
?NP(Det NN(dog))?, etc.
4.4 Complexity
Processing time will be mainly dependent on two
factors: the number of treelets in a query tree that
need to be computed, and the average time to com-
pute a treelet.
Let NC be the size of the corpus. It can be shown
quite easily that the time needed to compute a treelet
with our method is proportional to its number of oc-
currences, which is itself growing as O(NC).
Let m be the size of the query tree. The number
of treelets needing to be computed is, in the worst
case, exponential in m. In practice, the only case
where most of the treelets are non-empty is when the
database contains trees similar to the query tree in
the database, and this is handled by the modification
of the algorithm is section 4.2. In other cases, most
of the treelets are empty, and empirically, we find
that the number of non-empty treelets in a query tree
514
Database size (#nodes) 6M 60M
Largest non-empty treelet size 4.6 8.7
Processing time (PtR Array) 0.02 s 0.7 s
Processing time (Inv. Index) 0.02 s 0.9 s
Size on disk 40 MB 500 MB
Figure 5: Performances averaged on 100 sentences.
grows approximately as O(m ?N0.5C ). It is also pos-
sible to bound the size of the retrieved treelets (only
retrieving treelets with less than 10 nodes, for exam-
ple), similarly to what is done in (Lopez, 2007). The
number of treelets will then only grows as O(m).
The total processing time of a given query tree
will therefore be on the order of O(m ? N1.5C ) (or
O(m ? NC) if we bound the treelet size). The fact
that this give a complexity worse than linear with
respect to the database size might seem a concern,
but this is actually only because we are retrieving
more and more different types of treelets. The cost
of retrieving one treelet remain linear with respect to
the size of the corpus. We empirically find that even
for very large values of NC , processing time remain
very reasonable (see next section).
It should be also noted that the constant hid-
den in the big-O notation can be (almost) arbitrar-
ily reduced by precomputing more and more of the
most common (and more expensive) treelets (a time-
memory trade-off).
5 Experiments
We conducted experiments on a large database of
2.9 million automatically parsed dependency trees,
with a total of nearly 60 million nodes1. The largest
trees in the database have around 100 nodes. In or-
der to see how performance scale with the size of the
database, we also used a smaller subset of 230,000
trees containing near 6 million nodes.
We computed, using our algorithm, 100 randomly
selected query trees having from 10 to 70 nodes,
with an average of 27 nodes per tree. Table 5
shows the average performances per sentence. Con-
sidering the huge size of the database, a process-
1This database was an aggregate of several Japanese-English
corpora, notably the Yomiuri newspaper corpus (Utiyama and
Isahara, 2003) and the JST paper abstract corpus created at
NICT(www.nict.go.jp) through (Utiyama and Isahara, 2007).
Method Treelet Our
dictionary method
Disk space used 23 GB 500 MB
BLEU 11.6% 12.0%
Figure 6: Comparison with a dictionary-based baseline
(performances averaged over 100 sentences).
ing time below 1 second seems reasonable. The
increase in processing time between the small and
the large database is in line with the explanations
of section 4.4. Path-to-Root Arrays are slightly bet-
ter than Inverted indexes (we suspect a better im-
plementation could increase the difference further).
Both methods use up about the same disk space:
around 500MB. We also find that the approach of
section 4.2 brings virtually no overhead and gives
similar performances whether the query tree is in the
database or not (effectively reducing the worst-case
computation time from days to seconds).
We also conducted a small English-to-Japanese
translation experiment with a simple translation sys-
tem using Synchronous Tree Substitution Grammars
(STSG) for translating dependency trees. The sys-
tem we used is still in an experimental state and
probably not quite at the state-of-the-art level yet.
However, we considered it was good enough for our
purpose, since we mainly want to test our algorithm
is a practical way. As a baseline, from our cor-
pus of 2.9 millions dependency trees, we automat-
ically extracted STSG rules of size smaller than 6
and stored them in a database, considering that ex-
tracting rules of larger sizes would lead to an un-
manageable database size. We compared MT results
using only the rules of size smaller than 6 to using
all the rules computed on-the-fly after treelet retriev-
ing by our method. These results are summarized on
figure 6.
6 Flexible matching
We now describe an extension of the algorithm for
approximate matching of treelets. We consider that
each node of the query tree and database is labeled
by 2 labels (or more) of different generality. For
concreteness, let?s consider dependency trees whose
nodes are labeled by words and the Part-Of-Speech
(POS) of these words. We want to retrieve treelets
515
that match by word or POS with the query tree.
6.1 Processing multi-Label trees
To do this, the inverted index will just need to
include entries for both words and POS. For ex-
ample, the dependency tree ?likes,V:1 (Paul,N:0
Salmon,N:2 (and,CC:3 (Tuna,N:4)))? would pro-
duce the following (node,parents) entries in the in-
verted index: {N:[(0,1) (2,1) (4,3)], Paul:[(0,1)],
Salmon:[(2,1)],. . . }. This allows to search for a
treelet containing any combination of labels, like
?likes(N Salmon(CC(N)))?.
We actually want to compute all of the treelets of
a query tree TQ labeled by words and POS (meaning
each node can be matched by either word or POS).
We can compute TQ without redundant computa-
tions by slightly modifying the algorithm 2. First,
we modify the recursive representation of a treelet
so that it also includes the chosen label of its root
node. Then, the only modifications needed in algo-
rithm 2 are the following: 1- at initialization (line 3),
the elementary treelets corresponding to every pos-
sible labels are added to the discovered treelets set
D; 2- in procedure compute-supertrees, at line 8, we
generate one ascending supertreelet per label.
6.2 Weighted search
While the previous method would allow us to com-
pute as efficiently as possible all the treelets in-
cluded in a multi-labeled query tree, there is still
a problem: even avoiding redundant computations,
the number of treelets to compute can be huge, since
we are computing all combinations of labels. For
each treelet of size m we would have had in a single
label query tree, we now virtually have 2m treelets.
Therefore, it is not reasonable in general to try to
compute all these treelets.
However, we are not really interested in comput-
ing all possible treelets. In our case, the POS la-
bels allow us to retrieve larger examples when none
containing only words would be available. But we
still prefer to find examples matched by words rather
than by POS. We therefore need to tell the algorithm
that some treelets are more important that some oth-
ers. While we have used the Computation Hypertree
representation to compute treelets efficiently, we can
also use it to prioritize the treelets we want to com-
pute. This is easily implemented by giving a weight
POS matchings Without With
Processing time 0.9 s 22 s
Largest non-empty treelet size 8.7 11.4
Treelets of size>8 0.4 102
BLEU 12.0% 12.1%
Figure 7: Effect of POS-matching
to every treelet. We can then modify our traversal
strategy of the Inclusion DAG to compute treelets
having the biggest weights first: we just need to
specify that the treelet popped out on line 3 is the
treelet with the highest score (more generally, we
could consider a A* search).
6.3 Experiments
Using the above ideas, we have made some experi-
ments for computing query dependency trees labeled
with both words and POS. We score the treelets by
giving them a penalty of -1 for each POS they con-
tain, and stop the search when all remaining treelets
have a score lower than -2 (in other words, treelets
are allowed at most 2 POS-matchings). We also re-
quire POS-matched nodes to be non-adjacent.
We only have some small modifications to do to
algorithm 2. In line 3 of algorithm 2, elementary
treelets are assigned a weight of 0 or -1 depend-
ing on whether their label is a word or POS. Line 5
is replaced by ?pop the first treelet with minimal
weight and break the loop if the minimal weight is
inferior to -2?. In compute-supertreelets, we give a
weight to the generated supertreelets by combining
the weights of the child treelets.
Table 7 shows the increase in the size of the
biggest non-empty treelets when allowing 2 nodes
to be matched by POS. It also shows the impact on
BLEU score of using these additional treelets for on-
the-fly rule generation in our simpleMT system. Im-
provement on BLEU is limited, but it might be due
to a very experimental handling of approximately
matched treelet examples in our MT system.
The computation time, while manageable, was
much slower than in the one-label case. This is due
to the increased number of treelets to be computed,
and to the fact that POS-labeled elementary treelets
have a high number of occurrences. It would be
more efficient to use more specific labeling (e.g V-
516
Figure 8: A packed forest.
mvt for verbs of movement instead of V).
7 Additional extensions
We briefly discuss here some additional extensions
to our algorithm that we will not detail for lack of
room and practical experiments.
7.1 Packed forest
Due to parsing ambiguities and automatic parsers
errors, it is often useful to use multiple parses of
a given sentence. These parses can be represented
by a packed forest such as the one in figure 8. Our
method allows the use of packed representation of
both the query tree and the database.
For the inverted index, the only difference is
that now, an occurrence of a label can have more
than one parent. For example, the inverted in-
dex of a database containing the packed forest
of figure 8 would contain the following entries:
{held: [(1,10a),(1,10b)], NP: [(6,9),(7,9),(9,10a)],
VP:[(10,N)], PP:[(8,10b)], a:[(2,6)], talk:[(3,6)],
with:[(4,7) (4,8)], Sharon:[(5,7) (5,8)]}. Where 10a
and 10b are some kind of virtual position that help to
specify that held and NP8 belong to the same chil-
dren list. We could also include a cost on edges
in the inverted index, which would allow to prune
matchings to unlikely parses.
The inverted index can now be used to search in
the trees contained in a packed forest database with-
out any modification. Modifications to the algorithm
in order to handle a packed forest query are similar
to the ones developed in section 6.
7.2 Discontinuous treelets
As we discussed in section 2.3, using a representa-
tion for the position of every node similar to (Zhang
et al, 2001), it is possible to determine the distance
and ancestor relationship of two nodes by just com-
paring their positions. This opens the possibility of
computing the occurrences of discontinuous treelets
in much the same way as is done in (Lopez, 2007)
for discontinuous substrings. We have not studied
this aspect in depth yet, especially since we are not
aware of any MT system making use of discontin-
uous syntax tree examples. This is nevertheless an
interesting future possibility.
8 Related work
As we previously mentioned, (Lopez, 2007) and
(Callison-Burch et al, 2005) propose a method sim-
ilar to ours for the string case.
We are not aware of previous proposals for ef-
ficient on-the-fly retrieving of translation examples
in the case of Syntax-Based Machine Translation.
Among the works involving rule precomputation,
(Zhang et al, 2009) describes a method for effi-
ciently matching precomputed treelets rules. These
rules are organized in a kind of prefix tree that al-
lows efficient matching of packed forests. (Liu et al,
2006) also propose a greedy algorithm for matching
TSC rules to a query tree.
9 Conclusion and future work
We have presented a method for efficiently retriev-
ing examples of treelets contained in a query tree,
thus allowing on-the-fly computation of translation
rules for Syntax-Based systems. We did this by
building on approaches previously proposed for the
case of string examples, proposing an adaptation of
the concept of Suffix Arrays to trees, and formaliz-
ing computation as the traversal of an hypergraph.
This hypergraph allows us to easily formalize dif-
ferent computation strategy, and adapt the methods
to flexible matchings. We still have a lot to do with
respect to improving our implementation, exploring
the different possibilities offered by this framework
and proceeding to more experiments.
Acknowledgments
We thank the anonymous reviewers for their useful
comments.
517
References
R. Baeza-Yates and A. Salinger. 2005. Experimental
analysis of a fast intersection algorithm for sorted se-
quences. In String Processing and Information Re-
trieval, page 1324.
N. Bruno, N. Koudas, and D. Srivastava. 2002. Holis-
tic twig joins: optimal XML pattern matching. In
Proceedings of the 2002 ACM SIGMOD international
conference on Management of data, page 310321.
C. Callison-Burch, C. Bannard, and J. Schroeder. 2005.
Scaling phrase-based statistical machine translation to
larger corpora and longer phrases. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, pages 255?262. Association for
Computational Linguistics Morristown, NJ, USA.
David Chiang. 2007. Hierarchical Phrase-Based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
M. Dubiner, Z. Galil, and E. Magen. 1994. Faster
tree pattern matching. Journal of the ACM (JACM),
41(2):205213.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of HLT-
NAACL, pages 48?54. Association for Computational
Linguistics.
Z. Liu, H. Wang, and H. Wu. 2006. Example-based
machine translation based on tree?string correspon-
dence and statistical generation. Machine translation,
20(1):25?41.
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proc. of EMNLP-CoNLL, page
976985.
U. Manber and G. Myers. 1990. Suffix arrays: a
new method for on-line string searches. In Proceed-
ings of the first annual ACM-SIAM symposium on Dis-
crete algorithms, pages 319?327, San Francisco, CA,
USA. Society for Industrial and Applied Mathematics
Philadelphia, PA, USA.
H. Mi, L. Huang, and Q. Liu. 2008. Forest based trans-
lation. Proceedings of ACL-08: HLT, page 192199.
Toshiaki Nakazawa and Sadao Kurohashi. 2008. Syn-
tactical EBMT system for NTCIR-7 patent translation
task. In Proceedings of NTCIR-7 Workshop Meeting,
Tokyo, Japon.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, page 279.
M. Utiyama and H. Isahara. 2003. Reliable measures for
aligning japanese-english news articles and sentences.
In Proceedings of ACL, pages 72?79, Sapporo,Japon.
M. Utiyama and H. Isahara. 2007. A japanese-english
patent parallel corpus. In MT summit XI, pages 475?
482.
M. Yamamoto and K. W. Church. 2001. Using suffix
arrays to compute term frequency and document fre-
quency for all substrings in a corpus. Computational
Linguistics, 27(1):1?30.
Y. Zhang and S. Vogel. 2005. An efficient phrase-to-
phrase alignment model for arbitrarily long phrase and
large corpora. In Proceedings of EAMT, pages 294?
301, Budapest, Hungary.
C. Zhang, J. Naughton, D. DeWitt, Q. Luo, and
G. Lohman. 2001. On supporting containment queries
in relational database management systems. In Pro-
ceedings of the 2001 ACM SIGMOD international
conference on Management of data, page 425436.
H. Zhang, M. Zhang, H. Li, and Chew Lim Tan. 2009.
Fast translation rule matching for syntax-based statis-
tical machine translation. In Proc. of EMNLP, pages
1037?1045.
518
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 605?615,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Non-parametric Bayesian Segmentation of Japanese Noun Phrases
Yugo Murawaki and Sadao Kurohashi
Graduate School of Informatics
Kyoto University
{murawaki, kuro}@i.kyoto-u.ac.jp
Abstract
A key factor of high quality word segmenta-
tion for Japanese is a high-coverage dictio-
nary, but it is costly to manually build such
a lexical resource. Although external lexical
resources for human readers are potentially
good knowledge sources, they have not been
utilized due to differences in segmentation cri-
teria. To supplement a morphological dictio-
nary with these resources, we propose a new
task of Japanese noun phrase segmentation.
We apply non-parametric Bayesian language
models to segment each noun phrase in these
resources according to the statistical behavior
of its supposed constituents in text. For in-
ference, we propose a novel block sampling
procedure named hybrid type-based sampling,
which has the ability to directly escape a lo-
cal optimum that is not too distant from the
global optimum. Experiments show that the
proposed method efficiently corrects the initial
segmentation given by a morphological ana-
lyzer.
1 Introduction
Word segmentation is the first step of natural lan-
guage processing for Japanese, Chinese and Thai
because they do not delimit words by white-space.
Segmentation for Japanese is a successful field of re-
search, achieving the F-score of nearly 99% (Kudo
et al, 2004). This success rests on a high-coverage
dictionary. Unknown words, or words not covered
by the dictionary, are often misidentified.
Historically, researchers have devoted exten-
sive human resources to build and maintain high-
coverage dictionaries (Yokoi, 1995). Since the or-
thography of Japanese does not specify a standard
for segmentation, researchers define their own crite-
ria before constructing lexical resources. For this
reason, it is difficult to exploit existing external
resources, such as dictionaries and encyclopedias
for human readers, where entry words are not seg-
mented according to the criteria. Among them,
encyclopedias are especially important in that they
contain a lot of terms that a morphological dictio-
nary fails to cover. Most of these terms are noun
phrases and consist of more than one word (mor-
pheme). For example, an encyclopedia has an en-
try ????? (tsuneyama-jou, ?Tsuneyama Castle?).
According to our segmentation criteria, it consists
of two words ???? (tsuneyama) and ??? (jou).
However, the morphological analyzer wrongly seg-
ments it into ??? (tsune) and ???? (yamashiro)
because ???? (tsuneyama) is an unknown word.
In this paper, we present the first attempt to uti-
lize encyclopedias for word segmentation. We seg-
ment each entry noun phrase into words. To do this,
we examine the main text of the entry, on the as-
sumption that if the noun phrase in question con-
sists of more than one word, its constituents appear
in the main text either freely or as part of other
noun phrases. For ????? (tsuneyama-jou), its
constituent ???? (tsune) appears by itself and as
constituents of other nouns phrases such as ????
?? (peak of Tsuneyama) and ????? (Tsuneyama
Station) while ???? (yamashiro) does not.
To segment each noun phrase, we use non-
parametric Bayesian language models (Goldwater et
al., 2009; Mochihashi et al, 2009). Our approach
605
is based on two key factors: the bigram model and
type-based block sampling. The bigram model al-
leviates a problem of the unigram model, that is, a
tendency to misidentify a sequence of words in com-
mon collocations as a single word. Type-based sam-
pling (Liang et al, 2010) has the ability to directly
escape a local optimum, making inference very ef-
ficient. However, type-based sampling is not easily
applicable to the bigrammodel owing to sparsity and
its dependence on latent assignments.
We propose a hybrid type-based sampling proce-
dure, which combines the Metropolis-Hastings al-
gorithm with Gibbs sampling. We circumvent the
sparsity problem by joint sampling of unigram-level
type. Also, instead of calculating the probability of
every possible state of the jointly sampled random
variables, we only compare the current state with
a proposed state. This greatly eases the sampling
procedure while retaining the efficiency of type-
based sampling. Experiments show that the pro-
posed method quickly corrects the initial segmen-
tation given by a morphological analyzer.
2 Related Work
Japanese Morphological Analysis and Lexical
Acquisition Word segmentation for Japanese is
usually solved as the joint task of segmentation and
part-of-speech tagging, which is called morpholog-
ical analysis (Kurohashi et al, 1994; Asahara and
Matsumoto, 2000; Kudo et al, 2004). The stan-
dard approach in Japanese morphological analysis
is lattice-based path selection instead of character-
based IOB tagging. Given a sentence, an analyzer
first builds a lattice of words with dictionary look-up
and then selects an optimal path using pre-defined
parameters. This approach enables fast decoding
and achieves accuracy high enough for practical use.
This success, however, depends on a high-
coverage dictionary, and unknown words are often
misidentified. Although a line of research attempts
to identify unknown words on the fly (Uchimoto et
al., 2001; Asahara and Matsumoto, 2004), it by no
means provides a definitive solution because it suf-
fers from locality of contextual information avail-
able for identification (Nakagawa and Matsumoto,
2006). Therefore we like to perform separate lexical
acquisition processes in which wider context can be
examined.
Our approach in this paper has a complementary
relationship with unknown word acquisition from
text, which we previously proposed (Murawaki and
Kurohashi, 2008). Since, unlike Chinese and Thai,
Japanese is rich in morphology, morphological reg-
ularity can be used to determine if an unknown
word candidate in text is indeed the word to be ac-
quired. In general, this method works pretty well,
but one exception is noun phrases. Noun phrases
can hardly be distinguished from single nouns be-
cause in Japanese, no morphological marker is at-
tached to join nouns to form a noun phrase. We
previously resort to a heuristic measure to segment
noun phrases. The new statistical method provides a
straightforward solution to this problem.
Meanwhile, our language models have their own
problem. The assumption that language is a se-
quence of invariant words fails to capture rich mor-
phology, as our segmentation criteria specify that
each verb or adjective consists of an invariant stem
and an ending that changes its form according to
its grammatical roles. For this reason, we limit our
scope to noun phrases in this paper.
Use of Noun Phrases Named entity recogni-
tion (NER) is a field where encyclopedic knowl-
edge plays an important role. Kazama and Tori-
sawa (2008) encode information extracted from a
gazetteer (e.g. Wikipedia) as features of a CRF-
based Japanese NE tagger. They formalize the NER
task as the character-based labeling of IOB tags.
Noun phrases extracted from a gazetteer are also
straightforwardly represented as IOB tags. How-
ever, this does not fully solve the knowledge bot-
tleneck problem. They also used the output of a
morphological analyzer, which does not utilize en-
cyclopedic knowledge. NER performance may be
affected by segmentation errors in morphological
analysis involving unknown words.
Chinese word segmentation is often formalized as
a character tagging problem (Xue, 2003). In this
setting, it is easy to incorporate external resources
into the model. Low et al (2005) introduce an exter-
nal dictionary as features of a discriminative model.
However, they only use words up to 4 characters in
length. We conjecture that words in their dictionary
are not noun phrases. External resources used by
606
Peng et al (2004) are also lists of short words and
characters.
Non-parametric Language Models Non-
parametric Bayesian statistics offers an elegant
solution to the task of unsupervised word segmen-
tation, in which the vocabulary size is not known in
advance (Goldwater et al, 2009; Mochihashi et al,
2009). It does not compete with supervised segmen-
tation, however. Unsupervised word segmentation
is used elsewhere, for example, with theoretical
interest in children?s language acquisition (Johnson,
2008; Johnson and Demuth, 2010) and with the
application to statistical machine translation, in
which segmented text is merely an intermediate rep-
resentation (Xu et al, 2008; Nguyen et al, 2010).
In this paper we demonstrate that non-parametric
models can complement supervised segmentation.
3 Japanese Noun Phrase Segmentation
Our goal is to overcome the unknown word prob-
lem in morphological analysis by utilizing existing
resources such as dictionaries and encyclopedias for
human readers. In our settings, we are given a list of
entries from external resources. Almost all of them
are noun phrases and each entry consists of one or
more words.
A na??ve implementation would be to use noun
phrases as they are. In fact, ipadic1 regards as single
words a large number of long proper nouns like ??
??????????? (literally, Kansai Interna-
tional Airport Company Connecting Bridge). How-
ever, this approach has various drawbacks. For ex-
ample, in information retrieval, the query ?Kansai
International Airport? does not match the ?single?
word for the bridge. So we apply segmentation.
Each entry is associated with text, which is usu-
ally the main text of the entry.2 We assume the text
as the key to segmenting the noun phrase. If the
noun phrase in question consists of more than one
word, its constituents would appear in the text either
freely or as part of other noun phrases.
We obtain the segmentation of an entry noun
phrase by considering the segmentation of the whole
1http://sourceforge.jp/projects/ipadic/
2We may augment the text with related documents if the
main text is not large enough.
text. One may instead consider a pipeline ap-
proach in which we first extract noun phrases in
text and then identify boundaries within these noun
phrases. However, noun phrases in text are not triv-
ially identifiable in the case that they contain un-
known words as their constituents. For example,
the analyzer erroneously segments the word ???
???? (chiNsukou) into ???? (chiN) and ???
?? (sukou), and since the latter is misidentified as
a verb, the incorrect noun phrase ???? (chiN) is
extracted.
We have a morphological analyzer with a dictio-
nary that covers frequent words. Although it often
misidentifies unknown words, the overall accuracy
is reasonably high. For this reason, we like to use
the segmentation given by the analyzer as the ini-
tial state and to make small changes to them to get
a desired output. We also use an annotated corpus,
which was used to build the analyzer. As the an-
notated corpus encodes our segmentation criteria, it
can be used to force the models to stick with our
segmentation criteria.
We concentrate on segmentation in this paper, but
we also need to assign a POS tag to each constituent
word and to incorporate segmented noun phrases
into the dictionary of the morphological analyzer.
We leave them for future work.3
4 Non-parametric Bayesian Language
Models
To correct the initial segmentation given by the an-
alyzer, we use non-parametric Bayesian language
models that have been applied to unsupervised word
segmentation (Goldwater et al, 2009). Specifically,
we adopt unigram and bigram models. We propose
a small modification to these models in order to ex-
ploit an annotated corpus when it is much larger than
raw text.
4.1 Unigram Model
In the unigram model, a word in the corpus wi is
generated as follows:
G|?0, P0 ? DP(?0, P0)
wi|G ? G
3Fortunately, the morphological analyzer JUMAN is capa-
ble of handling phrases, each of which consists of more than
one word. All we need to do is POS tagging.
607
where G is a distribution over a countably infinite
set of words, and DP(?0, P0) is a Dirichlet pro-
cess (Ferguson, 1973) with the concentration param-
eter ?0 and the base distribution P0, for which we
use a zerogram model described in Section 4.3.
Marginalizing out G, we can interpret the model
as a Chinese restaurant process. Suppose that we
have observed i ? 1 words w?i = w1, ? ? ? , wi?1,
the probability of wi is given by
P1(wi = w|w?i) =
nw?iw + ?0P0
i? 1 + ?0
, (1)
where nw?iw is the number of word label w observed
in w?i.
The unigram model is known for its tendency to
misidentify a sequence of words in common collo-
cations as a single word (Goldwater et al, 2009). In
preliminary experiments, we found that the unigram
model often interpreted a noun phrase as a single
word, even in the case that its constituents frequently
appeared in text.
4.2 Bigram Model
The problem of the unigram model can be alleviated
by the bigram model based on a hierarchical Dirich-
let process (Goldwater et al, 2009). In the bigram
model, word wi is generated as follows:
G|?0, P0 ? DP(?0, P0)
Hl|?1, G ? DP(?1, G)
wi|wi?1 = l,Hl ? Hl
Marginalizing out G and Hl, we can again explain
the model with the Chinese restaurant process. Un-
like the unigram model, however, the bigram model
depends on the latent table assignments z?i.
P2(wi|h?i) =
nh?i(wi?1,wi) + ?1P1(wi|h?i)
nh?i(wi?1,?) + ?1
(2)
P1(wi|h?i) =
th?iwi + ?0P0(wi)
th?i? + ?0
(3)
where h?i = (w?i, z?i), th?iwi is the number of ta-
bles labeled with wi and th?i? is the total number of
tables. Thanks to exchangeability, we do not need to
track the exact seating assignments. Still, we need to
maintain a histogram for each w that consists of fre-
quencies of table customers (Blunsom et al, 2009).
4.3 Zerogram Model
Following Nagata (1996) and Mochihashi et al
(2009), we model the zerogram distribution P0 with
the word length k and the character sequence w =
c1, ? ? ? , ck. Specifically, we define P0 as the combi-
nation of a Poisson distribution with mean ? and a
bigram distribution over characters.
P0(w) = P (k;?)
P (c1, ? ? ? , ck, k|?)
P (k|?)
P (k;?) = e???
k
k!
P (c1, ? ? ? , ck, k|?) =
k+1?
i=1
P (ci|ci?1)
? is the zerogrammodel, and c0 and ck+1 are a word
boundary marker. P (k|?) can be estimated by ran-
domly generating words from the model. We use
different ? for different scripts. The Japanese writ-
ing system uses several scripts, and each word can
be classified by script such as hiragana, katakana,
kanji, the mixture of hiragana and kanji, etc. The op-
timal value for ? depends on scripts. For example,
katakana, which predominantly denotes loan words,
is longer on average than hiragana, which is often
used for short function words.
We obtain the parameters and counts from an an-
notated corpus and fix them during noun phrase seg-
mentation. This greatly simplifies inference but may
make the model fragile with unknown words. For
this reason, we set a hierarchical Pitman-Yor process
prior (Teh, 2006; Goldwater et al, 2006) for the bi-
gram probability P (ci|ci?1) with the base distribu-
tion of character unigrams. Note that even character
bigrams are sparse because thousands of characters
are used in Japanese.
4.4 Mixing an Annotated Corpus
An annotated corpus can be used to force the mod-
els to stick with our segmentation criteria. A
straightforward way to do this is to mix it with
raw text while fixing the segmentation during infer-
ence (Mochihashi et al, 2009). A word found in
the annotated corpus is generally preferred because
it has fixed counts obtained from the annotated cor-
pus. We call this method direct mixing.
Direct mixing is problematic when raw text is
much smaller than the annotated corpus. With this
608
situation, the role of raw text associated with the
noun phrase in question is marginalized by the an-
notated corpus.
As a solution to this problem, we propose another
mixing method called back-off mixing. In back-off
mixing, the annotated corpus is used as part of the
base distribution. In the unigram model, P0 in (1) is
replaced by
PBM0 = ?IPP0 + (1? ?IP)PREF1 ,
where ?IP is a parameter for linear interpolation and
PREF1 is the unigram probability obtained from the
annotated text. The loose coupling makes the mod-
els robust to an imbalanced pair of texts. Similarly,
the back-off mixing bigram model replaces P1 in (2)
with
PBM1 = ?IPP1 + (1? ?IP)PREF2 .
5 Inference
Collapsed Gibbs sampling is widely used to find
an optimal segmentation (Goldwater et al, 2009).
In this section, we first show that simple collapsed
sampling can hardly escape the initial segmentation.
To address this problem, we apply a block sam-
pling algorithm named type-based sampling (Liang
et al, 2010) to the unigram model. Since type-based
sampling is not applicable to the bigram model, we
propose a novel sampling procedure for the bigram
model, which we call hybrid type-based sampling.
5.1 Collapsed Sampling
In collapsed Gibbs sampling, the sampler repeatedly
samples every possible boundary position, condi-
tioned on the current state of the rest of the corpus.
It stochastically decides whether the corresponding
local area consists of a single word w1 or two words
w2w3 (w1 = w2.w3). The conditional probabilities
can be derived from (1).
Collapsed sampling is known for slow conver-
gence. This property is especially problematic in
our settings where the initial segmentation is given
by a morphological analyzer. Since the analyzer de-
terministically segments text using pre-defined pa-
rameters, the resultant segmentation is fairly consis-
tent. Segmentation errors involving unknown words
also occur in a regular way. Intuitively, we start with
a local optimum although it is not too distant from
the global optimum. The collapsed Gibbs sampler is
easily entrapped by this local optimum. For this rea-
son, the initial segmentation is usually chosen at ran-
dom (Goldwater et al, 2009). Sentence-based block
sampling is also susceptible to consistent initializa-
tion (Liang et al, 2010).
5.2 Type-based Sampling
To achieve fast convergence, we adopt a block sam-
pling algorithm named type-based sampling (Liang
et al, 2010). For the unigram model, a type-based
sampler jointly samples multiple positions that share
the same type. Two positions have the same type
if the corresponding areas are both of the form w1
or w2w3. Type-based sampling takes advantage of
the exchangeability of multiple positions with the
same type. Given n positions with the same type,
the sampler first samples the number of new bound-
aries m? (0 ? m? ? n), and then uniformly arranges
m? boundaries out of n positions.
Type-based sampling has the ability to jump from
a local optimum (e.g. consistently segmented) to an-
other stable state (consistently unsegmented). While
Liang et al (2010) used random initialization, we
take particular note of the possibility of efficiently
correcting the consistent segmentation by the ana-
lyzer.
Type-based sampling is, however, not applicable
to the bigram model for two reasons. The first prob-
lem is sparsity. For the bigram model, we need to
consider adjacent words, wl on the left and wr on
the right. This means that each type consists of
three or four words, wlw1wr or wlw2w3wr. Con-
sequently, few positions share the same type and
we fail to change closely-related areas wl?w1wr? and
wl?w2w3wr? , making inference inefficient.
The second and more fundamental problem arises
from the hierarchical settings. Since the bigram
model depends on latent table assignments, the joint
distribution of multiple positions is no longer a
closed-form function of counts.
Strictly speaking, we need to update the model
counts even when sampling one position because
the observation of the bigram ?wlw1?, for exam-
ple, may affect the probability P2(w2|h?, ?wlw1?).
Goldwater et al (2009) approximate the probability
by not updating the model counts in collapsed Gibbs
609
sampling (i.e. P2(w2|h?, ?wlw1?) ? P2(w2|h?)).
They rely on the assumption that repeated bigrams
are rare. Obviously this does not hold true for type-
based sampling. Hence for type-based sampling, we
have to update the model counts whenever we ob-
serve a new word.
One way to obtain the joint probability is to ex-
plicitly simulate the updates of histograms and other
model counts. This is very cumbersome as we need
to simulate n+ 1 ways of model updates.
5.3 Hybrid Type-based Sampling
To address these problems, we propose a hybrid
sampler which incorporates the Metropolis-Hastings
algorithm into blocked Gibbs sampling. Metropolis-
Hastings is another technique for sampling from a
Markov chain. It first draws a proposed next state
h? based on the current state h according to some
proposal distribution Q(h?;h). Then it accepts the
proposal with the probability of
min
{P (h?)Q(h;h?)
P (h)Q(h?;h) , 1
}
. (4)
If the proposal is not accepted, the current state is
used as the next state. Metropolis-Hastings is useful
when it is difficult to directly sample from P .
We use the Metropolis-Hastings algorithm within
Gibbs sampling. Instead of calculating the n + 1
probabilities of the number of boundaries, we only
compare the current state with a proposed bound-
ary arrangement. Also, the set of positions sampled
jointly is chosen at unigram-level type instead of
bigram-level type. The positions are no longer ex-
changeable. Therefore we calculate the conditional
probability of one specific boundary arrangement.
When n = 1, the only choice is to flip the cur-
rent state (i.e. (m,m?) ? {(0, 1), (1, 0)}). This re-
duces to simple collapsed sampling. Otherwise we
draw a proposed state in two steps. Given the n
positions and the number of current boundaries m,
we first draw the number of proposed boundaries m?
from a probability distribution fn(m?;m). We then
randomly arrange m? boundaries. The probability
mass is uniformly divided by nCm? arrangements.
One exception is the case when m /? {0, n} and
m? = m. In this case we perform permutation to
obtain h? ?= h. To sum up, the proposal distribution
 0
 0.1
 0.2
 0.3
 0.4
 0  2  4  6  8  10 0
 0.1
 0.2
 0.3
 0.4
p
r
o
b
a
b
i
l
i
t
y
m?
Figure 1: Probability of # of boundaries f10(m?; 3).
is defined as follows:
Q(h?;h) = fn(m
?;m)
nCm? ? In(m,m?)
, (5)
where In(m,m?) is 1 if m /? {0, n} and m? = m;
otherwise 0.
We construct fn(m?;m) by discretizing a beta
distribution (? = ? < 1) and a normal distribution
with mean m, as shown in Figure 1. The former fa-
vors extreme values while the latter prefers smaller
moves.
The sampling of each type is done in the follow-
ing steps.
1. Collect n positions that share a unigram-level
type.
2. Propose a new boundary arrangement. In what
follows, we only focus on flipped boundaries
because the rest does not change the likelihood
ratio of the current and proposed states.
3. Calculate the current conditional probability.
This can be done by repeatedly applying (2)
while removing words one-by-one and updat-
ing the model counts accordingly.
4. Calculate the proposed conditional probability
while adding words one-by-one.
5. Decide whether to accept the proposal accord-
ing to (4). If the proposal is accepted, we final-
ize the arrangement; otherwise we revert to the
current state.
We implement skip approximation (Liang et al,
2010) and sample each type once per iteration. This
is motivated by the observation that although the
610
joint sampling of a large number of positions is com-
putationally expensive, the proposal is accepted very
infrequently.
5.4 Additional Constraints
Partial annotations (Tsuboi et al, 2008; Neubig and
Mori, 2010) can be used for inference. If we know in
advance that a certain position is a boundary or non-
boundary, we simply keep it unaltered. As partially-
annotated text, we can use markup. Suppose that the
original text is written with wiki markup as follows:
*JR[[???]][[???]]
[gloss] JR Ube Line Tsuneyama Station
It is clear that the position between ??? (line) and
??? (tsune) is a boundary.
Similarly, we can impose our trivial rules of seg-
mentation on the model. For example, we can keep
punctuation markers (Li and Sun, 2009) separate
from others.
6 Experiments
6.1 Settings
Data Set We evaluated our approach on Japanese
Wikipedia. For each entry of Wikipedia, we re-
garded the title as a noun phrase and used both the
title and main text for segmentation. We separately
applied our segmentation procedure to each entry.
We constructed the data set as follows. We ex-
tracted each entry from an XML dump of Japanese
Wikipedia.4 We normalized the title by dropping
trailing parentheses that disambiguate entries with
similar names (e.g. ??? (??)? for Akagi (aircraft
carrier)). We extracted the main text from wikitext
and used wiki markup as boundary markers. We ap-
plied both the title and main text to the morphologi-
cal analyzer JUMAN5 to get an initial segmentation.
If the resultant segmentation conflicted with markup
information, we overrode the former. The initial seg-
mentation was also used as the baseline.
We only used entries that satisfied all of the fol-
lowing conditions.
1. The (normalized) title is longer than one char-
acter and contains hiragana, katakana and/or
kanji.
4http://download.wikimedia.org/jawiki/
5http://nlp.ist.i.kyoto-u.ac.jp/EN/
index.php?JUMAN
2. The main text is longer than 1,000 characters.
3. The title appears at least 5 times in the main
text.
The first condition ensures that there are segmenta-
tion ambiguities. The second and third conditions
exclude entries unsuitable for statistical methods.
14% of the entries satisfied these conditions.
We randomly selected 500 entries and manually
segmented their titles for evaluation. The 2-person
inter-annotator Kappa score was 0.95.
As an annotated corpus, we used Kyoto Text Cor-
pus.6 It contained 1,675,188 characters.
Models We compared the unigram and bigram
models. As for inference procedures, we used col-
lapsed Gibbs sampling (CL) for both models, type-
based sampling (TB) for the unigram model and
hybrid type-based sampling (HTB) for the bigram
model.
We tested two mixing methods of the annotated
corpus, direct mixing (DM) and back-off mixing
(BM).
To investigate the effect of initialization, we also
tried randomly segmented text as the initial state
(RAND). For random initialization, we placed a
boundary with probability 0.5 on each position un-
less it was a fixed boundary.
The unigram model has one Dirichlet process
concentration hyperparameter ?0 and the bigram
model has ?0 and ?1. For each model, we experi-
mented with the following values.
?0: 0.1, 0.5, 1 5 10, 50, 100, 500, 1,000 and 5,000
?1: 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100 and 500
For comparison, we also performed hyperparame-
ter sampling. Following Escobar and West (1995),
we set a gamma prior and introduced auxiliary vari-
ables to infer concentration parameters from data.
For back-off mixing, we used the linear interpola-
tion parameter ?IP = 0.5. The zerogram model was
trained on the annotated corpus.
In each run, we performed 10 burn-in iterations.
We then performed another 10 iterations to collect
samples.
6http://nlp.ist.i.kyoto-u.ac.jp/EN/
index.php?Kyoto%20University%20Text%
20Corpus
611
Table 1: Results of segmentation of entry titles (F-score (precision/recall)).
model best median inferred
unigram + CL 81.35 (77.78/85.27)** 80.09 (75.80/84.89) 80.86 (76.81/85.36)
unigram + TB 55.87 (66.71/48.06) 51.04 (62.64/43.06) 42.63 (54.91/34.84)
bigram + CL 80.65 (76.73/84.99) 79.96 (75.50/84.99) 80.54 (76.84/84.61)
bigram + HTB 83.23 (85.25/81.30)** 74.52 (71.33/78.00) 34.52 (46.69/27.38)
unigram + CL + DM 85.29 (83.14/87.54)** 81.62 (77.93/85.70)** 80.91 (82.87/79.04)
unigram + TB + DM 35.26 (47.74/29.95) 33.81 (46.20/26.66) 31.90 (44.30/24.93)
bigram + CL + DM 80.37 (76.01/85.27) 79.88 (75.42/84.89) 73.77 (78.49/69.59)
bigram + HTB + DM 69.66 (67.68/71.77) 67.39 (64.35/70.73) 31.54 (43.79/24.64)
unigram + CL + BM 81.28 (77.48/85.46) 80.23 (76.06/84.89) 81.42 (77.75/85.46)
unigram + TB + BM 57.22 (68.01/49.39) 52.98 (64.50/44.95) 42.43 (54.69/34.66)
bigram + CL + BM 81.33 (77.34/85.74) 80.07 (75.69/84.99) 81.46 (77.82/85.46)**
bigram + HTB + BM 86.32 (85.67/86.97)** 76.35 (71.89/81.40) 40.81 (53.35/33.05)
unigram + TB + RAND 56.01 (66.93/48.16) 50.89 (62.21/43.06) 42.68 (54.81/34.94)
bigram + HTB + RAND 79.68 (80.13/79.23) 68.16 (63.64/73.37) 34.99 (47.05/27.86)
unigram + TB + BM + RAND 57.44 (67.91/49.76) 50.86 (61.92/43.15) 42.31 (54.55/34.56)
bigram + HTB + BM + RAND 84.03 (83.10/84.99) 70.46 (65.25/76.58) 40.16 (52.60/32.48)
baseline (JUMAN) 80.09 (75.80/84.89)
** Statistically significant improvement with p < 0.01.
Evaluation Metrics We evaluated the segmenta-
tion accuracy of 500 entry titles. Specifically we
evaluated the performance of a model with preci-
sion, recall and the F-score, all of which were based
on tokens. We report the score of the most frequent
segmentation among 10 samples.
Following Lee et al (2010), we report the best and
median settings of hyperparameters based on the F-
score, in addition to inferred values.
In order to evaluate the degree of difference
between a pair of segmentations, we employed
character-based evaluation. Following Kudo et
al. (2004), we converted a word sequence into
character-based BI labels and examined labeling dis-
agreements. McNemar?s test of significance was
based on this metric.
6.2 Results
Table 1 shows segmentation accuracy of various
models. One would notice that the baseline score
is much lower than the score previously reported re-
garding newspaper articles (Kudo et al, 2004). It
is because unlike newspaper articles, the titles of
Wikipedia entries contain an unusually high pro-
portion of unknown words. As suggested by rel-
atively low precision, unknown words tend to be
over-segmented by the morphological analyzer.
In the best hyperparameter settings, the back-off
mixing bigram model with hybrid type-based sam-
pling (bigram + HTB + BM) significantly outper-
formed the baseline and achieved the best F-score.
It did not performed well in the median setting as
it was sensitive to the value of ?1. Hyperparameter
estimation led to catastrophic decreases in bigram
models as it made the hyperparameters much larger
than those in the best settings.
Collapsed sampling (+CL) returned scores com-
parable to that of the baseline. It is simply because
it did not change the initial segmentation a lot. In
contrast, type-based sampling (+TB) brought large
moves to the unigram model and significantly hurt
accuracy. As suggested by relatively low recall, the
unigram model prefers under-segmentation.
When combined with (hybrid) type-based sam-
pling (+TB/+HTB), back-off mixing (+BM) in-
creased accuracy from the corresponding non-
mixing models. By contrast, direct mixing (+DM)
drastically decreased accuracy from the non-mixing
models. We can confirm that when the main text
is orders of magnitude smaller than the annotated
text, the role of constituent words in the main text
is underestimated. To our surprise, collapsed sam-
pling with mixing models (+CL, +DM/+BM) out-
performed the baseline. However, the scores of type-
based sampling (+TB) suggest that with much more
iterations, the models would converge to undesired
states.
612
The unigram model with random initialization
was indifferent from that with default initialization.
By contrast, the performance of the bigram model
slightly degenerated with random initialization.
6.3 Convergence
Figure 2 shows how segmentations differed from
the initial state in the course of inference.7 A diff
is defined as the number of character-based dis-
agreements between the baseline segmentation and a
model output. Hyperparameters used were those of
the best model with (hybrid) type-based sampling.
We can see that collapsed sampling was almost
unable to escape the initial state. With type-based
sampling (+TB), the unigram model went further
than the bigram model, but to an undesired direc-
tion. The bigram model with hybrid type-based
sampling (bigram + HTB) converged in few itera-
tions. Although the model with random initializa-
tion (+RAND) converged to a nearby point, the ini-
tial segmentation by the morphological analyzer re-
alized a bit faster convergence and better accuracy.
Figure 2 shows how acceptance rates changed
during inference. For comparison, a sample by a
type-based Gibbs sampler was treated as ?accepted?
if the number of new boundaries was different from
that of the current boundaries (i.e. m? ?= m). The
acceptance rates were low and samplers seemingly
stayed around modes.
6.4 Approximation
Up to this point, we consider every possible bound-
ary position. However, this seems wasteful, given
that a large portion of text has only marginal influ-
ence on the segmentation of the noun phrase in ques-
tion. For this reason, we implemented approxima-
tion named matching skip. We sampled a boundary
only if the corresponding local area contained a sub-
string of the noun phrase in question.
Table 2 shows the result of approximation. Hy-
perparameters used were those of the best models
with full sampling. Matching skip steadily worsened
performance although not to a large extent. Mean-
7For a fair comparison, we might need to report changes
over time instead of iterations. However, the difference of con-
vergence speed is obvious in the iteration-based comparison al-
though (hybrid) type-based sampling takes several times longer
than collapsed sampling in the current na??ve implementation.
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 0  5  10  15  20
d
i
f
f
iteration
unigram + CLunigram + TBbigram + CLbigram + HTBunigram + TB + RANDbigram + HTB + RAND
Figure 2: Diffs in the course of iteration. All models were
with back-off mixing (+BM).
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0  5  10  15  20
a
c
c
e
p
t
a
n
c
e
 
r
a
t
e
iteration
unigram + TBbigram + HTBunigram + TB + RANDbigram + HTB + RAND
Figure 3: Acceptance rates for a noun phrase in the
course of iteration. All models were with back-off mix-
ing (+BM).
while it drastically reduced the number of sampled
positions. The median skip rate was 90.87%, with a
standard deviation of 8.5.
6.5 Discussion
Figure 4 shows some segmentations corrected by
the back-off mixing bigram model with hybrid type-
based sampling. ????? (ichihino) is a rare place
name but can be identified by the model because
it is frequently used in the article. ???????
(konamiruku in hiragana) seems a pun on ????
?? (kona miruku, ?powdered milk?) and ?????
(konami in katakana, a company). We consider it
as a single word because we cannot reconstruct the
etymology solely based on the main text. Note the
different scripts. In Japanese, people often change
the script to derive a proper noun from a common
noun, which a na??ve analyzer fails to recognize. It is
613
Table 2: Effect of matching skip (F-score (precision/recall)).
model full matching skip
bigram + HTB 83.23 (85.25/81.30)** 82.86 (84.27/81.49)
bigram + HTB + BM 86.32 (85.67/86.97)** 83.87 (82.60/85.17)**
bigram + HTB + RAND 79.68 (80.13/79.23) 78.81 (78.64/75.07)
bigram + HTB + BM + RAND 84.03 (83.10/84.99) 81.08 (80.22/81.96)
baseline (JUMAN) 80.09 (75.80/84.89)
** Statistically significant improvement with p < 0.01.
? +? +? +? +? +????
hiwaki
+?
chou
+???
ichihino
(Ichihino, Hiwaki Town, an address)
? +?? +???????
risona
+???
kaRdo
(Risona Card, a company)
?? +?? +?????????
chiritotechiN (name of a play)
?? +?? +???????
konamiruku
(a shop affiliated with Konami Corporation)
?? +???????
haiziI (stage name of a comedian)
?? +?????????
chiNsukou (a traditional sweet)
??????????????????
koNtora
+???
aruto
+??????
kurarineQto (Contra-alto clarinet)
Figure 4: Examples of improved segmentations.
very important to identify hiragana words correctly.
As hiragana is mainly used to write function words
and other basic words, segmentation errors concern-
ing hiragana often bring disastrous effects on ap-
plications of morphological analysis. For example,
the analyzer over-segments ???????? (chiri-
totechiN) into three shorter words among which the
second word ???? (tote) is a particle, and this se-
quence of words is transformed into a terrible parse
tree.
Most improvements come from correction of
over-segmentation because the initial segmenta-
tion by the analyzer shows a tendency of over-
segmentation. An example of corrected under-
segmentation is ?contra-alto clarinet.? The pres-
ence of ?clarinet,? ?alto? and ?contrabass? and oth-
ers in the main text allowed the model to iden-
tify the constituents. On the other hand, the seg-
mentation failed when our assumption about con-
stituents does not hold. For example, the person
name ?????? (kikuchi shuNkichi) is two words
but was erroneously combined into a single word by
the model because unfortunately he was always re-
ferred to by the full name.
7 Conclusions
In this paper, we proposed a new task of Japanese
noun phrase segmentation. We adopted non-
parametric Bayesian language models and proposed
hybrid type-based sampling that can efficiently cor-
rect segmentation given by the morphological an-
alyzer. Although supervised segmentation is very
competitive, we showed that it can be supplemented
with our unsupervised approach.
We applied the proposed method to encyclopedic
text to segment noun phrases in it. The proposed
method can be applied to other tasks. For example,
in unknown word acquisition (Murawaki and Kuro-
hashi, 2008), noun phrases are often acquired from
text as single words. We can now segment them into
words in a more sophisticated way.
In the future we will assign a POS tag to each
word in order to use segmented noun phrases in mor-
phological analysis. We assume that the meaning
of constituents in a noun phrase rarely depends on
outer context. So it would be helpful to augment
them with rich semantic information in advance in-
stead of disambiguating their meaning every time we
analyze given text.
Acknowledgments
This work was partly supported by JST CREST.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
614
tagger. In Proc. of COLING 2000, pages 21?27.
Masayuki Asahara and Yuji Matsumoto. 2004.
Japanese unknown word identification by character-
based chunking. In Proc. COLING 2004, pages 459?
465.
Phil Blunsom, Trevor Cohn, Sharon Goldwater, andMark
Johnson. 2009. A note on the implementation of hier-
archical Dirichlet processes. In Proc. of ACL-IJCNLP
2009: Short Papers, pages 337?340.
Michael D. Escobar and Mike West. 1995. Bayesian
density estimation and inference using mixtures.
Journal of the American Statistical Association,
90(430):577?588.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. Annals of Statistics,
1(2):209?230.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens by
estimating power-law generators. In NIPS 18, pages
459?466.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Mark Johnson and Katherine Demuth. 2010. Unsu-
pervised phonemic Chinese word segmentation using
adaptor grammars. In Proc. of COLING 2010, pages
528?536.
Mark Johnson. 2008. Using adaptor grammars to iden-
tify synergies in the unsupervised acquisition of lin-
guistic structure. In Proc. of ACL 2008, pages 398?
406.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of ACL
2008, pages 407?415, June.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proc. of EMNLP 2004,
pages 230?237.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Proc. of The In-
ternational Workshop on Sharable Natural Language
Resources, pages 22?38.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised POS tagging.
In Proc. of EMNLP 2010, pages 853?861.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for Chinese word segmentation.
Computational Linguistics, 35(4):505?512.
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-based MCMC. In Proc. of NAACL 2010, pages
573?581.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmen-
tation. In Proc. of the 4th SIGHAN Workshop, pages
161?164.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proc. of
ACL-IJCNLP 2009, pages 100?108.
Yugo Murawaki and Sadao Kurohashi. 2008. Online
acquisition of Japanese unknown morphemes using
morphological constraints. In Proc. of EMNLP 2008,
pages 429?437.
Masaaki Nagata. 1996. Automatic extraction of new
words from Japanese texts using generalized forward-
backward search. In Proc. of EMNLP 1996, pages 48?
59.
Tetsuji Nakagawa and Yuji Matsumoto. 2006. Guessing
parts-of-speech of unknown words using global infor-
mation. In Proc. of COLING-ACL 2006, pages 705?
712.
Graham Neubig and Shinsuke Mori. 2010. Word-based
partial annotation for efficient corpus construction. In
Proc. of LREC 2010.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proc. of COLING 2010, pages 815?
823.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proc. of COLING
?04, pages 562?568.
Yee Whye Teh. 2006. A Bayesian interpretation of in-
terpolated Kneser-Ney. Technical Report TRA2/06,
School of Computing, National University of Singa-
pore.
Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki
Oda, and Yuji Matsumoto. 2008. Training conditional
random fields using incomplete annotations. In Proc.
of COLING 2008, pages 897?904.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
2001. The unknown word problem: a morphological
analysis of Japanese using maximum entropy aided by
a dictionary. In Proc. of EMNLP 2001, pages 91?99.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann
Ney. 2008. Bayesian semi-supervised Chinese word
segmentation for statistical machine translation. In
Proc. of COLING 2008, pages 1017?1024.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29?48.
Toshio Yokoi. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42?44.
615
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 924?934,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Japanese Zero Reference Resolution
Considering Exophora and Author/Reader Mentions
Masatsugu Hangyo Daisuke Kawahara Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan
{hangyo,dk,kuro}@nlp.ist.i.kyoto-u.ac.jp
Abstract
In Japanese, zero references often occur and
many of them are categorized into zero ex-
ophora, in which a referent is not mentioned in
the document. However, previous studies have
focused on only zero endophora, in which
a referent explicitly appears. We present a
zero reference resolution model considering
zero exophora and author/reader of a docu-
ment. To deal with zero exophora, our model
adds pseudo entities corresponding to zero
exophora to candidate referents of zero pro-
nouns. In addition, we automatically detect
mentions that refer to the author and reader of
a document by using lexico-syntactic patterns.
We represent their particular behavior in a dis-
course as a feature vector of a machine learn-
ing model. The experimental results demon-
strate the effectiveness of our model for not
only zero exophora but also zero endophora.
1 Introduction
Zero reference resolution is the task of detecting and
identifying omitted arguments of a predicate. Since
the arguments are often omitted in Japanese, zero
reference resolution is essential in a wide range of
Japanese natural language processing (NLP) appli-
cations such as information retrieval and machine
translation.
(1) ????
pasta-NOM
???
like
??
everyday
(??)
(?-NOM)
(??)
(?-ACC)
?????
eat
(Liking pasta, (?) eats (?) every day)
For example, in example (1) , the accusative argu-
ment of the predicate ?????? (eat) is omitted .1
The omitted argument is called a zero pronoun. In
this example, the zero pronoun refers to ?????
(pasta).
Zero reference resolution is divided into two sub-
tasks: zero pronoun detection and referent identifi-
cation. Zero pronoun detection is the task that de-
tects omitted zero pronouns from a document. In
example (1), this task detects that there are the zero
pronouns in the accusative and nominative cases of
?????? (eat) and there is no zero pronoun in
the dative case of ??????. Referent identifica-
tion is the task that identifies the referent of a zero
pronoun. In example (1), this task identifies that the
referent of the zero pronoun in the accusative case of
?????? is ????? (pasta). These two subtasks
are often resolved simultaneously and our proposed
model is a unified model.
Many previous studies (Imamura et al, 2009;
Sasano et al, 2008; Sasano and Kurohashi, 2011)
have treated only zero endophora, which is a phe-
nomenon that a referent is mentioned in a document,
such as ????? (pasta) in example (1). However,
zero exophora, which is a phenomenon that a ref-
erent does not appear in a document, often occurs in
Japanese when a referent is an author or reader of a
document or an indefinite pronoun. For example, in
example (1), the referent of the zero pronoun of the
nominative case of ?????? (eat) is the author of
1In this paper, we use the following abbreviations: NOM
(nominative), ABL(ablative), ACC (accusative), DAT (dative),
ALL (allative), GEN (genitive), CMI (comitative), CNJ (con-
junction), INS(instrumental) and TOP (topic marker).
924
Zero pronoun Referent Examplein the document
Zero endophora Exist Exist ??????????? (????)??????(I like cafes and go (to a cafe) everyday.)
Zero exophora Exist Not exist
??????? ([reader]?)
????????????
(I would like to explain the advantage (to [reader]).)
No zero reference Not exist Not exist
????????????? (??)?????
(You can have a relaxing time.)
*There is no dative case.
Table 1: Examples of zero endophora, zero exophora and no zero reference.
the document, but the author is not mentioned ex-
plicitly.
(2) ???
recently
?????
PC-INS
???
movie-ACC
([unspecified:person]?)
([unspecified:person]-NOM)
????
can see
(Recently, (people) can see movies by a PC.)
Similarly, in example (2), the referent of the zero
pronoun of the nominative case of ????? (can
see) is an unspecified person.2
Most previous studies have neglected zero ex-
ophora, as though a zero pronoun does not exist in
a sentence. However, such a rough approximation
has impeded the zero reference resolution research.
In Table 1, in ?zero exophora,? the dative case of
the predicate has the zero pronoun, but in ?no zero
reference,? the dative case of the predicate does not
have a zero pronoun. Treating them with no dis-
tinction causes a decrease in accuracy of machine
learning-based zero pronoun detection due to a gap
between the valency of a predicate and observed ar-
guments of the predicate. In this work, to deal with
zero exophora explicitly, we provide pseudo entities
such as [author], [reader] and [unspecified:person]
as candidate referents of zero pronouns.
In the referent identification, selectional prefer-
ences of a predicate (Sasano et al, 2008; Sasano and
Kurohashi, 2011) and contextual information (Iida
et al, 2006) have been widely used. The author and
reader (A/R) of a document have not been used for
contextual clues because the A/R rarely appear in
the discourse in corpora based on newspaper arti-
cles, which are main targets of the previous studies.
However, in other domain documents such as blog
2In the following examples, omitted arguments are put in
parentheses and exophoric referents are put in square brackets.
articles and shopping sites, the A/R often appear in
the discourse. The A/R tend to be omitted and there
are many clues for the referent identification about
the A/R such as honorific expressions and modal-
ity expressions. Therefore, it is important to deal
with the A/R of a document explicitly for the refer-
ent identification.
The A/R appear as not only the exophora but also
the endophora.
(3) ? author ?
I-TOP
???
Kyoto-DAT
(??)
(I-NOM)
????
will go
???????
have thought
(I have thought (I) will go to Kyoto.)
??? reader ?
you all-TOP
???
where-DAT
?????
want to go
(????)
(you all-NOM)
(??)
(I-DAT)
????????
let me know
(Please let (me) know where do you want to go.)
In example (3), ??? (I), which is explicitly men-
tioned in the document, is the author of the docu-
ment and ????? (you all) is the reader. In this pa-
per, we call these expressions, which refer to the au-
thor and reader, author mention and reader men-
tion. We treat them explicitly to improve the per-
formance of zero reference resolution. Since the
A/R are mentioned as various expressions besides
personal pronouns in Japanese, it is difficult to de-
tect the A/R mentions based merely on lexical in-
formation. In this work, we automatically detect
the A/R mentions by using a learning-to-rank al-
gorithm(Herbrich et al, 1998; Joachims, 2002) that
uses lexico-syntactic patterns as features.
Once the A/R mentions can be detected, their in-
formation is useful for the referent identification.
925
The A/R mentions have both a property of the dis-
course element mentioned in a document and a prop-
erty of the zero exophoric A/R. In the first sentence
of example (3), it can be estimated that the referent
of the zero pronoun of the nominative case of ???
?? (will go) from a contextual clue that ??? (I) is
the topic of this sentence and a syntactic clues that ?
?? (I) depends on ???????? (have thought)
over the predicate ????? (will go).3 Such con-
textual clues can be available only for the discourse
entities that are mentioned explicitly. On the other
hand, in the second sentence, since ???????
?? (let me know) is a request form, it can be as-
sumed that the referent of the zero pronoun of the
nominative case is ??? (I), which is the author,
and the one of the dative case is ???? (you all),
which is the reader. The clues such as request forms,
honorific expressions and modality expressions are
available for the author and reader. In this work, to
represent such aspect of the A/R mentions, both the
endophora and exophora features are given to them.
In this paper, we propose a zero reference reso-
lution model considering the zero exophora and the
author/reader mentions, which resolves the zero ref-
erence as a part of a predicate-argument structure
analysis.
2 Related Work
Several approaches to Japanese zero reference reso-
lution have been proposed.
Iida et al (2006) proposed a zero reference resolu-
tion model that uses the syntactic relations between
a zero pronoun and a candidate referent as a feature.
They deal with zero exophora by judging that a zero
pronoun does not have anaphoricity. However, the
information of zero pronoun existences is given and
thus they did not address zero pronoun detection.
Zero reference resolution has been tackled as a
part of predicate-argument structure analysis. Ima-
mura et al (2009) proposed a predicate-argument
structure analysis model based on a log-linear model
that simultaneously conducts zero endophora resolu-
tion. They assumed a particular candidate referent,
NULL, and when the analyzer selected this refer-
ent, the analyzer outputs ?zero exophora or no zero
3Since ??? (I) depends on ???????? (have thought),
the relation between ??? (I) and ????? (will go) is the zero
reference.
pronoun,? in which they are treated without distinc-
tion. Sasano et al (2008) proposed a probabilis-
tic predicate-argument structure analysis model in-
cluding zero endophora resolution by using wide-
coverage case frames constructed from a web cor-
pus. Sasano and Kurohashi (2011) extended the
Sasano et al (2008)?s model by focusing on zero en-
dophora. Their model is based on a log-linear model
that uses case frame information and the location of
a candidate referent as features. In their work, zero
exophora is not treated and they assumed that a zero
pronoun is absent when there is no referent in a doc-
ument.
For languages other than Japanese, zero pronoun
resolution methods have been proposed for Chinese,
Portuguese, Spanish and other languages. In Chi-
nese, Kong and Zhou (2010) proposed tree-kernel
based models for three subtasks: zero pronoun de-
tection, anaphoricity decision and referent selection.
In Portuguese and Spanish, only a subject word is
omitted and zero pronoun resolution has been tack-
led as a part of coreference resolution. Poesio et
al. (2010) and Rello et al (2012) detected omitted
subjects and made a decision whether the omitted
subject has anaphoricity or not as preprocessing of
coreference resolution systems.
3 Baseline Model
In this section, we describe a baseline zero refer-
ence resolution system. In our model, the zero refer-
ence resolution is conducted as a part of predicate-
argument structure (PAS) analysis. The PAS con-
sists of a case frame and an alignment between case
slots and referents. The case frames are constructed
for each meaning of a predicate. Each case frame
describes surface cases that each predicate has (case
slot) and words that can fill each case slot (exam-
ple). In this study, the case frames are constructed
from 6.9 billion Web sentences by using Kawahara
and Kurohashi (2006a)?s method.
The baseline model does not treat zero exophora
as the previous studies. The baseline model analyzes
a document in the following procedure in the same
way as the previous study (Sasano and Kurohashi,
2011).4
4For learning, the previous study used a log-linear model,
but we use a learning-to-rank model. In our preliminary exper-
926
 
????
Kyoto station-DAT
??
stand
?????
curry shop-NOM
????
like
????
the shop
??
often
?????
go
(I like a curry shop in Kyoto station and often go to the shop.)
???
Today-TOP
????
you all-DAT
(?????)
(curry shop-ACC)
??????
will introduce
(Today, I will introduce (the shop) to you.)
Discourse entities 
{??? (Kyoto station)}, {???? (curry shop),??? (the shop)}, {?? (today)},
{??? (you all)}
 
Candidate predicate-argument structures of ??????? in the baseline model 
[1-1] case frame:[???? (1)], { NOM:Null, ACC:Null, DAT:???, TIME:?? }
[1-2] case frame:[???? (1)], { NOM:Null, ACC:????, DAT:???, TIME:?? }
[1-3] case frame:[???? (1)], { NOM:???, ACC:????, DAT:???, TIME:?? }
.
.
.
[2-1] case frame:[???? (2)], { NOM:Null, ACC:Null, DAT:???, TIME:?? }
[2-2] case frame:[???? (2)], { NOM:Null, ACC:????, DAT:???, TIME:?? }
.
.
.
 
 
Figure 1: Examples of discourse entities and predicate-argument structures
1. Parse the input document and recognize named
entities.
2. Resolve coreferential relations and set dis-
course entities.
3. Analyze the predicate-argument structure for
each predicate using the following steps:
(a) Generate candidate predicate-argument
structures.
(b) Calculate the score of each predicate-
argument structure and select the one with
the highest score.
We illustrate the details of the above procedure.
First, we describe how to set the discourse entities
in step 2. In our model, we treat referents of a zero
pronoun using a unit called discourse entity, which
is what mentions in a coreference chain are bound
into. In Figure 1, we treat ?????? (curry shop)
and ????? (the shop), which are in a coreference
chain, as one discourse entity. In Figure 1, the dis-
course entity {????, ??? } is selected for
the referent of the accusative case of the predicate ?
?????? (will introduce).
Next, we illustrate the PAS analysis in step 3. In
step 3a, possible combinations of the case frame
(cf ) and the alignment (a) between case slots and
iment of the baseline model, there is little difference between
the results of these methods.
discourse entities are listed. First, one case frame is
selected from case frames for the predicate. Next,
overt arguments, which have dependency relations
with the predicate, are aligned to a case slot of the
case frame. Finally, each of zero pronouns of re-
maining case slots is assigned to a discourse entity
or is not assigned to any discourse entities. The case
slot whose zero pronoun is not assigned to any dis-
course entities corresponds to the case that does not
have a zero pronoun. In Figure 1, we show the ex-
amples of candidate PASs. In these examples, [??
?? (1)] and [???? (2)] are case frames corre-
sponding to each meaning of ??????. Referents
of each case slot are actually selected from discourse
entities but are explained as a representative word
for illustration. ?Null? indicates that a case slot is
not assigned to any discourse entities. Since align-
ments between case slots and discourse entities of
the PAS [1-2] and [2-2] are the same but their case
frames are different, we deal with them as discrete
PASs. In this case, however, the results of zero ref-
erence resolution are the same.
We represent each PAS as a feature vector, which
is described in section 3.1, and calculate a score of
each PAS with the learned weights. Finally, the sys-
tem outputs the PAS with the highest score.
927
Type Value Description
Log Probabilities that {words, categories and named entity types} of e is assigned to c of cf
Log Generative probabilities of {words, categories and named entity types} of e
Log PMIs between {words, categories and named entity types} of e and c of cf
Case Log Max of PMIs between {words, categories and named entity types} of e and c of cf
frame Log Probability that c of cf is assigned to any words
Log Ratio of examples of c to ones of cf
Binary c of cf is {adjacent and obligate} case
Predicate
Binary Modality types of p
Binary Honorific expressions of p
Binary Tenses of p
Binary p is potential form
Binary Modifier of p (predicate, noun and end of sentence)
Binary p is {dynamic and stative} verb
Context
Binary Named entity types of e
Integer Number of mentions about e in t
Integer Number of mentions about e {before and after} p in t
Binary e is mentioned with post position ??? in a target sentence
Binary Sentence distances between e and p
Binary Location categories of e (Sasano and Kurohashi, 2011)
Binary e is mentioned at head of a target sentence
Binary e is mentioned with post position {??? and ??? } at head of a target sentence
Binary e is mentioned at head of the first sentence
Binary e is mentioned with post position ??? at head of the first sentence
Binary e is mentioned at end of the first sentence
Binary e is mentioned with copula at end of the first sentence
Binary e is mentioned with noun phrase stop at end of the first sentence
Binary Salience score of e is larger than 1 (Sasano and Kurohashi, 2011)
other Binary c is assigned
Table 2: The features of ?assigned(cf, c? e, p, t)
3.1 Feature Representation of
Predicate-Argument Structure
When text t and target predicate p are given and PAS
(cf, a) is chosen, we represent a feature vector of the
PAS as ?(cf, a, p, t). ?(cf, a, p, t) consists of a fea-
ture vector ?overt-PAS(cf, a, p, t) and feature vec-
tors ?(cf, c/e, p, t). Where ?overt-PAS(cf, a, p, t)
corresponds to alignment between case slots and
overt (not omitted) arguments and ?(cf, c/e, p, t)
represents that a case slot c is assigned to a discourse
entity e. If a case slot is assigned to an overt entity,
?(cf, c/e, p, t) is set to a zero vector.
Each feature vector ?(cf, c/e, p, t) consists
of ?A(cf, c/e, p, t) and ?NA(cf, c/Null, p, t).
?A(cf, c/e, p, t) becomes active when the case
slot c is assigned to the discourse entity e and
?NA(cf, c/Null, p, t) becomes active when the
case slot c is not assigned to any discourse entities.
For example, the PAS [1-2] in Figure 1 is repre-
sented as:
(?overt-PAS(???? (1), {NOM:Null,ACC:Null,
NOM:???,TIME:?? }),0?A ,
?NA(???? (1),NOM/Null),
?A(???? (1),ACC/????),
0?NA ,0?A ,0?NA). 5
In our feature representation, the second and third
terms correspond to the nominative case, the forth
and fifth ones correspond to the accusative and the
sixth and seventh ones correspond to the dative
case.
We present the details of ?overt-PAS(cf, a, p, t),
?A(cf, c/e, p, t) and ?NA(cf, c/Null, p, t). We use
a score of the probabilistic PAS analysis (Kawahara
and Kurohashi, 2006b) to ?overt-PAS(cf, a, p, t).
We list the features of ?A(cf, c/e, p, t) in Table 2
and the features of ?NA(cf, c/Null, p, t) in Table
5In the following example, p and t are sometimes omitted
and 0?is 0 vector that has the same dimension as ?.
928
Type Value Description
Case frame
Log Probability that c of cf is
not assigned
Log Ratio of number of examples
of c to ones of cf
Binary c of cf is{adjacent and obligate} case
Table 3: The features of ?NA(cf, c/Null, p, t)
3.
3.2 Weight Learning
In the previous section, we defined the feature vec-
tor ?(cf, a, p, t), which represents a PAS. In this
section, we illustrate the learning method of the
weight vector corresponding to the feature vector.
The weight vector is learned by using a learning-to-
rank algorithm.
In a corpus, gold-standard alignments a? are man-
ually annotated but case frames are not annotated.
Since the case frames are constructed for each mean-
ing, some of them are unsuitable for a usage of a
prdicate in a context. If training data includes PASs
(cf, a?) whose cf is such case a frame as correct
instances, these are harmful for training. Hence,
we treat a case frame cf? which is selected by a
heuristic method as a correct case frame and remove
(cf, a?) which has other cf .
In particular, we make ranking data for the learn-
ing for each target predicate p in the following steps.
1. List possible PASs (cf, a) for predicate p.
2. Calculate a probabilistic zero reference resolu-
tion score for each (cf, a?) and define the one
with highest score as (cf?, a?).
3. Remove (cf, a?) except (cf?, a?) from the
learning instance.
4. Make ranking data that (cf?, a?) has a higher
rank than other (cf, a).
In the above steps, we make ranking data for each
predicate and use the ranking data collected from all
target predicates as training data.
4 Corpus
In this work, we use Diverse Document Leads Cor-
pus (DDLC) (Hangyo et al, 2012) for experiments.
In DDLC, documents collected from the web are
annotated with morpheme, syntax, named entity,
coreference, PAS and A/R mention. Morpheme,
syntax, named entity, coreference and PAS are an-
notated on the basis of Kyoto University Text Cor-
pus (Kawahara et al, 2002). The PAS annotation in-
cludes zero reference information and the exophora
referents are defined as five elements, [author],
[reader], [US(unspecified):person], [US:matter] and
[US:situation]. The A/R mentions are annotated
to head phrases of compound nouns when the A/R
mentions consist of compound nouns. If the A/R
is mentioned by multiple expressions, only one of
them is annotated with the A/R mention tag and all
of these mentions are linked by a coreference chain.
In other words, the A/R mentions are annotated to
discourse entities. In the web site of an organiza-
tion such as a company, the site administrator often
writes the document on behalf of the organization.
In such a case, the organization is annotated as the
author.
5 Author/Reader Mention Detection
A/R mentions, which refer to A/R of a document,
have different properties from other discourse enti-
ties. The A/R are mentioned as very various expres-
sions such as personal pronouns, proper expressions
and role expressions.
(4) ??????
Hello
??????
project team-GEN
?? author ???
am Umetsuji
(Hello, I?m Umetsuji on the project team.)
(5) ???
problem-NOM
???
exist
??? author ??
to moderator
?????????
let me know
(Please let me know if there are any problems.)
In example (4), the author is mentioned as ????
(Umetsuji), which is the name of the author, and in
example (5), the author is mentioned as ?????
(moderator), which expresses the status of the au-
thor. Likewise, the reader is sometimes mentioned
as ????? (customer) and others. However, since
such expressions often refer to someone other than
the A/R, whether an expression indicates the A/R of
a document depends on the context of the document.
In English and other languages, the A/R mentions
can be detected from coreference information be-
cause it can be assumed that the expression that has
929
a coreference relation with first or second personal
pronoun is the A/R mention. However, since the
A/R tend to be omitted and personal pronouns are
rarely used in Japanese, it is difficult to detect the
A/R mentions from coreference information. Be-
cause of these reasons, it is difficult to detect which
discourse entity is the A/R mention from lexical in-
formation of the entities. In this study, the A/R men-
tions are detected from lexico-syntactic (LS) pat-
terns in the document. We use a learning-to-rank
algorithm to detect A/R mentions by using the LS
patterns as features.
5.1 Author/Reader Detection Model
We use a learning-to-rank method for detecting A/R
mentions. This method learns the ranking that en-
tities of the A/R mentions have a higher rank than
other discourse entities. Here, it is an important
point that there are no A/R mentions in some doc-
uments. The documents in which the A/R mentions
do not appear are classified into two types. The first
type is a document that the A/R do not appear in
the discourse of the document such as newspaper ar-
ticles and novels. The second type is a document
that the A/R appear in the discourse but all of their
mentions are omitted. For example, in Figure 1, the
author appears in the discourse (e.g. the nominative
argument of ?like?) but is not mentioned explicitly.
We introduce two pseudo entities corresponding to
these types. The first pseudo entity ?no A/R men-
tion (discourse)? represents the document that the
A/R do not appear in the discourse. It is considered
that the document that the A/R do not appear have
characteristics of writing style such that honorific
expressions and request expressions are rarely used.
This pseudo entity is represented as a document vec-
tor that consists of LS pattern features of the whole
document, which reflect a writing style of a doc-
ument. The second pseudo entity ?no A/R men-
tion (omitted)? represents the document in which all
mentions of the A/R are omitted and this pseudo en-
tity is represented as 0 vector. Since a decision score
of this pseudo entity is allways 0, discourse entities
whose score is lower than the score of this pseudo
entity can be treated as a negative example in a bi-
nary classification.
When there are A/R mentions in a document, we
make ranking data where the discourse entity of
the A/R mention has a higher rank than other dis-
course entities and ?no A/R mention? pseudo enti-
ties. When the A/R do not appear in the discourse,
we make ranking data where ?no A/R mention (dis-
course)? has a higher rank than all discourse enti-
ties and ?no A/R mention (omitted)?. When the A/R
appear in the discourse but all mentions are omit-
ted, we make ranking data where ?no A/R mention
(omitted)? has a higher rank than all discourse en-
tities and ?no A/R mention (discourse)?. We judge
that the A/R appear in the discourse if the A/R ap-
pear as a referent of zero reference in gold-standard
PASs and this judgment is used only in the training
phase. After making the ranking data for each doc-
ument, all of the ranking data are merged and the
merged data is fed into the learning-to-rank model.
For the A/R mention detection, we calculate the
score of all discourse entities and the pseudo entities
and select the discourse entity with the highest score
to the A/R mention. If any ?no A/R mention? have
the highest score, we decide that there are no A/R
mentions in the document.
5.2 Lexico-Syntactic Patterns
For each discourse entity, phrases of the discourse
entity, its parent and their dependency relations are
used to make LS patterns that represent the discourse
entity. When a discourse entity is mentioned multi-
ple times, the phrases of all mentions are used to
make the LS patterns. LS patterns of phrases are
made by generalizing these phrases on various lev-
els (types). LS patterns of dependencies are made
from combining the LS patterns of phrases.
Table 4 lists generalization types. On the word
type, we make a phrase LS pattern by generalizing
each content word and jointing them. For example, a
LS pattern of the phrase ????? generalized on the
<representative form> is ????. The word+ type
is the same as word except all content words are gen-
eralized on the <part of speech and conjugation>.
For example, a LS pattern of the dependency rela-
tion ????????? generalized on the <named
entity> is ?NE:PERSON+?? verb:past?. We also
use the LS patterns of generalized individual mor-
phemes. On the phrase type, each phrase is gener-
alized according to the information assigned to the
phrase and all content words are generalized on the
<part of speech and conjugation> if the information
930
Unit Type Example (original phrase)
word
<no generalization> ?? (??)
<original form> ??? (??)
<representative form> ?? (???)
<part of speech and conjugation> verb:past (???)
word+
<category> Category:PERSON+? (??)
<named entity> NE:PERSON+? (???)
<first person pronoun> FirstPersonPronoun+? (??)
<second person pronoun> SecondPersonPronoun+? (????)
phrase
<modality> modality:request (??????????)
<honorific expression> honorific:modest (??????)
<attached words> ???? (??????????)
Table 4: Generalization types of the LS patterns
is not assigned to the phrase.
For ?no A/R mention (discourse)? instance, the
above features of all mentions, including verbs and
adjectives, and their dependencies in the document
are gathered and used as the features representing
the instance.
6 Zero Reference Resolution Considering
Exophora and Author/Reader Mentions
In this section, we describe the zero reference reso-
lution system that considers the zero exophora and
the A/R mentions. The proposed model resolves
zero reference as a part of the PAS analysis based
on the baseline model.
The proposed model analyzes the PASs in the fol-
lowing steps:
1. Parse the input document and recognize named
entities.
2. Resolve coreferential relations and set dis-
course entities.
3. Detect the A/R mentions of the document.
4. Set pseudo entities from the estimated A/R
mentions.
5. Analyze the PAS for each predicate using the
same procedure as the baseline model.
The differences form baseline model are the estima-
tion of the A/R mentions in step 3 and the setting of
pseudo entities in step 4.
6.1 Pseudo Entities and Author/Reader
Mentions for Zero Exophora
In the baseline model, referents of zero pronouns
are selected form discourse entities. The proposed
model adds pseudo entities([author], [reader],
[US:person] (unspecified:person) and [US:others]
(unspecified:others)6) to deal with zero exophora.
When the A/R mentions appear in a document,
the A/R pseudo entities raise an issue. The zero en-
dophora are given priority to zero exophora. In other
words, the A/R mentions are selected to the referents
in preference to pseudo entities when there are A/R
mentions. Therefore, when the system estimates that
A/R mentions appear, the A/R pseudo entities are
not created.
In the PAS analysis, referents are selected from
discourse entities and the pseudo entities. A zero
reference is the zero exophora when a case slot is
assigned to pseudo entities. Candidate PASs of ??
????? in Figure 1 are shown in Figure 2.
6.2 Feature Representation of Predicate
Argument Structure
In the same way as the baseline model, the
proposed model represents a PAS as a fea-
ture vector that consists of the feature vector
?overt-PAS(cf, a, p, t) and the feature vectors
?(cf, c/e, p, t). The difference from the baseline
model is a composition of ?A(cf, c/e, p, t). In the
proposed model, each ?A(cf, c/e) is composed of
vectors, ?discourse(cf, c/e), ?[author ](cf, c/e),
?[reader ](cf, c/e), ?[US :person](cf, c/e),
?[US :others](cf, c/e) and ?max(cf, c/e). Their
contents and dimensions are the same and similar to
?A(cf, c/e) of the baseline model the except for the
6We merge [US:matter] and [US:situation] because of the
small amount of [US:situation] in the corpus.
931
 
[1-1] case frame:[???? (1)], { NOM:[author], ACC:Null, DAT:??? reader, TIME:?? }
[1-2] case frame:[???? (1)], { NOM:[US:person], ACC:Null, DAT:??? reader, TIME:?? }
[1-3] case frame:[???? (1)], { NOM:[author], ACC:????, DAT:??? reader, TIME:?? }
[1-4] case frame:[???? (1)], { NOM:???, ACC:????, DAT:??? reader, TIME:?? }
[1-5] case frame:[???? (1)], { NOM:[author], ACC:[US:others], DAT:??? reader, TIME:?? }
.
.
.
[2-1] case frame:[???? (2)], { NOM:[author], ACC:Null, DAT:??? reader, TIME:?? }
[2-2] case frame:[???? (2)], { NOM:[US:person], ACC:Null, DAT:??? reader, TIME:?? }
.
.
.
 
Figure 2: Candidate predicate-argument structures of ??????? in the proposed model
Expressions Categories
author ? (I),?? (we),? (I),? (I), PERSON, ORGANIZATION?? (our company),?? (our company),?? (our shop)
reader ??? (you),? (customer),? (you),?? (you all), PERSON??? (you all),? (person),?? (people)
US:person ? (person),?? (people) PERSON
US:others ?? (thing)??? (situation) all categories exceptPERSON and ORGANIZATION
Table 5: Expressions and categories for pseudo entities
addition of a few features described in section 6.3.
?discourse corresponds to the discourse entities,
which are mentioned explicitly and becomes active
when e is a discourse entity including the A/R men-
tions. ?discourse is the same as ?A of the base-
line model and the difference is explained in section
6.3. ?[author ] and ?[reader ] become active when e is
[author]/[reader] or the discourse entity correspond-
ing to the A/R mention. In particular, when e is
the discourse entity corresponding to the A/R men-
tion, both ?discourse and ?[author ]/?[reader ] become
active. This representation gives the A/R mentions
the properties of the discourse entity and the A/R.
?[US :person] and ?[US :others] become active when e
is [US:person] and [US:others].
Because ?[author ], ?[reader ], ?[US :person] and
?[US :others] correspond to the pseudo entities, which
are not mentioned explicitly, we cannot use word in-
formation such as expressions and categories. We
assume that the pseudo entities have expressions and
categories shown in Table 5 and use these to cal-
culate case frame features. Finally, ?max consists
of the highest value of correspondent feature of the
above feature vectors.
6.3 Author/Reader Mention Score
We add A/R mention score features to the feature
vector ?A(cf, c/e, p, t) described in Table 2. The
A/R mention scores are the discriminant function
scores of the A/R mention detection. When e is the
A/R mention, we set the A/R mention score to the
feature.
7 Experiments
7.1 Experimental Settings
We used 1,000 documents from DDLC and per-
formed 5-fold cross-validation. 1,440 zero en-
dophora and 1,935 zero exophora are annotated in
these documents. 258 documens are annotated with
author mentions and 105 documens are annotated
with reader mentions. We used gold-standard (man-
ually annotated) morphemes, named entities, depen-
dency structures and coreference relations to focus
on the A/R detection and the zero reference resolu-
tion. We used SV M rank7 for the learning-to-rank
method of the A/R detection and the PAS analysis.
The categories of words are given by the morpho-
logical analyzer JUMAN8. Named entities and pred-
icate features (e.g., honorific expressions, modality)
7http://www.cs.cornell.edu/people/tj/svm light/svm rank.html
8http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
932
System output
Exist NoneCorrect Wrong
Gold Exist 140 6 112
-standard None - 38 704
Table 6: Result of the author mention detection
System output
Exist NoneCorrect Wrong
Gold Exist 56 2 47
-standard None - 23 872
Table 7: Result of the reader mention detection
are given by the syntactic parser KNP.9
7.2 Results of Author/Reader Mention
Detection
We show the results of the author and reader men-
tion detection in Table 6 and Table 7. In these tables,
?exist? indicates numbers of documents in which the
A/R mentions are manually annotated or our system
estimated that some discourse entities are A/R men-
tions. From these results, the A/R mentions includ-
ing ?none? can be predicted to accuracies of approx-
imately 80%. On the other hand, the recalls are not
particularly high: the recall of author is 140/258 and
the recall of reader is 56/105. This is because the
documents in which the A/R do not appear are more
than the ones in which the A/R appear and the sys-
tem prefers to output ?no author/reader mention? as
the result of training.
7.3 Results of Zero Reference Resolution
We show the results of zero reference resolution
in Table 8 and Table 9. The difference between
the baseline and the proposed model is statistically
significant (p < 0.05) from the McNemar?s test.
In Table 8, we evaluate only the zero endophora
for comparison to the baseline model, which deals
with only the zero endophora. ?Proposed model
(estimate)? shows the result of the proposed model
which estimated the A/R mentions and ?Proposed
model (gold-standard)? shows the result of the pro-
posed model which is given the A/R mentions of
gold-standard from the corpus.
From Table 8, considering the zero exophora and
9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
Recall Precision F1
Baseline 0.269 0.377 0.314
Proposed model 0.282 0.448 0.346(estimate)
Proposed model 0.388 0.522 0.445(gold-standard)
Table 8: Results of zero endophora resolution
Recall Precision F1
Baseline 0.115 0.377 0.176
Proposed model 0.317 0.411 0.358(estimate)
Proposed model 0.377 0.485 0.424(gold-standard)
Table 9: Results of zero reference resolution
the A/R mentions improves accuracy of zero en-
dophora resolution as well as zero reference reso-
lution including zero exophora.
From Table 8 and Table 9, the proposed model
given the gold-standard A/R mentions achieves ex-
traordinarily high accuracies. This result indicates
that improvement of the A/R mention detection im-
proves the accuracy of zero reference resolution in
the proposed model.
8 Conclusion
This paper presented a zero reference resolution
model considering exophora and author/reader men-
tions. In the experiments, our proposed model
achieves higher accuracy than the baseline model.
As future work, we plan to improve the au-
thor/reader detection model to improve the zero ref-
erence resolution.
References
Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kuro-
hashi. 2012. Building a diverse document leads
corpus annotated with semantic relations. In Pro-
ceedings of the 26th Pacific Asia Conference on Lan-
guage, Information, and Computation, pages 535?
544, Bali,Indonesia, November. Faculty of Computer
Science, Universitas Indonesia.
Ralf Herbrich, Thore Graepel, Peter Bollmann-Sdorra,
and Klaus Obermayer. 1998. Learning preference re-
lations for information retrieval. In ICML-98 Work-
shop: text categorization and machine learning, pages
80?84.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
933
resolution. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 625?632, Sydney, Australia, July.
Association for Computational Linguistics.
Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009.
Discriminative approach to predicate-argument struc-
ture analysis with zero-anaphora resolution. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 85?88, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133?142.
ACM.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using high-
performance computing. In Proceedings of the 5th
International Conference on Language Resources and
Evaluation, pages 1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for japanese syn-
tactic and case structure analysis. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 176?183, New York
City, USA, June. Association for Computational Lin-
guistics.
Daisuke Kawahara, Sadao Kurohashi, and Koiti Hasida.
2002. Construction of a japanese relevance-tagged
corpus. In Proc. of The Third International Confer-
ence on Language Resources Evaluation, May.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for chinese zero anaphora
resolution. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 882?891, Cambridge, MA, October. Associa-
tion for Computational Linguistics.
Massimo Poesio, Olga Uryupina, and Yannick Versley.
2010. Creating a coreference resolution system for
italian. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Luz Rello, Ricardo Baeza-Yates, and Ruslan Mitkov.
2012. Elliphant: Improved automatic detection of zero
subjects and impersonal constructions in spanish. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 706?715. Association for Computational
Linguistics.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 758?766, Chiang
Mai, Thailand, November. Asian Federation of Natu-
ral Language Processing.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 769?776,
Manchester, UK, August. Coling 2008 Organizing
Committee.
934
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213?1223,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatic Knowledge Acquisition for Case Alternation
between the Passive and Active Voices in Japanese
Ryohei Sasano1 Daisuke Kawahara2 Sadao Kurohashi2 Manabu Okumura1
1 Precision and Intelligence Laboratory, Tokyo Institute of Technology
2 Graduate School of Informatics, Kyoto University
{sasano,oku}@pi.titech.ac.jp, {dk,kuro}@i.kyoto-u.ac.jp
Abstract
We present a method for automatically acquir-
ing knowledge for case alternation between
the passive and active voices in Japanese. By
leveraging several linguistic constraints on al-
ternation patterns and lexical case frames ob-
tained from a large Web corpus, our method
aligns a case frame in the passive voice to a
corresponding case frame in the active voice
and finds an alignment between their cases.
We then apply the acquired knowledge to a
case alternation task and prove its usefulness.
1 Introduction
Predicate-argument structure analysis is one of the
fundamental techniques for many natural language
applications such as recognition of textual entail-
ment, information retrieval, and machine transla-
tion. In Japanese, the relationship between a pred-
icate and its argument is usually represented by us-
ing case particles1 (Kawahara and Kurohashi, 2006;
Taira et al, 2008; Yoshikawa et al, 2011). However,
since case particles vary depending on the voices,
we have to take case alternation into account to rep-
resent predicate-argument structure. There are thus
two major types of representations: one uses surface
cases, and the other uses normalized-cases for the
base form of predicates. For example, while the Ky-
oto University Text Corpus (Kawahara et al, 2004),
one of the major Japanese corpora that contains an-
notations of predicate-argument structures, adopts
1Japanese is a head-final language. Word order does not
mark syntactic relations. Instead, postpositional case particles
function as case markers.
the former representation, the NAIST Text Corpora
(Iida et al, 2007), another major Japanese corpus,
adopts the latter representation.
Examples (1) and (2) describe the same event in
the passive and active voices, respectively. When
we use surface cases to represent the relationship be-
tween the predicate and its argument in Example (1),
the case of ?? (woman)? is ga2 and the case of ??
(man)? is ni.2 On the other hand, when we use the
normalized-cases for the base form, the case of ??
(woman)? is wo2 and the case of ?? (man)? is ga,
which are the same as the surface cases in the active
voice as in Example (2).
(1) ?? ?? ????????
woman-ga man-ni was pushed down
(A woman was pushed down by a man.)
(2) ?? ?? ???????
man-ga woman-wo pushed down
(A man pushed down a woman.)
Both representations have their own advantages.
Surface case analysis is easier than normalized-case
analysis, especially when we consider omitted ar-
guments, which are also called zero anaphors (Na-
gao and Hasida, 1998). In Japanese, zero anaphora
frequently occurs, and the omitted unnormalized-
case of a zero anaphor is often the same as the
surface case of its antecedent (Sasano and Kuro-
hashi, 2011). Therefore, surface case analysis suits
zero anaphora resolution. On the other hand, when
2Ga, wo, and ni are typical Japanese postpositional case par-
ticles. In most cases, they indicate nominative, accusative, and
dative, respectively.
1213
we focus on the resulting predicate argument struc-
tures, the normalized-case structure is more useful.
Specifically, since a normalized-case structure rep-
resents the same meaning in the same representa-
tion, normalized-case analysis is useful for recog-
nizing textual entailment and information retrieval.
Therefore, we need a system that first analyzes
surface cases and then alternates the surface cases
with normalized-cases. In particular, we focus on
the transformation of the passive voice into the ac-
tive voice in this paper. Passive-to-active voice
transformation in English can be performed system-
atically, which does not depend on lexical infor-
mation in most cases. However, in Japanese, the
method of transformation depends on lexical infor-
mation. For example, while the case particle ni in
Example (1) is alternated with ga in the active voice,
the case particle ni in Example (3) is not alternated in
the active voice as in Example (4) even though both
their predicates are ???????? (be pushed
down).?
(3) ?? ?? ????????
woman-ga sea-ni was pushed down
(A woman was pushed down into the sea.)
(4) ?? ?? ???????
woman-wo sea-ni pushed down
(? pushed down a woman into the sea.)
The ni case in Example (1) indicates agent. On
the other hand, the ni case in Example (3) indicates
direction. To determine the difference is important
for many NLP applications including machine trans-
lation. In fact, Google Translate (GT)3 translates
Examples (1) and (3) as ?Woman was pushed down
in the man? and ?Woman was pushed down in the
sea,? respectively, which may be because GT cannot
distinguish between the roles of ni in Examples (1)
and (3).
(5) ?? ?? ?????
prize-ga man-ni was awarded
(A prize was awarded to a man.)
In example (5), although the ni-case argument
?? (man)? is the same as in Example (1), the case
particle ni indicates recipient and is not alternated
in the active voice. These examples show that case
3http://translate.google.com, accessed 2013-2-20.
alternation between the passive and active voices in
Japanese depends on not only predicates but also ar-
guments, and we have to consider their combina-
tions. Since it is impractical to manually describe
the case alternation rules for all combinations of
predicates and arguments, we have to acquire such
knowledge automatically.
Thus, in this paper, we present a method for ac-
quiring the knowledge for case alternation between
the passive and active voices in Japanese. Our
method leverages several linguistic constraints on al-
ternation patterns and lexical case frames obtained
from a large Web corpus, which are constructed for
each meaning and voice of each predicate.
2 Related Work
Levin (1993) grouped English verbs into classes on
the basis of their shared meaning components and
syntactic behavior, defined in terms of diathesis al-
ternations. Hence, diathesis alternations have been
the topic of interest for a number of researchers
in the field of automatic verb classification, which
aims to induce possible verb frames from corpora
(e.g., McCarthy 2000; Lapata and Brew 2004; Joa-
nis et al 2008; Schulte im Walde et al 2008; Li and
Brew 2008; Sun and Korhonen 2009; Theijssen et al
2012). Baroni and Lenci (2010) used distributional
slot similarity to distinguish between verbs undergo-
ing the causative-inchoative alternations, and verbs
that do not alternate.
There is some work on passive-to-active voice
transformation in Japanese. Baldwin and Tanaka
(2000) empirically identified the range and fre-
quency of basic verb alternation, including active-
passive alternation, in Japanese. They automatically
extracted alternation types by using hand-crafted
case frames but did not evaluate the quality. Kondo
et al (2001) dealt with case alternation between the
passive and active voices as a subtask of paraphras-
ing a simple sentence. They manually introduced
case alternation rules on the basis of verb types and
case patterns and transformed passive sentences into
active sentences.
Murata et al (2006) developed a machine-
learning-based method for Japanese case alterna-
tion. They extracted 3,576 case particles in passive
sentences from the Kyoto University Text Corpus
1214
Case particle Grammatical function
ga nominative
wo accusative
ni dative
de locative, instrumental
kara ablative
no genitive
Table 1: Examples of Japanese postpositional case parti-
cles and their typical grammatical functions.
and tagged their cases in the active voice. Then,
they trained SVM classifiers using the tagged cor-
pus. Their features for training SVM were made
by using several lexical resources such as IPAL
(IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo
(NLRI, 1993), and the output of Kondo et al?s
method.
3 Lexicalized Case Frames
To acquire knowledge for case alternation, we ex-
ploit lexicalized case frames that are automatically
constructed from 6.9 billion Web sentences by using
Kawahara and Kurohashi (2002)?s method. In short,
their method first parses the input sentences, and
then constructs case frames by collecting reliable
modifier-head relations from the resulting parses.
These case frames are constructed for each predi-
cate like PropBank frames (Palmer et al, 2005), for
each meaning of the predicate like FrameNet frames
(Fillmore et al, 2003), and for each voice. However,
neither pseudo-semantic role labels such as Arg1 in
PropBank nor information about frames defined in
FrameNet are included in these case frames. Each
case frame describes surface cases that each predi-
cate has and instances that can fill a case slot, which
is fully lexicalized like the subcategorization lexicon
VALEX (Korhonen et al, 2006).
We list some Japanese postpositional case parti-
cles with their typical grammatical functions in Ta-
ble 1 and show examples of case frames in Table
2.4 Ideally, one case frame is constructed for each
meaning and voice of the target predicate. However,
since Kawahara and Kurohashi?s method is unsuper-
vised, several case frames are actually constructed
4Niyotte in Table 2 is a Japanese functional phrase that in-
dicates agent in this case. We treat niyotte as a case particle in
this paper for the sake of simplicity.
Case Frame: ????????-4 (be pushed down-4)?
{?? (woman):5,? (I):2,? (woman):2, ? ? ? }-ga
{? (sea):229,? (bottom):115,? (pond):51, ? ? ? }-ni
{??(stepmother):2,????(Pegasus):2, ? ? ? }-niyotte
? ? ?
Case Frame: ????????-5 (be pushed down-5)?
{?? (Kyoko):3,?? (manager):1, ? ? ? }-ga
{?? (someone):143,??? (somebody):85, ? ? ? }-ni
{?? (stair):20,? (ship):7,? (cliff):7, ? ? ? }-kara
? ? ?
Case Frame: ??????-2 (push down-2)?
{? (man):14,?? (lion):5,? (tiger):3, ? ? ? }-ga
{?(child):316,??(child):81,?(person):51, ? ? ? }-wo
{? (sea):580,? (ravine):576,? (river):352 ? ? ? }-ni
? ? ?
Case Frame: ??????-4 (push down-4)?
{?? (someone):14,???? (lion):5, ? ? ? }-ga
{? (person):257,? (I):214,? (child):137, ? ? ? }-wo
{? (cliff):53,?? (stair):28, ? ? ? }-kara
? ? ?
Table 2: Examples of case frames for ???????
? (be pushed down)? and ?????? (push down).?
Words in curly braces denote instances that can fill cor-
responding cases and the numbers following these words
denote their frequency in the corpus.
for each meaning and voice. For example, 59 and
eight case frames were respectively constructed for
the predicate in the passive voice ????????
(be pushed down)? and in the active voice ????
?? (push down)? from 6.9 billion Web sentences.
Table 2 shows the 4th and 5th case frames for ???
????? (be pushed down)? and the 2nd and 4th
case frames for ?????? (push down).?
Table 3 shows an example of case frames for
??? (hit),? which includes no-case. Here, the
Japanese postpositional case particle ?no? roughly
corresponds to ?of,? that is, ?X no Y? means ?Y of
X,? and thus no-case is not an argument of the target
predicate. While Kawahara and Kurohashi?s method
basically collects arguments of the target predicate,
the phrase of no-case that modifies the direct object
of the predicate is also collected as no-case. This
is because, as we will show in the next section, this
phrase can be represented as ga-case in the passive
voice.
1215
Case Frame: ???-2 (hit-2)?
{? (man):51,? (fist):30,?? (someone):23, ? ? ? }-ga
{?? (myself):360,? (I):223, ? ? ? }-no
{? (head):5424,? (face):3215, ? ? ? }-wo
{? (fist):316,?? (palm):157,?? (fist):126, ? ? ? }-de
? ? ?
Table 3: An example of case frames for ??? (hit).?
4 Passive-Active Transformation in
Japanese
Morphologically speaking, the passive voice in
Japanese is expressed by using the auxiliary verbs
??? (reru)? and ???? (rareru),? whose past
forms are ??? (reta)? and ???? (rareta),? re-
spectively. For example, the verb in the base form
?????? (tsukiotosu, push down)? is trans-
formed into the past passive form ???????
? (tsukiotosa-reta, was pushed down).? Case al-
ternations accompany passive-active transformation
in Japanese. There are only two case alternations
at most in passive-active transformation. One is the
case represented as ga in the passive voice, and the
other is the case represented as ga in the active voice.
Japanese passive sentences can be classified into
three types in accordance with what is represented
as ga-case in the passive voice: direct passive, in-
direct passive, and possessor passive.
In direct passive sentence, the object of the pred-
icate in the active voice is represented as ga-case.
Examples (1), (3), and (5) are all direct passive sen-
tences. The case that is represented as ga in the ac-
tive voice is usually represented as ni, niyotte, kara,
or de in the passive sentence. In the first sentence of
Examples (6) and (7),5 ga-cases in the active voice
are represented as niyotte and kara, respectively. On
the other hand, ga-case in the passive sentence is al-
ternated with wo or ni as shown with broken lines in
the second sentence of Examples (6) and (7).
(6) P: ???...... ????? ??????
cause-ga..... man-niyotte was identified
(The cause was identified by a man.)
A: ?? ???...... ?????
man-ga cause-wo...... identified
(A man identified the cause.)
5?P? denotes a passive sentence and ?A? denotes the corre-
sponding active sentence in these examples.
(7) P: ??...... ??? ????????
man-ga..... woman-kara was talked to
(A man was talked to by a woman.)
A: ?? ??...... ??????
woman-ga man-ni.... talked to
(A woman talked to a man.)
Indirect passive is also called adversative pas-
sive, in which an indirectly influenced agent is repre-
sented with ga. For example, ?? (I),? the argument
represented with ga in the first sentence of Exam-
ple (8), does not appear in the active voice, i.e. the
second sentence of Example (8). In the case of in-
direct passive, ga-case in the active sentence is al-
ways alternated with ni-case in the passive sentence
as shown with solid lines in Examples (8).
(8) P: ??...... ??? ?????
I-ga..... child-ni was cried
(I?ve got a child crying.)
A: ??? ????(A child cried.)
child-ga cried
Possessor passive is similar to indirect passive in
that the argument represented with ga-case does not
appear as an argument of the predicate in the ac-
tive voice. Therefore, possessor passive is some-
times treated as a kind of indirect passive. How-
ever, in the case of possessor passive, the argument
appears in the active sentence as a possessor of the
direct object. For example, the ga-case argument
?? (woman)? in the passive sentence of Example
(9) does not appear as an argument of the predicate
???? (hit)? in the active sentence but appears in
the phrase that modifies the direct object ?? (head)?
with the case particle no, which indicates that ??
(woman)? is the possessor of ?? (head).?
(9) P: ??...... ?? ?? ?????
woman-ga..... man-ni head-wo was hit
(A woman was hit on the head by a man.)
A:?? ??...... ?? ????
man-ga woman-no..... head-wo hit
(A man hit the head of a woman.)
In conclusion, the number of case alternation pat-
terns accompanying passive-active transformation in
Japanese is limited. Ga-case in the passive voice can
1216
be alternated only with either wo, ni, or no, or does
not appear in the active voice. Ga-case in the active
voice can be represented only by ni, niyotte, kara,
or de in the passive voice. Hence, it is sufficient to
consider only their combinations.
5 Knowledge Acquisition for Case
Alternation
5.1 Task Definition
Our objective is to acquire knowledge for case al-
ternation between the passive and active voices in
Japanese. We leverage lexical case frames obtained
from a large Web corpus by using Kawahara and
Kurohashi (2002)?s method and align cases of a case
frame in the passive voice and cases of a case frame
in the active voice. As described in Section 2, sev-
eral case frames are constructed for each voice of
each predicate. Our task consists of the following
two subtasks:
1. Identify a corresponding case frame in the ac-
tive voice.
2. Find an alignment between cases of case
frames in the passive and active voice.
Figure 1 shows the overview of our task. If a case
frame in the passive voice is input, we identify a cor-
responding case frame in the active voice, and find
an alignment between cases by using the algorithm
described in Section 5.3. In this example, an active
case frame ??????-4 (push down-4)? is iden-
tified as a corresponding case frame for the input
passive case frame ????????-5 (be pushed
down-5)? and ga, ni, and kara-cases in the passive
case frame are aligned to wo, ga, and kara-cases in
the active case frame, respectively.
5.2 Clues for Knowledge Acquisition
We exploit three clues for corresponding case frame
identification and case alignment as follows:
1. Semantic similarity between the instances of
the aligned cases: simSEM .
2. Case distribution similarity between the corre-
sponding case frames: simDIST .
3. Preference of alternation patterns: fPP .
&DVH)UDPH SXVKGRZQ
^DZRUG 	ZRUGV `JD
^
,SHUVRQ`ZR
^ ERWWRPKHOO`QL

&DVH)UDPH SXVKGRZQ
^PDQ OLRQ `JD
^FKLOGFKLOG`ZR
^VHDUDYLQHProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 577?588,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Translation Rules with Right-Hand Side Lattices
Fabien Cromi?res
Japan Science and Technology Agency
Kawaguchi-shi
Saitama 332-0012
fabien@pa.jst.jp
Sadao Kurohashi
Graduate School of Informatics
Kyoto University
Kyoto 606-8501
kuro@i.kyoto-u.ac.jp
Abstract
In Corpus-Based Machine Translation,
the search space of the translation
candidates for a given input sentence
is often defined by a set of (cycle-
free) context-free grammar rules. This
happens naturally in Syntax-Based
Machine Translation and Hierarchi-
cal Phrase-Based Machine Translation
(where the representation will be the
set of the target-side half of the syn-
chronous rules used to parse the input
sentence). But it is also possible to
describe Phrase-Based Machine Trans-
lation in this framework. We propose
a natural extension to this representa-
tion by using lattice-rules that allow
to easily encode an exponential num-
ber of variations of each rules. We also
demonstrate how the representation of
the search space has an impact on de-
coding efficiency, and how it is possible
to optimize this representation.
1 Introduction
A popular approach to modern Machine
Translation is to decompose the translation
problem into a modeling step and a search
step. The modeling step will consist in defin-
ing implicitly a set of possible translations T
for each input sentence. Each translation in
T being associated with a real-valued model
score. The search step will then consist in find-
ing the translation in T with the highest model
score. The search is non-trivial because it is
usually impossible to enumerate all members
of T (its cardinality being typically exponen-
tially dependent on the size of the sentence to
be translated).
Since at least (Chiang, 2007), a common
way of representing T has been through a
cycle-free context-free grammar. In such
a grammar, T is represented as a set of
context-free rules such as can be seen on fig-
ure 1. These rules themselves can be gener-
ated by the modeling step through the use
of phrase tables, synchronous parsing, tree-to-
string rules, etc. If the model score of each
translation is taken to be the sum of rule scores
independently given to each rule, the search
for the optimal translation is easy with some
classic dynamic programming techniques.
However, if the model score is going to take
into account informations such as the lan-
guage model score of each sentence, it cannot
be expressed in such a way. Since the lan-
guage model score has proven empirically to
be a very good source of information, (Chiang,
2007) proposed an approximate search algo-
rithm called cube pruning.
We propose here to represent T using
context-free lattice-rules such as shown in fig-
ure 2. This allows us to compactly encode a
large number of rules. One benefit is that it
adds flexibility to the modeling step, making
it easier: many choices such as whether or not
a function word should be included, the rela-
tive position of words and non-terminal in the
translation, as well as morphological variations
can be delegated to the search step by encod-
ing them in the lattice rules. While it is true
that the same could be achieved by an explicit
enumeration, lattice rules make this easier and
more efficient.
In particular, we show that a decoding al-
gorithm working with such lattice rules can
be more efficient than one working directly on
the enumeration of the rules encoded in the
lattice.
A distinct but related idea of this paper is
to consider how transforming the structure of
the rules defining T can lead to improvements
577
Figure 1: A simple cycle-free context grammar
describing a set of possible translations.
in the speed/memory performances of the de-
coding. In particular, we propose a method to
merge and reduce the size of the lattice rules
and show that it translates into better perfor-
mances at decoding time.
In this paper, we will first define more pre-
cisely our concept of lattice-rules, then try to
give some motivation for them in the context
of a tree-to-tree MT system (section 3). In sec-
tion 4, we then propose an algorithm for pre-
processing a representation given in a lattice-
rule form that allows for more efficient search.
In section 5, we describe a decoding algorithm
specially designed for handling lattice-rules.
In section 6, we perform some experiments
demonstrating the merit of our approach.
2 Notations and Terminology
Here, we define semi-formally the terms we
will use in this paper. We assume knowledge
of the classic terminology of graph theory and
context-free grammar.
2.1 Expansion rules
A flat expansion rule is the association of a
non-terminal and a ?flat? right hand side that
we note RHS. A flat RHS is a sequence of
words and non-terminal. See figure 1 for an
example of a set of flat expansion rules.
A set of expansion rules is often produced
in Hierarchical or Syntax-Based MT, by pars-
ing with synchronous grammars or otherwise.
In such a case, the set of rules define a rep-
resentation of the (weighted) set of possible
translations T of an input sentence.
2.2 Lattice
In the general sense, a lattice can be described
as a labeled directed acyclic graph. More pre-
cisely, the type of lattice that we consider in
this work is such that:
? Edges are labeled by either a word, a
non-terminal or an epsilon (ie. an empty
string).
? Vertices are only labeled by a unique id
by which they can be designated.
Additionally, edges can also be labeled by a
real-valued edge score and some real-valued
edge features. Alternatively, a lattice could
also be seen as an acyclic Finite State Automa-
ton, with vertices and edges corresponding to
states and transitions in the FSA terminology.
For simplicity, we also set the constraint
that each lattice has a unique ?start? ver-
tex labeled v
S
from which each vertex can be
reached and a unique ?end? vertex v
E
that can
be reached from each vertex. Each path from
v
S
to v
E
define thus a flat RHS, with score
and features obtained by summing the score
and features of each edge of the path.
A lattice expansion rule is similar to a flat
expansion rule, but with the RHS being a lat-
tice. Thus a set of lattice expansion rules can
also define a set of possible translations T of
an input sentence.
For a given lattice L, we will often note v ?
L a vertex of L and e : v
1
? v
2
? L an edge
of L going from vertex v
1
to vertex v
2
.
Figures 2 and 3 show examples of such lat-
tices.
2.3 Translation set and
Representations
We note T a set of weighted sentences. T is in-
tended as representing the set of scored trans-
lation candidates generated by a MT system
for a given input sentence. As is customary in
Corpus-Based MT literature, we will call de-
coding the process of searching for the trans-
lation with highest score in T .
A representation of T , noted R
T
is a set of
rules in a given formalism that implicitly de-
fine T . As we mentioned earlier, in MT, R
T
is
often a set of cycle-free context-free grammar
rules.
In this paper, we consider representations
R
T
consisting in a set of lattice expansion
rules. With normal context-free grammar, it
is usually necessary that a non-terminal is the
578
Figure 2: A simple example of lattice rule for
non-terminal X0. The lower part list the set
of ?flat? rules that would be equivalent to the
ones expressed by the lattice.
left-hand side of several rules. Using lattice
expansion rules, however, it is not necessary,
as one lattice RHS can encode an arbitrary
number of flat rules (see for example the RHS
of X0 in figure 3). Therefore, we set the con-
straint that there is only one lattice expansion
rule for each left-hand non-terminal. And we
will note unambiguously RHS(X) the lattice
that is the right hand side of this rule.
3 Motivation
3.1 Setting
This work was developed mainly in the context
of a syntactic-dependency-based tree-to-tree
translation system described in (Richardson et
al., 2014). Although it is a tree-to-tree sys-
tem, we simplify the decoding step by ?flatten-
ing? the target-side tree translation rules into
string expansion rules (keeping track of the de-
pendency structure in state features). Thus
our setting is actually quite similar to that
of many tree-to-string and string-to-string sys-
tems. Aiming at simplicity and generality, we
will set aside the question of target-side syn-
tactic information and only describe our algo-
rithms in a ?tree-to-string? setting. We will
also consider a n-gram language model score
as our only stateful non-local feature.
However, this tree-to-tree original setting
should be kept in mind, in particular when
we describe the issue of the relative position
of heads and dependents in section 3.2.2, as
such issues do not appear as commonly in ?X-
to-string? settings.
3.2 Rule ambiguities
Expansion rules are typically created by
matching part of the input sentence with
some aligned example bilingual sentence. The
alignment (and the linguistic structure of
the phrase in the case of Syntax-Based Ma-
chine Translation) is then used to produce the
target-side rule. However, it is often the case
that it is difficult to fully specify a rule from
an example. Such cases often come from two
main reasons:
? Imperfect knowledge (eg. it is unclear
whether a given unaligned word should
belong to the translation)
? Context dependency (eg. the question of
whether ?to be? should be in plural form
or not, depending on its subject in the
constructed translation).
In both situation, it seems like it would be
better to delay the full specification of the
rule until decoding time, when the decoder
can have access to the surrounding context of
the rule and make a more informed choice. In
particular, we can expect features such as lan-
guage model or governor-dependent features
(in the case of tree-to-tree Machine transla-
tion) to help remove the ambiguities.
We detail some cases for which we encode
variations as lattice-rule.
3.2.1 Non-aligned words
When rules are extracted from aligned exam-
ples, we often find some target words which
are not aligned to any source-side word and
for which it is difficult to decide whether or
not they should be included in the rule. Such
words are often function words that do not
have an equivalent in the source language.
In Japanese-English translations, for example,
articles such as ?a? and ?the? do not typically
have equivalent in the Japanese side, and their
necessity in the final sentence will often be a
matter of context. We can make these edges
579
optionals by doubling them with an epsilon-
edge. Different weights and features can be
given to the epsilon edges to balance the ten-
dency of the decoder to skip edges. In figure 2,
this is illustrated by the epsilon edges allowing
to skip ?for? and ?the?
3.2.2 Non-terminal positions
In the context of our tree-to-tree translation
system, we often find that we know which tar-
get word should be the governor of a given
non-terminal, but that we are unsure of the
order of the words and non-terminals sharing
a common governor. It can be convenient to
represent such ambiguities in a lattice format
as shown in figure 2. In this figure, one can see
that the RHS of X0 encode two possible order-
ing for the word ?bus? and the non-terminal
X2.
3.2.3 Word variations
Linguistics phenomenons such as morpholog-
ical variations can naturally create many mi-
nor problems in the setting of Corpus-Based
Translation. Especially if the variations in
the target language have no equivalence in
the source language. An example of this in
Japanese-English translation is the fact that
verbs in Japanese are ?plural-independent?,
while the verb ?to be? in English is not. There-
fore, a RHS that is a candidate for translating
a large part of a Japanese input sentence can
easily use one of the variant of ?to be? that is
not consistent with the full sentence. To solve
this, for each edge corresponding to the words
?is? or ?are?, we add an alternative edge with
the same start and end vertices as the other
word. The decoder will then be able to choose
the edge that gives the best language model
score. The same can be done, for example, for
the article ?a/an?. Figure 2 provides an exam-
ple of this, with two edges ?is? and ?are? in
the RHS of X0.
Alternative edges can be labeled with differ-
ent weights and features to tune the tendency
of the decoder to choose a morphological vari-
ation.
While such variations could be fixed in a
post-processing step, we feel it is a better op-
tion to let the decoder be aware of the possible
options, lest it would discard rules due to lan-
guage model considerations when these rules
Figure 3: The lattice RHS(X0) optimized with
the algorithm described in section 4
could actually have been useful with a simple
change.
4 Representation optimisation
4.1 Goal
Given a description as a set of rule and scores
R
1
T
of T , it is often possible to find another de-
scription R2
T
of T having the same formalism
but a different set of rules. Although the T
that is described remains the same, the same
search algorithm applied to R1
T
or R2
T
might
make approximations in a different way, be
faster or use less memory.
It is an interesting question to try to trans-
form an initial representation R1
T
into a rep-
resentation R2
T
that will make the search step
faster. This is especially interesting if one is
going to search the same T several times, as is
often done when one is fine-tuning the param-
eters of a model, as this representation opti-
misation needs only be done once.
The optimisation we propose is a natural fit
to our framework of lattice rules. As lattice are
a special case of Finite-State Automata (FSA),
it is easy to adapt existing algorithms for FSA
minimization. We describe a procedure in al-
gorithm 1, which is essentially a simplification
and adaptation to our case of the more gen-
eral algorithm of (Hopcroft, 1971) for FSA.
The central parts of the algorithm are the two
sub-procedures backward vertex merging and
forward vertex merging. An example of the
result of an optimisation is given on figure 3.
580
Data: Representation R
T
Result: Optimized Representation
1 for non-terminal X ? R
T
do
2 Apply backward vertex merging to
RHS(X);
3 Apply forward vertex merging to
RHS(X);
4 end
Algorithm 1: Representation optimisation
4.2 Forward and backward merging
We describe the forward vertex merging in
algorithm 2. This merging will merge ver-
tices and suppress redundant edges, proceed-
ing from left to right. The end result is a
lattice with a reduced number of vertices and
edges, but encoding the same paths as the ini-
tial one.
The basic idea here is to check the vertices
from left to right and merge the ones that have
identical incoming edges. After having been
processed by the algorithm, a vertex is put in
the set P (line 9). At each iteration, the can-
didate set C contains the set of vertices that
can potentially be merged together. It is up-
dated at each iteration to contain the set of
not-yet-processed vertices for which all incom-
ing edges come from processed vertices (done
by marking edges at line 6 and then updating
C at line 10). At each iteration, the merging
process consists in:
1. Eliminating duplicate edges from the pro-
cessed vertices to the candidate vertices
(line 5). These duplicate edges could have
been introduced by the merging of previ-
ously processed vertices.
2. Merging vertices whose set of incom-
ing edges is identical. Here, merg-
ing two vertices v
1
and v
2
means
that we create a third vertex v
3
such that incoming(v
3
) = incoming(v
1
)
= incoming(v
2
), and outgoing(v
3
) =
outgoing(v
3
1) ? outgoing(v
2
), then re-
move v
1
and v
2
.
The backward vertex merging is defined
similarly to the forward merging, but with go-
ing right to left and inverting the role of the
incoming and outgoing edges.
Data: Lattice RHS L
Result: Optimized Lattice RHS
1 P ? ? //processed vertices;
2 C ? {v
S
} //candidate set ;
3 while |C| > 0 do
4 for v ? C do
5 Eliminate duplicate edges in
incoming(v);
6 Mark edges in outgoing(v);
7 end
8 Merge all vertices v
1
, v
2
? C such that
incoming(v
1
) = incoming(v
2
);
9 P ? P
?
C;
10 C ? {v ? L? P s.t. all edges in
incoming(v) are marked};
11 end
Algorithm 2: Forward Vertex Merging
4.3 Optimizing the whole
representation
Algorithm 1 describe the global optimisation
procedure. For each lattice RHS, we just per-
form first a backward merge and then a for-
ward merge.
We have set the constraint in section 2.3
that each non-terminal should have only one
lattice RHS. Note here that if there are sev-
eral RHS for a given non-terminal, we can first
merge them by merging their start vertex and
end vertex, then apply this optimisation al-
gorithm to obtain a representation with one
optimised RHS per non-terminal.
This optimisation could be seen as doing
some form of hypothesis recombination, but of-
fline.
In term of rule optimisations, we only con-
sider here transformations that do not mod-
ify the number of non-terminals. But it is
worthwhile to note that there are some se-
quence appearing in the middle of some rules
that cannot be merged through a lattice rep-
resentation, but could be factored as sub-rules
appearing in different non-terminals. Indeed,
a lattice rule could actually be encoded as a
set of ?flat? rules by introducing a sufficient
number of non-terminals, but this could pos-
sibly be less efficient from the search algorithm
point of view. We plan to investigate the ef-
fects of this type of rule optimisations in con-
junction with the described lattice-type opti-
581
misations in the future.
4.4 Handling of Edge Features
In the context of parameter tuning, we usually
want the decoder to output not only the trans-
lations, but also a list of features characteriz-
ing the way the translation was constructed.
Such features are, for example, the number of
rules used, the language model of the transla-
tion, etc. In out context, some features will be
dependent on the specific edges used in a rule.
For example, the epsilon edge used to option-
ally skip non-aligned words (see section 3.2.1)
is labeled with a feature ?nb-words-skipped?
set to 1, so that we can obtain the number
of words skipped in a given translation and
tune a score penalty for skipping such words.
Similar features also exist for picking a word
variation (section 3.2.3).
In the description of the merging process
of section 4.2, one should thus be aware that
two edges are to be considered identical only
if both their associated word and their set of
feature values are identical. This can some-
times prevent useful merging of states to take
place. A solution to this could be to follow
(de Gispert et al., 2010) and to discard all
these features information during the decod-
ing. The features values are then re-estimated
afterward by aligning the translation and the
input with a constrained version of the de-
coder.
We prefer to actually keep track of the fea-
tures values, even if it can reduce the efficiency
of vertex merging. In that setting, we can also
adapt the so-called Weight Pushing algorithm
(Mohri, 2004) to a multivalues case in order
to improve the ?mergeability? of vertices. The
results of section 6.1 shows that it is still pos-
sible to strongly reduce the size of the lattices
even when keeping track of the features values.
5 Decoding algorithm
In order to make an optimal use of these
lattice-rule representations, we developed a
decoding algorithm for translation candidate
sets represented as a set of lattice-rules. For
the most part, this algorithm re-use many of
the techniques previously developed for decod-
ing translation search spaces, but adapt them
to our setting.
5.1 Overview
The outline of the decoding algorithm is de-
scribed by algorithm 3. For simplicity, the
description only compute the optimal model
score over the translations in the candidate set.
It is however trivial to adapt the description
to keep track of which sentence correspond to
this optimal score and output it instead of the
score. Likewise, using the technique described
in (Huang and Chiang, 2005), one can easily
output k-best lists of translations. For sim-
plicity again, we consider that a n-gram lan-
guage model score is the only stateful non-
local feature used for computing the model
score, although in a tree-to-tree setting, other
features (local in a tree representation but not
in a string representation) could be used. The
model score of a translation t has therefore the
shape:
score(t) = ? ? lm(t) +
?
e
score(e)
where ? is the weight of the language model,
lm(t) is the language model log-probability of
t and the sum is over all edges e crossed to
obtain t.
5.2 Scored language model states
Conceptually, in a lattice L, at each vertex
v, we can consider the partial translations ob-
tained by starting at v
S
and concatenating the
words labeling each edge not labeled by a non-
terminal until v. If an edge is labeled by a non-
terminal X, we first traverse the correspond-
ing lattice RHS(X) following the same pro-
cess. Such a partial translation can be reduced
compactly to a scored language model state
(l, r, s), where l represent the first n words1 of
the partial translation, r its last n words and s
its partial score. It is clear that if two partial
translations have the same l and r parts but
different score, we can discard the one with
the lowest score, as it cannot be a part of the
optimal translation.
Further, using the state reduction tech-
niques described in (Li and Khudanpur, 2008)
and (Heafield et al., 2011), we can often reduce
the size of l and r to less than n, allowing fur-
ther opportunities for discarding sub-optimal
1
n being the order of the language mode
582
partial translations. For better behavior dur-
ing the cube-pruning step of the algorithm (see
later), the partial score s of a partial transla-
tion includes rest-costs estimates (Heafield et
al., 2012).
We define the concatenation operation
on scored language model states to be:
(l
1
, r
1
, s
1
) ? (l
2
, r
2
, s
2
) = (l
3
, r
3
, s
3
), where
s
3
= s
1
+ s
2
+ ?lm(r
1
, l
2
), with lm(r
1
, l
2
) be-
ing the language model probability of l
2
given
r
1
with rest-costs adjustments. r
3
and l
3
are
the resulting minimized states. Similarly, if
an edge e is labeled by a word, we define
the concatenation of a scored state with an
edge to be (l
1
, r
1
, s
1
) ? e = (l
2
, r
2
, s
2
) where
s
2
= s
1
+ score(e) + ?lm(word(e)|r
1
).
Conveniently for us, the KenLM2 open-
source library (Heafield, 2011) provides func-
tionalities for easily computing such concate-
nation operations.
5.3 Algorithm
Having defined these operations, we can now
more easily describe algorithm 3. Each vertex
v has a list best[v] of the scored states of the
best partial translations found to be ending
at v. On line 1, we initialize best[v
S
] with
(., ., 0), where ?.? represent an empty language
model state. We then traverse the vertices of
the lattice in topological order.
For each edge e : v
1
? v
2
, we compute new
scored states for best[v
2
] as follow:
? if e is labeled by a word or an epsilon, we
create a state st
2
= st
1
? e for each st
1
in
best[v
1
] (line 10).
? if e is labeled by a non-terminal X, we re-
cursively call the decoding algorithm on
the lattice RHS(X). The value returned
by the line 15 will be a set of states corre-
sponding to optimal partial translations
traversing RHS(X). We can concate-
nate these states with the ones in best[v
1
]
to obtain states corresponding to partial
translations ending at v
2
(line 6).
Results of the calls decode(X) are memo-
ized, as the same non-terminal is likely to ap-
pear in several edges of a RHS and in several
RHS.
2http://kheafield.com/code/kenlm/
Lines 5 and 6 are the ?cube-pruning-like?
part of the algorithm. The function prune
K
returns the K best combinations of states
in best[v] and decode(RHS(X)), where best
means ?whose sum of partial score is highest?.
It can be implemented efficiently through the
algorithms proposed in (Huang and Chiang,
2005) or (Chiang, 2007).
The L ?
max
st operation on lines 6 and
10 has the following meaning: L is a list of
scored language model state and st is a scored
language model state. L?
max
st means that,
if L already contains a state st
2
with same left
and right state as st, L is updated to contain
only the scored state with the maximum score.
If L do not contain a state similar to st, st in
simply inserted into L. This is the ?hypothe-
sis recombination? part of the algorithm. The
function trunc
K
? truncate the list best[v] to its
K
? highest-scored elements.
The final result is obtained by calling
decode(X
0
), where X
0
is the ?top-level? non-
terminal. The result of decode(X
0
) will
contain only one scored state of the form
(BOS,EOS, s), with s being the optimal
score.
The search procedure of algorithm 3 could
be described as ?breadth-first?, since we sys-
tematically visit each edge of the lattice. An
alternative would be to use a ?best-first?
search with an A*-like procedure. We have
tried this, but either because of optimisation
issues or heuristics of insufficient qualities, we
did not obtain better results than with the al-
gorithm we describe here.
6 Evaluation
We now describe a set of experiments aimed
at evaluating our approach.
We use the Japanese-English data from the
NTCIR-10 Patent MT task3 (Goto et al.,
2013). The training data contains 3 millions
parallel sentences for Japanese-English.
6.1 Effect of Lattice Representation
and Optimisation
We first evaluate the impact of the lattice rep-
resentation on the performances of our decod-
ing algorithm. This will allow us to measure
3http://ntcir.nii.ac.jp/PatentMT-2/
583
Data: Lattice RHS L
Result: Sorted list of best states
1 best[v
E
] = {(.,.,0.0)};
2 for vertex v ? L in topological order do
3 for edge e : v ? v
2
? outgoing(v) do
4 if label(e) = X then
5 for st
1
, st
2
? prune
K
(best[v],
decode(RHS(X)) do
6 best[v
2
]?
max
st
1
? st
2
;
7 end
8 else
9 for st ? trunc
K
?(best[v]) do
10 best[v
2
]?
max
st? e;
11 end
12 end
13 end
14 end
15 return best[v
E
];
Algorithm 3: Lattice-rule decoding. See
body for detailed explanations.
the benefits of our compact lattice represen-
tation of rules, as well as the benefits of the
representation optimisation algorithm of sec-
tion 4.
We use our Syntactic-dependency system to
generate a lattice-rule representation of the
possible translations of the 1800 sentences of
the development set of the NTCIR-10 Patent
MT task. We then produce two additional rep-
resentations:
1. An optimized lattice-rule representation
using the method described in section 4.
2. An expanded representation, that un-
fold the original lattice-rule representa-
tion into ?flat rules? enumerating each
path in the original lattice-rule represen-
tation (like the listX0? enumerate the lat-
tice X0 in figure 2).
Table 1 shows 3 columns. One for each of
these 3 representations. We can see that, as
expected, the performances in term of average
search time or peak memory used are directly
related to the number of vertices and edges
in the representation. We can also see that
our representation optimisation step is quite
efficient, since it is able to divide by two the
number of vertices in the representation, on
average. This leads to a 2-fold speed improve-
ment in the decoding step, as well as a large
reduction of memory usage.
6.2 Decoding performances
In order to further evaluate the merit of our
approach, we now compare the results ob-
tained by using our decoder with lattice-rules
with using a state-of-the-art decoder on the
set of flat expanded rules equivalent to these
lattice rules.
We use the decoder described in (Heafield
et al., 2013), which is available under an open-
source license4 (henceforth called K-decoder).
In this experience, we expanded the lattice
rules generated by our MT system for 1800
sentences into files having the required format
for the K-decoder. This basically mean we
computed an equivalent of the expanded rep-
resentation of section 6.1. This process gener-
ated files ranging in size from 20MB to 17GB
depending on the sentence. We then ran the
K-decoder on these files and compared the re-
sults with our own. We used a beam-width
of 10000 for the K-decoder. Experiments were
run in single thread mode. Partly to obtain
more consistent results, and partly because the
K-decoder was risking using too much memory
for our system.
The results on table 3 show that, as the K-
decoder do not have access to a more compact
representation of the rules, it end up needing
a much larger amount of memory for decoding
the same sentences.
In term of model score obtained, the perfor-
mances are quite similar, with the lattice-rule
decoder providing slightly better model score.
It is interesting to note that, on ?fair-
ground? comparison, that is if our decoder do
not have the benefit of a more compact lattice-
rule representation, it actually perform quite
worse as we can see by comparing with the
third column of table 1 (at least in term of de-
coding time and memory usage, while it would
still have a very slight edge in term of model
score with the selected settings). On the other
hand, the K-decoder is a rather strong base-
line, shown to perform several times faster
than a previous state-of-the-art implementa-
tion in (Heafield et al., 2013). It is well opti-
4http://kheafield.com/code/search/
584
Representation: Original Optimized Expanded
Peak memory used 39 GB 16GB 85GB
Average search time 6.13s 3.31s 9.95s
#vertices (avg/max) 65K (1300K) 32K (446K) 263K (5421K)
#edges (avg/max) 92K (1512K) 83K (541K) 263K (5421K)
Table 1: Impact of the lattice representation on performances.
System JA?EN
Lattice 29.43
No-variations 28.91
Moses (for scale) 28.86
Table 2: Impact on BLEU of using flexible
lattice rules.
mized and makes use of advanced techniques
with the language model (as the one described
in (Heafield et al., 2013)) for which we do not
have implemented an equivalent yet. There-
fore, we are hopeful we can further improve
our decoder in the future.
Also, note that, for practical reason, while
we only measured the decoding time for our
decoder 5, the K-decoder time include the time
taken for loading the rule files.
6.3 Translation quality
Finally, we evaluate the advantages of ex-
tracting lattice rules such as proposed in sec-
tion 3. That is, we consider rules for which
null-aligned words are bypassable by epsilon-
edges, for which Non-terminal are allowed to
take several alternative positions around the
word that is thought to be their governor, and
for which we consider alternative morphologies
of a few words (?is/are?, ?a/an?). We compare
this approach with heuristically selecting only
one possibility for each variation present in the
lattice rule extracted from a single example.
Results shown on figure 2 show that we
do obtain a significant improvement in trans-
lation quality. Note that the Moses score
(Koehn et al., 2007), taken from the official re-
sults of NTCIR-10 is only here ?for scale?, as
our MT system uses a quite different pipeline.
5in particular, we factored out the representation
optimisation time, which is reasonable if we are in the
setting of a parameter tuning step in which the same
sentences are translated repeatedly
7 Related work
Searching for the most optimal translation in
an implicitly defined set has been the focus of
a lot of research in Machine Translation and
it would be difficult to cover all of it. Among
the most influential approaches, (Koehn et al.,
2003) was using a form of stack based de-
coding for Phrase-Based Machine Translation.
(Chiang, 2007) introduced the cube-pruning
approach, which has been further improved
in the previously mentioned (Heafield et al.,
2013). (Rush and Collins, 2011) recently pro-
posed an algorithm promising to find the op-
timal solution, but that is rather slow in prac-
tice.
Weighted Finite State Machines have seen
a variety of use in NLP (Mohri, 1997). More
specifically, some other previous work on Ma-
chine Translation have used lattices (or more
generally Weighted Finite State Machines). In
the context of Corpus-Based Machine Trans-
lation, (Knight and Al-Onaizan, 1998) was al-
ready proposing to use Weighted Transducers
to decode the ?IBM? models of translation
(Brown et al., 1993). (Casacuberta and Vi-
dal, 2004) and (Kumar et al., 2006) also pro-
pose to directly model the translation process
with Finite State Transducers. (Graehl and
Knight, 2004) propose to use Tree Transducers
for modeling Syntactic Machine Translation.
These approaches are however based on differ-
ent paradigm, typically trying to directly learn
a transducer rather than extracting SCFG-like
rules.
Closer to our context, (de Gispert et al.,
2010) propose to use Finite-State Transducers
in the context of Hierarchical Phrase Based
Translation. Their method is to iteratively
construct and minimize the full ?top-level lat-
tice? representing the whole set of translations
bottom-up. It is an approach more focused
on the Finite State Machine aspect than our,
585
System K-decoder Lattice-rule decoder
Peak memory used 52G 16G
Average search time 3.47s 3.31s
Average model score -107.55 -107.39
Nb wins 401 579
Table 3: Evaluation of the performances of our lattice-rule decoder compared with a state-of-
the-art decoder using an expanded flat representation of the lattice rules. ?Nb wins? is the
number of times one of the decoder found a strictly better model score than the other one, out
of 1800 search.
which is more of an hybrid approach that stays
closer to the paradigm of cube-pruning. The
merit of their approach is that they can apply
minimization globally, allowing for more possi-
bilities for vertex merging. On the other hand,
for large grammars, the ?top-level lattice? will
be huge, creating the need to prune vertices
during the construction. Furthermore, the
composition of the ?top-level lattice? with a
language model will imply redundant compu-
tations (as lower-level lattices will potentially
be expanded several times in the top-level lat-
tice). As we do not construct the global lattice
explicitly, we do not need to prune vertices (we
only prune language model states). And each
edge of each lattice rule is crossed only once
during our decoding.
Very recently, (Heafield et al., 2014) also
considered using the redundancy of translation
hypotheses to optimize phrase-based stack de-
coding. To do so, they group the partial hy-
potheses in a trie structure.
We are not aware of other work proposing
?lattice rules? as a native format for express-
ing translational equivalences. Work like (de
Gispert et al., 2010) rely on SCFG rules cre-
ated along the (Chiang, 2007) approach, while
work like (Casacuberta and Vidal, 2004) adopt
a pure Finite State Transducer paradigm (thus
without explicit SCFG-like rules).
8 Conclusion
This work proposes to use a lattice-rule repre-
sentation of the translation search space with
two main goals:
? Easily represent the translation ambigui-
ties that arise either due to lack of context
or imperfect knowledge.
? Have a method for optimizing the repre-
sentation of a search space to make this
search more efficient.
We demonstrate that many types of am-
biguities arising when extracting translation
rules can easily be expressed in this frame-
work, and that making these ambiguities ex-
plicit and solvable at compile time through
lattice-rules leads to improvement in transla-
tion quality.
We also demonstrate that making a direct
use of the lattice-rules representation allows a
decoder to perform better than if working on
the expanded set of corresponding ?flat rules?.
And we propose an algorithm for computing
more efficient representations of a translation
candidate set.
We believe that the the link between the
representation of a candidate set and the de-
coding efficiency is an interesting issue and
we intend to explore further the possibilities
of optimizing representations both in the con-
texts we considered in this paper and in others
such as Phrase-Based Machine Translation.
The code of the decoder we implemented for
this paper is to be released under a GPL li-
cense6.
Acknowledgements
This work is supported by the Japanese Sci-
ence and Technology Agency. We want to
thank the anonymous reviewers for many very
useful comments.
References
Peter F Brown, Vincent J Della Pietra, Stephen
A Della Pietra, and Robert L Mercer. 1993. The
mathematics of statistical machine translation:
6http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/
586
Parameter estimation. Computational linguis-
tics, 19(2):263?311.
Francisco Casacuberta and Enrique Vidal. 2004.
Machine translation with inferred stochastic
finite-state transducers. Computational Linguis-
tics, 30(2):205?225.
David Chiang. 2007. Hierarchical phrase-
based translation. computational linguistics,
33(2):201?228.
Adri? de Gispert, Gonzalo Iglesias, Graeme
Blackwood, Eduardo R Banga, and William
Byrne. 2010. Hierarchical phrase-based transla-
tion with weighted finite-state transducers and
shallow-n grammars. Computational linguistics,
36(3):505?533.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita,
and Benjamin K Tsou. 2013. Overview of the
patent machine translation task at the ntcir-
10 workshop. In Proceedings of the 10th NT-
CIR Workshop Meeting on Evaluation of Infor-
mation Access Technologies: Information Re-
trieval, Question Answering and Cross-Lingual
Information Access, NTCIR-10.
Jonathan Graehl and Kevin Knight. 2004. Train-
ing tree transducers. In Proceedings of HLT-
NAACL.
Kenneth Heafield, Hieu Hoang, Philipp Koehn,
Tetsuo Kiso, and Marcello Federico. 2011.
Left language model state for syntactic ma-
chine translation. In Proceedings of the Inter-
national Workshop on Spoken Language Trans-
lation, pages 183?190, San Francisco, California,
USA, December.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012. Language model rest costs and space-
efficient storage. In Proceedings of the Joint
Conference on Empirical Methods in Natural
Language Processing and Computational Natu-
ral Language Learning, pages 1169?1178, Jeju
Island, Korea, July.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary
words to speed k-best extraction from hyper-
graphs. In Proceedings of the 2013 Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human
Language Technologies, pages 958?968, Atlanta,
Georgia, USA, June.
Kenneth Heafield, Michael Kayser, and Christo-
pher D. Manning. 2014. Faster Phrase-Based
decoding by refining feature state. In Proceed-
ings of the Association for Computational Lin-
guistics, Baltimore, MD, USA, June.
Kenneth Heafield. 2011. KenLM: faster and
smaller language model queries. In Proceedings
of the EMNLP 2011 Sixth Workshop on Statis-
tical Machine Translation, pages 187?197, Edin-
burgh, Scotland, United Kingdom, July.
John Hopcroft. 1971. An n log n algorithm for
minimizing states in a finite automaton. Theory
of Machines and Computations, pages 189?196.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Proceedings of the Ninth In-
ternational Workshop on Parsing Technology,
pages 53?64. Association for Computational Lin-
guistics.
Kevin Knight and Yaser Al-Onaizan. 1998. Trans-
lation with finite-state devices. In Machine
translation and the information soup, pages 421?
437. Springer.
Philipp Koehn, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of the 2003 Conference of
the North American Chapter of the Association
for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 48?54. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, et al. 2007. Moses: Open
source toolkit for statistical machine translation.
In Proceedings of the 45th Annual Meeting of the
ACL on Interactive Poster and Demonstration
Sessions, pages 177?180. Association for Com-
putational Linguistics.
Shankar Kumar, Yonggang Deng, and William
Byrne. 2006. A weighted finite state transducer
translation template model for statistical ma-
chine translation. Natural Language Engineer-
ing, 12(01):35?75.
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine transla-
tion with equivalent language model state main-
tenance. In Proceedings of the Second Workshop
on Syntax and Structure in Statistical Trans-
lation, pages 10?18. Association for Computa-
tional Linguistics.
Mehryar Mohri. 1997. Finite-state transducers in
language and speech processing. Computational
linguistics, 23(2):269?311.
Mehryar Mohri. 2004. Weighted finite-state
transducer algorithms. an overview. In For-
mal Languages and Applications, pages 551?563.
Springer.
John Richardson, Fabien Cromi?res, Toshiaki
Nakazawa, and Sadao Kurohashi. 2014. Ky-
otoebmt: An example-based dependency-to-
dependency translation framework. In Proceed-
ings of ACL (System Demonstration), Balti-
more, MD, USA, June.
587
Alexander M Rush and Michael Collins. 2011.
Exact decoding of syntactic translation mod-
els through lagrangian relaxation. In Proceed-
ings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human
Language Technologies-Volume 1, pages 72?82.
Association for Computational Linguistics.
588
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 786?794,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Dependency Tree-based Sentiment Classification using CRFs with Hidden
Variables
Tetsuji Nakagawa?, Kentaro Inui?? and Sadao Kurohashi??
?National Institute of Information and Communications Technology
?Tohoku University
?Kyoto University
tnaka@nict.go.jp, inui@ecei.tohoku.ac.jp, kuro@i.kyoto-u.ac.jp
Abstract
In this paper, we present a dependency tree-
based method for sentiment classification of
Japanese and English subjective sentences us-
ing conditional random fields with hidden
variables. Subjective sentences often con-
tain words which reverse the sentiment po-
larities of other words. Therefore, interac-
tions between words need to be considered
in sentiment classification, which is difficult
to be handled with simple bag-of-words ap-
proaches, and the syntactic dependency struc-
tures of subjective sentences are exploited in
our method. In the method, the sentiment po-
larity of each dependency subtree in a sen-
tence, which is not observable in training data,
is represented by a hidden variable. The po-
larity of the whole sentence is calculated in
consideration of interactions between the hid-
den variables. Sum-product belief propaga-
tion is used for inference. Experimental re-
sults of sentiment classification for Japanese
and English subjective sentences showed that
the method performs better than other meth-
ods based on bag-of-features.
1 Introduction
Sentiment classification is a useful technique for an-
alyzing subjective information in a large number of
texts, and many studies have been conducted (Pang
and Lee, 2008). A typical approach for sentiment
classification is to use supervised machine learning
algorithms with bag-of-words as features (Pang et
al., 2002), which is widely used in topic-based text
classification. In the approach, a subjective sen-
tence is represented as a set of words in the sen-
tence, ignoring word order and head-modifier rela-
tion between words. However, sentiment classifi-
cation is different from traditional topic-based text
classification. Topic-based text classification is gen-
erally a linearly separable problem ((Chakrabarti,
2002), p.168). For example, when a document con-
tains some domain-specific words, the document
will probably belong to the domain. However, in
sentiment classification, sentiment polarities can be
reversed. For example, let us consider the sentence
?The medicine kills cancer cells.? While the phrase
cancer cells has negative polarity, the word kills re-
verses the polarity, and the whole sentence has pos-
itive polarity. Thus, in sentiment classification, a
sentence which contains positive (or negative) polar-
ity words does not necessarily have the same polar-
ity as a whole, and we need to consider interactions
between words instead of handling words indepen-
dently.
Recently, several methods have been proposed to
cope with the problem (Zaenen, 2004; Ikeda et al,
2008). However, these methods are based on flat
bag-of-features representation, and do not consider
syntactic structures which seem essential to infer
the polarity of a whole sentence. Other methods
have been proposed which utilize composition of
sentences (Moilanen and Pulman, 2007; Choi and
Cardie, 2008; Jia et al, 2009), but these methods
use rules to handle polarity reversal, and whether po-
larity reversal occurs or not cannot be learned from
labeled data. Statistical machine learning can learn
useful information from training data and generally
robust for noisy data, and using it instead of rigid
rules seems useful. Wilson et al (2005) proposed
a method for sentiment classification which utilizes
head-modifier relation and machine learning. How-
ever, the method is based on bag-of-features and po-
larity reversal occurred by content words is not han-
dled. One issue of the approach to use sentence
composition and machine learning is that only the
whole sentence is labeled with its polarity in gen-
eral corpora for sentiment classification, and each
component of the sentence is not labeled, though
such information is necessary for supervised ma-
786
Whole Dependency Tree
Polarities of Dependency Subtrees
It cancer and heart disease.
prevents
cancer and heart disease.
prevents
cancer and heart disease.
+? ?
Figure 1: Polarities of Dependency Subtrees
chine learning to infer the sentence polarity from its
components.
In this paper, we propose a dependency tree-based
method for Japanese and English sentiment classifi-
cation using conditional random fields (CRFs) with
hidden variables. In the method, the sentiment po-
larity of each dependency subtree, which is not ob-
servable in training data, is represented by a hidden
variable. The polarity of the whole sentence is cal-
culated in consideration of interactions between the
hidden variables.
The rest of this paper is organized as follows: Sec-
tion 2 describes a dependency tree-based method
for sentiment classification using CRFs with hid-
den variables, and Section 3 shows experimental re-
sults on Japanese and English corpora. Section 4
discusses related work, and Section 5 gives conclu-
sions.
2 Dependency Tree-based Sentiment
Classification using CRFs with Hidden
Variables
In this study, we handle a task to classify the polar-
ities (positive or negative) of given subjective sen-
tences. In the rest of this section, we describe a prob-
abilistic model for sentiment classification based on
dependency trees, methods for inference and param-
eter estimation, and features we use.
2.1 A Probabilistic Model based on
Dependency Trees
Let us consider the subjective sentence ?It prevents
cancer and heart disease.? In the sentence, cancer
and heart disease have themselves negative polari-
It cancer and heart disease.prevents
s0+
<root>
s10 s2+ s3? s4?
Figure 2: Probabilistic Model based on Dependency Tree
s0 s1 s2 s3 s4
g1 g2 g3 g4
g5g6 g7 g8
Figure 3: Factor Graph
ties. However, the polarities are reversed by modi-
fying the word prevents, and the dependency subtree
?prevents cancer and heart disease? has positive po-
larity. As a result, the whole dependency tree ?It
prevents cancer and heart disease.? has positive po-
larity (Figure 1). In such a way, we can consider
the sentiment polarity for each dependency subtree
of a subjective sentence. Note that we use phrases as
a basic unit instead of words in this study, because
phrases are useful as a meaningful unit for sentiment
classification1. In this paper, a dependency subtree
means the subtree of a dependency tree whose root
node is one of the phrases in the sentence.
We use a probabilistic model as shown in Fig-
ure 2. We consider that each phrase in the subjective
sentence has a random variable (indicated by a cir-
cle in Figure 2). The random variable represents the
polarity of the dependency subtree whose root node
is the corresponding phrase. Two random variables
are dependent (indicated by an edge in Figure 2) if
their corresponding phrases have head-modifier re-
lation in the dependency tree. The node denoted as
<root> in Figure 2 indicates a virtual phrase which
represents the root node of the sentence, and we re-
gard that the random variable of the root node is the
polarity of the whole sentence. In usual annotated
corpora for sentiment classification, only each sen-
tence is labeled with its polarity, and each phrase
(dependency subtree) is not labeled, so all the ran-
dom variables except the one for the root node are
1From an empirical view, in our preliminary experiments
with the proposed method, phrase-based processing performed
better than word-based processing in accuracy and in computa-
tional efficiency.
787
hidden variables that cannot be observed in labeled
data (indicated by gray circles in Figure 2). With
such a probabilistic model, it is possible to utilize
properties such that phrases which contain positive
(or negative) words tend to have positive (negative)
polarities, and two phrases with head-modifier rela-
tion tend to have opposite polarities if the head con-
tains a word which reverses sentiment polarity.
Next, we define the probabilistic model as shown
in Figure 2 in detail. Let n denote the number of
phrases in a subjective sentence, wi the i-th phrase,
and hi the head index of the i-th phrase. Let si de-
note the random variable which represents the po-
larity of the dependency subtree whose root is the
i-th phrase (si ? {+1,?1}), and let p denote the
polarity of the whole sentence (p ? {+1,?1}). We
regard the 0-th phrase as a virtual phrase which rep-
resents the root of the sentence. w,h, s respectively
denote the sequence of wi, hi, si.
w = w1 ? ? ?wn, h = h1 ? ? ?hn, s = s0 ? ? ? sn,
p = s0.
For the example sentence in Figure 1, w1 =It,
w2 =prevents, w3 =cancer, w4 =and heart dis-
ease., h1 = 2, h2 = 0, h3 = 2, h4 = 2. We define
the joint probability distribution of the sentiment po-
larities of dependency subtrees s, given a subjective
sentence w and its dependency tree h, using log-
linear models:
P?(s|w,h)=
1
Z?(w,h)
exp
{ K
?
k=1
?kFk(w,h, s)
}
,
(1)
Z?(w,h)=
?
s
exp
{ K
?
k=1
?kFk(w,h, s)
}
, (2)
Fk(w,h, s)=
n
?
i=1
fk(i,w,h, s), (3)
where ? = {?1, ? ? ? , ?K} is the set of parameters
of the model. fk(i,w,h, s) is the feature function
of the i-th phrase, and is classified to node feature
which considers only the corresponding node, or
edge feature which considers both the correspond-
ing node and its head, as follows:
fk(i,w,h, s)=
{ fnk (wi, si) (k ? Kn),
f ek(wi, si, whi , shi) (k ? Ke),
(4)
where Kn and Ke respectively represent the sets of
indices of node features and edge features.
2.2 Classification of Sentiment Polarity
Let us consider how to infer the sentiment polarity
p ? {+1,?1}, given a subjective sentence w and
its dependency tree h. The polarity of the root node
(s0) is regarded as the polarity of the whole sentence,
and p can be calculated as follows:
p=argmax
p?
P?(p?|w,h), (5)
P?(p|w,h)=
?
s:s0=p
P?(s|w,h). (6)
That is, the polarity of the subjective sentence is ob-
tained as the marginal probability of the root node
polarity, by summing the probabilities for all the
possible configurations of hidden variables. How-
ever, enumerating all the possible configurations of
hidden variables is computationally hard, and we use
sum-product belief propagation (MacKay, 2003) for
the calculation.
Belief propagation enables us to efficiently calcu-
late marginal probabilities. In this study, the graph-
ical model to be solved has a tree structure (identi-
cal to the syntactic dependency tree) which has no
loops, and an exact solution can be obtained us-
ing belief propagation. Dependencies among ran-
dom variables in Figure 2 are represented by a factor
graph in Figure 3. The factor graph consists of vari-
able nodes si indicated by circles, and factor (fea-
ture) nodes gi indicated by squares. In the exam-
ple in Figure 3, gi(1 ? i ? 4) correspond to the
node features in Equation (4), and gi(5 ? i ? 8)
correspond to the edge features. In belief propa-
gation, marginal distribution is calculated by pass-
ing messages (beliefs) among the variables and fac-
tors connected by edges in the factor graph (Refer
to (MacKay, 2003) for detailed description of belief
propagation).
2.3 Parameter Estimation
Let us consider how to estimate model parameters?,
given L training examples D = {?wl,hl, pl?}Ll=1.
In this study, we use the maximum a posteriori es-
timation with Gaussian priors for parameter estima-
tion. We define the following objective function L?,
788
and calculate the parameters ?? which maximize the
value:
L?=
L
?
l=1
logP?(pl|wl,hl) ?
1
2?2
K
?
k=1
?2k, (7)
??=argmax
?
L?, (8)
where ? is a parameter of Gaussian priors and is set
to 1.0 in later experiments. The partial derivatives of
L? are as follows:
?L?
??k
=
L
?
l=1
[
?
s
P?(s|wl,hl, pl)Fk(wl,hl, s)
?
?
s
P?(s|wl,hl)Fk(wl,hl, s)
]
? 1
?2
?k.
(9)
The model parameters can be calculated with the
L-BFGS quasi-Newton method (Liu and Nocedal,
1989) using the objective function and its partial
derivatives. While the partial derivatives contain
summation over all the possible configurations of
hidden variables, it can be calculated efficiently us-
ing belief propagation as explained in Section 2.2.
This parameter estimation method is same to one
used for Latent-Dynamic Conditional Random Field
(Morency et al, 2007). Note that the objective func-
tion L? is not convex, and there is no guarantee for
global optimality. The estimated model parameters
depend on the initial values of the parameters, and
the setting of the initial values of model parameters
will be explained in Section 2.4.
2.4 Features
Table 1 shows the features used in this study. Fea-
tures (a)?(h) in Table 1 are used as the node fea-
tures (Equation (4)) for the i-th phrase, and fea-
tures (A)?(E) are used as the edge features for the
i-th and j-th phrases (j=hi). In Table 1, si denotes
the hidden variable which represents the polarity of
the dependency subtree whose root node is the i-
th phrase, qi denotes the prior polarity of the i-th
phrase (explained later), ri denotes the polarity re-
versal of the i-th phrase (explained later), mi de-
notes the number of words in the i-th phrase, ui,k,
bi,k, ci,k, fi,k respectively denote the surface form,
base form, coarse-grained part-of-speech (POS) tag,
Node Features
a si
b si&qi
c si&qi&ri
d si&ui,1, ? ? ? , si&ui,mi
e si&ci,1, ? ? ? , si&ci,mi
f si&fi,1, ? ? ? , si&fi,mi
g si&ui,1&ui,2, ? ? ? , si&ui,mi?1&ui,mi
h si&bi,1&bi,2, ? ? ? , si&bi,mi?1&bi,mi
Edge Features
A si&sj
B si&sj&rj
C si&sj&rj&qj
D si&sj&bi,1, ? ? ? , si&sj&bi,mi
E si&sj&bj,1, ? ? ? , si&sj&bj,mj
Table 1: Features Used in This Study
fine-grained POS tag of the k-th word in the i-th
phrase.
We used the morphological analysis system JU-
MAN and the dependency parser KNP2 for pro-
cessing Japanese data, and the POS tagger MX-
POST (Ratnaparkhi, 1996) and the dependency
parser MaltParser3 for English data. KNP outputs
phrase-based dependency trees, but MaltParser out-
puts word-based dependency trees, and we con-
verted the word-based ones to phrase-based ones us-
ing simple heuristic rules explained in Appendix A.
The prior polarity of a phrase qi ? {+1, 0,?1} is
the innate sentiment polarity of a word contained in
the phrase, which can be obtained from sentiment
polarity dictionaries. We used sentiment polarity
dictionaries made by Kobayashi et al (2007) and Hi-
gashiyama et al (2008)4 for Japanese experiments
(The resulting dictionary contains 6,974 positive ex-
pressions and 8,428 negative expressions), and a dic-
tionary made by Wilson et al (2005)5 for English
experiments (The dictionary contains 2,289 positive
expressions and 4,143 negative expressions). When
a phrase contains the words registered in the dictio-
naries, its prior polarity is set to the registered po-
larity, otherwise the prior polarity is set to 0. When
a phrase contains multiple words in the dictionaries,
the registered polarity of the last (nearest to the end
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/
3http://maltparser.org/
4http://cl.naist.jp/?inui/research/EM/sentiment-lexicon.html
5http://www.cs.pitt.edu/mpqa/
789
of the sentence) word is used.
The polarity reversal of a phrase ri ? {0, 1} rep-
resents whether it reverses the polarities of other
phrases (1) of not (0). We prepared polarity revers-
ing word dictionaries, and the polarity reversal of
a phrase is set to 1 if the phrase contains a word
in the dictionaries, otherwise set to 0. We con-
structed polarity reversing word dictionaries which
contain such words as decrease and vanish that re-
verse sentiment polarity. A Japanese polarity revers-
ing word dictionary was constructed from an auto-
matically constructed corpus, and the construction
procedure is described in Appendix B (The dictio-
nary contains 219 polarity reversing words). An
English polarity reversing word dictionary was con-
structed from the General Inquirer dictionary6 in the
same way as Choi and Cardie (2008), by collecting
words which belong to either NOTLW or DECREAS
categories (The dictionary contains 121 polarity re-
versing words).
Choi and Cardie (2008) categorized polarity re-
versing words into two categories: function-word
negators such as not and content-word negators such
as eliminate. The polarity reversal of a phrase ri ex-
plained above handles only the content-word nega-
tors, and function-word negators are handled in an-
other way, since the scope of a function-word nega-
tor is generally limited to the phrase containing it in
Japanese, and the number of function-word negators
is small. The prior polarity qi and the polarity rever-
sal ri of a phrase are changed to the following q?i and
r?i, if the phrase contains a function-word negator (in
Japanese) or if the phrase is modified by a function-
word negator (in English):
q?i=?qi, (10)
r?i=1 ? ri. (11)
In this paper, unless otherwise noted, the word po-
larity reversal is used to indicate polarity reversing
caused by content-word negators, and function-word
negators are assumed to be applied to qi and ri in the
above way beforehand.
As described in Section 2.3, there is no guaran-
tee of global optimality for estimated parameters,
since the objective function is not convex. In our
6http://www.wjh.harvard.edu/ inquirer/
preliminary experiments, L-BFGS often did not con-
verge and classification accuracy was unstable when
the initial values of parameters were randomly set.
Therefore, in later experiments, we set the initial
values in the following way. For the feature (A) in
Table 1 in which si and sj are equal, we set the ini-
tial parameter ?i of the feature to a random number
in [0.9, 1.1], otherwise we set to a random number in
[?0.1, 0.1]7. By setting such initial values, the initial
model parameters have a property that two phrases
with head-modifier relation tend to have the same
polarity, which is intuitively reasonable.
3 Experiments
We conducted experiments of sentiment classifica-
tion on four Japanese corpora and four English cor-
pora.
3.1 Data
We used four corpora for experiments of Japanese
sentiment classification: the Automatically Con-
structed Polarity-tagged corpus (ACP) (Kaji and
Kitsuregawa, 2006), the Kyoto University and NTT
Blog corpus (KNB) 8, the NTCIR Japanese opinion
corpus (NTC-J) (Seki et al, 2007; Seki et al, 2008),
the 50 Topics Evaluative Information corpus (50
Topics) (Nakagawa et al, 2008). The ACP corpus
is an automatically constructed corpus from HTML
documents on the Web using lexico-syntactic pat-
terns and layout structures. The size of the corpus
is large (it consists of 650,951 instances), and we
used 1/100 of the whole corpus. The KNB corpus
consists of Japanese blogs, and is manually anno-
tated. The NTC-J corpus consists of Japanese news-
paper articles. There are two NTCIR Japanese opin-
ion corpora available, the NTCIR-6 corpus and the
NTCIR-7 corpus; and we combined the two cor-
pora. The 50 Topics corpus is collected from various
pages on the Web, and is manually annotated.
We used four corpora for experiments of English
sentiment classification: the Customer Review data
7The values of most learned parameters distributed between
-1.0 and 1.0 in our preliminary experiments. Therefore, we de-
cided to give values around the upper bound (1.0) and the mean
(0.0) to the features in order to incorporate minimal prior knowl-
edge into the model.
8http://nlp.kuee.kyoto-u.ac.jp/kuntt/
790
(CR)9, the MPQA Opinion corpus (MPQA)10, the
Movie Review Data (MR) 11, and the NTCIR En-
glish opinion corpus (NTC-E) (Seki et al, 2007;
Seki et al, 2008). The CR corpus consists of re-
view articles about products such as digital cameras
and cellular phones. There are two customer review
datasets, the 5 products dataset and the 9 products
dataset, and we combined the two datasets. In the
MPQA corpus, sentiment polarities are attached not
to sentences but expressions (sub-sentences), and we
regarded the expressions as sentences and classified
the polarities. There are two NTCIR English cor-
pora available, the NTCIR-6 corpus and the NTCIR-
7 corpus, and we combined the two corpora.
The statistical information of the corpora we used
is shown in Table 2. We randomly split each corpus
into 10 portions, and conducted 10-fold cross valida-
tion. Accuracy of sentiment classification was cal-
culated as the number of correctly predicted labels
(polarities) divided by the number of test examples.
3.2 Compared Methods
We compared our method to 6 baseline methods,
and this section describes them. In the following,
p0 ? {+1,?1} denotes the major polarity in train-
ing data, Hi denotes the set consisting of all the an-
cestor nodes of the i-th phrase in the dependency
tree, and sgn(x) is defined as below:
sgn(x)=
?
?
?
?
?
+1 (x > 0),
0 (x = 0),
?1 (x < 0).
Voting without Polarity Reversal The polarity of
a subjective sentence is decided by voting of
each phrase?s prior polarity. In the case of a
tie, the major polarity in the training data is
adopted.
p=sgn
( n
?
i=1
qi + 0.5p0
)
. (12)
Voting with Polarity Reversal Same to Voting
without Polarity Reversal, except that the po-
larities of phrases which have odd numbers of
9http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html
10http://www.cs.pitt.edu/mpqa/
11http://www.cs.cornell.edu/People/pabo/movie-review-
data/
reversal phrases in their ancestors are reversed
before voting.
p=sgn
( n
?
i=1
qi
?
j?Hi
(?1)rj + 0.5p0
)
. (13)
Rule The polarity of a subjective sentence is deter-
ministically decided basing on rules, by con-
sidering the sentiment polarities of dependency
subtrees. The polarity of the dependency sub-
tree whose root is the i-th phrase is decided by
voting the prior polarity of the i-th phrase and
the polarities of the dependency subtrees whose
root nodes are the modifiers of the i-th phrase.
The polarities of the modifiers are reversed if
their head phrase has a reversal word. The de-
cision rule is applied from leaf nodes in the de-
pendency tree, and the polarity of the root node
is decided at the last.
si=sgn
(
qi +
?
j:hj=i
sj(?1)ri
)
, (14)
p=sgn(s0 + 0.5p0). (15)
Bag-of-Features with No Dictionaries The polar-
ity of a subjective sentence is classified us-
ing Support Vector Machines. Surface forms,
base forms, coarse-grained POS tags and fine-
grained POS tags of word unigrams and bi-
grams in the subjective sentence are used as
features12. The second order polynomial ker-
nel is used and the cost parameter C is set to
1.0. No prior polarity information (dictionary)
is used.
Bag-of-Features without Polarity Reversal Same
to Bag-of-Features with No Dictionaries, ex-
cept that the voting result of prior polarities
(one of positive, negative or tie) is also used
as a feature.
Bag-of-Features with Polarity Reversal Same to
Bag-of-Features without Polarity Reversal, ex-
cept that the polarities of phrases which have
12In experiments on English corpora, only the features of un-
igrams are used and those of bigrams are not used, since the
bigram features decreased accuracies in our preliminary experi-
ments as reported in previous work (Andreevskaia and Bergler,
2008).
791
Language Corpus Number of Instances (Positive / Negative)
ACP 6,510 (2,738 / 3,772)
Japanese KNB 2,288 (1,423 / 865)
NTC-J 3,485 (1,083 / 2,402)
50 Topics 5,366 (3,175 / 2,191)
CR 3,772 (2,406 / 1,366)
English MPQA 10,624 (3,316 / 7,308)
MR 10,662 (5,331 / 5,331)
NTC-E 3,812 (1,226 / 2,586)
Table 2: Statistical Information of Corpora
Method Japanese English
ACP KNB NTC-J 50 Topics CR MPQA MR NTC-E
Voting-w/o Rev. 0.686 0.764 0.665 0.727 0.714 0.804 0.629 0.730
Voting-w/ Rev. 0.732 0.792 0.714 0.765 0.742 0.817 0.631 0.740
Rule 0.734 0.792 0.742 0.764 0.743 0.818 0.629 0.750
BoF-no Dic. 0.798 0.758 0.754 0.761 0.793 0.818 0.757 0.768
BoF-w/o Rev. 0.812 0.823 0.794 0.805 0.802 0.840 0.761 0.793
BoF-w/ Rev. 0.822 0.830 0.804 0.819 0.814 0.841 0.764 0.797
Tree-CRF 0.846* 0.847* 0.826* 0.841* 0.814 0.861* 0.773* 0.804
(* indicates statistical significance at p < 0.05)
Table 3: Accuracy of Sentiment Classification
odd numbers of reversal phrases in their ances-
tors are reversed before voting.
Tree-CRF The proposed method based on depen-
dency trees using CRFs, described in Section 2.
3.3 Experimental Results
The experimental results are shown in Table 3. The
proposed method Tree-CRF obtained the best ac-
curacies for all the four Japanese corpora and the
four English corpora, and the differences against
the second best methods were statistically signifi-
cant (p < 0.05) with the paired t-test for the six
of the eight corpora. Tree-CRF performed better
for the Japanese corpora than for the English cor-
pora. For both the Voting methods and the Bag-of-
Features methods, the methods with polarity rever-
sal performed better than those without it13.
Both BoF-w/ Rev. and Tree-CRF use supervised
machine learning and the same dictionaries (the
13The Japanese polarity reversing word dictionary was con-
structed from the ACP corpus as described in Appendix B, and
it is not reasonable to compare the methods with and without
polarity reversal on the ACP corpus. However, the tendency
can be seen on the other 7 corpora.
prior polarity dictionaries and the polarity revers-
ing word dictionaries), but the latter performed bet-
ter than the former. Our error analysis showed that
BoF-w/ Rev. was not robust for erroneous words in
the prior polarity dictionaries. BoF-w/ Rev. uses the
voting result of the prior polarities as a feature, and
the feature is sensitive to the errors in the dictionary,
while Tree-CRF uses several information as well as
the prior polarities to decide the polarities of depen-
dency subtrees, and was robust to the dictionary er-
rors. We investigated the trained model parameters
of Tree-CRF, and found that the features (E) in Ta-
ble 1, in which the head and the modifier have op-
posite polarities and the head word is such as pro-
tect and withdraw, have large positive weights. Al-
though these words were not included in the polar-
ity reversing word dictionary, the property that these
words reverse polarities of other words seems to be
learned with the model.
4 Related Work
Various studies on sentiment classification have
been conducted, and there are several methods pro-
792
posed for handling reversal of polarities. In this pa-
per, our method was not directly compared with the
other methods, since it is difficult to completely im-
plement them or conduct experiments with exactly
the same settings.
Choi and Cardie (2008) proposed a method to
classify the sentiment polarity of a sentence bas-
ing on compositional semantics. In their method,
the polarity of the whole sentence is determined
from the prior polarities of the composing words by
pre-defined rules, and the method differs from ours
which uses the probabilistic model to handle interac-
tions between hidden variables. Syntactic structures
were used in the studies of Moilanen and Pulman
(2007) and, Jia et al (2009), but their methods are
based on rules and supervised learning was not used
to handle polarity reversal. As discussed in Sec-
tion 1, Wilson et al (2005) studied a bag-of-features
based statistical sentiment classification method in-
corporating head-modifier relation.
Ikeda et al (2008) proposed a machine learning
approach to handle sentiment polarity reversal. For
each word with prior polarity, whether the polarity is
reversed or not is learned with a statistical learning
algorithm using its surrounding words as features.
The method can handle only words with prior polar-
ities, and does not use syntactic dependency struc-
tures.
Conditional random fields with hidden variables
have been studied so far for other tasks. Latent-
Dynamic Conditional Random Fields (LDCRF)
(Morency et al, 2007; Sun et al, 2008) are prob-
abilistic models with hidden variables for sequen-
tial labeling, and belief propagation is used for in-
ference. Out method is similar to the models, but
there are several differences. In our method, only
one variable which represents the polarity of the
whole sentence is observable, and dependency re-
lation among random variables is not a linear chain
but a tree structure which is identical to the syntactic
dependency.
5 Conclusion
In this paper, we presented a dependency tree-based
method for sentiment classification using condi-
tional random fields with hidden variables. In this
method, the polarity of each dependency subtree
of a subjective sentence is represented by a hid-
den variable. The values of the hidden variables
are calculated in consideration of interactions be-
tween variables whose nodes have head-modifier re-
lation in the dependency tree. The value of the
hidden variable of the root node is identified with
the polarity of the whole sentence. Experimental
results showed that the proposed method performs
better for Japanese and English data than the base-
line methods which represents subjective sentences
as bag-of-features.
Appendix
A Rules for Converting Word Sequence to
Phrase Sequence
Let v1, ? ? ? , vN denote an English word sequence, yi
the part-of-speech of the i-th word, and zi the head
index of the i-th word. The word sequence was con-
verted to a phrase sequence as follows, by applying
rules which combine two adjacent words:
LT ? {?,(,-LRB-,-LSB-,-LCB-,CC}
RT ? {?,),,,--,.,:,POS,-RRB-,-RSB-,-RCB-}
PP ? {IN,RP,TO,DT,PDT,PRP,WDT,WP,WP$,WRB}
NN ? {CD,FW,NN,NNP,NNPS,NNS,SYM,JJ}
do
for i := 1 to N ? 1
if xi and xi+1 are not yet combined ?
(xi ? LT ?
xi+1 ? RT ?
((yi = yi+1 ? yi = i+ 1 ? yi+1 = i) ?
(xi ? PP ?
(xi ? NN ? xi+1 ? NN )))) then
Combine the words vi and vi+1
until No rules are applied
B Construction of Japanese Polarity
Reversing Word Dictionary
We constructed a Japanese polarity reversing word
dictionary from the Automatically Constructed
Polarity-tagged corpus (Kaji and Kitsuregawa,
2006). First, we collected sentences, each of which
contains just one phrase having prior polarity, and
the phrase modifies a phrase which modifies the root
node. Among them, we selected sentences in which
the prior polarity is not equal to the polarity of the
whole sentence. We extracted all the words in the
head phrase, and manually checked them whether
they should be put into the dictionary or not. The ra-
tionale behind the procedure is that the prior polarity
can be considered to be reversed by a certain word
in the head phrase.
793
References
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Overcom-
ing Domain Dependence in Sentiment Tagging. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 290?298.
Soumen Chakrabarti. 2002. Mining the Web: Dis-
covering Knowledge from Hypertext Data. Morgan-
Kauffman.
Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 793?801.
Masahiko Higashiyama, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Acquiring Noun Polarity Knowledge
Using Selectional Preferences. In Proceedings of the
14th Annual Meeting of the Association for Natural
Language Processing, pages 584?587. (in Japanese).
Daisuke Ikeda, Hiroya Takamura, Lev-Arie Ratinov, and
Manabu Okumura. 2008. Learning to Shift the Po-
larity of Words for Sentiment Classification. In Pro-
ceedings of the 3rd International Joint Conference on
Natural Language Processing, pages 296?303.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The Ef-
fect of Negation on Sentiment Analysis and Retrieval
Effectiveness. In Proceeding of the 18th ACM Con-
ference on Information and Knowledge Management,
pages 1827?1830.
Nobuhiro Kaji and Masaru Kitsuregawa. 2006. Auto-
matic Construction of Polarity-Tagged Corpus from
HTML Documents. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 452?459.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Opinion Mining from Web Documents: Extrac-
tion and Structurization. Journal of the Japanese So-
ciety for Artificial Intelligence, 22(2):227?238.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503?528.
David J. C. MacKay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of the Recent Advances
in Natural Language Processing International Confer-
ence, pages 378?382.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-Dynamic Discriminative Mod-
els for Continuous Gesture Recognition. In Proceed-
ings of the 2007 IEEE Conference on Computer Vision
and Pattern Recognition, pages 1?8.
Tetsuji Nakagawa, Takuya Kawada, Kentaro Inui, and
Sadao Kurohashi. 2008. Extracting Subjective and
Objective Evaluative Expressions from the Web. In
Proceedings of the 2nd International Symposium on
Universal Communication.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 79?86.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-of-Speech Tagging. In Proceedings of the
1996 Conference on Empirical Methods in Natural
Language Processing Conference, pages 133?142.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-His
Chen, Noriko Kando, and Chin-Yew Lin. 2007.
Overview of Opinion Analysis Pilot Task at NTCIR-
6. In Proceedings of the 6th NTCIR Workshop, pages
265?278.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, and Noriko Kando. 2008. Overview
ofMultilingual Opinion Analysis Task at NTCIR-7. In
Proceedings of the 7th NTCIR Workshop.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun?ichi Tsujii. 2008. Modeling Latent-Dynamic
in Shallow Parsing: A Latent Conditional Model with
Improved Inference. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 841?848.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of the 2005
Joint Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 347?354.
Livia Polanyi Annie Zaenen. 2004. Contextual Lexical
Valence Shifters. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text.
794
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 356?365,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Using Smaller Constituents Rather Than Sentences
in Active Learning for Japanese Dependency Parsing
Manabu Sassano
Yahoo Japan Corporation
Midtown Tower,
9-7-1 Akasaka, Minato-ku,
Tokyo 107-6211, Japan
msassano@yahoo-corp.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
We investigate active learning methods for
Japanese dependency parsing. We propose
active learning methods of using partial
dependency relations in a given sentence
for parsing and evaluate their effective-
ness empirically. Furthermore, we utilize
syntactic constraints of Japanese to ob-
tain more labeled examples from precious
labeled ones that annotators give. Ex-
perimental results show that our proposed
methods improve considerably the learn-
ing curve of Japanese dependency parsing.
In order to achieve an accuracy of over
88.3%, one of our methods requires only
34.4% of labeled examples as compared to
passive learning.
1 Introduction
Reducing annotation cost is very important be-
cause supervised learning approaches, which have
been successful in natural language processing, re-
quire typically a large number of labeled exam-
ples. Preparing many labeled examples is time
consuming and labor intensive.
One of most promising approaches to this is-
sue is active learning. Recently much attention has
been paid to it in the field of natural language pro-
cessing. Various tasks have been targeted in the
research on active learning. They include word
sense disambiguation, e.g., (Zhu and Hovy, 2007),
POS tagging (Ringger et al, 2007), named entity
recognition (Laws and Schu?tze, 2008), word seg-
mentation, e.g., (Sassano, 2002), and parsing, e.g.,
(Tang et al, 2002; Hwa, 2004).
It is the main purpose of this study to propose
methods of improving active learning for parsing
by using a smaller constituent than a sentence as
a unit that is selected at each iteration of active
learning. Typically in active learning for parsing a
sentence has been considered to be a basic unit for
selection. Small constituents such as chunks have
not been used in sample selection for parsing. We
use Japanese dependency parsing as a target task
in this study since a simple and efficient algorithm
of parsing is proposed and, to our knowledge, ac-
tive learning for Japanese dependency parsing has
never been studied.
The remainder of the paper is organized as fol-
lows. Section 2 describes the basic framework of
active learning which is employed in this research.
Section 3 describes the syntactic characteristics of
Japanese and the parsing algorithm that we use.
Section 4 briefly reviews previous work on active
learning for parsing and discusses several research
challenges. In Section 5 we describe our proposed
methods and others of active learning for Japanese
dependency parsing. Section 6 describes experi-
mental evaluation and discussion. Finally, in Sec-
tion 7 we conclude this paper and point out some
future directions.
2 Active Learning
2.1 Pool-based Active Learning
Our base framework of active learning is based on
the algorithm of (Lewis and Gale, 1994), which is
called pool-based active learning. Following their
sequential sampling algorithm, we show in Fig-
ure 1 the basic flow of pool-based active learning.
Various methods for selecting informative exam-
ples can be combined with this framework.
2.2 Selection Algorithm for Large Margin
Classifiers
One of the most accurate approaches to classifica-
tion tasks is an approach with large margin classi-
fiers. Suppose that we are given data points {xi}
such that the associated label yi will be either ?1
or 1, and we have a hyperplane of some large mar-
gin classifier defined by {x : f(x) = 0} where the
356
1. Build an initial classifier from an initial la-
beled training set.
2. While resources for labeling examples are
available
(a) Apply the current classifier to each un-
labeled example
(b) Find the m examples which are most in-
formative for the classifier
(c) Have annotators label the m examples
(d) Train a new classifier on all labeled ex-
amples
Figure 1: Flow of the pool-based active learning
Lisa-ga kare-ni ano pen-wo age-ta.
Lisa-subj to him that pen-acc give-past.
ID 0 1 2 3 4
Head 4 4 3 4 -
Figure 2: Sample sentence. An English translation
is ?Lisa gave that pen to him.?
classification function is G(x) = sign{f(x)}. In
pool-based active learning with large margin clas-
sifiers, selection of examples can be done as fol-
lows:
1. Compute f(xi) over all unlabeled examples
xi in the pool.
2. Sort xi with |f(xi)| in ascending order.
3. Select top m examples.
This type of selection methods with SVMs is dis-
cussed in (Tong and Koller, 2000; Schohn and
Cohn, 2000). They obtain excellent results on text
classification. These selection methods are simple
but very effective.
3 Japanese Parsing
3.1 Syntactic Units
A basic syntactic unit used in Japanese parsing is
a bunsetsu, the concept of which was initially in-
troduced by Hashimoto (1934). We assume that
in Japanese we have a sequence of bunsetsus be-
fore parsing a sentence. A bunsetsu contains one
or more content words and zero or more function
words.
A sample sentence in Japanese is shown in Fig-
ure 2. This sentence consists of five bunsetsus:
Lisa-ga, kare-ni, ano, pen-wo, and age-ta where
ga, ni, and wo are postpositions and ta is a verb
ending for past tense.
3.2 Constraints of Japanese Dependency
Analysis
Japanese is a head final language and in written
Japanese we usually hypothesize the following:
? Each bunsetsu has only one head except the
rightmost one.
? Dependency links between bunsetsus go
from left to right.
? Dependencies do not cross one another.
We can see that these constraints are satisfied in
the sample sentence in Figure 2. In this paper we
also assume that the above constraints hold true
when we discuss algorithms of Japanese parsing
and active learning for it.
3.3 Algorithm of Japanese Dependency
Parsing
We use Sassano?s algorithm (Sassano, 2004) for
Japanese dependency parsing. The reason for this
is that it is very accurate and efficient1. Further-
more, it is easy to implement. His algorithm is
one of the simplest form of shift-reduce parsers
and runs in linear-time.2 Since Japanese is a head
final language and its dependencies are projective
as described in Section 3.2, that simplification can
be made.
The basic flow of Sassano?s algorithm is shown
in Figure 3, which is slightly simplified from the
original by Sassano (2004). When we use this al-
gorithm with a machine learning-based classifier,
function Dep() in Figure 3 uses the classifier to
decide whether two bunsetsus have a dependency
relation. In order to prepare training examples for
the trainable classifier used with his algorithm, we
first have to convert a treebank to suitable labeled
instances by using the algorithm in Figure 4. Note
1Iwatate et al (2008) compare their proposed algorithm
with various ones that include Sassano?s, cascaded chunk-
ing (Kudo and Matsumoto, 2002), and one in (McDonald et
al., 2005). Kudo and Matsumoto (2002) compare cascaded
chunking with the CYK method (Kudo and Matsumoto,
2000). After considering these results, we have concluded
so far that Sassano?s is a reasonable choice for our purpose.
2Roughly speaking, Sassano?s is considered to be a sim-
plified version, which is modified for head final languages, of
Nivre?s (Nivre, 2003). Classifiers with Nivre?s are required
to handle multiclass prediction, while binary classifiers can
work with Sassano?s for Japanese.
357
Input: wi: bunsetsus in a given sentence.
N : the number of bunsetsus.
Output: hj : the head IDs of bunsetsus wj .
Functions: Push(i, s): pushes i on the stack s.
Pop(s): pops a value off the stack s.
Dep(j, i, w): returns true when wj should
modify wi. Otherwise returns false.
procedure Analyze(w, N , h)
var s: a stack for IDs of modifier bunsetsus
begin
{?1 indicates no modifier candidate}
Push(?1, s);
Push(0, s);
for i ? 1 to N ? 1 do begin
j ? Pop(s);
while (j 6= ?1
and ((i = N ? 1) or Dep(j, i, w)) ) do
begin
hj ? i;
j ? Pop(s)
end
Push(j, s);
Push(i, s)
end
end
Figure 3: Algorithm of Japanese dependency pars-
ing
that the algorithm in Figure 4 does not generate
every pair of bunsetsus.3
4 Active Learning for Parsing
Most of the methods of active learning for parsing
in previous work use selection of sentences that
seem to contribute to the improvement of accuracy
(Tang et al, 2002; Hwa, 2004; Baldridge and Os-
borne, 2004). Although Hwa suggests that sample
selection for parsing would be improved by select-
ing finer grained constituents rather than sentences
(Hwa, 2004), such methods have not been investi-
gated so far.
Typical methods of selecting sentences are
3We show a sample set of generated examples for training
the classifier of the parser in Figure 3. By using the algorithm
in Figure 4, we can obtain labeled examples from the sample
sentences in Figure 2: {0, 1, ?O?}, {1, 2, ?O?}, {2, 3, ?D?},
and {1, 3, ?O?}. Please see Section 5.2 for the notation
used here. For example, an actual labeled instance generated
from {2, 3, ?D?} will be like ?label=D, features={modifier-
content-word=ano, ..., head-content-word=pen, ...}.?
Input: hi: the head IDs of bunsetsus wi.
Function: Dep(j, i, w, h): returns true if hj = i.
Otherwise returns false. Also prints a
feature vector with a label according to hj .
procedure Generate(w, N , h)
begin
Push(?1, s);
Push(0, s);
for i ? 1 to N ? 1 do begin
j ? Pop(s);
while (j 6= ?1
and ((i = N ? 1) or Dep(j, i, w, h)) ) do
begin
j ? Pop(s)
end
Push(j, s);
Push(i, s)
end
end
Figure 4: Algorithm of generating training exam-
ples
based on some entropy-based measure of a given
sentence (e.g., (Tang et al, 2002)). We cannot
use this kind of measures when we want to select
other smaller constituents than sentences. Other
bigger problem is an algorithm of parsing itself.
If we sample smaller units rather than sentences,
we have partially annotated sentences and have to
use a parsing algorithm that can be trained from
incompletely annotated sentences. Therefore, it is
difficult to use some of probabilistic models for
parsing. 4
5 Active Learning for Japanese
Dependency Parsing
In this section we describe sample selection meth-
ods which we investigated.
5.1 Sentence-wise Sample Selection
Passive Selection (Passive) This method is to
select sequentially sentences that appear in the
training corpus. Since it gets harder for the read-
ers to reproduce the same experimental setting, we
4We did not employ query-by-committee (QBC) (Seung
et al, 1992), which is another important general framework
of active learning, since the selection strategy with large mar-
gin classifiers (Section 2.2) is much simpler and seems more
practical for active learning in Japanese dependency parsing
with smaller constituents.
358
avoid to use random sampling in this paper.
Minimum Margin Selection (Min) This
method is to select sentences that contain bun-
setsu pairs which have smaller margin values
of outputs of the classifier used in parsing. The
procedure of selection of MIN are summarized as
follows. Assume that we have sentences si in the
pool of unlabeled sentences.
1. Parse si in the pool with the current model.
2. Sort si with min |f(xk)| where xk are bun-
setsu pairs in the sentence si. Note that xk
are not all possible bunsetsu pairs in si and
they are limited to bunsetsu pairs checked in
the process of parsing si.
3. Select top m sentences.
Averaged Margin Selection (Avg) This method
is to select sentences that have smaller values of
averaged margin values of outputs of the classi-
fier in a give sentences over the number of deci-
sions which are carried out in parsing. The differ-
ence between AVG and MIN is that for AVG we
use
?
|f(xk)|/l where l is the number of calling
Dep() in Figure 3 for the sentence si instead of
min |f(xk)| for MIN.
5.2 Chunk-wise Sample Selection
In chunk-wise sample selection, we select bun-
setsu pairs rather than sentences. Bunsetsu pairs
are selected from different sentences in a pool.
This means that structures of sentences in the pool
are partially annotated.
Note that we do not use every bunsetsu pair in
a sentence. When we use Sassano?s algorithm, we
have to generate training examples for the classi-
fier by using the algorithm in Figure 4. In other
words, we should not sample bunsetsu pairs inde-
pendently from a given sentence.
Therefore, we select bunsetsu pairs that have
smaller margin values of outputs given by the clas-
sifier during the parsing process. All the sentences
in the pool are processed by the current parser. We
cannot simply split the sentences in the pool into
labeled and unlabeled ones because we do not se-
lect every bunsetsu pair in a given sentence.
Naive Selection (Naive) This method is to select
bunsetsu pairs that have smaller margin values of
outputs of the classifier. Then it is assumed that
annotators would label either ?D? for the two bun-
setsu having a dependency relation or ?O?, which
represents the two does not.
Modified Simple Selection (ModSimple) Al-
though NAIVE seems to work well, it did not (dis-
cussed later). MODSIMPLE is to select bunsetsu
pairs that have smaller margin values of outputs
of the classifier, which is the same as in NAIVE.
The difference between MODSIMPLE and NAIVE
is the way annotators label examples. Assume that
we have an annotator and the learner selects some
bunsetsu pair of the j-th bunsetsu and the i-th bun-
setsu such that j < i. The annotator is then asked
what the head of the j-th bunsetsu is. We define
here the head bunsetsu is the k-th one.
We differently generate labeled examples from
the information annotators give according to the
relation among bunsetsus j, i, and k.
Below we use the notation {s, t, ?D?} to de-
note that the s-th bunsetsu modifies the t-th one.
The use of ?O? instead of ?D? indicates that the
s-th does not modify the t-th. That is generating
{s, t, ?D?} means outputting an example with the
label ?D?.
Case 1 if j < i < k, then generate {j, i, ?O?} and
{j, k, ?D?}.
Case 2 if j < i = k, then generate {j, k, ?D?}.
Case 3 if j < k < i, then generate {j, k, ?D?}.
Note that we do not generate {j, i, ?O?} in
this case because in Sassano?s algorithm we
do not need such labeled examples if j de-
pends on k such that k < i.
Syntactically Extended Selection (Syn) This
selection method is one based on MODSIMPLE
and extended to generate more labeled examples
for the classifier. You may notice that more labeled
examples for the classifier can be generated from
a single label which the annotator gives. Syntac-
tic constraints of the Japanese language allow us
to extend labeled examples.
For example, suppose that we have four bunset-
sus A, B, C, and D in this order. If A depends
on C, i.e., the head of A is C, then it is automati-
cally derived that B also should depend on C be-
cause the Japanese language has the no-crossing
constraint for dependencies (Section 3.2). By uti-
lizing this property we can obtain more labeled ex-
amples from a single labeled one annotators give.
In the example above, we obtain {A,B, ?O?} and
{B,C, ?D?} from {A,C, ?D?}.
359
Although we can employ various extensions to
MODSIMPLE, we use a rather simple extension in
this research.
Case 1 if (j < i < k), then generate
? {j, i, ?O?},
? {k ? 1, k, ?D?} if k ? 1 > j,
? and {j, k, ?D?}.
Case 2 if (j < i = k), then generate
? {k ? 1, k, ?D?} if k ? 1 > j,
? and {j, k, ?D?}.
Case 3 if (j < k < i), then generate
? {k ? 1, k, ?D?} if k ? 1 > j,
? and {j, k, ?D?}.
In SYN as well as MODSIMPLE, we generate
examples with ?O? only for bunsetsu pairs that oc-
cur to the left of the correct head (i.e., case 1).
6 Experimental Evaluation and
Discussion
6.1 Corpus
In our experiments we used the Kyoto University
Corpus Version 2 (Kurohashi and Nagao, 1998).
Initial seed sentences and a pool of unlabeled sen-
tences for training are taken from the articles on
January 1st through 8th (7,958 sentences) and the
test data is a set of sentences in the articles on Jan-
uary 9th (1,246 sentences). The articles on Jan-
uary 10th were used for development. The split of
these articles for training/test/development is the
same as in (Uchimoto et al, 1999).
6.2 Averaged Perceptron
We used the averaged perceptron (AP) (Freund
and Schapire, 1999) with polynomial kernels. We
set the degree of the kernels to 3 since cubic ker-
nels with SVM have proved effective for Japanese
dependency parsing (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2002). We found the best
value of the epoch T of the averaged perceptron
by using the development set. We fixed T = 12
through all experiments for simplicity.
6.3 Features
There are features that have been commonly used
for Japanese dependency parsing among related
papers, e.g., (Kudo and Matsumoto, 2002; Sas-
sano, 2004; Iwatate et al, 2008). We also used
the same features here. They are divided into three
groups: modifier bunsetsu features, head bunsetsu
features, and gap features. A summary of the fea-
tures is described in Table 1.
6.4 Implementation
We implemented a parser and a tool for the av-
eraged perceptron in C++ and used them for ex-
periments. We wrote the main program of active
learning and some additional scripts in Perl and sh.
6.5 Settings of Active Learning
For initial seed sentences, first 500 sentences are
taken from the articles on January 1st. In ex-
periments about sentence wise selection, 500 sen-
tences are selected at each iteration of active learn-
ing and labeled5 and added into the training data.
In experiments about chunk wise selection 4000
pairs of bunsetsus, which are roughly equal to the
averaged number of bunsetsus in 500 sentences,
are selected at each iteration of active learning.
6.6 Dependency Accuracy
We use dependency accuracy as a performance
measure of a parser. The dependency accuracy is
the percentage of correct dependencies. This mea-
sure is commonly used for the Kyoto University
Corpus.
6.7 Results and Discussion
Learning Curves First we compare methods for
sentence wise selection. Figure 5 shows that MIN
is the best among them, while AVG is not good
and similar to PASSIVE. It is observed that active
learning with large margin classifiers also works
well for Sassano?s algorithm of Japanese depen-
dency parsing.
Next we compare chunk-wise selection with
sentence-wise one. The comparison is shown in
Figure 6. Note that we must carefully consider
how to count labeled examples. In sentence wise
selection we obviously count the number of sen-
tences. However, it is impossible to count such
number when we label bunsetsus pairs.
Therefore, we use the number of bunsetsus that
have an annotated head. Although we know this
may not be a completely fair comparison, we be-
lieve our choice in this experiment is reasonable
5In our experiments human annotators do not give labels.
Instead, labels are given virtually from correct ones that the
Kyoto University Corpus has.
360
Bunsetsu features for modifiers rightmost content word, rightmost function word, punctuation,
and heads parentheses, location (BOS or EOS)
Gap features distance (1, 2?5, or 6 ?), particles, parentheses, punctuation
Table 1: Features for deciding a dependency relation between two bunsetsus. Morphological features
for each word (morpheme) are major part-of-speech (POS), minor POS, conjugation type, conjugation
form, and surface form.
for assessing the effect of reduction by chunk-wise
selection.
In Figure 6 NAIVE has a better learning curve
compared to MIN at the early stage of learning.
However, the curve of NAIVE declines at the later
stage and gets worse than PASSIVE and MIN.
Why does this phenomenon occur? It is because
each bunsetsu pair is not independent and pairs in
the same sentence are related to each other. They
satisfy the constraints discussed in Section 3.2.
Furthermore, the algorithm we use, i.e., Sassano?s,
assumes these constraints and has the specific or-
der for processing bunsetsu pairs as we see in Fig-
ure 3. Let us consider the meaning of {j, i, ?O?} if
the head of the j-th bunsetsu is the k-th one such
that j < k < i. In the context of the algorithm in
Figure 3, {j, i, ?O?} actually means that the j-th
bunsetsu modifies th l-th one such that i < l. That
is ?O? does not simply mean that two bunsetsus
does not have a dependency relation. Therefore,
we should not generate {j, i, ?O?} in the case of
j < k < i. Such labeled instances are not needed
and the algorithm in Figure 4 does not generate
them even if a fully annotated sentence is given.
Based on the analysis above, we modified NAIVE
and defined MODSIMPLE, where unnecessary la-
beled examples are not generated.
Now let us compare NAIVE with MODSIMPLE
(Figure 7). MODSIMPLE is almost always better
than PASSIVE and does not cause a significant de-
terioration of accuracy unlike NAIVE.6
Comparison of MODSIMPLE and SYN is shown
in Figure 8. Both exhibit a similar curve. Figure 9
shows the same comparison in terms of required
queries to human annotators. It shows that SYN is
better than MODSIMPLE especially at the earlier
stage of active learning.
Reduction of Annotations Next we examined
the number of labeled bunsetsus to be required in
6We have to carefully see the curves of NAIVE and MOD-
SIMPLE. In Figure 7 at the early stage NAIVE is slightly
better than MODSIMPLE, while in Figure 9 NAIVE does not
outperform MODSIMPLE. This is due to the difference of the
way of accessing annotation efforts.
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
 0  1000 2000 3000 4000 5000 6000 7000 8000
A
cc
ur
ac
y
Number of Labeled Sentences
Passive
Min
Average
Figure 5: Learning curves of methods for sentence
wise selection
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
 0  10000  20000  30000  40000  50000
A
cc
ur
ac
y
Number of bunsetsus which have a head
Passive
Min
Naive
Figure 6: Learning curves of MIN (sentence-wise)
and NAIVE (chunk-wise).
361
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
 0  10000  20000  30000  40000  50000
A
cc
ur
ac
y
Number of bunsetsus which have a head
Passive
ModSimple
Naive
Figure 7: Learning curves of NAIVE, MODSIM-
PLE and PASSIVE in terms of the number of bun-
setsus that have a head.
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
 0  10000  20000  30000  40000  50000
A
cc
ur
ac
y
Number of bunsetsus which have a head
Passive
ModSimple
Syntax
Figure 8: Learning curves of MODSIMPLE and
SYN in terms of the number of bunsetsus which
have a head.
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
 0  10000  20000  30000  40000  50000  60000
A
cc
ur
ac
y
Number of queris to human annotators
ModSimple
Syntax
Naive
Figure 9: Comparison of MODSIMPLE and SYN
in terms of the number of queries to human anno-
tators
 0
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 40000
Passive Min Avg Naive Mod
Simple
Syn
# 
of
 b
un
se
ts
us
 th
at
 h
av
e 
a 
he
ad
Selection strategy
Figure 10: Number of labeled bunsetsus to be re-
quired to achieve an accuracy of over 88.3%.
 0
 5000
 10000
 15000
 20000
 25000
 0  1000 2000 3000 4000 5000 6000 7000 8000
N
um
be
r o
f S
up
po
rt 
Ve
ct
or
s
Number of Labeled Sentences
Passive
Min
Figure 11: Changes of number of support vectors
in sentence-wise active learning
 0
 5000
 10000
 15000
 20000
 25000
 0  10000  20000  30000  40000  50000  60000
N
um
be
r o
f S
up
po
rt 
Ve
ct
or
s
Number of Queries
ModSimple
Figure 12: Changes of number of support vectors
in chunk-wise active learning (MODSIMPLE)
362
order to achieve a certain level of accuracy. Fig-
ure 10 shows that the number of labeled bunsetsus
to achieve an accuracy of over 88.3% depending
on the active learning methods discussed in this
research.
PASSIVE needs 37766 labeled bunsetsus which
have a head to achieve an accuracy of 88.48%,
while SYN needs 13021 labeled bunsetsus to
achieve an accuracy of 88.56%. SYN requires only
34.4% of the labeled bunsetsu pairs that PASSIVE
requires.
Stopping Criteria It is known that increment
rate of the number of support vectors in SVM in-
dicates saturation of accuracy improvement dur-
ing iterations of active learning (Schohn and Cohn,
2000). It is interesting to examine whether the
observation for SVM is also useful for support
vectors7 of the averaged perceptron. We plotted
changes of the number of support vectors in the
cases of both PASSIVE and MIN in Figure 11 and
changes of the number of support vectors in the
case of MODSIMPLE in Figure 12. We observed
that the increment rate of support vectors mildly
gets smaller. However, it is not so clear as in the
case of text classification in (Schohn and Cohn,
2000).
Issues on Accessing the Total Cost of Annota-
tion In this paper, we assume that each annota-
tion cost for dependency relations is constant. It
is however not true in an actual annotation work.8
In addition, we have to note that it may be easier
to annotate a whole sentence than some bunsetsu
pairs in a sentence9. In a real annotation task, it
will be better to show a whole sentence to anno-
tators even when annotating some part of the sen-
tence.
Nevertheless, it is noteworthy that our research
shows the minimum number of annotations in
preparing training examples for Japanese depen-
dency parsing. The methods we have proposed
must be helpful when checking repeatedly anno-
tations that are important and might be wrong or
difficult to label while building an annotated cor-
7Following (Freund and Schapire, 1999), we use the term
?support vectors? for AP as well as SVM. ?Support vectors?
of AP means vectors which are selected in the training phase
and contribute to the prediction.
8Thus it is very important to construct models for estimat-
ing the actual annotation cost as Haertel et al (2008) do.
9Hwa (2004) discusses similar aspects of researches on
active learning.
pus. They also will be useful for domain adapta-
tion of a dependency parser.10
Applicability to Other Languages and Other
Parsing Algorithms We discuss here whether
or not the proposed methods and the experiments
are useful for other languages and other parsing
algorithms. First we take languages similar to
Japanese in terms of syntax, i.e., Korean and Mon-
golian. These two languages are basically head-
final languages and have similar constraints in
Section 3.2. Although no one has reported appli-
cation of (Sassano, 2004) to the languages so far,
we believe that similar parsing algorithms will be
applicable to them and the discussion in this study
would be useful.
On the other hand, the algorithm of (Sassano,
2004) cannot be applied to head-initial languages
such as English. If target languages are assumed
to be projective, the algorithm of (Nivre, 2003)
can be used. It is highly likely that we will invent
the effective use of finer-grained constituents, e.g.,
head-modifier pairs, rather than sentences in active
learning for Nivre?s algorithm with large margin
classifiers since Sassano?s seems to be a simplified
version of Nivre?s and they have several properties
in common. However, syntactic constraints in Eu-
ropean languages like English may be less helpful
than those in Japanese because their dependency
links do not have a single direction.
Even though the use of syntactic constraints is
limited, smaller constituents will still be useful for
other parsing algorithms that use some determin-
istic methods with machine learning-based classi-
fiers. There are many algorithms that have such
a framework, which include (Yamada and Mat-
sumoto, 2003) for English and (Kudo and Mat-
sumoto, 2002; Iwatate et al, 2008) for Japanese.
Therefore, effective use of smaller constituents in
active learning would not be limited to the specific
algorithm.
7 Conclusion
We have investigated that active learning methods
for Japanese dependency parsing. It is observed
that active learning of parsing with the averaged
perceptron, which is one of the large margin clas-
sifiers, works also well for Japanese dependency
analysis.
10Ohtake (2006) examines heuristic methods of selecting
sentences.
363
In addition, as far as we know, we are the first
to propose the active learning methods of using
partial dependency relations in a given sentence
for parsing and we have evaluated the effective-
ness of our methods. Furthermore, we have tried
to obtain more labeled examples from precious la-
beled ones that annotators give by utilizing syntac-
tic constraints of the Japanese language. It is note-
worthy that linguistic constraints have been shown
useful for reducing annotations in active learning
for NLP.
Experimental results show that our proposed
methods have improved considerably the learning
curve of Japanese dependency parsing.
We are currently building a new annotated cor-
pus with an annotation tool. We have a plan to in-
corporate our proposed methods to the annotation
tool. We will use it to accelerate building of the
large annotated corpus to improved our Japanese
parser.
It would be interesting to explore the use of par-
tially labeled constituents in a sentence in another
language, e.g., English, for active learning.
Acknowledgements
We would like to thank the anonymous review-
ers and Tomohide Shibata for their valuable com-
ments.
References
Jason Baldridge and Miles Osborne. 2004. Active
learning and the total cost of annotation. In Proc.
of EMNLP 2004, pages 9?16.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
costs of sampling methods in active learning for an-
notation. In Proc. of ACL-08: HLT, short papers
(Companion Volume), pages 65?68.
Shinkichi Hashimoto. 1934. Essentials of Japanese
Grammar (Kokugoho Yousetsu) (in Japanese).
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
Masakazu Iwatate, Masayuki Asahara, and Yuji Mat-
sumoto. 2008. Japanese dependency parsing us-
ing a tournament model. In Proc. of COLING 2008,
pages 361?368.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proc. of EMNLP/VLC 2000, pages 18?
25.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of CoNLL-2002, pages 63?69.
Sadao Kurohashi and Makoto Nagao. 1998. Building a
Japanese parsed corpus while improving the parsing
system. In Proc. of LREC-1998, pages 719?724.
Florian Laws and Hinrich Schu?tze. 2008. Stopping cri-
teria for active learning of named entity recognition.
In Proc. of COLING 2008, pages 465?472.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In
Proc. of the Seventeenth Annual International ACM-
SIGIR Conference on Research and Development in
Information Retrieval, pages 3?12.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. of ACL-2005, pages
523?530.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proc. of IWPT-03,
pages 149?160.
Kiyonori Ohtake. 2006. Analysis of selective strate-
gies to build a dependency-analyzed corpus. In
Proc. of COLING/ACL 2006 Main Conf. Poster Ses-
sions, pages 635?642.
Eric Ringger, Peter McClanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, Kevin
Seppi, and Deryle Lonsdale. 2007. Active learn-
ing for part-of-speech tagging: Accelerating corpus
annotation. In Proc. of the Linguistic Annotation
Workshop, pages 101?108.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for Japanese
word segmentation. In Proc. of ACL-2002, pages
505?512.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proc. of COLING 2004, pages
8?14.
Greg Schohn and David Cohn. 2000. Less is more:
Active learning with support vector machines. In
Proc. of ICML-2000, pages 839?846.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proc. of COLT ?92, pages
287?294.
Min Tang, Xaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proc. of ACL-2002, pages 120?127.
364
Simon Tong and Daphne Koller. 2000. Support vec-
tor machine active learning with applications to text
classification. In Proc. of ICML-2000, pages 999?
1006.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 1999. Japanese dependency structure analy-
sis based on maximum entropy models. In Proc. of
EACL-99, pages 196?203.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT 2003, pages 195?206.
Jingbo Zhu and Eduard Hovy. 2007. Active learning
for word sense disambiguation with methods for ad-
dressing the class imbalance problem. In Proc. of
EMNLP-CoNLL 2007, pages 783?790.
365
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087?1097,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Extracting Paraphrases from Definition Sentences on the Web
Chikara Hashimoto? Kentaro Torisawa? Stijn De Saeger?
Jun?ichi Kazama? Sadao Kurohashi?
? ? ? ? National Institute of Information and Communications Technology
Kyoto, 619-0237, JAPAN
? ?Graduate School of Informatics, Kyoto University
Kyoto, 606-8501, JAPAN
{? ch,? torisawa, ? stijn,? kazama}@nict.go.jp
?kuro@i.kyoto-u.ac.jp
Abstract
We propose an automatic method of extracting
paraphrases from definition sentences, which
are also automatically acquired from the Web.
We observe that a huge number of concepts
are defined in Web documents, and that the
sentences that define the same concept tend
to convey mostly the same information using
different expressions and thus contain many
paraphrases. We show that a large number
of paraphrases can be automatically extracted
with high precision by regarding the sentences
that define the same concept as parallel cor-
pora. Experimental results indicated that with
our method it was possible to extract about
300,000 paraphrases from 6? 108 Web docu-
ments with a precision rate of about 94%.
1 Introduction
Natural language allows us to express the same in-
formation in many ways, which makes natural lan-
guage processing (NLP) a challenging area. Ac-
cordingly, many researchers have recognized that
automatic paraphrasing is an indispensable compo-
nent of intelligent NLP systems (Iordanskaja et al,
1991; McKeown et al, 2002; Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Kauchak and Barzi-
lay, 2006; Callison-Burch et al, 2006) and have tried
to acquire a large amount of paraphrase knowledge,
which is a key to achieving robust automatic para-
phrasing, from corpora (Lin and Pantel, 2001; Barzi-
lay and McKeown, 2001; Shinyama et al, 2002;
Barzilay and Lee, 2003).
We propose a method to extract phrasal para-
phrases from pairs of sentences that define the same
concept. The method is based on our observation
that two sentences defining the same concept can
be regarded as a parallel corpus since they largely
convey the same information using different expres-
sions. Such definition sentences abound on the Web.
This suggests that we may be able to extract a large
amount of phrasal paraphrase knowledge from the
definition sentences on the Web.
For instance, the following two sentences, both of
which define the same concept ?osteoporosis?, in-
clude two pairs of phrasal paraphrases, which are
indicated by underlines 1? and 2?, respectively.
(1) a. Osteoporosis is a disease that 1? decreases the
quantity of bone and 2? makes bones fragile.
b. Osteoporosis is a disease that 1? reduces bone
mass and 2? increases the risk of bone fracture.
We define paraphrase as a pair of expressions be-
tween which entailment relations of both directions
hold. (Androutsopoulos and Malakasiotis, 2010).
Our objective is to extract phrasal paraphrases
from pairs of sentences that define the same con-
cept. We propose a supervised method that exploits
various kinds of lexical similarity features and con-
textual features. Sentences defining certain concepts
are acquired automatically on a large scale from the
Web by applying a quite simple supervised method.
Previous methods most relevant to our work
used parallel corpora such as multiple translations
of the same source text (Barzilay and McKeown,
2001) or automatically acquired parallel news texts
(Shinyama et al, 2002; Barzilay and Lee, 2003;
Dolan et al, 2004). The former requires a large
amount of manual labor to translate the same texts
1087
in several ways. The latter would suffer from the
fact that it is not easy to automatically retrieve large
bodies of parallel news text with high accuracy. On
the contrary, recognizing definition sentences for
the same concept is quite an easy task at least for
Japanese, as we will show, and we were able to find
a huge amount of definition sentence pairs from nor-
mal Web texts. In our experiments, about 30 million
definition sentence pairs were extracted from 6?108
Web documents, and the estimated number of para-
phrases recognized in the definition sentences using
our method was about 300,000, for a precision rate
of about 94%. Also, our experimental results show
that our method is superior to well-known compet-
ing methods (Barzilay and McKeown, 2001; Koehn
et al, 2007) for extracting paraphrases from defini-
tion sentence pairs.
Our evaluation is based on bidirectional check-
ing of entailment relations between paraphrases that
considers the context dependence of a paraphrase.
Note that using definition sentences is only the
beginning of our research on paraphrase extraction.
We have a more general hypothesis that sentences
fulfilling the same pragmatic function (e.g. defini-
tion) for the same topic (e.g. osteoporosis) convey
mostly the same information using different expres-
sions. Such functions other than definition may in-
clude the usage of the same Linux command, the
recipe for the same cuisine, or the description of re-
lated work on the same research issue.
Section 2 describes related works. Section 3
presents our proposed method. Section 4 reports on
evaluation results. Section 5 concludes the paper.
2 Related Work
The existing work for paraphrase extraction is cat-
egorized into two groups. The first involves a dis-
tributional similarity approach pioneered by Lin and
Pantel (2001). Basically, this approach assumes that
two expressions that have a large distributional simi-
larity are paraphrases. There are also variants of this
approach that address entailment acquisition (Geffet
and Dagan, 2005; Bhagat et al, 2007; Szpektor and
Dagan, 2008; Hashimoto et al, 2009). These meth-
ods can be applied to a normal monolingual corpus,
and it has been shown that a large number of para-
phrases or entailment rules could be extracted. How-
ever, the precision of these methods has been rela-
tively low. This is due to the fact that the evidence,
i.e., distributional similarity, is just indirect evidence
of paraphrase/entailment. Accordingly, these meth-
ods occasionally mistake antonymous pairs for para-
phrases/entailment pairs, since an expression and its
antonymous counterpart are also likely to have a
large distributional similarity. Another limitation of
these methods is that they can find only paraphrases
consisting of frequently observed expressions since
they must have reliable distributional similarity val-
ues for expressions that constitute paraphrases.
The second category is a parallel corpus approach
(Barzilay and McKeown, 2001; Shinyama et al,
2002; Barzilay and Lee, 2003; Dolan et al, 2004).
Our method belongs to this category. This approach
aligns expressions between two sentences in par-
allel corpora, based on, for example, the overlap
of words/contexts. The aligned expressions are as-
sumed to be paraphrases. In this approach, the ex-
pressions do not need to appear frequently in the
corpora. Furthermore, the approach rarely mistakes
antonymous pairs for paraphrases/entailment pairs.
However, its limitation is the difficulty in preparing
a large amount of parallel corpora, as noted before.
We avoid this by using definition sentences, which
can be easily acquired on a large scale from theWeb,
as parallel corpora.
Murata et al (2004) used definition sentences in
two manually compiled dictionaries, which are con-
siderably fewer in the number of definition sen-
tences than those on the Web. Thus, the coverage of
their method should be quite limited. Furthermore,
the precision of their method is much poorer than
ours as we report in Section 4.
For a more extensive survey on paraphrasing
methods, see Androutsopoulos and Malakasiotis
(2010) and Madnani and Dorr (2010).
3 Proposed method
Our method, targeting the Japanese language, con-
sists of two steps: definition sentence acquisition
and paraphrase extraction. We describe them below.
3.1 Definition sentence acquisition
We acquire sentences that define a concept (defini-
tion sentences) as in Example (2), which defines ??
1088
???? (osteoporosis), from the 6?108 Web pages
(Akamine et al, 2010) and the Japanese Wikipedia.
(2) ??????????????????????
(Osteoporosis is a disease that makes bones fragile.)
Fujii and Ishikawa (2002) developed an unsuper-
vised method to find definition sentences from the
Web using 18 sentential templates and a language
model constructed from an encyclopedia. On the
other hand, we developed a supervised method to
achieve a higher precision.
We use one sentential template and an SVM clas-
sifier. Specifically, we first collect definition sen-
tence candidates by a template ??NP??.*?, where
? is the beginning of sentence and NP is the noun
phrase expressing the concept to be defined followed
by a particle sequence, ??? (comitative) and ???
(topic) (and optionally followed by comma), as ex-
emplified in (2). As a result, we collected 3,027,101
sentences. Although the particle sequence tends
to mark the topic of the definition sentence, it can
also appear in interrogative sentences and normal as-
sertive sentences in which a topic is strongly empha-
sized. To remove such non-definition sentences, we
classify the candidate sentences using an SVM clas-
sifier with a polynominal kernel (d = 2).1 Since
Japanese is a head-final language and we can judge
whether a sentence is interrogative or not from the
last words in the sentence, we included morpheme
N -grams and bag-of-words (with the window size
of N ) at the end of sentences in the feature set. The
features are also useful for confirming that the head
verb is in the present tense, which definition sen-
tences should be. Also, we added the morpheme
N -grams and bag-of-words right after the particle
sequence in the feature set since we observe that
non-definition sentences tend to have interrogative
related words like ??? (what) or ???? ((what) on
earth) right after the particle sequence. We chose 5
as N from our preliminary experiments.
Our training data was constructed from 2,911 sen-
tences randomly sampled from all of the collected
sentences. 61.1% of them were labeled as positive.
In the 10-fold cross validation, the classifier?s ac-
curacy, precision, recall, and F1 were 89.4, 90.7,
1We use SVMlight available at http://svmlight.
joachims.org/.
92.2, and 91.4, respectively. Using the classifier,
we acquired 1,925,052 positive sentences from all
of the collected sentences. After adding definition
sentences from Wikipedia articles, which are typi-
cally the first sentence of the body of each article
(Kazama and Torisawa, 2007), we obtained a total
of 2,141,878 definition sentence candidates, which
covered 867,321 concepts ranging from weapons to
rules of baseball. Then, we coupled two definition
sentences whose defined concepts were the same
and obtained 29,661,812 definition sentence pairs.
Obviously, our method is tailored to Japanese. For
a language-independent method of definition acqui-
sition, see Navigli and Velardi (2010) as an example.
3.2 Paraphrase extraction
Paraphrase extraction proceeds as follows. First,
each sentence in a pair is parsed by the depen-
dency parser KNP2 and dependency tree frag-
ments that constitute linguistically well-formed con-
stituents are extracted. The extracted dependency
tree fragments are called candidate phrases here-
after. We restricted candidate phrases to predicate
phrases that consist of at least one dependency re-
lation, do not contain demonstratives, and in which
all the leaf nodes are nominal and all of the con-
stituents are consecutive in the sentence. KNP indi-
cates whether each candidate phrase is a predicate
based on the POS of the head morpheme. Then,
we check all the pairs of candidate phrases between
two definition sentences to find paraphrase pairs.3
In (1), repeated in (3), candidate phrase pairs to be
checked include ( 1? decreases the quantity of bone,
1? reduces bone mass), ( 1? decreases the quantity
of bone, 2? increases the risk of bone fracture), ( 2?
makes bones fragile, 1? reduces bone mass), and ( 2?
makes bones fragile, 2? increases the risk of bone
fracture).
(3) a. Osteoporosis is a disease that 1? decreases the
quantity of bone and 2? makes bones fragile.
b. Osteoporosis is a disease that 1? reduces bone
mass and 2? increases the risk of bone fracture.
2http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp.html.
3Our method discards candidate phrase pairs in which one
subsumes the other in terms of their character string, or the dif-
ference is only one proper noun like ?toner cartridges that Ap-
ple Inc. made? and ?toner cartridges that Xerox made.? Proper
nouns are recognized by KNP.
1089
f1 The ratio of the number of morphemes shared between two candidate phrases to the number of all of the morphemes in the two phrases.
f2 The ratio of the number of a candidate phrase?s morphemes, for which there is a morpheme with small edit distance (1 in our experiment) in
another candidate phrase, to the number of all of the morphemes in the two phrases. Note that Japanese has many orthographical variations
and edit distance is useful for identifying them.
f3 The ratio of the number of a candidate phrase?s morphemes, for which there is a morpheme with the same pronunciation in another candidate
phrase, to the number of all of the morphemes in the two phrases. Pronunciation is also useful for identifying orthographic variations.
Pronunciation is given by KNP.
f4 The ratio of the number of morphemes of a shorter candidate phrase to that of a longer one.
f5 The identity of the inflected form of the head morpheme between two candidate phrases: 1 if they are identical, 0 otherwise.
f6 The identity of the POS of the head morpheme between two candidate phrases: 1 or 0.
f7 The identity of the inflection (conjugation) of the head morpheme between two candidate phrases: 1 or 0.
f8 The ratio of the number of morphemes that appear in a candidate phrase segment of a definition sentence s1 and in a segment that is NOT a
part of the candidate phrase of another definition sentence s2 to the number of all of the morphemes of s1?s candidate phrase, i.e. how many
extra morphemes are incorporated into s1?s candidate phrase.
f9 The reversed (s1 ? s2) version of f8.
f10 The ratio of the number of parent dependency tree fragments that are shared by two candidate phrases to the number of all of the parent de-
pendency tree fragments of the two phrases. Dependency tree fragments are represented by the pronunciation of their component morphemes.
f11 A variation of f10; tree fragments are represented by the base form of their component morphemes.
f12 A variation of f10; tree fragments are represented by the POS of their component morphemes.
f13 The ratio of the number of unigrams (morphemes) that appear in the child context of both candidate phrases to the number of all of the child
context morphemes of both candidate phrases. Unigrams are represented by the pronunciation of the morpheme.
f14 A variation of f13; unigrams are represented by the base form of the morpheme.
f15 A variation of f14; the numerator is the number of child context unigrams that are adjacent to both candidate phrases.
f16 The ratio of the number of trigrams that appear in the child context of both candidate phrases to the number of all of the child context
morphemes of both candidate phrases. Trigrams are represented by the pronunciation of the morpheme.
f17 Cosine similarity between two definition sentences from which a candidate phrase pair is extracted.
Table 1: Features used by paraphrase classifier.
The paraphrase checking of candidate phrase
pairs is performed by an SVM classifier with a linear
kernel that classifies each pair of candidate phrases
into a paraphrase or a non-paraphrase.4 Candidate
phrase pairs are ranked by their distance from the
SVM?s hyperplane. Features for the classifier are
based on our observation that two candidate phrases
tend to be paraphrases if the candidate phrases them-
selves are sufficiently similar and/or their surround-
ing contexts are sufficiently similar. Table 1 lists the
features used by the classifier.5 Basically, they rep-
resent either the similarity of candidate phrases (f1-
9) or that of their contexts (f10-17). We think that
they have various degrees of discriminative power,
and thus we use the SVM to adjust their weights.
Figure 1 illustrates features f8-12, for which you
may need supplemental remarks. English is used for
ease of explanation. In the figure, f8 has a positive
value since the candidate phrase of s1 contains mor-
phemes ?of bone?, which do not appear in the can-
4We use SVMperf available at http://svmlight.
joachims.org/svm perf.html.
5In the table, the parent context of a candidate phrase con-
sists of expressions that appear in ancestor nodes of the candi-
date phrase in terms of the dependency structure of the sentence.
Child contexts are defined similarly.
Figure 1: Illustration of features f8-12.
didate phrase of s2 but do appear in the other part
of s2, i.e. they are extra morphemes for s1?s candi-
date phrase. On the other hand, f9 is zero since there
is no such extra morpheme in s2?s candidate phrase.
Also, features f10-12 have positive values since the
two candidate phrases share two parent dependency
tree fragments, (that increases) and (of fracture).
We have also tried the following features, which
we do not detail due to space limitation: the sim-
ilarity of candidate phrases based on semantically
similar nouns (Kazama and Torisawa, 2008), entail-
ing/entailed verbs (Hashimoto et al, 2009), and the
identity of the pronunciation and base form of the
head morpheme; N -grams (N=1,2,3) of child and
parent contexts represented by either the inflected
form, base form, pronunciation, or POS of mor-
1090
Original definition sentence pair (s1, s2) Paraphrased definition sentence pair (s?1, s?2)
s1: Osteoporosis is a disease that reduces bonemass and makes bones
fragile.
s?1: Osteoporosis is a disease that decreases the quantity of bone and
makes bones fragile.
s2: Osteoporosis is a disease that decreases the quantity of bone and
increases the risk of bone fracture.
s?2: Osteoporosis is a disease that reduces bone mass and increases
the risk of bone fracture.
Figure 2: Bidirectional checking of entailment relation (?) of p1 ? p2 and p2 ? p1. p1 is ?reduces bone mass?
in s1 and p2 is ?decreases the quantity of bone? in s2. p1 and p2 are exchanged between s1 and s2 to generate
corresponding paraphrased sentences s?1 and s?2. p1 ? p2 (p2 ? p1) is verified if s1 ? s?1 (s2 ? s?2) holds. In this
case, both of them hold. English is used for ease of explanation.
pheme; parent/child dependency tree fragments rep-
resented by either the inflected form, base form, pro-
nunciation, or POS; adjacent versions (cf. f15) of
N -gram features and parent/child dependency tree
features. These amount to 78 features, but we even-
tually settled on the 17 features in Table 1 through
ablation tests to evaluate the discriminative power
of each feature.
The ablation tests were conducted using training
data that we prepared. In preparing the training data,
we faced the problem that the completely random
sampling of candidate paraphrase pairs provided us
with only a small number of positive examples.
Thus, we automatically collected candidate para-
phrase pairs that were expected to have a high like-
lihood of being positive as examples to be labeled.
The likelihood was calculated by simply summing
all of the 78 feature values that we have tried, since
they indicate the likelihood of a given candidate
paraphrase pair?s being a paraphrase. Note that val-
ues of the features f8 and f9 are weighted with ?1,
since they indicate the unlikelihood. Specifically,
we first randomly sampled 30,000 definition sen-
tence pairs from the 29,661,812 pairs, and collected
3,000 candidate phrase pairs that had the highest
likelihood from them. The manual labeling of each
candidate phrase pair (p1, p2) was based on bidirec-
tional checking of entailment relation, p1 ? p2 and
p2 ? p1, with p1 and p2 embedded in contexts.
This scheme is similar to the one proposed by
Szpektor et al (2007). We adopt this scheme since
paraphrase judgment might be unstable between an-
notators unless they are given a particular context
based on which they make a judgment. As de-
scribed below, we use definition sentences as con-
texts. We admit that annotators might be biased by
this in some unexpected way, but we believe that
this is a more stable method than that without con-
texts. The labeling process is as follows. First, from
each candidate phrase pair (p1, p2) and its source
definition sentence pair (s1, s2), we create two para-
phrase sentence pairs (s?1, s?2) by exchanging p1 and
p2 between s1 and s2. Then, annotators check if s1
entails s?1 and s2 entails s?2 so that entailment rela-
tions of both directions p1 ? p2 and p2 ? p1 are
checked. Figure 2 shows an example of bidirectional
checking. In this example, both entailment relations,
s1 ? s?1 and s2 ? s?2, hold, and thus the candidate
phrase pair (p1, p2) is judged as positive. We used
(p1, p2), for which entailment relations of both di-
rections held, as positive examples (1,092 pairs) and
the others as negative ones (1,872 pairs).6
We built the paraphrase classifier from the train-
ing data. As mentioned, candidate phrase pairs were
ranked by the distance from the SVM?s hyperplane.
4 Experiment
In this paper, our claims are twofold.
I. Definition sentences on the Web are a treasure
trove of paraphrase knowledge (Section 4.2).
II. Our method of paraphrase acquisition from
definition sentences is more accurate than well-
known competing methods (Section 4.1).
We first verify claim II by comparing our method
with that of Barzilay and McKeown (2001) (BM
method), Moses7 (Koehn et al, 2007) (SMT
method), and that of Murata et al (2004) (Mrt
method). The first two methods are well known for
accurately extracting semantically equivalent phrase
pairs from parallel corpora.8 Then, we verify claim
6The remaining 36 pairs were discarded as they contained
garbled characters of Japanese.
7http://www.statmt.org/moses/
8As anonymous reviewers pointed out, they are unsuper-
vised methods and thus unable to be adapted to definition sen-
1091
I by comparing definition sentence pairs with sen-
tence pairs that are acquired from the Web using Ya-
hoo!JAPANAPI9 as a paraphrase knowledge source.
In the latter data set, two sentences of each pair
are expected to be semantically similar regardless of
whether they are definition sentences. Both sets con-
tain 100,000 pairs.
Three annotators (not the authors) checked evalu-
ation samples. Fleiss? kappa (Fleiss, 1971) was 0.69
(substantial agreement (Landis and Koch, 1977)).
4.1 Our method vs. competing methods
In this experiment, paraphrase pairs are extracted
from 100,000 definition sentence pairs that are ran-
domly sampled from the 29,661,812 pairs. Before
reporting the experimental results, we briefly de-
scribe the BM, SMT, and Mrt methods.
BM method Given parallel sentences like multi-
ple translations of the same source text, the BM
method works iteratively as follows. First, it collects
from the parallel sentences identical word pairs and
their contexts (POS N -grams with indices indicat-
ing corresponding words between paired contexts)
as positive examples and those of different word
pairs as negative ones. Then, each context is ranked
based on the frequency with which it appears in pos-
itive (negative) examples. The most likely K posi-
tive (negative) contexts are used to extract positive
(negative) paraphrases from the parallel sentences.
Extracted positive (negative) paraphrases and their
morpho-syntactic patterns are used to collect addi-
tional positive (negative) contexts. All the positive
(negative) contexts are ranked, and additional para-
phrases and their morpho-syntactic patterns are ex-
tracted again. This iterative process finishes if no
further paraphrase is extracted or the number of iter-
ations reaches a predefined threshold T . In this ex-
periment, following Barzilay and McKeown (2001),
K is 10 and N is 1 to 3. The value of T is not given
in their paper. We chose 3 as its value based on our
preliminary experiments. Note that paraphrases ex-
tracted by this method are not ranked.
tences. Nevertheless, we believe that comparing these methods
with ours is very informative, since they are known to be accu-
rate and have been influential.
9http://developer.yahoo.co.jp/webapi/
SMT method Our SMT method uses Moses
(Koehn et al, 2007) and extracts a phrase table, a
set of two phrases that are translations of each other,
given a set of two sentences that are translations of
each other. If you give Moses monolingual parallel
sentence pairs, it should extract a set of two phrases
that are paraphrases of each other. In this experi-
ment, default values were used for all parameters.
To rank extracted phrase pairs, we assigned each of
them the product of two phrase translation probabil-
ities of both directions that were given by Moses.
For other SMT methods, see Quirk et al (2004) and
Bannard and Callison-Burch (2005) among others.
Mrt method Murata et al (2004) proposed a
method to extract paraphrases from two manually
compiled dictionaries. It simply regards a difference
between two definition sentences of the same word
as a paraphrase candidate. Paraphrase candidates are
ranked according to an unsupervised scoring scheme
that implements their assumption. They assume that
a paraphrase candidate tends to be a valid paraphrase
if it is surrounded by infrequent strings and/or if it
appears multiple times in the data.
In this experiment, we evaluated the unsupervised
version of our method in addition to the supervised
one described in Section 3.2, in order to compare
it fairly with the other methods. The unsupervised
method works in the same way as the supervised
one, except that it ranks candidate phrase pairs by
the sum of all 17 feature values, instead of the dis-
tance from the SVM?s hyperplane. In other words,
no supervised learning is used. All the feature val-
ues are weighted with 1, except for f8 and f9, which
are weighted with ?1 since they indicate the unlike-
lihood of a candidate phrase pair being paraphrases.
BM, SMT, Mrt, and the two versions of our method
were used to extract paraphrase pairs from the same
100,000 definition sentence pairs.
Evaluation scheme Evaluation of each para-
phrase pair (p1, p2) was based on bidirectional
checking of entailment relations p1 ? p2 and p2 ?
p1 in a way similar to the labeling of the training
data. The difference is that contexts for evaluation
are two sentences that are retrieved from the Web
and contain p1 and p2, instead of definition sen-
tences from which p1 and p2 are extracted. This
1092
is intended to check whether extracted paraphrases
are also valid for contexts other than those from
which they are extracted. The evaluation proceeds
as follows. For the top m paraphrase pairs of each
method (in the case of the BM method, randomly
sampled m pairs were used, since the method does
not rank paraphrase pairs), we retrieved a sentence
pair (s1, s2) for each paraphrase pair (p1, p2) from
the Web, such that s1 contains p1 and s2 contains p2.
In doing so, we make sure that neither s1 nor s2 are
the definition sentences from which p1 and p2 are
extracted. For each method, we randomly sample
n samples from all of the paraphrase pairs (p1, p2)
for which both s1 and s2 are retrieved. Then, from
each (p1, p2) and (s1, s2), we create two paraphrase
sentence pairs (s?1, s?2) by exchanging p1 and p2 be-
tween s1 and s2. All samples, each consisting of
(p1, p2), (s1, s2), and (s?1, s?2), are checked by three
human annotators to determine whether s1 entails
s?1 and s2 entails s?2 so that entailment relations of
both directions are verified. In advance of evaluation
annotation, all the evaluation samples are shuffled
so that the annotators cannot find out which sample
is given by which method for fairness. We regard
each paraphrase pair as correct if at least two annota-
tors judge that entailment relations of both directions
hold for it. You may wonder whether only one pair
of sentences (s1, s2) is enough for evaluation since a
correct (wrong) paraphrase pair might be judged as
wrong (correct) accidentally. Nevertheless, we sup-
pose that the final evaluation results are reliable if
the number of evaluation samples is sufficient. In
this experiment, m is 5,000 and n is 200. We use
Yahoo!JAPAN API to retrieve sentences.
Graph (a) in Figure 3 shows a precision curve
for each method. Sup and Uns respectively indi-
cate the supervised and unsupervised versions of our
method. The figure indicates that Sup outperforms
all the others and shows a high precision rate of
about 94% at the top 1,000. Remember that this
is the result of using 100,000 definition sentence
pairs. Thus, we estimate that Sup can extract about
300,000 paraphrase pairs with a precision rate of
about 94%, if we use all 29,661,812 definition sen-
tence pairs that we acquired.
Furthermore, we measured precision after trivial
paraphrase pairs were discarded from the evaluation
samples of each method. A candidate phrase pair
Definition sentence pairs Sup Uns BM SMT Mrt
with trivial 1,381,424 24,049 9,562 18,184
without trivial 1,377,573 23,490 7,256 18,139
Web sentence pairs Sup Uns BM SMT Mrt
with trivial 277,172 5,101 4,586 4,978
without trivial 274,720 4,399 2,342 4,958
Table 2: Number of extracted paraphrases.
(p1, p2) is regarded as trivial if the pronunciation is
the same between p1 and p2,10 or all of the con-
tent words contained in p1 are the same as those
of p2. Graph (b) gives a precision curve for each
method. Again, Sup outperforms the others too, and
maintains a precision rate of about 90% until the top
1,000. These results support our claim II.
The upper half of Table 2 shows the number of
extracted paraphrases with/without trivial pairs for
each method.11 Sup and Uns extracted many more
paraphrases. It is noteworthy that Sup performed the
best in terms of both precision rate and the number
of extracted paraphrases.
Table 3 shows examples of correct and incorrect
outputs of Sup. As the examples indicate, many of
the extracted paraphrases are not specific to defini-
tion sentences and seem very reusable. However,
there are few paraphrases involving metaphors or id-
ioms in the outputs due to the nature of definition
sentences. In this regard, we do not claim that our
method is almighty. We agree with Sekine (2005)
who claims that several different methods are re-
quired to discover a wider variety of paraphrases.
In graphs (a) and (b), the precision of the SMT
method goes up as rank goes down. This strange be-
havior is due to the scoring by Moses that worked
poorly for the data; it gave 1.0 to 82.5% of all the
samples, 38.8% of which were incorrect. We suspect
SMTmethods are poor at monolingual alignment for
paraphrasing or entailment tasks since, in the tasks,
data is much noisier than that used for SMT. See
MacCartney et al (2008) for similar discussion.
4.2 Definition pairs vs. Web sentence pairs
To collect Web sentence pairs, first, we randomly
sampled 1.8 million sentences from the Web corpus.
10There are many kinds of orthographic variants in Japanese,
which can be identified by their pronunciation.
11We set no threshold for candidate phrase pairs of each
method, and counted all the candidate phrase pairs in Table 2.
1093
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_def?
?Uns_def?
?SMT_def?
?BM_def?
?Mrt_def?
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_def_n?
?Uns_def_n?
?SMT_def_n?
?BM_def_n?
?Mrt_def_n?
(a) Definition sentence pairs with trivial paraphrases (b) Definition sentence pairs without trivial paraphrases
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_www?
?Uns_www?
?SMT_www?
?BM_www?
?Mrt_www?
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_www_n?
?Uns_www_n?
?SMT_www_n?
?BM_www_n?
?Mrt_www_n?
(c) Web sentence pairs with trivial paraphrases (d) Web sentence pairs without trivial paraphrases
Figure 3: Precision curves of paraphrase extraction.
Rank Paraphrase pair
Correct
13 ?????????????? (send a message to the e-mail address)????????????????? (send
an e-mail message to the e-mail address)
19 ????????? (requested by a customer)?????????? (commissioned by a customer)
70 ?????????? (describe the fiscal condition of company) ??????????? (indicate the fiscal state
of company)
112 ???????????? (get information)???????? (get news)
656 ???????? (it is a convention)????????? (it is a rule)
841 ??????????????? (represent the energy scale of earthquake)????????? (represent the scale
of earthquake)
929 ???????? (cause the oxidation of cells)????????? (cause cellular aging)
1,553 ??????? (remove dead skin cells)??????? (peel off dead skin cells)
2,243 ????????? (required for the development of fetus)??????????????? (indispensable for the
growth and development of fetus)
2,855 ??????? (correct eyesight)???????? (perform eyesight correction)
2,931 ????????? (call it even)?????????? (call it quits)
3,667 ?????????????? (accumulated on a hard disk)?????????????????? (stored on a
hard disk drive)
4,870 ????????? (excrete harmful substance)?????????? (discharge harmful toxin)
5,501 ????????????????????????? (mount two processor cores on one CPU)????????
????????????????? (build two processor cores into one package)
10,675 ??????? (trade foreign currencies)???????? (exchange one currency for another)
112,819 ??????????? (become a regular staff member of the company where (s)he has worked as a temp) ???
????????? (employed by the company where (s)he has worked as a temp)
193,553 ????????????? (access Web sites)??????????? (visit WWW sites)
Incorrect
903 ?????????? (send to a Web browser)??????????? (send to a PC)
2,530 ?????? (intend to balance)?????????? (intend to refresh)
3,008 ???????????? (unable to digest with digestive enzymes)???????????? (hard to digest with
digestive enzymes)
Table 3: Examples of correct and incorrect paraphrases extracted by our supervised method with their rank.
1094
We call them sampled sentences. Then, using Ya-
hoo!JAPANAPI, we retrieved up to 20 snippets rele-
vant to each sampled sentence using all of the nouns
in each sentence as a query. After that, each snippet
was split into sentences, which we call snippet sen-
tences. We paired a sampled sentence and a snippet
sentence that was the most similar to the sampled
sentence. Similarity is the number of nouns shared
by the two sentences. Finally, we randomly sampled
100,000 pairs from all the pairs.
Paraphrase pairs were extracted from the Web
sentence pairs by using BM, SMT, Mrt and the su-
pervised and unsupervised versions of our method.
The features used with our methods were selected
from all of the 78 features mentioned in Section 3.2
so that they performed well for Web sentence pairs.
Specifically, the features were selected by ablation
tests using training data that was tailored to Web
sentence pairs. The training data consisted of 2,741
sentence pairs that were collected in the same way as
the Web sentence pairs and was labeled in the same
way as described in Section 3.2.
Graph (c) of Figure 3 shows precision curves. We
also measured precision without trivial pairs in the
same way as the previous experiment. Graph (d)
shows the results. The lower half of Table 2 shows
the number of extracted paraphrases with/without
trivial pairs for each method.
Note that precision figures of our methods in
graphs (c) and (d) are lower than those of our meth-
ods in graphs (a) and (b). Additionally, none of the
methods achieved a precision rate of 90% using Web
sentence pairs.12 We think that a precision rate of
at least 90% would be necessary if you apply auto-
matically extracted paraphrases to NLP tasks with-
out manual annotation. Only the combination of Sup
and definition sentence pairs achieved that precision.
Also note that, for all of the methods, the numbers
of extracted paraphrases from Web sentence pairs
are fewer than those from definition sentence pairs.
From all of these results, we conclude that our
claim I is verified.
12Precision of SMT is unexpectedly good. We found some
Web sentence pairs consisting of two mostly identical sentences
on rare occasions. The method worked relatively well for them.
5 Conclusion
We proposed a method of extracting paraphrases
from definition sentences on the Web. From the ex-
perimental results, we conclude that the following
two claims of this paper are verified.
1. Definition sentences on the Web are a treasure
trove of paraphrase knowledge.
2. Our method extracts many paraphrases from
the definition sentences on the Web accurately;
it can extract about 300,000 paraphrases from
6 ? 108 Web documents with a precision rate
of about 94%.
Our future work is threefold. First, we will release
extracted paraphrases from all of the 29,661,812
definition sentence pairs that we acquired, after hu-
man annotators check their validity. The result will
be available through the ALAGIN forum.13
Second, we plan to induce paraphrase rules
from paraphrase instances. Though our method
can extract a variety of paraphrase instances on
a large scale, their coverage might be insufficient
for real NLP applications since some paraphrase
phenomena are highly productive. Therefore, we
need paraphrase rules in addition to paraphrase in-
stances. Barzilay and McKeown (2001) induced
simple POS-based paraphrase rules from paraphrase
instances, which can be a good starting point.
Finally, as mentioned in Section 1, the work in
this paper is only the beginning of our research on
paraphrase extraction. We are trying to extract far
more paraphrases from a set of sentences fulfilling
the same pragmatic function (e.g. definition) for the
same topic (e.g. osteoporosis) on the Web. Such
functions other than definition may include the us-
age of the same Linux command, the recipe for the
same cuisine, or the description of related work on
the same research issue.
Acknowledgments
We would like to thank Atsushi Fujita, Francis
Bond, and all of the members of the Information
Analysis Laboratory, Universal Communication Re-
search Institute at NICT.
13http://alagin.jp/
1095
References
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122?129.
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. Journal of Artificial Intelligence Research,
38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL-2005), pages 597?
604.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003, pages 16?23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the ACL joint
with the 10th Meeting of the European Chapter of the
ACL (ACL/EACL 2001), pages 50?57.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning direc-
tionality of inference rules. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP2007), pages 161?170.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the 2006 Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL 2006), pages 17?24.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING 2004), pages 350?
356.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Atsushi Fujii and Tetsuya Ishikawa. 2002. Extraction
and organization of encyclopedic knowledge informa-
tion using the World Wide Web (written in Japanese).
Institute of Electronics, Information, and Communica-
tion Engineers, J85-D-II(2):300?307.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), pages
107?114.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama.
2009. Large-scale verb entailment acquisition from
the web. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2009), pages 1172?1181.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical selection and paraphrase in
a meaning-text generation model. In Ce?cile L. Paris,
William R. Swartout, and William C. Mann, editors,
Natural language generation in artificial intelligence
and computational linguistics, pages 293?312. Kluwer
Academic Press.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
the 2006 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2006), pages
455?462.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named entity
recognition. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2007), pages 698?707, June.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL-08: HLT), pages 407?415.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2007), pages
177?180.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the 2008
1096
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2008), pages 802?811.
Nitin Madnani and Bonnie Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news
on a daily basis with columbia?s newsblaster. In Pro-
ceedings of the 2nd international conference on Hu-
man Language Technology Research, pages 280?285.
Masaki Murata, Toshiyuki Kanemaru, and Hitoshi Isa-
hara. 2004. Automatic paraphrase acquisition based
on matching of definition sentences in plural dictionar-
ies (written in Japanese). Journal of Natural Language
Processing, 11(5):135?149.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL
2010), pages 1318?1327.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2004), pages 142?149.
Deepak Ravichandran and Eduard H. Hovy. 2002.
Learning surface text patterns for a question answer-
ing system. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 41?47.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP-2005), pages 80?87.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of the 2nd international Con-
ference on Human Language Technology Research
(HLT2002), pages 313?318.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary template. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING2008), pages 849?856.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL 2007),
pages 456?463.
1097
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 155?165,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Distortion Model Considering Rich Context
for Statistical Machine Translation
Isao Goto?,? Masao Utiyama? Eiichiro Sumita?
Akihiro Tamura? Sadao Kurohashi?
?National Institute of Information and Communications Technology
?Kyoto University
goto.i-es@nhk.or.jp
{mutiyama, eiichiro.sumita, akihiro.tamura}@nict.go.jp
kuro@i.kyoto-u.ac.jp
Abstract
This paper proposes new distortion mod-
els for phrase-based SMT. In decoding, a
distortion model estimates the source word
position to be translated next (NP) given
the last translated source word position
(CP). We propose a distortion model that
can consider the word at the CP, a word
at an NP candidate, and the context of the
CP and the NP candidate simultaneously.
Moreover, we propose a further improved
model that considers richer context by dis-
criminating label sequences that specify
spans from the CP to NP candidates. It
enables our model to learn the effect of
relative word order among NP candidates
as well as to learn the effect of distances
from the training data. In our experiments,
our model improved 2.9 BLEU points for
Japanese-English and 2.6 BLEU points for
Chinese-English translation compared to
the lexical reordering models.
1 Introduction
Estimating appropriate word order in a target lan-
guage is one of the most difficult problems for
statistical machine translation (SMT). This is par-
ticularly true when translating between languages
with widely different word orders.
To address this problem, there has been a lot
of research done into word reordering: lexical
reordering model (Tillman, 2004), which is one
of the distortion models, reordering constraint
(Zens et al, 2004), pre-ordering (Xia and Mc-
Cord, 2004), hierarchical phrase-based SMT (Chi-
ang, 2007), and syntax-based SMT (Yamada and
Knight, 2001).
In general, source language syntax is useful for
handling long distance word reordering. However,
obtaining syntax requires a syntactic parser, which
is not available for many languages. Phrase-based
SMT (Koehn et al, 2007) is a widely used SMT
method that does not use a parser.
Phrase-based SMT mainly1 estimates word re-
ordering using distortion models2. Therefore, dis-
tortion models are one of the most important com-
ponents for phrase-based SMT. On the other hand,
there are methods other than distortion models for
improving word reordering for phrase-based SMT,
such as pre-ordering or reordering constraints.
However, these methods also use distortion mod-
els when translating by phrase-based SMT. There-
fore, distortion models do not compete against
these methods and are commonly used with them.
If there is a good distortion model, it will improve
the translation quality of phrase-based SMT and
benefit to the methods using distortion models.
In this paper, we propose two distortion mod-
els for phrase-based SMT. In decoding, a distor-
tion model estimates the source word position to
be translated next (NP) given the last translated
source word position (CP). The proposed models
are the pair model and the sequence model. The
pair model utilizes the word at the CP, a word at
an NP candidate site, and the words surrounding
the CP and the NP candidates (context) simultane-
ously. In addition, the sequence model, which is
the further improved model, considers richer con-
text by identifying the label sequence that spec-
ify the span from the CP to the NP. It enables
our model to learn the effect of relative word or-
der among NP candidates as well as to learn the
effect of distances from the training data. Our
model learns the preference relations among NP
1A language model also supports the estimation.
2In this paper, reordering models for phrase-based SMT,
which are intended to estimate the source word position to
be translated next in decoding, are called distortion models.
This estimation is used to produce a hypothesis in the target
language word order sequentially from left to right.
155
kinou  kare  wa  pari  de  hon  wo  katta
he   bought   books   in   Paris   yesterday
Source:
Target:
Figure 1: An example of left-to-right translation
for Japanese-English. Boxes represent phrases
and arrows indicate the translation order of the
phrases.
candidates. Our model consists of one probabilis-
tic model and does not require a parser. Exper-
iments confirmed the effectiveness of our method
for Japanese-English and Chinese-English transla-
tion, using NTCIR-9 Patent Machine Translation
Task data sets (Goto et al, 2011).
2 Distortion Model for Phrase-Based
SMT
A Moses-style phrase-based SMT generates target
hypotheses sequentially from left to right. There-
fore, the role of the distortion model is to esti-
mate the source phrase position to be translated
next whose target side phrase will be located im-
mediately to the right of the already generated hy-
potheses. An example is shown in Figure 1. In
Figure 1, we assume that only the kare wa (En-
glish side: ?he?) has been translated. The target
word to be generated next will be ?bought? and the
source word to be selected next will be its corre-
sponding Japanese word katta. Thus, a distortion
model should estimate phrases including katta as
a source phrase position to be translated next.
To explain the distortion model task in more de-
tail, we need to redefine more precisely two terms,
the current position (CP) and next position (NP) in
the source sentence. CP is the source sentence po-
sition corresponding to the rightmost aligned tar-
get word in the generated target word sequence.
NP is the source sentence position corresponding
to the leftmost aligned target word in the target
phrase to be generated next. The task of the distor-
tion model is to estimate the NP3 from NP candi-
dates (NPCs) for each CP in the source sentence.4
3NP is not always one position, because there may be mul-
tiple correct hypotheses.
4This definition is slightly different from that of existing
methods such as Moses and (Green et al, 2010). In existing
methods, CP is the rightmost position of the last translated
source phrase and NP is the leftmost position of the source
phrase to be translated next. Note that existing methods do
kinou
1
 kare
2
 wa
3
 pari
4
 de
5
 hon
6
 wo
7
 katta
8
he   bought   books   in   Paris   yesterday
(a)
kinou
1
 kare
2
 wa
3
 pari
4
 de
5
 ni
6
 satsu
7
 hon
8
 wo
9
 katta
10
he   bought   two   books   in   Paris   yesterday
(b)
kinou
1
 kare
2
 wa
3
 hon
4
 wo
5
 karita
6
 ga
7
 kanojo
8
 wa
9
 katta
10
he   borrowed   books  yesterday  but  she  bought
(c)
kinou
1
 kare
2
 wa
3
 kanojo
4
 ga
5
 katta
6
 hon
7
 wo
8
 karita
9
yesterday  he  borrowed  the  books  that  she  bought
(e)
kinou
1
 kare
2
 wa
3
 hon
4
 wo
5
 katta
6
 ga
7
 kanojo
8
 wa
9
 karita
10
he   bought   books   yesterday   but   she   borrowed
(d)
??
?~
??
??
?~
??
?~
???~
CP NP
Figure 2: Examples of CP and NP for Japanese-
English translation. The upper sentence is the
source sentence and the sentence underneath is a
target hypothesis for each example. The NP is in
bold, and the CP is in bold italics. The point of an
arrow with a ? mark indicates a wrong NP candi-
date.
Estimating NP is a difficult task. Figure 2 shows
some examples. The superscript numbers indicate
the word position in the source sentence.
In Figure 2 (a), the NP is 8. However, in Fig-
ure 2 (b), the word (kare) at the CP is the same as
(a), but the NP is different (the NP is 10). From
these examples, we see that distance is not the es-
sential factor in deciding an NP. And it also turns
out that the word at the CP alone is not enough to
estimate the NP. Thus, not only the word at the CP
but also the word at a NP candidate (NPC) should
be considered simultaneously.
In (c) and (d) in Figure 2, the word (kare) at the
CP is the same and karita (borrowed) and katta
(bought) are at the NPCs. Karita is the word at
the NP and katta is not the word at the NP for
(c), while katta is the word at the NP and karita
is not the word at the NP for (d). From these ex-
amples, considering what the word is at the NP
not consider word-level correspondences.
156
is not enough to estimate the NP. One of the rea-
sons for this difference is the relative word order
between words. Thus, considering relative word
order is important.
In (d) and (e) in Figure 2, the word (kare) at the
CP and the word order between katta and karita
are the same. However, the word at the NP for
(d) and the word at the NP for (e) are different.
From these examples, we can see that selecting
a nearby word is not always correct. The differ-
ence is caused by the words surrounding the NPCs
(context), the CP context, and the words between
the CP and the NPC. Thus, these should be con-
sidered when estimating the NP.
In summary, in order to estimate the NP, the fol-
lowing should be considered simultaneously: the
word at the NP, the word at the CP, the relative
word order among the NPCs, the words surround-
ing NP and CP (context), and the words between
the CP and the NPC.
There are distortion models that do not require
a parser for phrase-based SMT. The linear dis-
tortion cost model used in Moses (Koehn et al,
2007), whose costs are linearly proportional to
the reordering distance, always gives a high cost
to long distance reordering, even if the reorder-
ing is correct. The MSD lexical reordering model
(Tillman, 2004; Koehn et al, 2005; Galley and
Manning, 2008) only calculates probabilities for
the three kinds of phrase reorderings (monotone,
swap, and discontinuous), and does not consider
relative word order or words between the CP and
the NPC. Thus, these models are not sufficient for
long distance word reordering.
Al-Onaizan and Papineni (2006) proposed a
distortion model that used the word at the CP and
the word at an NPC. However, their model did not
use context, relative word order, or words between
the CP and the NPC.
Ni et al (2009) proposed a method that adjusts
the linear distortion cost using the word at the CP
and its context. Their model does not simultane-
ously consider both the word specified at the CP
and the word specified at the NPCs.
Green et al (2010) proposed distortion mod-
els that used context. Their model (the outbound
model) estimates how far the NP should be from
the CP using the word at the CP and its con-
text.5 Their model does not simultaneously con-
5They also proposed another model (the inbound model)
sider both the word specified at the CP and the
word specified at an NPC. For example, the out-
bound model considers the word specified at the
CP, but does not consider the word specified at an
NPC. Their models also do not consider relative
word order.
In contrast, our distortion model solves the
aforementioned problems. Our distortion models
utilize the word specified at the CP, the word spec-
ified at an NPC, and also the context of the CP
and the NPC simultaneously. Furthermore, our se-
quence model considers richer context including
the relative word order among NPCs and also in-
cluding all the words between the CP and the NPC.
In addition, unlike previous methods, our models
learn the preference relations among NPCs.
3 Proposed Method
In this section, we first define our distortion model
and explain our learning strategy. Then, we de-
scribe two proposed models: the pair model and
the sequence model that is the further improved
model.
3.1 Distortion Model and Learning Strategy
First, we define our distortion model. Let i be a
CP, j be an NPC, S be a source sentence, andX be
the random variable of the NP. In this paper, dis-
tortion probability is defined as P (X = j|i, S),
which is the probability of an NPC j being the NP.
Our distortion model is defined as the model cal-
culating the distortion probability.
Next, we explain the learning strategy for our
distortion model. We train this model as a dis-
criminative model that discriminates the NP from
NPCs. Let J be a set of word positions in S other
than i. We train the distortion model subject to
?
j?J
P (X = j|i, S) = 1.
The model parameters are learned to maximize the
distortion probability of the NP among all of the
NPCs J in each source sentence. This learning
strategy is a kind of preference relation learning
(Evgniou and Pontil, 2002). In this learning, the
that estimates reverse direction distance. Each NPC is re-
garded as an NP, and the inbound model estimates how far
the corresponding CP should be from the NP using the word
at the NP and its context.
157
distortion probability of the actual NP will be rel-
atively higher than those of all the other NPCs J .
This learning strategy is different from that of
(Al-Onaizan and Papineni, 2006; Green et al,
2010). For example, Green et al (2010) trained
their outbound model subject to ?c?C P (Y =
c|i, S) = 1, where C is the set of the nine distor-
tion classes6 and Y is the random variable of the
correct distortion class that the correct distortion is
classified into. Distortion is defined as j ? i ? 1.
Namely, the model probabilities that they learned
were the probabilities of distortion classes in all
of the training data, not the relative preferences
among the NPCs in each source sentence.
3.2 Pair Model
The pair model utilizes the word at the CP, the
word at an NPC, and the context of the CP and the
NPC simultaneously to estimate the NP. This can
be done by our distortion model definition and the
learning strategy described in the previous section.
In this work, we use the maximum entropy
method (Berger et al, 1996) as a discriminative
machine learning method. The reason for this
is that a model based on the maximum entropy
method can calculate probabilities. However, if
we use scores as an approximation of the distor-
tion probabilities, various discriminative machine
learning methods can be applied to build the dis-
tortion model.
Let s be a source word and sn1 = s1s2...sn be
a source sentence. We add a beginning of sen-
tence (BOS) marker to the head of the source sen-
tence and an end of sentence (EOS) marker to the
end, so the source sentence S is expressed as sn+10
(s0 = BOS, sn+1 = EOS). Our distortion model
calculates the distortion probability for an NPC
j ? {j|1 ? j ? n + 1 ? j ?= i} for each CP
i ? {i|0 ? i ? n}
P (X = j|i, S) = 1Zi
exp
(
wTf (i, j, S, o, d)
)
(1)
where
o =
{
0 (i < j)
1 (i > j) , d =
?
??
??
0 (|j ? i| = 1)
1 (2 ? |j ? i| ? 5)
2 (6 ? |j ? i|)
,
6(??,?8], [?7,?5], [?4,?3], ?2, 0, 1, [2, 3], [4, 6],
and [7,?). In (Green et al, 2010), ?1 was used as one of
distortion classes. However, ?1 represents the CP in our def-
inition, and CP is not an NPC. Thus, we shifted all of the
distortion classes for negative distortions by ?1.
Template
?o?, ?o, sp?1, ?o, ti?, ?o, tj?, ?o, d?, ?o, sp, sq?2,
?o, ti, tj?, ?o, ti?1, ti, tj?, ?o, ti, ti+1, tj?,
?o, ti, tj?1, tj?, ?o, ti, tj , tj+1?, ?o, si, ti, tj?,
?o, sj , ti, tj?
1 p ? {p|i? 2 ? p ? i + 2 ? j ? 2 ? p ? j + 2}
2 (p, q) ? {(p, q)|i ? 2 ? p ? i + 2 ? j ? 2 ? q ?
j + 2 ? (|p? i| ? 1 ? |q ? j| ? 1)}
Table 1: Feature templates. t is the part of speech
of s.
w is a weight parameter vector, each element
of f(?) is a binary feature function, and Zi =?
j?{j|1?j?n+1 ? j ?=i}(numerator of Equation 1)
is a normalization factor. o is an orientation of i to
j and d is a distance class.
The binary feature function that constitutes an
element of f(?) returns 1 when its feature is
matched and if else, returns 0. Table 1 shows the
feature templates used to produce the features. A
feature is an instance of a feature template.
In Equation 1, i, j, and S are used by the feature
functions. Thus, Equation 1 can utilize features
consisting of both si, which is the word specified
at i, and sj , which is the word specified at j, or
both the context of i and the context of j simulta-
neously. Distance is considered using the distance
class d. Distortion is represented by distance and
orientation. The pair model considers distortion
using six joint classes of d and o.
3.3 Sequence Model
The pair model does not consider relative word or-
der among NPCs or all the words between the CP
and an NPC. In this section, we propose a further
improved model, the sequence model, which con-
siders richer context including relative word order
among NPCs and also including all the words be-
tween the CP and an NPC.
In (c) and (d) in Figure 2, karita (borrowed) and
katta (bought) occur in the source sentences. The
pair model considers the effect of distances using
only the distance class d. If these positions are
in the same distance class, the pair model cannot
consider the differences in distances. In this case,
these are conflict instances during training and it
is difficult to distinguish the NP for translation.
Now to explain how to consider the relative
word order by the sequence model. The sequence
model considers the relative word order by dis-
criminating the label sequence corresponding to
the NP from the label sequences corresponding to
158
Label Description
C A position is the CP.
I A position is a position between the CP
and the NPC.
N A position is the NPC.
Table 2: The ?C, I, and N? label set.
La
be
ls
eq
ue
nc
e
ID
1 N C
3 C N
4 C I N
5 C I I N
6 C I I I N
7 C I I I I N
8 C I I I I I N
9 C I I I I I I N
10 C I I I I I I I N
11 C I I I I I I I I N
B
O
S0
ki
no
u1
ka
re
2
w
a3
ho
n4
w
o5
ka
ri
ta
6
ga
7
ka
no
jo
8
w
a9
ka
tta
10
EO
S1
1
(y
es
te
rd
ay
)
(h
e)
(b
oo
k)
(b
or
ro
w
ed
)
(s
he
)
(b
ou
gh
t)
Source sentence
Figure 3: Example of label sequences that specify
spans from the CP to each NPC for the case of
Figure 2 (c). The labels (C, I, and N) in the boxes
are the label sequences.
each NPC in each sentence. Each label sequence
corresponds to one NPC. Therefore, if we identify
the label sequence that corresponds to the NP, we
can obtain the NP. The label sequences specify the
spans from the CP to each NPC using three kinds
of labels indicating the type of word positions in
the spans. The three kinds of labels, ?C, I, and N,?
are shown in Table 2. Figure 3 shows examples
of the label sequences for the case of Figure 2 (c).
In Figure 3, the label sequences are represented by
boxes and the elements of the sequences are labels.
The NPC is used as the label sequence ID for each
label sequence.
The label sequence can treat relative word or-
der. For example, the label sequence ID of 10 in
Figure 3 knows that karita exists to the left of the
NPC of 10. This is because karita6 carries a la-
bel I while katta10 carries a label N, and a position
with label I is defined as relatively closer to the CP
than a position with label N. By utilizing the label
sequence and corresponding words, the model can
reflect the effect of karita existing between the CP
and the NPC of 10 on the probability.
For the sequence model, karita (borrowed) and
katta (bought) in (c) and (d) in Figure 2 are not
conflict instances in training, whereas they are
conflict instances in training for the pair model.
The reason is as follows. In order to make the
probability of the NPC of 10 smaller than the NPC
of 6, instead of making the weight parameters for
the features with respect to the word at the position
of 10 with label N smaller than the weight param-
eters for the features with respect to the word at
the position of 6 with label N, the sequence model
can give negative weight parameters for the fea-
tures with respect to the word at the position of 6
with label I.
We use a sequence discrimination technique
based on CRF (Lafferty et al, 2001) to identify the
label sequence that corresponds to the NP. There
are two differences between our task and the CRF
task. One difference is that CRF discriminates la-
bel sequences that consist of labels from all of the
label candidates, whereas we constrain the label
sequences to sequences where the label at the CP
is C, the label at an NPC is N, and the labels be-
tween the CP and the NPC are I. The other dif-
ference is that CRF is designed for discriminat-
ing label sequences corresponding to the same ob-
ject sequence, whereas we do not assign labels to
words outside the spans from the CP to each NPC.
However, when we assume that another label such
as E has been assigned to the words outside the
spans and there are no features involving label E,
CRF with our label constraints can be applied to
our task. In this paper, the method designed to
discriminate label sequences corresponding to the
different word sequence lengths is called partial
CRF.
The sequence model based on partial CRF is de-
rived by extending the pair model. We introduce
the label l and extend the pair model to discrimi-
nating the label sequences. There are two exten-
sions to the pair model. One extension uses la-
bels. We suppose that label sequences specify the
spans from the CP to each NPC. We conjoined all
the feature templates in Table 1 with an additional
feature template ?li, lj? to include the labels into
features where li is the label corresponding to the
position of i. The other extension uses sequence.
In the pair model, the position pair of (i, j) is used
to derive features. In contrast, to descriminate la-
bel sequences in the sequence model, the position
pairs of (i, k), k ? {k|i < k ? j ? j ? k < i}
159
and (k, j), k ? {k|i ? k < j ? j < k ? i}
are used to derive features. Note that in the feature
templates in Table 1, i and j are used to specify
two positions. When features are used for the se-
quence model, one of the positions is regarded as
k.
The distortion probability for an NPC j being
the NP given a CP i and a source sentence S is
calculated as:
P (X = j|i, S) =
1
Zi
exp
( ?
k?M?{j}
wTf (i, k, S, o, d, li, lk)
+
?
k?M?{i}
wTf (k, j, S, o, d, lk, lj)
)
(2)
where
M =
{
{m|i < m < j} (i < j)
{m|j < m < i} (i > j)
and Zi = ?j?{j|1?j?n+1 ? j ?=i}(numerator of
Equation 2) is a normalization factor. Since j is
used as the label sequence ID, discriminating j
also means discriminating label sequence IDs.
The first term in exp(?) in Equation 2 considers
all of the word pairs located at i and other posi-
tions in the sequence, and also their context. The
second term in exp(?) in Equation 2 considers all
of the word pairs located at j and other positions
in the sequence, and also their context.
By designing our model to discriminate among
different length label sequences, our model can
naturally handle the effect of distances. Many fea-
tures are derived from a long label sequence be-
cause it will contain many labels between the CP
and the NPC. On the other hand, fewer features
are derived from a short label sequence because a
short label sequence will contain fewer labels be-
tween the CP and the NPC. The bias from these
differences provides important clues for learning
the effect of distances.7
7Note that the sequence model does not only consider
larger context than the pair model, but that it also considers
labels. The pair model does not discriminate labels, whereas
the sequence model uses label N and label I for the positions
except for the CP, depending on each situation. For example,
in Figure 3, at position 6, label N is used in the label sequence
ID of 6, but label I is used in the label sequence IDs of 7 to
11. Namely, even if they are at the same position, the labels
in the label sequences are different. The sequence model dis-
criminates the label differences.
BOS  kare  wa  pari  de  hon  wo  katta  EOS
BOS  he  bought  books  in  Paris  EOS
Source:
Target:
training data
Figure 4: Examples of supervised training data.
The lines represent word alignments. The English
side arrows point to the nearest word aligned on
the right.
3.4 Training Data for Discriminative
Distortion Model
To train our discriminative distortion model, su-
pervised training data is needed. The training data
is built from a parallel corpus and word alignments
between corresponding source words and target
words. Figure 4 shows examples of training data.
We select the target words aligned to the source
words sequentially from left to right (target side
arrows). Then, the order of the source words in
the target word order is decided (source side ar-
rows). The source sentence and the source side
arrows are the training data.
4 Experiment
In order to confirm the effects of our distortion
model, we conducted a series of Japanese to En-
glish (JE) and Chinese to English (CE) translation
experiments.8
4.1 Common Settings
We used the patent data for the Japanese to En-
glish and Chinese to English translation subtasks
from the NTCIR-9 Patent Machine Translation
Task (Goto et al, 2011). There were 2,000 sen-
tences for the test data and 2,000 sentences for the
development data.
Mecab9 was used for the Japanese morpholog-
ical analysis. The Stanford segmenter10 and tag-
ger11 were used for Chinese segmentation and
POS tagging. The translation model was trained
using sentences of 40 words or less from the train-
ing data. So approximately 2.05 million sen-
tence pairs consisting of approximately 54 million
8We conducted JE and CE translation as examples of
language pairs with different word orders and of languages
where there is a great need for translation into English.
9http://mecab.sourceforge.net/
10http://nlp.stanford.edu/software/segmenter.shtml
11http://nlp.stanford.edu/software/tagger.shtml
160
Japanese tokens whose lexicon size was 134k and
50 million English tokens whose lexicon size was
213k were used for JE. And approximately 0.49
million sentence pairs consisting of 14.9 million
Chinese tokens whose lexicon size was 169k and
16.3 million English tokens whose lexicon size
was 240k were used for CE. GIZA++ and grow-
diag-final-and heuristics were used to obtain word
alignments. In order to reduce word alignment er-
rors, we removed articles {a, an, the} in English
and particles {ga, wo, wa} in Japanese before per-
forming word alignments because these function
words do not correspond to any words in the other
languages. After word alignment, we restored the
removed words and shifted the word alignment po-
sitions to the original word positions. We used 5-
gram language models that were trained using the
English side of each set of bilingual training data.
We used an in-house standard phrase-based
SMT system compatible with the Moses decoder
(Koehn et al, 2007). The SMT weighting param-
eters were tuned by MERT (Och, 2003) using the
development data. To stabilize the MERT results,
we tuned three times by MERT using the first half
of the development data and we selected the SMT
weighting parameter set that performed the best on
the second half of the development data based on
the BLEU scores from the three SMT weighting
parameter sets.
We compared systems that used a common
SMT feature set from standard SMT features and
different distortion model features. The com-
mon SMT feature set consists of: four translation
model features, phrase penalty, word penalty, and
a language model feature. The compared different
distortion model features are: the linear distortion
cost model feature (LINEAR), the linear distortion
cost model feature and the six MSD bidirectional
lexical distortion model (Koehn et al, 2005) fea-
tures (LINEAR+LEX), the outbound and inbound
distortion model features discriminating nine dis-
tortion classes (Green et al, 2010) (9-CLASS), the
proposed pair model feature (PAIR), and the pro-
posed sequence model feature (SEQUENCE).
4.2 Training for the Proposed Models
Our distortion model was trained as follows: We
used 0.2 million sentence pairs and their word
alignments from the data used to build the trans-
lation model as the training data for our distortion
models. The features that were selected and used
were the ones that had been counted12, using the
feature templates in Table 1, at least four times
for all of the (i, j) position pairs in the training
sentences. We conjoined the features with three
types of label pairs ?C, I?, ?I,N?, or ?C,N? as in-
stances of the feature template ?li, lj? to produce
features for SEQUENCE. The L-BFGS method
(Liu and Nocedal, 1989) was used to estimate the
weight parameters of maximum entropy models.
The Gaussian prior (Chen and Rosenfeld, 1999)
was used for smoothing.
4.3 Training for the Compared Models
For 9-CLASS, we used the same training data as
for our distortion models. Let ti be the part of
speech of si. We used the following feature tem-
plates to produce features for the outbound model:
?si?2?, ?si?1?, ?si?, ?si+1?, ?si+2?, ?ti?, ?ti?1, ti?,
?ti, ti+1?, and ?si, ti?. These feature templates corre-
spond to the components of the feature templates
of our distortion models. In addition to these fea-
tures, we used a feature consisting of the relative
source sentence position as the feature used by
(Green et al, 2010). The relative source sentence
position is discretized into five bins, one for each
quintile of the sentence. For the inbound model13,
i of the feature templates was changed to j. Fea-
tures occurring four or more times in the train-
ing sentences were used. The maximum entropy
method with Gaussian prior smoothing was used
to estimate the model parameters.
The MSD bidirectional lexical distortion model
was built using all of the data used to build the
translation model.
4.4 Results and Discussion
We evaluated translation quality based on the case-
insensitive automatic evaluation score BLEU-4
(Papineni et al, 2002). We used distortion lim-
its of 10, 20, 30, and unlimited (?), which limited
the number of words for word reordering to a max-
imum number. Table 3 presents our main results.
The proposed SEQUENCE outperformed the base-
lines for both Japanese to English and Chinese to
English translation. This demonstrates the effec-
tiveness of the proposed SEQUENCE. The scores
of the proposed SEQUENCE were higher than those
12When we counted features for selection, we only counted
features that were from the feature templates of ?si, sj?,
?ti, tj?, ?si, ti, tj?, and ?sj , ti, tj? in Table 1 when j was not
the NP, in order to avoid increasing the number of features.
13The inbound model is explained in footnote 5.
161
Japanese-English Chinese-English
Distortion limit 10 20 30 ? 10 20 30 ?
LINEAR 27.98 27.74 27.75 27.30 29.18 28.74 28.31 28.33
LINEAR+LEX 30.25 30.37 30.17 29.98 30.81 30.24 30.16 30.13
9-CLASS 30.74 30.98 30.92 30.75 31.80 31.56 31.31 30.84
PAIR 31.62 32.36 31.96 32.03 32.51 32.30 32.25 32.32
SEQUENCE 32.02 32.96 33.29 32.81 33.41 33.44 33.35 33.41
Table 3: Evaluation results for each method. The values are case-insensitive BLEU scores. Bold numbers
indicate no significant difference from the best result in each language pair using the bootstrap resampling
test at a significance level ? = 0.01 (Koehn, 2004).
Japanese-English Chinese-English
HIER 30.47 32.66
Table 4: Evaluation results for hierarchical phrase-
based SMT.
of the proposed PAIR. This confirms the effective-
ness for considering relative word order and words
between the CP and an NPC. The proposed PAIR
outperformed 9-CLASS, confirming that consider-
ing both the word specified at the CP and the word
specified at the NPC simultaneously was more ef-
fective than that of 9-CLASS.
For translating between languages with widely
different word orders such as Japanese and En-
glish, a small distortion limit is undesirable be-
cause there are cases where correct translations
cannot be produced with a small distortion limit,
since the distortion limit prunes the search space
that does not meet the constraint. Therefore,
a large distortion limit is required to translate
correctly. For JE translation, our SEQUENCE
achieved significantly better results at distortion
limits of 20 and 30 than that at a distortion limit
of 10, while the baseline systems of LINEAR,
LINEAR+LEX, and 9-CLASS did not achieve this.
This indicate that SEQUENCE could treat long
distance reordering candidates more appropriately
than the compared methods.
We also tested hierarchical phrase-based SMT
(Chiang, 2007) (HIER) using the Moses imple-
mentation. The common data was used to train
HIER. We used unlimited max-chart-span for the
system setting. Results are given in Table 4. Our
SEQUENCE outperformed HIER. The gain for JE
was large but the gain for CE was modest. Since
phrase-based SMT is generally faster in decod-
ing speed than hierarchical phrase-based SMT,
achieving better or comparable scores is worth-
Distortion
P
r
o
b
a
b
i
l
i
t
y
Figure 5: Average probabilities for large distortion
for Japanese-English translation.
while.
To investigate the tolerance for sparsity of the
training data, we reduced the training data for
the sequence model to 20,000 sentences for JE
translation.14 SEQUENCE using this model with
a distortion limit of 30 achieved a BLEU score
of 32.22.15 Although the score is lower than the
score of SEQUENCE with a distortion limit of 30
in Table 3, the score was still higher than those
of LINEAR, LINEAR+LEX, and 9-CLASS for JE
in Table 3. This indicates that the sequence model
also works even when the training data is not large.
This is because the sequence model considers not
only the word at the CP and the word at an NPC
but also rich context, and rich context would be ef-
fective even for a smaller set of training data.
14We did not conduct experiments using larger training
data because there would have been a very high computa-
tional cost to build models using the L-BFGS method.
15To avoid effects from differences in the SMT weighting
parameters, we used the same SMT weighting parameters for
SEQUENCE, with a distortion limit of 30, in Table 3.
162
To investigate how well SEQUENCE learns the
effect of distance, we checked the average distor-
tion probabilities for large distortions of j ? i? 1.
Figure 5 shows three kinds of probabilities for dis-
tortions from 3 to 20 for Japanese-English transla-
tion. One is the average distortion probabilities
in the Japanese test sentences for each distortion
for SEQUENCE, and another is this for PAIR. The
third (CORPUS) is the probabilities for the actual
distortions in the training data that were obtained
from the word alignments used to build the trans-
lation model. The probability for a distortion for
CORPUS was calculated by the number of the dis-
tortion divided by the total number of distortions
in the training data.
Figure 5 shows that when a distance class fea-
ture used in the model was the same (e.g., distor-
tions from 5 to 20 were the same distance class
feature), PAIR produced average distortion prob-
abilities that were almost the same. In contrast,
the average distortion probabilities for SEQUENCE
decreased when the lengths of the distortions in-
creased, even if the distance class feature was
the same, and this behavior was the same as that
of CORPUS. This confirms that the proposed
SEQUENCE could learn the effect of distances ap-
propriately from the training data.16
5 Related Works
We discuss related works other than discussed in
Section 2. Xiong et al (2012) proposed a model
predicting the orientation of an argument with re-
spect to its verb using a parser. Syntactic struc-
tures and predicate-argument structures are useful
for reordering. However, orientations do not han-
dle distances. Thus, our distortion model does not
compete against the methods predicting orienta-
tions using a parser and would assist them if used
16We also checked the average distortion probabilities for
the 9-CLASS outbound model in the Japanese test sentences
for Japanese-English translation. We averaged the average
probabilities for distortions in a distortion span of [4, 6] and
also averaged those in a distortion span of [7, 20], where the
distortions in each span are in the same distortion class. The
average probability for [4, 6] was 0.058 and that for [7, 20]
was 0.165. From CORPUS, the average probabilities in the
training data for each distortion in [4, 6] were higher than
those for each distortion in [7, 20]. However, the converse
was true for the comparison between the two average prob-
abilities for the outbound model. This is because the sum
of probabilities for distortions from 7 and above was larger
than the sum of probabilities for distortions from 4 to 6 in the
training data. This comparison indicates that the 9-CLASS
outbound model could not appropriately learn the effects of
large distances for JE translation.
together.
There are word reordering constraint methods
using ITG (Wu, 1997) for phrase-based SMT
(Zens et al, 2004; Yamamoto et al, 2008; Feng et
al., 2010). These methods consider sentence level
consistency with respect to ITG. The ITG con-
straint does not consider distances of reordering
and was used with other distortion models. Our
distortion model does not consider sentence level
consistency, so our distortion model and ITG con-
straint methods are thought to be complementary.
There are tree-based SMT methods (Chiang,
2007; Galley et al, 2004; Liu et al, 2006). In
many cases, tree-based SMT methods do not use
the distortion models that consider reordering dis-
tance apart from translation rules because it is not
trivial to use distortion scores considering the dis-
tances for decoders that do not generate hypothe-
ses from left to right. If it could be applied to these
methods, our distortion model might contribute to
tree-based SMT methods. Investigating the effects
will be for future work.
6 Conclusion
This paper described our distortion models for
phrase-based SMT. Our sequence model simply
consists of only one probabilistic model, but it can
consider rich context. Experiments indicate that
our models achieved better performance and the
sequence model could learn the effect of distances
appropriately. Since our models do not require a
parser, they can be applied to many languages. Fu-
ture work includes application to other language
pairs, incorporation into ITG constraint methods
and other reordering methods, and application to
tree-based SMT methods.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 529?536, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39?71, March.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical report.
163
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Theodoros Evgniou and Massimiliano Pontil. 2002.
Learning preference relations from data. Neural
Nets Lecture Notes in Computer Science, 2486:23?
32.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010.
An efficient shift-reduce decoding algorithm for
phrased-based machine translation. In Coling 2010:
Posters, pages 285?293, Beijing, China, August.
Coling 2010 Organizing Committee.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 273?280, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR-9, pages 559?578.
Spence Green, Michel Galley, and Christopher D.Man-
ning. 2010. Improved models of distortion cost
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 867?875, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of 18th International Conference on Machine Learn-
ing, pages 282?289.
D.C. Liu and J. Nocedal. 1989. On the limited memory
method for large scale optimization. Mathematical
Programming B, 45(3):503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609?616, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-
hesan Niranjan. 2009. Handling phrase reorder-
ings for machine translation. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
241?244, Suntec, Singapore, August. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 101?
104, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of Coling 2004, pages 508?
514, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 902?911, Jeju
Island, Korea, July. Association for Computational
Linguistics.
164
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523?530, Toulouse,
France, July. Association for Computational Lin-
guistics.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro
Sumita. 2008. Imposing constraints from the source
tree on ITG constraints for SMT. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
1?9, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In Pro-
ceedings of Coling 2004, pages 205?211, Geneva,
Switzerland, Aug 23?Aug 27. COLING.
165
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 253?258,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Chinese Morphological Analysis with Character-level POS Tagging 
 
Mo Shen?, Hongxiao Liu?, Daisuke Kawahara?, and Sadao Kurohashi? 
?Graduate School of Informatics, Kyoto University, Japan 
?School of Computer Science, Fudan University, China 
shen@nlp.ist.i.kyoto-u.ac.jp {dk,kuro}@i.kyoto-u.ac.jp 
12210240027@fudan.edu.cn 
 
  
 
Abstract 
The focus of recent studies on Chinese word 
segmentation, part-of-speech (POS) tagging 
and parsing has been shifting from words to 
characters. However, existing methods have 
not yet fully utilized the potentials of Chinese 
characters. In this paper, we investigate the 
usefulness of character-level part-of-speech 
in the task of Chinese morphological analysis. 
We propose the first tagset designed for the 
task of character-level POS tagging. We pro-
pose a method that performs character-level 
POS tagging jointly with word segmentation 
and word-level POS tagging. Through exper-
iments, we demonstrate that by introducing 
character-level POS information, the perfor-
mance of a baseline morphological analyzer 
can be significantly improved. 
1 Introduction 
In recent years, the focus of research on Chinese 
word segmentation, part-of-speech (POS) tag-
ging and parsing has been shifting from words 
toward characters. Character-based methods 
have shown superior performance in these tasks 
compared to traditional word-based methods (Ng 
and Low, 2004; Nakagawa, 2004; Zhao et al, 
2006; Kruengkrai et al, 2009; Xue, 2003; Sun, 
2010). Studies investigating the morphological-
level and character-level internal structures of 
words, which treat character as the true atom of 
morphological and syntactic processing, have 
demonstrated encouraging results (Li, 2011; Li 
and Zhou, 2012; Zhang et al, 2013). This line of 
research has provided great insight in revealing 
the roles of characters in word formation and 
syntax of Chinese language. 
However, existing methods have not yet fully 
utilized the potentials of Chinese characters. 
While Li (2011) pointed out that some characters  
Character-level 
Part-of-Speech 
Examples of Verb 
verb + noun ?? (invest : throw + wealth) 
noun + verb ?? (feel sorry : heart + hurt) 
verb + adjective 
?? (realize : recognize + 
clear) 
adjective + verb ?? (hate : pain + hate) 
verb + verb 
?? (inspect : examine + re-
view) 
Table 1. Character-level POS sequence as a 
more specified version of word-level POS: an 
example of verb. 
can productively form new words by attaching to 
existing words, these characters consist only a 
portion of all Chinese characters and appear in 
35% of the words in Chinese Treebank 5.0 
(CTB5) (Xue et al, 2005). Zhang (2013) took 
one step further by investigating the character-
level structures of words; however, the machine 
learning of inferring these internal structures re-
lies on the character forms, which still suffers 
from data sparseness.  
In our view, since each Chinese character is in 
fact created as a word in origin with complete 
and independent meaning, it should be treated as 
the actual minimal morphological unit in Chinese 
language, and therefore should carry specific 
part-of-speech. For example, the character ??? 
(beat) is a verb and the character ??? (broken) is 
an adjective. A word on the other hand, is either 
single-character, or a compound formed by sin-
gle-character words. For example, the verb ??
?? (break) can be seen as a compound formed 
by the two single-character words with the con-
struction ?verb + adjective?. 
Under this treatment, we observe that words 
with the same construction in terms of character-
level POS tend to also have similar syntactic 
roles. For example, the words having the con-
253
struction ?verb + adjective? are typically verbs, 
and those having the construction ?adjective + 
noun? are typically nouns, as shown in the fol-
lowing examples:  
 
(a) verb : verb + adjective  
????(break) : ???(beat) + ???(broken) 
????(update) : ???(replace) + ???(new) 
????(bleach) : ???(wash) + ???(white) 
 
(b) noun : adjective + noun 
????(theme) : ???(main) + ???(topic) 
????(newcomer) : ???(new) + ???(person) 
????(express) : ???(fast) + ???(car) 
 
This suggests that character-level POS can be 
used as cues in predicting the part-of-speech of 
unknown words. 
Another advantage of character-level POS is 
that, the sequence of character-level POS in a 
word can be seen as a more fine-grained version 
of word-level POS. An example is shown in Ta-
ble 1. The five words in this table are very likely 
to be tagged with the same word-level POS as 
verb in any available annotated corpora, while it 
can be commonly agreed among native speakers 
of Chinese that the syntactic behaviors of these 
words are different from each other, due to their 
distinctions in word constructions. For example, 
verbs having the construction ?verb + noun? (e.g. 
??) or ?verb + verb? (e.g. ??) can also be 
nouns in some context, while others cannot; And 
verbs having the constructions ?verb + adjective? 
(e.g. ??) require exact one object argument, 
while others generally do not. Therefore, com-
pared to word-level POS, the character-level 
POS can produce information for more expres-
sive features during the learning process of a 
morphological analyzer. 
In this paper, we investigate the usefulness of 
character-level POS in the task of Chinese mor-
phological analysis. We propose the first tagset 
designed for the task of character-level POS tag-
ging, based on which we manually annotate the 
entire CTB5. We propose a method that performs 
character-level POS tagging jointly with word 
segmentation and word-level POS tagging. 
Through experiments, we demonstrate that by 
introducing character-level POS information, the 
performance of a baseline morphological analyz-
er can be significantly improved. 
 
 
 
 
Tag Part-of-Speech Example 
n noun ??/NN (bill) 
v verb ??/VV (publish) 
j adj./adv. ??/VA (vast)  
t numerical ????/CD (3.14) 
m quantifier ?/CD ?/M (a piece of) 
d date ???/NT (1995) 
k proper noun ??/NR (sino-US) 
b prefix ???/NN (vice mayor) 
e suffix 
???/NN (construction 
inductry) 
r transliteration ????/NR (?rp?d) 
u punctuation 
???????/NR 
(Charles Dickens) 
f foreign chars X??/NN (X-ray) 
o onomatopoeia ??/AD (rumble) 
s surname 
???/NR (Wang 
Xinmin) 
p pronoun ??/PN (they) 
c other functional ??/VV (be used for) 
Table 2. Tagset for character-level part-of-
speech tagging. The underlined characters in 
the examples correspond to the tags on the 
left-most column. The CTB-style word-level 
POS are also shown for the examples. 
2 Character-level POS Tagset 
We propose a tagset for the task of character-
level POS tagging. This tagset contains 16 tags, 
as illustrated in Table 2. The tagset is designed 
by treating each Chinese character as a single-
character word, and each (multi-character) word 
as a phrase of single-character words. Some of 
these tags are directly derived from the common-
ly accepted word-level part-of-speech, such as 
noun, verb, adjective and adverb. It should be 
noted that, for single-character words, the differ-
ence between adjective and adverb can almost be 
ignored, because for any of such words that can 
be used as an adjective, it usually can also be 
used as an adverb. Therefore, we have merged 
these two tags into one.  
On the other hand, some other tags are de-
signed specifically for characters, such as trans-
literation, surname, prefix and suffix. Unlike 
some Asian languages such as Japanese, there is 
no explicit character set in Chinese that are used 
exclusively for expressing names of foreign per-
sons, places or organizations. However, some 
characters are used much more frequently than 
others in these situations. For example, in the 
person?s name ?????? (?rp?d), all the four 
characters can be frequently observed in words  
254
 
Figure 1. A Word-character hybrid lattice of a Chinese sentence. Correct path is represented by blue 
bold lines. 
 
 
Word Length 1 2 3 4 5 6 7 or more 
Tags S BE BB2E BB2B3E BB2B3ME BB2B3MME BB2B3M...ME 
Table 3. Word representation with a 6-tag tagset: S, B, B2, B3, M, E 
 
of transliterations. Similarly, surnames in Chi-
nese are also drawn from a set of limited number 
of characters. We therefore assign specific tags 
for this kind of character sets. The tags for pre-
fixes and suffixes are motivated by the previous 
studies (Li, 2011; Li and Zhou, 2012). 
We have annotated character-level POS for all 
words in CTB5 1 . Fortunately, character-level 
POS in most words are independent of context, 
which means it is sufficient to annotate word 
forms unless there is an ambiguity. The annota-
tion was conducted by two persons, where each 
one of them was responsible for about 70% of 
the documents in the corpus. The redundancy 
was set for the purposes of style unification and 
quality control, on which we find that the inter- 
annotator agreement is 96.2%. Although the an-
notation also includes the test set, we blind this 
portion in all the experiments.  
                                                 
1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?CharPosCN 
3 Chinese Morphological Analysis with 
Character-level POS 
3.1 System Description 
Previous studies have shown that jointly pro-
cessing word segmentation and POS tagging is 
preferable to pipeline processing, which can 
propagate errors (Nakagawa and Uchimoto, 2007; 
Kruengkrai et al, 2009). Based on these studies, 
we propose a word-character hybrid model 
which can also utilize the character-level POS 
information. This hybrid model constructs a lat-
tice that consists of word-level and character-
level nodes from a given input sentence. Word-
level nodes correspond to words found in the 
system?s lexicon, which has been compiled from 
training data. Character-level nodes have special 
tags called position-of-character (POC) that indi-
cate the word-internal position (Asahara, 2003; 
Nakagawa, 2004). We have adopted the 6-tag 
tagset, which (Zhao et al, 2006) reported to be 
optimal. This tagset is illustrated in Table 3. 
Figure 2 shows an example of a lattice for the 
Chinese sentence: ????????? (Chen 
Deming answers to journalists? questions). The 
correct path is marked with blue bold lines. The 
255
Category Template Condition 
Baseline-unigram ?  ? ?  ? ?     ? ?     ? ?            ? ?          ?     ?                    ? 
 ?      ? ?      ? ?     ? ?     ? ?     ?     ?          ? ?         ? ?        ? ?        ? ?         ? 
Baseline-bigram ?      ? ?      ? ?      ? ?      ? ?          ? ?         ? 
       
 ?          ? ?         ? ?             ? ?          ? ?         ? 
 ?          ? ?         ? ?             ? ?           ? 
 ?             ? ?               ? ?                ? 
 ?      ? ?      ? ?          ? ?         ?         ?          ? ?         ? ?             ? 
 ?      ? Otherwise 
Proposed-unigram ?         ?    
Proposed-bigram ?              ? ?                  ?         
 ?             ? ?                 ? 
 ?                      ? ?                     ?        
 ?                     ? ?                    ?  
 ?          ? ?             ?        
 ?          ? ?              ?        
 ?                  ? ?                 ? ?                     ?        
Table 4. Feature templates. The ?Condition? column describes when to apply the templates:    
and   denote the previous and the current word-level node;     and    denote the previous and 
the current character-level node;     and    denote the previous and the current node of any 
types. Word-level nodes represent known words that can be found in the system?s lexicon. 
 
upper part of the lattice (word-level nodes) rep-
resents known words, where each node carries 
information such as character form, character-
level POS , and word-level POS. A word that 
contains multiple characters is represented by a 
sub-lattice (the dashed rectangle in the figure), 
where a path stands for a possible sequence of 
character-level POS for this word. For example, 
the word ???? (journalist) has two possible 
paths of character-level POS: ?verb + suffix? and 
?noun + suffix?. Nodes that are inside a sub-
lattice cannot be linked to nodes that are outside, 
except from the boundaries. The lower part of 
the lattice (character-level nodes) represents un-
known words, where each node carries a posi-
tion-of-character tag, in addition to other types of 
information that can also be found on a word-
level node. A sequence of character-level nodes 
are considered as an unknown word if and only if 
the sequence of POC tags forms one of the cases 
listed in Table 3. This table also illustrates the 
permitted transitions between adjacent character-
level nodes. We use the standard dynamic pro-
gramming technique to search for the best path in 
the lattice. We use the averaged perceptron (Col-
lins, 2002), an efficient online learning algorithm, 
to train the model. 
3.2 Features 
We show the feature templates of our model in 
Table 4. The features consist of two categories: 
baseline features, which are modified from the 
templates proposed in (Kruengkrai et al, 2009); 
and proposed features, which encode character-
level POS information.  
Baseline features: For word-level nodes that 
represent known words, we use the symbols  ,   
and   to denote the word form, POS tag and 
length of the word, respectively. The functions 
         and        return the first and last 
character of  . If   has only one character, we 
omit the templates that contain          or 
      . We use the subscript indices 0 and -1 to 
indicate the current node and the previous node 
during a Viterbi search, respectively. For charac-
ter-level nodes,   denotes the surface character, 
and   denotes the combination of POS and POC 
(position-of-character) tags.  
Proposed features: For word-level nodes, the 
function           returns the pair of the char-
acter-level POS tags of the first and last charac-
ters of  , and          returns the sequence of 
character-level POS tags of . If either the pair 
or the sequence of character-level POS is ambig-
uous, which means there are multiple paths in the 
sub-lattice of the word-level node, then the val-
ues on the current best path (with local context) 
during the Viterbi search will be returned. If   
has only one character, we omit the templates 
that contain          . For character-level nodes, 
the function       returns its character-level 
POS. The subscript indices 0 and -1 as well as 
256
other symbols stand for the same meaning as 
they are in the baseline features.  
4 Evaluation 
4.1 Settings 
To evaluate our proposed method, we have con-
ducted two sets of experiments on CTB5: word 
segmentation, and joint word segmentation and 
word-level POS tagging. We have adopted the 
same data division as in (Jiang et al, 2008a; 
Jiang et al, 2008b; Kruengkrai et al, 2009; 
Zhang and Clark, 2010; Sun, 2011): the training 
set, dev set and test set have 18,089, 350 and 348 
sentences, respectively. The models applied on 
all test sets are those that result in the best per-
formance on the CTB5 dev set. 
We have annotated character-level POS in-
formation for all 508,768 word tokens in CTB5. 
As mentioned in section 2, we blind the annota-
tion in the test set in all the experiments. To learn 
the characteristics of unknown words, we built 
the system?s lexicon using only the words in the 
training data that appear at least 3 times. We ap-
plied a similar strategy in building the lexicon for 
character-level POS, where the threshold we 
choose is 2. These thresholds were tuned using 
the development data.  
We have used precision, recall and the F-score 
to measure the performance of the systems. Pre-
cision ( ) is defined as the percentage of output 
tokens that are consistent with the gold standard 
test data, and recall ( ) is the percentage of to-
kens in the gold standard test data that are recog-
nized in the output. The balanced F-score ( ) is 
defined as  
     
   
. 
4.2 Experimental Results 
We compare the performance between a baseline 
model and our proposed approach. The results of 
the word segmentation experiment and the joint 
experiment of segmentation and POS tagging are 
shown in Table 5(a) and Table 5(b), respectively. 
Each row in these tables shows the performance 
of the corresponding system. ?CharPos? stands 
for our proposed model which has been de-
scribed in section 3. ?Baseline? stands for the 
same model except it only enables features from 
the baseline templates. 
The results show that, while the differences 
between the baseline model and the proposed 
model in word segmentation accuracies are small, 
the proposed model achieves significant im-
provement in the experiment of joint segmentati- 
(a) Word Segmentation Results 
System P R F 
Baseline 97.48 98.44 97.96 
CharPOS 97.55 98.51 98.03 
 
(b) Joint Segmentation and POS Tagging Results 
System P R F 
Baseline 93.01 93.95 93.48 
CharPOS 93.42 94.18 93.80 
Table 5. Experimental results on CTB5. 
 
System Segmentation Joint 
Baseline 97.96 93.48 
CharPOS 98.03 93.80 
Jiang2008a 97.85 93.41 
Jiang2008b 97.74 93.37 
Kruengkrai2009 97.87 93.67 
Zhang2010 97.78 93.67 
Sun2011 98.17 94.02 
Table 6. Comparison with previous studies on 
CTB5. 
on and POS tagging2. This suggests that our pro-
posed method is particularly effective in predict-
ing the word-level POS, which is consistent with 
our observations mentioned in section 1. 
In Table 6 we compare our approach with 
morphological analyzers in previous studies. The 
accuracies of the systems in previous work are 
directly taken from the original paper. As the 
results show, despite the fact that the perfor-
mance of our baseline model is relatively weak 
in the joint segmentation and POS tagging task, 
our proposed model achieves the second-best 
performance in both segmentation and joint tasks. 
5 Conclusion 
We believe that by treating characters as the true 
atoms of Chinese morphological and syntactic 
analysis, it is possible to address the out-of-
vocabulary problem that word-based methods 
have been long suffered from. In our error analy-
sis, we believe that by exploring the character-
level POS and the internal word structure (Zhang 
et al, 2013) at the same time, it is possible to 
further improve the performance of morphologi-
cal analysis and parsing. We will address these 
issues in our future work. 
  
                                                 
2        in McNemar?s test. 
257
Reference 
Masayuki Asahara. 2003. Corpus-based Japanese 
Morphological Analysis. Nara Institute of Science 
and Technology, Doctor?s Thesis. 
Michael Collins. 2002. Discriminative Training 
Methods for Hidden Markov Models: Theory and 
Experiments with Perceptron Algorithms. In Pro-
ceedings of EMNLP, pages 1?8. 
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?. 
2008a. A Cascaded Linear Model for Joint Chinese 
Word Segmentation and Part-of-speech Tagging. 
In Proceedings of ACL. 
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word 
Lattice Reranking for Chinese Word Segmentation 
and Part-of-speech Tagging. In Proceedings of COL-
ING. 
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi 
Kazama, YiouWang, Kentaro Torisawa, and Hi-
toshi Isahara. 2009. An Error-Driven Word-
Character Hybird Model for Joint Chinese Word 
Segmentation and POS Tagging. In Proceedings of 
ACL-IJCNLP, pages 513-521. 
Zhongguo Li. 2011. Parsing the Internal Structure of 
Words: A New Paradigm for Chinese Word Seg-
mentation. In Proceedings of ACL-HLT, pages 
1405?1414. 
Zhongguo Li and Guodong Zhou. 2012. Unified De-
pendency Parsing of Chinese Morphological and 
Syntactic Structures. In Proceedings of EMNLP, 
pages 1445?1454. 
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-speech Tagging: One-at-a-time or All-at-once? 
Word-based or Character-based? In Proceedings of 
EMNLP, pages 277?284. 
Tetsuji Nakagawa. 2004. Chinese and japanese word 
segmentation using word-level and character-level 
information. In Proceedings of COLING, pages 
466?472.  
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. 
Hybrid Approach to Word Segmentation and Pos 
Tagging. In Proceedings of ACL Demo and Poster 
Sessions, pages 217-220. 
Weiwei Sun. 2010. Word-based and Character-based 
Word Segmentation Models: Comparison and 
Combination. In Proceedings of COLING Poster 
Sessions, pages 1211?1219. 
Weiwei Sun. 2011. A Stacked Sub-word Model for 
Joint Chinese Word Segmentation and Part-of-
speech Tagging. In Proceedings of ACL-HLT, 
pages 1385?1394. 
Nianwen Xue. 2003. Chinese Word Segmentation as 
Character Tagging. In International Journal of 
Computational Linguistics and Chinese Language 
Processing. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha 
Palmer. 2005. The Penn Chinese Treebank: Phrase 
Structure Annotation of a Large Corpus. Natural 
Language Engineering, 11(2):207?238. 
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang 
Lu. 2006. Effective Tag Set Selection in Chinese 
Word Segmentation via Conditional Random Field 
Modeling. In Proceedings of PACLIC, pages 87-94. 
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting 
Liu. 2013. Chinese Parsing Exploiting Characters. 
In Proceedings of ACL, page 125-134. 
Yue Zhang and Stephen Clark. 2010. A Fast Decoder 
for Joint Word Segmentation and POS-tagging Us-
ing a Single Discriminative Model. In Proceedings 
of EMNLP, pages 843?852. 
258
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 79?84,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
KyotoEBMT: An Example-Based Dependency-to-Dependency
Translation Framework
John Richardson? Fabien Cromi?res? Toshiaki Nakazawa? Sadao Kurohashi?
?Graduate School of Informatics, Kyoto University, Kyoto 606-8501
?Japan Science and Technology Agency, Kawaguchi-shi, Saitama 332-0012
john@nlp.ist.i.kyoto-u.ac.jp, {fabien, nakazawa}@pa.jst.jp,
kuro@i.kyoto-u.ac.jp
Abstract
This paper introduces the Ky-
otoEBMT Example-Based Machine
Translation framework. Our system
uses a tree-to-tree approach, employing
syntactic dependency analysis for
both source and target languages
in an attempt to preserve non-local
structure. The effectiveness of our
system is maximized with online ex-
ample matching and a flexible decoder.
Evaluation demonstrates BLEU scores
competitive with state-of-the-art SMT
systems such as Moses. The current
implementation is intended to be
released as open-source in the near
future.
1 Introduction
Corpus-based approaches have become a ma-
jor focus of Machine Translation research.
We present here a fully-fledged Example-
Based Machine Translation (EBMT) plat-
form making use of both source-language
and target-language dependency structure.
This paradigm has been explored compar-
atively less, as studies on Syntactic-based
SMT/EBMT tend to focus on constituent
trees rather than dependency trees, and
on tree-to-string rather than tree-to-tree ap-
proaches. Furthermore, we employ separate
dependency parsers for each language rather
than projecting the dependencies from one lan-
guage to another, as in (Quirk et. al, 2005).
The dependency structure information is
used end-to-end: for improving the quality
of the alignment of the translation examples,
for constraining the translation rule extraction
and for guiding the decoding. We believe that
dependency structure, which considers more
than just local context, is important in order
to generate fluent and accurate translations
of complex sentences across distant language
pairs.
Our experiments focus on technical do-
main translation for Japanese-Chinese and
Japanese-English, however our implementa-
tion is applicable to any domain and language
pair for which there exist translation examples
and dependency parsers.
A further unique characteristic of our sys-
tem is that, again contrary to the majority of
similar systems, it does not rely on precompu-
tation of translation rules. Instead it matches
each input sentence to the full database of
translation examples before extracting trans-
lation rules online. This has the merit of max-
imizing the information available when creat-
ing and combining translation rules, while re-
taining the ability to produce excellent trans-
lations for input sentences similar to an exist-
ing translation example.
The system is mostly developed in C++ and
incorporates a web-based translation interface
for ease of use. The web interface (see Fig-
ure 1) also displays information useful for error
analysis such as the list of translation exam-
ples used. Experiments are facilitated through
the inclusion of a curses-based graphical in-
terface for performing tuning and evaluation.
The decoder supports multiple threads.
We are currently making preparations for
the project to be released with an open-
source license. The code will be available at
http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/.
2 System Overview
Figure 2 shows the basic structure of the pro-
posed translation pipeline.
The training process begins with parsing
and aligning parallel sentences from the train-
79
Figure 1: A screenshot of the web interface
showing a Japanese-English translation. The
interface provides the source and target side
dependency tree, as well as the list of exam-
ples used with their alignments. The web in-
terface facilitates easy and intuitive error anal-
ysis, and can be used as a tool for computer-
aided translation.
ing corpus. Alignment uses a Bayesian sub-
tree alignment model based on dependency
trees. This contains a tree-based reorder-
ing model and can capture non-local reorder-
ings, which sequential word-based models of-
ten cannot handle effectively. The alignments
are then used to build an example database
(?translation memory?) containing ?examples?
or ?treelets? that form the hypotheses to be
combined during decoding.
Translation is performed by first parsing
an input sentence then searching for treelets
matching entries in the example database.
The retrieved treelets are combined by a de-
coder that optimizes a log linear model score.
The example retrieval and decoding steps are
explained in more detail in sections 3 and 4
respectively. The choice of features and the
tuning of the log linear model is described in
section 5.
Figure 3 shows the process of combining ex-
amples matching the input tree to create an
output sentence.
Figure 2: Translation pipeline. An example
database is first trained from a parallel cor-
pus. Translation is performed by the decoder,
which combines initial hypotheses generated
by the example retrieval module. Weights can
be improved with batch tuning.
3 Example retrieval and translation
hypothesis construction
An important characteristic of our system is
that we do not extract and store translation
rules in advance: the alignment of translation
examples is performed offline. However, for a
given input sentence i, the steps for finding
examples partially matching i and extracting
their translation hypotheses is an online pro-
cess. This approach could be considered to be
more faithful to the original EBMT approach
advocated by Nagao (1984). It has already
been proposed for phrase-based (Callison-
Burch et al., 2005), hierarchical (Lopez, 2007),
and syntax-based (Cromi?res and Kurohashi,
2011) systems. It does not however, seem to
be very commonly integrated in syntax-based
MT.
This approach has several benefits. The first
is that we are not required to impose a limit
on the size of translation hypotheses. Systems
extracting rules in advance typically restrict
the size and number of extracted rules for fear
of becoming unmanageable. In particular, if
an input sentence is the same or very similar
to one of our translation examples, we will be
able to retrieve a perfect translation. A second
advantage is that we can make use of the full
context of the example to assign features and
scores to each translation hypothesis.
The main drawback of our approach is that
it can be computationally more expensive to
retrieve arbitrarily large matchings in the ex-
80
!"
#$%&"
!""
#$%#&''("
')"
#*!)+,!-')"
.!/01)$"
'()234"
*+2546,-.274"
!284"
/2946 02:4"
#$%&2;4"
*$!<"
!"
=0$6 #$%#&''("
')"
#*!)+,!-')"
.!/01)$"
>)?@#6A*$$6 B@#?@#6A*$$6
C%!.?,$6D!#!&!+$6
,-."
'()"
*6 1"
/6 06
*$!<"
!"
E$6 &''(6
2F54" 2F74"
2F54" 2F74"
23" ?!?$*+"
2F84" 2F84"2F84"
*+6 =0$6
2GBBA4"
2F54"
2F74"
2F84"
Figure 3: The process of translation. The source sentence is parsed and matching subtrees from
the example database are retrieved. From the examples, we extract translation hypotheses than
can contain optional target words and several position for each non-terminals. For example the
translation hypothesis containing ?textbook? has three possible position for the non-terminal X3
(as a left-child before ?a?, as a left-child after ?a? or as a right-child). The translation hypotheses
are then combined during decoding. Choice of optional words and final Non-Terminal positions
is also done during decoding.
ample database online than it is to match pre-
computed rules. We use the techniques de-
scribed in (Cromi?res and Kurohashi, 2011)
to perform this step as efficiently as possible.
Once we have found an example translation
(s, t) for which s partially matches i, we pro-
ceed to extract a translation hypothesis from
it. A translation hypothesis is defined as a
generic translation rule for a part p of the in-
put sentence that is represented as a target-
language treelet, with non-terminals repre-
senting the insertion positions for the transla-
tions of other parts of the sentence. A trans-
lation hypothesis is created from a translation
example as follows:
1. We project the part of s that is matched
into the target side t using the alignment
of s and t. This is trivial if each word of
s and t is aligned, but this is not typi-
cally the case. Therefore our translation
hypotheses will often have some target
words/nodes marked as optionals: this
means that we will decide if they should
be added to the final translation only at
the moment of combination.
2. We insert the non-terminals as child
nodes of the projected subtree. This is
simple if i, s and t have the same struc-
ture and are perfectly aligned, but again
this is not typically the case. A conse-
quence is that we will sometimes have sev-
eral possible insertion positions for each
non-terminal. The choice of insertion po-
sition is again made during combination.
4 Decoding
After having extracted translation hypotheses
for as many parts of the input tree as possible,
we need to decide how to select and combine
them. Our approach here is similar to what
81
Figure 4: A translation hypothesis endoded
as a lattice. This representation allows us to
handle efficiently the ambiguities of our trans-
lation rules. Note that each path in this lat-
tice corresponds to different choices of inser-
tion position for X2, morphological forms of
?be?, and the optional insertion of ?at?.
has been proposed for Corpus-Based Machine
Translation. We first choose a number of fea-
tures and create a linear model scoring each
possible combination of hypotheses (see Sec-
tion 5). We then attempt to find the combi-
nation that maximizes this model score.
The combination of rules is constrained by
the structure of the input dependency tree. If
we only consider local features1, then a simple
bottom-up dynamic programming approach
can efficiently find the optimal combination
with linear O(|H|) complexity2. However,
non-local features (such as language models)
will force us to prune the search space. This
pruning is done efficiently through a varia-
tion of cube-pruning (Chiang, 2007). We
use KenLM3 (Heafield, 2011) for computing
the target language model score. Decoding
is made more efficient by using some of the
more advanced features of KenLM such as
state-reduction ((Li and Khudanpur, 2008),
(Heafield et al., 2011)) and rest-cost estima-
tions(Heafield et al., 2012).
Compared with the original cube-pruning
algorithm, our decoder is designed to handle
an arbitrary number of non-terminals. In ad-
dition, as we have seen in Section 3, the trans-
lation hypotheses we initially extract from ex-
amples are ambiguous in term of which target
word is going to be used and which will be the
final position of each non-terminal. In order to
handle such ambiguities, we use a lattice-based
internal representation that can encode them
efficiently (see Figure 4). This lattice represen-
tation also allows the decoder to make choices
between various morphological variations of a
1The score of a combination will be the sum of the
local scores of each translation hypothesis.
2
H = set of translation hypotheses
3http://kheafield.com/code/kenlm/
word (e.g. be/is/are).
5 Features and Tuning
During decoding we use a linear model to score
each possible combination of hypotheses. This
linear model is based on a linear combination
of both local features (local to each translation
hypothesis) and non-local features (such as a
5-gram language model score of the final trans-
lation). The decoder considers in total a com-
bination of 34 features, a selection of which are
given below.
? Example penalty and example size
? Translation probability
? Language model score
? Optional words added/removed
The optimal weights for each feature are
estimated using the Pairwise Ranking Op-
timization (PRO) algorithm (Hopkins and
May, 2011) and parameter optimization with
MegaM4. We use the implementation of PRO
that is provided with the Moses SMT system
and the default settings of MegaM.
6 Experiments
In order to evaluate our system, we conducted
translation experiments on four language
pairs: Japanese-English (JA?EN), English-
Japanese (EN?JA), Japanese-Chinese (JA?
ZH) and Chinese-Japanese (ZH?JA).
For Japanese-English, we evaluated on the
NTCIR-10 PatentMT task data (patents)
(Goto et al., 2013) and compared our system
with the official baseline scores. For Japanese-
Chinese, we used parallel scientific paper ex-
cerpts from the ASPEC5 corpus and com-
pared against the same baseline system as for
Japanese-English. The corpora contain 3M
parallel sentences for Japanese-English and
670K for Japanese-Chinese.
The two baseline systems are based on the
open-source GIZA++/Moses pipeline. The
baseline labeled ?Moses? uses the classic
phrase-based engine, while ?Moses-Hiero? uses
the Hierarchical Phrase-Based decoder. These
4http://www.umiacs.umd.edu/~hal/megam/
5http://orchid.kuee.kyoto-u.ac.jp/ASPEC/
82
System JA?EN EN?JA JA?ZH ZH?JA
Moses 28.86 33.61 32.90 42.79
Moses-Hiero 28.56 32.98 ? ?
Proposed 29.00 32.15 32.99 37.64
Table 1: Scores
System BLEU Translation
Moses 31.09 Further, the expansion stroke, the sectional area of the inner tube 12,
and the oil is supplied to the lower oil chamber S2 from the oil reservoir
chamber R ? stroke.
Moses-
Hiero
21.49 Also, the expansion stroke, the cross-sectional area of the inner tube
12 ? stroke of oil supplied from the oil reservoir chamber R lower oil
chamber S2.
Proposed 44.99 Further in this expansion stroke, the oil at an amount obtained by mul-
tiplying cross sectional area of the inner tube 12 from the oil reservoir
chamber R is resupplied to the lower oil chamber S2.
Reference 100.00 In this expansion stroke, oil in an amount obtained by multiplying the
cross sectional area of the inner tube 12 by the stroke is resupplied from
the upper oil reservoir chamber R to the lower oil chamber S2.
Table 2: Example of JA?EN translation with better translation quality than baselines.
correspond to the highest performing official
baselines for the NTCIR-10 PatentMT task.
As it appeared Moses was giving similar
and slightly higher BLEU scores than Moses-
Hiero for Japanese-English, we restricted eval-
uation to the standard settings for Moses for
our Japanese-Chinese experiments.
The following dependency parsers were
used. The scores in parentheses are the ap-
proximate parsing accuracies (micro-average),
which were evaluated by hand on a random
subset of sentences from the test data. The
parsers were trained on domains different to
those used in the experiments.
? English: NLParser6 (92%) (Charniak and
Johnson, 2005)
? Japanese: KNP (96%) (Kawahara and
Kurohashi, 2006)
? Chinese: SKP (88%) (Shen et al., 2012)
6.1 Results
The results shown are for evaluation on the
test set after tuning. Tuning was conducted
over 50 iterations on the development set using
an n-best list of length 500.
Table 2 shows an example sentence showing
significant improvement over the baseline. In
6Converted to dependency parses with in-house
tool.
particular, non-local structure has been pre-
served by the proposed system, such as the
modification of ?oil? by the ?in an amount... by
the stroke? phrase. Another example is the in-
correct location of ?? stroke? in the Moses out-
put. The proposed system produces a much
more fluent output than the hierarchical-based
baseline Moses-Hiero.
The proposed system also outperforms the
baseline for JA?ZH, however falls short for
ZH?JA. We believe this is due to the low qual-
ity of parsing for Chinese input.
The decoder requires on average 0.94 sec-
onds per sentence when loading from precom-
piled hypothesis files. As a comparison, Moses
(default settings) takes 1.78 seconds per sen-
tence, loading from a binarized and filtered
phrase table.
7 Conclusion
This paper introduces an example-based
translation system exploiting both source and
target dependency analysis and online exam-
ple retrieving, allowing the availability of full
translation examples at translation time.
We believe that the use of dependency pars-
ing is important for accurate translation across
distant language pairs, especially in settings
such as ours with many long sentences. We
have designed a complete translation frame-
83
work around this idea, using dependency-
parsed trees at each step from alignment to
example retrieval to example combination.
The current performance (BLEU) of our
system is similar to (or even slightly bet-
ter than) state-of-the-art open-source SMT
systems. As we have been able to obtain
steady performance improvements during de-
velopment, we are hopeful that this trend will
continue and we will shortly obtain even bet-
ter results. Future plans include enriching
the feature set, adding a tree-based language
model and considering forest input for multi-
ple parses to provide robustness against pars-
ing errors. When the code base is sufficiently
stable, we intend to release the entire system
as open-source, in the hope of providing a
more syntactically-focused alternative to ex-
isting open-source SMT engines.
Acknowledgements
This work was partially supported by the
Japan Science and Technology Agency. The
first author is supported by a Japanese Gov-
ernment (MEXT) research scholarship. We
would like to thank the anonymous reviewers.
References
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Dis-
criminative Reranking. In Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, ACL 2005.
Fabien Cromi?res and Sadao Kurohashi. 2011. Ef-
ficient retrieval of tree translation examples for
syntax-based machine translation. In Proceed-
ings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita
and Benjamin Tsou. 2013. Overview of
the Patent Machine Translation Task at the
NTCIR-10 Workshop. In Proceedings of the 10th
NTCIR Workshop Meeting on Evaluation of In-
formation Access Technologies (NTCIR-10).
Mark Hopkins and Jonathan May. 2011. Tuning
as Ranking. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing.
Daisuke Kawahara and Sadao Kurohashi. 2006.
A Fully-Lexicalized Probabilistic Model for
Japanese Syntactic and Case Structure Anal-
ysis. In Proceedings of the Human Language
Technology Conference of the NAACL.
Makoto Nagao. 1984. A framework of a mechan-
ical translation between Japanese and English
by analogy principle. In A. Elithorn and R.
Banerji. Artificial and Human Intelligence.
Toshiaki Nakazawa and Sadao Kurohashi. 2012.
Alignment by bilingual generation and mono-
lingual derivation. In Proceedings of COLING
2012.
Mo Shen, Daisuke Kawahara and Sadao Kuro-
hashi. 2012. A Reranking Approach for De-
pendency Parsing with Variable-sized Subtree
Features. In Proceedings of 26th Pacific Asia
Conference on Language Information and Com-
puting.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. Scaling phrase-based statistical ma-
chine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual
Meeting on Association for Computational Lin-
guistics, pages 255?262. Association for Compu-
tational Linguistics, 2005.
David Chiang. 2007. Hierarchical phrase-based
translation. In Computational Linguistics.
Kenneth Heafield. 2011. KenLM: faster and
smaller language model queries. In Proceedings
of the EMNLP 2011 Sixth Workshop on Statis-
tical Machine Translation, 2011.
Kenneth Heafield, Hieu Hoang, Philipp Koehn,
Tetsuo Kiso, and Marcello Federico. 2011.
Left language model state for syntactic ma-
chine translation. In Proceedings of the Inter-
national Workshop on Spoken Language Trans-
lation, 2011.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012. Language model rest costs and space-
efficient storage. In Proceedings of the Joint
Conference on Empirical Methods in Natural
Language Processing and Computational Natu-
ral Language Learning, 2012.
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine transla-
tion with equivalent language model state main-
tenance. In Proceedings of the Second Workshop
on Syntax and Structure in Statistical Transla-
tion. Association for Computational Linguistics,
2008.
Adam Lopez. 2007. Hierarchical phrase-based
translation with suffix arrays. In EMNLP-
CoNLL 2007.
Chris Quirk, Arul Menezes, and Colin Cherry.
2005. Dependency Treelet Translation: Syn-
tactically Informed Phrasal SMT. In Proceed-
ings of the 43rd Annual Meeting on Association
for Computational Linguistics. Association for
Computational Linguistics, 2005.
84
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 2?11,
Beijing, August 2010
Exploiting Term Importance Categories and
Dependency Relations for Natural Language Search
Keiji Shinzato
Graduate School of Informatics,
Kyoto University
shinzato@i.kyoto-u.ac.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
In this paper, we propose a method that
clearly separates terms (words and de-
pendency relations) in a natural language
query into important and other terms, and
differently handles the terms according to
their importance. The proposed method
uses three types of term importance: nec-
essary, optional, and unnecessary. The
importance are detected using linguistic
clues. We evaluated the proposed method
using a test collection for Japanese infor-
mation retrieval. Performance was resul-
tantly improved by differently handling
terms according to their importance.
1 Introduction
Currently, search engines that receive a couple of
keywords reflecting users? information needs pre-
dominate. These keyword-based searches have
been focused on evaluation conferences for infor-
mation retrieval (IR) such as TREC and NTCIR.
Search engines based on keywords, however, have
a crucial problem that it is difficult for their users
to represent complex needs, such as ?I want to
know what Steve Jobs said about the iPod.? A
natural language sentence can more adeptly ac-
commodate such information needs than a couple
of keywords because users can straightforwardly
present their needs. We call a query represented
by a sentence a natural language query (NLQ).
The other advantage of NLQs is that search
engines can leverage dependency relations be-
tween words in a given query. Dependency rela-
tions allow search engines to retrieve documents
with a similar linguistic structure to that of the
query. Search performance improvement can be
expected through the use of dependency relations.
For handling an NLQ, we can consider a con-
junctive search (AND search) that retrieves docu-
ments that include all terms in the query, a simple
methodology similar to real-world Web searches.
This methodology, however, often leads to insuf-
ficient amounts of search results. In some in-
stances, no documents match the query. This
problem occurs because the amount of search re-
sults is inversely proportional to the number of
terms used in a search; and an NLQ includes many
terms. Hence, a conjunctive search simply using
all terms in an NLQ is problematic.
Apart from this, we can consider conventional
IR methodology. This approach performs a dis-
junctive search (OR search), and then ranks re-
trieved documents according to scores that are
computed by term weights derived from retrieval
models. The methodology attempts to use term
weights to distinguish important terms and other
items. However, a problem arises in that irrelevant
documents are more highly ranked than relevant
ones when giving NLQs. This is because an NLQ
tends to contain some important terms and many
noisy (redundant) terms and document relevancy
is calculated from the combinations of these term
weights.
Avoiding the above problems, we define three
discrete categories of term importance: necessary;
optional, and unnecessary, and propose a method
that classifies words and dependency relations in
an NLQ into term importance, and then, when per-
forming document retrieval, differently handles
the terms according to their importance. The nec-
essary type includes expressions in Named Enti-
2
ties (NEs) and compound nouns, the optional in-
cludes redundant verbs and the unnecessary in-
cludes expressions that express inquiries such as
?I want to find.? The process of IR consists of two
steps: document collecting and document scor-
ing. The proposed method uses only necessary
terms for document collecting and necessary and
optional terms for document scoring.
We evaluated the proposed method using
the test collections built at the NTCIR-3 and
NTCIR-4 conferences for evaluating Japanese IR.
Search performance was resultantly improved by
differently handling terms (words and dependency
relations) according to their importance.
This paper is organized as follows. Section 2
shows related work, and section 3 describes how
to leverage dependency relations in our retrieval
method. Section 4 presents term importance cate-
gories, and section 5 gives methodology for de-
tecting such categories. Experiment results are
shown in section 6.
2 Related Work
A large amount of the IR methodology that has
been proposed (Robertson et al, 1992; Ponte and
Croft, 1998) depends on retrieval models such as
probabilistic and language models. Bendersky
and Croft (Bendersky and Croft, 2008), for in-
stance, proposed a new language model in which
important noun phrases can be considered.
IR methodology based on important term detec-
tion has also been proposed (Callan et al, 1995;
Allan et al, 1997; Liu et al, 2004; Wei et al,
2007). These previous methods have commonly
focused on noun phrases because the methods as-
sumed that a document relates to a query if the
two have common noun phrases. Liu et al (Liu et
al., 2004) classified noun phrases into four types:
proper nouns, dictionary phrases (e.g., computer
monitor), simple phrases, and complex phrases,
and detected them from a keyword-based query
by using named entity taggers, part-of-speech pat-
terns, and dictionaries such as WordNet. The
detected phrases were assigned different window
sizes in a proximity operator according to their
types. Wei et al (Wei et al, 2007) extended Liu?s
work for precisely detecting noun phrases. Their
method used hit counts obtained from Google and
Wikipedia in addition to clues used in Liu?s work.
The differences between the proposed method and
these methods are (i) the proposed method fo-
cuses on an NLQ while the previous methods fo-
cus on a keyword-based query, (ii) the proposed
method needs no dictionaries, and (iii) while the
previous methods retrieve documents by proxim-
ity searches of words in phrases, the proposed
method retrieves them by dependency relations
in phrases. Therefore, the proposed method does
not need to adjust window size, and naturally per-
forms document retrieval based on noun phrases
by using dependency relations.
Linguistically motivated IR research pointed
out that dependency relations did not con-
tribute to significantly improving performance
due to low accuracy and robustness of syntac-
tic parsers (Jones, 1999). Current state-of-the-art
parsers, however, can perform high accuracy for
real-world sentences. Therefore, dependency re-
lations are remarked in IR (Miyao et al, 2006;
Shinzato et al, 2008b). For instance, Miyao et
al. (Miyao et al, 2006) proposed an IR system for
a biomedical domain that performs deep linguis-
tic analysis on a query and each document. Their
system represented relations between words by a
predicate-argument structure, and used ontologi-
cal databases for handling synonyms. Their ex-
periments using a small number of short queries
showed that their proposed system significantly
improved search performance versus a system not
performing deep linguistic analysis. Shinzato
et al (Shinzato et al, 2008b) proposed a Web
search system that handles not only words but
also dependency relations as terms; yet they did
not discuss the effectiveness of dependency rela-
tions. This paper reveals the effectiveness of de-
pendency relations through experiments using test
collections for Japanese Web searches.
3 Exploitation of Dependency Relation
One of the advantages of an NLQ is leveraging
dependency relations between words in the query.
We can expect that search performance improves
because the dependency relations allow systems
to retrieve documents that have similar linguistic
structure to that of the query. Therefore the pro-
posed method exploits dependency relations for
3
 



return to


	





spectacular




 


active







Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 12?20,
Beijing, August 2010
Summarizing Search Results using PLSI
Jun Harashima? and Sadao Kurohashi
Graduate School of Informatics
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
{harashima,kuro}@nlp.kuee.kyoto-u.ac.jp
Abstract
In this paper, we investigate generating
a set of query-focused summaries from
search results. Since there may be many
topics related to a given query in the
search results, in order to summarize
these results, they should first be clas-
sified into topics, and then each topic
should be summarized individually. In
this summarization process, two types of
redundancies need to be reduced. First,
each topic summary should not contain
any redundancy (we refer to this prob-
lem as redundancy within a summary).
Second, a topic summary should not be
similar to any other topic summary (we
refer to this problem as redundancy be-
tween summaries). In this paper, we
focus on the document clustering pro-
cess and the reduction of redundancy be-
tween summaries in the summarization
process. We also propose a method using
PLSI to summarize search results. Eval-
uation results confirm that our method
performs well in classifying search re-
sults and reducing the redundancy be-
tween summaries.
1 Introduction
Currently, the World Wide Web contains vast
amounts of information. To make efficient use of
this information, search engines are indispens-
able. However, search engines generally return
*Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
only a long list containing the title and a snip-
pet of each of the retrieved documents. While
such lists are effective for navigational queries,
they are not helpful to users with informational
queries. Some systems (e.g., Clusty1) present
keywords related to a given query together with
the search results. It is, however, difficult for
users to understand the relation between the key-
words and the query, as the keywords are merely
words or phrases out of context. To solve this
problem, we address the task of generating a set
of query-focused summaries from search results
to present information about a given query using
natural sentences.
Since there are generally many topics re-
lated to a query in the search results, the task
of summarizing these results is one of, so to
speak, multi-topic multi-document summariza-
tion. Studies on multi-document summariza-
tion typically address summarizing documents
related to a single topic (e.g., TAC2). However
we need to address summarizing documents re-
lated to multiple topics when considering the
summarization of search results.
To summarize documents containing multiple
topics, we first need to classify them into top-
ics. For example, if a set of documents related to
swine flu contains topics such as the outbreaks of
swine flu, the measures to treat swine flu, and so
on, the documents should be divided into these
topics and summarized individually. Note that a
method for soft clustering should be employed
in this process, as one document may belong to
several topics.
1http://clusty.com/
2http://www.nist.gov/tac/
12
In the summarization process, two types of
redundancies need to be addressed. First, each
topic summary should not contain any redun-
dancy. We refer to this problem as redun-
dancy within a summary. This problem is well
known in the field of multi-document summa-
rization (Mani, 2001) and several methods have
been proposed to solve it, such as Maximum
Marginal Relevance (MMR) (Goldstein et al,
2000) (Mori et al, 2004), using Integer Lin-
ear Programming (ILP) (Filatova and Hatzivas-
siloglou, 2004) (McDonald, 2007) (Takamura
and Okumura, 2009), and so on.
Second, no topic summary should be similar
to any of the other topic summaries. We re-
fer to this problem as redundancy between sum-
maries. For example, to summarize the above-
mentioned documents related to swine flu, the
summary for outbreaks should contain specific
information about outbreaks, whereas the sum-
mary for measures should contain specific infor-
mation about measures. This problem is char-
acteristic of multi-topic multi-document summa-
rization. Some methods have been proposed
to generate topic summaries from documents
(Radev and Fan, 2000) (Haghighi and Vander-
wende, 2009), but to the best of our knowledge,
the redundancy between summaries has not yet
been addressed in any study.
In this paper, we focus on the document clus-
tering process and the reduction of redundancy
between summaries in the summarization pro-
cess. Furthermore, we propose a method using
PLSI (Hofmann, 1999) to summarize search re-
sults. In the proposed method, we employ PLSI
to estimate the membership degree of each doc-
ument to each topic, and then classify the search
results into topics using this information. In the
same way, we employ PLSI to estimate the mem-
bership degree of each keyword to each topic,
and then extract the important sentences spe-
cific to each topic using this information in order
to reduce the redundancy between summaries.
The evaluation results show that our method per-
forms well in classifying search results and suc-
cessfully reduces the redundancy between sum-
maries.
	
	
D
	

	
 Dz
  
Sz	

  
Sz?

 
		 
	


 	 
 	

	

 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 34?42,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Chinese?Japanese Parallel Sentence Extraction
from Quasi?Comparable Corpora
Chenhui Chu, Toshiaki Nakazawa, Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan
{chu,nakazawa}@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
Abstract
Parallel sentences are crucial for statistical
machine translation (SMT). However, they
are quite scarce for most language pairs,
such as Chinese?Japanese. Many studies
have been conducted on extracting parallel
sentences from noisy parallel or compara-
ble corpora. We extract Chinese?Japanese
parallel sentences from quasi?comparable
corpora, which are available in far larger
quantities. The task is significantly more
difficult than the extraction from noisy
parallel or comparable corpora. We ex-
tend a previous study that treats parallel
sentence identification as a binary classifi-
cation problem. Previous method of clas-
sifier training by the Cartesian product is
not practical, because it differs from the
real process of parallel sentence extrac-
tion. We propose a novel classifier train-
ing method that simulates the real sentence
extraction process. Furthermore, we use
linguistic knowledge of Chinese character
features. Experimental results on quasi?
comparable corpora indicate that our pro-
posed approach performs significantly bet-
ter than the previous study.
1 Introduction
In statistical machine translation (SMT) (Brown
et al, 1993; Koehn et al, 2007), the quality
and quantity of the parallel sentences are cru-
cial, because translation knowledge is acquired
from a sentence?level aligned parallel corpus.
However, except for a few language pairs, such
as English?French, English?Arabic and English?
Chinese, parallel corpora remain a scarce re-
source. The cost of manual construction for paral-
lel corpora is high. As non?parallel corpora are far
more available, constructing parallel corpora from
non?parallel corpora is an attractive research field.
Non?parallel corpora include various levels of
comparability: noisy parallel, comparable and
quasi?comparable. Noisy parallel corpora con-
tain non?aligned sentences that are nevertheless
mostly bilingual translations of the same docu-
ment, comparable corpora contain non?sentence?
aligned, non?translated bilingual documents that
are topic?aligned, while quasi?comparable cor-
pora contain far more disparate very?non?parallel
bilingual documents that could either be on the
same topic (in?topic) or not (out?topic) (Fung and
Cheung, 2004). Most studies focus on extracting
parallel sentences from noisy parallel corpora or
comparable corpora, such as bilingual news ar-
ticles (Zhao and Vogel, 2002; Utiyama and Isa-
hara, 2003; Munteanu andMarcu, 2005; Tillmann,
2009; Abdul-Rauf and Schwenk, 2011), patent
data (Utiyama and Isahara, 2007; Lu et al, 2010)
and Wikipedia (Adafre and de Rijke, 2006; Smith
et al, 2010). Few studies have been conducted
on quasi?comparable corpora. Quasi?comparable
corpora are available in far larger quantities than
noisy parallel or comparable corpora, while the
parallel sentence extraction task is significantly
more difficult.
While most studies are interested in language
pairs between English and other languages, we
focus on Chinese?Japanese, where parallel cor-
pora are very scarce. This study extracts
Chinese?Japanese parallel sentences from quasi?
comparable corpora. We adopt a system pro-
posed by Munteanu and Marcu (2005), which is
for parallel sentence extraction from comparable
corpora. We extend the system in several aspects
to make it even suitable for quasi?comparable cor-
pora. The core component of the system is a clas-
sifier which can identify parallel sentences from
non?parallel sentences. Previous method of clas-
sifier training by the Cartesian product is not prac-
tical, because it differs from the real process of
parallel sentence extraction. We propose a novel
34
Translated Ja
sentences as 
queriesSMT
IR: top N results
Common
Chinese
characters
Candidate 
sentence 
pairs
Parallel
sentencesFiltering 
Chinese 
corpora
Japanese 
corpora
ClassifierProbabilisticdictionary
(2)
(1)
(3)
(4)
Zh-Ja parallel 
corpus
5k sentences
Figure 1: Parallel sentence extraction system.
method of classifier training and testing that sim-
ulates the real sentence extraction process, which
guarantees the quality of the extracted sentences.
Since Chinese characters are used both in Chi-
nese and Japanese, they can be powerful linguistic
clues to identify parallel sentences. Therefore, we
use Chinese character features, which significantly
improve the accuracy of the classifier. We con-
duct parallel sentence extraction experiments on
quasi?comparable corpora, and evaluate the qual-
ity of the extracted sentences from the perspective
of MT performance. Experimental results show
that our proposed system performs significantly
better than the previous study.
2 Parallel Sentence Extraction System
The overview of our parallel sentence extraction
system is presented in Figure 1. Source sentences
are translated to target language using a SMT sys-
tem (1). We retrieve the top N documents from tar-
get language corpora with a information retrieval
(IR) framework, using the translated sentences as
queries (2). For each source sentence, we treat
all target sentences in the retrieved documents as
candidates. Then, we pass the candidate sentence
pairs through a sentence ratio filter and a word?
overlap?based filter based on a probabilistic dic-
tionary, to reduce the candidates keeping more re-
liable sentences (3). Finally, a classifier trained on
a small number of parallel sentences, is used to
identify the parallel sentences from the candidates
(4). A parallel corpus is needed to train the SMT
system, generate the probabilistic dictionary and
train the classifier.
Our system is inspired by Munteanu and Marcu
(2005), however, there are several differences. The
first difference is query generation. Munteanu and
Marcu (2005) generate queries by taking the top
N translations of each source word according to
the probabilistic dictionary. This method is im-
precise due to the noise in the dictionary. In-
stead, we adopt a method proposed by Abdul?
Rauf and Schwenk (2011). We translate the source
sentences to target language with a SMT system
trained on the parallel corpus. Then use the trans-
lated sentences as queries. This method can gen-
erate more precise queries, because phrase?based
MT is better than word?based translation.
Another difference is that we do not conduct
document matching. The reason is that docu-
ments on the same topic may not exist in quasi?
comparable corpora. Instead, we retrieve the top
N documents for each source sentence. In com-
parable corpora, it is reasonable to only use the
best target sentence in the retrieved documents as
candidates (Abdul-Rauf and Schwenk, 2011). In
quasi?comparable corpora, it is important to fur-
ther guarantee the recall. Therefore, we keep all
target sentences in the retrieved documents as can-
didates.
Our system also differs by the way of classi-
fier training and testing, which is described in Sec-
tion 3 in detail.
3 Binary Classification of Parallel
Sentence Identification
Parallel sentence identification from non?parallel
sentences can be seen as a binary classification
problem (Munteanu and Marcu, 2005; Tillmann,
2009; Smith et al, 2010; S?tefa?nescu et al, 2012).
35
Since the quality of the extracted sentences is de-
termined by the accuracy of the classifier, the clas-
sifier becomes the core component of the extrac-
tion system. In this section, we first describe the
training and testing process, then introduce the
features we use for the classifier.
3.1 Training and Testing
Munteanu and Marcu (2005) propose a method of
creating training and test instances for the classi-
fier. They use a small number of parallel sentences
as positive instances, and generate non?parallel
sentences from the parallel sentences as negative
instances. They generate all the sentence pairs
except the original parallel sentence pairs in the
Cartesian product, and discard the pairs that do not
fulfill the condition of a sentence ratio filter and a
word?overlap?based filter. Furthermore, they ran-
domly discard some of the non?parallel sentences
when necessary, to guarantee the ratio of negative
to positive instances smaller than five for the per-
formance of the classifier.
Creating instances by using the Cartesian prod-
uct is not practical, because it differs from the real
process of parallel sentence extraction. Here, we
propose a novel method of classifier training and
testing that simulates the real parallel sentence ex-
traction process. For training, we first select 5k
parallel sentences from a parallel corpus. Then
translate the source side of the selected sentences
to target language with a SMT system trained on
the parallel corpus excluding the selected parallel
sentences. We retrieve the top N documents from
the target language side of the parallel corpus, us-
ing the translated sentences as queries. For each
source sentence, we consider all target sentences
in the retrieved documents as candidates. Finally,
we pass the candidate sentence pairs through a
sentence ratio filter and a word?overlap?based fil-
ter, and get the training instances. We treat the
sentence pairs that exist in the original 5k parallel
sentences as positive instances, while the remain-
der as negative instances. Note that positive in-
stances may be less than 5k, because some of the
parallel sentences do not pass the IR framework
and the filters. For the negative instances, we also
randomly discard some of them when necessary,
to guarantee the ratio of negative to positive in-
stances smaller than five. Test instances are gen-
erated by another 5k parallel sentences from the
parallel corpus using the same method.
There are several merits of the proposed
method. It can guarantee the quality of the ex-
tracted sentences, because of the similarity be-
tween the real sentence extraction process. Also,
features from the IR results can be used to further
improve the accuracy of the classifier. The pro-
posed method can be evaluated not only on the
test sentences that passed the IR framework and
the filters, but also on all the test sentences, which
is similar to the evaluation for the real extraction
process. However, there is a limitation of our
method that a both sentence?level and document?
level aligned parallel corpus is needed.
3.2 Features
3.2.1 Basic Features
The following features are the basic features we
use for the classifier, which are proposed by
Munteanu and Marcu (2005):
? Sentence length, length difference and length
ratio.
? Percentage of words on each side that have a
translation on the other side (according to the
probabilistic dictionary).
? Alignment features:
? Percentage and number of words that
have no connection.
? The top three largest fertilities.
? Length of the longest contiguous con-
nected span.
? Length of the longest unconnected sub-
string.
Alignment features are extracted from the align-
ment results of the parallel and non?parallel sen-
tences used as instances for the classifier. Note
that alignment features may be unreliable when
the quantity of non?parallel sentences is signifi-
cantly larger than parallel sentences.
3.2.2 Chinese Character Features
Different from other language pairs, Chinese and
Japanese share Chinese characters. In Chinese
the Chinese characters are called Hanzi, while in
Japanese they are called Kanji. Hanzi can be di-
vided into two groups, Simplified Chinese (used
in mainland China and Singapore) and Traditional
Chinese (used in Taiwan, Hong Kong and Macau).
The number of strokes needed to write characters
36
????????????????????
????????????????????????????????
Wash ether phase with saturated saline,  and dry it with anhydrous magnesium.
Zh:
Ja:
Ref:
Figure 2: Example of common Chinese characters in a Chinese?Japanese parallel sentence pair.
Meaning snow love begin
TC ? (U+96EA) ? (U+611B) ? (U+767C)
SC ? (U+96EA) ?(U+7231) ?(U+53D1)
Kanji ? (U+96EA) ? (U+611B) ? (U+767A)
Table 1: Examples of common Chinese characters
(TC denotes Traditional Chinese and SC denotes
Simplified Chinese).
has been largely reduced in Simplified Chinese,
and the shapes may be different from those in Tra-
ditional Chinese. Because Kanji characters origi-
nated from ancient China, many common Chinese
characters exist between Hanzi and Kanji. Table 1
gives some examples of common Chinese char-
acters in Traditional Chinese, Simplified Chinese
and Japanese with their Unicode.
Since Chinese characters contain significant se-
mantic information, and common Chinese charac-
ters share the same meaning, they can be valuable
linguistic clues for many Chinese?Japanese NLP
tasks. Many studies have exploited common Chi-
nese characters. Tan et al (1995) used the occur-
rence of identical common Chinese characters in
Chinese and Japanese (e.g. ?snow? in Table 1) in
automatic sentence alignment task for document?
level aligned text. Goh et al (2005) detected com-
mon Chinese characters where Kanji are identical
to Traditional Chinese, but different from Simpli-
fied Chinese (e.g. ?love? in Table 1). Using a Chi-
nese encoding converter1 that can convert Tradi-
tional Chinese into Simplified Chinese, they built
a Japanese?Simplified Chinese dictionary partly
using direct conversion of Japanese into Chinese
for Japanese Kanji words. Chu et al (2011) made
use of the Unihan database2 to detect common
Chinese characters which are visual variants of
each other (e.g. ?begin? in Table 1), and proved
the effectiveness of common Chinese characters
in Chinese?Japanese phrase alignment. Chu et
al. (2012a) exploited common Chinese charac-
ters in Chinese word segmentation optimization,
which improved the translation performance.
In this study, we exploit common Chinese char-
1http://www.mandarintools.com/zhcode.html
2http://unicode.org/charts/unihan.html
acters in parallel sentence extraction. Chu et
al. (2011) investigated the coverage of common
Chinese characters on a scientific paper abstract
parallel corpus, and showed that over 45% Chi-
nese Hanzi and 75% Japanese Kanji are common
Chinese characters. Therefore, common Chinese
characters can be powerful linguistic clues to iden-
tify parallel sentences.
We make use of the Chinese character map-
ping table created by Chu et al (2012b) to de-
tect common Chinese characters. Following fea-
tures are used. We use an example of Chinese?
Japanese parallel sentence presented in Figure 2 to
explain the features in detail, where common Chi-
nese characters are in bold and linked with dotted
lines.
? Number of Chinese characters on each side
(Zh: 18, Ja: 14).
? Percentage of Chinese characters out of all
characters on each side (Zh: 18/20=90%, Ja:
14/32=43%).
? Ratio of Chinese character numbers on both
sides (18/14=128%).
? Number of n?gram common Chinese charac-
ters (1?gram: 12, 2?gram: 6, 3?gram: 2, 4?
gram: 1).
? Percentage of n?gram common Chinese char-
acters out of all n?gram Chinese characters
on each side (Zh: 1?gram: 12/18=66%, 2?
gram: 6/16=37%, 3?gram: 2/14=14%, 4?
gram: 1/12=8%; Ja: 1?gram: 12/14=85%,
2?gram: 6/9=66%, 3?gram=: 2/5=40%, 4?
gram: 1/3=33%).
Note that Chinese character features are only
applicable to Chinese?Japanese. However, since
Chinese and Japanese character information is a
kind of cognates (words or languages which have
the same origin), the similar idea can be applied to
other language pairs by using cognates. Cognates
among European languages have been shown ef-
fective in word alignments (Kondrak et al, 2003).
We also can use cognates for parallel sentence ex-
traction.
37
3.3 Rank Feature
One merit of our classifier training and testing
method is that features from the IR results can be
used. Here, we use the ranks of the retrieved doc-
uments returned by the IR framework as feature.
4 Experiments
We conducted classification and translation exper-
iments to evaluate the effectiveness of our pro-
posed parallel sentence extraction system.
4.1 Data
4.1.1 Parallel Corpus
The parallel corpus we used is a scientific
paper abstract corpus provided by JST3 and
NICT4. This corpus was created by the Japanese
project ?Development and Research of Chinese?
Japanese Natural Language Processing Technol-
ogy?, containing various domains such as chem-
istry, physics, biology and agriculture etc. This
corpus is aligned in both sentence?level and
document?level, containing 680k sentences and
100k articles.
4.1.2 Quasi?Comparable Corpora
The quasi?comparable corpora we used are scien-
tific paper abstracts collected from academic web-
sites. The Chinese corpora were collected from
CNKI5, containing 420k sentences and 90k arti-
cles. The Japanese corpora were collected from
CiNii6 web portal, containing 5M sentences and
880k articles. Note that since the paper abstracts
in these two websites were written by Chinese and
Japanese researchers respectively through differ-
ent periods, documents on the same topic may not
exist in the collected corpora. We investigated
the domains of the Chinese and Japanese corpora
in detail. We found that most documents in the
Chinese corpora belong to the domain of chem-
istry. While the Japanese corpora contain various
domains such as chemistry, physics, biology and
computer science etc. However, the domain infor-
mation is unannotated in both corpora.
4.2 Classification Experiments
We conducted experiments to evaluate the accu-
racy of the proposed method of classification, us-
3http://www.jst.go.jp
4http://www.nict.go.jp
5http://www.cnki.net
6http://ci.nii.ac.jp
ing different 5k parallel sentences from the paral-
lel corpus as training and test data.
4.2.1 Settings
? Probabilistic dictionary: We took the top
5 translations with translation probability
larger than 0.1 created from the parallel cor-
pus.
? IR tool: Indri7 with the top 10 results.
? Segmenter: For Chinese, we used a
segmenter optimized for Chinese?Japanese
SMT (Chu et al, 2012a). For Japanese, we
used JUMAN (Kurohashi et al, 1994).
? Alignment: GIZA++8.
? SMT: We used the state?of?the?art phrase?
based SMT toolkit Moses (Koehn et al,
2007) with default options, except for the dis-
tortion limit (6?20).
? Classifier: LIBSVM9 with 5?fold cross?
validation and radial basis function (RBF)
kernel.
? Sentence ratio filter threshold: 2.
? Word?overlap?based filter threshold: 0.25.
? Classifier probability threshold: 0.5.
4.2.2 Evaluation
We evaluate the performance of classification by
computing precision, recall and F?value, defined
as:
precision = 100? classified well
classified parallel
, (1)
recall = 100? classified well
true parallel
, (2)
F ? value = 2? precision ? recall
precision + recall
. (3)
Where classified well is the number of pairs
that the classifier correctly identified as parallel,
classified parallel is the number of pairs that
the classifier identified as parallel, true parallel
is the number of real parallel pairs in the test set.
Note that we only use the top 1 result identified as
parallel by the classifier for evaluation.
7http://www.lemurproject.org/indri
8http://code.google.com/p/giza-pp
9http://www.csie.ntu.edu.tw/?cjlin/libsvm
38
Features Precision Recall F?value
Munteanu+ 2005 88.43 85.20/79.76 86.78/83.87
+Chinese character 91.62 93.63/87.66 92.61/89.60
+Rank 92.15 94.53/88.50 93.32/90.29
Table 2: Classification results for the filtered test
sentences (before ?/?) and all the test sentences
(after ?/?).
4.2.3 Results
We conducted classification experiments, compar-
ing the following three experimental settings:
? Munteanu+ 2005: Only using the features
proposed by Munteanu and Marcu (2005).
? +Chinese character: Add the Chinese charac-
ter features.
? +Rank: Further add the rank feature.
Results evaluated for the test sentences that
passed the IR framework and the filters, and all
the test sentences are shown in Table 2. We can
see that the Chinese character features can signifi-
cantly improve the accuracy. The accuracy can be
further improved by the rank feature.
4.3 Translation Experiments
We extracted parallel sentences from the quasi?
comparable corpora, and evaluated Chinese?to?
Japanese MT performance by appending the ex-
tracted sentences to two baseline settings.
4.3.1 Settings
? Baseline: Using all the 680k parallel sen-
tences in the parallel corpus as training data
(containing 11k sentences of chemistry do-
main).
? Tuning: Using another 368 sentences of
chemistry domain.
? Test: Using another 367 sentences of chem-
istry domain.
? Language model: 5?gram LM trained on the
Japanese side of the parallel corpus (680k
sentences) using SRILM toolkit10.
? Classifier probability threshold: 0.6.
10http://www.speech.sri.com/projects/srilm
Classifier # sentences
Munteanu+ 2005 (Cartesian) 27,077
Munteanu+ 2005 (Proposed) 5,994
+Chinese character (Proposed) 3,936
+Rank (Proposed) 3,516
Table 3: Number of extracted sentences.
The reason we evaluate on chemistry domain is
the one we described in Section 4.1.2 that most
documents in the Chinese corpora belong to the
domain of chemistry. We keep all the sentence
pairs rather than the top 1 result (used in the clas-
sification evaluation) identified as parallel by the
classifier. The other settings are the same as the
ones used in the classification experiments.
4.3.2 Results
Numbers of extracted sentences using different
classifiers are shown in Table 3, where
? Munteanu+ 2005 (Cartesian): Classifier
trained using the Cartesian product, and only
using the features proposed by Munteanu and
Marcu (2005).
? Munteanu+ 2005 (Proposed): Classifier
trained using the proposed method, and only
using the features proposed by Munteanu and
Marcu (2005).
? +Chinese character (Proposed): Add the Chi-
nese character features.
? +Rank (Proposed): Further add the rank fea-
ture.
We can see that the extracted number is signif-
icantly decreased by the proposed method com-
pared to the Cartesian product, which may indi-
cate the quality improvement of the extracted sen-
tences. Adding more features further decreases the
number.
We conducted Chinese?to?Japanese translation
experiments by appending the extracted sentences
to the baseline. BLEU?4 scores for experiments
are shown in Table 4. We can see that our proposed
method of classifier training performs better than
the Cartesian product. Adding the Chinese charac-
ter features and rank feature further improves the
translation performance significantly.
39
Example 1
Zh: ??????????????????
(Finally, this article explains the physical meaning of the optical operator.)
Ja: ?????????????????????????????
(Finally, briefly explain the physical meaning of the chemical potential.)
Example 2
Zh: ?????????????????????
(Discussion of detection limit and measurement methods of emission spectral  analysis method.)
Ja: ??????????????????????
(Detection limit of emission spectral analysis method by photoelectric photometry.)
Figure 3: Examples of extracted sentences (parallel subsentential fragments are in bold).
System BLEU
Baseline 38.64
Munteanu+ 2005 (Cartesian) 38.10
Munteanu+ 2005 (Proposed) 38.54
+Chinese character (Proposed) 38.87?
+Rank (Proposed) 39.47??
Table 4: BLEU scores for Chinese?to?Japanese
translation experiments (??? and ??? denotes the
result is better than ?Munteanu+ 2005 (Cartesian)?
significantly at p < 0.05 and p < 0.01 respec-
tively, ?*? denotes the result is better than ?Base-
line? significantly at p < 0.01).
4.3.3 Discussion
The translation results indicate that compared to
the previous study, our proposed method can ex-
tract sentences with better qualities. However,
when we investigated the extracted sentences, we
found that most of the extracted sentences are
not sentence?level parallel. Instead, they contain
many parallel subsentential fragments. Figure 3
presents two examples of sentence pairs extracted
by ?+Rank (Proposed)?, where parallel subsenten-
tial fragments are in bold. We investigated the
alignment results of the extracted sentences. We
found that most of the parallel subsentential frag-
ments were correctly aligned with the help of the
parallel sentences in the baseline system. There-
fore, translation performance was improved by ap-
pending the extracted sentences. However, it also
led to many wrong alignments among the non?
parallel fragments which are harmful to transla-
tion. In the future, we plan to further extract
these parallel subsentential fragments, which can
be more effective for SMT (Munteanu and Marcu,
2006).
5 Related Work
As parallel sentences trend to appear in similar
document pairs, many studies first conduct doc-
ument matching, then identify the parallel sen-
tences from the matched document pairs (Utiyama
and Isahara, 2003; Fung and Cheung, 2004;
Munteanu and Marcu, 2005). Approaches with-
out document matching also have been proposed
(Tillmann, 2009; Abdul-Rauf and Schwenk, 2011;
S?tefa?nescu et al, 2012). These studies directly re-
trieve candidate sentence pairs, and select the par-
allel sentences using some filtering methods. We
adopt a moderate strategy, which retrieves candi-
date documents for sentences.
The way of parallel sentence identification can
be specified with two different approaches: bi-
nary classification (Munteanu and Marcu, 2005;
Tillmann, 2009; Smith et al, 2010; S?tefa?nescu
et al, 2012) and translation similarity measures
(Utiyama and Isahara, 2003; Fung and Cheung,
2004; Abdul-Rauf and Schwenk, 2011). We adopt
the binary classification approach with a novel
classifier training and testing method and Chinese
character features.
Few studies have been conducted for extract-
ing parallel sentences from quasi?comparable cor-
pora. We are aware of only two previous efforts.
Fung and Cheung (2004) proposed a multi-level
bootstrapping approach. Wu and Fung (2005) ex-
ploited generic bracketing Inversion Transduction
Grammars (ITG) for this task. Our approach dif-
fers from the previous studies that we extend the
approach for comparable corpora in several as-
pects to make it work well for quasi?comparable
corpora.
6 Conclusion and Future Work
In this paper, we proposed a novel method of clas-
sifier training and testing that simulates the real
parallel sentence extraction process. Furthermore,
we used linguistic knowledge of Chinese charac-
ter features. Experimental results of parallel sen-
tence extraction from quasi?comparable corpora
indicated that our proposed system performs sig-
nificantly better than the previous study.
40
Our approach can be improved in several as-
pects. One is bootstrapping, which has been
proven effective in some related works (Fung and
Cheung, 2004; Munteanu and Marcu, 2005). In
our system, bootstrapping can be done not only
for extension of the probabilistic dictionary, but
also for improvement of the SMT system used to
translate the source language to target language for
query generation. Moreover, as parallel sentences
rarely exist in quasi?comparable corpora, we plan
to extend our system to parallel subsentential frag-
ment extraction. Our study showed that Chi-
nese character features are helpful for Chinese?
Japanese parallel sentence extraction. We plan to
apply the similar idea to other language pairs by
using cognates.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2011. Par-
allel sentence generation from comparable corpora
for improved smt. Machine Translation, 25(4):341?
375.
Sisay Fissaha Adafre and Maarten de Rijke. 2006.
Finding similar sentences across multiple languages
in wikipedia. In Proceedings of EACL, pages 62?69.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Association for Computational
Linguistics, 19(2):263?312.
Chenhui Chu, Toshiaki Nakazawa, and Sadao Kuro-
hashi. 2011. Japanese-chinese phrase alignment
using common chinese characters information. In
Proceedings of MT Summit XIII, pages 475?482, Xi-
amen, China, September.
Chenhui Chu, Toshiaki Nakazawa, Daisuke Kawahara,
and Sadao Kurohashi. 2012a. Exploiting shared
Chinese characters in Chinese word segmentation
optimization for Chinese-Japanese machine transla-
tion. In Proceedings of the 16th Annual Conference
of the European Association for Machine Transla-
tion (EAMT?12), Trento, Italy, May.
Chenhui Chu, Toshiaki Nakazawa, and Sadao Kuro-
hashi. 2012b. Chinese characters mapping table of
Japanese, Traditional Chinese and Simplified Chi-
nese. In Proceedings of the Eighth Conference on
International Language Resources and Evaluation
(LREC?12), Istanbul, Turkey, May.
Dan S?tefa?nescu, Radu Ion, and Sabine Hunsicker.
2012. Hybrid parallel sentence mining from com-
parable corpora. In Proceedings of the 16th Annual
Conference of the European Association for Ma-
chine Translation (EAMT?12), Trento, Italy, May.
Pascale Fung and Percy Cheung. 2004. Multi-level
bootstrapping for extracting parallel sentences from
a quasi-comparable corpus. In Proceedings of Col-
ing 2004, pages 1051?1057, Geneva, Switzerland,
Aug 23?Aug 27. COLING.
Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2005. Building a Japanese-Chinese dic-
tionary using kanji/hanzi conversion. In Proceed-
ings of the International Joint Conference on Natu-
ral Language Processing, pages 670?681.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical transla-
tion models. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 46?48.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improve-
ments of Japanese morphological analyzer JUMAN.
In Proceedings of the International Workshop on
Sharable Natural Language, pages 22?28.
Bin Lu, Tao Jiang, Kapo Chow, and Benjamin K. Tsou.
2010. Building a large english-chinese parallel cor-
pus from comparable patents and its experimental
application to smt. In Proceedings of the 3rd Work-
shop on Building and Using Comparable Corpora,
LREC 2010, pages 42?49, Valletta, Malta, May.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504, December.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 81?88, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
403?411, Los Angeles, California, June. Associa-
tion for Computational Linguistics.
41
Chew Lim Tan and Makoto Nagao. 1995. Automatic
alignment of Japanese-Chinese bilingual texts. IE-
ICE Transactions on Information and Systems, E78-
D(1):68?76.
Christoph Tillmann. 2009. A beam-search extrac-
tion algorithm for comparable data. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 225?228, Suntec, Singapore, August. Associ-
ation for Computational Linguistics.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning japanese-english news articles
and sentences. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 72?79, Sapporo, Japan, July. Associ-
ation for Computational Linguistics.
Masao Utiyama and Hitoshi Isahara. 2007. A
japanese-english patent parallel corpus. In Proceed-
ings of MT summit XI, pages 475?482.
Dekai Wu and Pascale Fung. 2005. Inversion trans-
duction grammar constraints for mining parallel sen-
tences from quasi-comparable corpora. In IJCNLP,
pages 257?268.
Bing Zhao and Stephan Vogel. 2002. Adaptive paral-
lel sentences mining from web abilingual news col-
lections. In Proceedings of the 2002 IEEE Interna-
tional Conference on Data Mining, pages 745?748,
Maebashi City, Japan. IEEE Computer Society.
42
