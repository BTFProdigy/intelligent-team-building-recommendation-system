Proceedings of the ACL Student Research Workshop, pages 9?15,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Crawling microblogging services to gather language-classified URLs
Workflow and case study
Adrien Barbaresi
ICAR Lab
ENS Lyon & University of Lyon
15 parvis Rene? Descartes, 69007 Lyon, France
adrien.barbaresi@ens-lyon.fr
Abstract
We present a way to extract links from
messages published on microblogging
platforms and we classify them according
to the language and possible relevance of
their target in order to build a text cor-
pus. Three platforms are taken into con-
sideration: FriendFeed, identi.ca and Red-
dit, as they account for a relative diver-
sity of user profiles and more importantly
user languages. In order to explore them,
we introduce a traversal algorithm based
on user pages. As we target lesser-known
languages, we try to focus on non-English
posts by filtering out English text. Us-
ing mature open-source software from the
NLP research field, a spell checker (as-
pell) and a language identification sys-
tem (langid.py), our case study and
our benchmarks give an insight into the
linguistic structure of the considered ser-
vices.
1 Introduction
1.1 The ?Web as Corpus? paradigm
The state of the art tools of the ?Web as Corpus?
framework rely heavily on URLs obtained from
search engines. As a matter of fact, the approach
followed by the most researchers of this field con-
sists in querying search engines (e.g. by tuples)
to gather links that are crawled in order to build a
corpus (Baroni et al, 2009).
This method could be used in free corpus build-
ing approach until recently, when it was made im-
possible because of increasing limitations on the
search engines? APIs, which make the gathering
process on a low budget very slow or impossible.
All in all, the APIs may be too expensive and/or
too unstable in time to support large-scale corpus
building projects.
Moreover, the question whether the method
used so far, i.e. randomizing keywords, provides
a good overview of a language is still open. Other
technical difficulties include diverse and partly un-
known search biases due, in part, to search en-
gine optimization tricks as well as undocumented
PageRank adjustments. Using diverse sources of
seed URLs could at least ensure that there is not a
single bias, but several ones.
The crawling method using these seeds for cor-
pus building may then yield better results, e.g.
ensure better randomness in a population of web
documents as described by (Henzinger et al,
2000).
1.2 User-based URL gathering
Our hypothesis is that microblogging services are
a good alternative to overcome the limitations of
seed URL collections and the biases implied by
search engine optimization techniques, PageRank
and link classification.
It is a user-based language approach. Its obvi-
ous limits are the amount of spam and advertise-
ment. Its obvious bias consists in the technology-
prone users who are familiar with these platforms
and account for numerous short messages which
in turn over-represent their own interests and hob-
bies.
However, user-related biases also have advan-
tages, most notably the fact that documents that
are most likely to be important are being shared,
which has benefits when it comes to gather links
in lesser-known languages, below the English-
speaking spammer?s radar.
1.3 Interest
The main goal is to provide well-documented,
feature-rich software and databases relevant for
linguistic studies. More specifically, we would
like to be able to cover languages which are more
rarely seen on the Internet, which implies the gath-
9
ering of higher proportions of URLs leading to
lesser-known languages. We think that social net-
works and microblogging services may be of great
help when it comes to focus on them.
In fact, the most engaged social networking na-
tions do arguably not use English as a first com-
municating language1. In addition, crawling these
services gives an opportunity to perform a case
study of existing tools and platforms.
Finally, the method presented here could be
used in other contexts : microtext collections, user
lists and relations could prove useful for microtext
corpus building, network visualization or social
network sampling purposes (Gjoka et al, 2011).
2 Data Sources
FriendFeed, identi.ca and Reddit are taken into
consideration for this study. These services pro-
vide a good overview of the peculiarities of social
networks. At least by the last two of them a crawl
appears to be manageable in terms of both API ac-
cessibility and corpus size, which is not the case
concerning Twitter for example.
2.1 identi.ca
identi.ca is a social microblogging service built on
open source tools and open standards, which is the
reason why we chose to crawl it at first.
The advantages compared to Twitter include the
Creative Commons license of the content, the ab-
sence of limitations on the total number of pages
seen (to our knowledge) and the relatively small
amount of messages, which can also be a prob-
lem. A full coverage of the network is theoreti-
cally possible, where all the information may be
publicly available. Thus, all interesting informa-
tion is collected and no language filtering is used
concerning this website.
2.2 FriendFeed
To our knowledge, FriendFeed is the most active
of the three microblogging services considered
here. It is also the one which seems to have been
studied the most by the research community. The
service works as an aggregator (Gupta et al, 2009)
that offers a broader spectrum of retrieved infor-
mation. Technically, FriendFeed and identi.ca can
overlap, as the latter is integrated in the former.
1http://www.comscore.com/Press Events/Press Releases/
2011/12/Social Networking Leads as Top Online
Activity Globally
But the size difference between the two platforms
makes this hypothesis unlikely.
The API of FriendFeed is somewhat liberal, as
no explicit limits are enforced. Nonetheless, our
tests showed that after a certain number of suc-
cessful requests with little or no sleep, the servers
start dropping most of the inbound connections.
All in all, the relative tolerance of this website
makes it a good candidate to gather a lot of text
in a short period of time.
2.3 Reddit
Reddit is a social bookmarking and a microblog-
ging platform, which ranks to the 7th place world-
wide in the news category according to Alexa.2
The entries are organized into areas of interest
called ?reddits? or ?subreddits?. The users account
for the linguistic relevance of their channel, the
moderation processes are mature, and since the
channels (or subreddits) have to be hand-picked,
they ensure a certain stability.
There are 16 target languages so far, which
can be accessed via so-called ?multi-reddit ex-
pressions?, i.e. compilations of subreddits: Croa-
tian, Czech, Danish, Finnish, French, German,
Hindi, Italian, Norwegian, Polish, Portuguese, Ro-
manian, Russian, Spanish, Swedish and Turkish3.
Sadly, it is currently not possible to go back in
time further than the 500th oldest post due to API
limitations, which severely restricts the number of
links one may crawl.
3 Methodology
The following workflow describes how the results
below are obtained:
1. URL harvesting: social network traversal,
obvious spam and non-text documents filter-
ing, optional spell check of the short message
to see if it could be English text, optional
record of user IDs for later crawls.
2. Operations on the URL queue: redirection
checks, sampling by domain name.
3. Download of the web documents and analy-
sis: HTML code stripping, document validity
check, language identification.
2http://www.alexa.com/topsites/category/Top/News
3Here is a possible expression to target Norwegian users:
http://www.reddit.com/r/norge+oslo+norskenyheter
10
The only difference between FriendFeed and
Reddit on one hand and identi.ca on the other hand
is the spell check performed on the short messages
in order to target non-English ones. Indeed, all
new messages can be taken into consideration on
the latter, making a selection unnecessary.
Links pointing to media documents, which rep-
resent a high volume of links shared on microblog-
ging services, are excluded from this study, as its
final purpose is to be able to build a text corpus. As
a page is downloaded or a query is executed, links
are filtered on the fly using a series of heuristics
described below, and finally the rest of the links is
stored.
3.1 TRUC: an algorithm for TRaversal and
User-based Crawls
Starting from a publicly available homepage, the
crawl engine selects users according to their lin-
guistic relevance based on a language filter (see
below), and then retrieves their messages, eventu-
ally discovering friends of friends and expanding
its scope and the size of the network it traverses.
As this is a breadth-first approach its applicability
depends greatly on the size of the network.
In this study, the goal is to concentrate on non-
English speaking messages in the hope of find-
ing non-English links. The main ?timeline? fos-
ters a users discovery approach, which then be-
comes user-centered as the spider focuses on a list
of users who are expected not to post messages in
English and/or spam. The messages are filtered at
each step to ensure relevant URLs are collected.
This implies that a lot of subtrees are pruned, so
that the chances of completing the traversal in-
crease. In fact, experience shows that a relatively
small fraction of users and URLs is selected.
This approach is ?static?, as it does not rely on
any long poll requests (which are for instance used
to capture a fraction of Twitter?s messages as they
are made public), it actively fetches the required
pages.
3.2 Check for redirection and sampling
Further work on the URL queue before the lan-
guage identification task ensures an even smaller
fraction of URLs really goes through the resource-
expensive process of fetching and analyzing web
documents.
The first step of preprocessing consists in find-
ing those URLs that lead to a redirect, which is
done using a list comprising all the major URL
shortening services and adding all intriguingly
short URLs, i.e. less than 26 characters in length,
which according to our FriendFeed data occurs at
a frequency of about 3%. To deal with shortened
URLs, one can perform HTTP HEAD requests for
each member of the list in order to determine and
store the final URL.
The second step is a sampling that reduces both
the size of the list and the probable impact of an
overrepresented domain names in the result set. If
several URLs contain the same domain name, the
group is reduced to a randomly chosen URL.
Due to the overlaps of domain names and the
amount of spam and advertisement on social net-
works such an approach is very useful when it
comes to analyze a large list of URLs.
3.3 Language identification
Microtext has characteristics that make it hard for
?classical? NLP approaches like web page lan-
guage identification based on URLs (Baykan et
al., 2008) to predict with certainty the languages
of the links. That is why mature NLP tools have to
be used to filter the incoming messages.
A similar work on language identification and
FriendFeed is described in (Celli, 2009), who uses
a dictionary-based approach: the software tries
to guess the language of microtext by identifying
very frequent words.
However, the fast-paced evolution of the vocab-
ulary used on social networks makes it hard to
rely only on lists of frequent terms, so that our ap-
proach seems more complete.
A first dictionary-based filter First, a quick test
is used in order to guess whether a microtext is En-
glish or not. Indeed, this operation cuts the amount
of microtexts in half and enables to select the users
or the friends which feature the desired response,
thus directing the traversal in a more fruitful direc-
tion.
The library used, enchant4, allows the use of
a variety of spell-checking backends, like aspell,
hunspell or ispell, with one or several locales5.
Basically, this approach can be used with other
languages as well, even if they are not used as
discriminating factors in this study. We consider
this option to be a well-balanced solution between
processing speed on one hand and coverage on
4http://www.abisource.com/projects/enchant/
5All software mentioned here is open-source.
11
the other. Spell checking algorithms benefit from
years of optimization in both areas.
This first filter uses a threshold to discriminate
between short messages, expressed as a percent-
age of tokens which do not pass the spell check.
The filter also relies on software biases, like Uni-
code errors, which make it nearly certain that the
given input microtext is not English.
langid.py A language identification tool is
used to classify the web documents and to bench-
mark the efficiency of the test mentioned above.
langid.py (Lui and Baldwin, 2011; Lui and
Baldwin, 2012) is open-source, it incorporates
a pre-trained model and it covers 97 languages,
which is ideal to tackle the diversity of the web.
Its use as a web service makes it a fast solution
enabling distant or distributed work.
The server version of langid.py was used,
the texts were downloaded, all the HTML markup
was stripped and the resulting text was discarded
if it was less than 1,000 characters long. Accord-
ing to its authors, langid.py could be used di-
rectly on microtexts. However, this feature was
discarded because it did not prove as efficient as
the approach used here when it comes to a sub-
stantial amounts of short messages.
4 Results
The surface crawl dealing with the main time-
line and one level of depth has been performed
on the three platforms6. In the case of identi.ca,
a deep miner was launched to explore the net-
work. FriendFeed proved too large to start such a
breadth-first crawler so that other strategies ought
to be used (Gjoka et al, 2011), whereas the multi-
reddit expressions used did not yield enough users.
FriendFeed is the biggest link provider on a reg-
ular basis (about 10,000 or 15,000 messages per
hour can easily be collected), whereas Reddit is
the weakest, as the total figures show.
The total number of English websites may be
a relevant indication when it comes to establish
a baseline for finding possibly non-English docu-
ments. Accordingly, English accounts for about
55 % of the websites7, with the second most-
used content-language, German, only representing
6Several techniques are used to keep the number of re-
quests as low as possible, most notably user profiling accord-
ing to the tweeting frequency. In the case of identi.ca this
results into approximately 300 page views every hour.
7http://w3techs.com/technologies/overview/content
language/all
about 6 % of the web pages. So, there is a gap be-
tween English and the other languages, and there
is also a discrepancy between the number of Inter-
net users and the content languages.
4.1 FriendFeed
To test whether the first language filter was ef-
ficient, a testing sample of URLs and users was
collected randomly. The first filter was emu-
lated by selecting about 8% of messages (based
on a random function) in the spam and media-
filtered posts of the public timeline. Indeed, the
messages selected by the algorithm approximately
amount to this fraction of the total. At the same
time, the corresponding users were retrieved, ex-
actly as described above, and then the user-based
step was run, keeping one half of the user?s mes-
sages, which is also realistic according to real-
world data.
The datasets compared here were both of an
order of magnitude of at least 105 unique URLs
before the redirection checks. At the end of the
toolchain, the randomly selected benchmark set
comprises 7,047 URLs and the regular set 19,573
URLs8. The first was collected in about 30 hours
and the second one in several weeks. According
to the methodology used, this phenomenon may
be explained by the fact that the domain names in
the URLs tend to be mentioned repeatedly.
Language URLs %
English 4,978 70.6
German 491 7.0
Japanese 297 4.2
Spanish 258 3.7
French 247 3.5
Table 1: 5 most frequent languages of URLs taken
at random on FriendFeed
According to the language identification system
(langid.py), the first language filter beats the
random function by nearly 30 points (see Table
2). The other top languages are accordingly better
represented. Other noteworthy languages are to be
found in the top 20, e.g. Indonesian and Persian
(Farsi).
8The figures given describe the situation at the end, after
the sampling by domain name and after the selection of doc-
uments based on a minimum length. The word URL is used
as a shortcut for the web documents they link to.
12
Language URLs %
English 8,031 41.0
Russian 2,475 12.6
Japanese 1,757 9.0
Turkish 1,415 7.2
German 1,289 6.6
Spanish 954 4.9
French 703 3.6
Italian 658 3.4
Portuguese 357 1.8
Arabic 263 1.3
Table 2: 10 most frequent languages of spell-
check-filtered URLs gathered on FriendFeed
4.2 identi.ca
The results of the two strategies followed on
identi.ca led to a total of 1,113,783 URLs checked
for redirection, which were collected in about a
week (the deep crawler reached 37,485 user IDs).
A large majority of the 192,327 total URLs ap-
parently lead to English texts (64.9 %), since no
language filter was used but only a spam filter.
Language URLs %
English 124,740 64.9
German 15,484 8.1
Spanish 15,295 8.0
French 12,550 6.5
Portuguese 5,485 2.9
Italian 3,384 1.8
Japanese 1,758 0.9
Dutch 1,610 0.8
Indonesian 1,229 0.6
Polish 1,151 0.6
Table 3: 10 most frequent languages of URLs
gathered on identi.ca
4.3 Reddit
The figures presented here are the results of a sin-
gle crawl of all available languages altogether, but
regular crawls are needed to compensate for the
500 posts limit. English accounted for 18.1 % of
the links found on channel pages (for a total of
4,769 URLs) and 55.9 % of the sum of the links
found on channel and on user pages (for a total of
20,173 URLs).
The results in Table 5 show that the first filter
was nearly sufficient to discriminate between the
Language URLs % Comb. %
English 863 18.1 55.9
Spanish 798 16.7 9.7
German 519 10.9 6.3
French 512 10.7 7.2
Swedish 306 6.4 2.9
Romanian 265 5.6 2.5
Portuguese 225 4.7 2.1
Finnish 213 4.5 1.6
Czech 199 4.2 1.4
Norwegian 194 4.1 2.1
Table 4: 10 most frequent languages of filtered
URLs gathered on Reddit channels and on a com-
bination of channels and user pages
links. Indeed, the microtexts that were under the
threshold led to a total of 204,170 URLs. 28,605
URLs remained at the end of the toolchain and En-
glish accounted for 76.7 % of the documents they
linked to.
Language URLs % of total
English 21,926 76.7
Spanish 1,402 4.9
French 1,141 4.0
German 997 3.5
Swedish 445 1.6
Table 5: 5 most frequent languages of links seen
on Reddit and rejected by the primary language
filter
The threshold was set at 90 % of the words for
FriendFeed and 33% for Reddit, each time after
a special punctuation strip to avoid the influence
of special uses of punctuation on social networks.
Yet, the lower filter achieved better results, which
may be explained by the moderation system of the
subreddits as well as by the greater regularity in
the posts of this platform.
5 Discussion
Three main technical challenges had to be ad-
dressed, which resulted in a separate workflow:
the shortened URLs are numerous, yet they ought
to be resolved in order to enable the use of heuris-
tics based on the nature of the URLs or a proper
sampling of the URLs themselves. The con-
frontation with the constantly increasing number
of URLs to analyze and the necessarily limited re-
13
sources make a website sampling by domain name
useful. Finally, the diversity of the web documents
put the language recognition tools to a test, so that
a few tweaks are necessary to correct the results.
The relatively low number of results for Russian
may be explained by weaknesses of langid.py
with deviations of encoding standards. Indeed, a
few tweaks are necessary to correct the biases of
the software in its pre-trained version, in particular
regarding texts falsely considered as being written
in Chinese, although URL-based heuristics indi-
cate that the website is most probably hosted in
Russia or in Japan. A few charset encodings found
in Asian countries are also a source of classifica-
tion problems. The low-confidence responses as
well as a few well-delimited cases were discarded
in this study, they account for no more than 2 % of
the results. Ideally, a full-fledged comparison with
other language identification software may be nec-
essary to identify its areas of expertise.
A common practice known as cloaking has not
been addressed so far: a substantial fraction of
web pages show a different content to crawler en-
gines and to browsers. This Janus-faced behavior
tends to alter the language characteristics of the
web page in favor of English results.
Regarding topics, a major user bias was not ad-
dressed either: among the most frequently shared
links on identi.ca for example, many are related to
technology, IT or software and are mostly written
in English. The social media analyzed here tend
to be dominated by English-speaking users, either
native speakers or second-language learners.
In general, there is room for improvement con-
cerning the first filter, the threshold could be tested
and adapted to several scenarios. This may involve
larger datasets for testing purposes and machine
learning techniques relying on feature extraction.
The contrasted results on Reddit shed a different
light on the exploration of user pages: in all like-
lihood, users mainly share links in English when
they are not posting them on a language-relevant
channel. The results on FriendFeed are better from
this point of view, which may suggest that English
is not used equally on all platforms by users who
speak other languages than English. Nonetheless,
the fact that the microblogging services studied
here are mainly English-speaking seems to be a
strong tendency.
Last but not least, the adequateness of the web
documents shared on social networks has yet to
be thoroughly assessed. From the output of this
toolchain to a full-fledged web corpus, other fine-
grained instruments (Scha?fer and Bildhauer, 2012)
as well as further decisions processes (Scha?fer et
al., 2013) are needed along the way.
6 Conclusion
We presented a methodology to gather multilin-
gual URLs on three microblogging platforms. In
order to do so, we perform traversals of the plat-
forms and use already available tools to filter the
URLs accordingly and identify their language.
We provide open source software to access the
APIs (FriendFeed and Reddit) and the HTML ver-
sion of identi.ca, as an authentication is mandatory
for the API. The TRUC algorithm is fully imple-
mented. All the operations described in this paper
can be reproduced using the same tools, which are
part of repositories currently hosted on the GitHub
platform9.
The main goal is achieved, as hundreds if not
thousands of URLs for lesser-known languages
such as Romanian or Indonesian can be gathered
on social networks and microblogging services.
When it comes to filter out English posts, a first
step using an English spell checker gives better
results than the baseline established using micro-
texts selected at random. However, the discrep-
ancy between the languages one would expect to
find based on demographic indicators and the re-
sults of the study is remarkable. English websites
stay numerous even when one tries to filter them
out.
This proof of concept is usable, but a better fil-
tering process and longer crawls may be necessary
to unlock the full potential of this approach. Last,
a random-walk crawl using these seeds and a state
of the art text categorization may provide more in-
formation on what is really shared on microblog-
ging platforms.
Future work perspectives include dealing with
live tweets (as Twitter and FriendFeed can be
queried continuously), exploring the depths of
identi.ca and FriendFeed and making the directory
of language-classified URLs collected during this
study publicly available.
9https://github.com/adbar/microblog-explorer
14
7 Acknowledgments
This work has been partially funded by an inter-
nal grant of the FU Berlin (COW project at the
German Grammar Dept.). Many thanks to Roland
Scha?fer and two anonymous reviewers for their
useful comments.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
Eda Baykan, Monika Henzinger, and Ingmar Weber.
2008. Web Page Language Identification Based
on URLs. Proceedings of the VLDB Endowment,
1(1):176?187.
Fabio Celli. 2009. Improving Language identifica-
tion performance with FriendFeed data. Technical
report, CLIC, University of Trento.
Minas Gjoka, Maciej Kurant, Carter T. Butts, and
Athina Markopoulou. 2011. Practical recom-
mendations on crawling online social networks.
IEEE Journal on Selected Areas in Communica-
tions, 29(9):1872?1892.
Trinabh Gupta, Sanchit Garg, Niklas Carlsson, Anirban
Mahanti, and Martin Arlitt. 2009. Characterization
of FriendFeed ? A Web-based Social Aggregation
Service. In Proceedings of the AAAI ICWSM, vol-
ume 9.
Monika R. Henzinger, Allan Heydon, Michael Mitzen-
macher, and Marc Najork. 2000. On near-uniform
URL sampling. In Proceedings of the 9th Inter-
national World Wide Web conference on Computer
Networks: The International Journal of Computer
and Telecommunications Networking, pages 295?
308. North-Holland Publishing Co.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
Feature Selection for Language Identification. In
Proceedings of the Fifth International Joint Con-
ference on Natural Language Processing (IJCNLP
2011), pages 553?561, Chiang Mai, Thailand.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
Off-the-shelf Language Identification Tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012),
Jeju, Republic of Korea.
Roland Scha?fer and Felix Bildhauer. 2012. Building
large corpora from the web using a new efficient
tool chain. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC?12), pages 486?493.
Roland Scha?fer, Adrien Barbaresi, and Felix Bildhauer.
2013. The Good, the Bad, and the Hazy: Design
Decisions in Web Corpus Construction. In Proceed-
ings of the 8th Web as Corpus Workshop (WAC8).
To appear.
15
Felix Bildhauer & Roland Sch?fer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 1?8,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
Finding viable seed URLs for web corpora: a scouting approach and
comparative study of available sources
Adrien Barbaresi
ICAR Lab
ENS Lyon & University of Lyon
15 parvis Ren?e Descartes, 69007 Lyon, France
adrien.barbaresi@ens-lyon.fr
Abstract
The conventional tools of the ?web as cor-
pus? framework rely heavily on URLs ob-
tained from search engines. Recently, the
corresponding querying process became
much slower or impossible to perform on a
low budget. I try to find acceptable substi-
tutes, i.e. viable link sources for web cor-
pus construction. To this end, I perform
a study of possible alternatives, includ-
ing social networks as well as the Open
Directory Project and Wikipedia. Four
different languages (Dutch, French, In-
donesian and Swedish) taken as exam-
ples show that complementary approaches
are needed. My scouting approach using
open-source software leads to a URL di-
rectory enriched with metadata which may
be used to start a web crawl. This is
more than a drop-in replacement for exist-
ing tools since said metadata enables re-
searchers to filter and select URLs that fit
particular needs, as they are classified ac-
cording to their language, their length and
a few other indicators such as host- and
markup-based data.
1 Introduction
1.1 The ?web as corpus? paradigm and its
URL seeds problem
The state of the art tools of the ?web as corpus?
framework rely heavily on URLs obtained from
search engines. The BootCaT method (Baroni and
Bernardini, 2004) consists in repeated search en-
gine queries using several word seeds that are ran-
domly combined, first coming from an initial list
and later from unigram extraction over the cor-
pus itself. As a result, so-called ?seed URLs?
are gathered which are used as a starting point for
web crawlers. This approach is not limited to En-
glish: it has been successfully used by Baroni et al.
(2009) and Kilgarriff et al. (2010) for major world
languages.
Until recently, the BootCaT method could be
used in free web corpus building approaches. To
my best knowledge it is now pass?e because of in-
creasing limitations on the search engines? APIs,
which make the querying process on a low budget
much slower or impossible. Other technical diffi-
culties include diverse and partly unknown search
biases due in part to search engine optimization
tricks as well as undocumented PageRank adjust-
ments. All in all, the APIs may be too expensive
and/or too unstable to support large-scale corpus
building projects.
API changes are combined with an evolv-
ing web document structure and a slow but in-
escapable shift from ?web as corpus? to ?web
for corpus? due to the increasing number of web
pages and the necessity of using sampling meth-
ods at some stage. This is what I call the post-
BootCaT world in web corpus construction.
1
Moreover, the question whether the method
used so far, i.e. randomizing keywords, provides
a good overview of a language is still open. It now
seems reasonable to look for alternatives, so that
research material does not depend on a single data
source, as this kind of black box effect combined
with paid queries really impedes reproducibility
of research. Using diverse sources of URL seeds
could at least ensure that there is not a single bias,
but several.
Additionally, the lack of interest and project fi-
nancing when dealing with certain less-resourced
languages makes it necessary to use light-weight
1
Note that the proponents of the BootCaT method seem to
acknowledge this evolution, see for example Marco Baroni?s
talk at this year?s BootCaTters of the world unite (BOTWU)
workshop: ?My love affair with the Web... and why it?s
over!?
1
approaches where costs are lowered as much as
possible (Scannell, 2007). In this perspective, a
preliminary light scouting approach and a full-
fledged focused crawler like those used by the
Spiderling (Suchomel and Pomik?alek, 2012) or
the COW (Sch?afer and Bildhauer, 2012) projects
are complementary. A ?web for corpus? crawling
method using a seed set enriched with metadata as
described in this article may yield better results,
e.g. ensure a more diverse and less skewed sam-
ple distribution in a population of web documents,
and/or reach faster a given quantitative goal.
1.2 Looking for alternatives, what issues do
we face?
Search engines have not been taken as a source
simply because they were convenient. They actu-
ally yield good results in terms of linguistic qual-
ity. The main advantage was to outsource oper-
ations such as web crawling and website quality
filtering, which are considered to be too costly or
too complicated to deal with while the main pur-
pose is actually to build a corpus.
In fact, it is not possible to start a web crawl
from scratch, so the main issue to tackle can be
put this way: where may we find web pages which
are bound to be interesting for corpus linguists and
which in turn contain many links to other interest-
ing web pages?
Researchers in the machine translation field
have started another attempt to outsource compe-
tence and computing power, making use of data
gathered by the CommonCrawl project
2
to find
parallel corpora (Smith et al., 2013). Nonetheless,
the quality of the links may not live up to their
expectations. First, purely URL-based approaches
are a trade-off in favor of speed which sacrifices
precision, and language identification tasks are
a good example of this phenomenon (Baykan et
al., 2008). Second, machine-translated content is
a major issue, so is text quality in general, es-
pecially when it comes to web texts (Arase and
Zhou, 2013). Third, mixed-language documents
slow down text gathering processes (King and Ab-
ney, 2013). Fourth, link diversity is a also prob-
lem, which in my opinion has not got the atten-
tion it deserves. Last, the resource is constantly
moving. There are not only fast URL changes
and ubiquitous redirections. Following the ?web
2.0? paradigm, much web content is being injected
2
http://commoncrawl.org/
from other sources, so that many web pages are
now expected to change any time.
3
Regular ex-
ploration and re-analysis could be the way to go to
ensure the durability of the resource.
In the remainder of this paper, I introduce a
scouting approach which considers the first issue,
touches on the second one, provides tools and met-
rics to address the third and fourth, and adapts to
the last. In the following section I describe my
methodology, then I show in detail which metrics
I decided to use, and last I discuss the results.
2 Method
2.1 Languages studied
I chose four different languages in order to see if
my approach generalizes well: Dutch, French, In-
donesian and Swedish. It enables me to compare
several language-dependent web spaces which
ought to have different if not incompatible char-
acteristics. In fact, the ?speaker to website quan-
tity? ratio is probably extremely different when it
comes to Swedish and Indonesian. I showed in a
previous study that this affects greatly link discov-
ery and corpus construction processes (Barbaresi,
2013a).
French is spoken on several continents and
Dutch is spoken in several countries (Afrikaans
was not part of this study). Indonesian offers an
interesting point of comparison, as the chances to
find web pages in this language during a crawl at
random are scarce. For this very reason, I explic-
itly chose not to study English or Chinese because
they are clearly the most prominently represented
languages on the web.
2.2 Data sources
I use two reference points, the first one being
the existing method depending on search engine
queries, upon which I hope to cast a new light
with this study. The comparison grounds on URLs
retrieved using the BootCaT seed method on the
meta-engine E-Tools
4
at the end of 2012. The sec-
ond reference point consists of social networks,
to whose linguistic structure I already dedicated
a study (Barbaresi, 2013b) where the method used
to find the URLs is described in detail. I chose
to adopt a different perspective, to re-examine the
URLs I gathered and to add relevant metadata
3
This is the reason why Marco Baroni states in the talk
mentioned above that his ?love affair with the web? is over.
4
http://www.etools.ch/
2
in order to see how they compared to the other
sources studied here.
I chose to focus on three different networks:
FriendFeed, an aggregator that offers a broader
spectrum of retrieved information; identi.ca, a mi-
croblogging service similar to Twitter; and Red-
dit, a social bookmarking and microblogging plat-
form. Perhaps not surprisingly, these data sources
display the issues linked to API instability men-
tioned above. The example of identi.ca is telling:
until March 2013, when the API was closed af-
ter the company was bought, it was a social mi-
croblogging service built on open source tools and
open standards, the advantages compared to Twit-
ter include the Creative Commons license of the
content, and the absence of limitations on the total
number of pages seen.
Another data source is the Open Directory
Project (DMOZ
5
), where a selection of links is cu-
rated according to their language and/or topic. The
language classification is expected to be adequate,
but the amount of viable links is an open question,
as well as the content.
Last, the free encyclopedia Wikipedia is another
spam-resilient data source in which the quality of
links is expected to be high. It is acknowledged
that the encyclopedia in a given language edition
is a useful resource, the open question resides in
the links pointing to the outside world, as it is hard
to get an idea of their characteristics due to the
large number of articles, which is rapidly increas-
ing even for an under-resourced language such as
Indonesian.
2.3 Processing pipeline
The following sketch describes how the results be-
low were obtained:
1. URL harvesting: queries or archive/dump
traversal, filtering of obvious spam and non-
text documents.
2. Operations on the URL queue: redirection
checks, sampling by domain name.
3. Download of the web documents and ana-
lysis: collection of host- and markup-based
data, HTML code stripping, document valid-
ity check, language identification.
Links pointing to media documents were ex-
cluded from this study, as its final purpose is
5
http://www.dmoz.org/
to enable construction of a text corpus. The
URL checker removes non-http protocols, images,
PDFs, audio and video files, ad banners, feeds and
unwanted hostnames like twitter.com, google.com,
youtube.com or flickr.com. Additionally, a proper
spam filtering is performed on the whole URL (us-
ing basic regular expressions) as well as at do-
main name level using a list of blacklisted domains
comparable to those used by e-mail services to fil-
ter spam. As a page is downloaded or a query is
executed, links are filtered on-the-fly using a se-
ries of heuristics described below, and finally the
rest of the links are stored.
There are two other major filtering operations to
be aware of. The first concerns the URLs, which
are sampled prior to the download. The main goal
of this operation is strongly related to my scout-
ing approach. Since I set my tools on an explo-
ration course, this allows for a faster execution
and provides us with a more realistic image of
what awaits a potential exhaustive crawler. Be-
cause of the sampling approach, the ?big picture?
cannot easily be distorted by a single website. This
also avoids ?hammering? a particular server un-
duly and facilitates compliance with robots.txt as
well as other ethical rules. The second filter deals
with the downloaded content: web pages are dis-
carded if they are too short. Web documents which
are more than a few megabytes long are also dis-
carded.
Regarding the web pages, the software fetches
them from a list, strips the HTML code, sends raw
text to a server instance of langid.py (description
below) and retrieves the server response, on which
it performs a basic heuristic tests.
3 Metadata
The metadata described in this section can be used
in classificatory or graph-based approaches. I use
some of them in the results below but did not ex-
haust all the possible combinations in this study.
There are nine of them in total, which can be
divided in three categories: corpus size metrics,
which are related to word count measures, web
science metrics, which ought to be given a higher
importance in web corpus building, and finally the
language identification, which is performed using
an external tool.
3
3.1 Corpus size metrics
Web page length (in characters) was used as a dis-
criminating factor. Web pages which were too
short (less than 1,000 characters long after HTML
stripping) were discarded in order to avoid docu-
ments containing just multimedia (pictures and/or
videos) or microtext collections for example, as
the purpose was to simulate the creation of a
general-purpose text corpus.
The page length in characters after stripping
was recorded, as well as the number of tokens,
so that the total number of tokens of a web cor-
pus built on this URL basis can be estimated. The
page length distribution is not normal, with a ma-
jority of short web texts and a few incredibly long
documents at the end of the spectrum, which is
emphasized by the differences between mean and
median values used in the results below and justi-
fies the mention of both.
3.2 Web science metrics
Host sampling is a very important step because
the number of web pages is drastically reduced,
which makes the whole process more feasible and
more well-balanced, i.e. less prone to host biases.
IP-based statistics corroborate this hypothesis, as
shown below.
The deduplication operation is elementary, it
takes place at document level, using a hash func-
tion. The IP diversity is partly a relevant indicator,
as it can be used to prove that not all domain names
lead to the same server. Nonetheless, it cannot de-
tect the duplication of the same document across
many different servers with different IPs, which in
turn the elementary deduplication is able to reveal.
Links that lead to pages within the same domain
name and links which lead to other domains are
extracted from the HTML markup. The first num-
ber can be used to find possible spam or irrelevant
links, with the notable exception of websites like
Amazon or Wikipedia, which are quite easy to list.
The latter may be used to assess the richness (or at
a given level the suspiciousness) of a website by
the company it keeps. While this indicator is not
perfect, it enables users to draw conclusions with-
out fetching all the downstream URLs.
Moreover, even if I do not take advantage of this
information in this study, the fetcher also records
all the links it ?sees? (as an origin-destination
pair), which enables graph-based approaches such
as visualization of the gathered network or the as-
sessment of the ?weight? of a website in the URL
directory. Also, these metadata may very well be
useful for finding promising start URLs.
3.3 Language identification
I consider the fact that a lot of web pages have
characteristics which make it hard for ?classical?
NLP approaches like web page language identifi-
cation based on URLs (Baykan et al., 2008) to pre-
dict the languages of the links with certainty. That
is why mature NLP tools have to be used to qualify
the incoming URLs and enable a language-based
filtering based on actual facts.
The language identification tool I used is
langid.py (Lui and Baldwin, 2012). It is open-
source, it incorporates a pre-trained model and it
covers 97 languages, which is ideal for tackling
the diversity of the web. Its use as a web ser-
vice makes it a fast solution enabling distant or
distributed work.
As the software is still under active develop-
ment, it can encounter difficulties with rare encod-
ings. As a result, the text gets falsely classified as
for example Russian or Chinese. The languages I
studied are not affected by these issues. Still, lan-
guage identification at document level raises a few
problems regarding ?parasite? languages (Scan-
nell, 2007).
Using a language identification system has a
few benefits: it enables finding ?regular? texts in
terms of statistical properties and excluding cer-
tain types of irregularities such as encoding prob-
lems. Web text collections are smoothed out in
relation to the statistical model applied for each
language target, which is a partly destructive but
interesting feature.
There are cases where the confidence interval
of the language identifier is highly relevant, for in-
stance if the page is multi-lingual. Then there are
two main effects: on one hand the confidence in-
dicator gets a lower value, so that it is possible to
isolate pages which are likely to be in the target
language only. On the other hand, the language
guessed is the one with the largest number of iden-
tifiable words: if a given web page contains 70 %
Danish and 30 % English, then it will be classified
as being written in Danish, with a low confidence
interval: this information is part of the metadata I
associate with each web page. Since nothing par-
ticular stood out in this respect I do not mention it
further.
4
URLs
% in
target
Length
Tokens
(total)
Different
IPs (%)
analyzed retained mean median
Dutch 12,839 1,577 84.6 27,153 3,600 5,325,275 73.1
French 16,763 4,215 70.2 47,634 8,518 19,865,833 50.5
Indonesian 110,333 11,386 66.9 49,731 8,634 50,339,311 18.6
Swedish 179,658 24,456 88.9 24,221 9,994 75,328,265 20.0
Table 1: URLs extracted from search engines queries
4 Results
4.1 Characteristics of the BootCaT approach
First of all, I let my toolchain run on URLs ob-
tained using the BootCaT approach, in order to
get a glimpse of its characteristics. I let the
URL extractor run for several weeks on Indone-
sian and Swedish and only a few days for Dutch
and French, since I was limited by the constraints
of this approach, which becomes exponentially
slower as one adds target languages.
6
The results
commented below are displayed in table 1.
The domain name reduction has a substantial
impact on the set of URLs, as about a quarter of
the URLs at best (for French) have different do-
main names. This is a first hint at the lack of
diversity of the URLs found using the BootCaT
technique.
Unsurprisingly, the majority of links appear to
be in the target language, although the language
filters do not seem to perform very well. As the
adequate matching of documents to the user?s lan-
guage is paramount for search engines, it is prob-
ably a bias of the querying methodology and its
random tuples of tokens. In fact, it is not rare to
find unexpected and undesirable documents such
as word lists or search engine optimization traps.
The length of web documents is remarkable, it
indicates that there are likely to contain long texts.
Moreover, the median length seems to be quite
constant across the three languages at about 8,000
tokens, whereas it is less than half that (3,600) for
Dutch. All in all, it appears to be an advantage
which clearly explains why this method has been
considered to be successful. The potential cor-
pus sizes are noteworthy, especially when enough
URLs where gathered in the first place, which was
6
The slow URL collection is explained by the cautious
handling of this free and reliable source, implying a query
rate limiting on my side. The scouting approach by itself is a
matter of hours.
already too impracticable in my case to be consid-
ered a sustainable option.
The number of different IPs, i.e. the diversity
in terms of hosts, seems to get gradually lower
as the URL list becomes larger. The fact that
the same phenomenon happens for Indonesian and
Swedish, with one host out of five being ?new?,
indicates a strong tendency.
4.2 Social networks
Due to the mixed nature of the experimental set-
ting, no conclusions can be drawn concerning the
single components. The more than 700,000 URLs
that were analyzed give an insight regarding the
usefulness of these sources. About a tenth of it re-
mained as responding websites with different do-
main names, which is the lowest ratio of this study.
It may be explained by the fast-paced evolution of
microblogs and also by the potential impurity of
the source compared to the user-reviewed directo-
ries whose results I describe next.
As I did not target the studied languages during
the URL collection process, there were merely a
few hundred different domain names to be found,
with the exception of French, which was a lot more
prominent.
Table 2 provides an overview of the results. The
mean and median lengths are clearly lower than
in the search engine experiment. In the case of
French, with a comparable number of remaining
URLs, the corpus size estimate is about 2.5 times
smaller. The host diversity is comparable, and
does not seem to be an issue at this point.
All in all, social networks are probably a good
candidate for web corpora, but they require a fo-
cused approach of microtext to target a particular
community of speakers.
4.3 DMOZ
As expected, the number of different domain
names on the Open Directory project is high, giv-
5
% in target
URLs
retained
Length
Tokens
(total)
Different
IPs (%)
mean median
Dutch 0.6 465 7,560 4,162 470,841 68.8
French 5.9 4,320 11,170 5,126 7,512,962 49.7
Indonesian 0.5 336 6,682 4,818 292,967 50.9
Swedish 1.1 817 13,807 7,059 1,881,970 58.5
Table 2: URLs extracted from a blend of social networks crawls (FriendFeed, identi.ca, and Reddit) with
no language target. 738,476 URLs analyzed, 73,271 URLs retained in the global process.
ing the best ratio in this study between unfiltered
and remaining URLs. The lack of web pages writ-
ten in Indonesian is a problem for this source,
whereas the other languages seem to be far bet-
ter covered. The adequacy of the web pages with
respect to their language is excellent, as shown in
table 3. These results underline the quality of the
resource.
On the other hand, document length is the
biggest issue here. The mean and median val-
ues indicate that this characteristic is quite ho-
mogeneous throughout the document collection.
This may easily be explained by the fact that the
URLs which are listed on DMOZ mostly lead
to corporate homepages for example, which are
clear and concise, the eventual ?real? text content
being somewhere else. What?s more, the web-
sites in question are not text reservoirs by nature.
Nonetheless, the sheer quantity of listed URLs
compensates for this fact. The corpus sizes for
Dutch and French are quite reasonable if one bears
in mind that the URLs were sampled.
The relative diversity of IPs compared to the
number of domain names visited is another indica-
tor that the Open Directory leads to a wide range of
websites. The directory performs well compared
to the sources mentioned above, it is also much
easier to crawl. It did not cost us more than a few
lines of code followed by a few minutes of runtime
to gather the URLs.
4.4 Wikipedia
The characteristics of Wikipedia are quite simi-
lar, since the free encyclopedia also makes dumps
available, which are easily combed through in or-
der to gather start URLs. Wikipedia also com-
pares favorably to search engines or social net-
works when it comes to the sampling operation
and page availability. It is a major source of URLs,
with numbers of gathered URLs in the millions for
languages like French. As Wikipedia is not a URL
directory by nature, it is interesting to see what are
the characteristics of the pages it links to are. The
results are shown in table 3.
First, the pages referenced in a particular lan-
guage edition of Wikipedia often point to web
pages written in a foreign language. According to
my figures, this is a clear case, all the more since
web pages in Indonesian are rare. Still, with a to-
tal of more than 4,000 retained web texts, it fares
a lot better than DMOZ or social networks.
The web pages are longer than the ones from
DMOZ, but shorter than the rest. This may also be
related to the large number of concise homepages
in the total. Nonetheless, the impressive num-
ber of URLs in the target language is decisive for
corpus building purposes, with the second-biggest
corpus size estimate obtained for French.
The IP-related indicator yields good results with
respect to the number of URLs that were retrieved.
Because to the high number of analyzed URLs the
figures between 30 and 46% give an insight into
the concentration of web hosting providers on the
market.
5 Discussion
I also analyzed the results regarding the num-
ber of links that lead out of the page?s domain
name. For all sources, I found no consistent re-
sults across languages, with figures varying by a
factor of three. Nonetheless, there seem to be a
tendency towards a hierarchy in which the search
engines are on top, followed by social networks,
Wikipedia and DMOZ. This is one more hint at
the heterogeneous nature of the data sources I ex-
amined with respect to the criteria I chose.
This hierarchy is also one more reason why
6
URLs
% in
target
Length
Tokens
(total)
Different
IPs (%)
analyzed retained mean median
DMOZ
Dutch 86,333 39,627 94.0 2,845 1,846 13,895,320 43.2
French 225,569 80,150 90.7 3,635 1,915 35,243,024 33.4
Indonesian 2,336 1,088 71.0 5,573 3,922 540,371 81.5
Swedish 27,293 11,316 91.1 3,008 1,838 3,877,588 44.8
Wikipedia
Dutch 489,506 91,007 31.3 4,055 2,305 15,398,721 43.1
French 1,472,202 201,471 39.4 5,939 2,710 64,329,516 29.5
Indonesian 204,784 45,934 9.5 6,055 4,070 3,335,740 46.3
Swedish 320,887 62,773 29.7 4,058 2,257 8,388,239 32.7
Table 3: URLs extracted from DMOZ and Wikipedia
search engines queries are believed to be fast and
reliable in terms of quantity. This method was
fast, as the web pages are long and full of links,
which enables to rapidly harvest a large number
of web pages without having to worry about going
round in circles. The researchers using the Boot-
CaT method probably took advantage of the undo-
cumented but efficient filtering operations which
search engines perform in order to lead to reli-
able documents. Since this process takes place in
a competitive sector where this kind of informa-
tion can be sold, it may explain why the companies
now try to avoid giving it away for free.
In the long run, several questions regarding
URL quality remain open. As I show using a high-
credibility source such as Wikipedia, the search
engines results are probably closer to the maxi-
mum amount of text that is to be found on a given
website than the other sources, all the more when
the sampling procedure chooses a page at random
without analyzing the rest of a website and thus
without maximizing its potential in terms of to-
kens. Nonetheless, confrontation with the con-
stantly increasing number of URLs to analyze and
necessarily limited resources make a website sam-
pling by domain name useful.
This is part of my cost-efficient approach, where
the relatively low performance of Wikipedia and
DMOZ is compensated by the ease of URL ex-
traction. Besides, the size of the potential corpora
mentioned here could increase dramatically if one
was to remove the domain name sampling process
and if one was to select the web pages with the
most out-domain links for the crawl.
What?s more, DMOZ and Wikipedia are likely
to improve over time concerning the number of
URLs they reference. As diversity and costs (tem-
poral or financial) are real issues, a combined ap-
proach could take the best of all worlds and pro-
vide a web crawler with distinct and distant start-
ing points, between the terse web pages referenced
in DMOZ and the expected ?freshness? of social
networks. This could be a track to consider, as
they could provide a not inconsiderable amount of
promising URLs.
Finally, from the output of the toolchain to
a full-fledged web corpus, other fine-grained in-
struments as well as further decisions processes
(Sch?afer et al., 2013) will be needed. The fact that
web documents coming from several sources al-
ready differ by our criteria does not exclude fur-
ther differences regarding text content. By way
of consequence, future work could include a few
more linguistically relevant text quality indicators
in order to go further in bridging the gap between
web data, NLP and corpus linguistics.
6 Conclusion
I evaluated several strategies for finding texts on
the web. The results distinguish no clear win-
ner, complementary approaches are called for. In
light of these results, it seems possible to replace
or at least to complement the existing BootCaT
approach. It is understandable why search en-
gine queries have been considered a useful data
source. However, I revealed that they lack diver-
7
sity at some point, which apart from their imprac-
ticality may provide sufficient impetus to look for
alternatives.
I discussed how I address several issues in or-
der to design robust processing tools which (com-
bined to the diversity of sources and usable meta-
data) enable researchers to get a better glimpse of
the course a crawl may take. The problem of link
diversity has not been well-studied in a corpus lin-
guistics context; I presented metrics to help quan-
tify it and I showed a possible way to go in order
to gather a corpus using several sources leading to
a satisfying proportion of different domain names
and hosts.
As a plea for a technicalities-aware corpus cre-
ation, I wish to bring to linguists? attention that the
first step of web corpus construction in itself can
change a lot of parameters. I argue that a minimum
of web science knowledge among the corpus lin-
guistics community could be very useful to fully
comprehend all the issues at stake when dealing
with corpora from the web.
The toolchain used to perform these experi-
ments is open-source and can be found online.
7
The resulting URL directory, which includes the
metadata used in this article, is available upon re-
quest. The light scouting approach allows for reg-
ular updates of the URL directory. It could also
take advantage of the strengths of other tools in
order to suit the needs of different communities.
Acknowledgments
This work has been partially supported by an in-
ternal grant of the FU Berlin as well as machine
power provided by the COW (COrpora from the
Web) project at the German Grammar Depart-
ment. Thanks to Roland Sch?afer for letting me use
the URLs extracted from E-Tools and DMOZ.
References
Yuki Arase and Ming Zhou. 2013. Machine Trans-
lation Detection from Monolingual Web-Text. In
Proceedings of the 51th Annual Meeting of the ACL,
pages 1597?1607.
Adrien Barbaresi. 2013a. Challenges in web cor-
pus construction for low-resource languages in a
post-BootCaT world. In Zygmunt Vetulani and
Hans Uszkoreit, editors, Proceedings of the 6th Lan-
guage & Technology Conference, Less Resourced
Languages special track, pages 69?73, Pozna?n.
7
FLUX: Filtering and Language-identification for URL
Crawling Seeds ? https://github.com/adbar/flux-toolchain
Adrien Barbaresi. 2013b. Crawling microblogging
services to gather language-classified URLs. Work-
flow and case study. In Proceedings of the 51th An-
nual Meeting of the ACL, Student Research Work-
shop, pages 9?15.
Marco Baroni and Silvia Bernardini. 2004. BootCaT:
Bootstrapping corpora and terms from the web. In
Proceedings of LREC, pages 1313?1316.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
E. Baykan, M. Henzinger, and I. Weber. 2008. Web
Page Language Identification Based on URLs. Pro-
ceedings of the VLDB Endowment, 1(1):176?187.
Adam Kilgarriff, Siva Reddy, Jan Pomik?alek, and PVS
Avinesh. 2010. A Corpus Factory for Many Lan-
guages. In Proceedings of LREC, pages 904?910.
Ben King and Steven Abney. 2013. Labeling the Lan-
guages of Words in Mixed-Language Documents us-
ing Weakly Supervised Methods. In Proceedings of
NAACL-HLT, pages 1110?1119.
Marco Lui and Timothy Baldwin. 2012. langid.py:
An Off-the-shelf Language Identification Tool. In
Proceedings of the 50th Annual Meeting of the ACL,
pages 25?30.
Kevin P. Scannell. 2007. The Cr?ubad?an Project:
Corpus building for under-resourced languages. In
Building and Exploring Web Corpora: Proceedings
of the 3rd Web as Corpus Workshop, volume 4,
pages 5?15.
Roland Sch?afer and Felix Bildhauer. 2012. Building
large corpora from the web using a new efficient tool
chain. In Proceedings of LREC, pages 486?493.
Roland Sch?afer, Adrien Barbaresi, and Felix Bildhauer.
2013. The Good, the Bad, and the Hazy: Design
Decisions in Web Corpus Construction. In Stefan
Evert, Egon Stemle, and Paul Rayson, editors, Pro-
ceedings of the 8th Web as Corpus Workshop, pages
7?15.
Jason R. Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt Cheap Web-Scale Paral-
lel Text from the Common Crawl. In Proceedings
of the 51th Annual Meeting of the ACL, pages 1374?
1383.
V??t Suchomel and Jan Pomik?alek. 2012. Efficient We-
bcrawling for large text corpora. In Adam Kilgarriff
and Serge Sharoff, editors, Proceedings of the 7th
Web as Corpus Workshop, pages 40?44.
8
Felix Bildhauer & Roland Sch?fer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 9?15,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
Focused Web Corpus Crawling
Roland Sch?fer
Freie Universit?t Berlin
roland.schaefer
@fu-berlin.de
Adrien Barbaresi
ENS Lyon
adrien.barbaresi
@ens.lyon.org
Felix Bildhauer
Freie Universit?t Berlin
felix.bildhauer
@fu-berlin.de
Abstract
In web corpus construction, crawling is a
necessary step, and it is probably the most
costly of all, because it requires expen-
sive bandwidth usage, and excess crawl-
ing increases storage requirements. Ex-
cess crawling results from the fact that the
web contains a lot of redundant content
(duplicates and near-duplicates), as well
as other material not suitable or desirable
for inclusion in web corpora or web in-
dexes (for example, pages with little text
or virtually no text at all). An optimized
crawler for web corpus construction would
ideally avoid crawling such content in the
first place, saving bandwidth, storage, and
post-processing costs. In this paper, we
show in three experiments that two simple
scores are suitable to improve the ratio be-
tween corpus size and crawling effort for
web corpus construction. The first score
is related to overall text quality of the page
containing the link, the other one is related
to the likelihood that the local block en-
closing a link is boilerplate.
1 Crawl Optimization and Yield Ratios
Optimizing a crawling strategy consists in maxi-
mizing its weighted coverage WC(t) at any time
t during a crawl (Olston and Najork, 2010, 29),
i. e., the summed weight of the documents down-
loaded until t, where the weight of each crawled
document is calculated as a measure of the useful-
ness of the document relative to the purpose of the
crawl. To maximize WC, it is vital to guess the
weight of the documents behind harvested links
before download, such that documents with poten-
tially lesser weight have a lower probability of be-
ing downloaded. So-called focused crawlers (in a
broad sense) are designed to maximize WC with
respect to some specific definition of document
weight, for example when documents with a high
search-engine relevance (measured as its Page-
Rank or a similar score), documents about specific
subjects, or documents in a specific language are
desired (Chakrabarti et al., 1999; Menczer et al.,
2004; Baykan et al., 2008; Safran et al., 2012).
For our purpose, i. e., web corpus crawling, a doc-
ument with a high weight can simply be defined as
one which is not removed from the corpus by the
post-processing tools due to low linguistic qual-
ity and/or a document which contributes a high
amount of text to the corpus. Recently, an inter-
esting approach to crawl optimization along such
lines was suggested which relies on statistics about
the corpus yield from known hosts (Suchomel
and Pomik?lek, 2012). Under this approach, the
weight (rather of a whole web host) is taken to be
the ratio of good documents from the host remain-
ing in the corpus after a specific post-processing
chain has been applied to the documents. Har-
vested URLs pointing to certain hosts are priori-
tized accordingly. We follow a similar route like
Suchomel and Pomik?lek, but look at document-
local features instead of host statistics.
Throughout this paper, we refer to the yield ra-
tio instead of WC, although they are related no-
tions. We define the yield ratio Y
d
for a set D
c
of
crawled unprocessed documents and a set D
r
of
retained documents after filtering and processing
for inclusion in a corpus, with D
r
? D
c
, as:
Y
d
=
|D
r
|
|D
c
|
(1)
For example, a document yield ratio Y
d
= 0.21
9
means that 21% of the crawled documents sur-
vived the cleaning procedure (i. e., were not clas-
sified as duplicates or spam, were long enough,
written in the target language, etc.) and ended up
in the corpus. In order to maximize Y
d
, 79% of
the documents should not have been downloaded
in the first place in this example. A parallel defini-
tion is assumed for Y
b
for the respective amounts
of bytes. The document yield ratio is easier to in-
terpret because the byte yield ratio depends on the
amount of markup which has to be stripped, and
which might vary independently of the quality of
the downloaded web pages.
Obviously, the yield ratio ? like the weighted
coverage ? depends highly on the definition of
what a good document is, i. e., what the goal of
the crawl is. We assume, similar to Suchomel and
Pomik?lek?s approach, that our tools reliably filter
out documents that are interesting documents for
inclusion a corpus, and that calculating a yield ra-
tio based on the output of those tools is therefore
reasonable.
1
2 Experiment 1: Seed and Crawl Quality
In this experiment, we examine the correlation be-
tween the yield ratio of crawler seed URLs and
the yield ratio of short Breadth-First Search (BFS)
crawls based on those URLs. We used the Her-
itrix (1.14) web crawler (Mohr et al., 2004) and
an older version of the texrex web page clean-
ing toolkit (Sch?fer and Bildhauer, 2012). The
tools perform, among other things, boilerplate de-
tection and text quality evaluation in the form of
the so-called Badness score (Sch?fer et al., 2013).
A document receives a low Badness score if the
most frequent function words of the target lan-
guage have a high enough frequency in the doc-
ument. The Badness score is based on previous
ideas from language identification and web doc-
ument filtering (Grefenstette, 1995; Baroni et al.,
2009).
Originally, this experiment was carried out in
the context of an evaluation of sources of differ-
ent seed URLs for crawls. In a preliminary step,
we began by collecting seed URLs from various
sources:
1
This claim should be backed up by forms of ex-
trinsic/task-based evaluation (Sch?fer and Bildhauer, 2013,
p. 104 ff). Such an evaluation (in the form of a collocation ex-
traction task) was recently presented for our corpora in work
by Stefan Evert (Biemann et al., 2013).
1. the DMOZ directory
2. the Etools meta search engine
3. the FriendFeed social service aggregator
4. the identi.ca social bookmarking service
5. Wikipedia dumps
We scraped the content behind the URLs and
ran a state-of-the-art language identifier (Lui and
Baldwin, 2012) on it in order to obtain language-
classified seed URLs (Barbaresi, 2013).
2
We then
looked specifically at the following languages as-
sociated as the single dominant language with at
least one top-level domain (TLD):
1. Dutch (.nl)
2. French (.fr)
3. Indonesian (.id)
4. Swedish (.se)
We randomly sampled 1, 000 seed URLs for
each of the 20 permutations of seed sources
and languages/TLDs, downloaded them and used
texrex to determine the document yield ratio
for the documents behind the 1, 000 seeds. The
software was configured to perform boilerplate re-
moval, removal of documents based on high Bad-
ness scores, perfect duplicate removal, and dele-
tion of documents shorter than 1, 000 characters
(after boilerplate removal). Then, we crawled
the respective TLDs, starting the crawls with the
1, 000 seed URLs, respectively. In each crawl, we
downloaded 2 GB of raw data, cleaned them, and
calculated the document yield ratio using the same
configuration of texrex as we used for cleaning
the seed documents. Figure 1 plots the data and an
appropriate linear model.
We see that there is a strong correlation (ad-
justed R
2
= 0.7831) between the yield ratio of
the documents behind the seed URLs and the yield
ratio of the documents found by using the seeds
for BFS crawling. It follows that giving high pri-
ority to links from pages which are themselves
considered high-quality documents by the post-
processing tools will likely lead to more efficient
crawling. Since there is no fundamental distinc-
tion between initial URL seeds and URLs har-
vested at a later time during the crawl, this effect
is likely to extend to the whole run time of a crawl.
2
See also Barbaresi, this volume.
10
0.10
0.15
0.20
0.25
0.30
0.35
0.050.100.150.20
Figure 1: Yield ratio Y
d
of the crawls (y axis) plot-
ted against the yield ratio of the documents be-
hind the crawls? 1,000 seeds (x axis). (Higher Y
d
is better.) Linear model: Intercept = ?0.0098,
Coefficient = 0.6332, R
2
= 0.7831 (adjusted),
p < 0.001 (ANOVA).
3 Experiment 2: Crawling
with Cyclic URL Selection
Using the same configuration of tools as in Sec-
tion 2, we performed a crawl targeting Flem-
ish documents in the Belgian .be national TLD,
which hosts both Flemish and French documents
in substantial proportions. Usually, even under
more favorable conditions (i. e., when we crawl a
TLD which contains mostly documents in the tar-
get language), the yield ratio of a BFS crawl de-
creases rapidly in the initial phase, then staying at
a low level (Sch?fer and Bildhauer, 2013, p. 31).
Figure 2 illustrates this with an analysis of a .de
BFS crawl from late 2011, also processed with the
same tools as mentioned in Section 2. Notice that
the .de domain hosts German documents almost
exclusively.
The interesting complication in this experiment
is thus the non-target language present in the
TLD scope of the crawler and the related question
whether, simply speaking, predominantly Flemish
documents link to other predominantly Flemish
documents rather than French documents. Since
the Badness score (calculated as described in Sec-
tion 2) includes a form of language identification,
the yield ratio takes into account this additional
complication.
We tested whether the decline of the yield ra-
tio could be compensated for by selecting ?high
quality? URLs in the following manner: The crawl
progressed in five phases. In the first short burn-
in phase, we crawled 1, 000, 000 documents, and
in each of the second to fifth phase, we crawled
10, 000, 000 documents. After each phase, the
0
200
400
600
800
1000
0.000.050.100.150.200.250.30
Figure 2: Yield ratio (y axis) over time for a
BFS crawl in .de in November/December 2011
started with 231, 484 seed URLs scraped from
Bing. The yield ratio was calculated at 1, 000
snapshots of 400 MB of data (= one Heritrix ARC
file). For snapshots s
1
..s
500
: Y
d
= 0.141, for
snapshots s
501
..s
1000
: Y
d
= 0.071. The vertical
bar marks the point at which the seeds were ex-
hausted. (Sch?fer and Bildhauer, 2013, p. 31)
crawl was halted, the crawler frontier was emptied,
and the crawl was then re-started with a selection
of the URLs harvested in the previous phase. Only
those URLs were used which came from docu-
ments with a Badness score of 10 or lower (= doc-
uments in which the distribution of the most fre-
quent function words fits the expected distribution
for Flemish very well, cf. Section 2), and from text
blocks with a boilerplate score (Sch?fer and Bild-
hauer, 2012) in [0.5, 1] (= likely not boilerplate).
Additionally, it was made sure that no URLs were
re-used between the five phases. The very promis-
ing results are plotted in Figure 3.
0
500
1000
1500
2000
0.000.100.200.30
Figure 3: Yield ratio over crawl time with cyclic
URL selection in the .be TLD. The x axis shows
the crawl progression in snapshots of 400 MB of
raw crawled data (= one Heritrix ARC file). The y
axis shows the yield ratio for each snapshot. The
five phases are clearly distinguishable by the sud-
den increases in yield ratio.
11
phase adjusted R
2
p (ANOVA)
1 0.8288 < 0.001
2 0.9187 < 0.001
3 0.8308 < 0.001
4 0.9125 < 0.001
5 0.9025 < 0.001
Table 1: Fit of linear models for the decrease in
the yield ratios of the first 100 snapshots in each
of the five phases of the .be crawl. For the first
phase, only 50 snapshots were crawled and fitted.
The decline of the yield ratio is almost linear
for the first 100 snapshots in the five phases (cf.
Table 1), where each phase has roughly 500 snap-
shots in total, and one snapshot corresponds to
400 MB of downloaded raw data. After this de-
cline, the yield ratio remains at low levels around
0.05. Cyclic URL selection, however, repeatedly
manages to push the yield ratio to above 0.2 for a
short period. The subsequent sharp decline shows
that link selection/prioritization should rather be
implemented in the crawler frontier management
in order to achieve a constant effect over longer
crawls (cf. Section 5).
4 Experiment 3: Internal Crawl Analysis
For the last experiment, we used the most recent
version of the texrex toolkit, which writes full
link structures for the processed documents as a
by-product.
3
An internal analysis of a small por-
tion of a crawled data set from the German TLD
was performed, which is part of the raw mate-
rial of the DECOW corpus (Sch?fer and Bild-
hauer, 2012). The data set contains 11, 557, 695
crawled HTML documents and 81, 255, 876 http
links extracted from the crawled documents (only
<a> tags). Among the link URLs in the sam-
ple, 711, 092 are actually links to documents in
the sample, so we could analyze exactly those
711, 092 links. It should be noticed that we only
looked at links to different hosts, such that host-
internal links (navigation to ?Home?, etc.) are not
included in the analysis.
In this experiment, we were interested specif-
ically in the many documents which we usually
discard right away simply because they are either
very short (below 2 KB of unstripped HTML) or
perfect duplicates of other documents. This is a
3
The new version (release name hyperhyper) has been
released and documented at http://texrex.sf.net/.
positives negatives
true 69, 273 342, 430
false 237, 959 61, 430
Table 2: Confusion matrix for binary download
decisions based on the Badness of the document
containing the URL for the DECOW crawl sam-
ple described in Section 4. Badness threshold at
10. Precision=0.225, Recall=0.530, F
1
=0.316.
step of document selection which usually precedes
the cleansing used for the experiments described
in Sections 2 and 3. The analysis shows that of the
711, 092 link URLs in the sample, 130, 703 point
to documents which are not perfect duplicates of
other documents and which are over 2 KB long.
580, 389 of them point to documents which do not
satisfy these criteria. We then evaluated the quality
of the link environments in terms of their Badness
and boilerplate scores. The results are shown in
Figures 4 and 5.
4
0.20.40.60.8
05
101
520
253
035
404
550
retained deleted
Figure 4: Badness scores of the links in the crawl
analysis described in Section 4. The x axis shows
the Badness scores of the documents which linked
to the retained (?good?) and the deleted (?bad?)
documents. The y axis shows the proportion of
retained/deleted documents for which the Badness
score is ? x. (Lower Badness scores are better.)
The observable correlation between the quality
of a link?s context and the quality of the page be-
hind the link is stronger for the boilerplate score
than for the Badness score. For example, had
we only followed links from documents with a
Badness score of 10 or lower (= better), then
4
Notice that the older version of texrex used in the
experiments described in Sections 2 and 3 assigns a boiler-
plate score of 1 to text blocks which are most likely good
text, while the new texrex-hyperhyper assigns 1 to text
blocks which are most likely boilerplate. Take this into ac-
count when comparing the thresholds mentioned there and
those reported here.
12
0.50.60.70.80.91.0
?
1?0.8
?
0.5
?
0.20
0.20
.40.6
0.81
retained deleted
Figure 5: Boilerplate scores of the links in the
crawl analysis described in Section 4. The x axis
shows the boilerplate scores of the blocks which
linked to the retained (?good?) and the deleted
(?bad?) documents. The y axis shows the propor-
tion of retained/deleted documents for which the
boilerplate score is? x. (Lower boilerplate scores
are better.)
positives negatives
true 83, 650 522, 350
false 58, 039 47, 053
Table 3: Confusion matrix for binary down-
load decisions based on the boilerplate score of
the block containing the URL for the DECOW
crawl sample described in Section 4. Boilerplate
threshold at 0.5. Precision=0.590, Recall=0.640,
F
1
=0.614.
0.59?580, 389 = 342, 430 bad documents would
not have been downloaded, but at the same time
0.47?130, 703 = 61, 430 good documents would
have been lost. Tables 2 and 3 show a confusion
matrix for a reasonable Badness threshold (10) and
a reasonable boilerplate threshold (0.5). Obvi-
ously, if we use Badness and boilerplate scores of
the link context to make a binary download deci-
sion, the accuracy is much too low, which is why
we suggest to merely prioritize URLs instead of
discarding them, cf. Section 5.
5 Conclusion and
Planned Crawler Architecture
We have shown that two standard cleaning algo-
rithms used in web corpus construction, i. e., text
quality evaluation based on frequent short words
and boilerplate detection (as implemented in the
texrex toolkit) have a high potential for optimiz-
ing web corpus crawling through the prioritization
of harvested URLs in a crawler system.
We are now in the process of designing a custom
web corpus crawler system called HeidiX, which
integrates the texrex post-processing tools for
weight estimation based on the methods described
in this paper. Cf. Figure 6, which schematically
shows the current design draft.
5
HeidiX is designed with a system of ranked
URL back queues for harvested links (cf.
UrlQueues). Each queue holds URLs for which
the weight estimation is within a specifiable in-
terval, such that the most promising URLs are in
one queue, etc. The actual downloading is per-
formed by massively parallel fetcher threads in
the FetcherPool, which (in the final software) will
talk to a DNS cacher and a politeness manager,
which handles caching of Robots Exclusion In-
formation and politeness intervals. The fetcher
threads pop URLs from one of the ranked queues,
which is selected randomly with prior probabili-
ties inversely proportional to the rank of the queue.
Thus, promising URLs are popped more often and
less promising ones less often.
For guessing the weight, pluggable modules
can be used and combined in the Focused-
Walker container. Currently, we have the stan-
dard UrlSeenFilter, which is based on our own
self-scaling Bloom Filter implementation (Bloom,
1970; Almeida et al., 2007), and which pre-
vents any URL from being queued more than
once. We have plans for a URL-based language
guesser (Baykan et al., 2008) in the form of
the LanguagePredictor, and a prioritizer based
on the yield from specific hosts as described in
Suchomel and Pomik?lek (2012) in the form of
the HostYieldPrioritizer, which reads statistics di-
rectly from the texrex module. The texrex
module extracts all hyperlinks from processed
documents and tags them with the quality scores
described in this paper, such that the QualityPri-
oritizer module can adjust the expected weight of
the document behind each URL.
The HeidiX architecture also features an al-
ternative queueing strategy in the form of the
RandomWalker, which allows users to obtain uni-
form random samples from the web based on ex-
isting algorithms (Henzinger et al., 2000; Rus-
mevichientong et al., 2001). Since obtaining such
samples is a goal which is mostly orthogonal to the
5
Like texrex, it is written entirely in the FreePascal
dialect of ObjectPascal (http://freepascal.org/),
uses only very few additional C libraries, and will be released
under the GPL 3.
13
texrex
UrlQueues
FetcherPoolRandomWalker(CLARAx)
HTML
CorpusURL
WWW
DNSCacher
PolitenessManager
Snapshots
Statistics
SnapshotKeeper
LanguagePredictor
QualityPrioritizer
HostYieldPrioritizer
FocusedWalkerUrlSeenFilter
Figure 6: HeidiX Crawler Architecture. Grayed modules are done as of March 2014. The Focused-
Walker implements an ?efficiently locate good corpus document? URL prioritization scheme; the Ran-
domWalker implements bias-corrected Random Walk URL selection for obtaining uniform random sam-
ples.
one assumed in this paper, we do not discuss this
further here. Finally, a SnapshotKeeper module
allows users to halt and continue crawls by writ-
ing/reading the current state of the relevant com-
ponents to/from disk.
We hope that HeidiX will become a valuable
tool in both the efficient construction of very large
web corpora (FocusedWalker) and the construc-
tion of smaller unbiased reference samples as well
as web analysis (RandomWalker).
References
Paulo S?rgio Almeida, Carlos Baquero, Nuno
Pregui?a, and David Hutchison. 2007. Scalable
bloom filters. Information Processing Letters,
101:255?261.
Adrien Barbaresi. 2013. Crawling microblogging ser-
vices to gather language-classified urls. workflow
and case study. In 51st Annual Meeting of the As-
sociation for Computational Linguistics Proceed-
ings of the Student Research Workshop, pages 9?15,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
Eda Baykan, Monika Henzinger, and Ingmar Weber.
2008. Web page language identification based on
URLs. In Proceedings of the VLDB Endowment,
pages 176?187.
Chris Biemann, Felix Bildhauer, Stefan Evert, Dirk
Goldhahn, Uwe Quasthoff, Roland Sch?fer, Jo-
hannes Simon, Leonard Swiezinski, and Torsten
Zesch. 2013. Scalable construction of high-quality
web corpora. Journal for Language Technology and
Computational Linguistics, 28(2):23?60.
Burton Bloom. 1970. Space/time trade-offs in hash
coding with allowable errors. Communications of
ACM, 13(7):422?426.
Soumen Chakrabarti, Martin van den Berg, and Byron
Dom. 1999. Focused crawling: a new approach
to topic-specific web resource discovery. Computer
Networks, 31:1623?1640.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of the 3rd In-
ternation conference on Statistical Analysis of Tex-
tual Data (JADT 1995), pages 263?268, Rome.
Monika R. Henzinger, Allan Heydon, Michael Mitzen-
macher, and Marc Najork. 2000. On near-uniform
URL sampling. In Proceedings of the 9th Inter-
national World Wide Web conference on Computer
Networks: The International Journal of Computer
and Telecommunications Networking, pages 295?
308. North-Holland Publishing Co.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
Off-the-shelf Language Identification Tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012),
Jeju, Republic of Korea.
Filippo Menczer, Gautam Pant, and Padmini Srini-
vasan. 2004. Topical web crawlers: Evaluating
adaptive algorithms. ACM Trans. Internet Technol.,
4(4):378?419.
14
Gordon Mohr, Michael Stack, Igor Ranitovic, Dan Av-
ery, and Michele Kimpton. 2004. Introduction
to Heritrix, an archival quality web crawler. In
Proceedings of the 4th International Web Archiving
Workshop (IWAW?04).
Christopher Olston and Marc Najork. 2010. Web
Crawling, volume 4(3) of Foundations and Trends
in Information Retrieval. now Publishers, Hanover,
MA.
Paat Rusmevichientong, David M. Pennock, Steve
Lawrence, and C. Lee Giles. 2001. Methods for
sampling pages uniformly from the World Wide
Web. In In AAAI Fall Symposium on Using Uncer-
tainty Within Computation, pages 121?128.
M.S. Safran, A. Althagafi, and Dunren Che. 2012.
Improving relevance prediction for focused Web
crawlers. In IEEE/ACIS 11th International Confer-
ence on Computer and Information Science (ICIS),
2012, pages 161?166.
Roland Sch?fer and Felix Bildhauer. 2012. Build-
ing large corpora from the web using a new ef-
ficient tool chain. In Nicoletta Calzolari, Khalid
Choukri, Thierry Declerck, Mehmet U
?
gur Do
?
gan,
Bente Maegaard, Joseph Mariani, Jan Odijk, and
Stelios Piperidis, editors, Proceedings of the Eight
International Conference on Language Resources
and Evaluation (LREC?12), pages 486?493, Istan-
bul. ELRA.
Roland Sch?fer and Felix Bildhauer. 2013. Web Cor-
pus Construction. Synthesis Lectures on Human
Language Technologies. Morgan and Claypool, San
Francisco.
Roland Sch?fer, Adrien Barbaresi, and Felix Bildhauer.
2013. The good, the bad, and the hazy: Design de-
cisions in web corpus construction. In Stefan Evert,
Egon Stemle, and Paul Rayson, editors, Proceedings
of the 8th Web as Corpus Workshop (WAC-8), pages
7?15, Lancaster. SIGWAC.
V?t Suchomel and Jan Pomik?lek. 2012. Effcient Web
crawling for large text corpora. In Adam Kilgarriff
and Serge Sharoff, editors, Proceedings of the sev-
enth Web as Corpus Workshop, pages 40?44.
15
