Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 305?312
Manchester, August 2008
Homotopy-based Semi-Supervised Hidden Markov Models
for Sequence Labeling?
Gholamreza Haffari and Anoop Sarkar
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
{ghaffar1,anoop}@cs.sfu.ca
Abstract
This paper explores the use of the homo-
topy method for training a semi-supervised
Hidden Markov Model (HMM) used for
sequence labeling. We provide a novel
polynomial-time algorithm to trace the lo-
cal maximum of the likelihood function
for HMMs from full weight on the la-
beled data to full weight on the unla-
beled data. We present an experimental
analysis of different techniques for choos-
ing the best balance between labeled and
unlabeled data based on the characteris-
tics observed along this path. Further-
more, experimental results on the field seg-
mentation task in information extraction
show that the Homotopy-based method
significantly outperforms EM-based semi-
supervised learning, and provides a more
accurate alternative to the use of held-out
data to pick the best balance for combin-
ing labeled and unlabeled data.
1 Introduction
In semi-supervised learning, given a sample con-
taining both labeled data L and unlabeled data
U , the maximum likelihood estimator ?mle maxi-
mizes:
L(?) :=
?
(x,y)?L
log P (x,y|?)+
?
x?U
log P (x|?)
(1)
where y is a structured output label, e.g. a se-
quence of tags in the part-of-speech tagging task,
or parse trees in the statistical parsing task. When
the number of labeled instances is very small com-
pared to the unlabeled instances, i.e. |L| ? |U |,
? We would like to thank Shihao Ji and the anonymous
reviewers for their comments. This research was supported in
part by NSERC, Canada.
? c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the likelihood of labeled data is dominated by that
of unlabeled data, and the valuable information in
the labeled data is almost completely ignored.
Several studies in the natural language process-
ing (NLP) literature have shown that as the size of
unlabeled data increases, the performance of the
model with ?mle may deteriorate, most notably
in (Merialdo, 1993; Nigam et al, 2000). One strat-
egy commonly used to alleviate this problem is to
explicitly weigh the contribution of labeled and un-
labeled data in (1) by ? ? [0, 1]. This new parame-
ter controls the influence of unlabeled data but is
estimated either by (a) an ad-hoc setting, where
labeled data is given more weight than unlabeled
data, or (b) by using the EM algorithm or (c) by
using a held-out set. But each of these alternatives
is problematic: the ad-hoc strategy does not work
well in general; the EM algorithm ignores the la-
beled data almost entirely; and using held-out data
involves finding a good step size for the search,
but small changes in ? may cause drastic changes
in the estimated parameters and the performance
of the resulting model. Moreover, if labeled data is
scarce, which is usually the case, using a held-out
set wastes a valuable resource1 .
In this paper, we use continuation techniques
(Corduneanu and Jaakkola, 2002) for determining
? for structured prediction tasks involving HMMs,
and more broadly, the product of multinomials
(PoM) model. We provide a polynomial-time al-
gorithm for HMMs to trace the local maxima of
the likelihood function from full weight on the la-
beled data to full weight on the unlabeled data. In
doing so, we introduce dynamic programming al-
gorithms for HMMs that enable the efficient com-
putation over unlabeled data of the covariance be-
tween pairs of state transition counts and pairs
of state-state and state-observation counts. We
present a detailed experimental analysis of differ-
ent techniques for choosing the best balance be-
1Apart from these reasons, we also provide an experimen-
tal comparision between the homotopy based approach, the
EM algorithm, and the use of a held out set.
305
tween labeled and unlabeled data based on the
characteristics observed along this path. Further-
more, experimental results on the field segmen-
tation task in information extraction show that
the Homotopy-based method significantly outper-
forms EM-based semi-supervised learning, and
provides a more accurate alternative to the use of
held-out data to pick the best balance for combin-
ing labeled and unlabeled data. We argue this ap-
proach is a best bet method which is robust to dif-
ferent settings and types of labeled and unlabeled
data combinations.
2 Homotopy Continuation
A continuation method embeds a given hard root
finding problem G(?) = 0 into a family of prob-
lems H(?(?), ?) = 0 parameterized by ? such
that H(?(1), 1) = 0 is the original given problem,
and H(?(0), 0) = 0 is an easy problem F (?) = 0
(Richter and DeCarlo, 1983). We start from a solu-
tion ?
0
for F (?) = 0, and deform it to a solution
?
1
for G(?) = 0 while keeping track of the so-
lutions of the intermediate problems2. A simple
deformation or homotopy function is:
H(?, ?) = (1? ?)F (?) + ?G(?) (2)
There are many ways to define a homotopy map,
but it is not trivial to always guarantee the exis-
tence of a path of solutions for the intermediate
problems. Fortunately for the homotopy map we
will consider in this paper, the path of solutions
which starts from ? = 0 to ? = 1 exists and is
unique.
In order to find the path numerically, we seek
a curve ?(?) which satisfies H(?(?), ?) = 0.
This is found by differentiating with respect to ?
and solving the resulting differential equation. To
handle singularities along the path and to be able
to follow the path beyond them, we introduce a
new variable s (which in our case is the unit path
length) and solve the following differential equa-
tion for (?(s), ?(s)):
?H(?, ?)
??
d?
ds
+
?H(?, ?)
??
d?
ds
= 0 (3)
subject to ||(d?
ds
,
d?
ds
)||
2
= 1 and the initial con-
dition (?(0), ?(0)) = (?
0
, 0). We use the Euler
2This deformation gives us a solution path (?(?), ?)
in Rd+1 for ? ? [0, 1], where each component of the d-
dimensional solution vector ?(?) = (?
1
(?), .., ?
d
(?)) is a
function of ?.
method (see Algorithm 1) to solve (3) but higher
order methods such as Runge-Kutta of order 2 or 3
can also be used.
3 Homotopy-based Parameter Estimation
One way to control the contribution of the labeled
and unlabeled data is to parameterize the log like-
lihood function as L
?
(?) defined by
1? ?
|L|
?
(x,y)?L
log P (x, y|?) +
?
|U |
?
x?U
log P (x|?)
How do we choose the best ?? An operator called
EM
?
is used with the property that its fixed points
(locally) maximize L
?
(?). Starting from a fixed
point of EM
?
when ? is zero3, the path of fixed
point of this operator is followed for ? > 0 by
continuation techniques. Finally the best value for
? is chosen based on the characteristics observed
along the path. One option is to choose an allo-
cation value where the first critical4 point occurs
had we followed the path based on ?, i.e. without
introducing s (see Sec. 2). Beyond the first criti-
cal point, the fixed points may not have their roots
in the starting point which has all the informa-
tion from labeled data (Corduneanu and Jaakkola,
2002). Alternatively, an allocation may be cho-
sen which corresponds to the model that gives the
maximum entropy for label distributions of unla-
beled instances (Ji et al, 2007). In our experi-
ments, we compare all of these methods for de-
termining the choice of ?.
3.1 Product of Multinomials Model
Product of Multinomials (PoM) model is an im-
portant class of probabilistic models especially for
NLP which includes HMMs and PCFGs among
others (Collins, 2005). In the PoM model, the
probability of a pair (x,y) is
P (x,y|?) =
M
?
m=1
?
???
m
?
m
(?)
Count(x,y,?) (4)
where Count(x,y, ?) shows how many times an
outcome ? ? ?
m
has been seen in the input-output
pair (x,y), and M is the total number of multino-
mials. A multinomial distribution parameterized
3In general, EM
0
can have multiple local maxima, but in
our case, EM
0
has only one global maximum, found analyti-
cally using relative frequency estimation.
4A critical point is where a discontinuity or bifurcation oc-
curs. In our setting, almost all of the critical points correspond
to discontinuities (Corduneanu, 2002).
306
by ?
m
is put on each discrete space ?
m
where the
probability of an outcome ? is denoted by ?
m
(?).
So for each space ?
m
, we have
?
???
m
?
m
(?) =
1.
Consider an HMM with K states. There are
three types of parameters: (i) initial state probabili-
ties P (s) which is a multinomial over states ?
0
(s),
(ii) state transition probabilities P (s?|s) which are
K multinomials over states ?
s
(s
?
) , and (iii) emis-
sion probabilities P (a|s) which are K multinomi-
als over observation alphabet ?
s+K
(a). To com-
pute the probability of a pair (x,y), normally we
go through the sequence and multiply the proba-
bility of the seen state-state and state-observation
events:
P (x,y|?) = ?
0
(y
0
)?
y
1
+K
(x
1
)
|y|
Y
t=2
?
y
t?1
(y
t
)?
y
t
+K
(x
t
)
which is in the form of (4) if it is written in terms
of the multinomials involved.
3.2 EM
?
Operator for the PoM Model
Usually EM is used to maximize L(?) and esti-
mate the model parameters in the situation where
some parts of the training data are hidden. EM has
an intuitive description for the PoM model: start-
ing from an arbitrary value for parameters, itera-
tively update the probability mass of each event
proportional to its count in labeled data plus its ex-
pected count in the unlabeled data, until conver-
gence.
By changing the EM?s update rule, we get an
algorithm for maximizing L
?
(?):
?
?
m
(?) =
1? ?
|L|
?
(x,y)?L
Count(x,y, ?) +
?
|U |
?
x?U
?
y?Y
x
Count(x,y, ?)P (y|x,?
old
) (5)
where ??
m
is the unnormalized parameter vector,
i.e. ?
m
(?) =
?
?
m
(?)
P
???
m
?
?
m
(?)
. The expected counts
can be computed efficiently based on the forward-
backward recurrence for HMMs (Rabiner, 1989)
and inside-outside recurrence for PCFGs (Lari and
Young, 1990). The right hand side of (5) is an op-
erator we call EM
?
which transforms the old pa-
rameter values to their new (unnormalized) values.
EM
0
and EM
1
correspond respectively to purely
supervised and unsupervised parameter estimation
settings, and:
EM
?
(?) = (1? ?)EM
0
(?) + ?EM
1
(?) (6)
3.3 Homotopy for the PoM Model
The iterative maximization algorithm, described in
the previous section, proceeds until it reaches a
fixed point EM
?
(?) =
?
?, where based on (6):
(1? ?) (
?
?? EM
0
(?))
| {z }
F (?)
+? (
?
?? EM
1
(?))
| {z }
G(?)
= 0 (7)
The above condition governs the (local) maxima
of EM
?
. Comparing to (2) we can see that (7) can
be viewed as a homotopy map.
We can generalize (7) by replacing (1? ?) with
a function g
1
(?) and ? with g
2
(?)
5
. This corre-
sponds to other ways of balancing labeled and un-
labeled data log-likelihoods in (1). Moreover, we
may partition the parameter set and use the homo-
topy method to just estimate the parameters in one
partition while keeping the rest of parameters fixed
(to inject some domain knowledge to the estima-
tion procedure), or repeat it through partitions. We
will see this in Sec. 5.2 where the transition matrix
of an HMM is frozen and the emission probabili-
ties are learned with the continuation method.
Algorithm 1 describes how to use continuation
techniques used for homotopy maps in order to
trace the path of fixed points for the EM
?
oper-
ator. The algorithm uses the Euler method to solve
the following differential equation governing the
fixed points of EM
?
:
[
??
?
?
EM
1
(?)? I EM
1
(?)? EM
0
]
[
d
?
?
d?
]
= 0
For PoM models?
?
?
EM
1
(?) can be written com-
pactly as follows6:
1
|U |
?
x?U
COV
P (y|x,?)
[
Count(x,y)
]
?H (8)
where COV
P (y|x,?)
[Count(x,y)] is the con-
ditional covariance matrix of all features
Count(x,y, ?) given an unlabeled instance
x. We denote the entry corresponding to events ?
1
and ?
2
of this matrix by COV
P (y|x,?)
(?
1
, ?
2
); H
is a block diagonal matrix built from H
?
i
where
H
?
i
= (
?
?
i
(?
1
), ..,
?
?
i
(?
|?
i
|
)) ? I?
1
|?
i
|?|?
i
|
?
???
i
?
?
i
(?)
5However the following two conditions must be satisfied:
(i) the deformation map is reduced to ( ???EM
0
(?)) at ? =
0 and ( ???EM
1
(?)) at ? = 1, and (ii) the path of solutions
exists for Eqn. (2).
6A full derivation is provided in (Haffari and Sarkar, 2008)
307
Algorithm 1 Homotopy Continuation for EM
?
1: Input: Labeled data set L
2: Input: Unlabeled data set U
3: Input: Step size ?
4: Initialize [ ?? ?] = [EM
0
0] based on L
5: ?old ? [0 1]
6: repeat
7: Compute ?
?
?
EM
1
(?) and EM
1
(?) based
on unlabeled data U
8: Compute ? = [d?? d?] as the kernel of
[??
?
?
EM
1
(?)? I EM
1
(?)? EM
0
]
9: if ? ? ?old < 0 then
10: ? ? ??
11: end if
12: [ ?? ?]? [ ?? ?] + ? ?
||?||
2
13: ?old ? ?
14: until ? ? 1
Computing the covariance matrix in (8) is a
challenging problem because it consists of sum-
ming quantities over all possible structures Y
x
as-
sociated with each unlabeled instance x, which is
exponential in the size of the input for HMMs.
4 Efficient Computation of the Covari-
ance Matrix
The entry COV
P (y|x,?)
(?
1
, ?
2
) of the features co-
variance matrix is
E[Count(x,y, ?
1
)Count(x,y, ?
2
)]?
E[Count(x,y, ?
1
)]E[Count(x,y, ?
2
)]
where the expectations are taken under P (y|x,?).
To efficiently calculate the covariance, we need
to be able to efficiently compute the expectations.
The linear count expectations can be computed ef-
ficiently by the forward-backward recurrence for
HMMs. However, we have to design new algo-
rithms for quadratic count expectations which will
be done in the rest of this section.
We add a special begin symbol to the se-
quences and replace the initial probabilities with
P (s|begin). Based on the terminology used in (4),
the outcomes belong to two categories: ? = (s, s?)
where state s? follows state s, and ? = (s, a)
where symbol a is emitted from state s. De-
fine the feature function f
?
(x,y, t) to be 1 if the
outcome ? happens at time step t, and 0 other-
wise. Based on the fact that Count(x,y, ?) =
?
|x|
t=1
f
?
(x,y, t), we have
E[Count(x,y, ?
1
)Count(x,y, ?
2
)] =
?
t
1
?
t
2
?
y?Y
x
f
?
1
(x,y, t
1
)f
?
2
(x,y, t
2
)P (y|x,?)
which is the summation of |x|2 different expecta-
tions. Fixing two positions t
1
and t
2
, each expec-
tation is the probability (over all possible labels) of
observing ?
1
and ?
2
at these two positions respec-
tively, which can be efficiently computed using the
following data structure. Prepare an auxiliary table
Z
x containing P (x
[i+1,j]
, s
i
, s
j
), for every pair of
states s
i
and s
j
for all positions i, j (i ? j):
Z
x
i,j
(s
i
, s
j
) =
X
s
i+1
,..,s
j?1
j?1
Y
k=i
P (s
k+1
|s
k
)P (x
k+1
|s
k+1
)
Let matrix Mx
k
= [M
x
k
(s, s
?
)] where Mx
k
(s, s
?
) =
P (s
?
|s)P (x
k
|s
?
); then Zx
i,j
=
?
j?1
k=i
M
x
k
. Forward
and backward probabilities can also be computed
from Zx, so building this table helps to compute
both linear and quadratic count expectations.
With this table, computing the quadratic counts
is straightforward. When both events are of type
state-observation, i.e. ? = (s, a) and ?? = (s?, a?),
their expected quadratic count can be computed as
?
t
1
?
t
2
?
x
t
1
,a
?
x
t
2
,a
?
[
?
k
P (k|begin)Zx
1,t
1
(k, s).
Z
x
t
1
,t
2
(s, s
?
).
?
k
Z
x
t
2
,n
(s
?
, k)P (end|k)
]
where ?
x
t
,a
is 1 if x
t
is equal to a and 0 otherwise.
Likewise we can compute the expected quadratic
counts for other combination of events: (i) both
are of type state-state, (ii) one is of type state-state
and the other state-observation.
There are L(L+1)
2
tables needed for a sequence
of length L, and the time complexity of building
each of them is O(K3) where K is the number of
states in the HMM. When computing the covari-
ance matrix, the observations are fixed and there
is no need to consider all possible combinations
of observations and states. The most expensive
part of the computation is the situation where the
two events are of type state-state which amounts
to O(L2K4) matrix updates. Noting that a single
entry needs O(K) for its updating, the time com-
plexity of computing expected quadratic counts for
a single sequence is O(L2K5). The space needed
to store the auxiliary tables is O(L2K2) and the
space needed for covariance matrix is O((K2 +
NK)
2
) where N is the alphabet size.
5 Experimental Results
In the field segmentation task, a document is con-
sidered to be a sequence of fields. The goal is
308
[EDITOR A. Elmagarmid, editor.] [TITLE Transaction Models for Advanced Database Applications] [PUBLISHER Morgan-
Kaufmann,] [DATE 1992.]
Figure 1: A field segmentation example for Citations dataset.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.12
0.13
0.14
0.15
0.16
0.17
0.18
?
Er
ro
r (
pe
r p
os
itio
n)
EM?2
freez
 Error on Citation Test (300L5000U)
 
 
Viterbi Decoding
SMS Decoding
?MLE
(a)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.12
0.13
0.14
0.15
0.16
0.17
0.18
0.19
0.2
0.21
?
Er
ro
r (
pe
r p
os
itio
n)
EM?
freez
 Error on Citation Test (300L5000U)
 
 
Viterbi Decoding
SMS Decoding
?MLE
(b)
Figure 2: EM
?
error rates while increasing the allocation from 0 to 1 by the step size 0.025.
to segment the document into fields, and to label
each field. In our experiments we use the bibli-
ographic citation dataset described in (Peng and
McCallum, 2004) (see Fig. 1 for an example of
the input and expected label output for this task).
This dataset has 500 annotated citations with 13
fields; 5000 unannotated citations were added to
it later by (Grenager et al, 2005). The annotated
data is split into a 300-document training set, a
100-document development (dev) set, and a 100-
document test set7.
We use a first order HMM with the size of hid-
den states equal to the number of fields (equal to
13). We freeze the transition probabilities to what
has been observed in the labeled data and only
learn the emission probabilities. The transition
probabilities are kept frozen due to the nature of
this task in which the transition information can
be learned with very little labeled data, e.g. first
start with ?author? then move to ?title? and so on.
However, the challenging aspect of this dataset is
to find the segment spans for each field, which de-
pends on learning the emission probabilities, based
on the fixed transition probabilities.
At test time, we use both Viterbi (most probable
sequence of states) decoding and sequence of most
probable states decoding methods, and abbreviate
them by Viterbi and SMS respectively. We report
results in terms of precision, recall and F-measure
for finding the citation fields, as well as accuracy
calculated per position, i.e. the ratio of the words
labeled correctly for sequences to all of the words.
The segment-based precision and recall scores are,
7From http://www.stanford.edu/grenager/data/unsupie.tgz
of course, lower than the accuracy computed on
the per-token basis. However, both these numbers
need to be taken into account in order to under-
stand performance in the field segmentation task.
Each input word sequence in this task is very long
(with an average length of 36.7) but the number of
fields to be recovered is a small number compar-
atively (on average there are 5.4 field segments in
a sentence where the average length of a segment
is 6.8). Even a few one-word mistakes in finding
the full segment span leads to a drastic fall in pre-
cision and recall. The situation is quite different
from part-of-speech tagging, or even noun-phrase
chunking using sequence learning methods. Thus,
for this task both the per-token accuracy as well as
the segment precision and recall are equally impor-
tant in gauging performance.
Smoothing to remove zero components in the
starting point is crucial otherwise these features do
not generalize well and yet we know that they have
been observed in the unlabeled data. We use a sim-
ple add-? smoothing, where ? is .2 for transition
table entries and .05 for the emission table entries.
In all experiments, we deal with unknown words in
test data by replacing words seen less than 5 times
in training by the unknown word token.
5.1 Problems with MLE
MLE chooses to set ? = |U |
|L|+|U |
which almost
ignores labeled data information and puts all the
weight on the unlabeled data8. To see this empir-
ically, we show the per position error rates at dif-
8One anonymous reviewer suggests using ? = |L|
|L|+|U|
but the ?best bet? for different tasks that we mention in the
Introduction may not necessarily be a small ? value.
309
ferent source allocation for HMMs trained on 300
labeled and 5000 unlabeled sequences for the Ci-
tation dataset in Fig. 2(a). For each allocation we
have run EM
?
algorithm, initialized to smoothed
counts from labeled data, until convergence. As
the plots show, initially the error decreases as ? in-
creases; however, it starts to increase after ? passes
a certain value. MLE has higher error rates com-
pared to complete data estimate, and its perfor-
mance is far from the best way of combining la-
beled and unlabeled data.
In Fig. 2(b), we have done similar experiment
with the difference that for each value of ?, the
starting point of the EM
?
is the final solution
found in the previous value of ?. As seen in the
plot, the intermediate local optima have better per-
formance compared to the previous experiment,
but still the imbalance between labeled and unla-
beled data negatively affects the quality of the so-
lutions compared to the purely supervised solution.
The likelihood surface is non-convex and has
many local maxima. Here EM performs hill climb-
ing on the likelihood surface, and arguably the re-
sulting (locally optimal) model may not reflect the
quality of the globally optimal MLE. But we con-
jecture that even the MLE model(s) which globally
maximize the likelihood may suffer from the prob-
lem of the size imbalance between labeled and un-
labeled data, since what matters is the influence
of unlabeled data on the likelihood. (Chang et.
al., 2007) also report on using hard-EM on these
datasets9 in which the performance degrades com-
pared to the purely supervised model.
5.2 Choosing ? in Homotopy-based HMM
We analyze different criteria in picking the best
value of ? based on inspection of the continuation
path. The following criteria are considered:
? monotone: The first iteration in which the
monotonicity of the path is changed, or equiva-
lently the first iteration in which the determinant
of ??
?
?
EM
1
(?)?I in Algorithm 1 becomes zero
(Corduneanu and Jaakkola, 2002).
? minEig: Instead of looking into the determinant
of the above matrix, consider its minimum eigen-
value. Across all iterations, choose the one for
which this minimum eigenvalue is the lowest.
?maxEnt: Choose the iteration whose model puts
the maximum entropy on the labeling distribution
for unlabeled data (Ji et al, 2007).
9In Hard-EM, the probability mass is fully assigned to the
most probable label, instead of all possible labels.
The second criterion is new, and experimentally
has shown a good performance; it indicates the
amount of singularity of a matrix.
100 150 200 250 300 350 400 450 500
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
# Unlabeled Sequences
?
Best Selected Allocations
 
 
monton
maxEnt
minEig
EM
Figure 3: ? values picked by different methods.
The size of the labeled data is fixed to 100, and
results are averaged over 4 runs. The ? values
picked by MaxEnt method for 500 unlabeled ex-
amples was .008.
We fix 100 labeled sequences and vary the num-
ber of unlabeled sequences from 100 to 500 by a
step of 50. All of the experiments are repeated
four times with different randomly chosen unla-
beled datasets, and the results are the average over
four runs. The chosen allocations based on the de-
scribed criteria are plotted in Figure 3, and their
associated performance measures can be seen in
Figure 4.
Figure 3 shows that as the unlabeled data set
grows, the reliance of ?minEig? and ?monotone?
methods on unlabeled data decreases whereas in
EM it increases. The ?minEig? method is more
conservative than ?monotone? in that it usually
chooses smaller ? values. The plots in Fig-
ure 4 show that homotopy-based HMM always
outperforms EM-based HMM. Moreover, ?max-
Ent? method outperforms other ways of picking
?. However, as the size of the unlabeled data in-
creases, the three methods tend to have similar per-
formances.
5.3 Homotopy v.s. other methods
In the second set of experiments, we compare
the performance of the homotopy based method
against the competitive methods for picking the
value of ?.
We use all of the labeled sequences (size is 300)
and vary the number of unlabeled sequences from
300 to 1000 by the step size of 100. For the first
competitive method, 100 labeled sequences are put
in a held out set and used to select the best value
310
100 200 300 400 500
0.2
0.25
F?measure
# Unlabeled Sequences
100 200 300 400 500
0.8
0.82
0.84
Total accuracy
# Unlabeled Sequences
100 200 300 400 500
0.1
0.15
0.2
0.25
0.3
F?measure
100 200 300 400 500
0.8
0.82
0.84
Total accuracy
 
 
monotone maxEnt minEig EM
Figure 4: The comparison of different techniques
for choosing the best allocation based on datasets
with 100 labeled sequences and varying number of
unlabeled sequences. Each figure shows the av-
erage over 4 runs. F-measure is calculated based
on the segments, and total accuracy is calculated
based on tokens in individual positions. The two
plots in the top represent Viterbi decoding, and the
two plots in the bottom represent SMS decoding.
of ? based on brute-force search using a fixed step
size; afterwards, this value is used to train HMM
(based on 200 remaining labeled sequences and
unlabeled data). The second competitive method,
which we call ?Oracle?, is similar to the previous
method except we use the test set as the held out set
and all of the 300 labeled sequences as the train-
ing set. In a sense, the resulting model is the best
we can expect from cross validation based on the
knowledge of true labels for the test set. Despite
the name ?Oracle?, in this setting the ? value is se-
lected based on the log-likelihood criterion, so it is
possible that the ?Oracle? method is outperformed
by another method in terms of precision/recall/f-
score. Finally, EM is considered as the third base-
line.
The results are summarized in Table 1. When
decoding based on SMS, the homotopy-based
HMM outperforms the ?Held-out? method for all
of performance measures, and generally behaves
better than the ?Oracle? method. When decoding
based on Viterbi, the accuracy of the homotopy-
based HMM is better than ?Held-out? and is in
the same range as the ?Oracle?; the three meth-
ods have roughly the same f-score. The ? value
found by Homotopy gives a small weight to unla-
beled data, and so it might seem that it is ignoring
the unlabeled data. This is not the case, even with
a small weight the unlabeled data has an impact,
as can be seen in the comparison with the purely
Supervised baseline in Table 1 where the Homo-
topy method outperforms the Supervised baseline
by more than 3.5 points of f-score with SMS-
decoding. Homotopy-based HMM with SMS-
decoding outperforms all of the other methods.
We noticed that accuracy was better for 700 un-
labeled examples in this dataset, and so we include
those results as well in Table 1. We observed some
noise in unlabeled sequences; so as the size of the
unlabeled data set grows, this noise increases as
well. In addition to finding the right balance be-
tween labeled and unlabeled data, this is another
factor in semi-supervised learning. For each par-
ticular unlabeled dataset size (we experimented us-
ing 300 to 1000 unlabeled data with a step size of
100) the Homotopy method outperforms the other
alternatives.
6 Related Previous Work
Homotopy based parameter estimation was orig-
inally proposed in (Corduneanu and Jaakkola,
2002) for Na??ve Bayes models and mixture of
Gaussians, and (Ji et al, 2007) used it for HMM-
based sequence classification which means that an
input sequence x is classified into a class label
y ? {1, . . . , k} (the class label is not structured,
i.e. not a sequence of tags). The classification is
done using a collection of k HMMs by computing
Pr(x, y | ?
y
) which sums over all states in each
HMM ?y for input x. The algorithms in (Ji et al,
2007) could be adapted to the task of sequence la-
beling, but we argue that our algorithms provide a
straightforward and direct solution.
There have been some studies using the Cita-
tion dataset, but it is not easy to directly compare
their results due to differences in preprocessing,
the amount of the previous knowledge and rich
features used by the models, and the training data
which were used. (Chang et. al., 2007) used a first
order HMM in order to investigate injecting prior
domain knowledge to self-training style bootstrap-
ping by encoding human knowledge into declara-
tive constraints. (Grenager et al, 2005) used a first
order HMM which has a diagonal transition matrix
and a specialized boundary model. In both works,
the number of randomly selected labeled and un-
labeled training data is varied, which makes a di-
311
size of ? Viterbi decoding SMS decoding
unlab data p, r, f-score accuracy p, r, f-score accuracy
Homotopy 700 .004 .292, .290, .290 87.1% .321, .332, .326 89%1000 .004 .292, .291, .291 87.9% .296, .298, .296 88.6%
Held-out 700 .220 .311, .291, .297 87.1% .295, .288, .289 87.2%1000 .320 .300, .276, .283 86.9% .308, .281, .287 87.2%
Oracle 700 .150 .284, .293, .287 87.8% .295, .313, .303 88%1000 .200 .285, .294, .289 87.9% .277, .292, .284 88.7%
EM 700 .700 .213, .211, .211 84.8% .213, .220, .216 85.2%1000 .770 .199, .198, .198 83.7% .187, .198, .192 83.6%
Supervised 0 0 .281, .278, .279 87% .298, .280, .288 88.4%
Table 1: Results using entire labeled data with segment precision/recall/f-score and token based accuracy.
rect numerical comparison impossible. (Peng and
McCallum, 2004) used only labeled data to train
conditional random fields and HMMs with second
order state transitions where they allow observa-
tion in each position to depend on the current state
as well as observation of the previous position.
7 Conclusion
In many NLP tasks, the addition of unlabeled data
to labeled data can decrease the performance on
that task. This is often because the unlabeled data
can overwhelm the information obtained from the
labeled data. In this paper, we have described a
methodology and provided efficient algorithms for
an approach that attempts to ensure that unlabeled
data does not hurt performance. The experimen-
tal results show that homotopy-based training per-
forms better than other commonly used compet-
itive methods. We plan to explore faster ways
for computing the (approximate) covariance ma-
trix, e.g., label sequences can be sampled from
P (y|x,?) and an approximation of the covari-
ance matrix can be computed based on these sam-
ples. Also, it is possible to compute the covariance
matrix in polynomial-time for labels which have
richer interdependencies such as those generated
by a context free grammars (Haffari and Sarkar,
2008). Finally, in Algorithm 1 we used a fixed
step size; the number of iterations in the homo-
topy path following can be reduced greatly with
adaptive step size methods (Allgower and Georg,
1993).
References
E. L. Allgower, K. Georg 1993. Continuation and Path
Following, Acta Numerica, 2:1-64.
M. Chang and L. Ratinov and D. Roth. 2007. Guiding
Semi-Supervision with Constraint-Driven Learning,
ACL 2007.
M. Collins 2005. Notes on the EM Algorithm, NLP
course notes, MIT.
A. Corduneanu. 2002. Stable Mixing of Complete and
Incomplete Information, Masters Thesis, MIT.
A. Corduneanu and T. Jaakkola. 2002. Continuation
Methods for Mixing Heterogeneous Sources, UAI
2002.
T. Grenager, D. Klein, and C. Manning. 2005. Unsu-
pervised Learning of Field Segmentation Models for
Information Extraction, ACL 2005.
G. Haffari and A. Sarkar. 2008. A Continuation
Method for Semi-supervised Learning in Product
of Multinomials Models, Technical Report. Simon
Fraser University. School of Computing Science.
K. Lari, and S. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm, Computer Speech and Language
(4).
S. Ji, L. Watson and L. Carin. 2007. Semi-Supervised
Learning of Hidden Markov Models via a Homotopy
Method, manuscript.
B. Merialdo. 1993. Tagging English text with a proba-
bilistic model, Computational Linguistics
K. Nigam, A. McCallum, S. Thrun and T. Mitchell.
2000. Text Classification from Labeled and Unla-
beled Documents using EM, Machine Learning, 39.
p. 103-134.
F. Peng and A. McCallum. 2004. Accurate Information
Extraction from Research Papers using Conditional
Random Fields, HLT-NAACL 2004.
L. Rabiner. 1989. A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recogni-
tion, Proc. of the IEEE, 77(2).
S. Richter, and R. DeCarlo. 1983. Continuation meth-
ods: Theory and applications, IEEE Trans. on Auto-
matic Control, Vol 26, issue 6.
312
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 307?308,
New York City, June 2006. c?2006 Association for Computational Linguistics
5. Inductive Semi-supervised Learning Methods for Natural Language Processing
Anoop Sarkar and Gholamreza Haffari, Simon Fraser University
Supervised machine learning methods which learn from labelled (or annotated) data are now widely
used in many different areas of Computational Linguistics and Natural Language Processing. There are
widespread data annotation endeavours but they face problems: there are a large number of languages and
annotation is expensive, while at the same time raw text data is plentiful. Semi-supervised learning methods
aim to close this gap.
The last 6-7 years have seen a surge of interest in semi-supervised methods in the machine learning
and NLP communities focused on the one hand on analysing the situations in which unlabelled data can be
useful, and on the other hand, providing feasible learning algorithms.
This recent research has resulted in a wide variety of interesting methods which are different with respect
to the assumptions they make about the learning task. In this tutorial, we survey recent semi-supervised
learning methods, discuss assumptions behind various approaches, and show how some of these methods
have been applied to NLP tasks.
5.1 Tutorial Outline
1. Introduction
? Spectrum of fully supervised to unsupervised learning, clustering vs. classifiers or model-
based learning
? Inductive vs. Transductive learning
? Generative vs. Discriminative learning
2. Mixtures of Generative Models
? Analysis
? Stable Mixing of Labelled and Unlabelled data
? Text Classification by EM
3. Multiple view Learning
? Co-training algorithm
? Yarowsky algorithm
? Co-EM algorithm
? Co-Boost algorithm
? Agreement Boost algorithm
? Multi-task Learning
4. Semi-supervised Learning for Structured Labels (Discriminative models)
? Simple case: Random Walk
? Potential extension to Structured SVM
5. NLP tasks and semi-supervised learning
? Using EM-based methods to combine labelled and unlabelled data
? When does it work? Some negative examples of semi-supervised learning in NLP
? Examples of various NLP tasks amenable to semi-supervised learning: chunking, parsing,
word-sense disambiguation, etc.
? Semi-supervised methods proposed within NLP and their relation to machine learning
methods covered in this tutorial
? Semi-supervised learning for structured models relevant for NLP such as sequence learning
and parsing
? Semi-supervised learning for domain adaptation in NLP
307
5.2 Target Audience
The target audience is expected to be researchers in computational linguistics and natural language process-
ing who wish to explore methods that will possibly allow learning from smaller size labelled datasets by
exploiting unlabelled data. In particular those who are interested in NLP research into new languages or
domains for which resources do not currently exist, or in novel NLP tasks that do not have existing large
amounts of annotated data. We assume some familiarity with commonly used supervised learning methods
in NLP.
Anoop Sarkar is an Assistant Professor in the School of Computing Science at Simon Fraser University. His
research has been focused on machine learning algorithms applied to the study of natural language. He is
especially interested in algorithms that combine labeled and unlabeled data and learn new information with
weak supervision. Anoop received his PhD from the Department of Computer and Information Science at
the University of Pennsylvania, with Prof. Aravind Joshi was his advisor. His PhD dissertation was entitled
Combining Labeled and Unlabeled Data in Statistical Natural Language Parsing. A full list of papers is
available at http://www.cs.sfu.ca/ anoop. His email address is anoop@cs.sfu.ca
Gholamreza Haffari is a second year PhD student in the School of Computing Science at Simon Fraser
University. He is working under the supervision of Prof. Sarkar towards a thesis on semi-supervised learning
for structured models in NLP. His home page is http://www.cs.sfu.ca/ ghaffar1, and his email address is
ghaffar1@cs.sfu.ca
308
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 173?181,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Dirichlet Trees for Information Retrieval
Gholamreza Haffari
School of Computing Sciences
Simon Fraser University
ghaffar1@cs.sfu.ca
Yee Whye Teh
Gatsby Computational Neuroscience
University College London
ywteh@gatsby.ucl.ac.uk
Abstract
We propose a principled probabilisitc frame-
work which uses trees over the vocabulary to
capture similarities among terms in an infor-
mation retrieval setting. This allows the re-
trieval of documents based not just on occur-
rences of specific query terms, but also on sim-
ilarities between terms (an effect similar to
query expansion). Additionally our principled
generative model exhibits an effect similar to
inverse document frequency. We give encour-
aging experimental evidence of the superiority
of the hierarchical Dirichlet tree compared to
standard baselines.
1 Introduction
Information retrieval (IR) is the task of retrieving,
given a query, the documents relevant to the user
from a large quantity of documents (Salton and
McGill, 1983). IR has become very important in
recent years, with the proliferation of large quanti-
ties of documents on the world wide web. Many IR
systems are based on some relevance score function
R(j, q) which returns the relevance of document j to
query q. Examples of such relevance score functions
include term frequency-inverse document frequency
(tf-idf) and Okapi BM25 (Robertson et al, 1992).
Besides the effect that documents containing
more query terms should be more relevant (term fre-
quency), the main effect that many relevance scores
try to capture is that of inverse document frequency:
the importance of a term is inversely related to the
number of documents that it appears in, i.e. the
popularity of the term. This is because popular
terms, e.g. common and stop words, are often un-
informative, while rare terms are often very infor-
mative. Another important effect is that related or
co-occurring terms are often useful in determining
the relevance of documents. Because most relevance
scores do not capture this effect, IR systems resort to
techniques like query expansion which includes syn-
onyms and other morphological forms of the origi-
nal query terms in order to improve retrieval results;
e.g. (Riezler et al, 2007; Metzler and Croft, 2007).
In this paper we explore a probabilistic model for
IR that simultaneously handles both effects in a prin-
cipled manner. It builds upon the work of (Cow-
ans, 2004) who proposed a hierarchical Dirichlet
document model. In this model, each document is
modeled using a multinomial distribution (making
the bag-of-words assumption) whose parameters are
given Dirichlet priors. The common mean of the
Dirichlet priors is itself assumed random and given
a Dirichlet hyperprior. (Cowans, 2004) showed that
the shared mean parameter induces sharing of infor-
mation across documents in the corpus, and leads to
an inverse document frequency effect.
We generalize the model of (Cowans, 2004) by re-
placing the Dirichlet distributions with Dirichlet tree
distributions (Minka, 2003), thus we call our model
the hierarchical Dirichlet tree. Related terms are
placed close by in the vocabulary tree, allowing the
model to take this knowledge into account when de-
termining document relevance. This makes it unnec-
essary to use ad-hoc query expansion methods, as re-
lated words such as synonyms will be taken into ac-
count by the retrieval rule. The structure of the tree
is learned from data in an unsupervised fashion, us-
173
ing a variety of agglomerative clustering techniques.
We review the hierarchical Dirichlet document
(HDD) model in section 2, and present our proposed
hierarchical Dirichlet tree (HDT) document model
in section 3. We describe three algorithms for con-
structing the vocabulary tree in section 4, and give
encouraging experimental evidence of the superi-
ority of the hierarchical Dirichlet tree compared to
standard baselines in section 5. We conclude the pa-
per in section 6.
2 Hierarchical Dirichlet Document Model
The probabilistic approach to IR assumes that each
document in a collection can be modeled probabilis-
tically. Given a query q, it is further assumed that
relevant documents j are those with highest gener-
ative probability p(q|j) for the query. Thus given q
the relevance score is R(j, q) = p(q|j) and the doc-
uments with highest relevance are returned.
Assume that each document is a bag of words,
with document j modeled as a multinomial distri-
bution over the words in j. Let V be the terms in
the vocabulary, njw be the number of occurrences
of term w ? V in document j, and ?flatjw be the proba-
bility of w occurring in document j (the superscript
?flat? denotes a flat Dirichlet as opposed to our pro-
posed Dirichlet tree). (Cowans, 2004) assumes the
following hierarchical Bayesian model for the docu-
ment collection:
?flat0 = (?flat0w)w?V ? Dirichlet(?u) (1)
?flatj = (?flatjw)w?V ? Dirichlet(??flat0 )
nj = (njw)w?V ? Multinomial(?flatj )
In the above, bold face a = (aw)w?V means that a
is a vector with |V | entries indexed by w ? V , and
u is a uniform distribution over V . The generative
process is as follows (Figure 1(a)). First a vector
?flat0 is drawn from a symmetric Dirichlet distribution
with concentration parameter ?. Then we draw the
parameters ?flatj for each document j from a common
Dirichlet distribution with mean ?flat0 and concentra-
tion parameter ?. Finally, the term frequencies of
the document are drawn from a multinomial distri-
bution with parameters ?flatj .
The insight of (Cowans, 2004) is that because
the common mean parameter ?flat0 is random, it in-
duces dependencies across the document models in
u
njw
nj
J
?flatj
?flat0 ?
?
?k0
?kj
J
uk
?k
?k
(a) (b)
?flatkb
Figure 1: (a) The graphical model representation of the
hierarchical Dirichlet document model. (b) The global
tree and local trees in hierarchical Dirichlet tree docu-
ment model. Triangles stand for trees with the same
structure, but different parameters at each node. The gen-
eration of words in each document is not shown.
the collection, and this in turn is the mechanism for
information sharing among documents. (Cowans,
2004) proposed a good estimate of ?flat0 :
?flat0w = ?/|V |+ n0w? +?w?V n0w
(2)
where n0w is simply the number of documents con-
taining term w, i.e. the document frequency. Inte-
grating out the document parameters ?flatj , we see that
the probability of query q being generated from doc-
ument j is:
p(q|j) =?
x?q
??flat0x + njx
? +?w?V njw
(3)
= Const ??
x?q
Const + njx?/|V |+n0x
? +?w?V njw
Where Const are terms not depending on j. We see
that njx is term frequency, its denominator ?/|V |+
n0x is an inverse document frequency factor, and
? + ?w?V njw normalizes for document length.
The inverse document frequency factor is directly
related to the shared mean parameter, in that popular
terms x will have high ?flat0x value, causing all docu-
ments to assign higher probability to x, and down
weighting the term frequency. This effect will be
inherited by our model in the next section.
174
3 Hierarchical Dirichlet Trees
Apart from the constraint that the parameters should
sum to one, the Dirichlet priors in the HDD model
do not impose any dependency among the param-
eters of the resulting multinomial. In other words,
the document models cannot capture the notion that
related terms tend to co-occur together. For exam-
ple, this model cannot incorporate the knowledge
that if the word ?computer? is seen in a document, it
is likely to observe the word ?software? in the same
document. We relax the independence assump-
tion of the Dirichlet distribution by using Dirichlet
tree distributions (Minka, 2003), which can capture
some dependencies among the resulting parameters.
This allows relationships among terms to be mod-
eled, and we will see that it improves retrieval per-
formance.
3.1 Model
Let us assume that we have a tree over the vocab-
ulary whose leaves correspond to vocabulary terms.
Each internal node k of the tree has a multinomial
distribution over its children C(k). Words are drawn
by starting at the root of the tree, recursively picking
a child l ? C(k) whenever we are in an internal node
k, until we reach a leaf of the tree which corresponds
to a vocabulary term (see Figure 2(b)). The Dirich-
let tree distribution is the product of Dirichlet dis-
tributions placed over the child probabilities of each
internal node, and serves as a (dependent) prior over
the parameters of multinomial distributions over the
vocabulary (the leaves).
Our model generalizes the HDD model by replac-
ing the Dirichlet distributions in (1) by Dirichlet tree
distributions. At each internal node k, define a hier-
archical Dirichlet prior over the choice of the chil-
dren:
?0k = (?0l)l?C(k) ? Dirichlet(?kuk) (4)
?jk = (?jl)l?C(k) ? Dirichlet(?k?0k)
where uk is a uniform distribution over the children
of node k, and each internal node has its own hy-
perparameters ?k and ?k. ?jl is the probability of
choosing child l if we are at internal node k. If the
tree is degenerate with just one internal node (the
root) and all leaves are direct children of the root we
recover the ?flat? HDD model in the previous sec-
tion. We call our model the hierarchical Dirichlet
tree (HDT).
3.2 Inference and Learning
Given a term, the path from the root to the corre-
sponding leaf is unique. Thus given the term fre-
quencies nj of document j as defined in (1), the
number of times njl child l ? C(k) was picked at
node k is known and fixed. The probability of all
words in document j, given the parameters, is then
a product of multinomials probabilities over internal
nodes k:
p(nj |{?jk}) =
?
k
njk!
Q
l?C(k) njl!
?
l?C(k)
?njljl (5)
The probability of the documents, integrating out the
?jk?s, is:
p({nj}|{?0k}) = (6)?
j
?
k
njk !
Q
l?C(k) njl!
?(?k)
?(?k+njk)
?
l?C(k)
?(?k?0l+njl)
?(?k?0l)
The probability of a query q under document j, i.e.
the relevance score, follows from (3):
p(q|j) =?
x?q
?
(kl)
?k?0l+njl
?k+njk (7)
where the second product is over pairs (kl) where k
is a parent of l on the path from the root to x.
The hierarchical Dirichlet tree model we pro-
posed has a large number of parameters and hy-
perparameters (even after integrating out the ?jk?s),
since the vocabulary trees we will consider later typ-
ically have large numbers of internal nodes. This
over flexibility might lead to overfitting or to param-
eter regimes that do not aid in the actual task of IR.
To avoid both issues, we constrain the hierarchical
Dirichlet tree to be centered over the flat hierarchi-
cal Dirichlet document model, and allow it to learn
only the ?k hyperparameters, integrating out the ?jk
parameters.
We set {?0k}, the hyperparameters of the global
tree, so that it induces the same distribution over vo-
cabulary terms as ?flat0 :
?0l = ?flat0l ?0k =
?
l?C(k)
?0l (8)
175
The hyperparameters of the local trees ?k?s are es-
timated using maximum a posteriori learning with
likelihood given by (6), and a gamma prior with
informative parameters. The density function of a
Gamma(a, b) distribution is
g(x; a, b) = x
a?1bae?bx
?(a)
where the mode happens at x = a?1b . We set the
mode of the prior such that the hierarchical Dirichlet
tree reduces to the hierarchical Dirichlet document
model at these values:
?flatl = ??flat0l ?flatk =
?
l?C(k)
?flatl (9)
?k ? Gamma(b?flatk + 1, b)
and b > 0 is an inverse scale hyperparameter to be
tuned, with large values giving a sharp peak around
?flatk . We tried a few values1 of b and have found that
the results we report in the next section are not sen-
sitive to b. This prior is constructed such that if there
is insufficient information in (6) the MAP value will
simply default back to the hierarchical Dirichlet doc-
ument model.
We used LBFGS2 which is a gradient based opti-
mization method to find the MAP values, where the
gradient of the likelihood part of the objective func-
tion (6) is:
? log p({nj}|{?0j})
??k
=?
j
?(?k)??(?k + njk)
+ ?
l?C(k)
?0l
(
?(?k?0l + njl)??(?k?0l)
)
where ?(x) := ? log ?(x)/?x is the digamma func-
tion. Because each ?k can be optimized separately,
the optimization is very fast (approximately 15-30
minutes in the experiments to follow on a Linux ma-
chine with 1.8 GH CPU speed).
4 Vocabulary Tree Structure Learning
The structure of the vocabulary tree plays an impor-
tant role in the quality of the HDT document model,
1Of the form 10i for i ? {?2,?1, 0, 1}.
2We used a C++ re-implementation of Jorge Nocedal?s
LBFGS library (Nocedal, 1980) from the ALGLIB website:
http://www.alglib.net.
Algorithm 1 Greedy Agglomerative Clustering
1: Place m words into m singleton clusters
2: repeat
3: Merge the two clusters with highest similarity, re-
sulting in one less cluster
4: If there still are unincluded words, pick one and
place it in a singleton cluster, resulting in one more
cluster
5: until all words have been included and there is only
one cluster left
since it encapsulates the similarities among words
captured by the model. In this paper we explored
using trees learned in an unsupervised fashion from
the training corpus.
The three methods are all agglomerative cluster-
ing algorithms (Duda et al, 2000) with different
similarity functions. Initially each vocabulary word
is placed in its own cluster; each iteration of the al-
gorithm finds the pair of clusters with highest sim-
ilarity and merges them, continuing until only one
cluster is left. The sequence of merges determines a
binary tree with vocabulary words as its leaves.
Using a heap data structure, this basic agglom-
erative clustering algorithm requires O(n2 log(n) +
sn2) computations where n is the size of the vocab-
ulary and s is the amount of computation needed to
compute the similarity between two clusters. Typi-
cally the vocabulary size n is large; to speed up the
algorithm, we use a greedy version described in Al-
gorithm 1 which restricts the number of cluster can-
didates to at most m ? n. This greedy version is
faster with complexity O(nm(logm + s)). In the
experiments we used m = 500.
Distributional clustering (Dcluster) (Pereira et
al., 1993) measures similarity among words in terms
of the similarity among their local contexts. Each
word is represented by the frequencies of various
words in a window around each occurrence of the
word. The similarity between two words is com-
puted to be a symmetrized KL divergence between
the distributions over neighboring words associated
with the two words. For a cluster of words the neigh-
boring words are the union of those associated with
each word in the cluster. Dcluster has been used
extensively in text classification (Baker and McCal-
lum, 1998).
Probabilistic hierarchical clustering (Pcluster)
176
(Friedman, 2003). Dcluster associates each word
with its local context, as a result it captures both
semantic and syntactic relationships among words.
Pcluster captures more relevant semantic relation-
ships by instead associating each word with the doc-
uments in which it appears. Specifically, each word
is associated with a binary vector indexed by doc-
uments in the corpus, where a 1 means the word
appears in the corresponding document. Pcluster
models a cluster of words probabilistically, with the
binary vectors being iid draws from a product of
Bernoulli distributions. The similarity of two clus-
ters c1 and c2 of words is P (c1 ? c2)/P (c1)P (c2),
i.e. two clusters of words are similar if their union
can be effectively modeled using one cluster, rela-
tive to modeling each separately. Conjugate beta pri-
ors are placed over the parameters of the Bernoulli
distributions and integrated out so that the similarity
scores are comparable.
Brown?s algorithm (Bcluster) (Brown et al,
1990) was originally proposed to build class-based
language models. In the 2-gram case, words are
clustered such that the class of the previous word
is most predictive of the class of the current word.
Thus the similarity between two clusters of words
is defined to be the resulting mutual information be-
tween adjacent classes corrresponding to a sequence
of words.
4.1 Operations to Simplify Trees
Trees constructed using the agglomerative hierarchi-
cal clustering algorithms described in this section
suffer from a few drawbacks. Firstly, because they
are binary trees they have large numbers of internal
nodes. Secondly, many internal nodes are simply not
informative in that the two clusters of words below
a node are indistinguishable. Thirdly, Pcluster and
Dcluster tend to produce long chain-like branches
which significantly slows down the computation of
the relevance score.
To address these issues, we considered operations
to simplify trees by contracting internal edges of the
tree while preserving as much of the word relation-
ship information as possible. Let L be the set of tree
leaves and ?(a) be the distance from node or edge a
to the leaves:
?(a) := min
l?L
#{edges between a and l} (10)
a
b
Figure 2: ?(root) = 2, while ?(v) = 1 for shaded ver-
tices v. Contracting a and b results in both child of b
being direct children of a while b is removed.
In the experiments we considered either contracting
edges3 close to the leaves ?(a) = 1 (thus remov-
ing many of the long branches described above), or
edges further up the tree ?(a) ? 2 (preserving the
informative subtrees closer to the leaves while re-
moving many internal nodes). See Figure 2.
(Miller et al, 2004) cut the BCluster tree at a cer-
tain depth k to simplify the tree, meaning every leaf
descending from a particular internal node at level
k is made an immediate child of that node. They
use the tree to get extra features for a discrimina-
tive model to tackle the problem of sparsity?the
features obtained from the new tree do not suffer
from sparsity since each node has several words as
its leaves. This technique did not work well for our
application so we will not report results using it in
our experiments.
5 Experiments
In this section we present experimental results on
two IR datasets: Cranfield and Medline4. The Cran-
field dataset consists of 1,400 documents and 225
queries; its vocabulary size after stemming and re-
moving stop words is 4,227. The Medline dataset
contains 1,033 documents and 30 queries with the
vocabulary size of 8,800 after stemming and remov-
ing stop words. We compare HDT with the flat
HDD model and Okapi BM25 (Robertson et al,
1992). Since one of our motivations has been to
3Contracting an edge means removing the edge and the adja-
cent child node and connecting the grandchildren to the parent.
4Both datasets can be downloaded from
http://www.dcs.gla.ac.uk/idom/ir resources/test collections.
177
Depth Statistics Performance
Tree Cranfield Medline Cranfield Medline
avg / max total avg / max total avg-pr top10-pr avg-pr top10-pr
BCluster 16.7 / 24 4226 16.4 / 22 8799 0.2675 0.3218 0.2131 0.6433
BC contract ? ? 2 6.2 / 16 3711 5.3 / 14 7473 0.2685 0.3147 0.2079 0.6533
BC contract ? = 1 16.1 / 23 3702 15.8 / 22 7672 0.2685 0.3204 0.1975 0.6400
DCluster 41.2 / 194 4226 38.1 / 176 8799 0.2552 0.3120 0.1906 0.6300
DC contract ? ? 2 2.3 / 8 2469 3.3 / 9 5091 0.2555 0.3156 0.1906 0.6167
DC contract ? = 1 40.9 / 194 3648 38.1 / 176 8799 0.2597 0.3129 0.1848 0.6300
PCluster 50.2 / 345 4226 37.1 / 561 8799 0.2613 0.3231 0.1681 0.6633
PC contract ? ? 2 35.2 / 318 3741 20.4 / 514 7280 0.2624 0.3213 0.1792 0.6767
PC contract ? = 1 33.6 / 345 2246 34.1 / 561 4209 0.2588 0.3240 0.1880 0.6633
flat model 1 / 1 1 1 / 1 1 0.2506 0.3089 0.1381 0.6133
BM25 ? ? ? ? 0.2566 0.3124 0.1804 0.6567
BM25QueryExp ? ? ? ? 0.2097 0.3191 0.2121 0.7366
Table 1: Average precision and Top-10 precision scores of HDT with different trees versus flat model and BM25. The
statistics for each tree shows its average/maximum depth of its leaf nodes as well as the number of its total internal
nodes. The bold numbers highlight the best results in the corresponding columns.
get away from query expansion, we also compare
against Okapi BM25 with query expansion. The
new terms to expand each query are chosen based
on Robertson-Sparck Jones weights (Robertson and
Sparck Jones, 1976) from the pseudo relevant docu-
ments. The comparison criteria are (i) top-10 preci-
sion, and (ii) average precision.
5.1 HDT vs Baselines
All the hierarchical clustering algorithms mentioned
in section 4 are used to generate trees, each of which
is further post-processed by tree simplification op-
erators described in section 4.1. We consider (i)
contracting nodes at higher levels of the hierarchy
(? ? 2), and (ii) contracting nodes right above the
leaves (? = 1).
The statistics of the trees before and after post-
processing are shown in Table 1. Roughly, the
Dcluster and BCluster trees do not have long chains
with leaves hanging directly off them, which is why
their average depths are reduced significantly by the
? ? 2 simplification, but not by the ? = 1 sim-
plification. The converse is true for Pcluster: the
trees have many chains with leaves hanging directly
off them, which is why average depth is not reduced
as much as the previous trees based on the ? ? 2
simplification. However the average depth is still re-
duced significantly compared to the original trees.
Table 1 presents the performance of HDT with
different trees against the baselines in terms of the
top-10 and average precision (we have bold faced
the performance values which are the maximum
of each column). HDT with every tree outper-
forms significantly the flat model in both datasets.
More specifically, HDT with (original) BCluster and
PCluster trees significantly outperforms the three
baselines in terms of both performance measure for
the Cranfield. Similar trends are observed on the
Medline except here the baseline Okapi BM25 with
query expansion is pretty strong5, which is still out-
performed by HDT with BCluster tree.
To further highlight the differences among the
methods, we have shown the precision at particular
recall points on Medline dataset in Figure 4 for HDT
with PCluster tree vs the baselines. As the recall
increases, the precision of the PCluster tree signifi-
cantly outperforms the flat model and BM25. We at-
tribute this to the ability of PCluster tree to give high
scores to documents which have words relevant to a
query word (an effect similar to query expansion).
5.2 Analysis
It is interesting to contrast the learned ?k?s for each
of the clustering methods. These ?k?s impose cor-
5Note that we tuned the parameters of the baselines BM25
with/without query expansion with respect to their performance
on the actual retrieval task, which in a sense makes them appear
better than they should.
178
10?1 100 101 102 103
10?1
100
101
102
103
BCluster
?0k ?parent(k)
?
k
10?2 10?1 100 101 102 103
10?2
10?1
100
101
102
103
DCluster
?0k ?parent(k)
?
k
10?2 10?1 100 101 102 103
10?2
10?1
100
101
102
103
PCluster
?0k ?parent(k)
?
k
Figure 3: The plots showing the contribution of internal nodes in trees constructed by the three clustering algorithms
for the Cranfield dataset. In each plot, a point represent an internal node showing a positive exponent in the node?s
contribution (i.e. positive correlation among its children) if the point is below x = y line. From left to the right plots,
the fraction of nodes below the line is 0.9044, 0.7977, and 0.3344 for a total of 4,226 internal nodes.
0 .1 .2 .3 .4 .5 .6 .7 .8 .9
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision at particular recall points
 
 
PCluster
Flat model
BM25
BM25 Query Expansion
Figure 4: The precision of all methods at particular recall
points for the Medline dataset.
relations on the probabilities of the children under k
in an interesting fashion. In particular, if we com-
pare ?k to ?0k?parent(k), then a larger value of ?k
implies that the probabilities of picking one of the
children of k (from among all nodes) are positively
correlated, while a smaller value of ?k implies neg-
ative correlation. Roughly speaking, this is because
drawn values of ?jl for l ? C(k) are more likely
to be closer to uniform (relative to the flat Dirichlet)
thus if we had picked one child of k we will likely
pick another child of k.
Figure 3 shows scatter plots of ?k values ver-
sus ?0k?parent(k) for the internal nodes of the trees.
Firstly, smaller values for both tend to be associ-
ated with lower levels of the trees, while large val-
ues are with higher levels of the trees. Thus we
see that PCluster tend to have subtrees of vocabu-
lary terms that are positively correlated with each
other?i.e. they tend to co-occur in the same docu-
ments. The converse is true of DCluster and BClus-
ter because they tend to put words with the same
meaning together, thus to express a particular con-
cept it is enough to select one of the words and not
to choose the rest. Figure 5 show some fragments
of the actual trees including the words they placed
together and ?k parameters learned by HDT model
for their internal nodes. Moreover, visual inspection
of the trees shows that DCluster can easily misplace
words in the tree, which explains its lower perfor-
mance compared to the other tree construction meth-
ods.
Secondly, we observed that for higher nodes of
the tree (corresponding generally to larger values of
?k and ?0k?parent(k)) PCluster ?k?s are smaller, thus
higher levels of the tree exhibit negative correlation.
This is reasonable, since if the subtrees capture pos-
itively correlated words, then higher up the tree the
different subtrees correspond to clusters of words
that do not co-occur together, i.e. negatively corre-
lated.
6 Conclusion and Future Work
We presented a hierarchical Dirichlet tree model for
information retrieval which can inject (semantical or
syntactical) word relationships as the domain knowl-
edge into a probabilistic model for information re-
trieval. Using trees to capture word relationships,
the model is highly efficient while making use of
both prior information about words and their occur-
rence statistics in the corpus. Furthermore, we inves-
tigated the effect of different tree construction algo-
rithms on the model performance.
On the Cranfield dataset, HDT achieves 26.85%
for average-precision and 32.40% for top-10 preci-
179
Figure 5: Small parts of the trees learned by clustering algorithms for the Cranfield dataset where the learned ?k for
each internal node is written close to it.
sion, and outperforms all baselines including BM25
which gets 25.66% and 31.24% for these two mea-
sures. On the Medline dataset, HDT is competi-
tive with BM25 with Query Expansion and outper-
forms all other baselines. These encouraging results
show the benefits of HDT as a principled probabilis-
tic model for information retrieval.
An interesting avenue of research is to construct
the vocabulary tree based on WordNet, as a way to
inject independent prior knowledge into the model.
However WordNet has a low coverage problem, i.e.
there are some words in the data which do not ex-
ist in it. One solution to this low coverage problem
is to combine trees generated by the clustering algo-
rithms mentioned in this paper and WordNet, which
we leave as a future work.
References
L. Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In SIGIR ?98: Proceedings of the
21st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 96?103.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1990. Class-based n-gram models
of natural language. Computational Linguistics.
P. J. Cowans. 2004. Information retrieval using hierar-
chical dirichlet processes. In Proceedings of the 27th
Annual International Conference on Research and De-
velopment in Information Retrieval (SIGIR).
R. O. Duda, P. E. Hart, and D. G. Stork. 2000. Pattern
Classification. Wiley-Interscience Publication.
N. Friedman. 2003. Pcluster: Probabilistic agglomera-
tive clustering of gene expression profiles. Available
from http://citeseer.ist.psu.edu/668029.html.
Donald Metzler and W. Bruce Croft. 2007. Latent con-
cept expansion using markov random fields. In Pro-
ceedings of the 30th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative training.
In Proceedings of North American Chapter of the As-
sociation for Computational Linguistics - Human Lan-
guage Technologies conference (NAACL HLT).
180
T. Minka. 2003. The dirichlet-tree distribu-
tion. Available from http://research.microsoft.com/
minka/papers/dirichlet/minka-dirtree.pdf.
J. Nocedal. 1980. Updating quasi-newton matrices with
limited storage. Mathematics of Computation, 35.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In 31st
Annual Meeting of the Association for Computational
Linguistics, pages 183?190.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics.
S. E. Robertson and K. Sparck Jones. 1976. Relevance
weighting of search terms. Journal of the American
Society for Information Science, 27(3):129?146.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at trec. In Text
REtrieval Conference, pages 21?30.
G. Salton and M.J. McGill. 1983. An Introduction to
Modern Information Retrieval. McGraw-Hill, New
York.
181
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 415?423,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Active Learning for Statistical Phrase-based Machine Translation?
Gholamreza Haffari and Maxim Roy and Anoop Sarkar
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
{ghaffar1,maximr,anoop}@cs.sfu.ca
Abstract
Statistical machine translation (SMT) mod-
els need large bilingual corpora for train-
ing, which are unavailable for some language
pairs. This paper provides the first serious ex-
perimental study of active learning for SMT.
We use active learning to improve the qual-
ity of a phrase-based SMT system, and show
significant improvements in translation com-
pared to a random sentence selection baseline,
when test and training data are taken from the
same or different domains. Experimental re-
sults are shown in a simulated setting using
three language pairs, and in a realistic situa-
tion for Bangla-English, a language pair with
limited translation resources.
1 Introduction
Statistical machine translation (SMT) systems have
made great strides in translation quality. However,
high quality translation output is dependent on the
availability of massive amounts of parallel text in
the source and target language. However, there are a
large number of languages that are considered ?low-
density?, either because the population speaking the
language is not very large, or even if millions of peo-
ple speak the language, insufficient amounts of par-
allel text are available in that language.
A statistical translation system can be improved
or adapted by incorporating new training data in the
form of parallel text. In this paper, we propose sev-
eral novel active learning (AL) strategies for statis-
tical machine translation in order to attack this prob-
lem. Conventional techniques for AL of classifiers
are problematic in the SMT setting. Selective sam-
pling of sentences for AL may lead to a parallel cor-
pus where each sentence does not share any phrase
?We would like to thank Chris Callison-Burch for fruitful
discussions. This research was partially supported by NSERC,
Canada (RGPIN: 264905) and by an IBM Faculty Award to the
third author.
pairs with the others. Thus, new sentences cannot
be translated since we lack evidence for how phrase
pairs combine to form novel translations. In this pa-
per, we take the approach of exploration vs. exploita-
tion: where in some cases we pick sentences that
are not entirely novel to improve translation statis-
tics, while also injecting novel translation pairs to
improve coverage.
There may be evidence to show that AL is use-
ful even when we have massive amounts of parallel
training data. (Turchi et al, 2008) presents a com-
prehensive learning curve analysis of a phrase-based
SMT system, and one of the conclusions they draw
is, ?The first obvious approach is an effort to iden-
tify or produce data sets on demand (active learning,
where the learning system can request translations of
specific sentences, to satisfy its information needs).?
Despite the promise of active learning for SMT
there has been very little experimental work pub-
lished on this issue (see Sec. 5). In this paper, we
make several novel contributions to the area of ac-
tive learning for SMT:
? We use a novel framework for AL, which to our
knowledge has not been used in AL experiments be-
fore. We assume a small amount of parallel text and
a large amount of monolingual source language text.
Using these resources, we create a large noisy par-
allel text which we then iteratively improve using
small injections of human translations.
? We provide many useful and novel features use-
ful for AL in SMT. In translation, we can leverage a
whole new set of features that were out of reach for
classification systems: we devise features that look
at the source language, but also devise features that
make an estimate of the potential utility of transla-
tions from the source, e.g. phrase pairs that could be
extracted.
? We show that AL can be useful in domain adapta-
tion. We provide the first experimental evidence in
SMT that active learning can be used to inject care-
415
fully selected translations in order to improve SMT
output in a new domain.
? We compare our proposed features to a random se-
lection baseline in a simulated setting for three lan-
guage pairs. We also use a realistic setting: using hu-
man expert annotations in our AL system we create
an improved SMT system to translate from Bangla
to English, a language pair with very few resources.
2 An Active Learning Framework for SMT
Starting from an SMT model trained initially on
bilingual data, the problem is to minimize the hu-
man effort in translating new sentences which will
be added to the training data to make the retrained
SMT model achieves a certain level of performance.
Thus, given a bitext L := {(fi, ei)} and a mono-
lingual source text U := {fj}, the goal is to select
a subset of highly informative sentences from U to
present to a human expert for translation. Highly in-
formative sentences are those which, together with
their translations, help the retrained SMT system
quickly reach a certain level of translation quality.
This learning scenario is known as active learning
with Selective Sampling (Cohn et al, 1994).
Algorithm 1 describes the experimental setup we
propose for active learning. We train our initial MT
system on the bilingual corpus L, and use it to trans-
late all monolingual sentences in U . We denote sen-
tences in U together with their translations as U+
(line 4 of Algorithm 1). Then we retrain the SMT
system on L?U+ and use the resulting model to de-
code the test set. Afterwards, we select and remove
a subset of highly informative sentences from U ,
and add those sentences together with their human-
provided translations to L. This process is continued
iteratively until a certain level of translation quality,
which in our case is measured by the BLEU score, is
met. In the baseline, against which we compare our
sentence selection methods, the sentences are cho-
sen randomly.
When (re-)training the model, two phrase tables
are learned: one from L and the other one from
U+. The phrase table obtained from U+ is added
as a new feature function in the log-linear trans-
lation model. The alternative is to ignore U+ as
in a conventional AL setting, however, in our ex-
periments we have found that using more bilingual
data, even noisy data, results in better translations.
Algorithm 1 AL-SMT
1: Given bilingual corpus L, and monolingual cor-
pus U .
2: MF?E = train(L, ?)
3: for t = 1, 2, ... do
4: U+ = translate(U,MF?E)
5: Select k sentence pairs from U+, and ask a
human for their true translations.
6: Remove the k sentences from U , and add the
k sentence pairs (translated by human) to L
7: MF?E = train(L,U+)
8: Monitor the performance on the test set T
9: end for
Phrase tables from U+ will get a 0 score in mini-
mum error rate training if they are not useful, so our
method is more general. Also, this method has been
shown empirically to be more effective (Ueffing et
al., 2007b) than (1) using the weighted combination
of the two phrase tables from L and U+, or (2) com-
bining the two sets of data and training from the bi-
text L ? U+.
The setup in Algorithm 1 helps us to investigate
how to maximally take advantage of human effort
(for sentence translation) when learning an SMT
model from the available data, that includes bilin-
gual and monolingual text.
3 Sentence Selection Strategies
Our sentence selection strategies can be divided into
two categories: (1) those which are independent of
the target language and just look into the source lan-
guage, and (2) those which also take into account the
target language. From the description of the meth-
ods, it will be clear to which category they belong to.
We will see in Sec. 4 that the most promising sen-
tence selection strategies belong to the second cate-
gory.
3.1 The Utility of Translation Units
Phrases are basic units of translation in phrase-based
SMT models. The phrases potentially extracted
from a sentence indicate its informativeness. The
more new phrases a sentence can offer, the more
informative it is. Additionally phrase translation
probabilities need to be estimated accurately, which
means sentences that contain rare phrases are also
informative. When selecting new sentences for hu-
416
man translation, we need to pay attention to this
tradeoff between exploration and exploitation, i.e.
selecting sentences to discover new phrases vs es-
timating accurately the phrase translation probabil-
ities. A similar argument can be made that empha-
sizes the importance of words rather than phrases for
any SMT model. Also we should take into account
that smoothing is a means for accurate estimation of
translation probabilities when events are rare. In our
work, we focus on methods that effectively expand
the lexicon or set of phrases of the model.
3.1.1 Phrases (Geom-Phrase, Arith-Phrase)1
The more frequent a phrase is in the unlabeled
data, the more important it is to know its translation;
since it is more likely to occur in the test data (es-
pecially when the test data is in-domain with respect
to unlabeled data). The more frequent a phrase is in
the labeled data, the more unimportant it is; since
probably we have observed most of its translations.
Based on the above observations, we measure the
importance score of a sentence as:
?pg(s) :=
[ ?
x?Xps
P (x|U)
P (x|L)
] 1
|Xps | (1)
where Xps is the set of possible phrases that sentence
s can offer, and P (x|D) is the probability of observ-
ing x in the data D: P (x|D) = Count(x)+?P
x?XpD
Count(x)+? .
The score (1) is the averaged probability ratio of
the set of candidate phrases, i.e. the probability of
the candidate phrases under a probabilistic phrase
model based on U divided by that based on L. In ad-
dition to the geometric average in (1), we may also
consider the arithmetic average score:
?pa(s) := 1|Xps |
?
x?Xps
P (x|U)
P (x|L) (2)
Note that (1) can be re-written as
1
|Xps |
?
x?Xps log P (x|U)P (x|L) in the logarithm space,
which is similar to (2) with the difference of
additional log.
In parallel data L, phrases are the ones which are
extracted by the usual phrase extraction algorithm;
but what are the candidate phrases in the unlabeled
1The names in the parentheses are short names used to iden-
tify the method in the experimental results.
data? Considering the k-best list of translations can
tell us the possible phrases the input sentence may
offer. For each translation, we have access to the
phrases used by the decoder to produce that output.
However, there may be islands of out-of-vocabulary
(OOV) words that were not in the phrase table and
not translated by the decoder as a phrase. We group
together such groups of OOV words to form an OOV
phrase. The set of possible phrases we extract from
the decoder output contain those coming from the
phrase table (from labeled data L) and those coming
from OOVs. OOV phrases are also used in our com-
putation, where P (x | L) for an OOV phrase x is
the uniform probability over all OOV phrases.
3.1.2 n-grams (Geom n-gram, Arith n-gram)
As an alternative to phrases, we consider n-grams
as basic units of generalization. The resulting score
is the weighted combination of the n-gram based
scores:
?Ng (s) :=
N?
n=1
wn
|Xns |
?
x?Xns
log P (x|U, n)P (x|L, n) (3)
where Xns denotes n-grams in the sentence s, and
P (x|D, n) is the probability of x in the set of n-
grams in D. The weights wn adjust the importance
of the scores of n-grams with different lengths. In
addition to taking geometric average, we also con-
sider the arithmetic average:
?Na (s) :=
N?
n=1
wn
|Xns |
?
x?Xns
P (x|U, n)
P (x|L, n) (4)
As a special case when N = 1, the score motivates
selecting sentences which increase the number of
unique words with new words appearing with higher
frequency in U than L.
3.2 Similarity to the Bilingual Training Data
(Similarity)
The simplest way to expand the lexicon set is to
choose sentences from U which are as dissimilar
as possible to L. We measure the similarity using
weighted n-gram coverage (Ueffing et al, 2007b).
3.3 Confidence of Translations (Confidence)
The decoder produces an output translation e using
the probability p(e | f). This probability can be
417
treated as a confidence score for the translation. To
make the confidence score for sentences with dif-
ferent lengths comparable, we normalize using the
sentence length (Ueffing et al, 2007b).
3.4 Feature Combination (Combined)
The idea is to take into account the information from
several simpler methods, e.g. those mentioned in
Sec. 3.1?3.3, when producing the final ranking of
sentences. We can either merge the output rankings
of those simpler models2, or use the scores gener-
ated by them as input features for a higher level
ranking model. We use a linear model:
F (s) = ?
k
?k?k(s) (5)
where ?k are the model parameters, and ?k(.) are
the feature functions from Sections 3.1?3.3, e.g.
confidence score, similarity to L, and score for the
utility of translation units. Using 20K of Spanish
unlabeled text we compared the r2 correlation co-
efficient between each of these scores which, apart
from the arithmetic and geometric versions of the
same score, showed low correlation. And so the in-
formation they provide should be complementary to
each other.
We train the parameters in (5) using two bilingual
development sets dev1 and dev2, the sentences in
dev1 can be ranked with respect to the amount by
which each particular sentence improves the BLEU
score of the retrained3 SMT model on dev2. Having
this ranking, we look for the weight vector which
produces the same ordering of sentences. As an al-
ternative to this method (or its computationally de-
manding generalization in which instead of a single
sentence, several sets of sentences of size k are se-
lected and ranked) we use a hill climbing search on
the surface of dev2?s BLEU score. For a fixed value
of the weight vector, dev1 sentences are ranked and
then the top-k output is selected and the amount
of improvement the retrained SMT system gives on
dev2?s BLEU score is measured. Starting from a
random initial value for ?k?s, we improve one di-
mension at a time and traverse the discrete grid
2To see how different rankings can be combined, see (Re-
ichart et al, 2008) which proposes this for multi-task AL.
3Here the retrained SMT model is the one learned by adding
a particular sentence from dev1 into L.
placed on the values of the weight vector. Starting
with a coarse grid, we make it finer when we get
stuck in local optima during hill climbing.
3.5 Hierarchical Adaptive Sampling (HAS)
(Dasgupta and Hsu, 2008) propose a technique for
sample selection that, under certain settings, is guar-
anteed to be no worse than random sampling. Their
method exploits the cluster structure (if there is any)
in the unlabeled data. Ideally, querying the label
of only one of the data points in a cluster would
be enough to determine the label of the other data
points in that cluster. Their method requires that the
data set is provided in the form of a tree represent-
ing a hierarchical clustering of the data. In AL for
SMT, such a unique clustering of the unlabeled data
would be inappropriate or ad-hoc. For this reason,
we present a new algorithm inspired by the ratio-
nale provided in (Dasgupta and Hsu, 2008) that can
be used in our setting, where we construct a tree-
based partition of the data dynamically4 . This dy-
namic tree construction allows us to extend the HAS
algorithm from classifiers to the SMT task.
The algorithm adaptively samples sentences from
U while building a hierarchical clustering of the sen-
tences in U (see Fig. 1 and Algorithm 2). At any it-
eration, first we retrain the SMT model and translate
all monolingual sentences. At this point one mono-
lingual set of sentences represented by one of the
tree leaves is chosen for further partitioning: a leaf
H is chosen which has the lowest average decoder
confidence score for its sentence translations. We
then rank all sentences in H based on their similar-
ity to L and put the top ?|H| sentences in H1 and
the rest in H2. To select K sentences, we randomly
sample ?K sentences from H1 and (1 ? ?)K sen-
tences from H2 and ask a human for their transla-
tions.
3.6 Reverse Model (Reverse)
While a translation system MF?E is built from lan-
guage F to language E, we also build a translation
system in the reverse direction ME?F . To mea-
sure how informative a monolingual sentence f is,
we translate it to English by MF?E and then project
4The dynamic nature of the hierarchy comes from two fac-
tors: (1) selecting a leaf node for splitting, and (2) splitting a
leaf node based on its similarity to the growing L.
418
Algorithm 2 Hierarchical-Adaptive-Sampling
1: MF?E = train(L, ?)
2: Initialize the tree T by setting its root to U
3: v := root(T )
4: for t = 1, 2, ... do
5: // rank and split sentence in v
X1, X2 := Partition(L, v, ?)
6: // randomly sample and remove sents from Xi
Y1, Y2 := Sampling(X1, X2, ?)
7: // make Xi children of node v in the tree T
T := UpdateTree(X1, X2, v, T )
8: // Y +i has sents in Yi together with human trans
L := L ? Y +1 ? Y +2
9: MF?E = train(L,U)
10: for all leaves l ? T do
11: Z[l] := Average normalized confidence scores
of sentence translations in l
12: end for
13: v := BestLeaf(T, Z)
14: Monitor the performance on the test set
15: end for
H1H2
H22 H21
H := U
Figure 1: Adaptively sampling the sentences while con-
structing a hierarchical clustering of U .
the translation back to French using ME?F . Denote
this reconstructed version of the original French
sentence by f? . Comparing f with f? using BLEU (or
other measures) can tell us how much information
has been lost due to our direct and/or reverse transla-
tion systems. The sentences with higher information
loss are selected for translation by a human.
4 Experiments
The SMT system we applied in our experiments is
PORTAGE (Ueffing et al, 2007a). The models (or
features) which are employed by the decoder are:
(a) one or several phrase table(s), which model the
translation direction p(f | e), (b) one or several
n-gram language model(s) trained with the SRILM
toolkit (Stolcke, 2002); in the experiments reported
here, we used 4-gram models on the NIST data,
and a trigram model on EuroParl, (c) a distortion
corpus language use sentences
EuroParl Fr,Ge,Sp
in-dom L 5K
in-dom U 20K
in-dom dev 2K
in-dom test 2K
See Sec. 4.2 Bangla
in-dom L 11K
in-dom U 20K
in-dom dev 450
in-dom test 1K
Hansards Fr out-dom L 5K
Table 1: Specification of different data sets we will use in
experiments. The target language is English in the bilin-
gual sets, and the source languages are either French (Fr),
German (Ge), Spanish (Sp), or Bangla.
model which assigns a penalty based on the number
of source words which are skipped when generating
a new target phrase, and (d) a word penalty. These
different models are combined log-linearly. Their
weights are optimized w.r.t. BLEU score using the
algorithm described in (Och, 2003). This is done on
a development corpus which we will call dev1 in this
paper.
The weight vectors in n-gram and similarity
methods are set to (.15, .2, .3, .35) to emphasize
longer n-grams. We set ? = ? = .35 for HAS,
and use the 100-best list of translations when identi-
fying candidate phrases while setting the maximum
phrase length to 10. We set ? = .5 to smooth proba-
bilities when computing scores based on translation
units.
4.1 Simulated Low Density Language Pairs
We use three language pairs (French-English,
German-English, Spanish-English) to compare all of
the proposed sentence selection strategies in a simu-
lated AL setting. The training data comes from Eu-
roParl corpus as distributed for the shared task in
the NAACL 2006 workshop on statistical machine
translation (WSMT06). For each language pair, the
first 5K sentences from its bilingual corpus consti-
tute L, and the next 20K sentences serve as U where
the target side translation is ignored. The size of L
was taken to be 5K in order to be close to a real-
istic setting in SMT. We use the first 2K sentences
from the test sets provided for WSMT06, which are
in-domain, as our test sets. The corpus statistics are
summarized in Table 1. The results are shown in
Fig. 2. After building the initial MT systems, we se-
419
0 5 10 15 20 25
0.19
0.195
0.2
0.205
0.21
0.215
0.22
0.225
Added Sentences (multiple of 200)
French to English
 
 
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0 5 10 15 20 25
0.145
0.15
0.155
0.16
0.165
0.17
0.175
Added Sentences (multiple of 200)
German to English
 
 
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0 5 10 15 20 25
0.2
0.205
0.21
0.215
0.22
0.225
0.23
Added Sentences (multiple of 200)
Spanish to English
 
 
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0 5 10 15 20 25
0.19
0.195
0.2
0.205
0.21
0.215
0.22
0.225
Added Sentences (multiple of 200)
French to English
 
 
Geom 4?gram
Geom 1?gram
Similarity
Combined
Random
0 5 10 15 20 25
0.145
0.15
0.155
0.16
0.165
0.17
0.175
Added Sentences (multiple of 200)
German to English
 
 
Geom 4?gram
Geom 1?gram
Similarity
Combined
Random
0 5 10 15 20 25
0.2
0.205
0.21
0.215
0.22
0.225
0.23
Added Sentences (multiple of 200)
Spanish to English
 
 
Geom 4?gram
Geom 1?gram
Similarity
Combined
Random
Figure 2: BLEU scores for different sentence selection strategies per iteration of the AL algorithm. Plots at the top
show the performance of sentence selection methods which depend on the target language in addition to the source
language (hierarchical adaptive sampling, reverse model, decoder confidence, average and geometric phrase-based
score), and plots at the bottom show methods which are independent of the target language (geometric 4-gram and
1-gram, similarity to L, and random sentence selection baseline).
lect and remove 200 sentence from U in each itera-
tion and add them together with translations to L for
25 iterations. Each experiment which involves ran-
domness, such as random sentence selection base-
line and HAS, is averaged over three independent
runs. Selecting sentences based on the phrase-based
utility score outperforms the strong random sentence
selection baseline and other methods (Table 2). De-
coder confidence performs poorly as a criterion for
sentence selection in this setting, and HAS which
is built on top of confidence and similarity scores
outperforms both of them. Although choosing sen-
tences based on their n-gram score ignores the re-
lationship between source and target languages, this
methods outperforms random sentence selection.
4.2 Realistic Low Density Language Pair
We apply active learning to the Bangla-English ma-
chine translation task. Bangla is the official lan-
guage of Bangladesh and second most spoken lan-
guage in India. It has more than 200 million speak-
ers around the world. However, Bangla has few
available language resources, and lacks resources
for machine translation. In our experiments, we use
training data provided by the Linguistic Data Con-
sortium5 containing ?11k sentences. It contains
newswire text from the BBC Asian Network and
some other South Asian news websites. A bilingual
Bangla-English dictionary collected from different
websites was also used as part of the training set
which contains around 85k words. Our monolingual
corpus6 is built by collecting text from the Prothom
Alo newspaper, and contains all the news available
for the year of 2005 ? including magazines and pe-
riodicals. The corpus has 18,067,470 word tokens
and 386,639 word types. For our language model we
used data from the English section of EuroParl. The
5LDC Catalog No.: LDC2008E29.
6Provided by the Center for Research on Bangla Language
Processing, BRAC University, Bangladesh.
420
development set used to optimize the model weights
in the decoder, and test set used for evaluation was
taken from the same LDC corpus mentioned above.
We applied our active learning framework to the
problem of creating a larger Bangla-English parallel
text resource. The second author is a native speaker
of Bangla and participated in the active learning
loop, translating 100 sentences in each iteration. We
compared a smaller number of alternative methods
to keep the annotation cost down. The results are
shown in Fig. 3. Unlike the simulated setting, in this
realistic setting for AL, adding more human transla-
tion does not always result in better translation per-
formance7. Geom 4-gram and Geom phrase are the
features that prove most useful in extracting useful
sentences for the human expert to translate.
4.3 Domain Adaptation
In this section, we investigate the behavior of the
proposed methods when unlabeled data U and test
data T are in-domain and parallel training text L is
out-of-domain.
We report experiments for French to English
translation task where T and development sets are
the same as those in section 4.1 but the bilingual
training data come from Hansards8 corpus. The do-
main is similar to EuroParl, but the vocabulary is
very different. The results are shown in Fig. 4, and
summarized in Table 3. As expected, unigram based
sentence selection performs well in this scenario
since it quickly expands the lexicon set of the bilin-
gual data in an effective manner (Fig 5). By ignor-
7This is likely due to the fact that the translator in the AL
loop was not the same as the original translator for the labeled
data.
8The transcription of official records of the Cana-
dian Parliament as distributed at http://www.isi.edu/natural-
language/download/hansard
Lang. Geom Phrase Random (baseline)
Pair bleu% per% wer% bleu% per% wer%
Fr-En 22.49 27.99 38.45 21.97 28.31 38.80
Gr-En 17.54 31.51 44.28 17.25 31.63 44.41
Sp-En 23.03 28.86 39.17 23.00 28.97 39.21
Table 2: Phrase-based utility selection is compared
with random sentence selection baseline with respect to
BLEU, wer (word error rate), and per (position indepen-
dent word error rate) across three language pairs.
method bleu% per% wer%
Geom 1-gram 14.92 34.83 46.06
Confidence 14.74 35.02 46.11
Random (baseline) 14.11 35.28 46.47
Table 3: Comparison of methods in domain adaptation
scenario. The bold numbers show statistically significant
improvement with respect to the baseline.
ing sentences for which the translations are already
known based on L, it does not waste resources. On
the other hand, it raises the importance of high fre-
quency words in U . Interestingly, decoder confi-
dence is also a good criterion for sentence selection
in this particular case.
5 Related Work
Despite the promise of active learning for SMT
for domain adaptation and low-density/low-resource
languages, there has been very little work published
on this issue. A Ph.D. proposal by Chris Callison-
Burch (Callison-burch, 2003) lays out the promise
of AL for SMT and proposes some algorithms.
However, the lack of experimental results means that
performance and feasibility of those methods can-
not be compared to ours. (Mohit and Hwa, 2007)
provide a technique to classify phrases as difficult
to translate (DTP), and incorporate human transla-
tions for these phrases. Their approach is differ-
ent from AL: they use human translations for DTPs
in order to improve translation output in the de-
coder. There is work on sampling sentence pairs for
SMT (Kauchak, 2006; Eck et al, 2005) but the goal
0 1 2 3 4 5
0.05
0.051
0.052
0.053
0.054
0.055
0.056
0.057
Added Sentences (multiple of 100)
BL
EU
 s
co
re
Bangla to English
 
 
Geom Phrase
HAS
Geom 4?gram
Random
Figure 3: Improving Bangla to English translation perfor-
mance using active learning.
421
0 5 10 15 20 25
0.125
0.13
0.135
0.14
0.145
0.15
Added Sentences (multiple of 200)
BL
EU
 s
co
re
French to English
 
 
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0 5 10 15 20 25
0.125
0.13
0.135
0.14
0.145
0.15
Added Sentences (multiple of 200)
BL
EU
 s
co
re
French to English
 
 
Geom 4?gram
Geom 1?gram
Combined
Random
Figure 4: Performance of different sentence selection
methods for domain adaptation scenario.
has been to limit the amount of training data in order
to reduce the memory footprint of the SMT decoder.
To compute this score, (Eck et al, 2005) use n-gram
features very different from the n-gram features pro-
posed in this paper. (Kato and Barnard, 2007) imple-
ment an AL system for SMT for language pairs with
limited resources (En-Xhosa, En-Zulu, En-Setswana
and En-Afrikaans), but the experiments are on a very
small simulated data set. The only feature used is
the confidence score of the SMT system, which we
showed in our experiments is not a reliable feature.
6 Conclusions
We provided a novel active learning framework for
SMT which utilizes both labeled and unlabeled data.
Several sentence selection strategies were proposed
and comprehensively compared across three simu-
lated language pairs and a realistic setting of Bangla-
English translation with scarce resources. Based
on our experiments, we conclude that paying atten-
tion to units of translations, i.e. words and candi-
date phrases in particular, is essential to sentence se-
Fr2En Ge2En Sp2En Ha2En
Avg # of trans
1.30 1.26 1.27 1.30
1.24 1.25 1.20 1.26
1.22 1.23 1.19 1.24
1.22 1.24 1.19 1.24
Avg phrase len
2.85 2.56 2.85 2.85
3.47 2.74 3.54 3.17
3.95 3.34 3.94 3.48
3.58 2.94 3.63 3.36
# of phrases
27,566 29,297 30,750 27,566
78,026 64,694 93,593 108,787
79,343 63,191 93,276 115,177
77,394 65,198 94,597 115,671
# unique events
31,824 33,141 34,937 31,824
103,124 84,512 125,094 117,214
86,210 69,357 100,176 127,314
84,787 72,280 101,636 128,912
Table 4: Average number of english phrases per source
language phrase, average length of the source language
phrases, number of source language phrases, and number
of phrase pairs which has been seen once in the phrase ta-
bles across three language pairs (French text taken from
Hansard is abbreviated by ?Ha?). From top to bottom
in each row, the numbers belong to: before starting AL,
and after finishing AL based on ?Geom Phrase?, ?Confi-
dence?, and ?Random?.
lection in AL-SMT. Increasing the coverage of the
bilingual training data is important but is not the
only factor (see Table 4 and Fig. 5). For exam-
ple, decoder confidence for sentence selection has
low coverage (in terms of new words), but performs
well in the domain adaptation scenario and performs
poorly otherwise. In future work, we plan to ex-
plore selection methods based on potential phrases,
adaptive sampling using features other than decoder
confidence and the use of features from confidence
estimation in MT (Ueffing and Ney, 2007).
0 5 10 15 20 25
2000
4000
6000
8000
10000
12000
14000
16000
18000
Added Sentences (multiple of 200)
N
um
be
r o
f N
ew
 W
or
ds
                 
French to English
 
 
Geom 4?gram
HAS
Reverse
Confidence
Similarity
Random
Geom 1?gram
Geom Phrase
Figure 5: Number of words in domain adaptation sce-
nario.
422
References
Chris Callison-burch. 2003. Active learning for statisti-
cal machine translation. In PhD Proposal, Edinburgh
University.
David Cohn, Les Atlas, and Richard Ladner. 1994. Im-
proving generalization with active learning. In Ma-
chine Learning Journal.
Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical
sampling for active learning. In proceedings of Inter-
national Conference on Machine Learning.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine translation
based in n-gram frequency and tf-idf. In proceedings
of International Workshop on Spoken Language Trans-
lation (IWSLT).
R.S.M. Kato and E. Barnard. 2007. Statistical transla-
tion with scarce resources: a south african case study.
SAIEE Africa Research Journal, 98(4):136?140, De-
cember.
David Kauchak. 2006. Contribution to research on ma-
chine translation. In PhD Thesis, University of Cali-
fornia at San Diego.
Behrang Mohit and Rebecca Hwa. 2007. Localization
of difficult-to-translate phrases. In proceedings of the
2nd ACL Workshop on Statistical Machine Transla-
tions.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In proceedings of
Annual Meeting of the Association for Computational
Linguistics (ACL).
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-
poport. 2008. Multi-task active learning for linguistic
annotations. In proceedings of Annual Meeting of the
Association for Computational Linguistics (ACL).
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In proceedings of Interna-
tional Conference on Spoken Language Processing
(ICSLP).
Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008.
Learning performance of a machine translation sys-
tem: a statistical and computational analysis. In pro-
ceedings of the Third Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics (ACL).
Nicola Ueffing and Hermann Ney. 2007. Word-level
confidence estimation for machine translation. Com-
putational Linguistics, 33(1):9?40.
N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.
2007a. NRC?s Portage system for WMT 2007. In
Proc. ACL Workshop on SMT.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007b. Transductive learning for statistical machine
translation. In proceedings of Annual Meeting of the
Association for Computational Linguistics (ACL).
423
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 25?32,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Transductive learning for statistical machine translation
Nicola Ueffing
National Research Council Canada
Gatineau, QC, Canada
nicola.ueffing@nrc.gc.ca
Gholamreza Haffari and Anoop Sarkar
Simon Fraser University
Burnaby, BC, Canada
{ghaffar1,anoop}@cs.sfu.ca
Abstract
Statistical machine translation systems are
usually trained on large amounts of bilin-
gual text and monolingual text in the tar-
get language. In this paper we explore the
use of transductive semi-supervised meth-
ods for the effective use of monolingual data
from the source language in order to im-
prove translation quality. We propose sev-
eral algorithms with this aim, and present the
strengths and weaknesses of each one. We
present detailed experimental evaluations on
the French?English EuroParl data set and on
data from the NIST Chinese?English large-
data track. We show a significant improve-
ment in translation quality on both tasks.
1 Introduction
In statistical machine translation (SMT), translation
is modeled as a decision process. The goal is to find
the translation t of source sentence s which maxi-
mizes the posterior probability:
argmax
t
p(t | s) = argmax
t
p(s | t) ? p(t) (1)
This decomposition of the probability yields two dif-
ferent statistical models which can be trained in-
dependently of each other: the translation model
p(s | t) and the target language model p(t).
State-of-the-art SMT systems are trained on large
collections of text which consist of bilingual corpora
(to learn the parameters of p(s | t)), and of monolin-
gual target language corpora (for p(t)). It has been
shown that adding large amounts of target language
text improves translation quality considerably. How-
ever, the availability of monolingual corpora in the
source language does not help improve the system?s
performance. We will show how such corpora can
be used to achieve higher translation quality.
Even if large amounts of bilingual text are given,
the training of the statistical models usually suffers
from sparse data. The number of possible events,
i.e. phrase pairs or pairs of subtrees in the two lan-
guages, is too big to reliably estimate a probabil-
ity distribution over such pairs. Another problem is
that for many language pairs the amount of available
bilingual text is very limited. In this work, we will
address this problem and propose a general frame-
work to solve it. Our hypothesis is that adding infor-
mation from source language text can also provide
improvements. Unlike adding target language text,
this hypothesis is a natural semi-supervised learn-
ing problem. To tackle this problem, we propose
algorithms for transductive semi-supervised learn-
ing. By transductive, we mean that we repeatedly
translate sentences from the development set or test
set and use the generated translations to improve the
performance of the SMT system. Note that the eval-
uation step is still done just once at the end of our
learning process. In this paper, we show that such
an approach can lead to better translations despite
the fact that the development and test data are typi-
cally much smaller in size than typical training data
for SMT systems.
Transductive learning can be seen as a means to
adapt the SMT system to a new type of text. Say a
system trained on newswire is used to translate we-
blog texts. The proposed method adapts the trained
models to the style and domain of the new input.
2 Baseline MT System
The SMT system we applied in our experiments is
PORTAGE. This is a state-of-the-art phrase-based
translation system which has been made available
25
to Canadian universities for research and education
purposes. We provide a basic description here; for a
detailed description see (Ueffing et al, 2007).
The models (or features) which are employed by
the decoder are: (a) one or several phrase table(s),
which model the translation direction p(s | t), (b) one
or several n-gram language model(s) trained with
the SRILM toolkit (Stolcke, 2002); in the experi-
ments reported here, we used 4-gram models on the
NIST data, and a trigram model on EuroParl, (c)
a distortion model which assigns a penalty based
on the number of source words which are skipped
when generating a new target phrase, and (d) a word
penalty. These different models are combined log-
linearly. Their weights are optimized w.r.t. BLEU
score using the algorithm described in (Och, 2003).
This is done on a development corpus which we will
call dev1 in this paper. The search algorithm imple-
mented in the decoder is a dynamic-programming
beam-search algorithm.
After the main decoding step, rescoring with ad-
ditional models is performed. The system generates
a 5,000-best list of alternative translations for each
source sentence. These lists are rescored with the
following models: (a) the different models used in
the decoder which are described above, (b) two dif-
ferent features based on IBM Model 1 (Brown et al,
1993), (c) posterior probabilities for words, phrases,
n-grams, and sentence length (Zens and Ney, 2006;
Ueffing and Ney, 2007), all calculated over the N -
best list and using the sentence probabilities which
the baseline system assigns to the translation hy-
potheses. The weights of these additional models
and of the decoder models are again optimized to
maximize BLEU score. This is performed on a sec-
ond development corpus, dev2.
3 The Framework
3.1 The Algorithm
Our transductive learning algorithm, Algorithm 1,
is inspired by the Yarowsky algorithm (Yarowsky,
1995; Abney, 2004). The algorithm works as fol-
lows: First, the translation model is estimated based
on the sentence pairs in the bilingual training data L.
Then, a set of source language sentences, U , is trans-
lated based on the current model. A subset of good
translations and their sources, Ti, is selected in each
iteration and added to the training data. These se-
lected sentence pairs are replaced in each iteration,
and only the original bilingual training data, L, is
kept fixed throughout the algorithm. The process
of generating sentence pairs, selecting a subset of
good sentence pairs, and updating the model is con-
tinued until a stopping condition is met. Note that
we run this algorithm in a transductive setting which
means that the set of sentences U is drawn either
from a development set or the test set that will be
used eventually to evaluate the SMT system or from
additional data which is relevant to the development
or test set. In Algorithm 1, changing the definition
of Estimate, Score and Select will give us the dif-
ferent semi-supervised learning algorithms we will
discuss in this paper.
Given the probability model p(t | s), consider the
distribution over all possible valid translations t for
a particular input sentence s. We can initialize
this probability distribution to the uniform distribu-
tion for each sentence s in the unlabeled data U .
Thus, this distribution over translations of sentences
from U will have the maximum entropy. Under
certain precise conditions, as described in (Abney,
2004), we can analyze Algorithm 1 as minimizing
the entropy of the distribution over translations of U .
However, this is true only when the functions Esti-
mate, Score and Select have very prescribed defini-
tions. In this paper, rather than analyze the conver-
gence of Algorithm 1 we run it for a fixed number
of iterations and instead focus on finding useful def-
initions for Estimate, Score and Select that can be
experimentally shown to improve MT performance.
3.2 The Estimate Function
We consider the following different definitions for
Estimate in Algorithm 1:
Full Re-training (of all translation models): If
Estimate(L, T ) estimates the model parameters
based on L ? T , then we have a semi-supervised al-
gorithm that re-trains a model on the original train-
ing data L plus the sentences decoded in the last it-
eration. The size of L can be controlled by filtering
the training data (see Section 3.5).
Additional Phrase Table: If, on the other hand, a
new phrase translation table is learned on T only
and then added as a new component in the log-linear
model, we have an alternative to the full re-training
26
Algorithm 1 Transductive learning algorithm for statistical machine translation
1: Input: training set L of parallel sentence pairs. // Bilingual training data.
2: Input: unlabeled set U of source text. // Monolingual source language data.
3: Input: number of iterations R, and size of n-best list N .
4: T?1 := {}. // Additional bilingual training data.
5: i := 0. // Iteration counter.
6: repeat
7: Training step: pi(i) := Estimate(L, Ti?1).
8: Xi := {}. // The set of generated translations for this iteration.
9: for sentence s ? U do
10: Labeling step: Decode s using pi(i) to obtain N best sentence pairs with their scores
11: Xi := Xi ? {(tn, s, pi(i)(tn | s))Nn=1}
12: end for
13: Scoring step: Si := Score(Xi) // Assign a score to sentence pairs (t, s) from X .
14: Selection step: Ti := Select(Xi, Si) // Choose a subset of good sentence pairs (t, s) from X .
15: i := i+ 1.
16: until i > R
of the model on labeled and unlabeled data which
can be very expensive if L is very large (as on the
Chinese?English data set). This additional phrase
table is small and specific to the development or
test set it is trained on. It overlaps with the origi-
nal phrase tables, but also contains many new phrase
pairs (Ueffing, 2006).
Mixture Model: Another alternative for Estimate
is to create a mixture model of the phrase table prob-
abilities with new phrase table probabilities
p(s | t) = ? ? Lp(s | t) + (1? ?) ? Tp(s | t) (2)
where Lp and Tp are phrase table probabilities esti-
mated on L and T , respectively. In cases where new
phrase pairs are learned from T , they get added into
the merged phrase table.
3.3 The Scoring Function
In Algorithm 1, the Score function assigns a score to
each translation hypothesis t. We used the following
scoring functions in our experiments:
Length-normalized Score: Each translated sen-
tence pair (t, s) is scored according to the model
probability p(t | s) normalized by the length |t| of the
target sentence:
Score(t, s) = p(t | s) 1|t| (3)
Confidence Estimation: The confidence estimation
which we implemented follows the approaches sug-
gested in (Blatz et al, 2003; Ueffing and Ney, 2007):
The confidence score of a target sentence t is cal-
culated as a log-linear combination of phrase pos-
terior probabilities, Levenshtein-based word poste-
rior probabilities, and a target language model score.
The weights of the different scores are optimized
w.r.t. classification error rate (CER).
The phrase posterior probabilities are determined
by summing the sentence probabilities of all trans-
lation hypotheses in the N -best list which contain
this phrase pair. The segmentation of the sentence
into phrases is provided by the decoder. This sum
is then normalized by the total probability mass of
the N -best list. To obtain a score for the whole tar-
get sentence, the posterior probabilities of all target
phrases are multiplied. The word posterior proba-
bilities are calculated on basis of the Levenshtein
alignment between the hypothesis under consider-
ation and all other translations contained in the N -
best list. For details, see (Ueffing and Ney, 2007).
Again, the single values are multiplied to obtain a
score for the whole sentence. For NIST, the lan-
guage model score is determined using a 5-gram
model trained on the English Gigaword corpus, and
on French?English, we use the trigram model which
was provided for the NAACL 2006 shared task.
3.4 The Selection Function
The Select function in Algorithm 1 is used to create
the additional training data Ti which will be used in
27
the next iteration i + 1 by Estimate to augment the
original bilingual training data. We use the follow-
ing selection functions:
Importance Sampling: For each sentence s in the
set of unlabeled sentences U , the Labeling step in
Algorithm 1 generates an N -best list of translations,
and the subsequent Scoring step assigns a score for
each translation t in this list. The set of generated
translations for all sentences in U is the event space
and the scores are used to put a probability distri-
bution over this space, simply by renormalizing the
scores described in Section 3.3. We use importance
sampling to select K translations from this distri-
bution. Sampling is done with replacement which
means that the same translation may be chosen sev-
eral times. These K sampled translations and their
associated source sentences make up the additional
training data Ti.
Selection using a Threshold: This method com-
pares the score of each single-best translation to a
threshold. The translation is considered reliable and
added to the set Ti if its score exceeds the thresh-
old. Else it is discarded and not used in the addi-
tional training data. The threshold is optimized on
the development beforehand. Since the scores of the
translations change in each iteration, the size of Ti
also changes.
Keep All: This method does not perform any fil-
tering at all. It is simply assumed that all transla-
tions in the set Xi are reliable, and none of them are
discarded. Thus, in each iteration, the result of the
selection step will be Ti = Xi. This method was
implemented mainly for comparison with other se-
lection methods.
3.5 Filtering the Training Data
In general, having more training data improves the
quality of the trained models. However, when it
comes to the translation of a particular test set, the
question is whether all of the available training data
are relevant to the translation task or not. Moreover,
working with large amounts of training data requires
more computational power. So if we can identify a
subset of training data which are relevant to the cur-
rent task and use only this to re-train the models, we
can reduce computational complexity significantly.
We propose to Filter the training data, either
bilingual or monolingual text, to identify the parts
corpus use sentences
EuroParl phrase table+LM 688K
train100k phrase table 100K
train150k phrase table 150K
dev06 dev1 2,000
test06 test 3,064
Table 1: French?English corpora
corpus use sentences
non-UN phrase table+LM 3.2M
UN phrase table+LM 5.0M
English Gigaword LM 11.7M
multi-p3 dev1 935
multi-p4 dev2 919
eval-04 test 1,788
eval-06 test 3,940
Table 2: NIST Chinese?English corpora
which are relevant w.r.t. the test set. This filtering
is based on n-gram coverage. For a source sentence
s in the training data, its n-gram coverage over the
sentences in the test set is computed. The average
over several n-gram lengths is used as a measure
of relevance of this training sentence w.r.t. the test
corpus. Based on this, we select the top K source
sentences or sentence pairs.
4 Experimental Results
4.1 Setting
We ran experiments on two different corpora: one
is the French?English translation task from the Eu-
roParl corpus, and the other one is Chinese?English
translation as performed in the NIST MT evaluation
(www.nist.gov/speech/tests/mt).
For the French?English translation task, we used
the EuroParl corpus as distributed for the shared task
in the NAACL 2006 workshop on statistical ma-
chine translation. The corpus statistics are shown
in Table 1. Furthermore we filtered the EuroParl
corpus, as explained in Section 3.5, to create two
smaller bilingual corpora (train100k and train150k
in Table 1). The development set is used to optimize
the model weights in the decoder, and the evaluation
is done on the test set provided for the NAACL 2006
shared task.
For the Chinese?English translation task, we used
the corpora distributed for the large-data track in the
28
setting EuroParl NIST
full re-training w/ filtering ? ??
full re-training ?? ?
mixture model ? ?
new phrase table ff:
keep all ?? ?
imp. sampling norm. ?? ?
conf. ?? ?
threshold norm. ?? ?
conf. ?? ?
Table 3: Feasibility of settings for Algorithm 1
2006 NIST evaluation (see Table 2). We used the
LDC segmenter for Chinese. The multiple transla-
tion corpora multi-p3 and multi-p4 were used as de-
velopment corpora. Evaluation was performed on
the 2004 and 2006 test sets. Note that the train-
ing data consists mainly of written text, whereas the
test sets comprise three and four different genres:
editorials, newswire and political speeches in the
2004 test set, and broadcast conversations, broad-
cast news, newsgroups and newswire in the 2006
test set. Most of these domains have characteristics
which are different from those of the training data,
e.g., broadcast conversations have characteristics of
spontaneous speech, and the newsgroup data is com-
paratively unstructured.
Given the particular data sets described above, Ta-
ble 3 shows the various options for the Estimate,
Score and Select functions (see Section 3). The ta-
ble provides a quick guide to the experiments we
present in this paper vs. those we did not attempt due
to computational infeasibility. We ran experiments
corresponding to all entries marked with ? (see Sec-
tion 4.2). For those marked ?? the experiments pro-
duced only minimal improvement over the baseline
and so we do not discuss them in this paper. The en-
tries marked as ? were not attempted because they
are not feasible (e.g. full re-training on the NIST
data). However, these were run on the smaller Eu-
roParl corpus.
Evaluation Metrics
We evaluated the generated translations using
three different evaluation metrics: BLEU score (Pa-
pineni et al, 2002), mWER (multi-reference word
error rate), and mPER (multi-reference position-
independent word error rate) (Nie?en et al, 2000).
Note that BLEU score measures quality, whereas
mWER and mPER measure translation errors. We
will present 95%-confidence intervals for the base-
line system which are calculated using bootstrap re-
sampling. The metrics are calculated w.r.t. one and
four English references: the EuroParl data comes
with one reference, the NIST 2004 evaluation set
and the NIST section of the 2006 evaluation set
are provided with four references each, whereas the
GALE section of the 2006 evaluation set comes
with one reference only. This results in much lower
BLEU scores and higher error rates for the transla-
tions of the GALE set (see Section 4.2). Note that
these values do not indicate lower translation qual-
ity, but are simply a result of using only one refer-
ence.
4.2 Results
EuroParl
We ran our initial experiments on EuroParl to ex-
plore the behavior of the transductive learning algo-
rithm. In all experiments reported in this subsec-
tion, the test set was used as unlabeled data. The
selection and scoring was carried out using impor-
tance sampling with normalized scores. In one set
of experiments, we used the 100K and 150K train-
ing sentences filtered according to n-gram coverage
over the test set. We fully re-trained the phrase ta-
bles on these data and 8,000 test sentence pairs sam-
pled from 20-best lists in each iteration. The results
on the test set can be seen in Figure 1. The BLEU
score increases, although with slight variation, over
the iterations. In total, it increases from 24.1 to 24.4
for the 100K filtered corpus, and from 24.5 to 24.8
for 150K, respectively. Moreover, we see that the
BLEU score of the system using 100K training sen-
tence pairs and transductive learning is the same as
that of the one trained on 150K sentence pairs. So
the information extracted from untranslated test sen-
tences is equivalent to having an additional 50K sen-
tence pairs.
In a second set of experiments, we used the whole
EuroParl corpus and the sampled sentences for fully
re-training the phrase tables in each iteration. We
ran the algorithm for three iterations and the BLEU
score increased from 25.3 to 25.6. Even though this
29
0 2 4 6 8 10 12 14 16 1824.05
24.1
24.15
24.2
24.25
24.3
24.35
24.4
24.45
Iteration
Bleu
 sco
re
0 2 4 6 8 10 12 14 1624.45
24.5
24.55
24.6
24.65
24.7
24.75
24.8
24.85
Iteration
Bleu
 sco
re
Figure 1: Translation quality for importance sampling with full re-training on train100k (left) and train150k
(right). EuroParl French?English task.
is a small increase, it shows that the unlabeled data
contains some information which can be explored in
transductive learning.
In a third experiment, we applied the mixture
model idea as explained in Section 3.2. The initially
learned phrase table was merged with the learned
phrase table in each iteration with a weight of ? =
0.1. This value for ? was found based on cross val-
idation on a development set. We ran the algorithm
for 20 iterations and BLEU score increased from
25.3 to 25.7. Since this is very similar to the re-
sult obtained with the previous method, but with an
additional parameter ? to optimize, we did not use
mixture models on NIST.
Note that the single improvements achieved here
are slightly below the 95%-significance level. How-
ever, we observe them consistently in all settings.
NIST
Table 4 presents translation results on NIST with
different versions of the scoring and selection meth-
ods introduced in Section 3. In these experiments,
the unlabeled data U for Algorithm 1 is the develop-
ment or test corpus. For this corpus U , 5,000-best
lists were generated using the baseline SMT system.
Since re-training the full phrase tables is not feasi-
ble here, a (small) additional phrase table, specific to
U , was trained and plugged into the SMT system as
an additional model. The decoder weights thus had
to be optimized again to determine the appropriate
weight for this new phrase table. This was done on
the dev1 corpus, using the phrase table specific to
dev1. Every time a new corpus is to be translated,
an adapted phrase table is created using transductive
learning and used with the weight which has been
learned on dev1. In the first experiment presented
in Table 4, all of the generated 1-best translations
were kept and used for training the adapted phrase
tables. This method yields slightly higher transla-
tion quality than the baseline system. The second
approach we studied is the use of importance sam-
pling (IS) over 20-best lists, based either on length-
normalized sentence scores (norm.) or confidence
scores (conf.). As the results in Table 4 show, both
variants outperform the first method, with a consis-
tent improvement over the baseline across all test
corpora and evaluation metrics. The third method
uses a threshold-based selection method. Combined
with confidence estimation as scoring method, this
yields the best results. All improvements over the
baseline are significant at the 95%-level.
Table 5 shows the translation quality achieved on
the NIST test sets when additional source language
data from the Chinese Gigaword corpus compris-
ing newswire text is used for transductive learning.
These Chinese sentences were sorted according to
their n-gram overlap (see Section 3.5) with the de-
velopment corpus, and the top 5,000 Chinese sen-
tences were used. The selection and scoring in Al-
gorithm 1 were performed using confidence estima-
tion with a threshold. Again, a new phrase table was
trained on these data. As can be seen in Table 5, this
30
select score BLEU[%] mWER[%] mPER[%]
eval-04 (4 refs.)
baseline 31.8?0.7 66.8?0.7 41.5?0.5
keep all 33.1 66.0 41.3
IS norm. 33.5 65.8 40.9
conf. 33.2 65.6 40.4
thr norm. 33.5 65.9 40.8
conf. 33.5 65.3 40.8
eval-06 GALE (1 ref.)
baseline 12.7?0.5 75.8?0.6 54.6?0.6
keep all 12.9 75.7 55.0
IS norm. 13.2 74.7 54.1
conf. 12.9 74.4 53.5
thr norm. 12.7 75.2 54.2
conf. 13.6 73.4 53.2
eval-06 NIST (4 refs.)
baseline 27.9?0.7 67.2?0.6 44.0?0.5
keep all 28.1 66.5 44.2
IS norm. 28.7 66.1 43.6
conf. 28.4 65.8 43.2
thr norm. 28.3 66.1 43.5
conf. 29.3 65.6 43.2
Table 4: Translation quality using an additional
adapted phrase table trained on the dev/test sets.
Different selection and scoring methods. NIST
Chinese?English, best results printed in boldface.
system outperforms the baseline system on all test
corpora. The error rates are significantly reduced in
all three settings, and BLEU score increases in all
cases. A comparison with Table 4 shows that trans-
ductive learning on the development set and test cor-
pora, adapting the system to their domain and style,
is more effective in improving the SMT system than
the use of additional source language data.
In all experiments on NIST, Algorithm 1 was run
for one iteration. We also investigated the use of an
iterative procedure here, but this did not yield any
improvement in translation quality.
5 Previous Work
Semi-supervised learning has been previously ap-
plied to improve word alignments. In (Callison-
Burch et al, 2004), a generative model for word
alignment is trained using unsupervised learning on
parallel text. In addition, another model is trained on
a small amount of hand-annotated word alignment
data. A mixture model provides a probability for
system BLEU[%] mWER[%] mPER[%]
eval-04 (4 refs.)
baseline 31.8?0.7 66.8?0.7 41.5?0.5
add Chin. data 32.8 65.7 40.9
eval-06 GALE (1 ref.)
baseline 12.7?0.5 75.8?0.6 54.6?0.6
add Chin. data 13.1 73.9 53.5
eval-06 NIST (4 refs.)
baseline 27.9?0.7 67.2?0.6 44.0?0.5
add Chin. data 28.1 65.8 43.2
Table 5: Translation quality using an additional
phrase table trained on monolingual Chinese news
data. Selection step using threshold on confidence
scores. NIST Chinese?English.
word alignment. Experiments showed that putting a
large weight on the model trained on labeled data
performs best. Along similar lines, (Fraser and
Marcu, 2006) combine a generative model of word
alignment with a log-linear discriminative model
trained on a small set of hand aligned sentences. The
word alignments are used to train a standard phrase-
based SMT system, resulting in increased translation
quality .
In (Callison-Burch, 2002) co-training is applied
to MT. This approach requires several source lan-
guages which are sentence-aligned with each other
and all translate into the same target language. One
language pair creates data for another language pair
and can be naturally used in a (Blum and Mitchell,
1998)-style co-training algorithm. Experiments on
the EuroParl corpus show a decrease in WER. How-
ever, the selection algorithm applied there is actually
supervised because it takes the reference translation
into account. Moreover, when the algorithm is run
long enough, large amounts of co-trained data in-
jected too much noise and performance degraded.
Self-training for SMT was proposed in (Ueffing,
2006). An existing SMT system is used to translate
the development or test corpus. Among the gener-
ated machine translations, the reliable ones are au-
tomatically identified using thresholding on confi-
dence scores. The work which we presented here
differs from (Ueffing, 2006) as follows:
? We investigated different ways of scoring and
selecting the reliable translations and compared
our method to this work. In addition to the con-
31
fidence estimation used there, we applied im-
portance sampling and combined it with confi-
dence estimation for transductive learning.
? We studied additional ways of exploring the
newly created bilingual data, namely re-
training the full phrase translation model or cre-
ating a mixture model.
? We proposed an iterative procedure which
translates the monolingual source language
data anew in each iteration and then re-trains
the phrase translation model.
? We showed how additional monolingual
source-language data can be used in transduc-
tive learning to improve the SMT system.
6 Discussion
It is not intuitively clear why the SMT system can
learn something from its own output and is improved
through semi-supervised learning. There are two
main reasons for this improvement: Firstly, the se-
lection step provides important feedback for the sys-
tem. The confidence estimation, for example, dis-
cards translations with low language model scores or
posterior probabilities. The selection step discards
bad machine translations and reinforces phrases of
high quality. As a result, the probabilities of low-
quality phrase pairs, such as noise in the table or
overly confident singletons, degrade. Our experi-
ments comparing the various settings for transduc-
tive learning shows that selection clearly outper-
forms the method which keeps all generated transla-
tions as additional training data. The selection meth-
ods investigated here have been shown to be well-
suited to boost the performance of semi-supervised
learning for SMT.
Secondly, our algorithm constitutes a way of
adapting the SMT system to a new domain or style
without requiring bilingual training or development
data. Those phrases in the existing phrase tables
which are relevant for translating the new data are
reinforced. The probability distribution over the
phrase pairs thus gets more focused on the (reliable)
parts which are relevant for the test data. For an anal-
ysis of the self-trained phrase tables, examples of
translated sentences, and the phrases used in trans-
lation, see (Ueffing, 2006).
References
S. Abney. 2004. Understanding the Yarowsky Algo-
rithm. Comput. Ling., 30(3).
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2003. Confidence estimation for machine transla-
tion. Final report, JHU/CLSP Summer Workshop.
www.clsp.jhu.edu/ws2003/groups/estimate/.
A. Blum and T. Mitchell. 1998. Combining Labeled and
Unlabeled Data with Co-Training. In Proc. Computa-
tional Learning Theory.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Compu-
tational Linguistics, 19(2).
C. Callison-Burch, D. Talbot, and M. Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In Proc. ACL.
C. Callison-Burch. 2002. Co-training for statistical ma-
chine translation. Master?s thesis, School of Informat-
ics, University of Edinburgh.
A. Fraser and D. Marcu. 2006. Semi-supervised training
for statistical word alignment. In Proc. ACL.
S. Nie?en, F. J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evalua-
tion for MT research. In Proc. LREC.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. ACL.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. ICSLP.
N. Ueffing and H. Ney. 2007. Word-level confidence es-
timation for machine translation. Computational Lin-
guistics, 33(1):9?40.
N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.
2007. NRC?s Portage system for WMT 2007. In
Proc. ACL Workshop on SMT.
N. Ueffing. 2006. Using monolingual source-language
data to improve MT performance. In Proc. IWSLT.
D. Yarowsky. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods. In
Proc. ACL.
R. Zens and H. Ney. 2006. N-gram posterior
probabilities for statistical machine translation. In
Proc. HLT/NAACL Workshop on SMT.
32
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 181?189,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Active Learning for Multilingual Statistical Machine Translation?
Gholamreza Haffari and Anoop Sarkar
School of Computing Science, Simon Fraser University
British Columbia, Canada
{ghaffar1,anoop}@cs.sfu.ca
Abstract
Statistical machine translation (SMT)
models require bilingual corpora for train-
ing, and these corpora are often multi-
lingual with parallel text in multiple lan-
guages simultaneously. We introduce an
active learning task of adding a new lan-
guage to an existing multilingual set of
parallel text and constructing high quality
MT systems, from each language in the
collection into this new target language.
We show that adding a new language using
active learning to the EuroParl corpus pro-
vides a significant improvement compared
to a random sentence selection baseline.
We also provide new highly effective sen-
tence selection methods that improve AL
for phrase-based SMT in the multilingual
and single language pair setting.
1 Introduction
The main source of training data for statistical
machine translation (SMT) models is a parallel
corpus. In many cases, the same information is
available in multiple languages simultaneously as
a multilingual parallel corpus, e.g., European Par-
liament (EuroParl) and U.N. proceedings. In this
paper, we consider how to use active learning (AL)
in order to add a new language to such a multilin-
gual parallel corpus and at the same time we con-
struct an MT system from each language in the
original corpus into this new target language. We
introduce a novel combined measure of translation
quality for multiple target language outputs (the
same content from multiple source languages).
The multilingual setting provides new opportu-
nities for AL over and above a single language
pair. This setting is similar to the multi-task AL
scenario (Reichart et al, 2008). In our case, the
multiple tasks are individual machine translation
tasks for several language pairs. The nature of the
translation processes vary from any of the source
?Thanks to James Peltier for systems support for our ex-
periments. This research was partially supported by NSERC,
Canada (RGPIN: 264905) and an IBM Faculty Award.
languages to the new language depending on the
characteristics of each source-target language pair,
hence these tasks are competing for annotating the
same resource. However it may be that in a single
language pair, AL would pick a particular sentence
for annotation, but in a multilingual setting, a dif-
ferent source language might be able to provide a
good translation, thus saving annotation effort. In
this paper, we explore how multiple MT systems
can be used to effectively pick instances that are
more likely to improve training quality.
Active learning is framed as an iterative learn-
ing process. In each iteration new human labeled
instances (manual translations) are added to the
training data based on their expected training qual-
ity. However, if we start with only a small amount
of initial parallel data for the new target language,
then translation quality is very poor and requires
a very large injection of human labeled data to
be effective. To deal with this, we use a novel
framework for active learning: we assume we are
given a small amount of parallel text and a large
amount of monolingual source language text; us-
ing these resources, we create a large noisy par-
allel text which we then iteratively improve using
small injections of human translations. When we
build multiple MT systems from multiple source
languages to the new target language, each MT
system can be seen as a different ?view? on the de-
sired output translation. Thus, we can train our
multiple MT systems using either self-training or
co-training (Blum and Mitchell, 1998). In self-
training each MT system is re-trained using human
labeled data plus its own noisy translation output
on the unlabeled data. In co-training each MT sys-
tem is re-trained using human labeled data plus
noisy translation output from the other MT sys-
tems in the ensemble. We use consensus transla-
tions (He et al, 2008; Rosti et al, 2007; Matusov
et al, 2006) as an effective method for co-training
between multiple MT systems.
This paper makes the following contributions:
? We provide a new framework for multilingual
MT, in which we build multiple MT systems
and add a new language to an existing multi-
lingual parallel corpus. The multilingual set-
181
ting allows new features for active learning
which we exploit to improve translation qual-
ity while reducing annotation effort.
? We introduce new highly effective sentence
selection methods that improve phrase-based
SMT in the multilingual and single language
pair setting.
? We describe a novel co-training based active
learning framework that exploits consensus
translations to effectively select only those
sentences that are difficult to translate for all
MT systems, thus sharing annotation cost.
? We show that using active learning to add
a new language to the EuroParl corpus pro-
vides a significant improvement compared to
the strong random sentence selection base-
line.
2 AL-SMT: Multilingual Setting
Consider a multilingual parallel corpus, such as
EuroParl, which contains parallel sentences for
several languages. Our goal is to add a new lan-
guage to this corpus, and at the same time to con-
struct high quality MT systems from the existing
languages (in the multilingual corpus) to the new
language. This goal is formalized by the following
objective function:
O =
D?
d=1
?d ? TQ(MF d?E) (1)
where F d?s are the source languages in the mul-
tilingual corpus (D is the total number of lan-
guages), and E is the new language. The transla-
tion quality is measured by TQ for individual sys-
temsMF d?E ; it can be BLEU score or WER/PER
(Word error rate and position independent WER)
which induces a maximization or minimization
problem, respectively. The non-negative weights
?d reflect the importance of the different transla-
tion tasks and
?
d ?d = 1. AL-SMT formulation
for single language pair is a special case of this
formulation where only one of the ?d?s in the ob-
jective function (1) is one and the rest are zero.
Moreover the algorithmic framework that we in-
troduce in Sec. 2.1 for AL in the multilingual set-
ting includes the single language pair setting as a
special case (Haffari et al, 2009).
We denote the large unlabeled multilingual cor-
pus by U := {(f1j , .., f
D
j )}, and the small labeled
multilingual corpus by L := {(f1i , .., f
D
i , ei)}. We
overload the term entry to denote a tuple in L or
in U (it should be clear from the context). For a
single language pair we use U and L.
2.1 The Algorithmic Framework
Algorithm 1 represents our AL approach for the
multilingual setting. We train our initial MT sys-
tems {MF d?E}
D
d=1 on the multilingual corpus L,
and use them to translate all monolingual sen-
tences in U. We denote sentences in U together
with their multiple translations by U+ (line 4 of
Algorithm 1). Then we retrain the SMT sys-
tems on L ? U+ and use the resulting model to
decode the test set. Afterwards, we select and
remove a subset of highly informative sentences
from U, and add those sentences together with
their human-provided translations to L. This pro-
cess is continued iteratively until a certain level of
translation quality is met (we use the BLEU score,
WER and PER) (Papineni et al, 2002). In the
baseline, against which we compare our sentence
selection methods, the sentences are chosen ran-
domly.
When (re-)training the models, two phrase ta-
bles are learned for each SMT model: one from
the labeled data L and the other one from pseudo-
labeled data U+ (which we call the main and aux-
iliary phrase tables respectively). (Ueffing et al,
2007; Haffari et al, 2009) show that treating U+
as a source for a new feature function in a log-
linear model for SMT (Och and Ney, 2004) allows
us to maximally take advantage of unlabeled data
by finding a weight for this feature using minimum
error-rate training (MERT) (Och, 2003).
Since each entry in U+ has multiple transla-
tions, there are two options when building the aux-
iliary table for a particular language pair (F d, E):
(i) to use the corresponding translation ed of the
source language in a self-training setting, or (ii) to
use the consensus translation among all the trans-
lation candidates (e1, .., eD) in a co-training set-
ting (sharing information between multiple SMT
models).
A whole range of methods exist in the literature
for combining the output translations of multiple
MT systems for a single language pair, operating
either at the sentence, phrase, or word level (He et
al., 2008; Rosti et al, 2007; Matusov et al, 2006).
The method that we use in this work operates at
the sentence level, and picks a single high qual-
ity translation from the union of the n-best lists
generated by multiple SMT models. Sec. 5 gives
182
Algorithm 1 AL-SMT-Multiple
1: Given multilingual corpora L and U
2: {MF d?E}
D
d=1 = multrain(L, ?)
3: for t = 1, 2, ... do
4: U+ = multranslate(U, {MF d?E}
D
d=1)
5: Select k sentences from U+, and ask a hu-
man for their true translations.
6: Remove the k sentences from U, and add
the k sentence pairs (translated by human)
to L
7: {MF d?E}
D
d=1 = multrain(L,U
+
)
8: Monitor the performance on the test set
9: end for
more details about features which are used in our
consensus finding method, and how it is trained.
Now let us address the important question of se-
lecting highly informative sentences (step 5 in the
Algorithm 1) in the following section.
3 Sentence Selection: Multiple Language
Pairs
The goal is to optimize the objective function
(1) with minimum human effort in providing the
translations. This motivates selecting sentences
which are maximally beneficial for all the MT sys-
tems. In this section, we present several protocols
for sentence selection based on the combined in-
formation from multiple language pairs.
3.1 Alternating Selection
The simplest selection protocol is to choose k sen-
tences (entries) in the first iteration of AL which
improve maximally the first modelMF 1?E , while
ignoring other models. In the second iteration, the
sentences are selected with respect to the second
model, and so on (Reichart et al, 2008).
3.2 Combined Ranking
Pick any AL-SMT scoring method for a single lan-
guage pair (see Sec. 4). Using this method, we
rank the entries in unlabeled data U for each trans-
lation task defined by language pair (F d, E). This
results in several ranking lists, each of which rep-
resents the importance of entries with respect to
a particular translation task. We combine these
rankings using a combined score:
Score
(
(f1, .., fD)
)
=
D?
d=1
?dRankd(f
d
)
Rankd(.) is the ranking of a sentence in the list for
the dth translation task (Reichart et al, 2008).
3.3 Disagreement Among the Translations
Disagreement among the candidate translations of
a particular entry is evidence for the difficulty of
that entry for different translation models. The
reason is that disagreement increases the possibil-
ity that most of the translations are not correct.
Therefore it would be beneficial to ask human for
the translation of these hard entries.
Now the question is how to quantify the no-
tion of disagreement among the candidate trans-
lations (e1, .., eD). We propose two measures of
disagreement which are related to the portion of
shared n-grams (n ? 4) among the translations:
? Let ec be the consensus among all the can-
didate translations, then define the disagree-
ment as
?
d ?d
(
1? BLEU(ec, ed)
)
.
? Based on the disagreement of every pair
of candidate translations:
?
d ?d
?
d?
(
1 ?
BLEU(ed
?
, ed)
)
.
For the single language pair setting, (Haffari et
al., 2009) presents and compares several sentence
selection methods for statistical phrase-based ma-
chine translation. We introduce novel techniques
which outperform those methods in the next sec-
tion.
4 Sentence Selection: Single Language
Pair
Phrases are basic units of translation in phrase-
based SMT models. The phrases which may po-
tentially be extracted from a sentence indicate its
informativeness. The more new phrases a sen-
tence can offer, the more informative it is; since it
boosts the generalization of the model. Addition-
ally phrase translation probabilities need to be es-
timated accurately, which means sentences that of-
fer phrases whose occurrences in the corpus were
rare are informative. When selecting new sen-
tences for human translation, we need to pay atten-
tion to this tradeoff between exploration and ex-
ploitation, i.e. selecting sentences to discover new
phrases v.s. estimating accurately the phrase trans-
lation probabilities. Smoothing techniques partly
handle accurate estimation of translation probabil-
ities when the events occur rarely (indeed it is the
main reason for smoothing). So we mainly focus
on how to expand effectively the lexicon or set of
phrases of the model.
The more frequent a phrase (not a phrase pair)
is in the unlabeled data, the more important it is to
183
know its translation; since it is more likely to see
it in test data (specially when the test data is in-
domain with respect to unlabeled data). The more
frequent a phrase is in the labeled data, the more
unimportant it is; since probably we have observed
most of its translations.
In the labeled dataL, phrases are the ones which
are extracted by the SMT models; but what are
the candidate phrases in the unlabeled data U?
We use the currently trained SMT models to an-
swer this question. Each translation in the n-best
list of translations (generated by the SMT mod-
els) corresponds to a particular segmentation of
a sentence, which breaks that sentence into sev-
eral fragments (see Fig. 1). Some of these frag-
ments are the source language part of a phrase pair
available in the phrase table, which we call regular
phrases and denote their set byXregs for a sentence
s. However, there are some fragments in the sen-
tence which are not covered by the phrase table ?
possibly because of the OOVs (out-of-vocabulary
words) or the constraints imposed by the phrase
extraction algorithm ? called Xoovs for a sentence
s. Each member of Xoovs offers a set of potential
phrases (also referred to as OOV phrases) which
are not observed due to the latent segmentation of
this fragment. We present two generative models
for the phrases and show how to estimate and use
them for sentence selection.
4.1 Model 1
In the first model, the generative story is to gen-
erate phrases for each sentence based on indepen-
dent draws from a multinomial. The sample space
of the multinomial consists of both regular and
OOV phrases.
We build two models, i.e. two multinomials,
one for labeled data and the other one for unla-
beled data. Each model is trained by maximizing
the log-likelihood of its corresponding data:
LD :=
?
s?D
?P (s)
?
x?Xs
logP (x|?D) (2)
where D is either L or U , ?P (s) is the empiri-
cal distribution of the sentences1, and ?D is the
parameter vector of the corresponding probability
1P? (s) is the number of times that the sentence s is seen
in D divided by the number of all sentences in D.
distribution. When x ? Xoovs , we will have
P (x|?U ) =
?
h?Hx
P (x, h|?U )
=
?
h?Hx
P (h)P (x|h,?U )
=
1
|Hx|
?
h?Hx
?
y?Y hx
?U (y) (3)
where Hx is the space of all possible segmenta-
tions for the OOV fragment x, Y hx is the result-
ing phrases from x based on the segmentation h,
and ?U (y) is the probability of the OOV phrase
y in the multinomial associated with U . We let
Hx to be all possible segmentations of the frag-
ment x for which the resulting phrase lengths are
not greater than the maximum length constraint for
phrase extraction in the underlying SMT model.
Since we do not know anything about the segmen-
tations a priori, we have put a uniform distribution
over such segmentations.
Maximizing (2) to find the maximum likelihood
parameters for this model is an extremely diffi-
cult problem2. Therefore, we maximize the fol-
lowing lower-bound on the log-likelihood which
is derived using Jensen?s inequality:
LD ?
?
s?D
?P (s)
[ ?
x?Xregs
log ?D(x)
+
?
x?Xoovs
?
h?Hx
1
|Hx|
?
y?Y hx
log ?D(y)
]
(4)
Maximizing (4) amounts to set the probability of
each regular / potential phrase proportional to its
count / expected count in the data D.
Let ?k(xi:j) be the number of possible segmen-
tations from position i to position j of an OOV
fragment x, and k is the maximum phrase length;
?k(x1:|x|) =
?
??
??
0, if |x| = 0
1, if |x| = 1
?k
i=1 ?k(xi+1:|x|), otherwise
which gives us a dynamic programming algorithm
to compute the number of segmentation |Hx| =
?k(x1:|x|) of the OOV fragment x. The expected
count of a potential phrase y based on an OOV
segment x is (see Fig. 1.c):
E[y|x] =
?
i?j ?[y=xi:j ]?k(x1:i?1)?k(xj+1:|x|)
?k(x)
2Setting partial derivatives of the Lagrangian to zero
amounts to finding the roots of a system of multivariate poly-
nomials (a major topic in Algebraic Geometry).
184
i will go to school on friday
Regular Phrases
OOV segment
go
to
school
go to
to school
2/3
2/3
1/3
1/3
1/3
i will
in friday
XXX
XXX
.01
.004
.
.
.
.
.
.
.
.
.
(a)
potential phr.
source
target prob
count
(b)
(c)
Figure 1: The given sentence in (b) is segmented, based on the source side phrases extracted from the phrase table in (a), to
yield regular phrases and OOV segment. The table in (c) shows the potential phrases extracted from the OOV segment ?go to
school? and their expected counts (denoted by count) where the maximum length for the potential phrases is set to 2. In the
example, ?go to school? has 3 segmentations with maximum phrase length 2: (go)(to school), (go to)(school), (go)(to)(school).
where ?[C] is 1 if the condition C is true, and zero
otherwise. We have used the fact that the num-
ber of occurrences of a phrase spanning the indices
[i, j] is the product of the number of segmentations
of the left and the right sub-fragments, which are
?k(x1:i?1) and ?k(xj+1:|x|) respectively.
4.2 Model 2
In the second model, we consider a mixture model
of two multinomials responsible for generating
phrases in each of the labeled and unlabeled data
sets. To generate a phrase, we first toss a coin and
depending on the outcome we either generate the
phrase from the multinomial associated with regu-
lar phrases ?regU or potential phrases ?
oov
U :
P (x|?U ) := ?U?
reg
U (x) + (1? ?U )?
oov
U (x)
where ?U includes the mixing weight ? and the
parameter vectors of the two multinomials. The
mixture model associated with L is written simi-
larly. The parameter estimation is based on maxi-
mizing a lower-bound on the log-likelihood which
is similar to what was done for the Model 1.
4.3 Sentence Scoring
The sentence score is a linear combination of two
terms: one coming from regular phrases and the
other from OOV phrases:
?1(s) :=
?
|Xregs |
?
x?Xregs
log
P (x|?U )
P (x|?L)
+
1? ?
|Xoovs |
?
x?Xoovs
?
h?Hx
1
|Hx|
log
?
y?Y hx
P (y|?U )
P (y|?L)
where we use either Model 1 or Model 2 for
P (.|?D). The first term is the log probability ra-
tio of regular phrases under phrase models corre-
sponding to unlabeled and labeled data, and the
second term is the expected log probability ratio
(ELPR) under the two models. Another option for
the contribution of OOV phrases is to take log of
expected probability ratio (LEPR):
?2(s) :=
?
|Xregs |
?
x?Xregs
log
P (x|?U )
P (x|?L)
+
1? ?
|Xoovs |
?
x?Xoovs
log
?
h?Hx
1
|Hx|
?
y?Y hx
P (y|?U )
P (y|?L)
It is not difficult to prove that there is no difference
between Model 1 and Model 2 when ELPR scor-
ing is used for sentence selection. However, the
situation is different for LEPR scoring: the two
models produce different sentence rankings in this
case.
5 Experiments
Corpora. We pre-processed the EuroParl corpus
(http://www.statmt.org/europarl) (Koehn, 2005)
and built a multilingual parallel corpus with
653,513 sentences, excluding the Q4/2000 por-
tion of the data (2000-10 to 2000-12) which is
reserved as the test set. We subsampled 5,000
sentences as the labeled data L and 20,000 sen-
tences as U for the pool of untranslated sentences
(while hiding the English part). The test set con-
sists of 2,000 multi-language sentences and comes
from the multilingual parallel corpus built from
Q4/2000 portion of the data.
Consensus Finding. Let T be the union of the n-
best lists of translations for a particular sentence.
The consensus translation tc is
argmax
t?T
w1
LM(t)
|t|
+w2
Qd(t)
|t|
+w3Rd(t)+w4,d
where LM(t) is the score from a 3-gram language
model, Qd(t) is the translation score generated by
the decoder for MF d?E if t is produced by the
dth SMT model, Rd(t) is the rank of the transla-
tion in the n-best list produced by the dth model,
w4,d is a bias term for each translation model to
make their scores comparable, and |t| is the length
185
1000 2000 3000 4000 500022.6
22.7
22.8
22.9
23
23.1
23.2
23.3
23.4
23.5
23.6
Added Sentences
BLE
U Sc
ore
French to English
 
 
Model 2 ? LEPRModel 1 ? ELPRGeom PhraseRandom
1000 2000 3000 4000 500023.2
23.4
23.6
23.8
24
24.2
24.4
24.6
24.8
25
Added Sentences
BLE
U Sc
ore
Spanish to English
 
 
Model 2 ? LEPRModel 1 ? ELPRGeom PhraseRandom
1000 2000 3000 4000 500016.2
16.4
16.6
16.8
17
17.2
17.4
17.6
17.8
Added Sentences
BLE
U Sc
ore
German to English
 
 
Model 2 ? LEPRModel 1 ? ELPRGeom PhraseRandom
Figure 2: The performance of different sentence selection strategies as the iteration of AL loop goes on for three translation
tasks. Plots show the performance of sentence selection methods for single language pair in Sec. 4 compared to the GeomPhrase
(Haffari et al, 2009) and random sentence selection baseline.
of the translation sentence. The number of weights
wi is 3 plus the number of source languages, and
they are trained using minimum error-rate training
(MERT) to maximize the BLEU score (Och, 2003)
on a development set.
Parameters. We use add- smoothing where  =
.5 to smooth the probabilities in Sec. 4; moreover
? = .4 for ELPR and LEPR sentence scoring and
maximum phrase length k is set to 4. For the mul-
tilingual experiments (which involve four source
languages) we set ?d = .25 to make the impor-
tance of individual translation tasks equal.
0 1000 2000 3000 4000 500018
18.5
19
19.5
20
20.5
Added Sentences
Avg 
BLEU
 Sco
re
Mulilingual da?de?nl?sv to en
 
 Self?TrainingCo?Training
Figure 3: Random sentence selection baseline using self-
training and co-training (Germanic languages to English).
5.1 Results
First we evaluate the proposed sentence selection
methods in Sec. 4 for the single language pair.
Then the best method from the single language
pair setting is used to evaluate sentence selection
methods for AL in multilingual setting. After
building the initial MT system for each experi-
ment, we select and remove 500 sentences from
U and add them together with translations to L for
10 total iterations. The random sentence selection
baselines are averaged over 3 independent runs.
mode self-train co-train
Method wer per wer per
Combined Rank 40.2 30.0 40.0 29.6
Alternate 41.0 30.2 40.1 30.1
Disagree-Pairwise 41.9 32.0 40.5 30.9
Disagree-Center 41.8 31.8 40.6 30.7
Random Baseline 41.6 31.0 40.5 30.7
Germanic languages to English
mode self-train co-train
Method wer per wer per
Combined Rank 37.7 27.3 37.3 27.0
Alternate 37.7 27.3 37.3 27.0
Random Baseline 38.6 28.1 38.1 27.6
Romance languages to English
Table 1: Comparison of multilingual selection methods with
WER (word error rate), PER (position independent WER).
95% confidence interval for WER numbers is 0.7 and for PER
numbers is 0.5. Bold: best result, italic: significantly better.
We use three language pairs in our single lan-
guage pair experiments: French-English, German-
English, and Spanish- English. In addition to ran-
dom sentence selection baseline, we also compare
the methods proposed in this paper to the best
method reported in (Haffari et al, 2009) denoted
by GeomPhrase, which differs from our models
since it considers each individual OOV segment as
a single OOV phrase and does not consider subse-
quences. The results are presented in Fig. 2. Se-
lecting sentences based on our proposed methods
outperform the random sentence selection baseline
and GeomPhrase. We suspect for the situations
where L is out-of-domain and the average phrase
length is relatively small, our method will outper-
form GeomPhrase even more.
For the multilingual experiments, we use Ger-
manic (German, Dutch, Danish, Swedish) and Ro-
mance (French, Spanish, Italian, Portuguese3) lan-
3A reviewer pointed out that EuroParl English-Portuguese
186
0 1000 2000 3000 4000 5000
18.2
18.4
18.6
18.8
19
19.2
19.4
19.6
19.8
20
Added Sentences
Avg 
BLE
U Sc
ore
Self?Train Mulilingual da?de?nl?sv to en
 
 
AlternateCombineRankDisagree?PairwiseDisagree?CenterRandom
1000 1500 2000 2500 3000 3500 4000 4500 500019.3
19.4
19.5
19.6
19.7
19.8
19.9
20
20.1
20.2
20.3
Added Sentences
Avg 
BLE
U Sc
ore
Co?Train Mulilingual da?de?nl?sv to en
 
 
AlternateCombineRankDisagree?PairwiseDisagree?CenterRandom
0 1000 2000 3000 4000 500021.6
21.8
22
22.2
22.4
22.6
22.8
23
23.2
23.4
23.6
Added Sentences
Avg 
BLE
U Sc
ore
Self?Train Mulilingual fr?es?it?pt to en
 
 
AlternateCombineRankRandom
1000 1500 2000 2500 3000 3500 4000 4500 500022.6
22.8
23
23.2
23.4
23.6
23.8
Added Sentences
Avg 
BLE
U Sc
ore
Co?Train Mulilingual fr?es?it?pt to en
 
 
AlternateCombineRankRandom
Figure 4: The left/right plot show the performance of our AL methods for multilingual setting combined with self-training/co-
training. The sentence selection methods from Sec. 3 are compared with random sentence selection baseline. The top plots cor-
respond to Danish-German-Dutch-Swedish to English, and the bottom plots correspond to French-Spanish-Italian-Portuguese
to English.
guages as the source and English as the target lan-
guage as two sets of experiments.4 Fig. 3 shows
the performance of random sentence selection for
AL combined with self-training/co-training for the
multi-source translation from the four Germanic
languages to English. It shows that the co-training
mode outperforms the self-training mode by al-
most 1 BLEU point. The results of selection
strategies in the multilingual setting are presented
in Fig. 4 and Tbl. 1. Having noticed that Model
1 with ELPR performs well in the single language
pair setting, we use it to rank entries for individual
translation tasks. Then these rankings are used by
?Alternate? and ?Combined Rank? selection strate-
gies in the multilingual case. The ?Combined
Rank? method outperforms all the other methods
including the strong random selection baseline in
both self-training and co-training modes. The
disagreement-based selection methods underper-
form the baseline for translation of Germanic lan-
guages to English, so we omitted them for the Ro-
mance language experiments.
5.2 Analysis
The basis for our proposed methods has been the
popularity of regular/OOV phrases in U and their
data is very noisy and future work should omit this pair.
4Choice of Germanic and Romance for our experimental
setting is inspired by results in (Cohn and Lapata, 2007)
unpopularity in L, which is measured by P (x|?U )P (x|?L) .
We need P (x|?U ), the estimated distribution of
phrases in U , to be as similar as possible to P ?(x),
the true distribution of phrases in U . We investi-
gate this issue for regular/OOV phrases as follows:
? Using the output of the initially trained MT sys-
tem on L, we extract the regular/OOV phrases as
described in ?4. The smoothed relative frequen-
cies give us the regular/OOV phrasal distributions.
? Using the true English translation of the sen-
tences in U , we extract the true phrases. Separat-
ing the phrases into two sets of regular and OOV
phrases defined by the previous step, we use the
smoothed relative frequencies and form the true
OOV/regular phrasal distributions.
We use the KL-divergence to see how dissim-
ilar are a pair of given probability distributions.
As Tbl. 2 shows, the KL-divergence between the
true and estimated distributions are less than that
De2En Fr2En Es2En
KL(P ?reg ? Preg) 4.37 4.17 4.38
KL(P ?reg ? unif ) 5.37 5.21 5.80
KL(P ?oov ? Poov) 3.04 4.58 4.73
KL(P ?oov ? unif ) 3.41 4.75 4.99
Table 2: For regular/OOV phrases, the KL-divergence be-
tween the true distribution (P ?) and the estimated (P ) or uni-
form (unif ) distributions are shown, where:
KL(P ? ? P ) :=
P
x P
?(x) log P
?(x)
P (x) .
187
100 101 102 103 104 10510
?6
10?5
10?4
10?3
10?2
10?1
100
Rank
Prob
abili
ty
Regular Phrases in U
 
 Estimated DistributionTrue Distribution
100 101 102 103 104 10510
?6
10?5
10?4
10?3
10?2
10?1
100
Rank
Prob
abili
ty
OOV Phrases in U
 
 Estimated DistributionTrue Distribution
Figure 5: The log-log Zipf plots representing the true and
estimated probabilities of a (source) phrase vs the rank of
that phrase in the German to English translation task. The
plots for the Spanish to English and French to English tasks
are also similar to the above plots, and confirm a power law
behavior in the true phrasal distributions.
between the true and uniform distributions, in all
three language pairs. Since uniform distribution
conveys no information, this is evidence that there
is some information encoded in the estimated dis-
tribution about the true distribution. However
we noticed that the true distributions of regu-
lar/OOV phrases exhibit Zipfian (power law) be-
havior5 which is not well captured by the esti-
mated distributions (see Fig. 5). Enhancing the es-
timated distributions to capture this power law be-
havior would improve the quality of the proposed
sentence selection methods.
6 Related Work
(Haffari et al, 2009) provides results for active
learning for MT using a single language pair. Our
work generalizes to the use of multilingual corpora
using new methods that are not possible with a sin-
gle language pair. In this paper, we also introduce
new selection methods that outperform the meth-
ods in (Haffari et al, 2009) even for MT with a
single language pair. In addition in this paper by
considering multilingual parallel corpora we were
able to introduce co-training for AL, while (Haf-
fari et al, 2009) only use self-training since they
are using a single language pair.
5This observation is at the phrase level and not at the word
(Zipf, 1932) or even n-gram level (Ha et al, 2002).
(Reichart et al, 2008) introduces multi-task ac-
tive learning where unlabeled data require annota-
tions for multiple tasks, e.g. they consider named-
entities and parse trees, and showed that multi-
ple tasks helps selection compared to individual
tasks. Our setting is different in that the target lan-
guage is the same across multiple MT tasks, which
we exploit to use consensus translations and co-
training to improve active learning performance.
(Callison-Burch and Osborne, 2003b; Callison-
Burch and Osborne, 2003a) provide a co-training
approach to MT, where one language pair creates
data for another language pair. In contrast, our
co-training approach uses consensus translations
and our setting for active learning is very differ-
ent from their semi-supervised setting. A Ph.D.
proposal by Chris Callison-Burch (Callison-burch,
2003) lays out the promise of AL for SMT and
proposes some algorithms. However, the lack of
experimental results means that performance and
feasibility of those methods cannot be compared
to ours.
While we use consensus translations (He et al,
2008; Rosti et al, 2007; Matusov et al, 2006)
as an effective method for co-training in this pa-
per, unlike consensus for system combination, the
source languages for each of our MT systems are
different, which rules out a set of popular methods
for obtaining consensus translations which assume
translation for a single language pair. Finally, we
briefly note that triangulation (see (Cohn and Lap-
ata, 2007)) is orthogonal to the use of co-training
in our work, since it only enhances each MT sys-
tem in our ensemble by exploiting the multilingual
data. In future work, we plan to incorporate trian-
gulation into our active learning approach.
7 Conclusion
This paper introduced the novel active learning
task of adding a new language to an existing multi-
lingual set of parallel text. We construct SMT sys-
tems from each language in the collection into the
new target language. We show that we can take ad-
vantage of multilingual corpora to decrease anno-
tation effort thanks to the highly effective sentence
selection methods we devised for active learning
in the single language-pair setting which we then
applied to the multilingual sentence selection pro-
tocols. In the multilingual setting, a novel co-
training method for active learning in SMT is pro-
posed using consensus translations which outper-
forms AL-SMT with self-training.
188
References
Avrim Blum and Tom Mitchell. 1998. Combin-
ing Labeled and Unlabeled Data with Co-Training.
In Proceedings of the Eleventh Annual Conference
on Computational Learning Theory (COLT 1998),
Madison, Wisconsin, USA, July 24-26. ACM.
Chris Callison-Burch and Miles Osborne. 2003a.
Bootstrapping parallel corpora. In NAACL work-
shop: Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond.
Chris Callison-Burch and Miles Osborne. 2003b. Co-
training for statistical machine translation. In Pro-
ceedings of the 6th Annual CLUK Research Collo-
quium.
Chris Callison-burch. 2003. Active learning for statis-
tical machine translation. In PhD Proposal, Edin-
burgh University.
Trevor Cohn and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use of
multi-parallel corpora. In ACL.
Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F.J.
Smith. 2002. Extension of zipf?s law to words and
phrases. In Proceedings of the 19th international
conference on Computational linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In NAACL.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In EACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for lin-
guistic annotations. In ACL.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard M. Schwartz, and Bon-
nie Jean Dorr. 2007. Combining outputs from mul-
tiple machine translation systems. In NAACL.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In ACL.
George Zipf. 1932. Selective Studies and the Principle
of Relative Frequency in Language. Harvard Uni-
versity Press.
189
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1937?1941,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Noisy Or-based model for Relation Extraction using Distant Supervision
Ajay Nagesh1,2,3
1IITB-Monash Research Academy
ajaynagesh@cse.iitb.ac.in
Gholamreza Haffari
2Faculty of IT, Monash University
gholamreza.haffari@monash.edu
Ganesh Ramakrishnan
3Dept. of CSE, IIT Bombay
ganesh@cse.iitb.ac.in
Abstract
Distant supervision, a paradigm of rela-
tion extraction where training data is cre-
ated by aligning facts in a database with a
large unannotated corpus, is an attractive
approach for training relation extractors.
Various models are proposed in recent lit-
erature to align the facts in the database
to their mentions in the corpus. In this
paper, we discuss and critically analyse a
popular alignment strategy called the ?at
least one? heuristic. We provide a sim-
ple, yet effective relaxation to this strat-
egy. We formulate the inference proce-
dures in training as integer linear program-
ming (ILP) problems and implement the
relaxation to the ?at least one ? heuris-
tic via a soft constraint in this formulation.
Empirically, we demonstrate that this sim-
ple strategy leads to a better performance
under certain settings over the existing ap-
proaches.
1 Introduction
Although supervised approaches to relation ex-
traction (GuoDong et al., 2005; Surdeanu and Cia-
ramita, 2007) achieve very high accuracies, they
do not scale as they are data intensive and the cost
of creating annotated data is quite high. To alle-
viate this problem, Mintz et al. (2009) proposed
relation extraction in the paradigm of distant su-
pervision. In this approach, given a database of
facts (e.g. Freebase1) and an unannotated docu-
ment collection, the goal is to heuristically align
the facts in the database to the sentences in the
corpus which contain the entities mentioned in the
fact. This is done to create weakly labeled train-
ing data to train a classifier for relation extraction.
The underlying assumption is that all mentions of
1www.freebase.com
an entity pair2 (i.e. sentences containing the en-
tity pair) in the corpus express the same relation
as stated in the database.
The above assumption is a weak one and is
often violated in natural language text. For in-
stance, the entity pair, (Barack Obama, United
States) participate in more than one relation:
citizenOf, presidentOf, bornIn and every men-
tion expresses either one of these fixed set of rela-
tions or none of them.
Consequently, a number of models have been
proposed in literature to provide better heuristics
for the mapping between the entity pair in the
database and its mentions in the sentences of the
corpus. Riedel et al. (2010) tightens the assump-
tion of distant supervision in the following man-
ner: ?Given a pair of entities and their mentions in
sentences from a corpus, at least one of the men-
tions express the relation given in the database?.
In other words, it models the problem as that of
multi-instance (mentions) single-label (relation)
learning. Following this, Hoffmann et al. (2011)
and Surdeanu et al. (2012) propose models that
consider the mapping as that of multi-instance
multi-label learning. The instances are the men-
tions of the entity pair in the sentences of the cor-
pus and the entity pair can participate in more than
one relation.
Although, these models work very well in prac-
tice, they have a number of shortcomings. One
of them is the possibility that during the align-
ment, a fact in the database might not have an in-
stantiation in the corpus. For instance, if our cor-
pus only contains documents from the years 2000
to 2005, the fact presidentOf(Barack Obama,
United States) will not be present in the corpus.
In such cases, the distant supervision assumption
fails to provide a mapping for the fact in the cor-
pus.
In this paper, we address this situation with a
2In this paper we restrict ourselves to binary relations
1937
noisy-or model (Srinivas, 2013) in training the re-
lation extractor by relaxing the ?at least one? as-
sumption discussed above. Our contributions in
this paper are as follows: (i) We formulate the in-
ference procedures in the training algorithm as in-
teger linear programming (ILP) problems, (ii) We
introduce a soft-constraint in the ILP objective to
model noisy-or in training, and (iii) Empirically,
our algorithm performs better than Hoffmann et
al. (2011) procedure under certain settings on two
benchmark datasets.
Our paper is organized as follows. In Section 2,
we discuss our methodology. We review the ap-
proach of Hoffmann et al. (2011) and explain our
modifications to it. In Section 3, we discuss re-
lated work. In Section 4, we discuss the experi-
mental setup and our preliminary results. We con-
clude in Section 4.
2 Methodology
Our work extends the work of Hoffmann et al.
(2011). So, we recapitulate Hoffmann?s model in
the following subsection. Following which our ad-
ditions to this model is explained in detail.
Hoffmann?s model
Hoffmann et al. (2011) present a multi-instance
multi-label model for relation extraction through
distant supervision. In this model, a pair of enti-
ties have multiple mentions (sentence containing
the entity pair) in the corpus. An entity pair can
have one or more relation labels (obtained from
the database).
Objective function
Consider an entity pair (e
1
, e
2
) denoted by the in-
dex i. The set of sentences containing the entity
pair is denoted x
i
and the set of relation labels for
the entity pair from the database is denoted by y
i
.
The mention-level labels are denoted by the latent
variable z (there is one variable z
j
for each sen-
tence j).
To learn the parameters ?, the training objective
to maximize is the likelihood of the facts observed
in the database conditioned on the sentences in the
text corpus.
?
?
= argmax
?
?
i
Pr(y
i
|x
i
; ?)
= argmax
?
?
i
?
z
Pr(y
i
, z|x
i
; ?)
The expression Pr(y
i
, z|x
i
) for a given entity
pair is defined by two types of factors in the factor
graph. They are extract factors for each mention
and mention factors between a relation label and
all the mentions.
The extract factors capture the local signal for
each mention and consists of a bunch of lexical
and syntactic features like POS tags, dependency
path between the entities and so on (Mintz et al.,
2009).
The mention factors capture the dependency be-
tween relation label and its mentions. Here, the at
least one assumption that was discussed in Section
1 is modeled. It is implemented as a simple deter-
ministic OR operator as given below:
f
mention
(y
r
, z) =
{
1 if y
r
is true ??i : z
i
= r
0 otherwise
Training algorithm
The learning algorithm is a perceptron-style
parameter update scheme with 2 modifications:
i) online learning ii) Viterbi approximation. The
inference is shown to reduce to the well-known
weighted edge-cover problem which can be
solved exactly, although Hoffmann et al. (2011)
provide an approximate solution.
Algorithm 1: Hoffmann et al. (2011) : Train-
ing
Input : i) ?: set of sentences, ii) E: set of entities
mentioned in the sentences, iii) R: set of
relation labels, iv) ?: database of facts
Output: Extraction model : ?
begin
for t? 1 to T ; /* training iterations */
do
for i? 1 to N ; /* No. of entity pairs */
do
y?, z?,= argmax
y,z
Pr
(
y, z
??
x
i
; ?
)
if y?! = y
i
then
z
?
= argmax
z
Pr
(
z
??
y
i
,x
i
; ?
)
?
new
= ?
old
+?(x
i
, z
?
)??(x
i
, z?)
end
Our additions to Hoffmann?s model
In the training algorithm described above, there
are two MAP inference procedures. Our con-
tributions in this space is two-fold. Firstly, we
1938
have formulated these as ILP problems. As a re-
sult of this, the approximate inference therein is
replaced by an exact inference procedure. Sec-
ondly, we replace the deterministic-or by a noisy-
or which provides a soft-constraint instead of the
hard-constraint of Hoffmann. (?at least one? as-
sumption)
ILP formulations
Some notations:
 z
ji
: The mention variable z
j
(or jth sen-
tence) taking the relation value i
 s
ji
: Score for z
j
taking the value of i. Scores
are computed from the extract factors
 y
i
: relation label being i
 m : number of mentions (sentences) for the
given entity pair
 R: total number of relation labels (excluding
the nil label)
Deterministic OR
The following is the ILP formulation for the exact
inference argmaxPr(y, z|x
i
) in the model based
on the deterministic-or:
max
Z,Y
{
m?
j=1
?
i?{R,nil}
[
z
ji
s
ji
]}
s.t 1.
?
i?{R,nil}
z
ji
= 1 ?j
2. z
ji
? y
i
?j, ?i
3. y
i
?
m?
j=1
z
ji
?i
where z
ji
? {0, 1}, y
i
? {0, 1}
The first constraint restricts a mention to have
only one label. The second and third constraints
impose the at least one assumption. This is the
same formulation as Hoffmann but expressed as
an ILP problem. However, posing the inference as
an ILP allows us to easily add more constraints to
it.
Noisy OR
As a case-study, we add the noisy-or soft-
constraint in the above objective function. The
idea is to model the situation where a fact is
present in the database but it is not instantiated in
the text. This is a common scenario, as the facts
populated in the database and the text of the corpus
can come from different domains and there might
not be a very good match.
max
Z,Y,?
{(
m?
j=1
?
i?{R,nil}
[
z
ji
s
ji
])
?
(
?
i?R
?
i
)}
s.t 1.
?
i?{R,nil}
z
ji
= 1 ?j
2. z
ji
? y
i
?j, ?i
3. y
i
?
m?
j=1
z
ji
+ ?
i
?i
where z
ji
? {0, 1}, y
i
? {0, 1}, ?
i
? {0, 1}
In the above formulation, the objective function
is augmented with a soft penalty. Also the third
constraint is modified with this penalty term. We
call this new term ?
i
and it is a binary variable to
model noise. Through this term we encourage at
least one type of configuration but will not disal-
low a configuration that does not conform to this.
Essentially, the consequence of this is to allow the
case where a fact is present in the database but is
not instantiated in the text.
3 Related Work
Relation Extraction in the paradigm of distant su-
pervision was introduced by Craven and Kum-
lien (1999). They used a biological database as
the source of distant supervision to discover rela-
tions between biological entities. The progression
of models for information extraction using distant
supervision was presented in Section 1.
Surdeanu et al. (2012) discuss a noisy-or
method for combining the scores of various sen-
tence level models to rank a relation during evalu-
ation. In our approach, we introduce the noisy-or
mechanism in the training phase of the algorithm.
Our work is inspired from previous works
like Roth and tau Yih (2004). The use of ILP
for this problem facilitates easy incorporation of
different constraints and to the best of our knowl-
edge, has not been investigated by the community.
4 Experiments
The experimental runs were carried out using the
publicly available Stanford?s distantly supervised
slot-filling system3 (Surdeanu et al., 2011) and
Hoffmann et al. (2011) code-base4.
3http://nlp.stanford.edu/software/
mimlre.shtml
4http://www.cs.washington.edu/ai/
raphaelh/mr/
1939
Datasets and Evaluation
We report results on two standard datasets used as
benchmarks by the community namely KBP and
Riedel datasets. A complete description of these
datasets is provided in Surdeanu et al. (2012).
The evaluation setup and module is the same
as that described in Surdeanu et al. (2012). We
also use the same set of features used by the var-
ious systems in the package to ensure that the ap-
proaches are comparable. As in previous work, we
report precision/recall (P/R) graphs to evaluate the
various techniques.
We used the publicly available lp solve pack-
age5 to solve our inference problems.
Performance of ILP
Use of ILP raises concerns about performance as
it is NP-hard. In our problem we solve a separate
ILP for every entity pair. The number of variables
is limited by the number of mentions for the given
entity pair. Empirically, on the KBP dataset (larger
of the two datasets), Hoffmann takes around 1hr
to run. Our ILP formulation takes around 8.5hrs.
However, MIMLRE algorithm (EM-based) takes
around 23hrs to converge.
Results
We would primarily like to highlight two settings
on which we report the P/R curves and contrast
it with Hoffmann et al. (2011). Firstly, we re-
place the approximate inference in that work with
our ILP-based exact inference; we call this set-
ting the hoffmann-ilp. Secondly, we replace the
deterministic-or in the model with a noisy-or, and
call this setting the noisy-or. We further compare
our approach with Surdeanu et al. (2012). The
P/R curves for the various techniques on the two
datasets are shown in Figures 1 and 2.
We further report the highest F1 point in the P/R
curve for both the datasets in Tables 1 and 2.
Table 1 : Highest F1 point in P/R curve : KBP Dataset
Precision Recall F1
Hoffmann 0.306451619 0.197916672 0.2405063349
MIMLRE 0.28061223 0.286458343 0.2835051518
Noisy-OR 0.297002733 0.189236104 0.2311770916
Hoffmann-ilp 0.293010741 0.189236104 0.2299577976
Discussion
We would like to discuss the results in the above
two scenarios.
5http://lpsolve.sourceforge.net/5.5/
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
pr
ec
isi
on
recall
hoffmann
noisyOr
hoffmann_ilp
mimlre
Figure 1: Results : KBP dataset
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  0.05  0.1  0.15  0.2  0.25  0.3
pr
ec
isi
on
recall
hoffmann
noisyOr
hoffmann_ilp
mimlre
Figure 2: Results : Riedel dataset
1. Performance of hoffmann-ilp
On the KBP dataset, we observe that
hoffmann-ilp has higher precision in the
range of 0.05 to 0.1 at lower recall (0 to 0.04).
In other parts of the curve it is very close to
the baseline (although hoffmann?s algorithm
is slightly better). In Table 1, we notice that
recall of hoffmann-ilp is lower in comparison
with hoffmann?s algorithm.
On the Riedel dataset, we observe that
hoffmann-ilp has better precision (0.15 to
0.2) than MIMLRE within recall of 0.1.
At recall > 0.1, precision drops drastically.
This is because, hoffmann-ilp predicts signif-
icantly more nil labels. However, nil labels
are not part of the label-set in the P/R curves
reported in the community. In Table 2, we see
that hoffmann-ilp has higher precision (0.04)
compared to Hoffmann?s algorithm.
2. Performance of noisy-or
1940
Table 2 : Highest F1 point in P/R curve : Riedel Dataset
Precision Recall F1
Hoffmann 0.32054795 0.24049332 0.27480916
MIMLRE 0.28061223 0.28645834 0.28350515
Noisy-OR 0.317 0.18139774 0.23075178
Hoffmann-ilp 0.36701337 0.12692702 0.18862161
In Figure 1 we see that there is a big jump
in precision (around 0.4) of noisy-or com-
pared to Hoffmann?s model in most parts of
the curve on the KBP dataset. However, in
Figure 2 (Riedel dataset), we do not see such
a trend. Although, we do perform better than
MIMLRE (Surdeanu et al., 2012) (precision
> 0.15 for recall < 0.15).
On both datasets, noisy-or has higher preci-
sion than MIMLRE, as seen from Tables 1
and 2. However, the recall reduces. More in-
vestigation in this direction is part of future
work.
5 Conclusion
In this paper we described an important addition to
Hoffmann?s model by the use of the noisy-or soft
constraint to further relax the at least one assump-
tion. Since we posed the inference procedures in
Hoffmann using ILP, we could easily add this con-
straint during the training and inference.
Empirically, we showed that the resulting P/R
curves have a significant performance boost over
Hoffmann?s algorithm as a result of this newly
added constraint. Although our system has a lower
recall when compared to MIMLRE (Surdeanu et
al., 2012), it performs competitively w.r.t the pre-
cision at low recall.
As part of immediate future work, we would
like to improve the system recall. Our ILP for-
mulation provides a good framework to add new
type of constraints to the problem. In the future,
we would like to experiment with other constraints
like modeling the selectional preferences of entity
types.
References
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology, pages 77?86. AAAI Press.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, ACL
?05, pages 427?434, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 541?550,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ?09, pages 1003?1011,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Proceedings of the
2010 European conference on Machine learning
and knowledge discovery in databases: Part III,
ECML PKDD?10, pages 148?163, Berlin, Heidel-
berg. Springer-Verlag.
Dan Roth and Wen tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In In Proceedings of CoNLL-2004,
pages 1?8.
Sampath Srinivas. 2013. A generalization of the noisy-
or model. CoRR, abs/1303.1479.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
Proceedings of the NIST 2007 Automatic Content
Extraction Workshop (ACE07), March.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky,
and Christopher D. Manning. 2011. Stanford?s
distantly-supervised slot-filling system. In Proceed-
ings of the Fourth Text Analysis Conference (TAC
2011), Gaithersburg, Maryland, USA, November.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ?12, pages 455?465, Stroudsburg, PA, USA.
Association for Computational Linguistics.
1941
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710?714,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Ensemble Model that Combines Syntactic and Semantic Clustering
for Discriminative Dependency Parsing
Gholamreza Haffari
Faculty of Information Technology
Monash University
Melbourne, Australia
reza@monash.edu
Marzieh Razavi and Anoop Sarkar
School of Computing Science
Simon Fraser University
Vancouver, Canada
{mrazavi,anoop}@cs.sfu.ca
Abstract
We combine multiple word representations
based on semantic clusters extracted from the
(Brown et al, 1992) algorithm and syntac-
tic clusters obtained from the Berkeley parser
(Petrov et al, 2006) in order to improve dis-
criminative dependency parsing in the MST-
Parser framework (McDonald et al, 2005).
We also provide an ensemble method for com-
bining diverse cluster-based models. The two
contributions together significantly improves
unlabeled dependency accuracy from 90.82%
to 92.13%.
1 Introduction
A simple method for using unlabeled data in
discriminative dependency parsing was provided
in (Koo et al, 2008) which involved clustering the
labeled and unlabeled data and then each word in the
dependency treebank was assigned a cluster identi-
fier. These identifiers were used to augment the fea-
ture representation of the edge-factored or second-
order features, and this extended feature set was
used to discriminatively train a dependency parser.
The use of clusters leads to the question of
how to integrate various types of clusters (possibly
from different clustering algorithms) in discrimina-
tive dependency parsing. Clusters obtained from the
(Brown et al, 1992) clustering algorithm are typi-
cally viewed as ?semantic?, e.g. one cluster might
contain plan, letter, request, memo, . . . while an-
other may contain people, customers, employees,
students, . . .. Another clustering view that is more
?syntactic? in nature comes from the use of state-
splitting in PCFGs. For instance, we could ex-
tract a syntactic cluster loss, time, profit, earnings,
performance, rating, . . .: all head words of noun
phrases corresponding to cluster of direct objects of
verbs like improve. In this paper, we obtain syn-
tactic clusters from the Berkeley parser (Petrov et
al., 2006). This paper makes two contributions: 1)
We combine together multiple word representations
based on semantic and syntactic clusters in order to
improve discriminative dependency parsing in the
MSTParser framework (McDonald et al, 2005), and
2) We provide an ensemble method for combining
diverse clustering algorithms that is the discrimina-
tive parsing analog to the generative product of ex-
perts model for parsing described in (Petrov, 2010).
These two contributions combined significantly im-
proves unlabeled dependency accuracy: 90.82% to
92.13% on Sec. 23 of the Penn Treebank, and we
see consistent improvements across all our test sets.
2 Dependency Parsing
A dependency tree represents the syntactic structure
of a sentence with a directed graph (Figure 1), where
nodes correspond to the words, and arcs indicate
head-modifier pairs (Mel?c?uk, 1987). Graph-based
dependency parsing searches for the highest-scoring
tree according to a part-factored scoring function. In
the first-order parsing models, the parts are individ-
ual head-modifier arcs in the dependency tree (Mc-
Donald et al, 2005). In the higher-order models, the
parts consist of arcs together with some context, e.g.
the parent or the sister arcs (McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010). With
a linear scoring function, the parse for a sentence s
is:
PARSE(s) = arg max
t?T (s)
?
r?t
w ? f(s, r) (1)
where T (s) is the space of dependency trees for s,
and f(s, r) is the feature vector for the part r which
is linearly combined using the model parameter w
to give the part score. The above argmax search
for non-projective dependency parsing is accom-
710
root ForIN-1
PP-2
0111
Japan
NNP-19
NP-10
0110
,
,-0
,-0
0010
the
DT-15
DT-15
1101
trend
NN-23
NP-18
1010
improves
VBZ-1
S-14
0101
access
NN-13
NP-24
0011
to
TO-0
TO-0
0011
American
JJ-31
JJ-31
0110
markets
NNS-25
NP-9
1011
Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first
row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is
the first 4 bits (to save space in this figure) of the (Brown et al, 1992) clusters.
plished using minimum spanning tree algorithms
(West, 2001) or approximate inference algorithms
(Smith and Eisner, 2008; Koo et al, 2010). The
(Eisner, 1996) algorithm is typically used for pro-
jective parsing. The model parameters are trained
using a discriminative learning algorithm, e.g. av-
eraged perceptron (Collins, 2002) or MIRA (Cram-
mer and Singer, 2003). In this paper, we work with
both first-order and second-order models, we train
the models using MIRA, and we use the (Eisner,
1996) algorithm for inference.
The baseline features capture information about
the lexical items and their part of speech (POS) tags
(as defined in (McDonald et al, 2005)). In this work,
following (Koo et al, 2008), we use word cluster
identifiers as the source of an additional set of fea-
tures. The reader is directed to (Koo et al, 2008)
for the list of cluster-based feature templates. The
clusters inject long distance syntactic or semantic in-
formation into the model (in contrast with the use
of POS tags in the baseline) and help alleviate the
sparse data problem for complex features that in-
clude n-grams.
3 The Ensemble Model
A word can have different syntactic or semantic
cluster representations, each of which may lead to a
different parsing model. We use ensemble learning
(Dietterich, 2002) in order to combine a collection
of diverse and accurate models into a more powerful
model. In this paper, we construct the base models
based on different syntactic/semantic clusters used
in the features in each model. Our ensemble parsing
model is a linear combination of the base models:
PARSE(s) = arg max
t?T (s)
?
k
?k
?
r?t
wk ? fk(s, r) (2)
where ?k is the weight of the kth base model, and
each base model has its own feature mapping fk(.)
based on its cluster annotation. Each expert pars-
ing model in the ensemble contains all of the base-
line and the cluster-based feature templates; there-
fore, the experts have in common (at least) the base-
line features. The only difference between individ-
ual parsing models is the assigned cluster labels, and
hence some of the cluster-based features. In a fu-
ture work, we plan to take the union of all of the
feature sets and train a joint discriminative parsing
model. The ensemble approach seems more scal-
able though, since we can incrementally add a large
number of clustering algorithms into the ensemble.
4 Syntactic and Semantic Clustering
In our ensemble model we use three different clus-
tering methods to obtain three types of word rep-
resentations that can help alleviate sparse data in a
dependency parser. Our first word representation is
exactly the same as the one used in (Koo et al, 2008)
where words are clustered using the Brown algo-
rithm (Brown et al, 1992). Our two other clusterings
are extracted from the split non-terminals obtained
from the PCFG-based Berkeley parser (Petrov et al,
2006). Split non-terminals from the Berkeley parser
output are converted into cluster identifiers in two
different ways: 1) the split POS tags for each word
are used as an alternate word representation. We
call this representation Syn-Low, and 2) head per-
colation rules are used to label each non-terminal in
the parse such that each non-terminal has a unique
daughter labeled as head. Each word is assigned a
cluster identifier which is defined as the parent split
non-terminal of that word if it is not marked as head,
else if the parent is marked as head we recursively
check its parent until we reach the unique split non-
terminal that is not marked as head. This recursion
terminates at the start symbol TOP. We call this rep-
resentation Syn-High. We only use cluster identi-
fiers from the Berkeley parser, rather than dependen-
cies, or any other information.
711
First order features
Sec Baseline BrownSyn-LowSyn-High Ensemble
00 89.61 90.39 90.01 89.97 90.82
34.68 36.97 34.42 34.94 37.96
01 90.44 91.48 90.89 90.76 91.84
36.36 38.62 35.66 36.56 39.67
23 90.02 91.13 90.46 90.35 91.30
34.13 39.64 36.95 35.00 39.43
24 88.84 90.06 89.44 89.40 90.33
30.85 34.49 32.49 31.22 34.05
Second order features
Sec Baseline BrownSyn-LowSyn-High Ensemble
00 90.34 90.98 90.89 90.59 91.41
38.02 41.04 38.80 39.16 40.93
01 91.48 92.13 91.95 91.72 92.51
41.48 43.84 42.24 41.28 45.05
23 90.82 91.84 91.31 91.21 92.13
39.18 43.66 40.84 39.97 44.28
24 89.87 90.61 90.28 90.31 91.18
35.53 37.99 37.32 35.61 39.55
Table 1: For each test section and model, the number in the
first/second row is the unlabeled-accuracy/unlabeled-complete-
correct. See the text for more explanation.
(TOP
(S-14
(PP-2 (IN-1 For)
(NP-10 (NNP-19 Japan)))
(,-0 ,)
(NP-18 (DT-15 the) (NN-23 trend))
(VP-6 (VBZ-1 improves)
(NP-24 (NN-13 access))
(PP-14 (TO-0 to)
(NP-9 (JJ-31 American)
(NNS-25 markets))))))
For the Berkeley parser output shown above, the
resulting word representations and dependency tree
is shown in Fig. 1. If we group all the head-words in
the training data that project up to split non-terminal
NP-24 then we get a cluster: loss, time, profit, earn-
ings, performance, rating, . . . which are head words
of the noun phrases that appear as direct object of
verbs like improve.
5 Experimental Results
The experiments were done on the English Penn
Treebank, using standard head-percolation rules
(Yamada and Matsumoto, 2003) to convert the
phrase structure into dependency trees. We split the
Treebank into a training set (Sections 2-21), a devel-
Verb Noun Pronoun Adverb Adjective Adpos. Conjunc.
0.0
4
0.0
6
0.0
8
0.1
0
0.1
2
0.1
4
BaselineBrownSyn?LowSyn?HighEnsemble
(a)
1 3 5 7 9 11 13 +15
0.80
0.85
0.90
0.95
Dependency length
Fsc
ore
!
!
!
!
!
!
!
!
!
!
! ! !
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
! ! ! !
!
!
!
!
!
!
!
BaselineBrownSyn?LowSyn?HighEnsemble
(b)
Figure 2: (a) Error rate of the head attachment for different
types of modifier categories. (b) F-score for each dependency
length.
opment set (Section 22), and test sets (Sections 0,
1, 23, and 24). All our experimental settings match
previous work (Yamada and Matsumoto, 2003; Mc-
Donald et al, 2005; Koo et al, 2008). POS tags for
the development and test data were assigned by MX-
POST (Ratnaparkhi, 1996), where the tagger was
trained on the entire training corpus. To generate
part of speech tags for the training data, we used 20-
way jackknifing, i.e. we tagged each fold with the
tagger trained on the other 19 folds. We set model
weights ?k in Eqn (2) to one for all experiments.
Syntactic State-Splitting The sentence-specific
word clusters are derived from the parse trees using
712
Berkeley parser1, which generates phrase-structure
parse trees with split syntactic categories. To gen-
erate parse trees for development and test data, the
parser is trained on the entire training data to learn
a PCFG with latent annotations using split-merge
operations for 5 iterations. To generate parse trees
for the training data, we used 20-way jackknifing as
with the tagger.
Word Clusterings from Brown Algorithm The
word clusters were derived using Percy Liang?s im-
plementation of the (Brown et al, 1992) algorithm
on the BLLIP corpus (Charniak et al, 2000) which
contains ?43M words of Wall Street Journal text.2
This produces a hierarchical clustering over the
words which is then sliced at a certain height to ob-
tain the clusters. In our experiments we use the clus-
ters obtained in (Koo et al, 2008)3, but were unable
to match the accuracy reported there, perhaps due to
additional features used in their implementation not
described in the paper.4
Results Table 1 presents our results for each
model on each test set. In this table, the baseline
(first column) does not use any cluster-based fea-
tures, the next three models use cluster-based fea-
tures using different clustering algorithms, and the
last column is our ensemble model which is the lin-
ear combination of the three cluster-based models.
As Table 1 shows, the ensemble model has out-
performed the baseline and individual models in al-
most all cases. Among the individual models, the
model with Brown semantic clusters clearly outper-
forms the baseline, but the two models with syntac-
tic clusters perform almost the same as the baseline.
The ensemble model outperforms all of the individ-
ual models and does so very consistently across both
first-order and second-order dependency models.
Error Analysis To better understand the contri-
bution of each model to the ensemble, we take a
closer look at the parsing errors for each model and
the ensemble. For each dependent to head depen-
1code.google.com/p/berkeleyparser
2Sentences of the Penn Treebank were excluded from the
text used for the clustering.
3people.csail.mit.edu/maestro/papers/bllip-clusters.gz
4Terry Koo was kind enough to share the source code for the
(Koo et al, 2008) paper with us, and we plan to incorporate all
the features in our future work.
dency, Fig. 2(a) shows the error rate for each depen-
dent grouped by a coarse POS tag (c.f. (McDonald
and Nivre, 2007)). For most POS categories, the
Brown cluster model is the best individual model,
but for Adjectives it is Syn-High, and for Pronouns
it is Syn-Low that is the best. But the ensemble al-
ways does the best in every grammatical category.
Fig. 2(b) shows the F-score of the different models
for various dependency lengths, where the length of
a dependency from word wi to word wj is equal to
|i ? j|. We see that different models are experts on
different lengths (Syn-Low on 8, Syn-High on 9),
while the ensemble model can always combine their
expertise and do better at each length.
6 Comparison to Related Work
Several ensemble models have been proposed for
dependency parsing (Sagae and Lavie, 2006; Hall et
al., 2007; Nivre and McDonald, 2008; Attardi and
Dell?Orletta, 2009; Surdeanu and Manning, 2010).
Essentially, all of these approaches combine dif-
ferent dependency parsing systems, i.e. transition-
based and graph-based. Although graph-based mod-
els are globally trained and can use exact inference
algorithms, their features are defined over a lim-
ited history of parsing decisions. Since transition-
based parsing models have the opposite character-
istics, the idea is to combine these two types of
models to exploit their complementary strengths.
The base parsing models are either independently
trained (Sagae and Lavie, 2006; Hall et al, 2007;
Attardi and Dell?Orletta, 2009; Surdeanu and Man-
ning, 2010), or their training is integrated, e.g. using
stacking (Nivre and McDonald, 2008; Attardi and
Dell?Orletta, 2009; Surdeanu and Manning, 2010).
Our work is distinguished from the aforemen-
tioned works in two dimensions. Firstly, we com-
bine various graph-based models, constructed using
different syntactic/semantic clusters. Secondly, we
do exact inference on the shared hypothesis space of
the base models. This is in contrast to previous work
which combine the best parse trees suggested by the
individual base-models to generate a final parse tree,
i.e. a two-phase inference scheme.
7 Conclusion
We presented an ensemble of different dependency
parsing models, each model corresponding to a dif-
713
ferent syntactic/semantic word clustering annota-
tion. The ensemble obtains consistent improve-
ments in unlabeled dependency parsing, e.g. from
90.82% to 92.13% for Sec. 23 of the Penn Tree-
bank. Our error analysis has revealed that each syn-
tactic/semantic parsing model is an expert in cap-
turing different dependency lengths, and the ensem-
ble model can always combine their expertise and
do better at each dependency length. We can in-
crementally add a large number models using dif-
ferent clustering algorithms, and our preliminary re-
sults show increased improvement in accuracy when
more models are added into the ensemble.
Acknowledgements
This research was partially supported by NSERC,
Canada (RGPIN: 264905). We would like to thank
Terry Koo for his help with the cluster-based fea-
tures for dependency parsing and Ryan McDonald
for the MSTParser source code which we modified
and used for the experiments in this paper.
References
G. Attardi and F. Dell?Orletta. 2009. Reverse revision
and linear tree combination for dependency parsing.
In Proc. of NAACL-HLT.
P. F. Brown, P. V. deSouza, R. L. Mercer, T. J. Watson,
V. J. Della Pietra, and J. C. Lai. 1992. Class-based
n-gram models of natural language. Computational
Linguistics, 18(4).
X. Carreras. 2007. Experiments with a higher-order pro-
jective dependency parser. In Proc. of EMNLP-CoNLL
Shared Task.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.
2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC
No. LDC2000T43, Linguistic Data Consortium.
M. Collins. 2002. Discriminative training methods for
hidden markov models: theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. J. Mach.
Learn. Res., 3:951?991.
T. Dietterich. 2002. Ensemble learning. In The Hand-
book of Brain Theory and Neural Networks, Second
Edition.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In COLING.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? a study in multilingual parser optimization.
In Proc. of CoNLL Shared Task.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. of ACL/HLT.
T. Koo, A. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CONLL.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
I. Mel?c?uk. 1987. Dependency syntax: theory and prac-
tice. State University of New York Press.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. COLING-ACL.
S. Petrov. 2010. Products of random latent variable
grammars. In Proc. of NAACL-HLT.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of NAACL-HLT.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
M. Surdeanu and C. Manning. 2010. Ensemble models
for dependency parsing: Cheap and good? In Proc. of
NAACL.
D. West. 2001. Introduction to Graph Theory. Prentice
Hall, 2nd editoin.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of IWPT.
714
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412?422,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The Haves and the Have-Nots: Leveraging Unlabelled Corpora for
Sentiment Analysis
Kashyap Popat2 Balamurali A R1,2,3 Pushpak Bhattacharyya2 Gholamreza Haffari3
1IITB-Monash Research Academy, IIT Bombay 3Monash University
2Dept. of Computer Science and Engineering, IIT Bombay Australia
{kashyap,balamurali,pb}@cse.iitb.ac.in reza@monash.edu
Abstract
Expensive feature engineering based on
WordNet senses has been shown to
be useful for document level sentiment
classification. A plausible reason for
such a performance improvement is the
reduction in data sparsity. However,
such a reduction could be achieved with
a lesser effort through the means of
syntagma based word clustering. In
this paper, the problem of data sparsity
in sentiment analysis, both monolingual
and cross-lingual, is addressed through
the means of clustering. Experiments
show that cluster based data sparsity
reduction leads to performance better than
sense based classification for sentiment
analysis at document level. Similar idea
is applied to Cross Lingual Sentiment
Analysis (CLSA), and it is shown that
reduction in data sparsity (after translation
or bilingual-mapping) produces accuracy
higher than Machine Translation based
CLSA and sense based CLSA.
1 Introduction
Data sparsity is the bane of Natural Language
Processing (NLP) (Xue et al, 2005; Minkov et al,
2007). Language units encountered in the test data
but absent in the training data severely degrade the
performance of an NLP task. NLP applications
innovatively handle data sparsity through various
means. A special, but very common kind of
data sparsity viz., word sparsity, can be addressed
in one of the two obvious ways: 1) sparsity
reduction through paradigmatically related words
or 2) sparsity reduction through syntagmatically
related words.
Paradigmatic analysis of text is the analysis
of concepts embedded in the text (Cruse, 1986;
Chandler, 2012). WordNet is a byproduct of such
an analysis. In WordNet, paradigms are manually
generated based on the principles of lexical and
semantic relationship among words (Fellbaum,
1998). WordNets are primarily used to address the
problem of word sense disambiguation. However,
at present there are many NLP applications
which use WordNet. One such application is
Sentiment Analysis (SA) (Pang and Lee, 2002).
Recent research has shown that word sense based
semantic features can improve the performance of
SA systems (Rentoumi et al, 2009; Tamara et al,
2010; Balamurali et al, 2011) compared to word
based features.
Syntagmatic analysis of text concentrates on
the surface properties of the text. Compared
to paradigmatic property extraction, syntagmatic
processing is relatively light weight. One of
the obvious syntagmas is words, and words are
grouped into equivalence classes or clusters, thus
reducing the model parameters of a statistical NLP
system (Brown et al, 1992). When used as
an additional feature with word based language
models, it has been shown to improve the system
performance viz., machine translation (Uszkoreit
and Brants, 2008; Stymne, 2012), speech
recognition (Martin et al, 1995; Samuelsson and
Reichl, 1999), dependency parsing (Koo et al,
2008; Haffari et al, 2011; Zhang and Nivre, 2011;
Tratz and Hovy, 2011) and NER (Miller et al,
2004; Faruqui and Pado?, 2010; Turian et al, 2010;
Ta?ckstro?m et al, 2012).
In this paper, the focus is on alleviating the
data sparsity faced by supervised approaches
for SA through the means of cluster based
features. As WordNets are essentially word
412
clusters wherein words with the same meaning
are clubbed together, they address the problem of
data sparsity at word level. The abstraction and
dimensionality reduction thus achieved attributes
to the superior performance for SA systems that
employs WordNet senses as features. However,
WordNets are manually created. Automatic
creation of the same is challenging and not much
successful because of the linguistic complexity
involved. In case of SA, manually creating the
features based on WordNet senses is a tedious and
an expensive process. Moreover, WordNets are
not present for many languages. All these factors
make the paradigmatic property based cluster
features like WordNet senses a less promising
pursuit for SA.
The syntagmatic analysis essentially makes use
of distributional similarity and may in many
circumstances subsume the paradigmatic analysis.
In the current work, this particular insight is
used to solve the data sparsity problem in
the sentiment analysis by leveraging unlabelled
monolingual corpora. Specifically, experiments
are performed to investigate whether features
developed from manually crafted clusterings
(coming from WordNet) can be replaced by those
generated from clustering based on syntagmatic
properties.
Further, cluster based features are used to
address the problem of scarcity of sentiment
annotated data in a language. Popular
approaches for Cross-Lingual Sentiment Analysis
(CLSA) (Wan, 2009; Duh et al, 2011) depend
on Machine Translation (MT) for converting
the labeled data from one language to the
other (Hiroshi et al, 2004; Banea et al, 2008;
Wan, 2009). However, many languages which
are truly resource scarce, do not have an MT
system or existing MT systems are not ripe to
be used for CLSA (Balamurali et al, 2013). To
perform CLSA, this study leverages unlabelled
parallel corpus to generate the word alignments.
These word alignments are then used to link
cluster based features to obliterate the language
gap for performing SA. No MT systems or
bilingual dictionaries are used for this study.
Instead, language gap for performing CLSA is
bridged using linked cluster or cross-lingual
clusters (explained in section 4) with the
help of unlabelled monolingual corpora. The
contributions of this paper are two fold:
1. Features created from manually built and
finer clusters can be replaced by inexpensive
cluster based features generated solely from
unlabelled corpora. Experiments performed
on four publicly available datasets in three
languages viz., English, Hindi and Marathi1
suggest that cluster based features can
considerably boost the performance of an SA
system. Moreover, state of the art result
is obtained for one of the publicly available
dataset.
2. An alternative and effective approach for
CLSA is demonstrated using clusters as
features. Word clustering is a powerful
mechanism to ?transfer? a sentiment
classifier from one language to another. Thus
can be used in truly resource scarce scenarios
like that of English-Marathi CLSA.
The rest of the paper is organized as follows:
section 2 presents related work. Section 3 explains
different word cluster based features employed
to reduce data sparsity for monolingual SA. In
section 4, alternative CLSA approaches based
on word clustering are elucidated. Experimental
details are explained in section 5. Results and
discussions are presented in section 6 and section
7 respectively. Finally, section 8 concludes
the paper pointing to some future research
possibilities.
2 Related Work
The problem of SA at document level is defined
as the classification of document into different
polarity classes (positive and negative) (Turney,
2002). Both supervised (Benamara et al, 2007;
Martineau and Finin, 2009) and unsupervised
approaches (Mei et al, 2007; Lin and He, 2009)
exist for this task.
Supervised approaches are popular because
of their superior classification accuracy (Mullen
and Collier, 2004; Pang and Lee, 2008).
Feature engineering plays an important role
in these systems. Apart from the commonly
used bag-of-words features based on
unigrams/bigrams/ngrams (Dave et al, 2003;
Ng et al, 2006; Martineau and Finin, 2009),
1Hindi and Marathi belong to the Indo-Aryan subgroup
of the Indo-European language family and are two widely
spoken Indian languages with a speaker population of 450
million and 72 million respectively.
413
syntax (Matsumoto et al, 2005; Nakagawa et
al., 2010), semantic (Balamurali et al, 2011)
and negation (Ikeda et al, 2008) have also been
explored for this task. There has been research
related to clustering and sentiment analysis. In
Rooney et al (2011), documents are clustered
based on the context of each document and
sentiment labels are attached at the cluster level.
Zhai et al (2011) attempts to cluster features of a
product to perform sentiment analysis on product
reviews. In this work, word clusters (syntagmatic
and paradigmatic) encoding a mixture of syntactic
and semantic information are used for feature
engineering.
In situations where labeled data is not present
in a language, approaches based on cross-lingual
sentiment analysis are used. Most often these
methods depend on an intermediary machine
translation system (Wan, 2009; Brooke et al,
2009) or a bilingual dictionary (Ghorbel and
Jacot, 2011; Lu et al, 2011) to bridge the
language gap. Given the subtle and different
ways the sentiment can be expressed which itself
manifested as a result of cultural diversity amongst
different languages, an MT system has to be of a
superior quality to capture them.
3 Clustering for Sentiment Analysis
The goal of this paper, to remind the reader, is to
investigate whether superior word cluster features
based on manually crafted and fine grained lexical
resource like WordNet can be replaced with the
syntagmatic property based word clusters created
from unlabelled monolingual corpora.
In this section, different clustering approaches
are presented for feature engineering in a
monolingual setting.
3.1 Approach 1: Clustering based on
WordNet Sense
A synonymous set of words in a WordNet is called
a synset. Each synset can be considered as a word
cluster comprising of semantically similar words.
Balamurali et al (2011) showed that WordNet
synsets can act as good features for document level
sentiment classification.
Motivation for their study stems from the fact
that different senses of a word can have different
polarities. To empirically prove the superiority
of sense based features, different variants of
a travel review domain corpus were generated
by using automatic/manual sense disambiguation
techniques. Thereafter, accuracies of classifiers
based on different sense-based and word-based
features were compared. The results suggested
that WordNet synset based features performed
better than word-based features.
In this study, synset identifiers are extracted
from manually/automatically sense annotated
corpora and used as features for creating sentiment
classifiers. The classifier thus build is used as
a baseline. Apart from this, another baseline
employing word based features are used for a
comprehensive comparison.
3.2 Approach 2: Syntagmatic Property based
Clustering
For this particular study, a co-occurrence based
algorithm is used to create word clusters. As
the algorithm is based on co-occurrence, one
can extract the classes that have the flavour of
syntagmatic grouping, depending on the nature
of underlying statistics. Agglomerative clustering
algorithm by Brown et al (1992) is used for this
purpose. It is a hard clustering algorithm i.e., each
word belongs to one cluster only.
Formally, as mentioned in Brown et al (1992),
let C be a hard clustering function which maps
vocabulary V to one of the K clusters. Then,
the likelihood (L()) of a sequence of word tokens,
w = [wj ]mj=1, with wj ? V , can be factored as,
L(w;C) =
m?
j=1
p(wj|C(wj))p(C(wj)|C(wj?1)))
(1)
Words are assigned to clusters such that the
above quantity is maximized. For the purpose
of sentiment classification, cluster identifiers
representing words in the document are used as
features for training.
4 Clustering for Cross Lingual
Sentiment Analysis
Existing approaches for CLSA depend on an
intermediary machine translation system to bridge
the language gap (Hiroshi et al, 2004; Banea et
al., 2008). Machine translation is very resource
intensive. If a language is truly resource scarce, it
is mostly unlikely to have an MT system. Given
that sentiment analysis is a less resource intensive
task compared to machine translation, the use of
an MT system is hard to justify for performing
414
CLSA. As a viable alternative, cluster linkages
could be learned from a bilingual parallel corpus
and these linkages can be used to bridge the
language gap for CLSA.
In this section, three approaches using clusters
as features for CLSA are compared. The language
whose annotated data is used for training is
called the source language (S), while the language
whose documents are to be sentiment classified is
referred to as the target language (T ).
4.1 Approach 1: Projection based on Sense
(PS)
In this approach, a Multidict is used to bridge the
language gap for SA. A Multidict is an instance
of WordNet where the same sense from different
languages are linked (Mohanty et al, 2008).
An entry in the multidict will have a WordNet
sense identifier from S and the corresponding
WordNet sense identifier from T . The approach
of projection based on sense is explained in
Algorithm 1. Note that after the Sense Mark
operation, each document will be represented as
a vector of WordNet sense identifiers.
Algorithm 1 Projection based on sense
Input: Polarity labeled data in source language
(S) and data in target language (T ) to be
labeled
Output: Classified documents
1: Sense mark the polarity labeled data from S
2: Project the sense marked corpora from S to T
using a Multidict
3: Model the sentiment classifier using the data
obtained in step-2
4: Sense mark the unlabelled data from T
5: Test the sentiment classifier on data obtained
in step-4 using model obtained in step-3
Sense identifiers are the features for the
classifier. For those sense identifiers which do not
have a corresponding entry in the Multidict, no
projection is performed.
4.2 Approach 2: Direct Cluster Linking
(DCL)
Given a parallel bilingual corpus, word clusters in
S can be aligned to clusters in T . Word alignments
are created using parallel corpora. Given two
aligned word sequences wS = [wSj ]mj=1 and
wT = [wTk ]nk=1, let ?T |S be a set of scored
alignments from the source language to the target
language. Here, an alignment from the akth source
word to the kth target word, with score sk,ak > ?
is represented as (wTk , wSak , sk,ak ) ? ?T |S . To
simplify, k ? ?T |S is used to denote those target
words wTk that are aligned to some source word
wSak .
The source and the target side clusters are linked
using the Equation (2).
LC(l) = argmax
t
?
k??T |S ? ?S|T
s.t.CT (wTk )=t
CS (wSak )=l
sk,ak (2)
Here, a target side cluster t ? CT is linked to
a source side cluster l ? CS such that the total
alignment score between words in l and words in
t is maximum. CS and CT stands for source and
target side cluster list respectively. LC(l) gives
the target side cluster t to which l is linked.
4.3 Approach 3: Cross-Lingual Clustering
(XC)
Direct cluster linking approach suffers from the
size of alignment dataset in the form of parallel
corpora. The size of the alignment dataset is
typically smaller than the monolingual dataset.
To circumvent this problem, Ta?ckstro?m et al
(2012) introduced cross-lingual clustering. In
cross-lingual clustering, the objective function
maximizes the joint likelihood of monolingual
and cross-lingual factors. Given a list of
words and clusters it belongs to, a clustering
algorithm tries to obtain word-cluster association
which maximizes the joint likelihood of words
and clusters. Whereas in case of cross-
lingual clustering, the same clustering can be
explained in terms of maximizing the likelihood
of monolingual word-cluster pairs of the source,
the target and alignments between them.
Formally, as stated in Ta?ckstro?m et al (2012),
Using the model of Uszkoreit and Brants (2008),
the likelihood of a sequence of word tokens,
w = [wj ]mj=1, with wj ? V , can be factored as,
L(w;C) =
m?
j=1
p(wj|C(wj))p(C(wj)|wj?1))
(3)
Note this is different from the likelihood
estimation of Brown et al (1992) (Equation (1)),
where C(wj) was conditioned on C(wj?1). This
415
makes the computation easier as suggested in the
original paper. The Equation (3) in a cross lingual
setting will be transformed as given below:
LS,T (wS , wT ;?T |S , ?S|T , CS , CT ) =
LS(...).LT (...).LT |S(...).LS|T (...) (4)
Here, LT |S(...) and LS|T (...) are factors based on
word alignments, which can be represented as:
LT |S(wT ;?T |S , CT , CS) =
?
k??T |S
p(wTk |CT (wTk ))p(CT (wTk )|CS(wSak)))
(5)
Based on the optimization objective in
Equation (4), a pseudo algorithm is defined in
Algorithm 2. For more information, readers are
requested to refer Ta?ckstro?m et al (2012).
Algorithm 2 Cross-lingual Clustering (XC)
Input: Source and target language corpus
Output: Cross-lingual clusters
1: ## CS , CT randomly initialized
2: for i? 1 to N do
3: Find CS? ? argmaxCS LS(wS ;CS)
4: Project CS? to CT
5: Find CT? ? argmaxCT LT (wT ;CT )
6: Project CT? to CS
7: end for
An MT based CLSA approach is used as the
baseline. Training data from S is translated to T
and classification model is learned using unigram
based features. Thereafter, the classifier is directly
tested on data from T .
5 Experimental Setup
Analysis was performed on three languages, viz.,
English (En), Hindi (Hi) and Marathi (Mar).
CLSA was performed on two language
pairs, English-Hindi and English-Marathi.
For clustering the words, monolingual data of
Indian Languages Corpora Initiative (ILCI)2 was
used. It should also be noted that sentiment
annotated data was also included in the data used
for the word clusterings process. For Brown
clustering, an implementation by Liang (2005)
was used. Cross-lingual clustering for CLSA
2http://sanskrit.jnu.ac.in/ilci/index.
jsp
was implemented as directed in Ta?ckstro?m et al
(2012).
Monolingual SA: For experiments in English,
two polarity datasets were used. The first
one (En-TD) by Ye et al (2009) contains user-
written reviews on travel destinations. The
dataset consists of approximately 600 positive
and 591 negative reviews. Reviews were also
manually sense annotated using WordNet 2.1.
The sense annotation was performed by two
annotators with an inter-annotation agreement of
93%. The second dataset (En-PD)3 on product
reviews (music instruments) from Amazon by
Blitzer et al (2007) contains 1000 positive and
1000 negative reviews. This dataset was sense
annotated using an automatic WSD engine which
was trained on tourism domain (Khapra et al,
2010). Experiments using this dataset were
done to study the effect of domain on CLSA.
For experiments in Hindi and Marathi, polarity
datasets by Balamurali et al (2012) were used.4
These are reviews collected from various Hindi
and Marathi blogs and Sunday editorials. Hindi
dataset consist of 98 positive and 100 negative
reviews. Whereas Marathi dataset contains 75
positive and 75 negative reviews. Apart from
being marked with polarity labels at document
level, they are also manually sense annotated using
Hindi and Marathi WordNet respectively.
CLSA: The same datasets used in SA are also
used for CLSA. Three approaches (as described
in section 4) were tested for English-Hindi
and English-Marathi language pairs. To create
alignments, English-Hindi and English-Marathi
parallel corpora from ILCI were used. English-
Hindi parallel corpus contains 45992 sentences
and English-Marathi parallel corpus contains
47881 sentences. To create alignments, GIZA++5
was used (Och and Ney, 2003).
As a preprocessing step, all stop words
were removed. Stemming was performed on
English and Hindi whereas for Marathi data,
Morphological Analyzer was used to reduce the
words to their respective lemmas.
All experiments were performed using C-SVM
3http://www.cs.jhu.edu/
?
mdredze/
datasets/sentiment/
4http://www.cfilt.iitb.ac.
in/resources/senti/MPLC_tour_
downloaderInfo.php
5http://www-i6.informatik.rwth-aachen.
de/Colleagues/och/software/GIZA++.html
416
Features En-TD En-PD Hi Mar
Words 87.02 77.60 77.36 92.28
WordNet Sense (Paradigmatic) 89.13 74.50 85.80 96.88
Clusters (Syntagmatic) 97.45 87.80 83.50 z 98.66
Table 1: Classification accuracy for monolingual sentiment analysis. For English, results are reported on
two publicly available datasets based on Travel Domain (TD) and Product Domain (PD).
Features Words Clust-200 Clust-500 Clust-1000 Clust-1500 Clust-2000 Clust-2500 Clust-3000
En-TD 87.02 97.37 97.45 96.94 96.94 96.52 96.52 96.52
En-PD 77.60 73.20 82.30 84.30 86.35 86.45 87.80 87.40
Table 2: Classification accuracy (in %) versus cluster size (number of clusters to be used).
(linear kernel with parameter optimized over
training set using 5 fold cross validation) available
as a part of LibSVM package6. SVM was used
since it is known to perform well for sentiment
classification (Pang et al, 2002). Results reported
are based on the average of ten-fold cross-
validation accuracies. Standard text metrics are
used for reporting the experimental results.
6 Results
Monolingual classification results are shown in
Table71. Table shows accuracies of SA systems
developed on feature set based on words, senses
and clusters. It must be noted that accuracies
reported for cluster based features are with
respect to the best accuracy based on different
cluster sizes. The improvements in results of
cluster features based approach is found to be
statistically significant over the word features
based approach and sense features based approach
at 95% confidence level when tested using a paired
t-test (except for Hindi cluster features based
approach). But in general, their accuracies do not
significantly vary after cluster size crosses 1500.
Table 2 shows the classification accuracy
variation when cluster size is altered. For,
En-TD and En-PD experiments, the cluster size
was varied between 200-3000 with an interval
of 500 (after a size of 500). In the En-TD
experiment, the best accuracy is achieved for
cluster size 500, which is lesser than the number of
unique-words/unique-senses (6435/6004) present
in the data. Similarly, for the En-PD experiment,
6http://www.csie.ntu.edu.tw/
?
cjlin/
libsvm
7All results reported here are based on 10-fold except for
Marathi (2-fold-5-repeats), as it had comparatively lesser data
samples.
the optimal cluster size of 2500 is also lesser
than the number of unique-words/unique-senses
(30468/4735) present in the data.
To see the effect of training data size variation
for different SA approaches in the En-TD
experiment, the training data size is varied
between 50 to 500. For this, a test set consisting
of 100 positive and 100 negative documents is
fixed. The training data size is varied by selecting
different number of documents from rest of the
dataset (?500 negative and ?500 positive) as a
training set. For each training data set 10 repeats
are performed, e.g., for training data size of 50, 50
negative and 50 positive documents are randomly
selected from the training data pool of ?500
negative and ?500 positive. This was repeated
10 times (with replacement). The results of this
experiment are presented in Figure 1.
 70
 75
 80
 85
 90
 95
 100
 0  100  200  300  400  500
Ac
cu
ra
cy
(%
)
Training data size
Words
Senses (Paradigmatic)
Clusters (Syntagmatic)
Figure 1: Training data variation on En-TD
dataset.
Cross-lingual SA accuracies are presented in
Table 3. As in monolingual case, the reported
accuracies are for features based on the best
cluster size.
417
Target Language MT PS DCL XC
T=Hi 63.13 53.80 51.51 66.16
T=Mar NA 54.00 56.00 60.30
Table 3: Cross-Lingual SA accuracy (%) on T=Hi and T=Mar with S=En for different approaches
(MT=Machine Translation, PS=Projection based on Sense, DCL=Direct Cluster Linking , XC=Cross-
Lingual Clustering. There is no MT system available for (S=En, T=Mar).
7 Discussions
In this section, some important observations from
the results are discussed.
1. Syntagmatic analysis may be used in lieu
of paradigmatic analysis for SA: The results
suggest that word cluster based features using
syntagmatic analysis is comparatively better than
cluster (sense) based features using paradigmatic
analysis. For two datasets in English and for
the one in Marathi this holds true. For English,
the gap between classification accuracy based on
sense features and cluster features is around 10%.
A state-of-art accuracy is obtained for the public
dataset on travel domain (En-TD).
The difference in accuracy reduces as the
language gets morphologically rich. In a
morphologically rich language, morphology
encompasses syntactical information, limiting the
context it can provide for clustering. This can be
seen from the classification results on Marathi.
However for Hindi, classifier built on features
based on syntagmatic analysis trails the one based
on paradigmatic analysis.
Compared to Marathi, Hindi is a less
morphologically rich language, hence, a better
result was expected. However, a contrary result
was obtained.z In Hindi, the subject and the
object of the sentence are linked using a case
marker. Upon error analysis, it was found that
there was a lot of irregular compounding based
on case markers. Case markers were compounded
with the succeeding word. This is a deviation
from the real scenario which would have resulted
in incorrect clustering leading to an unexpected
result. However, the same would not have
occurred for a classifier developed on sense based
features as it was manually sense tagged.
Clustering induces a reduction in the data
sparsity. For example, on En-PD, percentage of
features present in the test set and not present in
the training set to those present in the test set
are 34.17%, 11.24%, 0.31% for words, synsets
and cluster based features respectively. The
improvement in the performance of classifiers
may be attributed to this feature size reduction.
However, it must be noted that clustering based
on unlabelled corpora is less taxing than manually
creating paradigmatic property based clusters like
WordNet synsets.
Barring one instance, both cluster based
features outperform word based features. The
reason for the drop in the accuracy of approach
based on sense features for En-PD dataset
is the domain specific nature of sentiment
analysis (Blitzer et al, 2007), which is explained
in the next point.
2. Domain issues are resolved while using
cluster based features: For En-PD, the classifier
developed using sense features based on
paradigmatic analysis performs inferior to
word based features. Compared to other datasets
used for analysis, this dataset was sense annotated
using an automatic WSD engine. This engine was
trained on a travel domain corpus and as WSD
is also domain specific, the final classification
performance suffered. Additionally, as the target
domain was on products, the automatic WSD
engine employed had an in-domain accuracy
of 78%. The sense disambiguation accuracy of
the same would have lowered in a cross-domain
setting. This might have had a degrading effect on
the SA accuracy.
However, it was seen that classifier developed
on cluster features based on syntagmatic analysis
do not suffer from this. Such clusters
obliterate domain relates issues. In addition, as
more unlabelled data is included for clustering,
the classification accuracy improves.8 Thus,
clustering may be employed to tackle other
specific domain related issues in SA.
8It was observed that adding 0.1 million unlabelled
documents, SA accuracy improved by 1%. This was observed
in the case of English for which there is abundant unlabelled
corpus.
418
3. Cluster based features using syntagmatic
analysis requires lesser training data: Cluster
based features drastically reduces the dimension
of the feature vector. For instance, the size
of sense based features for En-TD dataset was
1/6th of the size of word based features. This
reduces the perplexity of the classification model.
The reduction in the perplexity leads to the
reduction of training documents to attain the same
classification accuracy without any dimensionality
reduction. This is evident from Figure 1
where accuracy of the cluster features based on
unlabelled corpora are higher even with lesser
training data.
4. Effect of cluster size: The cluster size
(number of clusters employed) has an implication
on the purity of each cluster with respect to the
application. The system performance improved
upon increasing the cluster size and converged
after attaining a certain level of accuracy. In
general, it was found that the best classification
accuracy was obtained for a cluster size between
1000 and 2500. As evident from Table 2, once
the optimal accuracy is obtained, no significant
changes were observed by increasing the cluster
size.
5. Clustering based CLSA is effective:
For target language as Hindi, CLSA accuracy
based on cross-lingual clustering (syntagmatic)
outperforms the one based on MT (refer to
Table 3). This was true for the constraint
clustering approach based on cross-lingual
clustering. Whereas, sentiment classifier using
sense (PS) or direct cluster linking (DCL) is
not very effective. In case of PS approach, the
coverage of the multidict was a problem. The
number of a linkages between sense from English
to Hindi is only around 1/3rd the size of Princeton
WordNet (Fellbaum, 1998). Similarly in case
of DCL approach, monolingual likelihood is
different from the cross-lingual likelihood in
terms of the linkages.
6. A note on CLSA for truly resource scarce
languages: Note that there is no publicly available
MT system for English to Marathi. Moreover,
the digital content in Marathi language does not
have a standard encoding format. This impedes
the automatic crawling of the web for corpora
creation for SA. Much manual effort has to be put
to collect enough corpora for analysis. However,
even in these languages, unlabelled corpora is
easy to obtain. Marathi was chosen to depict
a truly resource scarce SA scenario. Cluster
features based classifier comparatively performed
well with 60% classification accuracy. An MT
based system would have suffered in this case as
Marathi, as stated earlier, is a morphologically
rich language and as compared to English, has a
different word ordering. This could degrade the
accuracy of the machine translation itself, limiting
the performance of an MT based CLSA system.
All this is obliterated by the use of a cluster based
CLSA approach. Moreover, as more monolingual
copora is added for clustering, the cross lingual
cluster linkages could be refined. This can further
boost the CLSA accuracy.
8 Conclusion and Future Work
This paper explored feasibility of using word
cluster based features in lieu of features based on
WordNet senses for sentiment analysis to alleviate
the problem of data sparsity. Abstractly, the
motivation was to see if highly effective features
based on paradigmatic property based clustering
could be replaced with the inexpensive ones based
on syntagmatic property for SA.
The study was performed for both monolingual
SA and cross-lingual SA. It was found that
cluster features based on syntagmatic analysis
are better than the WordNet sense features based
on paradigmatic analysis for SA. Invesitgation
revealed that a considerable decrease in the
training data could be achieved while using such
class based features. Moreover, as syntagma based
word clusters are homogenous, it was able to
address domain specific nature of SA as well.
For CLSA, clusters linked together using
unlabelled parallel corpora do away with the need
of translating labelled corpora from one language
to another using an intermediary MT system or
bilingual dictionary. Such a method outperforms
an MT based CLSA approach. Further, this
approach was found to be useful in cases where
there are no MT systems to perform CLSA and
the language of analysis is truly resource scarce.
Thus, wider implication of this study is that many
widely spoken yet resource scare languages like
Pashto, Sundanese, Hausa, Gujarati and Punjabi
which do not have an MT system could now be
analysed for sentiment. The approach presented
here for CLSA will still require a parallel corpora.
However, the size of the parallel corpora required
419
for CLSA can considerably be much lesser than
the size of the parallel corpora required to train an
MT system.
A naive cluster linkage algorithm based on word
alignments was used to perform CLSA. As a
result, there were many erroneous linkages which
lowered the final SA accuracy. Better cluster-
linking approaches could be explored to alleviate
this problem. There are many applications which
use WordNet like IR, IE etc. It would be
interesting to see if these could be replaced by
clusters based on the syntagmatic property.
References
A. R. Balamurali, Aditya Joshi, and Pushpak Bhat-
tacharyya. 2011. Harnessing wordnet senses for su-
pervised sentiment classification. In Proceedings of
EMNLP 2011, pages 1081?1091, Stroudsburg, PA,
USA.
A. R. Balamurali, Aditya Joshi, and Pushpak Bhat-
tacharyya. 2012. Cross-lingual sentiment analysis
for Indian languages using linked wordnets. In Pro-
ceedings of COLING 2012, pages 73?82, Mumbai,
India.
A. R. Balamurali, Mitesh M. Khapra, and Pushpak
Bhattacharyya. 2013. Lost in translation: viability
of machine translation for cross language sentiment
analysis. In Proceedings of CICLing 2013, pages
38?49, Berlin, Heidelberg.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
of EMNLP 2008, pages 127?135, Honolulu, Hawaii.
Farah Benamara, Sabatier Irit, Carmine Cesarano,
Napoli Federico, and Diego Reforgiato. 2007. Sen-
timent analysis: Adjectives and adverbs are better
than adjectives alone. In Proceedings of the Inter-
national Conference on Weblogs and Social Media.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL 2007, pages 440?
447, Prague, Czech Republic.
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-linguistic sentiment analysis: From en-
glish to spanish. In Proceedings of the International
Conference RANLP-2009, pages 50?54, Borovets,
Bulgaria.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, pages 467?479,
December.
D. Chandler. 2012. Semiotics for begin-
ners. http://users.aber.ac.uk/dgc/
Documents/S4B/sem01.html. Online, ac-
cessed 20-February-2013.
D. A. Cruse. 1986. Lexical Semantics. Cambridge
University Press.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of WWW 2003, pages 519?528, New
York, NY, USA.
Kevin Duh, Akinori Fujino, and Masaaki Nagata.
2011. Is machine translation ripe for cross-lingual
sentiment classification? In Proceedings of ACL-
HLT 2011, pages 429?433, Stroudsburg, PA, USA.
Manaal Faruqui and Sebastian Pado?. 2010. Training
and Evaluating a German Named Entity Recognizer
with Semantic Generalization. In Proceedings of
KONVENS 2010, Saarbru?cken, Germany.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Hatem Ghorbel and David Jacot. 2011. Further ex-
periments in sentiment analysis of french movie re-
views. In Proceedings of AWIC 2011, pages 19?28,
Fribourg, Switzerland.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of ACL-HLT
2011, pages 710?714, Stroudsburg, PA, USA.
Kanayama Hiroshi, Nasukawa Tetsuya, and Watanabe
Hideo. 2004. Deeper sentiment analysis using
machine translation technology. In Proceedings of
COLING 2004, Stroudsburg, PA, USA.
Daisuke Ikeda, Hiroya Takamura, Lev arie Ratinov, and
Manabu Okumura. 2008. Learning to shift the po-
larity of words for sentiment classification. In Pro-
ceedings of the Third International Joint Conference
on Natural Language Processing.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
sense disambiguation combining corpus based and
wordnet based parameters. In Proceedings of
Global Wordnet Conference.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-HLT 2008, pages 595?603,
Columbus, Ohio.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. M. eng. thesis, Massachusetts Institute
of Technology.
420
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of CIKM 2009, pages 375?384, New York,
NY, USA.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K.
Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of
ACL-HLT 2011, pages 320?330, Stroudsburg, PA,
USA.
Sven Martin, Jrg Liermann, and Hermann Ney. 1995.
Algorithms for bigram and trigram word clustering.
In Speech Communication, pages 1253?1256.
Justin Martineau and Tim Finin. 2009. Delta TFIDF:
An improved feature space for sentiment analysis.
In Proceedings of ICWSM.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using
word sub-sequences and dependency sub-trees. In
Advances in Knowledge Discovery and Data Min-
ing, Lecture Notes in Computer Science, pages 301?
311.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of WWW 2007, pages 171?180, New
York, NY, USA.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proceedings of HLT-NAACL
2004: Main Proceedings, pages 337?342, Boston,
Massachusetts, USA.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of ACL 2007, pages
128?135, Prague, Czech Republic.
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dictio-
nary: Insights, applications and challenges. In Pro-
ceedings of Global Wordnet Conference.
Tony Mullen and Nigel Collier. 2004. Sentiment anal-
ysis using support vector machines with diverse in-
formation sources. In Proceedings of EMNLP 2004,
pages 412?418, Barcelona, Spain.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Proceed-
ings of HLT-NAACL 2010, pages 786?794, Strouds-
burg, PA, USA.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006. Examining the role of linguistic knowledge
sources in the automatic identification and classifi-
cation of reviews. In Proceedings of the COLING
2006, pages 611?618, Stroudsburg, PA, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Bo Pang and Lillian Lee. 2002. Thumbs up? sen-
timent classification using machine learning tech-
niques. In Proceedings of EMNLP 2002, pages 79?
86, Stroudsburg, PA, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135, January.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Senti-
ment analysis of figurative language using a word
sense disambiguation approach. In Proceedings of
RANLP 2009, pages 370?375, Borovets, Bulgaria,
September.
Niall Rooney, Hui Wang, Fiona Browne, Fergal Mon-
aghan, Jann Mller, Alan Sergeant, Zhiwei Lin,
Philip Taylor, and Vladimir Dobrynin. 2011. An ex-
ploration into the use of contextual document clus-
tering for cluster sentiment analysis. In Proceedings
of RANLP 2011, pages 140?145, Hissar, Bulgaria.
C. Samuelsson and W. Reichl. 1999. A class-based
language model for large-vocabulary speech recog-
nition extracted from part-of-speech statistics. In
Proceedings of ICASSP 1999, pages 537?540.
Sara Stymne. 2012. Clustered word classes for pre-
ordering in statistical machine translation. In Pro-
ceedings of the Joint Workshop on Unsupervised and
Semi-Supervised Learning in NLP, pages 28?34.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkor-
eit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proceedings
of NAACL-HLT 2012, pages 477?487, Montre?al,
Canada.
Martin Tamara, Balahur Alexandra, and Montoyo An-
dres. 2010. Word sense disambiguation in opinion
mining: Pros and cons. Journal Research in Com-
puting Science, 46:119?130.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP 2011, pages 1257?1268,
Stroudsburg, PA, USA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL 2010, pages 384?394, Stroudsburg, PA, USA.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of ACL 2002,
pages 417?424, Stroudsburg, PA, USA.
421
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In Pro-
ceedings of ACL-HLT 2008, pages 755?762, Colum-
bus, Ohio.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of ACL 2009,
pages 235?243, Stroudsburg, PA, USA.
Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi,
Hua-Jun Zeng, Yong Yu, and Zheng Chen. 2005.
Scalable collaborative filtering using cluster-based
smoothing. In Proceedings of SIGIR 2005, pages
114?121, New York, NY, USA.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009.
Sentiment classification of online reviews to travel
destinations by supervised machine learning ap-
proaches. Expert Systems with Applications, 36(3,
Part 2):6527?6535.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011.
Clustering product features for opinion mining. In
Proceedings of WSDM 2011, pages 347?354, New
York, NY, USA.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local fea-
tures. In Proceedings of ACL-HLT 2011, pages 188?
193, Stroudsburg, PA, USA.
422
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780?790,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Infinite Hierarchical Bayesian Model of Phrasal Translation
Trevor Cohn
Department of Computer Science
The University of Sheffield
Sheffield, United Kingdom
t.cohn@sheffield.ac.uk
Gholamreza Haffari
Faculty of Information Technology
Monash University
Clayton, Australia
reza@monash.edu
Abstract
Modern phrase-based machine translation
systems make extensive use of word-
based translation models for inducing
alignments from parallel corpora. This
is problematic, as the systems are inca-
pable of accurately modelling many trans-
lation phenomena that do not decompose
into word-for-word translation. This pa-
per presents a novel method for induc-
ing phrase-based translation units directly
from parallel data, which we frame as
learning an inverse transduction grammar
(ITG) using a recursive Bayesian prior.
Overall this leads to a model which learns
translations of entire sentences, while also
learning their decomposition into smaller
units (phrase-pairs) recursively, terminat-
ing at word translations. Our experiments
on Arabic, Urdu and Farsi to English
demonstrate improvements over competi-
tive baseline systems.
1 Introduction
The phrase-based approach (Koehn et al, 2003)
to machine translation (MT) has transformed MT
from a narrow research topic into a truly useful
technology to end users. Leading translation sys-
tems (Chiang, 2007; Koehn et al, 2007; Marcu et
al., 2006) all use some kind of multi-word transla-
tion unit, which allows translations to be produced
from large canned units of text from the training
corpus. Larger phrases allow for the lexical con-
text to be considered in choosing the translation,
and also limit the number of reordering decisions
required to produce a full translation.
Word-based translation models (Brown et al,
1993) remain central to phrase-based model train-
ing, where they are used to infer word-level align-
ments from sentence aligned parallel data, from
which phrasal translation units are extracted us-
ing a heuristic. Although this approach demon-
strably works, it suffers from a number of short-
comings. Firstly, many phrase-based phenomena
which do not decompose into word translations
(e.g., idioms) will be missed, as the underlying
word-based alignment model is unlikely to pro-
pose the correct alignments. Secondly, the rela-
tionship between different phrase-pairs is not con-
sidered, such as between single word translations
and larger multi-word phrase-pairs or where one
large phrase-pair subsumes another.
This paper develops a phrase-based translation
model which aims to address the above short-
comings of the phrase-based translation pipeline.
Specifically, we formulate translation using in-
verse transduction grammar (ITG), and seek to
learn an ITG from parallel corpora. The novelty
of our approach is that we develop a Bayesian
prior over the grammar, such that a nontermi-
nal becomes a ?cache? learning each production
and its complete yield, which in turn is recur-
sively composed of its child constituents. This is
closely related to adaptor grammars (Johnson et
al., 2007a), which also generate full tree rewrites
in a monolingual setting. Our model learns trans-
lations of entire sentences while also learning their
decomposition into smaller units (phrase-pairs) re-
cursively, terminating at word translations. The
model is richly parameterised, such that it can de-
scribe phrase-based phenomena while also explic-
itly modelling the relationships between phrase-
pairs and their component expansions, thus ame-
liorating the disconnect between the treatment of
words versus phrases in the current MT pipeline.
We develop a Bayesian approach using a Pitman-
Yor process prior, which is capable of modelling
a diverse range of geometrically decaying distri-
butions over infinite event spaces (here translation
phrase-pairs), an approach shown to be state of the
art for language modelling (Teh, 2006).
780
We are not the first to consider this idea; Neu-
big et al (2011) developed a similar approach for
learning an ITG using a form of Pitman-Yor adap-
tor grammar. However Neubig et al?s work was
flawed in a number of respects, most notably in
terms of their heuristic beam sampling algorithm
which does not meet either of the Markov Chain
Monte Carlo criteria of ergodicity or detailed bal-
ance. Consequently their approach does not con-
stitute a valid Bayesian model. In contrast, this
paper provides a more rigorous and theoretically
sound method. Moreover our approach results in
consistent translation improvements across a num-
ber of translation tasks compared to Neubig et al?s
method, and a competitive phrase-based baseline.
2 Related Work
Inversion transduction grammar (or ITG) (Wu,
1997) is a well studied synchronous grammar for-
malism. Terminal productions of the form X ?
e/f generate a word in two languages, and non-
terminal productions allow phrasal movement in
the translation process. Straight productions, de-
noted by their non-terminals inside square brack-
ets [...], generate their symbols in the given or-
der in both languages, while inverted productions,
indicated by angled brackets ?...?, generate their
symbols in the reverse order in the target language.
In the context of machine translation, ITG
has been explored for statistical word alignment
in both unsupervised (Zhang and Gildea, 2005;
Cherry and Lin, 2007; Zhang et al, 2008; Pauls et
al., 2010) and supervised (Haghighi et al, 2009;
Cherry and Lin, 2006) settings, and for decoding
(Petrov et al, 2008). Our paper fits into the re-
cent line of work for jointly inducing the phrase ta-
ble and word alignment (DeNero and Klein, 2010;
Neubig et al, 2011). The work of DeNero and
Klein (2010) presents a supervised approach to
this problem, whereas our work is unsupervised
hence more closely related to Neubig et al (2011)
which we describe in detail below.
A number of other approaches have been de-
veloped for learning phrase-based models from
bilingual data, starting with Marcu and Wong
(2002) who developed an extension to IBM model
1 to handle multi-word units. This pioneer-
ing approach suffered from intractable inference
and moreover, suffers from degenerate solutions
(DeNero and Klein, 2010). Our approach is simi-
lar to these previous works, except that we impose
additional constraints on how phrase-pairs can be
tiled to produce a sentence pair, and moreover,
we seek to model the embedding of phrase-pairs
in one another, something not considered by this
prior work. Another strand of related research is
in estimating a broader class of synchronous gram-
mars than ITGs, such as SCFGs (Blunsom et al,
2009b; Levenberg et al, 2012). Conceptually, our
work could be readily adapted to general SCFGs
using similar techniques.
This work was inspired by adaptor grammars
(Johnson et al, 2007a), a monolingual grammar
formalism whereby a non-terminal rewrites in a
single step as a complete subtree. The model prior
allows for trees to be generated as a mixture of a
cache and a base adaptor grammar. In our case,
we have generalised to a bilingual setting using an
ITG. Additionally, we have extended the model to
allow recursive nesting of adapted non-terminals,
such that we end up with an infinitely recursive
formulation where the top-level and base distribu-
tions are explicitly linked together.
As mentioned above, ours is not the first work
attempting to generalise adaptor grammars for ma-
chine translation; (Neubig et al, 2011) also devel-
oped a similar approach based around ITG using a
Pitman-Yor Process prior. Our approach improves
upon theirs in terms of the model and inference,
and critically, this is borne out in our experiments
where we show uniform improvements in transla-
tion quality over a baseline system, as compared
to their almost entirely negative results. We be-
lieve that their approach had a number of flaws:
For inference they use a beam-search, which may
speed up processing but means that they are no
longer sampling from the true distribution, nor a
distribution with the same support as the posterior.
Moreover they include a Metropolis-Hastings cor-
rection step, which is required to correct the sam-
ples to account for repeated substructures which
will be otherwise underrepresented. Consequently
their approach does not constitute a Markov Chain
Monte Carlo sampler, but rather a complex heuris-
tic.
The other respect in which this work differs
from Neubig et al (2011) is in terms of model for-
mulation. They develop an ITG which generates
phrase-pairs as terminals, while we employ a more
restrictive word-based model which forces the de-
composition of every phrase-pair. This is an im-
portant restriction as it means that we jointly learn
781
a word and phrase based model, such that word
based phenomena can affect the phrasal struc-
tures. Finally our approach models separately the
three different types of ITG production (mono-
tone, swap and lexical emission), allowing for a
richer parameterisation which the model exploits
by learning different hyper-parameter values.
3 Model
The generative process of the model follows that
of ITG with the following simple grammar
X ? [X X] | ?X X?
X ? e/f | e/? | ?/f ,
where [?] denotes monotone ordering and ??? de-
notes a swap in one language. The symbol ? de-
notes the empty string. This corresponds to a sim-
ple generative story, with each stage being a non-
terminal rewrite starting with X and terminating
when there are no frontier non-terminals.
A popular variant is a phrasal ITG, where the
leaves of the ITG tree are phrase-pairs and the
training seeks to learn a segmentation of the source
and target which yields good phrases. We would
not expect this model to do very well as it cannot
consider overlapping phrases, but instead is forced
into selecting between many competing ? and of-
ten equally viable ? options. Our approach im-
proves over the phrasal model by recursively gen-
erating complete phrases. This way we don?t insist
on a single tiling of phrases for a sentence pair, but
explicitly model the set of hierarchically nested
phrases as defined by an ITG derivation. This ap-
proach is closer in spirit to the phrase-extraction
heuristic, which defines a set of ?atomic? terminal
phrase-pairs and then extracts every combination
of these atomic phase-pairs which is contiguous in
the source and target.1
The generative process is that we draw a com-
plete ITG tree, t ? P2(?), as follows:
1. choose the rule type, r ? R, where r ?
{mono, swap, emit}
2. for r = mono
(a) draw the complete subtree expansion,
t = X ? [. . .] ? TM
3. for r = swap
(a) draw the complete subtree expansion,
t = X ? ?. . .? ? TS
1Our technique considers the subset of phrase-pairs which
are consistent with the ITG tree.
4. for r = emit
(a) draw a pair of strings, (e, f) ? E
(b) set t = X ? e/f
Note that we split the problem of drawing a tree
into two steps: first choosing the top-level rule
type and then drawing a rule of that type. This
gives us greater control than simply drawing a tree
of any type from one distribution, due to our pa-
rameterisation of the priors over the model param-
eters TM , TS and E.
To complete the generative story, we need to
specify the prior distributions for TM , TS and
E. First, we deal with the emission distribu-
tion, E which we drawn from a Dirichlet Pro-
cess prior E ? DP(bE , P0). We restrict the emis-
sion rules to generate word pairs rather than phrase
pairs.2 For the base distribution, P0, we use a sim-
ple uniform distribution over word pairs,
P0(e, f) =
?
??
??
?2 1VEVF e 6= ?, f 6= ?
?(1? ?) 1VF e = ?, f 6= ?
?(1? ?) 1VE e 6= ?, f = ?
,
where the constant ? denotes the binomial proba-
bility of a word being aligned.3
We use Pitman-Yor Process priors for the TM
and TS parameters
TM ? PYP(aM , bM , P1(?|r = mono))
TS ? PYP(aS , bS , P1(?|r = swap))
where P1(t1, t2|r) is a distribution over a pair of
trees (the left and right children of a monotone or
swap production). P1 is defined as follows:
1. choose the complete left subtree t1 ? P2,
2. choose the complete right subtree t2 ? P2,
3. set t = X ? [t1 t2] or t = X ? ?t1 t2?
depending on r
This generative process is mutually recursive: P2
makes draws from P1 and P1 makes draws from
P2. The recursion is terminated when the rule type
r = emit is drawn.
Following standard practice in Bayesian mod-
els, we integrate out R, TM , TS and E. This
means draws from P2 (or P1) are no longer iid:
for any non-trivial tree, computing its probabil-
ity under this model is complicated by the fact
2Note that we could allow phrases here, but given the
model can already reason over phrases by way of its hier-
archical formulation, this is an unnecessary complication.
3We also experimented with using word translation prob-
abilities from IBM model 1, based on the prior used by Lev-
enberg et al (2012), however we found little empirical differ-
ence compared with this simpler uniform model.
782
that the probability of its two subtrees are inter-
dependent. This is best understood in terms of
the Chinese Restaurant Franchise (CRF; Teh et al
(2006)), which describes the posterior distribution
after integrating out the model parameters. In our
case we can consider the process of drawing a tree
from P2 as a customer entering a restaurant and
choosing where to sit, from an infinite set of ta-
bles. The seating decision is based on the number
of other customers at each table, such that popular
tables are more likely to be joined than unpopular
or empty ones. If the customer chooses an occu-
pied table, the identity of the tree is then set to
be the same as for the other customers also seated
there. For empty tables the tree must be sampled
from the base distribution P1. In the standard CRF
analogy, this leads to another customer entering
the restaurant one step up in the hierarchy, and
this process can be chained many times. In our
case, however, every new table leads to new cus-
tomers reentering the original restaurant ? these
correspond to the left and right child trees of a
monotone or swap rule. The recursion terminates
when a table is shared, or a new table is labelled
with a emit rule.
3.1 Inference
The probability of a tree (i.e., a draw from P2) un-
der the model is
P2(t) = P (r)P2(t|r) (1)
where r is the rule type, one of mono, swap or
emit. The distribution over types, P (r), is de-
fined as
P (r) = n
T,?
r + bT 13
nT,? + bT
where nT,? are the counts over rules of types.4
The second component in (1), P2(t|r), is de-
fined separately for each rule type. For r = mono
or r = swap rules, it is defined as
P2(t|r) =
n?t,r ?K?t,rar
n?r + br
+ K
?
r ar + br
n?r + br
P1(t1, t2|r) ,
(2)
where n?t,r is the count for tree t in the other train-
ing sentences, K?t,r is the table count for t and n?r
4The conditioning on event and table counts, n?,K? is
omitted for clarity.
and K?r are the total count of trees and tables, re-
spectively. Finally, the probability for r = emit
is given by
P2(t|r = emit) =
n?t,E + bEP0(e, f)
n?r + br
,
where t = X ? e/f .
To complete the derivation we still need to de-
fine P1, which is formulated as
P1(t1, t2) = P2(t1)P2(t2|t1) ,
where the conditioning of the second recursive call
to P2 reflects that the counts n? and K? may
be affected by the first draw from P2. Although
these two draws are assumed iid in the prior, after
marginalising out T they are no longer indepen-
dent. For this reason, evaluating P2(t) is computa-
tionally expensive, requiring tracking of repeated
substructures in descendent sub-trees of t, which
may affect other descendants. This results in an
asymptotic complexity exponential in the number
of nodes in the tree. For this reason we consider
trees annotated with binary values denoting their
table assignment, namely whether they share a ta-
ble or are seated alone. Given this, the calculation
is greatly simplified, and has linear complexity.5
We construct an approximating ITG following
the technique used for sampling trees from mono-
lingual tree-substitution grammars (Cohn et al,
2010). To do so we encode the first term from
(2) separately from the second term (correspond-
ing to draws from P1). Summing together these
two alternate paths ? i.e., during inside inference ?
we recover P2 as shown in (2). The full grammar
transform for inside inference is shown in Table 1.
The sampling algorithm closely follows the
process for sampling derivations from Bayesian
PCFGs (Johnson et al, 2007b). For each sentence-
pair, we first decrement the counts associated with
its current tree, and then sample a new deriva-
tion. This involves first constructing the inside
lattice using the productions in Table 1, and then
performing a top-down sampling pass. After
sampling each derivation from the approximating
grammar, we then convert this into its correspond-
ing ITG tree, which we then score with the full
model and accept or reject the sample using the
5To support this computation, we track explicit table as-
signments for every training tree and their component sub-
trees. We also sample trees labelled with seating indicator
variables.
783
Typ
e X ?M P (r = mono)
X ? S P (r = swap)
X ? E P (r = emit)
Bas
e M ? [XX] K
?
MaM+bM
n?M+bM
S ? ?XX? K?S aS+bSn?S +bS
Co
unt
For every tree, t, of type r = mono, with nt,M > 0:
M ? sig(t) n?t,M?K?t,Marn?M+bMsig(t)? yield(t) 1
For every tree, t, of type r = swap, with nt,S > 0:
S ? sig(t) n?t,S?K?t,SaSn?S +bSsig(t)? yield(t) 1
Em
it For every word pair, e/f in sentence pair,
where one of e, f can be ?:
E ? e/f P2(t)
Table 1: Grammar transformation rules for MAP
inside inference. The function sig(t) returns a
unique identifier for the complete tree t, and
the function yield(t) returns the pair of terminal
strings from the yield of t.
Metropolis-Hastings algorithm.6 Accepted sam-
ples then replace the old tree (otherwise the old
tree is retained) and the model counts are incre-
mented. This process is then repeated for each
sentence pair in the corpus in a random order.
4 Experiments
Datasets We train our model across three
language pairs: Urdu?English (UR-EN),
Farsi?English (FA-EN), and Arabic?English
(AR-EN). The corpora statistics of these trans-
lation tasks are summarised in Table 2. The
UR-EN corpus comes from NIST 2009 translation
evaluation.7 The AR-EN training data consists
of the eTIRR corpus (LDC2004E72), the Ara-
bic news corpus (LDC2004T17), the Ummah
corpus (LDC2004T18), and the sentences with
confidence c > 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
For FA-EN, we use TEP8 Tehran English-Persian
Parallel corpus (Pilevar and Faili, 2011), which
consists of conversational/informal text extracted
6The full model differs from the approximating grammar
in that it accounts for inter-dependencies between subtrees
by recursively tracking the changes in the customer and table
counts while scoring the tree. Around 98% of samples were
accepted in our experiments.
7http://www.itl.nist.gov/iad/mig/tests/mt/2009
8http://ece.ut.ac.ir/NLP/resources.htm
source target sentences
UR-EN 745K 575K 148K
FA-EN 4.7M 4.4M 498K
AR-EN 1.94M 2.08M 113K
Table 2: Corpora statistics showing numbers of
parallel sentences and source and target words for
the training sets.
from 1600 movie subtitles. We tokenized this
corpus, removed noisy single-word sentences,
randomly selected the development and test sets,
and used the rest of the corpus as the training set.
We discard sentences with length above 30 from
the datasets for all experiments.9
Sampler configuration Samplers are initialised
with trees created from GIZA++ alignments
constructed using a SCFG factorisation method
(Blunsom et al, 2009a). This algorithm repre-
sents the translation of a sentence as a large SCFG
rule, which it then factorises into lower rank SCFG
rules, a process akin to rule binarisation com-
monly used in SCFG decoding. Rules that can-
not be reduced to a rank-2 SCFG are simplified
by dropping alignment edges until they can be
factorised, the net result being an ITG derivation
largely respecting the alignments.10
The blocked sampler was run 1000 iterations
for UR-EN, 100 iterations for FA-EN and AR-
EN. After each full sampling iteration, we resam-
ple all the hyper-parameters using slice-sampling,
with the following priors: a ? Beta(1, 1),
b ? Gamma(10, 0.1). Figure 1 shows the poste-
rior probability improves with each full sampling
iterations. The alignment probability was set to
? = 0.99. The sampling was repeated for 5 in-
dependent runs, and we present results where we
combine the outputs of these runs. This is a form
of Monte Carlo integration which allows us to rep-
resent the uncertainty in the posterior, while also
representing multiple modes, if present.
The time complexity of our inference algorithm
is O(n6), which can be prohibitive for large scale
machine translation tasks. We reduce the com-
plexity by constraining the inside inference to
consider only derivations which are compatible
9Hence the BLEU scores we get for the baselines may
appear lower than what reported in the literature.
10Using the factorised alignments directly in a translation
system resulted in a slight loss in BLEU versus using the un-
factorised alignments. Our baseline system uses the latter.
784
0 100 200 300 400 500?9
1000
00
?
9050
000
?
9000
000
?
8950
000
iteration
log p
oster
ior
Figure 1: Training progress on the UR-EN corpus,
showing the posterior probability improving with
each full sampling iteration. Different colours de-
note independent sampling runs.
l
ll
ll
l
l
l
ll
l
l
l
lll
l
l
ll
l
l
l
ll
l
l
lll
l
l l
l l
l
l l
llll
l
l
lll
l
l
ll
lll
lll
l
l
ll
l
l
ll
ll
l
ll l
lll
l
l
lll
ll
llll
lll
l
ll l
l
lll
l
l l
ll
llll
ll
llll
l
l l
ll
l
ll
l l
l
l l
llll
l l
l
llll
lll
l
l l
l
l
l ll
l ll
l
l l
l
l
l ll
llll
l
l
0 5 10 15 20 25 30
1e?0
5
1e?0
3
1e?0
1
average sentence length
time 
(s)
Figure 2: The runtime cost of bottom-up inside in-
ference and top-down sampling as a function of
sentence length (UR-EN), with time shown on a
logarithmic scale. Full ITG inference is shown
with red circles, and restricted inference using the
intersection constraints with blue triangles. The
average time complexity for the latter is roughly
O(l4), as plotted in green t = 2? 10?7l4.
with high confidence alignments from GIZA++.11
Figure 2 shows the sampling time with respect
to the average sentence length, showing that our
alignment-constrained sampling algorithm is bet-
ter than the unconstrained algorithm with empir-
ical complexity of n4. However, the time com-
plexity is still high, so we set the maximum sen-
tence length to 30 to keep our experiments practi-
cable. Presumably other means of inference may
be more efficient, such as Gibbs sampling (Lev-
enberg et al, 2012) or auxiliary variable sampling
(Blunsom and Cohn, 2010); we leave these exten-
sions to future work.
Baselines. Following (Levenberg et al, 2012;
Neubig et al, 2011), we evaluate our model by
using its output word alignments to construct a
phrase table. As a baseline, we train a phrase-
based model using the moses toolkit12 based on
the word alignments obtained using GIZA++ in
both directions and symmetrized using the grow-
diag-final-and heuristic13 (Koehn et al, 2003).
This alignment is used as input to the rule fac-
torisation algorithm, producing the ITG trees with
which we initialise our sampler. To put our results
in the context of the previous work, we also com-
pare against pialign (Neubig et al, 2011), an ITG
algorithm using a Pitman-Yor process prior, as de-
scribed in Section 2.14
In the end-to-end MT pipeline we use a stan-
dard set of features: relative-frequency and lexical
translation model probabilities in both directions;
distance-based distortion model; language model
and word count. We set the distortion limit to
6 and max-phrase-length to 7 in all experiments.
We train 3-gram language models using modified
Kneser-Ney smoothing. For AR-EN experiments
the language model is trained on English data as
(Blunsom et al, 2009a), and for FA-EN and UR-
EN the English data are the target sides of the
bilingual training data. We use minimum error
rate training (Och, 2003) with nbest list size 100
to optimize the feature weights for maximum de-
velopment BLEU.
11These are taken from the final model 4 word alignments,
using the intersection of the source-target and target-source
models. These alignments are very high precision (but have
low recall), and therefore are unlikely to harm the model.
12http://www.statmt.org/moses
13We use the default parameter settings in both moses and
GIZA++.
14http://www.phontron.com/pialign
785
Baselines This paper
GIZA++ pialign individual combination
UR-EN 16.95 15.65 16.68 ? .12 16.97
FA-EN 20.69 21.41 21.36 ? .17 21.50
AR-EN
MT03 44.05 43.30 44.8 ? .28 45.10
MT04 38.15 37.78 38.4 ? .08 38.4
MT05 42.81 42.18 43.13 ? .23 43.45
MT08 32.43 33.00 32.7 ? .15 32.80
Table 3: The BLEU scores for the translation tasks of three language pairs. The individual column show
the average and 95% confidence intervals for 5 independent runs, whereas the combination column show
the results for combining the phrase tables of all these runs. The baselines are GIZA++ alignments and
those generated by the pialign (Neubig et al, 2011) bold: the best result.
1 2 5 10 20 50 100
1e?0
5
1e?0
3
1e?0
1
rule frequency
fracti
on of
 gram
mar
monotoneswapemit
Figure 3: Fraction of rules with a given frequency,
using a single sample grammar (UR-EN).
4.1 Results
Table 3 shows the BLEU scores for the three trans-
lation tasks UR/AR/FA?EN based on our method
against the baselines. For our models, we report
the average BLEU score of the 5 independent runs
as well as that of the aggregate phrase table gen-
erated by these 5 independent runs. There are
a number of interesting observations in Table 3.
Firstly, combining the phrase tables from indepen-
dent runs results in increased BLEU scores, possi-
bly due to the representation of uncertainty in the
outputs, and the representation of different modes
captured by the individual models. We believe this
type of Monte Carlo model averaging should be
considered in general when sampling techniques
are employed for grammatical inference, e.g. in
parsing and translation. Secondly, our approach
consistently improves over the Giza++ baseline
often by a large margin, whereas pialign under-
performs the GIZA++ baseline in many cases.
Thirdly, our model consistently outperforms pi-
align (except in AR-EN MT08 which is very
close). This highlights the modeling and inference
differences between our method and the pialign.
5 Analysis
In this section, we present some insights about the
learned grammar and the model hyper-parameters.
Firstly, we start by presenting various statistics
about different learned grammars. Figure 3 shows
the fraction of rules with a given frequency for
each of the three rule types. The three types of rule
exhibit differing amounts of high versus low fre-
quency rules, and all roughly follow power laws.
As expected, there is a higher tendency to reuse
high-frequency emissions (or single-word transla-
tion) compared to other rule types, which are the
basic building blocks to compose larger rules (or
phrases). Table 4 lists the high frequency mono-
tone and swap rules in the learned grammar. We
observe the high frequency swap rules capture re-
ordering in verb clusters, preposition-noun inver-
sions and adjective-noun reordering. Similar pat-
terns are seen in the monotone rules, along with
some common canned phrases. Note that ?in Iraq?
appears twice, once as an inversion in UR-EN and
another time in monotone order for AR-EN.
Secondly, we analyse the values learned for
the model hyper-parameters; Figure 4.(a) shows
the posterior distribution over the hyper-parameter
values. There is very little spread in the inferred
values, suggesting the sampling chains may have
converged. Furthermore, there is a large differ-
ence between the learned hyper-parameters for the
monotone rules versus the swap rules. For the
Pitman-Yor Process prior, the values of the hyper-
786
6`bB@1M;HBb?
TBHB;M bm`2fKhK M-+?B27f` vb- vQm bm`2fKhK Mv- BK bm`2fKhK MK- #Qbbf` vb- KF2 bm`2fKhK M-
`2 vQm bm`2fKhK Mv- Mvrvf?` >H- T`2bB/2Mif` vb DK?r`- MQi bm`2fKhK M MvbiK- BK
bm`2fKhK MK F?- B?K bm`2fKhK MK- bm`2fKhK M
Qm` K2i?Q/ bm`2fKhK }- ?p2f/ $ i?- #2f# $- ?p2f/ $ i? # $- H2i K2f `- #2+mb2 Q7fth`-
bm`2fKhK } M- /QfF` `- +QK2 QMfxr/ #- 2t+mb2 K2f##t- FBHHf` #F- +QK2 QMfxr/#-
KQ`2 i?Mf$ i`- #2?BM/fT $- r?i /QfKMwr`i- r?i /Q vQmfKMwr`i - FBHHfF $- /QMi
rQ``vfM;`M M#- Bb Bif$ /?- r2H+QK2ftr $ %- +?B27f` } vb- KF2 bm`2fKhK- BbfKv $-
KF2 bm`2fKhK }- KF2 bm`2fKhK} M- BK bQ``vf##t- H27if;  - B7f;` $
`#B+@1M;HBb?
TBHB;M bB/ Xf??? ?- bii2bf???????- mMBi2/f????????- H@r7/f ? ?????- 2zQ`ibf???- Q7 Kbb
/2bi`m+iBQMf?????? ??????- vQmKf?????- DBMiQf??? ???- HK H vQmKf????? ??????- H@BiiB?/
-f???????- i?2 }2H/ Q7f????- BbHK#/f?????- b+?2/mH2/f?????? ??- H@HK H@vQmKf????? ??????
?- f H@?vif? ??????- T2MBMbmHf??????? ???- K2Mr?BH2f???? ?????- T`BK2f?????- i?2
BMpBiiBQMf?? ????- f H@?vi -f? ??????- FQ`2M T2MBMbmHf??????? ??????? ???- H@M?` ?f??????
DD- /2T`iK2Mif???????? ?????- +Qi2f???- b TQbbB#H2f???? ???- H HK H vQmKf????? ??????- -
H@HK H@vQmK -f????? ??????- i i?2 BMpBiiBQMf?? ???? ?- D+[m2bf??? ???????- r2HH bf???-
TQBMibf???- pH/BKB` TmiBMf????? ???????? ??????- ;2Q`;2 rX #mb?f????
Qm` K2i?Q/ i?2 mMBi2/f???????- mb /QHH`bf??????- T`BK2f??????? ????- +?BM ?f?????- bTQF2bKMf???????
?- KMvf??- Bb 2tT2+i2/f???????- Bb 2tT2+i2/ iQf???????- i H2bif?????- QM im2b/vf???- 2;vTi
?f???- i?m`b/vf???- i?2 mMf???????- QM i?m`b/vf???- 7`B/vf???- QM 7`B/vf???- iQf???-
H@r7/ -f? ?????- i?2 mbf???????- 7Q`f? ??????- }`bi iBK2f?????? ?????- 7m`i?2`f??- B`[
?f??????- Bb`2HB T`BK2f?????????? ??????? ????- i?2 irQf???????- QM bim`/vf???- QM bmM/vf???-
mXbXf???????- pB2rbf?????- b?`QM ?f?????- +QmMi`v ?f??????- ?2 bB/f???- Bb`2H ?f???????- T2QTH2
?f?????- ?2`2f????? ???- +?BM ?f?????- - ?2 bB/f???? ?- 2`HB2`f??? ??- +?BM ?f???????-
i H2bif??? ?? ??- i?2 mXbXf???????- i?2 ;xf????- i?2 ;x bi`BTf????- ?2 //2/f???- `2
2tT2+i2/f???????- `2 2tT2+i2/ iQf???????- `2 2tT2+i2/ iQf??????? ??- KBHHBQM mXbXf?????-
++Q`/BM;f???- iQf?????- Q`/2`f??- BM Q`/2`f??- ?2 TQBMi2/f????- K7 - b?`[ Hf? ??????- K7
- b?`[ H rbif? ??????- `7i ?f?????
k
Table 5: Good phrase pairs in the top-100 high frequency phrase pairs specific to the phrase tables
coming from our method vs that of pialign for FA-EN and AR-EN translation tasks.
parameters affects the rate at which the number of
types grows compared to the number of tokens.
Specifically, as the discount a or the concentra-
tion b parameters increases we expect for a rela-
tive increase in the number of types. If the number
of observed monotone and swap rules were equal,
then there would be a higher chance in reusing the
monotone rules. However, the number of observed
monotone and swap rules are not equal, as plotted
in Figure 4.(b). Similar results were observed for
the other language pairs (figures omitted for space
reasons).
Thirdly, we performed a manual evaluation for
the quality of the phrase-pairs learned exclusively
by our method vs pialign. For each method,
we considered the top-100 high frequency phrase-
pairs which are specific to that method. Then we
asked a bilingual human expert to identify rea-
sonably well phrase-pairs among these top-100
phrase-pairs. The results are summarized in Ta-
ble 5, and show that we learn roughly twice as
many reasonably good phrase-pairs for AR-EN
and FA-EN compared to pialign.
Conclusions
We have presented a novel method for learn-
ing a phrase-based model of translation directly
from parallel data which we have framed as learn-
ing an inverse transduction grammar (ITG) us-
ing a recursive Bayesian prior. This has led
to a model which learns translations of en-
tire sentences, while also learning their decom-
position into smaller units (phrase-pairs) recur-
sively, terminating at word translations. We have
presented a Metropolis-Hastings sampling algo-
rithm for blocked inference in our non-parametric
ITG. Our experiments on Urdu-English, Arabic-
English, and Farsi-English translation tasks all
demonstrate improvements over competitive base-
line systems.
Acknowledgements
The first author was supported by the EPSRC
(grant EP/I034750/1) and an Erasmus-Mundus
scholarship funding a research visit to Melbourne.
The second author was supported by an early ca-
reer research award from Monash University.
787
0.905 0.910 0.915 0.920 0.925
0
200
400
600
800
100
0
am and as
Den
sity
1000 2000 3000 40000
.000
0
0.00
10
0.00
20
bm and bs
Den
sity
0 5 10 15 20 25 30
0.00
0.01
0.02
0.03
0.04
0.05
0.06
be
Den
sity
65000 65500 660000
.000
0
0.00
05
0.00
10
0.00
15
bt
Den
sity
(a)
291000 292000 2930000
.000
0
0.00
04
0.00
08
0.00
12
monotone
176000 1770000
.000
0
0.00
04
0.00
08
0.00
12
swap
Den
sity
(b)
Figure 4: (a) Posterior over the hyper-parameters,
aM , aS , bM , bS , bE , bT , measured for UR-EN us-
ing samples 400?500 for 3 independent sampling
chains, and the intersection constraints. (b) Poste-
rior over the number of monotone and swap rules
in the resultant grammars. The distribution for
emission rules was also peaked about 147k rules.
key ( ( ( ?2fM?rL ) ( ?fMu ) ) ( bB/fF? ) )
kkk ( ? ( bB/fF? ) ( ?fMu ) ? ( i?ifF? ) )
kRN ( ( ( ( ?2fM?rL ) ( ?fMu ) )
( bB/fF? ) ) ( i?ifF? ) )
R93 ( ( ( B7f;` ) ( ?f% ) ) ( vQmfT ) )
Ry3 ( ( ?2fM?rL ) ? ( bB/fF? ) ( ?fMu ) ? )
R3k ? ( rBHHf; ) ( #2f?r ) ?
RkN ? ( Bbf?u ) ( MQifM?vL ) ?
Rkj ? ( ?bf?u ) ( #22Mf;v ) ?
Ry9 ? ( rBHHf; ) ( #2fDvu ) ?
Ryj ? ( BMfKvL ) ( B`[f1`[ ) ?
l`/m@1M;HBb?
3Ny ( ( QM2fvFv ) ( Q7fx ) )
39j ( ? ( v2?f`? ) ( ?f% ) ? ( XfX ) )
dj3 ( ( rBi?f# ) ( K2fKM ) )
e99 ( ( ( ( QFvf# ) ( ?f$ ) ) ( ?f? ) ) ( XfX ) )
ey3 ( ( iQf#? ) ( K2fKM ) )
k8R ? ( Bbf/? ) ( Bif$ ) ?
kky ? ( i2HHf#;r ) ( K2fKM ) ?
RNN ? I ( Bf? ) ( +MfirMK ) ? ( ?ifMKv ) =
RNy ? ( ( r?QfFv ) ( `2f?biv ) ) ( vQmfir ) ?
R3d ? ( iQH/f;7i ) ( K2fKM ) ?
6`bB@1M;HBb?
8ee ( ( BMf?? ) ( B`[f?????? ) )
9R9 ( ( BMf?? ) ( 2;vTif??? ) )
jNR ( ( i?Bbf??? ) ( v2`f????? ) )
j8e ( ( b?`[f????? ) ( H@rbif?????? ) )
jyy ( ( BMf?? ) ( B`[f?????? ) )
9yk9 ? ( Xf ) ( ?f ) ?
RjRk ? I ( i?2f ) ( mMBi2/f??????? ) ?
( bii2bf???????? ) =
ee8 ? ( mMBi2/f??????? ) ( bii2bf???????? ) ?
e8y ? ( Hbif?????? ) ( v2`f????? ) ?
9ed ? I ( i?2f ) ( mMBi2/f??????? ) ?
( MiBQMbf????? ) =
`#B+@1M;HBb?
aQK2 HiBM i2ti M/ BMHBM2 `#B+, ????? ??????
l`/m- 6`bB- `#B+
Table 4: Top 5 monotone and swap productions
and their counts. Rules with mostly punctuation
or encoding 1:many or many:1 alignments were
omitted.
788
References
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 238?241,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009a. A Gibbs sampler for phrasal syn-
chronous grammar induction. In ACL2009, Singa-
pore, August.
Phil Blunsom, Trevor Cohn, and Miles Osborne.
2009b. Bayesian synchronous grammar induction.
In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot-
tou, editors, Advances in Neural Information Pro-
cessing Systems 21, pages 161?168. MIT Press.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In Proceedings of COLING/ACL. As-
sociation for Computational Linguistics.
Colin Cherry and Dekany Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In Proc. of the HLT-NAACL Workshop on
Syntax and Structure in Statistical Translation (SSST
2007), Rochester, USA.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, pages 3053?3096.
John DeNero and Dan Klein. 2010. Discriminative
modeling of extraction sets for machine translation.
In The 48th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL).
Aria Haghighi, John Blitzer, and Dan Klein. 2009.
Better word alignments with supervised itg models.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, Suntec, Singapore. Association for
Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007a. Adaptor grammars: A framework for
specifying compositional nonparametric bayesian
models. In B. Scho?lkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641?648. MIT Press, Cambridge,
MA.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007b. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proc. of the 7th Inter-
national Conference on Human Language Technol-
ogy Research and 8th Annual Meeting of the NAACL
(HLT-NAACL 2007), pages 139?146.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of the 3rd International Conference on Human Lan-
guage Technology Research and 4th Annual Meet-
ing of the NAACL (HLT-NAACL 2003), pages 81?88,
Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the ACL (ACL-
2007), Prague.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 223?232, Jeju Island, Korea, July.
Association for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proc. of the 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2002), pages 133?139, Philadelphia, July.
Association for Computational Linguistics.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. of the 2006 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2006), pages 44?52, Sydney, Australia, July.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In The 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT), pages 632?641,
Portland, Oregon, USA, 6.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the ACL (ACL-2003), pages 160?
167, Sapporo, Japan.
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment
with inversion transduction grammars. In Proceed-
ings of the North American Conference of the Asso-
ciation for Computational Linguistics (NAACL). As-
sociation for Computational Linguistics.
789
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of EMNLP.
Association for Computational Linguistics.
M. T. Pilevar and H. Faili. 2011. Tep: Tehran english-
persian parallel corpus. In Proc. International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing).
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M.
Blei. 2006. Hierarchical Dirichlet processes.
Journal of the American Statistical Association,
101(476):1566?1581.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
985?992.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the 43rd Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL). Association for Computational Linguis-
tics.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proc. of the 46th Annual Conference of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), pages 97?105,
Columbus, Ohio, June.
790
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 533?541,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Bayesian Extraction of Minimal SCFG Rules for
Hierarchical Phrase-based Translation
Baskaran Sankaran
Simon Fraser University
Burnaby BC, Canada
baskaran@cs.sfu.ca
Gholamreza Haffari
Monash University
Melbourne, Australia
reza@monash.edu
Anoop Sarkar
Simon Fraser University
Burnaby BC, Canada
anoop@cs.sfu.ca
Abstract
We present a novel approach for extracting
a minimal synchronous context-free grammar
(SCFG) for Hiero-style statistical machine
translation using a non-parametric Bayesian
framework. Our approach is designed to ex-
tract rules that are licensed by the word align-
ments and heuristically extracted phrase pairs.
Our Bayesian model limits the number of
SCFG rules extracted, by sampling from the
space of all possible hierarchical rules; addi-
tionally our informed prior based on the lex-
ical alignment probabilities biases the gram-
mar to extract high quality rules leading to im-
proved generalization and the automatic iden-
tification of commonly re-used rules. We
show that our Bayesian model is able to ex-
tract minimal set of hierarchical phrase rules
without impacting the translation quality as
measured by the BLEU score.
1 Introduction
Hierarchical phrase-based (Hiero) machine transla-
tion (Chiang, 2007) has attracted significant interest
within the Machine Translation community. It ex-
tends phrase-based translation by automatically in-
ferring a synchronous grammar from an aligned bi-
text. The synchronous context-free grammar links
non-terminals in source and target languages. De-
coding in such systems employ a modified CKY-
parser that is integrated with a language model.
The primary advantage of Hiero-style systems lie
in their unsupervised model of syntax for transla-
tion: allowing long-distance reordering and cap-
turing certain syntactic constructions, particularly
those that involve discontiguous phrases. It has
been demonstrated to be a successful framework
with comparable performance with other statisti-
cal frameworks and suitable for large-scale cor-
pora (Zollmann et al, 2008). However, one of the
major difficulties in Hiero-style systems has been on
learning a concise and general synchronous gram-
mar from the bitext.
While most of the research in Hiero-style sys-
tems is focused on the improving the decoder, and
in particular the link to the language model, compar-
atively few papers have considered the inference of
the probabilistic SCFG from the word alignments.
A majority of the systems employ the classic rule-
extraction algorithm (Chiang, 2007) which extracts
rules by replacing possible sub-spans (permitted by
the word alignments) with a non-terminal and then
using relative frequencies to estimate the probabilis-
tic synchronous context-free grammar. One of the
issues in building Hiero-style systems is in manag-
ing the size of the synchronous grammar. The origi-
nal approach extracts a larger number of rules when
compared to a phrase-based system on the same data
leading to practical issues in terms of memory re-
quirements and decoding speed.
Extremely large Hiero phrase tables may also lead
to statistical issues, where the probability mass has
to be shared by more rules: the probability p(e|f)
has to be shared by all the rules having the same
source side string f , leading to fragmentation and
resulting in many rules having very poor probability.
Approaches to improve the inference (the induc-
tion of the SCFG rules from the bitext) typically
follows two streams. One focusses on filtering the
extracted hierarchical rules either by removing re-
dundancy (He et al, 2009) or by filtering rules
based on certain patterns (Iglesias et al, 2009),
while the other stream is concerned about alterna-
tive approaches for learning the synchronous gram-
mar (Blunsom et al, 2008; Blunsom et al, 2009; de
Gispert et al, 2010). This paper falls under the lat-
ter category and we use a non-parametric Bayesian
approach for rule extraction for Hiero-style systems.
Our objective in this paper is to provide a principled
533
rule extraction method using a Bayesian framework
that can extract the minimal SCFG rules without re-
ducing the BLEU score.
2 Motivation and Related Work
The large number of rules in Hiero-style systems
leads to slow decoding and increased memory re-
quirements. The heuristic rule extraction algo-
rithm (Chiang, 2007) introduces redundant mono-
tone composed rules (He et al, 2009) in the SCFG
grammar. The research on Hiero rule extraction falls
into two broad categories: i) rule reduction by elim-
inating a subset of rules extracted by the heuristic
approach and ii) alternate approaches for rule extrac-
tion.
There have been approaches to reduce the size of
Hiero phrase table, without significantly affecting
the translation quality. He et. al. (2009) proposed the
idea of discarding monotone composed rules from
the phrase table that can instead be obtained dynami-
cally by combining the minimal rules in the same or-
der. They achieve up to 70% reduction in the phrase
table by discarding these redundant rules, without
appreciable reduction in the performance as mea-
sured by BLEU. Empirically analyzing the effective-
ness of specific rule patterns, (Iglesias et al, 2009)
show that some patterns having over 95% of the to-
tal SCFG rules can be safely eliminated without any
reduction in the BLEU score.
Along a different track, some prior works have
employed alternate rule extraction approaches using
a Bayesian framework (DeNero et al, 2008; Blun-
som et al, 2008; Blunsom et al, 2009). (DeNero
et al, 2008) use a Maximum likelihood model of
learning phrase pairs (Marcu and Wong, 2002), but
use sampling to compute the expected counts of the
phrase pairs for the E-step. Other recent approaches
use Gibbs sampler for learning the SCFG by explor-
ing a fixed grammar having pre-defined rule tem-
plates (Blunsom et al, 2008) or by reasoning over
the space of derivations (Blunsom et al, 2009).
We differ from earlier Bayesian approaches in that
our model is guided by the word alignments to rea-
son over the space of the SCFG rules and this re-
stricts the search space of our model. We believe
the word alignments to encode information, useful
for identifying the good phrase-pairs. For example,
several attempts have been made to learn a phrasal
translation model directly from the bitext without
the word alignments (Marcu and Wong, 2002; DeN-
ero et al, 2008; Blunsom et al, 2008), but without
any clear breakthrough that can scale to larger cor-
pora.
Our model exploits the word alignment informa-
tion in the form of lexical alignment probability in
order to construct an informative prior over SCFG
rules and it moves away from a heuristic framework,
instead using a Bayesian non-parametric model to
infer a minimal, high-quality grammar from the
data.
3 Model
Our model is based on similar assumptions as the
original Hiero system. We assume that the bitext has
been word aligned, and that we can use that word
alignment to extract phrase pairs.
Given the word alignments and the heuristically
extracted phrase pairs Rp, our goal is to extract the
minimal set of hierarchical rules Rg that would best
explain Rp. This is achieved by inferring a distribu-
tion over the derivations for each phrase pair, where
the set of derivations collectively specify the gram-
mar. In the following, we denote the sequence of
derivations for the set of phrase pairs by r, which is
composed of grammar rules r. We will essentially
read off our learned grammar from the sequence of
derivations r.
Our non-parametric model reasons over the space
of the (hierarchical and terminal) rules and sam-
ples a set of rules by employing a prior based on
the alignment probability of the words in the phrase
pairs. We hypothesize that the resulting grammar
will be compact and also will explain the phrase
pairs better (the SCFG rules will maximize the like-
lihood of producing the entire set of observed phrase
pairs).
Using Bayes? rule, the posterior over the deriva-
tions r given the phrase pairs Rp can be written as:
P (r|Rp) ? P (Rp|r)P (r) (1)
where P (Rp|r) is equal to one when the sequence
of rules r and phrase-pairs Rp are consistent, i.e. r
can be partitioned into derivations to compose the
set of phrase-pairs such that the derivations respect
534
the given word alignments; otherwise P (Rp|r) is
zero. The overall structure of the model is analo-
gous to the Bayesian model for inducing Tree Sub-
stitution Grammars proposed by Cohn et al (2009).
Note that, our model extracts hierarchical rules for
the word-aligned phrase pairs and not for the sen-
tences.
Similar to the other Hiero-style systems, we use
two types of rules: terminal and hierarchical rules.
For each phrase-pair, our model either generates a
terminal rule by not segmenting the phrase-pair, or
decides to segment the phrase-pair and extract some
rules.
Though it is possible to segment phrase-pairs by
two (or more) non-overlapping spans, we propose
a simpler model in this paper and restrict the hierar-
chical rules to contain only one non-terminal (unlike
the case of classic Hiero-style grammars containing
two non-terminals). This simpler model, samples
the space of derivations and identifies a sub-span
for introducing the non-terminal, which can be ex-
pressed as terminal rules (it is not decomposed fur-
ther). Figure 1 shows an example phrase-pair with
the Viterbi-best word alignment and Figure 2 shows
two possible derivations for the same phrase-pair
with the non-terminals introduced at different sub-
spans. It can be seen that the sub-phrase correspond-
ing to the non-terminal spanX1 is directly written as
a terminal rule and is not decomposed further.
While the resulting model is slightly weaker than
the original Hiero grammar, it should be noted our
simpler model does allow reordering and discontigu-
ous alignments. For example our model includes
rules such as, X ? (?X1?, ????X1), which can
capture phrases like (not X1, ne X1 pas) in the case
of English-French translation. In terms of the re-
ordering, our model lies in between the hierarchi-
cal phrase-based and phrase-based models. To sum-
marize, the segmentation of each phrase-pair in our
model results in two rules: a hierarchical rule with
one nonterminal as well as a terminal rule.
More specifically, the generative process for gen-
erating a phrase pair x from the grammar rules
may have two steps as follows. In the first step,
the model decides on the type of the rule tx ?
{TERMINAL,HIERARCHICAL} used to generate the
phrase-pair based on a Bernoulli distribution, having
a prior ? coming from a Beta distribution:
tx ? Bernoulli(?)
? ? Beta(lx, 0.5)
The lexical alignment probability lx controls the
tendency for extracting hierarchical rules from the
phrase-pair x. For a given phrase-pair, lx is com-
puted by taking the (geometric or arithmetic) aver-
age of the reverse and forward alignment probabil-
ities, which we explain later in this section. Inte-
grating out ? gives us the conditional probabilities
of choosing the rule type tx as:
p(tterm|x) ? n
x
term + lx (2)
p(thier|x) ? n
x
hier + 0.5 (3)
where nxterm and n
x
hier denote the number of termi-
nal or hierarchical rules, among the rules extracted
so far from the phrase-pair x during the sampling.
In the second step, if the rule type tx =
HIERARCHICAL, the model generates the phrase-
pair by sampling from the hierarchical and terminal
rules. We use a Dirichlet Process (DP) to model the
generation of hierarchical rules r:
G ? DP (?h, P0(r))
r ? G
Integrating out the grammar G, the predictive dis-
tribution of a hierarchical rule rx for generating the
current phrase-pair (conditioned on the rules from
the rest of the phrase-pairs) is:
p(rx|r
?x, ?h, P0) ? n
?x
rx + ?hP0(rx) (4)
where n?xrx is the count of the rule rx in the rest of
the phrase-pairs that is represented by r?x, P0 is the
base measure, and ?h is the concentration parameter
controlling the model?s preference towards using an
existing hierarchical rule from the cache or to create
a new rule sanctioned by the base distribution. We
use the lexical alignment probabilities of the compo-
nent rules as our base measure P0:
P0(r) =
[( ?
(k,l)?a
p(el|fk)
) 1
|a|
( ?
(k,l)?a
p(fk|el)
) 1
|a|
] 1
2
(5)
535
octavo y noveno Fondos Europeos de Desarrollo para el ejercicio
Eighth and Ninth European Development Funds for the financial year
Figure 1: An example phrase-pair with Viterbi alignments
X ? (Eighth and Ninth X1 for the financial year, octavo y noveno X1 para el ejercicio)
X ? (European Development Funds, Fondos Europeos de Desarrollo)
X ? (Eighth and Ninth X1, octavo y noveno X1)
X ? (European Development Funds for the financial year,
Fondos Europeos de Desarrollo para el ejercicio)
Figure 2: Two possible derivations of the phrase-pair in Figure 1
where a is the set of alignments in the given sub-
span; if the sub-span has multiple Viterbi alignments
from different phrase-pairs, we consider the union of
all such alignments. DeNero et al (2008) use a sim-
ilar prior- geometric mean of the forward and reverse
IBM-1 alignments. However, we use the product of
geometric means of the forward and reverse align-
ment scores. We also experimented with the arith-
metic mean of the lexical alignment probabilities.
The lexical prior lx in the first step can be defined
similarly. We found the particular combination of,
?arithmetic mean? for the lexical prior lx (in the first
step) and ?geometric mean? for the base distribution
P0 (in the second step) to work better, as we discuss
later in Section 5.
Assuming the heuristically extracted phrase pairs
to be the input to our inference algorithm, our
approach samples the space of rules to find the
best possible segmentation for the sentences as de-
fined by the cache and base distribution. We ex-
plore a subset of the space of rules being consid-
ered by (Blunsom et al, 2009) ? i.e., only those
rules satisfying the word alignments and heuristi-
cally grown phrase alignments.
4 Inference
We train our model by using a Gibbs sampler ? a
Markov Chain Monte Carlo (MCMC) method for
sampling one variable in the model, conditional to
the other variables. The sampling procedure is re-
peated for what is called a long Gibbs chain span-
ning several iterations, while the counts are collected
at fixed thin intervals in the chain. As is common in
the MCMC procedures, we ignore samples from a
fixed number of initial burn-in iterations, allowing
the model to move away from the initial bias. The
rules in the final sampler state at the end of the Gibbs
chain along with their counts averaged by the num-
ber of thin iterations become our translation model.
In our model, a sample for a given phrase pair
corresponds either to its terminal derivation or two
rules in a hierarchical derivation. The model sam-
ples a derivation from the space of derivations that
are consistent with the word alignments. In order
to achieve this, we need an efficient way to enumer-
ate the derivations for a phrase pair such that they
are consistent with the alignments. We use the lin-
ear time algorithm to maximally decompose a word-
aligned phrase pair, so as to encode it as a compact
alignment tree (Zhang et al, 2008).
f0 f1 f2 f3 f4
e0 e1 e2 e3 e4 e5
Figure 3: Example phrase pair with alignments.
536
For a phrase-pair with a given alignment as shown
in Figure 3, Zhang et al (2008) generalize theO(n+
K) time algorithm for computing all K common in-
tervals of two different permutations of length n.
The contiguous blocks of the alignment are cap-
tured as the nodes in the alignment tree and the tree
structure for the example phrase pair in Figure 3 is
shown in Figure 4. The italicized nodes form a left-
branching chain in the alignment tree and the sub-
spans of this chain also lead to alignment nodes that
are not explicitly captured in the tree (Please refer
to Zhang et al (2008) for details). In our work, each
node in the tree (and also each sub-span in the left-
branching chain) corresponds to an aligned source-
target sub-span within the phrase-pair, and is a po-
tential site for introducing the non-terminal X to
generate hierarchical rules.
Given this alignment tree for a phrase pair, a
derivation can be obtained by introducing a non-
terminal at some node nd in the tree and re-writing
the span rooted at nd as a separate rule. As men-
tioned earlier, we compute the derivation probability
as a product of the probabilities of the component
rules, which are computed using the Equation 4.
We initialize the sampler by using our lexical
alignment prior and sampling from the distribution
of derivations as suggested by the priors. We found
this to perform better in practice, than a naive sam-
pler without an initializer.
At each iteration, the Gibbs sampler processes the
phrase pairs in random order. For each phrase pair
Rp, it visits the nodes in the corresponding align-
ment tree and computes the posterior probability of
the derivations and samples from this posterior dis-
tribution. To speedup the sampling, we store the
pre-computed alignment tree for the phrase pairs and
just recompute the derivation probabilities based on
the sampler state at every iteration. While the sam-
pler state is updated with the counts at each iteration,
we accumulate the counts only at fixed intervals in
the Gibbs chain. In applying the model for decoding,
we use the grammar from the final sampler state.
Since our model includes only one hyperparam-
eter ?h, we tune its value manually by empirically
experimenting on a small set of initial phrase pairs.
We keep for future work the task of automatically
tuning for hyper-parameter values by sampling.
([0,5],[0,4])
([0,2],[0,2])
([0,1],[0,1])
([0,0],[0,0]) ([1,1],[1,1])
([2,2],[2,2])
([4,5],[3,4])
Figure 4: Decomposed alignment tree for the example
alignment in Fig. 3.
5 Experiments
We use the English-Spanish data from WMT-10
shared task for the experiments to evaluate the effec-
tiveness of our Bayesian rule extraction approach.
We used the entire shared task training set except
the UN data for training translation model and the
language model was trained with the same set and
an additional 2 million sentences from the UN data,
using SRILM toolkit with Knesser-Ney discounting.
We tuned the feature weights on the WMT-10 dev-
set using MERT (Och, 2003) and evaluate on the
test set by computing lower-cased BLEU score (Pa-
pineni et al, 2002) using the WMT-10 standard eval-
uation script.
We use Kriya ? an in-house implementation of hi-
erarchical phrase-based translation written predom-
inantly in Python. Kriya supports the entire transla-
tion pipeline of SCFG rule extraction and decoding
with cube pruning (Huang and Chiang, 2007) and
LM integration (Chiang, 2007). We use the 7 fea-
tures (4 translation model features, extracted rules
penalty, word penalty and language model) as is typ-
ical in Hiero-style systems. For tuning the feature
weights, we have adapted the MERT implementa-
tion in Moses1 for use with Kriya as the decoder.
We started by training and evaluating the two
baseline systems using i) two non-terminals and
ii) one non-terminal, which were trained using the
conventional heuristic extraction approach. For the
baseline with one non-terminal, we modified the
heuristic rule extraction algorithm appropriately2.
1www.statmt.org/moses/
2Given an initial phrase pair, the algorithm would introduce
a non-terminal for each sub-span consistent with the alignments
and extract rules corresponding to each sub-span. The con-
537
Experiment
# of rules filtered
for devset
(in millions)
BLEU
Baseline (w/ 2 non-terminals) 52.36 27.45
Baseline (w/ 1 non-terminal) 22.09 26.71
Pattern-based filtering? 18.78 24.61
1 non-terminal; monotone & non-monotone 10.36 24.17
1 non-terminal; non-monotone 3.62 23.99
Table 1: Kriya: Baseline and Filtering experiments. ?: This is the initial rule set used in Iglesias et al (2009) obtained
by greedy filtering. Rows 4 and 5 represents the filtering that uses single non-terminal rules with row 4 allowing
monotone rules in addition to the non-monotone (reordering) rules.
As part of the baseline methods to be applied to min-
imize the number of SCFG rules, We also wanted to
assess the effect of a simpler rule filtering, where
the idea is to filter the heuristically extracted rules
based on certain patterns. Our first baseline filtering
strategy uses the heuristic methods in Iglesias et al
(2009) in order to minimize the number of rules3.
For the other baseline filtering experiments, we re-
tained only one non-terminal rules and then further
limited it by retaining only non-monotone one non-
terminal rules; in both cases the terminal rules were
retained.
Table 1 shows the results for baseline and the rule
filtering experiments. Restricting rule extraction to
just one non-terminal doesn?t affect the BLEU score
significantly and this justifies the simpler model
used in this paper. Secondly, we find significant re-
duction in the BLEU for the pattern-based filtering
strategy and this is because we only use the initial
rule set obtained by greedy filtering without aug-
menting it with other specific patterns. The other
two filtering methods reduced the BLEU further but
not significantly. The second column in the table
gives the number of SCFG rules filtered for the dev-
set, which is typically much less than the full set of
rules. We later use this to put in perspective the
effective reduction in the model size achieved by
our Bayesian model. We can ideally compare our
Bayesian rule extraction using Gibbs sampling with
straints relating to two non-terminals (such as, no adjacent non-
terminals in source side) does not apply for the one non-terminal
case.
3It should be noted that we didn?t use the augmentations to
the initial rule set (Iglesias et al, 2009) and our objective is to
find the impact of the filtering approaches.
the baselines and the filtering approaches. However,
running our Gibbs sampler on the full set of phrase
pairs demand sampling to be distributed, possibly
with approximation (?; ?), which we reserve for our
future work.
In this work, we focus on evaluating our Gibbs
sampler on reasonable sized set of phrase pairs with
corresponding baselines. We filter the initial phrase
pairs based on their frequency using three different
thresholds, viz. 20, 10 and 3- resulting in smaller
sets of initial phrase pairs because we throw out in-
frequent phrase pairs (the threshold-20 case is the
smallest initial set of phrase pairs). This allows us
to run our sampler as a stand-alone instance for the
three sets, obviating the need for distributed sam-
pling.
Table 2 shows the number of unique phrase pairs
in each set. While, the filtering reduces the number
of phrase pairs to a small fraction of the total phrase
pairs, it also increases the unknown words (OOV)
in the test set by a factor between 1.8 and 3. In or-
der to address this issue due to the OOV words, we
additionally added non-decomposable phrase pairs
having just one word at either source or target side,
Phrase-pairs set
# of Unique
phrase-pairs
Testset
OOV
All phrase-pairs 110782174 1136
Threshold-20 292336 3735
Threshold-10 606590 3056
Threshold-3 2689855 2067
Table 2: Phrase-pair statistics for different frequency
threshold
538
Experiment Threshold-20 Threshold-10 Threshold-3
Baseline (w/ 2 non-terminals) 24.30 25.96 26.34
Baseline (w/ 1 non-terminal) 24.00 25.90 26.83
Bayesian rule extraction 23.39 24.30 25.22
Table 3: BLEU scores: Heuristic vs Bayesian rule extraction
Experiment Rules Extracted (in millions) Reduction
Heuristic (1 nt) Bayesian
Threshold-20 1.93 (0.117) 1.86 (0.07) 3.57 (38.34)
Threshold-10 2.91 (1.09) 2.10 (0.28) 27.7 (73.95)
Threshold-3 7.46 (5.64) 2.45 (0.71) 67.17 (87.28)
Table 4: Model compression: Heuristic vs Bayesian rule extraction
Priors ?h BLEU
Arith + Arith means 0.5 22.46
Arith + Geom means 0.5 23.39
Geom + Arith means 0.5 22.96
Arith + Geom means 0.5 22.83
Arith + Geom means 0.1 22.88
Arith + Geom means 0.2 22.97
Arith + Geom means 0.3 22.98
Arith + Geom means 0.4 22.69
Arith + Geom means 0.5 23.39
Arith + Geom means 0.6 22.89
Arith + Geom means 0.7 22.82
Arith + Geom means 0.8 22.82
Arith + Geom means 0.9 22.67
Table 5: Effect of different priors and ?h on Threshold-
20 set. The two priors correspond to the lexical prior lx
in the first step and the base distribution P0 in the second
step.
as coverage rules. The coverage rules (about 1.8
million) were added separately to the SCFG rules
induced by both heuristic algorithm and Gibbs sam-
pler. This is justified because we only add the rules
that can not be decomposed further by both rule ex-
traction approaches. However, note that both ap-
proaches can independently induce rules that over-
lap with the coverage rules set and in such cases we
simply add the original corpus count to the counts
returned by the respective rule extraction method.
The Gibbs sampler considers the phrase pairs in
random order at each iteration and induces SCFG
rules by sampling a derivation for each phrase pair.
Given a phrase pair x with raw corpus frequency fx,
we simply scale the count for its sampled deriva-
tion r by its frequency fx. Alternately, we also ex-
perimented with independently sampling for each
instance of the phrase pair and found their perfor-
mances to be comparable. Sampling phrase pairs
once and then scaling the sampled derivation, help
us to speed up the sampling process. In our experi-
ments, we ran the Gibbs sampler for 2000 iterations
with a burn-in period of 200, collecting counts every
50 iterations. We set the concentration parameter ?h
to be 0.5 based on our experiments detailed later in
this section.
The BLEU scores for the SCFG learned from the
Gibbs sampler are shown in Table 3. We first note
that, the threshold-20 set has lower baseline BLEU
than threshold-10 and threshold-3 sets, as can be ex-
pected because threshold-20 set uses a much smaller
subset of the full set of phrase pairs to extract hier-
archical rules. The Bayesian approach results in a
maximum BLEU score reduction of 1.6 for the sets
using thresholds 10 and 3, compared to the one non-
terminal baseline. The two non-terminal baseline is
also provided to place our results in perspective.
Table 4 shows the model size, including the cov-
erage rules for the two rule extraction approaches.
The number of extracted rules, excluding the cov-
erage rules are shown within the parenthesis. The
last column shows the reduction in the model size
for both with and without the coverage rules; yield-
ing a maximum absolute reduction of 67.17% for the
539
threshold-3 phrase pairs set. It can be seen that the
number of rules are far fewer than the rules extracted
using the baseline heuristic methods for filtering de-
tailed in Table 1. Interestingly, we obtain a smaller
model size, even as we decrease the threshold to in-
clude more initial phrase pairs used as input to the
inference procedure, e.g. a 67.17% reduction over
the rules extracted from the threshold-3 phrase pairs
v.s. a 27.7% reduction for threshold-10.
These results show that our model is capable of
extracting high-value Hiero-style SCFG rules, albeit
with a reduction in the BLEU score. However, our
current approach offers scope for improvement in
several avenues, for example we can use annealing
to perturb the initial sampling iterations to encour-
age the Gibbs sampler to explore several derivations
for each phrase pair. Though this might result in
slightly large models than the current ones, we still
expect substantial reduction than the original Hiero
rule extraction. In future, we also plan to sample the
hyperparameter ?h, instead of using a fixed value.
Table 5 shows the effect of different values of
the concentration parameter ?h and the priors used
in the model. The order of priors in each setting
correspond to the prior used in deciding the rule-
type and identifying the non-terminal span for sam-
pling a derivation. We found the geometric mean to
work better in both cases. We further found that the
concentration parameter ?h value 0.5 gives the best
BLEU score.
6 Conclusion and Future Work
We proposed a novel method for extracting mini-
mal set of hierarchical rules using non-parametric
Bayesian framework. We demonstrated substantial
reduction in the size of extracted grammar with the
best case reduction of 67.17%, as compared to the
heuristic approach, albeit with a slight reduction in
the BLEU scores.
We plan to extend our model to handle two non-
terminals to allow for better reordering. We also
plan to run our sampler on the full set of phrase
pairs using distributed sampling and our prelimi-
nary results in this direction are encouraging. Fi-
nally, we would like to directly sample from the
Viterbi aligned sentence pairs instead of relying on
the heuristically extracted phrase pairs. This can
be accomplished by using a model that is closer
to the Tree Substitution Grammar induction model
in (Cohn et al, 2009) but in our case the model
would infer a Hiero-style SCFG from word-aligned
sentence pairs.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of Neural Information Processing Systems-
08.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of Association of
Computational Linguistics-09, pages 782?790. Asso-
ciation for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proceedings of Human Language Tech-
nologies: North American Chapter of the Association
for Computational Linguistics-09, pages 548?556. As-
sociation for Computational Linguistics.
Adria` de Gispert, Juan Pino, and William Byrne. 2010.
Hierarchical phrase-based translation grammars ex-
tracted from alignment posterior probabilities. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 545?554.
Association for Computational Linguistics.
John DeNero, Alexandre Bouchard-Cote, and Klein Dan.
2008. Sampling alignment structure under a bayesian
translation model. In In Proceedings of Empirical
Methods in Natural Language Processing-08, pages
314?323. Association for Computational Linguistics.
Zhongjun He, Yao Meng, and Hao Yu. 2009. Discarding
monotone composed rule for hierarchical phrase-based
statistical machine translation. In Proceedings of the
3rd International Universal Communication Sympo-
sium, pages 25?29. ACM.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151.
Association for Computational Linguistics.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern for
efficient hierarchical translation. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 380?388. Association for Com-
putational Linguistics.
540
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In In Proceedings of Empirical Methods in Natu-
ral Language Processing-02, pages 133?139. Associ-
ation for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In In Proceedings of
Association of Computational Linguistics, pages 311?
318. Association for Computational Linguistics.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING) - Volume 1, pages 1081?1088. As-
sociation for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING) - Vol-
ume 1, pages 1145?1152. Association for Computa-
tional Linguistics.
541
