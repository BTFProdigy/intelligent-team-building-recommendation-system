A Probabilistic Genre-Independent Model of Pronominalization 
Michael Strube 
European Media Laboratory GmbH 
Villa Bosch 
Schlol3-Wolfsbrunnenweg 33 
69118 Heidelberg, Germany 
Michael. St rube@eml .vil la-bosch. de 
Maria Wolters 
Inst. f. Kommunikationsforschung u. Phonetik 
Universitiit Bonn 
Poppelsdorfer Allee 47 
53115 Bonn, Germany 
wolters@ikp.uni -bonn.de 
Abstract 
Our aim in this paper is to identify genre- 
independent factors that influence the decision to 
pronominalize. Results based on the annotation of 
twelve texts from four genres show that only a few 
factors have a strong influence on pronominaliza- 
tion across genres, i.e. distance from last mention, 
agreement, and form of the antecedent. Finally, we 
describe aprobabilistic model of pronominalization 
derived from our data. 
1 Introduction 
Generating adequate referring expressions i an ac- 
tive research topic in Natural Language Generation. 
Adequate referring expressions are those that en- 
able the user to quickly and unambiguously identify 
the discourse ntity that the expression co-specifies 
with. In this paper, we concentrate on an important 
aspect of that question, which has received less at- 
tention than the question of anaphora resolution in 
discourse interpretation, i.e., when is it feasible to 
pronominalize? 
Our aim is to identify the central factors that in- 
fluence pronominalization across genres. Section 2 
motivates and presents the factors that were investi- 
gated in this study: distance from last mention, par- 
allelism, ambiguity, syntactic function, agreement, 
sortal class, syntactic function of the antecedent and 
form of the antecedent. Our analyses are based on 
a corpus of twelve texts from four different genres 
with a total of more than 24,000 words and 7126 
referring expressions (Section 3). The results of 
the statistical analyses are summarized in Section 
4. There are strong statistical associations between 
each of the factors and pronominalization. Only 
when we combine them into a probabilistic model 
we can identify those factors whose contribution 
is really important, i.e. distance from last mention, 
agreement, and to a certain degree form of the an- 
tecedent. Since these factors can be annotated tel- 
atively cheaply, we conclude that it is possible to 
develop reasonable statistical pronominalization al- 
gorithms. 
2 Factors in Pronoun Generation 
2.1 Previous Work 
Lately, a number of researchers have done corpus- 
based work on NP generation and pronoun resolu- 
tion, and a number of studies have found differences 
in the frequency of both personal and demonstrative 
pronouns across genres. However, none of these 
studies compares the influence of different factors 
on pronoun generation across genres. 
Recently, Poesio et al (1999) have described a
corpus-based approach to statistical NP generation. 
While they ask the same question as previous re- 
searchers (e.g. Dale (1992)), their methods differ 
from traditional work on NP generation. Poesio 
et al (1999) use two kinds of factors: (1) factors 
related to the NP under consideration such as agree- 
ment information, semantic factors, and discourse 
factors, and (2) factors related to the antecedent, 
such as animacy, clause type, thematic role, proxim- 
ity, etc. Poesio et al (1999) report hat they were not 
able to annotate many of these factors reliably. On 
the basis of these annotations, they constructed de- 
cision trees for predicting surface forms of referring 
expressions based on these factors - with good re- 
sults: all 28 personal pronouns in their corpus were 
generated correctly. Unfortunately, they do not eval- 
uate the contribution of each of these factors, so we 
do not know which ones are important. 
Work on corpus-based approaches to anaphora 
resolution is more numerous. Ge et al (1998) 
describe a supervised probabilistic pronoun resolu- 
tion algorithm which is based on complete syntac- 
tic information. The factors they use include dis- 
tance from last mention, syntactic function and con- 
text, agreement information, animacy of the refer- 
ent, a simplified notion of selectional restrictions, 
18 
Agree 
Syn 
Class 
SynAnte 
FormAnte 
Dist 
Dist4 
Par 
Ambig 
Agreement in person, gender, and number 
Syntactic function 
Sortal Class (cf. Tab. 2) 
Syntactic function of antecedent. 
"F" for first mention, "N" for deadend 
Form of antecedent (pers. pron., poss. 
pron., def. NP, indef. NP, proper name) 
Distance to last mention in units 
Dist reduced to 4 values (deadend, 
Dist=0, Dist= 1, Dist>=2) 
Parallelism (Syn=SynAnte) 
Number of competing discourse ntities 
Table 1: Overview of factors 
and the length of the coreference chain. Cardie & 
Wagstaff (1999) describe an unsupervised algorithm 
for noun phrase coreference resolution. Their fac- 
tors are taken from Ge et al (1998), with two excep- 
tions. First, they replace complete syntactic infor- 
mation with information about NP bracketing. Sec- 
ond, they use the sortal class of the referent which 
they determine on the basis of WordNet (Fellbaum, 
1998). 
There has been no comparison between corpus- 
based approaches for anaphora resolution and more 
traditional algorithms based on focusing (Sidner, 
1983) or centering (Grosz et al, 1995) except for 
Azzam et al (1998). However, their comparison 
is flawed by evaluating a syntax-based focus algo- 
rithm on the basis of insufficient syntactic informa- 
tion. For pronoun generation, the original centering 
model (Grosz et al, 1995) provides a rule which is 
supposed to decide whether a referring expression 
has to be realized as a pronoun. However, this rule 
applies only to the referring expression which is the 
backward-looking center (Cb) of the current utter- 
ance. With respect o all other referring expression 
in this utterance centering is underspecified. 
Yeh & Mellish (1997) propose a set of hand- 
crafted rules for the generation of anaphora (zero 
and personal pronouns, full NPs) in Chinese. How- 
ever, the factors which appear to be important in 
their evaluation are similar to factors described 
by authors mentioned above: distance, syntactic 
constraints on zero pronouns, discourse structure, 
salience and animacy of discourse ntities. 
2.2 Our Factors 
The factors we investigate in this paper only rely on 
annotations of NPs and their co-specification rela- 
tions. We did not add any discourse structural anno- 
tation, because (1) the texts are extracts from larger 
texts which are not available to us, and (2) we have 
not yet found a labelling scheme for discourse struc- 
ture that has an inter-coder reliability comparable to 
the MUC coreference annotation scheme. 
Based on our review of the literature and relevant 
work in linguistics (for sortal class, mainly Fraurud 
(1996) and Fellbaum (1998)), we have chosen the 
nine factors listed in Table 1. Methodologically, we 
distinguish two kinds of factors: 
NP-level factors are independent from co- 
specification relations. They depend on the 
semantics of the discourse entity or on discourse 
information supplied for the NP generation algo- 
rithm by the NLG system. Typical examples are 
NP agreement by gender, number, person and case, 
the syntactic function of the NP (subject, object, 
PP adjunct, other), the sortal class of the discourse 
entity to which an NP refers, discourse structure, or 
topicality of the discourse ntities. In this paper, we 
focus on the first three factors, agreement (Agree), 
syntactic function (Syn), and sortal class (Class). 
Since we are using syntactically annotated ata 
in the Penn Treebank-II format, the syntactic func- 
tion of an NP was derived from these annotations. 
Agreement for gender, number, and person was la- 
belled by hand. Since English has almost no nomi- 
nal case morphemes, case was not annotated. 
Sortal classes provide information about the dis- 
course entity that a referring expression evokes or 
accesses. The classes, summarized in Table 2, were 
derived from EuroWordNet BaseTypes (Vossen, 
1998) and are defined extensionally on the basis 
of WordNet synsets. Their selection was motivated 
by two main considerations: all classes hould oc- 
cur in all genres, and the number of classes hould 
be as small as possible in order to avoid problems 
with sparse data. Four classes, State, Event, Action, 
and Property, cover different types of situations, 
two cover spatiotemporal characteristics of situa- 
tions (Loc/Time). The four remaining classes cover 
the two dimensions "concrete vs. abstract (Con- 
cept)" and "human (Pers) vs. non-human (PhysObj) 
vs. institutionalised groups of humans (Group)". 
Since we are only interested in the decision 
whether to employ pronouns rather than full NPs 
and less in the form of the NP itself, and since our 
methodology is based on corpus annotation, we did 
not take into account more formal semantic ate- 
gories such as kinds vs. individuals. 
Co-specification-level factors depend on infor- 
mation about sequences of referring expressions 
19 
Person 
Group 
PhysObj 
Concept 
Loc 
Time 
Event 
Action 
State 
Property 
one or more human beings 
institutionalized group of human beings 
physical object 
abstract oncept 
geographical location 
date, time span 
sth. which takes place in space and time 
sth. which is done 
state of affairs, feeling . . . .  
characteristic orattribute of sth. 
Table 2: Overview of Sortal Classes with rough 
characterizations of relevant synsets 
which co-specify with each other. Such a sequence 
consists of all referring expressions that evoke or ac- 
cess the same discourse ntity. In this paper, we use 
the following factors from the literature: distance 
to last mention (Dist and Dist4), ambiguity (Am- 
big), parallelism (Par), form of the antecedent (For- 
mAnte), and syntactic function of the antecedent 
(SynAnte). We also distinguish between discourse 
entities that are only evoked once, deadend entities, 
and entities that are accessed repeatedly. 
Parallelism is defined on the basis of syntactic 
function: a referring expression and its antecedent 
are parallel if they have the same syntactic function. 
For calculating distance and ambiguity, we seg- 
mented the texts into major clause units (MCUs). 
Each MCU consists of a major clause C plus 
any subordinate clauses and any coordinated major 
clauses whose subject is the same as that of C and 
where that subject has been elided. 
Dist provides the number of MCUs between the 
current and the last previous mention of a discourse 
entity. When an entity is evoked for the first time, 
Dist is set to "D". Dist4 is derived from Dist by as- 
signing the fixed distance 2 to all referring expres- 
sions whose antecedent is more than 1 MCU away. 
Ambiguity is defined as the number of all discourse 
entities with the same agreement features that occur 
in the previous unit or in the same unit before the 
current referring expression. 
3 Data 
Our data consisted of twelve (plus two) texts from 
the Brown corpus and the corresponding part-of- 
speech and syntactic annotations from the Penn 
Treebank (LDC, 1995). The texts were selected 
because they contained relatively little or no direct 
speech; segments of direct speech pose problems for 
both pronoun resolution and generation because of 
the change in point of view. Morpho-syntactic in- 
formation such as markables, part-of-speech labels, 
grammatical role labels, and form of referring ex- 
pression were automatically extracted from the ex- 
isting Treebank annotations. 
The texts come from four different genres: Popu- 
lar Lore (CF), Belles Lettres (CG), Fiction/General 
(CK), and Fiction/Mystery (CL). The choice of 
genres was dictated by the availability of detailed 
Treebank-II parses. Table 3 shows that the distri- 
bution of referring expressions differs considerably 
between genres. 
The texts from the two non-narrative types, CF 
and CG, contain far more discourse ntities and 
far less pronouns than the narrative genres CK and 
CL. The high number of pronouns in CK and CL 
is partly due to the fact that in one text from each 
genre, we have a first person singular narrator. CK 
patterns with CF and CG in the average number 
of MCUs; the sentences in the sample from mys- 
tery fiction are shorter and arguably less complex. 
CL also has disproportionally few deadend refer- 
ents. The high percentage of deadend referents in 
CK is due to the fact that two of the texts deal with 
relationship between two people. These four dis- 
course referents account for the 4 longest corefer- 
ence chains in CK (85, 96, 109, and 127 mentions). 
Two annotators (the authors, both trained lin- 
guists), hand-labeled the texts with co-specification 
information based on the specifications for the Mes- 
sage Understanding Coreference task (Hirschman 
& Chinchor (1997); for theoretical reasons, we did 
not mark reflexive pronouns and appositives as co- 
specifying). The MCUs were labelled by the sec- 
ond author. All referring expressions were anno- 
tated with agreement and sortal class information. 
Labels were placed using the GUI-based annotation 
tool REFEREE (DeCristofaro et al, 1999). 
The annotators developed the Sortal Class anno- 
tation guidelines on the basis of two training texts. 
Then, both labellers annotated two texts from each 
genre independently (eight in total). These eight 
texts were used to determine the reliability of the 
sortal class coding scheme. Since sortal class an- 
notation is intrinsically hard, the annotators looked 
up the senses of the head noun of each referring NP 
that was not a pronoun or a proper name in Word- 
Net. Each sense was mapped irectly to one or more 
of the ten classes given in Table 2. The annotators 
then chose the adequate sense. 
The reliability of the annotations were measured 
9111 20
Genre 
CF 
CG 
CK 
CL 
words ref. expr. entities sequ.. MCUs % pron. %deadend med. len. 
6097 1725 1223 125 304 19.59% (1.8%, 0.3%, 58.3%) 89.78% 3 
6103 1707 1290 120 269 16.17% (9.8%, 1.1%, 4%) 90.70% 2 
6020 1848 1071 113 386 36.15% (19.5%, 1.2%, 56.1%) 89.45% 2 
6018 1846 954 170 477 35.64% (14.0%, 1.5%, 53.6%) 80.09% 4 
Table 3: Relevant quantitative characteristics of the texts. Average length: 2020 words, 120 MCUs. sequ.: 
number of sequences of co-specifying referring expressions. % deadend: percentage of discourse ntities 
mentioned only once. % pronouns: percentage ofall referring expressions realized as pronouns, in brackets: 
perc. of first person singular pronouns, perc. of second person singular pronouns, perc. of third person 
singular masculine and feminine pronouns, reed. len.: median length of sequences ofco-specifying referring 
expressions 
with Cohen's n (Cohen, 1960; Carletta, 1996). Co- 
hen (1960) shows that a n between 0.68 and 0.80 al- 
lows tentative conclusions, while e; > 0.80 indicates 
reliable annotations. For genres CF (n = 0.83), CK 
(n = 0.84) and CL (n = 0.83), the sortal class an- 
notations were indeed reliable, but not for genre CG 
(n = 0.63). Nevertheless, overall, the sortal class 
annotations were reliable (n ---- 0.8). Problems are 
mainly due to the abstract classes Concept, Action, 
Event, State, and Property. Abstract head nouns 
sometimes have several senses that fit the context 
almost equally well, but that lead to different sor- 
tal classes. Another problem is metaphorical usage. 
This explains the bad results for CG, which features 
many abstract discourse ntities. 
4 Towards a Probabilistic 
Genre- Independent  Model  
In this section, we investigate owhat extent the fac- 
tors proposed in section 2.2 influence the decision to 
prominalize. For the purpose of the statistical naly- 
sis, pronominalization is modelled by a feature Pro. 
For a given referring expression, that feature has the 
value "P" if the referring expression is a personal 
or a possessive pronoun, else "N". We model this 
variable with a binomial distribution. I 
4.1 How do the Factors Affect 
Pronominalization? 
First, we examine for all nine factors if there is a 
statistical ssociation between these factors and Pro. 
Standard non-parametric tests how a strong associ- 
ation between all nine factors and Pro. 2 This holds 
~For all statistical calculations and for the logistic regres- 
sion analyses reported below, we used R (Ihaka & Gentleman, 
1996). 
2We used the KruskaI-Wallis test for the ordinal Ambig 
variable and the X2-test for the other, nominal, variables. Since 
first mentions and deadends are coded by the character "D" in 
both for all referring expressions and for those that 
occur in sequences of co-specifying referring ex- 
pressions. All of the tests were significant at the 
p < 0.001-level, with the exception of Par: for ex- 
pressions that are part of co-specification sequences 
the effect of that factor is not significant. 
In the next analysis tep, we determine which of 
the feature values are associated isproportionally 
often with pronouns, and which values tend to be 
associated with full NPs. More specifically, we test 
for each feature-value pair if the pronominalization 
probability is significantly higher or lower than that 
computed over (a) the complete data set, (b) all re- 
ferring expressions in sequences of co-specifying 
referring expressions, (c) all third person referring 
expressions in sequences. Almost all feature values 
show highly significant effects for (a) and (b), but 
some of these effects vanish in condition (c). Be- 
low, we report on associations which are significant 
at p < 0.001 under all three conditions. 
Unsurprisingly, there is a strong effect of agree- 
ment values: NPs referring to the first and second 
person are always pronominalized, and third person 
masculine or feminine NPs, which can refer to per- 
sons, are pronominalized more frequently than third 
person neuter and third person plural. Pronouns are 
strongly preferred if the distance to the antecedent is 
0 or 1 MCUs. Referring expressions are more likely 
to be pronominalized in subject position than as a 
PP adjunct, and referring expressions with adjuncts 
as antecedents are also pronominalized less often 
than those with antecedents in subject or object po- 
sition. There is a clear preference for pronouns as 
possessive determiners, and referring expressions 
that co-specify with an antecedent possessive pro- 
noun are highly likely to be pronominalised. We 
both Dist and Dist4, both are treated as a categorical variable 
by R. For more on these tests, see (Agresti, 1990). 
"1,1 21
also notice strong genre-independent ffects of par- 
allelism. Although at first glance, Ambig appears to 
have a significant effect as well, (median ambiguity 
for nouns is 3, median ambiguity for pronouns 0), 
closer inspection reveals that this is mainly due to 
first and second person and third person masculine 
and feminine pronouns. 
The sortal classes show a number of interest- 
ing patterns (cf. Table 4). Not only do the classes 
differ in the percentage of deadend entities, there 
are also marked differences in pronominalizabil- 
ity. There appear to be three groups of sortal 
classes: Person/Group, with the lowest rate of dead- 
end entities and the highest percentage of pro- 
nouns - not only due to the first and second per- 
son personal pronouns- ,  Location/PhysObj, with 
roughly two thirds of all entities not in sequences 
and a significantly lower pronominalization rate, 
and Concept/Action/Event/Property/State/Concept, 
with over 80% deadend entities. Within this group, 
Action, Event, and Concept are pronominalized 
more frequently than State and Property. Time is the 
least frequently pronominalized class. An impor- 
tant reason for the difference between Loc and Time 
might be that Times are almost always referred back 
to by temporal adverbs, while locations, especially 
towns and countries, can also be accessed via third 
person neuter personal pronouns. 
Interactions between the factors and genre were 
examined by an analysis of deviance run on a fit- 
ted logistic regression model; significance was cal- 
culated using the F-test. All factors except for Par 
show strong (p < 0.001) interactions with Genre. 
In other words, the influence of all factors but paral- 
lelism on pronominalization is mediated by Genre. 
There are two main reasons for this effect: first, 
some genres contain far more first and second per- 
son personal pronouns, which adds to the weight of 
Agree, and second, texts which are about persons 
and the actions of persons, such as the texts in CK 
and CL, tend to use more pronouns than texts which 
are mainly argumentative or expository. 
4.2 Which Factors are Important? 
To separate the important from the unimportant fac- 
tors, many researchers use decision and regression 
trees, mostly the binary CART variant (Breiman 
et al, 1984). We use a different kind of model here, 
logistic regression, which is especially well suited 
for categorical data analysis (cf. eg. Agresti (1990) 
or Kessler et al (1997)). In this model, the value 
of the binary target variable is predicted by a lin- 
ear combination of the predictor variables. Vari- 
able weights indicate the importance of a variable 
for classification: the higher the absolute value of 
the weight, the more important i is. 
Logistic regression models are not only evaluated 
by their performance on training and test data. We 
could easily construct a perfect model of any train- 
ing data set with n variables, where n is the size of 
the data set. But we need models that are small, yet 
predict the target values well. A suitable criterion 
is the Akaike Information Criterion (AIC, Akaike 
(1974)), which punishes both models that do not fit 
the data well and models that have too many pa- 
rameters. The quality of a factor is judged by the 
amount of variation in the target variable that it ex- 
plains. Note that increased prediction accuracy does 
not necessarily mean an increase in the amount of 
variation explained. As the model itself is a contin- 
uous approximation of the categorical distinctions 
to be modelled, it may occur that the numerical vari- 
ation in the predictions decreases, but that this de- 
crease is lost when re-translating numerical predic- 
tions into categorical ones. 
The factors for our model were selected based on 
the following procedure: We start with a model that 
always predicts the most frequent class. We then de- 
termine which factor provides the greatest reduction 
in the AIC, add that factor to the model and retrain. 
This step is repeated until all factors have been used 
or adding another factor does not yield any signifi- 
cant improvements anymore. 3 
This procedure invariably yields the sequence 
Dist4, Agree, Class, FormAnte, Syn, SynAnte, Am- 
big, Par, both when training models on the complete 
data set and when training on a single genre. Inspec- 
tion of the AIC values suggests that parallelism is 
the least important factor, and does not improve the 
AIC significantly. Therefore, we will discard it from 
the outset. All other factors are maintained in the 
initial full model. This model is purely additive; it 
does not include interactions between factors. This 
approach allows us to filter out factors which only 
mediate the influence of other factors, but do not ex- 
ert any significant influence of their own. Note that 
this probabilistic model only provides a numerical 
description of how its factors affect pronominaliza- 
tion in our corpus. As such, it is not equivalent to 
a theoretical model, but rather provides data for fur- 
3We excluded Dist from this stepwise procedure, since the 
relevant information is covered already by Dist4, which fur- 
thermore has much fewer values. 
22
Class 
%deadend 
% pronouns 
% pron.(sequences) 
Act Concept Event Group Loc Pers PhysObj Prop State Time 
84.1 80.0 88.0 46.1 63.3 17.3 65.5 88.5 87.8 92.9 
6.2 8,5 6.0 28.4 5.7 63.4 10.2 2.5 3.2 0.3 
32.5 29.6 33.3 51.6 15.4 73.8 27.2 21.4 23,7 4.5 
Table 4: Results for Sortal Classes. % deadend: percentage of deadend entities; % pronouns: percent 
pronominalised, % pron. (sequences: percent pronominalised relative to all occurrences in co-specification 
sequences 
% correct 
AIC 
% variation 
CF CG CK CL all 
97.1 93.5 93.6 91.5 
324.7 654.8 786.1 904.0 
83.0 65.4 70.1 65.4 
93.1 
2685.8 
68.7 
Table 5: Quality of models fitted to each of the 
genre-specific corpora (CF, CG, CK, CL) and the 
complete data set (all). % correct: correctly pre- 
dicted pronominalization decition, AIC: Akaike In- 
formation Criterion, % variation: percentage of 
original variation in the data (as measured by de- 
viance) accounted for by the model 
ther theoretical interpretation. 
Results of a first evaluation of the full model 
are summarized in Table 5. The model can ex- 
plain more than two thirds of the variation in the 
complete data set and can predict pronominalization 
quite well on the data it was fitted on. The mat- 
ter becomes more interesting when we examine the 
genre-specific results. Although overall prediction 
performance r mains table, the model is obviously 
suited better to some genres than to others. The best 
results are obtained on CF, the worst on CL (mys- 
tery fiction). In the CL texts, MCUs are short, a 
third of all referring expressions are pronouns, there 
is no first person singular narrator, and most para- 
graphs which mention persons are about the inter- 
action between two persons. 
The Relative Importance of Factors. All val- 
ues of Dist4 have very strong weights in all mod- 
els; this is clearly the most important factor. The 
same goes for Agree, where the first and second per- 
son are strong signs of pronominalization, and, to a 
lesser degree, masculine and feminine third person 
singular. The most important distinction provided 
by Class appears to be that between Persons, non- 
Persons, and Times. This holds as well when the 
model is only trained on third person referring ex- 
pressions. For singular referring expressions, Per- 
sonhood information is reflected in gender, but not 
for plural referring expressions. Another important 
influence is the form of the antecedent. The syn- 
tactic function of the referring expression and of its 
antecedent are less important, as is ambiguity. 
In order to examine the importance of the fac- 
tors in more detail, we refitted the models on the 
complete data set while omitting one or more of the 
three central features Dist4, Agree, and Class. The 
results are summarized in Table 6. The most inter- 
esting finding is that even if we exclude all three 
factors, prediction accuracy only drops by 3.2%. 
This means that the remaining 4 factors also con- 
tain most of the relevant information, but that this 
information is coded more "efficiently", so to speak, 
in the first three. Speaking of these factors, ques- 
tions concerning the effect of sortal class remains. 
Remarkably enough, when sortal class is omitted, 
accuracy increases by 0.7%. The increase in A1C 
can be explained by a decrease in the amount of 
explained variation. A third result is that informa- 
tion about the form of the antecedent can substitute 
for distance information, if that information is miss- 
ing. Both variables code the crucial distinctions be- 
tween expressions that evoke entities and those that 
access evoked entities. Furthermore, a pronominal 
antecedent tends to occur at a distance of less than 2 
MCUs. The contribution of syntactic function re- 
mains stable and significant, albeit comparatively 
unimportant. 
Predictive Power: To evaluate the predictive 
power of the models computed so far, we determine 
the percentage of correctly predicted pronouns and 
NPs. The performance of the trained models was 
compared to two very simple algorithms: 
Algorithm A: Always choose the most frequent 
option (i.e. noun). 
Algorithm B: If the antecedent is in the same 
MCU, or if it is in the previous MCU and there 
is no ambiguity, choose a pronoun; else choose 
a noun. 
Table 7 summarises the results of the compari- 
son. To determine the overall predictive power of 
23 
excluded 
none 
Class 
Agree 
Dist4 
Dist4 + Class 
Dist4 + Agree 
Agree + Class 
Dist4 + Agree + Class 
AIC 
fit 
%correct 
2686 92.6 
2785 93.3 
2984 92.6 
3346 90.2 
3443 90.2 
3597 89.6 
3098 92.6 
3739 89.4 
~ explained variation 
Dist4 Agree Class PForm Syn PSyn Ambig 
54.4 21.1 5.7 3.8 2.3 0.5 l.l 
54.4 21.1 n.a. 4.7 2.8 0.5 1.1 
54.4 n.a. 14.3 6.2 2.7 0.6 1.1 
n.a. 35.8 6.1 32 3 0.8 0.I 
n.a. 35.8 n.a. 33.7 3.4 0.8 0.1 
n.a. n.a. 31.4 35.4 3.1 0.8 0.2 
54.4 n.a. n.a. 13.11 3.5 0.5 3.6 
n.a. n.a. n.a. 52.62 4 0.7 1.7 
Table 6: Effect of leaving out any one of the three most important factors on model fit. italics: significance 
is p < 0.0.5, for all other factors, p < 0.005 or better. 
test data set 
CF CG CK CL all 
Alg. A 80.4 83.8 63.8 65.4 72.8 
Alg. B 91.1 93.0 88.6 84.7 89.4 
Model 96.5 92.2 91.8 90.9 92.6 + 0.02 
w/oClass 96.8 92.4 91.7 90.7 93.0+ 0.01 
pothesize that the decrease in performance is mainly 
due to the model itself, not to the training data. The 
results presented in both Table 5 and 7 show that 
although the model we have found is not quite as 
genre-independent as we would want it to be, it pro- 
vides a reasonable fit to all the genres we examined. 
Table 7: Results of algorithms vs. models on test 
data in % correct prediction if referring expression 
is to be pronominalised or not. Setup for genres: 
model is trained on three genres, tested on the re- 
maining one 
the model, we used 10-fold cross-validation. Al- 
gorithm A always fares worst, while algorithm B, 
which is based mainly on distance, the strongest fac- 
tor in the model, performs quite well. Its overall 
performance is 3.2% below that of  the full model, 
and 3.6% below that of the full model without sor- 
tal class information. It even outperforms the mod- 
els on CG, which has the lowest percentage of Per- 
sons (12.9% vs. 35% for CF and 43.4% and 43.5% 
for CL and CK). For all other genres, the statistical 
models outperform the simple heuristics. Excluding 
sortal class information can boost prediction perfor- 
mance on unseen data by as much as 0.4% for the 
complete corpus. The apparent contradiction be- 
tween this finding and the results reported in the 
previous section can be explained if we consider 
that not only were some sortal classes comparatively 
rare in the data (Property, Event), but that our sortal 
class definition may still be too fine-grained. 
We evaluated the genre-independence of the 
model by training on three genres and testing on the 
fourth. The results show that the model fares quite 
well for genre CF, which is also the genre where the 
overall fit was best (see Table 5). We therefore hy- 
5 Future  Work  
We have described a probabilistic model of pronom- 
inalization that is able to correctly predict 93% of 
all pronouns in a corpus that consists of twelve texts 
from four different genres. Since the model was de- 
rived from a limited corpus and a limited number of 
genres, we cannot guarantee that our results are ap- 
plicable to all texts without modifications. But since 
its performance on our sample is consistently above 
90% correct, we are reasonably confident hat our 
main findings will hold for a wide variety of texts 
and text types. In particular, we isolated several fac- 
tors which are robust predictors of pronominaliza- 
tion across genres: distance from last mention and 
agreement, and to a certain extent he form of the 
antecedent, which appears to be a good substitute if
the other two factors are not available. All three fea- 
tures can be computed on the basis of a chunk parse, 
a rough morphosyntactic analysis of the resulting 
NPs, and co-specification sequences. In computa- 
tional terms, they are comparatively cheap. Large 
corpora can be annotated relatively quickly with this 
information, which can then be used for statistical 
pronoun generation. 
The comparatively expensive sortai class anno- 
tation, on the other hand, was not very important 
in the final model; in fact, prediction accuracy de- 
creased when sortal class was included. There 
are two main reasons for this: first, the proposed 
sortal class annotation scheme needs further work, 
24 
second, the relationship between sortal class and 
pronominalization may well be too intricate to be 
modelled by the factor Class alone. 
We set out to find a genre-independent model 
of pronominalization. The model we found per- 
forms quite well, but genre still considerably affects 
its performance. Where does the remaining, unex- 
plained variation come from? The variation might 
be just that - stylistic variation. It might stem from 
one of the traditional factors that we did not take 
into account here, such as thematic role. However, 
we suspect that the crucial factor at play here is dis- 
course structure (McCoy & Strube, 1999). 
Acknowledgements Work on this paper was be- 
gun while Michael Strube was a postdoctoral fellow 
at the Institute for Research in Cognitive Science, 
University of Pennsylvania, nd Maria Wolters vis- 
ited the Institute for a week in summer 1999. We 
would like to thank Kathleen McCoy, Jonathan De- 
Cristofaro, and the three anonymous reviewers for 
their comments on earlier stages of this work. 
References 
Agresti, Alan (1990). Categorical Data Analysis. New 
York, N.Y.: Wiley. 
Akaike, H. (1974). A new look at statistical model 
identification. 1EEE Transactions Automatic Control, 
19:716-722. 
Azzam, Saliha, Kevin Humphreys & Robert Gaizauskas 
(1998). Evaluating a focus-based approach to 
anaphora resolution. In Proceedings of the 17 th In- 
ternational Conference on Computational Linguistics 
and 36 th Annual Meeting of the Association for Com- 
putational Linguistics, Montr6al, Qu6bec, Canada, 
10--14 August 1998, pp. 74-78. 
Breiman, Leo, Jerome H. Friedman, Charles J. Stone & 
R.A. Olshen (1984). Classification and Regression 
Trees. Belmont, Cal.: Wadsworth and Brooks/Cole. 
Cardie, Claire & Kiri Wagstaff (1999). Noun phrase 
coreference as clustering. In Proceedings of the 1999 
SIGDAT Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora, Col- 
lege Park, Md., 21-22 June 1999, pp. 82-89. 
Carletta, Jean (1996). Assessing agreement on classifi- 
cation tasks: The kappa statistic. Computational Lin- 
guistics, 22(2):249-254. 
Cohen, Jacob (1960). A coefficient of agreement for 
nominal scales. Educational and Psychological Mea- 
surement, 20:37--46. 
Dale, Robert (1992). Generating Referring Expressions: 
Constructing Descriptions in a Domain of Objects 
and Processes. Cambridge, Mass.: MIT Press. 
DeCristofaro, Jonathan, Michael Strube & Kathleen E 
McCoy (1999). Building a tool for annotating ref- 
erence in discourse. In ACL '99 Workshop on the 
Relationship between Discourse~Dialogue Structure 
and Reference, University of Maryland, Maryland, 21 
June, 1999, pp. 54-62. 
Fellbaum, Christiane (Ed.) (1998). WordNet: An Elec- 
tronic Lexical Database. Cambridge, Mass.: MIT 
Press. 
Fraurud, Kari (1996). Cognitive ontology and NP form. 
In T. Fretheim & J. Gundel (Eds.), Reference and 
Referent Accessibility, pp. 65-87. Amsterdam, The 
Netherlands: Benjamins. 
Ge, Niyu, John Hale & Eugene Charniak (1998). A sta- 
tistical approach to anaphora resolution. In Proceed- 
ings of the Sixth Workshop on Very Large Corpora, 
Montr6al, Canada, pp. 161-170. 
Grosz, Barbara J., Aravind K. Joshi & Scott Weinstein 
(1995). Centering: A framework for modeling the lo- 
cal coherence of discourse. Computational Linguis- 
tics, 21 (2):203-225. 
Hirschman, Lynette & Nancy Chinchor (1997). MUC- 
7 Coreference Task Definition, http://www. 
muc. sais.  com/proceed ings / .  
Ihaka, Ross & Ross Gentleman (1996). R: A language 
for data analysis and graphics. Journal of Computa- 
tional and Graphical Statistics, 5:299-314. 
Kessler, Brett, Geoffrey Nunberg & Hinrich Schiitze 
(1997). Automatic detection of text genre. In Proceed- 
ings of the 35 th Annual Meeting of the Association 
for Computational Linguistics and of the 8 th Confer- 
ence of the European Chapter of the Association for 
Computational Linguistics, Madrid, Spain, 7-12 July 
1997, pp. 32-38. 
LDC (1995). Penn Treebank-H. Linguistic Data Consor- 
tium. University of Pennsylvania, Philadelphia, Penn. 
McCoy, Kathleen F. & Michael Strube (1999). Gener- 
ating anaphoric expressions: Pronoun or definite de- 
scription? In ACL '99 Workshop on the Relationship 
between Discourse/Dialogue Structure and Reference, 
University of Maryland, Maryland, 21 June, 1999, pp. 
63-71. 
Poesio, Massimo, Renate Henschel, Janet Hitzeman & 
Rodger Kibble (1999). Statistical NP generation: A
first report. In R. Kibble & K. van Deemter (Eds.), 
Proceedings of the Workshop on The Generation of 
Nominal Expressions, 11th European Summer School 
on Logic, Language, and Information, Utrecht, 9-13 
August 1999. 
Sidner, Candace L. (1983). Focusing in the compre- 
hension of definite anaphora. In M. Brady & R.C. 
Berwick (Eds.), Computational Models of Discourse, 
pp. 267-330. Cambridge, Mass.: MIT Press. 
Vossen, Piek (Ed.) (1998). EuroWordNet: A Multilingual 
Database with Lexical Semantic Networks. Dordrecht, 
The Netherlands: Kluwer. 
Yeh, Ching-Long & Chris Mellish (1997). An empiri- 
cal study on the generation of anaphora in Chinese. 
Computational Linguistics, 23( ! ): 169-190. 
25
Semantic Similarity Applied to Spoken Dialogue Summarization
Iryna Gurevych and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/english/homes/{gurevych|strube}
Abstract
We present a novel approach to spoken dialogue
summarization. Our system employs a set of
semantic similarity metrics using the noun por-
tion of WordNet as a knowledge source. So far,
the noun senses have been disambiguated man-
ually. The algorithm aims to extract utterances
carrying the essential content of dialogues. We
evaluate the system on 20 Switchboard dia-
logues. The results show that our system out-
performs LEAD, RANDOM and TF*IDF base-
lines.
1 Introduction
Research in automatic text summarization began in
the late 1950s and has been receiving more atten-
tion again over the last decade. The maturity of this
research area is indicated by recent large-scale eval-
uation efforts (Radev et al, 2003). In comparison,
speech summarization is a rather new research area
which emerged only a few years ago. However, the
demand for speech summarization is growing be-
cause of the increasing availability of (digitally en-
coded) speech databases (e.g. spoken news, politi-
cal speeches).
Our research is concerned with the development
of a system for automatically generating summaries
of conversational speech. As a potential applica-
tion we envision the automatic generation of meet-
ing minutes. The approach to spoken dialogue
summarization presented herein unifies corpus- and
knowledge-based approaches to summarization, i.e.
we develop a shallow knowledge-based approach.
Our system employs a set of semantic similar-
ity metrics which utilize WordNet as a knowledge
source. We claim that semantic similarity between
a given utterance and the dialogue as a whole is an
appropriate criterion for the selection of utterances
which carry the essential content of the dialogue, i.e.
relevant utterances. ? In order to study the perfor-
mance of semantic similarity methods, we remove
the noise from the pre-processing modules by man-
ually disambiguating lexical noun senses.
In Section 2, we briefly describe research on sum-
marization and how spoken dialogue summarization
differs from text summarization. Section 3 gives
the semantic similarity metrics we use and describes
how they are applied to the summarization problem.
Section 4 provides information about the data used
in our experiments, while Section 5 describes the
experiments and the results together with their sta-
tistical significance.
2 Text, Speech and Dialogue
Summarization
Most research on automatic summarization dealt
with written text. This work was based either on
corpus-based, statistical methods or on knowledge-
based techniques (for an overview over both strands
of research see Mani & Maybury (1999)). Re-
cent advances in text summarization are mostly due
to statistical techniques with some additional us-
age of linguistic knowledge, e.g. (Marcu, 2000;
Teufel & Moens, 2002), which can be applied to un-
restricted input.
Research on speech summarization focused
mainly on single-speaker, written-to-be-spoken text
(e.g. spoken news, political speeches, etc.). The
methods were mostly derived from work on text
summarization, but extended it by exploiting partic-
ular characteristics of spoken language, e.g. acous-
tic confidence scores or intonation. Difficulties arise
because speech recognition systems are not perfect.
Therefore, spoken dialogue summarization systems
have to deal with errors in the input. There are no
sentence boundaries in spoken language either.
Work on spoken dialogue summarization is still
in its infancy (Reithinger et al, 2000; Zechner,
2002). Multiparty dialogue is much more difficult
to process than written text. In addition to the dif-
ficulties speech summarization has to face, spoken
dialogue contains a whole range of dialogue phe-
nomena as disfluencies, hesitations, interruptions,
etc. Also, the information to be summarized may be
contributed by different speakers (e.g. in question-
answer pairs). Finally, the language used in spoken
dialogue differs from language used in texts. Be-
cause discourse participants are able to immediately
clarify misunderstandings, the language used does
not have to be that explicit.
3 Semantic Similarity
3.1 Semantic Similarity Metrics
Experiments reported here employed Ted Peder-
sen?s (2002) semantic similarity package. We ap-
plied five of the metrics, which rely on WordNet as
a knowledge base and were developed in the context
of work on word sense disambiguation. The first
measure is Leacock and Chodorow?s (1998) Nor-
malized Path Length (we will refer to it as lch). Se-
mantic similarity sim between words w1 and w2 is
defined as given in Equation 1:
simc1,c2 = ?log
len(c1, c2)
2 ? D (1)
c1 and c2 are concepts corresponding to w1 and w2.1
len(c1, c2) is the length of the shortest path between
them. D is the maximum depth of the taxonomy.
The following measures incorporate an addi-
tional, qualitatively different knowledge source
based on some kind of corpus analysis. The ex-
tended gloss overlaps measure introduced by Baner-
jee & Pedersen (2003) (referred to as lesk in the
following) is based on the number of shared words
(overlaps) in the WordNet definitions (glosses) of
the respective concepts. It also extends the glosses
to include the definitions of concepts related to the
concept under consideration based on the WordNet
hierarchy. Formally, semantic relatedness sim be-
tween words w1 and w2 is defined by the following
equation:
simc1,c2 =
?
score(R1(c1), R2(c2)) (2)
where R is a set of semantic relations, score() is
a function accepting two glosses as input, finding
overlaps between them, and returning a correspond-
ing relatedness score.
The remaining three methods require an addi-
tional knowledge source, an information content file
(ICF). This file contains information content values
for WordNet concepts, which are needed for com-
puting the semantic similarity score for two con-
cepts. Information content values are based on the
frequency counts for respective concepts. Resnik
(1995) (res for short) calculates the information
content of the concept that subsumes the given two
1This also refers to the rest of the methods.
concepts in the taxonomy (see Equation 3):
simc1,c2 = maxc?S(c1,c2)
[? log p(c)] (3)
where S(c1, c2) is the set of concepts which sub-
sume both c1 and c2 and ? log p(c) is the negative
log likelihood (information content). The probabil-
ity p is computed as the relative frequency of the
concept. Resnik?s measure is based on the intuition
that the semantic similarity between concepts may
be quantified on the basis of information shared be-
tween them. In this case the WordNet hierarchy
is used to determine the closest super-ordinate of a
pair of concepts.
Jiang & Conrath (1997) proposed to combine
edge- and node-based techniques in counting the
edges and enhancing it by the node-based calcu-
lation of the information content as introduced by
Resnik (1995) (the method is abbreviated as jcn).
The distance between two concepts c1 and c2 is for-
malized as given in Equation 4:
distc1,c2 = IC(c1) + IC(c2) ? 2 ? IC(lso(c1, c2))
(4)
where IC is the information content value of the
concept, and lso(c1, c2) is the closest subsumer of
the two concepts.
The last method is that of Lin (1998) (we call this
metric lin). He defined semantic similarity using a
formula derived from information theory. This mea-
sure is sometimes called a universal semantic sim-
ilarity measure as it is supposed to be application-,
domain-, and resource independent. According to
this method, the similarity is given in Equation 5:
simc1,c2 =
2 ? log p(lso(c1, c2))
log p(c1) + log p(c2)
(5)
3.2 Semantic Similarity in Summarization
The process of automatic dialogue summarization,
as defined in the context of this work, means to ex-
tract the most relevant utterances from the dialogue.
We restate this as a classification problem, which
is similar to the definition given by Kupiec et al
(1995). This means that utterances are classified as
relevant or irrelevant for the summary of a specific
dialogue. By relevant utterances we mean those car-
rying the most essential parts of the dialogue?s con-
tent. The summarization task is, then, to extract the
set of utterances from the transcript, which a human
would use to make a dialogue summary.
The key idea behind the algorithm presented here
is to quantify the degree of semantic similarity be-
tween a given utterance and the whole dialogue. We
argue that semantic similarity between an utterance
A.: Utt1 Okay.
Utt2 Tell me about your home.
B.: Utt3 Well, it?s an older home.
Utt4 It was made back in the early sixties.
Utt5 It?s a pier beam house.
A.: Utt6 Huh-uh.
B.: Utt7 Got three bedrooms, one bath
Utt8 and that just makes me scream.
A.: Utt9 That?s pretty tough.
Utt10 What area do you live in?
B.: Utt11 I live in Houston.
Table 1: Switchboard dialogue fragment
Number Concepts Sense Number
CRUtt1 ? ?
CRUtt2 home 2
CRUtt3 home 2
CRUtt4 sixties 1
CRUtt5 pier, beam, house 2, 2, 1
CRUtt6 ? ?
CRUtt7 bedrooms, bath 1, 5
CRUtt8 ? ?
CRUtt9 ? ?
CRUtt10 area 1
CRUtt11 Houston 1
Table 2: Utterances mapped to WordNet concepts
and the dialogue as a whole represents an appropri-
ate criterion for the selection of relevant utterances.
We describe each of the processing steps, employ-
ing the example dialogue D from Table 1. This
example consists of the set of utterances {Utt1,...,
Utt11}.
3.2.1 Creating conceptual representations
The semantic similarity algorithms introduced in
Section 3.1 operate on the noun portion of WordNet.
Our approach to dialogue summarization, as previ-
ously stated, is to compute semantic similarity for a
given pair {Uttn,D}. In order to do that, we require
a WordNet-based conceptual representation of both
Uttn, i.e. CRUttn , and D, i.e. CRD, and com-
pare them using the semantic similarity measures.
Therefore, we map the nouns contained in the utter-
ances to their respective WordNet senses and oper-
ate on these representations in the subsequent steps.
The results of this operation are shown in Table 2.
The number in the last column indicates the disam-
biguated WordNet sense.
The resulting dialogue representation CRD will
be the set of concepts resulting from adding individ-
ual utterance representations, i.e. CRD = {home,
home, sixties, pier, beam, house, bedrooms, bath,
area, Houston}.
3.2.2 Computing average semantic similarity
For each utterance Uttn, we create a two-
dimensional matrix C with the dimen-
sions (#CRD ? #CRUttn), where # de-
notes the number of elements in the set.
C = (cij)i=1,...,#CRD,j=1,...,#CRUttn , see Ta-
ble 3. Then, we compute the semantic similarity
SSscore(i, j) employing any of the semantic sim-
ilarity metrics described above for each pair of
concepts. The semantic similarity score SSfinal for
CRUttn and CRD is then defined as the average
pairwise semantic similarity between all concepts
in CRUttn and CRD:
SSfinal =
?#CRUttn
i=1
?#CRD
j=1 SSscore(i, j)
#CRUttn ? #CRD
Computing SSfinal results in a list of utterances
with scores from the respective scoring methods,
Table 4. Note that the absolute utterance scores are
taken from the real data, i.e. they have been nor-
malized w.r.t. the conceptual representation for the
whole dialogue, and not for the dialogue fragment
given in Table 1. The rankings were produced for
this specific example to make it more illustrative.
3.2.3 Extracting relevant utterances
In order to produce a summary of the dialogue, the
utterances first have to be sorted numerically, i.e.
ranked on the basis of their scores, see Table 4
for the results of the ranking procedure.2 Given a
compression rate COMP with the range [1,100],
the number of utterances classified as relevant by
an individual scoring method PNr is a function
of the total number of utterances in the dialogue:
PNr = (COMP/100) ? Numbertotal .3 Then,
given a specific compression rate COMP , the top-
ranked PNr utterances will be automatically classi-
fied as relevant. ? Returning to the example in Table
1, we obtain the summaries given in Table 5.
COMP Selected Utterances
20% I live in Houston.
Got three bedrooms, one bath.
35% I live in Houston.
Got three bedrooms, one bath.
Tell me about your home.
Well, it?s an older home.
Table 5: Summaries based on Resnik?s measure
2If two or more utterances get an equal score, they are
ranked according to the order of their occurrence.
3Note that this number must be rounded to a natural number.
home home sixties pier beam house bedrooms bath area Houston
bedrooms 3.8021 3.8021 0 2.5158 2.5158 3.8021 9.3157 5.8706 0.8287 0.8287
bath 3.8021 3.8021 0 2.5158 2.5158 3.8021 5.8706 10.7821 0.8287 0.8287
Table 3: Concept matrix C for Utt7 from Table 1 based on Resnik?s measure
Number Utterance Resnik?s score Rank
Utt1 Okay. ? 8
Utt2 Tell me about your home. 1.4181106409372 3
Utt3 Well, it?s an older home. 1.4181106409372 4
Utt4 It was made back in the early sixties. 0.551830914995721 7
Utt5 It?s a pier beam house. 1.18821772523631 6
Utt6 Huh-uh. ? 9
Utt7 Got three bedrooms, one bath 1.50689651387565 2
Utt8 and that just makes me scream. ? 10
Utt9 That?s pretty tough. ? 11
Utt10 What area do you live in? 1.25186984433606 5
Utt11 I live in Houston. 1.51301080520959 1
Table 4: Utterance scores based on Resnik?s measure
Tokens Utterances Turns
Total 34830 3275 1852
Average 1741.5 163.75 92.6
Table 6: Descriptive corpus statistics
4 Data
The data used in the experiments are 20 randomly
chosen Switchboard dialogues (Greenberg, 1996).
These dialogues contain two-sided telephone con-
versations among American speakers of at least 10
minutes duration. The callers were given a cer-
tain topic for discussion. The recordings of spon-
taneous speech were, then, transcribed. Statistical
data about the corpus, i.e. total numbers and aver-
ages for separate dialogues, are given in Table 6. To-
kens are defined as running words and punctuation.
An utterance is a complete unit of speech spoken by
a single speaker, while a turn is a joint sequence of
utterances produced by one speaker.
In the annotation experiments, we tested whether
humans could reliably determine the utterances con-
veying the overall meaning of the dialogue. There-
fore, each utterance is assumed to be a markable,
i.e. the expression to be annotated resulting in a to-
tal of 3275 markables in the corpus. Three anno-
tators were instructed to select the most important
utterances. They were supposed to first read the di-
alogue und then to mark about 10% of all utterances
in the dialogue as being relevant. Then, we pro-
duced two kinds of Gold Standards from these data.
Gold Standard 1 included the utterances which were
marked by all three annotators as being relevant.
Gold Standard 2 included the utterances which were
selected by at least two annotators.
Table 7 shows the results of these experiments.
We present the absolute number of markables se-
lected as relevant by separate annotators and in two
Gold Standards. Also, we indicate the percentage,
given the total number of markables 3275. As the
table shows, Gold Standard 1 includes only 3.69%
of all markables. Therefore, we used Gold Stan-
dard 2 in the evaluation reported in Section 5. The
Kappa coefficient for inter-annotator agreement var-
ied from 0.1808 to 0.6057 for individual dialogues.
An examination of the particular dialogue with the
very low Kappa rate showed that this was one of the
shortest ones. It did not have a well-defined topical
structure, resulting in a low agreement rate between
annotators. For the whole corpus, the Kappa co-
efficient yielded 0.4309. While this is not a high
agreement rate on a general scale, it is compara-
ble to what has been reported concerning the task
of summarization and in particular dialogue sum-
marization.
5 Evaluation
5.1 Evaluation Metrics and Baselines
We reformulated the problem in terms of stan-
dard information retrieval evaluation metrics:
Precision = PP/PNr, Recall = PP/NP , and
Fmeasure = 2 ? Prec ? Rec/(Prec + Rec). PP
is the number of cases where the individual scoring
method and the Gold Standard agree. PNr is com-
puted according to the definition given in Section 3.
Annotator 1 Annotator 2 Annotator 3 Gold Standard 1 Gold Standard 2
? 417 12.73% 350 10.69% 347 10.6% 121 3.69% 310 9.47%
Table 7: Number of markables labeled as relevant
NP is the total number of utterances marked as rel-
evant in the Gold Standard. For comparison, three
baseline systems were implemented. The first sys-
tem is the RANDOM baseline, where relevant ut-
terances (depending on the compression rate) were
selected by chance. The second baseline system is
based on the TF*IDF scoring metric. A large cor-
pus is required to make this method fully power-
ful. Therefore, we computed TF*IDF scores for
every word on the basis of 2431 Switchboard dia-
logues (ca. 19.3 MB of ASCII text). Then, an av-
erage TF*IDF score for each utterance of the 20 di-
alogues in our corpus was computed by adding the
individual scores for all words in the utterance and
normalizing by the number of words. The LEAD
baseline is based on the intuition that the most im-
portant utterances tend to occur at the beginning of
the discourse. While this observation is true for the
domain of news, the LEAD baseline is not necessar-
ily efficient for the genre of spontaneous dialogues.
However, given the Switchboard experimental data
collection setup, the dialogues usually directly start
with the discussions of the topic. This hypothesis
was supported by evidence from our own annota-
tion experiments, too.
5.2 Results
Experiments were performed using the semantic
similarity package V0.05 (Pedersen, 2002) and
WordNet 1.7.1. We employed Gold Standard 2
(see Section 4). Three of the methods, namely res,
lin, jcn, require the information content file (ICF).
A method for computing the information content
of concepts from large corpora of text is given in
Resnik (1995). ICF contains a list of synsets along
with their part of speech and frequency count. We
compare the results obtained with 2 different ICFs:
? a WordNet-based ICF, provided at the time of
the installation of the similarity package with
pre-computed frequency values on the basis of
WordNet (WD ICF);
? an ICF, generated specifically on the basis of
2431 Switchboard dialogues with the help of
utilities distributed together with the similarity
package (SW ICF).
Figures 1 and 2 indicate the performance of all
methods in terms of F-measure. The results of the
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 5 10 15 20 25 30 35 40
F-
m
ea
su
re
Compression rate in %
res
lin
jcn
lesk
LEAD
TF*IDF
lch
RANDOM
Figure 1: Results based on WordNet ICF
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 5 10 15 20 25 30 35 40
F-
m
ea
su
re
Compression rate in %
res
lin
jcn
lesk
LEAD
TF*IDF
lch
RANDOM
Figure 2: Results based on Switchboard ICF
semantic similarity methods making use of the in-
formation content file generally improve when the
Switchboard-based ICF is used. The improvements
are especially significant for the jcn and lin mea-
sures, while this does not seem to be the case for the
res measure (depending on a specific compression
rate).
The summarization methods perform best for the
compression rates in the interval [20,30]. Given
these rates and the Switchboard-based ICF, the com-
peting methods display the following performance
(in descending order): jcn, res, lin, lesk, lch, tf*idf,
lead, random. For the default ICF the picture is
slightly different: res, jcn and lesk, lch, lin, tf*idf,
lead, random (see Table 8). lch relying on WordNet
structure only performs worse than the rest of simi-
larity metrics incorporating some corpus evidence.
A direct comparison of our evaluation with alter-
native results, e.g., Zechner?s (2002) is problematic.
Though Zechner?s results are based on Switchboard,
too, he employs a different evaluation scheme. The
evaluation is broken down to the word level. The re-
sults are compared with multiple human annotations
instead of a Gold Standard.
5.3 Statistical Significance and Error Analysis
For determining whether there is a significant differ-
ence between the summarization approaches pair-
wise, we use a paired related t-test (as the parent
distribution is unknown). The null hypothesis states
there is no difference between the two distributions.
On consulting the t-test tables, we obtain the signif-
icance values presented in Table 9, given the com-
pression rate 25%4 and the Switchboard ICF. These
results indicate that there is no statistically signifi-
cant difference in the performance between the res,
lin, jcn and lesk methods. However, all of them
significantly outperform the LEAD, TF*IDF and
RANDOM baselines.
The maximum Recall of the semantic similarity-
based summarization methods in the current imple-
mentation is limited to about 90%, given COMP =
100%. This means that if the system compiled a
100% ?summary?, it would miss 10% of all utter-
ances marked as relevant. The reason lies in the fact
that the algorithm operates on the concepts created
by mapping nouns to their WordNet senses. Thus,
the relevant utterances which do not have nouns on
the surface, but contain for example anaphorical ex-
pressions realized as pronouns, are missed in the in-
put. Resolving anaphorical expressions in the pre-
processing stage may eliminate this error source.
6 Concluding Remarks
We introduced a new approach to spoken dialogue
summarization. Our approach combines statistical,
i.e. corpus-based, and knowledge-based techniques.
It utilizes the knowledge encoded in the noun part
of WordNet and applies a set of semantic similar-
ity metrics to dialogue summarization. All seman-
tic similarity-based summarization methods outper-
form RANDOM, LEAD and TF*IDF baseline sys-
tems. In the following, we discuss some remaining
challenges and future research.
More sophisticated data pre-processing. We plan
4Roughly speaking, the differences are most evident for
compression rates between 20% and 30%.
to incorporate the pre-processing components used
by Zechner (2002) and evaluate their contribution to
our task. Including an anaphora resolution compo-
nent would also result in better Recall.
Automatic word sense disambiguation. Switch-
board conversational speech is highly ambiguous.
Automatic disambiguation of noun senses to Word-
Net concepts is important in order to integrate our
approach into real-life summarization systems.
Investigating other types of information in parallel.
A clear desideratum will be assessing the overall co-
herence of the discourse, speaker info, turn type, in-
formation about non-nouns.
Application to text and speech summarization. Our
approach can be applied to written-to-be-spoken
speech and text summarization. It will be interest-
ing to investigate whether conceptual structures of
texts (the input to our system) are comparable to the
conceptual structures found in dialogues.
Readability, coherence, and usability of the sum-
maries produced. A close examination of sum-
maries based on human comprehension will be in-
teresting. It may be necessary to introduce filtering
or other post-processing techniques improving the
quality of summaries.
Even without very sophisticated pre-processing of
the dialogue data, our algorithm yields promising
results. It was evaluated on the Switchboard data,
which is a challenging evaluation corpus. Our vi-
sion is to adopt the summarization approach pre-
sented here in a system used for the automatic pro-
duction of meeting minutes.
Acknowledgments
This work has been funded by the Klaus Tschira
Foundation. We thank Christoph Zwirello for
his valuable contributions, the annotators Tatjana
Medvedeva, Vanessa Michelli and Iryna Zhmaka,
and Ted Pederson and colleagues for their software.
References
Banerjee, S. & T. Pedersen (2003). Extended gloss over-
lap as a measure of semantic relatedness. In Proceed-
ings of the 18th International Joint Conference on Ar-
tificial Intelligence, Acapulco, Mexico, 9?15 August,
2003, pp. 805?810.
Greenberg, S. (1996). The Switchboard transcription
project. In Proceedings of the Large Vocabulary Con-
tinuous Speech Recognition Summer Research Work-
shop, Baltimore, Maryland, USA, April 1996.
Jiang, J. J. & D. W. Conrath (1997). Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the 10th International Conference on
Research in Computational Linguistics (ROCLING),
pp. 19?33. Tapei, Taiwan.
RANDOM LEAD TF*IDF Res Lin Jcn Lesk Lch
Precision 10% WD ICF .08563 .23242 .21101 .25076
SW ICF .07645 .18654 .17125 .23853 .22936 .26606 .22936 .25076
20% WD ICF .1026 .23933 .23018 .24390
SW ICF .07963 .16159 .19512 .23780 .24085 .24695 .23780 .23780
30% WD ICF .10429 .22380 .21261 .21668
SW ICF .09407 .14140 .17192 .22075 .22482 .23093 .22584 .21567
Recall 10% WD ICF .09032 .24516 .22258 .26452
SW ICF .08065 .19677 .18065 .25161 .24194 .28065 .24194 .26452
20% WD ICF .21613 .50645 .48710 .51613
SW ICF .16774 .34194 .41290 .50323 .50968 .52258 .50323 .50323
30% WD ICF .32903 .70968 .67419 .68710
SW ICF .29677 .44839 .54516 .70000 .71290 .73226 .71613 .68387
F-measure 10% WD ICF .08791 .23862 .21664 .25746
SW ICF .07849 .19152 .17582 .24490 .23548 .27316 .23548 .25746
20% WD ICF .13915 .32505 .31263 .33126
SW ICF .108 .21946 .26501 .32298 .32712 .33540 .32298 .32298
30% WD ICF .15839 .34029 .32328 .32947
SW ICF .14286 .21500 .26141 .33565 .34184 .35112 .34339 .32792
Table 8: Precision, Recall and F-measure for 10%, 20% and 30% and two ICFs
res lin jcn lesk lead tf*idf lch random
res XXX p>0.05 p>0.05 p>0.05 p<0.01 p<0.01 p>0.05 p<0.01
lin XXX p>0.05 p>0.05 p<0.01 p<0.05 p>0.05 p<0.01
jcn XXX p>0.05 p<0.01 p<0.01 p<0.05 p<0.01
lesk XXX p<0.01 p<0.05 p>0.05 p<0.01
lead XXX p>0.05 p<0.01 p<0.01
tf*idf XXX p>0.05 p<0.01
lch XXX p<0.01
random XXX
Table 9: Statistical significance of results at COMP=25% and based on SW ICF
Kupiec, J., J. O. Pedersen & F. Chen (1995). A trainable
document summarizer. In Research and Development
in Information Retrieval, pp. 68?73.
Leacock, C. & M. Chodorow (1998). Combining local
context and WordNet similarity for word sense iden-
tification. In C. Fellbaum (Ed.), WordNet: An Elec-
tronic Lexical Database, pp. 265?283. Cambridge:
MIT Press.
Lin, D. (1998). An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, pp. 296?304. Mor-
gan Kaufmann, San Francisco, CA.
Mani, I. & M. T. Maybury (Eds.) (1999). Advances in
Automatic Text Summarization. Cambridge/MA, Lon-
don/England: MIT Press.
Marcu, D. (2000). The Theory and Practice of Dis-
course Parsing and Summarization. Cambridge/MA:
The MIT Press.
Pedersen, T. (2002). Semantic Similarity Package.
http://www.d.umn.edu/?tpederse/similarity.html.
Radev, D. R., S. Teufel, H. Saggion, W. Lam, J. Blitzer,
H. Qi, A. Celebi, D. Liu & E. Drabek (2003). Evalu-
ation challenges in large-scale document summariza-
tion. In Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, Sapporo,
Japan, 7?12 July 2003, pp. 375?382.
Reithinger, N., M. Kipp, R. Engel & J. Alexandersson
(2000). Summarizing multilingual spoken negotiation
dialogues. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics,
Hong Kong, 1?8 August 2000, pp. 310?317.
Resnik, P. (1995). Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
the 14th International Joint Conference on Artificial
Intelligence, Montre?al, Canada, 1995, Vol. 1, pp. 448?
453.
Teufel, S. & M. Moens (2002). Summarizing scientific
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?445.
Zechner, K. (2002). Automatic summarization of open-
domain multiparty dialogues in diverse genres. Com-
putational Linguistics, 28(4):447?485.
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 177?185,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Sentence Fusion via Dependency Graph Compression
Katja Filippova and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We present a novel unsupervised sentence fu-
sion method which we apply to a corpus of bi-
ographies in German. Given a group of related
sentences, we align their dependency trees and
build a dependency graph. Using integer lin-
ear programming we compress this graph to
a new tree, which we then linearize. We use
GermaNet and Wikipedia for checking seman-
tic compatibility of co-arguments. In an eval-
uation with human judges our method out-
performs the fusion approach of Barzilay &
McKeown (2005) with respect to readability.
1 Introduction
Automatic text summarization is a rapidly develop-
ing field in computational linguistics. Summariza-
tion systems can be classified as either extractive or
abstractive ones (Spa?rck Jones, 1999). To date, most
systems are extractive: sentences are selected from
one or several documents and then ordered. This
method exhibits problems, because input sentences
very often overlap and complement each other at the
same time. As a result there is a trade-off between
non-redundancy and completeness of the output. Al-
though the need for abstractive approaches has been
recognized before (e.g. McKeown et al (1999)), so
far almost all attempts to get closer to abstractive
summarization using scalable, statistical techniques
have been limited to sentence compression.
The main reason why there is little progress on ab-
stractive summarization is that this task seems to re-
quire a conceptual representation of the text which is
not yet available (see e.g. Hovy (2003, p.589)). Sen-
tence fusion (Barzilay & McKeown, 2005), where a
new sentence is generated from a group of related
sentences and where complete semantic and con-
ceptual representation is not required, can be seen
as a middle-ground between extractive and abstrac-
tive summarization. Our work regards a corpus of
biographies in German where multiple documents
about the same person should be merged into a sin-
gle one. An example of a fused sentence (3) with the
source sentences (1,2) is given below:
(1) Bohr
Bohr
studierte
studied
an
at
der
the
Universita?t
University
Kopenhagen
Copenhagen
und
and
erlangte
got
dort
there
seine
his
Doktorwu?rde.
PhD
?Bohr studied at the University of Copenhagen
and got his PhD there?
(2) Nach
After
dem
the
Abitur
school
studierte
studied
er
he
Physik
physics
und
and
Mathematik
mathematics
an
at
der
the
Universita?t
University
Kopenhagen.
Copenhagen
?After school he studied physics and mathemat-
ics at the University of Copenhagen?
(3) Nach
After
dem
the
Abitur
school
studierte
studied
Bohr
Bohr
Physik
physics
und
and
Mathematik
mathematics
an
at
der
the
Universita?t
University
Kopenhagen
Copenhagen
und
and
erlangte
got
dort
there
seine
his
Doktorwu?rde.
PhD
?After school Bohr studied physics and mathe-
matics at the University of Copenhagen and got
his PhD there?
177
Having both (1) and (2) in a summary would make
it redundant. Selecting only one of them would not
give all the information from the input. (3), fused
from both (1) and (2), conveys the necessary infor-
mation without being redundant and is more appro-
priate for a summary.
To this end, we present a novel sentence fusion
method based on dependency structure alignment
and semantically and syntactically informed phrase
aggregation and pruning. We address the problem in
an unsupervised manner and use integer linear pro-
gramming (ILP) to find a globally optimal solution.
We argue that our method has three important advan-
tages compared to existing methods. First, we ad-
dress the grammaticality issue empirically by means
of knowledge obtained from an automatically parsed
corpus. We do not require such resources as subcat-
egorization lexicons or hand-crafted rules, but de-
cide to retain a dependency based on its syntactic
importance score. The second point concerns inte-
grating semantics. Being definitely important, ?this
source of information remains relatively unused in
work on aggregation1 within NLG? (Reiter & Dale,
2000, p.141). To our knowledge, in the text-to-text
generation field, we are the first to use semantic in-
formation not only for alignment but also for aggre-
gation in that we check coarguments? compatibility.
Apart from that, our method is not limited to sen-
tence fusion and can be easily applied to sentence
compression. In Filippova & Strube (2008) we com-
press English sentences with the same approach and
achieve state-of-the-art performance.
The paper is organized as follows: Section 2 gives
an overview of related work and Section 3 presents
our data. Section 4 introduces our method and Sec-
tion 5 describes the experiments and discusses the
results of the evaluation. The conclusions follow in
the final section.
2 Related Work
Most studies on text-to-text generation concern sen-
tence compression where the input consists of ex-
actly one sentence (Jing, 2001; Hori & Furui, 2004;
Clarke & Lapata, 2008, inter alia). In such set-
ting, redundancy, incompleteness and compatibility
1We follow Barzilay & McKeown (2005) and refer to aggre-
gation within text-to-text generation as sentence fusion.
issues do not arise. Apart from that, there is no
obvious way of how existing sentence compression
methods can be adapted to sentence fusion.
Barzilay & McKeown (2005) present a sentence
fusion method for multi-document news summariza-
tion which crucially relies on the assumption that in-
formation appearing in many sources is important.
Consequently, their method produces an intersec-
tion of input sentences by, first, finding the centroid
of the input, second, augmenting it with informa-
tion from other sentences and, finally, pruning a pre-
defined set of constituents (e.g. PPs). The resulting
structure is not necessarily a tree and allows for ex-
traction of several trees, each of which can be lin-
earized in many ways.
Marsi & Krahmer (2005) extend the approach of
Barzilay & McKeown to do not only intersection
but also union fusion. Like Barzilay & McKeown
(2005), they find the best linearization with a lan-
guage model which, as they point out, often pro-
duces inadequate rankings being unable to deal with
word order, agreement and subcategorization con-
straints. In our work we aim at producing a valid
dependency tree structure so that most grammatical-
ity issues are resolved before the linearization stage.
Wan et al (2007) introduce a global revision
method of how a novel sentence can be generated
from a set of input words. They formulate the prob-
lem as a search for a maximum spanning tree which
is incrementally constructed by connecting words or
phrases with dependency relations. The grammat-
icality issue is addressed by a number of hard con-
straints. As Wan et al point out, one of the problems
with their method is that the output built up from
dependencies found in a corpus might have a mean-
ing different from the intended one. Since we build
our trees from the input dependencies, this problem
does not arise with our method. Apart from that, in
our opinion, the optimization formulation we adopt
is more appropriate as it allows to integrate many
constraints without complex rescoring rules.
3 Data
The comparable corpus we work with is a collection
of about 400 biographies in German gathered from
178
the Internet2. These biographies describe 140 differ-
ent people, and the number of articles for one person
ranges from 2 to 4, being 3 on average. Despite ob-
vious similarities between articles about one person,
neither identical content nor identical ordering of in-
formation can be expected.
Fully automatic preprocessing in our system com-
prises the following steps: sentence boundaries are
identified with a Perl CPAN module3. Then the
sentences are split into tokens and the TnT tagger
(Brants, 2000) and the TreeTagger (Schmid, 1997)
are used for tagging and lemmatization respectively.
Finally, the biographies are parsed with the CDG de-
pendency parser (Foth & Menzel, 2006). We also
identify references to the biographee (pronominal as
well as proper names) and temporal expressions (ab-
solute and relative) with a few rules.
4 Our Method
Groups of related sentences serve as input to a sen-
tence fusion system and thus need to be identified
first (4.1). Then the dependency trees of the sen-
tences are modified (4.2) and aligned (4.3). Syntac-
tic importance (4.4) and word informativeness (4.5)
scores are used to extract a new dependency tree
from a graph of aligned trees (4.6). Finally, the tree
is linearized (4.7).
4.1 Sentence Alignment
Sentence alignment for comparable corpora requires
methods different from those used in machine trans-
lation for parallel corpora. For example, given two
biographies of a person, one of them may follow the
timeline from birth to death whereas the other may
group events thematically or tell only about the sci-
entific contribution of the person. Thus one can-
not assume that the sentence order or the content
is the same in two biographies. Shallow methods
like word or bigram overlap, (weighted) cosine or
Jaccard similarity are appealing as they are cheap
and robust. In particular, Nelken & Schieber (2006)
2http://de.wikipedia.org, http://home.
datacomm.ch/biografien, http://biographie.
net/de, http://www.weltchronik.de/ws/bio/
main.htm, http://www.brockhaus-suche.de/
suche
3http://search.cpan.org/?holsten/
Lingua-DE-Sentence-0.07/Sentence.pm
demonstrate the efficacy of a sentence-based tf*idf
score when applied to comparable corpora. Follow-
ing them, we define the similarity of two sentences
sim(s1, s2) as
S1 ? S2
|S1| ? |S2|
=
?
t wS1(t) ? wS2(t)
?
?
t w2S1(t)
?
t w2S2(t)
(1)
where S is the set of all lemmas but stop-words from
s, and wS(t) is the weight of the term t:
wS(t) = S(t)
1
Nt
(2)
where S(t) is the indicator function of S, Nt is the
number of sentences in the biographies of one per-
son which contain t. We enhance the similarity mea-
sure by looking up synonymy in GermaNet (Lem-
nitzer & Kunze, 2002).
We discard identical or nearly identical sen-
tences (sim(s1, s2) > 0.8) and greedily build
sentence clusters using a hierarchical groupwise-
average technique. As a result, one sentence may
belong to one cluster at most. These sentence clus-
ters serve as input to the fusion algorithm.
4.2 Dependency Tree Modification
We apply a set of transformations to a dependency
tree to emphasize its important properties and elim-
inate unimportant ones. These transformations are
necessary for the compression stage. An example of
a dependency tree and its modifed version are given
in Fig. 1.
PREP preposition nodes (an, in) are removed and
placed as labels on the edges to the respective
nouns;
CONJ a chain of conjuncts (Mathematik und
Physik) is split and each node is attached to the
parent node (studierte) provided they are not
verbs;
APP a chain of words analyzed as appositions by
CDG (Niels Bohr) is collapsed into one node;
FUNC function words like determiners (der), aux-
iliary verbs or negative particles are removed
from the tree and memorized with their lexical
heads (memorizing negative particles preserves
negation in the output);
179
Bohr
Mathematik
und
Physik
an in
Kopenhagen
der
Uni
studierte
subj obja pp
pp
kon
cj
pn
pn
det
(a) Dependency tree
studierte
root
s
bio
Mathematik
Physik Uni
Kopenhagen
obja
obja
an
in
subj
(b) Modified tree
Figure 1: The dependency tree of the sentence Bohr studierte Mathematik und Physik an der Uni in Kopenhagen
(Bohr studied mathematics and physics at university in Copenhagen) as produced by the parser (a) and after all
transformations applied (b)
ROOT every dependency tree gets an explicit root
which is connected to every verb node;
BIO all occurrences of the biographee (Niels Bohr)
are replaced with the bio tag.
4.3 Node Alignment
Once we have a group of two to four strongly related
sentences and their transformed dependency trees,
we aim at finding the best node alignment. We use
a simple, fast and transparent method and align any
two words provided that they
1. are content words;
2. have the same part-of-speech;
3. have identical lemmas or are synonyms.
In case of multiple possibilities, which are extremely
rare in our data, the choice is made randomly. By
merging all aligned nodes we get a dependency
graph which consists of all dependencies from the
input trees. In case it contains a cycle, one of the
alignments from the cycle is eliminated.
We prefer this very simple method to bottom-up
ones (Barzilay & McKeown, 2005; Marsi & Krah-
mer, 2005) for two main reasons. Pursuing local
subtree alignments, bottom-up methods may leave
identical words unaligned and thus prohibit fusion
of complementary information. On the other hand,
they may force alignment of two unrelated words if
the subtrees they root are largely aligned. Although
in some cases it helps discover paraphrases, it con-
siderably increases chances of generating ungram-
matical output which we want to avoid at any cost.
4.4 Syntactic Importance Score
Given a dependency graph we want to get a new de-
pendency tree from it. Intuitively, we want to re-
tain obligatory dependencies (e.g. subject) while re-
moving less important ones (e.g. adv). When de-
ciding on pruning an argument, previous approaches
either used a set of hand-crafted rules (e.g. Barzilay
& McKeown (2005)), or utilized a subcategorization
lexicon (e.g. Jing (2001)). The hand-crafted rules
are often too general to ensure a grammatical argu-
ment structure for different verbs (e.g. PPs can be
pruned). Subcategorization lexicons are not readily
available for many languages and cover only verbs.
E.g. they do not tell that the noun son is very of-
ten modified by a PP using the preposition of, as in
the son of Niels Bohr, and that the NP without a PP
modifier may appear incomplete.
To overcome these problems, we decide on prun-
ing an edge by estimating the conditional proba-
bility of its label given its head, P (l|h)4. For ex-
ample, P (subj|studieren) ? the probability of the
label subject given the verb study ? is higher than
P (in|studieren), and therefore the subject will be
preserved whereas the prepositional label and thus
the whole PP can be pruned, if needed. Table 1
presents the probabilities of several labels given that
the head is studieren and shows that some preposi-
tions are more important than other ones. Note that
if we did not apply the PREP modification we would
be unable to distinguish between different prepo-
sitions and could only calculate P (pp|studieren)
4The probabilities are calculated from a corpus of approx.
3,000 biographies from Wikipedia which we annotated auto-
matically as described in Section 3.
180
which would not be very informative.
subj obja in an nach mit zu
0.88 0.74 0.44 0.42 0.09 0.02 0.01
Table 1: Probabilities of subj, obja(ccusative), in, at, af-
ter, with, to given the verb studieren (study)
4.5 Word Informativeness Score
We also want to retain informative words in the out-
put tree. There are many ways in which word im-
portance can be defined. Here, we use a formula
introduced by Clarke & Lapata (2008) which is a
modification of the significance score of Hori & Fu-
rui (2004):
I(wi) =
l
N ? fi log
FA
Fi
(3)
wi is the topic word (either noun or verb), fi is the
frequency of wi in the aligned biographies, Fi is the
frequency of wi in the corpus, and FA is the sum
of frequencies of all topic words in the corpus. l is
the number of clause nodes above w and N is the
maximum level of embedding of the sentence which
w belongs to. By defining word importance differ-
ently, e.g. as relatedness of a word to the topic, we
could apply our method to topic-based summariza-
tion (Krahmer et al, 2008).
4.6 New Sentence Generation
We formulate the task of getting a tree from a depen-
dency graph as an optimization problem and solve
it with ILP5. In order to decide which edges of the
graph to remove, for each directed dependency edge
from head h to word w we introduce a binary vari-
able xlh,w, where l stands for the label of the edge:
xlh,w =
{
1 if the dependency is preserved
0 otherwise
(4)
The goal is to find a subtree of the graph which
gets the highest score of the objective function (5) to
which both the probability of dependencies (P (l|h) )
and the importance of dependent words (I(w)) con-
tribute:
5We use lp solve in our implementation http://
sourceforge.net/projects/lpsolve.
f(X) =
?
x
xlh,w ? P (l|h) ? I(w) (5)
The objective function is subject to four types of
constraints presented below (W stands for the set of
graph nodes minus root, i.e. the set of words).
STRUCTURAL constraints allow to get a tree from
the graph: (6) ensures that each word has one head
at most. (7) ensures connectivity in the tree. (8) is
optional and restricts the size of the resulting tree to
? words (? = min(0.6? ? |W |, 10)).
?w ? W,
?
h,l
xlh,w ? 1 (6)
?w ? W,
?
h,l
xlh,w ?
1
|W |
?
u,l
xlw,u ? 0 (7)
?
x
xlh,w ? ? (8)
SYNTACTIC constraints ensure the syntactic validity
of the output tree and explicitly state which argu-
ments should be preserved. We have only one syn-
tactic constraint which guarantees that a subordinat-
ing conjunction (sc) is preserved (9) if and only if the
clause it belongs to serves as a subordinate clause
(sub) in the output.
?xscw,u,
?
h,l
xsubh,w ? xscw,u = 0 (9)
SEMANTIC constraints restrict coordination to se-
mantically compatible elements. The idea behind
these constraints is the following (see Fig. 2). It
can be that one sentence says He studied math and
another one He studied physics, so the output may
unite the two words under coordination: He studied
math and physics. But if the input sentences are He
studied physics and He studied sciences, then one
should not unite both, because sciences is the gen-
eralization of physics. Neither should one unite two
unrelated words: He studied with pleasure and He
studied with Bohr cannot be fused into He studied
with pleasure and Bohr.
To formalize these intuitions we define two func-
tions hm(w,u) and rel(w,u): hm(w,u) is a binary func-
tion, whereas rel(w,u) returns a value from [0, 1]. We
181
root
s
studied
sciencesbio
pleasure
mathphysics
subj
with
with
obja
obja
obja
Bohr
Figure 2: Graph obtained from sentences He studied sci-
ences with pleasure and He studied math and physics with
Bohr
also introduce additional variables ylw,u (represented
by dashed lines in Fig. 2):
ylw,u =
{
1 if ?h, l : xlh,w = 1 ? xlh,u = 1
0 otherwise
(10)
For two edges sharing a head and having identical
labels to be retained we check in GermaNet and
in the taxonomy derived from Wikipedia (Kassner
et al, 2008) that their dependents are not in the
hyponymy or meronymy relation (11). We prohibit
verb coordination unless it is found in one of the
input sentences. If the dependents are nouns, we
also check that their semantic relatedness as mea-
sured with WikiRelate! (Strube & Ponzetto, 2006)
is above a certain threshold (12). We empirically
determined the value of ? = 0.36 by calculating an
average similarity of coordinated nouns in the cor-
pus.
?ylw,u, hm(w, u) ? ylw,u = 0 (11)
?ylw,u, (rel(w, u) ? ?) ? ylw,u ? 0 (12)
(11) prohibits that physics (or math) and sciences ap-
pear together since, according to GermaNet, physics
(Physik) is a hyponym of science (Wissenschaft).
(12) blocks taking both pleasure (Freude) and Bohr
because rel(Freude,Bohr) = 0.17. math and physics
are neither in ISA, nor part-of relation and are suffi-
ciently related (rel(Mathematik, Physik) = 0.67) to
become conjuncts.
META constraints (equations (13) and (14)) guar-
antee that ylw,u = xlh,w ? xlh,u i.e. they ensure that
the semantic constraints are applied only if both the
labels from h to w and from h to u are preserved.
?ylw,u, xlh,w + xlh,u ? 2ylw,u (13)
?ylw,u, 1 ? xlh,w + 1 ? xlh,u ? 1 ? ylw,u (14)
4.7 Linearization
The ?overgenerate-and-rank? approach to statisti-
cal surface realization is very common (Langk-
ilde & Knight, 1998). Unfortunately, in its sim-
plest and most popular version, it ignores syntac-
tical constraints and may produce ungrammatical
output. For example, an inviolable rule of Ger-
man grammar states that the finite verb must be in
the second position in the main clause. Since it is
hard to enforce such rules with an ngram language
model, syntax-informed linearization methods have
been developed for German (Ringger et al, 2004;
Filippova & Strube, 2007). We apply our recent
method to order constituents and, using the CMU
toolkit (Clarkson & Rosenfeld, 1997), build a tri-
gram language model from Wikipedia (approx. 1GB
plain text) to find the best word order within con-
stituents. Some constraints on word order are in-
ferred from the input. Only interclause punctuation
is generated.
5 Experiments and Evaluation
We choose Barzilay & McKeown?s system as a non-
trivial baseline since, to our knowledge, there is no
other system which outperforms theirs (Sec. 5.1). It
is important for us to evaluate the fusion part of our
system, so the input and the linearization module of
our method and the baseline are identical. We are
also interested in how many errors are due to the lin-
earization module and thus define the readability up-
per bound (Sec. 5.2). We further present and discuss
the experiments (Sec. 5.3 and 5.5).
5.1 Baseline
The algorithm of Barzilay & McKeown (2005) pro-
ceeds as follows: Given a group of related sentences,
a dependency tree is built for each sentence. These
trees are modified so that grammatical features are
eliminated from the representation and memorized;
noun phrases are flattened to facilitate alignment.
A locally optimal pairwise alignment of modified
182
dependency trees is recursively found with Word-
Net and a paraphrase lexicon. From the alignment
costs the centroid of the group is identified. Then
this tree is augmented with information from other
trees given that it appears in at least half of the sen-
tences from this group. A rule-based pruning mod-
ule prunes optional constituents, such as PPs or rel-
ative clauses. The linearization of the resulting tree
(or graph) is done with a trigram language model.
To adapt this system to German, we use the Ger-
maNet API (Gurevych & Niederlich, 2005) instead
of WordNet. We do not use a paraphrase lexicon,
because there is no comparable corpus of sufficient
size available for German. We readjust the align-
ment parameters of the system to prevent dissimi-
lar nodes from being aligned. The input to the al-
gorithm is generated as described in Sec. 4.1. The
linearization is done as described in Sec. 4.7. In
cases when there is a graph to linearize, all possible
trees covering the maximum number of nodes are
extracted from it and linearized. The most probable
string is selected as the final output with a language
model. For the rest of the reimplementation we fol-
low the algorithm as presented.
5.2 Readability Upper Bound
To find the upper bound on readability, we select one
sentence from the input randomly, parse it and lin-
earize the dependency tree as described in Sec. 4.7.
This way we obtain a sentence which may differ in
form from the input sentences but whose content is
identical to one of them.
5.3 Experiments
It is notoriously difficult to evaluate generation and
summarization systems as there are many dimen-
sions in which the quality of the output can be as-
sessed. The goal of our present evaluation is in the
first place to check whether our method is able to
produce sensible output.
We evaluated the three systems (GRAPH-
COMPRESSION, BARZILAY & MCKEOWN and
READABILITY UB) with 50 native German speakers
on 120 fused sentences generated from 40 randomly
drawn related sentences groups (3 ? 40). In an
online experiment, the participants were asked to
read a fused sentence preceded by the input and
to rate its readability (read) and informativity in
respect to the input (inf ) on a five point scale. The
experiment was designed so that every participant
rated 40 sentences in total. No participant saw
two sentences generated from the same input. The
results are presented in Table 2. len is an average
length in words of the output.
read inf len
READABILITY UB 4.0 3.5 12.9
BARZILAY & MCKEOWN 3.1 3.0 15.5
GRAPH-COMPRESSION 3.7 3.1 13.0
Table 2: Average readability and informativity on a five
point scale, average length in words
5.4 Error Analysis
The main disadvantage of our method, as well as
other methods designed to work on syntactic struc-
tures, is that it requires a very accurate parser. In
some cases, errors in the preprocessing made ex-
tracting a valid dependency tree impossible. The
poor rating of READABILITY UB also shows that er-
rors of the parser and of the linearization module af-
fect the output considerably.
Although the semantic constraints ruled out
many anomalous combinations, the limited cover-
age of GermaNet and the taxonomy derived from
Wikipedia was the reason for some semantic oddi-
ties in the sentences generated by our method. For
example, it generated phrases like aus England und
Gro?britannien (from England and Great Britain).
A larger taxonomy would presumably increase the
recall of the semantic constraints which proved help-
ful. Such errors were not observed in the output of
the baseline because it does not fuse within NPs.
Both the baseline and our method made subcate-
gorization errors, although these are more common
for the baseline which aligns not only synonyms
but also verbs which share some arguments. Also,
the baseline pruned some PPs necessary for a sen-
tence to be complete. For example, it pruned an
der Atombombe (on the atom bomb) and generated
an incomplete sentence Er arbeitete (He worked).
For the baseline, alignment of flattened NPs instead
of words caused generating very wordy and redun-
dant sentences when the input parse trees were in-
correct. In other cases, our method made mistakes
183
in linearizing constituents because it had to rely on a
language model whereas the baseline used unmod-
ified constituents from the input. Absense of intra-
clause commas caused a drop in readability in some
otherwise grammatical sentences.
5.5 Discussion
A paired t-test revealed significant differences be-
tween the readability ratings of the three systems
(p = 0.01) but found no significant differences be-
tween the informativity scores of our system and the
baseline. Some participants reported informativity
hard to estimate and to be assessable for grammat-
ical sentences only. The higher readability rating
of our method supports our claim that the method
based on syntactic importance score and global con-
straints generates more grammatical sentences than
existing systems. An important advantage of our
method is that it addresses the subcategorization is-
sue directly without shifting the burden of selecting
the right arguments to the linearization module. The
dependency structure it outputs is a tree and not a
graph as it may happen with the method of Barzi-
lay & McKeown (2005). Moreover, our method can
distinguish between more and less obligatory argu-
ments. For example, it knows that at is more impor-
tant than to for study whereas for go it is the other
way round. Unlike our differentiated approach, the
baseline rule states that PPs can generally be pruned.
Since the baseline generates a new sentence by
modifying the tree of an input sentence, in some
cases it outputs a compression of this sentence. Un-
like this, our method is not based on an input tree
and generates a new sentence without being biased
to any of the input sentences.
Our method can also be applied to non-trivial sen-
tence compression, whereas the baseline and similar
methods, such as Marsi & Krahmer (2005), would
then boil down to a few very general pruning rules.
We tested our method on the English compression
corpus6 and evaluated the compressions automati-
cally the same way as Clarke & Lapata (2008) did.
The results (Filippova & Strube, 2008) were as good
as or significantly better than the state-of-the-art, de-
pending on the choice of dependency parser.
6The corpus is available from http://homepages.
inf.ed.ac.uk/s0460084/data.
6 Conclusions
We presented a novel sentence fusion method which
formulates the fusion task as an optimization prob-
lem. It is unsupervised and finds a globally optimal
solution taking semantics, syntax and word informa-
tiveness into account. The method does not require
hand-crafted rules or lexicons to generate grammat-
ical output but relies on the syntactic importance
score calculated from an automatically parsed cor-
pus. An experiment with native speakers demon-
strated that our method generates more grammatical
sentences than existing systems.
There are several directions to explore in the fu-
ture. Recently query-based sentence fusion has been
shown to be a better defined task than generic sen-
tence fusion (Krahmer et al, 2008). By modify-
ing the word informativeness score, e.g. by giving
higher scores to words semantically related to the
query, one could force our system to retain words
relevant to the query in the output. To generate co-
herent texts we plan to move beyond sentence gen-
eration and add discourse constraints to our system.
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004). Part of the data has been used
with a permission of Bibliographisches Institut & F.
A. Brockhaus AG, Mannheim, Germany. We would
like to thank the participants in our online evalua-
tion. We are also grateful to Regina Barzilay and the
three reviewers for their helpful comments.
References
Barzilay, Regina & Kathleen R. McKeown (2005). Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?327.
Brants, Thorsten (2000). TnT ? A statistical Part-of-
Speech tagger. In Proceedings of the 6th Confer-
ence on Applied Natural Language Processing, Seat-
tle, Wash., 29 April ? 4 May 2000, pp. 224?231.
Clarke, James & Mirella Lapata (2008). Global inference
for sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search, 31:399?429.
Clarkson, Philip & Ronald Rosenfeld (1997). Statis-
tical language modeling using the CMU-Cambridge
toolkit. In Proceedings of the 5th European Con-
ference on Speech Communication and Technology,
184
Rhodes, Greece, 22-25 September 1997, pp. 2707?
2710.
Filippova, Katja & Michael Strube (2007). Generating
constituent order in German clauses. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics, Prague, Czech Republic, 23?30
June 2007, pp. 320?327.
Filippova, Katja & Michael Strube (2008). Dependency
tree based sentence compression. In Proceedings of
the 5th International Conference on Natural Language
Generation, Salt Fork, Ohio, 12?14 June 2008, pp. 25?
32.
Foth, Kilian & Wolfgang Menzel (2006). Hybrid pars-
ing: Using probabilistic models as predictors for a
symbolic parser. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, Sydney, Australia, 17?21 July 2006,
pp. 321?327.
Gurevych, Iryna & Hendrik Niederlich (2005). Access-
ing GermaNet data and computing semantic related-
ness. In Companion Volume to the Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, Mich., 25?30 June
2005, pp. 5?8.
Hori, Chiori & Sadaoki Furui (2004). Speech summa-
rization: An approach through word extraction and a
method for evaluation. IEEE Transactions on Infor-
mation and Systems, E87-D(1):15?25.
Hovy, Eduard (2003). Text summarization. In Ruslan
Mitkov (Ed.), The Oxford Handbook of Computational
Linguistics, pp. 583?598. Oxford, U.K.: Oxford Uni-
versity Press.
Jing, Hongyan (2001). Cut-and-Paste Text Summariza-
tion, (Ph.D. thesis). Computer Science Department,
Columbia University, New York, N.Y.
Kassner, Laura, Vivi Nastase & Michael Strube (2008).
Acquiring a taxonomy from the German Wikipedia.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation, Marrakech, Mo-
rocco, 26 May ? 1 June 2008.
Krahmer, Emiel, Erwin Marsi & Paul van Pelt (2008).
Query-based sentence fusion is better defined and
leads to more preferred results than generic sentence
fusion. In Companion Volume to the Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, Columbus, Ohio, 15?20 June
2008, pp. 193?196.
Langkilde, Irene & Kevin Knight (1998). Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 17th International Conference on
Computational Linguistics and 36th Annual Meet-
ing of the Association for Computational Linguistics,
Montre?al, Que?bec, Canada, 10?14 August 1998, pp.
704?710.
Lemnitzer, Lothar & Claudia Kunze (2002). GermaNet
? representation, visualization, application. In Pro-
ceedings of the 3rd International Conference on Lan-
guage Resources and Evaluation, Las Palmas, Canary
Islands, Spain, 29?31 May 2002, pp. 1485?1491.
Marsi, Erwin & Emiel Krahmer (2005). Explorations in
sentence fusion. In Proceedings of the European Work-
shop on Natural Language Generation, Aberdeen,
Scotland, 8?10 August, 2005, pp. 109?117.
McKeown, Kathleen R., Judith L. Klavans, Vassileios
Hatzivassiloglou, Regina Barzilay & Eleazar Eskin
(1999). Towards multidocument summarization by re-
formulation: Progress and prospects. In Proceedings
of the 16th National Conference on Artificial Intelli-
gence, Orlando, Flo., 18?22 July 1999, pp. 453?460.
Nelken, Rani & Stuart Schieber (2006). Towards robust
context-sensitive sentence alignment for monolingual
corpora. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, Trento, Italy, 3?7 April 2006, pp.
161?168.
Reiter, Ehud & Robert Dale (2000). Building Natu-
ral Language Generation Systems. Cambridge, U.K.:
Cambridge University Press.
Ringger, Eric, Michael Gamon, Robert C. Moore, David
Rojas, Martine Smets & Simon Corston-Oliver (2004).
Linguistically informed statistical models of con-
stituent structure for ordering in sentence realization.
In Proceedings of the 20th International Conference
on Computational Linguistics, Geneva, Switzerland,
23?27 August 2004, pp. 673?679.
Schmid, Helmut (1997). Probabilistic Part-of-Speech
tagging using decision trees. In Daniel Jones & Harold
Somers (Eds.), New Methods in Language Processing,
pp. 154?164. London, U.K.: UCL Press.
Spa?rck Jones, Karen (1999). Automatic summarizing:
Factors and directions. In Inderjeet Mani & Mark T.
Maybury (Eds.), Advances in Automatic Text Summa-
rization, pp. 1?12. Cambridge, Mass.: MIT Press.
Strube, Michael & Simone Paolo Ponzetto (2006).
WikiRelate! Computing semantic relatedness using
Wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence, Boston, Mass., 16?
20 July 2006, pp. 1419?1424.
Wan, Stephen, Robert Dale, Mark Dras & Cecile Paris
(2007). Global revision in summarization: Generating
novel sentences with Prim?s algorithm. In Proceedings
of the 10th Conference of the Pacific Association for
Computational Linguistics, Melbourne, Australia, 19?
21 September, 2007, pp. 226?235.
185
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 910?918,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Combining Collocations, Lexical and Encyclopedic Knowledge
for Metonymy Resolution
Vivi Nastase and Michael Strube
EML Research gGmbH
Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
This paper presents a supervised method
for resolving metonymies. We enhance
a commonly used feature set with fea-
tures extracted based on collocation in-
formation from corpora, generalized us-
ing lexical and encyclopedic knowledge
to determine the preferred sense of the
potentially metonymic word using meth-
ods from unsupervised word sense disam-
biguation. The methodology developed
addresses one issue related to metonymy
resolution ? the influence of local context.
The method developed is applied to the
metonymy resolution task from SemEval
2007. The results obtained, higher for the
countries subtask, on a par for the compa-
nies subtask ? compared to participating
systems ? confirm that lexical, encyclo-
pedic and collocation information can be
successfully combined for metonymy res-
olution.
1 Introduction
Metonymies are a pervasive phenomenon in
language. They occur because in communicating,
we use words as pointers to a larger body of
knowledge, that encompasses various facets of the
concept evoked by a given word.
A listener need not understand the cello to
be moved by its playing, just as it is unnecessary
for a rider to understand technical jargon; all
that matters is sensation, and here the Kawasaki
excels. The cockpit is sensibly designed, with a
narrow front seat portion ...
Kawasaki is a company, it has an organization, fa-
cilities, employees, it makes specific products. In
the context above, the company name stands in
for its products ? motorcycles. Motorcycles have
parts, the cockpit and front seat are some of them,
and this provides the discourse links between the
two sentences. Constraints on the interpretation
of a word w in context comes both from the local
and global context, and are applied to the infor-
mation/knowledge evoked by w. The local con-
straints come from the words with which w is
(grammatically) related to. The global constraints
come from the domain/topic of the text, discourse
relations that span across sentences.
Metonymic words have a rather small num-
ber of possible interpretations (also called read-
ings) which occur frequently (Markert and Nissim,
2002). Idiosyncratic interpretations are also pos-
sible, but very rare. One can view the possible
interpretations of a potentially metonymic word
(PMW) as corresponding to the word?s possible
senses (Nissim and Markert, 2003), bringing the
task close to word sense disambiguation.
The approach to metonymy resolution pre-
sented here is supervised, with unsupervised fea-
ture enrichment. We apply techniques inspired by
unsupervised word sense disambiguation, which
allow us to go beyond the annotated data provided
in training, and quantify the restrictions imposed
on the interpretation of a PMW by its grammat-
ically related neighbours through collocation in-
formation extracted from corpora. The only anno-
tation required for the corpora are automatically
induced part-of-speech tags from which we ob-
tain grammatical relations through regular expres-
sion matching over sequences of parts-of-speech.
Collocation information is combined with lexical
resources ? WordNet ? and encyclopedic knowl-
edge extracted from Wikipedia to help us gener-
alize the collocations found to determine higher
level constraints on a word?s grammatical collo-
cates. In the example above, Kawasaki is gram-
matically related to the verb excel ? it is its sub-
ject. To determine the most likely interpretation
of Kawasaki given that it is in the subject relation
with excel we look at all the nouns in the corpora
910
that appear as this verb?s subjects, and estimate
from this the preferences excel has for its subjects.
Let us say the corpus contains the following col-
locations in subject position (with frequency in-
formation in parentheses): player (4), musician
(50), car (30), computer (12), camera (40), driver
(55), bike (20) .... The knowledge resources ?
WordNet, isa relations extracted from Wikipedia
? will help generalize these collocations: player,
musician, driver to person and car, computer,
camera, bike to artifact. This together with
frequency of occurrence are used to estimate the
probability that the verb excel takes a person or
artifact-type subject. These are excel?s se-
lectional preferences towards certain collocates,
and will help determine which possible interpre-
tation for the PMW Kawasaki is appropriate in
this context ? organization-for-people
or organization-for-product.
The paper continues with related work in Sec-
tion 2 and the description of the data in Section 3.
The representation used is introduced in Section
4. The results and the discussion are presented in
Section 5. The paper wraps up with conclusions
and future work.
2 Related Work
Analysis of metonymies as a linguistic phe-
nomenon dates back at least to the 1930s (Stern,
1931), and are increasingly recognized as an im-
portant phenomenon to tackle in the interest of
higher level language processing tasks, such as
anaphora resolution (Harabagiu, 1998; Markert
and Hahn, 2002), question answering (Stallard,
1993) or machine translation (Kamei and Wakao,
1992).
Until the early 90s, the main view about
metonymies was that they violate semantic con-
straints in their immediate context. To resolve
metonymies then amounts to detecting violated
constraints, usually from those imposed by the
verbs on their arguments (Pustejovsky, 1991;
Hobbs et al, 1993; Fass, 1991). Markert and
Hahn (2002) showed that this approach misses
metonymies which do not violate selectional re-
strictions. In this case referential cohesion re-
lations may indicate that the literal reading is
not appropriate and give clues about the intended
metonymic interpretation.
Markert and Nissim (2003) have combined
observations from the linguistic analysis of
metonymies with results of corpus studies. Lin-
guistic research has postulated that (i) conven-
tional metonymic readings are very systematic;
(ii) unconventional metonymies can be created on
the fly and their interpretation is context depen-
dent; (iii) metonymies are frequent. The fact
that most metonymic interpretations are system-
atic and correspond to a small set of possible read-
ings allow the metonymy resolution to be mod-
elled as a classifier learning task. Markert and Nis-
sim (2002) and Nissim and Markert (2003) have
shown that conventional metonymies can be effec-
tively resolved using a supervised machine learn-
ing approach. Moreover, grammatically related
words are crucial in determining the interpretation
of a PMW. The shortcoming is that manually an-
notated data is in short supply, and the approach
suffers from data sparseness. To address this prob-
lem, Nissim and Markert (2003) proposed a word
similarity-based method. They use Lin?s thesaurus
(Lin, 1998) to determine how close two lexical
heads are, and use this instead of the more re-
strictive identity constraint when comparing two
instances. This technique is complex, requiring
smoothing, multiple iterations over the thesaurus
and hybrid methods to allow a back-off to gram-
matical roles.
The supervised approach to resolving
metonymies was encouraged by the metonymy
resolution task at the semantic evaluation exercise
SemEval 2007 (Markert and Nissim, 2007). The
participating systems in this task were varied.
Most of them (four out of five) have used super-
vised machine learning techniques. The systems
that beat the baseline used either the grammatical
annotations provided by the organizers (Farkas
et al, 2007; Nicolae et al, 2007), or a robust
and deep (not freely available) parser (Brun et
al., 2007). These systems represented instances
in a manner similar to (Nissim and Markert,
2005). They used additional manually built
resources ? WordNet, FrameNet, Levin?s verb
classes, manually built lists of ?trigger? words
? to generalize the existing features. Brun et
al. (2007) also used the British National Corpus
(BNC) for computing the distance between words
based on their syntactic distribution.
While lexical resources and corpora are used
to estimate word similarity, all these systems rely
exclusively on the data provided by the organiz-
ers ? instance representation captures only infor-
mation that can be derived from or between the
data points provided. The approach presented here
goes beyond the given data, and induces from cor-
pora measures that allow the system to determine
911
what are the preferences of the words surround-
ing a PMW towards each of PMW?s possible read-
ings. The technique employed is adapted from
unsupervised word sense disambiguation (WSD).
In short, we use the local grammatical context
as it is commonly used in WSD approaches, to
guide the system in choosing the reading that fits
best. The benefits of using grammatical informa-
tion for automatic WSD were first explored by
Yarowsky (1995) and Resnik (1996) in unsuper-
vised approaches to disambiguating single words
in context. The method described here uses au-
tomatically induced selectional preferences, com-
puted from sense-untagged data, similar to Nas-
tase (2008).
3 Data
We work with the data from the metonymy reso-
lution task at SemEval 2007 (Markert and Nissim,
2007), generated based on a scheme developed by
Markert and Nissim (2003).
The metonymy resolution task at SemEval 2007
consisted of two subtasks ? one for resolving
country names, the other for companies. For each
subtask there is a training and a test portion. Fig-
ure 1 shows the text fragment for one sample,
and Table 1 the data statistics. The reading col-
umn shows the possible interpretations of a PMW
for countries and companies respectively. For ex-
ample, org-for-product would be the inter-
pretation of the PMW Kawasaki in the example
shown in the introduction.
Occurrences of country and company names
were annotated with a small number of possi-
ble readings, as shown in Table 1. This reflects
previous analyses of the metonymy phenomenon,
which showed that there is a rather small number
of possible interpretations that appear more fre-
quently (Markert and Nissim, 2002). Special in-
terpretations are very rarely encountered.
Within the framework of the SemEval task,
metonymy resolution is evaluated on the
given test data, on three levels of granular-
ity: coarse ? distinguish between literal and
non-literal readings; medium ? distinguish
between literal, mixed and non-literal
readings; fine ? identify the specific reading of the
target word/words (potentially metonymic word -
PMW).
4 Representation
The method presented in this paper is a supervised
learning method, along the same general lines as
reading train test
locations 925 908
literal 737 721
mixed 15 20
othermet 9 11
obj-for-name 0 4
obj-for-representation 0 0
place-for-people 161 141
place-for-event 3 10
place-for-product 0 1
organizations 1090 842
literal 690 520
mixed 59 60
othermet 14 8
obj-for-name 8 6
obj-for-representation 1 0
org-for-members 220 161
org-for-event 2 1
org-for-product 74 67
org-for-facility 15 16
org-for-index 7 3
Table 1: Reading distributions
the systems which participated in the SemEval
competition. As such, it represents each PMW in
the data through features that describe its context
and some semantic characteristics. The minimum
set of necessary features is taken to be that pre-
sented by Nissim and Markert (2005), and proved
to be effective in solving metonymies. These
are the M&N features (Markert and Nissim fea-
tures). We expand on these features and estimate
preferences from words in a PMW?s context to-
wards specific PMW interpretations. These con-
stitute the selectional preference features. Finally,
Wikipedia is a source of facts which can be used
to derive information that can bias the decision to-
wards certain interpretations for a PMW. Each of
these features are described in more detail in the
following subsections.
4.1 M&N features
The features used by Nissim and Markert (2005)
are:
? grammatical role of PMW (subj, obj, ...);
? lemmatized head/modifier of PMW (an-
nounce, say, ...);
? determiner of PMW (def, indef, bare,
demonst, other, ...);
912
XML tagged text
<sample id=?samp114?>
<bnc:title> Computergram international
</bnc:title>
<par>
LITTLE FEAR OF MICHELANGELO
The computer industry equivalent of ?Small
earthquake in Chile? ...
The Michelangelo computer virus that received
worldwide attention last year is expected to cause
even fewer problems this Saturday than it did
when it struck last year, a team of <annot><org
reading=?literal?> IBM </org></annot> re-
searchers said.
</par>
</sample>
Grammatical annotations
SampleID|Lemma|PMW|GrRole|Reading
samp114|researcher|IBM|premod|literal
samp4|be|Williams Holdings|subj|literal
samp5|parent|Fujitsu Ltd|app|mixed
samp5|have|Fujitsu Ltd|subj|mixed
samp5|keep|Fujitsu Ltd|subj|mixed
samp8|against|IBM|pp|literal
POS tags
<bnc:s id=?samp114-bncCNJ-s341?> ...
<bnc:w id=?samp114-bncCNJ-s343-w29?
bnc:type=?NN0?> team </bnc:w> <bnc:w
id=?samp114-bncCNJ-s343-w30?
bnc:type=?PRF?> of </bnc:w> <annot> <org
reading=?literal?> <bnc:w possmeto=?yes?
id=?samp114-bncCNJ-s343-w31?
bnc:type=?NP0?> IBM </bnc:w> </org>
</annot> <bnc:w
id=?samp114-bncCNJ-s343-w32?
bnc:type=?NN2?> researchers </bnc:w> ...
Figure 1: Sample annotation
? grammatical number of PMW (sg, pl);
? number of grammatical roles in which the
PMW appears in its current context;
? number of words in PMW;
All these features can be extracted from the
grammatically annotated and POS tagged data
provided by the organizers.
4.2 Selectional preference features
The grammatical relations and the connected
words are important to describe the local context
of the target PMW. Because of the limited amount
of annotated data (a few thousand instances), lem-
mas of PMW?s grammatically related words will
make for very sparse data that a machine learn-
ing system would not be able to generalize over.
Nissim and Markert (2003) and the teams partici-
pating in the metonymy resolution task have then
supplemented their systems with Lin?s thesaurus,
WordNet, Beth Levin?s verb groups, FrameNet in-
formation, or manually designed lists of words to
generalize the grammatically related words and
thus find shared characteristics across instances of
metonymies in text.
The notion of selectional restrictions used in
metonymy resolution ? meaning the restrictions
imposed on the interpretation of a PMW by its
context ? is similar to the notion of selectional
preferences from word sense disambiguation ?
meaning the preferences of a word for the senses
of the words in its context. We import this no-
tion, and compute selectional preferences for the
words in a PMW?s (grammatical) neighbourhood,
and allow them to influence the chosen reading for
the PMW. Applying methods from unsupervised
WSD allow us to estimate such preferences from
(sense/metonymy) untagged corpora.
A potentially metonymic word (or phrase) has
a small number of possible readings. These can
be viewed as possible senses, and the task is to
choose the one that fits best in the given context.
The preference for each possible sense can be
determined based on the PMW?s grammatically
related words. To estimate these sense preferences
we use grammatical collocations extracted from
the British National Corpus (BNC), detected
using regular expression matching over sequences
of POS using the Word Sketch Engine (Kilgarriff
et al, 2004). The scores are computed following
a technique similar to Nastase (2008), which is
illustrated using the following example:
The Kawasaki drives well, steers brilliantly
both under power and in tight corners ...
The PMW Kawasaki is involved in the follow-
ing grammatical relations in the previous sentence:
(drive,subject,Kawasaki)
(steer,subject,Kawasaki)
913
SampleID Lemma PMW GrRole Reading act animal artifact ... person ...
samp190 say Sun subj org-for-members 0.00056 0.01171 0.01958 ... 0.61422 ...
samp190 claim Sun subj org-for-members 0.00198 0.00099 0.00893 ... 0.50211 ...
Table 2: Grammatical annotation file enhanced with selectional preference estimates
The BNC provides the collocations
(drive,subject,X) and (steer,subject,Y), to de-
termine what kind of subject drive and steer
prefer, in ?word-POS:frequency? format:
drive subject chauffeur-n:12, engine-
n:30, car-n:62, taxi-n:13,
motorist-n:10, disk-n:15,
truck-n:11, man-n:75, ...
steer subject power-n:6, car-n:3, sport-
n:2, firm-n:2, boy-n:2,
government-n:2, man-n:2,
people-n:2 ...
The target whose interpretation must be deter-
mined is Kawasaki. If for a potentially metonymic
word representing a company name, there are the
following possible interpretations: company,
member/person, product/artifact,
facility, name, we compute the preference
for each of these interpretations based on the
extracted collocations. For the verb drive for
example, the collocations engine, car, taxi, truck
are all artifacts (according to WordNet), and
thus vote for the product/artifact reading,
while chauffeur, motorist, man are all person,
and vote for the member/person reading.
Preferences from different grammatical relation
for the same PMW are summed.
Formally, we choose the PMWs? ?senses? ?
a set of words which are close to the possible
readings of metonymic words in the data. In
this work, these senses are the WordNet 3.0
supersenses:
S = { act, animal, artifact,
attribute, body, cognition,
communication, event, feeling,
food, group, location, motive,
object, person, phenomenon,
plant, possession, process,
quantity, relation, shape, state,
substance, time }.
Because none of these can be seen as a sense
for ?company?, the list is supplemented with
company and organization. Granted, there
is no 1:1 mapping from these supersenses to PMW
readings, but find such a strict correspondence is
not necessary because the context preferences for
each of these senses are used as features, and the
mapping to PMW readings is found through a su-
pervised learned model.
To compute the preference of a word w in
the grammatical context of a PMW t (the target)
towards each of t?s possible senses, we consider
each relation (w,R, t), where R is the grammati-
cal relation. The set C of word collocations are
extracted from the BNC
C = {(w,R,w
j
: f
j
)|(w,R,w
j
) ? BNC,
f
j
is the frequency of occurrence}
and used to compute a preference score P
s
i
for each sense s
i
? S:
P
s
i
=
?
(w,R,w
i,j
:f
i,j
)?C
s
i
f
i,j
?
(w,R,w
j
:f
j
)?C
f
j
where
C
s
i
= {(w,R,w
j
: f
j
)|(w,R,w
j
: f
j
) ? C;
supersense(w
j
, s
i
) ? isa(w
j
, s
i
)}.
supersense(w
j
, s
i
) is true if s
i
is a super-
sense of one of w
j
?s senses;
isa(w
j
, s
i
) is true if s
i
is a hypernym of one
of w
j
?s senses in WordNet, or is a fact extracted
from Wikipedia.
To determine the supersense and isa relation we
use WordNet 3.0, and a set of 7,578,112 isa rela-
tions extracted by processing the page and cate-
gory network of Wikipedia1 (Nastase and Strube,
2008). The collocations extracted from BNC con-
tain numerous named entities, most of which are
not part of WordNet. If an isa relation be-
tween a collocate from the corpus w
j
and a pos-
sible sense of a PMW s
i
cannot be established us-
ing supersense information (for the supersenses)
or through transitive closure in the hypernym-
hyponym hierarchy in WordNet (for company
1http://www/eml-research.de/nlp/
download/wikirelations.php
914
and organization) for any sense of w
j
, it is
tried against the Wikipedia-based links.
This process transforms the grammatical anno-
tation file and enhances it with the collocation es-
timates, as shown in Table 2 (compare this with a
sample of the original file presented in Figure 1).
4.3 Product and event features
Farkas et al (2007) observed that using the PMWs
themselves as features leads to improvement on
determining the reading for organization names,
and postulate that this is because some company
names are more likely to be used in a metonymic
way. This is often the case with companies that
make products which are commonly used (cars,
for example).
Brun et al (2007) note that certain locations,
such as Vietnam, are more likely to be used with
an event reading than others locations. Generally,
locations strongly associated with events tend to
be used to refer to the event, and more often have
a place-for-event interpretation rather than
a literal one.
These two observations have lead us to mine for
these pieces of information in the Wikipedia rela-
tions, and to add two more features for a target
PMW:
has-product will take a value of 1 if any of the
PMW?s hypernyms (according to the isa re-
lations extracted from Wikipedia) contains
the string manufacturer, will have the value
0 otherwise;
has-event will have the value 1 if any of the
PMW?s hypernyms refers to an event (move-
ments/operations/riots), and value 0 other-
wise.
4.4 Data representation
As mentioned before, the representation built can
be seen as consisting of roughly three subsets of
features:
? the M&N features proposed by Nissim and
Markert (2005). To combine the grammati-
cal information from all relations, we trans-
form the grammatical relations into features
(as opposed to values). For a relation subject
for example, we generate a binary subject
feature that indicates whether for a given
target this grammatical relation is filled or
not, and a subject lemma feature , whose
value is the lemma of the grammatically re-
lated word.
? the selectional preference scores. Each of
these features corresponds to one of the ele-
ments of S, presented above. These features
combine the selectional preferences of all the
grammatical relations for one target PMW.
? product and event information from
Wikipedia ? has-product and has-event.
The grammatical annotation file consists of one
entry for each grammatical relation in which a
PMW appears. For the final representation, in-
formation about all relations of a given PMW is
compressed into one instance. Because the ba-
sic features were binarized, and instead of having
one grammatical role feature now each possible
grammatical relation has its own feature, combin-
ing several entries for one PMW is easy, as it only
implies setting the correct value for the grammati-
cal relations that are valid in the PMWs context.
The final representation consists of 63 features
+ class feature for the subset for company PMWs,
59 features + class feature for the subset contain-
ing countries PMWs. The sample ID and the
PMW itself were not part of this representation.
5 Results
The models for determining a PMW?s correct in-
terpretation are learned on the training data pro-
vided, and evaluated on the test portion, using
the answer keys and evaluation script provided
with the data. For learning the models we use
Weka (Witten and Frank, 2005), and select the
final learning algorithms based on 10-fold cross-
validation on the training data. We have settled on
support vector machines (SMO in Weka), and we
use the learner?s default settings.
Tables 3 and Table 4 show the results obtained,
and the baseline and the best results from the Sem-
Eval task for comparison (Markert and Nissim,
2007). The baseline in Table 3 corresponds to
classifying everything as the most frequent class
? literal interpretation. The M&N feat. and
M&N feat.bin. correspond to datasets that con-
tain only the M&N features and the binarized
versions of these features, respectively. SemEval
best gives the best results obtained on each task
in the SemEval 2007 task (Markert and Nissim,
2007). SMO
wiki
are the results obtained with the
complete feature set described in Section 4, and
SMO
SP
are the results obtained when only the
new features are used ? only selectional prefer-
ence, has-product and has-event features (none of
915
task ? method ? baseline SemEval best SMO
wiki
SMO
SP
M&N
feat
. M&N
feat.bin
.
LOCATION-COARSE 79.4 85.2 86.1 82.8 79.4 83.4
LOCATION-MEDIUM 79.4 84.8 85.9 82.6 79.4 82.3
LOCATION-FINE 79.4 84.1 85.0 82.0 79.4 81.3
ORGANIZATION-COARSE 61.8 76.7 74.9 66.6 73.8 74.0
ORGANIZATION-MEDIUM 61.8 73.3 72.4 65.0 69.8 69.4
ORGANIZATION-FINE 61.8 72.8 71.0 64.7 68.4 68.5
Table 3: Accuracy scores
task ? method ? base max SMO
wiki
SMO
LOCATION-COARSE
literal 79.4 91.2 91.6 91.6
non-literal 20.6 57.6 59.1 58.8
LOCATION-MEDIUM
literal 79.4 91.2 91.6 91.6
metonymic 18.4 58.0 61.5 61.5
mixed 2.2 8.3 16 8.7
LOCATION-FINE
literal 79.4 91.2 91.6 91.6
place-for-people 15.5 58.9 61.7 61.7
place-for-event 1.1 16.7 0 0
place-for-product 1.1 0 0 0
obj-for-name 0.4 66.7 0 0
obj-for-rep 0 0 0 0
othermet 1.2 0 0 0
mixed 2.2 8.3 16 8.7
ORGANIZATION-COARSE
literal 61.8 82.5 81.4 81.2
non-literal 38.2 65.2 61.6 60.7
ORGANIZATION-MEDIUM
literal 61.8 82.5 81.4 81.2
metonymic 31.0 60.4 58.7 58.1
mixed 7.2 30.8 26.8 28.9
ORGANIZATION-FINE
literal 61.8 82.6 81.4 81.2
org-for-members 19.1 63.0 59.7 59.2
org-for-event 0.1 0 0 0
org-for-product 8.0 50.0 44.4 44
org-for-facility 2.0 22.2 36.3 38.1
org-for-index 0.3 0 0 0
org-for-name 0.7 80.0 58.8 58.8
org-for-rep 0 0 0 0
othermet 1.0 0 0 0
mixed 7.2 34.3 27.1 29.3
Table 4: Detailed F-scores
the M&N features). The baseline for detailed read-
ing results in Table 4 reflects the distribution of
the classes in the test file. The max column shows
the best performance for each task in the SemEval
2007 competition (Markert and Nissim, 2007).
The SMO column shows the results of learning
when Wikipedia information is not used to com-
pute the values of the collocation, has-product and
has-event features.
Nissim and Markert (2003) have shown that
grammatical roles are very strong features. Exper-
iments on the data represented exclusively through
grammatical role features confirm this observa-
tion, as the results obtained using only the syn-
tactic features (no lexical head information) give
the same results as the M&N feat.bin. which does
include lexical information.
On the location metonymies, the current ap-
proach performs better on all evaluation types
(coarse, medium, fine) by 0.9, 1.1 and 0.9% points
respectively. The improvement comes from rec-
ognizing better the metonymic readings, as it is
apparent from the detailed F-score results in Ta-
ble 4. For the coarse readings, the F-score for
the non-literal reading is 1.5% points higher
than the best performance at SemEval, and 2.5%
and 7.7% points respectively for the metonymic
and mixed readings for the medium and fine
coarseness. It is interesting that the learning is
quite successful even when only selectional pref-
erence and Wikipedia-based has-product and has-
event features are used ? the SMO
SP
column in
Table 3. The grammatical role and the related
lemma were used to derive these collocation fea-
tures, but they do not appear as such in the repre-
sentation used for this batch of experiments.
For company metonymies the current approach
does not perform better than the state-of-the-art.
For these metonymies the syntactic information is
not as useful. This is evidenced by the lower per-
formance of the classifier that uses only syntactic
information (column M&N feat.bin. in Table 3),
despite the fact that the training dataset for com-
916
panies is larger than the one for countries. This
observation is further supported by the low results
when using only selectional preference features.
It indicates that for company metonymies the lo-
cal context does not provide as strong clues as it
does for locations. For such PMWs we should
explore the larger context. We have made a start
with the Wikipedia-based features built following
the observation about companies and their prod-
ucts made by Farkas et al (2007) and Brun et
al. (2007). In future work we plan to analyse
this matter further, and find a method to derive
more such features, and without manually pro-
vided clues (such as manufacturer or riots).
Wikipedia derived information does not con-
tribute very much, but as expected it is helpful
to identify other classes than the literal one.
It is helpful to detect the mixed class ? 16%
F-score when using Wikipedia information com-
pared to 8.7% for the countries data when we esti-
mate preferences using only WordNet. It also in-
creases the performance on the non-literal,
metonymic and org-for-members classes
in coarse, medium and fine classification re-
spectively for both countries and companies.
There is a small improvement for recognizing the
org-for-product reading for organizations
when using Wikipedia-based features. It is an in-
dication that the has-product feature is useful. We
cannot draw conclusions about the has-event fea-
ture, as there are only 3 training instances for the
place-for-event reading. The results are en-
couraging, as we have just scraped the surface of
the information that Wikipedia can provide.
The corpus derived selectional preferences per-
form very well, especially for determining the
reading of locations. Analysis of the data and
the features gives some indication as to why this
happens: in the grammatical annotations provided,
when the PMW is a prepositional complement or
has a prepositional complement, the grammati-
cally related word is a preposition. We extract only
grammatical collocations for open-class words, re-
stricted by the grammatical relation of interest,
so we do not extract collocations for preposi-
tions. Location prepositions (in, at, from) are
less ambiguous than others (e.g. for), which are
more common for the organization data. We have
attempted to bypass this problem by generating
parses using the dependency output of the Stan-
ford Parser (de Marneffe et al, 2006), and bypass-
ing the preposition ? incorporate it in the gram-
matical role (pp in, for example), and using as
lemma the head of the prepositional complement
or the constituent which dominates the preposi-
tional phrase, depending on the position of the
PMW. Now we can use the grammatical relation
and the associated open-class word to look for col-
locations. This approach did not lead to good re-
sults, because the quality of the automatic parses
is far from the manually provided information.
6 Conclusions
We have explored the use of selectional preference
scores derived from a sense untagged corpus as lo-
cal constrains for determining the interpretation of
potentially metonymic words. Such methods were
previously successfully used for word sense dis-
ambiguation, and transfer nicely to the metonymy
resolution task. Adding encyclopedic knowledge
to the mix improved the results further, by filling
in gaps for WordNet, and extracting information
particular to PMW. We plan to expand on this, and
find methods to extract more such features auto-
matically, without manually provided clues.
For a more comprehensive treatment of
metonymies one must take into consideration not
only local context but also discourse relations.
A possible avenue of research is to build upon
coreference resolution systems, and use the
mentions they detect and link to each other in a
manner similar to using grammatical information
and grammatically related words to determine
constraints from a larger context. Determining
the link between two mentions in a text can take
advantage of encyclopedic knowledge, and the
system?s ability to infer the connection between
the mentions.
There is much work on unsupervised word
sense disambiguation. Working with untagged
data gives a system access to a much larger in-
formation base. Since selectional preferences ac-
quired from sense-untagged corpora have worked
well for the metonymy resolution task, we plan to
push further towards unsupervised metonymy res-
olution, putting to use the lessons learned from un-
supervised WSD.
Acknowledgements
We thank the Klaus Tschira Foundation, Heidel-
berg, for financial support, and the organizers of
the SemEval-2007 Task #8 Metonymy Resolution
? Katja Markert and Malvina Nissim ? for making
the annotated data freely available.
917
References
Caroline Brun, Maud Ehrmann, and Guillaume
Jacquet. 2007. XRCE-M: A hybrid system for
named entity metonymy resolution. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations (SemEval-1), Prague, Czech Republic, 23?
24 June 2007, pages 488?491.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006, pages 449?454.
Richard Farkas, Eszter Simon, Gyorgy Szarvas, and
Daniel Varga. 2007. GYDER: Maxent metonymy
resolution. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-1),
Prague, Czech Republic, 23?24 June 2007, pages
161?164.
Dan C. Fass. 1991. met?: A method for discriminating
metonomy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
Sanda M. Harabagiu. 1998. Deriving metonymic coer-
cions from WordNet. In In Workshop on the Usage
of WordNet in Natural Language Processing Sys-
tems, Montreal, Canada, August 16, 1998, pages
142?148.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1-2):69?142.
Shin-ichiro Kamei and Takahiro Wakao. 1992.
Metonymy: Reassessment, survey of acceptability,
and its treatment in a machine translation system.
In Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics, Newark,
Del., 28 June ? 2 July 1992, pages 309?311.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The Sketch Engine. In Proceedings
of the 11th International Congress of the European
Association for Lexicography, Lorient, France, 6?10
July 2004, pages 105?116.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning, Madi-
son, Wisc., 24?27 July 1998, pages 296?304.
Katja Markert and Udo Hahn. 2002. Metonymies in
discourse. Artificial Intelligence, 135(1/2):145?198.
Katja Markert and Malvina Nissim. 2002. Metonymy
resolution as classification task. In Proceedings of
the 2002 Conference on Empirical Methods in Nat-
ural Language Processing, Philadelphia, Penn., 6?7
July 2002, pages 204?213.
Katja Markert and Malvina Nissim. 2003. Corpus-
based metonymy analysis. Metaphor and Symbol,
18(3):175?188.
Katja Markert and Malvina Nissim. 2007. SemEval-
2007 Task 08: Metonymy Resolution at SemEval-
2007. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-1), Prague,
Czech Republic, 23?24 June 2007, pages 36?41.
Vivi Nastase and Michael Strube. 2008. Decoding
Wikipedia category names for knowledge acquisi-
tion. In Proceedings of the 23rd Conference on the
Advancement of Artificial Intelligence, Chicago, Ill.,
13?17 July 2008, pages 1219?1224.
Vivi Nastase. 2008. Unsupervised all-words word
sense disambiguation with grammatical dependen-
cies. In Proceedings of the 3rd International Joint
Conference on Natural Language Processing, Hy-
derabad, India, 7?12 January 2008, pages 757?762.
Cristina Nicolae, Gabriel Nicolae, and Sanda
Harabagiu. 2007. UTD-HLT-CG: Semantic
architecture for metonymy resolution and classifica-
tion of nominal relations. In Proceedings of the 4th
International Workshop on Semantic Evaluations
(SemEval-1), Prague, Czech Republic, 23?24 June
2007, pages 454?459.
Malvina Nissim and Katja Markert. 2003. Syn-
tactic features and word similarity for supervised
metonymy resolution. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, Sapporo, Japan, 7?12 July 2003,
pages 56?63.
Malvina Nissim and Katja Markert. 2005. Learning
to buy a Renault and talk to BMW: A supervised
approach to conventional metonymy. In Proceed-
ings of the 6th International Workshop on Computa-
tional Semantics, Tilburg, Netherlands, January 12-
14, 2005.
James Pustejovsky. 1991. The generative lexicon.
Computational Linguistics, 17(4):209?241.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, (61):127?159.
David Stallard. 1993. Two kinds of metonymy. In
Proceedings of the 31st Annual Meeting of the As-
sociation for Computational Linguistics, Columbus,
Ohio, 22?26 June 1993, pages 87?94.
Gustaf Stern. 1931. Meaning and Changes of Mean-
ing. Indiana University Press, Bloomington, Indi-
ana. (1968; first published in Sweden 1931).
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco, Cal., 2nd edi-
tion.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivalling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, Cambridge,
Mass., 26?30 June 1995, pages 189?196.
918
Semantic Role Labeling for Coreference Resolution
Simone Paolo Ponzetto and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp/
Abstract
Extending a machine learning based coref-
erence resolution system with a feature
capturing automatically generated infor-
mation about semantic roles improves its
performance.
1 Introduction
The last years have seen a boost of work devoted
to the development of machine learning based
coreference resolution systems (Soon et al, 2001;
Ng & Cardie, 2002; Kehler et al, 2004, inter alia).
Similarly, many researchers have explored tech-
niques for robust, broad coverage semantic pars-
ing in terms of semantic role labeling (Gildea &
Jurafsky, 2002; Carreras & Ma`rquez, 2005, SRL
henceforth).
This paper explores whether coreference reso-
lution can benefit from SRL, more specifically,
which phenomena are affected by such informa-
tion. The motivation comes from the fact that cur-
rent coreference resolution systems are mostly re-
lying on rather shallow features, such as the dis-
tance between the coreferent expressions, string
matching, and linguistic form. On the other hand,
the literature emphasizes since the very begin-
ning the relevance of world knowledge and infer-
ence (Charniak, 1973). As an example, consider
a sentence from the Automatic Content Extraction
(ACE) 2003 data.
(1) A state commission of inquiry into the sinking of the
Kursk will convene in Moscow on Wednesday, the
Interfax news agency reported. It said that the diving
operation will be completed by the end of next week.
It seems that in this example, knowing that the In-
terfax news agency is the AGENT of the report
predicate, and It being the AGENT of say, could
trigger the (semantic parallelism based) inference
required to correctly link the two expressions, in
contrast to anchoring the pronoun to Moscow.
SRL provides the semantic relationships that
constituents have with predicates, thus allowing
us to include document-level event descriptive in-
formation into the relations holding between re-
ferring expressions (REs). This layer of semantic
context abstracts from the specific lexical expres-
sions used, and therefore represents a higher level
of abstraction than predicate argument statistics
(Kehler et al, 2004) and Latent Semantic Analy-
sis used as a model of world knowledge (Klebanov
& Wiemer-Hastings, 2002). In this respect, the
present work is closer in spirit to Ji et al (2005),
who explore the employment of the ACE 2004 re-
lation ontology as a semantic filter.
2 Coreference Resolution Using SRL
2.1 Corpora Used
The system was initially prototyped using the
MUC-6 and MUC-7 data sets (Chinchor & Sund-
heim, 2003; Chinchor, 2001), using the standard
partitioning of 30 texts for training and 20-30 texts
for testing. Then, we developed and tested the
system with the ACE 2003 Training Data cor-
pus (Mitchell et al, 2003)1. Both the Newswire
(NWIRE) and Broadcast News (BNEWS) sections
where split into 60-20-20% document-based par-
titions for training, development, and testing, and
later per-partition merged (MERGED) for system
evaluation. The distribution of coreference chains
and referring expressions is given in Table 1.
2.2 Learning Algorithm
For learning coreference decisions, we used a
Maximum Entropy (Berger et al, 1996) model.
Coreference resolution is viewed as a binary clas-
sification task: given a pair of REs, the classifier
has to decide whether they are coreferent or not.
First, a set of pre-processing components includ-
1We used the training data corpus only, as the availability
of the test data was restricted to ACE participants.
143
BNEWS NWIRE
#coref ch. #pron. #comm. nouns #prop. names #coref ch. #pron. #comm. nouns #prop. names
TRAIN. 587 876 572 980 904 1037 1210 2023
DEVEL 201 315 163 465 399 358 485 923
TEST 228 291 238 420 354 329 484 712
Table 1: Partitions of the ACE 2003 training data corpus
ing a chunker and a named entity recognizer is
applied to the text in order to identify the noun
phrases, which are further taken as REs to be used
for instance generation. Instances are created fol-
lowing Soon et al (2001). During testing the
classifier imposes a partitioning on the available
REs by clustering each set of expressions labeled
as coreferent into the same coreference chain.
2.3 Baseline System Features
Following Ng & Cardie (2002), our baseline sys-
tem reimplements the Soon et al (2001) system.
The system uses 12 features. Given a pair of can-
didate referring expressions REi and REj the fea-
tures are computed as follows2.
(a) Lexical features
STRING MATCH T if REi and REj have the
same spelling, else F.
ALIAS T if one RE is an alias of the other; else
F.
(b) Grammatical features
I PRONOUN T if REi is a pronoun; else F.
J PRONOUN T if REj is a pronoun; else F.
J DEF T if REj starts with the; else F.
J DEM T if REj starts with this, that, these, or
those; else F.
NUMBER T if both REi and REj agree in num-
ber; else F.
GENDER U if REi or REj have an undefined
gender. Else if they are both defined and agree
T; else F.
PROPER NAME T if both REi and REj are
proper names; else F.
APPOSITIVE T if REj is in apposition with
REi; else F.
(c) Semantic features
WN CLASS U if REi or REj have an undefined
WordNet semantic class. Else if they both have
a defined one and it is the same T; else F.
2Possible values are U(nknown), T(rue) and F(alse). Note
that in contrast to Ng & Cardie (2002) we classify ALIAS as
a lexical feature, as it solely relies on string comparison and
acronym string matching.
(d) Distance features
DISTANCE how many sentences REi and REj
are apart.
2.4 Semantic Role Features
The baseline system employs only a limited
amount of semantic knowledge. In particular, se-
mantic information is limited to WordNet seman-
tic class matching. Unfortunately, a simple Word-
Net semantic class lookup exhibits problems such
as coverage and sense disambiguation3, which
make the WN CLASS feature very noisy. As a
consequence, we propose in the following to en-
rich the semantic knowledge made available to the
classifier by using SRL information.
In our experiments we use the ASSERT
parser (Pradhan et al, 2004), an SVM based se-
mantic role tagger which uses a full syntactic
analysis to automatically identify all verb predi-
cates in a sentence together with their semantic
arguments, which are output as PropBank argu-
ments (Palmer et al, 2005). It is often the case
that the semantic arguments output by the parser
do not align with any of the previously identified
noun phrases. In this case, we pass a semantic role
label to a RE only in case the two phrases share the
same head. Labels have the form ?ARG1 pred1 . . .
ARGn predn? for n semantic roles filled by a
constituent, where each semantic argument label
ARGi is always defined with respect to a predicate
lemma predi. Given such level of semantic infor-
mation available at the RE level, we introduce two
new features4.
I SEMROLE the semantic role argument-
predicate pairs of REi.
3Following the system to be replicated, we simply
mapped each RE to the first WordNet sense of the head noun.
4During prototyping we experimented unpairing the ar-
guments from the predicates, which yielded worse results.
This is supported by the PropBank arguments always being
defined with respect to a target predicate. Binarizing the fea-
tures ? i.e. do REi and REj have the same argument or
predicate label with respect to their closest predicate? ? also
gave worse results.
144
MUC-6 MUC-7
original R P F1 R P F1
Soon et al 58.6 67.3 62.3 56.1 65.5 60.4
duplicated
baseline 64.9 65.6 65.3 55.1 68.5 61.1
Table 2: Results on MUC
J SEMROLE the semantic role argument-
predicate pairs of REj .
For the ACE 2003 data, 11,406 of 32,502 auto-
matically extracted noun phrases were tagged with
2,801 different argument-predicate pairs.
3 Experiments
3.1 Performance Metrics
We report in the following tables the MUC
score (Vilain et al, 1995). Scores in Table 2 are
computed for all noun phrases appearing in either
the key or the system response, whereas Tables 3
and 4 refer to scoring only those phrases which ap-
pear in both the key and the response. We discard
therefore those responses not present in the key,
as we are interested here in establishing the upper
limit of the improvements given by SRL.
We also report the accuracy score for all three
types of ACE mentions, namely pronouns, com-
mon nouns and proper names. Accuracy is the
percentage of REs of a given mention type cor-
rectly resolved divided by the total number of REs
of the same type given in the key. A RE is said
to be correctly resolved when both it and its direct
antecedent are in the same key coreference class.
In all experiments, the REs given to the clas-
sifier are noun phrases automatically extracted by
a pipeline of pre-processing components (i.e. PoS
tagger, NP chunker, Named Entity Recognizer).
3.2 Results
Table 2 compares the results between our du-
plicated Soon baseline and the original system.
The systems show a similar performance w.r.t. F-
measure. We speculate that the result improve-
ments are due to the use of current pre-processing
components and another classifier.
Tables 3 and 4 show a comparison of the per-
formance between our baseline system and the
one incremented with SRL. Performance improve-
ments are highlighted in bold. The tables show
that SRL tends to improve system recall, rather
than acting as a ?semantic filter? improving pre-
cision. Semantic roles therefore seem to trigger a
R P F1 Ap Acn Apn
baseline 54.5 88.0 67.3 34.7 20.4 53.1
+SRL 56.4 88.2 68.8 40.3 22.0 52.1
Table 4: Results ACE (merged BNEWS/NWIRE)
Feature Chi-square
STR MATCH 1.0
J SEMROLE 0.2096
ALIAS 0.1852
I SEMROLE 0.1594
SEMCLASS 0.1474
DIST 0.1107
GENDER 0.1013
J PRONOUN 0.0982
NUMBER 0.0578
I PRONOUN 0.0489
APPOSITIVE 0.0397
PROPER NAME 0.0141
DEF NP 0.0016
DEM NP 0.0
Table 5: ?2 statistic for each feature
response in cases where more shallow features do
not seem to suffice (see example (1)).
The RE types which are most positively affected
by SRL are pronouns and common nouns. On the
other hand, SRL information has a limited or even
worsening effect on the performance on proper
names, where features such as string matching and
alias seem to suffice. This suggests that SRL plays
a role in pronoun and common noun resolution,
where surface features cannot account for complex
preferences and semantic knowledge is required.
3.3 Feature Evaluation
We investigated the contribution of the different
features in the learning process. Table 5 shows
the chi-square statistic (normalized in the [0, 1] in-
terval) for each feature occurring in the training
data of the MERGED dataset. SRL features show
a high ?2 value, ranking immediately after string
matching and alias, which indicates a high corre-
lation of these features to the decision classes.
The importance of SRL is also indicated by the
analysis of the contribution of individual features
to the overall performance. Table 6 shows the per-
formance variations obtained by leaving out each
feature in turn. Again, it can be seen that remov-
ing both I and J SEMROLE induces a relatively
high performance degradation when compared to
other features. Their removal ranks 5th out of
12, following only essential features such as string
matching, alias, pronoun and number. Similarly
to Table 5, the semantic role of the anaphor ranks
higher than the one of the antecedent. This re-
145
BNEWS NWIRE
R P F1 Ap Acn Apn R P F1 Ap Acn Apn
baseline 46.7 86.2 60.6 36.4 10.5 44.0 56.7 88.2 69.0 37.7 23.1 55.6
+SRL 50.9 86.1 64.0 36.8 14.3 45.7 58.3 86.9 69.8 38.0 25.8 55.8
Table 3: Results on the ACE 2003 data (BNEWS and NWIRE sections)
Feature(s) removed ? F1
all features 68.8
STR MATCH ?21.02
ALIAS ?2.96
I/J PRONOUN ?2.94
NUMBER ?1.63
I/J SEMROLE ?1.50
J SEMROLE ?1.26
APPOSITIVE ?1.20
GENDER ?1.13
I SEMROLE ?0.74
DIST ?0.69
WN CLASS ?0.56
DEF NP ?0.57
DEM NP ?0.50
PROPER NAME ?0.49
Table 6: ? F1 from feature removal
lates to the improved performance on pronouns, as
it indicates that SRL helps for linking anaphoric
pronouns to preceding REs. Finally, it should
be noted that SRL provides much more solid and
noise-free semantic features when compared to the
WordNet class feature, whose removal induces al-
ways a lower performance degradation.
4 Conclusion
In this paper we have investigated the effects
of using semantic role information within a ma-
chine learning based coreference resolution sys-
tem. Empirical results show that coreference res-
olution can benefit from SRL. The analysis of the
relevance of features, which had not been previ-
ously addressed, indicates that incorporating se-
mantic information as shallow event descriptions
improves the performance of the classifier. The
generated model is able to learn selection pref-
erences in cases where surface morpho-syntactic
features do not suffice, i.e. pronoun resolution.
We speculate that this contrasts with the disap-
pointing findings of Kehler et al (2004) since SRL
provides a more fine grained level of information
when compared to predicate argument statistics.
As it models the semantic relationship that a syn-
tactic constituent has with a predicate, it carries in-
directly syntactic preference information. In addi-
tion, when used as a feature it allows the classifier
to infer semantic role co-occurrence, thus induc-
ing deep representations of the predicate argument
relations for learning in coreferential contexts.
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a
KTF grant (09.003.2004).
References
Berger, A., S. A. Della Pietra & V. J. Della Pietra (1996). A
maximum entropy approach to natural language process-
ing. Computational Linguistics, 22(1):39?71.
Carreras, X. & L. Ma`rquez (2005). Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proc. of CoNLL-05, pp. 152?164.
Charniak, E. (1973). Jack and Janet in search of a theory
of knowledge. In Advance Papers from the Third Inter-
national Joint Conference on Artificial Intelligence, Stan-
ford, Cal., pp. 337?343.
Chinchor, N. (2001). Message Understanding Conference
(MUC) 7. LDC2001T02, Philadelphia, Penn: Linguistic
Data Consortium.
Chinchor, N. & B. Sundheim (2003). Message Understand-
ing Conference (MUC) 6. LDC2003T13, Philadelphia,
Penn: Linguistic Data Consortium.
Gildea, D. & D. Jurafsky (2002). Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?288.
Ji, H., D. Westbrook & R. Grishman (2005). Using semantic
relations to refine coreference decisions. In Proc. HLT-
EMNLP ?05, pp. 17?24.
Kehler, A., D. Appelt, L. Taylor & A. Simma (2004). The
(non)utility of predicate-argument frequencies for pro-
noun interpretation. In Proc. of HLT-NAACL-04, pp. 289?
296.
Klebanov, B. & P. Wiemer-Hastings (2002). The role of
wor(l)d knowledge in pronominal anaphora resolution. In
Proceedings of the International Symposium on Reference
Resolution for Natural Language Processing, Alicante,
Spain, 3?4 June, 2002, pp. 1?8.
Mitchell, A., S. Strassel, M. Przybocki, J. Davis, G. Dod-
dington, R. Grishman, A. Meyers, A. Brunstain, L. Ferro
& B. Sundheim (2003). TIDES Extraction (ACE) 2003
Multilingual Training Data. LDC2004T09, Philadelphia,
Penn.: Linguistic Data Consortium.
Ng, V. & C. Cardie (2002). Improving machine learning ap-
proaches to coreference resolution. In Proc. of ACL-02,
pp. 104?111.
Palmer, M., D. Gildea & P. Kingsbury (2005). The proposi-
tion bank: An annotated corpus of semantic roles. Com-
putational Linguistics, 31(1):71?105.
Pradhan, S., W. Ward, K. Hacioglu, J. H. Martin & D. Juraf-
sky (2004). Shallow semantic parsing using support vector
machines. In Proc. of HLT-NAACL-04, pp. 233?240.
Soon, W. M., H. T. Ng & D. C. Y. Lim (2001). A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly &
L. Hirschman (1995). A model-theoretic coreference scor-
ing scheme. In Proceedings of the 6th Message Under-
standing Conference (MUC-6), pp. 45?52.
146
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 192?199,
New York, June 2006. c?2006 Association for Computational Linguistics
Exploiting Semantic Role Labeling, WordNet and Wikipedia
for Coreference Resolution
Simone Paolo Ponzetto and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
In this paper we present an extension of
a machine learning based coreference res-
olution system which uses features in-
duced from different semantic knowledge
sources. These features represent knowl-
edge mined from WordNet and Wikipedia,
as well as information about semantic role
labels. We show that semantic features in-
deed improve the performance on differ-
ent referring expression types such as pro-
nouns and common nouns.
1 Introduction
The last years have seen a boost of work devoted to
the development of machine learning based coref-
erence resolution systems (Soon et al, 2001; Ng &
Cardie, 2002; Yang et al, 2003; Luo et al, 2004,
inter alia). While machine learning has proved to
yield performance rates fully competitive with rule
based systems, current coreference resolution sys-
tems are mostly relying on rather shallow features,
such as the distance between the coreferent expres-
sions, string matching, and linguistic form. How-
ever, the literature emphasizes since the very begin-
ning the relevance of world knowledge and infer-
ence for coreference resolution (Charniak, 1973).
This paper explores whether coreference resolu-
tion can benefit from semantic knowledge sources.
More specifically, whether a machine learning based
approach to coreference resolution can be improved
and which phenomena are affected by such infor-
mation. We investigate the use of the WordNet and
Wikipedia taxonomies for extracting semantic simi-
larity and relatedness measures, as well as semantic
parsing information in terms of semantic role label-
ing (Gildea & Jurafsky, 2002, SRL henceforth).
We believe that the lack of semantics in the cur-
rent systems leads to a performance bottleneck.
In order to correctly identify the discourse entities
which are referred to in a text, it seems essential to
reason over the lexical semantic relations, as well as
the event representations embedded in the text. As
an example, consider a fragment from the Automatic
Content Extraction (ACE) 2003 data.
(1) But frequent visitors say that given the sheer weight of
the country?s totalitarian ideology and generations of
mass indoctrination, changing this country?s course will
be something akin to turning a huge ship at sea. Opening
North Korea up, even modestly, and exposing people to
the idea that Westerners ? and South Koreans ? are not
devils, alone represents an extraordinary change. [...] as
his people begin to get a clearer idea of the deprivation
they have suffered, especially relative to their neighbors.
?This is a society that has been focused most of all on
stability, [...]?.
In order to correctly resolve the anaphoric expres-
sions highlighted in bold, it seems that some kind
of lexical semantic and encyclopedic knowledge is
required. This includes that North Korea is a coun-
try, that countries consist of people and are soci-
eties. The resolution requires an encyclopedia (i.e.
Wikipedia) look-up and reasoning on the content re-
latedness holding between the different expressions
(i.e. as a path measure along the links of the Word-
Net and Wikipedia taxonomies). Event representa-
tions seem also to be important for coreference res-
olution, as shown below:
(2) A state commission of inquiry into the sinking of the
Kursk will convene in Moscow on Wednesday, the
Interfax news agency reported. It said that the diving
operation will be completed by the end of next week.
192
In this example, knowing that the Interfax news
agency is the AGENT of the report predicate and It
being the AGENT of say could trigger the (seman-
tic parallelism based) inference required to correctly
link the two expressions, in contrast to anchoring
the pronoun to Moscow. SRL provides the seman-
tic relationships that constituents have with predi-
cates, thus allowing us to include such document-
level event descriptive information into the relations
holding between referring expressions (REs).
Instead of exploring different kinds of data rep-
resentations, task definitions or machine learning
techniques (Ng & Cardie, 2002; Yang et al, 2003;
Luo et al, 2004) we focus on a few promising se-
mantic features which we evaluate in a controlled
environment. That way we try to overcome the
plateauing in performance in coreference resolution
observed by Kehler et al (2004).
2 Related Work
Vieira & Poesio (2000), Harabagiu et al (2001),
and Markert & Nissim (2005) explore the use of
WordNet for different coreference resolution sub-
tasks, such as resolving bridging reference, other-
and definite NP anaphora, and MUC-style corefer-
ence resolution. All of them present systems which
infer coreference relations from a set of potential an-
tecedents by means of a WordNet search. Our ap-
proach to WordNet here is to cast the search results
in terms of semantic similarity measures. Their out-
put can be used as features for a learner. These mea-
sures are not specifically developed for coreference
resolution but simply taken ?off-the-shelf? and ap-
plied to our task without any specific tuning ? i.e.
in contrast to Harabagiu et al (2001), who weight
WordNet relations differently in order to compute
the confidence measure of the path.
To the best of our knowledge, we do not know
of any previous work using Wikipedia or SRL for
coreference resolution. In the case of SRL, this
layer of semantic context abstracts from the specific
lexical expressions used, and therefore represents a
higher level of abstraction than (still related) work
involving predicate argument statistics. Kehler et al
(2004) observe no significant improvement due to
predicate argument statistics. The improvement re-
ported by Yang et al (2005) is rather caused by their
twin-candidate model than by the semantic knowl-
edge. Employing SRL is closer in spirit to Ji et al
(2005), who explore the employment of the ACE
2004 relation ontology as a semantic filter.
3 Coreference Resolution Using Semantic
Knowledge Sources
3.1 Corpora Used
To establish a competitive coreference resolver, the
system was initially prototyped using the MUC-6
and MUC-7 data sets (Chinchor & Sundheim, 2003;
Chinchor, 2001), using the standard partitioning
of 30 texts for training and 20-30 texts for test-
ing. Then, we moved on and developed and tested
the system with the ACE 2003 Training Data cor-
pus (Mitchell et al, 2003)1. Both the Newswire
(NWIRE) and Broadcast News (BNEWS) sections
where split into 60-20-20% document-based par-
titions for training, development, and testing, and
later per-partition merged (MERGED) for system
evaluation. The distribution of coreference chains
and referring expressions is given in Table 1.
3.2 Learning Algorithm
For learning coreference decisions, we used a Maxi-
mum Entropy (Berger et al, 1996) model. This was
implemented using the MALLET library (McCal-
lum, 2002). To prevent the model from overfitting,
we employed a tunable Gaussian prior as a smooth-
ing method. The best parameter value is found by
searching in the [0,10] interval with step value of
0.5 for the variance parameter yielding the highest
MUC score F-measure on the development data.
Coreference resolution is viewed as a binary clas-
sification task: given a pair of REs, the classifier has
to decide whether they are coreferent or not. The
MaxEnt model produces a probability for each cat-
egory y (coreferent or not) of a candidate pair, con-
ditioned on the context x in which the candidate oc-
curs. The conditional probability is calculated by:
p(y|x) = 1Zx
[
?
i
?ifi(x, y)
]
1We used the training data corpus only, as the availability
of the test data is restricted to ACE participants. Therefore, the
results we report cannot be compared directly with those using
the official test data.
193
BNEWS (147 docs ? 33,479 tokens) NWIRE (105 docs ? 57,205 tokens)
#coref ch. #pron. #comm. nouns #prop. names #coref ch. #pron. #comm. nouns #prop. names
TRAIN. 587 876 572 980 904 1,037 1,210 2,023
DEVEL 201 315 163 465 399 358 485 923
TEST 228 291 238 420 354 329 484 712
TOTAL 1,016 1,482 973 1,865 1,657 1,724 2,179 3,658
TOTAL (%) 34.3% 22.5% 43.2% 22.8% 28.8% 48.4%
Table 1: Partitions of the ACE 2003 training data corpus
where fi(x, y) is the value of feature i on outcome y
in context x, and ?i is the weight associated with i in
the model. Zx is a normalization constant. The fea-
tures used in our model are all binary-valued feature
functions (or indicator functions), e.g.
fI SEMROLE(ARG0/RUN, COREF) =
?
?
?
?
?
?
?
?
?
?
?
1 if candidate pair is
coreferent and antecedent
is the semantic argument
ARG0 of predicate run
0 else
In our system, a set of pre-processing compo-
nents including a POS tagger (Gime?nez & Ma`rquez,
2004), NP chunker (Kudoh & Matsumoto, 2000)
and the Alias-I LingPipe Named Entity Recognizer2
is applied to the text in order to identify the noun
phrases, which are further taken as referring ex-
pressions (REs) to be used for instance generation.
Therefore, we use automatically extracted noun
phrases, rather than assuming perfect NP chunk-
ing. This is in contrast to other related works
in coreference resolution (e.g. Luo et al (2004),
Kehler et al (2004)).
Instances are created following Soon et al (2001).
We create a positive training instance from each pair
of adjacent coreferent REs. Negative instances are
obtained by pairing the anaphoric REs with any RE
occurring between the anaphor and the antecedent.
During testing each text is processed from left to
right: each RE is paired with any preceding RE from
right to left, until a pair labeled as coreferent is out-
put, or the beginning of the document is reached.
The classifier imposes a partitioning on the available
REs by clustering each set of expressions labeled as
coreferent into the same coreference chain.
2http://alias-i.com/lingpipe
3.3 Baseline System Features
Following Ng & Cardie (2002), our baseline sys-
tem reimplements the Soon et al (2001) system.
The system uses 12 features. Given a potential an-
tecedent REi and a potential anaphor REj the fea-
tures are computed as follows3.
(a) Lexical features
STRING MATCH T if REi and REj have the
same spelling, else F.
ALIAS T if one RE is an alias of the other; else F.
(b) Grammatical features
I PRONOUN T if REi is a pronoun; else F.
J PRONOUN T if REj is a pronoun; else F.
J DEF T if REj starts with the; else F.
J DEM T if REj starts with this, that, these, or
those; else F.
NUMBER T if both REi and REj agree in number;
else F.
GENDER U if either REi or REj have an undefined
gender. Else if they are both defined and agree
T; else F.
PROPER NAME T if both REi and REj are
proper names; else F.
APPOSITIVE T if REj is in apposition with REi;
else F.
(c) Semantic features
WN CLASS U if either REi or REj have an unde-
fined WordNet semantic class. Else if they both
have a defined one and it is the same T; else F.
(d) Distance features
DISTANCE how many sentences REi and REj are
apart.
3Possible values are U(nknown), T(rue) and F(alse). Note
that in contrast to Ng & Cardie (2002) we interpret ALIAS as
a lexical feature, as it solely relies on string comparison and
acronym string matching.
194
3.4 WordNet Features
In the baseline system semantic information is lim-
ited to WordNet semantic class matching. Unfor-
tunately, a WordNet semantic class lookup exhibits
problems such as coverage, sense proliferation and
ambiguity4, which make the WN CLASS feature
very noisy. We enrich the semantic information
available to the classifier by using semantic similar-
ity measures based on the WordNet taxonomy (Ped-
ersen et al, 2004). The measures we use include
path length based measures (Rada et al, 1989; Wu &
Palmer, 1994; Leacock & Chodorow, 1998), as well
as ones based on information content (Resnik, 1995;
Jiang & Conrath, 1997; Lin, 1998).
In our case, the measures are obtained by comput-
ing the similarity scores between the head lemmata
of each potential antecedent-anaphor pair. In order
to overcome the sense disambiguation problem, we
factorise over all possible sense pairs: given a can-
didate pair, we take the cross product of each an-
tecedent and anaphor sense to form pairs of synsets.
For each measure WN SIMILARITY, we compute
the similarity score for all synset pairs, and create
the following features.
WN SIMILARITY BEST the highest similarity
score from all ?SENSEREi,n, SENSEREj ,m? synset
pairs.
WN SIMILARITY AVG the average similarity
score from all ?SENSEREi,n, SENSEREj ,m? synset
pairs.
Pairs containing REs which cannot be mapped to
WordNet synsets are assumed to have a null simi-
larity measure.
3.5 Wikipedia Features
Wikipedia is a multilingual Web-based free-content
encyclopedia5 . The English version, as of 14 Febru-
ary 2006, contains 971,518 articles with 16.8 mil-
lion internal hyperlinks thus providing a large cover-
age available knowledge resource. In addition, since
May 2004 it provides also a taxonomy by means of
the category feature: articles can be placed in one
4Following the system to be replicated, we simply mapped
each RE to the first WordNet sense of the head noun.
5Wikipedia can be downloaded at http://download.
wikimedia.org/. In our experiments we use the English
Wikipedia database dump from 19 February 2006.
or more categories, which are further categorized to
provide a category tree. In practice, the taxonomy
is not designed as a strict hierarchy or tree of cat-
egories, but allows multiple categorisation schemes
to co-exist simultaneously. Because each article can
appear in more than one category, and each category
can appear in more than one parent category, the cat-
egories do not form a tree structure, but a more gen-
eral directed graph. As of December 2005, 78% of
the articles have been categorized into 87,000 differ-
ent categories.
Wikipedia mining works as follows (for an in-
depth description of the methods for computing
semantic relatedness in Wikipedia see Strube &
Ponzetto (2006)): given the candidate referring ex-
pressions REi and REj we first pull the pages they
refer to. This is accomplished by querying the page
titled as the head lemma or, in the case of NEs, the
full NP. We follow all redirects and check for dis-
ambiguation pages, i.e. pages for ambiguous entries
which contain links only (e.g. Lincoln). If a disam-
biguation page is hit, we first get al the hyperlinks
in the page. If a link containing the other queried RE
is found (i.e. a link containing president in the Lin-
coln page), the linked page (President of the United
States) is returned, else we return the first article
linked in the disambiguation page. Given a candi-
date coreference pair REi/j and the Wikipedia pages
PREi/j they point to, obtained by querying pages ti-
tled as TREi/j , we extract the following features:
I/J GLOSS CONTAINS U if no Wikipedia page
titled TREi/j is available. Else T if the first para-
graph of text of PREi/j contains TREj/i ; else F.
I/J RELATED CONTAINS U if no Wikipedia
page titled as TREi/j is available. Else T if at
least one Wikipedia hyperlink of PREi/j con-
tains TREj/i ; else F.
I/J CATEGORIES CONTAINS U if no Wiki-
pedia page titled as TREi/j is available. Else T if
the list of categories PREi/j belongs to contains
TREj/i ; else F.
GLOSS OVERLAP the overlap score between the
first paragraph of text of PREi and PREj . Fol-
lowing Banerjee & Pedersen (2003) we compute
the score as
?
n m2 for n phrasal m-word over-
laps.
195
Additionally, we use the Wikipedia category graph.
We ported the WordNet similarity path length based
measures to the Wikipedia category graph. How-
ever, the category relations in Wikipedia cannot only
be interpreted as corresponding to is-a links in a
taxonomy since they denote meronymic relations
as well. Therefore, the Wikipedia-based measures
are to be taken as semantic relatedness measures.
The measures from Rada et al (1989), Leacock &
Chodorow (1998) and Wu & Palmer (1994) are com-
puted in the same way as for WordNet. Path search
takes place as a depth-limited search of maximum
depth of 4 for a least common subsumer. We no-
ticed that limiting the search improves the results as
it yields a better correlation of the relatedness scores
with human judgements (Strube & Ponzetto, 2006).
This is due to the high regions of the Wikipedia cat-
egory tree being too strongly connected.
In addition, we use the measure from Resnik
(1995), which is computed using an intrinsic in-
formation content measure relying on the hierar-
chical structure of the category tree (Seco et al,
2004). Given PREi/j and the lists of categories
CREi/j they belong to, we factorise over all pos-
sible category pairs. That is, we take the cross
product of each antecedent and anaphor category to
form pairs of ?Wikipedia synsets?. For each mea-
sure WIKI RELATEDNESS, we compute the relat-
edness score for all category pairs, and create the
following features.
WIKI RELATEDNESS BEST the highest relat-
edness score from all ?CREi,n, CREj ,m? cate-
gory pairs.
WIKI RELATEDNESS AVG the average relat-
edness score from all ?CREi,n, CREj ,m? cate-
gory pairs.
3.6 Semantic Role Features
The last semantic knowledge enhancement for the
baseline system uses SRL information. In our exper-
iments we use the ASSERT parser (Pradhan et al,
2004), an SVM based semantic role tagger which
uses a full syntactic analysis to automatically iden-
tify all verb predicates in a sentence together with
their semantic arguments, which are output as Prop-
Bank arguments (Palmer et al, 2005). It is of-
ten the case that the semantic arguments output by
the parser do not align with any of the previously
identified noun phrases. In this case, we pass a
semantic role label to a RE only when the two
phrases share the same head. Labels have the form
?ARG1 pred1 . . . ARGn predn? for n semantic roles
filled by a constituent, where each semantic argu-
ment label is always defined with respect to a predi-
cate. Given such level of semantic information avail-
able at the RE level, we introduce two new features6.
I SEMROLE the semantic role argument-
predicate pairs of REi.
J SEMROLE the semantic role argument-
predicate pairs of REj .
For the ACE 2003 data, 11,406 of 32,502 automati-
cally extracted noun phrases were tagged with 2,801
different argument-predicate pairs.
4 Experiments
4.1 Performance Metrics
We report in the following tables the MUC
score (Vilain et al, 1995). Scores in Table 2 are
computed for all noun phrases appearing in either
the key or the system response, whereas Tables 3
and 4 refer to scoring only those phrases which ap-
pear in both the key and the response. We therefore
discard those responses not present in the key, as we
are interested in establishing the upper limit of the
improvements given by our semantic features. That
is, we want to define a baseline against which to es-
tablish the contribution of the semantic information
sources explored here for coreference resolution.
In addition, we report the accuracy score for all
three types of ACE mentions, namely pronouns,
common nouns and proper names. Accuracy is the
percentage of REs of a given mention type correctly
resolved divided by the total number of REs of the
same type given in the key. A RE is said to be cor-
rectly resolved when both it and its direct antecedent
are placed by the key in the same coreference class.
6During prototyping we experimented unpairing the argu-
ments from the predicates, which yielded worse results. This
is supported by the PropBank arguments always being defined
with respect to a target predicate. Binarizing the features ? i.e.
do REi and REj have the same argument or predicate label with
respect to their closest predicate? ? also gave worse results.
196
MUC-6 MUC-7
original R P F1 R P F1
Soon et al 58.6 67.3 62.3 56.1 65.5 60.4
duplicated
baseline 64.9 65.6 65.3 55.1 68.5 61.1
Table 2: Results on MUC
4.2 Feature Selection
For determining the relevant feature sets we follow
an iterative procedure similar to the wrapper ap-
proach for feature selection (Kohavi & John, 1997)
using the development data. The feature subset se-
lection algorithm performs a hill-climbing search
along the feature space. We start with a model
based on all available features. Then we train mod-
els obtained by removing one feature at a time. We
choose the worst performing feature, namely the one
whose removal gives the largest improvement based
on the MUC score F-measure, and remove it from
the model. We then train classifiers removing each
of the remaining features separately from the en-
hanced model. The process is iteratively run as long
as significant improvement is observed.
4.3 Results
Table 2 compares the results between our duplicated
Soon baseline and the original system. We assume
that the slight improvements of our system are due
to the use of current pre-processing components and
another classifier. Tables 3 and 4 show a comparison
of the performance between our baseline system and
the ones incremented with semantic features. Per-
formance improvements are highlighted in bold7.
4.4 Discussion
The tables show that semantic features improve sys-
tem recall, rather than acting as a ?semantic filter?
improving precision. Semantics therefore seems to
trigger a response in cases where more shallow fea-
tures do not seem to suffice (see examples (1-2)).
Different feature sources account for different
RE type improvements. WordNet and Wikipedia
features tend to increase performance on common
7All changes in F-measure are statistically significant at the
0.05 level or higher. We follow Soon et al (2001) in performing
a simple one-tailed, paired sample t-test between the baseline
system?s MUC score F-measure and each of the other systems?
F-measure scores on the test documents.
nouns, whereas SRL improves pronouns. Word-
Net features are able to improve by 14.3% and
7.7% the accuracy rate for common nouns on the
BNEWS and NWIRE datasets (+34 and +37 cor-
rectly resolved common nouns out of 238 and 484
respectively), whereas employing Wikipedia yields
slightly smaller improvements (+13.0% and +6.6%
accuracy increase on the same datasets). Similarly,
when SRL features are added to the baseline system,
we register an increase in the accuracy rate for pro-
nouns, ranging from 0.7% in BNEWS and NWIRE
up to 4.2% in the MERGED dataset (+26 correctly
resolved pronouns out of 620).
If semantics helps for pronouns and common
nouns, it does not affect performance on proper
names, where features such as string matching and
alias suffice. This suggests that semantics plays a
role in pronoun and common noun resolution, where
surface features cannot account for complex prefer-
ences and semantic knowledge is required.
The best accuracy improvement on pronoun res-
olution is obtained on the MERGED dataset. This
is due to making more data available to the classi-
fier, as the SRL features are very sparse and inher-
ently suffer from data fragmentation. Using a larger
dataset highlights the importance of SRL, whose
features are never removed in any feature selection
process8. The accuracy on common nouns shows
that features induced from Wikipedia are competi-
tive with the ones from WordNet. The performance
gap on all three datasets is quite small, which indi-
cates the usefulness of using an encyclopedic knowl-
edge base as a replacement for a lexical taxonomy.
As a consequence of having different knowledge
sources accounting for the resolution of different RE
types, the best results are obtained by (1) combin-
ing features generated from different sources; (2)
performing feature selection. When combining dif-
ferent feature sources, we register an accuracy im-
provement on pronouns and common nouns, as well
as an increase in F-measure due to a higher recall.
Feature selection always improves results. This
is due to the fact that our full feature set is ex-
8To our knowledge, most of the recent work in coreference
resolution on the ACE data keeps the document source sepa-
rated for evaluation. However, we believe that document source
independent evaluation provides useful insights on the robust-
ness of the system (cf. the CoNLL 2005 shared task cross-
corpora evaluation).
197
BNEWS NWIRE
R P F1 Ap Acn Apn R P F1 Ap Acn Apn
baseline 46.7 86.2 60.6 36.4 10.5 44.0 56.7 88.2 69.0 37.6 23.1 55.6
+WordNet 54.8 86.1 66.9 36.8 24.8 47.6 61.3 84.9 71.2 38.9 30.8 55.5
+Wiki 52.7 86.8 65.6 36.1 23.5 46.2 60.6 83.6 70.3 38.0 29.7 55.2
+SRL 53.3 85.1 65.5 37.1 13.9 46.2 58.0 89.0 70.2 38.3 25.0 56.0
all features 59.1 84.4 69.5 37.5 27.3 48.1 63.1 83.0 71.7 39.8 31.8 52.8
Table 3: Results on the ACE 2003 data (BNEWS and NWIRE sections)
R P F1 Ap Acn Apn
baseline 54.5 88.0 67.3 34.7 20.4 53.1
+WordNet 56.7 87.1 68.6 35.6 28.5 49.6
+Wikipedia 55.8 87.5 68.1 34.8 26.0 50.5
+SRL 56.3 88.4 68.8 38.9 21.6 51.7
all features 61.0 84.2 70.7 38.9 29.9 51.2
Table 4: Results ACE (merged BNEWS/NWIRE)
tremely redundant: in order to explore the useful-
ness of the knowledge sources we included overlap-
ping features (i.e. using best and average similar-
ity/relatedness measures at the same time), as well as
features capturing the same phenomenon from dif-
ferent point of views (i.e. using multiple measures
at the same time). In order to yield the desired per-
formance improvements, it turns out to be essential
to filter out irrelevant features.
Table 5 shows the relevance of the best perform-
ing features on the BNEWS section. As our fea-
ture selection mechanism chooses the best set of fea-
tures by removing them (see Section 4.2), we eval-
uate the contributions of the remaining features as
follows. We start with a baseline system using all
the features from Soon et al (2001) that were not
removed in the feature selection process (i.e. DIS-
TANCE). We then train classifiers combining the
current feature set with each feature in turn. We
then choose the best performing feature based on the
MUC score F-measure and add it to the model. We
iterate the process until all features are added to the
baseline system. The table indicates that all knowl-
edge sources are relevant for coreference resolution,
as it includes SRL, WordNet and Wikipedia features.
The Wikipedia features rank high, indicating again
that it provides a valid knowledge base.
5 Conclusions and Future Work
The results are somehow surprising, as one would
not expect a community-generated categorization
to be almost as informative as a well structured
Feature set F1
baseline (Soon w/o DISTANCE) 58.4%
+WIKI WU PALMER BEST +4.3%
+J SEMROLE +1.8%
+WIKI PATH AVG +1.2%
+I SEMROLE +0.8%
+WN WU PALMER BEST +0.7%
Table 5: Feature selection (BNEWS section)
lexical taxonomy such as WordNet. Nevertheless
Wikipedia offers promising results, which we expect
to improve as well as the encyclopedia goes under
further development.
In this paper we investigated the effects of using
different semantic knowledge sources within a ma-
chine learning based coreference resolution system.
This involved mining the WordNet taxonomy and
the Wikipedia encyclopedic knowledge base, as well
as including semantic parsing information, in order
to induce semantic features for coreference learning.
Empirical results show that coreference resolution
benefits from semantics. The generated model is
able to learn selectional preferences in cases where
surface morpho-syntactic features do not suffice, i.e.
pronoun and common name resolution. While the
results given by using ?the free encyclopedia that
anyone can edit? are satisfactory, major improve-
ments can come from developing efficient query
strategies ? i.e. a more refined disambiguation tech-
nique taking advantage of the context in which the
queries (e.g. referring expressions) occur.
Future work will include turning Wikipedia into
an ontology with well defined taxonomic relations,
as well as exploring its usefulness of for other NLP
applications. We believe that an interesting aspect of
Wikipedia is that it offers large coverage resources
for many languages, thus making it a natural choice
for multilingual NLP systems.
Semantics plays indeed a role in coreference
resolution. But semantic features are expensive to
198
compute and the development of efficient methods
is required to embed them into large scale systems.
Nevertheless, we believe that exploiting semantic
knowledge in the manner we described will assist
the research on coreference resolution to overcome
the plateauing in performance observed by Kehler
et al (2004).
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a
KTF grant (09.003.2004). We thank Katja Filip-
pova, Margot Mieskes and the three anonymous re-
viewers for their useful comments.
References
Banerjee, S. & T. Pedersen (2003). Extended gloss overlap as
a measure of semantic relatedness. In Proc. of IJCAI-03, pp.
805?810.
Berger, A., S. A. Della Pietra & V. J. Della Pietra (1996). A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
Charniak, E. (1973). Jack and Janet in search of a theory of
knowledge. In Advance Papers from the Third International
Joint Conference on Artificial Intelligence, Stanford, Cal.,
pp. 337?343.
Chinchor, N. (2001). Message Understanding Conference
(MUC) 7. LDC2001T02, Philadelphia, Penn: Linguistic
Data Consortium.
Chinchor, N. & B. Sundheim (2003). Message Understanding
Conference (MUC) 6. LDC2003T13, Philadelphia, Penn:
Linguistic Data Consortium.
Gildea, D. & D. Jurafsky (2002). Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
Gime?nez, J. & L. Ma`rquez (2004). SVMTool: A general POS
tagger generator based on support vector machines. In Proc.
of LREC ?04, pp. 43?46.
Harabagiu, S. M., R. C. Bunescu & S. J. Maiorano (2001). Text
and knowledge mining for coreference resolution. In Proc.
of NAACL-01, pp. 55?62.
Ji, H., D. Westbrook & R. Grishman (2005). Using semantic re-
lations to refine coreference decisions. In Proc. HLT-EMNLP
?05, pp. 17?24.
Jiang, J. J. & D. W. Conrath (1997). Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceedings of
the 10th International Conference on Research in Computa-
tional Linguistics (ROCLING).
Kehler, A., D. Appelt, L. Taylor & A. Simma (2004). The
(non)utility of predicate-argument frequencies for pronoun
interpretation. In Proc. of HLT-NAACL-04, pp. 289?296.
Kohavi, R. & G. H. John (1997). Wrappers for feature subset
selection. Artificial Intelligence Journal, 97(1-2):273?324.
Kudoh, T. & Y. Matsumoto (2000). Use of Support Vector Ma-
chines for chunk identification. In Proc. of CoNLL-00, pp.
142?144.
Leacock, C. & M. Chodorow (1998). Combining local con-
text and WordNet similarity for word sense identifica-
tion. In C. Fellbaum (Ed.), WordNet. An Electronic Lexical
Database, Chp. 11, pp. 265?283. Cambridge, Mass.: MIT
Press.
Lin, D. (1998). An information-theoretic definition of similar-
ity. In Proceedings of the 15th International Conference on
Machine Learning, pp. 296?304.
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla & S. Roukos
(2004). A mention-synchronous coreference resolution al-
gorithm based on the Bell Tree. In Proc. of ACL-04, pp.
136?143.
Markert, K. & M. Nissim (2005). Comparing knowledge
sources for nominal anaphora resolution. Computational
Linguistics, 31(3):367?401.
McCallum, A. K. (2002). MALLET: A Machine Learning for
Language Toolkit.
Mitchell, A., S. Strassel, M. Przybocki, J. Davis, G. Dodding-
ton, R. Grishman, A. Meyers, A. Brunstain, L. Ferro &
B. Sundheim (2003). TIDES Extraction (ACE) 2003 Mul-
tilingual Training Data. LDC2004T09, Philadelphia, Penn.:
Linguistic Data Consortium.
Ng, V. & C. Cardie (2002). Improving machine learning ap-
proaches to coreference resolution. In Proc. of ACL-02, pp.
104?111.
Palmer, M., D. Gildea & P. Kingsbury (2005). The proposition
bank: An annotated corpus of semantic roles. Computational
Linguistics, 31(1):71?105.
Pedersen, T., S. Patwardhan & J. Michelizzi (2004). Word-
Net::Similarity ? Measuring the relatedness of concepts. In
Companion Volume of the Proceedings of the Human Tech-
nology Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pp. 267?270.
Pradhan, S., W. Ward, K. Hacioglu, J. H. Martin & D. Juraf-
sky (2004). Shallow semantic parsing using Support Vector
Machines. In Proc. of HLT-NAACL-04, pp. 233?240.
Rada, R., H. Mili, E. Bicknell & M. Blettner (1989). Devel-
opment and application of a metric to semantic nets. IEEE
Transactions on Systems, Man and Cybernetics, 19(1):17?
30.
Resnik, P. (1995). Using information content to evaluate seman-
tic similarity in a taxonomy. In Proc. of IJCAI-95, Vol. 1, pp.
448?453.
Seco, N., T. Veale & J. Hayes (2004). An intrinsic information
content metric for semantic similarity in WordNet. In Proc.
of ECAI-04, pp. 1089?1090.
Soon, W. M., H. T. Ng & D. C. Y. Lim (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Strube, M. & S. P. Ponzetto (2006). WikiRelate! Computing
semantic relatedness using Wikipedia. In Proc. of AAAI-06.
Vieira, R. & M. Poesio (2000). An empirically-based system for
processing definite descriptions. Computational Linguistics,
26(4):539?593.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly & L. Hirschman
(1995). A model-theoretic coreference scoring scheme. In
Proceedings of the 6th Message Understanding Conference
(MUC-6), pp. 45?52.
Wu, Z. & M. Palmer (1994). Verb semantics and lexical selec-
tion. In Proc. of ACL-94, pp. 133?138.
Yang, X., J. Su & C. L. Tan (2005). Improving pronoun reso-
lution using statistics-based semantic compatibility informa-
tion. In Proc. of ACL-05, pp. 165?172.
Yang, X., G. Zhou, J. Su & C. L. Tan (2003). Coreference
resolution using competition learning approach. In Proc. of
ACL-03, pp. 176?183.
199
Proceedings of NAACL HLT 2009: Short Papers, pages 225?228,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Tree Linearization in English:
Improving Language Model Based Approaches
Katja Filippova and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We compare two approaches to dependency
tree linearization, a task which arises in many
NLP applications. The first one is the widely
used ?overgenerate and rank? approach which
relies exclusively on a trigram language model
(LM); the second one combines language
modeling with a maximum entropy classifier
trained on a range of linguistic features. The
results provide strong support for the com-
bined method and show that trigram LMs are
appropriate for phrase linearization while on
the clause level a richer representation is nec-
essary to achieve comparable performance.
1 Introduction
To date, many natural language processing appli-
cations rely on syntactic representations and also
modify them by compressing, fusing, or translating
into a different language. A syntactic tree emerg-
ing as a result of such operations has to be lin-
earized to a string of words before it can be out-
put to the end-user. The simple and most widely
used trigram LM has become a standard tool for
tree linearization in English (Langkilde & Knight,
1998). For languages with less rigid word order,
LM-based approaches have been shown to perform
poorly (e.g., Marsi & Krahmer (2005) for Dutch),
and methods relying on a range of linguistic fea-
tures have been successfully applied instead (see
Uchimoto et al (2000) and Ringger et al (2004),
Filippova & Strube (2007) for Japanese and German
resp.). To our knowledge, none of the linearization
studies have compared a LM-based method with
an alternative. Thus, it would be of interest to
draw such a comparison, especially on English data,
where LMs are usually expected to work well.
As an improvement to the LM-based approach,
we propose a combined method which distinguishes
between the phrase and the clause levels:
? it relies on a trigram LM to order words within
phrases;
? it finds the order of clause constituents (i.e.,
constituents dependent on a finite verb) with a
maximum entropy classifier trained on a range
of linguistic features.
We show that such a differentiated approach is ben-
eficial and that the proposed combination outper-
forms the method which relies solely on a LM.
Hence, our results challenge the widespread attitude
that trigram LMs provide an appropriate way to lin-
earize syntactic trees in English but also indicate
that they perform well in linearizing subtrees cor-
responding to phrases.
2 LM-based Approach
Trigram models are easy to build and use, and it has
been shown that more sophisticated n-gram models
(e.g., with higher n, complex smoothing techniques,
skipping, clustering or caching) are often not worth
the effort of implementing them due to data sparse-
ness and other issues (Goodman, 2001). This ex-
plains the popularity of trigram LMs in a variety
of NLP tasks (Jurafsky & Martin, 2008), in partic-
ular, in tree linearization where they have become
225
brothers
predet det
the of
neighbor
my
poss
pobj
prep
all
Figure 1: A tree of the noun phrase all the brothers of my
neighbor
de facto the standard tree linearization tool in ac-
cordance with the ?overgenerate and rank? principle:
given a syntactic tree, one needs to consider all pos-
sible linearizations and then choose the one with the
lowest entropy. Given a projective dependency tree1,
all linearizations can be found recursively by gener-
ating permutations of a node and its children. Unfor-
tunately, the number of possible permutations grows
factorially with the branching factor. Hence it is
highly desirable to prohibit generation of clearly un-
acceptable permutations by putting hard constraints
encoded in the English grammar. The constraints
which we implement in our study are the following:
determiners, possessives, quantifiers and noun or ad-
jective modifiers always precede their heads. Con-
junctions, coordinated elements, prepositional ob-
jects always follow their heads. These constraints
allow us to limit, e.g., the total of 96 (2 ? 2 ? 4!)
possibilities for the tree corresponding to the phrase
all the brothers of my neighbor (see Figure 1) to only
two (all the brothers of my neighbor, the all brothers
of my neighbor).
Still, even with such constraints, in some cases the
list of possible linearizations is too long and has to
be reduced to the first N , where N is supposed to be
sufficiently large. In our experiments we break the
permutation generation process if the limit of 20,000
variants is reached.
3 Combined Approach
The LM approach described above has at least two
disadvantages: (1) long distance dependencies are
not captured, and (2) the list of all possible lineariza-
tions can be huge which makes the search for the
1Note that a phrase structure tree can be converted into a
dependency tree, and some PCFG parsers provide this option.
best string unfeasible. However, our combined ap-
proach is based on the premise that trigram LMs are
well-suited for finding the order within NPs, PPs and
other phrases where the head is not a finite verb.
E.g., given a noun modified by the words big, red
and the, a LM can reliably rank the correct order
higher than incorrect ones ( the big red N vs. the red
big N, etc.).
Next, on the clause level, for every finite verb in
the tree we find the order of its dependents using the
method which we originally developed for German
(Filippova & Strube, 2007), which utilizes a range
of such linguistic features as PoS tag, syntactic role,
length in words, pronominalization, semantic class,
etc.2 For the experiments presented in this paper, we
train two maximum entropy classifiers on all but the
semantic features:
1. The first classifier determines the best starting
point for a sentence: for each constituent de-
pendent on the verb it returns the probability of
this constituent being the first one in a sentence.
The subject and also adjuncts (e.g. temporal ad-
juncts like yesterday) are usually found in the
beginning of the sentence.
2. The second classifier is trained to determine
whether the precedence relation holds between
two adjacent constituents and is applied to all
constituents but the one selected by the first
classifier. The precedence relation defined by
this classifier has been shown to be transitive
and thus can be used to sort randomly ordered
constituents. Note that we do not need to con-
sider all possible orders to find the best one.
Once the order within clause constituents as well as
the order among them is found, the verb is placed
right after the subject. The verb placing step com-
pletes the linearization process.
The need for two distinct classifiers can be illus-
trated with the following example:
(1) a [Earlier today] [she] sent [him] [an email].
b [She] sent [him] [an email] [earlier today].
c *[She] sent [earlier today] [him] [an email].
2See the cited paper for the full list of features and imple-
mentation details.
226
(1a,b) are grammatical while (1c) is hardly accept-
able, and no simple precedence rule can be learned
from pairs of constituents in (1a) and (1b): the tem-
poral adjunct earlier today can precede or follow
each of the other constituents dependent on the verb
(she, him, an email). Thus, the classifier which
determines the precedence relation is not enough.
However, an adequate rule can be inferred with
an additional classifier trained to find good starting
points: a temporal adjunct may appear as the first
constituent in a sentence; if it is not chosen for this
position, it should be preceded by the pronominal-
ized subject (she), the indirect object (him) and the
short non-pronominalized object (an email).
4 Experiments
The goal of our experiments is to check the follow-
ing hypotheses:
1. That trigram LMs are well-suited for phrase
linearization.
2. That there is a considerable drop in perfor-
mance when one uses them for linearization on
the clause level.
3. That an approach which uses a richer represen-
tation on the clause level is more appropriate.
4.1 Data
We take a subset of the TIPSTER3 corpus ? all Wall
Street Journal articles from the period of 1987-92
(approx. 72 mill. words) ? and automatically anno-
tate them with sentence boundaries, part of speech
tags and dependency relations using the Stanford
parser (Klein & Manning, 2003). We reserve a
small subset of about 600 articles (340,000 words)
for testing and use the rest to build a trigram LM
with the CMU toolkit (Clarkson & Rosenfeld, 1997,
with Good-Turing smoothing and vocabulary size of
30,000). To train the maximum entropy classifiers
we use about 41,000 sentences.
4.2 Evaluation
To test the trigram-based approach, we generate all
possible permutations of clause constituents, place
3Description at http://www.ldc.upenn.edu/
Catalog/CatalogEntry.jsp?catalogId=
LDC93T3A.
the verb right after the subject and then rank the re-
sulting strings with the LM taking the information
on sentence boundaries into account. To test the
combined approach, we find the best candidate for
the first position in the clause, then put the remain-
ing constituents in a random order, and finally sort
them by consulting the second classifier.
The purpose of the evaluation is to assess how
good a method is at reproducing the input from its
dependency tree. We separately evaluate the perfor-
mance on the phrase and the clause levels. When
comparing the two methods on the clause level, we
take the clause constituents as they are presented
in the input sentence. Although English allows for
some minor variation in word order and it might
happen that the generated order is not necessarily
wrong if different from the original one, we do not
expect this to happen often and evaluate the perfor-
mance rigorously: only the original order counts as
the correct one. The default evaluation metric is per-
phrase/per-clause accuracy:
acc = |correct||total|
Other metrics we use to measure how different a
generated order of N elements is from the correct
one are:
1. Kendall?s ? , ? = 1 ? 4 tN(N?1) where t is
the minimum number of interchanges of con-
secutive elements to achieve the right order
(Kendall, 1938; Lapata, 2006).
2. Edit distance related di, di = 1 ? mN where m
is the minimum number of deletions combined
with insertions to get to the right order (Ringger
et al, 2004).
E.g., on the phrase level, the incorrectly generated
phrase the all brothers of my neighbor (?1-0-2-3-4-
5?) gets ? = 0.87, di = 0.83. Likewise, given the
input sentence from (1a), the incorrectly generated
order of the four clause constituents in (1c) ? ?1-0-
2-3? ? gets ? of 0.67 and di of 0.75.
4.3 Results
The results of the experiments on the phrase and the
clause levels are presented in Tables 1 and 2 respec-
tively. From the total of 5,000 phrases, 55 (about
227
1%) were discarded because the number of admis-
sible linearizations exceeded the limit of 20,000. In
the first row of Table 1 we give the results for cases
where, with all constraints applied, there were still
several possible linearizations (non-triv; 1,797); the
second row is for all phrases which were longer than
one word (> 1; 2,791); the bottom row presents the
results for the total of 4,945 phrases (all).
acc ? di
non-triv 76% 0.85 0.94
> 1 85% 0.90 0.96
all 91% 0.94 0.98
Table 1: Results of the trigram method on the phrase level
Table 2 presents the results of the trigram-based
(TRIGRAM) and combined (COMBINED) methods on
the clause level. Here, we filtered out trivial cases
and considered only clauses which had at least two
constituents dependent on the verb (approx. 5,000
clauses in total).
acc ? di
TRIGRAM 49% 0.49 0.81
COMBINED 67% 0.71 0.88
Table 2: Results of the two methods on the clause level
4.4 Discussion
The difference in accuracy between the performance
of the trigram model on the phrase and the clause
level is considerable ? 76% vs. 49%. The accuracy
of 76% is remarkable given that the average length
of phrases which counted as non-triv is 6.2 words,
whereas the average clause length in constituents is
3.3. This statistically significant difference in per-
formance supports our hypothesis that the ?overgen-
erate and rank? approach advocated in earlier studies
is more adequate for finding the optimal order within
phrases. The ? value of 0.85 also indicates that many
of the wrong phrase linearizations were near misses.
On the clause level, where long distance dependen-
cies are frequent, an approach which takes a range
of grammatical features into account is more appro-
priate ? this is confirmed by the significantly better
results of the combined method (67%).
5 Conclusions
We investigated two tree linearization methods in
English: the mainstream trigram-based approach
and the one which combines a trigram LM on the
phrase level with two classifiers trained on a range
of linguistic features on the clause level. The results
demonstrate (1) that the combined approach repro-
duces the word order more accurately, and (2) that
the performance of the trigram LM-based method on
phrases is significantly better than on clauses.
Acknowledgments: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004). We would like to thank the
anonymous reviewers for their feedback.
References
Clarkson, P. & R. Rosenfeld (1997). Statistical language
modeling using the CMU-Cambridge toolkit. In Proc.
of EUROSPEECH-97, pp. 2707?2710.
Filippova, K. & M. Strube (2007). Generating constituent
order in German clauses. In Proc. of ACL-07, pp. 320?
327.
Goodman, J. T. (2001). A bit of progress in language
modeling. Computer Speech and Language, pp. 403?
434.
Jurafsky, D. & J. H. Martin (2008). Speech and Language
Processing. Upper Saddle River, N.J.: Prentice Hall.
Kendall, M. G. (1938). A new measure of rank correla-
tion. Biometrika, 30:81?93.
Klein, D. & C. D. Manning (2003). Accurate unlexical-
ized parsing. In Proc. of ACL-03, pp. 423?430.
Langkilde, I. & K. Knight (1998). Generation that ex-
ploits corpus-based statistical knowledge. In Proc. of
COLING-ACL-98, pp. 704?710.
Lapata, M. (2006). Automatic evaluation of information
ordering: Kendall?s tau. Computational Linguistics,
32(4):471?484.
Marsi, E. & E. Krahmer (2005). Explorations in sentence
fusion. In Proc. of ENLG-05, pp. 109?117.
Ringger, E., M. Gamon, R. C. Moore, D. Rojas, M. Smets
& S. Corston-Oliver (2004). Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In Proc. of COLING-04, pp.
673?679.
Uchimoto, K., M. Murata, Q. Ma, S. Sekine & H. Isahara
(2000). Word order acquisition from corpora. In Proc.
of COLING-00, pp. 871?877.
228
Proceedings of NAACL HLT 2009: Tutorials, pages 7?8,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extracting World and Linguistic Knowledge fromWikipedia
Simone Paolo Ponzetto
Dept. of Computational Linguistics
University of Heidelberg
Heidelberg, Germany
http://www.cl.uni-heidelberg.de/?ponzetto
Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
Heidelberg, Germany
http://www.eml-research.de/?strube
Overview
Many research efforts have been devoted to develop robust statistical modeling techniques for many NLP
tasks. Our field is now moving towards more complex tasks (e.g. RTE, QA), which require to complement
these methods with a semantically rich representation based on world and linguistic knowledge (i.e. anno-
tated linguistic data). In this tutorial we show several approaches to extract this knowledge from Wikipedia.
This resource has attracted the attention of much work in the AI community, mainly because it provides
semi-structured information and a large amount of manual annotations. The purpose of this tutorial is to in-
troduce Wikipedia as a resource to the NLP community and to provide an introduction for NLP researchers
both from a scientific and a practical (i.e. data acquisition and processing issues) perspective.
Outline
The tutorial is divided into three main parts:
1. Extracting world knowledge from Wikipedia. We review methods aiming at extracting fully struc-
tured world knowledge from the content of the online encyclopedia. We show how to take categories,
hyperlinks and infoboxes as building blocks for a semantic network with unlabeled relations between
the concepts. The task of taxonomy induction then boils down to labeling the relations between these
concepts, e.g. with isa, part-of, instance-of, located-in, etc. relations.
2. Leveraging linguistic knowledge from Wikipedia. Wikipedia provides shallow markup annotations
which can be interpreted as manual annotations of linguistic phenomena. These ?annotations? include
word boundaries, word senses, named entities, translations of concepts in many languages. Further-
more, Wikipedia can be used as a multilingual comparable corpus.
3. Future directions. Knowledge derived from Wikipedia has the potential to become a resource as
important for NLP as WordNet. Also the Wikipedia edit history provides a repository of linguistic
knowledge which is to be exploited. Potential applications of the knowledge implicitly encoded in the
edit history include spelling corrections, natural language generation, text summarization, etc.
Target audience
This tutorial is designed for students and researchers in Computer Science and Computational Linguistics.
No prior knowledge of information extraction topics is assumed.
7
Speakers? bios
Simone Paolo Ponzetto is an assistant professor at the Computational Linguistics Department of the Univer-
sity of Heidelberg, Germany. His main research interests lie in the area of information extraction, knowledge
acquisition and engineering, lexical semantics, and their application to discourse-based phenomena.
Michael Strube is group leader of the NLP group at EML Research, a privately funded research institute
in Heidelberg, Germany. The NLP group focuses on the areas of semantics, pragmatics and discourse and
applications like summarization and information extraction.
8
Applying Co-Training to Reference Resolution
Christoph Mu?ller
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
mueller@eml.villa-bosch.de
Stefan Rapp
Sony International (Europe) GmbH
Advanced Technology Center Stuttgart
Heinrich-Hertz-Stra?e 1
70327 Stuttgart, Germany
rapp@sony.de
Michael Strube
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
strube@eml.villa-bosch.de
Abstract
In this paper, we investigate the practical
applicability of Co-Training for the task
of building a classifier for reference reso-
lution. We are concerned with the ques-
tion if Co-Training can significantly re-
duce the amount of manual labeling work
and still produce a classifier with an ac-
ceptable performance.
1 Introduction
A major obstacle for natural language processing
systems which analyze natural language texts or
utterances is the need to identify the entities re-
ferred to by means of referring expressions. Among
referring expressions, pronouns and definite noun
phrases (NPs) are the most prominent.
Supervised machine learning algorithms were
used for pronoun resolution with good results (Ge et
al., 1998), and for definite NPs with fairly good re-
sults (Aone and Bennett, 1995; McCarthy and Lehn-
ert, 1995; Soon et al, 2001). However, the defi-
ciency of supervised machine learning approaches is
the need for an unknown amount of annotated train-
ing data for optimal performance.
So, researchers in NLP began to experiment with
weakly supervised machine learning algorithms
such as Co-Training (Blum and Mitchell, 1998).
Among others Co-Training was applied to document
classification (Blum and Mitchell, 1998), named-
entity recognition (Collins and Singer, 1999), noun
phrase bracketing (Pierce and Cardie, 2001), and
statistical parsing (Sarkar, 2001). In this paper we
apply Co-Training to the problem of reference reso-
lution in German texts from the tourism domain in
order to provide answers to the following questions:
  Does Co-Training work at all for this task
(when compared to conventional C4.5 decision
tree learning)?
  How much labeled training data is required for
achieving a reasonable performance?
First, we discuss features that have been found to
be relevant for the task of reference resolution, and
describe the feature set that we are using (Section 2).
Then we briefly introduce the Co-Training paradigm
(Section 3), which is followed by a description of the
corpus we use, the corpus annotation, and the way
we prepared the data for using a binary classifier in
the Co-Training algorithm (Section 4). In Section 5
we specify the experimental setup and report on the
results.
2 Features for Reference Resolution
2.1 Previous Work
Driven by the necessity to provide robust systems
for the MUC system evaluations, researchers began
to look for those features which were particular im-
portant for the task of reference resolution. While
most features for pronoun resolution have been de-
scribed in the literature for decades, researchers only
recently began to look for robust and cheap features,
i.e., those which perform well over several domains
and can be annotated (semi-) automatically. Also,
the relative quantitative contribution of each of these
features came into focus only after the advent of
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 352-359.
                         Proceedings of the 40th Annual Meeting of the Association for
corpus-based and statistical methods. In the follow-
ing, we describe a few earlier contributions with re-
spect to the features used.
Decision tree algorithms were used for ref-
erence resolution by Aone and Bennett (1995,
C4.5), McCarthy and Lehnert (1995, C4.5) and
Soon et al (2001, C5.0). This approach requires
the definition of a set of training features de-
scribing pairs of anaphors and their antecedents.
Aone and Bennett (1995), working on reference
resolution in Japanese newspaper articles, use
66 features. They do not mention all of these
explicitly but emphasize the features POS-tag,
grammatical role, semantic class and distance.
The set of semantic classes they use appears to be
rather elaborated and highly domain-dependent.
Aone and Bennett (1995) report that their best
classifier achieved an F-measure of about 77% after
training on 250 documents. They mention that
it was important for the training data to contain
transitive positives, i.e., all possible coreference
relations within an anaphoric chain.
McCarthy and Lehnert (1995) describe a refer-
ence resolution component which they evaluated on
the MUC-5 English Joint Venture corpus. They dis-
tinguish between features which focus on individ-
ual noun phrases (e.g. Does noun phrase contain a
name?) and features which focus on the anaphoric
relation (e.g. Do both share a common NP?). It
was criticized (Soon et al, 2001) that the features
used by McCarthy and Lehnert (1995) are highly id-
iosyncratic and applicable only to one particular do-
main. McCarthy and Lehnert (1995) achieved re-
sults of about 86% F-measure (evaluated accord-
ing to Vilain et al (1995)) on the MUC-5 data set.
However, only a defined subset of all possible ref-
erence resolution cases was considered relevant in
the MUC-5 task description, e.g., only entity refer-
ences. For this case, the domain-dependent features
may have been particularly important, making it dif-
ficult to compare the results of this approach to oth-
ers working on less restricted domains.
Soon et al (2001) use twelve features (see Ta-
ble 1). They show a part of their decision tree in
which the weak string identity feature (i.e. iden-
tity after determiners have been removed) appears
to be the most important one. They also report
on the relative contribution of the features where
? distance in sentences between anaphor and antecedent
? antecedent is a pronoun?
? anaphor is a pronoun?
? weak string identity between anaphor and antecedent
? anaphor is a definite noun phrase?
? anaphor is a demonstrative pronoun?
? number agreement between anaphor and antecedent
? semantic class agreement between anaphor and an-
tecedent
? gender agreement between anaphor and antecedent
? anaphor and antecedent are both proper names?
? an alias feature (used for proper names and acronyms)
? an appositive feature
Table 1: Features used by Soon et al
the three features weak string identity, alias (which
maps named entities in order to resolve dates, per-
son names, acronyms, etc.) and appositive seem to
cover most of the cases (the other nine features con-
tribute only 2.3% F-measure for MUC-6 texts and
1% F-measure for MUC-7 texts). Soon et al (2001)
include all noun phrases returned by their NP iden-
tifier and report an F-measure of 62.6% for MUC-6
data and 60.4% for MUC-7 data. They only used
pairs of anaphors and their closest antecedents as
positive examples in training, but evaluated accord-
ing to Vilain et al (1995).
Cardie and Wagstaff (1999) describe an unsuper-
vised clustering approach to noun phrase corefer-
ence resolution in which features are assigned to sin-
gle noun phrases only. They use the features shown
in Table 2, all of which are obtained automatically
without any manual tagging.
? position (NPs are numbered sequentially)
? pronoun type (nom., acc., possessive, ambiguous)
? article (indefinite, definite, none)
? appositive (yes, no)
? number (singular, plural)
? proper name (yes, no)
? semantic class (based on WordNet: time, city, animal,
human, object; based on a separate algorithm: number,
money, company)
? gender (masculine, feminine, either, neuter)
? animacy (anim, inanim)
Table 2: Features used by Cardie and Wagstaff
The feature semantic class used by
Cardie and Wagstaff (1999) seems to be a
domain-dependent one which can only be
used for the MUC domain and similar ones.
Cardie and Wagstaff (1999) report a performance
of 53,6% F-measure (evaluated according to
Vilain et al (1995)).
2.2 Our Features
We consider the features we use for our weakly
supervised approach to be domain-independent.
We distinguish between features assigned to noun
phrases and features assigned to the potential coref-
erence relation. They are listed in Table 3 together
with their respective possible values. In the liter-
ature on reference resolution it is claimed that the
antecedent?s grammatical function and its realiza-
tion are important. Hence we introduce the features
ante gram func and ante npform. The identity in
grammatical function of a potential anaphor and an-
tecedent is captured in the feature syn par. Since
in German the gender and the semantic class do not
necessarily coincide (i.e. objects are not necessarily
neuter as in English) we also provide a semantic-
class feature which captures the difference between
human, concrete, and abstract objects. This basi-
cally corresponds to the gender attribute in English.
The feature wdist captures the distance in words be-
tween anaphor and antecedent, the feature ddist cap-
tures the distance in sentences, the feature mdist the
number of markables (NPs) between anaphor and
antecedent. Features like the string ident and sub-
string match features were used by other researchers
(Soon et al, 2001), while the features ante med and
ana med were used by Strube et al (2002) in order
to improve the performance for definite NPs. The
minimum edit distance (MED) computes the simi-
larity of strings by taking into account the minimum
number of editing operations (substitutions s, inser-
tions i, deletions d) needed to transform one string
into the other (Wagner and Fischer, 1974). The
MED is computed from these editing operations and
the length of the potential antecedent m or the length
of the anaphor n.
3 Co-Training
Co-Training (Blum and Mitchell, 1998) is a meta-
learning algorithm which exploits unlabeled in ad-
dition to labeled training data for classifier learn-
ing. A Co-Training classifier is complex in the sense
that it consists of two simple classifiers (most often
Naive Bayes, e.g. by Blum and Mitchell (1998) and
Pierce and Cardie (2001)). Initially, these classifiers
are trained in the conventional way using a small set
of size L of labeled training data. In this process,
each of the two classifiers is trained on a different
subset of features of the training data. These feature
subsets are commonly referred to as different views
that the classifiers have on the data, i.e., each classi-
fier describes a given instance in terms of different
features. The Co-Training algorithm is supposed to
bootstrap by gradually extending the training data
with self-labeled instances. It utilizes the two classi-
fiers by letting them in turn label the p best positive
and n best negative instances from a set of size P
of unlabeled training data (referred to in the litera-
ture as the pool). Instances labeled by one classifier
are then added to the other?s training data, and vice
versa. After each turn, both classifiers are re-trained
on their augmented training sets, and the pool is re-
filled with
	

unlabeled training instances
drawn at random. This process is repeated either for
a given number of iterations I or until all the unla-
beled data has been labeled. In particular the defi-
nition of the two data views appears to be a crucial
factor which can strongly influence the behaviour of
Co-Training. A number of requirements for these
views are mentioned in the literature, e.g., that they
have to be disjoint or even conditionally indepen-
dent (but cf. Nigam and Ghani (2000)). Another im-
portant factor is the ratio between p and n, i.e., the
number of positive and negative instances added in
each iteration. These values are commonly chosen
in such a way as to reflect the empirical class distri-
bution of the respective instances.
4 Data
4.1 Text Corpus
Our corpus consists of 250 short German texts (total
36924 tokens, 9399 NPs, 2179 anaphoric NPs) about
sights, historic events and persons in Heidelberg.
The average length of the texts was 149 tokens. The
texts were POS-tagged using TnT (Brants, 2000). A
basic identification of markables (i.e. NPs) was ob-
tained by using the NP-Chunker Chunkie (Skut and
Brants, 1998). The POS-tagger was also used for
assigning attributes to markables (e.g. the NP form).
The automatic annotation was followed by a man-
Document level features
1. doc id document number (1 . . . 250)
NP-level features
2. ante gram func grammatical function of antecedent (subject, object, other)
3. ante npform form of antecedent (definite NP, indefinite NP, personal pronoun,
demonstrative pronoun, possessive pronoun, proper name)
4. ante agree agreement in person, gender, number
5. ante semanticclass semantic class of antecedent (human, concrete object, abstract object)
6. ana gram func grammatical function of anaphor (subject, object, other)
7. ana npform form of anaphor (definite NP, indefinite NP, personal pronoun,
demonstrative pronoun, possessive pronoun, proper name)
8. ana agree agreement in person, gender, number
9. ana semanticclass semantic class of anaphor (human, concrete object, abstract object)
Coreference-level features
10. wdist distance between anaphor and antecedent in words (1 . . . n)
11. ddist distance between anaphor and antecedent in sentences (0, 1,  1)
12. mdist distance between anaphor and antecedent in markables (NPs) (1 . . . n)
13. syn par anaphor and antecedent have the same grammatical function (yes, no)
14. string ident anaphor and antecedent consist of identical strings (yes, no)
15. substring match one string contains the other (yes, no)
16. ante med minimum edit distance to anaphor:  ffA Machine Learning Approach to Pronoun Resolution in Spoken Dialogue
Michael Strube and Christoph Mu?ller
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
 
michael.strube|christoph.mueller  @eml.villa-bosch.de
Abstract
We apply a decision tree based approach
to pronoun resolution in spoken dialogue.
Our system deals with pronouns with NP-
and non-NP-antecedents. We present a set
of features designed for pronoun resolu-
tion in spoken dialogue and determine the
most promising features. We evaluate the
system on twenty Switchboard dialogues
and show that it compares well to Byron?s
(2002) manually tuned system.
1 Introduction
Corpus-based methods and machine learning tech-
niques have been applied to anaphora resolution in
written text with considerable success (Soon et al,
2001; Ng & Cardie, 2002, among others). It has
been demonstrated that systems based on these ap-
proaches achieve a performance that is comparable
to hand-crafted systems. Since they can easily be
applied to new domains it seems also feasible to
port a given corpus-based anaphora resolution sys-
tem from written text to spoken dialogue. This pa-
per describes the extensions and adaptations needed
for applying our anaphora resolution system (Mu?ller
et al, 2002; Strube et al, 2002) to pronoun resolu-
tion in spoken dialogue.
There are important differences between written
text and spoken dialogue which have to be accounted
for. The most obvious difference is that in spo-
ken dialogue there is an abundance of (personal and
demonstrative) pronouns with non-NP-antecedents
or no antecedents at all. Corpus studies have shown
that a significant amount of pronouns in spoken di-
alogue have non-NP-antecedents: Byron & Allen
(1998) report that about 50% of the pronouns in the
TRAINS93 corpus have non-NP-antecedents. Eck-
ert & Strube (2000) note that only about 45% of
the pronouns in a set of Switchboard dialogues have
NP-antecedents. The remainder consists of 22%
which have non-NP-antecedents and 33% without
antecedents. These studies suggest that the perfor-
mance of a pronoun resolution algorithm can be im-
proved considerably by enabling it to resolve also
pronouns with non-NP-antecedents.
Because of the difficulties a pronoun resolution
algorithm encounters in spoken dialogue, previous
approaches were applied only to tiny domains, they
needed deep semantic analysis and discourse pro-
cessing and relied on hand-crafted knowledge bases.
In contrast, we build on our existing anaphora res-
olution system and incrementally add new features
specifically devised for spoken dialogue. That way
we are able to determine relatively powerful yet
computationally cheap features. To our knowledge
the work presented here describes the first imple-
mented system for corpus-based anaphora resolution
dealing also with non-NP-antecedents.
2 NP- vs. Non-NP-Antecedents
Spoken dialogue contains more pronouns with non-
NP-antecedents than written text does. However,
pronouns with NP-antecedents (like 3rd pers. mas-
culine/feminine pronouns, cf. he in the example be-
low) still constitute the largest fraction of all coref-
erential pronouns in the Switchboard corpus.
In spoken dialogue there are considerable num-
bers of pronouns that pick up different kinds of
abstract objects from the previous discourse, e.g.
events, states, concepts, propositions or facts (Web-
ber, 1991; Asher, 1993). These anaphors then have
VP-antecedents (?it   ? in (B6) below) or sentential
antecedents (?that  ? in (B5)).
A1: . . . [he]  ?s nine months old. . . .
A2: [He]  likes to dig around a little bit.
A3: [His]  mother comes in and says, why did you let [him] 
[play in the dirt] ,
A:4 I guess [[he]  ?s enjoying himself]  .
B5: [That]  ?s right.
B6: [It] ?s healthy, . . .
A major problem for pronoun resolution in spo-
ken dialogue is the large number of personal and
demonstrative pronouns which are either not refer-
ential at all (e.g. expletive pronouns) or for which a
particular antecedent cannot easily be determined by
humans (called vague anaphors by Eckert & Strube
(2000)).
In the following example, the ?that  ? in utter-
ance (A3) refers back to utterance (A1). As for
the first two pronouns in (B4), following Eckert &
Strube (2000) and Byron (2002) we assume that re-
ferring expressions in disfluencies, abandoned utter-
ances etc. are excluded from the resolution. The
third pronoun in (B4) is an expletive. The pronoun
in (A5) is different in that it is indeed referential: it
refers back to?that  ? from (A3).
A1: . . . [There is a lot of theft, a lot of assault dealing with, uh,
people trying to get money for drugs.  ]
B2: Yeah.
A3: And, uh, I think [that  ]?s a national problem, though.
B4: It, it, it?s pretty bad here, too.
A5: [It  ]?s not unique . . .
Pronoun resolution in spoken dialogue also has
to deal with the whole range of difficulties that
come with processing spoken language: disfluen-
cies, hesitations, abandoned utterances, interrup-
tions, backchannels, etc. These phenomena have to
be taken into account when formulating constraints
on e.g. the search space in which an anaphor looks
for its antecedent. E.g., utterance (B2) in the previ-
ous example does not contain any referring expres-
sions. So the demonstrative pronoun in (A3) has to
have access not only to (B2) but also to (A1).
3 Data
3.1 Corpus
Our work is based on twenty randomly chosen
Switchboard dialogues. Taken together, the dia-
logues contain 30810 tokens (words and punctua-
tion) in 3275 sentences / 1771 turns. The annotation
consists of 16601 markables, i.e. sequences of words
and attributes associated with them. On the top level,
different types of markables are distinguished: NP-
markables identify referring expressions like noun
phrases, pronouns and proper names. Some of
the attributes for these markables are derived from
the Penn Treebank version of the Switchboard dia-
logues, e.g. grammatical function, NP form, gram-
matical case and depth of embedding in the syn-
tactical structure. VP-markables are verb phrases,
S-markables sentences. Disfluency-markables are
noun phrases or pronouns which occur in unfin-
ished or abandoned utterances. Among other (type-
dependent) attributes, markables contain a member
attribute with the ID of the coreference class they
are part of (if any). If an expression is used to re-
fer to an entity that is not referred to by any other
expression, it is considered a singleton.
Table 1 gives the distribution of the npform at-
tribute for NP-markables. The second and third row
give the number of non-singletons and singletons re-
spectively that add up to the total number given in
the first row.
Table 2 shows the distribution of the agreement
attribute (i.e. person, gender, and number) for the
pronominal expressions in our corpus. The left fig-
ure in each cell gives the total number of expres-
sions, the right figure gives the number of non-
singletons. Note the relatively high number of sin-
gletons among the personal and demonstrative pro-
nouns (223 for it, 60 for they and 82 for that). These
pronouns are either expletive or vague, and cause
the most trouble for a pronoun resolution algorithm,
which will usually attempt to find an antecedent
nonetheless. Singleton they pronouns, in particu-
lar, are typical for spoken language (as opposed to
defNP indefNP NNP prp prp$ dtpro
Total 1080 1899 217 1075 70 392
In coreference relation 219 163 94 786 56 309
Singletons 861 1736 123 289 14 83
Table 1: Distribution of npform Feature on Markables (w/o 1st and 2nd Persons)
3m 3f 3n 3p
prp 67 63 49 47 541 318 418 358
prp$ 18 15 14 11 3 3 35 27
dtpro 0 0 0 0 380 298 12 11
  85 78 63 58 924 619 465 396
Table 2: Distribution of Agreement Feature on Pronominal Expressions
written text). The same is true for anaphors with
non-NP-antecedents. However, while they are far
more frequent in spoken language than in written
text, they still constitute only a fraction of all coref-
erential expressions in our corpus. This defines an
upper limit for what the resolution of these kinds of
anaphors can contribute at all. These facts have to be
kept in mind when comparing our results to results
of coreference resolution in written text.
3.2 Data Generation
Training and test data instances were generated from
our corpus as follows. All markables were sorted
in document order, and markables for first and sec-
ond person pronouns were removed. The resulting
list was then processed from top to bottom. If the
list contained an NP-markable at the current posi-
tion and if this markable was not an indefinite noun
phrase, it was considered a potential anaphor. In
that case, pairs of potentially coreferring expressions
were generated by combining the potential anaphor
with each compatible1 NP-markable preceding2 it
in the list. The resulting pairs were labelled P if
both markables had the same (non-empty) value in
their member attribute, N otherwise. For anaphors
with non-NP-antecedents, additional training and
test data instances had to be generated. This process
was triggered by the markable at the current position
being it or that. In that case, a small set of poten-
tial non-NP-antecedents was generated by selecting
S- and VP-markables from the last two valid sen-
tences preceding the potential anaphor. The choice
1Markables are considered compatible if they do not mis-
match in terms of agreement.
2We disregard the phenomenon of cataphor here.
of the last two sentences was motivated pragmat-
ically by considerations to keep the search space
(and the number of instances) small. A sentence
was considered valid if it was neither unfinished
nor a backchannel utterance (like e.g. ?Uh-huh?,
?Yeah?, etc.). From the selected markables, inac-
cessible non-NP-expressions were automatically re-
moved. We considered an expression inaccessible
if it ended before the sentence in which it was con-
tained. This was intended to be a rough approxi-
mation of the concept of the right frontier (Webber,
1991). The remaining expressions were then com-
bined with the potential anaphor. Finally, the result-
ing pairs were labelled P or N and added to the in-
stances generated with NP-antecedents.
4 Features
We distinguish two classes of features: NP-level
features specify e.g. the grammatical function, NP
form, morpho-syntax, grammatical case and the
depth of embedding in the syntactical structure.
For these features, each instance contains one
value for the antecedent and one for the anaphor.
Coreference-level features, on the other hand, de-
scribe the relation between antecedent and anaphor
in terms of e.g. distance (in words, markables and
sentences), compatibility in terms of agreement and
identity of syntactic function. For these features,
each instance contains only one value.
In addition, we introduce a set of features which
is partly tailored to the processing of spoken dia-
logue. The feature ante exp type (17) is a rather
obvious yet useful feature to distinguish NP- from
non-NP-antecedents. The features ana np , vp and
NP-level features
1. ante gram func grammatical function of antecedent
2. ante npform form of antecedent
3. ante agree person, gender, number
4. ante case grammatical case of antecedent
5. ante s depth the level of embedding in a sentence
6. ana gram func grammatical function of anaphor
7. ana npform form of anaphor
8. ana agree person, gender, number
9. ana case grammatical case of anaphor
10. ana s depth the level of embedding in a sentence
Coreference-level features
11. agree comp compatibility in agreement between anaphor and antecedent
12. npform comp compatibilty in NP form between anaphor and antecedent
13. wdist distance between anaphor and antecedent in words
14. mdist distance between anaphor and antecedent in markables
15. sdist distance between anaphor and antecedent in sentences
16. syn par anaphor and antecedent have the same grammatical function (yes, no)
Features introduced for spoken dialogue
17. ante exp type type of antecedent (NP, S, VP)
18. ana np pref preference for NP arguments
19. ana vp pref preference for VP arguments
20. ana s pref preference for S arguments
21. mdist 3mf3p (see text)
22. mdist 3n (see text)
23. ante tfidf (see text)
24. ante ic (see text)
25. wdist ic (see text)
Table 3: Our Features
s pref (18, 19, 20) describe a verb?s preference for
arguments of a particular type. Inspired by the
work of Eckert & Strube (2000) and Byron (2002),
these features capture preferences for NP- or non-
NP-antecedents by taking a pronoun?s predicative
context into account. The underlying assumption is
that if a verb preceding a personal or demonstrative
pronoun preferentially subcategorizes sentences or
VPs, then the pronoun will be likely to have a non-
NP-antecedent. The features are based on a verb list
compiled from 553 Switchboard dialogues.3 For ev-
ery verb occurring in the corpus, this list contains
up to three entries giving the absolute count of cases
where the verb has a direct argument of type NP, VP
or S. When the verb list was produced, pronominal
arguments were ignored. The features mdist 3mf3p
and mdist 3n (21, 22) are refinements of the mdist
feature. They measure the distance in markables be-
tween antecedent and anaphor, but in doing so they
take the agreement value of the anaphor into ac-
count. For anaphors with an agreement value of 3mf
or 3p, mdist 3mf3p is measured as D = 1 + the num-
3It seemed preferable to compile our own list instead of us-
ing existing ones like Briscoe & Carroll (1997).
ber of NP-markables between anaphor and potential
antecedent. Anaphors with an agreement value of
3n, (i.e. it or that), on the other hand, potentially
have non-NP-antecedents, so mdist 3n is measured
as D + the number of anaphorically accessible4 S-
and VP-markables between anaphor and potential
antecedent.
The feature ante tfifd (23) is supposed to capture
the relative importance of an expression for a dia-
logue. The underlying assumption is that the higher
the importance of a non-NP expression, the higher
the probability of its being referred back to. For
our purposes, we calculated TF for every word by
counting its frequency in each of our twenty Switch-
board dialogues separately. The calculation of IDF
was based on a set of 553 Switchboard dialogues.
For every word, we calculated IDF as log(553/N   ),
with N   =number of documents containing the word.
For every non-NP-markable, an average TF*IDF
value was calculated as the TF*IDF sum of all words
comprising the markable, divided by the number of
4As mentioned earlier, the definition of accessibility of non-
NP-antecedents is inspired by the concept of the right frontier
(Webber, 1991).
words in the markable. The feature ante ic (24) as
an alternative to ante tfidf is based on the same as-
sumptions as the former. The information content of
a non-NP-markable is calculated as follows, based
on a set of 553 Switchboard dialogues: For each
word in the markable, the IC value was calculated
as the negative log of the total frequency of the word
divided by the total number of words in all 553 dia-
logues. The average IC value was then calculated as
the IC sum of all words in the markable, divided by
the number of words in the markable. Finally, the
feature wdist ic (25) measures the word-based dis-
tance between two expressions. It does so in terms
of the sum of the individual words? IC. The calcula-
tion of the IC was done as described for the ante ic
feature.
5 Experiments and Results
5.1 Experimental Setup
All experiments were performed using the decision
tree learner RPART (Therneau & Atkinson, 1997),
which is a CART (Breiman et al, 1984) reimple-
mentation for the S-Plus and R statistical comput-
ing environments (we use R, Ihaka & Gentleman
(1996), see http://www.r-project.org). We used the
standard pruning and control settings for RPART
(cp=0.0001, minsplit=20, minbucket=7). All results
reported were obtained by performing 20-fold cross-
validation.
In the prediction phase, the trained classifier is ex-
posed to unlabeled instances of test data. The classi-
fier?s task is to label each instance. When an instance
is labeled as coreferring, the IDs of the anaphor and
antecedent are kept in a response list for the evalua-
tion according to Vilain et al (1995).
For determining the relevant feature set we fol-
lowed an iterative procedure similar to the wrap-
per approach for feature selection (Kohavi & John,
1997). We start with a model based on a set of prede-
fined baseline features. Then we train models com-
bining the baseline with all additional features sep-
arately. We choose the best performing feature (f-
measure according to Vilain et al (1995)), adding
it to the model. We then train classifiers combining
the enhanced model with each of the remaining fea-
tures separately. We again choose the best perform-
ing classifier and add the corresponding new feature
to the model. This process is repeated as long as
significant improvement can be observed.
5.2 Results
In our experiments we split the data in three sets ac-
cording to the agreement of the anaphor: third per-
son masculine and feminine pronouns (3mf), third
person neuter pronouns (3n), and third person plural
pronouns (3p). Since only 3n-pronouns have non-
NP-antecedents, we were mainly interested in im-
provements in this data set.
We used the same baseline model for each data
set. The baseline model corresponds to a pronoun
resolution algorithm commonly applied to written
text, i.e., it uses only the features in the first two
parts of Table 3. For the baseline model we gener-
ated training and test data which included only NP-
antecedents.
Then we performed experiments using the fea-
tures introduced for spoken dialogue. The training
and test data for the models using additional features
included NP- and non-NP-antecedents. For each
data set we followed the iterative procedure outlined
in Section 5.1.
In the following tables we present the results of
our experiments. The first column gives the number
of coreference links correctly found by the classifier,
the second column gives the number of all corefer-
ence links found. The third column gives the total
number of coreference links (1250) in the corpus.
During evaluation, the list of all correct links is used
as the key list against which the response list pro-
duced by the classifier (cf. above) is compared. The
remaining three columns show precision, recall and
f-measure, respectively.
Table 4 gives the results for 3mf pronouns. The
baseline model performs very well on this data set
(the low recall figure is due to the fact that the 3mf
data set contains only a small subset of the coref-
erence links expected by the evaluation). The re-
sults are comparable to any pronoun resolution al-
gorithm dealing with written text. This shows that
our pronoun resolution system could be ported to the
spoken dialogue domain without sacrificing perfor-
mance.
Table 5 shows the results for 3n pronouns. The
baseline model does not perform very well. As men-
tioned above, for evaluating the performance of the
correct found total found total correct precision recall f-measure
baseline, features 1-16 120 150 1250 80.00 9.60 17.14
plus mdist 3mf3p 121 153 1250 79.08 9.68 17.25
Table 4: Results for Third Person Masculine and Feminine Pronouns (3mf)
correct found total found total correct precision recall f-measure
baseline, features 1-16 109 235 1250 46.38 8.72 14.68
plus none 97 232 1250 41.81 7.76 13.09
plus ante exp type 137 359 1250 38.16 10.96 17.03
plus wdist ic 154 389 1250 39.59 12.32 18.79
plus ante tfidf 158 391 1250 40.41 12.64 19.26
Table 5: Results for Third Person Neuter Pronouns (3n)
baseline model we removed all potential non-NP-
antecedents from the data. This corresponds to a
naive application of a model developed for written
text to spoken dialogue.
First, we applied the same model to the data set
containing all kinds of antecedents. The perfor-
mance drops somewhat as the classifier is exposed
to non-NP-antecedents without being able to differ-
entiate between NP- and non-NP-antecedents. By
adding the feature ante exp type the classifier is en-
abled to address NP- and non-NP-antecedents dif-
ferently, which results in a considerable gain in per-
formance. Substituting the wdist feature with the
wdist ic feature also improves the performance con-
siderably. The ante tfidf feature only contributes
marginally to the overall performance. ? These re-
sults show that it pays off to consider features par-
ticularly designed for spoken dialogue.
Table 6 presents the results for 3p pronouns,
which do not have non-NP-antecedents. Many of
these pronouns do not have an antecedent at all. Oth-
ers are vague in that human annotators felt them
to be referential, but could not determine an an-
tecedent. Since we did not address that issue in
depth, the classifier tries to find antecedents for these
pronouns indiscriminately, which results in rather
low precision figures, as compared to e.g. those for
3mf. Only the feature wdist ic leads to an improve-
ment over the baseline.
Table 7 shows the results for the combined clas-
sifiers. The improvement in f-measure is due to the
increase in recall while the precision shows only a
slight decrease.
Though some of the features of the baseline
model (features 1-16) still occur in the decision
tree learned, the feature ante exp type divides ma-
jor parts of the tree quite nicely (see Figure 1). Be-
low that node the feature ana npform is used to dis-
tinguish between negative (personal pronouns) and
potential positive cases (demonstrative pronouns).
This confirms the hypothesis by Eckert & Strube
(2000) and Byron (2002) to give high priority to
these features. The decision tree fragment in Figure
1 correctly assigns the P label to 23-7=16 sentential
antecedents.
split, n, loss, yval
* denotes terminal node
...
anteexptype=s,vp 1110 55 N
ananpform=prp 747,11 N *
ananpform=dtpro 363 44 N
anteexptype=vp 177 3 N *
anteexptype=s 186 41 N
udist>=1.5 95 14 N *
udist<1.5 91 27 N
wdistic<43.32 33 4 N *
wdistic>=43.32 58 23 N
anasdepth>=2.5 23 4 N *
anasdepth<2.5 35 16 N
wdistic>=63.62 24 11 N
wdistic<80.60 12 3 N *
wdistic>=80.60 12 4 P *
wdistic<63.62 11 3 P *
Figure 1: Decision Tree Fragment
However, the most important problem is the large
amount of pronouns without antecedents. The
model does find (wrong) antecedents for a lot of pro-
nouns which should not have one. Only a small frac-
tion of these pronouns are true expletives (i.e., they
precede a ?weather? verb or are in constructions like
?It seems that . . . ?. The majority of these cases are
referential, but have no antecedent in the data (i.e.,
correct found total found total correct precision recall f-measure
baseline, features 1-16 227 354 1250 64.12 18.16 28.30
plus wdist ic 230 353 1250 65.16 18.40 28.70
Table 6: Results for Third Person Plural Pronouns (3p)
correct found total found total correct precision recall f-measure
baseline, features 1-16 456 739 1250 61.71 36.48 45.85
combined 509 897 1250 56.74 40.72 47.42
Table 7: Combined Results for All Pronouns
they are vague pronouns).
The overall numbers for precision, recall and f-
measure are fairly low. One reason is that we did not
attempt to resolve anaphoric definite NPs and proper
names though these coreference links are contained
in the evaluation key list. If we removed them from
there, the recall of our experiments would approach
the 51% Byron (2002) mentioned for her system us-
ing only domain-independent semantic restrictions.
6 Comparison to Related Work
Our approach for determining the feature set for pro-
noun resolution resembles the so-called wrapper ap-
proach for feature selection (Kohavi & John, 1997).
This is in contrast to the majority of other work on
feature selection for anaphora resolution, which was
hardly ever done systematically. E.g. Soon et al
(2001) only compared baseline systems consisting
of one feature each, only three of which yielded an
f-measure greater than zero. Then they combined
these features and achieved results which were close
to the best overall results they report. While this tells
us which features contribute a lot, it does not give
any information about potential (positive or nega-
tive) influence of the rest. Ng & Cardie (2002) select
the set of features by hand, giving a preference to
high precision features. They admit that this method
is quite subjective.
Corpus-based work about pronoun resolution in
spoken dialogue is almost non-existent. However,
there are a few papers dealing with neuter pronouns
with NP-antecedents. E.g., Dagan & Itai (1991) pre-
sented a corpus-based approach to the resolution of
the pronoun it, but they use a written text corpus and
do not mention non-NP-antecedents at all. Paul et al
(1999) presented a corpus-based anaphora resolu-
tion algorithm for spoken dialogue. For their exper-
iments, however, they restricted anaphoric relations
to those with NP-antecedents.
Byron (2002) presented a symbolic approach
which resolves pronouns with NP- and non-NP-
antecedents in spoken dialogue in the TRAINS do-
main. Byron extends a pronoun resolution al-
gorithm (Tetrault, 2001) with semantic filtering,
thus enabling it to resolve anaphors with non-NP-
antecedents as well. Semantic filtering relies on
knowledge about semantic restrictions associated
with verbs, like semantic compatibility between sub-
ject and predicative noun or predicative adjective.
An evaluation on ten TRAINS93 dialogues with
80 3rd person pronouns and 100 demonstrative pro-
nouns shows that semantic filtering and the im-
plementation of different search strategies for per-
sonal and demonstrative pronouns yields a suc-
cess rate of 72%. As Byron admits, the ma-
jor limitation of her algorithm is its dependence
on domain-dependent resources which cover the
domain entirely. When evaluating her algorithm
with only domain-independent semantics, Byron
achieved 51% success rate. What is problematic
with her approach is that she assumes the input to
her algorithm to be only referential pronouns. This
simplifies the task considerably.
7 Conclusions and Future Work
We presented a machine learning approach to pro-
noun resolution in spoken dialogue. We built upon
a system we used for anaphora resolution in writ-
ten text and extended it with a set of features de-
signed for spoken dialogue. We refined distance
features and used metrics from information retrieval
for determining non-NP-antecedents. Inspired by
the more linguistically oriented work by Eckert &
Strube (2000) and Byron (2002) we also evaluated
the contribution of features which used the predica-
tive context of the pronoun to be resolved. However,
these features did not show up in the final models
since they did not lead to an improvement. Instead,
rather simple distance metrics were preferred. While
we were (almost) satisfied with the performance of
these features, the major problem for a spoken dia-
logue pronoun resolution algorithm is the abundance
of pronouns without antecedents. Previous research
could avoid dealing with this phenomenon by either
applying the algorithm by hand (Eckert & Strube,
2000) or excluding these cases (Byron, 2002) from
the evaluation. Because we included these cases
in our evaluation we consider our approach at least
comparable to Byron?s system when she uses only
domain-independent semantics. We believe that our
system is more robust than hers and that it can more
easily be ported to new domains.
Acknowledgements. The work presented here has
been partially funded by the German Ministry of
Research and Technology as part of the EMBASSI
project (01 IL 904 D/2) and by the Klaus Tschira
Foundation. We would like to thank Susanne
Wilhelm and Lutz Wind for doing the annota-
tions, Kerstin Schu?rmann, Torben Pastuch and Klaus
Rothenha?usler for helping with the data prepara-
tion.
References
Asher, Nicholas (1993). Reference to Abstract Objects in Dis-
course. Dordrecht, The Netherlands: Kluwer.
Breiman, Leo, Jerome H. Friedman, Charles J. Stone & R.A.
Olshen (1984). Classification and Regression Trees. Bel-
mont, Cal.: Wadsworth and Brooks/Cole.
Briscoe, Ted & John Carroll (1997). Automatic extraction
of subcategorization from corpora. In Proceedings of the
5th Conference on Applied Natural Language Processing,
Washington, D.C., 31 March ? 3 April 1997, pp. 356?363.
Byron, Donna K. (2002). Resolving pronominal reference to
abstract entities. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics, Philadel-
phia, Penn., 7?12 July 2002, pp. 80?87.
Byron, Donna K. & James F. Allen (1998). Resolving demon-
strative pronouns in the TRAINS93 corpus. In New Ap-
proaches to Discourse Anaphora: Proceedings of the Sec-
ond Colloquium on Discourse Anaphora and Anaphor Res-
olution (DAARC2), pp. 68?81.
Dagan, Ido & Alon Itai (1991). A statistical filter for resolv-
ing pronoun references. In Y.A. Feldman & A. Bruckstein
(Eds.), Artificial Intelligence and Computer Vision, pp. 125?
135. Amsterdam: Elsevier.
Eckert, Miriam & Michael Strube (2000). Dialogue acts, syn-
chronising units and anaphora resolution. Journal of Seman-
tics, 17(1):51?89.
Ihaka, Ross & Robert Gentleman (1996). R: A language for
data analysis and graphics. Journal of Computational and
Graphical Statistics, 5:299?314.
Kohavi, Ron & George H. John (1997). Wrappers for fea-
ture subset selection. Artificial Intelligence Journal, 97(1-
2):273?324.
Mu?ller, Christoph, Stefan Rapp & Michael Strube (2002). Ap-
plying Co-Training to reference resolution. In Proceedings
of the 40th Annual Meeting of the Association for Computa-
tional Linguistics, Philadelphia, Penn., 7?12 July 2002, pp.
352?359.
Ng, Vincent & Claire Cardie (2002). Improving machine learn-
ing approaches to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association for Computa-
tional Linguistics, Philadelphia, Penn., 7?12 July 2002, pp.
104?111.
Paul, Michael, Kazuhide Yamamoto & Eiichiro Sumita (1999).
Corpus-based anaphora resolution towards antecedent pref-
erence. In Proc. of the 37th ACL, Workshop Coreference and
Its Applications, College Park, Md., 1999, pp. 47?52.
Soon, Wee Meng, Hwee Tou Ng & Daniel Chung Yong Lim
(2001). A machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics, 27(4):521?
544.
Strube, Michael, Stefan Rapp & Christoph Mu?ller (2002). The
influence of minimum edit distance on reference resolution.
In Proceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing, Philadelphia, Pa., 6?7
July 2002, pp. 312?319.
Tetrault, Joel R. (2001). A corpus-based evaluation of cen-
tering and pronoun resolution. Computational Linguistics,
27(4):507?520.
Therneau, Terry M. & Elizabeth J. Atkinson (1997). An intro-
duction to recursive partitioning using the RPART routines.
Technical Report: Mayo Foundation. Distributed with the
RPART package.
Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly &
Lynette Hirschman (1995). A model-theoretic coreference
scoring scheme. In Proceedings of the 6th Message Under-
standing Conference (MUC-6), pp. 45?52. San Mateo, Cal.:
Morgan Kaufmann.
Webber, Bonnie L. (1991). Structure and ostension in the inter-
pretation of discourse deixis. Language and Cognitive Pro-
cesses, 6(2):107?135.
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 320?327,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generating Constituent Order in German Clauses
Katja Filippova and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We investigate the factors which determine
constituent order in German clauses and pro-
pose an algorithm which performs the task
in two steps: First, the best candidate for
the initial sentence position is chosen. Then,
the order for the remaining constituents is
determined. The first task is more difficult
than the second one because of properties
of the German sentence-initial position. Ex-
periments show a significant improvement
over competing approaches. Our algorithm
is also more efficient than these.
1 Introduction
Many natural languages allow variation in the word
order. This is a challenge for natural language gen-
eration and machine translation systems, or for text
summarizers. E.g., in text-to-text generation (Barzi-
lay & McKeown, 2005; Marsi & Krahmer, 2005;
Wan et al, 2005), new sentences are fused from de-
pendency structures of input sentences. The last step
of sentence fusion is linearization of the resulting
parse. Even for English, which is a language with
fixed word order, this is not a trivial task.
German has a relatively free word order. This
concerns the order of constituents1 within sentences
while the order of words within constituents is rela-
tively rigid. The grammar only partially prescribes
how constituents dependent on the verb should be
ordered, and for many clauses each of the n! possi-
ble permutations of n constituents is grammatical.
1Henceforth, we will use this term to refer to constituents
dependent on the clausal top node, i.e. a verb, only.
In spite of the permanent interest in German word
order in the linguistics community, most studies
have limited their scope to the order of verb argu-
ments and few researchers have implemented ? and
even less evaluated ? a generation algorithm. In this
paper, we present an algorithm, which orders not
only verb arguments but all kinds of constituents,
and evaluate it on a corpus of biographies. For
each parsed sentence in the test set, our maximum-
entropy-based algorithm aims at reproducing the or-
der found in the original text. We investigate the
importance of different linguistic factors and sug-
gest an algorithm to constituent ordering which first
determines the sentence initial constituent and then
orders the remaining ones. We provide evidence
that the task requires language-specific knowledge
to achieve better results and point to the most diffi-
cult part of it. Similar to Langkilde & Knight (1998)
we utilize statistical methods. Unlike overgenera-
tion approaches (Varges & Mellish, 2001, inter alia)
which select the best of all possible outputs ours is
more efficient, because we do not need to generate
every permutation.
2 Theoretical Premises
2.1 Background
It has been suggested that several factors have an in-
fluence on German constituent order. Apart from
the constraints posed by the grammar, information
structure, surface form, and discourse status have
also been shown to play a role. It has also been
observed that there are preferences for a particular
order. The preferences summarized below have mo-
320
tivated our choice of features:
? constituents in the nominative case precede
those in other cases, and dative constituents
often precede those in the accusative case
(Uszkoreit, 1987; Keller, 2000);
? the verb arguments? order depends on the
verb?s subcategorization properties (Kurz,
2000);
? constituents with a definite article precede
those with an indefinite one (Weber & Mu?ller,
2004);
? pronominalized constituents precede non-
pronominalized ones (Kempen & Harbusch,
2004);
? animate referents precede inanimate ones (Pap-
pert et al, 2007);
? short constituents precede longer ones (Kim-
ball, 1973);
? the preferred topic position is right after the
verb (Frey, 2004);
? the initial position is usually occupied by
scene-setting elements and topics (Speyer,
2005).
? there is a default order based on semantic prop-
erties of constituents (Sgall et al, 1986):
Actor < Temporal < SpaceLocative < Means < Ad-
dressee < Patient < Source < Destination < Purpose
Note that most of these preferences were identified
in corpus studies and experiments with native speak-
ers and concern the order of verb arguments only.
Little has been said so far about how non-arguments
should be ordered.
German is a verb second language, i.e., the po-
sition of the verb in the main clause is determined
exclusively by the grammar and is insensitive to
other factors. Thus, the German main clause is di-
vided into two parts by the finite verb: Vorfeld (VF),
which contains exactly one constituent, and Mit-
telfeld (MF), where the remaining constituents are
located. The subordinate clause normally has only
MF. The VF and MF are marked with brackets in
Example 1:
(1) [Au?erdem]
Apart from that
entwickelte
developed
[Lummer
Lummer
eine
a
Quecksilberdampflampe,
Mercury-vapor lamp
um
to
monochromatisches
monochrome
Licht
light
herzustellen].
produce.
?Apart from that, Lummer developed a
Mercury-vapor lamp to produce monochrome
light?.
2.2 Our Hypothesis
The essential contribution of our study is that we
treat preverbal and postverbal parts of the sentence
differently. The sentence-initial position, which in
German is the VF, has been shown to be cognitively
more prominent than other positions (Gernsbacher
& Hargreaves, 1988). Motivated by the theoretical
work by Chafe (1976) and Jacobs (2001), we view
the VF as the place for elements which modify the
situation described in the sentence, i.e. for so called
frame-setting topics (Jacobs, 2001). For example,
temporal or locational constituents, or anaphoric ad-
verbs are good candidates for the VF. We hypoth-
esize that the reasons which bring a constituent to
the VF are different from those which place it, say,
to the beginning of the MF, for the order in the MF
has been shown to be relatively rigid (Keller, 2000;
Kempen & Harbusch, 2004). Speakers have the
freedom of selecting the outgoing point for a sen-
tence. Once they have selected it, the remaining con-
stituents are arranged in the MF, mainly according to
their grammatical properties.
This last observation motivates another hypothe-
sis we make: The cumulation of the properties of
a constituent determines its salience. This salience
can be calculated and used for ordering with a sim-
ple rule stating that more salient constituents should
precede less salient ones. In this case there is no
need to generate all possible orders and rank them.
The best order can be obtained from a random one
by sorting. Our experiments support this view. A
two-step approach, which first selects the best can-
didate for the VF and then arranges the remaining
constituents in the MF with respect to their salience
performs better than algorithms which generate the
order for a sentence as a whole.
321
3 Related Work
Uszkoreit (1987) addresses the problem from a
mostly grammar-based perspective and suggests
weighted constraints, such as [+NOM] ? [+DAT],
[+PRO] ? [?PRO], [?FOCUS] ? [+FOCUS], etc.
Kruijff et al (2001) describe an architecture
which supports generating the appropriate word or-
der for different languages. Inspired by the findings
of the Prague School (Sgall et al, 1986) and Sys-
temic Functional Linguistics (Halliday, 1985), they
focus on the role that information structure plays
in constituent ordering. Kruijff-Korbayova? et al
(2002) address the task of word order generation in
the same vein. Similar to ours, their algorithm rec-
ognizes the special role of the sentence-initial po-
sition which they reserve for the theme ? the point
of departure of the message. Unfortunately, they did
not implement their algorithm, and it is hard to judge
how well the system would perform on real data.
Harbusch et al (2006) present a generation work-
bench, which has the goal of producing not the most
appropriate order, but all grammatical ones. They
also do not provide experimental results.
The work of Uchimoto et al (2000) is done on
the free word order language Japanese. They de-
termine the order of phrasal units dependent on the
same modifiee. Their approach is similar to ours in
that they aim at regenerating the original order from
a dependency parse, but differs in the scope of the
problem as they regenerate the order of modifers for
all and not only for the top clausal node. Using a
maximum entropy framework, they choose the most
probable order from the set of all permutations of n
words by the following formula:
P (1|h) = P ({Wi,i+j = 1|1 ? i ? n? 1, 1 ? j ? n? i}|h)
?
n?1
Y
i=1
n?i
Y
j=1
P (Wi,i+j = 1|hi,i+j)
=
n?1
Y
i=1
n?i
Y
j=1
PME(1|hi,i+j)
(1)
For each permutation, for every pair of words , they
multiply the probability of their being in the correct2
order given the history h. Random variable Wi,i+j
2Only reference orders are assumed to be correct.
is 1 if word wi precedes wi+j in the reference sen-
tence, 0 otherwise. The features they use are akin
to those which play a role in determining German
word order. We use their approach as a non-trivial
baseline in our study.
Ringger et al (2004) aim at regenerating the or-
der of constituents as well as the order within them
for German and French technical manuals. Utilizing
syntactic, semantic, sub-categorization and length
features, they test several statistical models to find
the order which maximizes the probability of an or-
dered tree. Using ?Markov grammars? as the start-
ing point and conditioning on the syntactic category
only, they expand a non-terminal node C by predict-
ing its daughters from left to right:
P (C|h) =
n
Y
i=1
P (di|di?1, ..., di?j , c, h) (2)
Here, c is the syntactic category of C, d and h
are the syntactic categories of C?s daughters and the
daughter which is the head of C respectively.
In their simplest system, whose performance is
only 2.5% worse than the performance of the best
one, they condition on both syntactic categories and
semantic relations (?) according to the formula:
P (C|h) =
n
Y
i=1
?
P (?i|di?1, ?i?1, ...di?j , ?i?j , c, h)
?P (di|?i, di?1, ?i?1..., di?j , ?i?j , c, h)
?
(3)
Although they test their system on German data,
it is hard to compare their results to ours directly.
First, the metric they use does not describe the per-
formance appropriately (see Section 6.1). Second,
while the word order within NPs and PPs as well as
the verb position are prescribed by the grammar to a
large extent, the constituents can theoretically be or-
dered in any way. Thus, by generating the order for
every non-terminal node, they combine two tasks of
different complexity and mix the results of the more
difficult task with those of the easier one.
4 Data
The data we work with is a collection of biogra-
phies from the German version of Wikipedia3. Fully
automatic preprocessing in our system comprises
the following steps: First, a list of people of a
certain Wikipedia category is taken and an article
is extracted for every person. Second, sentence
3http://de.wikipedia.org
322
entwickelte
um herzustellen SUB
monochromatisches Licht
eine Quecksilberdampflampe OBJAau?erdem ADV (conn)Lummer SUBJ (pers)
Figure 1: The representation of the sentence in Example 1
boundaries are identified with a Perl CPAN mod-
ule4 whose performance we improved by extend-
ing the list of abbreviations. Next, the sentences
are split into tokens. The TnT tagger (Brants, 2000)
and the TreeTagger (Schmid, 1997) are used for tag-
ging and lemmatization. Finally, the articles are
parsed with the CDG dependency parser (Foth &
Menzel, 2006). Named entities are classified accord-
ing to their semantic type using lists and category
information from Wikipedia: person (pers), location
(loc), organization (org), or undefined named entity
(undef ne). Temporal expressions (Oktober 1915,
danach (after that) etc.) are identified automatically
by a set of patterns. Inevitable during automatic an-
notation, errors at one of the preprocessing stages
cause errors at the ordering stage.
Distinguishing between main and subordinate
clauses, we split the total of about 19 000 sentences
into training, development and test sets (Table 1).
Clauses with one constituent are sorted out as trivial.
The distribution of both types of clauses according
to their length in constituents is given in Table 2.
train dev test
main 14324 3344 1683
sub 3304 777 408
total 17628 4121 2091
Table 1: Size of the data sets in clauses
2 3 4 5 6+
main 20% 35% 27% 12% 6%
sub 49% 35% 11% 2% 3%
Table 2: Proportion of clauses with certain lengths
4http://search.cpan.org/?holsten/Lingua-DE-Sentence-
0.07/Sentence.pm
Given the sentence in Example 1, we first trans-
form its dependency parse into a more general
representation (Figure 15) and then, based on the
predictions of our learner, arrange the four con-
stituents. For evaluation, we compare the arranged
order against the original one.
Note that we predict neither the position of the
verb, nor the order within constituents as the former
is explicitly determined by the grammar, and the lat-
ter is much more rigid than the order of constituents.
5 Baselines and Algorithms
We compare the performance of two our algorithms
with four baselines.
5.1 Random
We improve a trivial random baseline (RAND) by
two syntax-oriented rules: the first position is re-
served for the subject and the second for the direct
object if there is any; the order of the remaining con-
stituents is generated randomly (RAND IMP).
5.2 Statistical Bigram Model
Similar to Ringger et al (2004), we find the order
with the highest probability conditioned on syntac-
tic and semantic categories. Unlike them we use de-
pendency parses and compute the probability of the
top node only, which is modified by all constituents.
With these adjustments the probability of an order
O given the history h, if conditioned on syntactic
functions of constituents (s1...sn), is simply:
P (O|h) =
n
?
i=1
P (si|si?1, h) (4)
Ringger et al (2004) do not make explicit, what
their set of semantic relations consists of. From the
5OBJA stands for the accusative object.
323
example in the paper, it seems that these are a mix-
ture of lexical and syntactic information6. Our anno-
tation does not specify semantic relations. Instead,
some of the constituents are categorized as pers, loc,
temp, org or undef ne if their heads bear one of these
labels. By joining these with possible syntactic func-
tions, we obtain a larger set of syntactic-semantic
tags as, e.g., subj-pers, pp-loc, adv-temp. We trans-
form each clause in the training set into a sequence
of such tags, plus three tags for the verb position (v),
the beginning (b) and the end (e) of the clause. Then
we compute the bigram probabilities7.
For our third baseline (BIGRAM), we select from
all possible orders the one with the highest probabil-
ity as calculated by the following formula:
P (O|h) =
n
?
i=1
P (ti|ti?1, h) (5)
where ti is from the set of joined tags. For Example
1, possible tag sequences (i.e. orders) are ?b subj-
pers v adv obja sub e?, ?b adv v subj-pers obja sub
e?, ?b obja v adv sub subj-pers e?, etc.
5.3 Uchimoto
For the fourth baseline (UCHIMOTO), we utilized a
maximum entropy learner (OpenNLP8) and reim-
plemented the algorithm of Uchimoto et al (2000).
For every possible permutation, its probability is es-
timated according to Formula (1). The binary clas-
sifier, whose task was to predict the probability that
the order of a pair of constituents is correct, was
trained on the following features describing the verb
or hc ? the head of a constituent c9:
vlex, vpass, vmod the lemma of the root of the
clause (non-auxiliary verb), the voice of the
verb and the number of constituents to order;
lex the lemma of hc or, if hc is a functional word,
the lemma of the word which depends on it;
pos part-of-speech tag of hc;
6E.g. DefDet, Coords, Possr, werden
7We use the CMU Toolkit (Clarkson & Rosenfeld, 1997).
8http://opennlp.sourceforge.net
9We disregarded features which use information specific to
Japanese and non-applicable to German (e.g. on postpositional
particles).
sem if defined, the semantic class of c; e.g. im April
1900 and mit Albert Einstein (with Albert Ein-
stein) are classified temp and pers respectively;
syn, same the syntactic function of hc and whether
it is the same for the two constituents;
mod number of modifiers of hc;
rep whether hc appears in the preceding sentence;
pro whether c contains a (anaphoric) pronoun.
5.4 Maximum Entropy
The first configuration of our system is an extended
version of the UCHIMOTO baseline (MAXENT). To
the features describing c we added the following
ones:
det the kind of determiner modifying hc (def, indef,
non-appl);
rel whether hc is modified by a relative clause (yes,
no, non-appl);
dep the depth of c;
len the length of c in words.
The first two features describe the discourse status
of a constituent; the other two provide information
on its ?weight?. Since our learner treats all values
as nominal, we discretized the values of dep and len
with a C4.5 classifier (Kohavi & Sahami, 1996).
Another modification concerns the efficiency of
the algorithm. Instead of calculating probabilities
for all pairs, we obtain the right order from a random
one by sorting. We compare adjacent elements by
consulting the learner as if we would sort an array of
numbers. Given two adjacent constituents, ci < cj ,
we check the probability of their being in the right
order, i.e. that ci precedes cj : Ppre(ci, cj). If it is
less than 0.5, we transpose the two and compare ci
with the next one.
Since the sorting method presupposes that the pre-
dicted relation is transitive, we checked whether this
is really so on the development and test data sets. We
looked for three constituents ci, cj , ck from a sen-
tence S, such that Ppre(ci, cj) > 0.5, Ppre(cj , ck) >
0.5, Ppre(ci, ck) < 0.5 and found none. Therefore,
unlike UCHIMOTO, where one needs to make exactly
N ! ? N(N ? 1)/2 comparisons, we have to make
N(N ? 1)/2 comparisons at most.
324
5.5 The Two-Step Approach
The main difference between our first algorithm
(MAXENT) and the second one (TWO-STEP) is that
we generate the order in two steps10 (both classifiers
are trained on the same features):
1. For the VF, using the OpenNLP maximum en-
tropy learner for a binary classification (VF vs.
MF), we select the constituent c with the high-
est probability of being in the VF.
2. For the MF, the remaining constituents are put
into a random order and then sorted the way it
is done for MAXENT. The training data for the
second task was generated only from the MF of
clauses.
6 Results
6.1 Evaluation Metrics
We use several metrics to evaluate our systems and
the baselines. The first is per-sentence accuracy
(acc) which is the proportion of correctly regener-
ated sentences. Kendall?s ? , which has been used for
evaluating sentence ordering tasks (Lapata, 2006),
is the second metric we use. ? is calculated as
1? 4 tN(N?1) , where t is the number of interchangesof consecutive elements to arrange N elements in
the right order. ? is sensitive to near misses and
assigns abdc (almost correct order) a score of 0.66
while dcba (inverse order) gets ?1. Note that it is
questionable whether this metric is as appropriate
for word ordering tasks as for sentence ordering ones
because a near miss might turn out to be ungrammat-
ical whereas a more different order stays acceptable.
Apart from acc and ? , we also adopt the metrics
used by Uchimoto et al (2000) and Ringger et al
(2004). The former use agreement rate (agr) cal-
culated as 2pN(N?1) : the number of correctly orderedpairs of constituents over the total number of all pos-
sible pairs, as well as complete agreement which is
basically per-sentence accuracy. Unlike ? , which
has ?1 as the lowest score, agr ranges from 0 to 1.
Ringger et al (2004) evaluate the performance only
in terms of per-constituent edit distance calculated
as mN , where m is the minimum number of moves11
10Since subordinate clauses do not have a VF, the first step is
not needed.
11A move is a deletion combined with an insertion.
needed to arrange N constituents in the right order.
This measure seems less appropriate than ? or agr
because it does not take the distance of the move into
account and scores abced and eabcd equally (0.2).
Since ? and agr, unlike edit distance, give higher
scores to better orders, we compute inverse distance:
inv = 1 ? edit distance instead. Thus, all three met-
rics (? , agr, inv) give the maximum of 1 if con-
stituents are ordered correctly. However, like ? , agr
and inv can give a positive score to an ungrammat-
ical order. Hence, none of the evaluation metrics
describes the performance perfectly. Human eval-
uation which reliably distinguishes between appro-
priate, acceptable, grammatical and ingrammatical
orders was out of choice because of its high cost.
6.2 Results
The results on the test data are presented in Table
3. The performance of TWO-STEP is significantly
better than any other method (?2, p < 0.01). The
performance of MAXENT does not significantly dif-
fer from UCHIMOTO. BIGRAM performed about as
good as UCHIMOTO and MAXENT. We also checked
how well TWO-STEP performs on each of the two
sub-tasks (Table 4) and found that the VF selection
is considerably more difficult than the sorting part.
acc ? agr inv
RAND 15% 0.02 0.51 0.64
RAND IMP 23% 0.24 0.62 0.71
BIGRAM 51% 0.60 0.80 0.83
UCHIMOTO 50% 0.65 0.82 0.83
MAXENT 52% 0.67 0.84 0.84
TWO-STEP 61% 0.72 0.86 0.87
Table 3: Per-clause mean of the results
The most important conclusion we draw from the
results is that the gain of 9% accuracy is due to the
VF selection only, because the feature sets are iden-
tical for MAXENT and TWO-STEP. From this fol-
lows that doing feature selection without splitting
the task in two is ineffective, because the importance
of a feature depends on whether the VF or the MF is
considered. For the MF, feature selection has shown
syn and pos to be the most relevant features. They
alone bring the performance in the MF up to 75%. In
contrast, these two features explain only 56% of the
325
cases in the VF. This implies that the order in the MF
mainly depends on grammatical features, while for
the VF all features are important because removal of
any feature caused a loss in accuracy.
acc ? agr inv
TWO-STEP VF 68% - - -
TWO-STEP MF 80% 0.92 0.96 0.95
Table 4: Mean of the results for the VF and the MF
Another important finding is that there is no need
to overgenerate to find the right order. Insignificant
for clauses with two or three constituents, for clauses
with 10 constituents, the number of comparisons is
reduced drastically from 163,296,000 to 45.
According to the inv metric, our results are con-
siderably worse than those reported by Ringger et al
(2004). As mentioned in Section 3, the fact that they
generate the order for every non-terminal node se-
riously inflates their numbers. Apart from that, they
do not report accuracy, and it is unknown, how many
sentences they actually reproduced correctly.
6.3 Error Analysis
To reveal the main error sources, we analyzed incor-
rect predictions concerning the VF and the MF, one
hundred for each. Most errors in the VF did not lead
to unacceptability or ungrammaticality. From lexi-
cal and semantic features, the classifier learned that
some expressions are often used in the beginning of
a sentence. These are temporal or locational PPs,
anaphoric adverbials, some connectives or phrases
starting with unlike X, together with X, as X, etc.
Such elements were placed in the VF instead of the
subject and caused an error although both variants
were equally acceptable. In other cases the classi-
fier could not find a better candidate but the subject
because it could not conclude from the provided fea-
tures that another constituent would nicely introduce
the sentence into the discourse. Mainly this con-
cerns recognizing information familiar to the reader
not by an already mentioned entity, but one which is
inferrable from what has been read.
In the MF, many orders had a PP transposed with
the direct object. In some cases the predicted order
seemed as good as the correct one. Often the algo-
rithm failed at identifying verb-specific preferences:
E.g., some verbs take PPs with the locational mean-
ing as an argument and normally have them right
next to them, whereas others do not. Another fre-
quent error was the wrong placement of superficially
identical constituents, e.g. two PPs of the same size.
To handle this error, the system needs more spe-
cific semantic information. Some errors were caused
by the parser, which created extra constituents (e.g.
false PP or adverb attachment) or confused the sub-
ject with the direct verb.
We retrained our system on a corpus of newspaper
articles (Telljohann et al, 2003, Tu?Ba-D/Z) which is
manually annotated but encodes no semantic knowl-
edge. The results for the MF were the same as on the
data from Wikipedia. The results for the VF were
much worse (45%) because of the lack of semantic
information.
7 Conclusion
We presented a novel approach to ordering con-
stituents in German. The results indicate that a
linguistically-motivated two-step system, which first
selects a constituent for the initial position and then
orders the remaining ones, works significantly better
than approaches which do not make this separation.
Our results also confirm the hypothesis ? which has
been attested in several corpus studies ? that the or-
der in the MF is rather rigid and dependent on gram-
matical properties.
We have also demonstrated that there is no need
to overgenerate to find the best order. On a prac-
tical side, this finding reduces the amount of work
considerably. Theoretically, it lets us conclude that
the relatively fixed order in the MF depends on the
salience which can be predicted mainly from gram-
matical features. It is much harder to predict which
element should be placed in the VF. We suppose that
this difficulty comes from the double function of the
initial position which can either introduce the ad-
dressation topic, or be the scene- or frame-setting
position (Jacobs, 2001).
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004). We would also like to thank
Elke Teich and the three anonymous reviewers for
their useful comments.
326
References
Barzilay, R. & K. R. McKeown (2005). Sentence fusion for
multidocument news summarization. Computational Lin-
guistics, 31(3):297?327.
Brants, T. (2000). TnT ? A statistical Part-of-Speech tagger. In
Proceedings of the 6th Conference on Applied Natural Lan-
guage Processing, Seattle, Wash., 29 April ? 4 May 2000,
pp. 224?231.
Chafe, W. (1976). Givenness, contrastiveness, definiteness, sub-
jects, topics, and point of view. In C. Li (Ed.), Subject and
Topic, pp. 25?55. New York, N.Y.: Academic Press.
Clarkson, P. & R. Rosenfeld (1997). Statistical language mod-
eling using the CMU-Cambridge toolkit. In Proceedings
of the 5th European Conference on Speech Communication
and Technology, Rhodes, Greece, 22-25 September 1997, pp.
2707?2710.
Foth, K. & W. Menzel (2006). Hybrid parsing: Using proba-
bilistic models as predictors for a symbolic parser. In Pro-
ceedings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, Sydney, Australia, 17?
21 July 2006, pp. 321?327.
Frey, W. (2004). A medial topic position for German. Linguis-
tische Berichte, 198:153?190.
Gernsbacher, M. A. & D. J. Hargreaves (1988). Accessing sen-
tence participants: The advantage of first mention. Journal
of Memory and Language, 27:699?717.
Halliday, M. A. K. (1985). Introduction to Functional Gram-
mar. London, UK: Arnold.
Harbusch, K., G. Kempen, C. van Breugel & U. Koch (2006).
A generation-oriented workbench for performance grammar:
Capturing linear order variability in German and Dutch. In
Proceedings of the International Workshop on Natural Lan-
guage Generation, Sydney, Australia, 15-16 July 2006, pp.
9?11.
Jacobs, J. (2001). The dimensions of topic-comment. Linguis-
tics, 39(4):641?681.
Keller, F. (2000). Gradience in Grammar: Experimental
and Computational Aspects of Degrees of Grammaticality,
(Ph.D. thesis). University of Edinburgh.
Kempen, G. & K. Harbusch (2004). How flexible is con-
stituent order in the midfield of German subordinate clauses?
A corpus study revealing unexpected rigidity. In Proceed-
ings of the International Conference on Linguistic Evidence,
Tu?bingen, Germany, 29?31 January 2004, pp. 81?85.
Kimball, J. (1973). Seven principles of surface structure parsing
in natural language. Cognition, 2:15?47.
Kohavi, R. & M. Sahami (1996). Error-based and entropy-based
discretization of continuous features. In Proceedings of the
2nd International Conference on Data Mining and Knowl-
edge Discovery, Portland, Oreg., 2?4 August, 1996, pp. 114?
119.
Kruijff, G.-J., I. Kruijff-Korbayova?, J. Bateman & E. Teich
(2001). Linear order as higher-level decision: Information
structure in strategic and tactical generation. In Proceedings
of the 8th European Workshop on Natural Language Gener-
ation, Toulouse, France, 6-7 July 2001, pp. 74?83.
Kruijff-Korbayova?, I., G.-J. Kruijff & J. Bateman (2002). Gen-
eration of appropriate word order. In K. van Deemter &
R. Kibble (Eds.), Information Sharing: Reference and Pre-
supposition in Language Generation and Interpretation, pp.
193?222. Stanford, Cal.: CSLI.
Kurz, D. (2000). A statistical account on word order variation
in German. In A. Abeille?, T. Brants & H. Uszkoreit (Eds.),
Proceedings of the COLING Workshop on Linguistically In-
terpreted Corpora, Luxembourg, 6 August 2000.
Langkilde, I. & K. Knight (1998). Generation that exploits
corpus-based statistical knowledge. In Proceedings of the
17th International Conference on Computational Linguistics
and 36th Annual Meeting of the Association for Computa-
tional Linguistics, Montre?al, Que?bec, Canada, 10?14 August
1998, pp. 704?710.
Lapata, M. (2006). Automatic evaluation of information order-
ing: Kendall?s tau. Computational Linguistics, 32(4):471?
484.
Marsi, E. & E. Krahmer (2005). Explorations in sentence fu-
sion. In Proceedings of the European Workshop on Nat-
ural Language Generation, Aberdeen, Scotland, 8?10 Au-
gust, 2005, pp. 109?117.
Pappert, S., J. Schliesser, D. P. Janssen & T. Pechmann (2007).
Corpus- and psycholinguistic investigations of linguistic
constraints on German word order. In A. Steube (Ed.),
The discourse potential of underspecified structures: Event
structures and information structures. Berlin, New York:
Mouton de Gruyter. In press.
Ringger, E., M. Gamon, R. C. Moore, D. Rojas, M. Smets &
S. Corston-Oliver (2004). Linguistically informed statistical
models of constituent structure for ordering in sentence real-
ization. In Proceedings of the 20th International Conference
on Computational Linguistics, Geneva, Switzerland, 23?27
August 2004, pp. 673?679.
Schmid, H. (1997). Probabilistic Part-of-Speech tagging using
decision trees. In D. Jones & H. Somers (Eds.), New Methods
in Language Processing, pp. 154?164. London, UK: UCL
Press.
Sgall, P., E. Hajic?ova? & J. Panevova? (1986). The Meaning of the
Sentence in Its Semantic and Pragmatic Aspects. Dordrecht,
The Netherlands: D. Reidel.
Speyer, A. (2005). Competing constraints on Vorfeldbesetzung
in German. In Proceedings of the Constraints in Discourse
Workshop, Dortmund, 3?5 July 2005, pp. 79?87.
Telljohann, H., E. W. Hinrichs & S. Ku?bler (2003). Stylebook
for the Tu?bingen treebank of written German (Tu?Ba-D/Z.
Technical Report: Seminar fu?r Sprachwissenschaft, Univer-
sita?t Tu?bingen, Tu?bingen, Germany.
Uchimoto, K., M. Murata, Q. Ma, S. Sekine & H. Isahara
(2000). Word order acquisition from corpora. In Proceedings
of the 18th International Conference on Computational Lin-
guistics, Saarbru?cken, Germany, 31 July ? 4 August 2000,
pp. 871?877.
Uszkoreit, H. (1987). Word Order and Constituent Structure in
German. CSLI Lecture Notes. Stanford: CSLI.
Varges, S. & C. Mellish (2001). Instance-based natural lan-
guage generation. In Proceedings of the 2nd Conference of
the North American Chapter of the Association for Compu-
tational Linguistics, Pittsburgh, Penn., 2?7 June, 2001, pp.
1?8.
Wan, S., R. Dale, M. Dras & C. Paris (2005). Searching for
grammaticality and consistency: Propagating dependencies
in the Viterbi algorithm. In Proceedings of the 10th Euro-
pean Workshop on Natural Language Generation, Aberdeen,
Scotland, 8?10 August, 2005, pp. 211?216.
Weber, A. & K. Mu?ller (2004). Word order variation in Ger-
man main clauses: A corpus analysis. In Proceedings of
the 5th International Workshop on Linguistically Interpreted
Corpora, 29 August, 2004, Geneva, Switzerland, pp. 71?77.
327
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 49?52,
Prague, June 2007. c?2007 Association for Computational Linguistics
An API for Measuring the Relatedness of Words in Wikipedia
Simone Paolo Ponzetto andMichael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We present an API for computing the seman-
tic relatedness of words in Wikipedia.
1 Introduction
The last years have seen a large amount of work in
Natural Language Processing (NLP) using measures
of semantic similarity and relatedness. We believe
that the extensive usage of such measures derives
also from the availability of robust and freely avail-
able software that allows to compute them (Pedersen
et al, 2004, WordNet::Similarity).
In Ponzetto & Strube (2006) and Strube &
Ponzetto (2006) we proposed to take the Wikipedia
categorization system as a semantic network which
served as basis for computing the semantic related-
ness of words. In the following we present the API
we used in our previous work, hoping that it will en-
courage further research in NLP using Wikipedia1.
2 Measures of Semantic Relatedness
Approaches to measuring semantic relatedness that
use lexical resources transform these resources into
a network or graph and compute relatedness using
paths in it (see Budanitsky & Hirst (2006) for an ex-
tensive review). For instance, Rada et al (1989)
traverse MeSH, a term hierarchy for indexing ar-
ticles in Medline, and compute semantic related-
ness straightforwardly in terms of the number of
edges between terms in the hierarchy. Jarmasz &
Szpakowicz (2003) use the same approach with Ro-
get?s Thesauruswhile Hirst & St-Onge (1998) apply
a similar strategy to WordNet.
1The software can be freely downloaded at http://www.
eml-research.de/nlp/download/wikipediasimilarity.php.
3 The Application Programming Interface
The API computes semantic relatedness by:
1. taking a pair of words as input;
2. retrieving the Wikipedia articles they refer to
(via a disambiguation strategy based on the link
structure of the articles);
3. computing paths in the Wikipedia categoriza-
tion graph between the categories the articles are
assigned to;
4. returning as output the set of paths found,
scored according to some measure definition.
The implementation includes path-length (Rada
et al, 1989; Wu & Palmer, 1994; Leacock &
Chodorow, 1998), information-content (Resnik,
1995; Seco et al, 2004) and text-overlap (Lesk,
1986; Banerjee & Pedersen, 2003) measures, as de-
scribed in Strube & Ponzetto (2006).
The API is built on top of several modules and can
be used for tasks other than Wikipedia-based relat-
edness computation. On a basic usage level, it can be
used to retrieve Wikipedia articles by name, option-
ally using disambiguation patterns, as well as to find
a ranked set of articles satisfying a search query (via
integration with the Lucene2 text search engine).
Additionally, it provides functionality for visualiz-
ing the computed paths along the Wikipedia cate-
gorization graph as either Java Swing components
or applets (see Figure 1), based on the JGraph li-
brary3, and methods for computing centrality scores
of the Wikipedia categories using the PageRank al-
gorithm (Brin & Page, 1998). Finally, it currently
2http://lucene.apache.org
3http://www.jgraph.com
49
Figure 1: Shortest path between computer and key-
board in the English Wikipedia.
provides multilingual support for the English, Ger-
man, French and Italian Wikipedias and can be eas-
ily extended to other languages4.
4 Software Architecture
Wikipedia is freely available for download, and can
be accessed using robust Open Source applications,
e.g. the MediaWiki software5, integrated within a
Linux, Apache, MySQL and PHP (LAMP) software
bundle. The architecture of the API consists of the
following modules:
1. RDBMS: at the lowest level, the encyclopedia
content is stored in a relational database manage-
ment system (e.g. MySQL).
2. MediaWiki: a suite of PHP routines for interact-
ing with the RDBMS.
3. WWW-Wikipedia Perl library6: responsible for
4In contrast to WordNet::Similarity, which due to the struc-
tural variations between the respective wordnets was reimple-
mented for German by Gurevych & Niederlich (2005).
5http://www.mediawiki.org
6http://search.cpan.org/dist/WWW-Wikipedia
querying MediaWiki, parsing and structuring the
returned encyclopedia pages.
4. XML-RPC server: an intermediate communica-
tion layer between Java and the Perl routines.
5. Java wrapper library: provides a simple inter-
face to create and access the encyclopedia page
objects and compute the relatedness scores.
The information flow of the API is summarized by
the sequence diagram in Figure 2. The higher in-
put/output layer the user interacts with is provided
by a Java API from which Wikipedia can be queried.
The Java library is responsible for issuing HTTP re-
quests to an XML-RPC daemon which provides a
layer for calling Perl routines from the Java API.
Perl routines take care of the bulk of querying ency-
clopedia entries to the MediaWiki software (which
in turn queries the database) and efficiently parsing
the text responses into structured objects.
5 Using the API
The API provides factory classes for querying
Wikipedia, in order to retrieve encyclopedia entries
as well as relatedness scores for word pairs. In
practice, the Java library provides a simple pro-
grammatic interface. Users can accordingly ac-
cess the library using only a few methods given
in the factory classes, e.g. getPage(word)
for retrieving Wikipedia articles titled word or
getRelatedness(word1,word2), for com-
puting the relatedness between word1 and word2,
and display(path) for displaying a path found
between two Wikipedia articles in the categorization
graph. Examples of programmatic usage of the API
are presented in Figure 3. In addition, the software
distribution includes UNIX shell scripts to access
the API interactively from a terminal, i.e. it does not
require any knowledge of Java.
6 Application scenarios
Semantic relatedness measures have proven use-
ful in many NLP applications such as word sense
disambiguation (Kohomban & Lee, 2005; Patward-
han et al, 2005), information retrieval (Finkelstein
et al, 2002), information extraction pattern induc-
tion (Stevenson & Greenwood, 2005), interpretation
of noun compounds (Kim & Baldwin, 2005), para-
50
:W
e
b
s
r
v
e
r
:
M
e
d
i
a
W
i
k
:
J
a
v
w
r
a
p
e
r
l
i
b
r
a
y
:
W

i
k
p
e
d
i
a
:
X
M
L

R
P
C
d
a
e
m
o
n
:
D
a
t
b
s
e
R
e
s
u
l
t
s
e
t
1
.
R
e
t
r
i
v
e
W
i
k
p
e
d
i
a
c
t
e
g
o
r
y
t
r
e
2
.
C
r
e
a
t
c
a
t
e
g
o
r
y
t
r
e
J
a
v
d
a
t
s
t
r
u
c
t
r
e
3
.
W
i
k
p
e
d
i
a
p
g
e
s
l
o
k
u
p
l
o
p
:
f
o
r
e
a
c
h
w
o
r
d
R
e
s
u
l
t
s
e
t
X
M
L

R
P
C
r
e
s
p
o
n
s
e
P
e
r
l
o
b
j
e
c
t
W
i
k
m
a
r
k
u
p
t
e
x
t
P
H
A
r
t
i
c
l
e
o
b
j
e
c
t
4
.
R
e
l
a
t
d
n
e
s
c
o
r
e
c
o
m
p
u
t
a
i
o
n
:
S
Q
L
q
u
e
r
y
(
c
a
t
e
g
o
r
i
e
s
a
n
d
l
i
k
s
)
:
H
T
P
r
e
q
u
s
t
:
P
e
r
l
m
o
d
u
l
e
c
a
l
:
H
T
P
r
e
q
u
s
t
:
P
H
m
o
d
u
l
e
c
a
l
:
S
Q
L
q
u
e
r
y
(
p
a
g
e
)
:
a
r
t
i
c
l
e
l
o
k
u
p
:
C
r
e
a
t
g
r
a
p
h
f
r
o
m
c
a
t
e
g
o
r
y
t
r
e
q
u
e
r
y
:
C
a
t
e
g
o
r
y
e
x
t
r
a
c
t
i
o
n
a
d
p
a
t
h
s
e
a
r
c
h
Figure 2: API processing sequence diagram. Wikipedia pages and relatedness measures are accessed
through a Java API. The wrapper communicates with a Perl library designed for Wikipedia access and pars-
ing through an XML-RPC server. WWW-Wikipedia in turn accesses the database where the encyclopedia
is stored by means of appropriate queries to MediaWiki.
51
// 1. Get the English Wikipedia page titled "King" using "chess" as disambiguation
WikipediaPage page = WikipediaPageFactory.getInstance().getWikipediaPage("King","chess");
// 2. Get the German Wikipedia page titled "Ufer" using "Kueste" as disambiguation
WikipediaPage page = WikipediaPageFactory.getInstance().getWikipediaPage("Ufer","Kueste",Language.DE);
// 3a. Get the Wikipedia-based path-length relatedness measure between "computer" and "keyboard"
WikiRelatedness relatedness = WikiRelatednessFactory.getInstance().getWikiRelatedness("computer","keyboard");
double shortestPathMeasure = relatedness.getShortestPathMeasure();
// 3b. Display the shortest path
WikiPathDisplayer.getInstance().display(relatedness.getShortestPath());
// 4. Score the importance of the categories in the English Wikipedia using PageRank
WikiCategoryGraph<DefaultScorableGraph<DefaultEdge>> categoryTree =
WikiCategoryGraphFactory.getCategoryGraphForLanguage(Language.EN);
categoryTree.getCategoryGraph().score(new PageRank());
Figure 3: Java API sample usage.
phrase detection (Mihalcea et al, 2006) and spelling
correction (Budanitsky & Hirst, 2006). Our API
provides a flexible tool to include such measures
into existing NLP systems while using Wikipedia
as a knowledge source. Programmatic access to the
encyclopedia makes also available in a straightfor-
ward manner the large amount of structured text in
Wikipedia (e.g. for building a language model), as
well as its rich internal link structure (e.g. the links
between articles provide phrase clusters to be used
for query expansion scenarios).
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.003.2004). We thank our colleagues Katja
Filippova and Christoph Mu?ller for helpful feed-
back.
References
Banerjee, S. & T. Pedersen (2003). Extended gloss overlap as
a measure of semantic relatedness. In Proc. of IJCAI-03, pp.
805?810.
Brin, S. & L. Page (1998). The anatomy of a large-scale hyper-
textual web search engine. Computer Networks and ISDN
Systems, 30(1?7):107?117.
Budanitsky, A. & G. Hirst (2006). Evaluating WordNet-based
measures of semantic distance. Computational Linguistics,
32(1).
Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan,
G. Wolfman & E. Ruppin (2002). Placing search in context:
The concept revisited. ACM Transactions on Information
Systems, 20(1):116?131.
Gurevych, I. & H. Niederlich (2005). Accessing GermaNet data
and computing semantic relatedness. In Comp. Vol. to Proc.
of ACL-05, pp. 5?8.
Hirst, G. & D. St-Onge (1998). Lexical chains as repre-
sentations of context for the detection and correction of
malapropisms. In C. Fellbaum (Ed.), WordNet: An Elec-
tronic Lexical Database, pp. 305?332. Cambridge, Mass.:
MIT Press.
Jarmasz, M. & S. Szpakowicz (2003). Roget?s Thesaurus and
semantic similarity. In Proc. of RANLP-03, pp. 212?219.
Kim, S. N. & T. Baldwin (2005). Automatic interpretation
of noun compounds using WordNet similarity. In Proc. of
IJCNLP-05, pp. 945?956.
Kohomban, U. S. & W. S. Lee (2005). Learning semantic
classes for word sense disambiguation. In Proc. of ACL-05,
pp. 34?41.
Leacock, C. & M. Chodorow (1998). Combining local con-
text and WordNet similarity for word sense identifica-
tion. In C. Fellbaum (Ed.), WordNet. An Electronic Lexical
Database, Chp. 11, pp. 265?283. Cambridge, Mass.: MIT
Press.
Lesk, M. (1986). Automatic sense disambiguation using ma-
chine readable dictionaries: How to tell a pine cone from an
ice cream cone. In Proceedings of the 5th Annual Confer-
ence on Systems Documentation, Toronto, Ontario, Canada,
pp. 24?26.
Mihalcea, R., C. Corley & C. Strapparava (2006). Corpus-based
and knowledge-based measures of text semantic similarity.
In Proc. of AAAI-06, pp. 775?780.
Patwardhan, S., S. Banerjee & T. Pedersen (2005). SenseRe-
late::TargetWord ? A generalized framework for word sense
disambiguation. In Proc. of AAAI-05.
Pedersen, T., S. Patwardhan & J. Michelizzi (2004). Word-
Net::Similarity ? Measuring the relatedness of concepts. In
Comp. Vol. to Proc. of HLT-NAACL-04, pp. 267?270.
Ponzetto, S. P. & M. Strube (2006). Exploiting semantic role
labeling, WordNet andWikipedia for coreference resolution.
In Proc. of HLT-NAACL-06, pp. 192?199.
Rada, R., H. Mili, E. Bicknell & M. Blettner (1989). Devel-
opment and application of a metric to semantic nets. IEEE
Transactions on Systems, Man and Cybernetics, 19(1):17?
30.
Resnik, P. (1995). Using information content to evaluate seman-
tic similarity in a taxonomy. In Proc. of IJCAI-95, Vol. 1, pp.
448?453.
Seco, N., T. Veale & J. Hayes (2004). An intrinsic information
content metric for semantic similarity in WordNet. In Proc.
of ECAI-04, pp. 1089?1090.
Stevenson, M. & M. Greenwood (2005). A semantic approach
to IE pattern induction. In Proc. of ACL-05, pp. 379?386.
Strube, M. & S. P. Ponzetto (2006). WikiRelate! Computing
semantic relatedness using Wikipedia. In Proc. of AAAI-06,
pp. 1419?1424.
Wu, Z. & M. Palmer (1994). Verb semantics and lexical selec-
tion. In Proc. of ACL-94, pp. 133?138.
52
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173?176,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Finding Hedges by Chasing Weasels: Hedge Detection Using
Wikipedia Tags and Shallow Linguistic Features
Viola Ganter and Michael Strube
EML Research gGmbH
Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We investigate the automatic detection of
sentences containing linguistic hedges us-
ing corpus statistics and syntactic pat-
terns. We take Wikipedia as an already
annotated corpus using its tagged weasel
words which mark sentences and phrases
as non-factual. We evaluate the quality of
Wikipedia as training data for hedge detec-
tion, as well as shallow linguistic features.
1 Introduction
While most research in natural language process-
ing is dealing with identifying, extracting and clas-
sifying facts, recent years have seen a surge in re-
search on sentiment and subjectivity (see Pang &
Lee (2008) for an overview). However, even opin-
ions have to be backed up by facts to be effective
as arguments. Distinguishing facts from fiction re-
quires to detect subtle variations in the use of lin-
guistic devices such as linguistic hedges which in-
dicate that speakers do not back up their opinions
with facts (Lakoff, 1973; Hyland, 1998).
Many NLP applications could benefit from
identifying linguistic hedges, e.g. question an-
swering systems (Riloff et al, 2003), information
extraction from biomedical documents (Medlock
& Briscoe, 2007; Szarvas, 2008), and deception
detection (Bachenko et al, 2008).
While NLP research on classifying linguistic
hedges has been restricted to analysing biomedi-
cal documents, the above (incomplete) list of ap-
plications suggests that domain- and language-
independent approaches for hedge detection need
to be developed. We investigate Wikipedia as a
source of training data for hedge classification. We
adopt Wikipedia?s notion of weasel words which
we argue to be closely related to hedges and pri-
vate states. Many Wikipedia articles contain a spe-
cific weasel tag, so that Wikipedia can be viewed
as a readily annotated corpus. Based on this data,
we have built a system to detect sentences that
contain linguistic hedges. We compare a base-
line relying on word frequency measures with one
combining word frequency with shallow linguistic
features.
2 Related Work
Research on hedge detection in NLP has been fo-
cused almost exclusively on the biomedical do-
main. Light et al (2004) present a study on an-
notating hedges in biomedical documents. They
show that the phenomenon can be annotated ten-
tatively reliably by non-domain experts when us-
ing a two-way distinction. They also perform first
experiments on automatic classification.
Medlock & Briscoe (2007) develop a weakly
supervised system for hedge classification in a
very narrow subdomain in the life sciences. They
start with a small set of seed examples known
to indicate hedging. Then they iterate and ac-
quire more training seeds without much manual
intervention (step 2 in their seed generation pro-
cedure indicates that there is some manual inter-
vention). Their best system results in a 0.76 pre-
cision/recall break-even-point (BEP). While Med-
lock & Briscoe use words as features, Szarvas
(2008) extends their work to n-grams. He also ap-
plies his method to (slightly) out of domain data
and observes a considerable drop in performance.
3 Weasel Words
Wikipedia editors are advised to avoid weasel
words, because they ?offer an opinion without re-
ally backing it up, and . . . are really used to ex-
press a non-neutral point of view.?1 Examples
for weasel words as given by the style guide-
1http://en.wikipedia.org/wiki/
Wikipedia:Guide_to_writing_better_
articles
173
lines2 are: ?Some people say . . . ?, ?I think . . . ?,
?Clearly . . . ?, ?. . . is widely regarded as . . . ?,
?It has been said/suggested/noticed . . . ?, ?It may
be that . . . ? We argue that this notion is sim-
ilar to linguistic hedging, which is defined by
Hyland (1998) as ?. . . any linguistic means used
to indicate either a) a lack of complete com-
mitment to the truth value of an accompany-
ing proposition, or b) a desire not to express
that commitment categorically.? The Wikipedia
style guidelines instruct editors to, if they notice
weasel words, insert a {{weasel-inline}} or
a {{weasel-word}} tag (both of which we will
hereafter refer to as weasel tag) to mark sentences
or phrases for improvement, e.g.
(1) Others argue {{weasel-inline}} that
the news media are simply catering
to public demand.
(2) ...therefore America is viewed by
some {{weasel-inline}} technology
planners as falling further behind
Europe ...
4 Data and Annotation
Weasel tags indicate that an article needs to be im-
proved, i.e., they are intended to be removed after
the objectionable sentence has been edited. This
implies that weasel tags are short lived, very sparse
and that ? because weasels may not have been
discovered yet ? not all occurrences of linguistic
hedges are tagged. Therefore we collected not one
but several Wikipedia dumps3 from the years 2006
to 2008. We extracted only those articles that con-
tained the string {{weasel. Out of these articles,
we extracted 168,923 unique sentences containing
437 weasel tags.
We use the dump completed on July 14, 2008
as development test data. Since weasel tags are
very sparse, any measure of precision would have
been overwhelmed by false positives. Thus we
created a balanced test set. We chose one random,
non-tagged sentence per tagged sentence, result-
ing (after removing corrupt data) in a set of 500
sentences. We removed formatting, comments and
links to references from all dumps. As testing data
we use the dump completed on March 6, 2009.
It comprises 70,437 sentences taken from articles
containing the string {{weasel with 328 weasel
2http://en.wikipedia.org/wiki/
Wikipedia:Avoid_weasel_words
3http://download.wikipedia.org/
S M C
K 0.45 0.71 0.6
S 0.78 0.6
M 0.8
Table 1: Pairwise inter-annotator agreement
tags. Again, we created a balanced set of 500 sen-
tences.
As the number of weasel tags is very low con-
sidering the number of sentences in the Wikipedia
dumps, we still expected there to be a much higher
number of potential weasel words which had not
yet been tagged leading to false positives. There-
fore, we also annotated a small sample manu-
ally. One of the authors, two linguists and one
computer scientist annotated 100 sentences each,
50 of which were the same for all annotators to
enable measuring agreement. The annotators la-
beled the data independently and following anno-
tation guidelines which were mainly adopted from
the Wikipedia style guide with only small adjust-
ments to match our pre-processed data. We then
used Cohen?s Kappa (?) to determine the level
of agreement (Carletta, 1996). Table 4 shows the
agreement between each possible pair of annota-
tors. The overall inter-annotator agreement was
? = 0.65, which is similar to what Light et al
(2004) report but worse than Medlock & Briscoe?s
(2007) results. As Gold standard we merged all
four annotations sets. From the 50 overlapping in-
stances, we removed those where less than three
annotators had agreed on one category, resulting
in a set of 246 sentences for evaluation.
5 Method
5.1 Words Preceding Weasel Tags
We investigate the five words occurring right be-
fore each weasel tag in the corpus (but within the
same sentence), assuming that weasel phrases con-
tain at most five words and weasel tags are mostly
inserted behind weasel words or phrases.
Each word within these 5-grams receives an in-
dividual score, based a) on the relative frequency
of this word in weasel contexts and the corpus in
general and b) on the average distance the word
has to a weasel tag, if found in a weasel context.
We assume that a word is an indicator for a weasel
if it occurs close before a weasel tag. The final
scoring function for each word in the training set
174
is thus:
Score(w) = RelF (w) + AvgDist(w) (1)
with
RelF (w) =
W (w)
log
2
(C(w))
(2)
and
AvgDist(w) =
W (w)
?
W (w)
j=0
dist(w,weaseltag
j
)
(3)
W (w) denotes the number of times word w oc-
curred in the context of a weasel tag, whereas
C(w) denotes the total number of times w oc-
curred in the corpus. The basic idea of the RelF
score is to give those words a high score, which oc-
cur frequently in the context of a weasel tag. How-
ever, due to the sparseness of tagged instances,
words that occur with a very high frequency in the
corpus automatically receive a lower score than
low-frequent words. We use the logarithmic func-
tion to diminish this effect.
In equation 3, for each weasel context j,
dist(w,weaseltag
j
) denotes the distance of word
w to the weasel tag in j. A word that always ap-
pears directly before the weasel tag will receive
an AvgDist value of 1, a word that always ap-
pears five words before the weasel tag will receive
an AvgDist value of 1
5
. The score for each word
is stored in a list, based on which we derive the
classifier (words preceding weasel (wpw)): Each
sentence S is classified by
S ? weasel if wpw(S) > ? (4)
where ? is an arbitrary threshold used to control
the precision/recall balance and wpw(S) is the
sum of scores over all words in S, normalized by
the hyperbolic tangent:
wpw(S) = tanh
|S|
?
i=0
Score(w
i
) (5)
with |S| = the number of words in the sentence.
5.2 Adding shallow linguistic features
A great number of the weasel words in Wikipedia
can be divided into three categories:
1. Numerically underspecified subjects (?Some
people?, ?Experts?, ?Many?)
2. Passive constructions (?It is believed?, ?It is
considered?)
3. Adverbs (?Often?, ?Probably?)
We POS-tagged the test data with the TnT tagger
(Brants, 2000) and developed finite state automata
to detect such constellations. We combine these
syntactic patterns with the word-scoring function
from above. If a pattern is found, only the head
of the pattern (i.e., adverbs, main verbs for passive
patterns, nouns and quantifiers for numerically un-
derspecified subjects) is assigned a score. The
scoring function adding syntactic patterns (asp)
for each sentence is:
asp(S) = tanh
heads
S
?
i=0
Score(w
i
) (6)
where heads
S
= the number of pattern heads
found in sentence S.
6 Results and Discussion
Both, the classifier based on words preceding
weasel (wpw) and the one based on added syntac-
tic patterns (asp) perform comparably well on the
development test data. wpw reaches a 0.69 preci-
sion/recall break-even-point (BEP) with a thresh-
old of ? = 0.99, while asp reaches a 0.70 BEP with
a threshold of ? = 0.76.
Applied to the test data these thresholds yield an
F-Score of 0.70 for wpw (prec. = 0.55/rec. = 0.98)
and an F-score of 0.68 (prec. = 0.69/rec. = 0.68)
for asp (Table 2 shows results at a few fixed thresh-
olds allowing for a better comparison). This indi-
cates that the syntactic patterns do not contribute
to the regeneration of weasel tags. Word frequency
and distance to the weasel tag are sufficient.
The decreasing precision of both approaches
when trained on more tagged sentences (i.e., com-
puted with a higher threshold) might be caused by
the great number of unannotated weasel words. In-
deed, an investigation of the sentences scored with
the added syntactic patterns showed that many
high-ranked sentences were weasels which had
not been tagged. A disadvantage of the weasel
tag is its short life span. The weasel tag marks a
phrase that needs to be edited, thus, once a weasel
word has been detected and tagged, it is likely to
get removed soon. The number of tagged sen-
tences is much smaller than the actual number of
weasel words. This leads to a great number of
false positives.
175
? .60 .70 .76 .80 .90 .98
balanced set
wpw .68 .68 .68 .69 .69 .70
asp .67 .68 .68 .68 .61 .59
manual annot.
wpw - .59 - - - .59
asp .68 .69 .69 .69 .70 .65
Table 2: F-scores at different thresholds (bold at
the precision/recall break-even-points determined
on the development data)
The difference between wpw and asp becomes
more distinct when the manually annotated data
form the test set. Here asp outperforms wpw by
a large margin, though this is also due to the fact
that wpw performs rather poorly. asp reaches an
F-score of 0.69 (prec. = 0.61/rec. = 0.78), while
wpw reaches only an F-Score of 0.59 (prec. = 0.42/
rec. = 1). This suggests that the added syntactic
patterns indeed manage to detect weasels that have
not yet been tagged.
When humans annotate the data they not only
take specific words into account but the whole
sentence, and this is why the syntactic patterns
achieve better results when tested on those data.
The word frequency measure derived from the
weasel tags is not sufficient to cover this more in-
telligible notion of hedging. If one is to be re-
stricted to words, it would be better to fall back
to the weakly supervised approaches by Medlock
& Briscoe (2007) and Szarvas (2008). These ap-
proaches could go beyond the original annotation
and learn further hedging indicators. However,
these approaches are, as argued by Szarvas (2008)
quite domain-dependent, while our approach cov-
ers the entire Wikipedia and thus as many domains
as are in Wikipedia.
7 Conclusions
We have described a hedge detection system based
on word frequency measures and syntactic pat-
terns. The main idea is to use Wikipedia as a read-
ily annotated corpus by relying on its weasel tag.
The experiments show that the syntactic patterns
work better when using a broader notion of hedg-
ing tested on manual annotations. When evalu-
ating on Wikipedia weasel tags itself, word fre-
quency and distance to the tag is sufficient.
Our approach takes a much broader domain into
account than previous work. It can also easily be
applied to different languages as the weasel tag ex-
ists in more than 20 different language versions of
Wikipedia. For a narrow domain, we suggest to
start with our approach for deriving a seed set of
hedging indicators and then to use a weakly super-
vised approach.
Though our classifiers were trained on data
from multiple Wikipedia dumps, there were only
a few hundred training instances available. The
transient nature of the weasel tag suggests to
use the Wikipedia edit history for future work,
since the edits faithfully record all occurrences of
weasel tags.
Acknowledgments. This work has been par-
tially funded by the European Union under the
project Judicial Management by Digital Libraries
Semantics (JUMAS FP7-214306) and by the
Klaus Tschira Foundation, Heidelberg, Germany.
References
Bachenko, Joan, Eileen Fitzpatrick & Michael Schonwet-
ter (2008). Verification and implementation of language-
based deception indicators in civil and criminal narratives.
In Proceedings of the 22nd International Conference on
Computational Linguistics, Manchester, U.K., 18?22 Au-
gust 2008, pp. 41?48.
Brants, Thorsten (2000). TnT ? A statistical Part-of-Speech
tagger. In Proceedings of the 6th Conference on Applied
Natural Language Processing, Seattle, Wash., 29 April ?
4 May 2000, pp. 224?231.
Carletta, Jean (1996). Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
Hyland, Ken (1998). Hedging in scientific research articles.
Amsterdam, The Netherlands: John Benjamins.
Lakoff, George (1973). Hedges: A study in meaning criteria
and the logic of fuzzy concepts. Journal of Philosophical
Logic, 2:458?508.
Light, Marc, Xin Ying Qiu & Padmini Srinivasan (2004). The
language of Bioscience: Facts, speculations, and state-
ments in between. In Proceedings of the HLT-NAACL
2004 Workshop: Biolink 2004, Linking Biological Liter-
ature, Ontologies and Databases, Boston, Mass., 6 May
2004, pp. 17?24.
Medlock, Ben & Ted Briscoe (2007). Weakly supervised
learning for hedge classification in scientific literature. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics, Prague, Czech Republic,
23?30 June 2007, pp. 992?999.
Pang, Bo & Lillian Lee (2008). Opinion mining and sen-
timent analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
Riloff, Ellen, Janyce Wiebe & Theresa Wilson (2003). Learn-
ing subjective nouns using extraction pattern bootstrap-
ping. In Proceedings of the 7th Conference on Compu-
tational Natural Language Learning, Edmonton, Alberta,
Canada, 31 May ? 1 June 2003, pp. 25?32.
Szarvas, Gyo?rgy (2008). Hedge classification in biomedical
texts with a weakly supervised selection of keywords. In
Proceedings of the 46th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, Columbus, Ohio, 15?20 June 2008, pp. 281?
289.
176
Annotating Anaphoric and Bridging Relations with MMAX
Christoph Mu?ller and Michael Strube
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
fchristoph.mueller, michael.strubeg@eml.villa-bosch.de
Abstract
We present a tool for the annotation of
anaphoric and bridging relations in a cor-
pus of written texts. Based on differences as
well as similarities between these phenom-
ena, we define an annotation scheme. We
then implement the scheme within an anno-
tation tool and demonstrate its use.
1 Introduction
Anaphoric and bridging relations between discourse
entities are of major importance for establishing and
maintaining textual coherence. Consider the follow-
ing examples, taken from the Heidelberg Text Corpus
(HTC). The HTC is a collection of 577 short texts de-
scriptive of the city of Heidelberg, which have been
collected at our lab for a tourist information system in
the course of the DeepMap project (Malaka & Zipf,
2000).
Im Gegensatz zu anderen Sta?dten steht [das Heidelberger
Stadttheater] nicht an herausgehobener Stelle, sondern [es]
fu?gt sich in die Stra?enflucht ohne Vorplatz ein. [Der
Haupteingang] zeigt noch das alte Arkadenmotiv mit den
flachen Segmentbo?gen. (HTC002)
In contrast to other cities, [the Heidelberg theatre] is not
situated at a particularly exposed position, but [it] blends in
with the street. [The main entrance] still shows the original
motif of the arcades with the flat segments.
In the first sentence, [das Heidelberger Stadtthe-
ater] is introduced into the universe of discourse, and
then referred to anaphorically by means of the pro-
noun [es]. In the next sentence, there is a bridging re-
lation between the entities denoted by [der Hauptein-
gang] and [das Heidelberger Stadttheater]. 1
Note that in each example it is the fact that the sec-
ond discourse entity is either a definite NP or a pro-
noun that triggers the attempt to establish a relation
1Note that in this analysis (which is only one of two that
are possible here), [das Heidelberger Stadttheater] is at the
same time the antecedent to an anaphoric and a bridging ex-
pression.
to some previous entity: Normally, the use of a def-
inite NP respectively a pronoun presupposes that the
entity thus denoted has either already been introduced
into the universe of discourse or is assumed to be fa-
miliar to the reader. This is the case in the anaphori-
cally used pronoun in the first sentence. In the second
sentence, the relation between the two discourse enti-
ties [das Heidelberger Stadttheater] and [der Hauptein-
gang] is less obvious: The second one does not denote
the same entity as the first, but an entity that the second
one is a part of.
The ability to automatically resolve these kinds of
relations is an important feature of text understand-
ing systems. For both the training as well as the
evaluation of these systems, manually annotated cor-
pora are required. The creation of these corpora in
turn has a number of preliminaries. The phenom-
ena anaphor, bridging and the closely related concept
of antecedence need to be sufficiently defined (Sec-
tion 2). On the basis of this definition, an annotation
scheme needs to be developed (Section 3). Finally, an
annotation tool is required which implements the an-
notation scheme in a robust and efficient way. We re-
view a selection of existing tools, then present MMAX
(Multi-Modal Annotation in XML), our versatile Java
tool (Section 4), and we demonstrate how the annota-
tion scheme for anaphoric and bridging relations can
be implemented in MMAX (Section 5).
2 Definition
In general, anaphoric as well as bridging relations hold
between specifying expressions. These are those ex-
pressions that specify (i.e. are used to refer to) a
particular extra-linguistic entity. In what follows, we
briefly discuss the approach of (Vieira & Poesio, 2000)
and present our own definition. Since (Vieira & Poe-
sio, 2000) address the problem of bridging annotation,
they try to find an operational and easily applicable
definition. This is the main motivation for choosing
(Vieira & Poesio, 2000) (and not e.g. (Clark, 1975),
who introduced the term bridging) as the background
of our discussion. In the following discussion, two
features of pairs of specifying expressions will be im-
portant. The first one is cospecification (Sidner, 1983),
also known as coreference, a relation holding between
two or more specifying expressions which specify the
same extra-linguistic entity. The second important fea-
ture is identity of the head noun. This feature is appli-
cable to full NPs only and simply states that in a pair
of NPs the head of each is realized by the same noun.
Anaphor. In (Vieira & Poesio, 2000), only those
relations are classified as anaphoric in which both
cospecification and identity of the head noun is given. 2
Consequently, their rather strict definition contains but
one type, which the authors call direct anaphor. In
contrast to this, we take only cospecification to be a
necessary condition for anaphor. In case that iden-
tity of the head noun is also given, we call this direct
anaphor as well. We believe, however, that additional
sub-types of anaphor should be defined depending on
the type of the anaphoric expression. Along these
lines, we further distinguish pronominal anaphors, and
those in which the object specified by the second ex-
pression is standing in a conceptual IS-A (or hyponym-
hyperonym) relation to the object specified by the first
one. Consider the NP [das Bauwerk] in the follow-
ing example, which denotes a super-concept of [dem
Geba?ude].
Seit 1972 befindet sich das Romanische Seminar in [dem
Geba?ude Seminarstra?e 3]. [Das Bauwerk] wurde 1847
[...] errichtet. (HTC010)
Since 1972, the Romance Seminar is situated in [the build-
ing Seminarstra?e 3]. [The structure] was built [...] in
1847.
Bridging. Due to their strict definition of anaphor,
the term bridging is applied rather widely in (Vieira &
Poesio, 2000). First, those expressions are classified
as bridging which cospecify with their antecedent, but
which do so not by means of an identical, but a differ-
ent head noun. Finally, also non-cospecifying expres-
sions are classified as bridging as long as they stand
in some lexical relation (i.e. hyponymy, meronymy
or co-hyponymy) to their antecedent. The respec-
tive bridging sub-types are introduced by the authors
accordingly. Our approach, in contrast, classifies
as bridging only those expressions which are non-
cospecifying and which stand in some conceptual rela-
tion to their antecedent. At this time, we consider the
following relations to be relevant: cause-effect, part-
whole, and entity-attribute, for which we give the fol-
lowing examples:
Deshalb wurden [verschiedene Untersuchungen] [...]
vorgenommen. [Das Ergebnis] (cause-effect) war die Er-
stellung von Leitlinien fu?r die gestalterische Behandlung des
2It must be added here that (Vieira & Poesio, 2000) con-
sider definite NPs only, and in particular no pronouns.
[Hortus Palatinus]. Danach plante man, [die Zwischen-
terasse] (part-whole) [...] wiederherzustellen. (HTC113)
Therefore, [various examinations] [...] were conducted.
[The result] (cause-effect) was the production of guidelines
for the design of the [Hortus Palatinus]. After that, plans
were made to restore [...] [the middle terrace] (part-whole).
The Concept of Antecedence. In pairs of anaphoric
or bridging expressions, one member is regarded as
the antecedent of the other. In fact, the task of resolv-
ing such a relation is often identified with finding the
antecedent. While there are certainly cases where it is
possible to find exactly one antecedent to a given ex-
pression, there are also cases where this decision is not
obvious. This is true for anaphoric as well as bridging
antecedents: Consider the case of an NP that has been
introduced into the universe of discourse, and that is
referred back to anaphorically twice by means of a
pronoun. We believe that it is not at all clear what is to
be regarded as the antecedent of the second pronoun,
since one could argue for the NP on the grounds of its
semantic explicitness, or for the first pronoun on the
grounds of its being the more recent cospecifying en-
tity. The same is true with bridging expressions. In the
example in Section 1, one could regard the pronoun
[es] as the bridging antecedent of [der Haupteingang]
as well.
Preliminary Conclusion. Our definition of anaphor
and bridging developed so far can be summed up as
follows: Anaphoric relations can be observed between
cospecifying expressions. A pair of antecedent and
anaphor is always cospecifying, while on the other
hand, given a set of (more than two) cospecifying ex-
pressions, determining the antecedent to a given ex-
pression is not necessarily trivial. Anaphors can be
further differentiated according to the nature of the
lexical items taking part in the relation. Bridging re-
lations, in contrast, occur between non-cospecifying
expressions only. Here, the criterion for division into
sub-types is the conceptual relation between the enti-
ties denoted by the expressions taking part in the re-
lation. Finally, it is possible for an expression to be
antecedent to more than one anaphoric and / or bridg-
ing expressions at the same time, while we believe that
the reverse, i.e. one expression having more than one
antecedent, is not possible.
3 Annotation Scheme
The first step in the development of an annotation
scheme is the definition of the relevant markables, i.e.
the class of entities in the text between which the rela-
tions to be annotated can possibly occur. It is in terms
of these markables (with their attributes) and labelled
relations between markables that the annotation itself
is expressed.
In Section 2, we already roughly defined what
counts as a markable by stating that anaphoric and
bridging relations hold between specifying expres-
sions. To further distinguish, we introduce the at-
tribute np form which allows to differentiate between
the following subclasses: Proper noun, definite NP, in-
definite NP, personal pronoun, possessive pronoun and
demonstrative pronoun. In addition, other grammati-
cal features of markables, like agreement or the gram-
matical role they play, might also be of interest. We
capture these in two respective attributes, for which we
specify a closed list of possible values to be assigned
during annotation. These possible values are the com-
bination person/number/gender for the first and sub-
ject, object and other for the second attribute.
In a given pair of expressions it is the way in which
the second expression relates to the first one that de-
termines whether an anaphoric or a bridging relation
exists. It is natural, therefore, to represent this infor-
mation on this second markable and only there. More-
over, this is the only way to allow for the represen-
tation of cases in which one markable is antecedent
to several others. Since we rule out the possibility of
one markable being anaphor or bridging expression to
more than one antecedent, this information is easily
represented by means of an attribute which identifies
the markable as an anaphor or a bridging expression.
We add a further attribute for the respective relation?s
sub-type. For anaphoric expressions, the possible val-
ues for this attribute include direct, pronominal and
IS-A, and for bridging expressions part-whole, cause-
effect and entity-attribute, respectively.
Finally, the annotation of an anaphoric or bridg-
ing markable has to be complemented with informa-
tion on which markable is its antecedent. This can
be accomplished by supplying the markable with a
further attribute. However, selecting the correct an-
tecedent from several candidates can contain a consid-
erable amount of interpretation on the part of the anno-
tator. This is highly undesirable, because it is likely to
force arbitrary decisions which in turn can introduce
error and inconsistency into the annotation. It would
be preferable, therefore, if the explicit identification of
the antecedent would be optional. We do this by sup-
plying in our annotation scheme a means to represent
cospecification. With this additional representation,
the annotation of anaphoric relations in our annotation
scheme is a two-step process: Upon encountering an
anaphoric markable and setting its general attributes,
the markable is first annotated as being cospecifying
with all other markables already in this set of cospeci-
fying expressions. This is the only mandatory annota-
tion, and together with the information that the mark-
able is of the anaphoric type it perfectly well repre-
sents the anaphoric relation. The second, optional step
consists in the specification of the markable?s exact an-
tecedent. By separating the annotation of anaphoric
relations in this way, the concept antecedent becomes
free to be used only in those cases where it is both
relevant and unambiguously decidable. It is impor-
tant to note that no relevant information appears to be
lost here: Supplied that the linear order of markables
within the text is preserved, it should be possible to es-
tablish an antecedent to any anaphoric expression from
a set of cospecifying expressions annotated within the
scheme described above. Moreover, the important
task of evaluating the annotation scheme is not af-
fected either, because common evaluation algorithms
for anaphor annotations (Vilain et al, 1995) do not de-
pend on antecedence information, but treat anaphoric
expressions as cospecifying equivalence classes.
What is even more important is that by the same
means we can render optional the explicit specifica-
tion of bridging antecedents as well. Two cases can be
distinguished here: Whenever only a single candidate
for antecedence exists, specifying it is trivial. Thus,
the only cases where uncertainty as to the correct an-
tecedent of a bridging expression can arise appear to
be those in which multiple cospecifying candidates are
available. Since bridging (as we define it) is a relation
not between lexical items, but between extra-linguistic
entities, and since cospecification is a transitive rela-
tion, a bridging relation can be sufficiently expressed
by specifying any of the candidates. The major dif-
ference to the annotation of anaphoric relations is that
in case of bridging, the selection of an antecedent is
mandatory, but can be made at random, because what
is really selected is not the markable but the extra-
linguistic entity that it specifies.
4 Annotation Tool
This section deals with the question how the annota-
tion scheme developed in the previous section can be
implemented in a real annotation tool. Before present-
ing our own tool MMAX, we briefly review a selection
of already existing tools.
4.1 Existing Tools
The Discourse Tagging Tool (DTTool) (Aone & Ben-
nett, 1994) is a Tcl/Tk program for the annotation
and display of antecedent-anaphor relations in SGML-
encoded multilingual texts. While this field of applica-
tion makes it a potential candidate for the implementa-
tion of our scheme as well, this is not the case, mainly
because the tool lacks the possibility of assigning ar-
bitrary combinations of attributes and possible values
to markables, a feature that obviously is needed for the
representation of different types of relations.
CLinkA3 (Ora?san, 2000) is a more recent Java tool
3http://www.wlv.ac.uk/sles/compling/software/
for coreference annotation. In this case, it is a more
structural constraint which prevents our annotation
scheme from being implemented in this tool. This
constraint results from the fact that CLinkA was built
to implement the annotation scheme proposed in the
MUC-7 Coreference Task Definition (Hirschman &
Chinchor, 1997). In this scheme, cospecification is ex-
pressed in terms of antecedence only, a concept which
we have shown to be problematic, and which our anno-
tation scheme therefore avoids. Another problem with
CLinkA is that it does not seem to support additional
user-defined attributes either.
The Alembic Workbench4 is an annotation tool
which, among other tasks, directly supports cospecifi-
cation annotation. In contrast to DTTool and CLinkA,
it also allows for the extension of the tag set, so that
in principle the handling of different coreference phe-
nomena is possible. The tool (like the other two men-
tioned before) processes SGML files, into which an-
notation tags are inserted directly during annotation.
We regard this approach to annotation as a drawback,
because it mixes the basic data (i.e. the texts to be an-
notated) with the annotation itself. This can give rise
to problems, e.g. in cases where alternative annota-
tions of the same data are to be compared.
Referee, a Tcl/Tk program for coreference annota-
tion (DeCristofaro et al, 1999), is better in this respect
in that it writes the annotations to a separate file, leav-
ing the annotated text itself unaltered. The format of
this annotation file, however, is highly idiosyncratic,
rendering very difficult the subsequent analysis of the
annotation. Moreover, this tool also represents cospec-
ification in terms of antecedence only, making it im-
possible to annotate the former without specifying the
latter. On the other hand, Referee directly supports the
definition of user-definable attributes.
Finally, the MATE Workbench5 is the most ambi-
tious tool that we considered for the implementation
of our annotation scheme. It has been developed in
Java as a highly customizable tool for the XML-based
annotation of arbitrary and possibly non-hierarchical
levels of linguistic description. From a theoretical
point of view, the MATE Workbench would thus be
an ideal platform for the implementation of our anno-
tation scheme. In practical terms, however, we found
the performance of the program to be rather poor, ren-
dering it practically unusable as soon as a certain cor-
pus size was reached.
4.2 MMAX, an XML Annotation Tool
Since we found the existing tools that we considered
to be insufficient for the task of implementing our an-
notation scheme, we decided to develop our own tool.
4http://www.mitre.org/technology/alembic-workbench/
5http://mate.nis.sdu.dk
MMAX6 is written in Java for reasons of platform
independence. It processes XML-encoded text cor-
pora which make use of standoff annotation (Thomp-
son & McKelvie, 1997). Using this technique al-
lows us to keep apart the basic data and the annota-
tion. XML support in Java is realized by means of the
Apache7 implementations of an XML parser and XSL
stylesheet processor.
4.2.1 The Data Model
In MMAX, written texts are represented in XML in
terms of base-level and supra-base level elements. For
each of these element types, Document Type Defini-
tions (DTDs) exist which describe the structure of a
well-formed element. In the following, we give DTD
fragments and discuss their semantics.
A word is the most straightforward base level ele-
ment for a written text. Apart from the representation
of the word itself, each element of this type has an ID
attribute which serves to uniquely identify the word
within the text.
<!ELEMENT words (word*)>
<!ELEMENT word (#PCDATA)>
<!ATTLIST word id ID #REQUIRED>
The sequence of words which as a whole constitutes
the complete text can be divided with respect to two
criteria, a formal and a pragmatic one: Each word is
part of a particular (formally defined) text, which con-
sists of sentences, which in turn may be grouped into
paragraphs. Each sentence has an ID and a span8
attribute which is a pointer to a sequence of word el-
ements. In addition, every text can have an optional
headline, which consists of any number of sentences.
The formal structure of a text is described by the fol-
lowing DTD:
<!ELEMENT text ((headline?),
((paragraph+) | (sentence+)))>
<!ELEMENT headline (sentence*)>
<!ELEMENT paragraph (sentence*)>
<!ATTLIST paragraph id ID #REQUIRED>
<!ELEMENT sentence (EMPTY)>
<!ATTLIST sentence id ID #REQUIRED>
<!ATTLIST sentence span CDATA #REQUIRED>
In pragmatic terms, on the other hand, a text can be
regarded as a discourse, consisting of a series of dis-
course segments. Again, each discourse segment has
an ID and a span attribute, as well as a function at-
tribute indicative of its communicative function. This
pragmatic structure can be translated into a DTD as
follows:
<!ELEMENT discourse (discourse_segment*)>
<!ELEMENT discourse_segment (EMPTY)>
<!ATTLIST discourse_segment id ID #REQUIRED>
<!ATTLIST discourse_segment span CDATA #REQUIRED>
<!ATTLIST discourse_segment function CDATA #IMPLIED>
6http://www.eml.villa-bosch.de/english/research/nlp/
7http://www.apache.org
8We use our own attribute here instead of the href at-
tribute as defined in XPointer, because our element differs
from the latter both in semantics and implementation.
4.2.2 Markables
In MMAX, the XML elements representing mark-
ables possess a set of attributes which is only partly
pre-defined: A closed set of fixed system attributes
is complemented by an open set of user-definable at-
tributes which depend on the annotation scheme that
is to be implemented.
System Attributes. Each markable has an ID at-
tribute which uniquely identifies it. In addition, a span
attribute is needed as well which maps the markable
to one or more word elements. Finally, we intro-
duce a type attribute the meaning of which will be de-
scribed in the next subsection. Two additional system
attributes serve to express the relations between mark-
ables. We argue that two basic relations are sufficient
here.
The first is an unlabelled and unordered relation be-
tween arbitrarily many markables, which can be inter-
preted as set-membership, i.e. markables standing in
this relation to each other are interpreted as constitut-
ing a set. Note that the interpretation of this relation
is not pre-defined and needs to be specified within the
annotation scheme. In order to express a markable?s
membership in a certain set, a member attribute is in-
troduced which has as its value some string specifica-
tion. Set membership can thus be established/checked
by unifying/comparing the member attribute values of
two or more markables.
The second is a labelled and ordered relation be-
tween two markables, which is interpreted as one
markable pointing to the other. Note that here, again,
the nature of this pointing is not pre-defined. However,
there is a structural constraint imposed on the pointing
relation which demands that each markable can point
to at most one other markable. Since there is no con-
straint as to how many different markables can point
to another one, n:1 relations can be represented. A
pointer attribute is required for the expression of the
pointing relation. The range of possible values for this
attribute is the range of existing markables? IDs, with
the exception of the current markable itself.
The DTD fragment for markables and their system
attributes looks as follows:
<!ELEMENT markables (markable*)>
<!ATTLIST markable id ID #REQUIRED>
<!ATTLIST markable span CDATA #REQUIRED>
<!ATTLIST markable type CDATA #REQUIRED>
<!ATTLIST markable member CDATA #IMPLIED>
<!ATTLIST markable pointer IDREF #IMPLIED>
User-definable Attributes. It is by means of its
user-definable attributes that a markable obtains its
semantic interpretation within an annotation scheme.
But even within a single scheme, it may be required
to discriminate between different types of markables.
In MMAX, the type attribute is introduced for this pur-
pose. This attribute does not have any pre-defined pos-
Figure 1: Pair of bridging expression and antecedent
sible values. Instead, a list of these has to be supplied
by the annotation scheme. For each of these values,
in turn, a list of relevant attributes and possible values
has to be defined by the user. Depending on which of
the mutually exclusive type attributes is assigned to a
given markable during annotation, only the attributes
relevant to this type will be offered in a separate at-
tribute window for further specification.
5 Implementation
We utilize the system attribute type to discriminate be-
tween the three basic classes of markables, i.e. nor-
mal9, anaphoric and bridging ones. The respective
attributes and possible values for these mutually ex-
clusive markable types can directly be adopted from
the annotation scheme. Note that a subset of these
is in fact identical for each type (np form, agreement
and grammatical role), while other attributes? possible
values vary with the type of markable: For anaphoric
markables, e.g., the sub-types direct, pronominal and
IS-A are relevant, which make no sense for bridging
expressions, and vice versa. This is directly supported
by the adaptive attribute window. Figure 1 shows the
attribute window in response to the selection of a value
for the type attribute.
Cospecification between two or more markables is
expressed by means of an identical member attribute.
This value, though at this time realised as a string
of the form set XX only, can be interpreted as what
has been called universe entity, elsewhere, e.g. in
the Mate Dialogue Annotation Guidelines10. Adding
9Normal markables are those that are either not part of
any relation or serve as the antecedent only.
10http://www.ims.uni-stuttgart.de/projekte/mdag
Figure 2: Annotation of an anaphor
a markable to a set of cospecifying markables is ac-
complished in two steps: First, the set as a whole is
selected by left-clicking any of its members. As a re-
sult, all members are displayed in a different color, the
selected one in addition being highlighted. The mark-
able to be added is then right-clicked, and the desired
action chosen from a popup menu. Figure 2 shows this
situation. Note that the attribute window has changed
in response to the selection of the value anaphoric
for the type attribute. Specifying the antecedent to
an anaphoric expression is done as follows: First,
the anaphoric markable is selected by left-clicking it.
The desired antecedent is then right-clicked. Finally,
selecting the appropriate menu item from a popup
menu causes the anaphoric markable to point to its an-
tecedent. The antecedent and the anaphoric respec-
tively bridging expression are displayed in a differ-
ent colour whenever the latter is selected. Note that
by combination of the member and pointer attributes,
cospecification and bridging can be represented simul-
taneously, which may be needed in cases of long-
distance anaphor and short-distance bridging.
6 Outlook: Multi-modal Corpora
The definitions of the phenomena anaphor and bridg-
ing presented in this paper as well as the annotation
scheme developed were tailored mainly to the needs of
written texts. This bias is apparent both in the way we
defined markables as well as in the attributes that we
specified for them. In addition, our conception of an-
tecedence was also influenced by it in that the notion
of linear order (as can be observed between words in
a written text) was at least implicit in it. All phenom-
ena, however, are not limited to the domain of writ-
ten text: They also occur in spoken language and di-
alogue. What is even more important: The means by
which they are accomplished there include non-verbal
elements like gazes and in particular pointing gestures.
It is not at all clear yet if and how traditional defini-
tions of phenomena like anaphor or bridging can be
transferred to multi-modal corpora. In particular, phe-
nomena like cross-modal anaphor and bridging need
to be studied in more detail. As a step in this direc-
tion, we have already applied MMAX to the annota-
tion of cospecification in multi-modal corpora (Mu?ller
& Strube, 2001).
Acknowledgements. We thank Lutz Wind for giv-
ing feedback on previous versions of MMAX. We also
thank the two anonymous reviewers for their useful
comments. The work presented here has been partially
funded by the German Ministry of Research and Tech-
nology under grant 01 IL 904 D/2 (EMBASSI) and by
the Klaus Tschira Foundation.
References
Aone, C. & S. W. Bennett (1994). Discourse tagging tool
and discourse-tagged multilingual corpora. In Pro-
ceedings of the International Workshop on Sharable
Natural Language Resources (SNLR), Ikoma, Nara,
Japan, 10?11 August, 1994, pp. 71?77.
Clark, H. H. (1975). Bridging. In Proc. of TINLAP-1, pp.
169?174.
DeCristofaro, J., M. Strube & K. F. McCoy (1999). Build-
ing a tool for annotating reference in discourse. In
ACL ?99 Workshop on the Relationship between Dis-
course/Dialogue Structure and Reference, University
of Maryland, Maryland, 21 June, 1999, pp. 54?62.
Hirschman, L. & N. Chinchor (1997). MUC-7
Coreference Task Definition, http://www.
muc.saic.com/proceedings/.
Malaka, R. & A. Zipf (2000). Deep Map: Challenging
IT research in the framework of a tourist information
system. In Proceedings of the International Confer-
ence on Information and Communication Technologies
in Tourism (ENTER 2000), Barcelona, Spain, 26-28
April, 2000.
Mu?ller, C. & M. Strube (2001). MMAX: A tool for the an-
notation of multi-modal corpora. In Proceedings of
2nd IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, Seattle, Wash., 5 August
2001.
Ora?san, C. (2000). ClinkA a coreferential links annotator. In
Proc. of LREC ?00, pp. 491?496.
Sidner, C. L. (1983). Focusing in the comprehension of defi-
nite anaphora. In M. Brady & R. Berwick (Eds.), Com-
putational Models of Discourse, pp. 267?330. Cam-
bridge, Mass.: MIT Press.
Thompson, H. S. & D. McKelvie (1997). Hyperlink seman-
tics for standoff markup of read-only documents. In
Proceedings of SGML Europe ?97, Barcelona, Spain,
May 1997.
Vieira, R. & M. Poesio (2000). An empirically-based system
for processing definite descriptions. Computational
Linguistics, 26(4):539?593.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly &
L. Hirschman (1995). A model-theoretic coreference
scoring scheme. In Proceedings fo the 6th Message
Understanding Conference (MUC-6), pp. 45?52.
Annotating Semantic Consistency of Speech Recognition Hypotheses 
Iryna Gurevych, Robert Porzel and Michael Strube  
European Media Laboratory GmbH 
Schlo?-Wolfsbrunnenweg 33 
D-69118 Heidelberg, Germany 
e-mail: {gurevych,porzel,strube}@eml.villa-bosch.de 
 
Abstract 
Recent work on natural language processing 
systems is aimed at more conversational, 
context-adaptive systems in multiple do-
mains. An important requirement for such a 
system is the automatic detection of the do-
main and a domain consistency check of the 
given speech recognition hypotheses. We 
report a pilot study addressing these tasks, 
the underlying data collection and investi-
gate the feasibility of annotating the data re-
liably by human annotators.  
1 Introduction 
The complete understanding of naturally oc-
curring discourse is still an unsolved task in 
computational linguistics. Several large re-
search efforts are underway to build multi-
domain and multimodal information systems, 
e.g. the DARPA Communicator Program1, the 
SmartKom research framework2 (Wahlster et 
al., 2001), the AT&T interactive speech and 
multimodal user interface program3.  
Dialogue systems which deal with com-
plex dialogues require the interaction of multi-
ple knowledge sources, e.g. domain, discourse 
and user model (Flycht-Eriksson, 1999). Fur-
thermore NLP systems have to adapt to differ-
ent environments and applications. This can 
only be achieved if the system is able to de-
termine how well a given speech recognition 
hypothesis (SRH) fits within the respective 
domain model and what domain should be 
considered by the system currently in focus. 
The purpose of this paper is to develop 
an annotation scheme for annotating a corpus 
of SRH with information on semantic consis-
tency and domain specificity. We investigate 
                                                     
1http://fofoca.mitre.org 
2http://www.smartkom.com 
3http://www.research.att.com/news/2002/January/IS
MUI.html 
the feasibility of an automatic solution by first 
looking at how reliably human annotators can 
solve the task.   
The structure of the paper is as follows: 
Section 2 gives an overview of the domain 
modeling component in the SmartKom system. 
In Section 3 we report on the data collection 
underlying our study. A description of the 
suggested annotation scheme is given in 
Section 4. Section 5 presents the results of an 
experiment in which the reliability of human 
annotations is investigated.  
2 Domain Modeling in SmartKom 
The SmartKom research project (a consortium 
of twelve academic and industrial partners) 
aims at developing a multi-modal and multi-
domain information system. Domains include 
cinema information, home electronic device 
control, etc. A central goal is the development 
of new computational methods for 
disambiguating different modalities on 
semantic and pragmatic levels. 
 The information flow in SmartKom is 
organized as follows: On the input side the 
parser picks an N-best list of hypotheses out of 
the speech recognizer?s word lattice (Oerder 
and Ney, 1993). This list is sent to the media 
fusion component and then handed over to the 
intention recognition component.  
 The main task of intention recognition 
in SmartKom is to select the best hypothesis 
from the N-best list produced by the parser. 
This is then sent to the dialogue management 
component for computing an appropriate 
action. In order to find the best hypothesis, the 
intention recognition module consults a 
number of other components involved in 
language, discourse and domain analysis and 
requests confidence scores to make an 
appropriate decision (s. Fig. 1).  
 Tasks of the domain modeling 
component are:  
       Philadelphia, July 2002, pp. 46-49.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
? to supply a confidence score on the 
consistency of SRH with respect to the 
domain model;  
? to detect the domain currently in focus. 
 
 
 
 
 
 
 
 
 
 
 
 
         Figure 1. Information flow 
 
These tasks are inherently related to each 
other: It is possible to assign SRH to certain 
domains only if they are consistent with the 
domain model. On the other hand, a 
consistency score can only be useful when it is 
given with respect to certain domains. 
3 Data  
We consider semantic consistency scoring and 
domain detection a classification task. The 
question is whether it is feasible to solve this 
task automatically. As a first step towards an 
answer we reformulate the problem: automatic 
classification of SRH is possible only if 
humans are able to do that reliably. 
3.1 Data Collection 
In order to test the reliability of such 
annotations we collected a corpus of SRH. The 
data collection was conducted by means of a 
hidden operator test (Rapp and Strube, 2002). 
In the test the SmartKom system was 
simulated. We had 29 subjects prompted to say 
certain inputs in 8 dialogues. 1479 turns were 
recorded. Each user-turn in the dialogue 
corresponded to a single intention, e.g. route 
request or sights information request.  
 
3.2 Data Preprocessing 
The data obtained from the hidden operator 
tests had to be prepared for our study to com-
pose a corpus with N-best SRH. For this pur-
pose we sent the audio files to the speech rec-
ognizer. The input for the domain modeling 
component, i.e. N-best lists of SRH were re-
corded in log-files and then processed with a 
couple of Perl scripts. The final corpus con-
sisted of ca. 2300 SRH. This corresponds to ca. 
1.55 speech recognition hypotheses per user?s 
turn.  
The SRH corpus was then transformed 
into a set of annotation files which could be 
read into MMAX, the annotation tool adopted 
for this task (Mueller and Strube, 2001). 
4 Annotation Scheme 
For our study, a markable, i.e. an expression to 
be annotated, is a single SRH. The annotators 
as well as the domain modeling component in 
SmartKom currently do not take the dialogue 
context into account and do not perform 
context-dependent analysis. Hence, we 
presented the markables completely out of 
dialogue order  and thus prevented the 
annotators from interpreting SRH context-
dependently. 
4.1 Semantic Consistency 
In the first step, the annotators had to classify 
markables with respect to semantic consis-
tency. Semantic consistency is defined as well-
formedness of an SRH on an abstract semantic 
level. We differentiate three classes of seman-
tic consistency: consistent, semi-consistent, or 
inconsistent. First, all nouns and verbs con-
tained in the hypothesis are extracted and cor-
responding concepts are retrieved from a 
lemma-concept dictionary (lexicon) supplied 
for the annotators. The decision regarding con-
sistency, semi-consistency and inconsistency 
has to be done on the basis of evaluating the 
set of concepts corresponding to the individual 
hypothesis. 
? Consistent means that all concepts are 
semantically related to each other, e.g. 
"ich moechte die kuerzeste Route"4 is 
mapped to the concepts "self", "wish",  
"route" all of which are related to each 
other. Therefore the hypothesis is con-
sidered consistent.   
? The label semi-consistent is used if at 
least a fragment of the hypothesis is 
                                                     
4 I?d like the shortest route. 
Language 
parsing 
Discourse 
model 
Domain  
model 
Intention 
recognition 
Media Fusion 
Dialogue  
Management 
meaningful. For example, the hypothe-
sis "ich moechte das Video sind"5 is 
considered semi-consistent as the 
fragment "ich moechte das Video", i.e. 
a set of corresponding concepts "self", 
"want", "video" is semantically well-
formed. 
? Inconsistent hypotheses are those 
whose conceptual mappings are not 
semantically related within the domain 
model. E.g. "ich wuerde die Karte ja 
Wiedersehen"6 is conceptualized as 
"self", "map", "parting". This set of 
concepts does not semantically make 
sense and the hypothesis should be re-
jected.  
4.2 Domain Detection 
One of our considerations was that it is princi-
pally not always feasible to detect domains 
from an SRH. This is because the output of 
speech recognition is often corrupt, which 
may, in many cases, lead to false domain as-
signments. We argue that domain detection is 
dependent on the semantic consistency score. 
Therefore, according to our annotation scheme 
no domain analysis should be given to the se-
mantically inconsistent SRH.    
 If the hypothesis is considered either 
consistent or semi-consistent, certain domains 
will be assigned to it. The list of SmartKom 
domains for this study is finite and includes the 
following: route planning, sights information, 
cinema information, electronic program guide, 
home electronic device control, personal assis-
tance, interaction management, small-talk and 
off-talk. 
In some cases multiple domains can be 
assigned to a single markable. The reason is 
that some domains are inherently so close to 
each other, e.g. cinema information and elec-
tronic program guide, that the distinction can 
only be made when the context is taken into 
account. As this is not the case for our study 
we allow for the specification of multiple do-
mains per SRH.  
 
 
 
 
                                                     
5 I?d like the video are. 
6 I would the map yes good-bye. 
5 Reliability of Annotations 
5.1 The Kappa Statistic 
To measure the reliability of annotations we 
used the Kappa statistic (Carletta, 1996).  
The value of Kappa statistic (K) for  se-
mantic consistency in our experiment was 
0.58, which shows that there was not a high 
level of agreement between annotators7. In the 
field of content analysis, where the Kappa 
statistic originated,  K>0.8 is usually taken to 
indicate good reliability, 0.68<K<0.8 allows to 
draw tentative conclusions. 
The distribution of semantic consistency 
classes and domain assignments is given in 
Fig. 2.  
 
Domain % 
Route planning 33,1 
Sights info 13,3 
Cinema info 10,8 
Electr. Program guide 15,9 
Home device control 12,0 
Personal assistance 1,1 
Interaction Management 13,1 
Other 0,7 
                     Figure 2. Distribution of Classes 
5.2 Discussion of the results 
One reason for the relatively low coefficient of 
agreement between annotators could be a small 
number of annotators (two) as compared to 
rather fine distinction between the classes in-
consistent vs. semi-consistent and semi-
consistent vs. consistent respectively. 
Another reason arises from the analysis 
of disagreements among annotators. We find 
many annotation errors caused by the fact that 
the annotators were not able to interpret the 
conceptualized SRH correctly. In spite of the 
fact that we emphasized the necessity of care-
                                                     
7
 Results on the reliability of domain assignments 
are not the subject of the present paper and will be 
published elsewhere. 
 
Type % 
Consistent 51 
Semi-consistent 10,3 
Inconsistent 38,7 
ful examination for high-quality annotations, 
the annotators tended to take functional words 
like prepositions into account. According to 
our annotation scheme, however, they had to 
be ignored during the analysis. 
5.3 Revisions to the annotation scheme 
As already noted, one possible reason for dis-
agreements among annotators is a rather fine 
distinction between the classes inconsistent vs. 
semi-consistent and semi-consistent vs. consis-
tent. We had difficulties in defining strict crite-
ria for separating semi-consistent as a class on 
its own. The percentage of its use is rather low 
as compared to the other two and amounts to 
10.3% on average.   
A possible solution to this problem 
might be to merge the class semi-consistent 
with either consistent or inconsistent. We con-
ducted a corresponding experiment with the 
available annotations.  
In the first case we merged the classes 
inconsistent and semi-consistent.  We then ran 
the Kappa statistic over the data and obtained 
K=0.7. We found this to be a considerable 
improvement as compared to earlier K=0.58.  
In the second case we merged the 
classes consistent and semi-consistent. The 
Kappa statistic with this data amounted to 
0.59, which could not be considered an im-
provement. 
6 Concluding Remarks 
In this work we raised the question whether it 
is possible to reliably annotate speech recogni-
tion hypotheses with information about seman-
tic consistency and domain specificity. The 
motivation for that was to find out whether it is 
feasible to develop and evaluate a computer 
program addressing the same task and imple-
menting the algorithm reflected in the annota-
tion scheme. 
We found that humans principally had 
problems in looking solely at the conceptual-
ized speech recognition hypotheses. This, 
however, should not be a problem for a ma-
chine where the word-to-concept mapping is 
done automatically and all so-called function 
words are discarded. In the future it would be 
interesting to have humans annotate not speech 
recognition hypotheses per se, but only their 
automatically generated conceptual mappings. 
Another finding was that the originally 
proposed annotation scheme does not allow for 
a high level of agreement between human an-
notators with respect to semantic consistency. 
Eliminating the class semi-consistent led us, 
however, to a considerably better reliability of 
annotations.  
We consider this study as a first attempt 
to show the feasibility of determining semantic 
consistency of the output of the speech recog-
nizer. We plan to integrate the results into the 
domain modeling component and conduct 
further experiments on semantic consistency 
and domain detection. 
 
Acknowledgements 
The work presented in this paper was con-
ducted within the SmartKom project partly 
founded by the German Ministry of Research 
and Technology under grant 01IL95I7 and by 
the Klaus Tschira Foundation.  
  
References 
Carletta, J. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational 
Linguistics, 22 (2):249-254.  
Flycht-Eriksson, A. 1999. A Survey of Knowledge 
Sources in Dialogue Systems. In: Proc. of 
IJCAI'99 Workshop on Knowledge and Reason-
ing in Practical Dialogue Systems. Stockholm, 
Sweden, pp. 41-48. 
Mueller, C., Strube, M. 2001. Annotating anaphoric 
and bridging expressions with MMAX. In: 
Proc. of the 2nd SIGdial Workshop on Discourse 
and Dialogue. Aalborg, Denmark, 2001, pp. 90-
95. 
Oerder, M., Ney, H. 1993. Word Graphs: An Effi-
cient Interface between Continuous Speech 
Recognition and Language Understanding. 
In:Proc. of the International Conf. on Acoustics, 
Speech and Signal Processing. IEEE Signal 
Processing Society. 
Rapp, S., Strube, M. 2002. An iterative data collec-
tion approach for multimodal dialogue sys-
tems. In: Proc. of the 3rd International Con-
ference on Language Resources and Evalua-
tion. Las Palmas, Canary Islands, Spain. To 
appear.  
Wahlster, W., Reithinger, N., Blocher, A. 2001. 
SmartKom: Multimodal Communication with a 
Life-Like Character. In: Proc. of Eurospeech 
2001. Aalborg, Danemark, pp. 1547-1550.  
 
The Influence of Minimum Edit Distance on Reference Resolution
Michael Strube
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
strube@eml.villa-bosch.de
Stefan Rapp
Sony International (Europe) GmbH
Advanced Technology Center Stuttgart
Heinrich-Hertz-Stra?e 1
70327 Stuttgart, Germany
rapp@sony.de
Christoph Mu?ller
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
mueller@eml.villa-bosch.de
Abstract
We report on experiments in reference res-
olution using a decision tree approach. We
started with a standard feature set used in
previous work, which led to moderate re-
sults. A closer examination of the perfor-
mance of the features for different forms
of anaphoric expressions showed good re-
sults for pronouns, moderate results for
proper names, and poor results for definite
noun phrases. We then included a cheap,
language and domain independent feature
based on the minimum edit distance be-
tween strings. This feature yielded a sig-
nificant improvement for data sets consist-
ing of definite noun phrases and proper
names, respectively. When applied to
the whole data set the feature produced a
smaller but still significant improvement.
1 Introduction
For the automatic understanding of written or spo-
ken natural language it is crucial to be able to iden-
tify the entities referred to by referring expressions.
The most common and thus most important types
of referring expressions are pronouns and definite
noun phrases (NPs). Supervised machine learning
algorithms have been used for pronoun resolution
(Ge et al, 1998) and for the resolution of definite
NPs (Aone and Bennett, 1995; McCarthy and Lehn-
ert, 1995; Soon et al, 2001). An unsupervised ap-
proach to the resolution of definite NPs was applied
by Cardie and Wagstaff (1999). However, though
machine learning algorithms may deduce to make
best use of a given set of features for a given prob-
lem, it is a linguistic question and a non-trivial task
to identify a set of features which describe the data
sufficiently.
We report on experiments in the resolution of
anaphoric expressions in general, including definite
noun phrases, proper names, and personal, posses-
sive and demonstrative pronouns. Based on the
work mentioned above we started with a feature set
including NP-level and coreference-level features.
Applied to the whole data set these features led
only to moderate results. Since the NP form of the
anaphor (i.e., whether the anaphoric expression is
realized as pronoun, definite NP or proper name) ap-
peared to be the most important feature, we divided
the data set into several subsets based on the NP
form of the anaphor. This led to the insight that the
moderate performance of our system was caused by
the low performance for definite NPs. We adopted
a new feature based on the minimum edit distance
(Wagner and Fischer, 1974) between anaphor and
antecedent, which led to a significant improvement
on definite NPs and proper names. When applied to
the whole data set the feature yielded a smaller but
still significant improvement.
In this paper, we first discuss features that have
been found to be relevant for the task of reference
resolution (Section 2). Then we describe our cor-
pus, the corpus annotation, and the way we prepared
the data for use with a binary machine learning clas-
sifier (Section 3). In Section 4 we first describe the
feature set used initially and the results it produced.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 312-319.
                         Proceedings of the Conference on Empirical Methods in Natural
We then introduce the minimum edit distance fea-
ture and give the results it yielded on different data
sets.
2 Features for Reference Resolution in
Previous Work
Driven by the necessity to provide robust systems
for the MUC system evaluations, researchers began
to look for those features which were particular im-
portant for the task of reference resolution. While
most features for pronoun resolution have been de-
scribed in the literature for decades, researchers only
recently began to look for robust and cheap features,
i.e., features which perform well over several do-
mains and can be annotated (semi-) automatically.
In the following, we describe a few earlier contri-
butions to reference resolution with respect to the
features used.
Decision tree algorithms were used for ref-
erence resolution by Aone and Bennett (1995,
C4.5), McCarthy and Lehnert (1995, C4.5) and
Soon et al (2001, C5.0). This approach requires
the definition of a set of features describing
pairs of anaphors and their antecedents, and col-
lecting a training corpus annotated with them.
Aone and Bennett (1995), working on reference
resolution in Japanese newspaper articles, use
66 features. They do not mention all of these
explicitly but emphasize the features POS-tag,
grammatical role, semantic class and distance.
The set of semantic classes they use appears to be
rather elaborated and highly domain-dependent.
Aone and Bennett (1995) report that their best
classifier achieved an F-measure of about 77% after
training on 250 documents. They mention that
it was important for the training data to contain
transitive positives, i.e., all possible coreference
relations within an anaphoric chain.
McCarthy and Lehnert (1995) describe a refer-
ence resolution component which they evaluated on
the MUC-5 English Joint Venture corpus. They dis-
tinguish between features which focus on individ-
ual noun phrases (e.g. Does noun phrase contain a
name?) and features which focus on the anaphoric
relation (e.g. Do both share a common NP?). It
was criticized (Soon et al, 2001) that the features
used by McCarthy and Lehnert (1995) are highly id-
iosyncratic and applicable only to one particular do-
main. McCarthy and Lehnert (1995) achieved re-
sults of about 86% F-measure (evaluated accord-
ing to Vilain et al (1995)) on the MUC-5 data set.
However, only a defined subset of all possible ref-
erence resolution cases was considered relevant in
the MUC-5 task description, e.g., only entity refer-
ences. For this case, the domain-dependent features
may have been particularly important, making it dif-
ficult to compare the results of this approach to oth-
ers working on less restricted domains.
Soon et al (2001) use twelve features (see Table
1). Soon et al (2001) show a part of their decision
tree in which the weak string identity feature (i.e.
identity after determiners have been removed) ap-
pears to be the most important one. They also report
on the relative contribution of the features where
the three features weak string identity, alias (which
maps named entities in order to resolve dates, per-
son names, acronyms, etc.) and appositive seem to
cover most of the cases (the other nine features con-
tribute only 2.3% F-measure for MUC-6 texts and
1% F-measure for MUC-7 texts). Soon et al (2001)
include all noun phrases returned by their NP iden-
tifier and report an F-measure of 62.6% for MUC-6
data and 60.4% for MUC-7 data. They only used
pairs of anaphors and their closest antecedents as
positive examples in training, but evaluated accord-
ing to Vilain et al (1995).
Cardie and Wagstaff (1999) describe an unsuper-
vised clustering approach to noun phrase corefer-
ence resolution in which features are assigned to
single noun phrases only. They use the features
shown in Table 2, all of which are obtained auto-
matically without any manual tagging. The feature
semantic class used by Cardie and Wagstaff (1999)
seems to be a domain-dependent one which can
only be used for the MUC domain and similar
ones. Cardie and Wagstaff (1999) report a perfor-
mance of 53,6% F-measure (evaluated according to
Vilain et al (1995)).
3 Data
3.1 Text Corpus
Our corpus consists of 242 short German texts (to-
tal 36924 tokens) about sights, historic events and
persons in Heidelberg. The average length is 151 to-
? distance in sentences between anaphor and antecedent
? antecedent is a pronoun?
? anaphor is a pronoun?
? weak string identity between anaphor and antecedent
? anaphor is a definite noun phrase?
? anaphor is a demonstrative pronoun?
? number agreement between anaphor and antecedent
? semantic class agreement between anaphor and antecedent
? gender agreement between anaphor and antecedent
? anaphor and antecedent are both proper names?
? an alias feature (used for proper names and acronyms)
? an appositive feature
Table 1: Features used by Soon et al
? position (NPs are numbered sequentially)
? pronoun type (nom., acc., possessive, ambiguous)
? article (indefinite, definite, none)
? appositive (yes, no)
? number (singular, plural)
? proper name (yes, no)
? semantic class (based on WordNet: time, city, animal,
human, object; based on a separate algorithm: num-
ber, money, company)
? gender (masculine, feminine, either, neuter)
? animacy (anim, inanim)
Table 2: Features used by Cardie and Wagstaff
kens. The texts were POS-tagged using TnT (Brants,
2000). A basic identification of markables (refer-
ring expressions, i.e. NPs) was obtained by using the
NP-Chunker Chunkie (Skut and Brants, 1998). The
POS-tagger was also used for assigning attributes
like e.g. the NP form to markables. The automatic
annotation was followed by a manual correction and
annotation phase in which the markables were anno-
tated with further tags (e.g. semantic class). In this
phase manual coreference annotation was performed
as well. In our annotation coreference is represented
in terms of a member attribute on markables. Mark-
ables with the same value in this attribute are con-
sidered coreferring expressions. The annotation was
performed by two students. The reliability of the an-
notations was checked using the kappa statistic (Car-
letta, 1996).
3.2 Data Generation
The problem of coreference resolution can easily be
formulated as a binary classification: Given a pair
of potential anaphor and potential antecedent, clas-
sify as positive if the antecedent is in fact the closest
antecedent, and as negative otherwise. In anaphoric
chains only the immediately adjacent pairs are clas-
sified as positive. We generated data suitable as in-
put to a machine learning algorithm from our corpus
using a straightforward algorithm which combined
potential anaphors and their potential antecedents.
We then applied the following filters to the resulting
pairs: Discard an antecedent-anaphor pair
  if the anaphor is an indefinite NP,
  if one entity is embedded into the other, e.g. if
the potential anaphor is the head of the poten-
tial antecedent NP (or vice versa),
  if both entities have different values in their se-
mantic class attributes1 ,
  if either entity has a value other than 3rd person
singular or plural in its agreement feature,
  if both entities have different values in their
agreement features2.
For some texts, these heuristics (which were ap-
plied to the entire corpus) reduced to up to 50%
the potential anaphor-antecedent pairs all of which
would have been negative cases. We consider the
cases discarded as irrelevant because they do not
contribute any knowledge for the classifier. After ap-
plication of the filters, the remaining candidate pairs
were labeled as follows:
  Pairs of anaphors and their direct (i.e. clos-
est) antecedents were labeled P. This means
that each anaphoric expression produced ex-
actly one positive instance.
  Pairs of anaphors and those non-antecedents
which occurred closer to the anaphor than the
direct antecedent were labeled N. The number
of negative instances that each expression pro-
duced thus depended on the number of non-
antecedents occurring between the anaphor and
the direct antecedent (or, the beginning of the
text if there was none).
Pairs of anaphors and non-antecedents which oc-
cured further away than the direct antecedent as well
as pairs of anaphors and non-direct (transitive) an-
tecedents were not considered in the data sets. This
produced 242 data sets with a total of 72093 in-
stances of potential antecedent-anaphor pairs.
4 Results
4.1 Our Features
The features for our study were selected according
to three criteria:
1This filter applies only if none of the expressions is a pro-
noun. Otherwise, filtering on semantic class is not possible be-
cause in a real-world setting, information about a pronoun?s se-
mantic class obviously is not available prior to its resolution.
2This filter applies only if the anaphor is a pronoun. This
restriction of the filter is necessary because German allows for
cases where an antecedent is referred back to by a non-pronoun
anaphor which has a different grammatical gender.
  relevance according to previous research,
  low annotation cost and/or high reliability of
automatic tagging,
  domain-independence.
We distinguish between features assigned to noun
phrases and features assigned to the potential coref-
erence relation. All features are listed in Table 3 to-
gether with their respective possible values.
The grammatical function of referring expres-
sions has often been claimed to be an important fac-
tor for reference resolution and was therefore in-
cluded (features 2 and 6). The surface realization
of referring expressions seems to have an influence
on coreference relations as well (features 3 and 7).
Since we use a German corpus and in this language
the gender and the semantic class do not necessar-
ily coincide (i.e., objects are not necessarily neuter
as they are in English) we also provide a semantic
class feature (5 and 9) which captures the difference
between human, concrete objects, and abstract ob-
jects. This basically corresponds to the gender at-
tribute in English, for which we introduced an agree-
ment feature (4 and 8). The feature wdist (10) cap-
tures the distance in words between anaphor and an-
tecedent, while the feature ddist (11) does the same
in terms of sentences and mdist (12) in terms of
markables. The equivalence in grammatical func-
tion between anaphor and potential antecedent is
captured in the feature syn par (13), which is true if
both anaphor and antecedent are subjects or both are
objects, and false in the other cases. The string ident
feature (14) appears to be of major importance since
it provides for high precision in reference resolution
(it almost never fails) while the substring match fea-
ture (15) could potentially provide better recall.
4.2 Baseline Results
Using the features of Table 3, we trained decision
tree classifiers using C5.0, with standard settings for
pre and post pruning. As several features are dis-
crete, we allowed the algorithm to use subsets of
feature values in questions such as ?Is ana npform in

PPER, PPOS, PDS  ??. We also let C5.0 construct
rules from the decision trees, as we found them to
give superior results. In our experiments, the value
Document level features
1. doc id document number (1 . . . 250)
NP-level features
2. ante gram func grammatical function of antecedent (subject, object, other)
3. ante npform form of antecedent (definite NP, indefinite NP, personal pronoun,
demonstrative pronoun, possessive pronoun, proper name)
4. ante agree agreement in person, gender, number
5. ante semanticclass semantic class of antecedent (human, concrete object, abstract object)
6. ana gram func grammatical function of anaphor (subject, object, other)
7. ana npform form of anaphor (definite NP, indefinite NP, personal pronoun,
demonstrative pronoun, possessive pronoun, proper name)
8. ana agree agreement in person, gender, number
9. ana semanticclass semantic class of anaphor (human, concrete object, abstract object)
Coreference-level features
10. wdist distance between anaphor and antecedent in words (1 . . . n)
11. ddist distance between anaphor and antecedent in sentences (0, 1,  1)
12. mdist distance between anaphor and antecedent in markables (1 . . . n)
13. syn par anaphor and antecedent have the same grammatical function (yes, no)
14. string ident anaphor and antecedent consist of identical strings (yes, no)
15. substring match one string contains the other (yes, no)
Table 3: Our Features
of the ana semanticclass attribute was reset to miss-
ing for pronominal anaphors, because in a realistic
setting the semantic class of a pronoun obviously is
not available prior to its resolution.
Using 10-fold cross validation (with about 25
documents for each of the 10 bins), we achieved an
overall error rate of 1.74%. Always guessing the by
far more frequent negative class would give an er-
ror rate of 2.88% (70019 out of 72093 cases). The
precision for finding positive cases is 88.60%, the
recall is 45.32%. The equally weighted F-measure3
is 59.97%.
Since we were not satisfied with this result we
examined the performance of the features. Surpris-
ingly, against our linguistic intuition the ana npform
feature appeared to be the most important one. Thus,
we expected considerable differences in the perfor-
mance of our classifier with respect to the NP form
of the anaphor under consideration. We split the data
into subsets defined by the NP form of the anaphor
and trained the classifier on these data sets. The re-
sults confirmed that the classifier performed poorly
on definite NPs (defNP) and demonstrative pronouns
3computed as 
		
(PDS), moderately on proper names (NE) and quite
good on personal pronouns (PPER) and possessive
pronouns (PPOS) (the results are reported in Ta-
ble 4). As definite NPs account for 792 out of
2074 (38.19%) of the positive cases (and for 48125
(66.75%) of all cases), it is evident that the weak
performance for the resolution of definite NPs, es-
pecially the low recall of only 8.71% clearly impairs
the overall results. Demonstrative pronouns appear
only in 0.87% of the positive cases, so the inferior
performance is not that important. Proper names
(NE) however are more problematic, as they have to
be considered in 644 or 31.05% of the positive cases
(22.96% of all).
P R F
defNP 87.34% 8.71% 15.84%
NE 90.83% 50.78% 65.14%
PDS 25.00% 11.11% 15.38%
PPER 88.12% 78.07% 82.79%
PPOS 82.69% 87.31% 84.94%
all 88.60% 45.32% 59.97%
Table 4: Baseline results using features 2?15.
Antecedent Anaphor
?Philips? ?Kurfu?rst Philip?
?vier Schu?lern? ?die Schu?ler?
?die alte Universita?t? ?der alten Universita?t?
?im Studentenkarzer in der Augustinergasse? ?des Studentenkarzers?
?diese hervorragende Bibliothek? ?dieser Bibliothek?
Table 5: Anaphors and their direct antecedents
New coreference-level features
16. ante med minimum edit distance to anaphor
ffMulti-Level Annotation in MMAX
Christoph Mu?ller and Michael Strube
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
 
Christoph.Mueller, Michael.Strube  @eml.villa-bosch.de
Abstract
We present a light-weight tool for the an-
notation of linguistic data on multiple lev-
els. It is based on the simplification of an-
notations to sets of markables having at-
tributes and standing in certain relations
to each other. We describe the main fea-
tures of the tool, emphasizing its simplic-
ity, customizability and versatility.
1 Introduction
In recent years, the development and use of anno-
tation tools has been a recurrent topic in corpus-
based computational linguistics. Currently, special-
ized tools for the annotation of a wide range of phe-
nomena on different levels of linguistic description
are available. In the more recent of these tools,
principles of design and implementation are real-
ized which over the years have emerged as quasi-
standards:
 XML as data storage format,
 file-level separation of base data (i.e. the data
to be annotated) from the annotation, use of
stand-off annotation (Ide and Priest-Dorman,
1996),
 implementation in Java for the sake of platform
independence.
Most of the available tools handle well the phenom-
ena on the linguistic level they are intended for, be
it coreference, dialogue acts, or discourse structure,
to name just a few. The annotations they yield do
exist independently of each other and cannot easily
be combined or applied to the same language data.
This, however, would be highly desirable because it
would allow for simultaneous browsing and anno-
tating on several linguistic levels. In addition, anno-
tation tasks could be distributed to several research
groups with different expertise, with one group spe-
cializing in e.g. dialogue act tagging, another in
coreference annotation, and so on. After completion
of the individual annotation tasks, the annotations
could be combined into one multi-level annotation
that a single group could not have produced.
The MMAX1 tool presented in this paper is in-
tended as a light-weight and highly customizable
implementation of multi-level annotation of (poten-
tially multi-modal) corpora. It is based on the as-
sumption that any annotation can be simplified to
sets of so-called markables carrying attributes and
standing in certain relations to each other. Conse-
quently, all a tool has to supply is efficient low-level
support for the creation and maintenance of mark-
ables on different levels.
The remainder of this paper is structured as fol-
lows: In Section 2 we describe in more detail the
basic concepts underlying our approach. Section 3
describes how annotation (or coding) schemes can
be defined by the user and how they are enforced
by the tool during the annotation process. Section
4 deals with how our approach extends naturally to
cover multiple linguistic levels simultaneously. Sec-
tion 5 gives a detailed description of both the tool
1MultiModal Annotation in XML. The current release ver-
sion of the tool can be downloaded at http://www.eml.org/nlp.
itself and its Discourse API which offers high-level
Java access to annotated corpora in MMAX format.
In Section 6 we briefly discuss some related work.
2 Concepts
Linguistic annotation is the process and result of
(manually) adding new information to existing lan-
guage data. This existing data can consist of writ-
ten texts (e.g. a newspaper corpus), but also of spo-
ken language (which may even be multi-modal, i.e.
contain e.g. pointing gestures). Before it can be
annotated, this data must be converted into some
machine-readable format. In addition, some rudi-
mentary structure has to be imposed on it. What is
important for both of these preprocessing steps is
that they should not alter the original data in any
way. In particular, they should not introduce ar-
bitrary decisions or implicit assumptions. Instead,
a format should be created that is as simple and
theory-neutral as possible. In our approach, written
text is simply modelled as a sequence of sentence
elements, each of which spans a number of word
elements. For spoken language (or dialogues), se-
quences of turn elements are used, each of which
spans sequences of word elements2. Since the tok-
enization into words and the segmentation into sen-
tences or turns can be performed on a mere formal
(i.e. surface-based) level, we believe these elements
to be sufficiently objective to serve as the structure
for what we call annotation base data. This is in
contrast to e.g. utterance segmentation, which has
been shown to require a considerable amount of in-
terpretation from human subjects. Therefore, we do
not support utterance elements on the level of the
annotation base data, but regard utterance segmen-
tation as one possible level of annotation.
As for the XML implementation of the annotation
base data, we simply model sentence and turn ele-
ments as XML elements with the respective name
and with two obligatory attributes: The ID attribute
assigns a unique label to each element, and the span
attribute contains a (condensed) list of IDs of those
base data elements that the sentence or turn contains.
<sentence id="sentence_1"
span="word_1..word_8"/>
2For multi-modal dialogue, turns can contain gesture ele-
ments in addition to word elements.
The   turn  element may have an additional
speaker and number attribute.
<turn id="turn_1" span="word_1..word_7"
speaker="A" number="1"/>
Each word element in the base data is modelled as
a   word  XML element with an ID attribute as the
only obligatory one. The word itself is represented
as a text child of the   word  element. If the orig-
inal language data was spoken language, this is the
transcription of the originally spoken word. In this
case, the   word  element may also have an ad-
ditional starttime and endtime attribute relating the
word to a time line.
<word id="word_1" starttime="0.000"
endtime="0.7567">
This
</word>
All elements comprising a MMAX document are
stored in a sentences or turns file and a words file
(and an additional gestures file for multimodal dia-
logue). These files define the annotation base data
and are not supposed to be modifiable through the
annotation tool.
2.1 Markables
Markables are the sole carriers of annotation infor-
mation. The concept of markable is defined in for-
mal terms only, i.e. without any implicit semantics.
A markable is simply an abstract entity which ag-
gregates an arbitrary set of elements from the base
data. It does so by means of a list of IDs of word
elements (and/or gesture elements), which are inter-
preted as pointers to the respective elements. Obvi-
ously, the question which sequences of elements are
to be represented as markables depends on the lin-
guistic level or phenomenon one is interested in: In
the case of coreference annotation, markables would
identify referring expressions in the base data, be-
cause it is on this level that information has to be
added. If the task is dialogue act tagging, markables
would be used to represent utterances.
Markables are modelled as   markable  XML
elements which are similar to   sentence  and
  turn  elements in that they consist (in their most
basic form) mainly of an ID and a span attribute.
The latter attribute, however, can be more complex
since it can reference discontinuous (or fragmented)
sequences of base data elements.
<markable id="markable_1"
span="word_1..word_5,word_7" ... />
The placeholder dots in the example above are to in-
dicate that a markable can indeed have many more
attributes. These are described in sections 2.2 and
2.3. Markables pertaining to the same linguistic
level are stored together in a markables XML file.
In its header, this file contains a reference to an an-
notation scheme XML file (cf. Section 3).
2.2 Attributes
In order to really add information to the base data,
it is not sufficient for a markable to identify the set
of elements it aggregates. It also has to associate
some attributes with them. In our approach, mark-
ables can have arbitrarily many attributes in the form
of name-value pairs. At this time, two types of at-
tributes are supported: Nominal attributes can take
one of a closed set of values, freetext attributes can
take any string (or numerical) value. The attribute
names, types and possible values to be defined de-
pend on the nature of the markables for which they
are intended: In dialogue act tagging, markables
represent utterances, thus a nominal attribute dia-
logue act with possible values like initiation, re-
sponse, and preparation etc. would be relevant.
On the XML level, attributes are expressed in the
standard name="value" format on markable ele-
ments in the markables file. Note, however, that both
the type of the attributes and their possible values
(for nominal attributes) cannot be determined from
the markables file alone, but only with reference to
the annotation scheme (cf. Section 3) linked to it.
2.3 Relations
While markables and their attributes are sufficient
to add information to independent sequences of
base data elements, they cannot relate these to
each other for the expression of structural infor-
mation. Therefore, our approach is complemented
by a means to express relations between mark-
ables. Currently, attributes of type member-relation
and pointer-relation are supported. Attributes of
type member-relation express undirected relations
between arbitrary many markables. This relation
can be interpreted as set-membership, i.e. mark-
ables having the same value in an attribute of type
member-relation constitute an unordered set. At-
tributes of type pointer-relation, on the other hand,
express directed relations between single source
markables and arbitrarily many target markables. As
the name suggests, this relation can be interpreted
as the source markable pointing to its target mark-
able(s). It is important to note that member-relation
and pointer-relation are not attributes themselves.
Rather, they are types of attributes (like nominal and
freetext) which can be realized by attributes of arbi-
trary names. That means that for one markable, sev-
eral different attributes of type member- and pointer-
relation can be defined within the same annota-
tion scheme. The attribute type simply defines how
these attributes are interpreted. Like the concept
of markable itself, relations are also defined only
formally, i.e. without any semantic interpretation.
Like markables, relations can be associated with any
kind of semantic interpretation, depending on the
annotation task at hand: For coreference annota-
tion, it would be natural to use a member-relation
attribute coref class to model classes of coreferring
expressions. In addition, a (binary) pointer-relation
attribute antecedent could be employed to anno-
tate the direct antecedent of a coreferring expres-
sion. As another example, if the task is annotating
the predicate-argument structure of verbs, (binary)
pointer-relation attributes like subject, direct object
and indirect object could be used to link a verb to its
arguments.
On the XML level, relations are expressed like
normal attributes, with the only difference that their
values are (lists of) markable element IDs (pointer-
relation) or strings of the form set x (member-
relation).
<markable id="markable_2"
span="word_14..16"
coref_class="set_4"
antecedent="markable_1" ... />
3 Annotation Schemes
Even on the same linguistic level, not every at-
tribute or relation is applicable all the time or to
every kind of markable. In coreference annotation,
e.g., a markable that has been explicitly annotated
as discourse-initial should not be allowed to have
an antecedent attribute. Along the same lines, in
predicate-argument structure annotation, a so-called
weather-verb like ?rain? should not be allowed to
have a pointer to its subject. Restricting the avail-
ability of attributes to only those that make sense in
a particular situation is an important means to ensure
annotation quality and consistency.
Dependencies of this kind can best be captured
by formulating constraints on which attributes can
occur together or which are mutually exclusive. In
our approach, constraints of various types can be
modelled in the annotation scheme. Generally, an-
notation (or coding) schemes are of central im-
portance to any annotation task. They describe
which phenomena are to be annotated using which
set of attributes. Within the MMAX tool, annota-
tion scheme development has been of special im-
portance, because the expressiveness and the de-
gree of customizability of the annotation scheme
strongly determine how versatile and generally ap-
plicable the tool is. The mechanism for defin-
ing and handling annotation schemes described in
what follows has been developed in collaboration
with the Brazilian-French project COMMOn-REFs
(Unisinos, Sa?o Lepoldo-RS, Brazil; LORIA/INRIA,
Nancy, France) (Salmon-Alt and Vieira, 2002).
An annotation scheme defines all attributes (nom-
inal, freetext, member-relation and pointer-relation)
valid for a linguistic level. It specifies possible val-
ues for nominal attributes, and it identifies default
attribute values. Attributes can be either branch-
ing or non-branching: If an attribute is branching,
its current value influences which other attributes
are available. In a branching nominal attribute, at
least one of the possible values is associated with
a reference to one or more following attributes. In
a branching freetext, member-relation or pointer-
relation attribute, on the other hand, at most two
references to following attributes are possible, de-
pending on whether the attribute does or does not
have a value. Consider the following example (see
Figure 1 for an illustration): In recent work dealing
with pronoun resolution in spoken dialogue (Strube
and Mu?ller, 2003), different types of expressions
(noun phrases, verb phrases, whole utterances and
disfluencies) had to be annotated. They were distin-
guished by setting for each expression the appropri-
ate value in a nominal attribute called Expressions-
Type. Since noun phrases have different attributes
than e.g. verb phrases, the attribute ExpressionsType
was a branching one because each of its possible val-
ues referenced a partially3 different set of following
attributes: For noun phrases, the whole range of lin-
guistic features like case, grammatical role, seman-
tic role, etc. is relevant, while e.g. verb phrases and
utterances (for our purposes) needed only be distin-
guished according to their type (attributes VP Type
resp. Utt. Type). For unclassified expressions (none)
and disfluencies, on the other hand, no further at-
tributes were defined at all.
Comment
Exp. Type
none
vp uttnp
disfluency
Member Member Member
Pointer
Case
Gram. Role
Sem. Role
Pointer Pointer
VP Type Utt. Type
Figure 1: Annotation Scheme Diagram (Fragment)
Attributes that are referenced by branching at-
tributes (e.g. Member, Pointer, Case, VP Type) are
dependent in that they are only valid and accessible
if the corresponding value is selected on the branch-
ing attribute (i.e. ExpressionsType). Thus, the avail-
ability of attributes can effectively be constrained.
Since an attribute that is dependent on some other at-
tribute can itself be branching, arbitrarily many lev-
els of dependency are possible.
An annotation scheme of the above form can also
be described as an annotation tree, where each node
in the tree consists of a number of non-branching
and (optionally) one branching attribute. If a node
does have a branching attribute, the dependent at-
tributes it references can be seen as child nodes. If
a node does not have a branching attribute, it corre-
sponds to a leaf in the annotation tree.
3The Member and Pointer attribute applies to noun phrases,
verb phrases and utterances.
4 Levels
In Section 2, the linguistic levels of coreference, di-
alogue acts and predicate-argument structure were
used for illustrative purposes. It was demonstrated
how these different linguistic phenomena can be rep-
resented by means of a few simple concepts. The
following section deals with how the same concepts
lend themselves to the simultaneous representation
of multiple levels of linguistic description.
Among others, the following levels of linguistic
description could be envisaged:
 morpho-syntax,
 syntax,
 valency/predicate-argument structure,
 coreference,
 dialogue acts,
 prosody/intonation,

. . .
Relating e.g. the utterance level to the coreference
level could be done, for instance, in order to find out
whether utterance boundaries in spoken dialogues
can be used to narrow down the search space for an-
tecedents of discourse-deictic anaphors. Along sim-
ilar lines, the prosody or intonation level could pro-
vide relevant information as well.
Though it would be tempting to merge markable
files from different levels, this would have some se-
rious disadvantages. First of all, subsequent modifi-
cation or removal of a level would be cumbersome.
Moreover, alternative versions of the same level (e.g.
utterance segmentations performed by different an-
notators) cannot easily be compared without having
to duplicate the other levels. For these reasons, our
approach favours the separation of the different de-
scription levels to different markables files.
Since markables (as we define them) are not di-
rectly embedded into the base data, but reference
base data elements by means of their span attribute,
the simultaneous application of several description
levels is straightforward: Given some annotation
base data set, sets of markables pertaining to dif-
ferent description levels can simply be applied to it,
i.e. be allowed to access the base data elements they
reference, thus adding level by level of annotation.
Since markables on different levels are related only
indirectly by virtue of shared base data elements, is-
sues like overlap or discontinuous elements do not
arise. This is made possible through what can be
seen as a rigorous implementation of the principle of
stand-off annotation (Ide and Priest-Dorman, 1996;
Thompson and McKelvie, 1997).
5 MMAX
5.1 The Annotation Tool
The MMAX annotation tool is written in Java. XML
and XSL functionality is supplied by the Apache4
Xerces and Xalan engines. The Java executable of
the tool itself is very small (ca. 300 kb). Installing
the tool (under Windows or Linux) is done by simply
extracting a directory structure to the local hard disk;
no further installation is required. Figure 2 shows a
typical annotation situation with the most important
GUI elements being visible, i.e. (clockwise, begin-
ning in the upper left corner): the main annotation
window, the SearchWindow, and the AttributeWin-
dow. In the SearchWindow, a query for 3rd per-
son neuter personal and possessive pronouns with
oblique case is specified. Attributes can be queried
by either selecting the desired value from a list, or by
specifying a regular expression. The AttributeWin-
dow shows the annotation scheme described in Fig-
ure 1.
Up to now, MMAX has been used for the creation
of several annotated corpora, e.g. uni-modal text-
only corpora (Salmon-Alt and Vieira, 2002; Mu?ller
et al, 2002; Strube and Mu?ller, 2003) and multi-
modal human-machine corpora (Mu?ller and Strube,
2001; Rapp and Strube, 2002).
In order to minimize the tool?s system require-
ments and maximize its performance, we deliber-
ately chose to use a text-only display (as opposed to
an HTML display). This imposes a couple of restric-
tions with respect to user interaction. We distinguish
between the display of content-bearing vs. merely
layout information:
Content-bearing information is conveyed by
markables and their properties. Within MMAX, it
4http://www.apache.org
Figure 2: MMAX Screenshot
is visualized by means of text foreground and back-
ground colours and graphical arrows (for relations
between markables). User actions like selecting,
adding or deleting a markable, adding a relation be-
tween two markables, or modifying a markable?s at-
tributes change the content-bearing information and
thus require frequent display updates. The MMAX
display offers hard-coded (and highly optimized)
methods for manipulating text colour and for draw-
ing lines between markables. Thus we achieve very
good performance (i.e. low response time) for these
by far most frequent types of user interactions.
Layout information, on the other hand, contains
formal properties of the display only. It includes
mainly line breaks and indentations, but also font
style properties like bold, italic, and underlined.
Within MMAX, the XSL style sheet supplied in
the .MMAX project file is responsible for rendering
the display layout. By modifying this style sheet,
the user can customize the display, e.g. by insert-
ing pseudo-HTML tags like   bold  ...   /bold 
or   italic  ...   /italic  . During a MMAX
session, changes to the layout can only be made
by explicitly reinvoking the style sheet processor,
which, depending on the data and style sheet com-
plexity, can take several seconds. In contrast to
content-bearing information, however, layout infor-
mation is not expected to require frequent updates.
Utterance segmentation is one example of how the
display layout might change as a result of markables
being added to the annotation, i.e. if the user wishes
to have line breaks inserted directly after markables
respresenting utterances. This, however, can be per-
formed reasonably well if the user does not rebuild
the display after each single markable creation, but
only after each, say, five or ten.
A MMAX session is started by opening a
.MMAX project file. This file contains references
to all files comprising a MMAX document:
 one sentences or turns XML file,
 a words XML file (and/or a gestures file),
 a list of markables XML files,
 an XSL style sheet file for rendering the layout
of the MMAX display,
 an XML file specifying colour attributes for
rendering the appearance of markables depend-
ing on their content.
When a .MMAX project file is opened, the tool
first builds an XML DOM tree representation of the
information supplied in the base data files. For the
whole session, this tree serves as the read-only ?scaf-
fold? to which annotations (given in the form of one
or more markables files) are applied. Then, depend-
ing on which annotation levels the user chose to
view, information about markables from these lev-
els is added to the DOM tree as well. The DOM tree
is then submitted to the XSL style sheet for transfor-
mation into a single string, which is then converted
in a Java object of type StyledDocument. In the last
step, markables in the StyledDocument are coloured
according to their initial attributes, and the Styled-
Document is finally presented to the user by assign-
ing it to the MMAX display.
Users can explicitly activate markables on differ-
ent annotation levels. Only if a level is active, mark-
ables on this level are displayed and can be accessed
or modified. Users can select an active markable by
left-clicking it. If the click is ambiguous, a popup
menu is displayed containing all active markables in
the clicked position. In this menu, markables are
tagged with their respective level, so that users can
easily select markables from a particular level (with-
out having to temporarily deactivate all other levels).
Once a markable is selected, its attributes are
displayed in a separate AttributeWindow. In addi-
tion, if it has a non-empty value for some member-
relation or pointer-relation attribute, those are visu-
alized by means of arrows drawn on the MMAX dis-
play. The AttributeWindow has access to the anno-
tation scheme defined for the markable it currently
displays. This enables the AttributeWindow to per-
form a consistency check on each markable by try-
ing to ?reproduce? the annotation process that lead to
this markable having this set of attributes.5 It does
so by traversing the annotation tree, beginning at the
root, and recursively trying to match the attributes
of the markable to the attributes defined at the cur-
rent annotation tree node. If an attribute could be
matched, it is consumed, and the AttributeWindow
5Thanks to Caroline Varaschin Gasperin (Unisinos, Sa?o
Lepoldo-RS, Brazil) for providing some initial ideas on this.
is changed so that dependent attributes are accessi-
ble. If the matching process terminates before all
attributes have been consumed, an annotation error
or inconsistency has been detected. The same is true
if an undefined attribute value is found on the mark-
able. In both cases, a warning message is displayed
to the user. Within MMAX, the AttributeWindow is
the central location where the annotation scheme is
enforced. Figure 3 gives an idea of the internal rela-
tions between different MMAX components. Bold
boxes represent GUI elements.
Markable
Annotation Scheme
annotation tree
Valid path through
n 1
has
1
n
containsn
n
sets properties for
1
1
specifies
n
1
1
1
finds
displays
displays
11
specifies 1
n
complies with
1
1
1
defines
contains
1
Valid A?V pairs
Attribute Window
Search Window
Level
1
n
1
1
displays
Document
MMAX tool
Figure 3: MMAX Components
Creating a new markable works straightforwardly
by selecting a sequence of text on the display and
right-clicking it. If only one annotation level is ac-
tive, a pop-up menu with only one option, i.e. creat-
ing a markable on this level, will apear. Otherwise,
the menu will contain options for creating a mark-
able on any one currently active level.
When a newly created markable is selected, it
does not have any attributes except for those de-
fined on the root node of the annotation tree. The
AttributeWindow utilizes the order of the annotation
tree nodes to guide the user through the annotation
process, allowing only those attributes to be dis-
played that are valid in the current annotation situa-
tion. As an additional help, each attribute and each
value in the annotation scheme can have a textual de-
scription associated with it. During annotation, this
description will be displayed to the user when they
move the mouse over the corresponding item in the
AttributeWindow.
Creation and deletion of relations between mark-
ables is performed in two steps: First, the source
markable is selected as described above. Then
the target markable is selected by right-clicking it.
Then, another pop-up menu appears, the options
of which depend on which relations have been de-
fined for the source and target markable: If one or
more member-relation attributes are defined for both
markables, the user will have the option of adding
the target markable to the set of the source mark-
able (if it is already a member of one). If one or
more pointer-relation attributes are defined for the
source markable, the user will also have the option
of pointing to the target markable. Deleting relations
between markables works analogously. After each
modification, the display is refreshed in order to re-
flect changes to the selected markable?s attributes.
5.2 The Discourse API
The MMAX Discourse API6 is intended as a plat-
form for the exploitation and reuse of annotated doc-
uments in MMAX format. It maps the elements of
the base data and the markables to Java classes and
defines a set of basic operations to be performed
on them. The entire document is wrapped in a
Java Discourse object which serves as the single
entry point. The Discourse object itself is created
from the .MMAX project file by a DiscourseLoader
class which parses the XML files and resolves ref-
erences between elements. The result is a tree-like
structure which can be navigated by accessing ele-
ments on a particular level and retrieving their child
elements, which are Java objects themselves and
can thus be used as entry points to their child el-
ements as well. Consider the following example:
getSentenceCount(), when called on the Discourse
object, returns the number of sentences in the cur-
rent document. This number can be used to iter-
ate over all those elements by means of the getSen-
tence(position) method, which returns the sentence
at position as a Java Sentence object. Calling get-
WordCount() on this object returns the number of
word elements the current sentence contains. get-
Word(position) returns the word at position as a Java
Word object. These objects contain, among other
things, a getMarkables() method which returns a list
6This section is based on (Mu?ller and Strube, 2002) where
an earlier version of the MMAX Discourse API is described in
more detail.
of all markables (as Java Markable objects) a word
is part of. Alternatively, getMarkables(level) returns
only those markables on a particular level. On the
level of Markable objects, the API contains a set
of basic methods for e.g. retrieving attribute val-
ues. It also supports procedures like determining the
formal relation between markables (identity, embed-
ding, overlap, and the like).
6 Related Work
The work described in this paper is relevant for two
distinct yet related topics: Representation models
for linguistic data and development of annotation
tools proper.
6.1 Linguistic Representation Models
The Annotation Graph model (Bird and Liberman,
2001) is a current representation model for linguis-
tic data. Within this model, annotations are repre-
sented as labelled arcs connecting nodes on a com-
mon timeline. Each arc is associated with a particu-
lar type (like a phone, word, dialogue act, etc.), and
a set of attribute-value pairs. While they are sim-
ilar to MMAX markables in this respect, Annota-
tion Graphs are much more powerful since they can
model any phenomenon which can be mapped to se-
quentially aligned elements with a temporal exten-
sion. On the other hand, the dependence on time-
aligned data might make it more difficult to model
corpora without time stamps, like e.g. written text
corpora. In principle, however, our approach and
the Annotation Graph model serve rather different
purposes: the former has been primarily designed
as the internal representation format for the MMAX
tool, and turned out to be useful as an independent
representation model as well, while the ambition of
the latter has been to create a general purpose model
for the unification of diverse annotation system for-
mats. Due to their similarity, however, both models
are compatible with each other, and conversion from
one into the other should be possible.
6.2 Annotation Tools
The NITE7 (Bernsen et al, 2002) initiative is a
project in which a workbench for multi-level, cross-
level and cross-modality annotation of language data
7http://nite.nis.sdu.dk
is developed. It is comparable to our tool only in
that it explicitly addresses the simultaneous annota-
tion on different levels. It is, however, much more
ambitious than MMAX, both with respect to its in-
tended scope of functionality and the features it of-
fers for display customization. For instance, NITE
offers plug-ins for speech signal visualization and
even video annotation (Soria et al, 2002): The lat-
ter allows the user/annotator to insert information di-
rectly into the video data. In contrast to that, MMAX
only supports read-only access for playback of au-
dio (and possibly video) files associated with indi-
vidual sentences or turns in the base data. NITE is
even more advanced with respect to the display ca-
pabilities. Users have at their disposal not only plain
text elements, but more powerful devices like tables,
list, buttons and the like, which can be used to create
highly functional displays by means of XSL style
sheets. The downside, however, appears to be that
even minor changes to the elements displayed make
it necessary to reinvoke the style sheet processor,
which may become time-critical for long or more
complex documents. The NITE workbench, which
still appears to be in a demo or prototype stage, is
implemented in C++ and runs only on the Windows
platform. This decision might be motivated by per-
formance requirements resulting from the features
mentioned above.
Apart from NITE, a number of smaller and more
specialized tools for the annotation of individual lin-
guistic phenomena exist, many of which are publicly
available. The Linguistic Annotation website8 con-
tains pointers to a large number of those.
7 Conclusions
This paper presented the MMAX annotation tool
which is based on the following major considera-
tions. On the theoretical side, there is the simplifica-
tion of annotations to a set of simple concepts based
on the notion of markable. Markables are versatile
in the sense that almost any kind of annotation can
be expressed through them. In addition, arbitrarily
many markables can refer to the same sequence of
data without interfering with each other, even if they
are overlapping or discontinuous. This makes it pos-
sible to use them for annotation of various levels of
8http://www.ldc.upenn.edu/annotation
linguistic description simultaneously. Another theo-
retical issue in the design of MMAX is its ability to
express and enforce highly customizable annotation
schemes. On the practical side, a main design fea-
ture is the deliberate restriction of the display capa-
bilities. This, taken together with the rather simple
markable concept, made it possible to implement a
display which is quickly updatable and thus easily
and conveniently usable, even if more than one an-
notation level (i.e. markables file) is displayed at the
same time. The tool is implemented in Java, which
has the additional advantage of being platform in-
dependent and easily extensible. We believe that all
this taken together outweighs the disadvantages of a
slightly ?impoverished? display.
Acknowledgements. The work presented here has
been partially funded by the German Ministry of
Research and Technology as part of the EMBASSI
project (01 IL 904 D/2) and by the Klaus Tschira
Foundation. We would like to thank the researchers
from the COMMOn-REFs project, in particular Car-
oline Varaschin Gasperin, for their useful criticism
and ideas on improving MMAX.
References
Niels Ole Bernsen, Laila Dybkjaer, and Mykola Kolod-
nytsky. 2002. THE NITE WORKBENCH ? A tool
for the annotation of natural interactivity and multi-
modal data. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation,
Las Palmas, Canary Islands, Spain, 29-31 May, 2002,
pages 43?49.
Stephen Bird and Mark Liberman. 2001. A formal
framework for linguistic annotation. Speech Commu-
nication, 33(1):23?60.
Nancy Ide and Greg Priest-Dorman. 1996. The corpus
encoding standard. http://www.cs.vassar.edu/CES.
Christoph Mu?ller and Michael Strube. 2001. MMAX:
A tool for the annotation of multi-modal corpora. In
Proceedings of 2nd IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Seattle,
Wash., 5 August 2001, pages 45?50.
Christoph Mu?ller and Michael Strube. 2002. An API
for discourse-level access to XML-encoded corpora.
In Proceedings of the 3rd International Conference on
Language Resources and Evaluation, Las Palmas, Ca-
nary Islands, Spain, 29-31 May, 2002, pages 26?30.
Christoph Mu?ller, Stefan Rapp, and Michael Strube.
2002. Applying Co-Training to reference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, Philadelphia,
Penn., 7?12 July 2002, pages 352?359.
Stefan Rapp and Michael Strube. 2002. An iterative data
collection approach for multimodal dialogue systems.
In Proceedings of the 3rd International Conference on
Language Resources and Evaluation, Las Palmas, Ca-
nary Islands, Spain, 29-31 May, 2002, pages 661?665.
Susanne Salmon-Alt and Renata Vieira. 2002. Nomi-
nal expressions in multilingual corpora: Definites and
demonstratives. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation, Las Palmas, Canary Islands, Spain, 29-31 May,
2002, pages 1627?1634.
Claudia Soria, Niels Ole Bernsen, Niels Cade?e, Jean Car-
letta, Laila Dybkjaer, Stefan Evert, Ulrich Heid, Amy
Isard, Mykola Kolodnytsky, Christoph Lauer, Wolf-
gang Lezius, Lucas P.J.J. Noldus, Vito Pirrelli, Norbert
Reithinger, and Andreas Vo?gele. 2002. Advanced
tools for the study of natural interactivity. In Pro-
ceedings of the 3rd International Conference on Lan-
guage Resources and Evaluation, Las Palmas, Canary
Islands, Spain, 29-31 May, 2002, pages 357?363.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics, Sap-
poro, Japan, 7?12 July 2003. To appear.
Henry S. Thompson and David McKelvie. 1997. Hy-
perlink semantics for standoff markup of read-only
documents. In Proceedings of SGML Europe ?97,
Barcelona, Spain, May 1997.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 136?143, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Beyond the Pipeline: Discrete Optimization in NLP
Tomasz Marciniak and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We present a discrete optimization model based on
a linear programming formulation as an alternative
to the cascade of classiers implemented in many
language processing systems. Since NLP tasks are
correlated with one another, sequential processing
does not guarantee optimal solutions. We apply our
model in an NLG application and show that it per-
forms better than a pipeline-based system.
1 Introduction
NLP applications involve mappings between com-
plex representations. In generation a representa-
tion of the semantic content is mapped onto the
grammatical form of an expression, and in analy-
sis the semantic representation is derived from the
linear structure of a text or utterance. Each such
mapping is typically split into a number of differ-
ent tasks handled by separate modules. As noted
by Daelemans & van den Bosch (1998), individ-
ual decisions that these tasks involve can be formu-
lated as classification problems falling in either of
two groups: disambiguation or segmentation. The
use of machine-learning to solve such tasks facil-
itates building complex applications out of many
light components. The architecture of choice for
such systems has become a pipeline, with strict or-
dering of the processing stages. An example of
a generic pipeline architecture is GATE (Cunning-
ham et al, 1997) which provides an infrastructure
for building NLP applications. Sequential process-
ing has also been used in several NLG systems (e.g.
Reiter (1994), Reiter & Dale (2000)), and has been
successfully used to combine standard preprocess-
ing tasks such as part-of-speech tagging, chunking
and named entity recognition (e.g. Buchholz et al
(1999), Soon et al (2001)).
In this paper we address the problem of aggregat-
ing the outputs of classifiers solving different NLP
tasks. We compare pipeline-based processing with
discrete optimization modeling used in the field of
computer vision and image recognition (Kleinberg
& Tardos, 2000; Chekuri et al, 2001) and recently
applied in NLP by Roth & Yih (2004), Punyakanok
et al (2004) and Althaus et al (2004). Whereas
Roth and Yih used optimization to solve two tasks
only, and Punyakanok et al and Althaus et al fo-
cused on a single task, we propose a general for-
mulation capable of combining a large number of
different NLP tasks. We apply the proposed model
to solving numerous tasks in the generation process
and compare it with two pipeline-based systems.
The paper is structured as follows: in Section 2 we
discuss the use of classifiers for handling NLP tasks
and point to the limitations of pipeline processing.
In Section 3 we present a general discrete optimiza-
tion model whose application in NLG is described
in Section 4. Finally, in Section 5 we report on the
experiments and evaluation of our approach.
2 Solving NLP Tasks with Classifiers
Classification can be defined as the task Ti of as-
signing one of a discrete set of mi possible labels
Li = {li1, ..., limi}1 to an unknown instance. Since
generic machine-learning algorithms can be applied
to solving single-valued predictions only, complex
1Since we consider different NLP tasks with varying num-
bers of labels we denote the cardinality of Li, i.e. the set of
possible labels for task Ti, as mi.
136
Start
l n1 l n2
l 22l 21
l 11 l 12
l
nnm
l 22m
l1m1
p(l    )11 p(l     )1m1
p(l
   )12
1T
T2
Tn
2m2
p(l     )p(l    )22
p(l   )
21
...
...
...
.
.
.
.
.
.
.
.
.
.
.
.
..
.
...
Figure 1: Sequential processing as a graph.
structures, such as parse trees, coreference chains or
sentence plans, can only be assembled from the out-
puts of many different classifiers.
In an application implemented as a cascade of
classifiers the output representation is built incre-
mentally, with subsequent classifiers having access
to the outputs of previous modules. An important
characteristic of this model is its extensibility: it
is generally easy to change the ordering or insert
new modules at any place in the pipeline2 . A ma-
jor problem with sequential processing of linguis-
tic data stems from the fact that elements of linguis-
tic structure, at the semantic or syntactic levels, are
strongly correlated with one another. Hence clas-
sifiers that have access to additional contextual in-
formation perform better than if this information is
withheld. In most cases, though, if task Tk can use
the output of Ti to increase its accuracy, the reverse
is also true. In practice this type of processing may
lead to error propagation. If due to the scarcity of
contextual information the accuracy of initial clas-
sifiers is low, erroneous values passed as input to
subsequent tasks can cause further misclassifications
which can distort the final outcome (also discussed
by Roth and Yih and van den Bosch et al (1998)).
As can be seen in Figure 1, solving classifica-
tion tasks sequentially corresponds to the best-first
traversal of a weighted multi-layered lattice. Nodes
at separate layers (T1, ..., Tn) represent labels of dif-
ferent classification tasks and transitions between
the nodes are augmented with probabilities of se-
2Both operations only require retraining classifiers with a
new selection of the input features.
lecting respective labels at the next layer. In the se-
quential model only transitions between nodes be-
longing to subsequent layers are allowed. At each
step, the transition with the highest local probability
is selected. Selected nodes correspond to outcomes
of individual classifiers. This graphical representa-
tion shows that sequential processing does not guar-
antee an optimal context-dependent assignment of
class labels and favors tasks that occur later, by pro-
viding them with contextual information, over those
that are solved first.
3 Discrete Optimization Model
As an alternative to sequential ordering of NLP
tasks we consider the metric labeling problem for-
mulated by Kleinberg & Tardos (2000), and orig-
inally applied in an image restoration application,
where classifiers determine the ?true? intensity val-
ues of individual pixels. This task is formulated as a
labeling function f : P ? L, that maps a set P of n
objects onto a set L of m possible labels. The goal
is to find an assignment that minimizes the overall
cost function Q(f), that has two components: as-
signment costs, i.e. the costs of selecting a particular
label for individual objects, and separation costs, i.e.
the costs of selecting a pair of labels for two related
objects3 . Chekuri et al (2001) proposed an integer
linear programming (ILP) formulation of the met-
ric labeling problem, with both assignment cost and
separation costs being modeled as binary variables
of the linear cost function.
Recently, Roth & Yih (2004) applied an ILP
model to the task of the simultaneous assignment
of semantic roles to the entities mentioned in a sen-
tence and recognition of the relations holding be-
tween them. The assignment costs were calculated
on the basis of predictions of basic classifiers, i.e.
trained for both tasks individually with no access to
the outcomes of the other task. The separation costs
were formulated in terms of binary constraints, that
specified whether a specific semantic role could oc-
cur in a given relation, or not.
In the remainder of this paper, we present a more
general model, that is arguably better suited to hand-
ling different NLP problems. More specifically, we
3These costs were calculated as the function of the metric
distance between a pair of pixels and the difference in intensity.
137
put no limits on the number of tasks being solved,
and express the separation costs as stochastic con-
straints, which for almost any NLP task can be cal-
culated off-line from the available linguistic data.
3.1 ILP Formulation
We consider a general context in which a specific
NLP problem consists of individual linguistic de-
cisions modeled as a set of n classification tasks
T = {T1, ..., Tn}, that potentially form mutually
related pairs. Each task Ti consists in assigning a
label from Li = {li1, ..., limi} to an instance that
represents the particular decision. Assignments are
modeled as variables of a linear cost function. We
differentiate between simple variables that model in-
dividual assignments of labels and compound vari-
ables that represent respective assignments for each
pair of related tasks.
To represent individual assignments the following
procedure is applied: for each task Ti, every label
from Li is associated with a binary variable x(lij).
Each such variable represents a binary choice, i.e. a
respective label lij is selected if x(lij) = 1 or re-
jected otherwise. The coefficient of variable x(lij),
that models the assignment cost c(lij), is given by:
c(lij) = ?log2(p(lij))
where p(lij) is the probability of lij being selected as
the outcome of task Ti. The probability distribution
for each task is provided by the basic classifiers that
do not consider the outcomes of other tasks4.
The role of compound variables is to provide pair-
wise constraints on the outcomes of individual tasks.
Since we are interested in constraining only those
tasks that are truly dependent on one another we first
apply the contingency coefficient C to measure the
degree of correlation for each pair of tasks5.
In the case of tasks Ti and Tk which are sig-
nificantly correlated, for each pair of labels from
4In this case the ordering of tasks is not necessary, and the
classifiers can run independently from each other.
5C is a test for measuring the association of two nominal
variables, and hence adequate for the type of tasks that we con-
sider here. The coefficient takes values from 0 (no correlation)
to 1 (complete correlation) and is calculated by the formula:
C = (?2/(N + ?2))1/2, where ?2 is the chi-squared statistic
and N the total number of instances. The significance of C is
then determined from the value of ?2 for the given data. See
e.g. Goodman & Kruskal (1972).
Li ? Lk we build a single variable x(lij , lkp). Each
such variable is associated with a coefficient repre-
senting the constraint on the respective pair of labels
lij , lkp calculated in the following way:
c(lij , lkp) = ?log2(p(lij,lkp))
with p(lij,lkp) denoting the prior joint probability of
labels lij, and lkp in the data, which is independent
from the general classification context and hence can
be calculated off-line6 .
The ILP model consists of the target function and
a set of constraints which block illegal assignments
(e.g. only one label of the given task can be se-
lected)7 . In our case the target function is the cost
function Q(f), which we want to minimize:
min Q(f) =
?
Ti?T
?
lij?Li
c(lij) ? x(lij)
+
?
Ti,Tk?T,i<k
?
lij ,lkp?Li?Lk
c(lij , lkp) ? x(lij , lkp)
Constraints need to be formulated for both the
simple and compound variables. First we want to
ensure that exactly one label lij belonging to task Ti
is selected, i.e. only one simple variable x(lij) rep-
resenting labels of a given task can be set to 1:
?
lij?Li
x(lij) = 1, ?i ? {1, ..., n}
We also require that if two simple variables x(lij)
and x(lkp), modeling respectively labels lij and lkp
are set to 1, then the compound variable x(lij , lkp),
which models co-occurrence of these labels, is also
set to 1. This is done in two steps: we first en-
sure that if x(lij) = 1, then exactly one variable
x(lij , lkp) must also be set to 1:
x(lij) ?
?
lkp?Lk
x(lij , lkp) = 0,
?i, k ? {1, ..., n}, i < k ? j ? {1, ..., mi}
and do the same for variable x(lkp):
6In Section 5 we discuss an alternative approach which con-
siders the actual input.
7For a detailed overview of linear programming and differ-
ent types of LP problems see e.g. Nemhauser & Wolsey (1999).
138
l 11
1T
Tn
l 21
T2
l n1
l 1m1
l 2m2
l nmn
...
...
...
c(l    )11
c(l    ,l     )11   2m
11   21
c(l    ,l    )
1m
   21
c(l    ,l    )
c(l    ,l 
    )
21   n
m
c(l     ,l     )
c(l 
   ,l
    )
21
   n
1
c(l
    
 ,l 
   )
2m
   
n1
2m
   n
m
c(l  
   ,l 
    )
c(l     )2m21c(l    )
c(l    )n1
c(l     )nm
1mc(l     )
1m   2m
Figure 2: Graph representation of the ILP model.
x(lkp) ?
?
lij?Li
x(lij , lkp) = 0,
?i, k ? {1, ..., n}, i < k ? p ? {1, ..., mk}
Finally, we constrain the values of both simple
and compound variables to be binary:
x(lij) ? {0, 1} ? x(lij , lkp) ? {0, 1},
?i, k ? {1, ..., n} ? j ? {1, ..., mi} ? p ? {1, ..., mk}
3.2 Graphical Representation
We can represent the decision process that our ILP
model involves as a graph, with the nodes corre-
sponding to individual labels and the edges marking
the association between labels belonging to corre-
lated tasks. In Figure 2, task T1 is correlated with
task T2 and task T2 with task Tn. No correlation
exists for pair T1, Tn. Both nodes and edges are
augmented with costs. The goal is to select a sub-
set of connected nodes, minimizing the overall cost,
given that for each group of nodes T1, T2, ..., Tn ex-
actly one node must be selected, and the selected
nodes, representing correlated tasks, must be con-
nected. We can see that in contrast to the pipeline
approach (cf. Figure 1), no local decisions determine
the overall assignment as the global distribution of
costs is considered.
4 Application for NL Generation Tasks
We applied the ILP model described in the previous
section to integrate different tasks in an NLG ap-
plication that we describe in detail in Marciniak &
Strube (2004). Our classification-based approach to
language generation assumes that different types of
linguistic decisions involved in the generation pro-
cess can be represented in a uniform way as clas-
sification problems. The linguistic knowledge re-
quired to solve the respective classifications is then
learned from a corpus annotated with both seman-
tic and grammatical information. We have applied
this framework to generating natural language route
directions, e.g.:
(a) Standing in front of the hotel (b) fol-
low Meridian street south for about 100
meters, (c) passing the First Union Bank
entrance on your right, (d) until you see
the river side in front of you.
We analyze the content of such texts in terms of
temporally related situations, i.e. actions (b), states
(a) and events (c,d), denoted by individual discourse
units8. The semantics of each discourse unit is fur-
ther given by a set of attributes specifying the se-
mantic frame and aspectual category of the pro-
filed situation. Our corpus of semantically anno-
tated route directions comprises 75 texts with a to-
tal number of 904 discourse units (see Marciniak &
Strube (2005)). The grammatical form of the texts
is modeled in terms of LTAG trees also represented
as feature vectors with individual features denoting
syntactic and lexical elements at both the discourse
and clause levels. The generation of each discourse
unit consists in assigning values to the respective
features, of which the LTAG trees are then assem-
bled. In Marciniak & Strube (2004) we implemented
the generation process sequentially as a cascade of
classifiers that realized incrementally the vector rep-
resentation of the generated text?s form, given the
meaning vector as input. The classifiers handled the
following eight tasks, all derived from the LTAG-
based representation of the grammatical form:
T1: Discourse Units Rank is concerned with or-
dering discourse units at the local level, i.e. only
clauses temporally related to the same parent clause
are considered. This task is further split into a series
of binary precedence classifications that determine
the relative position of two discourse units at a time
8The temporal structure was represented as a tree, with dis-
course units as nodes.
139
Discourse Unit T3 T4 T5
Pass the First Union Bank ... null vp bare inf.
It is necessary that you pass ... null np+vp bare inf.
Passing the First Union Bank ... null vp gerund
After passing ... after vp gerund
After your passing . . . after np+vp gerund
As you pass ... as np+vp fin. pres.
Until you pass ... until np+vp fin. pres.
Until passing . . . until vp gerund
Table 1: Different realizations of tasks: Connective, Verb
Form and S Exp. Rare but correct constructions are in italics.
T :  Verb Lex6
T :  Phrase Type7
T :  Phrase Rank8
4T :  S Exp.
T :  Disc. Units Dir.2
T :  Verb Form5
T :  Disc. Units Rank1
T :  Connective3
Figure 3: Correlation network for the generation tasks. Cor-
related tasks, are connected with lines.
(e.g. (a) before (c), (c) before (d), etc.). These partial
results are later combined to determine the ordering.
T2: Discourse Unit Position specifies the position
of the child discourse unit relative to the parent one
(e.g. (a) left of (b), (c) right of (b), etc.).
T3: Discourse Connective determines the lexical
form of the discourse connective (e.g. null in (a), un-
til in (d)).
T4: S Expansion specifies whether a given dis-
course unit would be realized as a clause with the
explicit subject (i.e. np+vp expansion of the root S
node in a clause) (e.g. (d)) or not (e.g. (a), (b)).
T5: Verb Form determines the form of the main
verb in a clause (e.g. gerund in (a), (c), bare infini-
tive in (b), finite present in (d)).
T6: Verb Lexicalization provides the lexical form
of the main verb (e.g. stand, follow, pass, etc.).
T7: Phrase Type determines for each verb argu-
ment in a clause its syntactic realization as a noun
phrase, prepositional phrase or a particle.
T8: Phrase Rank determines the ordering of verb
arguments within a clause. As in T1 this task is split
into a number binary classifications.
To apply the LP model to the generation problem
discussed above, we first determined which pairs of
tasks are correlated. The obtained network (Fig-
ure 3) is consistent with traditional analyses of the
linguistic structure in terms of adjacent but sepa-
rate levels: discourse, clause, phrase. Only a few
correlations extend over level boundaries and tasks
within those levels are correlated. As an example
consider three interrelated tasks: Connective, S Exp.
and Verb Form and their different realizations pre-
sented in Table 1. Apparently different realization
of any of these tasks can affect the overall meaning
of a discourse unit or its stylistics. It can also be seen
that only certain combinations of different forms are
allowed in the given semantic context. We can con-
clude that for such groups of tasks sequential pro-
cessing may fail to deliver an optimal assignment.
5 Experiments and Results
In order to evaluate our approach we conducted
experiments with two implementations of the ILP
model and two different pipelines (presented below).
Each system takes as input a tree structure, repre-
senting the temporal structure of the text. Individ-
ual nodes correspond to single discourse units and
their semantic content is given by respective feature
vectors. Generation occurs in a number of stages,
during which individual discourse units are realized.
5.1 Implemented Systems
We used the ILP model described in Section 3 to
build two generation systems. To obtain assignment
costs, both systems get a probability distribution for
each task from basic classifiers trained on the train-
ing data. To calculate the separation costs, modeling
the stochastic constraints on the co-occurrence of la-
bels, we considered correlated tasks only (cf. Figure
3) and applied two calculation methods, which re-
sulted in two different system implementations.
In ILP1, for each pair of tasks we computed the
joint distribution of the respective labels consider-
ing all discourse units in the training data before the
actual input was known. Such obtained joint distri-
butions were used for generating all discourse units
from the test data. An example matrix with joint dis-
tribution for selected labels of tasks Connective and
Verb Form is given in Table 2. An advantage of this
140
null and as after until T3 Connective
T5 Verb Form
0.40 0.18 0 0 0 bare inf
0 0 0 0.04 0.01 gerund
0.05 0.01 0.06 0.03 0.06 n pres
0.06 0.05 0 0 0 will inf
Table 2: Joint distribution matrix for selected labels of tasks
Connective (horizontal) and Verb Form (vertical), computed for
all discourse units in a corpus.
null and as after until T3 Connective
T5 Verb Form
0.13 0.02 0 0 0 bare inf
0 0 0 0 0 gerund
0 0 0.05 0.02 0.27 n pres
0.36 0.13 0 0 0 will inf
Table 3: Joint distribution matrix for tasks Connective and
Verb Form, considering only discourse units similar to (c): until
you see the river side in front of you, at Phi-threshold ? 0.8.
approach is that the computation can be done in an
offline mode and has no impact on the run-time.
In ILP2, the joint distribution for a pair of tasks
was calculated at run-time, i.e. only after the actual
input had been known. This time we did not con-
sider all discourse units in the training data, but only
those whose meaning, represented as a feature vec-
tor was similar to the meaning vector of the input
discourse unit. As a similarity metric we used the
Phi coefficient9 , and set the similarity threshold at
0.8. As can be seen from Table 3, the probability
distribution computed in this way is better suited to
the specific semantic context. This is especially im-
portant if the available corpus is small and the fre-
quency of certain pairs of labels might be too low to
have a significant impact on the final assignment.
As a baseline we implemented two pipeline sys-
tems. In the first one we used the ordering of
tasks most closely resembling the conventional NLG
pipeline (see Figure 4). Individual classifiers had ac-
cess to both the semantic features, and those output
by the previous modules. To train the classifiers,
the correct feature values were extracted from the
training data and during testing the generated, and
hence possibly erroneous, values were taken. In the
9Phi is a measure of the extent of correlation between two
sets of binary variables, see e.g. Edwards (1976). To represent
multi-class features on a binary scale we applied dummy cod-
ing which transforms multi class-nominal variables to a set of
dummy variables with binary values.
other pipeline system we wanted to minimize the
error-propagation effect and placed the tasks in the
order of decreasing accuracy. To determine the or-
dering of tasks we applied the following procedure:
the classifier with the highest baseline accuracy was
selected as the first one. The remaining classifiers
were trained and tested again, but this time they had
access to the additional feature. Again, the classi-
fier with the highest accuracy was selected and the
procedure was repeated until all classifiers were or-
dered.
5.2 Evaluation
We evaluated our system using leave-one-out cross-
validation, i.e. for all texts in the corpus, each
text was used once for testing, and the remaining
texts provided the training data. To solve individ-
ual classification tasks we used the decision tree
learner C4.5 in the pipeline systems and the Naive
Bayes algorithm10 in the ILP systems. Both learn-
ing schemes yielded highest results in the respec-
tive configurations11 . For each task we applied
a feature selection procedure (cf. Kohavi & John
(1997)) to determine which semantic features should
be taken as the input by the respective basic classi-
fiers12. We started with an empty feature set, and
then performed experiments checking classification
accuracy with only one new feature at a time. The
feature that scored highest was then added to the fea-
ture set and the whole procedure was repeated itera-
tively until no performance improvement took place,
or no more features were left.
To evaluate individual tasks we applied two met-
rics: accuracy, calculated as the proportion of cor-
rect classifications to the total number of instances,
and the ? statistic, which corrects for the propor-
tion of classifications that might occur by chance13
10Both implemented in the Weka machine learning software
(Witten & Frank, 2000).
11We have found that in direct comparison C4.5 reaches
higher accuracies than Naive Bayes but the probability distri-
bution that it outputs is strongly biased towards the winning la-
bel. In this case it is practically impossible for the ILP system
to change the classifier?s decision, as the costs of other labels
get extremely high. Hence the more balanced probability dis-
tribution given by Naive Bayes can be easier corrected in the
optimization process.
12I.e. trained using the semantic features only, with no access
to the outputs of other tasks.
13Hence the ? values obtained for tasks of different difcul-
141
Pipeline 1 Pipeline 2 ILP 1 ILP 2
Tasks Pos. Accuracy ? Pos. Accuracy ? Accuracy ? Accuracy ?
Dis.Un. Rank 1 96.81% 90.90% 2 96.81% 90.90% 97.43% 92.66% 97.43% 92.66%
Dis.Un. Pos. 2 98.04% 89.64% 1 98.04% 89.64% 96.10% 77.19% 97.95% 89.05%
Connective 3 78.64% 60.33% 7 79.10% 61.14% 79.15% 61.22% 79.36% 61.31%
S Exp. 4 95.90% 89.45% 3 96.20% 90.17% 99.48% 98.65% 99.49% 98.65%
Verb Form 5 86.76% 77.01% 4 87.83% 78.90% 92.81% 87.60% 93.22% 88.30%
Verb Lex 6 64.58% 60.87% 8 67.40% 64.19% 75.87% 73.69% 76.08% 74.00%
Phr. Type 7 86.93% 75.07% 5 87.08% 75.36% 87.33% 76.75% 88.03% 77.17%
Phr. Rank 8 84.73% 75.24% 6 86.95% 78.65% 90.22% 84.02% 91.27% 85.72%
Phi 0.85 0.87 0.89 0.90
Table 4: Results reached by the implemented ILP systems and two baselines. For both pipeline systems, Pos. stands for the
position of the tasks in the pipeline.
(Siegel & Castellan, 1988). For end-to-end evalua-
tion, we applied the Phi coefficient to measure the
degree of similarity between the vector representa-
tions of the generated form and the reference form
obtained from the test data. The Phi statistic is sim-
ilar to ? as it compensates for the fact that a match
between two multi-label features is more difficult to
obtain than in the case of binary features. This mea-
sure tells us how well all the tasks have been solved
together, which in our case amounts to generating
the whole text.
The results presented in Table 4 show that the ILP
systems achieved highest accuracy and ? for most
tasks and reached the highest overall Phi score. No-
tice that for the three correlated tasks that we consid-
ered before, i.e. Connective, S Exp. and Verb Form,
ILP2 scored noticeably higher than the pipeline sys-
tems. It is interesting to see the effect of sequential
processing on the results for another group of cor-
related tasks, i.e. Verb Lex, Phrase Type and Phrase
Rank (cf. Figure 3). Verb Lex got higher scores
in Pipeline2, with outputs from both Phrase Type
and Phrase Rank (see the respective pipeline posi-
tions), but the reverse effect did not occur: scores
for both phrase tasks were lower in Pipeline1 when
they had access to the output from Verb Lex, con-
trary to what we might expect. Apparently, this was
due to the low accuracy for Verb Lex which caused
the already mentioned error propagation14 . This ex-
ample shows well the advantage that optimization
processing brings: both ILP systems reached much
ties can be directly compared, which gives a clear notion how
well individual tasks have been solved.
14Apparantly, tasks which involve lexical choice get low
scores with retrieval measures as the semantic content allows
typically more than one correct form
higher scores for all three tasks.
5.3 Technical Notes
The size of an LP model is typically expressed in the
number of variables and constraints. In the model
presented here it depends on the number of tasks in
T , the number of possible labels for each task, and
the number of correlated tasks. For n different tasks
with the average of m labels, and assuming every
two tasks are correlated with each other, the num-
ber of variables in the LP target functions is given
by: num(var) = n ? m + 1/2 ? n(n ? 1) ? m2
and the number of constraints by: num(cons) =
n + n ? (n ? 1) ? m. To solve the ILP models in our
system we use lp solve, an efficient GNU-licence
Mixed Integer Programming (MIP) solver15, which
implements the Branch-and-Bound algorithm. In
our application, the models varied in size from: 557
variables and 178 constraints to 709 variables and
240 constraints, depending on the number of ar-
guments in a sentence. Generation of a text with
23 discourse units took under 7 seconds on a two-
processor 2000 MHz AMD machine.
6 Conclusions
In this paper we argued that pipeline architectures in
NLP can be successfully replaced by optimization
models which are better suited to handling corre-
lated tasks. The ILP formulation that we proposed
extends the classification paradigm already estab-
lished in NLP and is general enough to accommo-
date various kinds of tasks, given the right kind of
data. We applied our model in an NLG applica-
tion. The results we obtained show that discrete
15http://www.geocities.com/lpsolve/
142
optimization eliminates some limitations of sequen-
tial processing, and we believe that it can be suc-
cessfully applied in other areas of NLP. We view
our work as an extension to Roth & Yih (2004) in
two important aspects. We experiment with a larger
number of tasks having a varying number of labels.
To lower the complexity of the models, we apply
correlation tests, which rule out pairs of unrelated
tasks. We also use stochastic constraints, which are
application-independent, and for any pair of tasks
can be obtained from the data.
A similar argument against sequential modular-
ization in NLP applications was raised by van den
Bosch et al (1998) in the context of word pronun-
ciation learning. This mapping between words and
their phonemic transcriptions traditionally assumes
a number of intermediate stages such as morpho-
logical segmentation, graphemic parsing, grapheme-
phoneme conversion, syllabification and stress as-
signment. The authors report an increase in gener-
alization accuracy when the the modular decompo-
sition is abandoned (i.e. the tasks of conversion to
phonemes and stress assignment get conflated and
the other intermediate tasks are skipped). It is inter-
esting to note that a similar dependence on the inter-
mediate abstraction levels is present in such applica-
tions as parsing and semantic role labelling, which
both assume POS tagging and chunking as their pre-
ceding stages.
Currently we are working on a uniform data for-
mat that would allow to represent different NLP ap-
plications as multi-task optimization problems. We
are planning to release a task-independent Java API
that would solve such problems. We want to use this
generic model for building NLP modules that tradi-
tionally are implemented sequentially.
Acknowledgements: The work presented here
has been funded by the Klaus Tschira Foundation,
Heidelberg, Germany. The first author receives a
scholarship from KTF (09.001.2004).
References
Althaus, E., N. Karamanis & A. Koller (2004). Computing lo-
cally coherent discourses. In Proceedings of the 42 Annual
Meeting of the Association for Computational Linguistics,
Barcelona, Spain, July 21-26, 2004, pp. 399?406.
Buchholz, S., J. Veenstra & W. Daelemans (1999). Cascaded
grammatical relation assignment. In Joint SIGDAT Confer-
ence on Empirical Methods in Natural Language Processing
and Very Large Corpora, College Park, Md., June 21-22,
1999, pp. 239?246.
Chekuri, C., S. Khanna, J. Naor & L. Zosin (2001). Approx-
imation algorithms for the metric labeling problem via a
new linear programming formulation. In Proceedings of the
12th Annual ACM SIAM Symposium on Discrete Algorithms,
Washington, DC, pp. 109?118.
Cunningham, H., K. Humphreys, Y. Wilks & R. Gaizauskas
(1997). Software infrastructure for natural language process-
ing. In Proceedings of the Fifth Conference on Applied Natu-
ral Language Processing Washington, DC, March 31 - April
3, 1997, pp. 237?244.
Daelemans, W. & A. van den Bosch (1998). Rapid develop-
ment of NLP modules with memory-based learning. In Pro-
ceedings of ELSNET in Wonderland. Utrecht: ELSNET, pp.
105?113.
Edwards, Allen, L. (1976). An Introduction to Linear Regres-
sion and Correlation. San Francisco, Cal.: W. H. Freeman.
Goodman, L. A. & W. H. Kruskal (1972). Measures of asso-
ciation for cross-classification, iv. Journal of the American
Statistical Association, 67:415?421.
Kleinberg, J. M. & E. Tardos (2000). Approximation algorithms
for classification problems with pairwise relationships: Met-
ric labeling and Markov random fields. Journal of the ACM,
49(5):616?639.
Kohavi, R. & G. H. John (1997). Wrappers for feature subset
selection. Articial Intelligence Journal, 97:273?324.
Marciniak, T. & M. Strube (2004). Classification-based gen-
eration using TAG. In Proceedings of the 3rd International
Conference on Natural Language Generation, Brockenhurst,
UK, 14-16 July, 2004, pp. 100?109.
Marciniak, T. & M. Strube (2005). Modeling and annotating the
semantics of route directions. In Proceedings of the 6th In-
ternational Workshop on Computational Semantics, Tilburg,
The Netherlands, January 12-14, 2005, pp. 151?162.
Nemhauser, G. L. & L. A. Wolsey (1999). Integer and combi-
natorial optimization. New York, NY: Wiley.
Punyakanok, V., D. Roth, W. Yih & Z. Dav (2004). Semantic
role labeling via integer linear programming inference. In
Proceedings of the 20th International Conference on Com-
putational Linguistics, Geneva, Switzerland, August 23-27,
2004, pp. 1346?1352.
Reiter, E. (1994). Has a consensus NL generation architecture
appeared, and is it psycholinguistically plausible? In Pro-
ceedings of the 7th International Workshop on Natural Lan-
guage Generation, Kennebunkport, Maine, pp. 160?173.
Reiter, E. & R. Dale (2000). Building Natural Language Gener-
ation Systems. Cambridge, UK: Cambridge University Press.
Roth, D. & W. Yih (2004). A linear programming formulation
for global inference in natural language tasks. In Proceed-
ings of the 8th Conference on Computational Natural Lan-
guage Learning, Boston, Mass., May 2-7, 2004, pp. 1?8.
Siegel, S. & N. J. Castellan (1988). Nonparametric Statistics
for the Behavioral Sciences. New York, NY: McGraw-Hill.
Soon, W. M., H. T. Ng & D. C. L. Lim (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
van den Bosch, A., T. Weijters & W. Daelemans (1998). Modu-
larity in inductively-learned word pronunciation systems. In
D. Powers (Ed.), Proceedings of NeMLaP3/CoNLL98, pp.
185?194.
Witten, I. H. & E. Frank (2000). Data Mining - Practical Ma-
chine Learning Tools and Techniques with Java Implementa-
tions. San Francisco, Cal.: Morgan Kaufmann.
143
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 213?216, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling Using Lexical Statistical Information
Simone Paolo Ponzetto and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp/
Abstract
Our system for semantic role labeling is
multi-stage in nature, being based on tree
pruning techniques, statistical methods for
lexicalised feature encoding, and a C4.5
decision tree classifier. We use both shal-
low and deep syntactic information from
automatically generated chunks and parse
trees, and develop a model for learning
the semantic arguments of predicates as a
multi-class decision problem. We evalu-
ate the performance on a set of relatively
?cheap? features and report an F1 score of
68.13% on the overall test set.
1 Introduction
This paper presents a system for the CoNLL 2005
Semantic Role Labeling shared task (Carreras &
Ma`rquez, 2005), which is based on the current re-
lease of the English PropBank data (Palmer et al,
2005). For the 2005 edition of the shared task are
available both syntactic and semantic information.
Accordingly, we make use of both clausal, chunk
and deep syntactic (tree structure) features, named
entity information, as well as statistical representa-
tions for lexical item encoding.
The set of features and their encoding reflect the
necessity of limiting the complexity and dimension-
ality of the input space. They also provide the clas-
sifier with enough information. We explore here the
use of a minimal set of compact features for seman-
tic role prediction, and show that a feature-based
statistical encoding of lexicalised features such as
predicates, head words, local contexts and PoS by
means of probability distributions provides an effi-
cient way of representing the data, with the feature
vectors having a small dimensionality and allowing
to abstract from single words.
2 System description
2.1 Preprocessing
During preprocessing the predicates? semantic argu-
ments are mapped to the nodes in the parse trees, a
set of hand-crafted shallow tree pruning rules are ap-
plied, probability distributions for feature represen-
tation are generated from training data1, and feature
vectors are extracted. Those are finally fed into the
classifier for semantic role classification.
2.1.1 Tree node mapping of semantic
arguments and named entities
Following Gildea & Jurafsky (2002), (i) labels
matching more than one constituent due to non-
branching nodes are taken as labels of higher con-
stituents, (ii) in cases of labels with no correspond-
ing parse constituent, these are assigned to the par-
tial match given by the constituent spanning the
shortest portion of the sentence beginning at the la-
bel?s span left boundary and lying entirely within it.
We drop the role or named entity label if such suit-
able constituent could not be found2.
1All other processing steps assume a uniform treatment of
both training and test data.
2The percentage of roles for which no valid tree node could
be found amounts to 3% for the training and 7% for the devel-
opment set. These results are compatible with the performance
of the employed parser (Collins, 1999).
213
2.1.2 Tree pruning
The tagged trees are further processed by applying
the following pruning rules:
? All punctuation nodes are removed. This is for
removing punctuation information, as well as
for aligning spans of the syntactic nodes with
PropBank constituents3.
? If a node is unary branching and its daughter is
also unary branching, the daughter is removed.
This allows to remove redundant nodes span-
ning the same tokens in the sentence.
? If a node has only preterminal children, these
are removed. This allows to internally collapse
base phrases such as base NPs.
Tree pruning was carried out in order to reduce the
number of nodes from which features were to be ex-
tracted later. This limits the number of candidate
constituents for role labeling, and removes redun-
dant information produced by the pipeline of previ-
ous components (i.e. PoS tags of preterminal labels),
as well as the sparseness and fragmentation of the
input data. These simple rules reduce the number
of constituents given by the parser output by 38.4%
on the training set, and by 38.7% on the develop-
ment set, at the cost of limiting the coverage of the
system by removing approximately 2% of the tar-
get role labeled constituents. On the development
set, the number of constituents remaining on top of
pruning is 81,193 of which 7,558 are semantic ar-
guments, with a performance upper-bound of 90.6%
F1.
2.1.3 Features
Given the pruned tree structures, we traverse the tree
bottom-up left-to-right. For each non-terminal node
whose span does not overlap the predicate we extract
the following features:
Phrase type: the syntactic category of the con-
stituent (NP, PP, ADVP, etc.). In order to reduce
the number of phrase labels, we retained only
3We noted during prototyping that in many cases no tree
node fully matching a role constituent could be found, as the
latter did not include punctuation tokens, whereas in Collins?
trees the punctuation terminals are included within the preced-
ing phrases. This precludes a priori the output to align to the
gold standard PropBank annotation and we use therefore prun-
ing as a recovery strategy.
those labels which account for at least 0.1% of
the overall available semantic arguments in the
training data. We replace the label for every
phrase type category below this threshold with
a generic UNK label. This reduces the number
of labels from 72 to 18.
Position: the position of the constituent with re-
spect to the target predicate (before or after).
Adjacency: whether the right (if before) or left (if
after) boundary of the constituent is adjacent,
non-adjacent or inside the predicate?s chunk.
Clause: whether the constituent belongs to the
clause of the predicate or not.
Proposition size: measures relative to the proposi-
tion size, such as (i) the number of constituents
and (ii) predicates in the proposition.
Constituent size: measures relative to the con-
stituent size, namely (i) the number of tokens
and (ii) subconstituents (viz., non-leaf rooted
subtrees) of the constituent.
Predicate: the predicate lemma, represented as the
probability distribution P (r|p) of the predicate
p of taking one of the available r semantic
roles. For unseen predicates we assume a uni-
form distribution.
Voice: whether the predicate is in active or passive
form. Passive voice is identified if the predi-
cate?s PoS tag is VBN and either it follows a
form of to be or to get, or it does not belong to
a VP chunk, or is immediately preceded by an
NP chunk.
Head word: the head word of the constituent,
represented as the probability distribution
P (r|hw) of the head word hw of heading a
phrase filling one of the available r seman-
tic roles. For unseen words we back off on a
phrasal model by using the probability distri-
bution P (r|pt) of the phrase type pt of filling a
semantic slot r.
Head word PoS: the PoS of the head word of the
constituent, similarly represented as the proba-
bility distribution P (r|pos) of a PoS pos of be-
longing to a constituent filling one of the avail-
able r semantic roles.
Local lexical context: the words in the constituent
other than the head word, represented as the
214
averaged probability distributions of each i-
th non-head word wi of occurring in one
of the available r semantic roles, namely
1
m
?m
i=1 P (r|wi) for m non-head words in the
constituent. For each unseen word we back off
by using the probability distribution P (r|posi)
of the PoS posi of filling a semantic role r4.
Named entities: the label of the named entity
which spans the same words as the constituent,
as well as the label of the largest named en-
tity embedded within the constituent. Both val-
ues are set to NULL if such labels could not be
found.
Path: the number of intervening NPB, NP, VP, VP-
A, PP, PP-A, S, S-A and SBAR nodes along the
path from the constituent to the predicate.
Distance: the distance from the target predicate,
measured as (i) the number of nodes from the
constituent to the lowest node in the tree dom-
inating both the constituent and the predicate,
(ii) the number of nodes from the predicate to
the former common dominating node5, (iii) the
number of chunks between the base phrase of
the constituent?s head and the predicate chunk,
(iv) the number of tokens between the head of
the constituent and the predicate.
2.2 Classifier
We used the YaDT6 implementation of the C4.5 de-
cision tree algorithm (Quinlan, 1993). Parameter
selection (99% pruning confidence, at least 10 in-
stances per leaf node) was carried out by performing
10-fold cross-validation on the development set.
Data preprocessing and feature vector generation
took approximately 2.5 hours (training set, including
probability distribution generation), 5 minutes (de-
velopment) and 7 minutes (test) on a 2GHz Opteron
4This feature was introduced as the information provided by
lexical heads does not seem to suffice in many cases. This is
shown by head word ambiguities, such as LOC and TMP ar-
guments occurring in similar prepositional syntactic configu-
rations ? i.e. the preposition in, which can be head of both
AM-TMP and AM-LOC constituents, as in in October and in
New York. The idea is therefore to look at the words in the con-
stituents other than the head, and build up an overall constituent
representation, thus making use of statistical lexical information
for role disambiguation.
5These distance measures along the tree path between the
constituent and the predicate were kept separate, in order to in-
directly include embedding level information into the model.
6http://www.di.unipi.it/?ruggieri/software.html
dual processor server with 2GB memory7. Training
time was of approximately 17 minutes. The final
system was trained using all of the available training
data from sections 2?21 of the Penn TreeBank. This
amounts to 2,250,887 input constituents of which
10% are non-NULL examples. Interestingly, during
prototyping we first limited ourselves to training and
drawing probability distributions for feature repre-
sentation from sections 15?18 only. This yielded
a very low performance (57.23% F1, development
set). A substantial performance increase was given
by still training on sections 15?18, but using the
probability distributions generated from sections 2?
21 (64.43% F1, development set). This suggests that
the system is only marginally sensitive to the train-
ing dataset size, but pivotally relies on taking proba-
bility distributions from a large amount of data.
In order to make the task easier and overcome the
uneven role class distribution, we limited the learner
to classify only those 16 roles accounting for at least
0.5% of the total number of semantic arguments in
the training data8.
2.3 Post-processing
As our system does not build an overall sen-
tence contextual representation, it systematically
produced errors such as embedded role labeling. In
particular, since no embedding is observed for the
semantic arguments of predicates, in case of (multi-
ple) embeddings the classifier output was automat-
ically post-processed to retain only the largest em-
bedding constituent. Evaluation on the development
set has shown that this does not significantly im-
prove performance, still it provides a much more
?sane? output. Besides, we make use of a simple
technique for avoiding multiple A0 or A1 role as-
signments within the same proposition, based on
constituent position and predicate voice. In case of
multiple A0 labels, if the predicate is in active form,
the second A0 occurrence is replaced with A1, else
we replace the first occurrence. Similarly, in case of
multiple A1 labels, if the predicate is in active form,
the first A1 occurrence is replaced with A0, else we
7We used only a single CPU at runtime, since the implemen-
tation is not parallelised.
8These include numbered arguments (A0 to A4), adjuncts
(ADV, DIS, LOC, MNR, MOD, NEG, PNC, TMP), and references
(R-A0 and R-A1).
215
Precision Recall F?=1
Development 71.82% 61.60% 66.32
Test WSJ 75.05% 64.81% 69.56
Test Brown 66.69% 52.14% 58.52
Test WSJ+Brown 74.02% 63.12% 68.13
Test WSJ Precision Recall F?=1
Overall 75.05% 64.81% 69.56
A0 78.52% 72.52% 75.40
A1 75.53% 65.39% 70.10
A2 62.28% 52.07% 56.72
A3 63.81% 38.73% 48.20
A4 73.03% 63.73% 68.06
A5 0.00% 0.00% 0.00
AM-ADV 60.00% 42.69% 49.88
AM-CAU 0.00% 0.00% 0.00
AM-DIR 0.00% 0.00% 0.00
AM-DIS 75.97% 73.12% 74.52
AM-EXT 0.00% 0.00% 0.00
AM-LOC 54.09% 47.38% 50.51
AM-MNR 58.67% 46.22% 51.71
AM-MOD 97.43% 96.37% 96.90
AM-NEG 97.78% 95.65% 96.70
AM-PNC 42.17% 30.43% 35.35
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 75.41% 71.11% 73.20
R-A0 82.09% 73.66% 77.65
R-A1 72.03% 66.03% 68.90
R-A2 0.00% 0.00% 0.00
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 98.63% 98.63% 98.63
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
replace the second occurrence.
3 Results
Table 1 shows the results on the test set. Problems
are inherently related with the skewed distribution of
role classes, so that roles which have a limited num-
ber of occurrences are harder to classify correctly.
This explains the performance gap on the A0 and
A1 roles on one hand, and the A2, A3, A4, AM- ar-
guments on the other.
One advantage of using a decision tree learning
algorithm is that it outputs a model which includes a
feature ranking, since the most informative features
are those close to the root of the tree. In the present
case, the most informative features were both dis-
tance/position metrics (distance and adjacency) and
lexicalized features (head word and predicate).
4 Conclusion
Semantic role labeling is a difficult task, and accord-
ingly, how to achieve an accurate and robust perfor-
mance is still an open question. In our work we
used a limited set of syntactic tree based distance
and size metrics coupled with raw lexical statistics,
and showed that such ?lazy learning? configuration
can still achieve a reasonable performance.
We concentrated on reducing the complexity
given by the number and dimensionality of the in-
stances to be classified during learning. This is the
core motivation behind performing tree pruning and
statistical feature encoding. This also helped us to
avoid the use of sparse features such as the explicit
path in the parse tree between the candidate con-
stituent and the predicate, and the predicate?s sub-
categorization rule (cf. e.g. Pradhan et al (2004)).
Future work will concentrate on benchmarking
this approach within alternative architectures (i.e.
two-phase with filtering) and different learning
schemes (i.e. vector-based methods such as Support
Vector Machines and Artificial Neural Networks).
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.003.2004).
References
Carreras, Xavier & Llu??s Ma`rquez (2005). Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. In Pro-
ceedings of CoNLL-2005.
Collins, Michael (1999). Head-driven statistical models for nat-
ural language parsing, (Ph.D. thesis). Philadelphia, Penn.,
USA: University of Pennsylvania.
Gildea, Daniel & Daniel Jurafsky (2002). Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?288.
Palmer, Martha, Dan Gildea & Paul Kingsbury (2005). The
proposition bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?105.
Pradhan, Sameer, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin & Daniel Jurafsky (2004). Support vec-
tor learning for semantic argument classification. Journal
of Machine Learning, Special issue on Speech and Natural
Language Processing. To appear.
Quinlan, J. Ross (1993). C4.5: programs for machine learn-
ing. San Francisco, Cal., USA: Morgan Kaufmann Publish-
ers Inc.
216
Discrete Optimization as an Alternative to Sequential Processing in NLG
Tomasz Marciniak and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We present an NLG system that uses Integer Lin-
ear Programming to integrate different decisions
involved in the generation process. Our approach
provides an alternative to pipeline-based sequential
processing which has become prevalent in today?s
NLG applications.
1 Introduction
From an engineering perspective, one of the major consid-
erations in building a Natural Language Generation (NLG)
system is the choice of the architecture. Two important issues
that need to be considered at this stage are firstly, the modu-
larization of the linguistic decisions involved in the genera-
tion process and secondly, the processing flow (cf. [De Smedt
et al, 1996]).
On one side of the spectrum lie integrated systems, with
all linguistic decisions being handled within a single process
(e.g. [Appelt, 1985]). Such architectures are theoretically at-
tractive, as they assume a close coordination of different types
of linguistic decisions, which are known to be dependent on
one another (cf. e.g. [Danlos, 1984]). A major disadvantage
of integrated models is the complexity that they necessarily
involve, which results in poor portability and scalability. On
the other side of the spectrum there are highly modularized
pipeline architectures. A prominent example of this second
case is the consensus pipeline architecture recognized by [Re-
iter, 1994] and further elaborated in [Reiter and Dale, 2000].
The modularization of Reiter?s model occurs at two levels.
First, individual linguistic decisions of the same type (e.g.
involving lexical or syntactic choice) are grouped together
within single low level tasks, such as lexicalization, aggre-
gation or ordering. Second, tasks are allocated to three high-
level generation stages, i.e. Document Planning, Microplan-
ning and Surface Realization. The processing flow in the
pipeline architecture is sequential, with individual tasks be-
ing executed in a predetermined order.
A study of applied NLG systems [Cahill and Reape, 1999]
reveals, however, that while most applied NLG systems rely
on sequential processing, they do not follow the strict modu-
larization that the consensus model assumes. Low-level tasks
are spread over various generation stages and may in fact be
executed more than once at diverse positions in the pipeline.
An attempt to account for commonalities that many NLG
systems share, without imposing too many restrictions, as is
the case with Reiter?s ?consensus? model, is the Reference
Architecture for Generation Systems (RAGS) [Mellish et al,
2004]. RAGS is an abstract specification of an NLG architec-
ture that focuses on two issues: data types that the generation
process manipulates and a generic model of the interactions
between modules, based on a common central server. An
important feature of RAGS is that it leaves the question of
processing flow to the actual implementation. Hence it is the-
oretically possible to build both fully integrated as well as
pipeline-based systems that would observe the RAGS princi-
ples. Two implementations of RAGS presented in [Mellish
and Evans, 2004] demonstrate an intermediate way.
In this paper we present a novel approach to building an
integrated NLG system, in which the generation process is
modeled as a discrete optimization problem. It provides an
extension to the classification-based generation framework,
presented in [Marciniak and Strube, 2004]. We first assume
modularization of the generation process at the lowest possi-
ble level: individual tasks correspond to realizations of sin-
gle form elements (FEs) that build up a linguistic expression.
The decisions that these tasks involve are then represented
as classification tasks and integrated via an Integer Linear
Programming (ILP) formulation (see e.g. [Nemhauser and
Wolsey, 1999]. This way we avoid the well known ordering
problem that is present in all pipeline-based systems. Observ-
ing, at least partially, the methodological principles of RAGS,
we specify the architecture of our system at two independent
levels. At the abstract level, the low-level generation tasks
are defined, all based on the same input/output interface. At
the implementation level, the processing flow and integration
method are determined.
The rest of the paper is organized as follows: in Section 2
we present briefly the classification-based generation frame-
work and remark on the shortcomings of pipeline-based pro-
cessing. In Section 3 we introduce the ILP formulation of the
generation task, and in Section 4 we report on the experiments
and evaluation of the system.
2 Classification-Based Generation
In informal terms, classification can be characterized as the
task of assigning a class label to an unknown instance, given a
set of its properties and attributes represented as a feature vec-
??
?
?
?
?
?
?
conn:
2
adj_dir_dsc:
adj_rank_dsc:
right
and3
1
4
prep_lex:
phr_type:
adj_rank_phr:
along
PP
1
1 verb_lex:
s_exp: VP
verb_form: bare_inf
continue
continue
V
VP
Dc
prep_lex:
phr_type:
adj_rank_phr: 2
to
PP
2
3
conn:
adj_rank_dsc: 1
null
adj_dir_dsc: left
Dc
4
conn:
adj_rank_dsc: ?
null
adj_dir_dsc:  ?
Dc
null
2
CONN
Dc
Dc
Dc
*
*
Dc
CONN
Dc
Dc
Dc
CONN
Dc
Dc
Dc
and
continue ...andfacing ...null
continue ...
*
CONN
Dc
Dc
Dc
*Dc
facing ...null
VP
along the road
to the sports ...
*
*
Dc
VP
VP
along the road
V
VP
continue
VP
VP
*
*
PP
to the sports ...
Dc
CONN Dc
null turn ...
Dc
CONN Dc
turn ...
PP PP
PP
Figure 1: LTAG-based derivation at the clause (left) and discourse levels (right). Elementary trees are represented as feature
vectors. Adjunction operations are marked with dashed arrows.
tor. In recent years supervised machine learning methods re-
lying on pre-classified training data have been applied in var-
ious areas of NLP to solve tasks formulated as classification
problems. In NLG machine learning methods have been used
to solve single tasks such as content selection and ordering
(e.g. [Duboue, 2004; Dimitromanolaki and Androutsopoulos,
2003]), lexicalization (e.g. [Reiter and Sripada, 2004]) and
referring expressions generation (e.g. [Cheng et al, 2001]).
In these applications classifiers trained on labeled data
have proven more robust and efficient than approaches us-
ing explicit expert knowledge. The difficulty of formaliz-
ing the linguistic knowledge involved in the development
of a knowledge-based system (a.k.a. knowledge-acquisition-
bottleneck) has been replaced with an effort of obtaining the
right kind of data, which typically involves annotating man-
ually a corpus of relevant texts with the required linguistic
information (cf. [Daelemans, 1993]).
The classification-based generation framework that we in-
troduced in [Marciniak and Strube, 2004] is based on a simple
idea that the linguistic form of an expression can be decom-
posed into a set of discrete form elements (FEs) representing
both its syntactic and lexical properties. The generation pro-
cess is then modeled as a series of classification tasks that re-
alize individual FEs. Realization of each FE is then regarded
as a single low-level generation task.
2.1 Route Directions
As the main application for this work we consider the task of
generating natural language route directions. An example of
such a text is given below:
(a) Facing the Wildcat statue, (b) turn left on the
brick sidewalk (c) and continue along the road to
the Sports Complex. (d) Make a right onto Concord
Road, (e) and keep going straight, (f) passing Pres-
byterian Church on your left, (g) until you reach
Copeland Street. (h) The library building will be
just around the corner on your right.
We analyze the content of instructional texts of this kind in
terms of temporally related situations, i.e. actions (b, c, d, e)
states (a, h) and events (f, g), denoted by individual discourse
units. The temporal structure of the texts is then modeled as a
tree, with nodes representing individual situation descriptions
and edges signaling the relations (see Figure 2). The seman-
tics of each discourse unit is further represented as a feature
vector describing the aspectual category and frame structure
of the profiled situation. This tree-based representation of the
semantic content of route directions constitutes the input to
the generation process. A detailed description of the underly-
ing conceptual model and the annotation process is presented
in [Marciniak and Strube, 2005].
initial subsequent
turn left on the brick sidewalk
Facing the Wildcat statue and continue along the road ...
subsequent
Make a right onto Concord Road
subsequent
and keep going straight
ongoing ongoing
subsequent
until you reach Copeland Street
The library building will be ...
passing Presbyterian Church ...
Figure 2: Temporal Structure
2.2 From LTAG to Form Elements
To specify an inventory of FEs that would become objects of
the low-level generation tasks, we first apply the Lexicalized
Tree Adjoining Grammar (LTAG) formalism (see e.g. [Joshi
and Schabes, 1991]) to model the linguistic form of the texts.
In LTAG, the derivation of a linguistic structure starts with a
selection of elementary trees, anchored by lexical items, such
as verbs or prepositions at the clause level and discourse con-
nectives at the discourse level (cf. [Webber and Joshi, 1998]).
In the next step, elementary trees are put together by means
of adjunction operations that follow the dependency structure
provided by the derivation tree. We take the temporal struc-
ture from Figure 2 to constitute the discourse level derivation
tree, with the temporal relationships corresponding to the syn-
tactic dependencies. At the clause level, the derivation tree is
isomorphic with the frame-based ontological representation
of individual situations (see [Marciniak and Strube, 2005]).
The clause- and discourse-level derivation of discourse unit
(c) from the above example in the context of (a) and (b) is
depicted in Figure 1. At the clause level, the set of elemen-
tary trees includes one initial tree ?1 anchored by the main
verb, which also specifies the syntactic frame of the clause,
and auxiliary trees ?1 and ?2 corresponding to the verb argu-
ments. At the discourse level, the discourse unit which occu-
pies the root position in the temporal structure (cf. Figure 2)
Adj. Rank Adj. Dir. Conn. S Exp. Verb Lex. Verb Form
1 right and VP continue Bare Inf.
Phr. Type1 Prep. Lex.1 Adj. Rank1 Phr. Type2 Prep. Lex.2 Adj. Rank2
PP along 1 PP to 2
Table 1: FEs based form representation of and continue along
the road to the sport complex.
is modeled as the initial tree ?2, and auxiliary trees ?3 and ?4
represent the remaining discourse units.
To model the whole process in a uniform way we en-
code the elementary trees as feature vectors, with individ-
ual features conveying syntactic (e.g. s exp) and lexical (e.g.
verb lex) information. Features adj rank and adj dir denote
respectively the ordering of the adjunction operations and the
adjunction direction, which both determine the linear struc-
ture of the text. Hence the form of the whole discourse can
be represented in terms of feature-value pairs used to encode
the initial trees and the derivation process. On that basis we
define a set of form elements building up a discourse as di-
rectly corresponding to the individual features. A detailed
description of the FEs is given below:
FE1: Adjunction Rank / Disc. Level specifies the linear
rank of each discourse unit at the local level, i.e. only clauses
temporally related to the same parent clause are considered.
FE2: Adjunction Direction is concerned with the position
of the child discourse unit relative to the parent one (e.g. (a)
left of (b), (c) right of (b), etc.).
FE3: Connective determines the lexical form of the dis-
course connective (e.g. null in (a), until in (g)).
FE4: S Expansion specifies whether a given discourse unit
is realized as a clause with the explicit subject (i.e. np+vp
expansion of the root S node in a clause) (e.g. (g, h)) or not
(e.g. (a), (b)).
FE5: Verb Form denotes the form of the main verb in a
clause (e.g. gerund in (a), (c), bare infinitive in (b), finite
present in (g), etc.).
FE6: Verb Lex. specifies the lexical form of the main verb
(e.g. turn in (b), pass in (f) or reach in (g)).
FE7: Phrase Type determines for each argument in a clause
its syntactic realization as a noun phrase (NP) , prepositional
phrase (PP) or a particle (P).
FE8: Preposition Lex. is concerned with the choice of a
lexical form for prepositions or particles in argument phrases
(e.g. left and on in (b) or along and to in (c)). If the value of
FE7 is NP, then this FE is set to none.
FE9: Adjunction Rank / Phr. Level specifies the linear
rank of each verb argument within a clause.
As an example, consider the FEs-based representation of
the form of clause (c) presented in Table 1. Realization of
each FEi is represented as a classification task Ti, with a set
of possible class labels corresponding to the different forms
that FEi may take. Only tasks T1 and T9 associated re-
spectively with Adjunction Rank / Disc. Level and Adjunction
Rank / Phr. Level are split into a series of binary precedence
classifications that determine the relative position of two dis-
course units or phrasal arguments at a time (e.g. (a) ? (c), (c)
? (d), and similarly along the road ? to the sports complex
etc.). These partial results are later combined to determine
the rank of the respective constituents.
Arguably, the above FEs and the corresponding tasks are
independent of the underlying grammatical model. In this
work we use the abstraction of the grammatical structure pro-
vided by LTAG, but the same or a similar set of FEs can
be readily derived from other formalisms (cf. e.g. [Meteer,
1990]). The role of the grammatical theory in defining form
elements is twofold. First, it specifies the exact position of
individual FEs in the grammatical structure, making it clear
how they should be assembled. Second, it ensures a wide
coverage: although the linguistic structures that we consider
here are relatively simple, the use of LTAG as the underlying
grammatical formalism guarantees that our generation frame-
work can be applied to producing much more complex con-
structions, both at the clause and discourse levels. Appar-
ently, this would require a richer feature vector representation
of the initial trees, and hence a larger number of FEs and the
corresponding generation tasks. The basic principles of the
generation process, however, would remain unchanged.
Notice also that the tasks considered here can be grouped
under the conventional NLG labels, such as text structuring
(i.e. T1, T2), lexicalization (i.e. T3, T6, T8) and sentence re-
alization (i.e. T4, T5, T9). Yet another important NLG task,
i.e. aggregation appears to be handled indirectly by T3 (e.g.
Turn left. Continue along the road. vs. Turn left and con-
tinue along the road.) and T5 (e.g. Keep going straight. You
will pass the Presbyterian Church on your right. vs. Keep go-
ing straight, passing the Presbyterian Church on your right.).
We view it as the strength of our approach that regardless of
their different linguistic character all these tasks are modeled
in exactly the same way.
2.3 System Architecture and Sequential
Processing
At an abstract level, the architecture of our system consists
of an unordered set of classifiers solving individual genera-
tion tasks. Each classifier is trained on a separate set of data
obtained from the corpus of route directions annotated with
both semantic and grammatical information.
In the previous work [Marciniak and Strube, 2004] we fol-
lowed the sequential paradigm advocated by [Daelemans and
van den Bosch, 1998] and implemented the system as a cas-
cade of classifiers. In such systems the output representation
is built incrementally, with subsequent classifiers having ac-
cess to the outputs of previous modules. An important char-
acteristic of this model is its extensibility. Since classifiers
rely on a uniform representation of the input (i.e. a feature
vector) and the output (i.e. a single feature value), it is easy to
change the ordering or insert new modules at any place in the
pipeline. Both operations only require retraining classifiers
with a new selection of the input features.
A major problem that we faced was that we found no
satisfactory method to determine the right ordering of in-
dividual classifiers that would guarantee optimal realization
of the grammatical form of the generated expression. We
found out that no matter what ordering we adopted tasks
that were solved at the begining had a lower accuracy as the
necessary contextual information, i.e. based on the outcomes
from other tasks, was missing. At the same time, subsequent
Start
l n1 l n2
l 22l 21
l 11 l 12
l
nnm
l 22m
l1m1 1T
T2
Tn
c(l    )11
c(l
   )12
c(l     )1m1
c(l    )22
2m2
c(l     )c(l   )21
...
...
...
.
.
.
.
.
.
.
.
.
.
.
.
..
.
...
Figure 3: Sequential processing as a graph.
tasks were influenced by the initial decisions, which in some
cases led to error propagation. Apparently, this was due to
the well known fact that elements of the linguistic structure
are strongly correlated with one another (see e.g. [Danlos,
1984]). Hence individual generation decisions should not be
handled in isolation and arranging them in a fixed order will
always involve a specific ordering bias.
To get a feeling for the limitations that sequential process-
ing of generation tasks involves, consider its graphical repre-
sentation in Figure 3. The process corresponds to the best-
first traversal of a weighted multi-layered lattice. Separate
layers T1, ..., Tn correspond to the individual tasks, and the
nodes at each layer (li1, ..., limi) represent class labels for
each task1. In the sequential model only transitions between
nodes belonging to subsequent layers are granted. Each such
transition is augmented with a transition cost, which may be
affected by the traversal history but does not consider the fu-
ture choices. Nodes selected in this process represent the out-
comes of individual tasks. As can be seen, the process is lo-
cally driven and it does not guarantee an optimal realization
of the tasks.
As an example consider three interrelated form elements:
Connective, S Exp. and Verb Form and their different real-
izations presented in Table 2. Apparently each of these FEs
has the potential to affect the overall meaning of the discourse
unit or its stylistics. It can also be seen that only certain com-
binations of different forms are allowed in the given semantic
context. Different realization of any of these FEs would re-
quire other elements to be changed accordingly. To conclude,
following Danlos? observation, we see no a priori reason to
impose any fixed ordering on the respective generation tasks,
and the experiments that we describe in Section 4 support this
position.
3 Discrete Optimization Model
As an alternative to sequential ordering of the generation
tasks we consider the metric labeling problem formulated by
[Kleinberg and Tardos, 2000], and originally applied in an
1Since different generation tasks may have varying numbers of
labels we denote the cardinality of Li, i.e. the set of possible labels
for task Ti, as mi.
Discourse Unit FE3 FE4 FE5
Pass the First Union Bank ... null vp bare inf.
It is necessary that you pass ... null np+vp bare inf.
Passing the First Union Bank ... null vp gerund
After passing the First Union Bank ... after vp gerund
After your passing . . . after np+vp gerund
As you pass the First Union Bank ... as np+vp fin. pres.
Until you pass the First Union Bank ... until np+vp fin. pres.
Until passing . . . until vp gerund
Table 2: Different realizations of form elements: Connective,
Verb Form and S Expansion. Rare but correct constructions
are in italics.
image restoration application, where classifiers determine the
?true? intensity values of individual pixels. This task is for-
mulated as a labeling function f : P ? L which maps a
set P of n objects onto a set L of m possible labels. The
goal is to find an assignment that minimizes the overall cost
function Q(f) which has two components: assignment costs,
i.e. the costs of selecting a particular label for individual ob-
jects, and separation costs, i.e. the costs of selecting a pair
of labels for two related objects2. [Chekuri et al, 2001] pro-
posed an integer linear programming (ILP) formulation of the
metric labeling problem, with both assignment cost and sep-
aration costs being modeled as binary variables of the linear
cost function.
Recently, [Roth and Yih, 2004] applied an ILP model to
the task of the simultaneous assignment of semantic roles to
the entities mentioned in a sentence and recognition of the
relations holding between them. The assignment costs were
calculated on the basis of predictions of basic classifiers, i.e.
trained for both tasks individually with no access to the out-
comes of the other task. The separation costs were formulated
in terms of binary constraints which specified whether a spe-
cific semantic role could occur in a given relation, or not.
In the remainder of this paper, we present a more general
model, which we apply to the generation tasks presented in
Section 2. We put no limits on the number of tasks being
solved, and express the separation costs as stochastic con-
straints, which can be calculated off-line from the available
linguistic data.
3.1 ILP Formulation
We consider a general context in which the generation process
comprises a range of linguistic decisions modeled as a set of n
classification tasks T = {T1, ..., Tn} which potentially form
mutually related pairs.
Each task Ti consists in assigning a label from Li =
{li1, ..., limi} to an instance that represents the particular de-
cision. Assignments are modeled as variables of a linear
cost function. We differentiate between simple variables that
model individual assignments of labels and compound vari-
ables that represent respective assignments for each pair of
related tasks.
To represent individual assignments the following proce-
dure is applied: for each task Ti, every label from Li is asso-
2These costs were calculated as the function of the metric dis-
tance between a pair of pixels and the difference in intensity.
ciated with a binary variable x(lij). Each such variable rep-
resents a binary choice, i.e. a respective label lij is selected if
x(lij) = 1 or rejected otherwise. The coefficient of variable
x(lij) which models the assignment cost c(lij) is given by:
c(lij) = ?log2(p(lij))
where p(lij) is the probability of lij being selected as the out-
come of task Ti. The probability distribution for each task
is provided by the basic classifiers that do not consider the
outcomes of other tasks3.
The role of compound variables is to provide pairwise con-
straints on the outcomes of individual tasks. Since we are
interested in constraining only those tasks are that truly de-
pendent on one another we first apply the contingency coeffi-
cient C to measure the degree of correlation for each pair of
tasks4. In the case of tasks Ti and Tk which are significantly
correlated, for each pair of labels fromLi?Lk we build a sin-
gle variable x(lij , lkp). Each such variable is associated with
a coefficient representing the constraint on the respective pair
of labels lij , lkp calculated in the following way:
c(lij , lkp) = ?log2(p(lij,lkp))
with p(lij,lkp) denoting the prior joint probability of labels
lij and lkp in the data, which is independent from the general
classification context and hence can be calculated off-line5.
The ILP model consists of the target function and a set of
constraints which block illegal assignments (e.g. only one la-
bel of the given task can be selected)6. In our case the target
function is the cost function Q(f), which we want to mini-
mize:
min Q(f) =
?
Ti?T
?
lij?Li
c(lij) ? x(lij)
+
?
Ti,Tk?T,i<k
?
<lij ,lkp>?Li?Lk
c(lij , lkp) ? x(lij , lkp)
Constraints need to be formulated for both the simple and
compound variables. First we want to ensure that exactly one
label lij belonging to task Ti is selected, i.e. only one simple
variable x(lij) representing labels of a given task can be set
to 1:
?
lij?Li
x(lij) = 1, ?i ? {1, ..., n}
3In this case the ordering of tasks is not necessary, and the clas-
sifiers can run independently from each other.
4C is a test for measuring the association of two nominal vari-
ables, and hence adequate for the type of tasks that we consider
here. The coefficient takes values from 0 (no correlation) to 1 (com-
plete correlation) and is calculated by the formula: C = (?2/(N +
?2))1/2, where ?2 is the chi-squared statistic and N the total num-
ber of instances. The significance of C is then determined from the
value of ?2 for the given data. See e.g. [Goodman and Kruskal,
1972].
5In Section 4 we discuss an alternative approach which considers
the actual input.
6For a detailed overview of linear programming and different
types of LP problems see e.g. [Nemhauser and Wolsey, 1999].
l 11
1T
Tn
l 21
T2
l n1
l 1m1
l 2m2
l nmn
...
...
...
c(l    )11
c(l    ,l     )11   2m
11   21
c(l    ,l    )
1m
   21
c(l    ,l    )
c(l    ,l 
    )
21   n
m
c(l     ,l     )
c(l 
   ,l
    )
21
   n
1
c(l
    
 ,l 
   )
2m
   
n1
2m
   n
m
c(l  
   ,l 
    )
c(l     )2m21c(l    )
c(l    )n1
c(l     )nm
1mc(l     )
1m   2m
Figure 4: Graph representation of the ILP model.
We also require that if two simple variables x(lij) and
x(lkp), modeling respectively labels lij and lkp, are set to
1, then the compound variable x(lij , lkp), which models co-
occurrence of these labels, is also set to 1. This is done in
two steps: we first ensure that if x(lij) = 1, then exactly one
variable x(lij , lkp) must also be set to 1:
x(lij)?
?
lkp?Lk
x(lij , lkp) = 0,
?i, k ? {1, ..., n}, i < k ? j ? {1, ..., mi}
and do the same for variable x(lkp):
x(lkp)?
?
lij?Li
x(lij , lkp) = 0,
?i, k ? {1, ..., n}, i < k ? p ? {1, ..., mk}
Finally, we constrain the values of both simple and com-
pound variables to be binary:
x(lij) ? {0, 1} ? x(lij , lkp) ? {0, 1},
?i, k ? {1, ..., n} ? j ? {1, ..., mi} ? p ? {1, ..., mk}
We can represent the decision process that our ILP model
involves as a graph, with the nodes corresponding to indi-
vidual labels and the edges marking the associations between
labels belonging to correlated tasks. In Figure 4, task T1 is
correlated with task T2 and task T2 with task Tn. No corre-
lation exists for pair T1, Tn. Both nodes and edges are aug-
mented with costs. The goal is to select a subset of connected
nodes, minimizing the overall cost, given that for each group
of nodes T1, T2, ..., Tn exactly one node must be selected,
and the selected nodes, representing correlated tasks, must
be connected. We can see that in contrast to the pipeline ap-
proach (cf. Figure 1), no local decisions determine the overall
assignment as the global distribution of costs is considered.
4 Experiments and Results
In order to evaluate our approach we conducted a series of
experiments with two implementations of the ILP model and
two different pipelines. Each system takes as input the tree-
based representation of the semantic content of route direc-
tions described in Section 2. The generation process traverses
the temporal tree in a depth-first fashion, and for each node a
single discourse unit is realized.
T :  Verb Form5
T :  Disc. Units Rank1
T :  Verb Lex6 4T :  S Exp.
T :  Connective3
T :  Disc. Units Dir.2
T :  Phrase Type7 T :  Prep. Lex8
T :  Phrase Rank9
Figure 5: Correlation network for the generation tasks.
null and as after until T3 Connective
T5 Verb Form
0.40 0.18 0 0 0 bare inf
0 0 0 0.04 0.01 gerund
0.05 0.01 0.06 0.03 0.06 fin pres
0.06 0.05 0 0 0 will inf
Table 3: Joint distribution matrix for selected labels of tasks
Connective (horizontal) and Verb Form (vertical), computed
for all discourse units in a corpus.
4.1 Correlations Between Tasks
We started with running the correlation tests for all pairs of
tasks. The obtained correlation network is presented in Fig-
ure 5. It is interesting to observe that tasks which realize FEs
belonging to the same levels of linguistic organization, and
have traditionally been handled within the same generation
stages (i.e. Text Planning, Microplanning and Realization) are
closely correlated with one another. This fact supports em-
pirically some assumptions behind Reiter?s consensus model.
On the other hand, there exist quite a few correlations that
extend over the stage boundaries, and all three lexicalization
tasks i.e. T3, T6 and T8 are correlated with many tasks of a
totally different linguistic character.
4.2 ILP Systems
We used the ILP model described in Section 3 to implement
two generation systems. To obtain assignment costs, both
systems get a probability distribution for each task from ba-
sic classifiers trained on the training data. To calculate the
separation costs, modeling the stochastic constraints on the
co-occurrence of labels, we considered correlated tasks only
(cf. Figure 5) and applied two calculation methods, which re-
sulted in two different system implementations.
In ILP1, for each pair of tasks we computed the joint distri-
bution of the respective labels considering all discourse units
in the training data before the actual input was known. Such
obtained joint distributions were used for generating all dis-
course units from the test data. An example matrix with joint
distribution for selected labels of tasks Connective and Verb
Form is given in Table 3. An advantage of this approach is
that the computation can be done in an offline mode and has
no impact on the run-time.
In ILP2, the joint distribution for a pair of tasks was cal-
culated at run-time, i.e. only after the actual input had been
null and as after until T3 Connective
T5 Verb Form
0.13 0.02 0 0 0 bare inf
0 0 0 0 0 gerund
0 0 0.05 0.02 0.27 fin pres
0.36 0.13 0 0 0 will inf
Table 4: Joint distribution matrix for tasks Connective and
Verb Form, considering only disc. units similar to (c): until
you see the river side in front of you, at Phi-threshold ? 0.8.
known. This time we did not consider all discourse units in
the training data, but only those whose meaning, represented
as a feature vector, was similar to the meaning of the input
discourse unit. As a similarity metric we used the Phi co-
efficient7, and set the similarity threshold at 0.8. As can be
seen from Table 4, the probability distribution computed in
this way is better suited to the specific semantic context. This
is especially important if the available corpus is small and the
frequency of certain pairs of labels might be too low to have
a significant impact on the final assignment.
4.3 Pipeline Systems
As a baseline we implemented two pipeline systems. In the
first one we used the ordering of tasks that resembles most
closely the standard NLG pipeline and which we also used
before in [Marciniak and Strube, 2004]8.
Individual classifiers had access to both the semantic fea-
tures, and the features output by the previous modules. To
train the classifiers, the correct feature values were extracted
from the training data and during testing the generated, and
hence possibly erroneous, values were taken.
In the other pipeline system we wanted to minimize the
error-propagation effect and placed the tasks in the order of
decreasing accuracy. To determine the ordering of tasks we
applied the following procedure: the classifier with the high-
est baseline accuracy was selected as the first one. The re-
maining classifiers were trained and tested again, but this time
they had access to the additional feature. Again, the classifier
with the highest accuracy was selected and the procedure was
repeated until all classifiers were ordered.
4.4 Evaluation
We evaluated our system using leave-one-out cross-
validation, i.e. for all texts in the corpus, each text was used
once for testing, and the remaining texts provided the training
data. To solve individual classification tasks we used the de-
cision tree learner C4.5 in the pipeline systems and the Naive
Bayes algorithm9 in the ILP systems. Both learning schemes
7Phi is a measure of the extent of correlation between two sets
of binary variables, see e.g. [Edwards, 1976]. To represent multi-
class features on a binary scale we applied dummy coding which
transforms multi class-nominal variables to a set of dummy variables
with binary values.
8The ordering of tasks is given in Table 5.
9Both implemented in the Weka machine learning software [Wit-
ten and Frank, 2000].
Pipeline 1 Pipeline 2 ILP 1 ILP 2
Tasks Pos. Accuracy ? Pos. Accuracy ? Accuracy ? Accuracy ?
Dis.Un. Rank 1 96.81% 90.90% 2 96.81% 90.90% 97.43% 92.66% 97.43% 92.66%
Dis.Un. Pos. 2 98.04% 89.64% 1 98.04% 89.64% 96.10% 77.19% 97.95% 89.05%
Connective 3 78.64% 60.33% 8 79.10% 61.14% 79.15% 61.22% 79.36% 61.31%
S Exp. 4 95.90% 89.45% 3 96.20% 90.17% 99.48% 98.65% 99.49% 98.65%
Verb Form 5 86.76% 77.01% 4 87.83% 78.90% 92.81% 87.60% 93.22% 88.30%
Verb Lex. 6 64.58% 60.87% 9 67.40% 64.19% 75.87% 73.69% 76.08% 74.00%
Phr. Type 7 86.93% 75.07% 5 87.08% 75.36% 87.33% 76.75% 88.03% 77.17%
Prep. Lex. 8 86.23% 81.12% 6 86.03% 81.10% 87.28% 82.20% 88.59% 83.24%
Phr. Rank 9 84.73% 75.24% 7 86.95% 78.65% 90.22% 84.02% 91.27% 85.72%
Phi 0.85 0.87 0.89 0.90
Table 5: Results reached by the implemented ILP systems and two baselines. For both pipeline systems, Pos. stands for the
position of the tasks in the pipeline.
yielded highest results in the respective configurations10. To
solve the ILP models we used lp solve, a highly efficient
GNU-licence Mixed Integer Programming (MIP) solver11,
that implements the Branch-and-Bound algorithm. For each
task we applied a feature selection procedure (cf. [Kohavi and
John, 1997]) to determine which semantic features should be
taken as the input by the basic classifiers.
To evaluate individual tasks we applied two metrics: accu-
racy, calculated as the proportion of correct classifications to
the total number of instances, and the ? statistic, which cor-
rects for the proportion of classifications that might occur by
chance. For end-to-end evaluation, we applied the Phi coef-
ficient to measure the degree of similarity between the vector
representations of the generated form (i.e. built from the out-
comes of individual tasks) and the reference form obtained
from the test data. The Phi-based similarity metric is simi-
lar to ? as it compensates for the fact that a match between
two multi-label features is more difficult to obtain than in the
case of binary features. This measure tells us how well all the
tasks have been solved together, which in our case amounts
to generating the whole text.
The results presented in Table 5 show that the ILP systems
achieved highest accuracy and ? for most tasks and reached
the highest overall Phi score. Notice that ILP2 improved the
accuracy of both pipeline systems for the three correlated
tasks that we discussed before, i.e. Connective, S Exp. and
Verb Form. Another group of correlated tasks for which the
results appear interesting are i.e. Verb Lex., Phrase Type and
Phrase Rank (cf. Figure 3). Notice that Verb Lex. got higher
scores in Pipeline2, with outputs from both Phrase Type and
Phrase Rank (see the respective pipeline positions), but the re-
verse effect did not occur: scores for both phrase tasks were
lower in Pipeline1 when they had access to the output from
Verb Lex., contrary to what we might expect. Apparently, this
was due to the low accuracy for Verb Lex. which caused the
10We have found that in direct comparison C4.5 performs better
than Naive Bayes but the probability distribution that it outputs is
strongly biased towards the winning label. In this case it is practi-
cally impossible for the ILP system to change the classifier?s deci-
sion, as the costs of other labels get extremely high. Hence the more
balanced probability distribution given by Naive Bayes can be easier
corrected in the optimization process.
11http://www.geocities.com/lpsolve/
already mentioned error propagation. This example shows
well the advantage that optimization processing brings: both
ILP systems reached much higher scores for all three tasks.
Finally, it appears as no coincidence that the three tasks in-
volving lexical choice, i.e. Connective, Verb Lex. and Prepo-
sition Lex. scored lower than the syntactic tasks in all sys-
tems. This can be attributed partially to the limitations of
retrieval measures which do not allow for the fact, that in a
given semantic content more than one lexical form can be ap-
propriate.
5 Conclusions
In this paper we showed that the pipeline architecture in an
NLG application can be successfully replaced with an inte-
grated ILP-based model which is better suited to handling
correlated generation decisions. To the best of our knowl-
edge, linear programming has been used in an NLG related
work only by [Althaus et al, 2004] to solve a single task of
determining the order of discourse constituents. In a some-
what related context [Dras, 1999] used ILP to optimize the
task of text paraphrasing, given global constraints such as text
and sentence length, readibilty, etc.
In contrast, in this work we use an ILP model to orga-
nize the entire process of generating the surface form from
an underlying semantic representation, which involves an
integration of different types of NLG tasks. Although in
our system we use machine learning as the primary deci-
sion making mechanism, we believe that the ILP model can
also be used with knowledge-based systems that observe the
classification-oriented formulation of the NLG tasks.
Finally, we are convinced that an adequate evaluation of an
NLG system must at some stage go beyond the application of
quantitative measures. Nevertheless, it is reasonable to expect
that the improvement that we reached with the ILP system,
especially the increase of the overall Phi score, must correlate
to some extent with the quality improvement. To verify it
we are currently proceeding with qualitative evaluation of the
output from our system.
Acknowledgements: The work presented here has been
funded by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author receives a scholarship from KTF
(09.001.2004).
References
[Althaus et al, 2004] Ernst Althaus, Nikiforos Karamanis, and
Alexander Koller. Computing locally coherent discourses. In
Proceedings of the 42nd Annual Meeting of the Association for
Computational Linguistics, Barcelona, Spain, July 21-26, 2004,
pages 399?406, 2004.
[Appelt, 1985] Douglas Appelt. Planning English Sentences. Cam-
bridge University Press, Cambridge, UK, 1985.
[Cahill and Reape, 1999] Lynne Cahill and Mike Reape. Compo-
nent tasks in applied NLG systems. Technical Report ITRI-99-
05, ITRI, University of Brighton, March 1999.
[Chekuri et al, 2001] Chandra Chekuri, Sanjeev Khanna, Joseph
Naor, and Leonid Zosin. Approximation algorithms for the met-
ric labeling problem via a new linear programming formulation.
In Proceedings of the 12th Annual ACM SIAM Symposium on
Discrete Algorithms, Washington, DC, pages 109?118, 2001.
[Cheng et al, 2001] Hua Cheng, Massimo Poesio, Renate Hen-
schel, and Chris Mellish. Corpus-based NP modifier generation.
In Proceedings of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics, Pittsburgh,
PA, 2-7 June, 2001, pages 9?16, 2001.
[Daelemans and van den Bosch, 1998] Walter Daelemans and An-
tal van den Bosch. Rapid development of NLP modules with
memory-based learning. In Proceedings of ELSNET in Wonder-
land. Utrecht: ELSNET, pages 105?113, 1998.
[Daelemans, 1993] Walter Daelemans. Memory-based lexical ac-
quisition and processing. In Proceedings of the Third Interna-
tional EAMT Workshop on Machine Translation and the Lexicon,
Heidelberg, Germany, 26-28 April, 1993, pages 85?98, 1993.
[Danlos, 1984] Laurence Danlos. Conceptual and linguistic deci-
sions in generation. In Proceedings of the 10th International
Conference on Computational Linguistics, Stanford, Cal., pages
501?504, 1984.
[De Smedt et al, 1996] Koenraad De Smedt, Helmut Horacek, and
Michael Zock. Architectures for natural language generation:
Problems and perspectives. In G. Adorni and M. Zock, editors,
Trends in Natural Language Generation: An Artificial Intelli-
gence Perspective, pages 17?46. Springer Verlag, 1996.
[Dimitromanolaki and Androutsopoulos, 2003] Aggeliki Dimitro-
manolaki and Ion Androutsopoulos. Learning to order facts for
discourse planning in natural language generation. In Proc. of
the 9th European Workshop on Natural Language Generation,
Budapest, Hungary, 13 ? 14 April 2003, pages 23?30, 2003.
[Dras, 1999] Mark Dras. Tree Adjoining Grammar and the Reluc-
tant Paraphrasing of Text. PhD thesis, Macquarie University,
Australia, 1999.
[Duboue, 2004] Pablo A. Duboue. Indirect supervised learning
of content selection logic. In Proceedings of the 3rd Interna-
tional Conference on Natural Language Generation, Brocken-
hurst, UK, 14-16 July, 2004, pages 41?50, 2004.
[Edwards, 1976] Allen L. Edwards. An Introduction to Linear Re-
gression and Correlation. W. H. Freema, San Francisco, Cal.,
1976.
[Goodman and Kruskal, 1972] Leo A. Goodman and W. H.
Kruskal. Measures of association for cross-classification, iv.
Journal of the American Statistical Association, 67:415?421,
1972.
[Joshi and Schabes, 1991] Aravind K. Joshi and Yves Schabes.
Tree-adjoining grammars and lexicalized grammars. In Maurice
Nivat and Andreas Podelski, editors, Definability and Recogniz-
ability of Sets of Trees. Elsevier, 1991.
[Kleinberg and Tardos, 2000] Jon M. Kleinberg and Eva Tardos.
Approximation algorithms for classification problems with pair-
wise relationships: Metric labeling and Markov random fields.
Journal of the ACM, 49(5):616?639, 2000.
[Kohavi and John, 1997] Ron Kohavi and George H. John. Wrap-
pers for feature subset selection. Artificial Intelligence Journal,
97:273?324, 1997.
[Marciniak and Strube, 2004] Tomasz Marciniak and Michael
Strube. Classification-based generation using TAG. In Pro-
ceedings of the 3rd International Conference on Natural
Language Generation, Brockenhurst, UK, 14-16 July, 2004,
pages 100?109, 2004.
[Marciniak and Strube, 2005] Tomasz Marciniak and Michael
Strube. Modeling and annotating the semantics of route di-
rections. In Proceedings of the 6th International Workshop on
Computational Semantics, Tilburg, The Netherlands, January
12-14, 2005, pages 151?162, 2005.
[Mellish and Evans, 2004] Chris Mellish and Roger Evans. Imple-
mentation architectures for natural language generation. Natural
Language Engineering, 10(3/4):261?282, 2004.
[Mellish et al, 2004] Chris Mellish, Mike Reape, Donia Scott,
Lynne Cahill, Roger Evans, and Daniel Paiva. A reference archi-
tecture for generation systems. Natural Language Engineering,
10(3/4):227?260, 2004.
[Meteer, 1990] Marie W. Meteer. Abstract linguistic resources for
text planning. In Proceedings of the 5th International Workshop
on Natural Language Generation, Pittsburgh, PA, 3-6 June 1990,
pages 62?69, 1990.
[Nemhauser and Wolsey, 1999] George L. Nemhauser and Lau-
rence A. Wolsey. Integer and combinatorial optimization. Wiley,
New York, NY, 1999.
[Reiter and Dale, 2000] Ehud Reiter and Robert Dale. Building
Natural Language Generation Systems. Cambridge University
Press, Cambridge, UK, 2000.
[Reiter and Sripada, 2004] Ehud Reiter and Somayajulu Sripada.
Contextual influences on near-synonym choice. In Proceedings
of the 3rd International Conference on Natural Language Gen-
eration, Brockenhurst, UK, 14-16 July, 2004, pages 161?170,
2004.
[Reiter, 1994] Ehud Reiter. Has a consensus NL generation archi-
tecture appeared, and is it psycholinguistically plausible? In
Proc. of the 7th International Workshop on Natural Language
Generation, Kennebunkport, MA, 21-24 June 1994, pages 160?
173, 1994.
[Roth and Yih, 2004] Dan Roth and Wen-tau Yih. A linear pro-
gramming formulation for global inference in natural language
tasks. In Proceedings of the 8th Conference on Computational
Natural Language Learning, Boston, Mass., May 2-7, 2004,
pages 1?8, 2004.
[Webber and Joshi, 1998] Bonnie Lynn Webber and Aravind Joshi.
Anchoring a lexicalized tree-adjoining grammar for discourse. In
Proceedings of the COLING/ACL ?98 Workshop on Discourse
Relations and Discourse Markers, Montre?al, Que?bec, Canada,
15 August 1998, pages 86?92, 1998.
[Witten and Frank, 2000] Ian H. Witten and Eibe Frank. Data Min-
ing - Practical Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann, San Francisco, Cal., 2000.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 267?274,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Linguistically Motivated Features
for Paragraph Boundary Identification
Katja Filippova and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
In this paper we propose a machine-
learning approach to paragraph boundary
identification which utilizes linguistically
motivated features. We investigate the re-
lation between paragraph boundaries and
discourse cues, pronominalization and in-
formation structure. We test our algorithm
on German data and report improvements
over three baselines including a reimple-
mentation of Sporleder & Lapata?s (2006)
work on paragraph segmentation. An
analysis of the features? contribution sug-
gests an interpretation of what paragraph
boundaries indicate and what they depend
on.
1 Introduction
Our work is concerned with multi-document sum-
marization, namely with the merging of multiple
documents about the same topic taken from the
web. We view summarization as extraction of im-
portant sentences from the text. As a consequence
of the merging process the layout of the documents
is lost. In order to create the layout of the out-
put, the document structure (Power et al, 2003)
has to be regenerated. One aspect of this struc-
ture is of particular importance for our work: the
paragraph structure. In web documents paragraph
boundaries are used to anchor figures and illustra-
tions, so that the figures are always aligned with
the same paragraph even when the font size or the
window size is changed. Since we want to include
figures in the generated summaries, paragraph seg-
mentation is an important subtask in our applica-
tion.
Besides multi-document summarization of web
documents, paragraph boundary identification
(PBI) could be useful for a number of different ap-
plications, such as producing the layout for tran-
scripts provided by speech recognizers and opti-
cal character recognition systems, and determin-
ing the layout of documents generated for output
devices with different screen size.
Though related to the task of topic segmenta-
tion which stimulated a large number of studies
(Hearst, 1997; Choi, 2000; Galley et al, 2003,
inter alia), paragraph segmentation has not been
thoroughly investigated so far. We explain this by
the fact that paragraphs are considered a stylistic
phenomenon and that there is no unanimous opin-
ion on what the function of the paragraph is. Some
authors (Irmscher (1972) as cited by Stark (1988))
suggest that paragraph structure is arbitrary and
can not be determined based solely on the prop-
erties of the text. Still, psycholinguistic studies
report that humans agree, at least to some extent,
on placing boundaries between paragraphs. These
studies also note that paragraph boundaries are in-
formative and make the reader perceive paragraph-
initial sentences as being important (Stark, 1988).
In contrast to topic segmentation, paragraph seg-
mentation has the advantage that large amounts of
annotated data are readily availabe for supervised
learning.
In this paper we describe our approach to para-
graph segmentation. Previous work (Sporleder &
Lapata, 2004; 2006) mainly focused on superficial
and easily obtainable surface features like punctu-
ation, quotes, distance and words in the sentence.
Their approach was claimed to be domain- and
language-independent. Our hypothesis, however,
is that linguistically motivated features, which we
compute automatically, provide a better paragraph
segmentation than Sporleder & Lapata?s surface
ones, though our approach may loose some of the
267
domain-independence. We test our hypothesis on
a corpus of biographies downloaded from the Ger-
man Wikipedia1. The results we report in this pa-
per indicate that linguistically motivated features
outperform surface features significantly. It turned
out that pronominalization and information struc-
ture contribute to the determination of paragraph
boundaries while discourse cues have a negative
effect.
The paper is organized as follows: First, we de-
scribe related work in Section 2, then in Section
3 our data is introduced. The baselines, the ma-
chine learners, the features and the experimental
setup are given in Section 4. Section 5 reports and
discusses the results.
2 Related Work
Compared to other text segmentation tasks, e.g.
topic segmentation, PBI has received relatively lit-
tle attention. We are aware of three studies which
approach the problem from different perspectives.
Bolshakov & Gelbukh (2001) assume that split-
ting text into paragraphs is determined by text co-
hesion: The link between a paragraph initial sen-
tence and the preceding context is weaker than the
links between sentences within a paragraph. They
evaluate text cohesion using a database of collo-
cations and semantic links and insert paragraph
boundaries where the cohesion is low.
The algorithm of Sporleder & Lapata (2004,
2006) uses surface, syntactic and language model
features and is applied to three different languages
and three domains (fiction, news, parliament).
This study is of particular interest to us since one
of the languages the algorithm is tested on is Ger-
man. They investigate the impact of different fea-
tures and data size, and report results significantly
better than a simple baseline. However, their re-
sults vary considerably between the languages and
the domains. Also, the features determined impor-
tant is different for each setting. So, it may be
possible that Sporleder & Lapata do not provide
conclusive results.
Genzel (2005) considers lexical and syntactic
features and reports accuracy obtained from En-
glish fiction data as well as from the WSJ corpus.
He points out that lexical coherence and structural
features turn out to be the most useful for his algo-
rithm. Unfortunately, the only evaluation measure
he provides is accuracy which, for the PBI task,
1http://de.wikipedia.org
does not describe the performance of a system suf-
ficiently.
In comparison to the mentioned studies, our
goal is to examine the influence of cohesive fea-
tures on the choice of paragraph boundary inser-
tion. Unlike Bolshakov & Gelbukh (2001), who
have similar motivation but measure cohesion by
collocations, we explore the role of discourse cues,
pronominalization and information structure.
The task of topic segmentation is closely related
to the task of paragraph segmentation. If there
is a topic boundary, it is very likely that it coin-
cides with a paragraph boundary. However, the
reverse is not true and one topic can extend over
several paragraphs. So, if determined reliably,
topic boundaries could be used as high precision,
low recall predictors for paragraph boundaries.
Still, there is an important difference: While work
on topic segmentation mainly depends on content
words (Hearst, 1997) and relations between them
which are computed using lexical chains (Galley
et al, 2003), paragraph segmentation as a stylistic
phenomenon may depend equally likely on func-
tion words. Hence, paragraph segmentation is
a task which encompasses the traditional borders
between content and style.
3 Data
The data we used is a collection of biographies
from the German version of Wikipedia. We se-
lected all biographies under the Wikipedia cate-
gories of physicists, chemists, mathematicians and
biologists and obtained 970 texts with an average
length of 20 sentences and 413,776 tokens in total.
Although our corpus is substantially smaller
than the German corpora of Sporleder & Lapata
(2006), it should be big enough for a fair com-
parison between their algorithm and the algorithm
proposed here. Having investigated the effect of
the training size, Sporleder & Lapata (2006) came
to the conclusion that their system performs well
being trained on a small data set. In particular,
the learning curve for German shows an improve-
ment of only about 2% when the amount of train-
ing data is increased from 20%, which in case of
German fiction approximately equals 370,000 to-
kens, to 100%.
Fully automatic preprocessing in our system
comprises the following stages: First, a list of peo-
ple of a certain Wikipedia category is taken and
for every person an article is extracted The text
268
training development test
tokens 347,763 39,228 19,943
sentences 15,583 1,823 922
paragraphs 5,323 654 362
Table 1: Number of tokens and sentences per set
is purged from Wiki tags and comments, the in-
formation on subtitles and paragraph structure is
preserved. Second, sentence boundaries are iden-
tified with a Perl CPAN module2 whose perfor-
mance we improved by extending the list of abbre-
viations and modifying the output format. Next,
the sentences are split into tokens. The TnT tag-
ger (Brants, 2000) and the TreeTagger (Schmid,
1997) are used for tagging and lemmatizing. Fi-
nally, the texts are parsed with the CDG depen-
dency parser (Foth & Menzel, 2006). Thus, the
text is split on three levels: paragraphs, sentences
and tokens, and morphological and syntactic in-
formation is provided.
A publicly available list of about 300 discourse
connectives was downloaded from the Internet site
of the Institute for the German Language3 (Insti-
tut fu?r Deutsche Sprache, Mannheim) and slightly
extended. These are identified in the text and an-
notated automatically as well. Named entities are
classified according to their type using informa-
tion from Wikipedia: person, location, organiza-
tion or undefined. Given the peculiarity of our cor-
pus, we are able to identify all mentions of the bi-
ographee in the text by simple string matching. We
also annotate different types of referring expres-
sions (first, last, full name) and resolve anaphora
by linking personal pronouns to the biographee
provided that they match in number and gender.
The annotated corpus is split into training
(85%), development (10%) and testing (5%) sets.
Distribution of data among the three sets is pre-
sented in Table 1. Sentences which serve as sub-
titles in a text are filtered out because they make
identifying a paragraph boundary for the follow-
ing sentence trivial.
4 Experiments
4.1 Machine Learners
The PBI task was reformulated as a binary classifi-
cation problem: every training instance represent-
2http://search.cpan.org/?holsten/Lingua-DE-Sentence-
0.07/Sentence.pm
3http://hypermedia.ids-mannheim.de/index.html
ing a sentence was classified either as paragraph-
initial or not.
We used two machine learners: BoosTexter
(Schapire & Singer, 2000) and TiMBL (Daele-
mans et al, 2004). BoosTexter was developed
for text categorization, and combines simple rules
(decision stumps) in a boosting manner. Sporleder
& Lapata used this learner because it has the abil-
ity to combine many only moderately accurate
hypotheses. TiMBL is a memory-based learner
which classifies every test instance by finding the
most similar examples in the training set, hence it
does not abstract from the data and is well suited
to handle features with many values, e.g. the list
of discourse cues. For both classifiers, all experi-
ments were run with the default settings.
4.2 Baselines
We compared the performance of our algorithm
against three baselines. The first one (distance)
trivially inserts a paragraph break after each third
sentence, which is the average number of sen-
tences in a paragraph. The second baseline (Gal-
ley) hypothesizes that paragraph breaks coincide
with topic boundaries and utilizes Galley et al?s
(2003) topic boundary identification tool LCseg.
The third baseline (Sporleder) is a reimplementa-
tion of Sporleder & Lapata?s 2006 algorithm with
the following features:
Word and Sentence Distances from the current
sentence to the previous paragraph break;
Sentence Length and Relative Position (relPos)
of the sentence in a text;
Quotes encodes whether this and the previous
sentences contain a quotation, and whether
the quotation is continued in the current sen-
tence or not;
Final Punctuation of the previous sentence;
Words ? the first (word1), the first two (word2),
the first three and all words from the sen-
tence;
Parsed has positive value in case the sentence is
parsed, negative otherwise;
Number of S, VP, NP and PP nodes in the sen-
tence;
Signature is the sequence of PoS tags with and
without punctuation;
269
Children of Top-Level Nodes are two features
representing the sequence of syntactic labels
of the children of the root of the parse tree
and the children of the highest S-node;
Branching Factor features express the average
number of children of S, VP, NP and PP
nodes in the parse;
Tree Depth is the average length of the path from
the root to the leaves;
Per-word Entropy is a feature based on Gen-
zel & Charniak?s (2003) observation that
paragraph-initial sentences have lower en-
tropy than non-initial ones;
Sentence Probability according to a language
model computed from the training data;
Character-level n-gram models are built using
the CMU toolkit (Clarkson & Rosenfeld,
1997).
Since the parser we used produces dependency
trees as an output, we could not distinguish be-
tween such features as children of the root of the
tree and children of the top-level S-node. Apart
from this minor change, we reimplemented the al-
gorithm in every detail.
4.3 Our Features
For our algorithm we first selected the features of
Sporleder & Lapata?s (2006) system which per-
formed best on the development set. These are
relative position, the first and the first two words
(relPos, word1, word2). Quote and final punctu-
ation features, which were particularly helpful in
Sporleder & Lapata?s experiments on the German
fiction data, turned out to be superfluous given the
infrequency of quotations and the prevalent use of
the period as sentence delimiter in our data.
We experimented with text cohesion features as-
suming that the paragraph structure crucially de-
pends on cohesion and that paragraph breaks are
likely to occur between sentences where cohesive
links are weak. In order to estimate the degree of
cohesion, we looked at lexical cohesion, pronom-
inalization, discourse cues and information struc-
ture.
4.3.1 Lexical Cohesion
nounOver, verbOver: Similar to Sporleder &
Lapata (2006), we introduced an overlap fea-
ture, but measured the degree of overlap as
a number of common noun and verb lem-
mas between two adjacent sentences. We pre-
ferred lemmas over words in order to match
all possible forms of the same word in Ger-
man.
LCseg: Apart from the overlap, a boolean feature
based on LCseg (Galley et al, 2003) marked
whether the tool suggests that a new topic be-
gins with the current sentence. This feature,
relying on lexical chains, was supposed to
provide more fine-grained information on the
degree of similarity between two sentences.
4.3.2 Pronominalization
As Stark (1988) points out, humans tend to in-
terpret over-reference as a clue for the beginning
of a new paragraph: In a sentence, if a non-
pronominal reference is preferred over a pronom-
inal one where the pronoun would be admissi-
ble, humans are likely to mark this sentence as a
paragraph-initial one. In order to check whether
over-reference indeed correlates with paragraph-
initial sentences, we described the way the bi-
ographee is referred to in the current and the pre-
vious sentences.
prevSPerson, currSPerson: This feature4 with
the values NA, biographee, other indicates
whether there is a reference to the biographee
or some other person in the sentence.
prevSRE, currSRE: This feature describes the
biographee?s referring expression and has
three possible values: NA, name, pronoun.
Although our annotation distinguishes between
first, last and full names, we found out that, for
the PBI task, the distinction is spurious and unify-
ing these three under the same category improves
the results.
REchange: Since our classifiers assume feature
independence and can not infer the informa-
tion on the change in referring expression, we
explicitly encoded that information by merg-
ing the values of the previous feature for the
current and the preceding sentences into one,
which has nine possible values (name-name,
NA-name, pronoun-name, etc.).
4Prefixes prevS-, currS- stand for the previous and the
current sentences respectively.
270
4.3.3 Discourse Cues
The intuition behind these features is that cue
words and phrases are used to signal the relation
between the current sentence and the preceding
sentence or context (Mann & Thompson, 1988).
Such connectives as endlich (finally), abgesehen
davon (apart from that), danach (afterwards) ex-
plicitly mark a certain relation between the sen-
tence they occur in and the preceding context. We
hypothesize that the relations which hold across
paragraph boundaries should differ from those
which hold within paragraphs and that the same is
true for the discourse cues. Absence of a connec-
tive is supposed to be informative as well, being
more typical for paragraph-initial sentences.
Three features describe the connective of the
current sentence. Another three features describe
the one from the preceding sentence.
prevSCue, currSCue: This feature is the con-
nective itself (NA in case of none).
prevSCueClass, currSCueClass: This feature
represents the semantic class of the cue word
or phrase as assigned by the IDS Mannheim.
There are 25 values, including NA in case
of no connective, altogether, with the most
frequent values being temporal, concessive,
conclusive, etc.
prevSProCue, currSProCue: The third binary
feature marks whether the connective is
proadverbial or not (NA if there is no connec-
tive). Being anaphors, proadverbials, such as
deswegen (because of that), daru?ber (about
that) explicitly link a sentence to the preced-
ing one(s).
4.3.4 Information Structure
Information structure, which is in German to a
large extent expressed by word order, provides
additional clues to the degree of connectedness
between two sentences. In respect to the PBI
task, Stark (1988) reports that paragraph-initial
sentences are often theme-marking which means
that the subject of such sentences is not the first
element. Given the lower frequency of paragraph-
initial sentences, this feature can not be considered
reliable, but in combination with others it provides
an additional clue. In German, the first element
best corresponds to the prefield (Vorfeld) ? nor-
mally, the single constituent placed before the fi-
nite verb in the main clause.
currSVF encodes whether the constituent in
the prefield is a NP, PP, ADV, CARD, or
Sub.Clause. Values different from NP un-
ambiguously represent theme-marking sen-
tences, whereas the NP value may stand for
both: theme-marking as well as not theme-
marking sentence.
4.4 Discussion
Note, that we did not exclude text-initial sentences
from the study because the encoding we used does
not make such cases trivial for classification. Al-
though some of the features refer to the previous
sentence, none of them has to be necessarily re-
alized and therefore none of them explicitly indi-
cates the absence of the preceding sentence. For
example, the label NA appears in cases where there
is no discourse cue in the preceding sentence as
well as in cases where there is no preceding sen-
tence. The same holds for all other features pre-
fixed with prevS-.
Another point concerns the use of
pronominalization-based features. Sporleder
& Lapata (2006) waive using such features be-
cause they consider pronominalization dependent
on the paragraph structure and not the other
way round. At the same time they mention
speech and optical character recognition tasks
as possible application domains for the PBI.
There, pronouns are already given and need
not be regenerated, hence for such applications
features which utilize pronouns are absolutely
appropriate. Unlike the recognition tasks, for
multi-document summarization both decisions
have to be made, and the order of the two tasks
is not self-evident. The best decision would
probably be to decide simultaneously on both
using optimization methods (Roth & Yih, 2004;
Marciniak & Strube, 2005). Generating pronouns
before inserting boundaries seems as reasonable
as doing it the other way round.
4.5 Feature Selection
We determine the relevant feature set and evaluate
which features from this set contribute most to the
performance of the system by the following pro-
cedures.
First, we follow an iterative algorithm similar
to the wrapper approach for feature selection (Ko-
havi & John, 1997) using the development data
and TiMBL. The feature subset selection algo-
rithm performs a hill-climbing search along the
271
Feature set F-measure
all 58.85%
?prevSCue 0.78%
?currSCue 0.32%
?currSCueClass 0.38%
?prevSCueClass 0.37%
?prevSProCue 1.02%
best 61.72%
Table 2: Removed features
Feature set F-measure
relPos, word1, word2 48.06%
+currSRE +10.50%
+currSVF +0.49%
+currSPerson +0.57%
+prevSPerson +1.32%
best 60.94%
Table 3: Best features
feature space. We start with a model based on all
available features. Then we train models obtained
by removing one feature at a time. We choose the
worst performing feature, namely the one whose
removal gives the largest improvement based on
the F-measure, and remove it from the model. We
then train classifiers removing each of the remain-
ing features separately from the enhanced model.
The process is iteratively run as long as significant
improvement is observed.
To measure the contribution of the relevant fea-
tures we start with the three best features from
Sporleder & Lapata (2006) (see Section 4.3) and
train TiMBL combining the current feature set
with each feature in turn. We then choose the best
performing feature based on the F-measure and
add it to the model. We iterate the process until
all features are added to the three-feature system.
Thus, we optimize the default setting and obtain
the information on what the paragraph structure
crucially depends.
5 Results
Having trained our algorithm on the development
data, we then determined the optimal feature com-
bination and finally evaluated the performance on
the previously unseen test data.
Table 2 and Table 3 present the ranking of the
least and of the most beneficial features respec-
tively. Somewhat surprising to us, Table 2 shows
that basically all features capturing information on
discourse cues actually worsened the performance
of the classifier. The bad performance of the
prevSCue and currSCue features may be caused
by their extreme sparseness. To test these fea-
tures reasonably, we plan to increase the data set
size by an order of magnitude. Then, at least, it
should be possible to determine which discourse
cues, if any, are correlated with paragraph bound-
aries. The bad performance of the prevSCueClass
and currSCueClass features may be caused by the
categorization provided by the IDS. This question
also requires further investigation, maybe with a
different categorization.
Table 3 also provides interesting insights in the
feature set. First, with only the three features
relPos, word1 and word2 the baseline performs
almost as well as the full feature set used by
Sporleder & Lapata. Then, as expected, currSRE
provides the largest gain in performance, fol-
lowed by currSVF, currSPerson and prevSPerson.
This result confirms our hypothesis that linguisti-
cally motivated features capturing information on
pronominalization and information structure play
an important role in determining paragraph seg-
mentation.
The results of our system and the baselines
for different classifiers (BT stands for BoosTex-
ter and Ti for TiMBL) are summarized in Table
4. Accuracy is calculated by dividing the num-
ber of matches over the total number of test in-
stances. Precision, recall and F-measure are ob-
tained by considering true positives, false positives
and false negatives. The latter metric, WindowDiff
(Pevzner & Hearst, 2002), is supposed to over-
come the disadvantage of the F-measure which pe-
nalizes near misses as harsh as more serious mis-
takes. The value of WindowDiff varies between 0
and 1, where a lesser count corresponds to better
performance.
The significance of our results was computed
using the    test. All results are significantly
better (on the      level or below) than
both baselines and the reimplemented version of
Sporleder & Lapata?s (2006) algorithm whose per-
formance on our data is comparable to what the
authors reported on their corpus of German fic-
tion. Interestingly, TiMBL does much better than
BoosTexter on Sporleder & Lapata?s feature set.
Apparently, Sporleder & Lapata?s presupposition,
that they would rely on many weak hypotheses,
272
Accuracy Precision Recall F-measure WindowDiff
distance 52.16 37.98 31.88 34.66 .426
Galley 56.83 43.04 26.15 32.54 .416
development
Sporleder BT 71.96 80.15 30.46 44.15 .327
Sporleder Ti 62.36 48.65 62.89 54.86 .338
all BT 74.93 72.10 50.67 59.52 .286
all Ti 70.54 59.81 57.91 58.85 .302
best Ti 73.39 64.73 58.97 61.72 .280
test
Sporleder BT 68.76 80.15 28.61 42.16 .341
Sporleder Ti 60.62 50.46 59.67 54.68 .345
all BT 72.12 71.31 50.13 58.88 .286
all Ti 67.13 59.14 56.40 57.74 .303
best Ti 68.00 60.46 56.67 58.50 .302
Table 4: Results for the development and test sets with the two classifiers
does not hold. This is also confirmed by the results
reported in Table 3 where only three of their fea-
tures perform surprisingly strong. In contrast, on
our feature set TiMBL and BoosTexter perform al-
most equally. However, BoosTexter achieves in all
cases a much higher precision which is preferable
over the higher recall provided by TiMBL.
6 Conclusion
In this paper, we proposed a novel approach to
paragraph boundary identification based on lin-
guistic features such as pronominalization, dis-
course cues and information structure. The results
are significantly higher than all baselines and a
reimplementation of Sporleder & Lapata?s (2006)
system and achieve an F-measure of about 59%.
We investigated to what extent the paragraph
structure is determined by each of the three fac-
tors and came to the conclusion that it crucially
depends on the use of pronouns and information
structure. Surprisingly, discourse cues did not turn
out to be useful for this task and even negatively
affected the results which we explain by the ex-
tremely sparseness of the cues in our data.
It turned out that the best results could be
achieved by a combination of surface features (rel-
Pos, word1, word2) and features capturing text
cohesion. This indicates that paragraph bound-
ary identification requires features usually used for
style analysis and ones describing cohesive rela-
tions. Therefore, paragraph boundary identifica-
tion is in fact a task which crosses the borders be-
tween content and style.
An obvious limitation of our study is that we
trained and tested the algorithm on one-genre do-
main where pronouns are used extensively. Ex-
perimenting with different genres should shed
light on whether our features are in fact domain-
dependent. In the future, we also want to ex-
periment with a larger data set for determining
whether discourse cues really do not correlate with
paragraph boundaries. Then, we will move on
towards multi-document summarization, the ap-
plication which motivates the research described
here.
Acknowledments: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a
KTF grant (09.009.2004). We would also like
to thank the three anonymous reviewers for their
comments.
References
Bolshakov, Igor A. & Alexander Gelbukh (2001).
Text segmentation into paragraph based on local
text cohesion. In Text, Speech and Dialogue, pp.
158?166.
Brants, Thorsten (2000). TnT ? A statistical Part-
of-Speech tagger. In Proceedings of the 6th
Conference on Applied Natural Language Pro-
cessing, Seattle, Wash., 29 April ? 4 May 2000,
pp. 224?231.
Choi, Freddy Y. Y. (2000). Advances in domain
independent linear text segmentation. In Pro-
273
ceedings of the 1st Conference of the North
American Chapter of the Association for Com-
putational Linguistics, Seattle, Wash., 29 April
? 3 May, 2000, pp. 26?33.
Clarkson, Philip & Roni Rosenfeld (1997). Sta-
tistical language modeling. In Proceedings
of ESCA, EuroSpeech?97. Rhodes, pp. 2707?
2710.
Daelemans, Walter, Jakub Zavrel, Ko van der
Sloot & Antal van den Bosch (2004). TiMBL:
Tilburg Memory Based Learner, version 5.1,
Reference Guide. Technical Report ILK 04-02:
ILK Tilburg.
Foth, Kilian & Wolfgang Menzel (2006). Robust
parsing: More with less. In Proceedings of the
11th Conference of the European Chapter of
the Association for Computational Linguistics,
Trento, Italy, 3?7 April 2006, pp. 25?32.
Galley, Michel, Kathleen R. McKeown, Eric
Fosler-Lussier & Hongyan Jing (2003). Dis-
course segmentation of multi-party conversa-
tion. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguis-
tics, Sapporo, Japan, 7?12 July 2003, pp. 562?
569.
Genzel, Dmitriy (2005). A paragraph bound-
ary detection system. In Proceedings of the
Sixth International Conference on Intelligent
Text Processing and Computational Linguistics,
Mexico City, Mexico.
Genzel, Dmitriy & Eugene Charniak (2003). Vari-
ation of entropy and parse trees of sentences as
a function of the sentence number. In Proceed-
ings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, Sapporo,
Japan, 11?12 July 2003, pp. 65?72.
Hearst, Marti A. (1997). TextTiling: Segment-
ing text into multi-paragraph subtopic passages.
Computational Linguistics, 23(1):33?64.
Irmscher, William F. (1972). The Holt Guide to
English. New-York: Holt, Rinehart Winston.
Kohavi, Ron & George H. John (1997). Wrap-
pers for feature subset selection. Artificial In-
telligence Journal, 97(1-2):273?324.
Mann, William C. & Sandra A. Thompson (1988).
Rhetorical structure theory. Toward a functional
theory of text organization. Text, 8(3):243?281.
Marciniak, Tomacz & Michael Strube (2005). Be-
yond the pipeline: Discrete optimization in
NLP. In Proceedings of the 9th Conference
on Computational Natural Language Learning,
Ann Arbor, Mich., USA, 29?30 June 2005, pp.
136?145.
Pevzner, Lev & Marti Hearst (2002). A critique
and improvement of an evaluation metric for
text segmentation. Computational Linguistics,
28(1):19?36.
Power, Richard, Donia Scott & Nadjet Bouayad-
Agha (2003). Document structure. Computa-
tional Linguistics, 29(2):211?260.
Roth, Dan & Wen-tau Yih (2004). A linear pro-
gramming formulation for global inference in
natural language tasks. In Proceedings of the
8th Conference on Computational Natural Lan-
guage Learning, Boston, Mass., USA, 6?7 May
2004, pp. 1?8.
Schapire, Robert E. & Yoram Singer (2000).
BoosTexter: A boosting-based system for
text categorization. Machine Learning,
39(2/3):135?168.
Schmid, Helmut (1997). Probabilistic part-of-
speech tagging using decision trees. In Daniel
Jones & Harold Somers (Eds.), New Methods
in Language Processing, pp. 154?164. London,
UK: UCL Press.
Sporleder, Caroline & Mirella Lapata (2004). Au-
tomatic paragraph identification: A study across
languages and domains. In Proceedings of the
2004 Conference on Empirical Methods in Nat-
ural Language Processing, Barcelona, Spain,
25?26 July 2004, pp. 72?79.
Sporleder, Caroline & Mirella Lapata (2006).
Broad coverage paragraph segmentation across
languages and domains. ACM Transactions in
Speech and Language Processing. To appear.
Stark, Heather (1988). What do paragraph mark-
ings do? Discourse Processes, (11):275?303.
274
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 72?76,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Creating an Annotated Corpus for Generating Walking Directions
Stephanie Schuldes
?
Michael Roth
?
?EML Research gGmbH
Heidelberg, Germany
http://www.eml-research.de/nlp
Anette Frank
?
Michael Strube
?
?Department of Computational Linguistics
University of Heidelberg, Germany
http://www.cl.uni-heidelberg.de
Abstract
This work describes first steps towards
building a system that synchronously gen-
erates multimodal (textual and visual)
route directions for pedestrians. We pur-
sue a corpus-based approach for building a
generation model that produces natural in-
structions in multiple languages. We con-
ducted an empirical study to collect ver-
bal route directions, and annotated the ac-
quired texts on different levels. Here we
describe the experimental setting and an
analysis of the collected data.
1 Introduction
Route directions guide a person unfamiliar with
the environment to their designated goal. We plan
to generate route instructions that are similar to
those given by humans by referring to landmarks
and by structuring the route in a way that it is easy
to memorize (Denis, 1997).
We develop a system for synchronously gen-
erating natural language route directions and 3D
scenes of a route. The core of the architecture
is a unified representation providing information
for both verbal and graphical output. The direct
correspondence between linguistic references and
shown objects facilitates the identification of the
visual scene in the real world and the choice of the
correct action while following the route. To cre-
ate a reusable system that is adaptable to different
navigational domains and languages, we use ma-
chine learning techniques to build a statistical gen-
eration model from annotated corpora. We report
on an empirical study to collect human-produced
walking directions to be used for statistical gener-
ation from underlying semantic structures. While
our scenario is ultimately multilingual, here we
give an analysis of the German dataset.
2 Related Work
The task of analyzing and generating cognitively
adequate route instructions has been addressed by
a number of authors (Taylor & Tversky, 1996;
Tappe, 2000; Habel, 2003; Richter, 2008; Vi-
ethen & Dale, 2008; Kelleher & Costello, 2009).
Marciniak & Strube (2005) showed that a system
for generating route directions can be successfully
trained on a small set of 75 route direction texts
(8418 tokens). In their approach directions are
represented in a graph, which encodes informa-
tion on various conceptual levels. While their ap-
proach is restricted to reproducing directions for
the learned graphs, we will generate directions for
a wide range of possible routes. Dale et al (2005)
developed a system that takes GIS data as input
and uses a pipeline architecture to generate verbal
route directions. In contrast to their approach, our
approach will be based on an integrated architec-
ture allowing for more interaction between the dif-
ferent stages of generation. The idea of combining
verbal directions with scenes from a virtual 3D en-
vironment has recently lead to a new framework
for evaluating NLG systems: The Challenge on
Generating Instructions in Virtual Environments
(GIVE) (Byron et al, 2009) is planned to become
a regular event for the NLG community.
3 Corpus Acquisition
For collecting naturally produced route instruc-
tions, we conducted a study with 29 native speak-
ers of German (66% female and 33% male). The
participants in our study were students from var-
ious fields aged between 20 and 34 years. We
designed two different settings: one on-site set-
ting, in which participants walked around in a real
world situation (specifically our university cam-
pus), and one desk-based setting, in which they
interacted with a web application. The former
was further divided into indoor and outdoor routes,
72
32
1
7
6
4
5
Figure 1: Example route from the indoor setting
(first task), leading from a room with photocopiers
(1) across an open space and downstairs (3) to a
students? union room (6), passing an information
board (4) and a coffee machine (5). A lecture room
(2) and a glass wall (7) are visible from the route.
while the latter was restricted to an outdoor sce-
nario. This design enables us to study possible
differences and commonalities between linguistic
realizations obtained for different environments as
well as different presentation modes.
For both scenarios, the task was to give written
directions to a person unfamiliar with the area as
to how to get to the destination the participants just
reached, taking the same route. First, participants
were led along a route to a given destination point
(on-site). Each participant was asked to give di-
rections for two routes inside buildings of the uni-
versity campus (e.g. from an office to a seminar
room, cf. Figure 1), and one outside route (e.g.
from the building entrance to a bus stop).
Second, participants were shown a web appli-
cation that guided them along a route by means of
a 2D animation (desk-based). Subjects were al-
lowed to use all information displayed by the web
application: named places, buildings, street and
bridge names, etc. (cf. Figure 2).
Setting GM CI CO Total
physical routes 9 6 3 18
directions 59 58 28 145
tokens 5353 4119 2674 12146
tokens/dir. (?) 91 71 96
Table 1: Number of routes, directions, and tokens
for the different settings. GM = Google Maps, CI
= Campus Indoor, CO = Campus Outdoor.
4 Corpus Annotation
The acquired texts were processed in several steps.
To ensure that all route directions consist of syn-
tactically and semantically correct sentences, we
Figure 2: Web application used in the second task.
Landmarks were introduced successively via pop-
ups as the animated walker encountered them.
manually corrected spelling mistakes, omissions
resulting in grammatical errors, and removed el-
liptical and unclear directions.
The preprocessed texts were annotated on the
following three levels:
pos lemma ? part-of-speech and lemma
syn dep ? dependency relations
sem frame ? frames and semantic roles
For the pos lemma and syn dep levels, we used
TreeTagger (Schmid, 1997) and XLE (Maxwell
& Kaplan, 1993). The corpus was parsed
with the German ParGram LFG grammar (Forst,
2007). The outputs were corrected manually
by two annotators. On the sem frame level an-
notation was carried out using the annotation
tool SALTO (Burchardt et al, 2006) and fol-
lowing the definiton of the FrameNet frames
SELF MOTION, PERCEPTION, BEING LOCATED
and LOCATIVE RELATION (Baker et al, 1998). In
terms of accuracy for unlabeled/labeled relations,
the annotation agreement was 78.88%/65.17% on
the syn dep level and 79.27%/68.39% for frames
and semantic roles.
5 Data Analysis
5.1 Corpus Statistics
We examined word frequencies with respect to the
experimental settings in order to determine simi-
larities and dissimilarities in lexical choice. Table
2 shows the three most frequent verbs and nouns
found in each corpus part.
The data reveals that the most frequent verbs are
typical among all settings. However, we found a
number of lower-frequency verbs that are rather
73
Top verbs (Campus) GM CI CO
gehen ?to walk? 11% 18% 14%
sein ?to be? 3.9% 8.2% 6.6%
stehen ?to stand? 0.0% 6.3% 5.3%
Top verbs (GM) GM CI CO
folgen ?to follow? 12% 2.9% 2.6%
gehen ?to walk? 11% 18% 14%
abbiegen ?to turn into? 9.0% 3.8% 8.9%
Top nouns (Campus) GM CI CO
T?ur ?door? 0.0% 12% 0.9%
Treppe ?stairs? 0.0% 8.3% 0.0%
Gang ?hallway? 0.0% 6.6% 0.0%
Top nouns (GM) GM CI CO
...stra?e ?. . . Street? 28% 0.0% 2.2%
Richtung ?direction? 3.5% 2.8% 2.6%
...platz ?. . . Square? 3.4% 0.0% 6.1%
Table 2: Relative frequency of the three most com-
mon verbs and nouns in both studies
scenario-specific. In many cases, the occurrence
or absence of a verb can be attributed to a verb?s
selectional restrictions. For example, some of
the verbs describing movements along streets (e.g.
folgen ?to follow?, abbiegen ?to turn into?) do not
occur within the indoor corpus whereas verbs de-
scribing ?3D movements? (e.g. durchqueren ?to
walk through?, hinuntergehen ?to walk down?) are
not mentioned with the Google Maps setting.
The most frequent nouns significantly differ be-
tween the indoor and outdoor settings. This corre-
lation does not come as a surprise, as most of the
mentioned objects cannot be found in all scenar-
ios. On the other hand, nouns that are common
to both indoor and outdoor scenarios can be di-
vided into two categories: Nouns denoting (1) ob-
jects that appear in both scenarios (e.g. Geb?aude
?building?) and (2) abstract concepts typical for
route directions in general, e.g. Richtung ?direc-
tion?, Nummer ?number?, Ziel ?goal?, and Start-
punkt ?starting point?.
5.2 Landmark Alignment
Landmark alignment serves the purpose of de-
tecting objects that are most frequently men-
tioned across directions, and how the same ob-
ject is referred to differently. We created a graph-
based representation of the landmarks mentioned
in each route instruction (single route representa-
tion, SRR) for use in two types of alignment. Fig-
ure 3 shows an example from the indoor study.
First, we created a combined graph for each phys-
ical route by merging the respective SRRs, taking
into account several criteria:
String matching of landmark names;
Semantic similarity using GermaNet (Lemnitzer
& Kunze, 2002), a lexical-semantic network
for German similar to WordNet;
Frequency of references across all directions;
Spatio-temporal proximity of references to the
same object;
Number of landmarks mentioned in a single di-
rection (i.e. length of the SRR).
The combined graphs show that there are strong
correspondences between the directions for the
same route. We also found that, in the campus
settings, there was a small number of frequently
used general objects and a large number of less
frequently used specific objects. This facilitates
merging and shows the importance of the objects
for people?s orientation, and at the same time sup-
ports our claim that other modalities are needed
to disambiguate references during navigation. For
generating informative referential expressions, the
combined graph needs to be refined so that object
properties are represented (Krahmer et al, 2003).
Second, we aligned the SRRs with the physical
route graph. Comparing the landmarks mentioned
in the campus settings revealed that, in 97.8% of
the cases, people adhere to the sequence in which
objects are encountered. Reversed order was only
found in special cases like distant objects.
5.3 Discourse Phenomena
We analyzed the use of anaphora, the temporal or-
der of instructions, and occurrences of prototypi-
cal event chains in the collected texts in order to
identify coherence-inducing elements.
Spatio-temporal adverbials: Most anaphors
mention intermediate goals on the route in order
to refer to the starting point of a new action (e.g.
da/hier ?here?, dort ?there?). This finding goes
hand in hand with the observation that the col-
lected route directions are typically structured in
a linear temporal order (cf. Table 3) as for ex-
ample indicated by the use of adverbs indicat-
ing temporal succession (e.g. jetzt ?now?, dann
?then? and danach ?afterwards?) and conjunctions
(e.g. bis ?until?, wenn ?when?). Interestingly, a re-
versed order can be found in a few cases, where
74
R?cken Raum Kopierer Treppe Treppe Brett Getr?nkeautomat
Treppe
H?rsaal
Treppe Kaffeeautomat T?r Fachschaft
Kopierer Treppe
H?rsaal
Brett S?ule
Glast?r
Getr?nkeautomat T?r
Fachschafts-
raum
Druckerraum Theoretikum 180-Grad-Kurve Fachschaftstafel
Glaswand Glaswand
Kopf
Medizin-
Fachschaft
Kopierzentrum R?cken Treppe Treppe Richtung
Glasfenster Glasfront
Eingang Fachschaft
3
2
1
7
64 5
Figure 3: Each line shows one SRR for the route in Figure 1. Correspondences are indicated by identical
node shapes, black dots substitute non-matched tokens. The bottom graph shows the physical route seen
as sequence of landmarks. Node size reflects the importance of the referred object as conveyed by SRRs.
Adverbs >
t
GM CI CO
dann ?then? 55 43 30
jetzt ?now? 4 7 5
danach ?afterwards? 12 5 3
Adverbs <
t
GM CI CO
vorher ?beforehand? 0 1 0
davor ?before? 1 0 2
Table 3: Frequencies of temporal adverbs indicat-
ing linear (>
t
) and reversed linear order (<
t
)
the following action or situation is not supposed
to take place (e.g. Gehen Sie vorher rechts ?be-
forehand turn right?).
Backward-looking event anaphors and refer-
ences to result states: We also found explicit
references to past events (e.g. Nach dem Durch-
queren ?after traversing?) and result states of
events, e.g. the adverbial phrase unten angekom-
men (here: ?downstairs?) was frequently used fol-
lowing an instruction to ?walk downstairs?.
6 Conclusions and Future Work
The lexical corpus analysis confirms our hypoth-
esis that there are strong commonalities in lexi-
cal choice for directions that persist across sce-
narios and presentation modes, with a small num-
ber of focused differences, and obvious domain-
dependent lexical differences regarding the nature
of objects in the respective scenarios. While our
current corpus data is rather broad, environment-
specific data can be extended quickly by setting up
web studies using 2D and 3D environments.
The alignment of the physical routes and ver-
bal instructions shows a clear tendency that linear
route structure is observed in verbal realization,
with only few exceptions. Since temporal order
is observed by default, temporal annotation can be
restricted to capture exceptional orderings, which
are recoverable from linguistic cues. The study of
discourse coherence effects yielded a number of
elements that will be given special attention in the
surface generation model. We observed a variety
of coherence-inducing elements that are generic
in nature and thus seem well-suited for a corpus-
based generation model. As other languages are
known to exhibit differences in verbal realization
of directions (von Stutterheim et al, 2002), we
have to extend our data collection in order to gen-
erate systematic linguistic variations from a single
underlying semantic structure for all languages.
The linguistic annotation levels of frames and
roles, syntactic dependencies, and basic word cat-
egories have been tested successfully with a sim-
ilar corpus (Roth & Frank, 2009). The next steps
will consist in the alignment of physical routes and
landmarks with semantic representations in an in-
tegrated generation architecture.
Acknowledgements: This work is supported by
the DFG-financed innovation fund FRONTIER as
part of the Excellence Initiative at Heidelberg Uni-
versity (ZUK 49/1) and partially funded by the
Klaus Tschira Foundation, Heidelberg, Germany.
We thank the participants in our study, our anno-
tators Tim Krones and Anna Schmidt, and student
assistants Jonathan Geiger and Carina Silberer.
75
References
Baker, Collin F., Charles J. Fillmore & John B. Lowe (1998).
The Berkeley FrameNet Project. In Proceedings of the
17th International Conference on Computational Linguis-
tics and 36th Annual Meeting of the Association for Com-
putational Linguistics, Montr?eal, Qu?ebec, Canada, 10?14
August 1998, pp. 86?90.
Burchardt, Aljoscha, Katrin Erk, Anette Frank, Andrea
Kowalski & Sebastian Pado (2006). SALTO: A versatile
multi-level annotation tool. In Proceedings of the 5th In-
ternational Conference on Language Resources and Eval-
uation, Genoa, Italy, 22?28 May 2006, pp. 517?520.
Byron, Donna, Alexander Koller, Kristina Striegnitz, Justine
Cassell, Robert Dale, Johanna Moore & Jon Oberlander
(2009). Report on the First NLG Challenge on Generating
Instructions in Virtual Environments (GIVE). In Proceed-
ings of the 12th European Workshop on Natural Language
Generation (ENLG 2009), Athens, Greece, 30-31 March
2009, pp. 165?173.
Dale, Robert, Sabine Geldof & Jean-Philippe Prost (2005).
Using natural language generation in automatic route de-
scription. Journal of Research and Practice in Information
Technology, 37(1):89?106.
Denis, Michel (1997). The description of routes: A cognitive
approach to the production of spatial discourse. Current
Psychology of Cognition, 16:409?458.
Forst, Martin (2007). Filling statistics with linguistics ?
Property design for the disambiguation of German LFG
parses. In Proceedings of the ACL 2007 Workshop on
Deep Linguistic Processing, Prague, Czech Republic, 28
June 2007, pp. 17?24.
Habel, Christopher (2003). Incremental generation of mul-
timodal route instructions. In Reva Freedman & Charles
Callaway (Eds.), Working Papers of the 2003 AAAI Spring
Symposium on Natural Language Generation in Spoken
and Written Dialogue, pp. 44?51. Menlo Park, California:
AAAI Press.
Kelleher, John D. & Fintan J. Costello (2009). Applying com-
putational models of spatial prepositions to visually situ-
ated dialog. Computational Linguistics, 35(2):271?306.
Krahmer, Emiel, Sebastiaan van Erk & Andr?e Verleg (2003).
Graph-based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
Lemnitzer, Lothar & Claudia Kunze (2002). GermaNet ? rep-
resentation, visualization, application. In Proceedings of
the 3rd International Conference on Language Resources
and Evaluation, Las Palmas, Canary Islands, Spain, 29?31
May 2002, pp. 1485?1491.
Marciniak, Tomacz & Michael Strube (2005). Beyond the
pipeline: Discrete optimization in NLP. In Proceedings of
the 9th Conference on Computational Natural Language
Learning, Ann Arbor, Mich., USA, 29?30 June 2005, pp.
136?145.
Maxwell, John T. & Ronald M. Kaplan (1993). The inter-
face between phrasal and functional constraints. Compu-
tational Linguistics, 19(4):571?590.
Richter, Kai-Florian (2008). Context-Specific Route Direc-
tions ? Generation of Cognitively Motivated Wayfinding
Instructions. Amsterdam: IOS Press.
Roth, Michael & Anette Frank (2009). A NLG-based appli-
cation for walking directions. In Companion Volume to
the Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Interna-
tional Joint Conference on Natural Language Processing
of the Asian Federation of Natural Language Processing,
Singapore, 2-7 August 2009. To appear.
Schmid, Helmut (1997). Probabilistic Part-of-Speech tagging
using decision trees. In Daniel Jones & Harold Somers
(Eds.), New Methods in Language Processing, pp. 154?
164. London, U.K.: UCL Press.
Tappe, Heike (2000). Perspektivenwahl in Beschreibun-
gen dynamischer und statischer Wegeskizzen. [Choice of
perspective in descriptions of dynamic and static sketch-
maps]. In Christopher Habel & Christiane v. Stutterheim
(Eds.), R?aumliche Konzepte und sprachliche Strukturen,
pp. 69?97. T?ubingen: Niemeyer.
Taylor, Holly & Barbara Tversky (1996). Perspective in
spatial descriptions. Journal of Memory and Language,
35:371?391.
Viethen, Jette & Robert Dale (2008). The use of spatial re-
lations in referring expression generation. In Proceedings
of the Fifth International Natural Language Generation
Conference, Salt Fork OH, USA, 12?14 June 2008, pp.
59?67.
von Stutterheim, Christiane, Ralf N?use & Jorge M. Serra
(2002). Crosslinguistic differences in the conceptuali-
sation of events. In Hilde Hasselg?ard, Stig Johansson,
Bergljot Behrens & Cathrine Fabricius-Hansen (Eds.), In-
formation Structure in a Cross-lingustic Perspective, pp.
179?198. Amsterdam: Rodopi.
76
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 143?151,
Beijing, August 2010
End-to-End Coreference Resolution via Hypergraph Partitioning
Jie Cai and Michael Strube
Natural Language Processing Group
Heidelberg Institute for Theoretical Studies gGmbH
(jie.cai|michael.strube)@h-its.org
Abstract
We describe a novel approach to coref-
erence resolution which implements a
global decision via hypergraph partition-
ing. In constrast to almost all previ-
ous approaches, we do not rely on sep-
arate classification and clustering steps,
but perform coreference resolution glob-
ally in one step. Our hypergraph-based
global model implemented within an end-
to-end coreference resolution system out-
performs two strong baselines (Soon et al,
2001; Bengtson & Roth, 2008) using sys-
tem mentions only.
1 Introduction
Coreference resolution is the task of grouping
mentions of entities into sets so that all mentions
in one set refer to the same entity. Most recent
approaches to coreference resolution divide this
task into two steps: (1) a classification step which
determines whether a pair of mentions is corefer-
ent or which outputs a confidence value, and (2)
a clustering step which groups mentions into enti-
ties based on the output of step 1.
The classification steps of most approaches
vary in the choice of the classifier (e.g. decision
tree classifiers (Soon et al, 2001), maximum en-
tropy classification (Luo et al, 2004), SVM clas-
sifiers (Rahman & Ng, 2009)) and the number of
features used (Soon et al (2001) employ a set of
twelve simple but effective features while e.g., Ng
& Cardie (2002) and Bengtson & Roth (2008) de-
vise much richer feature sets).
The clustering step exhibits much more varia-
tion: Local variants utilize a closest-first decision
(Soon et al, 2001), where a mention is resolved to
its closest possible antecedent, or a best-first deci-
sion (Ng & Cardie, 2002), where a mention is re-
solved to its most confident antecedent (based on
the confidence value returned by step 1). Global
variants attempt to consider all possible cluster-
ing possibilites by creating and searching a Bell
tree (Luo et al, 2004), by learning the optimal
search strategy itself (Daume? III & Marcu, 2005),
by building a graph representation and applying
graph clustering techniques (Nicolae & Nicolae,
2006), or by employing integer linear program-
ming (Klenner, 2007; Denis & Baldridge, 2009).
Since these methods base their global clustering
step on a local pairwise model, some global infor-
mation which could have guided step 2 is already
lost. The twin-candidate model (Yang et al, 2008)
replaces the pairwise model by learning prefer-
ences between two antecedent candidates in step
1 and applies tournament schemes instead of the
clustering in step 2.
There is little work which deviates from this
two-step scheme. Culotta et al (2007) introduce a
first-order probabilistic model which implements
features over sets of mentions and thus operates
directly on entities.
In this paper we describe a novel approach to
coreference resolution which avoids the division
into two steps and instead performs a global deci-
sion in one step. We represent a document as a hy-
pergraph, where the vertices denote mentions and
the edges denote relational features between men-
tions. Coreference resolution is performed glob-
ally in one step by partitioning the hypergraph into
subhypergraphs so that all mentions in one subhy-
pergraph refer to the same entity. Our model out-
143
performs two strong baselines, Soon et al (2001)
and Bengtson & Roth (2008).
Soon et al (2001) developed an end-to-end
coreference resolution system for the MUC data,
i.e., a system which processes raw documents
as input and produces annotated ones as output.
However, with the advent of the ACE data, many
systems either evaluated only true mentions, i.e.
mentions which are included in the annotation,
the so-called key, or even received true informa-
tion for mention boundaries, heads of mentions
and mention type (Culotta et al, 2007, inter alia).
While these papers report impressive results it has
been concluded that this experimental setup sim-
plifies the task and leads to an unrealistic surro-
gate for the coreference resolution problem (Stoy-
anov et al, 2009, p.657, p660). We argue that
the field should move towards a realistic setting
using system mentions, i.e. automatically deter-
mined mention boundaries and types. In this pa-
per we report results using our end-to-end coref-
erence resolution system, COPA, without relying
on unrealistic assumptions.
2 Related Work
Soon et al (2001) transform the coreference res-
olution problem straightforwardly into a pairwise
classification task making it accessible to standard
machine learning classifiers. They use a set of
twelve powerful features. Their system is based
solely on information of the mention pair anaphor
and antecedent. It does not take any information
of other mentions into account. However, it turned
out that it is difficult to improve upon their re-
sults just by applying a more sophisticated learn-
ing method and without improving the features.
We use a reimplementation of their system as first
baseline. Bengtson & Roth (2008) push this ap-
proach to the limit by devising a much more in-
formative feature set. They report the best results
to date on the ACE 2004 data using true mentions.
We use their system combined with our prepro-
cessing components as second baseline.
Luo et al (2004) perform the clustering step
within a Bell tree representation. Hence their
system theoretically has access to all possible
outcomes making it a potentially global system.
However, the classification step is still based on
a pairwise model. Also since the search space in
the Bell tree is too large they have to apply search
heuristics. Hence, their approach loses much of
the power of a truly global approach.
Culotta et al (2007) introduce a first-order
probabilistic model which implements features
over sets of mentions. They use four features for
their first-order model. The first is an enumeration
over pairs of noun phrases. The second is the out-
put of a pairwise model. The third is the cluster
size. The fourth counts mention type, number and
gender in each cluster. Still, their model is based
mostly on information about pairs of mentions.
They assume true mentions as input. It is not clear
whether the improvement in results translates to
system mentions.
Nicolae & Nicolae (2006) describe a graph-
based approach which superficially resembles our
approach. However, they still implement a two
step coreference resolution approach and apply
the global graph-based model only to step 2. They
report considerable improvements over state-of-
the-art systems including Luo et al (2004). How-
ever, since they not only change the clustering
strategy but also the features for step 1, it is not
clear whether the improvements are due to the
graph-based clustering technique. We, instead,
describe a graph-based approach which performs
classification and clustering in one step. We com-
pare our approach with two competitive systems
using the same feature sets.
3 COPA: Coreference Partitioner
The COPA system consists of learning modules
which learn hyperedge weights from the training
data, and resolution modules which create a hy-
pergraph representation for the testing data and
perform partitioning to produce subhypergraphs,
each of which represents an entity. An example
analysis of a short document involving the two en-
tities, BARACK OBAMA and NICOLAS SARKOZY
illustrates how COPA works.
[US President Barack Obama] came to Toronto today.
[Obama] discussed the financial crisis with [President
Sarkozy].
[He] talked to him [him] about the recent downturn of the
European markets.
[Barack Obama] will leave Toronto tomorrow.
144
A hypergraph (Figure (1a)) is built for this
document based on three features. Two hyper-
edges denote the feature partial string match,
{US President Barack Obama, Barack Obama,
Obama} and {US President Barack Obama, Pres-
ident Sarkozy}. One hyperedge denotes the fea-
ture pronoun match, {he, him}. Two hyperedges
denote the feature all speak, {Obama, he} and
{President Sarkozy, him}.
On this initial representation, a spectral clus-
tering technique is applied to find two partitions
which have the strongest within-cluster connec-
tions and the weakest between-clusters relations.
The cut found is called Normalized Cut, which
avoids trivial partitions frequently output by the
min-cut algorithm. The two output subhyper-
graphs (Figure (1b)) correspond to two resolved
entities shown on both sides of the bold dashed
line. In real cases, recursive cutting is applied
to all the subhypergraphs resulting from previous
steps, until a stopping criterion is reached.
Figure 1: Hypergraph-based representation
3.1 HyperEdgeLearner
COPA needs training data only for computing the
hyperedge weights. Hyperedges represent fea-
tures. Each hyperedge corresponds to a feature
instance modeling a simple relation between two
or more mentions. This leads to initially overlap-
ping sets of mentions. Hyperedges are assigned
weights which are calculated based on the train-
ing data as the percentage of the initial edges (as
illustrated in Figure (1a)) being in fact coreferent.
The weights for some of Soon et al (2001)?s fea-
tures learned from the ACE 2004 training data are
given in Table 1.
Edge Name Weight
Alias 0.777
StrMatch Pron 0.702
Appositive 0.568
StrMatch Npron 0.657
ContinuousDistAgree 0.403
Table 1: Hyperedge weights for ACE 2004 data
3.2 Coreference Resolution Modules
Unlike pairwise models, COPA processes a docu-
ment globally in one step, taking care of the pref-
erence information among all the mentions at the
same time and clustering them into sets directly.
A raw document is represented as a single hyper-
graph with multiple edges. The hypergraph re-
solver partitions the simple hypergraph into sev-
eral subhypergraphs, each corresponding to one
set of coreferent mentions (see e.g. Figure (1b)
which contains two subhypergraphs).
3.2.1 HGModelBuilder
A single document is represented in a hyper-
graph with basic relational features. Each hyper-
edge in a graph corresponds to an instance of one
of those features with the weight assigned by the
HyperEdgeLearner. Instead of connecting nodes
with the target relation as usually done in graph
models, COPA builds the graph directly out of a
set of low dimensional features without any as-
sumptions for a distance metric.
3.2.2 HGResolver
In order to partition the hypergraph we adopt
a spectral clustering algorithm. Spectral cluster-
ing techniques use information obtained from the
eigenvalues and eigenvectors of the graph Lapla-
cian to cluster the vertices. They are simple to im-
plement and reasonably fast and have been shown
to frequently outperform traditional clustering al-
gorithms such as k-means. These techniques have
145
Algorithm 1 R2 partitioner
Note: { L = I ?Dv?
1
2HWDe?1HTDv?
1
2 }
Note: { Ncut(S) := vol?S( 1volS + 1volSc )}
input: target hypergraph HG, predefined ??
Given a HG, construct its Dv , H , W and De
Compute L for HG
Solve the L for the second smallest eigenvector V2
for each splitting point in V2 do
calculate Ncuti
end for
Choose the splitting point with min
i
(Ncuti)
Generate two subHGs
if min
i
(Ncuti) < ?? then
for each subHG do
Bi-partition the subHG with the R2 partitioner
end for
else
Output the current subHG
end if
output: partitioned HG
many applications, e.g. image segmentation (Shi
& Malik, 2000).
We adopt two variants of spectral clustering,
recursive 2-way partitioning (R2 partitioner) and
flat-K partitioning. Since flat-K partitioning did
not perform as well we focus here on recursive 2-
way partitioning. In contrast to flat-K partitioning,
this method does not need any information about
the number of target sets. Instead a stopping cri-
terion ?? has to be provided. ?? is adjusted on
development data (see Algorithm 1).
In order to apply spectral clustering to hyper-
graphs we follow Agarwal et al (2005). All ex-
perimental results are obtained using symmetric
Laplacians (Lsym) (von Luxburg, 2007).
Given a hypergraph HG, a set of matrices is
generated. Dv and De denote the diagonal matri-
ces containing the vertex and hyperedge degrees
respectively. |V | ? |E| matrix H represents the
HG with the entries h(v, e) = 1 if v ? e and 0
otherwise. HT is the transpose of H . W is the
diagonal matrix with the edge weights. S is one
of the subhypergraphs generated from a cut in the
HG, where Ncut(S) is the cut?s value.
Using Normalized Cut does not generate sin-
gleton clusters, hence a heuristic singleton detec-
tion strategy is used in COPA. We apply a thresh-
old ? to each node in the graph. If a node?s degree
is below the threshold, the node will be removed.
3.3 Complexity of HGResolver
Since edge weights are assigned using simple de-
scriptive statistics, the time HGResolver needs for
building the graph Laplacian matrix is insubstan-
tial. For eigensolving, we use an open source li-
brary provided by the Colt project1which imple-
ments a Householder-QL algorithm to solve the
eigenvalue decomposition. When applied to the
symmetric graph Laplacian, the complexity of the
eigensolving is given by O(n3), where n is the
number of mentions in a hypergraph. Since there
are only a few hundred mentions per document in
our data, this complexity is not an issue (spectral
clustering gets problematic when applied to mil-
lions of data points).
4 Features
The HGModelBuilder allows hyperedges with a
degree higher than two to grow throughout the
building process. This type of edge is mergeable.
Edges with a degree of two describe pairwise rela-
tions. Thus these edges are non-mergeable. This
way any kind of relational features can be incor-
porated into the hypergraph model.
Features are represented as types of hyperedges
(in Figure (1b) the two hyperedges marked by ??
??? are of the same type). Any realized edge is an
instance of the corresponding edge type. All in-
stances derived from the same type have the same
weight, but they may get reweighted by the dis-
tance feature (Section 4.4).
In the following Subsections we describe the
features used in our experiments. We use the en-
tire set for obtaining the final results. We restrict
ourselves to Soon et al (2001)?s features when we
compare our system with theirs in order to assess
the impact of our model regardless of features (we
use features 1., 2., 3., 6., 7., 11., 13.).
4.1 Hyperedges With a Degree > 2
High degree edges are the particular property of
the hypergraph which allows to include all types
of relational features into our model. The edges
are built through pairwise relations and, if consis-
tent, get incrementally merged into larger edges.
1http://acs.lbl.gov/
?
hoschek/colt/
146
High degree edges are not sensitive to positional
information from the documents.
(1) StrMatch Npron & (2) StrMatch Pron:
After discarding stop words, if the strings of men-
tions completely match and are not pronouns, they
are put into edges of the StrMatch Npron type.
When the matched mentions are pronouns, they
are put into the StrMatch Pron type edges.
(3) Alias: After discarding stop words, if men-
tions are aliases of each other (i.e. proper names
with partial match, full names and acronyms of
organizations, etc.), they are put into edges of the
Alias type.
(4) Synonym: If, according to WordNet, men-
tions are synonymous, they are put into an edge of
the Synonym type.
(5) AllSpeak: Mentions which appear within a
window of two words of a verb meaning to say
form an edge of the AllSpeak type.
(6) Agreement: If mentions agree in Gender,
Number and Semantic Class they are put in edges
of the Agreement type. Because Gender, Num-
ber and Semantic Class are strong negative coref-
erence indicators ? in contrast to e.g. StrMatch ?
and hence weak positive features, they are com-
bined into the one feature Agreement.
4.2 Hyperedges With a Degree = 2
Features which have been used by pairwise mod-
els are easily integrated into the hypergraph model
by generating edges with only two vertices. Infor-
mation sensitive to relative distance is represented
by pairwise edges.
(7) Apposition & (8) RelativePronoun: If two
mentions are in a appositive structure, they are put
in an edge of type Apposition. If the latter mention
is a relative pronoun, the mentions are put in an
edge of type RelativePronoun.
(9) HeadModMatch: If the syntactic heads of
two mentions match, and if their modifiers do not
contradict each other, the mentions are put in an
edge of type HeadModMatch.
(10) SubString: If a mention is the substring
of another one, they are put into an edge of type
SubString.
4.3 MentionType and EntityType
In our model (11) mention type can only reason-
ably be used when it is conjoined with other fea-
tures, since mention type itself describes an at-
tribute of single mentions. In COPA, it is con-
joined with other features to form hyperedges, e.g.
the StrMatch Pron edge. We use the same strat-
egy to represent (12) entity type.
4.4 Distance Weights
Our hypergraph model does not have any obvi-
ous means to encode distance information. How-
ever, the distance between two mentions plays
an important role in coreference resolution, es-
pecially for resolving pronouns. We do not en-
code distance as feature, because this would intro-
duce many two-degree-hyperedges which would
be computationally very expensive without much
gain in performance. Instead, we use distance to
reweight two-degree-hyperedges, which are sen-
sitive to positional information.
We experimented with two types of distance
weights: One is (13) sentence distance as used in
Soon et al (2001)?s feature set, while the other is
(14) compatible mentions distance as introduced
by Bengtson & Roth (2008).
5 Experiments
We compare COPA?s performance with two im-
plementations of pairwise models. The first base-
line is the BART (Versley et al, 2008) reimple-
mentation of Soon et al (2001), with few but ef-
fective features. Our second baseline is Bengtson
& Roth (2008), which exploits a much larger fea-
ture set while keeping the machine learning ap-
proach simple. Bengtson & Roth (2008) show
that their system outperforms much more sophis-
ticated machine learning approaches such as Cu-
lotta et al (2007), who reported the best results
on true mentions before Bengtson & Roth (2008).
Hence, Bengtson & Roth (2008) seems to be a rea-
sonable competitor for evaluating COPA.
In order to report realistic results, we neither
assume true mentions as input nor do we evalu-
ate only on true mentions. Instead, we use an in-
house mention tagger for automatically extracting
mentions.
147
5.1 Data
We use the MUC6 data (Chinchor & Sund-
heim, 2003) with standard training/testing divi-
sions (30/30) as well as the MUC7 data (Chin-
chor, 2001) (30/20). Since we do not have ac-
cess to the official ACE testing data (only avail-
able to ACE participants), we follow Bengtson &
Roth (2008) for dividing the ACE 2004 English
training data (Mitchell et al, 2004) into training,
development and testing partitions (268/76/107).
We randomly split the 252 ACE 2003 training
documents (Mitchell et al, 2003) using the same
proportions into training, development and testing
(151/38/63). The systems were tuned on develop-
ment and run only once on testing data.
5.2 Mention Tagger
We implement a classification-based mention tag-
ger, which tags each NP chunk as ACE mention or
not, with neccessary post-processing for embed-
ded mentions. For the ACE 2004 testing data, we
cover 75.8% of the heads with 73.5% accuracy.
5.3 Evaluation Metrics
We evaluate COPA with three coreference resolu-
tion evaluation metrics: the B3-algorithm (Bagga
& Baldwin, 1998), the CEAF-algorithm (Luo,
2005), and, for the sake of completeness, the
MUC-score (Vilain et al, 1995).
Since the MUC-score does not evaluate single-
ton entities, it only partially evaluates the perfor-
mance for ACE data, which includes singleton
entities in the keys. The B3-algorithm (Bagga
& Baldwin, 1998) addresses this problem of the
MUC-score by conducting calculations based on
mentions instead of coreference relations. How-
ever, another problematic issue emerges when
system mentions have to be dealt with: B3 as-
sumes the mentions in the key and in the response
to be identical, which is unlikely when a men-
tion tagger is used to create system mentions.
The CEAF-algorithm aligns entities in key and
response by means of a similarity metric, which
is motivated by B3?s shortcoming of using one
entity multiple times (Luo, 2005). However, al-
though CEAF theoretically does not require to
have the same number of mentions in key and
response, the algorithm still cannot be directly
applied to end-to-end coreference resolution sys-
tems, because the similarity metric is influenced
by the number of mentions in key and response.
Hence, both the B3- and CEAF-algorithms
have to be extended to deal with system mentions
which are not in the key and true mentions not
extracted by the system, so called twinless men-
tions (Stoyanov et al, 2009). Two variants of
the B3-algorithm are proposed by Stoyanov et al
(2009), B3all and B30 . B3all tries to assign intu-
itive precision and recall to the twinless system
mentions and twinless key mentions, while keep-
ing the size of the system mention set and the key
mention set unchanged (which are different from
each other). For twinless mentions, B3all discards
twinless key mentions for precision and twinless
system mentions for recall. Discarding parts of
the key mentions, however, makes the fair com-
parison of precision values difficult. B30 produces
counter-intuitive precision by discarding all twin-
less system mentions. Although it penalizes the
recall of all twinless key mentions, so that the F-
scores are balanced, it is still too lenient (for fur-
ther analyses see Cai & Strube (2010)).
We devise two variants of the B3- and CEAF-
algorithms, namely B3sys and CEAFsys. For com-
puting precision, the algorithms put all twinless
true mentions into the response even if they were
not extracted. All twinless system mentions which
were deemed not coreferent are discarded. Only
twinless system mentions which were mistakenly
resolved are put into the key. Hence, the system
is penalized for resolving mentions not found in
the key. For recall the algorithms only consider
mentions from the original key by discarding all
the twinless system mentions and putting twin-
less true mentions into the response as singletons
(algorithm details, simulations and comparison of
different systems and metrics are provided in Cai
& Strube (2010)). For CEAFsys, ?3 (Luo, 2005)
is used. B3sys and CEAFsys report results for end-
to-end coreference resolution systems adequately.
5.4 Baselines
We compare COPA?s performance with two base-
lines: SOON ? the BART (Versley et al, 2008)
reimplementation of Soon et al (2001) ? and
148
SOON COPA with R2 partitioner
R P F R P F ?? ?
MUC MUC6 59.4 67.9 63.4 62.8 66.4 64.5 0.08 0.03
MUC7 52.3 67.1 58.8 55.2 66.1 60.1 0.05 0.01
ACE 2003 56.7 75.8 64.9 60.8 75.1 67.2 0.07 0.03
ACE 2004 50.4 67.4 57.7 54.1 67.3 60.0 0.05 0.04
B3sys MUC6 53.1 78.9 63.5 56.4 76.3 64.1 0.08 0.03
MUC7 49.8 80.0 61.4 53.3 76.1 62.7 0.05 0.01
ACE 2003 66.9 87.7 75.9 71.5 83.3 77.0 0.07 0.03
ACE 2004 64.7 85.7 73.8 67.3 83.4 74.5 0.07 0.03
CEAFsys MUC6 56.9 53.0 54.9 62.2 57.5 59.8 0.08 0.03
MUC7 57.3 54.3 55.7 58.3 54.2 56.2 0.06 0.01
ACE 2003 71.0 68.7 69.8 71.1 68.3 69.7 0.07 0.03
ACE 2004 67.9 65.2 66.5 68.5 65.5 67.0 0.07 0.03
Table 3: SOON vs. COPA R2 (SOON features, system mentions, bold indicates significant improvement
in F-score over SOON according to a paired-t test with p < 0.05)
SOON B&R
R P F R P F
B3sys 64.7 85.7 73.8 66.3 85.8 74.8
Table 2: Baselines on ACE 2004
B&R ? Bengtson & Roth (2008)2. All systems
share BART?s preprocessing components and our
in-house ACE mention tagger.
In Table 2 we report the performance of SOON
and B&R on the ACE 2004 testing data using
the BART preprocessing components and our in-
house ACE mention tagger. For evaluation we use
B3sys only, since Bengtson & Roth (2008)?s sys-
tem does not allow to easily integrate CEAF.
B&R considerably outperforms SOON (we can-
not compute statistical significance, because we
do not have access to results for single documents
in B&R). The difference, however, is not as big
as we expected. Bengtson & Roth (2008) re-
ported very good results when using true men-
tions. For evaluating on system mentions, how-
ever, they were using a too lenient variant of B3
(Stoyanov et al, 2009) which discards all twinless
mentions. When replacing this with B3sys the dif-
ference between SOON and B&R shrinks.
5.5 Results
In both comparisons, COPA uses the same fea-
tures as the corresponding baseline system.
2http://l2r.cs.uiuc.edu/
?
cogcomp/
asoftware.php?skey=FLBJCOREF
5.5.1 COPA vs. SOON
In Table 3 we compare the SOON-baseline with
COPA using the R2 partitioner (parameters ?? and
? optimized on development data). Even though
COPA and SOON use the same features, COPA
consistently outperforms SOON on all data sets
using all evaluation metrics. With the exception of
the MUC7, the ACE 2003 and the ACE 2004 data
evaluated with CEAFsys, all of COPA?s improve-
ments are statistically significant. When evaluated
using MUC and B3sys, COPA with the R2 parti-
tioner boosts recall in all datasets while losing in
precision. This shows that global hypergraph par-
titioning models the coreference resolution task
more adequately than Soon et al (2001)?s local
model ? even when using the very same features.
5.5.2 COPA vs. B&R
In Table 4 we compare the B&R system (using our
preprocessing components and mention tagger),
and COPA with the R2 partitioner using B&R fea-
tures. COPA does not use the learned features
from B&R, as this would have implied to embed a
pairwise coreference resolution system in COPA.
We report results for ACE 2003 and ACE 2004.
The parameters are optimized on the ACE 2004
data. COPA with the R2 partitioner outperforms
B&R on both datasets (we cannot compute statisti-
cal significance, because we do not have access to
results for single documents in B&R). Bengtson &
Roth (2008) developed their system on ACE 2004
data and never exposed it to ACE 2003 data. We
suspect that the relatively poor result of B&R on
ACE 2003 data is caused by overfitting to ACE
149
B&R COPA with R2 partitioner
R P F R P F
B3sys ACE 2003 56.4 97.3 71.4 70.3 86.5 77.5
ACE 2004 66.3 85.8 74.8 68.4 84.4 75.6
Table 4: B&R vs. COPA R2 (B&R features, system mentions)
2004. Again, COPA gains in recall and loses
in precision. This shows that COPA is a highly
competetive system as it outperforms Bengtson &
Roth (2008)?s system which has been claimed to
have the best performance on the ACE 2004 data.
5.5.3 Running Time
On a machine with 2 AMD Opteron CPUs and 8
GB RAM, COPA finishes preprocessing, training
and partitioning the ACE 2004 dataset in 15 min-
utes, which is slightly faster than our duplicated
SOON baseline.
6 Discussion and Outlook
Most previous attempts to solve the coreference
resolution task globally have been hampered by
employing a local pairwise model in the classifi-
cation step (step 1) while only the clustering step
realizes a global approach, e.g. Luo et al (2004),
Nicolae & Nicolae (2006), Klenner (2007), De-
nis & Baldridge (2009), lesser so Culotta et al
(2007). It has been also observed that improve-
ments in performance on true mentions do not
necessarily translate into performance improve-
ments on system mentions (Ng, 2008).
In this paper we describe a coreference reso-
lution system, COPA, which implements a global
decision in one step via hypergraph partitioning.
COPA looks at the whole graph at once which en-
ables it to outperform two strong baselines (Soon
et al, 2001; Bengtson & Roth, 2008). COPA?s
hypergraph-based strategy can be taken as a gen-
eral preference model, where the preference for
one mention depends on information on all other
mentions.
We follow Stoyanov et al (2009) and argue
that evaluating the performance of coreference
resolution systems on true mentions is unrealis-
tic. Hence we integrate an ACE mention tag-
ger into our system, tune the system towards the
real task, and evaluate only using system men-
tions. While Ng (2008) could not show that su-
perior models achieved superior results on sys-
tem mentions, COPA was able to outperform
Bengtson & Roth (2008)?s system which has been
claimed to achieve the best performance on the
ACE 2004 data (using true mentions, Bengtson &
Roth (2008) did not report any comparison with
other systems using system mentions).
An error analysis revealed that there were some
cluster-level inconsistencies in the COPA output.
Enforcing this consistency would require a global
strategy to propagate constraints, so that con-
straints can be included in the hypergraph parti-
tioning properly. We are currently exploring con-
strained clustering, a field which has been very
active recently (Basu et al, 2009). Using con-
strained clustering methods may allow us to in-
tegrate negative information as constraints instead
of combining several weak positive features to one
which is still weak (e.g. our Agreement feature).
For an application of constrained clustering to the
related task of database record linkage, see Bhat-
tacharya & Getoor (2009).
Graph models cannot deal well with positional
information, such as distance between mentions
or the sequential ordering of mentions in a doc-
ument. We implemented distance as weights on
hyperedges which resulted in decent performance.
However, this is limited to pairwise relations and
thus does not exploit the power of the high de-
gree relations available in COPA. We expect fur-
ther improvements, once we manage to include
positional information directly.
Acknowledgements. This work has been
funded by the Klaus Tschira Foundation, Hei-
delberg, Germany. The first author has been
supported by a HITS PhD. scholarship. We would
like to thank Byoung-Tak Zhang for bringing
hypergraphs to our attention and `Eva Mu?jdricza-
Maydt for implementing the mention tagger.
Finally we would like to thank our colleagues in
the HITS NLP group for providing us with useful
comments.
150
References
Agarwal, Sameer, Jonwoo Lim, Lihi Zelnik-Manor, Pietro
Perona, David Kriegman & Serge Belongie (2005). Be-
yond pairwise clustering. In Proceedings of the IEEE
Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR?05), Vol. 2, pp. 838?845.
Bagga, Amit & Breck Baldwin (1998). Algorithms for scor-
ing coreference chains. In Proceedings of the 1st Inter-
national Conference on Language Resources and Evalu-
ation, Granada, Spain, 28?30 May 1998, pp. 563?566.
Basu, Sugato, Ian Davidson & Kiri L. Wagstaff (Eds.)
(2009). Constrained Clustering: Advances in Algorithms,
Theory, and Applications. Boca Raton, Flo.: CRC Press.
Bengtson, Eric & Dan Roth (2008). Understanding the value
of features for coreference resolution. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu, Hawaii, 25-27
October 2008, pp. 294?303.
Bhattacharya, Indrajit & Lise Getoor (2009). Collective re-
lational clustering. In S. Basu, I. Davidson & K. Wagstaff
(Eds.), Constrained Clustering: Advances in Algorithms,
Theory, and Applications, pp. 221?244. Boca Raton, Flo.:
CRC Press.
Cai, Jie & Michael Strube (2010). Evaluation metrics for
end-to-end coreference resolution systems. In Proceed-
ings of the SIGdial 2010 Conference: The 11th Annual
Meeting of the Special Interest Group on Discourse and
Dialogue, Tokyo, Japan, 24?25 September 2010. To ap-
pear.
Chinchor, Nancy (2001). Message Understanding Confer-
ence (MUC) 7. LDC2001T02, Philadelphia, Penn: Lin-
guistic Data Consortium.
Chinchor, Nancy & Beth Sundheim (2003). Message Under-
standing Conference (MUC) 6. LDC2003T13, Philadel-
phia, Penn: Linguistic Data Consortium.
Culotta, Aron, Michael Wick & Andrew McCallum (2007).
First-order probabilistic models for coreference resolu-
tion. In Proceedings of Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics, Rochester,
N.Y., 22?27 April 2007, pp. 81?88.
Daume? III, Hal & Daniel Marcu (2005). A large-scale ex-
ploration of effective global features for a joint entity de-
tection and tracking model. In Proceedings of the Human
Language Technology Conference and the 2005 Confer-
ence on Empirical Methods in Natural Language Process-
ing, Vancouver, B.C., Canada, 6?8 October 2005, pp. 97?
104.
Denis, Pascal & Jason Baldridge (2009). Global joint models
for coreference resolution and named entity classification.
Procesamiento del Lenguaje Natural, 42:87?96.
Klenner, Manfred (2007). Enforcing consistency on coref-
erence sets. In Proceedings of the International Confer-
ence on Recent Advances in Natural Language Process-
ing, Borovets, Bulgaria, 27?29 September 2007, pp. 323?
328.
Luo, Xiaoqiang (2005). On coreference resolution perfor-
mance metrics. In Proceedings of the Human Language
Technology Conference and the 2005 Conference on Em-
pirical Methods in Natural Language Processing, Van-
couver, B.C., Canada, 6?8 October 2005, pp. 25?32.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla & Salim Roukos (2004). A mention-
synchronous coreference resolution algorithm based on
the Bell Tree. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pp. 136?143.
Mitchell, Alexis, Stephanie Strassel, Shudong Huang &
Ramez Zakhary (2004). ACE 2004 Multilingual Training
Corpus. LDC2005T09, Philadelphia, Penn.: Linguistic
Data Consortium.
Mitchell, Alexis, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman, Adam
Meyers, Ada Brunstain, Lisa Ferro & Beth Sundheim
(2003). TIDES Extraction (ACE) 2003 Multilingual
Training Data. LDC2004T09, Philadelphia, Penn.: Lin-
guistic Data Consortium.
Ng, Vincent (2008). Unsupervised models for corefer-
ence resolution. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Processing,
Waikiki, Honolulu, Hawaii, 25-27 October 2008, pp. 640?
649.
Ng, Vincent & Claire Cardie (2002). Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, Philadelphia, Penn., 7?12
July 2002, pp. 104?111.
Nicolae, Cristina & Gabriel Nicolae (2006). BestCut: A
graph algorithm for coreference resolution. In Proceed-
ings of the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, Sydney, Australia, 22?23 July
2006, pp. 275?283.
Rahman, Altaf & Vincent Ng (2009). Supervised models
for coreference resolution. In Proceedings of the 2009
Conference on Empirical Methods in Natural Language
Processing, Singapore, 6-7 August 2009, pp. 968?977.
Shi, Jianbo & Jitendra Malik (2000). Normalized cuts and
image segmentation. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 22(8):888?905.
Soon, Wee Meng, Hwee Tou Ng & Daniel Chung Yong
Lim (2001). A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics,
27(4):521?544.
Stoyanov, Veselin, Nathan Gilbert, Claire Cardie & Ellen
Riloff (2009). Conundrums in noun phrase coreference
resolution: Making sense of the state-of-the-art. In Pro-
ceedings of the Joint Conference of the 47th Annual Meet-
ing of the Association for Computational Linguistics and
the 4th International Joint Conference on Natural Lan-
guage Processing, Singapore, 2?7 August 2009, pp. 656?
664.
Versley, Yannick, Simone Paolo Ponzetto, Massimo Poesio,
Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng
Yang & Alessandro Moschitti (2008). BART: A mod-
ular toolkit for coreference resolution. In Companion
Volume to the Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics, Colum-
bus, Ohio, 15?20 June 2008, pp. 9?12.
Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly
& Lynette Hirschman (1995). A model-theoretic corefer-
ence scoring scheme. In Proceedings of the 6th Message
Understanding Conference (MUC-6), pp. 45?52. San Ma-
teo, Cal.: Morgan Kaufmann.
von Luxburg, Ulrike (2007). A tutorial on spectral clustering.
Statistics and Computing, 17(4):395?416.
Yang, Xiaofeng, Jian Su & Chew Lim Tan (2008). A twin-
candidate model for learning-based anaphora resolution.
Computational Linguistics, 34(3):327?356.
151
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 644?655, Dublin, Ireland, August 23-29 2014.
Unsupervised Coreference Resolution
by Utilizing the Most Informative Relations
Nafise Sadat Moosavi and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
{nafise.moosavi|michael.strube}@h-its.org
Abstract
In this paper we present a novel method for unsupervised coreference resolution. We introduce a
precision-oriented inference method that scores a candidate entity of a mention based on the most
informative mention pair relation between the given mention entity pair. We introduce an infor-
mativeness score for determining the most precise relation of a mention entity pair regarding the
coreference decisions. The informativeness score is learned robustly during few iterations of the
expectation maximization algorithm. The proposed unsupervised system outperforms existing
unsupervised methods on all benchmark data sets.
1 Introduction
Due to the advent of the internet, the world wide web, social media, the electronic distribution of infor-
mation and new means of communication, the amount of text available in many different languages is
rising. Natural language processing (NLP) is in charge of automatic processing this growing data. NLP
research has mainly focused on English and very few other languages. Therefore there is a rich set of
annotated corpora for linguistic analysis tasks for these languages. However, there are no such corpora
for thousands of other languages. Since unsupervised methods do not require annotated data for learning
a model, employing unsupervised methods has become a popular and important area of research in NLP.
In this paper, we propose a new precision oriented method for unsupervised coreference resolution.
Our method evaluates the candidate entities of mentions based on the most precise relation of each
mention and its candidate entity. Though we develop and evaluate our method for the English language,
we intend to apply it to low resource languages in the future.
Common coreference resolution approaches rely on a combination of different features for each de-
cision (for an overview over such approaches, see Ng (2010)). However, a few approaches break down
this combination having precision in mind (Baldwin, 1997; Zhou and Su, 2004; Haghighi and Klein,
2009; Lee et al., 2013). The idea of starting with high precision knowledge is used in various NLP tasks
including parsing (Borghesi and Favareto, 1982), word alignment (Brown et al., 1993), and named en-
tity classification (Collins and Singer, 1999) with different names like ?islands of reliability?, ?stepping
stones?, and ?cautiousness?. Lee et al. (2013) is a successful recent work that implements this idea as
?sieve architecture?. Lee et al. (2013) first decide on the basis of more precise features, and then they
extend these decisions by using less precise features in later sieves. In this system less precise knowledge
is used for extending the decisions made by high precision knowledge.
Our proposed inference method goes in the same direction but in a different way. The probability of
each coreference decision is computed based on a single relation of a mention-entity. This single relation
is the most precise relation that exist between the mention-entity. In contrast to Lee et al. (2013), our
inference method will never take into account less precise relations if more precise ones are present. The
relative precision of relations can be determined based on our linguistic intuition. If we would rely on
linguistic intuition, our system would look much like Lee et al.?s (2013)?s system, except that it processes
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
644
all mentions in a single sieve, instead of iterating over all mentions for each input relation. However, it
is not a trivial task to determine the relative importance of relations for each new relation, new domain,
or new language. In this regard, we propose an informativeness score for automatically determining the
relative precision of relations.
The informativeness score is computed based on the distinguishing power of relations among corefer-
ring and non-coreferring mentions. We learn the informativeness score in an unsupervised way via few
iterations of the Expectation Maximization (EM) algorithm. Overall, our inference method first finds the
most precise relation that a mention has with its candidate entity based on the computed informativeness
scores. It then computes the probability of joining the mention to the entity based on this best relation
and its distribution among all candidate entities.
We empirically validate our approach on the OntoNotes and ACE data sets, showing that despite being
entirely unsupervised, our system performs well on all benchmark data sets.
2 Related Work
Early coreference resolution systems were mainly rule-based systems (Lappin and Leass, 1994; Bald-
win, 1997). The success of statistical approaches in different NLP tasks together with the availability
of coreference annotated corpora (like MUC-6 (Chinchor and Sundheim, 2003) and MUC-7 (Chinchor,
2001)) facilitated a shift from deploying rule-based methods to machine learning approaches in corefer-
ence research in the 1990s.
The increasing importance of multilingual processing, brought the deployment of semi-supervised
and unsupervised methods into attention for automatic processing of limited resource languages. There
are several works which treat coreference resolution as an unsupervised problem (Cardie and Wagstaff,
1999; Angheluta et al., 2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighi
and Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011). We compare our results with the
unsupervised systems of Haghighi and Klein (2007), Poon and Domingos (2008), Haghighi and Klein
(2009), and Kobdani et al. (2011). The Haghighi and Klein (2010) approach is an almost unsupervised
approach, and we do not include this system in our comparisons.
We use the expectation maximization algorithm for unsupervised learning. EM has been previously
used for coreference resolution (Cherry and Bergsma, 2005; Ng, 2008; Charniak and Elsner, 2009).
Cherry and Bergsma (2005) and Charniak and Elsner (2009) use EM for pronoun resolution, and Ng
(2008) models coreference resolution as EM clustering. The model parameters of Ng (2008) are of the
form P (f
1
, . . . , f
k
|C
ij
), where f
i
is a feature, and C
ij
corresponds to the coreference decision of two
mentionsm
i
andm
j
. These parameters along with the entity set, are two sets of unknown variables in Ng
(2008). He computes the posterior probabilities of entities in the E-step, and determines the parameters
from the N-best clustering (i.e. estimated entities) in the M-step. Ng (2008) starts from an initial guess
about the entities and determines the parameters based on this initial guess (M-step). In order to compute
the N-best clustering, Ng (2008) uses the Bell tree approach of Luo et al. (2004).
The informativeness scores of mention pair relations (Section 3.2.1) are our unknown parameters.
Our inference method only requires the ranking of the informativeness scores (and not their exact val-
ues). Therefore, it is much easier to estimate the ranking of these parameters than parameters like
P (f
1
, . . . , f
k
|C
ij
), and our search space for finding an optimized ranking of the informativeness scores
is very small. Since it is easier to have an initial guess about the ranking of informativeness scores (rather
than guessing an initial entity set), we start from an E-step with a random ranking.
In our experiments, EM converges very fast regardless of the initial state. Indeed, in the M-step, we
use our new inference method for computing an estimation of entities. The use of the EM algorithm in
our approach is discussed in more detail in Section 3.3.
3 Method Description
Our coreference resolution method is a mention-entity approach which works at mention-mention granu-
larity for processing candidate entities. It estimates entities incrementally while processing the mentions.
645
For resolving each mention, our inference method scores all candidate entities. For scoring each candi-
date entity, it first finds the most informative mention-mention relation that exists between the mention
and the candidate entity. It then computes the probability of joining the mention to the entity (i.e. the
score of the candidate entity) based on the distribution of this relation among all candidate entities of the
mention.
In order to find the best mention-mention relation of a mention and an entity, we introduce an informa-
tiveness score that scores mention pair relations based on their association with coreference links. This
measure is a global measure, and it is computed based on the association analysis of the mention pair
relations and coreference links on a whole entity set of all input documents.
We learn the informativeness score in an unsupervised way by using the EM algorithm. Inference is
performed at each E-step of the EM iterations. At each E-step, the whole set of entities is constructed
from scratch. The informativeness score of the input relations is computed in the M-step based on the
estimated entities of the E-step.
3.1 Notations
Assume that M is a mention set of the input document, and each document consists of a set of entities E
in which each entity contains one or more mentions of M . R = {r
1
, . . . , r
K
} is a set of input relations
with the following property:
?r ? R : r(m,n) ? {0, 1} (1)
where m and n are two mentions and r can be any arbitrary relation between two mentions like having a
specific feature-value (in which the feature can be a combinational feature), or a linguistic rule.
In order to capture the natural left-to-right ordering of mentions, r(m,n) is zero when n is positioned
after m in the input document.
3.2 Inference Method
The inference method processes mentions in the text from the beginning of a document to its end. Ini-
tially, each mention is in its own entity. For each mention m ? M , all partial entities that have been
estimated so far (i.e. entities constructed while processing mentions which are positioned before m) are
considered as candidate entities of m (i.e. E
m
).
For each candidate entity u, the inference method first determines the best relation among all existing
mention pair relations betweenm and u that can indicate a coreference link based on the informativeness
score. We call this relation r
u
:
r
u
= argmax
r?R
(IS(r)?max
n?u
r(m,n)) (2)
where IS(r) is the informativeness score of the r relation.
Apparently, when IS(r)?max
n?u
r(m,n) is equal to zero, u will be removed from E
m
.
After finding the most informative relation that exists between m and u (i.e. r
u
), we compute the
probability of joining m to u based on r
u
as follows:
Pr[m? u] =
?
n?u
r
u
(m,n)
?
v?E
m
?
x?v
r
u
(m,x)
(3)
Equation 3 computes the local distribution of r
u
among all entities belonging to E
m
. After computing
the probability of Equation 3 for all candidate entities, m will be joined to the u? that has the highest
probability:
u? = argmax
u?E
m
Pr[m? u] (4)
In case of a tie condition (?
u,v?E
m
Pr[m ? u] = Pr[m ? v]), u? will be the entity whose most
informative relation is more precise than the most informative relation of the other candidates:
u? = argmax
u?E
[max
r?R
(IS(r)?max
n?u
r(n,m))] (5)
646
After finding the best candidate entity of m, the method proceeds to find the best entity of the next
mention, based on the new updated E.
A mentionmwill be left in its own entity in two cases: 1) whenE
m
is empty, and 2) when the value of
Pr[m? u?] is below a predefined threshold. We consider this threshold equal to 0.5 in our experiments.
This threshold indicates situations in which less than half of the occurrences of r
u?
exist between m and
u?, and the others are spread among other entities. This entity can be extended while processing later
mentions or it may remain as a singleton.
Please note that the inference method does not care about the exact values of {IS(r)}, and it only
needs to have a ranking of the informativeness scores for the given relations in order to select the most
informative one.
3.2.1 Informativeness Score
We want to score a set of given relations based on their discriminative power in making coreference
decisions. From a statistical point of view, this can be expressed as to determine whether the existence of
a relation indicates a coreference link or is due to chance. In this regard, we can examine the following
two hypotheses:
Hypothesis 0: P (C = 1|r = 1) = p = P (C = 1|r = 0) (6)
Hypothesis 1: P (C = 1|r = 1) = p
1
6= p
2
= P (C = 1|r = 0) (7)
where C ? {0, 1} is a random variable for coreference decisions.
Hypothesis 0 (null hypothesis) formalizes independence (the coreference decisions are independent
of relation r). Hypothesis 1 formalizes dependence, which in case p
1
 p
2
indicates a strong positive
association between r and C. This is the pattern that we are interested in.
We use theG
2
log-likelihood ratio statistics for testing these hypotheses. The statistics was introduced
to the NLP community by Dunning (1993), and is defined as follows:
?2 log ? = 2 ? log
L(H1)
L(H0)
(8)
where L(H) is the likelihood of a hypothesis based on observed data assuming a binomial probability
distribution for the existence of r between coreferring mentions. Asymptotically, ?2 log ? is ?
2
dis-
tributed with one degree of freedom.
Assuming that we have the whole set of entities of input documents, we can use the maximum likeli-
hood estimator to compute p
1
, p
2
, and p as follows:
p
1
=
?
u?E
?
m?u
?
n?u
n 6=m
r(m,n)
?
x?M
?
y?M
y 6=x
r(x, y)
p
2
=
?
u?E
?
m?u
?
n?u
n 6=m
(1? r(m,n))
?
x?M
?
y?M
y 6=x
(1? r(x, y))
p =
?
u?E
?
m?u
?
n?u
n 6=m
1
?
x?M
?
y?M
y 6=x
1
(9)
The log-likelihood ratio statistics can be used both for filtering out non-informative relations and for
scoring the remaining relations. The filtering is done by comparing the value of ?2 log ? to the desired
threshold value obtained from the ?
2
table (15.0 in our experiments) and removing the relations that are
not significant at the desired level.
Similar to Dunning (1993), the test statistics can be used as a measure for scoring. In our formulation,
the test statistics scores given mention pair relations based on their association with coreference links
in a way that more precise relations (relations that indicate a coreference link more strongly) will get a
647
higher score, and less precise relations (relations that are randomly spread among coreferring and non-
coreferring mentions) will get a lower score.
The formulation of the log-likelihood ratio in Dunning (1993) is a two-tailed statistical test that if p
1
and p
2
significantly diverge from each other, the?2 log ?would get a high value. However, as mentioned
above, we are just interested in the cases that p
1
is much higher than p
2
, because, otherwise, coreference
links among the mentions which have the relation r in common are less frequent than expected.
Therefore, we use the one-sidedness condition as discussed by Kiss and Strunk (2006) for the log-
likelihood test. In this case, a relation r is selected as an informative relation for coreference resolution
when the ?2 log ? is larger than the desired threshold, and also p
1
> p
2
:
IS(r) =
{
?2 log ? if p
1
> p
2
0 otherwise
(10)
We compute the values of {IS(r)} based on entities of the whole set of input documents in order to
have a global estimation of the associations in the input data. In order to have a domain- or genre-
specific model, one should learn different {IS(r)} for each different domain/genre. The domain/genre
adaptation is discussed in more detail in the discussion part.
3.3 Learning Method
From what we have discussed so far, {IS(r)} values and document entities (E) are two unknown sets of
variables that we want to find. When {IS(r)} is known, we can estimate entities by using the inference
method described in Section 3.2. When the entities are known, we can compute the {IS(r)} as described
in Section 3.2.1. We can see that these two steps (i.e. determining entities and the informativeness scores),
correspond to the E- and M-steps of the expectation maximization algorithm, respectively.
Expectation maximization is an iterative procedure for computing the maximum likelihood estimator
of a parameter set when only a subset of data is available. The EM model involves some hidden variables
(Z), observed data (X) and a set of unknown parameters (?). In our modeling, the informativeness scores
are the unknown parameters, the observed data is a set of relations corresponding to R, and entities are
hidden variables.
In the M-step, the model estimates {IS(r)} by using the association analysis of mention pair relations
and coreference links over the entire entity set of the input documents. In the E-step, the algorithm per-
forms the inference method of Section 3.2 and reconstructs the whole set of entities based on the given
{IS(r)} values. As mentioned before, the inference method only needs the ranking of the informative-
ness scores, and therefore different values of {IS(r)} with similar ordering will lead to the same result.
Our model starts from an initial E-step, in which the values of {IS(r)} are ranked randomly. The itera-
tion between the E- and M-steps continues until {IS(r)} converges to steady values. The convergence
and the initial state of the EM algorithm are discussed in more detail in the discussion part.
4 Experiments
4.1 Mention Pair Relations
Here is the list of pairwise relations that we use for common and proper nouns:
? String match: Two mentions have the same string after removing their post-modifiers.
? Compatible head match: Two mentions have the same head, and the pre-modifiers of the anaphor
are a subset of the pre-modifiers of the antecedent.
? Proper head match: Two proper names have the same head, and they do not contain numeric or
location pre-modifiers.
? Substring: All words of the anaphor appear in the antecedent (possibly in different order).
? Acronym: One mention is an acronym of the other.
648
For the ACE data, we use additionally the following relations:
? Apposition: Two mentions are in an apposition structure.
? Demonym: One mention is a name for a resident of a place that derives from the name of the place,
and the other mention is the place name itself.
? Predicate nominative: The anaphor follows a linking verb and renames or describes the subject
mention.
? Role apposition: The antecedent (with a noun head) is a modifier of a noun phrase whose head is
the anaphor.
For the OntoNotes data sets, Same speaker (Lee et al., 2013) is the only feature for resolving pronouns.
For the ACE data Relative pronoun (i.e. the anaphor is a relative pronoun that modifies the head of the
antecedent) is also used. Pronouns, for which we do not have any feature, are linked to the nearest
antecedent (based on the Hobbs distance) that currently belongs to a partial entity which is compatible
with the pronoun. The compatibility is measured in terms of number, gender, person, animacy, and
named entity label. This approach corresponds to the pronoun resolution strategy of the Stanford system.
The differences between the relations of the OntoNotes and ACE corpora is due to the fact that these
two corpora have different annotation schemes. Some of the relations mentioned (e.g. Apposition) are
considered as coreference relations only in the ACE data.
4.2 Data
We evaluate our method on the following data sets:
? OntoNotes-Dev: Development set of the OntoNotes data provided by the CoNLL2012 shared task
(Pradhan et al., 2012). This data set consists of 303 documents.
? OntoNotes-Test: Test set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhan
et al., 2012). This data set consists of 322 documents.
? ACE2004-nwire: Newswire subset of the ACE 2004 data set consisting of 128 documents. This
split of ACE2004 has been utilized in previous work (Poon and Domingos, 2008; Finkel and Man-
ning, 2008; Haghighi and Klein, 2009; Lee et al., 2013).
? ACE2004-Culotta-Test: One of the test splits of the ACE 2004 data set that has been used in
previous work (Culotta et al., 2007; Bengtson and Roth, 2008; Haghighi and Klein, 2009; Lee et
al., 2013). This data set consists of 107 documents.
? ACE2003-BNEWS: BNEWS subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani
et al. (2011) consisting of 51 documents.
? ACE2003-NWIRE: NWIRE subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani et
al. (2011) consisting of 29 documents.
4.3 Preprocessing
The mention detection of the Stanford coreference system (Lee et al., 2013) is used for the OntoNotes
data sets. We use the predicted information in the OntoNotes data sets for named entity labels, and
syntactic roles. For experiments on the ACE data sets, gold mentions are used, so that comparison with
previous work is possible. For preprocessing, the Stanford parser (Klein and Manning, 2003) and named
entity recognizer (Finkel et al., 2005) are deployed.
We also use the singleton detection of the Stanford system (Recasens et al., 2013) for the OntoNotes
data sets. When both mentions are detected as a singleton by the singleton detection module, the value
of all their corresponding relations will be set to zero. In other words, r(m,n) is set to zero when both
n and m have been detected as a singleton. For examining the effect of the singleton detection module
649
MUC B
3
CEAF
e
Avg.
System R P F1 R P F1 R P F1 F1
OntoNotes-Test
Supervised
Berkeley 67.48 72.97 70.12 54.4 61.94 57.92 53.84 55.48 54.65 60.90
IMS 65.23 70.10 76.58 49.41 60.69 54.47 51.34 49.14 50.21 57.42
Rule-based Stanford 63.95 65.43 64.68 48.65 56.66 52.35 51.04 46.77 48.81 55.28
Unsupervised This Work 65 64.27 64.64 49.96 55.35 52.52 51.82 46.66 49.11 55.42
OntoNotes-Dev
Unsupervised
This Work 65.05 65.69 65.37 51.78 58.31 54.85 54.26 48.72 51.34 57.19
? Singleton 65.44 63.83 64.62 52.26 56.29 54.2 54.63 46.45 50.21 56.34
& Genre 65.09 65.7 65.39 51.84 58.31 54.89 54.26 48.75 51.36 57.21
Table 1: Experimental results on OntoNotes data sets.
in our inference method, we evaluate our system without this module. The result is shown in Table 1
(specified as ?? Singleton?). The results of the Stanford system are also reported using the singleton
detection module of Recasens et al. (2013).
MUC B
3
System R P F1 R P F1
ACE2003-NWIRE
This Work 72.92 86.13 78.98 74.68 90.05 81.65
Haghighi07 44.7 55.5 49.5 - - -
Ng08 47.0 68.3 55.7 - - -
Kobdani11 (UNSEL) 68.6 64.8 66.6 73.6 61.5 67.0
ACE2003-BNEWS
This Work 67.36 84.72 75.05 70.35 89.56 78.80
Haghighi07 56.8 68.3 62.0 - - -
Ng08 56.1 71.4 62.8 - - -
Kobdani11 (UNSEL) 65.0 69.5 67.1 65.9 70.2 68.0
ACE2004-nwire
This Work 74.77 84.53 79.35 74.21 87.50 80.31
Haghighi07 62.3 66.7 64.2 - - -
Poon08 71.3 70.5 70.9 - - -
Haghighi09 75.09 77.0 76.5 74.5 79.4 76.9
ACE2004-Culotta-Test
This Work 68.88 82.42 75.04 73.62 88.87 80.53
Haghighi09 77.7 74.8 79.6 78.5 79.6 79.0
Table 2: Comparison with other unsupervised systems on ACE data sets.
4.4 Results
We evaluate our proposed model with the most commonly used metrics for coreference resolution: for
the OntoNotes data sets MUC (Vilain et al., 1995), B
3
(Bagga and Baldwin, 1998), CEAF (Luo, 2005)
and their average F1 as used in the CoNLL 2011 and 2012 shared tasks; for the ACE data sets MUC
and B
3
. The experimental results for the OntoNotes and ACE data sets are presented in Tables 1 and 2,
respectively.
On the OntoNotes test set, we compare our method with the three best publicly available coreference
systems including the Berkeley system (Durrett and Klein, 2013), the IMS system (Bj?orkelund and
Farkas, 2012), and the Stanford system (Lee et al., 2013; Recasens et al., 2013). The Berkeley and IMS
systems are both supervised approaches with a rich set of lexical features. At the other hand, the Stanford
system is a deterministic system with a set of entity-level features that needs to go through all mentions
for incorporating each of the input features. The Stanford system is the winner of the CoNLL2011 shared
650
OntoNotes-Dev
Same speaker > Compatible head match > Substring > String match > Proper head match > Acronym
ACE2004-nwire
Compatible head match > Substring > Proper head match > String match > Demonym >
Apposition > Same speaker > Role apposition > Relative pronoun > Acronym > Predicate nominative
Table 3: The resulting ranking of informativeness scores on different data sets.
task. The IMS system is the 3rd best system on the CoNLL2012 shared task. The Berkeley system is
a state-of-the-art supervised coreference system that outperforms both the Stanford and IMS systems.
Despite being totally unsupervised and using pairwise features, the results of our system are on par with
those of the Stanford system (according to the approximate randomization test, there is no significant
difference). The comparison with this state-of-the-art rule based system (Lee et al., 2013), indicates the
effectiveness of our coreference resolution approach, as it uses the same preprocessing modules and a
simpler and smaller set of features. All results in the Table 1 are reported using the scorer-v7
1
of the
CoNLL-2012 shared task (Pradhan et al., 2014).
On the ACE data sets, we compare our performance to those of the unsupervised systems mentioned
in Section 2. As Table 2 shows, our method considerably outperforms other unsupervised systems on all
data sets (except only for the MUC measure on the ACE2004-Culotta-Test data set).
5 Discussion
5.1 Informativeness Score
As discussed in Section 3.2.1, we determine the discriminative power of mention pair relations in corefer-
ence decisions based on the informativeness score (Equation 10), in which the statistical test is computed
on the unsupervised estimated set of entities. The resulting ranking of the informativeness score for our
input relations is presented in Table 3 on both OntoNotes and ACE data sets.
Another point that needs to be mentioned here is that we are currently using a set of simple and
precise input relations. While using these input relations, the informativeness score cannot be efficiently
used. The effectiveness of our informativeness score can be usefully assessed with complex relations
(i.e. combinatorial features). However, learning of the informativeness scores for complex relations is
not possible in a totally unsupervised configuration and one should at least use an informative initial state
to guide the learning. We address this issue in our future work.
5.2 Domain/Genre Adaptation
The OntoNotes data set has seven genres regarding the type of text?s sources: newswire (NW), broadcast
news (BN), broadcast conversation (BC), magazine (MZ), telephone conversation (TC), web data (WB),
pivot text (PT). Domain or genre adaptation is one of the current obstacles in language processing. In
order to test the effect of genre adaptation in our approach, we try a variant of our approach in which the
informativeness scores of the input relations (i.e. {IS(r)}) are learned separately for each genre. The
results of this evaluation are presented in Table 1 by the name ?& Genre?.
As can be seen in Table 1, the genre-specific variant of our system is performing as well as the base
version. This experiment indicates the robustness of our approach regarding the genre/domain adapta-
tion. It can learn an appropriate approximation of the informativeness scores from a small amount of
data (i.e. the data provided for a single genre instead of the data from all genres). The learned orderings
of the informativeness scores for all genres are presented in Table 4.
When evaluated on each genre separately, the system has the best performance on PT, and the worst
performance on the WB genre. The total ordering of genres based on the performance of our system is
1
http://conll.cemantix.org/2012/software.html
651
Broadcast conversation, Web data
Same speaker > Compatible head match > Substring > String match > Proper head match > Acronym
Telephone conversation
Same speaker > Compatible head match > Substring > String match > Proper head match
Broadcast news, Newswire
Substring > Compatible head match > String match > Proper head match > Same speaker > Acronym
Pivot text
Same speaker > Compatible head match > String match > Substring > Proper head match
Magazine
Compatible head match > Substring > String match > Proper head match > Same speaker > Acronym
Table 4: The genre-specific ranking of informativeness scores.
as follow: PT, MZ, TC, BN, NW, BC, WB.
5.3 EM Initial State and Convergence
For the initial state of our EM algorithm, we need a ranking of the informativeness scores of the input
relations. We try different initial states for the EM algorithm, from an informative ranking based on
linguistic intuition about the precision of input relations to a misleading ranking (the informative order
reversed). However, in all cases, the EM algorithm leads to the same ranking (as listed in Table 3). This
indicates the robustness of our modeling.
It is more likely that a more precise relation will also get a higher value for its corresponding join
probability of Equation 3, because it is unlikely that a precise relation connects a mention to several
candidate entities. However, relations with low precision may connect a mention to several different
entities, because they are spread over more different entities than relations with higher precision.
In our experiments, for all tested initial states, the model converges in 4 iterations on the OntoNotes
data sets and 5 iterations on the ACE data sets.
5.4 Promising Alternative for the Stanford System
Our coreference resolution method is a self- contained approach, that does not need any external linguis-
tic knowledge regarding the coreference relations. However, we can also consider a simple variant of
this system in which a predefined ordering of features (based on linguistic intuition) is given, like the
Stanford system. In this case, the EM algorithm will be no longer needed, and therefore, the algorithm
resolves all mentions in a single iteration.
Therefore, this variant of our system can be considered as an efficient alternative to the Stanford
system, that uses a simpler (pairwise instead of entity-based) and smaller (5 instead of 7 string matches)
set of relations, and more importantly processes all mentions in a single iteration (instead of iterating
over all mentions for each relation), and it still performs as well as its entity-based multi-sieve variant.
6 Conclusions
In this paper, we presented a new unsupervised coreference resolution method. We deploy a new
precision-oriented inference method that decides about joining a mention to a candidate entity based
on only the most informative mention pair relation that exists between the given mention entity pair.
In order to determine the most informative relation of a mention and its candidate entity, we introduce
an informativeness score for scoring mention-mention relations based on their global association with
652
coreference links. A relation whose existence strongly indicates a coreference link will get a high score,
and a relation which is randomly spread among coreferring and non-coreferring mentions will get a low
score. The informativeness score is robustly learned during a very few iterations of the EM algorithm.
Our proposed method performs well on all benchmark data sets. In the future we intend to apply this
robust and efficient approach to new genres, domains, and also new languages.
Acknowledgments
The authors would like to thank Sebastian Martschat for his helpful comments. This work has been
funded by the Klaus Tschira Foundation, Heidelberg, Germany. The first author has been supported by a
Heidelberg Institute for Theoretical Studies PhD. scholarship.
References
Roxana Angheluta, Patrick Jeuniaux, Rudradeb Mitra, and Marie-Francine Moens. 2004. Clustering algorithms
for noun phrase coreference resolution. In Proceedings of the 7`emes Journ?ees Internationales d?Analyse Statis-
tique des Donn?ees Textuelles, Louvain La Neuve, Belgium, 10?12 March 2004, pages 60?70.
Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the 1st
International Conference on Language Resources and Evaluation, Granada, Spain, 28?30 May 1998, pages
563?566.
Breck Baldwin. 1997. CogNIAC: High precision coreference with limited knowledge and linguistic resources.
In Proceedings of the ACL Workshop on Operational Factors in Practical, Robust Anaphora Resolution for
Unrestricted Text, Madrid, Spain, July 1997, pages 38?45.
Eric Bengtson and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings
of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25?
27 October 2008, pages 294?303.
Anders Bj?orkelund and Rich?ard Farkas. 2012. Data-driven multilingual coreference resolution using resolver
stacking. In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language
Learning, Jeju Island, Korea, 12?14 July 2012, pages 49?55.
Luigi Borghesi and Chiara Favareto. 1982. Flexible parsing of discretely uttered sentences. In Proceedings of
the 9th International Conference on Computational Linguistics, Prague, Czechoslovakia, 5?10 July 1982, pages
37?42.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263?311.
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase coreference as clustering. In Proceedings of the 1999
SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, College
Park, Md., 21?22 June 1999, pages 82?89.
Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the Association for Computational Linguistics, Athens, Greece, 30
March ? 3 April 2009, pages 148?156.
Colin Cherry and Shane Bergsma. 2005. An expectation maximization approach to pronoun resolution. Proceed-
ings of the Ninth Conference on Computational Natural Language Learning, pages 88?95.
Nancy Chinchor and Beth Sundheim. 2003. Message Understanding Conference (MUC) 6. LDC2003T13,
Philadelphia, Penn: Linguistic Data Consortium.
Nancy Chinchor. 2001. Message Understanding Conference (MUC) 7. LDC2001T02, Philadelphia, Penn: Lin-
guistic Data Consortium.
Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of
the 1999 SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,
College Park, Md., 21?22 June 1999, pages 100?110.
Aron Culotta, Michael Wick, and Andrew McCallum. 2007. First-order probabilistic models for coreference
resolution. In Proceedings of Human Language Technologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguistics, Rochester, N.Y., 22?27 April 2007, pages 81?88.
653
Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics,
19(1):61?74.
Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Wash., 18?21 October
2013, pages 1971?1982.
Jenny Rose Finkel and Christopher Manning. 2008. Enforcing transitivity in coreference resolution. In Compan-
ion Volume to the Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,
Columbus, Ohio, 15?20 June 2008, pages 45?48.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into in-
formation extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics, Ann Arbor, Mich., 25?30 June 2005, pages 363?370.
Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric Bayesian model.
In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, Czech
Republic, 23?30 June 2007, pages 848?855.
Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore, 6?7
August 2009, pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity centered model. In Proceedings
of Human Language Technologies 2010: The Conference of the North American Chapter of the Association for
Computational Linguistics, Los Angeles, Cal., 2?4 June 2010, pages 385?393.
Tibor Kiss and Jan Strunk. 2006. Unsuperivsed multilingual sentence boundary detection. Computational Lin-
guistics, 32(4):485?525.
Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics, Sapporo, Japan, 7?12 July 2003, pages 423?430.
Hamidreza Kobdani, Hinrich Schuetze, Michael Schiehlen, and Hans Kamp. 2011. Bootstrapping coreference
resolution using word associations. In Proceedings of the 49th Annual Meeting of the Association for Compu-
tational Linguistics, Portland, Oreg., 19?24 June 2011, pages 783?792.
Shalom Lappin and Herbert J. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational
Linguistics, 20(4):535?561.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics,
39(4):885?916.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based on the Bell Tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguistics, Barcelona, Spain, 21?26 July 2004, pages 136?143.
Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of the Human Language
Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, Van-
couver, B.C., Canada, 6?8 October 2005, pages 25?32.
Vincent Ng. 2008. Unsupervised models for coreference resolution. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25?27 October 2008, pages
640?649.
Vincent Ng. 2010. Supervised noun phrase coreference research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 11?16 July 2010,
pages 1396?1411.
Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov Logic. In
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu,
Hawaii, 25?27 October 2008, pages 650?659.
Sameer Pradhan, Alessandro Moschitti, and Nianwen Xue. 2012. CoNLL-2012 Shared Task: Modeling multi-
lingual unrestricted coreference in OntoNotes. In Proceedings of the Shared Task of the 16th Conference on
Computational Natural Language Learning, Jeju Island, Korea, 12?14 July 2012, pages 1?40.
654
Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Eduard Hovy, Vincent Ng, and Michael Strube. 2014. Scoring
coreference partitions of predicted mentions: A reference implementation. In Proceedings of the ACL 2014
Conference Short Papers, Baltimore, Md., 22?27 June 2014. To appear.
Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse
entities: Identifying singleton mentions. In Proceedings of the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Atlanta, Georgia, 9?14 June
2013, pages 627?633.
Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference (MUC-6), pages
45?52, San Mateo, Cal. Morgan Kaufmann.
Guodong Zhou and Jian Su. 2004. A high-performance coreference resolution system using a constraint-based
multi-agent strategy. In the 16th International Conference on Computational Linguistics (COLING).
655
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 183?193, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Local and Global Context
for Supervised and Unsupervised Metonymy Resolution
Vivi Nastase
HITS gGmbH
Heidelberg, Germany
vivi.nastase@h-its.org
Alex Judea
University of Stuttgart
Stuttgart, Germany
alexander.judea@ims.uni-stuttgart.de
Katja Markert
University of Leeds
Leeds, UK
K.Markert@leeds.ac.uk
Michael Strube
HITS gGmbH
Heidelberg, Germany
michael.strube@h-its.org
Abstract
Computational approaches to metonymy res-
olution have focused almost exclusively on
the local context, especially the constraints
placed on a potentially metonymic word by
its grammatical collocates. We expand such
approaches by taking into account the larger
context. Our algorithm is tested on the data
from the metonymy resolution task (Task 8) at
SemEval 2007. The results show that incorpo-
ration of the global context can improve over
the use of the local context alone, depending
on the types of metonymies addressed. As a
second contribution, we move towards unsu-
pervised resolution of metonymies, made fea-
sible by considering ontological relations as
possible readings. We show that such an unsu-
pervised approach delivers promising results:
it beats the supervised most frequent sense
baseline and performs close to a supervised
approach using only standard lexico-syntactic
features.
1 Introduction
With the exception of explicit tasks in metonymy
and metaphor analysis, computational treatment of
language relies on the assumption that the texts to be
processed have a literal interpretation. This contrasts
with the fact that figurative expressions are com-
mon in language, as exemplified by the metonymy
in the excerpt from a Wikipedia article in Exam-
ple 1 and another in Example 2 from the SemEval
2007 metonymy resolution task (Markert and Nis-
sim, 2009).
(1) In the gold medal game, Canada defeated the
American team 2-0 to win their third consecu-
tive gold.
(2) This keyword is only required when your rela-
tional database is Oracle.
The defeating in Example 1 will not be done
by the country as such, but by a team represent-
ing the country in a sporting event. Hence, in a
metonymy a potentially metonymic expression or
word (here Canada) stands for a conceptually re-
lated entity (here, people of Canada). In the sec-
ond Example, a company name (Oracle) stands for
a product (database) developed by the company.
Metonymy resolution can be important for a
variety of tasks. Textual entailment may need
metonymy resolution (Bentivogli et al2007): for
example, we would like to be able to induce from
Example 1 the hypothesis
The Canadian team won . . . .
Leveling and Hartrumpf (2008) show that
metonymy recognition on location proper names
helps geographical information retrieval by ex-
cluding metonymically used place names from
consideration (such as Example 1 or the use of
Vietnam for the Vietnam war). Metonymies also fre-
quently interact with anaphora resolution (Nunberg,
1995; Markert and Hahn, 2002), as in Example 1
where the metonymic use of Canada is referred to
by a plural pronoun afterward (their).
Metonymies can be quite regular: company
names can be used for their management or their
products, country names can be used for associated
sports teams. Following from this, the currently
183
prevalent set-up for metonymy resolution ? as in
the SemEval 2007 task ? provides a manually com-
piled list of frequent readings or metonymic patterns
such as organization-for-product for pre-
specified semantic classes (such as organizations) as
well as annotated examples for these patterns so that
systems can then treat metonymy resolution as a (su-
pervised) word sense disambiguation task. How-
ever, this approach needs novel, manual provision
of readings as well as annotated examples for each
new semantic class.
In contrast, we will see readings as relations be-
tween the potentially metonymic word (PMW) and
other concepts in a large concept network, a priori
allowing all possible relations as readings. We base
this approach on the observation that metonymic
words stand in for concepts that they are related
with ? e.g. the part for the whole, the company
for the product. These readings are obtained on
the fly and are therefore independent of manually
provided, preclassified interpretations or semantic
classes, leading eventually to the possibility of un-
supervised metonymy resolution. We achieve this
by first linking a PMW to an article in Wikipedia.
Then we extract from a large concept network de-
rived from Wikipedia the relations surrounding the
PMW.
As there will be (many) more than one such rela-
tion, these need to be ranked or scored. We achieve
this in a probabilistic framework where we condi-
tion the probability of a relation on the context of
the PMW. This ranking showcases our second major
innovation in that the flexibility of our framework al-
lows us to incorporate a wider context than in most
prior approaches. Let us consider the indications for
metonymic readings and its interpretation in Exam-
ple 1, on the one hand, and Example 2, on the other
hand. In Example 1, the grammatical relation to the
verb defeat and the verb?s selectional preferences in-
dicate the metonymy. We will call all such grammat-
ically related words and the grammatical relations
the local context of the PMW. Such types of local
context have been used by most prior approaches
(Pustejovsky, 1991; Hobbs et al1993; Fass, 1991;
Nastase and Strube, 2009, among others). However,
Example 2 shows that the local context can be am-
biguous or often weak, such as the verb to be. In
these examples, the wider context (database, key-
word) is a better indication for a metonymy but has
not been satisfactorily integrated in prior approaches
(see Section 2). We here call all words surround-
ing the PMW but not grammatically related to it the
global context.
In our approach we integrate both the local and
the global context in our probabilistic framework.
For the local context, we compute the selectional
preferences for the words related to the PMW from a
corpus of English Wikipedia articles and generalize
them in the Wikipedia concept network, thus (auto-
matically) providing a set of abstractions ? general
concepts in the network that capture the semantic
classes required by the local context. In the next
step we compute probabilities of the global con-
text surrounding the PMWs under each (locally re-
quired) abstraction, and combine this with the se-
lectional preferences of the grammatically related
words. That we can integrate local and global con-
text in one probabilistic but also knowledge-based
framework is possible because we combine two de-
scriptions of meaning ? ontological and distribu-
tional ? by exploiting different sources of informa-
tion in Wikipedia (category-article hierarchy and ar-
ticle texts).
We compute the probabilities of the relations (=
readings) between the concept corresponding to the
PMW and its directly related concepts. These can
be used either (i) as additional features in a super-
vised approach or (ii) directly for unsupervised res-
olution. We do both in this paper and show that (i)
the supervised approach using both local and global
context can outperform one using just local con-
text, dependent on the semantic class studied and
(ii) that an unsupervised approach ? although lower
than the supervised one ? outperforms the super-
vised most frequent reading baseline and performs
close to a standard supervised model with the basic
set of lexico-syntactic features (Nissim and Markert,
2005).
2 Related Work
The word sense disambiguation setting for
metonymy resolution as developed by Nissim
and Markert (2005) and used for the SemEval 2007
task (Markert and Nissim, 2009) uses a small, pre-
specified number of frequently occurring readings.
184
The approaches building on this work (Farkas et
al., 2007; Nicolae et al2007, among others) are
supervised, mostly using shallow surface features
as well as grammatical relations.1 Most effective
in the SemEval task as summarized in Markert
and Nissim (2009) has been the local, grammatical
context, with the two systems relying on the global
context or the local/global context in a BOW model
(Leveling, 2007; Poibeau, 2007) not outperforming
the most frequent reading baseline. We believe
that might be due to the lack of a link between the
local and global context in these approaches ? in
our work, we condition the global context on the
abstractions and selectional preferences yielded by
the local context and achieve better results.
Lapata (2003), Shutova (2009) as well as Roberts
and Harabagiu (2011) deal with the issue of logical
metonymy, where the participant stands in for the
full event: e.g. Mary enjoyed the book., where book
stands in for reading the book, and this missing event
(reading) can be inferred from a corpus. Utiyama
et al2000), Lapata (2003) propose a probabilis-
tic model for finding the correct interpretation of
such metonymies in an unsupervised manner. How-
ever, these event type metonymies differ from the
problem dealt with in our paper and the SemEval
2007 task in that their recognition (i.e. their distinc-
tion from literal occurrences) is achieved simply by
grammatical patterns (a noun instead of a gerund or
to-infinitive following the verb) and the problem is
limited to interpretation.
Our view of relations in a concept network being
the interpretations of metonymies is strongly remi-
niscent of older work in metonymy resolution such
as Hobbs et al1993), Fass (1991), Markert and
Hahn (2002) or the use of a generative lexicon and
its relations in Pustejovsky (1991), which also are
unsupervised. However, these approaches lacked
scalability due to the use of small hand-modeled
knowledge bases which our use of a very large
Wikipedia-derived ontology overcomes. In addition,
most of these approaches (Fass, 1991; Hobbs et al
1993; Pustejovsky, 1991; Harabagiu, 1998) rely on
the view that metonymies violate selectional restric-
tions in their immediate, local context, usually those
1Brun et al2007) is semi-supervised but again relies on the
local grammatical context.
imposed by the verbs on their arguments. As can
be seen in the Example 2, this misses metonymies
which do not violate selectional restrictions. Nas-
tase and Strube (2009) use more flexible proba-
bilistic selectional preferences instead of strict con-
straint violations as well as WordNet as a larger tax-
onomy but are also restricted to the local context.
Markert and Hahn (2002) do propose a treatment of
metonymies that takes into account the larger dis-
course in the form of anaphoric relations between
a metonymy and the prior context. However, they
constrain discourse integration to potential PMWs
that are definite NPs and the context to few previous
noun phrases. In addition, their framework uses a
strict rule-based ranking of competing readings that
cannot be easily extended.
The work presented here also relies on a con-
cept network, built automatically from Wikipedia.
This resource provides us with links between enti-
ties in the text, and also a variety of ontological re-
lations for the PMW, that will allow us to identify a
wide variety of metonymic interpretations. Our ap-
proach combines information from the concept net-
work with automatically acquired selectional prefer-
ences as well as a possibility to combine in a prob-
abilistic framework the influence of the local and
global context on the interpretation of a potentially
metonymic word.
3 The Approach
The approach we present takes into account both
the local, grammatical, context and the larger textual
context of a potentially metonymic word. Figure 1
presents a graphical representation of our approach.
On the one hand, the word/term to be interpreted
(the potentially metonymic word/term ? PMW) is
mapped onto a concept in the concept network (Sec-
tion 3.3), which gives us access to the conceptual
relations (Ri) between the PMW and other concepts
(cx ? CRi). On the other hand, any word w gram-
matically related to the PMW via a grammatical re-
lation r provides us with semantic restrictions on the
interpretation of the PMW, namely preferred seman-
tic classes Aj (we call them abstractions) and a se-
lectional preference score.2 These are automatically
2We restrict the grammatical context that provides selec-
tional preferences to verbs or adjectives grammatically related
185
A 1 A 2
1R kR
A n
12c
14c
11c
13c 1n?1c
1nc
k1c k2
c
k4ck3c
kmc
km?1c w1w2w3
wl
w
r
...
...
PMW
p(Ri|Aj) p(Aj|Cont,w,r)
Global context
...
... ...
...
... ...
Figure 1: Metonymy resolution using selectional preferencesAj derived from local contextw and r, semantic relations
Ri to the PMW from a concept network, and the global context surrounding a term to be interpreted
acquired by using a corpus of Wikipedia articles and
a repository of encyclopedic knowledge (presented
in Section 3.1), as described in detail in 3.2. Because
the abstractions Aj and the PMW?s related concepts
(cx) come from the same structured resource, we
can compute the probabilities for each Ri given the
grammatically related word w and the grammatical
relation r. The global context can also easily be
added to the computation, as the probability of each
word in the context relative to an abstraction Aj can
be computed through the resource?s is a hierarchy
and its link to Wikipedia articles. This is detailed in
Section 3.4.
3.1 A concept network obtained from
Wikipedia
We use a Wikipedia article dump (January 2011)
which provided over 3.5 million English articles,
interconnected through a hierarchy of categories
and hyperlinks. This partly structured repository
is transformed into a large-scale multilingual con-
cept network, whose nodes are concepts correspond-
ing to articles or categories in Wikipedia (Nastase
et al2010). Concepts in this network are con-
nected through a variety of semantic relations (e.g.
is a, member of, nationality) derived from category
names and infoboxes. The version of WikiNet used
to the PMW.
had 3,707,718 nodes and 49,931,266 relation in-
stances of 494 types, and is freely available3.
WikiNet is used here as a concept inventory,
and its links and structure to generalize more spe-
cific concepts identified in texts to general concepts.
The fact that nodes in WikiNet correspond to arti-
cles/categories in Wikipedia is used to link article
texts in Wikipedia to general concepts, for the pur-
pose of computing various probability scores (de-
tailed in Section 3.4).
3.2 Selectional preferences and abstractions
To compute selectional preferences we use the set of
English Wikipedia articles, which describe specific
concepts. Wikipedia contributors are encouraged to
insert hyperlinks, which link important terms in an
article to the corresponding articles. A hyperlink
consists of two parts, the actual link (i.e. a URL)
and a phrase to appear in the text. Hyperlinks then
constitute a bridge from the textual level to the con-
ceptual level without the need for word sense dis-
ambiguation. We exploit these links to gather con-
cept arguments for verbs and adjectives, and gen-
eralize these using the concept network built from
Wikipedia.
The corpus of Wikipedia articles was first en-
riched with hyperlinks, making the ?one sense per
3http://www.h-its.org/english/research/
nlp/download/wikinet.php
186
Algorithm 1 computeSelPrefs(G,WkN)
Input: G ? grammatical relation triples
WkN ? WikiNet
M ? maximum number of generalization steps
Output: ?
? = {}
for all (w, r) such that (c, r, w) ? G do
S = {(c, f)|f is the frequency of (c, r, w) in G}
?w,r = S
mdl = MDL(?w,r,S)
for all i = 1,M do
?? = abstract(S,WkN)
mdl?? = MDL(??,S)
if mdl?? < mdl then
?w,r = ??
? = {?w,r} ? ?
return ?
Algorithm 2 MDL(?,S)
Input: ? = {(c, f)} ? a scored list of concepts
S ? the set of observations (concept collocates)
Output: MDL(?,S)
?? =< f1, ..., fn >; (ci, fi) ? ?
remove {(c, f) ? ?|f = 1} // parameter description
length :
L(??|?) = |?|?12 ? log(|S|) // data description length :
for all (c, f) ? ? do
L(S|?, ??) = L(S|?, ??) + f ? log( fhyponyms(c)?|?| )
return L(??|?)? L(S|?, ??)
Algorithm 3 abstract(S,WkN)
Input: S = {(c, f)|(w,R, c) ? G}
WkN ? WikiNet
Output: S ?
S ? = {}
for c|(c, ) ? S do
while c has only one is a link do
c = c?, (c, is a, c?) ?WkN
C = {(c?, c)|(c, is a, c?) ?WkN}
for (c?, c) ? C do
if (c?, f ?) ? S ? then
replace (c?, f ?) with (c?, f ? + f|C| ), (c, f) ? S
in S ?
else
S ?? = {(c?, f)}, (c, f) ? S
// Remove hyponyms.
for all {(c, c?) ? S ?|(c?, is a, c) ?WkN} do
// update frequency f of c
fc = fc + fc? , f ? S
delete c?
return S ?
discourse? assumption ? a phrase that appears as-
sociated with a hyperlink once in the article body
will be associated with the same hyperlink through-
out the article (this applies to the article title as well,
which is not hyperlinked in the article itself). This
new version of the corpus was then split into sen-
tences, and those without hyperlinks were removed.
The remaining 18 million sentences were parsed
with a parallelized version of Ensemble4 (Surdeanu
and Manning, 2010), and we extracted G, the set of
all grammatical relations of the type (verb, depen-
dency, hyperlink) and (adjective, dependency, hy-
perlink), with the hyperlinks resolved to their cor-
responding node (concept) in the network ( |G| =
1,578,413 triples). For each verb and adjective in the
extracted collocations, and for each of their depen-
dency relations, their collocates were generalized in
the network defined by the hypernym/hyponym re-
lations in WikiNet following a method similar to the
Minimum Description Length principle (Li and Abe,
1998).
Essentially, we aimed to determine a small set of
(more general) concepts that describe the set of col-
locates for a word w and grammatical relation r.
Starting from the concept collocates gathered, we
go upwards following WikiNet?s is a links, and for
each node found that covers at least N concept col-
locates (N is a parameter, N=2 in the experiments
presented here), the MDL score of the node is com-
puted (Algorithm 2). We place a limit M on the
number of upward steps in the hierarchy (M=3 in
our experiments). The disjoint set of nodes that has
the lowest overall MDL score is chosen (?), and for
each node in this cut (which we call abstraction),
we compute the selectional preference score, based
on the number of concepts it dominates.
As an example, for the verb defeat, the corpus
leads to collocations such as5:
defeat
nsubj
Earle Page (10357) ? 8, Manuela Maleeva
(1092361) ? 7, New York Yankees
(10128601) ? 5, Tommy Haas (1118005)
? 5, . . .
obj
4http://www.surdeanu.name/mihai/
ensemble/
5The format is:
Article name (Article Id) ? frequency.
187
New York Yankees (10128601) ? 9, Oak-
land Athletics (11641124) ? 6, Phoenix
Suns (11309373) ? 4, Jason Suttie
(10080653) ? 3, Ravana (100234) ? 3, . . .
Determining abstractions and selectional prefer-
ences leads to the following information6:
defeat
nsubj
Martial artists (118977183) ? 0.5, Person
(219599) ? 0.3518, Interest (146738) ?
0.037, . . .
obj
Video games (9570081) ? 0.25, British
games (24489088) ? 0.25, Person (219599)
? 0.1445, Interest (146738) ? 0.1341, . . .
3.3 Linking the PMW to the concept network
In our environment, linking the PMW to the con-
cept network is equivalent to finding its correspond-
ing concept in our ontology, WikiNet. We see this
corresponding concept as the literal reading of the
PMW. Doing so is a non-trivial task (see the Cross-
Lingual Link Discovery task at NTCIR-9 (Tang et
al., 2011) and the Cross-Lingual Entity Linking task
? part of the Knowledge Base Population track ? at
TAC 20117). In our particular setting, where we use
the metonymy data from SemEval 2007, the domain
of the PMW is well defined: locations and compa-
nies, respectively. Using these constraints, finding
the corresponding Wikipedia articles is much sim-
plified, by using the category hierarchy and con-
straining the concepts to fall under the Geography
and Companies categories respectively. When mul-
tiple options are present, we find instead a matching
disambiguation page. In this case we pick the article
that is listed first on this disambiguation page. On
a manually checked random sample, the accuracy of
the approach was 100% (on a sample of 100 PMWs).
3.4 Scoring conceptual relations with local and
global context
We work under the assumption that the concept cor-
responding to the PMW is related to the possible in-
terpretations through a semantic relation, in particu-
lar one that is captured in the concept network. After
6The format is:
Concept name (Concept Id) ? selectional preference score.
7http://nlp.cs.qc.cuny.edu/kbp/2011/
countries : Administrator of, Architect of,
Based in, Built in, Continent, ...
companies : Association, Brand, Company, Dis-
tributed by, Executive of, ...
Table 1: Example conceptual relations
establishing the connection to the resource by link-
ing the PMW to the concept cPMW corresponding to
its literal interpretation (see Section 3.3), we extract
the relations in which it is involved (Ri, i = 1, k),
and the concepts it is connected to through these re-
lations (CRi = {cx|(cPMWRicx)}). Table 1 shows
examples of conceptual relations extracted for com-
panies and countries.
We are interested in computing the likelihood of
a conceptual relation being the correct interpreta-
tion of a PMW, given its local and global context
p(Ri|Cont, w, r).
3.4.1 The local context
The local context considered in this work are all
grammatically related verbs and adjectives w and
their associated grammatical relation r. The gram-
matical analysis (see Section 3.2) provides the set of
abstractions corresponding to the grammatically re-
lated word w and grammatical relation r: Aj , j =
1, n. Remember that these are local context con-
straints on the interpretation of the PMW.
Through the knowledge resource used we can es-
tablish and quantify connections between each cx
and Aj , and thus between eachRi and Aj :
p(Ri|Aj) =
?
x?CRi
p(cx|Aj)(3)
where p(cx|Aj) is the probability of concept cx un-
der abstraction Aj , which is computed based on the
semantic relations in WikiNet:
p(cx|Aj) =
?
H
?
hi?H
p(hi|hi+1)
whereH is in turn each path from cx toAj following
is a links in WikiNet, starting with cx (i.e. h0 = cx)
and ending in Aj . p(hi|hi+1) is the probability of
the child node hi given its ancestor hi+1. Within this
work we assume a uniform probability distribution
in each node:
188
p(hi|hi+1) =
1
|descendants(hi+1)|
Through this, it is straightforward that
?
cx p(cx|Aj) = 1 when cx ranges over all
concepts subsumed by Aj , and is thus a valid
probability distribution.
3.4.2 The global context
The abstractions obtained before are concepts.
We extract all nodes in the network subsumed
by these concepts, and their corresponding articles
in Wikipedia (if they have one). This produces
?abstraction-specific? article sets, based on which
we compute the probability of the global context of
a PMW for each abstraction. We are interested in
the probability of an abstraction, given the context
and the word w and grammatical relation r, which
we compute as:
p(Aj |Cont, w, r) =
p(Cont|Aj , w, r) ? p(Aj , w, r)
p(Cont, w, r)
which, considering that p(Cont, w, r) is the same
for a given context, we approximate as
p(Aj |Cont) ? p(Cont|Aj) ? p(Aj , w, r)
p(Aj , w, r) = p(Aj |w, r)?p(w, r), and we approxi-
mate it through the computed selectional preference
p(Aj |w, r), since p(w, r) is constant for a given ex-
ample to analyze.
p(Cont|Aj , w, r) =
n?
j=1
p(Cont|Aj)p(Aj |w, r)
=
n?
j=1
(
m?
l=1
p(wl|Aj))p(Aj |w, r)
where Cont is the global context consisting of m
words wl, l = 1,m.8
8The global context therefore could be all words in a text
or all words in a sentence or any other token-based definition
in our framework. As the SemEval 2007 data gives metonymic
examples in a three-sentence context we use all the words in the
3 sentences as our global context.
p(wl|Aj) =
count(wl,Aj)
|Aj |
where Aj is the set of articles subsumed by abstrac-
tion Aj , and count(wl,Aj) is the number of times
word wl appears in the article collection Aj .
3.4.3 Putting it all together
This enables us now to compute p(Ri|Cont, w, r)
based on the formulas 3, 4:
p(Ri|Cont, w, r) =
n?
j=1
(p(Ri|Aj)?p(Aj |Cont, w, r))
4 Experiments
The computed probabilities for each conceptual re-
lation (= potential readings) of the PMW in the con-
cept network can be used as features in a supervised
framework or directly as an unsupervised prediction,
returning the most likely conceptual relation given
the context as the required reading.
Although the latter is our ultimate goal, to allow
comparison with related work from the metonymy
resolution task (Task 8) at SemEval 2007, we first
investigate the supervised set-up. We then simulate
the unsupervised setting in Section 4.3.
4.1 Data
We use the data from the metonymy resolution task
(Task 8) at SemEval 2007. It consists of training and
test data for country and company names which are
potentially metonymic. Table 2 shows the statistics
of the data, and the possible interpretations for the
PMWs. The training-test division was achieved ran-
domly so that the test data can have metonymic read-
ings for which no training data exists, showing again
the limitations of a supervised approach of prespec-
ified readings.
Grammatical features The features used by Nis-
sim and Markert (2005), and commonly used for
the supervised classification of metonymy readings
(Markert and Nissim, 2009):
? grammatical role of PMW (subj, obj, ...);
? lemmatized head/modifier of PMW (announce,
say, ...);
189
reading train test
locations 925 908
literal 737 721
mixed 15 20
othermet 9 11
obj-for-name 0 4
obj-for-representation 0 0
place-for-people 161 141
place-for-event 3 10
place-for-product 0 1
organizations 1090 842
literal 690 520
mixed 59 60
othermet 14 8
obj-for-name 8 6
obj-for-representation 1 0
org-for-members 220 161
org-for-event 2 1
org-for-product 74 67
org-for-facility 15 16
org-for-index 7 3
Table 2: Statistics for the Task 8 data
? determiner of PMW (def, indef, bare, demonst,
other, ...);
? grammatical number of PMW (sg, pl);
? number of grammatical roles in which the
PMW appears in its current context;
? number of words in PMW.
All these features can be extracted from the gram-
matically annotated and POS tagged data provided
by the organizers.
The annotations provided are dependency rela-
tions, many of which contain a preposition as an ar-
gument (e.g. (to, pp, UK) from the example ... the
visit to the UK of ...). Such relations are not infor-
mative, but together with the head that dominates the
prepositional complement (e.g. visit to) they may be.
Because of this, we process the provided annotations
and add wherever possible to the simple prepositions
the head of their subsuming constituent. This would
change the above mentioned dependency to (visit,
prep-to, UK).
Semantic relations as features To evaluate the
proposed approach we use the PMW?s conceptual
relations as features. The feature values are the
p(Ri|Cont, w, r) scores.
For the ?countries? portion of the data this adds
109 semantic relation features, and for companies
29 features. Table 1 showed examples of these new
features.
4.2 Supervised learning
We use the SMO classifier in the WEKA machine
learning toolkit (Witten and Frank, 2000) with its
standard settings, training on the SemEval 2007
(Task 8) training set.
Table 3 shows the results of various configura-
tions on the test data, in comparison with a most
frequent reading baseline (assigning literal to all
PMWs) as well as a system M&N that shows the re-
sults computed using only the features proposed by
Nissim and Markert (2005). In addition, we com-
pare to the best results9 at SemEval 2007 (SEmax)
and Nastase and Strube (2009) (N09). Nastase and
Strube (2009) added WordNet supersenses as fea-
tures, and their values are selectional preferences
computed with reference to WordNet. These are
similar to our abstractions, which in our approach
serve to link the local and the global context to the
ontological relations, but do not appear as features.
Our system SP shows the results obtained us-
ing the M&N features plus the conceptual relation
features conditioned on both local and global con-
text whereas SPlocal and SPglobal use conceptual
relations conditioned on local (p(Aj |Cont, w, r) ?
p(Aj |w, r)) or global context (p(Aj |Cont, w, r) ?
p(Aj |Cont) =
?n
j=1(
?m
l=1 p(wl|Aj))) only.
While the differences in overall accuracies are
small, there are significant differences in classifying
individual classes, as shown in Tables 4 ? 510, where
the distrib. column shows the class distribution in
the test data. It is interesting to note that, in our set-
ting, the global context is more useful than the local
9We show the best result for each category, not necessarily
from the overall best performing system. This holds for Tables
4 and 5 as well.
10The detailed results for previous approaches are reproduced
from (Nastase and Strube, 2009). We include only the classes
that have a non-zero F-score for at least one of the presented
approaches.
190
task ? method? baseline SEmax N09 M&N SP SPlocal SPglobal SPunsup
LOCATION-COARSE 79.4 85.2 86.1 83.4 85.8 83.0 85.0 81.6
LOCATION-MEDIUM 79.4 84.8 85.9 82.3 85.7 82.7 84.6 81.5
LOCATION-FINE 79.4 84.4 85.0 81.3 84.7 82.1 83.8 81.0
ORGANIZATION-COARSE 61.8 76.7 74.9 74.0 77.0 76.4 76.8 67.8
ORGANIZATION-MEDIUM 61.8 73.3 72.4 69.4 74.6 74.0 74.4 66.3
ORGANIZATION-FINE 61.8 72.8 71.0 68.5 72.8 71.9 72.7 65.3
Table 3: Accuracy scores
task ? method? distrib. SEmax N09 SP
LOCATION-COARSE
literal 79.4 91.2 91.6 91.4
non-literal 20.6 57.6 59.1 58.5
LOCATION-MEDIUM
literal 79.4 91.2 91.6 91.4
metonymic 18.4 58.0 61.5 61.6
mixed 2.2 8.3 16 9.1
LOCATION-FINE
literal 79.4 91.2 91.6 91.4
place-for-people 15.5 58.9 61.7 61.1
place-for-event 1.1 16.7 0 0
obj-for-name 0.4 66.7 0 0
mixed 2.2 8.3 16 9.1
Table 4: Fine-grained results for each classification task
for countries (F-scores)
one for resolving metonymies. Combining local and
global evidence improves over both, indicating that
the information they provide is not redundant.
For companies the difference is small in terms of
accuracy, but in classification of individual classes
the difference in performance is higher, but because
of the small data size not statistically significant.
Countries in WikiNet have a high number of sur-
rounding relations, because they are used as cat-
egorization criteria for professionals, for example,
which generates fine-grained relations such as Ad-
ministrator of, Ambassador of, Chemist of .... Such
a fine grained distinction between different profes-
sions for people in a country is not necessary, or in-
deed, desirable, for the metonymy resolution task.
The results show that despite this shortcoming, the
results are on par with the state-of-the-art, but in fu-
ture work we plan to explore the task of relation gen-
eralization and its impact on the current task.
task ? method? distrib. SEmax N09 SP
ORGANIZATION-COARSE
literal 61.8 82.5 81.4 82.7
non-literal 38.2 65.2 61.6 65.5
ORGANIZATION-MEDIUM
literal 61.8 82.5 81.4 82.7
metonymic 31.0 60.4 58.7 63.1
mixed 7.2 30.8 26.8 27.4
ORGANIZATION-FINE
literal 61.8 82.6 81.4 82.7
org-for-members 19.1 63.0 59.7 66.5
org-for-product 8.0 50.0 44.4 35.0
org-for-facility 2.0 22.2 36.3 45.5
org-for-name 0.7 80.0 58.8 44.4
mixed 7.2 34.3 27.1 27.4
Table 5: Fine-grained results for each classification task
for companies (F-scores)
4.3 Simulating unsupervised metonymy
resolution
In an unsupervised metonymy resolution approach,
we would assign as interpretation the conceptual re-
lation whose probability given the PMW, global and
local contexts is highest. To simulate then the un-
supervised metonymy resolution task, we make the
relation features (used in the supervised approach)
binary, where for each instance the relation that has
highest probability has the value 1, the others 0.
Using only the relation features simulates an un-
supervised approach ? this set-up learns a map-
ping between the relations used as features and
the metonymy classes in the data used. Column
SPUnsup in Table 3 shows the results obtained in
this configuration. As expected the results are lower,
but still close to the supervised method when using
only grammatical features (M&N) for the location
191
setting. The results also significantly beat the base-
line (apart from the Location-Fine setting). One fea-
ture that contributes greatly to the results, especially
for the company semantic class, is the grammatical
role of the PMW, but we could not incorporate this
in the unsupervised setting.
The results in the simulated unsupervised set-
ting indicate that relations are a viable substitute
for manually provided classes in an unsupervised
framework, while leaving space for improvement.
5 Conclusion
We have explored the usage of local and global con-
text for the task of metonymy resolution in a prob-
abilistic framework. The global context has been
rarely used for the task of determining the intended
reading of a potentially metonymic word (PMW)
in context. We rely on automatically computed se-
lectional preferences, extracted from a corpus of
Wikipedia articles, and generalized based on a con-
cept network also extracted from Wikipedia. De-
spite relying on automatically derived resources, the
presented approach produces results on-a-par with
current state-of-the-art systems. The method de-
scribed here is also a step towards the unsupervised
resolution of metonymic words in context, by tak-
ing into account knowledge about the concept cor-
responding to the literal interpretation of the PMW,
and its relations to other concepts. This frame-
work would also allow for exploring the metonymy
resolution phenomena in various languages (since
Wikipedia and WikiNet are multilingual), and inves-
tigate whether the same relations apply or different
languages have different metonymic patterns.
Acknowledgments
Katja Markert is the recipient of an Alexander-von-
Humboldt Fellowship for Experienced Researchers.
This work was financially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853) and the
Klaus Tschirra Foundation. We thank the review-
ers for the helpful comments, and Helga Kra?mer-
Houska for additional support for conference partic-
ipation.
References
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, and Bernardo Magnini.
2007. Building textual entailment specialized data
sets: A methodology for isolating linguistic phenom-
ena relevant to inference. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation, La Valetta, Malta, 17?23 May 2010.
Caroline Brun, Maud Ehrmann, and Guillaume Jacquet.
2007. XRCE-M: A hybrid system for named en-
tity metonymy resolution. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-1), Prague, Czech Republic, 23?24 June
2007, pages 488?491.
Richa?rd Farkas, Eszter Simon, Gyo?rgy Szarvas, and
Da?niel Varga. 2007. GYDER: Maxent metonymy res-
olution. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-1), Prague,
Czech Republic, 23?24 June 2007, pages 161?164.
Dan C. Fass. 1991. met?: A method for discriminating
metonomy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
Sanda M. Harabagiu. 1998. Deriving metonymic co-
ercions from WordNet. In Proceedings of the Work-
shop on the Usage of WordNet in Natural Language
Systems, Montral, Quebec, Canada, 16 August, 1998,
pages 142?148.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1-2):69?142.
Maria Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, Sapporo, Japan, 7?12 July 2003,
pages 545?552.
Johannes Leveling and Sven Hartrumpf. 2008. On
metonymy recognition for geographic information re-
trieval. International Journal of Geographical Infor-
mation Science, 22(3):289?299.
Johannes Leveling. 2007. FUH (FernUniversita?t in Ha-
gen): Metonymy recognition using different kinds of
context for a memory-based learner. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations (SemEval-1), Prague, Czech Republic, 23?24
June 2007, pages 153?156.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Katja Markert and Udo Hahn. 2002. Metonymies in dis-
course. Artificial Intelligence, 135(1/2):145?198.
Katja Markert and Malvina Nissim. 2009. Data and
models for metonymy resolution. Language Re-
sources and Evaluation, 43(2):123?138.
192
Vivi Nastase and Michael Strube. 2009. Combining
collocations, lexical and encyclopedic knowledge for
metonymy resolution. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, Singapore, 6-7 August 2009, pages
910?918.
Vivi Nastase, Michael Strube, Benjamin Bo?rschinger,
Ca?cilia Zirn, and Anas Elghafari. 2010. WikiNet:
A very large scale multi-lingual concept network.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation, La Valetta,
Malta, 17?23 May 2010.
Cristina Nicolae, Gabriel Nicolae, and Sanda Harabagiu.
2007. UTD-HLT-CG: Semantic architecture for
metonymy resolution and classification of nominal re-
lations. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-1), Prague,
Czech Republic, 23?24 June 2007, pages 454?459.
Malvina Nissim and Katja Markert. 2005. Learning to
buy a Renault and talk to BMW: A supervised ap-
proach to conventional metonymy. In Proceedings of
the 6th International Workshop on Computational Se-
mantics, Tilburg, Netherlands, January 12-14, 2005.
Geoffrey Nunberg. 1995. Transfers of meaning. Journal
of Semantics, 12(1):109?132.
Thierry Poibeau. 2007. Up13: Knowledge-poor meth-
ods (sometimes) perform poorly. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations (SemEval-1), Prague, Czech Republic, 23?24
June 2007, pages 418?421.
James Pustejovsky. 1991. The generative lexicon. Com-
putational Linguistics, 17(4):209?241.
Kirk Roberts and Sanda M. Harabagiu. 2011. Unsuper-
vised learning of selectional restrictions and detection
of argument coercions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, UK, 27-29 July 2011,
pages 980?990.
Ekaterina Shutova. 2009. Sense-based interpretation of
logical metonymy using a statistical method. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the Association for Computational Lin-
guistics and the 4th International Joint Conference on
Natural Language Processing, Singapore, 2?7 August
2009, pages 1?9.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble Models for Dependency Parsing: Cheap and
Good? In Proceedings of Human Language Tech-
nologies 2010: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Los Angeles, Cal., 2?4 June 2010, pages 649?
652.
Ling-Xiang Tang, Shlomo Geva, Andrew Trotman, Yue
Xu, and Kelly Y. Itakura. 2011. Overview of the
NTCIR-9 crosslink task: Cross-lingual link discovery.
In Proceedings of the 9th NII Test Collection for IR
Systems Workshop meeting ? NTCIR-9 Tokyo, Japan,
6?9 December 2011.
Masao Utiyama, Masaki Murata, and Hitoshi Isahara.
2000. A statistical approach to the processing
of metonymy. In Proceedings of the 18th Inter-
national Conference on Computational Linguistics,
Saarbru?cken, Germany, 31 July ? 4 August 2000,
pages 885?891.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, San
Diego, CA.
193
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 814?820,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Cascading Collective Classification for Bridging Anaphora Recognition
Using a Rich Linguistic Feature Set
Yufang Hou1, Katja Markert2, Michael Strube1
1 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
2School of Computing, University of Leeds, UK
scskm@leeds.ac.uk
Abstract
Recognizing bridging anaphora is difficult due
to the wide variation within the phenomenon,
the resulting lack of easily identifiable surface
markers and their relative rarity. We develop
linguistically motivated discourse structure,
lexico-semantic and genericity detection fea-
tures and integrate these into a cascaded mi-
nority preference algorithm that models bridg-
ing recognition as a subtask of learning fine-
grained information status (IS). We substan-
tially improve bridging recognition without
impairing performance on other IS classes.
1 Introduction
In bridging or associative anaphora (Clark, 1975;
Prince, 1981; Gundel et al, 1993), the antecedent
and anaphor are not coreferent but are linked via a
variety of contiguity relations.1 In Example 1, the
phrases a resident, the stairs and the lobby are bridg-
ing anaphors with the antecedent One building.2
(1) One building was upgraded to red status while peo-
ple were taking things out, and a resident called up the
stairs to his girlfriend, telling her to keep sending things
down to the lobby.
Bridging is an important problem as it affects lin-
guistic theory and applications alike. For exam-
ple, without bridging resolution, entity coherence
between the first and second coordinated clause in
1We exclude comparative anaphora where anaphor and an-
tecedent are in a similarity/exclusion relation, indicated by ana-
phor modifiers such as other or similar (Modjeska et al, 2003).
2Examples are from OntoNotes (Weischedel et al, 2011).
Bridging anaphora are set in boldface; antecedents in italics.
Example 1 cannot be established. This is a prob-
lem both for coherence theories such as Centering
(Grosz et al, 1995) (where bridging is therefore in-
corporated as an indirect realization of previous en-
tities) as well as applications relying on entity co-
herence modelling, such as readability assessment
or sentence ordering (Barzilay and Lapata, 2008).
Full bridging resolution needs (i) recognition that
a bridging anaphor is present and (ii) identification
of the antecedent and contiguity relation. In re-
cent work, these two tasks have been tackled sep-
arately, with bridging recognition handled as part of
information status (IS) classification (Markert et al,
2012; Cahill and Riester, 2012; Rahman and Ng,
2012). Each mention in a text gets assigned one IS
class that describes its accessibility to the reader at
a given point in a text, bridging being one possible
class. We stay within this framework.
Bridging recognition is a difficult task, so that we
had to report very low results on this IS class in pre-
vious work (Markert et al, 2012). This is due to the
phenomenon?s variety, leading to a lack of clear sur-
face features for recognition. Instead, we formulate
in this paper novel discourse structure and lexico-
semantic features as well as features that distinguish
bridging from generics (see Section 3). In addition,
making up between 5% and 20% of definite descrip-
tions (Gardent and Manue?lian, 2005; Caselli and
Prodanof, 2006) and around 6% of all NPs (Mark-
ert et al, 2012), bridging is still less frequent than
many other IS classes and recognition of minority
classes is well known to be more difficult. We there-
fore use a cascaded classification algorithm to ad-
dress this problem (Omuya et al, 2013).
814
2 Related Work
Most bridging research concentrates on antecedent
selection only (Poesio and Vieira, 1998; Poesio et
al., 2004a; Markert et al, 2003; Lassalle and De-
nis, 2011; Hou et al, 2013), assuming that bridg-
ing recognition has already been performed. Previ-
ous work on recognition is either limited to definite
NPs based on heuristics evaluated on small datasets
(Hahn et al, 1996; Vieira and Poesio, 2000), or
models it as a subtask of learning fine-grained IS
(Rahman and Ng, 2012; Markert et al, 2012; Cahill
and Riester, 2012). Results within this latter frame-
work for bridging have been mixed: We reported in
Markert et al (2012) low results for bridging in writ-
ten news text whereas Rahman and Ng (2012) re-
port high results for the four subcategories of bridg-
ing annotated in the Switchboard dialogue corpus by
Nissim et al (2004). We believe this discrepancy to
be due to differences in corpus size and genre as well
as in bridging definition. Bridging in Switchboard
includes non-anaphoric, syntactically linked part-of
and set-member relationships (such as the building?s
lobby), as well as comparative anaphora, the latter
being marked by surface indicators such as other,
another etc. Both types are much easier to identify
than anaphoric bridging cases.3 In addition, many
non-anaphoric lexical cohesion cases have been an-
notated as bridging in Switchbard as well.
We also separate bridging recognition and ante-
cedent selection. One could argue that a joint model
is more attractive as potential antecedents such as
building ?trigger? subsequent bridging cases such as
stairs (Example 1). However, bridging can be indi-
cated by referential patterns without world knowl-
edge about the anaphor/antecedent NPs, as the non-
sense example 2 shows: the wug is clearly a bridging
anaphor although we do not know the antecedent.4
(2) The blicket couldn?t be connected to the dax. The
wug failed.
Similarly, Clark (1975) distinguishes between
bridging via necessary, probable and inducible
parts/roles and argues that only in the first and
maybe the second case the antecedent triggers the
3See also the high results for our specific category for com-
parative anaphora (Markert et al, 2012).
4We thank an anonymous reviewer for pointing this out.
bridging anaphor in the sense that we already spon-
taneously think of the anaphor when we read the an-
tecedent. Also, bridging recognition on its own can
be valuable for applications: for example, prosody is
influenced by IS status without needing antecedent
knowledge (Baumann and Riester, 2013).
3 Characterizing Bridging Anaphora for
Automatic Recognition
3.1 Properties of bridging anaphora
Bridging anaphors are rarely marked by surface fea-
tures. Indeed, even the common practice (Vieira and
Poesio, 2000; Lassalle and Denis, 2011; Cahill and
Riester, 2012) to limit bridging to definite NPs does
not seem to be correct: We report in previous work
(Hou et al, 2013) that less than 40% of the bridg-
ing anaphora in our corpus are definites. Instead,
bridging is diverse with regard to syntactic form
and function: bridging anaphora can be definite NPs
(Examples 4 and 6), indefinite NPs (Example 5) or
bare NPs (Examples 3, 8 and 9). The only frequent
syntactic property shared is that bridging NPs tend
to have a simple internal structure with regards to
modification. Bridging is also easily confused with
generics: friends is used as bridging anaphor in Ex-
ample 9 but generically in Example 10.
(3) . . . meat . . . The Communists froze prices instead.
(4) . . . the fund?s building . . . The budget was only
$400,000.
(5) . . . employees . . . A food caterer stashed stones in the
false bottom of a milk pail.
(6) . . . his truck . . . The farmer at the next truck shouts,
?Wheat!?
(7) . . . the firms . . . Crime was the reason that 26% re-
ported difficulty recruiting personnel and that 19% said
they were considering moving.
(8) . . . the company . . . His father was chairman and
chief executive until his death in an accident five years
ago.
(9) . . . Josephine Baker . . . Friends pitched in.
(10) Friends are part of the glue that holds life and faith
together.
Bridging anaphora can have almost limitless varia-
tion. However, we observe that bridging anaphors
are often licensed because of discourse structure
815
Markert et al (2012) local feature set
f1 FullPrevMention (b) f2 FullPreMentionTime (n)
f3 PartialPreMention (b) f4 ContentWordPreMention (b)
f5 Determiner (n) f6 NPtype (n)
f7 NPlength (int) f8 GrammaticalRole (n)
f9 NPNumber (n) f10 PreModByCompMarker (b)
f11 SemanticClass (n)
Markert et al (2012) relational feature set
f12 HasChild (r) f13 Precedes (r)
Table 1: Markert et al?s (2012) feature set, b indi-
cates binary, n nominal, r relational features.
and/or lexical or world knowledge. With regard to
discourse structure, Grosz et al (1995) observe that
bridging is often needed to establish entity coher-
ence between two adjacent sentences (Examples 1,
2, 4, 5, 6, 7 and 9). With regard to lexical and world
knowledge, relational noun phrases (Examples 3, 4,
8 and 9), building parts (Example 1), set member-
ship elements (Example 7), or, more rarely, tem-
poral/spatial modification (Example 6) may favor a
bridging reading. Motivated by these observations,
we develop discourse structure and lexico-semantic
features indicating bridging anaphora as well as fea-
tures designed to separate genericity from bridging.
3.2 Features
In Markert et al (2012) we classify eight fine-
grained IS categories for NPs in written text: old,
new and 6 mediated categories (syntactic, world-
Knowledge, bridging, comparative, aggregate and
function). This feature set (Table 1, f1-f13) works
well to identify old, new and several mediated cate-
gories. However, it fails to recognize most bridging
anaphora which we try to remedy in this work by
including more diverse features.
Discourse structure features (Table 2, f1-f3).
Bridging occurs frequently in sentences where oth-
erwise there would no entity coherence to previous
sentences/clauses (see Grosz et al (1995) and Poe-
sio et al (2004b) for discussions about bridging, en-
tity coherence and centering transitions in the Cen-
tering framework). This is especially true for topic
NPs (Halliday and Hasan, 1976) in such sentences.
We follow these insights by identifying coherence
gap sentences (see Examples 1, 4, 5, 6, 7, 9 and also
2): a sentence has a coherence gap (f1) if it has none
new local features for bridging
discourse f1 IsCoherenceGap (b)
structure f2 IsSentFirstMention (b)
f3 IsDocFirstMention (b)
semantics f4 IsWordNetRelationalNoun (b)
f5 IsInquirerRoleNoun (b)
f6 IsBuildingPart (b)
f7 IsSetElement (b)
f8 PreModSpatialTemporal (b)
f9 IsYear (b)
f10 PreModifiedByCountry (b)
generic f11 AppearInIfClause (b)
NP f12 VerbPosTag (l)
features f13 IsFrequentGenericNP (b)
f14 WorldKnowledgeNP (l)
f15 PreModByGeneralQuantifier (b)
other features f16 Unigrams (l)
f17 BridgingHeadNP (l)
f18 HasChildNP (b)
new features for other mediated categories
aggregate f19 HasChildCoordination (r)
function f20 DependOnChangeVerb (b)
worldKnowledge f21 IsFrequentProperName (b)
Table 2: New feature set, l indicates lexical features.
of the following three coherence elements: (1) entity
coreference to previous sentences, as approximated
via string match or presence of pronouns, (2) com-
parative anaphora approximated by mentions modi-
fied via a small set of comparative markers (see also
Table 1, f10 PreModByCompMarker), or (3) proper
names. We approximate the topic of a sentence via
the first mention (f2).
f3 models that bridging anaphors do not appear
at the beginning of a text.
Semantic features (Table 2, f4-f10). In contrast
to generic patterns, our semantic features capture
lexical properties of nouns that make them more
likely to be the head of a bridging NP. We create
f4-f8 to capture four kinds of bridging anaphora.
Lo?bner (1985) distinguishes between relational
nouns that take on at least one obligatory semantic
role (such as friend) and sortal nouns. It is likely that
relational nouns are more frequently used as bridg-
ing than sortal nouns (see Examples 3, 4, 8 and 9).
We extract a list containing around 4,000 relational
nouns from WordNet and a list containing around
500 nouns that specify professional roles from the
General Inquirer lexicon (Stone et al, 1966), then
determine whether the NP head appears in these lists
816
or not (f4 and f5). The obligatory semantic role for
a relational noun can of course also be filled NP in-
ternally instead of anaphorically and we use the fea-
tures f10 (for instances such as the Egyptian presi-
dent) and f18 (for complex NPs that are likely to fill
needed roles NP internally) to address this.
Because part-of relations are typical bridging re-
lations (see Example 1 and Clark (1975)), we use f6
to determine whether the NP is a part of the building
or not, using again a list extracted from Inquirer.
f7 is used to identify set membership bridging
cases (see Example 7), by checking whether the
NP head is a number or indefinite pronoun (such as
none, one, some) or modified by each, one. How-
ever, not all numbers are bridging cases (such as
1976) and we use f9 to exclude such cases.
Lassalle and Denis (2011) note that some bridging
anaphors are indicated by spatial or temporal modi-
fications (see Example 6). We use f8 to detect this
by compiling 20 such adjectives from Inquirer.
Features to detect generic nouns (Table 2, f11-
f15). Generic NPs (Example 10) are easily con-
fused with bridging anaphora. Inspired by Reiter
and Frank (2010) who build on linguistic research,
we develop features (f11-f15) to exclude generics.
First, hypothetical entities are likely to refer to
generic entities (Mitchell et al, 2002), We approx-
imate this by determining whether the NP appears
in an if-clause (f11). Also the clause tense and
mood may play a role to decide genericity (Reiter
and Frank, 2010). This is often reflected by the main
verb of a clause, so we extract its POS tag (f12).
Some NPs are commonly used generically, such
as children, men, or the dollar. The ACE-2 corpus
(distinct from our corpus) contains generic annota-
tion . We collect all NPs from ACE-2 that are always
used generically (f13). We also try to learn NPs that
are uniquely identifiable without further description
or anaphoric links such as the sun or the pope. We
do this by extracting common nouns which are an-
notated as worldKnowledge from the training part of
our corpus5 and use these as lexical features (f14).
Finally, motivated by the ACE-2 annotation
guidelines, we identify six quantifiers that may in-
dicate genericity, such as all, no, neither (f15).
5This list varies for each run of our algorithm in 10-fold
cross validation.
Other features for bridging (Table 2, f16-f18).
Following Rahman and Ng (2012), we use unigrams
(f16). We also extract heads of bridging anaphors
from the training data as lexical features (f17) to
learn typical nouns used for bridging that we did not
cover in lexicon extraction (f4 to f6).
Feature f18 models that bridging anaphora most
often have a simple internal structure and usually do
not contain any other NPs.
Features for other IS categories (Table 2, f19-
f21). We propose three features to improve other
IS categories. In the relational feature f19, we sep-
arate coordination parent-child from other parent-
child relations to help with the class aggregate. f20
determines whether a number is the object of an in-
crease/decrease verb (using a list extracted from In-
quirer) and therefore likely to be the IS class func-
tion. Frequent proper names are more likely to be
hearer old and hence of the class worldKnowledge.
f21 extracts proper names that occur in at least 100
documents in the Tipster corpus to approximate this.
4 Experiments and Results
Experimental setup. We perform experiments on
the corpus provided in Markert et al (2012)6. It con-
sists of 50 texts taken from the WSJ portion of the
OntoNotes corpus (Weischedel et al, 2011) with al-
most 11,000 NPs annotated for information status
including 663 bridging NPs and their antecedents.
All experiments are performed via 10-fold cross-
validation on documents. We use gold standard
mentions and the OntoNotes named entity and syn-
tactic annotation layers for feature extraction.
Reimplemented baseline system (rbls). rbls uses
the same features as Markert et al (2012) (Table 1)
but replaces the local decision tree classifier with
LibSVM as we will need to include lexical features.
rbls + Table 2 feature set (rbls+newfeat). Based
on rbls, all the new features from Table 2 are added.
Cascading minority preference system (cmps).
Minority classes such as bridging suffer during stan-
dard multi-class classification. Inspired by Omuya
6http://www.h-its.org/nlp/download/
isnotes.php
817
collective cascade + collective
markert 12 rbls rbls+newfeat cmps cmps?newfeat
R P F R P F R P F R P F R P F
old 84.1 85.2 84.6 84.6 85.5 85.1 84.4 86.0 85.2 82.2 87.2 84.7 78.9 89.5 83.8
med/worldKnowledge 60.6 70.0 65.0 65.9 69.6 67.7 67.4 77.3 72.0 67.2 77.2 71.9 67.5 66.7 67.1
med/syntactic 75.7 80.1 77.9 77.8 81.2 79.4 82.2 81.9 82.0 81.6 82.5 82.0 73.9 81.7 77.6
med/aggregate 43.1 55.8 48.7 47.9 58.0 52.5 64.5 79.5 71.2 63.5 77.9 70.0 46.9 60.0 52.7
med/function 35.4 53.5 48.7 33.8 56.4 42.3 67.7 72.1 69.8 67.7 72.1 69.8 41.5 50.0 45.4
med/comparative 81.4 82.0 81.7 81.8 82.5 82.1 81.8 82.1 82.0 86.6 78.2 82.2 86.2 78.7 82.3
med/bridging 12.2 41.7 18.9 10.7 36.6 16.6 19.3 39.0 25.8 44.9 39.8 42.2 31.8 23.9 27.3
new 87.7 73.3 79.8 87.5 74.8 80.7 86.5 76.1 81.0 83.0 78.1 80.5 82.4 76.1 79.1
acc 76.8 77.6 78.9 78.6 75.0
Table 3: Experimental results
et al (2013), we develop a cascading minority pref-
erence system for fine-grained IS classification. For
the five minority classes (function, aggregate, com-
parative, bridging and worldKnowledge) that each
make up less than the expected 18 of the data set, we
develop five binary classifiers with LibSVM7 using
all features from Tables 1 and 2 and apply them in
order from rarest to more frequent category. When-
ever a minority classifier predicts true, this class is
assigned. When all minority classifiers say false, we
back off to the multiclass rbls+newfeat system.
cmps ? Table 2 feature set (cmps?newfeat). To
test the effect of using the minority preference sys-
tem without additional features, we employ a cmps
system with baseline features from Table 1 only.
Results and Discussion (Table 3). Our novel
features in rbls+newfeat show improvements for
worldKnowledge, aggregate and function as well as
bridging categories compared to both baseline sys-
tems, although the performance for bridging is still
low. In addition, the overall accuracy is significantly
better than the two baseline systems (at the level of
1% using McNemar?s test). Using the cascaded mi-
nority preference system cmps in addition improves
bridging results substantially while the performance
on other categories does not worsen. The algorithm
needs both our novel feature classes as well as cas-
caded modelling to achieve this improvement as the
comparison to cmps?newfeat shows: the latter low-
ers overall accuracy as it tends to overgenerate rare
7Parameter against data imbalance is set according to the
ratio between positive and negative instances in the training set.
classes (including bridging) with low precision if the
features are not strong enough. Our novel features
(addressing linguistic properties of bridging) and the
cascaded algorithm (addressing data sparseness) ap-
pear to be complementary.
To look at the impact of features in our best sys-
tem, we performed an ablation study. Lexical fea-
tures as well as semantic ones have the most impact.
Discourse structure and genericity information fea-
tures have less of an impact. We believe the latter to
be due to noise involved in extracting these features
(such as approximating coreference for the coher-
ence gap feature) as well as genericity recognition
still being in its infancy (Reiter and Frank, 2010).
5 Conclusions
This paper aims to recognize bridging anaphora in
written text. We develop discourse structure, lexico-
semantic and genericity features based on linguis-
tic intuition and corpus research. By using a cas-
cading minority preference system, we show that
our approach outperforms the bridging recognition
in Markert et al (2012) by a large margin without
impairing the performance on other IS classes.
Acknowledgements. Yufang Hou is funded by a PhD
scholarship from the Research Training Group Coher-
ence in Language Processing at Heidelberg University.
Katja Markert receives a Fellowship for Experienced Re-
searchers by the Alexander-von-Humboldt Foundation.
We thank HITS gGmbH for hosting Katja Markert and
funding the annotation.
818
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Stefan Baumann and Arndt Riester. 2013. Coreference,
lexical givenness and prosody in German. Lingua.
Accepted.
Aoife Cahill and Arndt Riester. 2012. Automatically ac-
quiring fine-grained information status distinctions in
German. In Proceedings of the SIGdial 2012 Confer-
ence: The 13th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, Seoul, Korea, 5?6
July 2012, pages 232?236.
Tommaso Caselli and Irina Prodanof. 2006. Annotat-
ing bridging anaphors in Italian: In search of reliabil-
ity. In Proceedings of the 5th International Conference
on Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006.
Herbert H. Clark. 1975. Bridging. In Proceedings of the
Conference on Theoretical Issues in Natural Language
Processing, Cambridge, Mass., June 1975, pages 169?
174.
Claire Gardent and He?le`ne Manue?lian. 2005. Cre?ation
d?un corpus annote? pour le traitement des descrip-
tions de?finies. Traitement Automatique des Langues,
46(1):115?140.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, 69:274?307.
Udo Hahn, Michael Strube, and Katja Markert. 1996.
Bridging textual ellipses. In Proceedings of the 16th
International Conference on Computational Linguis-
tics, Copenhagen, Denmark, 5?9 August 1996, vol-
ume 1, pages 496?501.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. London, U.K.: Longman.
Yufang Hou, Katja Markert, and Michael Strube. 2013.
Global inference for bridging anaphora resolution. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, Georgia, 9?14 June 2013, pages 907?917.
Emmanuel Lassalle and Pascal Denis. 2011. Leverag-
ing different meronym discovery methods for bridging
resolution in French. In Proceedings of the 8th Dis-
course Anaphora and Anaphor Resolution Colloquium
(DAARC 2011), Faro, Algarve, Portugal, 6?7 October
2011, pages 35?46.
Sebastian Lo?bner. 1985. Definites. Journal of Seman-
tics, 4:279?326.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of the EACL Workshop on
the Computational Treatment of Anaphora. Budapest,
Hungary, 14 April 2003, pages 39?46.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, Jeju Is-
land, Korea, 8?14 July 2012, pages 795?804.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman,
Adam Meyers, Ada Brunstain, Lisa Ferro, and Beth
Sundheim. 2002. ACE-2 Version 1.0. LDC2003T11,
Philadelphia, Penn.: Linguistic Data Consortium.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan, 11?12 July 2003,
pages 176?183.
Malvina Nissim, Shipara Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for in-
formation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation, Lisbon, Portugal, 26?28 May 2004, pages
1023?1026.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, Sydney, Australia, 22?23 July 2006, pages
94?012.
Adinoyi Omuya, Vinodkumar Prabhakaran, and Owen
Rambow. 2013. Improving the quality of minority
class identification in dialog act tagging. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, Atlanta, Geor-
gia, 9?14 June 2013, pages 802?807.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004a. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pages 143?150.
Massimo Poesio, Rosemary Stevenson, Barbara Di Euge-
nio, and Janet Hitzeman. 2004b. Centering: A para-
metric theory and its instantiations. Computational
Linguistics, 30(3). 309-363.
819
Ellen F. Prince. 1981. Towards a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York, N.Y.
Altaf Rahman and Vincent Ng. 2012. Learning the fine-
grained information status of discourse entities. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, Avignon, France, 23?27 April 2012, pages 798?
807.
Nils Reiter and Anette Frank. 2010. Identifying generic
noun phrases. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11?16 July 2010, pages 40?49.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and Cambridge Computer Asso-
ciates. 1966. General Inquirer: A Computer Ap-
proach to Content Analysis. MIT Press, Cambridge,
Mass.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
820
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2070?2081,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Recall Error Analysis for Coreference Resolution
Sebastian Martschat and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH
Schloss-Wolfsbrunnenweg 35, 69118 Heidelberg, Germany
(sebastian.martschat|michael.strube)@h-its.org
Abstract
We present a novel method for coreference
resolution error analysis which we apply
to perform a recall error analysis of four
state-of-the-art English coreference reso-
lution systems. Our analysis highlights
differences between the systems and iden-
tifies that the majority of recall errors for
nouns and names are shared by all sys-
tems. We characterize this set of com-
mon challenging errors in terms of a broad
range of lexical and semantic properties.
1 Introduction
Coreference resolution is the task of determining
which mentions in a text refer to the same entity.
State-of-the-art approaches include both learning-
based (Fernandes et al., 2012; Bj?orkelund and
Farkas, 2012; Durrett and Klein, 2013) and de-
terministic models (Lee et al., 2013; Martschat,
2013). These approaches achieve state-of-the-art
performance mainly relying on morphosyntactic
and lexical factors. However, consider the follow-
ing example.
In order to improving the added value
of oil products, the second phase project
of the Qinghai Petroleum Bureau?s
Ge?ermu oil refinery has been put into
production. This will further improve
the factory?s oil products structure.
Due to the lack of any string overlap, most
state-of-the-art systems will miss the link between
the factory and the Qinghai Petroleum Bureau?s
Ge?ermu oil refinery. The information that factory
is a hypernym of refinery, however, may be useful
to resolve such links.
The aim of this paper is to quantify and char-
acterize such recall errors made by state-of-the-
art coreference resolution systems. By doing so,
we provide a solid foundation for work on em-
ploying knowledge sources for improving recall
for coreference resolution (Ponzetto and Strube,
2006; Rahman and Ng, 2011; Ratinov and Roth,
2012; Bansal and Klein, 2012, inter alia). In par-
ticular, we make the following contributions:
We present a novel framework for coreference
resolution error analysis. This yields a formal
foundation for previous work on link-based error
analysis (Uryupina, 2008; Martschat, 2013) and
complements work on transformation-based error
analysis (Kummerfeld and Klein, 2013).
We apply the method proposed in this paper to
perform a recall error analysis of four state-of-
the-art systems, encompassing deterministic and
learning-based approaches. In particular, we iden-
tify and characterize a set of challenging errors
common to all systems, and discuss strengths and
weaknesses of each system regarding specific er-
ror types. We also present a brief precision error
analysis.
A toolkit which implements the framework pro-
posed in this paper is available for download.
1
2 A Link-Based Analysis Framework
In this section we discuss challenges in corefer-
ence resolution error analysis and devise an error
analysis framework to overcome these challenges.
2.1 Motivation
Suppose a document contains the entity BARACK
OBAMA, which is referenced by four mentions
in the following order: Obama, he, the president
and his. A typical output of a current system not
equipped with world knowledge will consist of
two entities: {Obama, he} and {the president, his}
Obviously, the system made a recall error. But,
due to the complex nature of the coreference reso-
lution task, it is not clear how to represent the re-
1
http://smartschat.de/software
2070
(a)
m
1
Obama
m
2
he
m
3
the president
m
4
his
(b)
m
1
m
2
m
3
the president
m
4
n
1
n
2
n
3
(c)
m
1
m
2
m
3
the president
m
4
(d)
m
1
m
2
m
3
the president
m
4
Figure 1: (a) a reference entity r, represented as a complete one-directional graph, (b) a set S of three
system entities, (c) the partition r
S
, (d) a spanning tree for r.
call error: is it missing the link between the pres-
ident and Obama? Can the error be attributed to
deficiencies in pronoun resolution?
Linguistically motivated error representations
would facilitate both understanding of current
challenges and make system development faster
and easier. The aim of this section is to devise
such representations.
2.2 Formalizing Coreference Resolution
To start with, we give a formal description of the
coreference resolution task following the termi-
nology used for the ACE (Mitchell et al., 2004)
and OntoNotes (Weischedel et al., 2013) projects.
A mention is a linguistic realization of a reference
to an entity. Two mentions corefer if they refer to
the same entity. Hence, coreference is reflexive,
symmetric and transitive, and therefore an equiva-
lence relation. The task of coreference resolution
is to predict equivalence classes of mentions in a
document according to the coreference relation.
In order to extract errors, we need to compare
the reference equivalence classes, given by the
annotation, with the system equivalence classes
obtained from system output. The key question
now is how we represent these equivalence classes
of mentions. Adapting common terminology, we
also refer to the equivalence classes as entities.
2.3 Representing Entities
The most straightforward entity representation ig-
nores any structure and models an entity as a set
of mentions. This representation was utilized for
error analysis by Kummerfeld and Klein (2013),
who extract errors by transforming reference into
system entities. In this set-based representation,
we can only extract whether two mentions corefer
at all. More fine-grained information, for example
about antecedent information, is not accessible.
We therefore propose to employ a structured en-
tity representation, which explicitly models links
established by the coreference relation between
mentions. This leads to a link-based error repre-
sentation which formalizes the methods presented
in Uryupina (2008) and Martschat (2013).
We employ for representation a complete one-
directional graph. That is, we represent an en-
tity e over mentions {m
1
, . . . ,m
n
} as a graph
e = (N,A), where N = {m
1
, . . . ,m
n
} and
A = {(m
k
,m
j
) | k > j}. The indices respect
the mention ordering. Mentions earlier in the text
have a lower index. An example graph for an en-
tity over four mentions m
1
, . . . ,m
4
(such as the
BARACK OBAMA entity) is depicted in Figure 1a.
In this graph, we express all coreference relations
between all pairs of mentions.
2
Using this representation, we can represent a set
of entities as a set of graphs. In particular, given a
document we consider the set of reference entities
R given by the annotation, and the set of system
entities S, given by the system output. In order to
extract errors, we compare the graphs in R with
the graphs in S.
In the following, we discuss how to compute re-
call errors for a reference entity r ? R with respect
to the system entities S. For computing precision
errors, we just switch the roles of R and S.
2.4 Comparing Reference and System
Entities
As we represent entities as sets of links between
mentions, errors can be quantified as differences in
the links. For example, if an edge (representing a
link) from some reference entity r ? R is missing
2
We could also use an undirected instead of a one-
directional graph, but using a one-directional graph conve-
niently models sequential information, which simplifies no-
tation and the algorithms we will present.
2071
in all system entities in S, this is a recall error.
In order to formalize this, we employ the notion
of a partition of an entity. Let r ? R be some ref-
erence entity, and let S be a set of system entities.
The partition of r by S, written r
S
, is obtained
by taking all edges in r that also appear in S. r
S
consists of all connected components of r (we will
refer to these as subentities) that are also in S. All
edges in r that are not in r
S
are candidates for re-
call errors, as these were not in any entity in S.
Figure 1b shows a set S of three system entities:
two consist of two mentions, one of three men-
tions. In our running example, this corresponds
to the system output {Obama, he} and {the pres-
ident, his} plus some spurious mentions, which
are colored gray. The graph r
S
for our example
is shown in Figure 1c. The two edges correspond
to the correctly recognized links (he, Obama) and
(his, the president). All edges in r (Figure 1a)
missing from this graph are candidates for errors.
2.5 Spanning Trees
However, taking all edges in r missing in r
S
as er-
rors leads to unintuitive results. In the BARACK
OBAMA example, this would lead to four errors
being extracted: (the president, Obama), (his,
Obama), (the president, he) and (his, he). But,
in order to correctly predict the BARACK OBAMA
entity, a coreference resolution system only needs
to predict three correct links, i.e. it has to provide a
spanning tree of the entity?s graph representation.
Therefore, to extract errors, we compute a span-
ning tree T
r
of r, and take all edges in T
r
that do
not appear in r
S
as errors. Figure 1d shows an ex-
ample spanning tree for the running example en-
tity r. The dashed edge, which corresponds to the
link (the president, Obama), does not appear in r
S
and is therefore extracted as an error.
The strategies for computing a spanning tree
may differ for recall and precision errors. Hence,
our extraction algorithm is parametrized by two
procedures ST
rec
(e, P ) and ST
prec
(e, P ) which,
given an entity e and a set of entities P , output
a spanning tree T
e
of e. The whole algorithm for
error extraction is summarized in Algorithm 1.
3 Spanning Tree Algorithms
In the last section we presented a framework for
link-based error analysis, which extracts errors by
comparing entity spanning trees to entity parti-
tions. Therefore we can accommodate different
Algorithm 1 Error Extraction from a Corpus
Input: A corpus C, algorithms ST
rec
, ST
prec
for
computing spanning trees.
function ERRORS(C, ST
rec
, ST
prec
)
recall errors = [ ]
precision errors = [ ]
for d ? C do
Let R
d
be the reference entities and S
d
be the system entities of document d.
for r ? R
d
do
Add all edges in ST
rec
(r, S
d
) not in
r
S
d
to recall errors.
for s ? S
d
do
Add all edges in ST
prec
(s,R
d
) not in
s
R
d
to precision errors.
Output: recall errors, precision errors
notions of errors by varying the algorithm for com-
puting spanning trees. We now present some span-
ning tree algorithms for extracting recall and pre-
cision errors.
3.1 Recall Errors
We first observe that for error extraction, the struc-
ture of the spanning trees of the subentities appear-
ing in r
S
does not play a role. Edges present in r
S
are not candidates for errors, since they appear in
both the reference entity r and the system output
S. Therefore, it does not matter which edges from
the subentities are in the spanning tree.
Hence, to build the spanning tree, we first
choose arbitrary spanning trees for the subentities
in the partition. We choose the remaining edges
according to the spanning tree algorithm.
Having settled on this, we only have to decide
which edges to choose that connect the trees rep-
resenting the subentities. There are many possible
choices for this. For example, the graph in Fig-
ure 1c has four candidate edges which connect the
trees for the subentities.
We can reduce the number of candidate edges
by only considering the first mention (with respect
to textual order) in a subentity as the source of
an edge to be added. This makes sense since all
other mentions in that subentity were correctly re-
solved to be coreferent with some preceding men-
tion. We still have to decide on the target of the
edge. In Figure 1c, we have two choices for edges:
(m
3
,m
1
) and (m
3
,m
2
). We now present two
methods for choosing edges.
2072
Choosing Edges by Distance. The most
straight-forward way to decide on an edge is to
take the edge with smallest mention distance
between source and target. This is the approach
taken by Martschat (2013).
Choosing Edges by Accessibility. However, the
distance-based approach may lead to unintuitive
results. Let us consider again the BARACK
OBAMA example from Figure 1. When choosing
edges by distance, we would extract the error (the
president, he). However, such links with a non-
pronominal anaphor and a pronominal antecedent
are difficult to process and considered unreliable
(Ng and Cardie, 2002; Bengtson and Roth, 2008).
On the other hand, the missed link (the president,
Obama) constitutes a well-defined hyponymy re-
lation which can be found in knowledge bases and
is easily interpretable by humans.
Uryupina (Uryupina, 2007; Uryupina, 2008)
presents a recall error analysis where she takes
the ?intuitively easiest? missing link to analyze
(Uryupina, 2007, p. 196). How can we formal-
ize such an intuition? We will employ a no-
tion grounded in accessibility theory (Ariel, 1988).
Names and nouns refer to less accessible entities
than pronouns do. For such anaphors, we prefer
descriptive (name/nominal) antecedents. Inspired
by Ariel?s degrees of accessibility, we choose a
target for a given anaphor m
i
as follows:
? If m
i
is a pronoun, choose the closest preced-
ing mention.
? If m
i
is not a pronoun, choose the closest
preceding proper name. If no such mention
exists, choose the closest preceding common
noun. If no such mention exists, choose the
closest preceding mention.
Applied to the example from Figure 1, this algo-
rithm extracts the error (the president, Obama).
3
3.2 Precision Errors
Virtually all approaches to coreference resolu-
tion obtain entities by outputting pairs of anaphor
and antecedent, subject to the constraint that one
anaphor has at most one antecedent.
We use this information to build spanning trees
for system entities: these spanning trees con-
sist of exactly the edges which correspond to
anaphor/antecedent pairs in the system output.
3
A similar procedure was used by Ng and Cardie (2002)
to extract meaningful antecedents when training a corefer-
ence resolution system.
4 Data and Systems
We now discuss data and coreference resolution
systems which we will employ for our analysis.
4.1 Data
We analyze the errors of the systems on the En-
glish development data of the CoNLL?12 shared
task on multilingual coreference resolution (Prad-
han et al., 2012). This corpus contains 343 docu-
ments, spanning seven genres: bible texts, broad-
cast conversation, broadcast news, magazine texts,
news wire, telephone conversations and web logs.
4.2 Systems
State-of-the-art approaches to coreference resolu-
tion encompass various paradigms, ranging from
deterministic pairwise systems to learning-based
structured prediction models. Hence, we want to
conduct our analysis on a representative sample of
the state of the art, which should be publicly avail-
able. Therefore, we decided on two deterministic
and two learning-based systems:
? StanfordSieve
4
(Lee et al., 2013) was the
winning system of the CoNLL?11 shared
task. It employs a multi-sieve approach by
making more confident decisions first.
? Multigraph
5
(Martschat, 2013) is a deter-
ministic pairwise system which is based on
Martschat et al. (2012), the second-ranking
system in the English track of the CoNLL?12
shared task. It uses a subset of features as
hard constraints and chooses an antecedent
for a mention by summing up the remaining
boolean features.
? IMSCoref
6
(Bj?orkelund and Farkas, 2012)
ranked second overall in the CoNLL?12
shared task (third for English). It stacks mul-
tiple decoders and relies on a combination of
standard pairwise and lexicalized features.
? BerkeleyCoref
7
(Durrett and Klein, 2013) is
a state-of-the-art system that uses mainly lex-
icalized features and a latent antecedent rank-
ing architecture. It outperforms Stanford-
Sieve and IMSCoref on the CoNLL?11 data.
4
Part of Stanford CoreNLP, available at http://nlp.
stanford.edu/software/corenlp.shtml. We
use version 3.4.
5
http://smartschat.de/software
6
http://www.ims.uni-stuttgart.de/
forschung/ressourcen/werkzeuge/IMSCoref.
en.html . We use the CoNLL 2012 system.
7
http://nlp.cs.berkeley.edu/
berkeleycoref.shtml
2073
System MUC B
3
CEAF
e
Average
Fernandes et al. 69.46 57.83 54.43 60.57
Martschat 66.22 55.47 51.90 57.86
StanfordSieve 64.96 54.49 51.24 56.90
Multigraph 69.13 58.61 56.06 61.28
IMSCoref 67.15 55.19 50.94 57.76
BerkeleyCoref 70.27 59.29 56.11 61.89
Table 1: Comparison of the systems with Fernan-
des et al. (2012) and with Martschat (2013) on
CoNLL?12 English development data.
For Multigraph, we modified the system described
in Martschat (2013) slightly to allow for the in-
corporation of distance (similar to Cai and Strube
(2010)). Inspired by Lappin and Leass (1994), we
add salience weights for subjects and objects to
the model to improve third-person pronoun reso-
lution. We also extended the feature set by a sub-
string feature. Furthermore, motivated by Chen
and Ng (2012), we added a lexicalized feature for
non-pronominal mentions that were coreferent in
at least 50% of the cases in the training data.
StanfordSieve was run with its standard CoNLL
shared task settings. The learning-based sys-
tems were trained on the CoNLL?12 training data.
We trained IMSCoref with its standard settings,
and trained BerkeleyCoref with the final feature
set from Durrett and Klein (2013) for twenty it-
erations. We evaluate the systems on English
CoNLL?12 development data and compare it with
the winning system of the CoNLL?12 shared task
(Fernandes et al., 2012) and with Martschat (2013)
in Table 1, using the reference implementation v7
of the CoNLL scorer (Pradhan et al., 2014).
BerkeleyCoref performs best according to all
metrics, followed by Multigraph. StanfordSieve
is the worst performing system: the gap to Berke-
leyCoref is five points in average score.
4.3 Discussion
Although we analyze recent systems on a recently
published coreference data set, we believe that the
results of our analysis will have implications for
coreference in general. The data set is the largest
and most genre-diverse coreference corpus so far.
The systems we investigate represent major di-
rections in coreference resolution model research,
and make use of large and diverse feature sets pro-
posed in the literature (Ng, 2010).
5 A Comparative Analysis
The coreference resolution systems presented in
the previous section are a representative sample of
the state of the art. Therefore, by analyzing the
errors they make, we can learn about remaining
challenges in coreference resolution and analyze
the qualitative differences between the systems.
The results of such an analysis will deepen our
understanding of coreference resolution and will
suggest promising directions for further research.
5.1 Experimental Settings
Previous studies identified the presence of recall
errors as a main bottleneck for improving per-
formance (Raghunathan et al., 2010; Durrett and
Klein, 2013; Kummerfeld and Klein, 2013). This
is also evidenced by the CoNLL shared tasks on
coreference resolution (Pradhan et al., 2011; Prad-
han et al., 2012), where most competitive systems
had higher precision than recall. This indicates
that an analysis of recall errors helps to understand
and improve the state of the art. Hence, we focus
on analyzing recall errors, and complement this by
a brief analysis of precision errors.
We analyze errors of the four systems presented
in the previous section on the CoNLL?12 English
development data. To extract recall errors we em-
ploy the spanning tree algorithm which chooses
edges by accessibility. We obtain precision errors
from the pairwise output of the systems.
5.2 A Recall Error Analysis of StanfordSieve
Since StanfordSieve is currently the most-widely
used coreference resolution system, it serves as a
good starting point for our analysis. Remember
that we represent each error as a pair of anaphor
and antecedent. For an initial analysis, we cate-
gorize each error by mention type, distinguishing
between proper name, common noun, pronoun,
demonstrative pronoun and verb.
8
StanfordSieve makes 5245 recall errors. To put
this number into context, we compare it with the
maximum number of recall errors a system can
make. This count is obtained by extracting recall
errors from the output of a system that puts each
mention in its own entity, which yields 14609 er-
rors. In Table 2 we present a detailed analysis.
For each pair of mention type of anaphor and an-
8
We obtain the type from the part-of-speech tag of the
mention?s head. Furthermore, we treat every mention whose
head has a NER label in the data as a proper name.
2074
Name Noun Pron. Dem. Verb
Name
Errors 1006 181 43 0 0
Maximum 3578 206 56 2 0
Noun
Errors 517 1127 46 14 91
Maximum 742 2063 51 14 91
Pron.
Errors 483 761 543 45 53
Maximum 1166 1535 4596 92 53
Dem.
Errors 23 86 41 31 117
Maximum 27 93 43 46 117
Verb
Errors 1 20 2 4 10
Maximum 1 20 2 4 11
Table 2: Number of StanfordSieve?s recall er-
rors according to mention type, compared to the
maximum possible number of errors. Rows are
anaphors, columns antecedents.
tecedent, the table displays the number of recall
errors and of maximum errors possible.
StanfordSieve gets almost none of the links in-
volving verbal or demonstrative mentions correct.
This is due to the system not attempting to handle
event coreference, and performing very poorly for
demonstratives. On the other hand, recall for pro-
noun resolution is quite good, at least when con-
sidering non-verbal antecedents. While Stanford-
Sieve makes 1885 recall errors when the anaphor
is a pronoun, it successfully resolves most of such
links present in the corpus. Finally, let us consider
the links involving only proper names and com-
mon nouns. In total, these amount to 6589 links
in the corpus (around 45% of all links). Stanford-
Sieve misses 2831 of these links. Pairs of proper
names seem to be easier to resolve than pairs of
common nouns. Links between a common noun
and a proper name are less frequent, but much
more difficult: most of the links are missing.
5.3 Analysis of the Other Systems
In the previous section we identified various char-
acteristics of the errors made by StanfordSieve:
only (comparatively) few errors are made for pro-
noun resolution and name coreference, while other
types of nominal anaphora and coreference of
demonstrative/verbal mentions pose a challenge
for the system. Do the other systems in our study
also have these characteristics? In order to answer
Proportion
System Total Anaphor Pron. Name/Noun
StanfordSieve 5245 36% 54%
Multigraph 4630 32% 56%
IMSCoref 5220 32% 58%
BerkeleyCoref 4635 32% 56%
Table 3: Recall error numbers for all systems.
this question, we repeated the analysis for the three
other systems described in Section 4. We summa-
rize the results in Table 3. We only report num-
bers for pronoun resolution and name/noun coref-
erence, as all systems do not resolve verbal men-
tions and perform poorly for demonstratives.
StanfordSieve makes the most recall errors,
closely followed by IMSCoref. Multigraph
and BerkeleyCoref make around 600 errors less.
While the total number of errors differs between
the systems, the distributions are similar. In par-
ticular, around 55% of recall errors made involve
only proper names and common nouns. The num-
ber is a bit higher for IMSCoref. We conclude
that, despite variations in performance, both de-
terministic and learning-based state-of-the-art sys-
tems have similar weaknesses regarding recall.
The results displayed in Table 3 suggest vari-
ous opportunities for future research. In this pa-
per, we will focus on analyzing name/noun recall
errors, as these constitute a large fraction of all re-
call errors. Future work should address the pro-
noun resolution errors and a characterization of the
verbal/demonstrative errors.
5.4 Analysis of the Name/Noun Recall Errors
We now turn towards a fine-grained analysis of the
name/noun recall errors.
Table 4 displays the number of such recall errors
made by each system, according to the mention
types of anaphor and antecedent. We are interested
in errors common to all systems, and in qualitative
differences of errors between the systems.
5.4.1 Common Errors
Let us first analyze the errors common to all sys-
tems. Our analysis is driven by the question how
these can be characterized, and which knowledge
is missing to resolve such links. We discuss the
errors depending on the mention types of anaphor
and antecedent. The lower part of Table 4 displays
the number of common errors for each category.
2075
Number of Recall Errors (Anaphor-Antecedent)
Description Name-Name Noun-Name Name-Noun Noun-Noun
StanfordSieve 1006 517 181 1127
Multigraph 753 501 189 1152
IMSCoref 1082 500 188 1264
BerkeleyCoref 910 456 171 1072
Common errors 475 371 147 835
Correct boundaries idenfified 257 273 108 563
excl. IMSCoref 156 222 97 475
Table 4: Name/Noun recall errors for all systems.
Common All
Type % Type %
ORG 25% PERSON 26%
PERSON 19% GPE 26%
GPE 16% ORG 20%
DATE 14% NONE 14%
NONE 9% DATE 6%
Table 5: Distribution of top five named entity
types of common name-name recall errors and all
possible name-name recall errors.
Furthermore, in order to assess the impact of
mention detection, the table shows the number of
common errors where boundaries for both men-
tions were identified correctly by some system.
We can see that boundary identification is a diffi-
cult problem, especially for proper name pairs: for
48% of such errors, no system found the correct
boundaries of both mentions participating in the
error. The number of errors where correct bound-
aries could be found drops significantly after ex-
cluding IMSCoref. This is due to the mention ex-
traction strategy of IMSCoref: the other systems
in our study discard the shorter mention when two
mentions have the same head, IMSCoref keeps
both mentions. Hence, the system is able to cor-
rectly identify some mentions even in the presence
of parsing or preprocessing errors. However, as
a result, IMSCoref has to process many spurious
mentions, which makes learning more difficult.
We conclude that mention detection still consti-
tutes a challenge. We now proceed to a detailed
analysis of errors common to all systems. In pass-
ing we will discuss difficulties in mention detec-
tion with regard to specific error types.
Errors between Pairs of Proper Names. The
systems share 475 recall errors between pairs of
proper names. In Table 5, we compare the distri-
bution of gold named entity types of these errors
with the distribution of gold named entity types of
all possible errors (obtained via a singleton sys-
tem). We see that especially difficult classes of
links are pairs with type ORG or DATE.
Let us now consider lexical features of the er-
rors.
9
In 154 errors, the strings match completely,
but the correct resolution was mostly prevented by
annotation inconsistencies (e.g. China instead of
China?s) or propagated parsing and NER errors,
which lead to deficiencies in mention extraction.
For 217 errors, at least one token appears in both
mention strings, as in the ?Cole? and the ?USS
Cole?. This shows the insufficiency of the features
which hint to alias relations, may it be heuristics or
learned lexical similarities (for 109 of the 217 er-
rors, both mention boundaries were identified cor-
rectly by at least one system). Disambiguation
with respect to knowledge bases could provide a
principled way to identify name variations.
We classified the remaining 104 errors manu-
ally, see Table 6. For a couple of categories such
as identifying acronyms, spelling variations and
aliases, disambiguation could also help. Many er-
rors happen for date mentions, which suggests the
use of temporal tagging features.
Errors for Noun-Name Pairs. We now inves-
tigate the errors where the anaphor is a common
noun and the antecedent is a proper name. 371 er-
rors are common to all systems. The high fraction
of common errors shows that this is an especially
challenging category. We again start by investigat-
ing how the distribution of the named entity type
9
When computing these, we ignored case and ignored all
tokens with part-of-speech tag DT or POS.
2076
Description Occ. Example
Acronyms 20 National Ice Hockey League and
NHL
Alias 24 Florida and the Sunshine State
Annotation 2 Annotation errors (pronoun as
name)
Context 2 Paula Coccoz and juror number ten
Date 29 1989 and last year?s
Metonymy 12 South Afria and Pretoria
Roles 8 Al Gore and the Vice President
Spelling 7 Hsiachuotzu and Hsiachuotsu
Table 6: Classification of common name-name re-
call errors without common tokens.
Common All
Type % Type %
ORG 28% ORG 27%
PERSON 22% GPE 22%
GPE 19% PERSON 18%
NONE 7% NONE 11%
DATE 5% DATE 5%
Table 7: Distribution of top five named entity
types of common noun-name recall errors and all
possible noun-name recall errors.
of the antecedent differs when we compare com-
mon errors to all possible errors. The results are
shown in Table 7. Links with a proper name an-
tecedent of type PERSON are especially difficult.
They constitute 22% of the common errors, but
only 18% of all possible errors.
Most mentions are in a hyponymy relation, like
the prime minister and Mr. Papandreou. This con-
firms that harnessing such relations could improve
coreference resolution (Rahman and Ng, 2011;
Uryupina et al., 2011). For 65 of the errors (18%)
there is lexical overlap: the head of the anaphor is
contained in the proper name antecedent, as in the
entire park and the Ocean Park.
When categorizing all common errors accord-
ing to the head of the anaphor, we observe 204 dif-
ferent heads. 142 heads appear only once, but the
top ten heads make up 88 of the 371 errors. The
most frequent heads are company (15), group (12),
government, country and nation (each 9). This
suggests that even with few reliable hyponymy re-
lations recall could be significantly improved.
We observe similar trends when the anaphor is
a proper name and the antecedent is a noun.
Reference System
System Stanford MG IMS Berkeley
Stanford - 51 47 61
MG 17 - 42 60
IMS 26 54 - 54
Berkeley 12 42 25 -
Table 8: Comparison of noun-name recall errors.
Entries are errors made by the system in the row,
while the participating mentions are coreferent ac-
cording to the the system in the column.
Errors between Pairs of Common Nouns. 835
errors between pairs of common nouns are shared
by all systems. For 174 of these, the anaphor is
an indefinite noun phase, which makes resolution
a lot harder, since most coreference resolution sys-
tems classify these as non-anaphoric and therefore
do not attempt resolution.
For further analysis, we split all 835 errors in
two categories, distinguishing whether the head
matches between the mentions or not. In 341 cases
the heads match. For many of these cases, parsing
errors propagate and prevent the systems from rec-
ognizing the correct mention boundaries.
In order to get a better understanding of the er-
rors for nouns with different heads, we randomly
extracted 50 of the 494 pairs and investigated the
relation that holds between the heads. In 23 cases,
the heads were related via hyponymy. In 10 cases
they were synonyms. The remaining 17 cases
involve many different phenomena, for example
meronymy. This confirms findings from previous
research (Vieira and Poesio, 2000).
Hence, looking up lexical relations, especially
hyponymy, might be helpful to solve these cases.
5.4.2 Differences between the Systems
In order to analyze differences between the sys-
tems, we compare the recall errors they make.
The information how recall errors differ between
systems will enable us to understand individual
strengths and weaknesses.
Exemplarily, we will have a look at the differ-
ences in the errors when the anaphor is a common
noun and the antecedent is a proper name. By sys-
tem design and by the total error numbers (Table
4) we expect the learning-based systems to have a
slight advantage over the deterministic systems.
In Table 8 we compare noun-name recall errors
made by each system. Entries are errors made by
2077
Number and Proportion of Precision Errors (Anaphor-Antecedent)
Description Name-Name Noun-Name Name-Noun Noun-Noun
StanfordSieve 1038 31% 64 59% 65 72% 944 48%
Multigraph 1131 30% 76 51% 24 56% 743 42%
IMSCoref 834 26% 74 59% 46 64% 1050 54%
BerkeleyCoref 810 24% 191 67% 60 62% 1015 48%
Common errors 158 1 2 167
Table 9: Name/Noun precision errors for all systems. The percentages are the proportion of precision
errors with respect to all decision of the system in that category.
the system in the row, while the participating men-
tions are coreferent according to the the system in
the column. The numbers confirm our hypothesis,
but also show that the deterministic systems are
able to recover a few links missed by the learning-
based systems.
For example, BerkeleyCoref recovers 60 links
that could not be found by Multigraph, including
34 links without any common token, such as the
airline and Pan Am. Multigraph recovers only 42
links not found by BerkeleyCoref, 21 without any
common token. Qualitatively, StanfordSieve and
Multigraph are able to resolve a few links thanks
to their engineered substring match, such as the
judge and Dallas District Judge Jack Hampton.
We also conducted similar investigations for
common noun and proper name pairs. For com-
mon nouns, the trends are similar: the learning-
based systems have an advantage over the deter-
ministic systems. However, only few relations be-
tween nouns with different heads are learned ?
compared to StanfordSieve, BerkeleyCoref recov-
ers only 11 such pairs, such as the man and an
expert in the law. Recall of the deterministic sys-
tems is further hampered by their strict checks for
modifier agreement, which they employ to keep
precision high. Both systems miss for example the
link from the anaphor the Milosevic regime to the
regime, since the nominal modifier of the anaphor
does not appear in the antecedent.
For proper names, Multigraph employs so-
phisticated alias heuristics which help to resolve
matches such as Marshall Ye Ting?s and his grand-
father Ye Ting. This explains the corresponding
low number in Table 4. The lexicalized features
of Multigraph, IMSCoref and BerkeleyCoref help
to learn aliases when there is no string match, es-
pecially for the bible part of the corpus (resolving
links such as Jesus and the Son of Man).
5.5 Precision Errors
In the above analysis we identified common
name/noun recall errors and discussed strengths
and weaknesses of each system. Let us comple-
ment this analysis by a brief discussion of corre-
sponding precision errors.
Table 9 gives an overview. It displays the num-
ber of precision errors for each category, and the
proportion of these errors compared to all deci-
sions in that category. We can see some general
trends from this table: first, more decisions lead to
a higher proportion of errors. This shows the dif-
ficulty of balancing recall and precision. Second,
proper name coreference seems much easier than
common noun coreference. Coreference involving
different mention types is a lot harder ? the sys-
tems only attempt few decisions, most of them are
wrong. This confirms findings from our recall er-
ror analysis. Third, the fraction of common errors
is very low, which indicates that precisions errors
stem from various sources, which are handled dif-
ferently by each system.
6 Related Work
We now discuss related work in coreference res-
olution error analysis and in the related field of
coreference resolution evaluation metrics.
Error Analysis. While many papers on coref-
erence resolution briefly discuss errors made and
resolved by the system under consideration, only
few concentrate on error analysis. Uryupina
(2008) presents a manual error analysis on the
small MUC-7 test set; Martschat (2013) performs
an automatic coarse-grained error classification on
CoNLL data. By extending and formalizing the
approach of Martschat (2013), we are able to per-
form a large-scale investigation of recall errors
made by state-of-the-art systems.
2078
Kummerfeld and Klein (2013) devise a method
to extract errors from transformations of reference
to system entities. They apply this method to a
variety of systems and aggregate errors over these
systems. By aggregating, they are not able to ana-
lyze differences. They furthermore focus on de-
scribing many different error classes, instead of
closely investigating particular phenomena.
Evaluation Metrics. We extract recall and pre-
cision errors. How does our error analysis frame-
work relate to coreference resolution evaluation
metrics, which quantify recall and precision er-
rors? We first observe a fundamental difference:
evaluation metrics deal with scoring coreference
chains, they provide no means of extracting recall
or precision errors. Therefore our analysis com-
plements insights obtained via evaluation metrics.
We follow Chen and Ng (2013) and distinguish
between linguistically agnostic metrics, which do
not employ linguistic information during scoring,
and linguistically informed metrics, which employ
linguistic information similar as we do when com-
puting spanning trees.
We limit the discussion of linguistically ag-
nostic metrics to the three most popular evalua-
tion metrics whose average constitutes the official
score in the CoNLL shared tasks on coreference
resolution: MUC (Vilain et al., 1995), B
3
(Bagga
and Baldwin, 1998) and CEAF
e
(Luo, 2005).
10
Our framework bears most similarities to the
MUC metric, as both are based on the same link-
based entity representation. In particular, when we
divide the number of errors extracted from an en-
tity by the size of a spanning tree for that entity, we
obtain a score linearly related
11
to the MUC score
for that entity (recall for reference entities, preci-
sion for system entities). B
3
and CEAF
e
are not
founded on a link-based structure. B
3
computes
recall by computing the relative overlap of refer-
ence and system entity for each reference mention,
and then normalizes by the number of mentions.
CEAF
e
computes an optimal entity alignment with
respect to the relative overlap, and then normalizes
by the number of entities. As the metrics are not
link-based, they do not provide means to extract
link-based errors. We leave determining whether
the framework of these metrics exhibits a useful
notion of errors to future work.
10
These are linguistically agnostic since they do not differ
between different mention or entity types when evaluating.
11
via the transformation x 7? 1? x
Recent work considered devising evaluation
metrics which take linguistic information into
account. Chen and Ng (2013) inject linguis-
tic knowledge into existing evaluation metrics by
weighting links in an entity representation graph.
Tuggener (2014) devises scoring algorithms tai-
lored for particular applications by redefining the
notion of a correct link. While both of these works
focus on scoring, they weight or explicitly define
links in the reference and system entities, thereby
they in principle allow error extraction. However,
the authors do not attempt this and it is not clear
whether the errors extracted that way are useful for
analysis and system development.
7 Conclusions
We presented a novel link-based framework for
coreference resolution error analysis, which ex-
tends and complements previous work. We ap-
plied the framework to analyze recall errors of four
state-of-the-art systems on a large English bench-
mark dataset. Concentrating on errors involving
only proper names and common nouns, we identi-
fied a core set of challenging errors common to all
systems in our study.
We characterized the common errors among a
broad range of properties. In particular, our anal-
ysis highlights and quantifies the usefulness of
world knowledge. Furthermore, by comparing the
recall errors made by each system, we identified
individual strengths and weaknesses. A brief pre-
cision error analysis highlighted the hardness of
resolving noun-name and noun-noun links.
The presented method and findings help to iden-
tify challenges in coreference resolution and to in-
vestigate ways to overcome these challenges.
Acknowledgments
This work has been funded by the Klaus Tschira
Foundation, Heidelberg, Germany. The first au-
thor has been supported by a HITS Ph.D. scholar-
ship.
References
Mira Ariel. 1988. Referring and accessibility. Journal
of Linguistics, 24(1):65?87.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings
of the 1st International Conference on Language
Resources and Evaluation, Granada, Spain, 28?30
May 1998, pages 563?566.
2079
Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), Jeju
Island, Korea, 8?14 July 2012, pages 389?398.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25?27 October 2008, pages 294?
303.
Anders Bj?orkelund and Rich?ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Proceedings of the Shared Task
of the 16th Conference on Computational Natural
Language Learning, Jeju Island, Korea, 12?14 July
2012, pages 49?55.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
Proceedings of the 23rd International Conference
on Computational Linguistics, Beijing, China, 23?
27 August 2010, pages 143?151.
Chen Chen and Vincent Ng. 2012. Combining the
best of two worlds: A hybrid approach to multilin-
gual coreference resolution. In Proceedings of the
Shared Task of the 16th Conference on Computa-
tional Natural Language Learning, Jeju Island, Ko-
rea, 12?14 July 2012, pages 56?63.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Proceed-
ings of the 6th International Joint Conference on
Natural Language Processing, Nagoya, Japan, 14?
18 October 2013, pages 1366?1374.
Greg Durrett and Dan Klein. 2013. Easy victories
and uphill battles in coreference resolution. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, Seattle, Wash.,
18?21 October 2013, pages 1971?1982.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidi?u.
2012. Latent structure perceptron with feature in-
duction for unrestricted coreference resolution. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012, pages 41?48.
Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
driven analysis of challenges in coreference reso-
lution. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, Seattle, Wash., 18?21 October 2013, pages 265?
277.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?561.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885?916.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of the Hu-
man Language Technology Conference and the 2005
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 6?8
October 2005, pages 25?32.
Sebastian Martschat, Jie Cai, Samuel Broscheit,
?
Eva
M?ujdricza-Maydt, and Michael Strube. 2012. A
multigraph model for coreference resolution. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012, pages 100?106.
Sebastian Martschat. 2013. Multigraph clustering for
unsupervised coreference resolution. In 51st Annual
Meeting of the Association for Computational Lin-
guistics: Proceedings of the Student Research Work-
shop, Sofia, Bulgaria, 5?7 August 2013, pages 81?
88.
Alexis Mitchell, Stephanie Strassel, Shudong Huang,
and Ramez Zakhary. 2004. ACE 2004 multilingual
training corpus. LDC2005T09, Philadelphia, Penn.:
Linguistic Data Consortium.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, Philadel-
phia, Penn., 7?12 July 2002, pages 104?111.
Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: The first fifteen years. In Proceed-
ings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, Uppsala, Swe-
den, 11?16 July 2010, pages 1396?1411.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, New York, N.Y.,
4?9 June 2006, pages 192?199.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of the Shared Task of the 15th Conference on
Computational Natural Language Learning, Port-
land, Oreg., 23?24 June 2011, pages 1?27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
2080
of the Shared Task of the 16th Conference on Com-
putational Natural Language Learning, Jeju Island,
Korea, 12?14 July 2012, pages 1?40.
Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted men-
tions: A reference implementation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), Baltimore, Md., 22?27 June 2014, pages 30?
35.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, Cambridge, Mass.,
9?11 October 2010, pages 492?501.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
Portland, Oreg., 19?24 June 2011, pages 814?824.
Lev Ratinov and Dan Roth. 2012. Learning-based
multi-sieve co-reference resolution with knowledge.
In Proceedings of the 2012 Conference on Empirical
Methods in Natural Language Processing and Nat-
ural Language Learning, Jeju Island, Korea, 12?14
July 2012, pages 1234?1244.
Don Tuggener. 2014. Coreference resolution evalua-
tion for higher level applications. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, Gothen-
burg, Sweden, 26?30 April 2014, pages 231?235.
Olga Uryupina, Massimo Poesio, Claudio Giuliano,
and Kateryna Tymoshenko. 2011. Disambigua-
tion and filtering methods in using web knowledge
for coreference resolution. In Proceedings of the
24th International Florida Artificial Intelligence Re-
search Society Conference, Palm Beach, USA, 18?
20 May 2011, pages 317?322.
Olga Uryupina. 2007. Knowledge acquisition for
coreference resolution. Ph.D. thesis, Saarland Uni-
versity, Saarbr?ucken, Germany.
Olga Uryupina. 2008. Error analysis for learning-
based coreference resolution. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation, Marrakech, Morocco, 26
May ? 1 June 2008, pages 1914?1919.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th Message Understanding Conference
(MUC-6), pages 45?52, San Mateo, Cal. Morgan
Kaufmann.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2013. OntoNotes release 5.0.
LDC2013T19, Philadelphia, Penn.: Linguistic Data
Consortium.
2081
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2082?2093,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Rule-Based System for Unrestricted Bridging Resolution:
Recognizing Bridging Anaphora and Finding Links to Antecedents
Yufang Hou
1
, Katja Markert
2
, Michael Strube
1
1
Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
2
School of Computing, University of Leeds, UK
scskm@leeds.ac.uk
Abstract
Bridging resolution plays an important
role in establishing (local) entity coher-
ence. This paper proposes a rule-based
approach for the challenging task of unre-
stricted bridging resolution, where bridg-
ing anaphors are not limited to defi-
nite NPs and semantic relations between
anaphors and their antecedents are not re-
stricted to meronymic relations. The sys-
tem consists of eight rules which target
different relations based on linguistic in-
sights. Our rule-based system significantly
outperforms a reimplementation of a pre-
vious rule-based system (Vieira and Poe-
sio, 2000). Furthermore, it performs better
than a learning-based approach which has
access to the same knowledge resources
as the rule-based system. Additionally,
incorporating the rules and more features
into the learning-based system yields a mi-
nor improvement over the rule-based sys-
tem.
1 Introduction
Bridging resolution recovers the various non-
identity relations between anaphora and an-
tecedents. It plays an important role in establish-
ing entity coherence in a text. In Example 1, the
links between the bridging anaphors (The five as-
tronauts and touchdown) and the antecedent (The
space shuttle Atlantis) establish (local) entity co-
herence.
1
(1) The space shuttle Atlantis landed at a desert
air strip at Edwards Air Force Base, Calif.,
ending a five-day mission that dispatched
the Jupiter-bound Galileo space probe. The
1
Examples are from OntoNotes (Weischedel et al., 2011).
Bridging anaphora are typed in boldface; antecedents in ital-
ics.
five astronauts returned to Earth about three
hours early because high winds had been pre-
dicted at the landing site. Fog shrouded the
base before touchdown.
Bridging or associative anaphora has been
widely discussed in the linguistic literature (Clark,
1975; Prince, 1981; Gundel et al., 1993;
L?obner, 1998). Poesio and Vieira (1998) and
Bunescu (2003) include cases where antecedent
and anaphor are coreferent but do not share the
same head noun (different-head coreference). We
follow our previous work (Hou et al., 2013b) and
restrict bridging to non-coreferential cases. We
also exclude comparative anaphora (Modjeska et
al., 2003).
Bridging resolution includes two subtasks: (1)
recognizing bridging anaphors and (2) finding the
correct antecedent among candidates. In recent
empirical work, these two subtasks have been
tackled separately: (Markert et al., 2012; Cahill
and Riester, 2012; Rahman and Ng, 2012; Hou et
al., 2013a) handle bridging recognition as part of
information status (IS) classification, while (Poe-
sio et al., 1997; Poesio et al., 2004; Markert et
al., 2003; Lassalle and Denis, 2011; Hou et al.,
2013b) concentrate on antecedent selection only,
assuming that bridging recognition has already
been performed. One exception is Vieira and Poe-
sio (2000). They propose a rule-based system for
processing definite NPs. However, they include
different-head coreference into bridging. They re-
port results for the whole anaphora resolution but
do not report results for bridging resolution only.
Another exception is R?osiger and Teufel (2014).
They apply a coreference resolution system with
several additional semantic features to find bridg-
ing links in scientific text where bridging anaphors
are limited to definite NPs. They report prelim-
inary results using the CoNLL scorer. However,
we think the coreference resolution system and the
evaluation metric for coreference resolution are
2082
not suitable for bridging resolution since bridging
is not a set problem.
Another vein of research for bridging resolu-
tion focuses on formal semantics. Asher and Las-
carides (1998) and Cimiano (2006) model bridg-
ing by integrating discourse structure and seman-
tics from a formal semantics viewpoint. However,
the implementation of such a theoretical frame-
work is beyond the current capabilities of NLP
since it depends heavily on commonsense entail-
ment.
In this paper, we propose a rule-based system
for unrestricted bridging resolution. The system
consists of eight rules which we carefully design
based on linguistic intuitions, i.e., how the nature
of bridging is reflected by various lexical, syntac-
tic and semantic features. We evaluate our rule-
based system on a corpus where bridging is reli-
ably annotated. We find that our rule-based sys-
tem significantly outperforms a reimplementation
of a previous rule-based system (Vieira and Poe-
sio, 2000). We further notice that our rule-based
system performs better than a learning-based ap-
proach which has access to the same knowledge
resources as the rule-based system. Surprisingly,
incorporating the rules and more features into the
learning-based approach only yields a minor im-
provement over the rule-based system. We ob-
serve that diverse bridging relations and relatively
small-scale data for each type of relations make
generalization difficult for the learning-based ap-
proach. This work is ? to the best of our
knowledge ? the first system recognizing bridging
anaphora and finding links to antecedents for unre-
stricted phenomenon where bridging anaphors are
not limited to definite NPs and semantic relations
between anaphors and their antecedents are not re-
stricted to meronymic relations.
2 Data
All the data used throughout the paper come
from the ISNotes corpus
2
released by Hou et al.
(2013b). This corpus contains around 11,000 NPs
annotated for information status including 663
bridging NPs and their antecedents in 50 texts
taken from the WSJ portion of the OntoNotes cor-
pus (Weischedel et al., 2011). ISNotes is reli-
ably annotated for bridging: for bridging anaphor
recognition, ? is over 60 for all three possible an-
2
http://www.h-its.org/english/research/nlp/download/
isnotes.php
notator pairings (? is over 70 for two expert anno-
tators); for selecting bridging antecedents, agree-
ment is around 80% for all annotator pairings.
It is notable that bridging anaphors in ISNotes
are not limited to definite NPs as in previous work
(Poesio et al., 1997; Poesio et al., 2004; Lassalle
and Denis, 2011). Table 1 shows the bridging
Bridging Anaphors 663
Non-determiner 44.9%
Definite 38.5%
Indefinite 15.4%
Other-determiner 1.2%
Table 1: Bridging anaphora distribution w.r.t. de-
terminers in ISNotes.
anaphora distribution with regard to determiners in
ISNotes: only around 38% of bridging anaphors
are definite NPs (NPs modified by the); 15.4%
of bridging anaphors are modified by determiners
such as a, an or one which normally indicate in-
definite NPs. Most bridging anaphors (43%) are
not modified by any determiners, such as touch-
down in Example 1. A small fraction of bridging
anaphors (1.2%) are modified by other determin-
ers, such as demonstratives.
The semantic relations between anaphor and
antecedent in the corpus are extremely diverse:
only 14% of anaphors have a part-of/attribute-
of relation with the antecedent (see Example 2)
and only 7% of anaphors stand in a set relation-
ship to the antecedent (see Example 3). 79%
of anaphors have ?other? relations with their an-
tecedents (without further distinction), including
encyclopedic relations such as The space shut-
tle Atlantis-The five astronauts (see Example 1)
as well as context-specific relations such as The
space shuttle Atlantis-touchdown (Example 1).
(2) At age eight, Josephine Baker was sent by
her mother to a white women?s house to do
chores in exchange for meals and a place to
sleep ? a place in the basement with coal.
(3) This creates several problems. One is that
there are not enough police to satisfy small
businesses.
In ISNotes, bridging anaphora with distant an-
tecedents are common when the antecedent is the
global focus of a document. 29% of the anaphors
in the corpus have antecedents that are three or
more sentences away.
2083
Bridging resolution is an extremely challeng-
ing task in ISNotes. In contrast with surface clues
for coreference resolution, there are no clear sur-
face clues for bridging resolution. In Example 4,
the bridging anaphor low-interest disaster loans
associates to the antecedent the Carolinas and
Caribbean, whereas in Example 5 the NP loans is
a generic use. In Example 6, the bridging anaphor
The opening show associates to the antecedent
Mancuso FBI, whereas the NP the show is coref-
erent with its antecedent Mancuso FBI.
(4) The $2.85 billion measure comes on top of
$1.1 billion appropriated after Hugo stuck
the Carolinas and Caribbean last month, and
these totals don?t reflect the additional benefit
of low-interest disaster loans.
(5) Many states already have Enterprise Zones
and legislation that combines tax incentives,
loans, and grants to encourage investment in
depressed areas.
(6) Over the first few weeks, Mancuso FBI has
sprung straight from the headlines. The
opening show featured a secretary of defense
designate accused of womanizing (a la John
Tower).
. . .
Most of all though, the show is redeemed
by the character of Mancuso.
Our previous work on bridging resolution on
this corpus only focuses on its subtasks. In
Hou et al. (2013a) we model bridging anaphora
recognition as a subtask of learning fine-grained
information status. We report an F-measure
of 0.42 for bridging anaphora recognition. In
Hou et al. (2013b) we propose a joint inference
framework for antecedent selection by exploring
Markov logic networks. We report an accuracy
of 0.41 for antecedent selection given gold bridg-
ing anaphora. In this paper, we aim to solve these
two substasks together, i.e., recognizing bridging
anaphora and finding links to antecedents.
3 Method
In this section, we describe our rule-based system
for unrestricted bridging resolution. We choose
ten documents randomly from the corpus as the
development set. Then we carefully design rules
for finding ?bridging links? among all NPs in a
document based on the generalizations of bridg-
ing in the linguistic literature as well as our in-
spections of bridging annotations in the develop-
ment set. The system consists of two components:
bridging link prediction and post processing.
3.1 Bridging Link Prediction
The bridging link prediction component consists
of eight rules. L?obner (1985; 1998) interprets
bridging anaphora as a particular kind of func-
tional concept, which in a given situation assign
a necessarily unique correlate to a (implicit) pos-
sessor argument. He distinguishes between rela-
tional nouns (e.g. parts terms, kinship terms, role
terms) and sortal nouns and points out that rela-
tional nouns are more frequently used as bridg-
ing anaphora than sortal nouns. Rule1 to Rule4 in
our system aim to resolve such relational nouns.
We design Rule5 and Rule6 to capture set bridg-
ing. Finally, Rule7 and Rule8 are motivated by
previous work on implicit semantic role labeling
(Laparra and Rigau, 2013) which focuses on few
predicates.
For all NPs in a document, each rule r is applied
separately to predict a set of potential bridging
links. Every rule has its own constraints on bridg-
ing anaphora and antecedents respectively. Bridg-
ing anaphors are diverse with regard to syntactic
form and function: they can be modified by def-
inite or indefinite determiners (Table 1), further-
more they can take the subject (e.g. Example 3
and Example 6) or other positions (e.g. Example
2 and Example 4) in sentences. The only fre-
quent syntactic property shared is that bridging
anaphors most often have a simple internal struc-
ture concerning modification. Therefore we first
create an initial list of potential bridging anaphora
A which excludes NPs which have a complex syn-
tactic structure. An NP is added to A if it does
not contain any other NPs and do not have modifi-
cations strongly indicating comparative NPs (such
as other symptoms)
3
. Since head match is a strong
indicator of coreference anaphora for definite NPs
(Vieira and Poesio, 2000; Soon et al., 2001), we
further exclude definite NPs from A if they have
the same head as a previous NP. Then a set of
potential bridging anaphors A
r
is chosen from A
based on r?s constraints on bridging anaphora. Fi-
nally, for each potential bridging anaphor ana ?
3
A small list of 10 markers such as such, another . . . and
the presence of adjectives or adverbs in the comparative form
are used to predict comparative NPs.
2084
Ar
, a single best antecedent ante from a list of
candidate NPs (C
ana
) is chosen if the rule?s con-
straint on antecedents is applied successfully.
Every rule has its own scope to form the
antecedent candidate set C
ana
. Instead of using
a static sentence window to construct the list of
antecedent candidates like most previous work for
resolving bridging anaphora (Poesio et al., 1997;
Markert et al., 2003; Poesio et al., 2004; Lassalle
and Denis, 2011), we use the development set
to estimate the proper scope for each rule. The
scope is influenced by the following factors: (1)
the nature of the target bridging link (e.g., set
bridging is a local coherence phenomenon where
the antecedent often occurs in the same or up
to two sentences prior to the anaphor); and (2)
the strength of the rule?s constraint to select the
correct antecedent (e.g., in Rule8, the ability
to select the correct antecedent decreases with
increasing the scope to contain more antecedent
candidates). In the following, we describe the mo-
tivation for each rule and their constraints in detail.
Rule1: building part NPs. To capture typical
part-of bridging (see Example 2), we extract a
list of 45 nouns which specify building parts (e.g.
room or roof ) from the General Inquirer lexicon
(Stone et al., 1966). A common noun phrase from
A is added to A
r1
if: (1) its head appears in the
building part list; and (2) it does not contain any
nominal pre-modifications. Then for each poten-
tial bridging anaphor ana ? A
r1
, the NP with
the strongest semantic connectivity to the potential
anaphor ana among all NPs preceding ana from
the same sentence as well as from the previous two
sentences is predicted to be the antecedent.
The semantic connectivity of an NP to a po-
tential anaphor is measured via the hit counts of
the preposition pattern query (anaphor preposi-
tion NP) in big corpora
4
. An initial effort to ex-
tract partOf relations using WordNet yields low
recall on the development set. Therefore we use
semantic connectivity expressed by prepositional
patterns (e.g. the basement of the house) to cap-
ture underlying semantic relations. Such syntactic
patterns are also explored in Poesio et al. (2004) to
resolve meronymy bridging.
4
We use Gigaword (Parker et al., 2011) with automatic
POS tag and NP chunk information.
Rule2: relative person NPs. This rule is used
to capture the bridging relation between a relative
(e.g. The husband) and its antecedent (e.g. She).
A list of 110 such relative nouns is extracted from
WordNet. However, some relative nouns are fre-
quently used generically instead of being bridging,
such as children. To exclude such cases, we com-
pute the argument taking ratio ? for an NP using
NomBank (Meyers et al., 2004). For each NP, ? is
calculated via its head frequency in the NomBank
annotation divided by the head?s total frequency
in the WSJ corpus in which the NomBank anno-
tation is conducted. The value of ? reflects how
likely an NP is to take arguments. For instance,
the value of ? is 0.90 for husband but 0.31 for
children. To predict bridging anaphora more ac-
curately, a conservative constraint is used. An NP
from A is added to A
r2
if: (1) its head appears in
the relative person list; (2) its argument taking ra-
tio ? is bigger than 0.5; and (3) it does not contain
any nominal or adjective pre-modifications. Then
for each potential bridging anaphor ana ? A
r2
,
the closest non-relative person NP among all NPs
preceding ana from the same sentence as well as
from the previous two sentences is chosen as its
antecedent.
Rule3: GPE job title NPs. In news articles, it is
common that a globally salient geo-political entity
(hence GPE, e.g. Japan or U.S.) is introduced in
the beginning, then later a related job title NP (e.g.
officials or the prime minister) is used directly
without referring to this GPE explicitly. To resolve
such bridging cases accurately, we compile a list
of twelve job titles which are related to GPEs (e.g.
mayor or official). An NP from A is added to A
r3
if its head appears in this list and does not have a
country pre-modification (e.g. the Egyptian pres-
ident). Then for each potential bridging anaphor
ana ? A
r3
, the most salient GPE NP among all
NPs preceding ana is predicted as its antecedent.
We use the NP?s frequency in the whole document
to measure its salience throughout the paper. In
case of a tie, the closest one is chosen to be the
predicted antecedent.
Rule4: role NPs. Compared to Rule3, Rule4
is designed to resolve more general role NPs to
their implicit possessor arguments. We extract a
list containing around 100 nouns which specify
professional roles from WordNet (e.g. chairman,
president or professor). An NP from A is added to
2085
Ar4
if its head appears in this list. Then for each
potential bridging anaphor ana ? A
r4
, the most
salient proper name NP which stands for an orga-
nization among all NPs preceding ana from the
same sentence as well as from the previous four
sentences is chosen as its antecedent (if such an
NP exists). Recency is again used to break ties.
Rule5: percentage NPs. In set bridging as
shown in Example 7, the anaphor (Seventeen per-
cent) is indicated by a percentage expression from
A, which is often in the subject position. The an-
tecedent (the firms) is predicted to be the closest
NP which modifies another percentage NP via the
preposition ?of? among all NPs occurring in the
same or up to two sentences prior to the potential
anaphor.
(7) 22% of the firms said employees or owners
had been robbed on their way to or from
work. Seventeen percent reported their cus-
tomers being robbed.
Rule6: other set member NPs. In set bridg-
ing, apart from percentage expressions, numbers
or indefinite pronouns are also good indicators for
bridging anaphora. For such cases, the anaphor
is predicted if it is: (1) a number expression (e.g.
One in Example 3) or an indefinite pronoun(e.g.
some, as shown in Example 8) from A; and (2) a
subject NP. The antecedent is predicted to be the
closest NP among all plural, subject NPs preced-
ing the potential anaphor from the same sentence
as well as from the previous two sentences (e.g.
Reds and yellows in Example 8). If such an NP
does not exist, the closest NP among all plural, ob-
ject NPs preceding the potential anaphor from the
same sentence as well as from the previous two
sentences is chosen to be the predicted antecedent
(e.g. several problems in Example 3).
(8) Reds and yellows went about their business
with a kind of measured grimness. Some
frantically dumped belongings into pillow-
cases.
Rule7: argument-taking NPs I. Laparra and
Rigau (2013) found that different instances of the
same predicate in a document likely maintain the
same argument fillers. Here we follow this as-
sumption but apply it to nouns and their nomi-
nal modifiers only: different instances of the same
noun predicate likely maintain the same argument
fillers indicated by nominal modifiers. First, a
common noun phrase from A is added to A
r7
if:
(1) its argument taking ratio ? is bigger than 0.5;
(2) it does not contain any nominal or adjective
pre-modifications; and (3) it is not modified by in-
definite determiners
5
which usually introduce new
discourse referents (Hawkins, 1978). Then for
each potential bridging anaphor ana ? A
r7
, we
choose the antecedent by performing the follow-
ing steps:
1. We take ana?s head lemma form ana
h
and collect all its syntactic modifications in
the document. We consider nominal pre-
modification, possessive modification as well
as prepositional post-modification. All real-
izations of these modifications which precede
ana form the antecedent candidates setC
ana
.
2. We choose the most recent NP from C
ana
as the predicted antecedent for the potential
bridging anaphor ana.
In Example 9, we first predict the two occur-
rences of residents as bridging anaphors. Since
in the text, other occurrences of the lemma ?res-
ident? are modified by ?Marina? (supported by
Marina residents) and ?buildings? (supported by
some residents of badly damaged buildings), we
collect all NPs whose syntactic head is ?Ma-
rina? or ?buildings? in C
ana
(i.e. Marina, badly
damaged buildings and buildings with substan-
tial damage). Then among all NPs in C
ana
, the
most recent NP is chosen to be the antecedent (i.e.
buildings with substantial damage).
(9) She finds the response of Marina residents to
the devastation of their homes ?incredible?.
. . .
Out on the streets, some residents of badly
damaged buildings were allowed a 15 minute
scavenger hunt through their possessions.
. . .
After being inspected, buildings with sub-
stantial damage were color - coded.
Green allowed residents to re-enter; red
allowed residents one last entry to gather
everything they could within 15 minutes.
Rule8: argument-taking NPs II. Prince (1992)
found that discourse-old entities are more likely
5
We compile a list of 17 such determiners, such as a, an
or one.
2086
to be represented by NPs in subject position.
Although she could not draw a similar conclu-
sion when collapsing Inferrable (= bridging) with
Discourse-old Nonpronominal, we find that in the
development set, an argument-taking NP in the
subject position is a good indicator for bridging
anaphora (e.g. participants in Example 10). A
common noun phrase from A is collected in A
r8
if: (1) its argument taking ratio ? is bigger than
0.5; (2) it does not contain any nominal or adjec-
tive pre-modifications; and (3) it is in the subject
position. Semantic connectivity again is used as
the criteria to choose the antecedent: for each po-
tential bridging anaphor ana ? A
r8
, the NP with
the strongest semantic connectivity to ana among
all NPs preceding ana from the same sentence as
well as from the previous two sentences is pre-
dicted to be the antecedent.
(10) Initial steps were taken at Poland?s first in-
ternational environmental conference, which
I attended last month. . . . While Polish data
have been freely available since 1980, it was
no accident that participants urged the free
flow of information.
3.2 Post-processing
In the bridging link prediction component, each
rule is applied separately. To resolve the conflicts
between different rules (e.g., two rules predict dif-
ferent antecedents for the same potential anaphor),
a post processing step is applied. We first order
the rules according to their precision for predicting
bridging pairs (i.e., recognizing bridging anaphors
and finding links to antecedents) in the develop-
ment set. When a conflict happens, the rule with
the highest order has the priority to decide the an-
tecedent. Table 2 summarizes the rules described
in Section 3.1, the numbers in square brackets in
the first column indicate the order of the rules. Ta-
ble 3 shows the precisions of bridging anaphora
recognition and bridging pairs prediction for each
rule in the development set. Firing rate is the
proportion of bridging links predicted by rule r
among all predicted links.
4 Experiments and Results
4.1 Experimental Setup
We conduct all experiments on the ISNotes cor-
pus. We use the OntoNotes named entity and syn-
tactic annotations to extract features. Ten doc-
uments containing 113 bridging anaphors from
the ISNotes corpus are set as the development set
to estimate parameters for the rule-based system.
The remaining 40 documents are used as the test
set. In order to compare the results of different
systems directly, we evaluate all systems on the
test set.
4.2 Evaluation Metric
In ISNotes, bridging is annotated mostly between
an NP (anaphor) and an entity (antecedent)
6
, so
that a bridging anaphor could have multiple links
to different instantiations of the same entity (entity
information is based on the Ontonotes coreference
annotation). For bridging resolution, we use an
evaluation metric based on bridging anaphors in-
stead of all links between bridging anaphors and
their antecedent instantiations. A link predicted by
the system is counted as correct if it recognizes the
bridging anaphor correctly and links the anaphor
to any instantiation of the right antecedent entity
preceding the anaphor.
In the evaluation metric, recall is calculated
via the number of the correct links predicted by
the system (one unique link per each predicted
anaphor) divided by the total number of the gold
bridging anaphors, precision is calculated via the
number of the correct links predicted by the sys-
tem divided by the total links predicted by the sys-
tem.
4.3 A Learning-based Approach
To compare our rule-based system (hence ruleSys-
tem, described in Section 3) with other ap-
proaches, we implement a learning-based system
for unrestricted bridging resolution. We adapt the
pairwise model which is widely used in corefer-
ence resolution (Soon et al., 2001). Similar to
the rule-based system, we first create an initial list
of possible bridging anaphora A
ml
with one more
constraint. The purpose is to exclude as many ob-
vious non-bridging anaphoric NPs from the list
as possible. An NP is added to A
ml
if: (1) it
does not contain any other NPs; (2) it is not mod-
ified by pre-modifications which strongly indicate
comparative NPs; and (3) it is not a pronoun or a
proper name. Then for each NP a ? A
ml
, a list
of antecedent candidates C
a
is created by includ-
ing all NPs preceding a from the same sentence
6
There are a few cases where bridging is annotated be-
tween an NP and a non-NP antecedent (e.g. verbs or clauses).
2087
antecedent
rule anaphor antecedent
candidates scope
rule1 [2] building part NPs the NP with the strongest semantic connectivity to the two
potential anaphor
rule2 [5] relative person NPs the closest person NP which is not a relative NP two
rule3 [6] GPE job title NPs the most salient GPE NP all
rule4 [7] role NPs the most salient organization NP four
rule5 [1] percentage NPs the closest NP which modifies another percentage NP two
via the preposition ?of?
rule6 [3] other set member NPs the closest subject, plural NP; two
otherwise the closest object, plural NP
rule7 [4] argument-taking NPs I the closest NP whose head is an unfilled role of the potential all
anaphor (such a role is predicted via syntactic modifications of NPs
which have the same head as the potential anaphor)
rule8 [8] argument-taking NPs II the NP with the strongest semantic connectivity to the two
potential anaphor
Table 2: Rules for unrestricted bridging resolution. Antecedent candidates scope are verified in the
development set: ?all? represents all NPs preceding the potential anaphor from the whole document,
?four? NPs occurring in the same or up to four sentences prior to the potential anaphor, ?two? NPs
occurring in the same or up to two sentences prior to the potential anaphor.
anaphora recognition bridging pairs prediction
rule anaphora
precision precision
firing rate
rule1 [2] building part NPs 75.0% 50.0% 6.1%
rule2 [5] relative person NPs 69.2% 46.2% 6.1%
rule3 [6] GPE job title NPs 52.6% 44.7% 19.4%
rule4 [7] role NPs 61.7% 32.1% 28.6%
rule5 [1] percentage NPs 100.0% 100.0% 2.6%
rule6 [3] other set member NPs 66.7% 46.7% 7.8%
rule7 [4] argument-taking NPs I 53.8% 46.4% 6.1%
rule8 [8] argument-taking NPs II 64.5% 25.0% 25.5%
Table 3: Precision of bridging anaphora recognition and bridging pairs prediction for each rule in the
development set. The numbers in square brackets in the first column indicate the order of the rules.
as well as from the previous two sentences
7
. We
create a pairwise instance (a, c) for every c ? C
a
.
We also add extra pairwise instances from the pre-
diction of ruleSystem to the learning-based sys-
tem. In the decoding stage, the best first strat-
egy (Ng and Cardie, 2002) is used to predict the
bridging links. Specifically, for each a ? A
ml
, we
predict the bridging link to be the most confident
pair (a, c
ante
) among all instances with the posi-
tive prediction. We use SVM
light
to conduct the
experiments
8
. All experiments are conducted via
10-fold cross-validation on the whole corpus
9
.
7
In ISNotes, 71% of NP antecedents occur in the same
or up to two sentences prior to the anaphor. Initial experi-
ments show that increasing the window size more than two
sentences decreases the performance.
8
To deal with data imbalance, the SVM
light
parameter
is set according to the ratio between positive and negative
instances in the training set.
9
To compare the learning-based approach to the rule-
based system described in Section 3 directly, we report the
mlSystem ruleFeats We provide mlSys-
tem ruleFeats with the same knowledge resources
as the rule-based system. All rules from the
rule-based system are incorporated into mlSys-
tem ruleFeats as the features.
mlSystem ruleFeats + atomFeats We augment
mlSystem ruleFeats with more features from our
previous work (Markert et al., 2012; Hou et al.,
2013a; Hou et al., 2013b) on bridging anaphora
recognition and antecedent selection. Some of
these features overlap with the atomic features
used in the rule-based system.
Table 4 shows all the features we use for rec-
ognizing bridging anaphora. ??? indicates the re-
sources are used in the rule-based system. We ap-
ply them to the first element a of a pairwise in-
stance (a, c). Markert et al. (2012) and Hou et
results of learning-based approaches on the same test set as
the rule-based system.
2088
Markert et al. local feature set
f1 FullPrevMention (b) ? f2 FullPreMentionTime (n) f3 PartialPreMention (b)
f4 ContentWordPreMention (b) f5 Determiner (n) ? f6 NPtype (n) ?
f7 NPlength (int) f8 GrammaticalRole (n) ? f9 NPNumber (n) ?
f10 PreModByCompMarker (b) ?
Hou et al. local feature set
features to identify bridging anaphora
f1 IsCoherenceGap (b) f2 IsSentFirstMention (b) f3 IsDocFirstMention (b)
f4 IsWordNetRelationalNoun (b) ? f5 IsInquirerRoleNoun (b) f6 IsBuildingPart (b) ?
f7 IsSetElement (b) ? f8 PreModSpatialTemporal (b) f9 IsYearExpression (b)
f10 PreModifiedByCountry (b) ? f11 AppearInIfClause (b) f12 VerbPosTag (l)
f13 IsFrequentGenericNP (b) f14 WorldKnowledgeNP (l) f15 Unigrams (l)
f16 PreModByGeneralQuantifier (b) f17 BridgingHeadNP (l) f18 HasChildNP (b) ?
features to identify function and worldKnowledge NPs
f20 DependOnChangeVerb (b) f21 IsFrequentProperName (b)
Table 4: Features for bridging anaphora recognition from Markert et al. (2012) and Hou et al. (2013a).
?b? indicates binary, ?n? nominal, ?l? lexical features, ??? resources used in the rule-based system.
Group Feature Value
semantic f1 preposition pattern ? the normalized hit counts of the preposition pattern query
a prep. c (e.g. participants of the conference) in big corpora
f2 verb pattern the normalized hit counts of the verb pattern query c verb
a
or
verb
a
c in big corpora (for set bridging in Example 7, the
pattern query is the firms reported)
f3 WordNet partOf whether a partOf relation holds between a and c in WordNet
f4 semantic class ? 16 classes, e.g. location, organization, GPE, rolePerson,
relativePerson, product, date, money, percent
salience f5 document span the normalized value of the span of text in which c is mentioned
f6 utterance distance the sentence distance between a and c
f7 local first mention whether c is the first mention within the previous five sentences
f8 global first mention whether c is the first mention in the whole document
syntactic f9 isSameHead whether a and c share the same head
& (exclude coreferent antecedent candidates)
lexical f10 isWordOverlap whether a is prenominally modified by the head of c (for
bridging where the anaphor is a compound noun, such as
the mine-mine security)
f11 isCoArgument whether subject c and object a are dependent on the same verb
(the subject can not be the bridging antecedent of the object
in the same clause)
f12 WordNet distance the inverse value of the shortest path length between a and c
in WordNet
Table 5: Features for antecedent selection from Hou et al. (2013b). ??? indicates resources used in the
rule-based system.
al. (2013a) classify eight fine-grained information
status (IS) categories for NPs: old, new and 6
mediated categories (syntactic, worldKnowledge,
bridging, comparative, aggregate and function).
Features from Markert et al. (2012) work well to
identify old, new and several mediated categories
but fail to recognize most bridging anaphora. Hou
et al. (2013a) remedy this by adding discourse
structure features (f1-f3), semantic features (f4-
f10) and features to detect generic nouns (f11-
2089
Feature Value
for anaphor candidate a
f1 preModByNominal whether a contains any nominal pre-modifications
f2 preModByAdj whether a contains any adjective modifications
f3 isGPEJobTitle whether a is a job title about GPE (e.g. mayor or official)
f4 isArgumentTakingNP whether the argument taking ratio of a is bigger than 0.5
for antecedent candidate c
f5 fullMentionTime the normalized value of the frequency of c in the whole document
for pairwise instance (a, c)
f6 word distance the token distance between a and c
Table 6: Additional atomic features from the rule-based system.
f14 and f16).
Table 5 shows all features we use for selecting
antecedents for bridging anaphora. ??? indicates
the resources that are used in the rule-based sys-
tem. These features are from Hou et al. (2013b)?s
local pairwise model. They try to model: (1) the
semantic relations between bridging anaphors and
their antecedents (f1 to f4); (2) the salience of
an antecedent from different perspectives (f5 to
f8); and (3) the syntactic and lexical constraints
between anaphor and antecedent (f9 to f12).
Apart from the features shown in Table
4 and Table 5, we further enrich mlSys-
tem ruleFeats+atomFeats with additional atomic
features used in the rule-based system (Table 6).
mlSystem atomFeats Based on mlSys-
tem ruleFeats+atomFeats, the rule features
from the rule-based system are removed.
4.4 Baseline
We also reimplement the rule-based system from
Vieira and Poesio (2000) as a baseline. The origi-
nal algorithm focuses on processing definite NPs.
It classifies four categories for the definite NPs:
discourse new, direct anaphora (same-head coref-
erent anaphora), lenient bridging and Unknown.
This algorithm also finds antecedents for NPs
which belong to direct anaphora or lenient bridg-
ing.
Since Vieira and Poesio (2000) include
different-head coreference into their lenient
bridging category, we further divide their le-
nient bridging category into two subcategories:
different-head coreference and bridging. Figure
1 shows the details of the division after failing
to classify an NP as discourse new or direct
anaphora. For more details about the whole
system, see Vieira and Poesio (2000). We then
apply this slightly revised algorithm to process
all NPs in the initial list of potential bridging
anaphoraA from ruleSystem (described in Section
3.1).
4.5 Results and Discussion
Table 7 shows the results on the same test set of
different approaches for unrestricted bridging res-
olution. The results reveal the difficulty of the
task, when evaluating on a realistic scenario with-
out constraints on types of bridging anaphora and
bridging relations.
Both our rule-based system and all learning-
based approaches significantly outperform the
baseline at p < 0.01 (randomization test). The
low recall in baseline is predictable, since it
only considers meronymy bridging and compound
noun anaphors whose head is prenominally mod-
ified by the antecedent head. (e.g. the state?
state gasoline taxes). Under the same features,
the learning-based approach (mlSystem ruleFeats)
performs slightly worse than the rule-based sys-
tem (ruleSystem) with regard to the F-score.
R P F
baseline 2.9 13.3 4.8
ruleSystem 11.9 42.9 18.6
mlSystem ruleFeats 12.1 35.0 18.0
mlSystem ruleFeats+atomFeats 16.7 21.2 18.7
mlSystem atomFeats 20.5 10.1 13.5
Table 7: Experimental results for the baseline, the
rule-based system and the learning-based systems.
Surprisingly, incorporating rich features
into the learning-based approach (mlSys-
tem ruleFeats+atomFeats) does not yield much
improvement over the rule-based system (with an
2090
Figure 1: Vieria & Poesio?s (2000) original algorithm for processing definite NPs. We further divide
their lenient bridging category into two subcategories: 2.1 Different-head coreference and 2.2 Bridging.
F-score of 18.7 in mlSystem ruleFeats+atomFeats
compared to 18.6 in ruleSystem). We suppose that
the learning-based system generalizes poorly with
only atomic features in Table 4, Table 5 and Table
6. Results on mlSystem atomFeats support our
assumption: the F-score drops considerably after
removing the rule features. Although ISNotes is
a reasonably sized corpus for bridging compared
to previous work, diverse bridging relations,
especially lots of context specific relations such
as pachinko-devotees or palms-the thieves, lead
to relatively small-scale training data for each
type of relation. Therefore it is difficult for the
learning-based approach to learn effective rules to
predict bridging links.
However, all learning-based systems tend to
have higher recall but lower precision compared
to the rule-based system. This suggests that the
learning-based systems are ?greedy? to predict
bridging links. A close look at these links in
mlSystem atomFeats indicates that the learning-
based system predicts more correct bridging
anaphors but fails to find the correct antecedents.
In fact, lots of those ?half? correct links sound
reasonable without the specific context, such as
the story-readers (gold bridging link: this novel-
readers) or the executive director?s office-the
desks (gold bridging link: the fund?s building-the
desks).
5 Conclusions
We proposed a rule-based approach for un-
restricted bridging resolution where bridging
anaphora are not limited to definite NPs and the
relations between anaphor and antecedent are not
restricted to meronymic relations. We designed
eight rules to resolve bridging based on linguis-
tic intuition. Our rule-based system performs bet-
ter than a learning-based approach which has ac-
cess to the same knowledge resources as the rule-
based system. Particularly, the learning-based sys-
tem enriched with more features does not yield
much improvement over the rule-based system.
We speculate that the learning-based system could
benefit from more training data. Furthermore, bet-
ter methods to model the semantics of the specific
context need to be explored in the future.
This work is ? to our knowledge ? the first
bridging resolution system that handles the unre-
stricted phenomenon in a realistic setting.
Acknowledgements
We thank Renata Vieira for excavating part of her
source code for us. We also thank the reviewers
for their helpful comments. Yufang Hou is funded
by a PhD scholarship from the Research Training
Group Coherence in Language Processing at Hei-
delberg University. This work has been partially
funded by the Klaus Tschira Foundation.
2091
References
Nicholas Asher and Alex Lascarides. 1998. Bridging.
Journal of Semantics, 15:83?113.
Razvan Bunescu. 2003. Associative anaphora resolu-
tion: A Web-based approach. In Proceedings of the
EACL 2003 Workshop on The Computational Treat-
ment of Anaphora, Budapest, Hungary, 14 April,
2003, pages 47?52.
Aoife Cahill and Arndt Riester. 2012. Automati-
cally acquiring fine-grained information status dis-
tinctions in German. In Proceedings of the SIGdial
2012 Conference: The 13th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
Seoul, Korea, 5?6 July 2012, pages 232?236.
Philipp Cimiano. 2006. Ingredients of a first-order ac-
count of bridging. In Proceedings of the 5th Inter-
national Workshop on Inference in Computational
Semantics, Buxton, U.K., 20?21 April 2006, pages
139?144.
Herbert H. Clark. 1975. Bridging. In Proceedings
of the Conference on Theoretical Issues in Natu-
ral Language Processing, Cambridge, Mass., June
1975, pages 169?174.
Jeanette K. Gundel, Nancy Hedberg, and Ron
Zacharski. 1993. Cognitive status and the form
of referring expressions in discourse. Language,
69:274?307.
John A. Hawkins. 1978. Definiteness and indefinite-
ness: A study in reference and grammaticality pre-
diction. Humanities Press, Atlantic Highlands, N.J.
Yufang Hou, Katja Markert, and Michael Strube.
2013a. Cascading collective classification for bridg-
ing anaphora recognition using a rich linguistic fea-
ture set. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, Seattle, Wash., 18?21 October 2013, pages 814?
820.
Yufang Hou, Katja Markert, and Michael Strube.
2013b. Global inference for bridging anaphora res-
olution. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, 9?14 June 2013, pages
907?917.
Egoitz Laparra and German Rigau. 2013. ImpAr: A
deterministic algorithm for implicit semantic role la-
belling. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, Sofia, Bulgaria, 4?9 August 2013, pages 1180?
1189.
Emmanuel Lassalle and Pascal Denis. 2011. Leverag-
ing different meronym discovery methods for bridg-
ing resolution in French. In Proceedings of the 8th
Discourse Anaphora and Anaphor Resolution Col-
loquium (DAARC 2011), Faro, Algarve, Portugal, 6?
7 October 2011, pages 35?46.
Sebastian L?obner. 1985. Definites. Journal of Seman-
tics, 4:279?326.
Sebastian L?obner. 1998. Definite associative
anaphora. Unpublished Manuscript, Heinrich-
Heine-Universit?at D?usseldorf.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of the EACL Workshop
on the Computational Treatment of Anaphora. Bu-
dapest, Hungary, 14 April 2003, pages 39?46.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, Jeju
Island, Korea, 8?14 July 2012, pages 795?804.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishaman. 2004. Annotating noun ar-
gument structure for NomBank. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, Lisbon, Portugal, 26?28
May 2004, pages 803?806.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning
for other-anaphora resolution. In Proceedings of the
2003 Conference on Empirical Methods in Natural
Language Processing, Sapporo, Japan, 11?12 July
2003, pages 176?183.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, Philadel-
phia, Penn., 7?12 July 2002, pages 104?111.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. LDC2011T07.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Renata Vieira, and Simone Teufel.
1997. Resolving bridging references in unrestricted
text. In Proceedings of the ACL Workshop on Oper-
ational Factors in Practical, Robust Anaphora Res-
olution for Unrestricted Text, Madrid, Spain, July
1997, pages 1?6.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridg-
ing references. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain, 21?26 July 2004, pages
143?150.
Ellen F. Prince. 1981. Towards a taxonomy of given-
new information. In P. Cole, editor, Radical Prag-
matics, pages 223?255. Academic Press, New York,
N.Y.
2092
Ellen F. Prince. 1992. The ZPG letter: Subjects, defi-
niteness, and information-status. In W.C. Mann and
S.A. Thompson, editors, Discourse Description. Di-
verse Linguistic Analyses of a Fund-Raising Text,
pages 295?325. John Benjamins, Amsterdam.
Altaf Rahman and Vincent Ng. 2012. Learning the
fine-grained information status of discourse entities.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Avignon, France, 23?27 April 2012,
pages 798?807.
Ina R?osiger and Simone Teufel. 2014. Resolving
coreference and associative noun phrases in scien-
tific text. In Proceedings of the Student Research
Workshop at the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, 26?30 April 2014,
pages 44?55.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and Cambridge Computer Asso-
ciates. 1966. General Inquirer: A Computer Ap-
proach to Content Analysis. MIT Press, Cambridge,
Mass.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
2093
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 491?500,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Latent Variable Model for
Discourse-aware Concept and Entity Disambiguation
Angela Fahrni and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
(angela.fahrni|michael.strube)@h-its.org
Abstract
This paper takes a discourse-oriented per-
spective for disambiguating common and
proper noun mentions with respect to
Wikipedia. Our novel approach mod-
els the relationship between disambigua-
tion and aspects of cohesion using Markov
Logic Networks with latent variables.
Considering cohesive aspects consistently
improves the disambiguation results on
various commonly used data sets.
1 Introduction
?I have to review a paper?, the super-
visor moaned from the office. ?Please
don?t disturb me until I?m done with the
review.? His student nodded, went to
the cafeteria, sat down in the sunshine
and started to read yesterday?s paper.
This text snippet illustrates two aspects that have
been neglected by previous disambiguation ap-
proaches. (1) The interpretation of different men-
tions, i.e. common and proper nouns, is deter-
mined by different notions of context: some men-
tions depend more on a local sentence-level con-
text (paper in read yesterday?s paper; the global
context is misleading), some more on a global one
(review in I?m done with the review; the local con-
text is not discriminative), some on both global
and local context (paper in review a paper). (2)
The context relevant to disambiguate a mention
depends on how it is embedded into discourse and
is not bound to the surface form of a mention (pa-
per in the first sentence vs. paper in the last one).
Starting from this observation, we argue that the
context relevant to disambiguate a mention cor-
relates with its cohesive scope, i.e. the text span
within which a mention establishes cohesive re-
lations. Therefore, we propose to disambiguate
mentions differently depending on their cohesive
scopes (Section 2). We distinguish between three
different cohesive scopes of mentions and model
them as latent variables using Markov Logic Net-
works (Section 3). The use of latent variables al-
lows us to learn and predict the cohesive scope
and the disambiguation of a mention jointly. This
comes with the advantage that the learning of the
scope assignment does not need annotated data by
itself but is guided by the annotations available for
the target prediction task, i.e. the disambiguation.
In this paper, we focus on concept and entity
disambiguation
1
with respect to an inventory de-
rived from Wikipedia and compare (1) to a state-
of-the-art approach that treats all mentions alike
and uses the same features for disambiguation,
(2) to a pipeline-based approach, and (3) to other
state-of-the-art approaches (Section 4).
While early work disambiguated concepts us-
ing the local context (Csomai and Mihalcea,
2008), current research focuses on exploiting the
global document context (Milne and Witten, 2008;
Kulkarni et al., 2009; Ratinov et al., 2011; Fahrni
and Strube, 2012; Cheng and Roth, 2013). Al-
though such global approaches try to balance be-
tween local and global context, they treat all men-
tions alike, i.e., they apply the same model and the
same weighting of local and global context fea-
tures for disambiguating all mentions (Section 5).
2 Motivation
Halliday and Hasan (1976) define cohesion as ?re-
lations of meaning that exist within the text, and
that define it as a text? (p. 4). A tie is one instance
of such a cohesive relation between two items. Co-
hesive ties occur on various linguistic levels, such
as on the entity level (e.g. coreference and bridg-
ing relations) or on the concept level (e.g. lexical
1
In the following, we use concept to refer to concepts and
what is usually called entities (e.g. Ji et al. (2011)).
491
chains). In this paper, we focus on concept-level
cohesion and assume that each concept referred to
by a mention can exhibit cohesive ties with con-
cepts from other lexical units. The cohesive scope
of a mention is the text span within which a con-
cept referred to by a mention shows such cohesive
ties. We distinguish three broad categories of co-
hesive scopes: (1) Mentions with local cohesive
scope exhibit cohesive ties with lexical units in
the same sentence; (2) mentions with intermedi-
ate cohesive scope show cohesive ties both within
the sentence and beyond; (3) mentions with global
cohesive scope form cohesive ties with mentions
across sentence boundaries.
The notion of scope is a means to define the ap-
propriate context to disambiguate a mention. A
mention of local scope does not exhibit relations
with lexical units outside its sentence. Hence, the
global context does not help to disambiguate it or
can even lead to the wrong disambiguation. For
a mention with global scope, the global context is
crucial, while the local context is not discrimina-
tive or even misleading. For a mention with in-
termediate scope both local and global context are
relevant. Hence, while the scope influences the ap-
propriate disambiguation context, the disambigua-
tion of a mention influences its scope. In the ex-
ample (Section 1), paper in read yesterday?s pa-
per refers to the concept NEWSPAPER. Its scope
is local, as it lacks some cohesive ties with men-
tions in other sentences. If it had been disam-
biguated to SCHOLARLY PAPER, its scope would
be global. This reciprocal relationship between
discourse structure and meaning has also been dis-
cussed by Asher and Lascarides (1995). They
use rhetorical relations for structuring discourse
while we rely on the notion of lexical cohesion
and model scope assignment and disambiguation
jointly.
Our notion of scope is related to work on
lexical chains (Morris and Hirst, 1991; Nelken
and Shieber, 2006; Mihalcea, 2006) and to work
in content modeling, e.g. Haghighi and Vander-
wende (2009) distinguish content vocabulary and
document-specific vocabulary.
3 Approach
Given a set of features for disambiguation, we
aim to weight them differently depending on the
scope. To model the reciprocal relationship be-
tween scope assignment and disambiguation, we
propose a latent variables based approach using
Markov Logic Networks that allows us to learn
the parameters for the scope assignment and the
disambiguation tasks jointly and enables us to per-
form joint inference.
Our approach is joint as we assign the scope s
and predict the concept c for a mention m simulta-
neously. As during learning training data is avail-
able for the disambiguation task but not for the
scope assignment task, we face a problem with
latent variables. Latent variables represent miss-
ing information in the input or a part of the out-
put which is not relevant except for supporting the
prediction of the target (Smith, 2011). In our ap-
proach, the different cohesive scopes are modeled
by latent variables. Each mention to be disam-
biguated is assigned a scope s. All feature weights
are parametrized by scope s. The parameters for
the disambiguation and scope assignment tasks are
learned jointly and are guided by the annotations
available for the disambiguation task.
Markov Logic Networks can be represented as
log-linear models, when grounded, and are there-
fore straightforward to extend with latent variables
(Smith, 2011; Poon and Domingos, 2008). In ad-
dition, global features can be conveniently inte-
grated.
3.1 Markov Logic Networks
Markov Logic (ML) incorporates first-order logic
and probabilities (Domingos and Lowd, 2009).
A Markov Logic Network (MLN) is a first-order
knowledge base and consists of a set of pairs
(F
i
, w
i
), where F
i
is a first-order formula and
w
i
? R is the weight of formula F
i
. It is a tem-
plate for constructing a Markov Network. This
Markov Network has a binary node for each pos-
sible grounding for each predicate of the MLN. If
the grounding of the predicate is true, the binary
node?s value is set to 1, otherwise to 0. Further-
more, it contains one feature
2
for each ground for-
mula F
i
. If a ground formula is true, its feature?s
value is set to 1, otherwise to 0. The feature?s
weight is provided by w
i
.
The probability distribution in the ground
Markov Network is given by
P (X = x) =
1
Z
exp
(
?
i
w
i
n
i
(x)
)
2
In this section feature is used differently than in the rest
of the paper.
492
where n
i
(x) is the number of true groundings of
F
i
in x. The normalization factor Z is the partition
function.
To perform MAP inference we use thebeast
3
which transforms the inference problem into an
Integer Linear Program and solves it using cutting
plane inference (Riedel, 2008).
3.1.1 Weight Learning with Latent Variables
Since no annotations are available for the scope
distinction, we face a latent variable learning prob-
lem. For learning weights in this situation we fol-
low Poon and Domingos (2008). We split our hid-
den predicates into two parts: V are the ones for
which the ground truth is known (concepts) and
U are the ones for which there is no annotation
(scopes). Let O be the observed predicates. Let
o and v be the values of O and V in the train-
ing data. u denotes values assigned to U . Weight
learning finds a w that maximizes the conditional
log-likelihood
L
w
(o, v) = logP
w
(V = v|O = o)
= log
?
u
P
w
(V = v, U = u|O = o),
where the sum is over all possible values of U .
Although L
w
(o, v) is not convex, a local opti-
mum can be found via gradient descent by itera-
tively solving
w
t+1
= w
t
+ ??
w
L
w
(o, v),
where the gradient?
w
L
w
(o, v) is given by
?
?w
i
L
w
(o, v) = E
w
[n
i
(o, v, U)]? E
w
[n
i
(o, V, U)].
E
w
denotes the expectation according to P
w
and n
i
(o, v, u) is the number of true ground-
ings of formula F
i
under the assignment spec-
ified by (o, v, u). We use a voted perceptron
(Lowd and Domingos, 2007) which approximates
the expectations via computing the MAP solution
with (o, v) fixed (E
w
[n
i
(o, v, U)]) and (o) fixed
(E
w
[n
i
(o, V, U)]) respectively.
3.1.2 Scope-aware Concept Disambiguation
Both the scope assignment and the disambiguation
task are performed jointly using Markov Logic
Networks.
3
http://code.google.com/p/thebeast.
Table 1 shows the core of our proposed ap-
proach in terms of predicates and first-order logic
formulas. We build upon our previous approach
for joint concept disambiguation and clustering
(Fahrni and Strube, 2012). For brevity, we only
discuss the scope-aware extension of the disam-
biguation part. The extension for clustering is
done analogously.
The purpose of assigning a scope to each
mention m is to learn scope-specific weights
for disambiguation to account for heteroge-
nous scopes of mentions. The learned weights
are parametrized by scopes. We indicate this
parametrization of learned weights by w(s) (cf.
Table 1, f8, f9).
For each relation to predict, a hidden predicate
is defined. We are interested in predicting two
relations: a relation between a mention m and a
concept c (p1: hasConcept(m, c)) and a relation
between a mention m and a scope s (p3: hasS-
cope(m, s)). To bridge between the disambigua-
tion and the scope assignment task a third hid-
den predicate relatesScopeToConcept(m, c, s) (p2)
models a relation between a mention m, a concept
c and a scope s. This predicate together with For-
mulas f4 ? f7 garantuees that the scope assign-
ment and the selection of a concept for a mention
influence each other and that the ground hidden
predicates are in accordance.
4
Hard cardinality
constraints (f1, f2, f3) enforce that each mention
m is assigned exactly one scope s and at most one
concept c.
The hidden predicates and formulas form the
core. Features for the disambiguation and the
scope assignment tasks are incorporated using lo-
cal and global formulas with learned weights. The
features are described in Section 3.2. Table 1 gives
formula templates for both tasks (please note that
these are templates not formulas (Section 3.2)):
(1) a template for formulas that add information
for scope assignment (f8) and (2) a template for
formulas that add information for disambigua-
tion (f9). All formulas with scope-parametrized
weights that are relevant for the concept prediction
task are defined for the predicate relatesScopeTo-
Concept. This enables us to activate the relevant
4
We also run experiments with just two hidden predi-
cates, i.e. hasConcept(m, c) and hasScope(m, s). All for-
mulas with learned weight were then defined in the fol-
lowing, less efficient way: ?m ? M, c ? C, s ? S :
featureDisambiguation(m, c, q) ? hasConcept(m, c) ?
hasScope(m, s). q is a score (Table 1).
493
Predicates
Hidden predicates
p1 hasConcept(m, c)
p2 relatesScopeToConcept(m, c, s)
p3 hasScope(m, s)
Predicate template for disambiguation features
p4 featureDisambiguation(m, c, q)
Predicate template for scope assignment features
p5 featureScope(m, q)
Formulas
Hard cardinality constraints
f1 ?m ?M : |{c ? C : hasConcept(m, c)}| ? 1
f2 ?m ?M : |{c ? C, s ? S : relatesScopeToConcept(m, c, s)}| ? 1
f3 ?m ?M : |{s ? S : hasScope(m, s)}| = 1
Hard constraints
f4 ?m ?M, c ? C, s ? S : relatesScopeToConcept(m, c, s)? hasConcept(m, c)
f5 ?m ?M, c ? C, s ? S : relatesScopeToConcept(m, c, s)? hasScope(m, s)
f6 ?m ?M, c ? C, s ? S : hasConcept(m, c) ? hasScope(m, s)
? relatesScopeToConcept(m, c, s)
f7 ?m ?M, c ? C : hasConcept(m, c)? (|{s ? S : relatesScopeToConcept(m, c, s)}| = 1)
Formula template with learned weights for scope assignment
f8 q ? w(s) ?m ?M, s ? S : featureScope(m, q)? hasScope(m, s)
Formula template with learned weights for disambiguation
f9 q ? w(s) ?m ?M, c ? C, s ? S : featureDisambiguation(m, c, q)
? relatesScopeToConcept(m, c, s)
Table 1: Predicates and formulas used for scope distinction and disambiguation (m represents a mention,
M sets of mentions, c a concept, C sets of concepts, s a scope, S sets of scopes, q scores, w weights and
w(s) a weight which is parametrized by s). The two template predicates and formulas are generalized
patterns to integrate the features for the scope assignment and disambiguation task (Section 3.2).
scope-specific weights w(s) which depend on the
chosen scope s. The final weight for a formula
can also include a score q defined by the observed
predicate.
3.2 Features
For disambiguation and clustering we build upon
our previous work (Fahrni and Strube, 2012). We
use the same features and formulas and adopt the
latter to learn scope-specific weights. Given for
example the local context similarity feature (pred-
icate hasContextSimilarity(m, c, q) where q is the
similarity score) and the corresponding formula
?m ?M, c ? C
m
: hasContextSimilarity(m, c, q)
? hasConcept(m, c)
with weight (q ? w) we adopt it in the following
way (cf. Table 1, template f9):
?m ?M, s ? S, c ? C
m
:
hasContextSimilarity(m, c, q)
? relatesScopeToConcept(m, c, s)
with weight (q ? w(s)).
In order to distinguish between the three pro-
posed scopes, we use the features described in Ta-
ble 2. The first column shows the predicate which
can be used for template f8 in Table 1.
4 Experiments
We compare our novel scope-aware approach to
our previous scope-ignorant approach (Fahrni and
Strube, 2012) ? which has achieved good results
in the English monolingual and Chinese and Span-
ish cross-lingual entity linking tasks at TAC 2012
and 2013 (Fahrni et al., 2014) ? and a scope-aware
pipeline-based approach using the same features
and preprocessing to ensure a fair comparison.
This allows us to identify the differences in the
results that are due to scope-awareness and differ-
ences in the results that are due to different learn-
ing strategies (joint vs. pipeline-based). In addi-
tion, we compare our joint scope-aware approach
to state-of-the-art approaches using various data
sets.
4.1 Data
Table 3 summarizes our test sets (ACE 2005, ACE
2004, MSNBC and TAC 2011) and our train-
ing and development sets derived from Wikipedia
(WP Training, WP Dev). For each data set we re-
port the total number of annotated mentions, the
number of mentions with a corresponding concept
in Wikipedia (non-NILs) and the number of NILs
(i.e. mentions that do not refer to a Wikipedia con-
494
Predicates Description
Mention-based Features
idfHead(m, q) The more frequent a mention is, the more likely it is to exert a local scope. This is inspired by
work on indexing for IR. We use the idf score of the head of a mention according the English
Gigaword Corpus (Parker et al., 2011).
propernoun(m) Proper nouns are usually more prominent than common nouns and are more likely to have an
intermediate or global scope than common nouns.
singlewordNoun(m) Single word NPs are often less prominent than multi-word NPs and are more likely to be of local
scope.
abbrev(m) Abbreviations with a terminal dot such as Mr. or Ltd. tend to have a local scope as they are usually
local modifiers or specifications.
Features Based on Modification
isPreModified(m) If a mention is pre-modified, it tends to be more prominent than unmodified mentions. If a mention
is more prominent, it is more likely to have a larger scope.
headOfRelClause(m) Mentions that are the head of a relative clause are usually more prominent and are more likely to
have an intermediate or global scope.
Features Based on the Text Structure
inSubjPosition(m) Mentions in theme position, which is in English often the subject, tend to pick up what has already
been mentioned before (Dane
?
s, 1974). Since this is not just the case on the reference-level, but
also on the concept-level, the mention in theme position tends to be related to other mentions in
the text and tends to have an intermediate or global scope.
posInSentence(m, q) The earlier a mention appears in the sentence in English, the more thematic it is, and the more
likely it has an intermediate or global scope.
focusingAdverb(m) Focusing adverbs in the text pattern <focusing adverb> <mention> ? e.g. ?particularly Jack? ?
indicate that the mention is thematic and therefore has larger scope.
modifiesArgument(m) A premodifier of a verbal argument is usually more likely to be of local scope.
passiveBy(m) A passive construction ? e.g. ?the thief was catched by the police? ? is a way to reduce the
prominency of the agent (e.g. police). The agent tends to be of local scope.
inConjunction(m) Conjunctions are often used for exemplifications. Therefore mentions in conjunctions are often
less prominent.
inDepRelPP(m
1
,m
2
)
inDepRelGen(m
1
,m
2
)
In NPs with prepositional or genitive modifiers usually at most one part ? either the modifying NP
or the head ? has intermediate or global scope.
morphoTiesHead(m, q) The more frequent the head of a mention appears in the text ? also as a derivation, e.g. a verb,
according to CatVar (Habash and Dorr, 2003) ?, the more prominent it is.
positionInText(m, q) The earlier a mention appears in text, the more likely it is to exhibit global cohesive scope (cf. the
hard-to-be-beat lead baseline in summarization (Radev et al., 2003)).
Table 2: Features for cohesive scope distinction. m,m
1
,m
2
denote mentions, q a score. The predicates
are plugged in the template formula f8 in Table 1.
Data set No. of
Men-
tions
Non-
NILs
NILs Avg.
Ambi-
guity
WP Training 56,372 53,097 3,275 2.31
WP Dev 9,992 9,375 617 2.28
ACE 2005 29,300 27,184 2,116 6.52
ACE 2004 306 257 49 5.04
TAC 2011 2,250 1,124 1,126 6.32
MSNBC 756 629 127 5.29
Table 3: Statistics for data sets.
cept). The average ambiguity of mentions is given
by our lexicon (see Section 4.2).
Our system is exclusively trained on the internal
hyperlinks in Wikipedia with the advantage that no
manual annotation effort is needed. We use 500 ar-
ticles for training and 100 articles for development
(Fahrni and Strube, 2012). Each internal hyper-
link is considered as an annotated mention. The
pointer to the Wikipedia article serves as the cor-
rect concept for this mention and all other candi-
date concepts we obtain from our lexicon as wrong
concepts for this mention.
For the detailed analysis of our approach, we
use a version of the ACE 2005 corpus which con-
tains Wikipedia link annotations (Bentivogli et al.,
2010). All ACE mentions, both common and
proper nouns, are annotated with one or more links
to the English Wikipedia or as NILs. If a men-
tion is annotated with more than one link, we con-
sider it as correctly disambiguated if one of the an-
notated concepts has been chosen by our system.
ACE 2005 consists of 597 texts from newswire re-
ports, broadcast news, internet sources and tran-
scribed audio data and contains more annotations
than the other data sets we use for comparison.
While ACE 2005 and ACE 2004 (Ratinov et
al., 2011) fit our target scenario most (both com-
mon and proper nouns are annotated), MSNBC
(Cucerzan, 2007) and TAC 2011 (Ji et al., 2011)
are only annotated for proper nouns.
495
4.2 Preprocessing
The training, development and testing data are all
preprocessed in the same way. We perform POS
tagging, syntactic parsing and named entity recog-
nition using the Stanford CoreNLP pipeline
5
. For
identifying mentions we extract all noun phrases
(excluding discontinuous phrases and determin-
ers) and look them up in our lexicon. Our lex-
icon and also all other information we obtained
from Wikipedia are extracted from the same En-
glish Wikipedia dump.
6
The lexicon consists of
anchor texts, article titles and redirects.
4.3 Settings
Upper bound: The upper bound shows the maxi-
mum performance we can reach given our lexicon
and preprocessing. If the correct concept is among
the candidate concepts of a mention, it is consid-
ered as correct.
First Concept: The first concept baseline is a
strong baseline in disambiguation. It chooses for
each mention its most frequent concept.
Scope-ignorant (Disambig.): Our previous
MLN-based approach for concept disambiguation
(Fahrni and Strube, 2012).
Scope-ignorant (Disambig. & Clust.): Our pre-
vious MLN-based approach for joint disambigua-
tion and clustering of concepts (Fahrni and Strube,
2012).
Pipeline-based Scope-aware (Disambig.): We
compare our joint approach to a pipeline-based
one in which the assignment of the cohesive scope
is done before disambiguation. The features for
the scope assignment and the disambiguation task
are exactly the same as in the joint setting and
implemented in Markov Logic. The weights for
the scope assignment and disambiguation task are
learned in a cascaded way. In contrast to the
joint approach, the hasScope(m, s) predicate is ob-
served during disambiguation.
Joint Scope-aware (Disambig.): This is our ap-
proach as described in Section 3 for concept dis-
ambiguation. As only local optimization is possi-
ble, initialization is crucial. We use the same ini-
tialization strategy as for the cascaded approach.
Joint Scope-aware (Disambig. & Clust.): This is
our approach as described in Section 3 for disam-
biguation and clustering of concepts.
5
http://nlp.stanford.edu/software/
corenlp.shtml
6
We use the English Wikipedia dump from Jan. 4, 2012.
4.4 Analysis of Scope-awareness on
ACE 2005
In Table 4 we report precision (P), recall (R) and
F-measure (F) for non-NILs and NILs for the ACE
2005 data. We also report overall accuracy (Acc)
(aka micro-average) and calculate significance us-
ing a paired t-test.
Differences in the results can be exclusively
traced back to differences in the modeling (scope-
ignorant vs. scope-aware) and learning (pipeline-
based vs. joint). Learning scope-specific models
(pipeline-based or joint) significantly improves the
result with p < 0.01 while using the same features
for disambiguation. Scope-aware joint approaches
significantly outperform the other corresponding
approaches (pipeline-based and scope-ignorant)
that use the same features for disambiguation (and
clustering) with p < 0.01. While the pipeline-
based approach suffers from error propagation,
the joint approach also benefits from the learn-
ing strategy: learning weights for scope distinction
can be guided by the training data available for
the disambiguation task. Joint disambiguation and
clustering of mentions improves the disambigua-
tion results for both the scope-ignorant (Fahrni and
Strube, 2012) and the scope-aware approach.
As Table 4 indicates, the gain of the joint scope-
aware approach with respect to non-NILs is sub-
stantial in both precision and recall. For NILs
the recall improves while the precision decreases.
This leads to a slightly worse F-Measure for the
NILs. As NILs are much rarer than non-NILs in
the corpus, the overall accurracy for which we op-
timize is significantly higher for the scope-aware
approaches.
As no gold annotations for cohesive scopes are
available, we present statistics on the distribution
of induced scopes. Table 5 shows the distribu-
tion of the mentions across induced scopes. Men-
tions with local scope are more frequent than men-
tions with intermediate scope followed by men-
tions with global scope. Table 5 compares the
overall accurracy of the scope-ignorant joint dis-
ambiguation and clustering approach (Fahrni and
Strube, 2012) with the accurracy of the corre-
sponding joint scope-aware approach. The joint
scope-aware approach improves the disambigua-
tion results for mentions of all three scopes. The
biggest gain (2.79) is achieved for mentions with
induced global scope. The gain for mentions with
local and intermediate scope is 1.27 and 0.3 re-
496
Non-NILs NILs
P R F P R F Acc
Upper bound 94.8 91.8 93.3 71.3 100.0 83.3 92.4
First Concept 68.6 70.0 69.3 55.3 40.3 46.6 67.9
Scope-ignorant (Disambig.) (Fahrni & Strube 2012) 77.3 76.0 76.6 44.7 54.2 49.0 74.4
Scope-ignorant (Disambig. & Clust.) (Fahrni & Strube 2012) 76.8 76.9 76.9 50.2 50.0 50.1 74.9
Pipeline-based Scope-aware (Disambig.) 80.1 75.8 77.9 37.3 63.4 47.0 74.9
Joint Scope-aware (Disambig.) 80.1 76.6 78.3 39.2 61.5 47.9 75.5
Joint Scope-aware (Disambig. & Clust.) 80.3 77.1 78.6 40.8 62.1 49.3 76.0
Table 4: Evaluation on ACE 2005 data
Scope-ignorant Approach
(Disambig. & Clust.)
(Fahrni & Strube 2012) (Acc)
Joint Scope-aware Approach
(Disambig. & Clust.) (Acc)
Scope Distribution (%)
Global Scope 73.20 75.99 8.54
Intermediate Scope 76.34 76.64 31.05
Local Scope 75.57 76.84 60.40
Total 75.61 76.71 100.00
Table 5: Evaluation on ACE 2005 data across induced scopes. The accurracy of the two compared
systems is slightly higher than in Table 4 as we consider here only mentions that have been recognized by
our mention identification strategy. In the evaluation in Table 4 mentions that have not been recognized
are considered as wrong.
spectively. A comparison of the learned weights
for the different scope-specific models shows that
for mentions with local scope the local context has
relatively more weight than for mentions with in-
termediate scope. For mentions with global scope,
it is striking that candidiate concepts that are not
related to the global context are relatively higher
punished than in the other two models.
To obtain some insights on the behaviour of the
joint scope-aware approach, we investigate some
examples. In a text on the 2004 US elections, the
mention Kerry in ?Kerry was the clear winner, but
victory was snatched from him? is wrongly disam-
biguated to KERRY GAA, a branch of the Gaelic
football association, by the scope-ignorant ap-
proach, because the local context strongly prefers
an interpretation in the domain of sports. In
the joint scope-aware approach, Kerry is assigned
global scope, and it is correctly disambiguated
to JOHN KERRY, an American politician, as the
global relatedness overrules the local context in
this model. In another text on U.S. troops in
Iraq, the scope-ignorant approach disambiguates
south in ?Monday?s advances came one day af-
ter British forces in the south made their deepest
push into Iraq?s second largest city? to SOUTHERN
UNITED STATES as concepts related to the USA
are quite prominent in the text. In the scope-aware
approach south is considered as being of local
scope and is correctly disambiguated as SOUTH.
In ?we happen to be at a very nice spot by the
beach where this is a chance for people to get
away from cnn coverage? spot is disambiguated
as SPOT (SATELLITE) in the scope-ignorant ap-
proach (misled by CNN), while it has been cor-
rectly recognized as NIL by the scope-aware ap-
proach in which it is considered as being of inter-
mediate scope. The remaining disambiguation er-
rors can be traced back to (1) scope assignment er-
rors and (2) disambiguation errors (e.g. Palmisano
(global scope) is disambiguated as SAMUEL J.
PALMISANO, but the text refers to a different un-
known Palmisano).
4.5 Comparison to State-of-the-art
Approaches
Compared to the state-of-the-art for concept and
entity disambiguation our approach performs fa-
vorably (Table 6). On ACE 2004 (Ratinov et
al., 2011) ? which contains annotations for com-
mon and proper nouns and fits our target scenario
most ? our scope-aware approach outperforms re-
cent state-of-the-art approaches for concept and
entity disambiguation, i.e. Ratinov et al. (2011)
and Cheng and Roth (2013). We also ran Rati-
nov et al.?s (2011) sytem on ACE 2005, but it
seems that its mention recognition is not designed
for ACE 2005.
We also evaluate our system on the task of en-
tity linking, i.e. the disambiguation of (selected)
proper nouns (MSNBC and TAC 2011). Our
system fails to beat the best systems, but still
497
System ACE 2004 MSNBC TAC 2011
BOC BOC Acc B
3
P B
3
R B
3
F1
Ratinov et al. 2011; Cogcomp 77.3 74.9 78.7 75.7 76.5 76.1
Cheng & Roth 2013 85.3 81.2 86.1 82.9 84.5 83.7
Monahan et al. 2011 (Best System at TAC 2011) 86.1 84.4 84.7 84.6
Scope-ignorant (Disambig. & Clust.) (Fahrni & Strube 2012) 83.4 76.5 84.8 82.5 83.0 82.8
Joint Scope-aware (Disambig. & Clust.) 86.3 79.0 85.5 83.6 82.7 83.1
Table 6: Evaluation on various data sets using the respective standard evaluation metrics. BOC stands
for Bag-of-Concepts. We use the code of Ratinov et al. (2011) to evaluate on ACE 2004 and MSNBC.
For TAC 2011, we use the offical evaluation script and report the micro-average (Acc) and B
3
scores.
Note that for TAC we use three additional disambiguation features ? they measure the similarity of the
article name to the context ? both in the scope-ignorant and the scope-aware approach.
achieves competitive performance without train-
ing on TAC data. On all data sets, the joint
scope-aware approach consistently outperforms
the scope-ignorant approach ceteris paribus.
5 Related Work
Joint approaches have been successful in the past
in NLP (e.g. Meza-Ruiz and Riedel (2009)). The
idea of augmenting a model with additional latent
variables to increase its expressiveness is known as
hidden or latent variable learning (Smith, 2011)
and is a promising research direction with success-
ful applications in e.g. syntactic parsing (Petrov
et al., 2006), statistical machine translation (Blun-
som et al., 2008) and sentiment analysis (Yesse-
nalina et al., 2010; Trivedi and Eisenstein, 2013).
For latent variable learning generative approaches
(Petrov et al., 2006), large margin methods (Smith,
2011) and conditional log-linear models have been
proposed. We focus here on conditional log-linear
models due to their flexibility and their previous
success for many tasks. Blunsom et al. (2008)
for instance use latent variables in the context of
discriminative machine translation and model the
derivation as a latent variable. Chang et al. (2010)
is close to our approach, as their latent variable ap-
proach also uses ILP. Poon and Domingos (2008)
also use latent variables with Markov Logic, al-
though with a completely different aim, i.e. for un-
supervised coreference resolution.
Most approaches that use Wikipedia as a re-
source for disambiguation focus on named enti-
ties (Bunescu and Pas?ca, 2006; Cucerzan, 2007;
Dredze et al., 2010; Ji and Grishman, 2011;
Hachey et al., 2013; Hoffart et al., 2011), while
only a few disambiguate common and proper
nouns like us (Csomai and Mihalcea, 2008; Milne
and Witten, 2008; Zhou et al., 2010; Ratinov et al.,
2011; Cheng and Roth, 2013). We build upon our
previous Markov Logic based approach for joint
concept disambiguation and clustering (Fahrni and
Strube, 2012). In contrast to us, most approaches
for lexical disambiguation use either one model
for all mentions (Milne and Witten, 2008; Rati-
nov et al., 2011) or a separate model for each men-
tion or concept which requires a lot of training data
(e.g. Bryl et al. (2010)). Only a few approaches try
to learn specific models for groups of mentions,
although none of them is discourse-motivated as
ours: Mihalcea and Csomai (2005) learn a specific
model for each POS, Ando (2006) uses alternating
structure optimization to simultantanously learn a
number of WSD problems and Dhillon and Ungar
(2009) improve feature selection for WSD by in-
tegrating knowledge from similar words.
6 Conclusions
In this paper, we discuss the relationship between
cohesion and concept disambiguation and pro-
pose a cohesive scope-aware disambiguation ap-
proach. We distinguish between three different co-
hesive scopes (local, intermediate and global) and
model the scope assignment and the disambigua-
tion jointly using latent variables in the framework
of MLN. The joint scope-aware approach signifi-
cantly improves over both a state-of-the-art and a
pipeline-based approach using the same features
for the disambiguation task.
For future work, we are planning to investigate
the relation between discourse structure and co-
hesive scope more deeply and to integrate scope-
specific disambiguation features.
Acknowledgments
We would like to thank Sebastian Martschat for his
valuable comments. This work has been partially
funded by the Klaus Tschira Foundation.
498
References
Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the 10th Conference on Computa-
tional Natural Language Learning, New York, N.Y.,
USA, 8?9 June 2006, pages 77?84.
Nicholas Asher and Alex Lascarides. 1995. Lexical
disambiguation in a discourse context. Journal of
Semantics, 12(1):69?108.
Luisa Bentivogli, Pamela Forner, Claudio Giu-
liano, Alessandro Marchetti, Emanuele Pianta, and
Kateryna Tymoshenko. 2010. Extending English
ACE 2005 corpus annotation with ground-truth links
to Wikipedia. In Proceedings of the 2nd Work-
shop on The People?s Web: Colloboratively Con-
structed Semantic Resources, Beijing, China, 28 Au-
gust 2010, pages 19?27.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
Columbus, Ohio, 15?20 June 2008, pages 200?208.
Volha Bryl, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Supporting natu-
ral language processing with background knowl-
edge: Coreference resolution case. In Proceedings
of the 9th International Semantic Web Conference,
Revised Selected Papers, Part I, Shanghai, China, 7-
11 November 2010, pages 80?95.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of the 11th Conference of the
European Chapter of the Association for Compu-
tational Linguistics, Trento, Italy, 3?7 April 2006,
pages 9?16.
Ming-Wei Chang, Vivek Srikumar, Dan Goldwasser,
and Dan Roth. 2010. Structured output learning
with indirect supervision. In Proceedings of the
27th International Conference on Machine Learn-
ing, Haifa, Israel, 21?24 June 2010, pages 199?206.
Xiao Cheng and Dan Roth. 2013. Relational infer-
ence for Wikification. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, Seattle, Wash., 18?21 October
2013, pages 1787?1796.
Andras Csomai and Rada Mihalcea. 2008. Linking
documents to encyclopedic knowledge. IEEE Intel-
ligent Systems, 23(5):34?41.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Language Learning, Prague, Czech Re-
public, 28?30 June 2007, pages 708?716.
Franti
?
sek Dane
?
s. 1974. Functional sentence perspec-
tive and the organization of the text. In F. Dane
?
s,
editor, Papers on Functional Sentence Perspective,
pages 106?128. Prague: Academia.
Paramveer S. Dhillon and Lyle H. Ungar. 2009. Trans-
fer learning, feature selection and word sense dis-
ambguation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, Singapore, 2?7 Au-
gust 2009, pages 257?260.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Morgan Claypool Publishers.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambigua-
tion for knowledge base population. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, Beijing, China, 23?27 Au-
gust 2010, pages 277?285.
Angela Fahrni and Michael Strube. 2012. Jointly
disambiguating and clustering concepts and entities
with Markov logic. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics,
Mumbai, India, 8?15 December 2012, pages 815?
832.
Angela Fahrni, Benjamin Heinzerling, Thierry G?ockel,
and Michael Strube. 2014. HITS? monolin-
gual and cross-lingual entity linking system at TAC
2013. In Proceedings of the Text Analysis Confer-
ence, National Institute of Standards and Technol-
ogy, Gaithersburg, Maryland, USA, 18?19 Novem-
ber 2013.
Nizar Habash and Bonnie Dorr. 2003. A catego-
rial variation database for English. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Edmonton, Al-
berta, Canada, 27 May ?1 June 2003, pages 17?23.
Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2013. Evaluat-
ing entity linking with Wikipedia. Artificial Intel-
ligence, 194:130?150.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of Human Language Technolo-
gies 2009: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Boulder, Col., 31 May ? 5 June 2009, pages
362?370.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohe-
sion in English. London, U.K.: Longman.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F?urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
499
Processing, Edinburgh, Scotland, U.K., 27?29 July
2011, pages 782?792.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, Portland,
Oreg., 19?24 June 2011, pages 1148?1158.
Heng Ji, Ralph Grishman, and Hoa Dang. 2011.
Overview of the TAC 2011 knowledge base popula-
tion track. In Proceedings of the Text Analysis Con-
ference, National Institute of Standards and Technol-
ogy, Gaithersburg, Maryland, USA, 14?15 Novem-
ber 2011.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective anno-
tation of Wikipedia entities in web text. In Pro-
ceedings of the 15th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining, Paris,
France, 28 June ? 1 July 2009, pages 457?466.
Daniel Lowd and Pedro Domingos. 2007. Efficient
weight learning for Markov logic networks. In
Proceedings of the 11th European Conference on
Principles and Practices of Knowledge Discovery
in Databases, Warsaw, Poland, 17?21 September
2007, pages 200?211.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
Markov logic. In Proceedings of Human Language
Technologies 2009: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Boulder, Col., 31 May ? 5 June
2009, pages 155?163.
Rada Mihalcea and Andras Csomai. 2005. Sense-
Learner: Word sense disambiguation for all words in
unrestricted text. In Proceedings of the Interactive
Poster and Demonstrations Sessions at the 43rd An-
nual Meeting of the Association for Computational
Linguistics, Ann Arbor, Mich., 25?30 June 2005,
pages 53?56.
Rada Mihalcea. 2006. Knowledge-based methods
for WSD. In E. Agirre and P.G. Edmonds, editors,
Word Sense Disambiguation: Algorithms and Appli-
cations, pages 107?131. Springer, Heidelberg, Ger-
many.
David Milne and Ian H. Witten. 2008. Learning to link
with Wikipedia. In Proceedings of the ACM 17th
Conference on Information and Knowledge Man-
agement (CIKM 2008), Napa Valley, Cal., USA, 26?
30 October 2008, pages 1046?1055.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21?48.
Rani Nelken and Stuart Shieber. 2006. Lexical chain-
ing and word-sense-disambiguation. Technical Re-
port TR-06-07, Computer Science Group, Harvard
University, Cambridge, Mass.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. LDC2011T07.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, Sydney, Aus-
tralia, 17?21 July 2006, pages 433?440.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25?27 October 2008, pages 650?
659.
Dragomir R. Radev, Simone Teufel, Horacio Saggion,
Wai Lam, John Blitzer, Hong Qi, Arda Celebi,
Danyu Liu, and Elliott Drabek. 2003. Evaluation
challenges in large-scale document summarization.
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, Sapporo,
Japan, 7?12 July 2003, pages 375?382.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to Wikipedia. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics, Portland, Oreg., 19?24 June
2011, pages 1375?1384.
Sebastian Riedel. 2008. Improving the accuracy and
efficiency of MAP inference for Markov logic. In
Proceedings of the 24th Conference on Uncertainty
in Artificial Intelligence, Helsinki, Finland, 9?12
July 2008, pages 468?475.
Noah A. Smith. 2011. Linguistic Structure Prediction.
Morgan & Claypool Publishers.
Rakshit Trivedi and Jacob Eisenstein. 2013. Dis-
course connectors for latent subjectivity in sentiment
analysis. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, 9?14 June 2013, pages
808?813.
Ainur Yessenalina, Yejin Choi, and Claire Cardie.
2010. Automatically generating annotator rationales
to improve sentiment classification. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, Uppsala, Sweden, 11?
16 July 2010, pages 336?341.
Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian
Vasile, and Scott Gaffney. 2010. Resolving sur-
face forms to Wikipedia topics. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics, Beijing, China, 23?27 August
2010, pages 1335?1343.
500
Proceedings of NAACL-HLT 2013, pages 907?917,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Global Inference for Bridging Anaphora Resolution
Yufang Hou1, Katja Markert2, Michael Strube1
1 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
2School of Computing, University of Leeds, UK
scskm@leeds.ac.uk
Abstract
We present the first work on antecedent se-
lection for bridging resolution without restric-
tions on anaphor or relation types. Our model
integrates global constraints on top of a rich
local feature set in the framework of Markov
logic networks. The global model improves
over the local one and both strongly outper-
form a reimplementation of prior work.
1 Introduction
Identity coreference is a relatively well understood
and well-studied instance of entity coherence. How-
ever, entity coherence can rely on more complex,
lexico-semantic, frame or encyclopedic relations
than identity. Anaphora linking distinct entities or
events this way are called bridging or associative
anaphora and have been widely discussed in the lin-
guistic literature (Clark, 1975; Prince, 1981; Gundel
et al, 1993).1 In Example 1, the phrases the win-
dows, the carpets and walls can be felicitously used
because they are semantically related via a part-of
relation to their antecedent the Polish center.2
(1) . . . as much as possible of the Polish center will
be made from aluminum, steel and glass recycled
from Warsaw?s abundant rubble. . . . The windows
will open. The carpets won?t be glued down and
walls will be coated with non-toxic finishes.
1Poesio and Vieira (1998) include cases where antecedent
and anaphor are coreferent but do not share the same head noun.
We restrict bridging to non-coreferential cases. We also exclude
comparative anaphora (Modjeska et al, 2003)
2Examples are from OntoNotes (Weischedel et al, 2011).
Bridging anaphora are typed in boldface; antecedents in italics.
Bridging is frequent amounting to between 5%
(Gardent and Manue?lian, 2005) and 20% (Caselli
and Prodanof, 2006) of definite descriptions (both
studies limited to NPs starting with the or non-
English equivalents). Bridging resolution is needed
to fill gaps in entity grids based on coreference only
(Barzilay and Lapata, 2008). Example 1 does not ex-
hibit any coreferential entity coherence. Coherence
can only be established when the bridging anaphora
are resolved. Bridging resolution may also be im-
portant for textual entailment (Mirkin et al, 2010).
Bridging resolution can be divided into two tasks,
recognizing that a bridging anaphor is present and
finding the correct antecedent among a list of candi-
dates. These two tasks have frequently been handled
in a pipeline with most research concentrating on an-
tecedent selection only. We also handle only the task
of antecedent selection.
Previous work on antecedent selection for bridg-
ing anaphora is restricted. It makes strong untested
assumptions about bridging anaphora types or rela-
tions, limiting it to definite NPs (Poesio and Vieira,
1998; Poesio et al, 2004; Lassalle and Denis, 2011)
or to part-of relations between anaphor and an-
tecedent (Poesio et al, 2004; Markert et al, 2003;
Lassalle and Denis, 2011). We break new ground
by considering all relations and anaphora/antecedent
types and show that the variety of bridging anaphora
is much higher than reported previously.
Following work on coreference resolution, we ap-
ply a local pairwise model (Soon et al, 2001) for an-
tecedent selection. We then develop novel semantic,
syntactic and salience features for this task, show-
ing strong improvements over one of the best known
907
prior models (Poesio et al, 2004).
However, this local model classifies each
anaphor-antecedent candidate pair in isolation.
Thus, it neglects that bridging anaphora referring to
a single antecedent often occur in clusters (see Ex-
ample 1). It also neglects that once an entity is an
antecedent for a bridging anaphor it is more likely to
be used again as antecedent. In addition, such local
models construct the list of possible antecedent can-
didates normally relying on a window size constraint
to restrict the set of candidates: is the window too
small, we miss too many correct antecedents; is it
too large, we include so many incorrect antecedents
as to lead to severe data imbalance in learning.
To remedy these flaws we change to a global
Markov logic model that allows us to:
? model constraints that certain anaphora are
likely to share the same antecedent;
? model the global semantic connectivity of a
salient potential antecedent to all anaphora in a
text;
? consider the union of potential antecedents for
all anaphora instead of a static window-sized
constraint.
We show that this global model with the same lo-
cal features but enhanced with global constraints im-
proves significantly over the local model.
2 Related Work
Prior corpus-linguistic studies on bridging are be-
set by three main problems. First, reliability is not
measured or low (Fraurud, 1990; Poesio, 2003; Gar-
dent and Manue?lian, 2005; Riester et al, 2010).3
Second, annotated corpora are small (Poesio et al,
2004; Korzen and Buch-Kromann, 2011). Third,
they are often based on strong untested assumptions
about bridging anaphora types, antecedent types or
relations, such as limiting it to definite NP anaphora
(Poesio and Vieira, 1998; Poesio et al, 2004; Gar-
dent and Manue?lian, 2005; Caselli and Prodanof,
2006; Riester et al, 2010; Lassalle and Denis,
2011), to NP antecedents (all prior work) or to part-
3Although the overall information status scheme in Riester
et al (2010) achieved high agreement, their confusion matrix
shows that the anaphoric bridging category (BRI) is frequently
confused with other categories so that the two annotators agreed
on only less than a third of bridging anaphors.
of relations between anaphor and antecedent (Mark-
ert et al, 2003; Poesio et al, 2004). In our own
work (Markert et al, 2012) we established a corpus
that circumvents these problems, i.e. human bridg-
ing recognition was reliable, it contains a medium
number of bridging cases that allows generalisable
statistics and we did not limit bridging anaphora or
antecedents according to their syntactic type or re-
lations between them. However, we only discussed
human agreement on bridging recognition in Mark-
ert et al (2012), disregarding antecedent annotation.
We also did not discuss the different types of bridg-
ing in the corpus. We will remedy this in Section 3.
Automatic work on bridging distinguishes be-
tween recognition (Vieira and Poesio, 2000; Rah-
man and Ng, 2012; Cahill and Riester, 2012; Mark-
ert et al, 2012) and antecedent selection. Work on
antecedent selection suffers from focusing on sub-
problems, e.g. only part-of bridging (Poesio et al,
2004; Markert et al, 2003) or definite NP anaphora
(Lassalle and Denis, 2011). Most relevant for us is
Lassalle and Denis (2011) who restrict anaphora to
definite descriptions but have no other restrictions
on relations or antecedent NPs (in a French corpus)
with an accuracy of 23%. Also the evaluation set-
up is sometimes not clear: The high results in Poe-
sio et al (2004) cannot be used for comparison as
they test unrealistically: they distinguish only be-
tween the correct antecedent and one or three false
candidates (baseline of 50% for the former). They
also restrict the phenomenon to part-of relations.
There is a partial overlap between bridging and
implicit noun roles (Ruppenhofer et al, 2010).
However, work on implicit noun roles is mostly
focused on few predicates (e.g. Gerber and Chai
(2012)). We consider all bridging anaphors in run-
ning text. The closest work to ours interpreting im-
plicit role filling as anaphora resolution is Silberer
and Frank (2012).
3 Corpus for Bridging: An Overview
We use the dataset we created in Markert et al
(2012) with almost 11,000 NPs annotated for infor-
mation status including 663 bridging NPs and their
antecedents in 50 texts taken from the WSJ portion
of the OntoNotes corpus (Weischedel et al, 2011).
Bridging anaphora can be any noun phrase. They
908
are not limited to definite NPs as in previous work.
In contrast to Nissim et al (2004), antecedents are
annotated and can be noun phrases, verb phrases or
even clauses. Our bridging annotation is also not
limited with regards to semantic relations between
anaphor and antecedent.
In Markert et al (2012) we achieved high agree-
ment for the overall information status annotation
scheme between three annotators (? between 75 and
80, dependent on annotator pairs) as well as for all
subcategories, including bridging (? over 60 for all
annotator pairings, over 70 for two expert annota-
tors). Here, we add the following new results:
? Agreement for selecting bridging antecedents
was around 80% for all annotator pairings.
? Surprisingly, only 255 of the 663 (38%) bridg-
ing anaphors are definite NPs, which calls into
question the strategy of prior approaches to limit
themselves to these types of bridging.
? NPs are the most frequent antecedents by far
with only 42 of 663 (6%) bridging anaphora hav-
ing a non-NP antecedent (mostly verb phrases).
? Bridging is a relatively local phenomenon with
71% of NP antecedents occurring in the same or
up to 2 sentences prior to the anaphor. However,
farther away antecedents are common when the
antecedent is the global focus of a document.
? The semantic relations between anaphor and an-
tecedent are extremely diverse with only 92 of
663 (14%) anaphors having a part-of/attribute-
of antecedent (see Example 1) and only 45 (7%)
anaphors standing in a set relationship to the an-
tecedent (see Example 2). This contrasts with
Gardent and Manue?lian?s (2005) finding that
52% of bridging cases had meronymic relations.
We find many different types of relations in our
corpus, including encyclopedic relations such as
restaurant ? the waiter as well as, frequently,
relational person nouns as bridging anaphors
such as friend, husband, president.
? There are only a few cases of bridging where
surface cues may indicate the antecedent. First,
some bridging anaphors are modified by a small
number of adjectives that have more than one
role filler, with the bridging relation often being
temporal or spatial sequence between two enti-
ties of the same semantic type as in Example 3
(see also Lassalle and Denis (2011) for a dis-
cussion of such cases). Second, some anaphors
are compounds where the nominal premodifier
matches the antecedent head as in Example 4.
(2) Still employees do occasionally try to smuggle
out a gem or two. One man wrapped several dia-
monds in the knot of his tie. Another poked a hole
in the heel of his shoe. None made it past the body
searches . . .
(3) His truck is parked across the field . . . The
farmer at the next truck shouts . . .
(4) . . . it doesn?t make the equipment needed to
produce those chips. And IBM worries that the
Japanese will take over that equipment market.
4 Models for Bridging Resolution
4.1 Pairwise mention-entity model
The pairwise model is widely used in coreference
resolution (Soon et al, 2001). We adapt it for bridg-
ing resolution4: Given an anaphor mention m and
the set of antecedent candidate entities Em which
appear before m, we create a pairwise instance
(m, e) for every e ? Em. A binary decision whether
m is bridged to e is made for each instance (m, e)
separately. A post-processing step to choose one an-
tecedent is necessary (closest first or best first are
common strategies). This model causes three prob-
lems for bridging resolution: First, the ratio between
positive and negative instances is 1 to 17 even if only
antecedent candidates from the current and the im-
mediately preceding two sentences are considered.
The ratio will be even worse with a larger win-
dow size. Therefore, usually a fixed window size is
used restricting the set of candidates. This, however,
causes a second problem: antecedents which are be-
yond the window cannot be found. In our data, only
81% of NP antecedents appear within the previous 5
sentences, and only 71% of NP antecedents appear
within the previous 2 sentences. The third problem
is a shortcoming of the pairwise model itself: deci-
sions are made for each instance separately, ignoring
4Different from coreference, we treat an anaphor as a men-
tion and an antecedent as an entity. The anaphor is the first
mention of the corresponding entity in the document.
909
relations between instances. We resolve these prob-
lems by employing a global model based on Markov
logic networks.
4.2 Markov Logic Networks
Bridging can be considered a document global phe-
nomenon, where globally salient entities are pre-
ferred as antecedents and two or more anaphors hav-
ing the same antecedent should be related or similar.
Motivated by this observation, we explore Markov
logic networks (Domingos and Lowd, 2009, MLNs)
to model bridging resolution on the global discourse
level.
MLNs are a powerful representation for joint
inference with uncertainty. An MLN consists
of a set of pairs (Fi, wi), where Fi is a formula
in first-order logic and wi is its associated real
numbered weight. It can be viewed as a template for
constructing Markov networks. Given different sets
of constants, an MLN will produce different ground
Markov networks which may vary in size but have
the same structure and parameters. For a ground
Markov network, the probability distribution over
possible worlds x is given by
P (X = x) = 1Z exp
(
?
i
wini(x)
)
(1)
where ni(x) is the number of true groundings of Fi
in x. The normalization factor Z is the partition
function.
MLNs have been applied to many NLP tasks and
achieved good performance by leveraging rich re-
lations among objects (Poon and Domingos, 2008;
Meza-Ruiz and Riedel, 2009; Fahrni and Strube,
2012, inter alia). We use thebeast5 to learn weights
for the formulas and to perform inference. thebeast
employs cutting plane inference (Riedel, 2008) to
improve the accuracy and efficiency of MAP infer-
ence for Markov logic.
With MLNs, we model bridging resolution glob-
ally on the discourse level: given the set M of all
anaphors and sets of local antecedent candidates Em
for each anaphor m ? M , we select antecedents for
all anaphors from E =?m?M Em at the same time.
Table 1 shows the hidden predicates and formulas
used. Each formula is associated with a weight. The
5http://code.google.com/p/thebeast
polarity of the weights is indicated by the leading
+ or ?. The weight value (except for hard con-
straints) is learned from training data. For some for-
mulas the final weight consists of a learned weight
w multiplied by a score d (e.g. inverse distance be-
tween antecedent and anaphor). In these cases the
final weight for a formula in a ground Markov net-
work does not just depend on the respective formula,
but also on the specific constants. We indicate such
combined weights by the term w ? d.
We tackle the previously mentioned problems of
the pairwise model: (1) We construct hard con-
straints to specify that each anaphor has at most
one antecedent entity (Table 1: f1) and that the an-
tecedent must precede the anaphor (f2). This elim-
inates the need for the post-processing step in the
pairwise model. (2) We select the antecedent en-
tity for each anaphor from the antecedent candidate
entities pool E which alleviates the missing true
antecedent problem in the pairwise model. Based
on (1) and (2), MLNs allow us to express relations
between anaphor-anaphor and anaphor-antecedent
pairs ((m,n) or (m,e)) on the global discourse level
improving accuracy by performing joint inference.
5 Features
5.1 Local features
5.1.1 Poesio et al?s feature set
Table 2 shows the feature set proposed by Poesio et
al. (2004) for part-of bridging. Google distance is
the inverse value of Google hit counts for the ofPat-
tern query (e.g. the windows of the center). Word-
Net distance is the inverse value of the shortest path
length between an anaphor and an antecedent candi-
date among all synset combinations. These features
are supposed to capture the meronymy relation be-
tween anaphor and antecedent. The other ones mea-
sure the salience of the antecedent candidate.
Group Feature Value
lexical Google distance numeric
WordNet distance numeric
salience utterance distance numeric
local first mention boolean
global first mention boolean
Table 2: Poesio et al?s feature set
910
Hidden predicates
p1 isBridging(m, e)
p2 hasSameAntecedent (m,n)
Formulas
Hard constraints
f1 ?m ? M : |e ? E : isBridging(m, e)| ? 1
f2 ?m ? M?e ? E : hasPairDistance(e,m, d) ? d < 0 ? ?isBridging(m, e)
f3 ?m,n ? M : m 6= n ? hasSameAntecedent (m,n)
? hasSameAntecedent (n,m)
f4 ?m,n, l ? M : m 6= n ?m 6= l ? n 6= l ? hasSameAntecedent (m,n)
? hasSameAntecedent (n, l) ? hasSameAntecedent (m, l)
f5 ?m,n ? M?e ? E : m 6= n ? hasSameAntecedent (m,n) ? isBridging(m, e)
? isBridging(n, e)
f6 ?m,n ? M?e ? E : m 6= n ? isBridging(m, e) ? isBridging(n, e)
? hasSameAntecedent (m,n)
Discourse level formulas
f7 + (w) ?m ? M?e ? E : predictedGlobalAnte(e) ? hasPairDistance(e,m, d)
? d > 0 ? isBridging(m, e)
f8 + (w) ?m,n ? M conjunction(m,n) ? hasSameAntecedent (m,n)
f9 + (w) ?m,n ? M sameHead(m,n) ? hasSameAntecedent (m,n)
f10 + (w) ?m,n ? M similarTo(m,n) ? hasSameAntecedent (m,n)
f11 + (w) ?m ? M?e ? E : hasSemanticClass (m, ?rolePerson?)
? hasSemanticClass(e, ?org|gpe?) ? hasPairDistance(e,m, d) ? d > 0
? isBridging(m, e)
f12 + (w ? d) ?m ? M?e ? E : hasSemanticClass (m, ?relativePerson?)
? hasSemanticClass(e, ?otherPerson?) ? hasPairDistanceInverse(e,m, d)
? isBridging(m, e)
f13 + (w ? d) ?m ? M?e ? E : hasSemanticClass (m, ?date?)
? hasSemanticClass(e, ?date?) ? hasPairDistanceInverse(e,m, d)
? isBridging(m, e)
Local formulas
f14 + (w) ?m ? M ?e ? Em : isTopRelativeRankPrepPattern (m, e) ? isBridging(m, e)
f15 + (w) ?m ? M ?e ? Em : isTopRelativeRankVerbPattern(m, e) ? isBridging(m, e)
f16 + (w ? d) ?m ? M ?e ? Em : isPartOf (m, e) ? hasPairDistanceInverse(e,m, d)
? isBridging(m, e)
f17 + (w) ?m ? M ?e ? Em : isTopRelativeRankDocSpan (m, e) ? isBridging(m, e)
f18 ? (w) ?m ? M ?e ? Em : isSameHead(m, e) ? isBridging(m, e)
f19 + (w) ?m ? M ?e ? Em : isPremodOverlap(m, e) ? isBridging(m, e)
f20 ? (w) ?m ? M ?e ? Em : isCoArgument(m, e) ? isBridging(m, e)
Table 1: Hidden predicates and formulas used for bridging resolution (m,n, l represent mentions, M the set of bridging
anaphora mentions in the whole document, e the antecedent candidate entity, Em the set of local antecedent candidate
entities for m, and E =
?
m?M Em )
911
5.1.2 Other features
Since Poesio et al (2004) deal exclusively with
meronymy bridging, we have to extend the fea-
ture set to capture more diverse relations between
anaphor and antecedent. All numeric features in Ta-
ble 3 are normalized among all antecedent candi-
dates of one anaphor. For anaphor mi and its an-
tecedent candidates Emi (eij ? Emi), the numeric
score for pair {mi, eik} is Sik. Then the value
NormSik for this pair is normalized (set to values
between 0 and 1) as below:
NormSik =
Sik ?minj Sij
maxj Sij ?minj Sij
(2)
A second variant of numeric features tells whether
the score of an anaphor-antecedent candidate pair is
the highest among all pairs for this anaphor.
Group Feature Value
semantic feat1 preposition pattern numeric
feat2 verb pattern numeric
feat3 WordNet partOf boolean
feat4 semantic class nominal
salience feat5 document span numeric
surface feat6 isSameHead boolean
feat7 isPremodOverlap boolean
syntactic feat8 isCoArgument boolean
Table 3: Local features we developed
Preposition pattern (feat1). The ofPattern pro-
posed by Poesio et al (2004) is useful for part-of
and attribute-of relations but cannot cover all bridg-
ing relations (such as sanctions against a country).
We extend the ofPattern to a generalised preposition
pattern by using the Gigaword (Parker et al, 2011)
and the Tipster (Harman and Liberman, 1993) cor-
pora (both automatically POS tagged and NP chun-
ked for improving query match precision).
First, we extract the three most highly associ-
ated prepositions for each anaphor. Then for each
anaphor-antecedent candidate pair, we use their head
words to create the query ?anaphor preposition an-
tecedent?. To improve recall, we take lowercase,
uppercase, singular and plural forms of the head
word into account, and replace proper names by
fine-grained named entity types (using a gazetteer).
All raw hit counts are converted into the Dunning
Root Loglikelihood association measure,6 then nor-
malized using Formula 2 within all antecedent can-
didates of one anaphor.
Verb pattern (feat2). A set-membership rela-
tion between anaphor and antecedent is often hard
to capture by the preposition pattern because the
anaphor often has no common noun head (see Ex-
ample 2 in Section 3). Hence, we measure the com-
patibility of the antecedent candidates with the verb
the anaphor depends on.
First, we hypothesise that anaphors whose lexi-
cal head is a pronoun or a number are potential set
bridging cases and then extract the verb the anaphor
depends on. In example 2, for the set anaphor An-
other, poked is the verb. Then for each antecedent
candidate, subject-verb or verb-object queries are
applied to the Web 1T 5-gram corpus (Brants and
Franz, 2006). In this case, employees poked and di-
amonds poked are example queries. The hit counts
are transformed into PMI and all pairs for one
anaphor are normalized as described in Formula 2.
WordNet partOf relation (feat3). To capture
part-of bridging, we extract whether the anaphor is
part of the antecedent candidate in WordNet. To im-
prove recall, we use hyponym information of the
antecedent. If an antecedent e is a hypernym of x
and an anaphor m is a meronym of x, then m is a
meronym of e.
Semantic class (feat4). The anaphor and the an-
tecedent candidate are assigned one of 16 coarse-
grained semantic classes, e.g. location, organiza-
tion, GPE, roleperson, relativePerson, otherPerson7,
product, language, NORP (nationalities, religious
or political groups) and several classes for numbers
(such as date, money or percent).
Salience feature (feat5). Salient entities are pre-
ferred as antecedents. We capture salience super-
ficially by computing the ?antecedent document
span? of an antecedent candidate. We compute the
6http://tdunning.blogspot.de/2008/03/
surprise-and-coincidence.html
7We use WordNet to extract lists for rolePerson (persons like
president or teacher playing a role in an organization) and rela-
tivePerson (persons like father or son indicating that they have
a relation with another person). Persons not in these two lists
are counted as otherPerson.
912
span of text (measured in sentences) in which the
antecedent candidate entity is mentioned. This is di-
vided by the number of sentences in the whole doc-
ument. This score is normalized using Formula 2 for
all antecedent candidates of one anaphor.
Surface features (feat6-feat7). isSameHead
(feat6) checks whether antecedent candidates have
the same head as the anaphor: this is rarely the
case in bridging anaphora (except in some cases
of set bridging and spatial/temporal sequence, see
Example 3) and can therefore be used to exclude
antecedent candidates. isPremodOverlap (feat7)
determines the antecedent for compound noun
anaphors whose head is prenominally modified by
the antecedent head (see Example 4).
Syntactic feature (feat8) The isCoArgument fea-
ture is based on the intuition that the subject can-
not be the bridging antecedent of the object in
the same clause. This feature excludes (some)
close antecedent candidates. In Example 4, the an-
tecedent candidate the Japanese isCoArgument with
the anaphor that equipment market.
5.2 Global features for MLNs
f1-f13 in Table 1 are discourse level constraints.
All antecedent candidates come from the antecedent
candidates pool E in the whole document.
Global salience (Table 1: f3-f10). The salience
feature in the pairwise model only measures the
salience for candidates within the local window.
However, globally salient antecedents are preferred
even if they are far away from the anaphor. We
model this from two perspectives:
f7 models the preference for globally salient an-
tecedents, which we derive for each document. For
m ? M and e ? E, let score(m, e) be the prepo-
sition pattern score for pair (m,e). Calculate pattern
semantic salience score esal for each e ? E as
esal =
?
m?M
score(m, e) (3)
If e appears in the title and also has the highest
pattern semantic salience score esal among all e in
E, then e is the predicted globally salient antecedent
for this document. Note that global salience here is
based on semantic connectivity to all anaphors in the
document and that not every document has a glob-
ally salient antecedent.
f3-f6 and f8-f10 model that similar or related
anaphors in one document are likely to have the
same antecedent. To make the ground Markov net-
work more sparse for more efficient inference, we
add the hidden predicate (p2) and hard constraints
(f3-f6) specifying relations among similar/related
anaphors m, n and l (reflexivity and transitivity).
Formulas f8-f10 explore three different ways (syn-
tactic and semantic) to compute the similarity be-
tween two anaphors. In f10, we use SVMlight (simi-
larity scores from WordNet plus sentence distance as
features) to predict whether two anaphors not shar-
ing the same head are similar or not.
Frequent bridging relations (Table 1: f11-f13).
Three common bridging relations are restricted by
semantic class of anaphor and antecedent (see also
Section 3). It is worth noting that in formula f11
(modeling that a role person mention like presi-
dent or chairman prefers organization or GPE an-
tecedents), we do not penalize the antecedents far
away from the anaphor. In formula f12 (modeling
that a relativePerson mention such as mother or hus-
band prefers close person antecedents) and f13, we
prefer close antecedents by including the distance
between antecedent and anaphor into the weights.
MLN formulation of local features (Table 1: f14-
f20). Corresponding to features of the pairwise
model (Table 3) ? we exclude only semantic class
as this is modelled globally via features f11-f13.
These local features are only used for an anaphor m
and its local antecedent candidate e from Em.
6 Experiments and Results
6.1 Experimental setup
We perform experiments on our gold standard cor-
pus via 10-fold cross-validation on documents. We
use gold standard mentions, true coreference infor-
mation, and the OntoNotes named entity and syntac-
tic annotation layers for feature extraction.
6.2 Improved baseline
We reimplement the algorithm from Poesio et al
(2004) as baseline. Since they did not explain
913
whether they used the mention-mention or mention-
entity model, we assume they treated antecedents as
entities and use a 2 and 5 sentence window for can-
didates8. Since the GoogleAPI is not available any
more, we use the Web 1T 5-gram corpus (Brants and
Franz, 2006) to extract the Google distance feature.
We improve it by taking all information about en-
tities via coreference into account as well as by re-
placing proper names. All other features (Table 2
in Section 5.1.1) are extracted as Poesio et al did.
A Naive Bayes classifier with standard settings in
WEKA (Witten and Frank, 2005) is used. In order
to evaluate their model in the more realistic setting
of our experiment, we apply the best first strategy to
select the antecedent for each anaphor.
6.3 Pairwise models
Pairwise model I: We use the preposition pattern
feature (feat1) plus Poesio et al?s salience features
(Table 2). We use a 2 sentence window as it per-
formed on a par with the 5 sentence window in the
baseline. We replace Naive Bayes with SVMlight
because it can deal better with imbalanced data9.
Pairwise model II: Based on Pairwise model I.
Local features feat2-feat8 from Table 2 are added.
Pairwise model III: Based on Pairwise model II.
We apply a more advanced antecedent candidate se-
lection strategy, which allows to include 77% of NP
antecedents compared to 71% in Pairwise model II.
For each anaphor, we add the top k salient enti-
ties measured through the length of the coreference
chains (k is set to 10%) as additional antecedent can-
didates. For potential set anaphors (as automatically
determined by pronoun or number heads), singu-
lar antecedent candidates are filtered out. We com-
piled a small set of adjectives (using FrameNet and
thesauri) that indicate spatial or temporal sequences
(see Example 3). For anaphors modified by such ad-
jectives we consider only antecedent candidates that
have the same semantic class as the anaphor.
8They use a 5 sentence window, because all antecedents in
their corpus are within the previous 5 sentences.
9The SVMlight parameter is set according to the ratio be-
tween positive and negative instances in the training set.
6.4 MLN models
MLN model I: MLN system using local formu-
las f1-f2 and f14-f20. The same strategy as in
Pairwise model III is used to select local antecedent
candidates Em for each anaphor m.
MLN model II: Based on MLN model I, all for-
mulas in Table 1 are used.
6.5 Results
Table 4 shows the comparison of our models to base-
lines. Significance tests are conducted using McNe-
mar?s test on overall accuracy at the level of 1%.
acc
improved baseline 2 sent. + NB 18.85
5 sent. + NB 18.40
pairwise model pairwise model I 29.11
pairwise model II 33.94
pairwise model III 36.35
MLN model MLN model I 35.60
MLN model II 41.32
Table 4: Results for MLN models compared to pairwise
models and baselines.
MLN model II, which is inspired by the linguis-
tic observation that globally salient entities are pre-
ferred as antecedents, performs significantly better
than all other systems. The gains come from three
aspects. First, by selecting the antecedent for each
anaphor from the antecedent candidate pool E in the
whole document 91% of NP antecedents are acces-
sible compared to 77% in pairwise model III. Sec-
ond, we leverage semantics and salience by using
local formulas and discourse level formulas. Lo-
cal formulas are used to capture semantic relations
for bridging pairs as well as surface and syntactic
constraints. Global formulas resolve several bridg-
ing anaphors together, often to a globally salient an-
tecedent beyond the local window. Third, the model
allows us to express specific relations among bridg-
ing anaphors and their antecedents (f11-f13).
However, our pairwise model I already outper-
forms improved baselines by about 10%, which sug-
gests that our preposition pattern feature can capture
more diverse semantic relations. The continuous im-
provements shown in pairwise model II and pair-
wise model III verify the contribution of our other
914
features and advanced antecedent candidate selec-
tion strategy. pairwise model III would become too
complex if we tried to integrate discourse level for-
mulas f7, f11-f13 into antecedent candidate selec-
tion. MLN model II solves this task elegantly.
6.6 Discussion and error analysis
We analyse our best model (MLN model II) and
compare it to the best local one (pairwise model III).
Anaphors with long distance antecedents are
harder to resolve. Table 5 shows the compari-
son of correctly resolved anaphors with regard to
anaphor-antecedent distance. We can see that the
global model is equal or better to the local model
for all anaphor types but that the difference is espe-
cially large for anaphora with antecedents that are
3 or more sentences away due to the use of global
salience and accessibility of possible antecedents
beyond a fixed window-size.
# pairs MLN II pairwise III
sent. distance
0 175 48.57 45.14
1 260 34.62 35
2 90 47.78 43.33
?3 158 35.44 16.46
Table 5: Comparison of the percentage of correctly re-
solved anaphors with regard to anaphor-antecedent dis-
tance. Significance tests are conducted using McNemar?s
test at the level of 1%.
We now distinguish between ?sibling anaphors?
(anaphors that share an antecedent with other bridg-
ing anaphors) and ?non-siblings? (anaphors that do
not share an antecedent with any other anaphor).
The performance of our MLN model II is 54%
on sibling anaphors but only 24% on non-sibling
anaphors. This shows that our use of global salience
and links between related anaphors does indeed help
to capture the behaviour of sibling anaphors.
However, our global model is good at predicting
the right antecedent for sibling anaphors where the
antecedent is globally salient but not as good for sib-
ling anaphors where the (shared) antecedent is a lo-
cally salient subtopic. Thus, in the future we need
to model equivalent constraints for local salience
of antecedents, taking into account topic segmen-
tation/shifts to improve over the 54% for sibling
anaphors.
The semantic knowledge we employ is still in-
sufficient. Typical cases where we have problems
are: (i) cases with very context-specific bridging re-
lations. For example, in one text about the stealing
of Sago Palms in California we found the thieves
as a bridging anaphor with the antecedent palms,
which is not a very usual semantic link. (ii) more
frequently, we have cases where several good an-
tecedents from a semantic perspective can be found.
For example, two laws are discussed and a later
anaphor the veto could be the veto of either bills.
Integration of the wider context apart from the two
NPs is necessary in these cases. This includes the se-
mantics of modification, whereas we currently con-
sider only head noun knowledge. An example is that
the anaphor the local council would preferably be
interpreted as the council of a village instead of the
council of a state due to the occurrence of local.
Finally, 6% of the anaphors in our corpus have a
non-NP antecedent. These cases are not correctly
resolved in our current model as we only extract NP
phrases as potential candidate antecedents.
7 Conclusions
We provide the first reasonably sized and reliably
annotated English corpus for bridging resolution. It
covers a diverse set of relations between anaphor and
antecedent as well as all anaphor/antecedent types.
We developed novel semantic, syntactic and salience
features based on linguistic intuition. Inspired by
the observation that salient entities are preferred as
antecedents, we implemented a global model for an-
tecedent selection within the framework of Markov
logic networks. We show that our global model sig-
nificantly outperforms other local models and base-
lines. This work is ? to our knowledge ? the first
bridging resolution algorithm that tackles the unre-
stricted phenomenon in a real setting.
Acknowledgements. Yufang Hou is funded by a PhD
scholarship from the Research Training Group Coher-
ence in Language Processing at Heidelberg University.
Katja Markert receives a Fellowship for Experienced Re-
searchers by the Alexander-von-Humboldt Foundation.
We thank HITS gGmbH for hosting Katja Markert and
funding the annotation. We thank our colleague Angela
Fahrni for advice on using Markov logic networks.
915
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. LDC2006T13, Philadelphia, Penn.: Lin-
guistic Data Consortium.
Aoife Cahill and Arndt Riester. 2012. Automatically ac-
quiring fine-grained information status distinctions in
German. In Proceedings of the SIGdial 2012 Confer-
ence: The 13th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, Seoul, Korea, 5?6
July 2012, pages 232?236.
Tommaso Caselli and Irina Prodanof. 2006. Annotat-
ing bridging anaphors in Italian: In search of reliabil-
ity. In Proceedings of the 5th International Conference
on Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006.
Herbert H. Clark. 1975. Bridging. In Proceedings of the
Conference on Theoretical Issues in Natural Language
Processing, Cambridge, Mass., June 1975, pages 169?
174.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Morgan Claypool Publishers.
Angela Fahrni and Michael Strube. 2012. Jointly
disambiguating and clustering concepts and entities
with Markov logic. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics,
Mumbai, India, 8?15 December 2012, pages 815?832.
Kari Fraurud. 1990. Definiteness and the processing of
noun phrases in natural discourse. Journal of Seman-
tics, 7:395?433.
Claire Gardent and He?le`ne Manue?lian. 2005. Cre?ation
d?un corpus annote? pour le traitement des descrip-
tions de?finies. Traitement Automatique des Langues,
46(1):115?140.
Matthew Gerber and Joyce Chai. 2012. Semantic role
labeling of implicit arguments for nominal predicates.
Computational Linguistics, 38(4):756?798.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, 69:274?307.
Donna Harman and Mark Liberman. 1993. TIPSTER
Complete. LDC93T3A, Philadelphia, Penn.: Linguis-
tic Data Consortium.
Iorn Korzen and Matthias Buch-Kromann. 2011.
Anaphoric relations in the Copenhagen dependency
treebanks. In S. Dipper and H. Zinsmeister, edi-
tors, Corpus-based Investigations of Pragmatic and
Discourse Phenomena, volume 3 of Bochumer Lin-
guistische Arbeitsberichte, pages 83?98. University of
Bochum, Bochum, Germany.
Emmanuel Lassalle and Pascal Denis. 2011. Leverag-
ing different meronym discovery methods for bridging
resolution in French. In Proceedings of the 8th Dis-
course Anaphora and Anaphor Resolution Colloquium
(DAARC 2011), Faro, Algarve, Portugal, 6?7 October
2011, pages 35?46.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of the EACL Workshop on
the Computational Treatment of Anaphora. Budapest,
Hungary, 14 April 2003, pages 39?46.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, Jeju Is-
land, Korea, 8?14 July 2012, pages 795?804.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
Markov logic. In Proceedings of Human Language
Technologies 2009: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Boulder, Col., 31 May ? 5 June
2009, pages 155?163.
Shachar Mirkin, Ido Dagan, and Sebastian Pado?. 2010.
Assessing the role of discourse references in entail-
ment inference. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11?16 July 2010, pages 1209?
1219.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan, 11?12 July 2003,
pages 176?183.
Malvina Nissim, Shipara Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for in-
formation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation, Lisbon, Portugal, 26?28 May 2004, pages
1023?1026.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. LDC2011T07.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pages 143?150.
Massimo Poesio. 2003. Associate descriptions and
salience: A preliminary investigation. In Proceedings
916
of the EACL Workshop on the Computational Treat-
ment of Anaphora. Budapest, Hungary, 14 April 2003,
pages 31?38.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25?27 October 2008, pages 650?
659.
Ellen F. Prince. 1981. Towards a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York, N.Y.
Altaf Rahman and Vincent Ng. 2012. Learning the fine-
grained information status of discourse entities. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, Avignon, France, 23?27 April 2012, pages 798?
807.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for Markov logic. In Pro-
ceedings of the 24th Conference on Uncertainty in Ar-
tificial Intelligence, Helsinki, Finland, 9?12 July 2008,
pages 468?475.
Arndt Riester, David Lorenz, and Nina Seemann. 2010.
A recursive annotation scheme for referential informa-
tion status. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
La Valetta, Malta, 17?23 May 2010, pages 717?722.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking events and their participants
in discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations (SemEval-2), Up-
psala, Sweden, 15?16 July 2010, pages 45?50.
Carina Silberer and Anette Frank. 2012. Casting
implicit role linking as an anaphora resolution task.
In Proceedings of STARSEM 2012: The First Joint
Conference on Lexical and Computational Semantics,
Montre?al, Que?bec, Canada, 7?8 June 2012, pages 1?
10.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, Cal., 2nd edition.
917
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 795?804,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Collective Classification for Fine-grained Information Status
Katja Markert1,2, Yufang Hou2, Michael Strube2
1 School of Computing, University of Leeds, UK, scskm@leeds.ac.uk
2 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
Abstract
Previous work on classifying information sta-
tus (Nissim, 2006; Rahman and Ng, 2011)
is restricted to coarse-grained classification
and focuses on conversational dialogue. We
here introduce the task of classifying fine-
grained information status and work on writ-
ten text. We add a fine-grained information
status layer to the Wall Street Journal portion
of the OntoNotes corpus. We claim that the
information status of a mention depends not
only on the mention itself but also on other
mentions in the vicinity and solve the task by
collectively classifying the information status
of all mentions. Our approach strongly outper-
forms reimplementations of previous work.
1 Introduction
Speakers present already known and yet to be es-
tablished information according to principles re-
ferred to as information structure (Prince, 1981;
Lambrecht, 1994; Kruijff-Korbayova? and Steedman,
2003, inter alia). While information structure af-
fects all kinds of constituents in a sentence, we here
adopt the more restricted notion of information sta-
tus which concerns only discourse entities realized
as noun phrases, i.e. mentions1. Information status
(IS henceforth) describes the degree to which a dis-
course entity is available to the hearer with regard to
the speaker?s assumptions about the hearer?s knowl-
edge and beliefs (Nissim et al, 2004). Old men-
tions are known to the hearer and have been referred
1Since not all noun phrases are referential, we call noun
phrases which carry information status mentions.
to previously. Mediated mentions have not been
mentioned before but are also not autonomous, i.e.,
they can only be correctly interpreted by reference
to another mention or to prior world knowledge. All
other mentions are new.
IS can be beneficial for a number of NLP tasks,
though the results have been mixed. Nenkova et
al. (2007) used IS as a feature for generating pitch
accent in conversational speech. As IS is restricted
to noun phrases, while pitch accent can be assigned
to any word in an utterance, the experiments were
not conclusive. For determining constituent order of
German sentences, Cahill and Riester (2009) incor-
porate features modeling IS to good effect. Rahman
and Ng (2011) showed that IS is a useful feature for
coreference resolution.
Previous work on learning IS (Nissim, 2006; Rah-
man and Ng, 2011) is restricted in several ways.
It deals with conversational dialogue, in particular
with the corpus annotated by Nissim et al (2004).
However, many applications that can profit from IS
concentrate on written texts, such as summariza-
tion. For example, Siddharthan et al (2011) show
that solving the IS subproblem of whether a per-
son proper name is already known to the reader im-
proves automatic summarization of news. There-
fore, we here model IS in written text, creating a
new dataset which adds an IS layer to the already
existing comprehensive annotation in the OntoNotes
corpus (Weischedel et al, 2011). We also report
the first results on fine-grained IS classification by
modelling further distinctions within the category
of mediated mentions, such as comparative and
bridging anaphora (see Examples 1 and 2, re-
795
spectively).2 Fine-grained IS is a prerequisite to
full bridging/comparative anaphora resolution, and
therefore necessary to fill gaps in entity grids (Barzi-
lay and Lapata, 2008) based on coreference only.
Thus, Examples 1 and 2 do not exhibit any corefer-
ential entity coherence but coherence can be estab-
lished when the comparative anaphor others is re-
solved to others than freeway survivor Buck Helm,
and the bridging anaphor the streets is resolved to
the streets of Oranjemund, respectively.
(1) the condition of freeway survivor Buck
Helm . . . , improved, hospital officials said.
Rescue crews, however, gave up hope that
others would be found.
(2) Oranjemund, the mine headquarters, is a
lonely corporate oasis of 9,000 residents.
Jackals roam the streets at night . . .
We approach the challenge of modeling IS via
collective classification, using several novel linguis-
tically motivated features. We reimplement Nissim?s
(2006) and Rahman and Ng?s (2011) approaches as
baselines and show that our approach outperforms
these by a large margin for both coarse- and fine-
grained IS classification.
2 Related Work
IS annotation schemes and corpora. We en-
hance the approach in Nissim et al (2004) in two
major ways (see also Section 3.1). First, compar-
ative anaphora are not specifically handled in Nis-
sim et al (2004) (and follow-on work such as Ritz
et al (2008) and Riester et al (2010)), although
some of them might be included in their respective
bridging subcategories. Second, we apply the
annotation scheme reliably to a new genre, namely
news. This is a non-trivial extension: Ritz et al
(2008) applied a variation of the Nissim et al (2004)
scheme to a small set of 220 NPs in a German
news/commentary corpus but found that reliability
then dropped significantly to the range of ? = 0.55
to 0.60. They attributed this to the higher syntac-
tic complexity and semantic vagueness in the com-
mentary corpus. Riester et al (2010) annotated a
2All examples in this paper are from the OntoNotes cor-
pus. The mention in question is typed in boldface; antecedents,
where applicable, are displayed in italics.
German news corpus marginally reliable (? = 0.66)
for their overall scheme but their confusion ma-
trix shows even lower reliability for several subcate-
gories, most importantly deixis and bridging.
While standard coreference corpora do not con-
tain IS annotation, some corpora annotated for
bridging are emerging (Poesio, 2004; Korzen and
Buch-Kromann, 2011) but they are (i) not annotated
for comparative anaphora or other IS categories, (ii)
often not tested for reliability or reach only low reli-
ability, (iii) often very small (Poesio, 2004).
To the best of our knowledge, we therefore
present the first English corpus reliably annotated
for a wide range of IS categories as well as full
anaphoric information for three main anaphora types
(coreference, bridging, comparative).
Automatic recognition of IS. Vieira and Poesio
(2000) describe heuristics for processing definite de-
scriptions in news text. As their approach is re-
stricted to definites, they only analyse a subset of
the mentions we consider carrying IS. Siddharthan
et al (2011) also concentrate on a subproblem of IS
only, namely the hearer-old/hearer-new distinctions
for person proper names.
Nissim (2006) and Rahman and Ng (2011) both
present algorithms for IS detection on Nissim et
al.?s (2004) Switchboard corpus. Both papers treat
IS classification as a local classification problem
whereas we look at dependencies between the IS
status of different mentions, leading to collective
classification. In addition, they only distinguish the
three main categories old, mediated and new.
Finally, we work on news corpora which poses dif-
ferent problems from dialogue.
Anaphoricity determination (Ng, 2009; Zhou and
Kong, 2009) identifies many or most old men-
tions. However, no distinction between mediated
and new mentions is made. Most approaches to
bridging resolution (Meyer and Dale, 2002; Poe-
sio et al, 2004) or comparative anaphora (Mod-
jeska et al, 2003; Markert and Nissim, 2005)
address only the selection of the antecedent for
the bridging/comparative anaphor, not its recogni-
tion. Sasano and Kurohashi (2009) do also tackle
bridging recognition, but they depend on language-
specific non-transferrable features for Japanese.
796
3 Corpus Creation
3.1 Annotation Scheme
Our scheme follows Nissim et al (2004) in dis-
tinguishing three major IS categories old, new
and mediated. A mention is old if it is ei-
ther coreferential with an already introduced entity
or a generic or deictic pronoun. We follow the
OntoNotes (Weischedel et al, 2011) definition of
coreference to be able to integrate our annotations
with it. This definition includes coreference with
noun phrase as well as verb phrase antecedents3 .
Mediated refers to entities which have not yet
been introduced in the text but are inferrable via
other mentions or are known via world knowl-
edge. We distinguish the following six subcate-
gories: The category mediated/comparative
comprises mentions compared via either a contrast
or similarity to another one (see Example 1). This
category is novel in our scheme. We also in-
clude a category mediated/bridging (see Ex-
amples 2, 3 and 4). Bridging anaphora can be
any noun phrase and are not limited to definite NPs
as in Poesio et al (2004), Gardent and Manue?lian
(2005), Riester et al (2010). In contrast to Nissim
et al (2004), antecedents for both comparative and
bridging categories are annotated and can be noun
phrases, verb phrases or even clauses. The category
mediated/knowledge is inspired by the hearer-
old distinction introduced by Prince (1992) and cov-
ers entities generally known to the hearer. It includes
many proper names, such as Poland.4 Mentions that
are syntactically linked via a possessive relation or a
PP modification to other, old or mediated men-
tions fall into the type mediated/synt (see Ex-
amples 5 and 6).5 With no change to Nissim et al?s
scheme, coordinated mentions where at least one el-
ement in the conjunction is old or mediated are
covered by the category mediated/aggregate,
and mentions referring to a value of a previously
mentioned function by the type mediated/func.
All other mentions are annotated as new, includ-
3In contrast to Nissim et al (2004), but in accordance with
OntoNotes, we do not consider generics for coreference.
4This class corresponds roughly to Nissim et al?s (2004)
mediated/general.
5This class expands Nissim et al?s (2004) poss category
that only considers possessives but not PP modification.
ing most generics as well as newly introduced, spe-
cific mentions such as Example 7.
(3) Initial steps were taken at Poland?s first en-
vironmental conference, which I attended
last month. . . . it was no accident that par-
ticipants urged the free flow of information
(4) The Bakersfield supermarket went out of
business last May. The reason was . . .
(5) One Washington couple sold their liquor
store
(6) the main artery into San Francisco
(7) the owner was murdered by robbers
3.2 Agreement Study
We carried out an agreement study with 3 annota-
tors, of which Annotator A was the scheme devel-
oper and first author of this paper. All texts used
were from the Wall Street Journal (WSJ) portion of
OntoNotes. There were no restrictions on which
texts to include apart from (i) exclusion of letters
to the editor as they contain cross-document links
and (ii) a preference for longer texts with potentially
richer discourse structure.
Mentions were automatically preselected for the
annotators using the gold-standard syntactic annota-
tion.6 The existing coreference annotation was auto-
matically carried over to the IS task by marking all
mentions in a coreference chain (apart from the first
mention in the chain) as old. The annotation task
consisted of marking all mentions for their IS (old,
mediated or new) as well as marking mediated
subcategories (see Section 3.1) and the antecedents
for comparative and bridging anaphora.
The scheme was developed on 9 texts, which were
also used for training the annotators. Inter-annotator
agreement was measured on 26 new texts, which in-
cluded 5905 pre-marked potential mentions. The an-
notations of 1499 of these were carried over from
OntoNotes, leaving 4406 potential mentions for an-
notation and agreement measurement. In addition to
6Some non-mentions such as idioms could not be filtered
out via the syntactic annotation and had to be excluded during
human annotation.
797
A-B A-C B-C
Overall Percentage coarse 87.5 86.3 86.5
Overall ? coarse 77.3 75.2 74.7
Overall Percentage fine 86.6 85.3 85.7
Overall ? fine 80.1 77.7 77.3
Table 1: Agreement Results
A-B A-C B-C
? Non-mention 81.5 78.9 86.0
? Old 80.5 83.2 79.3
? New 76.6 74.0 74.3
? Mediated/Knowledge 82.1 78.4 74.1
? Mediated/Synt 88.4 87.8 87.6
? Mediated/Aggregate 87.0 85.4 86.0
? Mediated/Func 6.0 83.2 6.9
? Mediated/Comp 81.8 78.3 81.2
? Mediated/Bridging 70.8 60.6 62.3
Table 2: Agreement Results for individual categories
percentage agreement, we measured Cohen?s ? (Art-
stein and Poesio, 2008) between all 3 possible anno-
tator pairings. We also report single-category agree-
ment for each category, where all categories but one
are merged and then ? is computed as usual. Table 1
shows agreement results for the overall scheme at
the coarse-grained (4 categories: non-mention, old,
new, mediated) and the fine-grained level (9 cate-
gories: non-mention, old, new and the 6 mediated
subtypes). The results show that the scheme is over-
all reliable, with not too many differences between
the different annotator pairings.7
Table 2 shows the individual category agreement
for all 9 categories. We achieve high reliability for
most categories.8 Particularly interesting is the fact
that hearer-old entities (mediated/knowledge)
can be identified reliably although all annotators had
substantially different backgrounds. The reliabil-
ity of the category bridging is more annotator-
dependent, although still higher, sometimes con-
siderably, than other previous attempts at bridg-
7Often, annotation is considered highly reliable when ? ex-
ceeds 0.80 and marginally reliable when between 0.67 and 0.80
(Carletta, 1996). However, the interpretation of ? is still under
discussion (Artstein and Poesio, 2008).
8The low reliability of the rare category func, when involv-
ing Annotator B, was explained by Annotator B forgetting about
this category after having used it once. Pair A-C achieved high
reliability (? 83.2 for pair A-C).
ing annotation (Poesio et al, 2004; Gardent and
Manue?lian, 2005; Riester et al, 2010).
3.3 Gold Standard
Our final gold standard corpus consists of 50 texts
from the WSJ portion of the OntoNotes corpus-
The corpus will be made publically available as
OntoNotes annotation layer via http://www.
h-its.org/nlp/download.
Disagreements in the 35 texts used for annota-
tor training (9 texts) and testing (26 texts) were re-
solved via discussion between the annotators. An
additional 15 texts were annotated by Annotator A.
Finally, Annotator A carried out consistency checks
over all texts. ? The gold standard includes 10,980
true mentions (see Table 3).
Texts 50
Mentions 10,980
old 3237
coref 3,143
generic deictic pr 94
mediated 3,708
world knowledge 924
syntactic 1,592
aggregate 211
func 65
comparative 253
bridging 663
new 4,035
Table 3: Gold Standard Distribution
4 Features
In this Section, we describe both the local as well as
the relational features we use.
4.1 Features for Local Classification
We use the following local features, including the
features in Nissim (2006) and Rahman and Ng
(2011) to be able to gauge how their systems fare on
our corpus and as a comparison point for our novel
collective classification approach.
The features developed by Nissim (2006) are
shown in Table 4. Nissim shows clearly that
these features are useful for IS classification.
Thus, subjects are more likely to be old as as-
sumed by, e.g., centering theory (Grosz et al,
798
Feature Value
full prev mention {yes, no, NA}9
mention time {first, second, more}
partial prev mention {yes, no, NA}
determiner {bare, def, dem, indef, poss, NA}
NP type {pronoun, common, proper, other}
NP length numeric
grammatical role {subject, subjpass, pp, other}
Table 4: Nissim?s (2006) feature set
1995). Also, previously unmentioned proper names
are more likely to be hearer-old and therefore
mediated/knowledge, although their exact sta-
tus will depend on how well known a particular
proper name is.
Rahman and Ng (2011) add all unigrams appear-
ing in any mention in the training set as features.
They also integrated (via a convolution tree-kernel
SVM (Collins and Duffy, 2001)) partial parse trees
that capture the generalised syntactic context of a
mention e and include the mention?s parent and sib-
ling nodes without lexical leaves. However, they use
no structure underneath the mention node e itself,
assuming that ?any NP-internal information has pre-
sumably been captured by the flat features?.
To these feature sets, we add a small set of other
local features otherlocal. These track partial previ-
ous mentions by also counting partial previous men-
tion time as well as the previous mention of con-
tent words only. We also add a mention?s number as
one of singular, plural or unknown, and whether the
mention is modified by an adjective. Another feature
encapsulates whether the mention is modified by a
comparative marker, using a small set of 10 markers
such as another, such, similar . . . and the presence
of adjectives or adverbs in the comparative. Finally,
we include the mention?s semantic class as one of 12
coarse-grained classes, including location, organisa-
tion, person and several classes for numbers (such as
date, money or percent).
4.2 Relations for Collective Classification
Both Nissim (2006) and Rahman and Ng (2011)
classify each mention individually in a standard su-
pervised ML setting, not considering potential de-
pendencies between the IS categories of different
9We changed the value of ?full prev mention? from ?nu-
meric? to {yes, no, NA}.
mentions. However, collective or joint classifica-
tion has made substantial impact in other NLP tasks,
such as opinion mining (Pang and Lee, 2004; Soma-
sundaran et al, 2009), text categorization (Yang et
al., 2002; Taskar et al, 2002) and the related task of
coreference resolution (Denis and Baldridge, 2007).
We investigate two types of relations between men-
tions that might impact on IS classification.
Syntactic parent-child relations. Two media-
ted subcategories account for accessibility via syn-
tactic links to another old or mediated men-
tion: mediated/synt is used when at least one
child of a mention is mediated or old, with child
relations restricted to pre- or postnominal posses-
sives as well as PP children in our scheme (see Sec-
tion 3.1). mediated/aggregate is for coordi-
nations in which at least one of the children is old
or mediated. In these two cases, a mention?s
IS depends directly on the IS of its children. We
therefore link a mention m1 to a mention m2 via a
hasChild relation if (i) m2 is a possessive or prepo-
sitional modification ofm1, or (ii)m1 is a coordina-
tion and m2 is one of its children.
Using such a relational feature catches two birds
with one stone: firstly, it integrates the internal struc-
ture of a mention into the algorithm, which Rah-
man and Ng (2011) ignore; secondly, it captures de-
pendencies between parent and child classification,
which would not be possible if we integrated the in-
ternal structure via flat features or additional tree
kernels. We hypothesise that the higher syntactic
complexity of our news genre (14.5% of all men-
tions are mediated/synt) will make this feature
highly effective in distinguishing between new and
mediated categories.
Syntactic precedence relations. IS is said to in-
fluence word order (Birner and Ward, 1998; Cahill
and Riester, 2009) and this fact has been exploited
in work on generation (Prevost, 1996; Filippova and
Strube, 2007; Cahill and Riester, 2009). Therefore,
we integrate dependencies between the IS classifica-
tion of mentions in precedence relations.
m1 precedes m2 if (i) m1 and m2 are in the same
clause, allowing for trace subjects in gerund and in-
finitive constructions, (ii) m1 and m2 are dependent
on the same verb or noun, allowing for interven-
ing nodes via modal, auxiliary, gerund and infinitive
799
constructions, (iii) m1 is neither a child nor a parent
of m2, and (iv) m1 occurs before m2.
For Example 8 (slightly simplified) we extract the
precedence relations shown in Table 5.
(8) She was sent by her mother to a white
woman?s house to do chores in exchange for
meals and a place to sleep.
(She)old >p (her mother)med/synt
(She)old >p (a white-woman?s house)new
(She)old >p (chores)new
(She)old >p (exchange .....sleep)new
(her mother)med/synt >p (a white woman?s house)new
(chores)new >p (exchange . . . sleep)new
(meals)new >p (a place to sleep)new
Table 5: Precedence Relations for Example 8. She is a
trace subject for do.
Proper names behave differently from common
nouns. For example, they can occur at many differ-
ent places in the clause when functioning as spatial
or temporal scene-setting elements, such as In New
York. We therefore exclude all precedence relations
where one element of the pair is a proper name.
We extract 2855 precedence relations. Table 6
shows the statistics on precedence with the first men-
tion in a pair in rows and the second in columns. Me-
diated and new mentions indeed rarely precede old
mentions, so that precedence should improve sepa-
rating of old vs other mentions.
old mediated new
old 136 387 519
mediated 88 357 379
new 85 291 613
Table 6: Precedence relations in our corpus
5 Experiments
5.1 Experimental Setup
We use our gold standard corpus (see Section 3.3)
via 10-fold cross-validation on documents for all ex-
periments. Following Nissim (2006) and Rahman
and Ng (2011), we perform all experiments on gold
standard mentions and use the human WSJ syntac-
tic annotation for feature extraction, when neces-
sary. For the extraction of semantic class, we use
OntoNotes entity type annotation for proper names
and an automatic assignment of semantic class via
WordNet hypernyms for common nouns.
Coarse-grained versions of all algorithms distin-
guish only between the three old, mediated,
new categories. Fine-grained versions distinguish
between the categories old, the six mediated
subtypes, and new. We report overall accuracy as
well as precision, recall and F-measure per category.
Significance tests are conducted using McNemar?s
test on overall algorithm accuracy, at the level of 1%.
5.2 Local Classifiers
We reimplemented the algorithms in Nissim (2006)
and Rahman and Ng (2011) as comparison base-
lines, using their feature and algorithm choices. Al-
gorithm Nissim is therefore a decision tree J48 with
standard settings in WEKA with the features in Ta-
ble 4. Algorithm RahmanNg is an SVM with a com-
posite kernel and one-vs-all training/testing (toolkit
SVMLight). They use the features in Table 4 plus
unigram and tree kernel features, described in Sec-
tion 4.1. We add our additional set of otherlocal
features to both baseline algorithms (yielding Nis-
sim+ol and RahmanNg+ol) as they aim specifically
at improving fine-grained classification.
5.3 Collective Classification
For incorporating our inter-mention links, we use a
variant of Iterative Collective classification (ICA),
which has shown good performance over a variety
of tasks (Lu and Getoor, 2003) and has been used
in NLP for example for opinion mining (Somasun-
daran et al, 2009). ICA is normally faster than
Gibbs sampling and ? in initial experiments ? did
not yield significantly different results from it.
ICA initializes each mention with its most likely
IS, according to the local classifier and features. It
then iterates a relational classifier, which uses both
local and relational features (our hasChild and pre-
cedes features) taking IS assignments to neighbour-
ing mentions into account. We use the exist aggre-
gator to define the dependence between mentions.
We use NetKit (Macskassy and Provost, 2007)
with its standard ICA settings for collective infer-
ence, as it allows direct comparison between local
and collective classification. The relational classi-
fiers are always exactly the same classifiers as the
800
local collective
Nissim+ol Nissim+olNissim Nissim+ol
+hasChild +hasChild+precedes
R P F R P F R P F R P F
Coarse
old 82.2 86.4 84.2 81.2 88.6 84.8 81.7 88.6 85.0 80.9 89.1 84.8
mediated 51.9 60.2 55.7 57.8 64.6 61.0 68.4 77.4 72.6 68.8 76.9 72.6
new 74.2 63.6 68.5 78.4 67.3 72.4 87.7 75.1 80.9 87.9 75.0 80.9
acc 69.0 72.3 79.4 79.4
Fine
old 84.0 83.3 83.6 85.0 83.9 84.5 84.3 84.7 84.5 84.1 85.2 84.6
med/knowledge 61.3 60.0 60.6 61.0 69.5 65.0 62.3 70.0 65.9 60.6 70.0 65.0
med/synt 37.2 59.7 45.8 44.7 60.0 51.3 76.8 81.4 79.0 75.7 80.1 77.9
med/agg 26.0 42.0 32.2 20.4 38.4 26.6 42.6 55.9 48.4 43.1 55.8 48.7
med/func 0.0 NA NA 32.3 65.6 43.3 33.8 53.7 41.5 35.4 53.5 48.7
med/comp 0.4 7.70 0.7 79.0 82.6 80.0 80.6 82.9 81.8 81.4 82.0 81.7
med/bridging 6.6 26.2 10.6 8.9 30.9 13.8 9.6 34.4 15.1 12.2 41.7 18.9
new 82.6 61.0 70.2 82.7 65.1 72.8 88.0 74.0 80.4 87.7 73.3 79.8
acc 66.6 70.0 77.0 76.8
Table 7: Collective classification compared to Nissim?s local classifier. Best performing algorithms are bolded.
local ones with the relational features added: thus, if
the local classifier is a tree kernel SVM so is the rela-
tional one. One problem when using the SVM Tree
kernel as relational classifier is that it allows only for
binary classification so that we need to train several
binary networks in a one-vs-all paradigm (see also
(Rahman and Ng, 2011)), which will not be able to
use the multiclass dependencies of the relational fea-
tures to optimum effect.
5.4 Results
Table 7 shows the comparison of collective classifi-
cation to local classification, using Nissim?s frame-
work and features, and Table 8 the equivalent table
for Rahman and Ng?s approach.
The improvements using the additional local fea-
tures over the original local classifiers are sta-
tistically significant in all cases. In particu-
lar, the inclusion of semantic classes improves
mediated/knowledge and mediated/func,
and comparative anaphora are recognised highly re-
liably via a small set of comparative markers.
The hasChild relation leads to significant im-
provement in accuracy over local classification in
all cases, showing the value of collective clas-
sification. The improvement here is centered
on the categories of mediated/synt (for both
cases) and mediated/aggregate (for Nis-
sim+ol+hasChild) as well as their distinction from
new.10 It is also interesting that collective clas-
sification with a concise feature set and a sim-
ple decision tree as used in Nissim+ol+hasChild,
performs equally well as RahmanNg+ol+hasChild,
which uses thousands of unigram and tree features
and a more sophisticated local classifier. It also
shows more consistent improvements over all fine-
grained classes.
The precedes relation does not lead to any fur-
ther improvement. We investigated several varia-
tions of the precedence link, such as restricting it
to certain grammatical relations, taking into account
definiteness or NP type but none of them led to
any improvement. We think there are two reasons
for this lack of success. First, the precedence of
mediated vs. new mentions does not follow a
clear order and is therefore not a very predictive fea-
ture (see Table 6). At first, this seems to contradict
studies such as Cahill and Riester (2009) that find
a variety of precedences according to information
status. However, many of the clearest precedences
they find are more specific variants of the old >p
mediated or old >p new precedence or they
are preferences at an even finer level than the one we
annotate, including for example the identification of
generics. Second, the clear old >p mediated
10For RhamanNg+ol+hasChild, the aggregate class suf-
fers from collective classification. We hypothesise that this is
an artefact of the one-vs-all training/testing for rare categories.
801
local collective
RahmanNg+ol RahmanNg+olRahmanNg RahmanNg+ol
+hasChild +hasChild+precedes
R P F R P F R P F R P F
Coarse
old 81.3 90.1 85.5 82.6 91.4 86.8 83.5 87.8 85.6 82.9 87.2 85.0
mediated 61.4 68.6 64.8 61.5 71.9 66.3 66.7 79.5 72.6 64.8 76.7 70.3
new 82.1 69.9 75.5 84.9 70.1 76.8 89.0 74.9 81.3 86.9 73.5 79.6
acc 74.9 76.3 79.8 78.3
Fine
old 85.1 87.0 86.0 85.6 87.9 86.7 85.3 87.4 86.3 85.8 87.5 86.4
med/knowledge 65.8 67.2 66.5 64.8 72.6 68.5 67.1 69.6 68.3 64.7 73.2 68.7
med/synt 55.8 72.1 62.9 55.8 72.6 63.1 79.8 78.1 78.9 79.8 78.1 78.9
med/agg 29.9 75.9 42.9 29.9 75.9 42.9 17.1 53.7 25.9 14.2 49.2 22.1
med/func 27.7 38.3 32.1 38.5 69.4 49.5 40.0 44.1 42.0 40.0 40.0 40.0
med/comp 25.3 86.5 39.1 76.7 82.2 79.3 74.3 62.7 68.0 74.3 62.7 68.0
med/bridging 10.6 44.6 17.1 9.0 47.2 15.2 1.0 15.2 2.0 1.0 13.7 1.9
new 87.3 66.3 75.4 89.0 67.8 77.0 89.2 74.6 81.2 89.2 74.6 81.2
acc 72.6 74.6 77.5 77.4
Table 8: Collective classification compared to Rahman and Ng?s local classifier. Best performing algorithms are
bolded.
and old >p new preferences are partially already
captured by the local features, especially the gram-
matical role, as, for example, subjects are often both
old as well as early on in a sentence.
With regard to fine-grained classification, many
categories including comparative anaphora, are
identified quite reliably, especially in the multiclass
classification setting (Nissim+ol+hasChild). Bridg-
ing seems to be the by far most difficult category
to identify with final best F-measures still very low.
Most bridging mentions do not have any clear inter-
nal structure or external syntactic contexts that sig-
nal their presence. Instead, they rely more on lexi-
cal and world knowledge for recognition. Unigrams
could potentially encapsulate some of this lexical
knowledge but ? without generalization ? are too
sparse for a relatively rare category such as bridg-
ing (6% of all mentions) to perform well. The diffi-
culty of bridging recognition is an important insight
of this paper as it casts doubt on the strategy in pre-
vious research to concentrate almost exclusively on
antecedent selection (see Section 2).
6 Conclusions
We presented a new approach to information sta-
tus classification in written text, for which we also
provide the first reliably annotated English language
corpus. Based on linguistic intuition, we define fea-
tures for classifying mentions collectively. We show
that our collective classification approach outper-
forms the state-of-the-art in coarse-grained IS classi-
fication by about 10% (Nissim, 2006) and 5% (Rah-
man and Ng, 2011) accuracy. The gain is almost
entirely due to improvements in distinguishing be-
tween new and mediatedmentions. For the latter,
we also report the ? to our knowledge ? first fine-
grained IS classification results.
Since the work reported in this paper relied ? fol-
lowing Nissim (2006) and Rahman and Ng (2011)
? on gold standard mentions and syntactic anno-
tations, we plan to perform experiments with pre-
dicted mentions as well. We also have to im-
prove the recognition of bridging, ideally combining
recognition and antecedent selection for a complete
resolution component. In addition, we plan to inte-
grate IS resolution with our coreference resolution
system (Cai et al, 2011) to provide us with a more
comprehensive discourse processing system.
Acknowledgements. Katja Markert received a Fel-
lowship for Experienced Researchers by the Alexander-
von-Humboldt Foundation and Yufang Hou is funded by
a PhD scholarship from the Research Training GroupCo-
herence in Language Processing at Heidelberg Univer-
sity. We thank the Heidelberg Institute for Theoretical
Studies for hosting Katja Markert and funding the anno-
tation study, and the annotators for their diligent work.
802
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Betty J. Birner and Gregory Ward. 1998. Information
Status and NoncanonicalWord Order in English. John
Benjamins, Amsterdam, The Netherlands.
Aoife Cahill and Arndt Riester. 2009. Incorporating in-
formation status into generation ranking. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing, Singapore, 2?7 August 2009,
pages 817?825.
Jie Cai, ?Eva Mu?jdricza-Maydt, and Michael Strube.
2011. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Shared
Task of the 15th Conference on Computational Natu-
ral Language Learning, Portland, Oreg., 23?24 June
2011, pages 56?60.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14, Vancouver, B.C.,
Canada, 3?8 December, 2001, pages 625?632, Cam-
bridge, Mass. MIT Press.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Rochester, N.Y., 22?27 April
2007, pages 236?243.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
23?30 June 2007, pages 320?327.
Claire Gardent and He?le`ne Manue?lian. 2005. Cre?ation
d?un corpus annote? pour le traitement des descrip-
tions de?finies. Traitement Automatique des Langues,
46(1):115?140.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Iorn Korzen and Matthias Buch-Kromann. 2011.
Anaphoric relations in the Copenhagen dependency
treebanks. In S. Dipper and H. Zinsmeister, edi-
tors, Corpus-based Investigations of Pragmatic and
Discourse Phenomena, volume 3 of Bochumer Lin-
guistische Arbeitsberichte, pages 83?98. University of
Bochum, Bochum, Germany.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003.
Discourse and information structure. Journal of Logic,
Language and Information. Special Issue on Dis-
cource and Information Structure, 12(3):149?259.
Knud Lambrecht. 1994. Information Structure and Sen-
tence Form. Cambridge, U.K.: Cambridge University
Press.
Qing Lu and Lise Getoor. 2003. Link-based classifica-
tion. In Proceedings of the 20th International Confer-
ence on Machine Learning, Washington, D.C., 21?24
August 2003, pages 496?503.
Sofus A. Macskassy and Foster Provost. 2007. Classi-
fication in networked data: A toolkit and a univariate
case study. Journal of Machine Learning Research,
8:935?983.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3):367?401.
Josef Meyer and Robert Dale. 2002. Mining a corpus to
support associative anaphora resolution. In Proceed-
ings of the 4th International Conference on Discourse
Anaphora and Anaphor Resolution, Lisbon, Portugal,
18?20 September, 2002.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan, 11?12 July 2003,
pages 176?183.
Ani Nenkova, Jason Brenier, Anubha Kothari, Sasha Cal-
houn, LauraWhitton, David Beaver, and Dan Jurafsky.
2007. To memorize or to predict: Prominence labeling
in conversational speech. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Rochester, N.Y., 22?27 April
2007, pages 9?16.
Vincent Ng. 2009. Graph-cut-based anaphoricity deter-
mination for coreference resolution. In Proceedings of
Human Language Technologies 2009: The Conference
of the North American Chapter of the Association for
Computational Linguistics, Boulder, Col., 31 May ? 5
June 2009, pages 575?583.
Malvina Nissim, Shipara Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for in-
formation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation, Lisbon, Portugal, 26?28 May 2004, pages
1023?1026.
803
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, Sydney, Australia, 22?23 July 2006, pages
94?012.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, Barcelona, Spain, 21?26 July 2004, pages
272?279.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pages 143?150.
Massimo Poesio. 2004. The MATE/GNOME proposals
for anaphoric annotation, revisited. In Proceedings of
the 5th SIGdial Workshop on Discourse and Dialogue,
Cambridge, Mass., 30 April ? 1 May 2004, pages 154?
162.
Scott Prevost. 1996. An information structural approach
to spoken language generation. In Proceedings of the
34th Annual Meeting of the Association for Computa-
tional Linguistics, Santa Cruz, Cal., 24?27 June 1996,
pages 294?301.
Ellen F. Prince. 1981. Towards a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York, N.Y.
Ellen F. Prince. 1992. The ZPG letter: Subjects,
definiteness, and information-status. In W.C. Mann
and S.A. Thompson, editors, Discourse Description.
Diverse Linguistic Analyses of a Fund-Raising Text,
pages 295?325. John Benjamins, Amsterdam.
Altaf Rahman and Vincent Ng. 2011. Learning the in-
formation status of noun phrases in spoken dialogues.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, Edinburgh,
Scotland, U.K., 27?29 July 2011, pages 1069?1080.
Arndt Riester, David Lorenz, and Nina Seemann. 2010.
A recursive annotation scheme for referential informa-
tion status. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
La Valetta, Malta, 17?23 May 2010, pages 717?722.
Julia Ritz, Stefanie Dipper, and Michael Go?tze. 2008.
Annotation of information structure: An evaluation
across different types of texts. In Proceedings of the
6th International Conference on Language Resources
and Evaluation, Marrakech, Morocco, 26 May ? 1
June 2008, pages 2137?2142.
Ryohei Sasano and Sadao Kurohashi. 2009. A prob-
abilistic model for associative anaphora resolution.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singapore,
6?7 August 2009, pages 1455?1464.
Advaith Siddharthan, Ani Nenkova, and Kathleen McK-
eown. 2011. Information status distinctions and re-
ferring expressions: An empirical study of references
to people in news summaries. Computational Linguis-
tics, 37(4):811?842.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, Singapore, 6?7 August 2009.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Proceedings of the 18th Conference on Uncertainty
in Artificial Intelligence, Edmonton, Alberta, Canada,
1-4 August 2002, pages 485?492.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
Yiming Yang, Sea?n Slattery, and Rayid Ghani. 2002. A
study of approaches to hypertext categorization. Jour-
nal of Intelligent Information Systems, 18(2-3):219?
241.
Guodong Zhou and Fang Kong. 2009. Global learning of
noun phrase anaphoricity in coreference resolution via
label propagation. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Singapore, 6?7 August 2009, pages 978?986.
804
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 93?103,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Graph-based Local Coherence Modeling
Camille Guinaudeau and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
(camille.guinaudeau|michael.strube)@h-its.org
Abstract
We propose a computationally efficient
graph-based approach for local coherence
modeling. We evaluate our system on
three tasks: sentence ordering, summary
coherence rating and readability assess-
ment. The performance is comparable to
entity grid based approaches though these
rely on a computationally expensive train-
ing phase and face data sparsity problems.
1 Introduction
Many NLP applications which process or gener-
ate texts rely on information about local coher-
ence, i.e. information about which entities occur
in which sentence and how the entities are dis-
tributed in the text. This led to the development
of many theories and models accounting for lo-
cal coherence. One popular model, the center-
ing model (Grosz et al, 1995), uses a ranking of
discourse entities realized in particular sentences
and computes transitions between adjacent sen-
tences to provide insight in the felicity of texts.
Centering models local coherence rather generally
and has been applied to the generation of refer-
ring expressions (Kibble and Power, 2004), to re-
solve pronouns (Brennan et al, 1987, inter alia),
to score essays (Miltsakaki and Kukich, 2004), to
arrange sentences in the correct order (Karamanis
et al, 2009), and to many other tasks. Poesio et
al. (2004) observe that it is not clear how to set
parameters in the centering model so that optimal
performance in different tasks and languages can
be achieved. Barzilay and Lapata (2008) criticize
research on centering to be too dependent on man-
ually annotated input. This led them to propose a
local coherence model relying on a more parsimo-
nious representation, the entity grid model.
The entity grid is a two dimensional array where
the rows represent sentences and the columns dis-
course entities. From this grid Barzilay and La-
pata (2008) derive probabilities of transitions be-
tween adjacent sentences which are used as fea-
tures for machine learning algorithms. They eval-
uate this approach successfully on sentence order-
ing, summary coherence rating, and readability as-
sessment. However, their approach has some dis-
advantages which they point out themselves: data
sparsity, domain dependence and computational
complexity, especially in terms of feature space is-
sues while building their model (Barzilay and La-
pata (2008, p.8, p.10, p.30), Elsner and Charniak
(2011, p.126, p.127)).
In order to overcome these problems we pro-
pose to represent entities in a graph and then
model local coherence by applying centrality mea-
sures to the nodes in the graph (Section 3). We
claim that a graph is a more powerful representa-
tion for local coherence than the entity grid (Barzi-
lay and Lapata, 2008) which is restricted to transi-
tions between adjacent sentences. The graph can
easily span the entire text without leading to com-
putational complexity and data sparsity problems.
Similar to the application of graph-based methods
in other areas of NLP (e.g. work on word sense
disambiguation by Navigli and Lapata (2010); for
an overview over graph-based methods in NLP
see Mihalcea and Radev (2011)) we model local
coherence by relying only on centrality measures
applied to the nodes in the graph. We apply our
graph-based model to the three tasks handled by
Barzilay and Lapata (2008) to show that it pro-
vides the same flexibility over disparate tasks as
the entity grid model: sentence ordering (Section
4.1), summary coherence ranking (Section 4.2),
and readability assessment (Section 4.3). In the
93
The Turkish government fell after mob-tie allegations.
Turkey?s constitution mandates a secular republic despite its
Muslim majority.
Military and secular leaders pressured President Demirel to
keep the Islamic-oriented Virtue Party on the fringe.
Business leaders feared Virtue would alienate the EU.
Table 1: Excerpt of a manual summary M from
DUC2003
experiments sections, we discuss the impact of
genre and stylistic properties of documents on the
local coherence computation. We also show that,
though we do not need a computationally expen-
sive learning phase, our model achieves state-of-
the-art performance. From this we conclude that a
graph is an alternative to the entity grid model: it is
computationally more tractable for modeling local
coherence and does not suffer from data sparsity
problems (Section 5).
2 The Entity Grid Model
Barzilay and Lapata (2005; 2008) introduced the
entity grid, a method for local coherence modeling
that captures the distribution of discourse entities
across sentences in a text.
An entity grid is a two dimensional array, where
rows correspond to sentences and columns to dis-
course entities. For each discourse entity ej and
each sentence si in the text, the corresponding grid
cell cij contains information about the presence or
absence of the entity in the sentence. If the entity
does not appear in the sentence, the correspond-
ing grid cell contains an absence marker ???. If
the entity is present in the sentence, the cell con-
tains a representation of the entity?s syntactic role:
?S? if the entity is a subject, ?O? if it is an object
and ?X? for all other syntactic roles (cf. Table 2).
When a noun is attested more than once with a
different grammatical role in the same sentence,
the role with the highest grammatical ranking is
chosen to represent the entity (a subject is ranked
higher than an object, which is ranked higher than
other syntactic roles).
Barzilay and Lapata (2008) capture local coher-
ence by means of local entity transitions, i.e. se-
quences of grid cells (c1j . . . cij . . . cnj) represent-
ing the syntactic function or absence of an entity in
adjacent sentences1. The coherence of a sentence
in relation to its local context is determined by the
1For complexity reasons, Barzilay and Lapata consider
only transitions between at most three sentences.
G
OV
ER
N
M
EN
T
A
LL
EG
AT
IO
N
TU
R
K
EY
CO
N
ST
IT
U
TI
O
N
SE
CU
LA
R
R
EP
U
B
LI
C
M
A
JO
R
IT
Y
M
IL
IT
A
RY
LE
A
D
ER
PR
ES
ID
EN
T
D
EM
IR
EL
V
IR
TU
E
PA
RT
Y
FR
IN
G
E
BU
SI
N
ES
S
EU
s1 S X ? ? ? ? ? ? ? ? ? ? ? ? ? ?
s2 ? ? X S X O X ? ? ? ? ? ? ? ? ?
s3 ? ? ? ? X ? ? X S X S X O X ? ?
s4 ? ? ? ? ? ? ? ? S ? ? S ? ? X O
Table 2: Entity Grid representation of summary M
local entity transitions of the entities present or ab-
sent in the sentence. To make this representation
accessible to machine learning algorithms, Barzi-
lay and Lapata (2008) compute for each document
the probability of each transition and generate fea-
ture vectors representing the sentences. Coherence
assessment is then formulated as a ranking learn-
ing problem where the ranking function is learned
with SVMlight (Joachims, 2002).
The entity grid approach has already been ap-
plied to many applications relying on local co-
herence estimation: summary rating (Barzilay
and Lapata, 2005), essay scoring (Burstein et al,
2010) or story generation (McIntyre and Lapata,
2010). It was also used successfully in com-
bination with other systems or features. Sori-
cut and Marcu (2006) show that the entity grid
model is a critical component in their sentence or-
dering model for discourse generation. Barzilay
and Lapata (2008) combine the entity grid with
readability-related features to discriminate docu-
ments between easy- and difficult-to-read cate-
gories. Lin et al (2011) use discourse relations to
transform the entity grid representation into a dis-
course role matrix that is used to generate feature
vectors for machine learning algorithms similarly
to Barzilay and Lapata (2008).
Several studies propose to extend the entity grid
model using different strategies for entity selec-
tion. Filippova and Strube (2007) aim to improve
the entity grid model performance by grouping en-
tities by means of semantic relatedness. In their
studies, Elsner and Charniak extend the number
and type of entities selected and consider that each
entity has to be dealt with accordingly with its in-
formation status (Elsner et al, 2007) or its named-
entity category (Elsner and Charniak, 2011). Fi-
nally, they include a heuristic coreference resolu-
tion component by linking mentions which share a
94
s1 s2 s3 s4
e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11 e12 e13 e14 e15 e16
3 1 1 3 1 2 1 1 1 3 1 3 1 2
1
3
3 1 2 s1 s2
s3 s4
1
1
s1 s2
s3 s4
1
2
(a) Bipartite Graph (b) Unweighted One-mode (c) Weighted One-mode
Projection Projection
e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11 e12 e13 e14 e15 e16
s1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
s2 0 0 1 3 1 2 1 0 0 0 0 0 0 0 0 0
s3 0 0 0 0 1 0 0 1 3 1 3 1 2 1 0 0
s4 0 0 0 0 0 0 0 0 3 0 0 3 0 0 1 2
s1 s2 s3 s4
s1 0 0 0 0
s2 0 0 1 0
s3 0 0 0 1
s4 0 0 0 0
s1 s2 s3 s4
s1 0 0 0 0
s2 0 0 1 0
s3 0 0 0 2
s4 0 0 0 0
(d) Incidence Matrix (e) Unweighted Adjacency (f) Weighted Adjacency
Matrix Matrix
Figure 1: Bipartite graph for summary M from Table 1, one-mode projections and associated incidence
and adjacency matrices. Weights in Figure 1(a) are assigned as follows: ?S? = 3, ?O? = 2, ?X? = 1,
??? = 0 (no edge).
head noun. These extensions led to the best results
reported so far for the sentence ordering task.
3 Method
Our model is based on the insight that the en-
tity grid (Barzilay and Lapata, 2008) corresponds
to the incidence matrix of a bipartite graph rep-
resenting the text (see Newman (2010) for more
details on graph representation). A fundamental
assumption underlying our model is that this bi-
partite graph contains the entity transition infor-
mation needed for local coherence computation,
rendering feature vectors and learning phase un-
necessary. The bipartite graph G = (Vs, Ve, L, w)
is defined by two independent sets of nodes ? that
correspond to the set of sentences Vs and the set of
entities Ve of the text ? and a set of edges L associ-
ated with weights w. An edge between a sentence
node si and an entity node ej is created in the bi-
partite graph if the corresponding cell cij in the
entity grid is not equal to ???. Each edge is asso-
ciated with a weight w(ej , si) that depends on the
grammatical role of the entity ej in the sentence
si2. In contrast to Barzilay and Lapata?s entity
grid that contains information about absent enti-
ties, our graph-based representation only contains
?positive? information. Figure 1(a) shows an ex-
ample of the bipartite graph that corresponds to the
grid in Table 2. The incidence matrix of this graph
(Figure 1(d)) is very similar to the entity grid.
2The assignment of weights is described in Section 4.
By modeling entity transitions, Barzilay and
Lapata rely on links that exist between sentences
to model local coherence. In the same spirit, we
apply different kinds of one-mode projections to
the sentence node set Vs of the bipartite graph to
represent the connections that exist between ? po-
tentially non adjacent ? sentences in the graph.
These projections result in graphs where nodes
correspond to sentences. An edge is created be-
tween two nodes if the corresponding sentences
have a least one entity in common. Contrary to the
bipartite graph, one-mode projections are directed
as they follow the text order. Therefore, in projec-
tion graphs an edge can exist between the first and
the second sentence while the inverse is not pos-
sible. In our model, we define three kinds of pro-
jection graphs, PU , PW and PAcc, depending on
the weighting scheme associated with their edges.
In PU , weights are binary and equal 1 when two
sentences have a least one entity in common (Fig-
ure 1(b)). In PW , edges are weighted according to
the number of entities ?shared? by two sentences
(Figure 1(c)). In PAcc syntactic information is ac-
counted for by integrating the edge weights in the
bipartite graph. In this case, weights are equal to
Wik =
?
e?Eik
w(e, si) ? w(e, sk) ,
where Eik is the set of entities shared by si and
sk. Distance between sentences si and sk can also
be integrated in the weight of one-mode projec-
tions to decrease the importance of links that ex-
95
ists between non adjacent sentences. In this case,
the weights of the projection graphs are divided by
k ? i.
From this graph-based representation, the local
coherence of a text T can be measured by comput-
ing the average outdegree of a projection graph P .
This centrality measure was chosen for two main
reasons. First, it allows us to evaluate to which ex-
tent a sentence is connected, in terms of discourse
entities, with the other sentences of the text. Sec-
ond, compared to other centrality measures, the
computational complexity of the average outde-
gree is low (O(N?(N?1)2 ) for a document com-
posed by N sentences), keeping the local coher-
ence estimation feasible on large documents and
on large corpora. Formally, the local coherence of
a text T is equal to
LocalCoherence(T ) = AvgOutDegree(P )
= 1N
?
i=1..N
OutDegree(si) ,
where OutDegree(si) is the sum of the weights as-
sociated to edges that leave si and N is the num-
ber of sentences in the text. This value can also be
seen as the sum of the values of the adjacency ma-
trix of the projection graph (Figures 1(e) and 1(f))
divided by the number of sentences.
4 Experiments
We compare our model with the entity grid ap-
proach and evaluate the influence of the different
weighting schemes used in the projection graphs,
either PW or PAcc, where weights are potentially
decreased by distance information Dist. Our
baseline corresponds to local coherence computa-
tion based on the unweighted projection graph PU .
For graph construction, all nouns in a document
are considered as discourse entities, even those
which do not head NPs as this is beneficial for
the entity grid model as described in Elsner and
Charniak (2011). We also propose to use a coref-
erence resolution system and consider coreferent
entities to be the same discourse entity. To do so,
we use one of the top performing systems from the
CoNLL 2012 shared task (Martschat et al, 2012).
As the coreference resolution system is trained on
well-formed textual documents and expects a cor-
rect sentence ordering, we use in all our experi-
ments only features that do not rely on sentence
order (e.g. alias relations, string matching, etc.).
Grammatical information associated with each
entity is extracted automatically thanks to the
Stanford parser using dependency conversion (de
Marneffe et al, 2006). Syntactic weights in the
bipartite graph are defined following the linguistic
intuition that subjects are more important than ob-
jects, which are themselves more important than
other syntactic roles. Preliminary experiments
show that as long as weight assignment follows
the scheme S > O > X, then more coherent docu-
ments are associated with a higher local coherence
value than less coherent document in 90% of cases
(while this value equals 49% when no restric-
tion is given on syntactic weights order). More-
over, as the local coherence computation is a lin-
ear combination of the syntactic weights, the func-
tion is smooth and no large variations of the local
coherence values are observed for small changes
of weights? values. For these reasons, weights
w(e, si) are set as follows: 3 if e is subject in si, 2
if e is an object and 1 otherwise.
We evaluate the ability of our graph-based
model to estimate the local coherence of a tex-
tual document with three different experiments.
First, we perfom a sentence ordering task (Sec-
tion 4.1) as proposed in Barzilay and Lapata
(2008). Then, as the first task uses ?artificial? doc-
uments, we also work on two other tasks that in-
volve ?real? documents: summary coherence rat-
ing (Section 4.2), and readability assessment (Sec-
tion 4.3). In these experiments, distance compu-
tation and syntactic weights are the same for all
tasks and all corpora. However, the model is also
flexible and can be adaptated to the different tasks
by optimizing the parameters on a development
data set, which may give better results.
4.1 Sentence Ordering
The first experiment consists in ranking alternative
sentence orderings of a document, as proposed by
Barzilay and Lapata (2008) and Elsner and Char-
niak (2011).
4.1.1 Experimental Settings
The sentence ordering task can be performed in
two ways: discrimination and insertion. Discrimi-
nation consists in comparing a document to a ran-
dom permutation of its sentences. For this, our
system associates local coherence values with the
original document and its permutation, the output
of our system being considered as correct if the
score for the original document is higher than the
96
score of its permutation. In the insertion task, pro-
posed by Elsner and Charniak (2011), we evaluate
the ability of our system to retrieve the original
position of a sentence previously removed from a
document. For this, each sentence is removed in
turn and a local coherence score is computed for
every possible reinsertion position. The system
output is considered as correct if the document as-
sociated with the highest local coherence score is
the one in which the sentence is reinserted in the
correct position.
These two tasks were performed on docu-
ments extracted from the English test part of the
CoNLL 2012 shared task (Pradhan et al, 2012).
This corpus, composed by documents of multiple
news sources ? spoken or written ? was preferred
to the ACCIDENTS and EARTHQUAKES corpora
used by Barzilay and Lapata (2008) for two rea-
sons. First, as mentioned by Elsner and Charniak
(2008), these corpora use a very constrained style
and are not typical of normal informative docu-
ments3. Second, we want to evaluate the influence
of automatically performed coreference resolution
in a controlled fashion. The coreference resolution
system used performs well on the CoNLL 2012
data. In this dataset, documents composed by the
concatenation of differents news articles or too
short to have at least 20 permutations were dis-
carded from the corpus. This filtering results in 61
documents composed of 36.1 sentences or 2064
word tokens on average. In both discrimination
and insertion, we compare our system against a
random baseline where random values are associ-
ated with the different orderings.
4.1.2 Discrimination
Accuracy is used to evaluate the ability of our sys-
tem to discriminate a document from 20 differ-
ent permutations. It equals the number of times
our system gives the highest score to the original
document, divided by the number of comparisons.
Since the model can give the same score for a per-
mutation and the original document, we also com-
pute F-measure where recall is correct/total and
precision equals correct/decisions. We test sig-
nificance using the Student?s t-test that can detect
significant differences between paired samples.
Moreover, as increasing the number of hypotheses
3Our graph-based model obtains for the discrimination
task an accuracy of 0.846 and 0.635 on the ACCIDENTS and
EARTHQUAKES datasets, respectively, compared to 0.904 and
0.872 as reported by Barzilay and Lapata (2008).
Acc F Acc F
Random 0.496 0.496
B&L 0.877 0.877
E&C 0.915 0.915
wo coref w coref
PU , Dist 0.830 0.830 0.833 0.833
PW , Dist 0.871 0.871 0.849 0.849
PAcc, Dist 0.889 0.889 0.852 0.852
Table 3: Discrimination, reproduced baselines
(B&L: Barzilay and Lapata (2008); E&C Elsner
and Charniak (2011)) vs. graph-based
in a test can also increase the likelihood of wit-
nessing a rare event, and therefore, the chance to
reject the null hypothesis when it is true, we use
the Bonferroni correction to adjust the increased
random likelihood of apparent significance.
Table 3 presents the values obtained by three
baseline systems when applied to our corpus. Re-
sults for the entity grid models described by Barzi-
lay and Lapata (2008) and Elsner and Charniak
(2011) are obtained by using Micha Elsner?s reim-
plementation in the Brown Coherence Toolkit4.
The system was trained on the English training
part of the CoNLL 2012 shared task filtered in the
same way as the test part.
Table 3 also displays the results for our model.
These values show that our system performs com-
parable to the state-of-the-art. Indeed, the differ-
ence between our best results and those of Elsner
and Charniak are not statistically significant.
In this experiment, distance information is criti-
cal. Without it, it is not possible to distinguish be-
tween an original document and one of its permu-
tation as both contain the same number and kind
of entities. Distance however can detect changes
in the distribution of entities within the document
as space between entities is significantly modi-
fied when sentence order is permuted. When the
number of entities ?shared? by two sentences is
taken into account (PW ), the accuracy of our sys-
tem grows (from 0.830 to 0.871). Table 3 finally
shows that syntactic information improves the per-
formance of our system (yet not significantly) and
gives the best results (PAcc).
We also evaluated the influence of coreference
resolution on the performance of our system. Us-
4https://bitbucket.org/melsner/
browncoherence; B&L is Elsner?s ?baseline entity
grid? (command line option ?-n?), E&C is Elsner?s ?extended
entity grid? (?-f?)
97
Acc. Ins. Acc. Ins.
Random 0.028 0.071
E&C 0.068 0.167
wo coref w coref
PU , Dist 0.062 0.101 0.068 0.120
PW , Dist 0.075 0.114 0.070 0.138
PAcc, Dist 0.071 0.102 0.067 0.097
Table 4: Insertion, reproduced baselines vs. graph-
based
ing coreference resolution improves the perfor-
mance of the system when distance information is
used alone in the system (Table 3). However, this
improvement is not statistically significant.
4.1.3 Insertion
Sentence insertion is much more difficult than dis-
crimination for two reasons. First, in insertion,
permutations only differ by one sentence. Second,
a document is compared to many more permuta-
tions in insertion task than in discrimination.
In complement to accuracy, we use the insertion
score introduced by Elsner and Charniak (2011)
for evaluation. This score ? the higher, the better
? computes the proximity between the initial and
the proposed position of a sentence, averaged by
the number of sentences.
Table 4 shows that, as expected, results for this
task are much lower than those obtained for dis-
crimination. However they are still comparable
with the results of Elsner and Charniak (2011)5.
As previously and for the same reasons, dis-
tance information is critical for this task. The best
results, that present a statistically significant im-
provement when compared to the random base-
line, are obtained when distance information and
the number of entities ?shared? by two sentences
are taken into account (PW ). We can see that the
accuracy value obtained with our system is higher
than the one provided with the entity grid model.
However, the entity grid model reaches a signifi-
cantly higher insertion score. This means that, if it
makes more mistakes than our system, the position
chosen by the entity grid model is usually closer
to the correct position. Finally, contrary to the
discrimination task, syntactic information (PAcc)
does not improve the performance of our system.
5Their results are slightly lower than those presented in
their paper, probably because our corpus is composed by doc-
uments that can be longer than the ones used in their experi-
ments (Wall Street Journal articles).
When the coreference resolution system is used,
the best accuracy value decreases while the inser-
tion score increases from 0.114 to 0.138 (Table 4).
Therefore, coreference resolution tends to asso-
ciate positions that are closer to the original ones.
4.2 Summary Coherence Rating
To reconfirm the hypothesis that our model can es-
timate the local coherence of a textual document,
we perform a second experiment, summary co-
herence rating. To this end, we apply our model
on the corpus used and proposed by Barzilay and
Lapata (2008). As the objective of our model is
to estimate the coherence of a summary, we pre-
fer this dataset to other summarization evaluation
task corpora, as these account for other dimen-
sions of the summaries: content selection, fluency,
etc. Starting with a pair of summaries, one slightly
more coherent than the other, the objective of the
task is to order the two summaries according to
local coherence.
4.2.1 Experimental Settings
For the summary coherence rating experiment,
pairs to be ordered are composed of summaries
extracted from the Document Understanding Con-
ference (DUC 2003). Summaries, provided either
by humans or by automatic systems, were judged
by seven humans annotators and associated with
a coherence score (for more details on this score
see Barzilay and Lapata (2008)). 80 pairs were
then created, each of these being composed by two
summaries of a same document where the score
of one of the summaries is significantly higher
than the score of the second one. Even though all
summaries are of approximately the same length
(114.2 words on average), their sentence length
can vary considerably. Indeed, more coherent
summaries tend to have more sentences and con-
tain less entities.
For evaluation purposes, the accuracy still cor-
responds to the number of correct ratings di-
vided by the number of comparisons, while the F-
measure combines recall and precision measures.
As before, significance is tested with the Student?s
t-test accounting for the Bonferroni correction.
4.2.2 Results
Table 5 compares the results reported by Barzilay
and Lapata (2008) on the exact same corpus with
the results obtained with our system. It shows that
98
Acc. F Acc. F
B&L 0.833
wo coref w coref
PU 0.800 0.815 0.700 0.718
PW 0.613 0.613 0.538 0.548
PAcc 0.700 0.704 0.638 0.638
PU , Dist 0.650 0.658 0.550 0.557
PW , Dist 0.525 0.525 0.513 0.513
PAcc, Dist 0.700 0.700 0.588 0.588
Table 5: Summary Coherence Rating, reported re-
sults from Barzilay and Lapata (2008) vs. graph-
based
our system gives results comparable to those ob-
tained by Barzilay and Lapata (2008).
This table also shows that, contrary to sentence
ordering task, accounting for the distance between
two sentences (Dist) tends to decrease the results.
This difference is explained by the fact that a man-
ual summary, usually considered as more coher-
ent by humans annotators, tends to contain more
(and shorter) sentences than an automatic one. As
adding distance information decreases the value of
our local coherence score, our graph-based model
gives better results without it.
Moreover, in contrast to the first experiment,
when accounting for the number of entities
?shared? by two sentences (PW ), values of accu-
racy and F-measure are lower. We explain this
behaviour by the number of sentences contained
in the less coherent documents. Indeed, they are
composed by a smaller number of sentences but
contain more entities on average. This means that,
in these documents, two sentences tend to share
a larger number of entities and therefore have a
higher local coherence score when the PW projec-
tion graph is used.
When combined with distance information,
syntactic information still improves the results
(PAcc), though not significantly, but does not lead
to the best results for this task.
Finally, Table 5 also shows that using a coref-
erence resolution system for document represen-
tation does not improve the performance of our
system. We believe that, as mentioned by Barzi-
lay and Lapata (2008), this degradation is related
to the fact that automatic summarization systems
do not use anaphoric expressions which makes the
coreference resolution system useless in this case.
With our graph-based model, the best results are
obtained by the baseline (PU ), and experiments
show that adding information about distance or
syntax does not help in this context. It seems
therefore necessary to integrate information that is
more appropriate to summaries. Although making
the model more appropriate for a specific task is
out of the scope of this paper, our model is flex-
ible and accounting for information about genre
differences or sentence length, by adding weights
in the graph-based representation of the document,
is feasible without any modification of the model.
4.3 Readability Assessment
Barzilay and Lapata (2008) argue that grid models
are domain and style dependent. Therefore they
proposed a readability assessment task to test if the
entity grid model can be used for style classifica-
tion. They combined their model with Schwarm
and Ostendorf?s (2005) readability features and
use Support Vector Machines to classify docu-
ments in two categories. With the same intention,
we evaluate the ability of our model to differenti-
ate ?easy to read? documents from difficult ones.
4.3.1 Experimental Settings
The objective of the readability assessment task
is to evaluate how difficult to read a document is.
We perform this task on the data used by Barzilay
and Lapata (2008), a corpus collected originally
by Barzilay and Elhadad (2003) from the Ency-
clopedia Britannica and its version for children,
the Britannica Elementary. Both versions contain
107 articles. In Encyclopedia Britannica, docu-
ments are composed by an average of 83.1 sen-
tences while they contain 36.6 sentences in Bri-
tannica Elementary. Although these texts are not
explicitly annotated with grade levels, they repre-
sent two broad readability categories.
In order to estimate the complexity of a doc-
ument, our model computes the local coherence
score for each article in the two categories. The
article associated with the higher score is consid-
ered to be the more readable as it is more coherent,
needing less interpretation from the reader than a
document associated with a lower local coherence
score. Values presented in the following section
correspond to accuracy, where the system is cor-
rect if it assigns the higher local coherence score to
the most ?easy to read? document, and F-measure.
99
Acc. F Acc. F
S&O 0.786
B&L 0.509
B&L + S&O 0.888
wo coref w coref
PU 0.589 0.589 0.374 0.374
PW 0.579 0.579 0.383 0.383
PAcc 0.645 0.645 0.421 0.421
PU , Dist 0.589 0.589 0.280 0.280
PW , Dist 0.570 0.570 0.290 0.290
PAcc, Dist 0.766 0.766 0.308 0.308
Table 6: Readability, reported results from Barzi-
lay and Lapata (2008) vs. graph-based (S&O:
Schwarm and Ostendorf (2005))
4.3.2 Results
In order to compare our results with those reported
by Barzilay and Lapata (2008), entities used for
the graph-based representation are discourse enti-
ties that head NPs.
Table 6 shows that, for this task, syntactic in-
formation plays a dominant role (PAcc). A sta-
tistically significant improvement is provided by
including syntactic information. It gives more
weight to subject entities that are more numerous
in the Britannica Elementary documents which
are composed by simpler and shorter sentences.
Finally, when distance is accounted for together
with syntactic information, the accuracy is signif-
icantly improved (p < 0.01) with regard to the re-
sults obtained with syntactic information only.
Table 6 also shows that when the number of en-
tities ?shared? by two sentences is accounted for
(PW ), the results are lower. Indeed, Encyclope-
dia Britannica documents are composed by longer
sentences, that contain a higher number of enti-
ties. This increases the local coherence value of
difficult documents more than the value of ?easy
to read? documents, that contain less entities.
When our graph-based representation used the
coreference resolution system, unlike the observa-
tion of Barzilay and Lapata (2008), the results of
our model decrease significantly. The poor perfor-
mance of our system in this case can be explained
by the fact that the coreference resolution system
regroups more entities in Encyclopedia Britannica
documents than in Britannica Elementary ones.
Therefore, the number of entities that are ?shared?
by two sentences increases more importantly in
the Encyclopedia Britannica corpus, while the dis-
tance between two occurrences of one entity de-
creases in a more significant manner. For these
reasons, the coherence scores associated with ?dif-
ficult to read? documents tend to be higher when
coreference resolution is performed on our data,
which reduces the performance of our system. As
before, syntactic information leads to the best re-
sults, but does not allow the accuracy to be higher
than random anymore.
Compared to the results provided by Barzi-
lay and Lapata (2008) with the entity grid model
alone, our representation outperforms their model
significantly. We believe that this difference is
caused by how syntactic information is introduced
in the document representation and by the fact
that our system can deal with entities that appear
throughout the whole document while the entity
grid model only looks at entities within a three
sentences windows. Our model which captures
exclusively local coherence is almost on par with
the results reported for Schwarm & Ostendorf?s
(2005) system which relies on a wide range of lex-
ical, syntactic and semantic features. Only when
Barzilay and Lapata (2008) combine the entity
grid with Schwarm & Ostendorf?s features they
reach performance considerably better than ours.
In addition to the experiments proposed by
Barzilay and Lapata (2008), we used a third read-
ability category, the Britannica Student, that con-
tains articles targeted for youths (from 11 to 14
years old). These documents, which are quite sim-
ilar to the Encyclopedia Britannica ones, are com-
posed by an average of 44.1 sentences. As we
were only able to find 99 articles out of the 107
original ones in this category, sub corpora of the
three categories were used for the comparison with
the Britannica Student articles.
Table 7 shows the results obtained for the com-
parisons between the two first categories and the
Britannica Student articles. As previously, coref-
erence resolution tends to lower the results, there-
fore only values obtained without coreference res-
olution are reported in the table.
When articles from Britannica Student are com-
pared to articles extracted from Encyclopedia Bri-
tannica, Table 7 shows that the different param-
eters have the same influence as for comparing
between Encyclopedia Britannica and Britannica
Elementary: statistically significant improvement
with syntactic information, higher values when
distance is taken into account, etc. However, it
100
Brit. vs. Stud. Stud. vs. Elem.
Acc. F Acc. F
PU 0.444 0.444 0.667 0.667
PW 0.434 0.434 0.636 0.636
PAcc 0.465 0.465 0.707 0.707
PU , Dist 0.475 0.475 0.646 0.646
PW , Dist 0.485 0.485 0.616 0.616
PAcc, Dist 0.556 0.556 0.657 0.657
Table 7: Readability, comparison between Ency-
clopedia Britannica, Britannica Elementary and
Britannica Student
can also be seen that accuracy and F-measure are
lower for comparing these two corpora. This is
probably due to the stylistic difference between
these two kinds of articles, which is less signifi-
cant than the difference between articles from En-
cyclopedia Britannica and Britannica Elementary.
Concerning the comparison between Britannica
Student and Britannica Elementary articles, Ta-
ble 7 shows that integrating distance information
gives slightly different results and tends to de-
crease the values of accuracy and F-measure. This
is explained by the fact that Britannica Elementary
documents contain fewer entities than Britannica
Student articles. As the length of the two kinds of
articles is similar, distance between entities in Bri-
tannica Elementary documents is more important.
As a result, accounting for distance information
lowers the local coherence values for the more co-
herent document, which reduces the performance
of our model. As previously, syntactic information
improves the results and, for this comparison, the
best result is obtained when syntactic information
alone is accounted for. This leads to an accuracy
which is almost equal to the one when comparing
Encyclopedia Britannica and Britannica Elemen-
tary (0.707 against 0.766).
These two additional experiments show that our
model is style dependent. It obtains better results
when it has to distinguish between Encyclopedia
Britannica and Britannica Elementary or Britan-
nica Student and Britannica Elementary articles
which present a more important difference from
a stylictic point of view than articles from Ency-
clopedia Britannica and Britannica Elementary.
5 Conclusions
In this paper, we proposed an unsupervised and
computationally efficient graph-based local coher-
ence model. Experiments show that our model is
robust among tasks and domains, and reaches rea-
sonable results for three tasks with the same pa-
rameter values and settings (i.e. accuracy values
of 0.889, 0.70 and 0.766 for sentence ordering,
summary coherence rating and readability assess-
ment tasks respectively (PAcc, Dist)). Moreover,
our model can be optimized and obtains results
comparable with entity grid based methods when
proper settings are used for each task.
Our model has two main advantages over the
entity grid model. First, as the graph used for doc-
ument representation contains information about
entity transitions, our model does not need a learn-
ing phase. Second, as it relies only on graph cen-
trality, our model does not suffer from the com-
putational complexity and data sparsity problems
mentioned by Barzilay and Lapata (2008).
Our current model leaves space for improve-
ment. Future work should first investigate the inte-
gration of information about entities. Indeed, our
model only uses entities as indications of sentence
connection although it has been shown that distin-
guishing important from unimportant entities, ac-
cording to their named-entity category, has a pos-
itive impact on local coherence computation (El-
sner and Charniak, 2011). Moreover, future work
should also examine the use of discourse relation
information, as proposed in (Lin et al, 2011). This
can be easily done by adding edges in the projec-
tion graphs when sentences contain entities related
from a discourse point of view while Lin et al?s
approach suffers from complexity and data spar-
sity problems similar to the entity grid model.
Finally, these promising results on local coher-
ence modeling make us believe that our graph-
based representation can be used without much
modification for other tasks, e.g. extractive sum-
marization or topic segmentation. This could be
achieved with link analysis algorithms such as
PageRank, that decide on the importance of a (sen-
tence) node within a graph based on global infor-
mation recursively drawn from the entire graph.
Acknowledgments. This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a
HITS postdoctoral scholarship. We would like
to thank Mirella Lapata and Regina Barzilay for
making their data available and Micha Elsner for
providing his toolkit.
101
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, Sapporo,
Japan, 11?12 July 2003, pages 25?32.
Regina Barzilay and Mirella Lapata. 2005. Model-
ing local coherence: An entity-based approach. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
Mich., 25?30 June 2005, pages 141?148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Susan E. Brennan, Marilyn W. Friedman, and Carl J.
Pollard. 1987. A centering approach to pronouns.
In Proceedings of the 25th Annual Meeting of the As-
sociation for Computational Linguistics, Stanford,
Cal., 6?9 July 1987, pages 155?162.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in
student essays. In Proceedings of Human Language
Technologies 2010: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, Los Angeles, Cal., 2?4 June
2010, pages 681?684.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006, pages 449?454.
Micha Elsner and Eugene Charniak. 2008.
Coreference-inspired coherence modeling. In
Proceedings ACL-HLT 2008 Conference Short
Papers, Columbus, Ohio, 15?20 June 2008, pages
41?44.
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the ACL 2011 Conference Short Papers,
Portland, Oreg., 19?24 June 2011, pages 125?129.
Micha Elsner, Joseph Austerweil, and Eugene Char-
niak. 2007. A unified local and global model
for discourse coherence. In Proceedings of Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics, Rochester, N.Y., 22?
27 April 2007, pages 436?443. Read this version:
http://www.cs.brown.edu/ melsner/order.pdf.
Katja Filippova and Michael Strube. 2007. Ex-
tending the entity-grid coherence model to seman-
tically related entities. In Proceedings of the 11th
European Workshop on Natural Language Genera-
tion, Schloss Dagstuhl, Germany, 17?20 June 2007,
pages 139?142.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203?225.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the 8th
International Conference on Knowledge Discovery
and Data Mining, Edmonton, Alberta, Canada, 23?
26 July 2002, pages 133?142.
Nikiforos Karamanis, Chris Mellish, Massimo Poesio,
and Jon Oberlander. 2009. Evaluating centering for
information ordering using corpora. Computational
Linguistics, 35(1):29?46.
Rodger Kibble and Richard Power. 2004. Optimizing
referential coherence in text generation. Computa-
tional Linguistics, 30(4):401?416.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics, Portland, Oreg., 19?24 June 2011, pages
997?1006.
Sebastian Martschat, Jie Cai, Samuel Broscheit, ?Eva
Mu?jdricza-Maydt, and Michael Strube. 2012. A
multigraph model for coreference resolution. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012, pages 100?106.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, Uppsala,
Sweden, 11?16 July 2010, pages 1562?1572.
Rada Mihalcea and Dragomir Radev. 2011. Graph-
based Natural Language Processing and Informa-
tion Retrieval. Cambridge Univ. Press, Cambridge,
U.K.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25?55.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsuper-
vised word sense disambiguation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(4):678?692.
Mark E.J. Newman. 2010. Networks: An Introduction.
Oxford University Press, New York, N.Y.
Massimo Poesio, Rosemary Stevenson, Barbara Di Eu-
genio, and Janet Hitzeman. 2004. Centering: A
parametric theory and its instantiations. Computa-
tional Linguistics, 30(3). 309-363.
Sameer Pradhan, Alessandro Moschitti, and Nianwen
Xue. 2012. CoNLL-2012 Shared Task: Modeling
multilingual unrestricted coreference in OntoNotes.
102
In Proceedings of the Shared Task of the 16th Con-
ference on Computational Natural Language Learn-
ing, Jeju Island, Korea, 12?14 July 2012, pages 1?
40.
Sarah E. Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, Mich., 25?30 June
2005, pages 523?530.
Radu Soricut and Daniel Marcu. 2006. Discourse gen-
eration using utility-trained coherence models. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics,
Sydney, Australia, 17?21 July 2006, pages 1105?
1112.
103
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30?35,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Scoring Coreference Partitions of Predicted Mentions:
A Reference Implementation
Sameer Pradhan
1
, Xiaoqiang Luo
2
, Marta Recasens
3
,
Eduard Hovy
4
, Vincent Ng
5
and Michael Strube
6
1
Harvard Medical School, Boston, MA,
2
Google Inc., New York, NY
3
Google Inc., Mountain View, CA,
4
Carnegie Mellon University, Pittsburgh, PA
5
HLTRI, University of Texas at Dallas, Richardson, TX,
6
HITS, Heidelberg, Germany
sameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com,
hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.org
Abstract
The definitions of two coreference scoring
metrics?B
3
and CEAF?are underspeci-
fied with respect to predicted, as opposed
to key (or gold) mentions. Several varia-
tions have been proposed that manipulate
either, or both, the key and predicted men-
tions in order to get a one-to-one mapping.
On the other hand, the metric BLANC was,
until recently, limited to scoring partitions
of key mentions. In this paper, we (i) ar-
gue that mention manipulation for scoring
predicted mentions is unnecessary, and po-
tentially harmful as it could produce unin-
tuitive results; (ii) illustrate the application
of all these measures to scoring predicted
mentions; (iii) make available an open-
source, thoroughly-tested reference imple-
mentation of the main coreference eval-
uation measures; and (iv) rescore the re-
sults of the CoNLL-2011/2012 shared task
systems with this implementation. This
will help the community accurately mea-
sure and compare new end-to-end corefer-
ence resolution algorithms.
1 Introduction
Coreference resolution is a key task in natural
language processing (Jurafsky and Martin, 2008)
aiming to detect the referential expressions (men-
tions) in a text that point to the same entity.
Roughly over the past two decades, research in
coreference (for the English language) had been
plagued by individually crafted evaluations based
on two central corpora?MUC (Hirschman and
Chinchor, 1997; Chinchor and Sundheim, 2003;
Chinchor, 2001) and ACE (Doddington et al,
2004). Experimental parameters ranged from us-
ing perfect (gold, or key) mentions as input for
purely testing the quality of the entity linking al-
gorithm, to an end-to-end evaluation where pre-
dicted mentions are used. Given the range of
evaluation parameters and disparity between the
annotation standards for the two corpora, it was
very hard to grasp the state of the art for the
task of coreference. This has been expounded in
Stoyanov et al (2009). The activity in this sub-
field of NLP can be gauged by: (i) the contin-
ual addition of corpora manually annotated for
coreference?The OntoNotes corpus (Pradhan et
al., 2007; Weischedel et al, 2011) in the general
domain, as well as the i2b2 (Uzuner et al, 2012)
and THYME (Styler et al, 2014) corpora in the
clinical domain would be a few examples of such
emerging corpora; and (ii) ongoing proposals for
refining the existing metrics to make them more
informative (Holen, 2013; Chen and Ng, 2013).
The CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus (Prad-
han et al, 2011; Pradhan et al, 2012) were an
attempt to standardize the evaluation settings by
providing a benchmark annotated corpus, scorer,
and state-of-the-art system results that would al-
low future systems to compare against them. Fol-
lowing the timely emphasis on end-to-end evalu-
ation, the official track used predicted mentions
and measured performance using five coreference
measures: MUC (Vilain et al, 1995), B
3
(Bagga
and Baldwin, 1998), CEAF
e
(Luo, 2005), CEAF
m
(Luo, 2005), and BLANC (Recasens and Hovy,
2011). The arithmetic mean of the first three was
the task?s final score.
An unfortunate setback to these evaluations had
its root in three issues: (i) the multiple variations
of two of the scoring metrics?B
3
and CEAF?
used by the community to handle predicted men-
tions; (ii) a buggy implementation of the Cai and
Strube (2010) proposal that tried to reconcile these
variations; and (iii) the erroneous computation of
30
the BLANC metric for partitions of predicted men-
tions. Different interpretations as to how to com-
pute B
3
and CEAF scores for coreference systems
when predicted mentions do not perfectly align
with key mentions?which is usually the case?
led to variations of these metrics that manipulate
the gold standard and system output in order to
get a one-to-one mention mapping (Stoyanov et
al., 2009; Cai and Strube, 2010). Some of these
variations arguably produce rather unintuitive re-
sults, while others are not faithful to the original
measures.
In this paper, we address the issues in scor-
ing coreference partitions of predicted mentions.
Specifically, we justify our decision to go back
to the original scoring algorithms by arguing that
manipulation of key or predicted mentions is un-
necessary and could in fact produce unintuitive re-
sults. We demonstrate the use of our recent ex-
tension of BLANC that can seamlessly handle pre-
dicted mentions (Luo et al, 2014). We make avail-
able an open-source, thoroughly-tested reference
implementation of the main coreference evalua-
tion measures that do not involve mention manip-
ulation and is faithful to the original intentions of
the proposers of these metrics. We republish the
CoNLL-2011/2012 results based on this scorer, so
that future systems can use it for evaluation and
have the CoNLL results available for comparison.
The rest of the paper is organized as follows.
Section 2 provides an overview of the variations
of the existing measures. We present our newly
updated coreference scoring package in Section 3
together with the rescored CoNLL-2011/2012 out-
puts. Section 4 walks through a scoring example
for all the measures, and we conclude in Section 5.
2 Variations of Scoring Measures
Two commonly used coreference scoring metrics
?B
3
and CEAF?are underspecified in their ap-
plication for scoring predicted, as opposed to key
mentions. The examples in the papers describing
these metrics assume perfect mentions where pre-
dicted mentions are the same set of mentions as
key mentions. The lack of accompanying refer-
ence implementation for these metrics by its pro-
posers made it harder to fill the gaps in the speci-
fication. Subsequently, different interpretations of
how one can evaluate coreference systems when
predicted mentions do not perfectly align with key
mentions led to variations of these metrics that ma-
nipulate the gold and/or predicted mentions (Stoy-
anov et al, 2009; Cai and Strube, 2010). All these
variations attempted to generate a one-to-one map-
ping between the key and predicted mentions, as-
suming that the original measures cannot be ap-
plied to predicted mentions. Below we first pro-
vide an overview of these variations and then dis-
cuss the unnecessity of this assumption.
Coining the term twinless mentions for those
mentions that are either spurious or missing from
the predicted mention set, Stoyanov et al (2009)
proposed two variations to B
3
? B
3
all
and B
3
0
?to
handle them. In the first variation, all predicted
twinless mentions are retained, whereas the lat-
ter discards them and penalizes recall for twin-
less predicted mentions. Rahman and Ng (2009)
proposed another variation by removing ?all and
only those twinless system mentions that are sin-
gletons before applying B
3
and CEAF.? Follow-
ing upon this line of research, Cai and Strube
(2010) proposed a unified solution for both B
3
and
CEAF
m
, leaving the question of handling CEAF
e
as future work because ?it produces unintuitive
results.? The essence of their solution involves
manipulating twinless key and predicted mentions
by adding them either from the predicted parti-
tion to the key partition or vice versa, depend-
ing on whether one is computing precision or re-
call. The Cai and Strube (2010) variation was used
by the CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus, and
by the i2b2 2011 shared task on coreference res-
olution using an assortment of clinical notes cor-
pora (Uzuner et al, 2012).
1
It was later identified
by Recasens et al (2013) that there was a bug in
the implementation of this variation in the scorer
used for the CoNLL-2011/2012 tasks. We have
not tested the correctness of this variation in the
scoring package used for the i2b2 shared task.
However, it turns out that the CEAF metric (Luo,
2005) was always intended to work seamlessly on
predicted mentions, and so has been the case with
the B
3
metric.
2
In a latter paper, Rahman and Ng
(2011) correctly state that ?CEAF can compare par-
titions with twinless mentions without any modifi-
cation.? We will look at this further in Section 4.3.
We argue that manipulations of key and re-
sponse mentions/entities, as is done in the exist-
ing B
3
variations, not only confound the evalu-
ation process, but are also subject to abuse and
can seriously jeopardize the fidelity of the evalu-
1
Personal communication with Andreea Bodnari, and
contents of the i2b2 scorer code.
2
Personal communication with Breck Baldwin.
31
ation. Given space constraints we use an exam-
ple worked out in Cai and Strube (2010). Let
the key contain an entity with mentions {a, b, c}
and the prediction contain an entity with mentions
{a, b, d}. As detailed in Cai and Strube (2010,
p. 29-30, Tables 1?3), B
3
0
assigns a perfect pre-
cision of 1.00 which is unintuitive as the system
has wrongly predicted a mention d as belonging to
the entity. For the same prediction, B
3
all
assigns a
precision of 0.556. But, if the prediction contains
two entities {a, b, d} and {c} (i.e., the mention c
is added as a spurious singleton), then B
3
all
preci-
sion increases to 0.667 which is counter-intuitive
as it does not penalize the fact that c is erroneously
placed in its own entity. The version illustrated in
Section 4.2, which is devoid of any mention ma-
nipulations, gives a precision of 0.444 in the first
scenario and the precision drops to 0.333 in the
second scenario with the addition of a spurious
singleton entity {c}. This is a more intuitive be-
havior.
Contrary to both B
3
and CEAF, the BLANC mea-
sure (Recasens and Hovy, 2011) was never de-
signed to handle predicted mentions. However, the
implementation used for the SemEval-2010 shared
task as well as the one for the CoNLL-2011/2012
shared tasks accepted predicted mentions as input,
producing undefined results. In Luo et al (2014)
we have extended the BLANC metric to deal with
predicted mentions
3 Reference Implementation
Given the potential unintuitive outcomes of men-
tion manipulation and the misunderstanding that
the original measures could not handle twinless
predicted mentions (Section 2), we redesigned the
CoNLL scorer. The new implementation:
? is faithful to the original measures;
? removes any prior mention manipulation,
which might depend on specific annotation
guidelines among other problems;
? has been thoroughly tested to ensure that it
gives the expected results according to the
original papers, and all test cases are included
as part of the release;
? is free of the reported bugs that the CoNLL
scorer (v4) suffered (Recasens et al, 2013);
? includes the extension of BLANC to handle
predicted mentions (Luo et al, 2014).
This is the open source scoring package
3
that
we present as a reference implementation for the
3
http://code.google.com/p/reference-coreference-scorers/
SYSTEM MD MUC B
3
CEAF BLANC CONLL
m e AVERAGE
F
1
F
1
1
F
2
1
F
1
F
3
1
CoNLL-2011; English
lee 70.7 59.6 48.9 53.0 46.1 48.8 51.5
sapena 68.4 59.5 46.5 51.3 44.0 44.5 50.0
nugues 69.0 58.6 45.0 48.4 40.0 46.0 47.9
chang 64.9 57.2 46.0 50.7 40.0 45.5 47.7
stoyanov 67.8 58.4 40.1 43.3 36.9 34.6 45.1
santos 65.5 56.7 42.9 45.1 35.6 41.3 45.0
song 67.3 60.0 41.4 41.0 33.1 30.9 44.8
sobha 64.8 50.5 39.5 44.2 39.4 36.3 43.1
yang 63.9 52.3 39.4 43.2 35.5 36.1 42.4
charton 64.3 52.5 38.0 42.6 34.5 35.7 41.6
hao 64.3 54.5 37.7 41.9 31.6 37.0 41.3
zhou 62.3 49.0 37.0 40.6 35.0 35.0 40.3
kobdani 61.0 53.5 34.8 38.1 34.1 32.6 38.7
xinxin 61.9 46.6 34.9 37.7 31.7 35.0 37.7
kummerfeld 62.7 42.7 34.2 38.8 35.5 31.0 37.5
zhang 61.1 47.9 34.4 37.8 29.2 35.7 37.2
zhekova 48.3 24.1 23.7 23.4 20.5 15.4 22.8
irwin 26.7 20.0 11.7 18.5 14.7 6.3 15.5
CoNLL-2012; English
fernandes 77.7 70.5 57.6 61.4 53.9 58.8 60.7
martschat 75.2 67.0 54.6 58.8 51.5 55.0 57.7
bjorkelund 75.4 67.6 54.5 58.2 50.2 55.4 57.4
chang 74.3 66.4 53.0 57.1 48.9 53.9 56.1
chen 73.8 63.7 51.8 55.8 48.1 52.9 54.5
chunyang 73.7 63.8 51.2 55.1 47.6 52.7 54.2
stamborg 73.9 65.1 51.7 55.1 46.6 54.4 54.2
yuan 72.5 62.6 50.1 54.5 46.0 52.1 52.9
xu 72.0 66.2 50.3 51.3 41.3 46.5 52.6
shou 73.7 62.9 49.4 53.2 46.7 50.4 53.0
uryupina 70.9 60.9 46.2 49.3 42.9 46.0 50.0
songyang 68.8 59.8 45.9 49.6 42.4 45.1 49.4
zhekova 67.1 53.5 35.7 39.7 32.2 34.8 40.5
xinxin 62.8 48.3 35.7 38.0 31.9 36.5 38.6
li 59.9 50.8 32.3 36.3 25.2 31.9 36.1
CoNLL-2012; Chinese
chen 71.6 62.2 55.7 60.0 55.0 54.1 57.6
yuan 68.2 60.3 52.4 55.8 50.2 43.2 54.3
bjorkelund 66.4 58.6 51.1 54.2 47.6 44.2 52.5
xu 65.2 58.1 49.5 51.9 46.6 38.5 51.4
fernandes 66.1 60.3 49.6 54.4 44.5 49.6 51.5
stamborg 64.0 57.8 47.4 51.6 41.9 45.9 49.0
uryupina 59.0 53.0 41.7 46.9 37.6 41.9 44.1
martschat 58.6 52.4 40.8 46.0 38.2 37.9 43.8
chunyang 61.6 49.8 39.6 44.2 37.3 36.8 42.2
xinxin 55.9 48.1 38.8 42.9 34.5 37.9 40.5
li 51.5 44.7 31.5 36.7 25.3 30.4 33.8
chang 47.6 37.9 28.8 36.1 29.6 25.7 32.1
zhekova 47.3 40.6 28.1 31.4 21.2 22.9 30.0
CoNLL-2012; Arabic
fernandes 64.8 46.5 42.5 49.2 46.5 38.0 45.2
bjorkelund 60.6 47.8 41.6 46.7 41.2 37.9 43.5
uryupina 55.4 41.5 36.1 41.4 35.0 33.0 37.5
stamborg 59.5 41.2 35.9 40.0 32.9 34.5 36.7
chen 59.8 39.0 32.1 34.7 26.0 30.8 32.4
zhekova 41.0 29.9 22.7 31.1 25.9 18.5 26.2
li 29.7 18.1 13.1 21.0 17.3 8.4 16.2
Table 1: Performance on the official, closed track
in percentages using all predicted information for
the CoNLL-2011 and 2012 shared tasks.
community to use. It is written in perl and stems
from the scorer that was initially used for the
SemEval-2010 shared task (Recasens et al, 2010)
and later modified for the CoNLL-2011/2012
shared tasks.
4
Partitioning detected mentions into entities (or
equivalence classes) typically comprises two dis-
tinct tasks: (i) mention detection; and (ii) coref-
erence resolution. A typical two-step coreference
algorithm uses mentions generated by the best
4
We would like to thank Emili Sapena for writing the first
version of the scoring package.
32
a     b
c
de fg
h
a     bc
de
hi i
f    g f    g
h i
cd
a     b
Solid: KeyDashed: Response Solid: KeyDashed: partition wrt Response Solid: Partition wrt KeyDashed: Response
Figure 1: Example key and response entities along
with the partitions for computing the MUC score.
possible mention detection algorithm as input to
the coreference algorithm. Therefore, ideally one
would want to score the two steps independently
of each other. A peculiarity of the OntoNotes
corpus is that singleton referential mentions are
not annotated, thereby preventing the computation
of a mention detection score independently of the
coreference resolution score. In corpora where all
referential mentions (including singletons) are an-
notated, the mention detection score generated by
this implementation is independent of the corefer-
ence resolution score.
We used this reference implementation to
rescore the CoNLL-2011/2012 system outputs for
the official task to enable future comparisons with
these benchmarks. The new CoNLL-2011/2012
results are in Table 1. We found that the over-
all system ranking remained largely unchanged for
both shared tasks, except for some of the lower
ranking systems that changed one or two places.
However, there was a considerable drop in the
magnitude of all B
3
scores owing to the combi-
nation of two things: (i) mention manipulation, as
proposed by Cai and Strube (2010), adds single-
tons to account for twinless mentions; and (ii) the
B
3
metric allows an entity to be used more than
once as pointed out by Luo (2005). This resulted
in a drop in the CoNLL averages (B
3
is one of the
three measures that make the average).
4 An Illustrative Example
This section walks through the process of com-
puting each of the commonly used metrics for
an example where the set of predicted mentions
has some missing key mentions and some spu-
rious mentions. While the mathematical formu-
lae for these metrics can be found in the original
papers (Vilain et al, 1995; Bagga and Baldwin,
1998; Luo, 2005), many misunderstandings dis-
cussed in Section 2 are due to the fact that these
papers lack an example showing how a metric is
computed on predicted mentions. A concrete ex-
ample goes a long way to prevent similar misun-
derstandings in the future. The example is adapted
from Vilain et al (1995) with some slight modifi-
cations so that the total number of mentions in the
key is different from the number of mentions in
the prediction. The key (K) contains two entities
with mentions {a, b, c} and {d, e, f, g} and the re-
sponse (R) contains three entities with mentions
{a, b}; {c, d} and {f, g, h, i}:
K =
K
1
? ?? ?
{a, b, c}
K
2
? ?? ?
{d, e, f, g} (1)
R =
R
1
? ?? ?
{a, b}
R
2
? ?? ?
{c, d}
R
3
? ?? ?
{f, g, h, i}. (2)
Mention e is missing from the response, and men-
tions h and i are spurious in the response. The fol-
lowing sections use R to denote recall and P for
precision.
4.1 MUC
The main step in the MUC scoring is creating the
partitions with respect to the key and response re-
spectively, as shown in Figure 1. Once we have
the partitions, then we compute the MUC score by:
R =
?
N
k
i=1
(|K
i
| ? |p(K
i
)|)
?
N
k
i=1
(|K
i
| ? 1)
=
(3? 2) + (4? 3)
(3? 1) + (4? 1)
= 0.40
P =
?
N
r
i=1
(|R
i
| ? |p
?
(R
i
)|)
?
N
r
i=1
(|R
i
| ? 1)
=
(2? 1) + (2? 2) + (4? 3)
(2? 1) + (2? 1) + (4? 1)
= 0.40,
where K
i
is the i
th
key entity and p(K
i
) is the
set of partitions created by intersecting K
i
with
response entities (cf. the middle sub-figure in Fig-
ure 1); R
i
is the i
th
response entity and p
?
(R
i
) is
the set of partitions created by intersectingR
i
with
key entities (cf. the right-most sub-figure in Fig-
ure 1); and N
k
and N
r
are the number of key and
response entities, respectively.
The MUC F
1
score in this case is 0.40.
4.2 B
3
For computing B
3
recall, each key mention is as-
signed a credit equal to the ratio of the number of
correct mentions in the predicted entity contain-
ing the key mention to the size of the key entity to
which the mention belongs, and the recall is just
33
the sum of credits over all key mentions normal-
ized over the number of key mentions. B
3
preci-
sion is computed similarly, except switching the
role of key and response. Applied to the example:
R =
?
N
k
i=1
?
N
r
j=1
|K
i
?R
j
|
2
|K
i
|
?
N
k
i=1
|K
i
|
=
1
7
? (
2
2
3
+
1
2
3
+
1
2
4
+
2
2
4
) =
1
7
?
35
12
? 0.42
P =
?
N
k
i=1
?
N
r
j=1
|K
i
?R
j
|
2
|R
j
|
?
N
r
i=1
|R
j
|
=
1
8
? (
2
2
2
+
1
2
2
+
1
2
2
+
2
2
4
) =
1
8
?
4
1
= 0.50
Note that terms with 0 value are omitted. The B
3
F
1
score is 0.46.
4.3 CEAF
The first step in the CEAF computation is getting
the best scoring alignment between the key and
response entities. In this case the alignment is
straightforward. Entity R
1
aligns with K
1
and R
3
aligns with K
2
. R
2
remains unaligned.
CEAF
m
CEAF
m
recall is the number of aligned mentions
divided by the number of key mentions, and preci-
sion is the number of aligned mentions divided by
the number of response mentions:
R =
|K
1
? R
1
|+ |K
2
? R
3
|
|K
1
|+ |K
2
|
=
(2 + 2)
(3 + 4)
? 0.57
P =
|K
1
? R
1
|+ |K
2
? R
3
|
|R
1
|+ |R
2
|+ |R
3
|
=
(2 + 2)
(2 + 2 + 4)
= 0.50
The CEAF
m
F
1
score is 0.53.
CEAF
e
We use the same notation as in Luo (2005):
?
4
(K
i
, R
j
) to denote the similarity between a key
entity K
i
and a response entity R
j
. ?
4
(K
i
, R
j
) is
defined as:
?
4
(K
i
, R
j
) =
2? |K
i
? R
j
|
|K
i
|+ |R
j
|
.
CEAF
e
recall and precision, when applied to this
example, are:
R =
?
4
(K
1
, R
1
) + ?
4
(K
2
, R
3
)
N
k
=
(2?2)
(3+2)
+
(2?2)
(4+4)
2
= 0.65
P =
?
4
(K
1
, R
1
) + ?
4
(K
2
, R
3
)
N
r
=
(2?2)
(3+2)
+
(2?2)
(4+4)
3
? 0.43
The CEAF
e
F
1
score is 0.52.
4.4 BLANC
The BLANC metric illustrated here is the one in
our implementation which extends the original
BLANC (Recasens and Hovy, 2011) to predicted
mentions (Luo et al, 2014).
Let C
k
and C
r
be the set of coreference links
in the key and response respectively, and N
k
and
N
r
be the set of non-coreference links in the key
and response respectively. A link between a men-
tion pair m and n is denoted by mn; then for the
example in Figure 1, we have
C
k
= {ab, ac, bc, de, df, dg, ef, eg, fg}
N
k
= {ad, ae, af, ag, bd, be, bf, bg, cd, ce, cf, cg}
C
r
= {ab, cd, fg, fh, fi, gh, gi, hi}
N
r
= {ac, ad, af, ag, ah, ai, bc, bd, bf, bg, bh, bi,
cf, cg, ch, ci, df, dg, dh, di}.
Recall and precision for coreference links are:
R
c
=
|C
k
? C
r
|
|C
k
|
=
2
9
? 0.22
P
c
=
|C
k
? C
r
|
|C
r
|
=
2
8
= 0.25
and the coreference F-measure, F
c
? 0.23. Sim-
ilarly, recall and precision for non-coreference
links are:
R
n
=
|N
k
?N
r
|
|N
k
|
=
8
12
? 0.67
P
n
=
|N
k
?N
r
|
|N
r
|
=
8
20
= 0.40,
and the non-coreference F-measure, F
n
= 0.50.
So the BLANC score is
F
c
+F
n
2
? 0.36.
5 Conclusion
We have cleared several misunderstandings about
coreference evaluation metrics, especially when a
response contains imperfect predicted mentions,
and have argued against mention manipulations
during coreference evaluation. These misunder-
standings are caused partially by the lack of il-
lustrative examples to show how a metric is com-
puted on predicted mentions not aligned perfectly
with key mentions. Therefore, we provide detailed
steps for computing all four metrics on a represen-
tative example. Furthermore, we have a reference
implementation of these metrics that has been rig-
orously tested and has been made available to the
public as open source software. We reported new
scores on the CoNLL 2011 and 2012 data sets,
which can serve as the benchmarks for future re-
search work.
Acknowledgments
This work was partially supported by grants
R01LM10090 from the National Library of
Medicine and IIS-1219142 from the National Sci-
ence Foundation.
34
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
LREC, pages 563?566.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In
Proceedings of SIGDIAL, pages 28?36.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Pro-
ceedings of the Sixth IJCNLP, pages 1366?1374,
Nagoya, Japan, October.
Nancy Chinchor and Beth Sundheim. 2003. Mes-
sage understanding conference (MUC) 6. In
LDC2003T13.
Nancy Chinchor. 2001. Message understanding con-
ference (MUC) 7. In LDC2001T02.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ACE) program-tasks, data, and evaluation. In
Proceedings of LREC.
Lynette Hirschman and Nancy Chinchor. 1997. Coref-
erence task definition (v3.0, 13 jul 97). In Proceed-
ings of the 7th Message Understanding Conference.
Gordana Ilic Holen. 2013. Critical reflections on
evaluation practices in coreference resolution. In
Proceedings of the NAACL-HLT Student Research
Workshop, pages 1?7, Atlanta, Georgia, June.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall. Second
Edition.
Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An extension of BLANC to
system mentions. In Proceedings of ACL, Balti-
more, Maryland, June.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT-EMNLP,
pages 25?32.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A Unified Rela-
tional Semantic Representation. International Jour-
nal of Semantic Computing, 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of CoNLL: Shared Task, pages 1?27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of CoNLL: Shared Task, pages 1?40.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of
EMNLP, pages 968?977.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of
ACL, pages 814?824.
Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand Index for coreference eval-
uation. Natural Language Engineering, 17(4):485?
510.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M. Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of SemEval,
pages 1?8.
Marta Recasens, Marie-Catherine de Marneffe, and
Chris Potts. 2013. The life and death of discourse
entities: Identifying singleton mentions. In Pro-
ceedings of NAACL-HLT, pages 627?633.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP, pages
656?664.
William F. Styler, Steven Bethard an Sean Finan,
Martha Palmer, Sameer Pradhan, Piet C de Groen,
Brad Erickson, Timothy Miller, Chen Lin, Guergana
Savova, and James Pustejovsky. 2014. Temporal
annotation in the clinical domain. Transactions of
Computational Linguistics, 2(April):143?154.
Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. Journal of
American Medical Informatics Association, 19(5),
September.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model theo-
retic coreference scoring scheme. In Proceedings of
the 6th Message Understanding Conference, pages
45?52.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Robert Belvin, Sameer Pradhan,
Lance Ramshaw, and Nianwen Xue. 2011.
OntoNotes: A large training corpus for enhanced
processing. In Joseph Olive, Caitlin Christian-
son, and John McCary, editors, Handbook of Natu-
ral Language Processing and Machine Translation:
DARPA Global Autonomous Language Exploitation.
Springer.
35
Dependency Tree Based Sentence Compression
Katja Filippova and Michael Strube
EML Research gGmbH
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We present a novel unsupervised method for
sentence compression which relies on a de-
pendency tree representation and shortens sen-
tences by removing subtrees. An automatic
evaluation shows that our method obtains re-
sult comparable or superior to the state of the
art. We demonstrate that the choice of the
parser affects the performance of the system.
We also apply the method to German and re-
port the results of an evaluation with humans.
1 Introduction
Within the field of text-to-text generation, the sen-
tence compression task can be defined as follows:
given a sentence S, consisting of words w1w2...wn,
what is a subset of the words of S, such that it
is grammatical and preserves essential information
from S? There are many applications which would
benefit from a robust compression system, such as
subtitle generation, compression for mobile devices
with a limited screen size, or news digests. Given
that to date most text and speech summarization
systems are extractive, sentence compression tech-
niques are a common way to deal with redundancy
in their output.
In recent years, a number of approaches to sen-
tence compression have been developed (Jing, 2001;
Knight & Marcu, 2002; Gagnon & Da Sylva, 2005;
Turner & Charniak, 2005; Clarke & Lapata, 2008,
inter alia). Many explicitly rely on a language
model, usually a trigram model, to produce gram-
matical output (Knight & Marcu, 2002; Hori & Fu-
rui, 2004; Turner & Charniak, 2005; Galley & McK-
eown, 2007). Testing the grammaticality of the out-
put with a language model is justified when work-
ing with a language with rigid word order like En-
glish, and all but one approach mentioned have
been applied to English data. However, compress-
ing sentences in languages with less rigid word or-
der needs a deeper analysis to test grammaticality.
And even for languages with rigid word order the
trigram model ignores the structure of the sentence
and therefore may significantly distort the meaning
of the source sentence. Approaches going beyond
the word level either require a comprehensive lexi-
con (Jing, 2001), or manually devised rules (Gagnon
& Da Sylva, 2005; Clarke & Lapata, 2008) to de-
termine prunable constituents. A lexicon is not al-
ways available, whereas the hand-crafted rules may
not cover all cases and are too general to be univer-
sally applicable (e.g. PPs can be pruned).
In this paper we present a novel unsupervised ap-
proach to sentence compression which is motivated
by the belief that the grammaticality of the output
can be better ensured by compressing trees. In par-
ticular, given a dependency tree, we want to prune
subtrees which are neither obligatory syntactic argu-
ments, nor contribute important information to the
content of the sentence. A tree pruning approach
does not generate new dependencies and is unlikely
to produce a compression with a totally different
meaning. Our approach is unsupervised and adapt-
able to other languages, the main requirement be-
ing that there are a dependency parser and a corpus
available for the languages. We test our approach
on English and German data sets and obtain results
comparable or superior to the state of the art.
25
2 Related Work
Many existing compression systems use a noisy-
channel approach and rely on a language model
to test the grammaticality of the output (Knight &
Marcu, 2002; Turner & Charniak, 2005; Galley &
McKeown, 2007). Other ways to ensure gram-
maticality and to decide whether a constituent is
obligatory or may be pruned are to utilize a sub-
categorization lexicon (Jing, 2001), or to define a
set of generally prunable constituents. Gagnon &
Da Sylva (2005) prune dependency trees by remov-
ing prepositional complements of the verb, subordi-
nate clauses and noun appositions. Apparently, this
does not guarantee grammaticality in all cases. It
may also eliminate important information from the
tree.
Most approaches are supervised and require train-
ing data to learn which words or constituents can
be dropped from a sentence (Riezler et al, 2003;
McDonald, 2006). However, it is difficult to obtain
training data. Still, there are few unsupervised meth-
ods. For example, Hori & Furui (2004) introduce
a scoring function which relies on such informa-
tion sources as word significance score and language
model. A compression of a given length which
maximizes the scoring function is then found with
dynamic programming. Clarke & Lapata (2008)
present another unsupervised approach. They for-
mulate the task as an optimization problem and solve
it with integer linear programming. Two scores con-
tribute to their objective function ? a trigram lan-
guage model score and a word significance score.
Additionally, the grammaticality of the output is en-
sured by a handful of linguistic constraints, stating
e.g. which arguments should be preserved.
In this paper we suggest an alternative to the pop-
ular language model basis for compression systems
? a method which compresses dependency trees and
not strings of words. We will argue that our formu-
lation has the following advantages: firstly, the ap-
proach is unsupervised, the only requirement being
that there is a sufficiently large corpus and a depen-
dency parser available. Secondly, it requires neither
a subcategorization lexicon nor hand-crafted rules to
decide which arguments are obligatory. Thirdly, it
finds a globally optimal compression by taking syn-
tax and word importance into account.
3 Dependency Based Compression
Our method compresses sentences in that it removes
dependency edges from the dependency tree of a
sentence. The aim is to preserve dependencies
which are either required for the output to be gram-
matical or have an important word as the dependent.
The algorithm proceeds in three steps: tree transfor-
mation (Section 3.1), tree compression (Section 3.2)
and tree linearization (Section 3.3).
3.1 Tree Transformation
Before a dependency tree is compressed, i.e. be-
fore some of the dependencies are removed, the tree
is modified. We will demonstrate the effect of the
transformations with the sentence below:
(1) He said that he lived in Paris and Berlin
The first transformation (ROOT) inserts an explicit
rootnode (Fig. 1(a)). The result of the second trans-
formation (VERB) is that every inflected verb in the
tree gets an edge originating from the rootnode (Fig.
1(b)). All edges outgoing from the rootnode bear the
s label. Apart from that we remove auxiliary edges
and memorize such grammatical properties as voice,
tense or negation for verbs.
The purpose of the remaining transformations is
to make relations between open-class words more
explicit. We want to decide on pruning an edge
judging from two considerations: (i) how important
for the head this argument is; (ii) how informative
the dependent word is. As an example, consider a
source sentence given in (2). Here, we want to de-
cide whether one prepositional phrase (or both) can
be pruned without making the resulting sentence un-
grammatical.
(2) After some time, he moved to London.
It would not be very helpful to check whether an ar-
gument attached with the label pp is obligatory for
the verb move. Looking at a particular preposition
(after vs. to) would be more enlightening. This
motivates the PREP transformation which removes
prepositional nodes and places them as labels on the
edge from their head to the respective noun (Fig.
1(c)). We also decompose a chain of conjoined ele-
ments (CONJ) and attach each of them to the head of
the first element in the chain with the label the first
26
Paris
Berlinand
he
live
say
he
in
root
s
(a) The source tree after ROOT.
Paris
Berlinand
he
live
say
he
in
root
s s
(b) After VERB
Paris
Berlinand
he
live
say
he
root
s
s
in
(c) After PREP
live
say
he
root
Paris Berlinhe
s
s
in
in
(d) After CONJ
Figure 1: The dependency structure of He said that he lived in Paris and Berlin after the transformations
element attaches to its head with (Fig. 1(d)). This
way we can retain any of the conjoined elements
in the compression and do not have to preserve the
whole sequence of them if we are interested in only
one. This last transformation is not applied to verbs.
3.2 Tree Compression
We formulate the compression task as an optimiza-
tion problem which we solve using integer linear
programming1. Given a transformed dependency
tree (a graph if new edges have been added), we de-
cide which dependency edges to remove. For each
directed dependency edge from head h to word w we
thus introduce a binary variable xlh,w where l stands
for the edge?s label:
xlh,w =
{
1 if the dependency is preserved
0 otherwise
(1)
The goal is to find a subtree which gets the highest
score of the objective function (2) to which both the
1In our implementation we use lp solve (http://
sourceforge.net/projects/lpsolve).
probability of dependencies (P (l|h)) and the impor-
tance of dependent words (I(w)) contribute:
f(X) =
?
x
xlh,w ? P (l|h) ? I(w) (2)
Intuitively, the conditional probabilities prevent us
from removing obligatory dependencies from the
tree. For example, P (subj|work) is higher than
P (with|work), and therefore the subject will be
preserved whereas the prepositional label and thus
the whole PP can be pruned. This way we do
not have to create an additional constraint for every
obligatory argument (e.g. subject or direct object).
Neither do we require a subcategorization lexicon to
look up which arguments are obligatory for a cer-
tain verb. Verb arguments are preserved because the
dependency edges, with which they are attached to
the head, get high scores. Table 1 presents the prob-
abilities of a number of labels given that the head
is study. Table 2 presents the probabilities for their
German counterparts.
Note that if we would not apply the PREP trans-
formation we would not be able to distinguish be-
27
subj dobj in at after with to
0.16 0.13 0.05 0.04 0.01 0.01 0.01
Table 1: Probabilities of subj, d(irect)obj, in, at, after,
with, to given the verb study
subj obja in an nach mit zu
0.88 0.74 0.44 0.42 0.09 0.02 0.01
Table 2: Probabilities of subj, obja(ccusative), in, at, af-
ter, with, to given the verb studieren
tween different prepositions and could only calcu-
late P (pp|studieren) which would not be very in-
formative. The probabilities for English are lower
than those for German because we calculate the con-
ditional probabilities given word lemma. In English,
the part of speech information cannot be induced
from the lemma and thus the set of possible labels
of a node is on average larger than in German.
There are many ways in which word importance,
I(w) can be defined. Here, we use the formula intro-
duced by Clarke & Lapata (2008) which is a modifi-
cation of the significance score of Hori et al (2003):
I(wi) =
l
N ? fi log
FA
Fi
(3)
wi is the topic word (either noun or verb), fi is the
frequency of wi in the document, Fi is the frequency
of wi in the corpus, and FA is the sum of frequencies
of all topic words in the corpus. l is the number of
clause nodes above w and N is the maximum level
of embedding of the sentence w belongs to.
The objective function is subject to constraints of
two kinds. The constraints of the first kind are stuc-
tural and ensure that the preserved dependencies re-
sult in a tree. (4) ensures that each word has one
head at most. (5) ensures connectivity in the tree.
(6) restricts the size of the resulting tree to ? words.
?w ? W,
?
h,l
xlh,w ? 1 (4)
?w ? W,
?
h,l
xlh,w ?
1
|W |
?
u,l
xlw,u ? 0 (5)
?
x
xlh,w ? ? (6)
? is a function of the length of the source sentence
in open-class words. The function is not linear since
the degree of compression increases with the length
of the sentence. The compression rate of human-
generated sentences is about 70% (Clarke & Lapata,
2008)2. To approximate this value, we set the pro-
portion of deleted words to be 20% for short sen-
tences (5-9 non-stop words), this value increases up
to 50% for long sentences (30+ words).
The constraints of the second type ensure the syn-
tactic validity of the output tree and explicitly state
which edges should be preserved. These constraints
can be general as well as conditional. The former
ensure that an edge is preserved if its source node
is retained in the output. Conditional syntactic con-
straints state that an edge has to be preserved if (and
only if) a certain other edge is preserved. We have
only one syntactic constraint which states that a sub-
ordinate conjunction (sc) should be preserved if and
only if the clause it belongs to functions as a sub-
ordinate clause (sub) in the output. If it is taken as
the main clause, the conjunction should be dropped.
In terms of edges, this can be formulated as follows
(7):
?xscw,u, xsubh,w ? xscw,u = 0 (7)
Due to the constraint (4), the compressed subtree
is always rooted in the node added as a result of the
first transformation. A compression of a sentence to
an embedded clause is not possible unless one pre-
serves the structure above the embedded clause. Of-
ten, however, main clauses are less important than an
embedded clause. For example, given the sentence
He said they have to be held in Beirut it is the em-
bedded clause which is informative and to which the
source sentence should be compressed. The purpose
of the VERB modification is to amend exactly this
problem. Having an edge from the rootnode to ev-
ery inflected verb allows us to compress the source
sentence to any clause.
3.3 Tree Linearization
A very simple but reasonable linearization technique
is to present the words of a compressed sentence in
the order they are found in the source sentence. This
method has been applied before and this is how we
2Higher rates correspond to longer compressions.
28
linearize the trees obtained for the English data. Un-
fortunately, this method cannot be directly applied to
German because of the constraints on word order in
this language. One of the rules of German grammar
states that in the main clause the inflected part of the
verb occupies the second position, the first position
being occupied by exactly one constituent. There-
fore, if the sentence initial position in a source sen-
tence is occupied by a constituent which got pruned
off as a result of compression, the verb becomes
the first element of the sentence which results in an
undesirable output. There are linearization meth-
ods developed for German which find an optimal
word order for a sentence (Ringger et al, 2004;
Filippova & Strube, 2007). We use our recent
method to linearize compressed trees.
4 Corpora and Annotation
We apply our method to sentences from two corpora
in English and German. These are presented below.
English Compression Corpus: The English data
we use is a document-based compression cor-
pus from the British National Corpus and
American News Text Corpus which consists of
82 news stories3. We parsed the corpus with
RASP (Briscoe et al, 2006) and with the Stan-
ford PCFG parser (Klein & Manning, 2003).
The output of the former is a set of dependency
relations whereas the latter provides an option
for converting the output into dependency for-
mat (de Marneffe et al, 2006) which we use.
Tu?Ba-D/Z: The German corpus we use is a col-
lection of 1,000 newspaper articles (Telljohann
et al, 2003)4. Sentence boundaries, morphol-
ogy, dependency structure and anaphoric rela-
tions are manually annotated in this corpus.
RASP has been used by Clarke & Lapata (2008)
whose state of the art results we compare with ours.
We use not only RASP but also the Stanford parser
for several reasons. Apart from being accurate, the
latter has an elaborated set of dependency relations
3The corpus is available from http://homepages.
inf.ed.ac.uk/s0460084/data.
4The corpus is available from http://www.sfs.
uni-tuebingen.de/en_tuebadz.shtml.
(48 vs. 15 of RASP) which is not overly large (com-
pared with the 106 grammatical relations of the Link
Parser). This is important for our system which
relies on syntactic information when making prun-
ing decisions. A comparison between the Stanford
parser and two dependency parsers, MiniPar and
Link Parser, showed a decent performance of the for-
mer (de Marneffe et al, 2006). It is also of interest to
see to what extent the choice of the parser influences
the results.
Apart from the corpora listed above, we use the
Tipster corpus to calculate conditional probabilities
of syntactic labels given head lemmas as well as
word significance scores. The significance score
is calculated from the total number of 128 mil-
lion nouns and verbs. Conditional probabilities are
calculated from a much smaller portion of Tipster
(about 6 million tokens). The latter number is com-
parable to the size of the data set we use to com-
pute the probabilities for German. There, we use
a corpus of about 4,000 articles from the German
Wikipedia to calculate conditional probabilities and
significance scores. The corpus is parsed with the
highly accurate CDG parser (Foth & Menzel, 2006)
and has the same dependency format as Tu?Ba-D/Z
(Versley, 2005).
Although all corpora are annotated with depen-
dency relations, there are considerable differences
between the annotation of the English and German
data sets. The phrase to dependency structure con-
version done by the Stanford parser makes the se-
mantic head of the constituent its syntactic head. For
example, in the sentence He is right it is the adjec-
tive right which is the root of the tree. Unlike that,
sentences from the German corpora always have a
verb as the root. To unify the formats, we write a set
of rules to make the verb the root of the tree in all
cases.
5 Evaluation
We evaluate the results automatically as well as with
human subjects. To assess the performance of the
method on the English data, we calculate the F-
measure on grammatical relations. Following Rie-
zler et al (2003), we calculate average precision and
recall as the amount of grammatical relations shared
between the output of our system and the gold stan-
29
dard variant divided over the total number of rela-
tions in the output and in the human-generated com-
pression respectively. According to Clarke & Lapata
(2006), this measure reliably correlates with human
judgements. The results of our evaluation as well as
the state of the art results reported by Clarke & Lap-
ata (2008) (LM+SIG+CONSTR), whose system uses
language model scoring (LM), word significance
score (SIG), and linguistic constraints (CONSTR),
are presented in Table 3. The F-measure reported
by Clarke & Lapata (2008) is calculated with RASP
which their system builds upon. For our system we
present the results obtained on the data parsed with
RASP as well as with the Stanford parser (SP). In
both cases the F-measure is found with RASP in or-
der to allow for a fair comparison between the three
systems. We recalculate the compression rate for the
gold standard ignoring punctuation. On the whole
corpus the compression rate turns out to be slightly
higher than that reported by Clarke & Lapata (2008)
(70.3%).
F-measure compr.rate
LM+SIG+CONSTR 40.5 72.0%
DEP-BASED (RASP) 40.7 49.6%
DEP-BASED (SP) 49.3 69.3%
GOLD - 72.1%
Table 3: Average results on the English corpus
As there are no human-generated compressions
for German data, we evaluate the performance of the
method in terms of grammaticality and importance
by means of an experiment with native speakers. In
the experiment, humans are presented with a source
sentence and its compression which they are asked
to evaluate on two five-point scales. Higher grades
are given to better sentences. Importance represents
the amount of relevant information from the source
sentence retained in the compression. Since our
method does not generate punctuation, the judges
are asked to ignore errors due to missing commas.
Five participants took part in the experiment and
each rated the total of 25 sentences originating from
a randomly chosen newspaper article. Their ratings
as well as the ratings reported by Clarke & Lapata
(2008) on English corpus are presented in Table 4.
grammar importance
LM+SIG+CONSTR 3.76 3.53
DEP-BASED (DE) 3.62 3.21
Table 4: Average results for the German data
6 Discussion
The results on the English data are comparable with
or superior to the state of the art. These were ob-
tained with a single linguistic constraint (7) and
without any elaborated resources which makes our
system adaptable to other languages. This suggests
that tree compression is a better basis for sentence
compression systems than language model-oriented
word deletion.
In order to explain why the choice of parser sig-
nificantly influences the performance of the method,
we calculate the precision P defined as the number
of dependencies shared by a human-generated com-
pression (depc) and the source sentence (deps) di-
vided over the total number of dependencies found
in the compression:
P = |depc ? deps||depc|
(8)
The intuition is that if a parser does not reach high
precision on gold standard sentences, i.e. if it does
not assign similar dependency structures to a source
sentence and its compression, then it is hopeless
to expect it to produce good compression with our
dependency-based method. However, the precision
does not have to be as high as 100% because of,
e.g., changes within a chain of conjoined elements
or appositions. The precision of the two parsers cal-
culated over the compression corpus is presented in
Table 5.
RASP Stanford parser
precision 79.6% 84.3%
Table 5: Precision of the parsers
The precision of the Stanford parser is about 5%
higher than that of RASP. In our opinion, this partly
explains why the use of the Stanford parser increases
the F-measure by 9 points. Another possible reason
for this improvement is that the Stanford parser iden-
tifies three times more dependency relations than
30
RASP and thus allows for finer distinctions between
the arguments of different types.
Another point concerns the compression rates.
The compressions generated with RASP are consid-
erably shorter than those generated with the Stanford
parser. This is mainly due to the fact that the struc-
ture output by RASP is not necessarily a tree or a
connected graph. In such cases only the first subtree
of the sentence is taken as input and compressed.
The results on the German set are not conclu-
sive since the number of human judges is relatively
small. Still, these preliminary results are compara-
ble to those reported for English and thus give us
some evidence that the method can be adapted to
languages other than English. Interestingly, the im-
portance score depends on the grammaticality of the
sentence. A grammatical sentence can convey unim-
portant information but it was never the case that an
ungrammatical sentence got a high rating on the im-
portance scale. Some of the human judges told us
that they had difficulties assigning the importance
score to ungrammatical sentences.
7 Conclusions
We presented a new compression method which
compresses dependency trees and does not rely on a
language model to test grammaticality. The method
is unsupervised and can be easily adapted to lan-
guages other than English. It does not require a
subcategorization lexicon or elaborated hand-crafted
rules to decide which arguments can be pruned and
finds a globally optimal compression taking syn-
tax and word importance into account. We demon-
strated that the performance of the system depends
on the parser and suggested a way to estimate how
well a parser is suited for the compression task. The
results indicate that the dependency-based approach
is an alternative to the language model-based one
which is worth pursuing.
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004). We would like to thank Yan-
nick Versley who helped us convert Tu?Ba-D/Z in the
CDG format and Elke Teich and the three anony-
mous reviewers for their useful comments.
References
Briscoe, Edward, John Carroll & Rebecca Watson
(2006). The second release of the RASP sys-
tem. In Proceedings of the COLING-ACL In-
teractive Presentation Session, Sydney, Australia,
2006, pp. 77?80.
Clarke, James & Mirella Lapata (2006). Models for
sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, 17?21
July 2006, pp. 377?385.
Clarke, James & Mirella Lapata (2008). Global in-
ference for sentence compression: An integer lin-
ear programming approach. Journal of Artificial
Intelligence Research, 31:399?429.
de Marneffe, Marie-Catherine, Bill MacCartney &
Christopher D. Manning (2006). Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalua-
tion, Genoa, Italy, 22?28 May 2006, pp. 449?454.
Filippova, Katja & Michael Strube (2007). Generat-
ing constituent order in German clauses. In Pro-
ceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, Prague,
Czech Republic, 23?30 June 2007, pp. 320?327.
Foth, Kilian & Wolfgang Menzel (2006). Hybrid
parsing: Using probabilistic models as predictors
for a symbolic parser. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, Sydney, Aus-
tralia, 17?21 July 2006, pp. 321?327.
Gagnon, Michel & Lyne Da Sylva (2005). Text
summarization by sentence extraction and syn-
tactic pruning. In Proceedings of Computational
Linguistics in the North East, Gatineau, Que?bec,
Canada, 26 August 2005.
Galley, Michel & Kathleen R. McKeown (2007).
Lexicalized Markov grammars for sentence com-
31
pression. In Proceedings of Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, Rochester, N.Y., 22?27 April
2007, pp. 180?187.
Hori, Chiori & Sadaoki Furui (2004). Speech sum-
marization: An approach through word extraction
and a method for evaluation. IEEE Transactions
on Information and Systems, E87-D(1):15?25.
Hori, Chiori, Sadaoki Furui, Rob Malkin, Hua Yu
& Alex Waibel (2003). A statistical approach to
automatic speech summarization. EURASIP Jour-
nal on Applied Signal Processing, 2:128?139.
Jing, Hongyan (2001). Cut-and-Paste Text Summa-
rization, (Ph.D. thesis). Computer Science De-
partment, Columbia University, New York, N.Y.
Klein, Dan & Christopher D. Manning (2003). Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Annual Meeting of the Association for
Computational Linguistics, Sapporo, Japan, 7?12
July 2003, pp. 423?430.
Knight, Kevin & Daniel Marcu (2002). Summariza-
tion beyond sentence extraction: A probabilistic
approach to sentence compression. Artificial In-
telligence, 139(1):91?107.
McDonald, Ryan (2006). Discriminative sentence
compression with soft syntactic evidence. In
Proceedings of the 11th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics, Trento, Italy, 3?7 April 2006,
pp. 297?304.
Riezler, Stefan, Tracy H. King, Richard Crouch &
Annie Zaenen (2003). Statistical sentence con-
densation using ambiguity packing and stochastic
disambiguation methods for Lexical-Functional
Grammar. In Proceedings of the Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics, Edmonton, Alberta, Canada,
27 May ?1 June 2003, pp. 118?125.
Ringger, Eric, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets & Simon Corston-
Oliver (2004). Linguistically informed statisti-
cal models of constituent structure for ordering
in sentence realization. In Proceedings of the
20th International Conference on Computational
Linguistics, Geneva, Switzerland, 23?27 August
2004, pp. 673?679.
Telljohann, Heike, Erhard W. Hinrichs & Sandra
Ku?bler (2003). Stylebook for the Tu?bingen tree-
bank of written German (Tu?Ba-D/Z). Technical
Report: Seminar fu?r Sprachwissenschaft, Univer-
sita?t Tu?bingen, Tu?bingen, Germany.
Turner, Jenine & Eugene Charniak (2005). Su-
pervised and unsupervised learning for sentence
compression. In Proceedings of the 43rd An-
nual Meeting of the Association for Computa-
tional Linguistics, Ann Arbor, Mich., 25?30 June
2005, pp. 290?297.
Versley, Yannick (2005). Parser evaluation across
text types. In Proceedings of the 4th Workshop
on Treebanks and Linguistic Theories, Barcelona,
Spain, 9-10 December 2005.
32
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 28?36,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Evaluation Metrics For End-to-End Coreference Resolution Systems
Jie Cai and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH
Schlo?-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
(jie.cai|michael.strube)@h-its.org
Abstract
Commonly used coreference resolution
evaluation metrics can only be applied to
key mentions, i.e. already annotated men-
tions. We here propose two variants of the
B3 and CEAF coreference resolution eval-
uation algorithms which can be applied
to coreference resolution systems dealing
with system mentions, i.e. automatically
determined mentions. Our experiments
show that our variants lead to intuitive and
reliable results.
1 Introduction
The coreference resolution problem can be di-
vided into two steps: (1) determining mentions,
i.e., whether an expression is referential and
can take part in a coreferential relationship, and
(2) deciding whether mentions are coreferent or
not. Most recent research on coreference res-
olution simplifies the resolution task by provid-
ing the system with key mentions, i.e. already an-
notated mentions (Luo et al (2004), Denis &
Baldridge (2007), Culotta et al (2007), Haghighi
& Klein (2007), inter alia; see also the task de-
scription of the recent SemEval task on coref-
erence resolution at http://stel.ub.edu/
semeval2010-coref), or ignores an impor-
tant part of the problem by evaluating on key men-
tions only (Ponzetto & Strube, 2006; Bengtson &
Roth, 2008, inter alia). We follow here Stoyanov
et al (2009, p.657) in arguing that such evalua-
tions are ?an unrealistic surrogate for the original
problem? and ask researchers to evaluate end-to-
end coreference resolution systems.
However, the evaluation of end-to-end coref-
erence resolution systems has been inconsistent
making it impossible to compare the results. Nico-
lae & Nicolae (2006) evaluate using the MUC
score (Vilain et al, 1995) and the CEAF algorithm
(Luo, 2005) without modifications. Yang et al
(2008) use only the MUC score. Bengtson & Roth
(2008) and Stoyanov et al (2009) derive variants
from the B3 algorithm (Bagga & Baldwin, 1998).
Rahman & Ng (2009) propose their own variants
of B3 and CEAF. Unfortunately, some of the met-
rics? descriptions are so concise that they leave too
much room for interpretation. Also, some of the
metrics proposed are too lenient or are more sen-
sitive to mention detection than to coreference res-
olution. Hence, though standard corpora are used,
the results are not comparable.
This paper attempts to fill that desideratum by
analysing several variants of the B3 and CEAF al-
gorithms. We propose two new variants, namely
B3sys and CEAFsys, and provide algorithmic de-
tails in Section 2. We describe two experiments in
Section 3 showing that B3sys and CEAFsys lead to
intuitive and reliable results. Implementations of
B3sys and CEAFsys are available open source along
with extended examples1.
2 Coreference Evaluation Metrics
We discuss the problems which arise when apply-
ing the most prevalent coreference resolution eval-
uation metrics to end-to-end systems and propose
our variants which overcome those problems. We
provide detailed analyses of illustrative examples.
2.1 MUC
The MUC score (Vilain et al, 1995) counts
the minimum number of links between mentions
to be inserted or deleted when mapping a sys-
tem response to a gold standard key set. Al-
though pairwise links capture the information
in a set, they cannot represent singleton en-
tities, i.e. entities, which are mentioned only
once. Therefore, the MUC score is not suitable
for the ACE data (http://www.itl.nist.
1http://www.h-its.org/nlp/download
28
gov/iad/mig/tests/ace/), which includes
singleton entities in the keys. Moreover, the MUC
score does not give credit for separating singleton
entities from other chains. This becomes problem-
atic in a realistic system setup, when mentions are
extracted automatically.
2.2 B3
The B3 algorithm (Bagga & Baldwin, 1998) over-
comes the shortcomings of the MUC score. In-
stead of looking at the links, B3 computes preci-
sion and recall for all mentions in the document,
which are then combined to produce the final pre-
cision and recall numbers for the entire output.
For each mention, the B3 algorithm computes a
precision and recall score using equations 1 and 2:
Precision(mi) =
|Rmi ? Kmi |
|Rmi |
(1)
Recall(mi) =
|Rmi ? Kmi |
|Kmi |
(2)
where Rmi is the response chain (i.e. the system
output) which includes the mention mi, and Kmi
is the key chain (manually annotated gold stan-
dard) with mi. The overall precision and recall are
computed by averaging them over all mentions.
Since B3?s calculations are based on mentions,
singletons are taken into account. However, a
problematic issue arises when system mentions
have to be dealt with: B3 assumes the mentions in
the key and in the response to be identical. Hence,
B3 has to be extended to deal with system men-
tions which are not in the key and key mentions
not extracted by the system, so called twinless
mentions (Stoyanov et al, 2009).
2.2.1 Existing B3 variants
A few variants of the B3 algorithm for dealing with
system mentions have been introduced recently.
Stoyanov et al (2009) suggest two variants of the
B3 algorithm to deal with system mentions, B30 and
B3all
2
. For example, a key and a response are pro-
vided as below:
Key : {a b c}
Response: {a b d}
B30 discards all twinless system mentions (i.e.
mention d) and penalizes recall by setting
recallmi = 0 for all twinless key mentions (i.e.
mention c). The B30 precision, recall and F-score
2Our discussion of B30 and B3all is based on the analysis
of the source code available at http://www.cs.utah.
edu/nlp/reconcile/.
Set 1
System 1 key {a b c}
response {a b d}
P R F
B30 1.0 0.444 0.615
B3all 0.556 0.556 0.556
B3r&n 0.556 0.556 0.556
B3sys 0.667 0.556 0.606
CEAFsys 0.5 0.667 0.572
System 2 key {a b c}
response {a b d e}
P R F
B30 1.0 0.444 0.615
B3all 0.375 0.556 0.448
B3r&n 0.375 0.556 0.448
B3sys 0.5 0.556 0.527
CEAFsys 0.4 0.667 0.500
Table 1: Problems of B30
(i.e. F = 2 ? Precision?RecallPrecision+Recall ) for the example are
calculated as:
PrB30 =
1
2 ( 22 + 22 ) = 1.0
RecB30 =
1
3 ( 23 + 23 + 0)
.= 0.444
FB30 = 2 ?
1.0?0.444
1.0+0.444
.= 0.615
B3all retains twinless system mentions. It assigns
1/|Rmi | to a twinless system mention as its preci-
sion and similarly 1/|Kmi | to a twinless key men-
tion as its recall. For the same example above, the
B3all precision, recall and F-score are given by:
PrB3
all
= 13 ( 23 + 23 + 13 )
.= 0.556
RecB3
all
= 13 ( 23 + 23 + 13 )
.= 0.556
FB3
all
= 2 ? 0.556?0.5560.556+0.444
.= 0.556
Tables 1, 2 and 3 illustrate the problems with B30
and B3all. The rows labeled System give the origi-
nal keys and system responses while the rows la-
beled B30, B3all and B3sys show the performance gen-
erated by Stoyanov et al?s variants and the one
we introduce in this paper, B3sys (the row labeled
CEAFsys is discussed in Subsection 2.3).
In Table 1, there are two system outputs (i.e.
System 1 and System 2). Mentions d and e are
the twinless system mentions erroneously resolved
and c a twinless key mention. System 1 is sup-
posed to be slightly better with respect to preci-
sion, because System 2 produces one more spu-
rious resolution (i.e. for mention e ). However,
B30 computes exactly the same numbers for both
systems. Hence, there is no penalty for erroneous
coreference relations in B30, if the mentions do not
appear in the key, e.g. putting mentions d or e in
Set 1 does not count as precision errors. ? B30
is too lenient by only evaluating the correctly ex-
tacted mentions.
29
Set 1 Singletons
System 1 key {a b c}
response {a b d}
P R F
B3all 0.556 0.556 0.556
B3r&n 0.556 0.556 0.556
B3sys 0.667 0.556 0.606
CEAFsys 0.5 0.667 0.572
System 2 key {a b c}
response {a b d} {c}
P R F
B3all 0.667 0.556 0.606
B3r&n 0.667 0.556 0.606
B3sys 0.667 0.556 0.606
CEAFsys 0.5 0.667 0.572
Table 2: Problems of B3all (1)
Set 1 Singletons
System 1 key {a b}
response {a b d}
P R F
B3all 0.556 1.0 0.715
B3r&n 0.556 1.0 0.715
B3sys 0.556 1.0 0.715
CEAFsys 0.667 1.0 0.800
System 2 key {a b}
response {a b d} {i} {j} {k}
P R F
B3all 0.778 1.0 0.875
B3r&n 0.556 1.0 0.715
B3sys 0.556 1.0 0.715
CEAFsys 0.667 1.0 0.800
Table 3: Problems of B3all (2)
B3all deals well with the problem illustrated in
Table 1, the figures reported correspond to in-
tuition. However, B3all can output different re-
sults for identical coreference resolutions when
exposed to different mention taggers as shown in
Tables 2 and 3. B3all manages to penalize erro-
neous resolutions for twinless system mentions,
however, it ignores twinless key mentions when
measuring precision. In Table 2, System 1 and Sys-
tem 2 generate the same outputs, except that the
mention tagger in System 2 also extracts mention
c. Intuitively, the same numbers are expected for
both systems. However, B3all gives a higher preci-
sion to System 2, which results in a higher F-score.
B3all retains all twinless system mentions, as can
be seen in Table 3. System 2?s mention tagger tags
more mentions (i.e. the mentions i, j and k), while
both System 1 and System 2 have identical coref-
erence resolution performance. Still, B3all outputs
quite different results for precision and thus for F-
score. This is due to the credit B3all takes from un-
resolved singleton twinless system mentions (i.e.
mention i, j, k in System 2). Since the metric is ex-
pected to evaluate the end-to-end coreference sys-
tem performance rather than the mention tagging
quality, it is not satisfying to observe that B3all?s
numbers actually fluctuate when the system is ex-
posed to different mention taggers.
Rahman & Ng (2009) apply another variant, de-
noted here as B3r&n. They remove only those twin-
less system mentions that are singletons before ap-
plying the B3 algorithm. So, a system would not
be rewarded by the the spurious mentions which
are correctly identified as singletons during reso-
lution (as has been the case with B3all?s higher pre-
cision for System 2 as can be seen in Table 3).
We assume that Rahman & Ng apply a strategy
similar to B3all after the removing step (this is not
clear in Rahman & Ng (2009)). While it avoids the
problem with singleton twinless system mentions,
B3r&n still suffers from the problem dealing with
twinless key mentions, as illustrated in Table 2.
2.2.2 B3sys
We here propose a coreference resolution evalua-
tion metric, B3sys, which deals with system men-
tions more adequately (see the rows labeled B3sys
in Tables 1, 2, 3, 4 and 5). We put all twinless key
mentions into the response as singletons which en-
ables B3sys to penalize non-resolved coreferent key
mentions without penalizing non-resolved single-
ton key mentions, and also avoids the problem B3all
and B3r&n have as shown in Table 2. All twinless
system mentions which were deemed not coref-
erent (hence being singletons) are discarded. To
calculate B3sys precision, all twinless system men-
tions which were mistakenly resolved are put into
the key since they are spurious resolutions (equiv-
alent to the assignment operations in B3all), which
should be penalized by precision. Unlike B3all,
B3sys does not benefit from unresolved twinless
system mentions (i.e. the twinless singleton sys-
tem mentions). For recall, the algorithm only goes
through the original key sets, similar to B3all and
B3r&n. Details are given in Algorithm 1.
For example, a coreference resolution system
has the following key and response:
Key : {a b c}
Response: {a b d} {i j}
To calculate the precision of B3sys, the key and re-
sponse are altered to:
Keyp : {a b c} {d} {i} {j}
Responsep: {a b d} {i j} {c}
30
Algorithm 1 B3sys
Input: key sets key, response sets response
Output: precision P , recall R and F-score F
1: Discard all the singleton twinless system mentions in
response;
2: Put all the twinless annotated mentions into response;
3: if calculating precision then
4: Merge all the remaining twinless system mentions
with key to form keyp;
5: Use response to form responsep
6: Through keyp and responsep;
7: Calculate B3 precision P .
8: end if
9: if calculating recall then
10: Discard all the remaining twinless system mentions in
response to from responser;
11: Use key to form keyr
12: Through keyr and responser;
13: Calculate B3 recall R
14: end if
15: Calculate F-score F
So, the precision of B3sys is given by:
PrB3sys =
1
6 ( 23 + 23 + 13 + 12 + 12 + 1)
.= 0.611
The modified key and response for recall are:
Keyr : {a b c}
Responser: {a b} {c}
The resulting recall of B3sys is:
RecB3sys =
1
3 ( 23 + 23 + 13 )
.= 0.556
Thus the F-score number is calculated as:
FB3sys = 2 ?
0.611?0.556
0.611+0.556
.= 0.582
B3sys indicates more adequately the performance of
end-to-end coreference resolution systems. It is
not easily tricked by different mention taggers3.
2.3 CEAF
Luo (2005) criticizes the B3 algorithm for using
entities more than one time, because B3 computes
precision and recall of mentions by comparing en-
tities containing that mention. Hence Luo pro-
poses the CEAF algorithm which aligns entities in
key and response. CEAF applies a similarity met-
ric (which could be either mention based or entity
based) for each pair of entities (i.e. a set of men-
tions) to measure the goodness of each possible
alignment. The best mapping is used for calculat-
ing CEAF precision, recall and F-measure.
Luo proposes two entity based similarity met-
rics (Equation 3 and 4) for an entity pair (Ki, Rj)
originating from key, Ki, and response, Rj .
?3(Ki, Rj) = |Ki ? Rj | (3)
?4(Ki, Rj) =
2|Ki ? Rj |
|Ki| + |Rj |
(4)
3Further example analyses can be found in Appendix A.
The CEAF precision and recall are derived from
the alignment which has the best total similarity
(denoted as ?(g?)), shown in Equations 5 and 6.
Precision = ?(g
?)
?
i ?(Ri, Ri)
(5)
Recall = ?(g
?)
?
i ?(Ki,Ki)
(6)
If not specified otherwise, we apply Luo?s ?3(?, ?)
in the example illustrations. We denote the origi-
nal CEAF algorithm as CEAForig.
Detailed calculations are illustrated below:
Key : {a b c}
Response: {a b d}
The CEAForig ?3(?, ?) are given by:
?3(K1, R1) = 2 (K1 : {abc};R1 : {abd})
?3(K1,K1) = 3
?3(R1, R1) = 3
So the CEAForig evaluation numbers are:
PrCEAForig = 23 = 0.667
RecCEAForig = 23 = 0.667
FCEAForig = 2 ? 0.667?0.6670.667+0.667 = 0.667
2.3.1 Problems of CEAForig
CEAForig was intended to deal with key mentions.
Its adaptation to system mentions has not been ad-
dressed explicitly. Although CEAForig theoreti-
cally does not require to have the same number of
mentions in key and response, it still cannot be di-
rectly applied to end-to-end systems, because the
entity alignments are based on mention mappings.
As can be seen from Table 4, CEAForig fails
to produce intuitive results for system mentions.
System 2 outputs one more spurious entity (con-
taining mention i and j) than System 1 does, how-
ever, achieves a same CEAForig precision. Since
twinless system mentions do not have mappings in
key, they contribute nothing to the mapping simi-
larity. So, resolution mistakes for system mentions
are not calculated, and moreover, the precision is
easily skewed by the number of output entities.
CEAForig reports very low precision for system
mentions (see also Stoyanov et al (2009)).
2.3.2 Existing CEAF variants
Rahman & Ng (2009) briefly introduce their
CEAF variant, which is denoted as CEAFr&n
here. They use ?3(?, ?), which results in equal
CEAFr&n precision and recall figures when using
true mentions. Since Rahman & Ng?s experiments
using system mentions produce unequal precision
and recall figures, we assume that, after removing
31
Set 1 Set 2 Singletons
System 1 key {a b c}
response {a b} {c} {i} {j}
P R F
CEAForig 0.4 0.667 0.500
B3sys 1.0 0.556 0.715
CEAFsys 0.667 0.667 0.667
System 2 key {a b c}
response {a b} {i j} {c}
P R F
CEAForig 0.4 0.667 0.500
B3sys 0.8 0.556 0.656
CEAFsys 0.6 0.667 0.632
Table 4: Problems of CEAForig
Set 1 Set 2 Set 3 Singletons
System 1 key {a b c}
response {a b} {i j} {k l} {c}
P R F
CEAFr&n 0.286 0.667 0.400
B3sys 0.714 0.556 0.625
CEAFsys 0.571 0.667 0.615
System 2 key {a b c}
response {a b} {i j k l} {c}
P R F
CEAFr&n 0.286 0.667 0.400
B3sys 0.571 0.556 0.563
CEAFsys 0.429 0.667 0.522
Table 5: Problems of CEAFr&n
twinless singleton system mentions, they do not
put any twinless mentions into the other set. In the
example in Table 5, CEAFr&n does not penalize
adequately the incorrectly resolved entities con-
sisting of twinless sytem mentions. So CEAFr&n
does not tell the difference between System 1 and
System 2. It can be concluded from the examples
that the same number of mentions in key and re-
sponse is needed for computing the CEAF score.
2.3.3 CEAFsys
We propose to adjust CEAF in the same way as
we did for B3sys, resulting in CEAFsys. We put
all twinless key mentions into the response as sin-
gletons. All singleton twinless system mentions
are discarded. For calculating CEAFsys precision,
all twinless system mentions which were mistak-
enly resolved are put into the key. For computing
CEAFsys recall, only the original key sets are con-
sidered. That way CEAFsys deals adequately with
system mentions (see Algorithm 2 for details).
Algorithm 2 CEAFsys
Input: key sets key, response sets response
Output: precision P , recall R and F-score F
1: Discard all the singleton twinless system mentions in
response;
2: Put all the twinless annotated mentions into response;
3: if calculating precision then
4: Merge all the remaining twinless system mentions
with key to form keyp;
5: Use response to form responsep
6: Form Map g? between keyp and responsep
7: Calculate CEAF precision P using ?3(?, ?)
8: end if
9: if calculating recall then
10: Discard all the remaining twinless system mentions in
response to form responser;
11: Use key to form keyr
12: Form Map g? between keyr and responser
13: Calculate CEAF recall R using ?3(?, ?)
14: end if
15: Calculate F-score F
Taking System 2 in Table 4 as an example, key and
response are altered for precision:
Keyp : {a b c} {i} {j}
Responsep: {a b d} {i j} {c}
So the ?3(?, ?) are as below, only listing the best
mappings:
?3(K1, R1) = 2 (K1 : {abc};R1 : {abd})
?3(K2, R2) = 1 (K2 : {i};R2 : {ij})
?3(?, R3) = 0 (R3 : {c})
?3(R1, R1) = 3
?3(R2, R2) = 2
?3(R3, R3) = 1
The precision is thus give by:
PrCEAFsys = 2+1+03+2+1 = 0.6
The key and response for recall are:
Keyr : {a b c}
Responser: {a b} {c}
The resulting ?3(?, ?) are:
?3(K1, R1) = 2(K1 : {abc};R1 : {ab})
?3(?, R2) = 0(R2 : {c})
?3(K1,K1) = 3
?3(R1, R1) = 2
?3(R2, R2) = 1
The recall and F-score are thus calculated as:
RecCEAFsys = 23 = 0.667
FCEAFsys = 2 ? 0.6?0.6670.6+0.667 = 0.632
However, one additional complication arises
with regard to the similarity metrics used by
CEAF. It turns out that only ?3(?, ?) is suitable
for dealing with system mentions while ?4(?, ?)
produces uninituitive results (see Table 6).
?4(?, ?) computes a normalized similarity for
each entity pair using the summed number of men-
tions in the key and the response. CEAF precision
then distributes that similarity evenly over the re-
sponse set. Spurious system entities, such as the
one with mention i and j in Table 6, are not pe-
nalized. ?3(?, ?) calculates unnormalized similar-
ities. It compares the two systems in Table 6 ade-
quately. Hence we use only ?3(?, ?) in CEAFsys.
32
Set 1 Singletons
System 1 key {a b c}
response {a b} {c} {i} {j}
P R F
?4(?, ?) 0.4 0.8 0.533
?3(?, ?) 0.667 0.667 0.667
System 2 key {a b c}
response {a b} {i j} {c}
P R F
?4(?, ?) 0.489 0.8 0.607
?3(?, ?) 0.6 0.667 0.632
Table 6: Problems of ?4(?, ?)
When normalizing the similarities by the num-
ber of entities or mentions in the key (for recall)
and the response (for precision), the CEAF al-
gorithm considers all entities or mentions to be
equally important. Hence CEAF tends to compute
quite low precision for system mentions which
does not represent the system performance ade-
quately. Here, we do not address this issue.
2.4 BLANC
Recently, a new coreference resolution evalua-
tion algorithm, BLANC, has been introduced (Re-
casens & Hovy, 2010). This measure implements
the Rand index (Rand, 1971) which has been orig-
inally developed to evaluate clustering methods.
The BLANC algorithm deals correctly with sin-
gleton entities and rewards correct entities accord-
ing to the number of mentions. However, a ba-
sic assumption behind BLANC is, that the sum of
all coreferential and non-coreferential links is con-
stant for a given set of mentions. This implies that
BLANC assumes identical mentions in key and re-
sponse. It is not clear how to adapt BLANC to sys-
tem mentions. We do not address this issue here.
3 Experiments
While Section 2 used toy examples to motivate our
metrics B3sys and CEAFsys, we here report results
on two larger experiments using ACE2004 data.
3.1 Data and Mention Taggers
We use the ACE2004 (Mitchell et al, 2004) En-
glish training data which we split into three sets
following Bengtson & Roth (2008): Train (268
docs), Dev (76), and Test (107). We use two in-
house mention taggers. The first (SM1) imple-
ments a heuristic aiming at high recall. The second
(SM2) uses the J48 decision tree classifier (Wit-
ten & Frank, 2005). The number of detected men-
tions, head coverage, and accuracy on testing data
SM1 SM2
training mentions 31,370 16,081
twin mentions 13,072 14,179
development mentions 8,045 ?
twin mentions 3,371 ?
test mentions 8,387 4,956
twin mentions 4,242 4,212
head coverage 79.3% 73.3%
accuracy 57.3% 81.2%
Table 7: Mention Taggers on ACE2004 Data
are shown in Table 7.
3.2 Artificial Setting
For the artificial setting we report results on the
development data using the SM1 tagger. To illus-
trate the stability of the evaluation metrics with
respect to different mention taggers, we reduce
the number of twinless system mentions in inter-
vals of 10%, while correct (non-twinless) ones are
kept untouched. The coreference resolution sys-
tem used is the BART (Versley et al, 2008) reim-
plementation of Soon et al (2001). The results are
plotted in Figures 1 and 2.
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0 0.2 0.4 0.6 0.8 1
F-
sc
or
e 
fo
r A
CE
04
 D
ev
el
op
m
en
t D
at
a
Proportion of twinless system mentions used in the experiment
MUC
BCubedsys
BCubed0
BCubedall
BCubedng
Figure 1: Artificial Setting B3 Variants
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0 0.2 0.4 0.6 0.8 1
F-
sc
or
e 
fo
r A
CE
04
 D
ev
el
op
m
en
t D
at
a
Proportion of twinless system mentions used in the experiment
MUC
CEAFsys
CEAForig
CEAFng
Figure 2: Artificial Setting CEAF Variants
33
MUC
R Pr F
Soon (SM1) 51.7 53.1 52.4
Soon (SM2) 49.1 69.9 57.7
Table 8: Realistic Setting MUC
Omitting twinless system mentions from the
training data while keeping the number of cor-
rect mentions constant should improve the corefer-
ence resolution performance, because a more pre-
cise coreference resolution model is obtained. As
can be seen from Figures 1 and 2, the MUC-score,
B3sys and CEAFsys follow this intuition.
B30 is almost constant. It does not take twinless
mentions into account. B3all?s curve, also, has a
lower slope in comparison to B3sys and MUC (i.e.
B3all computes similar numbers for worse models).
This shows that the B3all score can be tricked by
using a high recall mention tagger, e.g. in cases
with the worse models (i.e. ones on the left side
of the figures) which have much more twinless
system mentions. The original CEAF algorithm,
CEAForig, is too sensitive to the input system
mentions making it less reliable. CEAFsys is par-
allel to B3sys. Thus both of our metrics exhibit the
same intuition.
3.3 Realistic Setting
3.3.1 Experiment 1
For the realistic setting we compare SM1 and SM2
as preprocessing components for the BART (Ver-
sley et al, 2008) reimplementation of Soon et al
(2001). The coreference resolution system with
the SM2 tagger performs better, because a better
coreference model is achieved from system men-
tions with higher accuracy.
The MUC, B3sys and CEAFsys metrics have the
same tendency when applied to systems with dif-
ferent mention taggers (Table 8, 9 and 10 and the
bold numbers are higher with a p-value of 0.05,
by a paired-t test). Since the MUC scorer does
not evaluate singleton entities, it produces too low
numbers which are not informative any more.
As shown in Table 9, B3all reports counter-
intuitive results when a system is fed with system
mentions generated by different mention taggers.
B3all cannot be used to evaluate two different end-
to-end coreference resolution systems, because the
mention tagger is likely to have bigger impact than
the coreference resolution system. B30 fails to gen-
erate the right comparison too, because it is too
B3sys B30
R Pr F R Pr F
Soon (SM2) 64.1 87.3 73.9 54.7 91.3 68.4
Bengtson 66.1 81.9 73.1 69.5 74.7 72.0
Table 11: Realistic Setting
lenient by ignoring all twinless mentions.
The CEAForig numbers in Table 10 illustrate the
big influence the system mentions have on preci-
sion (e.g. the very low precision number for Soon
(SM1)). The big improvement for Soon (SM2) is
largely due to the system mentions it uses, rather
than to different coreference models.
Both B3r&n and CEAFr&n show no serious prob-
lems in the experimental results. However, as dis-
cussed before, they fail to penalize the spurious
entities with twinless system mentions adequately.
3.3.2 Experiment 2
We compare results of Bengtson & Roth?s (2008)
system with our Soon (SM2) system. Bengtson &
Roth?s embedded mention tagger aims at high pre-
cision, generating half of the mentions SM1 gen-
erates (explicit statistics are not available to us).
Bengtson & Roth report a B3 F-score for sys-
tem mentions, which is very close to the one for
true mentions. Their B3-variant does not impute
errors of twinless mentions and is assumed to be
quite similar to the B30 strategy.
We integrate both the B30 and B3sys variants into
their system and show results in Table 11 (we can-
not report significance, because we do not have ac-
cess to results for single documents in Bengtson &
Roth?s system). It can be seen that, when different
variants of evaluation metrics are applied, the per-
formance of the systems vary wildly.
4 Conclusions
In this paper, we address problems of commonly
used evaluation metrics for coreference resolution
and suggest two variants for B3 and CEAF, called
B3sys and CEAFsys. In contrast to the variants
proposed by Stoyanov et al (2009), B3sys and
CEAFsys are able to deal with end-to-end systems
which do not use any gold information. The num-
bers produced by B3sys and CEAFsys are able to
indicate the resolution performance of a system
more adequately, without being tricked easily by
twisting preprocessing components. We believe
that the explicit description of evaluation metrics,
as given in this paper, is a precondition for the re-
34
B3sys B30 B3all B3r&n
R Pr F R Pr F R Pr F R Pr F
Soon (SM1) 65.7 76.8 70.8 57.0 91.1 70.1 65.1 85.8 74.0 65.1 78.7 71.2
Soon (SM2) 64.1 87.3 73.9 54.7 91.3 68.4 64.3 87.1 73.9 64.3 84.9 73.2
Table 9: Realistic Setting B3 Variants
CEAFsys CEAForig CEAFr&n
R Pr F R Pr F R Pr F
Soon (SM1) 66.4 61.2 63.7 62.0 39.9 48.5 62.1 59.8 60.9
Soon (SM2) 67.4 65.2 66.3 60.0 56.6 58.2 60.0 66.2 62.9
Table 10: Realistic Setting CEAF Variants
liabe comparison of end-to-end coreference reso-
lution systems.
Acknowledgements. This work has been
funded by the Klaus Tschira Foundation, Hei-
delberg, Germany. The first author has been
supported by a HITS PhD. scholarship. We
would like to thank ?Eva Mu?jdricza-Maydt for
implementing the mention taggers.
References
Bagga, Amit & Breck Baldwin (1998). Algorithms for scor-
ing coreference chains. In Proceedings of the 1st Inter-
national Conference on Language Resources and Evalua-
tion, Granada, Spain, 28?30 May 1998, pp. 563?566.
Bengtson, Eric & Dan Roth (2008). Understanding the value
of features for coreference resolution. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu, Hawaii, 25-27
October 2008, pp. 294?303.
Culotta, Aron, Michael Wick & Andrew McCallum (2007).
First-order probabilistic models for coreference resolu-
tion. In Proceedings of Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics, Rochester,
N.Y., 22?27 April 2007, pp. 81?88.
Denis, Pascal & Jason Baldridge (2007). Joint determination
of anaphoricity and coreference resolution using integer
programming. In Proceedings of Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguistics,
Rochester, N.Y., 22?27 April 2007, pp. 236?243.
Haghighi, Aria & Dan Klein (2007). Unsupervised coref-
erence resolution in a nonparametric Bayesian model. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics, Prague, Czech Republic,
23?30 June 2007, pp. 848?855.
Luo, Xiaoqiang (2005). On coreference resolution perfor-
mance metrics. In Proceedings of the Human Language
Technology Conference and the 2005 Conference on Em-
pirical Methods in Natural Language Processing, Vancou-
ver, B.C., Canada, 6?8 October 2005, pp. 25?32.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla & Salim Roukos (2004). A mention-
synchronous coreference resolution algorithm based on
the Bell Tree. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pp. 136?143.
Mitchell, Alexis, Stephanie Strassel, Shudong Huang &
Ramez Zakhary (2004). ACE 2004 Multilingual Training
Corpus. LDC2005T09, Philadelphia, Penn.: Linguistic
Data Consortium.
Nicolae, Cristina & Gabriel Nicolae (2006). BestCut: A
graph algorithm for coreference resolution. In Proceed-
ings of the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, Sydney, Australia, 22?23 July
2006, pp. 275?283.
Ponzetto, Simone Paolo & Michael Strube (2006). Exploiting
semantic role labeling, WordNet and Wikipedia for coref-
erence resolution. In Proceedings of the Human Language
Technology Conference of the North American Chapter of
the Association for Computational Linguistics, New York,
N.Y., 4?9 June 2006, pp. 192?199.
Rahman, Altaf & Vincent Ng (2009). Supervised models for
coreference resolution. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language Pro-
cessing, Singapore, 6-7 August 2009, pp. 968?977.
Rand, William R. (1971). Objective criteria for the evaluation
of clustering methods. Journal of the American Statistical
Association, 66(336):846?850.
Recasens, Marta & Eduard Hovy (2010). BLANC: Imple-
menting the Rand index for coreference evaluation. Sub-
mitted.
Soon, Wee Meng, Hwee Tou Ng & Daniel Chung Yong
Lim (2001). A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics,
27(4):521?544.
Stoyanov, Veselin, Nathan Gilbert, Claire Cardie & Ellen
Riloff (2009). Conundrums in noun phrase coreference
resolution: Making sense of the state-of-the-art. In Pro-
ceedings of the Joint Conference of the 47th Annual Meet-
ing of the Association for Computational Linguistics and
the 4th International Joint Conference on Natural Lan-
guage Processing, Singapore, 2?7 August 2009, pp. 656?
664.
Versley, Yannick, Simone Paolo Ponzetto, Massimo Poesio,
Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng
Yang & Alessandro Moschitti (2008). BART: A modular
toolkit for coreference resolution. In Companion Volume
to the Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics, Columbus, Ohio,
15?20 June 2008, pp. 9?12.
Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly
& Lynette Hirschman (1995). A model-theoretic corefer-
ence scoring scheme. In Proceedings of the 6th Message
Understanding Conference (MUC-6), pp. 45?52. San Ma-
teo, Cal.: Morgan Kaufmann.
Witten, Ian H. & Eibe Frank (2005). Data Mining: Practical
Machine Learning Tools and Techniques (2nd ed.). San
Francisco, Cal.: Morgan Kaufmann.
Yang, Xiaofeng, Jian Su & Chew Lim Tan (2008). A twin-
candidate model for learning-based anaphora resolution.
Computational Linguistics, 34(3):327?356.
35
A B3sys Example Output
Here, we provide additional examples for analyzing the behavior of B3sys where we systematically vary
system outputs. Since we proposed B3sys for dealing with end-to-end systems, we consider only examples
also containing twinless mentions. The systems in Table 12 and 14 generate different twinless key
mentions while keeping the twinless system mentions untouched. In Table 13 and 15, the number of
twinless system mentions changes through different responses and the number of twinless key mentions
is fixed.
In Table 12, B3sys recall goes up when more key mentions are resolved into the correct set. And the
precision stays the same, because there is no change in the number of the erroneous resolutoins (i.e. the
spurious cluster with mentions i and j). For the examples in Tables 13 and 15, B3sys gives worse precision
to the outputs with more spurious resolutions, and the same recall if the systems resolve key mentions in
the same way. Since the set of key mentions intersects with the set of twinless system mentions in Table
14, we do not have an intuitive explanation for the decrease in precision from response1 to response4.
However, both the F-score and the recall still show the right tendency.
Set 1 Set 2 B3sys
key {a b c d e} P R F
response1 {a b} {i j} 0.857 0.280 0.422
response2 {a b c} {i j} 0.857 0.440 0.581
response3 {a b c d} {i j} 0.857 0.68 0.784
response4 {a b c d e} {i j} 0.857 1.0 0.923
Table 12: Analysis of B3sys 1
Set 1 Set 2 B3sys
key {a b c d e} P R F
response1 {a b c} {i j} 0.857 0.440 0.581
response2 {a b c} {i j k} 0.75 0.440 0.555
response3 {a b c} {i j k l} 0.667 0.440 0.530
response4 {a b c} {i j k l m} 0.6 0.440 0.508
Table 13: Analysis of B3sys 2
Set 1 B3sys
key {a b c d e} P R F
response1 {a b i j} 0.643 0.280 0.390
response2 {a b c i j} 0.6 0.440 0.508
response3 {a b c d i j} 0.571 0.68 0.621
response4 {a b c d e i j} 0.551 1.0 0.711
Table 14: Analysis of B3sys 3
Set 1 B3sys
key {a b c d e} P R F
response1 {a b c i j} 0.6 0.440 0.508
response2 {a b c i j k} 0.5 0.440 0.468
response3 {a b c i j k l} 0.429 0.440 0.434
response4 {a b c i j k l m} 0.375 0.440 0.405
Table 15: Analysis of B3sys 4
36
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 56?60,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Unrestricted Coreference Resolution via Global Hypergraph Partitioning
Jie Cai and ?Eva Mu?jdricza-Maydt and Michael Strube
Natural Language Processing Group
Heidelberg Institute for Theoretical Studies gGmbH
Heidelberg, Germany
(jie.cai|eva.mujdriczamaydt|michael.strube)@h-its.org
Abstract
We present our end-to-end coreference res-
olution system, COPA, which implements a
global decision via hypergraph partitioning.
In constrast to almost all previous approaches,
we do not rely on separate classification and
clustering steps, but perform coreference res-
olution globally in one step. COPA represents
each document as a hypergraph and partitions
it with a spectral clustering algorithm. Various
types of relational features can be easily incor-
porated in this framwork. COPA has partici-
pated in the open setting of the CoNLL shared
task on modeling unrestricted coreference.
1 Introduction
Coreference resolution is the task of grouping men-
tions of entities into sets so that all mentions in
one set refer to the same entity. Most recent ap-
proaches to coreference resolution divide this task
into two steps: (1) a classification step which de-
termines whether a pair of mentions is coreferent or
which outputs a confidence value, and (2) a cluster-
ing step which groups mentions into entities based
on the output of step 1.
In this paper we present an end-to-end corefer-
ence resolution system, COPA, which avoids the di-
vision into two steps and instead performs a global
decision in one step. The system presents a doc-
ument as a hypergraph, where the vertices denote
mentions and the edges denote relational features
between mentions. Coreference resolution is then
performed globally in one step by partitioning the
hypergraph into subhypergraphs so that all mentions
in one subhypergraph refer to the same entity (Cai
and Strube, 2010). COPA assigns edge weights by
applying simple descriptive statistics on the tranin-
ing data. Since COPA does not need to learn an
explicit model, we used only 30% of the CoNLL
shared task training data. We did this not for effi-
ciency reasons, only for convenience.
While COPA has been developed originally to
perform coreference resolution on MUC and ACE
data (Cai and Strube, 2010), the move to the
OntoNotes data (Weischedel et al, 2011) required
mainly to update the mention detector and the fea-
ture set. Since several off-the-shelf preprocessing
components are used, COPA participated in the open
setting of the CoNLL shared task on modeling unre-
stricted coreference (Pradhan et al, 2011). We did
not make extensive use of information beyond infor-
mation from the closed class setting.
2 Preprocessing
COPA is implemented on top of the BART-toolkit
(Versley et al, 2008). Documents are transformed
into the MMAX2-format (Mu?ller and Strube, 2006)
which allows for easy visualization and (linguis-
tic) debugging. Each document is stored in several
XML-files representing different layers of annota-
tions. These annotations are created by a pipeline
of preprocessing components. We use the Stan-
ford MaxentTagger (Toutanova et al, 2003) for part-
of-speech tagging, and the Stanford Named En-
tity Recognizer (Finkel et al, 2005) for annotat-
ing named entities. In order to derive syntactic
information, we use the Charniak/Johnson rerank-
ing parser (Charniak and Johnson, 2005) com-
56
bined with a constituent-to-dependency conversion
Tool (http://nlp.cs.lth.se/software/
treebank_converter). The preprocessing
models are not trained on CoNLL data, so we only
participated in the open task.
We have implemented an in-house mention detec-
tor, which makes use of the parsing output, the part-
of-speech tags, as well as the chunks from the Yam-
cha Chunker (Kudoh and Matsumoto, 2000). For
the OntoNotes data, the mention detector annotates
the biggest noun phrase spans.
3 COPA: Coreference Partitioner
The COPA system consists of modules which derive
hyperedges from features and assign edge weights
indicating a positive correlation with the coreference
relation, and resolution modules which create a hy-
pergraph representation for the testing data and per-
form partitioning to produce subhypergraphs, each
of which represents an entity.
3.1 HyperEdgeCreator
COPA needs training data only for computing the
hyperedge weights. Hyperedges represent features.
Each hyperedge corresponds to a feature instance
modeling a simple relation between two or more
mentions. This leads to initially overlapping sets of
mentions. Hyperedges are assigned weights which
are calculated on the training data as the percentage
of the initial edges being in fact coreferent. Due to
the simple strategy of assigning edge weights, only
a reasonable size of training data is needed.
3.2 Coreference Resolution Modules
Unlike pairwise models, COPA processes a docu-
ment globally in one step, taking care of the pref-
erence information among all the mentions simul-
taneously and clustering them into sets directly. A
document is represented as a single hypergraph with
multiple edges. The hypergraph resolver partitions
the hypergraph into several sub-hypergraphs, each
corresponding to one set of coreferent mentions.
3.2.1 HGModelBuilder
A single document is represented in a hypergraph
with basic relational features. Each hyperedge in a
graph corresponds to an instance of one of those fea-
tures with the weight assigned by the HyperEdge-
Learner. Instead of connecting nodes with the tar-
get relation as usually done in graph models, COPA
builds the graph directly out of low dimensional fea-
tures without assuming a distance metric.
3.2.2 HGResolver
In order to partition the hypergraph we adopt a
spectral clustering algorithm (Agarwal et al, 2005).
All experimental results are obtained using symmet-
ric Laplacians (Lsym) (von Luxburg, 2007).
We apply the recursive variant of spectral clus-
tering, recursive 2-way partitioning (R2 partitioner)
(Cai and Strube, 2010). This method does not need
any information about the number of target sets (the
number k of clusters). Instead a stopping criterion
?? has to be provided which is adjusted on develop-
ment data.
3.3 Complexity of HGResolver
Since edge weights are assigned using simple de-
scriptive statistics, the time HGResolver needs for
building the graph Laplacian matrix is not substan-
tial. For eigensolving, we use an open source library
provided by the Colt project1which implements a
Householder-QL algorithm to solve the eigenvalue
decomposition. When applied to the symmetric
graph Laplacian, the complexity of the eigensolv-
ing is given by O(n3), where n is the number of
mentions in a hypergraph. Since there are only a
few hundred mentions per document in our data, this
complexity is not an issue. Spectral clustering gets
problematic when applied to millions of data points.
4 Features
In our system, features are represented as types of
hyperedges. Any realized edge is an instance of the
corresponding edge type. All instances derived from
the same type have the same weight, but they may
get reweighed by the distance feature (see Cai and
Strube (2010)). We use three types of features:
negative: prevent edges between mentions;
positive: generate strong edges between mentions;
weak: add edges to an existing graph without intro-
ducing new vertices;
1http://acs.lbl.gov/
?
hoschek/colt/
57
In the following subsections we describe the fea-
tures used in our experiments. Some of the fea-
tures described in Cai and Strube (2010) had to be
changed to cope with the OntoNotes data. We also
introduced a few more features (in particular in or-
der to deal with the dialogue section in the data).
4.1 Negative Features
Negative features describe pairwise relations which
are most likely not coreferent. While we imple-
mented this information as weak positive features in
Cai and Strube (2010), here we apply these features
before graph construction as global variables.
When two mentions are connected by a negative
relation, no edges will be built between them in the
graph. For instance, no edges are allowed between
the mention Hillary Clinton and the mention he due
to incompatible gender.
(1) N Gender, (2) N Number: Two mentions do
not agree in gender or number.
(3) N SemanticClass: Two mentions do not
agree in semantic class (only the Object, Date and
Person top categories derived from WordNet (Fell-
baum, 1998) are used).
(4) N Mod: Two mentions have the same syntac-
tic heads, and the anaphor has a pre-modifier which
does not occur in the antecedent and does not con-
tradict the antecedent.
(5) N DSPrn: Two first person pronouns in direct
speeches assigned to different speakers.
(6) N ContraSubjObj: Two mentions are in the
subject and object positions of the same verb, and
the anaphor is a non-possesive pronoun.
4.2 Positive Features
The majority of well studied coreference features
(e.g. Stoyanov et al (2009)) are actually positive
coreference indicators. In our system, the mentions
which participate in positive relations are included
in the graph representation.
(7) StrMatch Npron & (8) StrMatch Pron: Af-
ter discarding stop words, if the strings of mentions
completely match and are not pronouns, they are put
into edges of the StrMatch Npron type. When the
matched mentions are pronouns, they are put into
the StrMatch Pron type edges.
(9) Alias: After discarding stop words, if men-
tions are aliases of each other (i.e. proper names with
partial match, full names and acronyms, etc.).
(10) HeadMatch: If the syntactic heads of men-
tions match.
(11) Nprn Prn: If the antecedent is not a pro-
noun and the anaphor is a pronoun. This feature is
restricted to a sentence distance of 2. Though it is
not highly weighted, it is crucial for integrating pro-
nouns into the graph.
(12) Speaker12Prn: If the speaker of the second
person pronoun is talking to the speaker of the first
person pronoun. The mentions contain only first or
second person pronouns.
(13) DSPrn: If one of the mentions is the subject
of a speak verb, and other mentions are first person
pronouns within the corresponding direct speech.
(14) ReflexivePrn: If the anaphor is a reflexive
pronoun, and the antecedent is subject of the sen-
tence.
(15) PossPrn: If the anaphor is a possesive pro-
noun, and the antecedent is the subject of the sen-
tence or the subclause.
(16) GPEIsA: If the antecedent is a Named Entity
of GPE entity type (i.e. one of the ACE entity type
(NIST, 2004)), and the anaphor is a definite expres-
sion of the same type.
(17) OrgIsA: If the antecedent is a Named En-
tity of Organization entity type, and the anaphor is a
definite expression of the same type.
4.3 Weak Features
Weak features are weak coreference indicators. Us-
ing them as positive features would introduce too
much noise to the graph (i.e. a graph with too many
singletons). We apply weak features only to men-
tions already integrated in the graph, so that weak
information provides it with a richer structure.
(18) W Speak: If mentions occur with a word
meaning to say in a window size of two words.
(19) W Subject: If mentions are subjects.
(20) W Synonym: If mentions are synonymous
as indicated by WordNet.
5 Results
We submitted COPA?s results to the open setting
in the CoNLL shared task on modeling unrestricted
coreference. We used only 30% of the training data
58
(randomly selected) and the 20 features described in
Section 4.
The stopping criterion ?? (see Section 3) is tuned
on development data to optimize the final corefer-
ence scores. A value of 0.06 is chosen for testing.
COPA?s results on development set (which con-
sists of 202 files) and on testing set are displayed in
Table 1 and Table 2 respectively. The Overall num-
bers in both tables are the average scores of MUC,
BCUBED and CEAF (E).
Metric R P F1
MUC 52.69 57.94 55.19
BCUBED 64.26 73.39 68.52
CEAF (M) 54.44 54.44 54.44
CEAF (E) 45.73 40.92 43.19
BLANC 69.78 75.26 72.13
Overall 55.63
Table 1: COPA?s results on CoNLL development set
Metric R P F1
MUC 56.73 58.90 57.80
BCUBED 64.60 71.03 67.66
CEAF (M) 53.37 53.37 53.37
CEAF (E) 42.71 40.68 41.67
BLANC 69.77 73.96 71.62
Overall 55.71
Table 2: COPA?s results on CoNLL testing set
6 Mention Detection Errors
As described in Section 2, our mention detection is
based on automatically extracted information, such
as syntactic parses and basic noun phrase chunks.
Since there is no minimum span information pro-
vided in the OntoNotes data (in constrast to the pre-
vious standard corpus, ACE), exact mention bound-
ary detection is required. A lot of the spurious
mentions in our system are generated due to mis-
matches of ending or starting punctuations, and the
OntoNotes annotation is also not consistent in this
regard. Our current mention detector does not ex-
tract verb phrases. Therefore it misses all the Event
mentions in the OntoNotes corpus.
We are planning to include idiomatic expression
identification into our mention detector, which will
help to avoid detecting a lot of spurious mentions,
such as God in the phrase for God?s sake.
7 COPA Errors
Besides the fact that the current COPA is not resolv-
ing any event coreferences, our in-house mention de-
tector performs weakly in extracting date mentions
too. As a result, the system outputs several spuri-
ous coreference sets, for instance a set containing
the September from the mention 15th September.
A large amount of the recall loss in our system is
due to the lack of the world knowledge. For exam-
ple, COPA does not resolve the mention the Europe
station correctly into the entity Radio Free Europe,
for it has no knowledge that the entity is a station.
Some more difficult coreference phenomena in
OntoNotes data might require a reasoning mecha-
nism. To be able to connect the mention the vic-
tim with the mention the groom?s brother, the event
of the brother being killed needs to be intepreted by
the system.
We also observed from the experiments that the
resolution of the it mentions are quite inaccurate.
Although our mention detector takes care of dis-
carding pleonastic it?s, there are still a lot of them
left which introduce wrong coreference sets. Since
the it?s do not contain enough information by them-
selves, more features exploring their local syntax are
necessary.
8 Conclusions
In this paper we described a coreference resolution
system, COPA, which implements a global decision
in one step via hypergraph partitioning. COPA?s
hypergraph-based strategy is a general preference
model, where the preference for one mention de-
pends on information on all other mentions.
The system implements three types of relational
features ? negative, positive and weak features, and
assigns the edge weights according to the statitics
from the training data. Since the weights are robust
with respect to the amount of training data we used
only 30% of the training data.
Acknowledgements. This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a HITS
PhD. scholarship.
59
References
Sameer Agarwal, Jonwoo Lim, Lihi Zelnik-Manor, Pietro
Perona, David Kriegman, and Serge Belongie. 2005.
Beyond pairwise clustering. In Proceedings of the
IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition (CVPR?05), volume 2,
pages 838?845.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
Proceedings of the 23rd International Conference on
Computational Linguistics, Beijing, China, 23?27 Au-
gust 2010, pages 143?151.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
Ann Arbor, Mich., 25?30 June 2005, pages 173?180.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics, Ann
Arbor, Mich., 25?30 June 2005, pages 363?370.
Taku Kudoh and Yuji Matsumoto. 2000. Use of Support
Vector Machines for chunk identification. In Proceed-
ings of the 4th Conference on Computational Natural
Language Learning, Lisbon, Portugal, 13?14 Septem-
ber 2000, pages 142?144.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197?214. Pe-
ter Lang: Frankfurt a.M., Germany.
NIST. 2004. The ACE evaluation plan:
Evaluation of the recognition of ACE en-
tities, ACE relations and ACE events.
http://www.itl.nist.gov/iad/mig//tests/ace/2004/doc/
ace04-evalplan-v7.pdf.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Shared Task of 15th Conference on Computational
Natural Language Learning, Portland, Oreg., 23?24
June 2011.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference
on Natural Language Processing, Singapore, 2?7 Au-
gust 2009, pages 656?664.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Edmonton, Al-
berta, Canada, 27 May ?1 June 2003, pages 252?259.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Companion Volume to the Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, Columbus, Ohio, 15?20 June 2008, pages
9?12.
Ulrike von Luxburg. 2007. A tutorial on spectral cluster-
ing. Statistics and Computing, 17(4):395?416.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
60
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 100?106,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
A Multigraph Model for Coreference Resolution
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va Mu?jdricza-Maydt, Michael Strube
Natural Language Processing Group
Heidelberg Institute for Theoretical Studies gGmbH
Heidelberg, Germany
(sebastian.martschat|jie.cai|michael.strube)@h-its.org
Abstract
This paper presents HITS? coreference reso-
lution system that participated in the CoNLL-
2012 shared task on multilingual unrestricted
coreference resolution. Our system employs a
simple multigraph representation of the rela-
tion between mentions in a document, where
the nodes correspond to mentions and the
edges correspond to relations between the
mentions. Entities are obtained via greedy
clustering. We participated in the closed tasks
for English and Chinese. Our system ranked
second in the English closed task.
1 Introduction
Coreference resolution is the task of determining
which mentions in a text refer to the same entity.
This paper describes HITS? system for the CoNLL-
2012 Shared Task on multilingual unrestricted coref-
erence resolution, where the goal is to build a system
for coreference resolution in an end-to-end multilin-
gual setting (Pradhan et al, 2012). We participated
in the closed tasks for English and Chinese and fo-
cused on English. Our system ranked second in the
English closed task.
Being conceptually similar to and building upon
Cai et al (2011b), our system is based on a directed
multigraph representation of a document. A multi-
graph is a graph where two nodes can be connected
by more than one edge. In our model, nodes rep-
resent mentions and edges are built from relations
between the mentions. The entities to be inferred
correspond to clusters in the multigraph.
Our model allows for directly representing any
kind of relations between pairs of mentions in a
graph structure. Inference over this graph can har-
ness structural properties and the rich set of encoded
relations. In order to serve as a basis for further
work, the components of our system were designed
to work as simple as possible. Therefore our system
relies mostly on local information between pairs of
mentions.
2 Architecture
Our system is implemented on top of the BART
toolkit (Versley et al, 2008). To compute the coref-
erence clusters in a document, we first extract a set
of mentions M = {m1, . . . ,mn} ordered according
to their position in the text (Section 2.1). We then
build a directed multigraph where the set of nodes
is M and edges are induced by relations between
mentions (Section 2.4). The relations we use in our
system are coreference indicators like string match-
ing or alias (Section 3). For every relation R, we
compute a weight wR using the training data (Sec-
tion 2.3). We then assign the weight wR to any edge
that is induced by the relation R. Depending on dis-
tance and connectivity properties of the graph the
weights may change (Section 2.4.1). Given the con-
structed graph with edge weights, we go through the
mentions according to their position in the text and
perform greedy clustering (Section 2.6). For Chi-
nese, we employ spectral clustering (Section 2.5) as
adopted in Cai et al (2011b) before the greedy clus-
tering step to reduce the number of candidate an-
tecedents for a mention. The components of our sys-
tem are described in the following subsections.
100
2.1 Mention Extraction
Noun phrases are extracted from the provided parse
and named entity annotation layers. For embedded
mentions with the same head, we only keep the men-
tion with the largest span.
2.1.1 English
For English we identify eight different mention
types: common noun, proper noun, personal pro-
noun, demonstrative pronoun, possessive pronoun,
coordinated noun phrase, quantifying noun phrase
(some of ..., 17 of ...) and quantified noun phrase
(the armed men in one of the armed men). The head
for a common noun or a quantified noun is com-
puted using the SemanticHeadFinder from the Stan-
ford Parser1. The head for a proper noun starts at
the first token tagged as a noun until a punctuation,
preposition or subclause is encountered. Coordina-
tions have the CC tagged token as head and quanti-
fying noun phrases have the quantifier as head.
In a postprocessing step we filter out adjectival
use of nations and named entities with semantic
class Money, Percent or Cardinal. We discard men-
tions whose head is embedded in another mention?s
head. Pleonastic pronouns are identified and dis-
carded via a modified version of the patterns used
by Lee et al (2011).
2.1.2 Chinese
For Chinese we detect four mention types: com-
mon noun, proper noun, pronoun and coordination.
The head detection for Chinese is provided by the
SunJurafskyChineseHeadFinder from the Standford
Parser, except for proper nouns whose head is set to
the mention?s rightmost token.
The remaining processing is similar to the men-
tion detection for English.
2.2 Preprocessing
We extract the information in the provided an-
notation layers and transform the predicted con-
stituent parse trees into dependency parse trees.
We work with two different dependency represen-
tations, one obtained via the converter implemented
1http://nlp.stanford.edu/software/
lex-parser.shtml
in Stanford?s NLP suite2, the other using LTH?s
constituent-to-dependency conversion tool3. For
pronouns, we determine number and gender using
tables containing a mapping of pronouns to their
gender and number.
2.2.1 English
For English, number and gender for common
nouns are computed via a comparison of head
lemma to head and using the number and gender
data of Bergsma and Lin (2006). Quantified noun
phrases are always plural. We compute semantic
classes via a WordNet (Fellbaum, 1998) lookup.
2.2.2 Chinese
For Chinese, we simply determine number and
gender by searching for the corresponding desig-
nators, since plural mentions mostly end with ?,
while ?? (sir) and ?? (lady) often suggest gen-
der information. To identify demonstrative and defi-
nite noun phrases, we check whether they start with
a definite/demonstrative indicator (e.g. ? (this) or
? (that)). We use lists of named entities extracted
from the training data to determine named entities
and their semantic class in development and testing
data.
2.3 Computing Weights for Relations
We compute weights for relations using simple de-
scriptive statistics on training documents. Since this
is a robust approach to learning weights for the type
of graph model we employ (Cai et al, 2011b; Cai
et al, 2011a), we use only a fraction of the available
training data. We took a random subset consisting of
around 20% for English and 15% for Chinese of the
training data. For every document in this set and ev-
ery relation R, we go through the set M of extracted
mentions and compute for every pair (mi,mj) with
i > j whether R holds for this pair. The weight wR
for R is then the number of coreferent pairs with R
divided by the number of all pairs with R.
2.4 Graph Construction
The set of relations we employ consists of two sub-
sets: negative relations R? which enforce that no
2http://nlp.stanford.edu/software/
stanford-dependencies.shtml
3http://nlp.cs.lth.se/software/treebank_
converter/
101
edge is built between two mentions, and positive re-
lations R+ that induce edges. Again, we go through
M in a left-to-right fashion. If for two mentions mi,
mj with i > j a negative relation R? holds, no edge
between mi and mj can be built. Otherwise we add
an edge from mi to mj for every positive relation
R+ such that R+(mi,mj) is true. The structure ob-
tained by this construction is a directed multigraph.
We handle copula relations similar to Lee et al
(2011): if mi is this and the pair (mi,mj) is in a
copula relation (like This is the World), we remove
mj and replace mj in all edges involving it by mi.
For Chinese, we handle ?role appositives? as intro-
duced by Haghighi and Klein (2009) analogously.
2.4.1 Assigning Weights to Edges
Initially, any edge (mi,mj) induced by the rela-
tion R has the weight wR computed as described
in Section 2.3. If R is a transitive relation, we di-
vide the weight by the number of mentions con-
nected by this relation. This corresponds to the way
edge weights are assigned during the spectral em-
bedding in Cai et al (2011b). If R is a relation sen-
sitive to distance like compatibility between a com-
mon/proper noun and a pronoun, the weight is al-
tered according to the distance between mi and mj .
2.4.2 An Example
We demonstrate the graph construction by a sim-
ple example. Consider a document consisting of the
following three sentences.
Barack Obama and Nicolas Sarkozy met
in Toronto yesterday. They discussed the
financial crisis. Sarkozy left today.
Let us assume that our system identifies Barack
Obama (m1), Nicolas Sarkozy (m2), Barack Obama
and Nicolas Sarkozy (m3), They (m4) and Sarkozy
(m5) as mentions. We consider these mentions and
the relations N Number, P Nprn Prn, P Alias and
P Subject described in Section 3. The graph con-
structed according to the algorithm described in this
section is displayed in Figure 1.
Observe the effect of the negative relation
N Number: while P Nprn Prn holds for the pair
Barack Obama (m1) and They (m4), the mentions
do not agree in number. Hence N Number holds for
this pair and no edge from m4 to m1 can be built.
m2 m5
m3 m4
P Alias
P Nprn Prn
P Subject
Figure 1: An example graph. Nodes represent mentions,
edges are induced by relations between the mentions.
2.5 Spectral Clustering
For Chinese we apply spectral clustering before the
final greedy clustering phase. In order to be able to
apply spectral clustering, we make the graph undi-
rected and merge parallel edges into one edge, sum-
ming up all weights. Due to the way edge weights
are computed, the resulting undirected simple graph
corresponds to the graph Cai et al (2011b) use as
input to the spectral clustering algorithm. Spectral
clustering is now performed as in Cai et al (2011b).
2.6 Greedy Clustering
To describe our clustering algorithm, we use some
additional terminology: if there exists an edge from
m to n we say that m is a parent of n and that n is a
child of m.
In the last step, we go through the mentions from
left to right. Let mi be the mention in focus. For
English, we consider all children of mi as possible
antecedents. For Chinese we restrict the possible an-
tecedents to all children that are in the same cluster
obtained by spectral clustering.
If mi is a pronoun, we determine mj such that
the sum over all weights of edges from mi to mj is
maximized. We then assign mi and mj to the same
entity. In English, if mi is a parent of a noun phrase
m that embeds mj , we instead assign mi and m to
the same entity.
For Chinese, all other noun phrases are assigned
to the same entity as all their children in the cluster
obtained by spectral clustering. For English, we are
more restrictive: definites and demonstratives are as-
signed to the same cluster as their closest (according
to the position of the mentions in the text) child.
Negative relations may also be applied as con-
straints in this phase. Before assigning mi to the
same entity as a set of mentions C, we check for
102
every m ? C and every negative relation R?
that we want to incorporate as a constraint whether
R?(mi,m) holds. If yes, we do not assign mi to the
same entity as the mentions in C.
2.7 Complexity
Our algorithms for weight computation, graph con-
struction and greedy clustering look at all pairs of
mentions in a document and perform simple calcu-
lations, which leads to a time complexity of O
(
n2
)
per document, where n is the number of mentions
in a document. When performing spectral cluster-
ing, this increases to O
(
n3
)
. Since we deal with
at most a few hundred mentions per document, the
cubic running time is not an issue.
3 Relations
In our system relations serve as templates for build-
ing or disallowing edges between mentions. We
distinguish between positive and negative relations:
negative relations disallow edges between mentions,
positive relations build edges between mentions.
Negative relations can also be used as constraints
during clustering, while positive relations may also
be applied as ?weak? relations: in this case, we only
add the induced edge when the two mentions under
consideration are already included in the graph after
considering all the non-weak relations.
Most of the relations presented here were already
used in our system for last year?s shared task (Cai et
al., 2011b). The set of relations was enriched mainly
to resolve pronouns in dialogue and to resolve pro-
nouns that do not carry much information by them-
selves like it and they.
3.1 Negative Relations
(1) N Gender, (2) N Number: Two mentions do
not agree in gender or number.
(3) N SemanticClass: Two mentions do not agree
in semantic class (only the Object, Date and Per-
son top categories derived from WordNet (Fell-
baum, 1998) are used).
(4) N ItDist: The anaphor is it or they and the sen-
tence distance to the antecedent is larger than
one.
(5) N BarePlural: Two mentions that are both bare
plurals.
(6) N Speaker12Prn: Two first person pronouns
or two second person pronouns with different
speakers, or one first person pronoun and one
second person pronoun with the same speaker.
(7) N DSprn: Two first person pronouns in direct
speech assigned to different speakers.
(8) N ContraSubjObj: Two mentions are in the
subject and object positions of the same verb,
and the anaphor is a non-possessive pronoun.
(9) N Mod: Two mentions have the same syntac-
tic heads, and the anaphor has a pre- or post-
modifier which does not occur in the antecedent
and does not contradict the antecedent.
(10) N Embedding: Two mentions where one em-
beds the other, which is not a reflexive or posses-
sive pronoun.
(11) N 2PrnNonSpeech: Two second person pro-
nouns without speaker information and not in di-
rect speech.
3.2 Positive Relations
(12) P StrMatch Npron, (13) P StrMatch Pron:
After discarding stop words, if the strings of
mentions completely match and are not pro-
nouns, the relation P StrMatch Npron holds.
When the matched mentions are pronouns,
P StrMatch Pron holds.
(14) P HeadMatch: If the syntactic heads of men-
tions match.
(15) P Nprn Prn: If the antecedent is not a pro-
noun and the anaphor is a pronoun. This relation
is restricted to a sentence distance of 1.
(16) P Alias: If mentions are aliases of each other
(i.e. proper names with partial match, full names
and acronyms, etc.).
(17) P Speaker12Prn: If the speaker of the second
person pronoun is talking to the speaker of the
first person pronoun. The mentions contain only
first or second person pronouns.
(18) P DSPrn: If one mention is subject of a speak
verb, and the other mention is a first person pro-
noun within the corresponding direct speech.
(19) P ReflPrn: If the anaphor is a reflexive pro-
noun, and the antecedent is the subject of the
sentence.
103
(20) P PossPrn: If the anaphor is a possessive pro-
noun, and the antecedent is the subject of the
sentence or the subclause.
(21) P GPEIsA: If the antecedent is a Named En-
tity of GPE entity type and the anaphor is a def-
inite expression of the same type.
(22) P PossPrnEmbedding: If the anaphor is a
possessive pronoun and is embedded in the an-
tecedent.
(23) P VerbAgree: If the anaphor is a pronoun and
has the same predicate as the antecedent.
(24) P Subject & (25) P Object: If both mentions
are subjects/objects (applies only if the anaphor
is it or they).
(26) P SemClassPrn: If the anaphor is a pronoun,
the antecedent is not a pronoun, and both have
semantic class Person.
For English, we used all relations except for (21) and
(26). Relations (1), (2) and (10) were incorporated
as constraints during greedy clustering. For Chinese,
we used relations (1) ? (6), (12) ? (15), (21) and (26).
(26) was incorporated as a weak relation.
4 Results
We submitted to the closed tasks for English and
Chinese. The results on the English development
set and testing set are displayed in Table 1 and Table
2 respectively. To indicate the progress we achieved
within one year, Table 3 shows the performance of
our system on the CoNLL ?11 development data set
compared to last year?s results (Cai et al, 2011b).
The Overall number is the average of MUC, B3
and CEAF (E), MD is the mention detection score.
Overall, we gained over 5% F1 some of which can
be attributed to improved mention detection.
Metric R P F1
MD 73.96 75.69 74.81
MUC 64.93 68.69 66.76
B3 68.42 75.77 71.91
CEAF (M) 61.23 61.23 61.23
CEAF (E) 49.61 45.60 47.52
BLANC 77.81 80.75 79.19
Overall 62.06
Table 1: Results on the English CoNLL ?12 development
set
Metric R P F1
MD 74.23 76.10 75.15
MUC 65.21 68.83 66.97
B3 66.50 74.69 70.36
CEAF (M) 59.61 59.61 59.61
CEAF (E) 48.64 44.72 46.60
BLANC 73.29 78.94 75.73
Overall 61.31
Table 2: Results on the English CoNLL ?12 testing set
Metric R P F1 2011 F1
MD 70.84 73.08 71.94 66.28
MUC 60.80 65.09 62.87 55.19
B3 68.37 75.89 71.94 68.52
CEAF (M) 60.42 60.42 60.42 54.44
CEAF (E) 50.40 46.11 48.16 43.19
BLANC 75.44 79.26 77.19 72.13
Overall 60.99 55.63
Table 3: Results on the English CoNLL ?11 development
set compared to Cai et al (2011b)
Table 4 and Table 5 display our results on Chinese
development data and testing data respectively.
Metric R P F1
MD 52.45 71.50 60.51
MUC 45.90 67.07 54.50
B3 58.94 84.26 69.36
CEAF (M) 53.60 53.60 53.60
CEAF (E) 50.73 34.24 40.89
BLANC 66.17 83.11 71.45
Overall 54.92
Table 4: Results on the Chinese CoNLL ?12 development
set
Metric R P F1
MD 48.49 74.02 58.60
MUC 42.71 67.80 52.41
B3 55.37 85.24 67.13
CEAF (M) 51.30 51.30 51.30
CEAF (E) 51.81 32.46 39.92
BLANC 63.96 82.81 69.18
Overall 53.15
Table 5: Results on the Chinese CoNLL ?12 testing set
Because none of our team members has knowl-
edge of the Arabic language we did not attempt to
104
run our system on the Arabic datasets and therefore
our official score for this language is considered to
be 0. The combined official score of our submission
is (0.0 + 53.15 + 61.31)/3 = 38.15. In the closed
task our system was the second best performing sys-
tem for English and the eighth best performing sys-
tem for Chinese.
5 Error analysis
We did not attempt to resolve event coreference and
did not incorporate world knowledge which is re-
sponsible for many recall errors our system makes.
Since we use a simple greedy strategy for clus-
tering that goes through the mentions left-to-right,
errors in clustering propagate, which gives rise to
cluster-level inconsistencies. We observed a drop in
performance when using more negative relations as
constraints. A more sophisticated clustering strat-
egy that allows a more refined use of constraints is
needed.
5.1 English
Our detection of copula and appositive relations is
quite inaccurate, which is why we limit the incor-
poration of copulas to cases where the antecedent is
this and left appositives out.
We aim for high precision regarding the usage of
the negative relation N Modifier. This leads to some
loss in recall. For example, our system does not as-
sign the just-completed Paralympics and the 12-day
Paralympics to the same entity. Such cases require a
more involved reasoning scheme to decide whether
the modifiers are actually contradicting each other.
Non-referring pronouns constitute another source
of errors. While we improved detection of pleonas-
tic it compared to last year?s system, a lot of them
are not filtered out. Our system also does not distin-
guish well between generic and non-generic uses of
you and we, which hurts precision.
5.2 Chinese
Since each Chinese character carries its own mean-
ing, there are multiple ways to express the same en-
tity by combining different characters into a word.
Both syntactic heads and modifiers can be replaced
by similar words or by abbreviated versions. From
??? (outside people) to ???? (outside eth-
nic group) the head is replaced, while from ??
? (Diana) to ?? ?? ? ?? (charming Di
Princess) the name is abbreviated.
Modifier replacement is more difficult to cope
with, our system does not recognize that ?? ?
??? (starting-over counting-votes job) and??
?? (verifying-votes job) are coreferent. It is also
not trivial to separate characters from words (e.g. by
separating ? and ?) to resolve such cases, since
it will introduce too much noise as a consequence.
In order to tackle this problem, a smart scheme to
propagate similarities from partial words to the en-
tire mentions and a knowledge base upon which re-
liable similarities can be retrieved are necessary.
In contrast to English there is no strict enforce-
ment of using definite noun phrases when referring
to an antecedent in Chinese. Both ???? (the
talk) and?? (talk) can corefer with the antecedent
??????????? (Clinton?s talk during
Hanoi election). This makes it very difficult to dis-
tinguish generic expressions from referential ones.
In the submitted version of our system, we simply
ignore the nominal anaphors which do not start with
definite articles or demonstratives.
6 Conclusions
In this paper we presented a graph-based model for
coreference resolution. It captures pairwise relations
between mentions via edges induced by relations.
Entities are obtained by graph clustering. Discrim-
inative information can be incorporated as negative
relations or as constraints during clustering.
We described our system?s architecture and the re-
lations it employs, highlighting differences and sim-
ilarities to our system from last year?s shared task.
Designed to work as a basis for further work, our
system works mainly by exploring the relationship
between pairs of mentions. Due to its modular archi-
tecture, our system can be extended by components
taking global information into account, for example
for weight learning or clustering.
We focused on the closed task for English in
which our system achieved competitive perfor-
mance, being ranked second out of 15 participants.
Acknowledgments. This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first and the second authors have been
supported by a HITS PhD. scholarship.
105
References
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, Sydney, Australia, 17?
21 July 2006, pages 33?40.
Jie Cai, E?va Mu?jdricza-Maydt, Yufang Hou, and Michael
Strube. 2011a. Weakly supervised graph-based coref-
erence resolution for clinical data. In Proceedings of
the 5th i2b2 Shared Tasks and Workshop on Chal-
lenges in Natural Language Processing for Clinical
Data, Washington, D.C., 20-21 October 2011. To ap-
pear.
Jie Cai, E?va Mu?jdricza-Maydt, and Michael Strube.
2011b. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Shared
Task of the 15th Conference on Computational Natu-
ral Language Learning, Portland, Oreg., 23?24 June
2011, pages 56?60.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singapore,
6?7 August 2009, pages 1152?1161.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Shared Task of the 15th Conference on Compu-
tational Natural Language Learning, Portland, Oreg.,
23?24 June 2011, pages 28?34.
Sameer Pradhan, Alessandro Moschitti, and Nianwen
Xue. 2012. CoNLL-2012 Shared Task: Modeling
multilingual unrestricted coreference in OntoNotes. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012. This volume.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Companion Volume to the Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, Columbus, Ohio, 15?20 June 2008, pages
9?12.
106
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 1?5,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Normalized Entity Graph for Computing Local Coherence
Mohsen Mesgar and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
(mohsen.mesgar|michael.strube)@h-its.org
Abstract
Guinaudeau and Strube (2013) introduce a
graph based model to compute local en-
tity coherence. We propose a computa-
tionally efficient normalization method for
these graphs and then evaluate it on three
tasks: sentence ordering, summary coher-
ence rating and readability assessment. In
all tasks normalization improves the re-
sults.
1 Introduction
Guinaudeau and Strube (2013) introduce a graph
based model (henceforth called entity graph) to
compute local entity coherence. Despite being un-
supervised, the entity graph performs on par with
Barzilay and Lapata?s (2005; 2008) supervised en-
tity grid on the tasks of sentence ordering, sum-
mary coherence rating and readability assessment.
The entity graph also overcomes shortcomings of
the entity grid with regard to computational com-
plexity, data sparsity and domain dependence.
The entity graph is a bipartite graph where one
set of nodes represents entities and the other set
of nodes represents the sentences of a document.
Guinaudeau and Strube (2013) apply a one mode
projection on sentence nodes (Newman, 2010) and
then compute the average out-degree of sentence
nodes to determine how coherent a document is.
They describe variants of their entity graph which
take the number of shared entities between sen-
tences and their grammatical functions into ac-
count thus resulting in weighted bipartite graphs
and weighted one mode projections. Here, we
propose to normalize weights for the entity graph.
Normalization allows to include distance between
mentions of the same entity, which improves the
performance on all three tasks thus confirming re-
search in related areas which states that normaliz-
ing weights leads to better performance (Zhou et
al., 2008; Zweig and Kaufmann, 2011).
2 The Entity Graph
The entity graph (Guinaudeau and Strube, 2013),
G = (V,E), represents the relations between sen-
tences and entities in a text, where node set V con-
tains all sentences and entities in a text and E is
the set of all edges between sentences and enti-
ties. Let function w(s
i
, e
j
) indicate the weight of
an edge which connects sentence s
i
and entity e
j
.
If w(s
i
, e
j
) = 1, then this edge indicates that there
is a mention of e
j
in sentence s
i
. In order to real-
ize the insight from Grosz et al. (1995) that certain
syntactic roles are more important than others, the
syntactic role of e
j
in s
i
can be mapped to an inte-
ger value (Guinaudeau and Strube, 2013):
w(s
i
, e
j
) =
{
3 if e
j
is subject in s
i
2 if e
j
is object in s
i
1 otherwise
Figure 1 illustrates a weighted entity graph for
three sentences.
1 3
2 3
2 1 1
3
1 1 1
1
S1 S2 S3
e1 e2 e3 e4 e5 e7e6 e8 e9 e10
1
Figure 1: Weighted entity graph
Three types of one-mode projections capture
relations between sentences, P
U
, P
W
and P
Acc
.
P
U
creates an edge between two sentences if they
share at least one entity. P
W
captures the intu-
ition that the connection between two sentences
is stronger the more entities they share by means
of weighted edges, where the weights equal the
number of entities shared by sentences (Newman,
2004). The third type of projection, P
Acc
, inte-
grates syntactic information in the edge weights
calculated by the following formula:
W
ik
=
?
e?E
ik
w(e, s
i
) ? w(e, s
k
) .
1
Figure 2 shows the three kinds of one-mode pro-
jections used in the entity graph.
S1 S2
S3
S1 S2
S3
S1 S2
S3
1
1
1
2
9
4
P P PU W Acc
Figure 2: One-mode projections
While the entity grid (Barzilay and Lapata,
2008) uses information about sentences which do
not share entities by means of the ?- -? transition,
the entity graph cannot employ this negative in-
formation. Here, we propose a normalization for
the entity graph and its corresponding one-mode
projections which is based on the relative impor-
tance of entities and, in turn, the relative impor-
tance of sentences. Including negative informa-
tion allows to normalize the importance of entities
according to sentence length (measured in terms
of entity mentions), and hence to capture distance
information between mentions of the same entity.
This brings the entity graph closer to Stoddard?s
(1991, p.30) notion of cohesion: ?The relative co-
hesiveness of a text depends on the number of co-
hesive ties [...] and on the distance between the
nodes and their associated cohesive elements.? By
using this information, edge weights are set less
arbitrary which leads to the more sound method
and higher performance in all tasks.
3 Normalized Entity Graph
The entity graph weighs edges by the number of
entities sentences share (P
W
) and which syntactic
functions the entities occupy (P
Acc
). Here we nor-
malize the weights by the number of entities in a
sentence. This takes negative information into ac-
count as entities which do not occur in other sen-
tences also count. Hence normalization captures
the relative importance of entities as well as the
relative importance of sentences.
We follow Newman (2004) by applying node
degree normalization. For P
W
, we divide the
weight of each edge by the degree of the corre-
sponding sentence node. If a sentence contains
many entities, then the amount of information
each entity contributes is reduced. Assume ?s
i
?
as the number of entities in sentence s
i
. The im-
portance of entity e
j
for s
i
is
Imp(s
i
, e
j
) =
1
?s
i
?
.
1
3
2 3
2 1 1
3
1 2 1
1
S1 S2 S3
e1 e2 e3 e4 e5 e7e6 e8 e9 e10
1
6 6
6 8
8 8 8
8
7
7 7
7
7
Figure 3: Normalized entity graph
For P
Acc
we divide the weight of each edge by the
sum of all edges? weights of a sentence. This gives
the importance of each entity in a sentence relative
to the sentence?s other entities (see Figure 3).
Imp(s
i
, e
j
) =
w(s
i
, e
j
)
?
e
e
?Entities
w(s
i
, e
e
)
.
For also normalizing the one-mode projection
we introduce a virtual node TC capturing the
textual content of all sentences (inspired by the
graph based information retrieval model of Rode
(2008)). The virtual node TC is connected to all
sentences (see Figure 4).
S1 S2 S3
e1 e2 e3 e4 e5 e7e6 e8 e9 e10
TC
w(
s1,
TC
)
w(s2,TC)
w(s3,TC)
Figure 4: Entity graph with virtual node
Rode (2008) uses the following formula to com-
pute weights on the edges between the sentence
nodes and TC:
w(s
i
, TC) =
Score(s
i
|TC)
?
s
t
Score(s
t
|TC)
,
where the function Score(s
i
|TC) is the number
of entities in s
i
which have overlap with TC. This
value is equal to the degree of each sentence.
Since we are interested in local coherence, we
restrict TC to pairs of sentences (See Figure 5).
Subsequently, instead of w(s
i
, TC), we use the
notation lw
s
j
s
i
(local weight of sentence s
i
accord-
ing to sentence s
j
).
We define the normalized one-mode projection
as follows:
W
s
ij
=
?
e?E
s
ij
{
(lw
s
j
s
i
?Imp(s
i
,e))+(lw
s
i
s
j
?Imp(s
j
,e))
}
.
2
Si Sj
e1 e2 e3 e4 e5 e7e6
RTC
=w
(si,R
TC) =w(sj,RTC)
b
b b
lwsi
sj
lw sisj
Figure 5: Restricted TC for a pair of sentences
Similar to Rode (2008), we use the product of
lw
s
j
s
i
and Imp(s
i
, e) to approximate the salience
of entity e in sentence s
i
. This prevents the model
to get biased by the length of sentences.
This method can be applied to graphs with
edges weighted according to syntactic role (P
Acc
).
To compute the connection?s strength of a pair of
sentences we follow Yang and Knoke?s (2001) ap-
proach: The path length in a weighted graph is the
sum of the edge weights in the path. In our case,
each path is defined between a pair of sentences
of the entity graph, so the number of edges of all
paths are equal to two. Figure 6 shows the nor-
malized projections where the weights have been
computed by the above formula.
S1 S2
S3
S1 S2
S3
S1 S2
S3
1
1
2
P P P
8
4
8
27
64
23
56
U W Acc
Figure 6: Normalized projections
4 Experiments
We compare the normalized entity graph with the
entity graph on all tasks, Guinaudeau and Strube
(2013) compared their work with the entity grid
(Barzilay and Lapata, 2008; Elsner and Charniak,
2011): sentence ordering, summary coherence rat-
ing and readability assessment. Following Guin-
audeau and Strube (2013) we test statistical sig-
nificance with the Student?s t-test and Bonferroni
correction, to check whether the best result (bold
value in the tables) is significantly different from
the results of the entity graph and the normalized
entity graph. Diacritics ** indicate significance
level 0.01, * indicates significance level 0.05.
Acc F
Random 0.496 0.496
B&L 0.877 0.877
E&C 0.915 0.915
Entity graph, G&S
P
U
, Dist 0.830 0.830**
P
W
, Dist 0.871 0.871
P
Acc
, Dist 0.889 0.889
Normalized entity graph
P
U
, Dist 0.830 0.830**
P
W
, Dist 0.886 0.886
P
Acc
, Dist 0.909 0.909
Table 1: Discrimination, baselines and entity
graph vs. normalized entity graph
4.1 Sentence Ordering
This task consists of two subtasks: discrimina-
tion and insertion. In both subtasks we evaluate
whether our model can distinguish between the
correct order of sentences in a document and an
incorrect one. Experimental setup and data fol-
low Guinaudeau and Strube (2013) (61 documents
from the English test part of the CoNLL 2012
shared task (Pradhan et al., 2012)).
For discrimination we use 20 permutations of
each text. Table 1 shows the results. Results
for Guinaudeau and Strube (2013), G&S, are re-
produced, results for Barzilay and Lapata (2008),
B&L, and Elsner and Charniak (2011), E&C, were
reproduced by Guinaudeau and Strube (2013).
The unweighted graph, P
U
, does not need nor-
malization. Hence the results for the entity graph
and the normalized entity graph are identical. Nor-
malization improves the results for the weighted
graphs P
W
and P
Acc
with P
Acc
outperforming
B&L considerably and closely approaching E&L.
Sentence insertion is more difficult than dis-
crimination. Following Elsner and Charniak
(2011), we use two measures for evaluation: Ac-
curacy (Acc.) and the average proportion of cor-
rect insertions per document (Ins.).
Acc. Ins.
Random 0.028 0.071
E&C 0.068 0.167
Entity graph, G&S
P
U
, Dist 0.062** 0.101**
P
W
, Dist 0.075 0.114**
P
Acc
, Dist 0.071 0.102**
Normalized entity graph
P
U
, Dist 0.062** 0.101**
P
W
, Dist 0.085 0.154
P
Acc
, Dist 0.077 0.157
Table 2: Insertion, baselines and entity graph vs.
normalized entity graph
3
Acc. F
B&L 0.833
Entity graph, G&S
P
U
0.800 0.815
P
W
0.613 0.613*
P
Acc
0.700 0.704
Normalized entity graph
P
U
0.800 0.815
P
W
0.775 0.775
P
Acc
0.788 0.788
Table 3: Summary Coherence Rating, B&L and
entity graph vs. normalized entity graph
Table 2 shows that the normalized entity graph
outperforms the entity graph for P
W
and P
Acc
(again, no difference for P
U
). The normalized
entity graph outperforms E&C in Acc. and ap-
proaches it in Ins. The high value for Ins. shows
that if the normalized entity graph makes false de-
cisions they are closer to the original ordering than
the mistakes of the entity graph.
4.2 Summary Coherence Rating
We follow Barzilay and Lapata (2008) for evalu-
ating whether the normalized entity graph can de-
cide whether automatic or human summaries are
more coherent (80 pairs of summaries extracted
from DUC 2003). Human coherence scores are as-
sociated with each pair of summarized documents
(Barzilay and Lapata, 2008).
Table 3 displays reported results of B&L and
reproduced results of the entity graph and our nor-
malized entity graph. Normalizing significantly
improves the results for P
W
and P
Acc
. P
U
is still
slightly better than both, but in contrast to the en-
tity graph, this difference is not statistically signif-
icant. We believe that better weighting schemes
based on linguistic insights eventually will outper-
form P
U
and B&L (left for future work). Distance
information always degrades the results for this
task (see Guinaudeau and Strube (2013)).
4.3 Readability Assessment
Readability assessment aims to distinguish texts
which are difficult to read from texts which are
easier to read. In experiments, Barzilay and La-
pata (2008) assume that articles taken from Ency-
clopedia Britannica are more difficult to read (less
coherent) than the corresponding articles from En-
cyclopedia Britannica Elementary, its version for
children. We follow them with regard to data (107
article pairs), experimental setup and evaluation.
Table 4 compares reported results by Schwarm
Acc. F
S&O 0.786
B&L 0.509
B&L + S&O 0.888
Entity graph, G&S
P
U
, Dist 0.589 0.589**
P
W
, Dist 0.570 0.570**
P
Acc
, Dist 0.766 0.766**
Normalized entity graph
P
U
, Dist 0.589 0.589**
P
W
, Dist 0.897 0.897
P
Acc
, Dist 0.850 0.850
Table 4: Readability assessment, baselines and en-
tity graph vs. normalized entity graph
and Ostendorf (2005), S&O, Barzilay and Lapata
(2008), B&L, a combined method, B&L + S&O,
reproduced results for the entity graph, G&S, and
our normalized entity graph. Distance information
always improves the results.
Sentences in the Britannica Elementary are
simpler and shorter than in the Encyclopedia Bri-
tannica. The entity graph does not take into ac-
count the effect of entities not shared between sen-
tences while the normalized entity graph assigns a
lower weight if there are more of these entities.
Hence, Britannica Elementary receives a higher
cohesion score than Encyclopedia Britannica in
our model. Adding grammatical information, does
not help, because of the influence of the number
of entities (shared and not shared) outweighs the
influence of syntactic roles. The normalized en-
tity graph (P
W
, Dist) does not only outperform
the entity graph (significantly) and B&L but also
S&O and the combination B&L + S&O.
5 Conclusion
We proposed a normalization method for the en-
tity graph (Guinaudeau and Strube, 2013). We
compared our model to the entity graph and
to the entity grid (Barzilay and Lapata, 2008)
and showed that normalization improves the re-
sults significantly in most tasks. Future work
will include adding more linguistic information,
stronger weighting schemes and application to
other readability datasets (Pitler and Nenkova,
2008; De Clercq et al., 2014).
Acknowledgments
This work has been funded by the Klaus Tschira
Foundation, Heidelberg, Germany. The first au-
thor has been supported by a Heidelberg Institute
for Theoretical Studies Ph.D. scholarship.
4
References
Regina Barzilay and Mirella Lapata. 2005. Model-
ing local coherence: An entity-based approach. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
Mich., 25?30 June 2005, pages 141?148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Orph?ee De Clercq, V?eronique Hoste, Bart Desmet,
Philip Van Oosten, Martine De Cock, and Lieve
Macken. 2014. Using the crowd for readability pre-
diction. Natural Language Engineering, 20(3):293?
325.
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), Portland, Oreg., 19?24 June 2011, pages
125?129.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203?225.
Camille Guinaudeau and Michael Strube. 2013.
Graph-based local coherence modeling. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), Sofia, Bulgaria, 4?9 August 2013, pages 93?
103.
Mark E.J. Newman. 2004. Analysis of weighted net-
works. Physical Review E, 70(5):056131.
Mark E.J. Newman. 2010. Networks: An Introduction.
Oxford University Press, New York, N.Y.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, Waikiki, Honolulu, Hawaii, 25?27 October
2008, pages 186?195.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of the Shared Task of the 16th Conference on Com-
putational Natural Language Learning, Jeju Island,
Korea, 12?14 July 2012, pages 1?40.
Henning Rode. 2008. From document to entity re-
trieval: Improving precision and performance of fo-
cused text search. Ph.D. thesis, Enschede, June.
Sarah E. Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, Mich., 25?30 June
2005, pages 523?530.
Sally Stoddard. 1991. Text and Texture: Patterns of
Cohesion. Ablex, Norwood, N.J.
Song Yang and David Knoke. 2001. Optimal connec-
tions: Strength and distance in valued graphs. Social
networks, 23(4):285?295.
Tao Zhou, Jie Ren, Mat?u?s Medo, and Yi-Cheng Zhang.
2008. Bipartite network projection and personal rec-
ommendation. Physical Review E, 76(4). 046115.
Katharina A. Zweig and Michael Kaufmann. 2011. A
systematic approach to the one-mode projection of
bipartite graphs. Social Network Analysis and Min-
ing, 1:187?218.
5
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 15?24,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Multi-document Summarization Using Bipartite Graphs
Daraksha Parveen and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
(daraksha.parveen|michael.strube)@h-its.org
Abstract
In this paper, we introduce a novel graph
based technique for topic based multi-
document summarization. We transform
documents into a bipartite graph where
one set of nodes represents entities and the
other set of nodes represents sentences. To
obtain the summary we apply a ranking
technique to the bipartite graph which is
followed by an optimization step. We test
the performance of our method on several
DUC datasets and compare it to the state-
of-the-art.
1 Introduction
Topic-based multi-document summarization aims
to create a single summary from a set of given
documents while considering the topic of inter-
est. The input documents can be created by query-
ing an information retrieval or search engine for a
particular topic and retaining highly ranked docu-
ments, or by clustering documents of a large col-
lection and then using each cluster as a set of input
documents (Galanis et al., 2012). Here, each clus-
ter of the set of documents contains a representa-
tive topic.
A summary extracted from a set of input doc-
uments must be related to the topic of that set.
If textual units (or sentences) extracted from
different documents convey the same informa-
tion, then those units are called redundant. Ide-
ally, the multi-document summary should be non-
redundant. Hence each textual unit in a summary
should convey unique information. Still, all ex-
tracted textual units should be related to the topic.
They should also make up a coherent summary.
When building summaries from multiple docu-
ments belonging to different sets, a system should
attempt to optimize these three basic properties:
1. Relevance: A summary should contain only
those textual units which are relevant to the
topic and provide useful information.
2. Non-redundancy: A summary should not
contain the same information twice.
3. Readability: A summary should have good
readability (syntactically well formed, no
dangling pronouns, coherent, . . . ).
Generally, multi-document summarization sys-
tems differ from each other on the basis of docu-
ment representation, sentence selection method or
on the requirements for the output summary. Pop-
ular methods for document representation include
graph-based representations (e.g. LexRank (Erkan
and Radev, 2004) and TextRank (Mihalcea and Ta-
rau, 2004)) and tf-idf vector-based representations
(Luhn, 1958; Nenkova and Vanderwende, 2005;
Goldstein et al., 2000). These document represen-
tations act as input for the next phase and provide
information about the importance of individual
sentences. Sentence selection is the crucial phase
of the summarizer where sentence redundancy
must be handled in an efficient way. A widely
used technique is the greedy approach introduced
by Carbonell and Goldstein (1998) and Goldstein
et al. (2000). They compute a relevance score for
all sentences with regard to the topic, start by ex-
tracting the most relevant sentence, and then itera-
tively extract further sentences which are relevant
to the topic and at the same time most dissimilar
to already extracted sentences. Later more fun-
damental optimization methods have been widely
used in multi-document summarization, e.g. Inte-
ger Linear Programming (ILP) (McDonald, 2007;
Gillick et al., 2009; Nishikawa et al., 2010; Gala-
nis et al., 2012). Unlike most other approaches
(Galanis et al., 2012) has also taken into account
the readability of the final summary.
In this work, we introduce an extractive
topic based multi-document summarization sys-
tem which represents documents graphically and
15
optimizes the importance of sentences and non-
redundancy. The importance of sentences is ob-
tained by means of applying the Hubs and Author-
ities ranking algorithm (Kleinberg, 1999) on the
unweighted bipartite graph whereas redundancy in
the final summary is dealt with entities in a graph.
In Section 2 we introduce the state-of-the-art in
topic based multi-document summarizaton. Sec-
tion 3 provides a detailed description of our ap-
proach. Experiments are described in Section 4
where we also briefly describe the datasets used
and the results. Section 5 discusses the results of
our approach, and in Section 6 we finally give con-
clusions.
2 Related work
A graph-based representation of documents for
summarization is adopted by various approaches.
For instance, TextRank by Mihalcea and Tarau
(2004) applies the PageRank algorithm (Brin and
Page, 1998) to extract important sentences for sin-
gle document summarization. This ranking algo-
rithm proclaims the importance of a sentence by
considering the global information which is com-
puted recursively from the entire graph. Later,
the graph is converted into a weighted graph in
which the weights are calculated by measuring the
similarity of sentences (Mihalcea, 2004). Simi-
larly, in the LexRank approach (Erkan and Radev,
2004), documents are represented as a similarity
graph in which the sentences are nodes and these
sentences are then ranked according to centrality
measures. The three centrality measures used are
degree, LexRank with threshold and continuous
LexRank. LexRank is a measure to calculate ranks
using the similarity graph of sentences. It is also
known as lexical PageRank. The summarization
approach developed by Gong and Liu (2001) is
also based on ranking sentences where important
sentences are selected using a relevance measure
and latent semantic analysis.
Later, for better performance, sentences are
classified according to their existence in their final
summary in binary format i.e. 1 (belongs to sum-
mary) and 0 (doesn?t belong to summary) (Shen et
al., 2007; Gong and Liu, 2001). Here, the sen-
tences are projected as feature vectors and con-
ditional random fields are used to classify them.
During document processing, most informative
sentences are selected by the summarizer (Shen
et al., 2007). Fattah and Ren (2009) also consid-
ers summarization as two class classification prob-
lem. They use a genetic algorithm and mathemati-
cal regression to select appropriate weights for the
features and used different classification technique
for e.g. feed forward neural network, probablistic
neural network and Gaussian mixture models.
In the summarization task, optimization of the
three properties discussed in Section 1, relevance,
non-redundancy and readability, is required. This
is a global inference problem, which can be solved
by two approaches. Firstly, relevance and redun-
dancy can be optimized simultaneously. For in-
stance, Goldstein et al. (2000) developed a met-
ric named MMR-MD (influenced by the Max-
imum Marginal Relevance (MMR) approach of
Carbonell and Goldstein (1998)) and applied it to
clusters of passages. Similarly, influenced by the
SumBasic system (Nenkova and Vanderwende,
2005), Yih et al. (2007) developed a system which
assigns a score to each term on the basis of po-
sition and frequency information and selects the
sentence having highest score. Other approaches
are based on an estimate of word importance (e.g.
Lin and Hovy (2000)) or the log likelihood ratio
test which identifies the importance of words using
a supervised model that considers a rich set of fea-
tures (Hong and Nenkova, 2014). Finally, Barzi-
lay and Elhadad (1999) extract sentences which
are strongly connected by lexical chains for sum-
marization. The second approach deals with rel-
evance and redundancy seperately. For instance,
McKeown et al. (1999) create clusters of similar
sentences and pick the representative one from ev-
ery cluster. The representative sentence of a clus-
ter of sentences takes care of the requirement to
extract relevant information whereas clustering re-
duces the redundancy.
McDonald (2007) proposes a new ILP opti-
mization method for extractive summarization. He
introduces an objective function which maximizes
the importance of sentences and minimizes the
similarity of sentences. ILP methods for optimiza-
tion have also been adopted by Berg-Kirkpatrick
et al. (2011),Woodsend and Lapata (2012) and
Galanis et al. (2012). Until now, Galanis et
al. (2012) have reported the highest scores for
multi-document summarization on DUC2005 and
DUC2007. However, their approach is not com-
pletely unsupervised.
16
3 Our method
This section describes the technique, which we
adopted for summarization. We start by discussing
the graphical representation of the text followed
by a description how to quantify the importance
of sentences in the input texts. We then discuss
the ILP technique which optimizes the importance
of sentences and redundancy.
3.1 Graphical representation of text
The graphical representation of a text makes it
more expressive than a traditional tf-idf depiction
for summarization. A graph can easily capture
the essence of the whole text without leading to
high computational complexity. Guinaudeau and
Strube (2013) introduced a bipartite graph repre-
sentation of text based on the entity grid (Barzilay
and Lapata, 2008) representation of text. The pro-
jection of this bipartite graph representation has
been used for calculating the local coherence of
a text (Guinaudeau and Strube, 2013). The basic
intuition to use a bipartite graph for summariza-
tion is that it contains entity transitions similar to
lexical chains (Barzilay and Elhadad, 1999). An
appropriate measure to determine the importance
of sentences by considering strong entity transi-
tions indicates the information central to a text bet-
ter than simply giving scores on the basis of most
frequent words. The unweighted bipartite graph
G = (V
s
, V
e
, L) contains two sets of nodes, V
s
corresponding to the sentences from the input text
and V
e
corresonding to the entities, and a set of
edges represented by L. Figure 1 shows a model
summary from the DUC 2006 data, which is trans-
formed into an entity grid in Figure 2 (Barzilay
and Lapata, 2008; Elsner and Charniak, 2011).
Here, cells are filled with the syntactic role a men-
tion of an entity occupies in a sentence. Subjects
are denoted by S, objects by O and all other roles
by X. If an entity is not mentioned in a sentence
then the corresponding cell contains ?-?. In the
corresponding bipartite graph (Figure 3), edges are
created between a sentence and an entity only if
the entity is mentioned in a sentence (the cell in
entity grid is not ?-?). Since this is a dyadic graph,
there are no edges between nodes of the same set.
3.2 Ranking the importance of sentences
A graph based ranking algorithm is used to cal-
culate the importance of a sentence represented
as a node in the graph discussed above. In con-
trast to the local information specific to a ver-
tex, graphical ranking algorithms take (graph-)
global information to calculate the rank of a node.
The Hyperlink-Induced Topic Search algorithm
(HITS, also known as Hubs and Authorities) by
Kleinberg (1999) is used to rank sentences in our
method. This algorithm considers two types of
nodes, hence it is well suited to rank sentences in
our bipartite graph. Entities are considered as hub
nodes, and sentences are considered as authority
nodes. The importance of a sentence is calculated
in two steps:
? Hub update rule: Update each node?s hub
score to be equal to the sum of the author-
ity scores of each node that it points to. It can
be written as:
HubScore = A ?AuthorityScore (1)
Here, A is an adjacency matrix which represents
the connection between the nodes in a graph.
? Authority update rule: In this step, each au-
thority node is updated by equating them to
the sum of the hub scores of each node, which
is pointing to that authority node. It can be
written as:
AuthorityScore = A
T
?HubScore (2)
Hence, the authority weight is high if it is
pointed at by a hub having high weights.
Given some intial ranks to all nodes in a graph,
the hub and authority update rules are applied un-
til convergence. After applying this algorithm, the
rank of every node is obtained. The rank is consid-
ered as importance of the node within the graph.
We normalize the ranks of sentences according to
sentence length to avoid assigning high ranks to
long sentences.
To incorporate important information from doc-
uments, ranks of entities are incremented by
Rank+tf
doc
?idf
doc
in every iteration, where tf
doc
shows the importance of an entity in a document
by calculating the frequency whereas idf
doc
is an
inverse document frequency from the current clus-
ter. Rank+ tf
doc
? idf
doc
is used in calculating the
AuthorityScore. Initially, theRank can be any nu-
merical value but after every iteration of the HITS
algorithm it will be updated accordingly.
17
S1
The treatment of osteoarthritis includes a number of non-steroidal anti-inflammatory drugs such as
aspirin, acetaminophen, and ibuprofen.
S
2
These drugs, however, cause liver damage and gastrointestinal bleeding and contribute to thousands
of hospitalizations and deaths per year.
S
3
New cox-2 inhibitor drugs are proven as effective against pain, with fewer gastrointestinal side
effects.
S
4
The two together appeared to reduce knee pain after 8 weeks.
Figure 1: Model summary from DUC 2006
T
R
E
A
T
M
E
N
T
(
e
1
)
O
S
T
E
O
A
R
T
H
R
I
T
I
S
(
e
2
)
N
U
M
B
E
R
(
e
3
)
D
R
U
G
S
(
e
4
)
A
S
P
I
R
I
N
(
e
5
)
A
C
E
T
A
M
I
N
O
P
H
E
N
(
e
6
)
I
B
U
P
R
O
F
E
N
(
e
7
)
D
A
M
A
G
E
(
e
8
)
B
L
E
E
D
I
N
G
(
e
9
)
T
H
O
U
S
A
N
D
S
(
e
1
0
)
D
E
A
T
H
S
(
e
1
1
)
Y
E
A
R
(
e
1
2
)
P
A
I
N
(
e
1
3
)
E
F
F
E
C
T
S
(
e
1
4
)
T
W
O
(
e
1
5
)
W
E
E
K
S
(
e
1
6
)
S
1
S X O X X X X - - - - - - - - -
S
2
- - - S - - - O O X X X - - - -
S
3
- - - S - - - - - - - - X X - -
S
4
- - - - - - - - - - - - O - S X
Figure 2: Entity grid of the model summary from Figure 1
Figure 3: Bipartite graph derived from the entity grid from Figure 2
3.3 Optimization algorithm
In topic-based multi-document summarization,
the final summary should be non-redundant. At
the same time it should contain the important in-
formation from the documents. To achieve these
two conditions, we employ integer linear program-
ming (ILP) to obtain an optimal solution. In ILP
we maximize an objective function. Our objective
function, given in Equation 3, has two parts: the
importance of a summary and the non-redundancy
of a summary. The values obtained after ranking
by the HITS algorithm are used as the importance
of sentences for ILP. Non-redundancy can not be
calculated for a single sentence. Instead, it has to
be evaluated with respect to other sentences. We
calculate non-redundancy by the number of un-
shared entities, i.e. entities which are not shared
by other sentences, after appending a sentence to
a summary. The least redundant sentence will in-
crease the number of entities in the final summary.
max(?
1
n
?
i=1
(Rank(s
i
) + topicsim(s
i
))?x
i
+?
2
m
?
j=1
y
j
)
(3)
Equation 3 is the objective function where m is
18
Topic Documents per topic Human Summaries Word limit in final summary
DUC 2005 50 25-50 4-9 250
DUC 2006 50 25 4 250
DUC 2007 45 25 4 250
Table 1: Document Statistics
the number of entities in a document and n is the
number of sentences in a document. x
i
and y
j
are
binary variables for sentences and entities respec-
tively. ?
1
and ?
2
are tuning parameters. Rank(s
i
)
is a rank of a sentence s
i
obtained by applying the
HITS algorithm. Since, we work on topic-based
multi-document summarization, we include topic
information by calculating topicsim(s
i
), which
captures the cosine similarity of a sentence s
i
with
the corresponding topic. If the topic contains more
than one sentence then we take an average of co-
sine similarity with a sentence s
i
. The constraints
on the variables are shown in Equations 4-6:
n
?
i=1
Len(s
i
) ? x
i
? Len(summary) (4)
Here, Len(s
i
) and Len(summary) are the
number of words in a sentence s
i
and in the fi-
nal summary, respectively. This constraint does
not allow the length of final summary to exceed its
maximum length. The maximum length varies de-
pending on the datasets discussed in Section 4.1.
?
jE
i
y
j
? Entities(s
i
), for i = 1, . . . , n (5)
In constraint 5, E
i
is a set of entities present in
a sentence s
i
. The number of entities present in a
sentence is represented as Entities(s
i
). If a sen-
tence s
i
is selected then the entities present in a
sentence are also selected(
?
y
j
= Entities(s
i
)).
Whereas, if a sentence s
i
is not selected then
some of its entities can also be selected because
they may appear in already selected sentences
(Entities(s
i
) = 0, ?
?
y
j
? 0). In both the
cases, constraint 5 is not violated.
?
iS
j
x
i
? y
j
, forj = 1, . . . ,m (6)
In constraint 6, S
j
is a set of sentences contain-
ing entity y
j
. This constraint shows that, if an en-
tity y
j
is selected then at least one sentence is se-
lected which contains it (y
j
= 1, ?
?
x
i
? 1). If
an entity y
j
is not selected, then it is possible that
none of the sentences which contain it may not be
selected (y
j
= 0, ?
?
x
i
= 0). Also, constraint 4
holds in either of the cases.
4 Experiments
We perform experiments on various DUC datasets
to compare the results with state-of-the-art sys-
tems.
4.1 Datasets
Datasets used for our experiments are DUC2005
(Dang, 2005), DUC2006 (Dang, 2006) and
DUC2007
1
. Each dataset contains group of re-
lated documents. Each group of documents con-
tains one related topic or a query consisting of a
few sentences. In DUC, the final summary should
respond to the corresponding topic. Also, the sum-
mary cannot exceed the maximum allowed length.
For instance, in DUC2005, 250 words are allowed
in the final summary. Every document cluster has
corresponding human summaries for evaluating
system summaries on the basis of ROUGE scores
(Lin, 2004). The sources of DUC datasets are Los
Angeles Times, Financial Times of London, As-
sociated Press, New York Times and Xinhua news
agency. We employ ROUGE SU4 and ROUGE 2
as evaluation metrics. ROUGE returns recall, pre-
cision and F-score of a system, but usually only re-
call is used in for evaluating automatic summariza-
tion systems, because the final summary does not
contain many words. Hence, if the recall is high
then the summarization system is working well.
Document statistics is provided in Table 1.
4.2 Experimental setup
We use raw documents from the various DUC
datasets as input for our system. We remove non-
alphabetical characters from the documents. Then
we obtain a clean sentence split by means of the
Stanford parser (Klein and Manning, 2003) so that
the sentences are compatible with the next steps.
1
http://www-nlpir.nist.gov/projects/
duc/index.html
19
ROUGE-2 ROUGE-SU4
?
1
= 0.5 & ?
2
= 0.5 0.07950 0.14060
?
1
= 0.6 & ?
2
= 0.4 0.07956 0.14071
?
1
= 0.7 & ?
2
= 0.3 0.07975 0.14105
?
1
= 0.8 & ?
2
= 0.2 0.07976 0.14106
?
1
= 0.9 & ?
2
= 0.1 0.07985 0.14107
Table 2: Results on different ??s on DUC 2005
We use the Brown coherence toolkit (Elsner and
Charniak, 2011) to convert the documents into the
entity grid representation from which the bipar-
tite graph is constructed (Guinaudeau and Strube,
2013). Entities in the graph correspond to head
nouns of noun phrase mentioned in the sentences.
The ranking algorithm from Section 3.2 is applied
to this graph and returns the importance score of
a sentence as required by the objective function
given in Equation 3. Next optimization using ILP
is performed as described in Section 3.3. We use
GUROBI Optimizer
2
for performing ILP. ILP re-
turns a binary value, i.e., if a sentence should be
included in the summary it returns 1, if not it re-
turns 0. We set ?
1
= 0.7 and ?
2
= 0.3 for
all datasets. We did not choose the optimal val-
ues, but rather opted for ones which favor impor-
tance over non-redundancy. We did not observe
significant differences between different ? values
as long as ?
1
> ?
2
(see Table 2). The sentences in
the output summary are ordered according to their
ranks. If the output summary contains pronouns,
we perform pronoun resolution in the source doc-
uments using the coreference resolution system by
Martschat (2013). If pronoun and antecedent oc-
cur in the same sentence, we leave the pronoun.
If the antecedent occurs in an earlier sentence, we
replace the pronoun in the summary by the first
element of the coreference chain the pronoun be-
longs to. Except for setting ?
1
and ?
2
on DUC
2005, our approach is unsupervised, as there is no
traning data required. The recall (ROUGE) scores
on different datasets are shown in Table 3.
Table 3 shows that our system would have per-
formed very well in the DUC 2005 and DUC 2006
competitions with ranks in the top 3 and well in
the DUC 2007 competition. Since the compe-
titions date a while back, we compare in addi-
tion to the current state-of-art in multi-document
summarization. To our knowledge Galanis et al.
2
Gurobi Optimization, Inc., http://www.gurobi.
com
Dataset ROUGE-2 ROUGE-SU4
DUC 2005 (32) 0.07975 (1) 0.14105 (1)
DUC 2006 (35) 0.08969 (3) 0.15070 (2)
DUC 2007 (32) 0.10928 (6) 0.16735 (5)
Table 3: System performance (and rank) on the
DUC 2005, 2006 and 2007 (main) data. The num-
ber in parenthesis after the DUC year indicates the
number of competing systems.
(2012) report the best results on DUC 2005 data.
While their ROUGE-2 score is slightly better than
ours, we outperform them in terms of ROUGE-
SU4 (0.14105 vs. 0.13640), where, to our knowl-
edge, our results are the highest reported so far.
However, their results on DUC 2007 (ROUGE-2
0.12517 and ROUGE-SU4 0.17603) are still quite
a bit better than our results. On the DUC 2006
data we outperform the HIERSUM system by
Haghighi and Vanderwende (2009) on ROUGE-
2 (0.08969 vs. 0.086) as well as on ROUGE-
SU4 (0.15070 vs. 0.143). On the DUC 2007
data, our results are worse than theirs on ROUGE-
2 (0.10928 vs. 0.118) and on par on ROUGE-
SU4 (0.16735 vs. 0.167). The system which won
the DUC 2007 task, PYTHY by Toutanova et al.
(2007), performs similar to HIERSUM and hence
slightly better than our system on these data. The
recent work by Suzuki and Fukumoto (2014) eval-
uates also on DUC 2007 but reports only ROUGE-
1 scores. We obtain a ROUGE-1 score of 0.448 on
DUC 2007 which is better than Suzuki and Fuku-
moto (2014) (0.438) as well as PYTHY (0.426).
The best ROUGE-1 score reported to date has
been reported by Celikyilmaz and Hakkani-T?ur
(2010) with 0.456. The difference between this
score and our score of 0.448 is rather small.
5 Discussion
Several approaches have been proposed for topic
based multi-document summarization on the DUC
datasets we use for our experiments. The best re-
sults to date have been obtained by supervised and
semi-supervised systems. The results of our sys-
tem are mostly on par with these systems though
our system is unsupervised (as mentioned in Sec-
tion 4 the values for ?
1
and ?
2
in the objective
function (Equation 3) were not tuned for optimal
ROUGE scores but rather set for favoring impor-
tance over non-redundancy).
We compared our results with various state-of-
20
S1
What is being learned from the study of deep water, seabeds, and deep water life?
S
2
What equipment and techniques are used?
S
3
What are plans for future related activity?
Figure 4: Topic containing interrogative words from DUC 2007
S
1
I?ve started to use irrigation hoses called ?leaky pipe?.
S
2
Soil?s usually best to water the target area a few days before I plan to dig.
S
3
If I don?t place element in the root zone , element can?t be added later when the plants are growing.
S
4
The new composts were much lighter and more suitable for container plants in garden centres and
through these were rapidly introduced to gardeners.
Figure 5: Sentences containing dangling first person pronoun from DUC 2005
the-art systems, and our system is giving compet-
itive results in both ROUGE-2 and ROUGE-SU4
scores. However, the ROUGE-2 score of Galanis
et al. (2012) on DUC 2005 is slightly better than
our score. This might be because they use bigram
information for redundancy reduction. However,
they need training data for sentence importance.
Hence their system has to be classified as super-
vised while ours is unsupervised.
We have also calculated the ROUGE-1 score
on DUC 2007 and compared it with state-of-
the-art approaches. HybHsum (Celikyilmaz and
Hakkani-T?ur, 2010) has obtained the top ROUGE-
1 score on DUC 2007 with 0.456. However,
HybHsum is a semi-supervised approach which
requires a labeled training data. The difference
between our ROUGE-1 score of 0.448 and HybH-
sum ROUGE-1 score on DUC2007 is not signif-
icant (to be fair, achieving significant improve-
ments in ROUGE scores on DUC data is very dif-
ficult). In contrast to HybHsum, our approach is
unsupervised.
Our method computes importance on the basis
of a bipartite graph. We believe that our bipartite
graph captures more information than the general
graphs used in earlier graph-based approaches to
automatic summarization. Entity transition infor-
mation present in the bipartite graph of a docu-
ment, helps us in finding the salient sentences. Our
approach works well if the graph is not sparse.
We observed a couple of problems in the out-
put of our system which we plan to address in
future work. If topics contain interrogative pro-
nouns as shown in Figure 4 the mapping between
topic and sentences from the documents does not
work well. We need to resolve which entities the
interrogative pronouns refer to. Another problem
occurs, because the coreference resolution system
employed does not resolve first person pronouns.
Hence, we end up with summaries containing dan-
gling first person pronouns as shown in Figure 5.
However, our system appears to work reasonably
well in other cases where the summaries are co-
herent and readable and also have a high ROUGE
score as shown in the summary from DUC 2007
data in Figure 6.
6 Conclusions
In this paper, we have presented an unsuper-
vised graph based approach for topic based multi-
document summarization. Our graph based ap-
proach provides state-of-the-art results on various
datasets taken from DUC competitions. The graph
based representation of a document makes com-
putation very efficient and less complex. In future
work, we incorporate the syntactic roles of enti-
ties, to provide more information in the method.
Acknowledgments
This work has been funded by Klaus Tschira
Foundation, Heidelberg, Germany. The first au-
thor has been supported by a Heidelberg Institute
for Theoretical Studies Ph.D. scholarship.
21
The European Parliament , angered by Turkey ?s human rights record , voted Thursday to freeze hundreds
of millions of US dollars in aid to Turkey for setting up a customs union with the EU. Since then , the
EU has been trying to patch up the relationship , with several leaders of member countries insisting
that Turkey ?s place is in the union. The special aid is part of the agreement between the European
Union EU and Turkey on the establishment of a customs union between the two sides. ? The European
Union , without renouncing its principles , ? will have to decide in December to allow Turkey to become
a formal candidate for EU membership. ANKARA , February 27 Xinhua Turkey today welcomed the
European Union ?s attitude toward its dispute with Greece and urged the EU to release financial assistance
immediately despite Greek efforts to block it. After the decision in December to exclude Turkey from
the first wave of enlargement talks , Turkey put its relations with the 15 member union on hold. During
Solana stay here , Turkish leaders reiterated their position to link the expansion of the NATO with Turkey
?s entry into the European Union. The European Union , European Union Ankara wants to join , is
pressing Turkey to find a peaceful solution to the war. The statement added that Greece , despite its
attempts , was unable to get the support of the other 14 European Union members in getting a statement
that would express solidarity with Greece and condemn Turkey. Both the European Union and the United
States criticized Turkey for jailing Birdal.
Figure 6: Output summary from DUC 2007
Acknowledgments
This work has been funded by Klaus Tschira
Foundation, Heidelberg, Germany. The first au-
thor has been supported by a Heidelberg Institute
for Theoretical Studies Ph.D. scholarship.
References
Regina Barzilay and Michael Elhadad. 1999. Us-
ing lexical chains for text summarization. In Inder-
jeet Mani and Mark T. Maybury, editors, Advances
in Automatic Text Summarization, pages 111?121.
Cambridge, Mass.: MIT Press.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, Portland,
Oreg., 19?24 June 2011, pages 481?490.
Sergey Brin and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. Computer Networks and ISDN Systems,
30(1?7):107?117.
Jaime G. Carbonell and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. In Proceed-
ings of the 21st Annual International ACM-SIGIR
Conference on Research and Development in Infor-
mation Retrieval, Melbourne, Australia, 24?28 Au-
gust 1998, pages 335?336.
Asli Celikyilmaz and Dilek Hakkani-T?ur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11?16 July 2010, pages 815?
824.
Hoa Trang Dang. 2005. Overview of DUC 2005. In
Proceedings of the 2005 Document Understanding
Conference held at the Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, Vancouver,
B.C., Canada, 9?10 October 2005.
Hoa Trang Dang. 2006. Overview of DUC 2006. In
Proceedings of the 2006 Document Understanding
Conference held at the Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics, New
York, N.Y., 8?9 June 2006.
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the ACL 2011 Conference Short Papers,
Portland, Oreg., 19?24 June 2011, pages 125?129.
G?unes? Erkan and Dragomir R. Radev. 2004. LexRank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research, 22:457?479.
Mohamed Abdel Fattah and Fuji Ren. 2009. GA,
MR, FFNN, PNN and GMM based models for au-
tomatic text summarization. Computer Speech and
Language, 23(1):126?144.
Dimitrios Galanis, Gerasimos Lampouras, and Ion An-
droutsopoulos. 2012. Extractive multi-document
summarization with integer linear programming and
support vector regression. In Proceedings of the
24th International Conference on Computational
Linguistics, Mumbai, India, 8?15 December 2012,
pages 911?926.
22
Daniel Gillick, Korbinian Riedhammer, Benoit Favre,
and Dilek Hakkani-T?ur. 2009. A global optimiza-
tion framework for meeting summarization. In Pro-
ceedings of the 2009 IEEE International Conference
on Acoustics, Speech, and Signal Processing, Taipei,
Taiwan, 19?24 June 2009, pages 4769?4772.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Proceedings
of the Workshop on Automatic Summarization at
ANLP/NAACL 2000, Seattle, Wash., 30 April 2000,
pages 40?48.
Yihong Gong and Xin Liu. 2001. Generic text summa-
rization using relevance measure and latent semantic
analysis. In Proceedings of the 24th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval New Orleans,
Louis., 9?12 September 2001, pages 19?25.
Camille Guinaudeau and Michael Strube. 2013.
Graph-based local coherence modeling. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics, Sofia, Bulgaria, 4?9
August 2013, pages 93?103.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of Human Language Technolo-
gies 2009: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Boulder, Col., 31 May ? 5 June 2009, pages
362?370.
Kai Hong and Ani Nenkova. 2014. Improving
the estimation of word importance for news multi-
document summarization. In Proceedings of the
14th Conference of the European Chapter of the
Association for Computational Linguistics, Gothen-
burg, Sweden, 26?30 April 2014, pages 712?721.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, Sapporo, Japan, 7?12 July
2003, pages 423?430.
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for automatic
summarization. In Proceedings of the 18th Inter-
national Conference on Computational Linguistics,
Saarbr?ucken, Germany, 31 July ? 4 August 2000,
pages 495?501.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Proceedings of
the Text Summarization Branches Out Workshop at
ACL ?04, Barcelona, Spain, 25?26 July 2004, pages
74?81.
H.P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal of Research and Develop-
ment, 2:159?165.
Sebastian Martschat. 2013. Multigraph clustering for
unsupervised coreference resolution. In Proceed-
ings of the Student Research Workshop at the 51st
Annual Meeting of the Association for Computa-
tional Linguistics, Sofia, Bulgaria, 5?7 August 2013,
pages 81?88.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Pro-
ceedings of the European Conference on Informa-
tion Retrieval, Rome, Italy, 2-5 April 2007.
Kathleen R. McKeown, Judith L. Klavans, Vassileios
Hatzivassiloglou, Regina Barzilay, and Eleazar Es-
kin. 1999. Towards multidocument summarization
by reformulation: Progress and prospects. In Pro-
ceedings of the 16th National Conference on Arti-
ficial Intelligence, Orlando, Flo., 18?22 July 1999,
pages 453?460.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain, 25?26 July
2004, pages 404?411.
Rada Mihalcea. 2004. Graph-based ranking algo-
rithms for sentence extraction, applied to text sum-
marization. In Companion Volume to the Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics, Barcelona, Spain,
21?26 July 2004, pages 170?173.
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization. Technical Re-
port MSR-TR-2005-101, Microsoft Research.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Genichiro Kikui. 2010. Opinion summa-
rization with integer linear programming formula-
tion for sentence extraction and ordering. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, Beijing, China, 23?27
August 2010, pages 910?918.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and
Zheng Chen. 2007. Document summarization us-
ing conditional random fields. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, Hyderabad, India, 6?12 January 2007,
pages 2862?2867.
Yoshimi Suzuki and Fumiyo Fukumoto. 2014. De-
tection of topic and its extrinsic evaluation through
multi-document summarization. In Proceedings of
the ACL 2014 Conference Short Papers, Baltimore,
Md., 22?27 June 2014, pages 241?246.
Kristina Toutanova, Chris Brockett, Michael Gamon,
Jagadeesh Jagarlamudi, Hisami Suzuki, and Lucy
Vanderwende. 2007. The PYTHY summariza-
tion system: Microsoft Research at DUC 2007.
23
In Proceedings of the 2007 Document Understand-
ing Conference held at the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
Rochester, N.Y., 26?27 April 2007.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of the 2012 Conference
on Empirical Methods in Natural Language Pro-
cessing and Natural Language Learning, Jeju Is-
land, Korea, 12?14 July 2012, pages 233?242.
Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document summa-
rization by maximizing informative content-words.
In Proceedings of the 20th International Joint Con-
ference on Artificial Intelligence, Hyderabad, India,
6?12 January 2007, pages 1776?1782.
24
