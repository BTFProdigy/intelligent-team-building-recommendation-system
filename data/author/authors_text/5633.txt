67
68
69
70
71
72
73
74
Information Presentation in Spoken Dialogue Systems
Vera Demberg
Institute for Natural Language Processing (IMS)
University of Stuttgart
D-70174 Stuttgart
V.Demberg@gmx.de
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
J.Moore@ed.ac.uk
Abstract
To tackle the problem of presenting a
large number of options in spoken dia-
logue systems, we identify compelling op-
tions based on a model of user preferences,
and present tradeoffs between alternative
options explicitly. Multiple attractive op-
tions are structured such that the user can
gradually refine her request to find the
optimal tradeoff. We show that our ap-
proach presents complex tradeoffs under-
standably, increases overall user satisfac-
tion, and significantly improves the user?s
overview of the available options. More-
over, our results suggest that presenting
users with a brief summary of the irrele-
vant options increases users? confidence in
having heard about all relevant options.
1 Introduction
The goal of spoken dialogue systems (SDS) is
to offer efficient and natural access to applica-
tions and services, such as email and calendars,
travel and entertainment booking, and product rec-
ommendation. In evaluating nine SDS in the
DARPA Communicator domain (flight, hotel, and
rental car hire), Walker et al (2001) found that (1)
shorter task duration correlates with higher user
satisfaction, and (2) the information presentation
phase of dialogues is the primary contributor to
dialogue duration. During this phase, the typical
system enumerates the set of options that match
the user?s constraints, as shown in Figure 1. The
user can then refine these options by offering new
constraints. When the number of options to be
presented is large, this process can be painstaking,
leading to reduced user satisfaction. Moreover, as
S: I found 28 round trips from Edinburgh to Brussels on
October 31st and returning Edinburgh on November
6th. For option 1, I have an outbound flight with KLM
departing at 7 am and arriving at 8:49 am non-stop.
A return flight is with KLM departing at 10:30 am and
arriving at 11:28 am with 1 stopover. Ticket price is
?165. Please say ?next option?, or, ?flight details?, or,
?I?ll take it?.
U: Next option
S: For option 2, I have an outbound flight with KLM de-
parting at 3:30 pm and arriving at 5:19 pm nonstop.
A return flight is with KLM departing at 8:25 pm and
arriving at 12:28 pm with 1 stopover. Ticket price is
?165 please say ?next option?, or, ?flight details?, or,
?I?ll take it.?
U: Next option
S: . . .
Figure 1: Typical Information Presentation Phase
of a Communicator Dialogue
Walker et al (2004) observe, having to access the
set of available options sequentially makes it diffi-
cult for the user to remember the various aspects of
multiple options and to compare them in memory.
Clearly, alternative strategies to sequential pre-
sentation of information in SDS are needed. Re-
cently, two approaches have been proposed. In
the user-model (UM) based approach, the sys-
tem identifies a small number of options that best
match the user?s preferences (Moore et al, 2004;
Walker et al, 2004). In the summarize and re-
fine (SR) approach, the system structures the large
number of options into a small number of clus-
ters that share attributes. The system summa-
rizes the clusters based on their attributes and then
prompts the user to provide additional constraints
(Polifroni et al, 2003; Chung, 2004).
In this paper, we present an algorithm that com-
bines the benefits of these two approaches in an
approach to information presentation that inte-
grates user modelling with automated clustering.
65
Thus, the system provides detail only about those
options that are of some relevance to the user,
where relevance is determined by the user model.
If there are multiple relevant options, a cluster-
based tree structure orders these options to allow
for stepwise refinement. The effectiveness of the
tree structure, which directs the dialogue flow, is
optimized by taking the user?s preferences into ac-
count. Complex tradeoffs between alternative op-
tions are presented explicitly to allow for a bet-
ter overview and a more informed choice. In ad-
dition, we address the issue of giving the user a
good overview of the option space, despite select-
ing only the relevant options, by briefly accounting
for the remaining (irrelevant) options.
In the remainder of this paper, we describe the
prior approaches in more detail, and discuss their
limitations (Section 2). In section 3, we describe
our approach, which integrates user preferences
with automated clustering and summarization in
an attempt to overcome the problems of the origi-
nal approaches. Section 4 presents our clustering
and content structuring algorithms and addresses
issues in information presentation. In Section 5,
we describe an evaluation of our approach and dis-
cuss its implications.
2 Previous Work in Information
Presentation
2.1 Tailoring to a User Model
Previous work in natural language generation
showed how a multi-attribute decision-theoretic
model of user preferences could be used to deter-
mine the attributes that are most relevant to men-
tion when generating recommendations tailored to
a particular user (Carenini and Moore, 2001). In
the MATCH system, Walker et al (2004) applied
this approach to information presentation in SDS,
and extended it to generate summaries and com-
parisons among options, thus showing how the
model can be used to determine which options to
mention, as well as the attributes that the user will
find most relevant to choosing among them. Eval-
uation showed that tailoring recommendations and
comparisons to the user increases argument effec-
tiveness and improves user satisfaction (Stent et
al., 2002).
MATCH included content planning algorithms
to determine what options and attributes to men-
tion, but used a simple template based approach
to realization. In the FLIGHTS system, Moore
et al (2004) focussed on organizing and express-
ing the descriptions of the selected options and at-
tributes, in ways that are both easy to understand
and memorable. For example, Figure 2 shows a
description of options that is tailored to a user who
prefers flying business class, on direct flights, and
on KLM, in that order. In FLIGHTS, coherence
and naturalness of descriptions were increased by
reasoning about information structure (Steedman,
2000) to control intonation, using referring expres-
sions that highlight attributes relevant to the user
(e.g., ?the cheapest flight? vs. ?a KLM flight? ),
and signalling discourse relations (e.g., contrast)
with appropriate intonational and discourse cues.
S: You can fly business class on KLM, arriving at four
twenty p.m., but you?d need to connect in London. There
is a direct flight on BMI, arriving at four ten p.m., but it
has no availability in business class.
Figure 2: Tailored description by FLIGHTS
This prior work demonstrated that the user
model-based approach can concisely present a rel-
atively small number of options, pointing out the
ways in which those options satisfy user prefer-
ences. It is an appropriate strategy for SDS when
there are a small number of options to present, ei-
ther because the number of options is limited or
because users can supply sufficient constraints to
winnow down a large set before querying the data-
base of options.
However, there are several limitations to this ap-
proach. First, it does not scale up to presenting
a large number of options. When there are hun-
dreds of options to consider (e.g., when choosing
among consumer products, hotels, or restaurants)
there may be many options that are close in score.
In addition, users may not be able to provide con-
straints until they hear more information about the
space of options. This brings up a second problem
with the UM-based approach, namely that it does
not provide the user with an overview of the op-
tion space, because options scoring below a speci-
fied threshold are not mentioned. This is related to
the third problem, which is that users might miss
out on options they would have chosen if they had
heard about them. These last two problems may
reduce user confidence in the system, if users have
the perception that the system is not telling them
about all of the available options. This may ulti-
mately lead to a decrease in user satisfaction.
66
2.2 Stepwise Refinement through Clustering
and Summarization
Polifroni et al (2003) developed an approach that
structures large datasets for summarization and
successive refinement. It supports the user in nar-
rowing in on a suitable option by clustering the
options in the database, summarizing the option
clusters and suggesting further constraints. In this
approach, the prompts presented to the user, and
the order in which they appear are determined at
run-time based on an algorithm that computes the
most useful set of attributes, as dictated by the cur-
rent data subset. For large data sets, attributes that
partition the data into the minimal number of clus-
ters are chosen, so that a concise summary can be
presented to the user to refine, see Figure 3.
M: What restaurants do you have?
S: I have found 983 restaurants. Most of them are located
in Boston and Cambridge. There are 32 choices for cui-
sine. I also have information about price range.
M: Okay tell me about the ones in Boston.
S: I have found 401 restaurants in Boston. There are 29
choices for cuisine.
M: Do you have any that serve seafood?
S: I have found 19 seafood restaurants. They are predomi-
nantly in Back Bay, the North End, South Boston and the
South End.
M: In the South End.
S: I have found 3 restaurants. Grill Fish on Columbus Av-
enue, McCormick & Schmick?s on Columbus Avenue and
Joseph?s on High Street.
Figure 3: Dialogue between simulator (M) and Po-
lifroni system (S)
Polifroni et al?s approach was extended by
Chung (2004), who proposed a constraint relax-
ation strategy for coping with queries that are too
restrictive to be satisfied by any option. Qu and
Beale (2003) had previously addressed the prob-
lem of responding to user queries with several
constraints and used linguistic cues to determine
which constraints had to be relaxed. Our discus-
sion and evaluation of the SR approach is based
on Chung?s version.
Although the SR approach provides a solution
to the problem of presenting information when
there are large numbers of options in a way that is
suitable for SDS, it has several limitations. First,
there may be long paths in the dialogue struc-
ture. Because the system does not know about the
user?s preferences, the option clusters may contain
many irrelevant entities which must be filtered out
successively with each refinement step. In addi-
tion, the difficulty of summarizing options typi-
cally increases with their number, because values
are more likely to be very diverse, to the point
that a summary about them gets uninformative (?I
found flights on 9 airlines.?).
A second problem with the SR approach is that
exploration of tradeoffs is difficult when there is
no optimal option. If at least one option satis-
fies all requirements, this option can be found effi-
ciently with the SR strategy. But the system does
not point out alternative tradeoffs if no ?optimal?
option exists. For example, in the flight book-
ing domain, suppose the user wants a flight that is
cheap and direct, but there are only expensive di-
rect and cheap indirect flights. In the SR approach,
as described by Polifroni, the user has to ask for
cheap flights and direct flights separately and thus
has to explore different refinement paths.
Finally, the attribute that suggests the next user
constraint may be suboptimal. The procedure for
computing the attribute to use in suggesting the
next restriction to the user is based on the con-
siderations for efficient summarization, that is, the
attribute that will partition the data set into the
smallest number of clusters. If the attribute that
is best for summarization is not of interest to this
particular user, dialogue duration is unnecessarily
increased, and the user may be less satisfied with
the system, as the results of our evaluation suggest
(see section 5.2).
3 Our Approach
Our work combines techniques from the UM and
SR approaches. We exploit information from a
user model to reduce dialogue duration by (1) se-
lecting all options that are relevant to the user,
and (2) introducing a content structuring algorithm
that supports stepwise refinement based on the
ranking of attributes in the user model. In this
way, we keep the benefits of user tailoring, while
extending the approach to handle presentation of
large numbers of options in an order that reflects
user preferences. To address the problem of user
confidence, we also briefly summarize options that
the user model determines to be irrelevant (see
section 4.3). Thus, we give users an overview of
the whole option space, and thereby reduce the
risk of leaving out options the user may wish to
choose in a given situation.
The integration of a user model with the cluster-
ing and structuring also alleviates the three prob-
lems we identified for the SR approach. When a
67
user model is available, it enables the system to
determine which options and which attributes of
options are likely to be of interest to the particu-
lar user. The system can then identify compelling
options, and delete irrelevant options from the re-
finement structure, leading to shorter refinement
paths. Furthermore, the user model allows the
system to determine the tradeoffs among options.
These tradeoffs can then be presented explicitly.
The user model also allows the identification of the
attribute that is most relevant at each stage in the
refinement process. Finally, the problem of sum-
marizing a large number of diverse attribute values
can be tackled by adapting the cluster criterion to
the user?s interest.
In our approach, information presentation is
driven by the user model, the actual dialogue con-
text and the available data. We allow for an arbi-
trarily large number of alternative options. These
are structured so that the user can narrow in on one
of them in successive steps. For this purpose, a
static option tree is built. Because the structure of
the option tree takes the user model into account,
it allows the system to ask the user to make the
most relevant decisions first. Moreover, the option
tree is pruned using an algorithm that takes advan-
tage of the tree structure, to avoid wasting time
by suggesting irrelevant options to the user. The
tradeoffs (e.g., cheap but indirect flights vs. direct
but expensive flights) are presented to the user ex-
plicitly, so that the user won?t have to ?guess? or
try out paths to find out what tradeoffs exist. Our
hypothesis was that explicit presentation of trade-
offs would lead to a more informed choice and de-
crease the risk that the user does not find the opti-
mal option.
4 Implementation
Our approach was implemented within a spoken
dialogue system for flight booking. While the con-
tent selection step is a new design, the content pre-
sentation part of the system is an adaptation and
extension of the work on generating natural sound-
ing tailored descriptions reported in (Moore et al,
2004).
4.1 Clustering
The clustering algorithm in our implementation is
based on that reported in (Polifroni et al, 2003).
The algorithm can be applied to any numerically
ordered dataset. It sorts the data into bins that
roughly correspond to small, medium and large
values in the following way. The values of each at-
tribute of the objects in the database (e.g., flights)
are clustered using agglomerative group-average
clustering. The algorithm begins by assigning
each unique attribute value to its own bin, and suc-
cessively merging adjacent bins whenever the dif-
ference between the means of the bins falls below
a varying threshold. This continues until a stop-
ping criterion (a target number of no more than
three clusters in our current implementation) is
met. The bins are then assigned predefined labels,
e.g., cheap, average-price, expensive
for the price attribute.
Clustering attribute values with the above algo-
rithm allows for database-dependent labelling. A
?300 flight gets the label cheap if it is a flight
from Edinburgh to Los Angeles (because most
other flights in the database are more costly) but
expensive if it is from Edinburgh to Stuttgart
(for which there are a lot of cheaper flights in the
data base). Clustering also allows the construc-
tion of user valuation-sensitive clusters for cat-
egorial values, such as the attribute airline:
They are clustered to a group of preferred air-
lines, dispreferred airlines and airlines the
user does not-care about.
4.2 Building up a Tree Structure
The tree building algorithm works on the clusters
produced by the clustering algorithm instead of the
original values. Options are arranged in a refine-
ment tree structure, where the nodes of an option
tree correspond to sets of options. The root of
the tree contains all options and its children con-
tain complementary subsets of these options. Each
child is homogeneous for a given attribute (e.g., if
the parent set includes all direct flights, one child
might include all direct cheap flights whereas an-
other child includes all direct expensive flights).
Leaf-nodes correspond either to a single option or
to a set of options with very similar values for all
attributes.
This tree structure determines the dialogue flow.
To minimize the need to explore several branches
of the tree, the user is asked for the most essential
criteria first, leaving less relevant criteria for later
in the dialogue. Thus, the branching criterion for
the first level of the tree is the attribute that has the
highest weight according to the user model. For
example, Figure 5 shows an option tree structure
68
rank attributes
1 fare class (preferred value: business)
2 arrival time, # of legs, departure time, travel time
6 airline (preferred value: KLM)
7 price, layover airport
Figure 4: Attribute ranking for business user
Figure 5: Option tree for business user
for our ?business? user model (Figure 4).
The advantage of this ordering is that it mini-
mizes the probability that the user needs to back-
track. If an irrelevant criterion had to be decided
on first, interesting tradeoffs would risk being scat-
tered across the different branches of the tree.
A special case occurs when an attribute is ho-
mogeneous for all options in an option set. Then a
unary node is inserted regardless of its importance.
This special case allows for more efficient summa-
rization, e.g., ?There are no business class flights
on KLM.? In the example of Figure 5, the attribute
airline is inserted far up in the tree despite its
low rank.
The user is not forced to impose a to-
tal ordering on the attributes but may specify
that two attributes, e.g., arrival-time and
number-of-legs, are equally important to her.
This partial ordering leads to several attributes
having the same ranking. For equally ranked at-
tributes, we follow the approach taken by Polifroni
et al (2003). The algorithm selects the attribute
that partitions the data into the smallest number
of sub-clusters. For example, in the tree in Fig-
ure 5, number-of-legs, which creates two
sub-clusters for the data set (direct and indirect),
comes before arrival-time, which splits the
set of economy class flights into three subsets.
The tree building algorithm introduces one of
the main differences between our structuring and
Polifroni?s refinement process. Polifroni et al?s
system chooses the attribute that partitions the data
into the smallest set of unique groups for sum-
marization, whereas in our system, the algorithm
takes the ranking of attributes in the user model
into account.
4.3 Pruning the Tree Structure
To determine the relevance of options, we did not
use the notion of compellingness (as was done in
(Moore et al, 2004; Carenini and Moore, 2001)),
but instead defined the weaker criterion of ?dom-
inance?. Dominant options are those for which
there is no other option in the data set that is better
on all attributes. A dominated option is in all re-
spects equal to or worse than some other option in
the relevant partition of the data base; it should not
be of interest for any rational user. All dominant
options represent some tradeoff, but depending on
the user?s interest, some of them are more interest-
ing tradeoffs than others.
Pruning dominated options is crucial to our
structuring process. The algorithm uses informa-
tion from the user model to prune all but the dom-
inant options. Paths from the root to a given op-
tion are thereby shortened considerably, leading to
a smaller average number of turns in our system
compared to Polifroni et al?s system.
An important by-product of the pruning al-
gorithm is the determination of attributes which
make an option cluster compelling with respect
to alternative clusters (e.g., for a cluster con-
taining direct flights, as opposed to flights that
require a connection, the justification would be
#-of-legs). We call such an attribute the ?jus-
tification? for a cluster, as it justifies its existence,
i.e., is the reason it is not pruned from the tree. Jus-
tifications are used by the generation algorithm to
present the tradeoffs between alternative options
explicitly.
Additionally, the reasons why options have
been pruned from the tree are registered and pro-
vide information for the summarization of bad op-
tions in order to give the user a better overview of
the option space (e.g., ?All other flights are either
indirect or arrive too late.?). To keep summaries
about irrelevant options short, we back off to a de-
fault statement ?or are undesirable in some other
way.? if these options are very heterogeneous.
69
4.4 Presenting Clusters
4.4.1 Turn Length
In a spoken dialogue system, it is important not
to mention too many facts in one turn in order to
keep the memory load on the user manageable.
Obviously, it is not possible to present all of the
options and tradeoffs represented in the tree in a
single turn. Therefore, it is necessary to split the
tree into several smaller trees that can then be pre-
sented over several turns. In the current implemen-
tation, a heuristic cut-off point (no deeper than two
branching nodes and their children, which corre-
sponds to the nodes shown in Figure 5) is used.
This procedure produces a small set of options to
present in a turn and includes the most relevant ad-
vantages and disadvantages of an option. The next
turn is determined by the user?s choice indicating
which of the options she would like to hear more
about (for illustration see Figure 6).
4.4.2 Identifying Clusters
The identification of an option set is based on
its justification. If an option is justified by several
attributes, only one of them is chosen for identi-
fication. If one of the justifications is a contex-
tually salient attribute, this one is preferred, lead-
ing to constructions like: ?. . . you?d have to make
a connection in Brussels. If you want to fly di-
rect,. . . ?). Otherwise, the cluster is identified by
the highest ranked attribute e.g.,?There are four
flights with availability in business class.?. If an
option cluster has no compelling homogeneous at-
tribute, but only a common negative homogeneous
attribute, this situation is acknowledged: e.g., ?If
you?re willing to travel economy / arrive later / ac-
cept a longer travel time, . . . ?.
4.4.3 Summarizing Clusters
After the identification of a cluster, more in-
formation is given about the cluster. All positive
homogeneous attributes are mentioned and con-
trasted against all average or negative attributes.
An attribute that was used for identification of
an option is not mentioned again in the elabora-
tion. In opposition to a single flight, attributes may
have different values for the entities within a set of
flights. In that case, these attribute values need to
be summarized.
There are three main cases to be distinguished:
1. The continuous values for the attributes
price, arrival-time etc. need to be
summarized, as they may differ in their val-
ues even if they are in the same cluster. One
way to summarize them is to use an ex-
pression that reflects their value range, e.g.
?between x and y?. Another solution is to
mention only the evaluation value, leading to
sentences like ?The two flights with shortest
travel time? or ?The cheapest flights.?
2. For discrete-valued attributes with a
small number of possible values, e.g.,
number-of-legs and fare-class,
summarization is not an issue, because when
homogeneous for a cluster, the attribute
values of its options are identical.
3. The third group are attributes with catego-
rial values, e.g., ?airline?. If there are no
more than three different values, we summa-
rize using quantifications like ?none/all/both
of them?, as done in (Polifroni et al, 2003).
If the values are more diverse, the user model
comes back into play to produce a tailored
summary based on user preferences (e.g., lik-
ing KLM). For example, we would generate
?None are on KLM.?, which takes into ac-
count the user?s preference and is shorter than
mentioning all airlines the flights are on.
An issue arising from summarization with nega-
tion is that the negated value has to be salient, oth-
erwise the utterance might be irritating. For exam-
ple, it would be better to say ?These flights are not
direct.? in a neutral context, but ?You would not
need to connect in London Heathrow.? if London
Heathrow had already been mentioned.
A sample dialogue produced by our system,
when given the business user model (see Figure 4),
is shown in Figure 6.
5 Evaluation
A within-participants laboratory experiment was
conducted in order to determine whether user
model-based clustering leads to increased overall
user satisfaction, a better overview of the avail-
able options, quicker accessibility to the optimal
option and higher confidence of having heard all
relevant options. The experiment furthermore as-
sessed whether the options were presented in a
way that users found understandable and recorded
the time users took to read a dialogue turn.
70
U: Hello. I?d like to book a flight from Edinburgh to Brus-
sels for tomorrow arriving at around 3pm.
S: There are four flights with availability in business class.
None are on KLM. The only direct flight arrives at 5:30
pm, which is later than you requested. To arrive earlier,
you?ll have to make a connection.
If you?re willing to travel economy, there is a direct flight
on BMI, arriving at 1:10 pm. There?s also a KLM flight
arriving at 1:50 pm, but it requires a connection in Am-
sterdam. All other economy flights take longer or arrive
after 4 pm.
U: Can you tell me more about the business class flights that
arrive earlier?
S: The two flights with shortest travel time are both on
British Airways and cost 308 pounds. The first takes 4
hours and 20 minutes and arrives at 1:20 pm. The other
takes 4 hours and 30 minutes and arrives at 3:30pm.
There?s a cheaper flight on Lufthansa. It arrives at
3:10pm and costs just 270 pounds, but it takes 5 hours
and 35 minutes. All other business class flights arrive
very early or too late.
Figure 6: Example Dialogue with our System
5.1 Experimental Design
Each of the 38 subjects who completed the exper-
iment was presented with six dialogue pairs, the
first of which was used for training and was thus
not included in the analysis. Each dialogue pair
consisted of one dialogue between a user and our
system and one dialogue between the same user
and a system designed as described in (Polifroni
et al, 2003; Chung, 2004) (cf. Section 2.2). Some
of the dialogues with our system were constructed
manually based on the content selection and struc-
turing step, because the generation component did
not cover all linguistic constructions needed. The
dialogues with the Chung system were designed
manually, as this system is implemented for an-
other domain. The order of the dialogues in a pair
was randomized. The dialogues were provided as
transcripts.
After reading each dialogue transcript, partici-
pants were asked four questions about the system?s
responses. They provided their answers using Lik-
ert scales.
1. Did the system give the information in a way that was
easy to understand?
1: very hard to understand
7: very easy to understand
2. Did the system give you a good overview of the avail-
able options?
1: very poor overview
7: very good overview
3. Do you think there may be flights that are better options
for X1 that the system did not tell X1 about?
1X was instantiated by name of our example users.
1: I think that is very possible
7: I feel the system gave a good overview of all options
that are relevant for X1.
4. How quickly did the system allow X1 to find the opti-
mal flight?
1: slowly
3: quickly
After reading each pair of dialogues, the partic-
ipants were also asked the forced choice question:
?Which of the two systems would you recommend
to a friend?? to assess user satisfaction.
5.2 Results
A significant preference for our system was ob-
served. (In the diagrams, our system which com-
bines user modelling and stepwise refinement is
called UMSR, whereas the system based on Po-
lifroni?s approach is called SR.) There were a total
of 190 forced choices in the experiment (38 par-
ticipants * 5 dialogue pairs). UMSR was preferred
120 times (? 0.63%), whereas SR was preferred
only 70 times (? 0.37%). This difference is highly
significant (p < 0.001) using a two-tailed bino-
mial test. Thus, the null-hypothesis that both sys-
tems are preferred equally often can be rejected
with high confidence.
The evaluation results for the Likert scale ques-
tions confirmed our expectations. The SR dia-
logues received on average slightly higher scores
for understandability (question 1), which can be
explained by the shorter length of the system turns
for that system. However, the difference is not
statistically significant (p = 0.97 using a two-
tailed paired t-test). The differences in results
for the other questions are all highly statistically
significant, especially for question 2, assessing
the quality of overview of the options given by
the system responses, and question 3, assessing
the confidence that all relevant options were men-
tioned by the system. Both were significant at
p < 0.0001. These results confirm our hypothe-
sis that our strategy of presenting tradeoffs explic-
itly and summarizing irrelevant options improves
users? overview of the option space and also in-
creases their confidence in having heard about all
relevant options, and thus their confidence in the
system. The difference for question 4 (accessibil-
ity of the optimal option) is also statistically sig-
nificant (p < 0.001). Quite surprisingly, subjects
reported that they felt they could access options
more quickly even though the dialogues were usu-
ally longer. The average scores (based on 190 val-
71
Figure 7: Results for all Questions
ues) are shown in Figure 7.
To get a feel for whether the content given by
our system is too complex for oral presentation
and requires participants to read system turns sev-
eral times, we recorded reading times and corre-
lated them to the number of characters in a system
turn. We found a linear relation, which indicates
that participants did not re-read passages and is a
promising sign for the use of our strategy in SDS.
6 Conclusions and Future Work
In this paper, we have shown that information pre-
sentation in SDS can be improved by an approach
that combines a user model with structuring of
options through clustering of attributes and suc-
cessive refinement. In particular, when presented
with dialogues generated by a system that com-
bines user modelling with successive refinement
(UMSR) and one that uses refinement without ref-
erence to a user model (SR), participants reported
that the combined system provided them with a
better overview of the available options and that
they felt more certain to have been presented with
all relevant options. Although the presentation of
complex tradeoffs usually requires relatively long
system turns, participants were still able to cope
with the amount of information presented. For
some dialogues, subjects even felt they could ac-
cess relevant options more quickly despite longer
system turn length.
In future work, we would like to extend the clus-
tering algorithm to not use a fixed number of tar-
get clusters but to depend on the number of natural
clusters the data falls into. We would also like to
extend it to be more sensitive to the user model
when forming clusters (e.g., to be more sensitive
at lower price levels for a user for whom price is
very important than for a user who does not care
about price).
The explicit presentation of tradeoffs made by
the UMSR system in many cases leads to dialogue
turns that are more complex than typical dialogue
turns in the SR system. Even though participants
did not report that our system was harder to under-
stand, it would be interesting to investigate how
well users can understand and remember informa-
tion from the system when part of their concentra-
tion is absorbed by another task, for example when
using the system while driving a car.
Acknowledgments
We would like to thank the anonymous review-
ers for their comments. The research is supported
by the TALK project (European Community IST
project no. 507802), http://www.talk-project.org.
The first author was supported by Evangelisches
Studienwerk e.V. Villigst.
References
G. Carenini and J.D. Moore. 2001. An empirical study of
the influence of user tailoring on evaluative argument ef-
fectiveness. In Proc. of IJCAI 2001.
G. Chung. 2004. Developing a flexible spoken dialog system
using simulation. In Proc. of ACL ?04.
V. Demberg. 2005. Information presentation in spoken di-
alogue systems. Master?s thesis, School of Informatics,
University of Edinburgh.
J.D. Moore, M.E. Foster, O. Lemon, and M. White. 2004.
Generating tailored, comparative descriptions in spoken
dialogue. In Proc. of the 17th International Florida Artifi-
cial Intelligence Research Sociey Conference, AAAI Press.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards au-
tomatic generation of mixed-initiative dialogue systems
from web content. In Proc. of Eurospeech ?03, Geneva,
Switzerland, pp. 193?196.
Y. Qu and S. Beale. 1999. A constraint-based model for
cooperative response generation in information dialogues.
In AAAI/IAAI 1999 pp. 148?155.
M. Steedman 2000. Information structure and the syntax-
phonology interface. In Linguistic Inquiry, 31(4): 649?
689.
A. Stent, M.A. Walker, S. Whittaker, and P. Maloor. 2002.
User-tailored generation for spoken dialogue: an experi-
ment. In Proc. of ICSLP-02.
M.A. Walker, S. Whittaker, A. Stent, P. Maloor, J.D. Moore,
M. Johnston, and G. Vasireddy. 2004. Generation and
evaluation of user tailored responses in dialogue. In Cog-
nitive Science 28: 811-840.
M.A.Walker, R. Passonneau, and J.E. Boland. 2001. Quanti-
tative and qualitative evaluation of DARPA communicator
spoken dialogue systems. In Proc of ACL-01.
72
Automatic Segmentation of Multiparty Dialogue
Pei-Yun Hsueh
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
p.hsueh@ed.ac.uk
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
J.Moore@ed.ac.uk
Steve Renals
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
s.renals@ed.ac.uk
Abstract
In this paper, we investigate the prob-
lem of automatically predicting segment
boundaries in spoken multiparty dialogue.
We extend prior work in two ways. We
first apply approaches that have been pro-
posed for predicting top-level topic shifts
to the problem of identifying subtopic
boundaries. We then explore the impact
on performance of using ASR output as
opposed to human transcription. Exam-
ination of the effect of features shows
that predicting top-level and predicting
subtopic boundaries are two distinct tasks:
(1) for predicting subtopic boundaries,
the lexical cohesion-based approach alone
can achieve competitive results, (2) for
predicting top-level boundaries, the ma-
chine learning approach that combines
lexical-cohesion and conversational fea-
tures performs best, and (3) conversational
cues, such as cue phrases and overlapping
speech, are better indicators for the top-
level prediction task. We also find that
the transcription errors inevitable in ASR
output have a negative impact on models
that combine lexical-cohesion and conver-
sational features, but do not change the
general preference of approach for the two
tasks.
1 Introduction
Text segmentation, i.e., determining the points at
which the topic changes in a stream of text, plays
an important role in applications such as topic
detection and tracking, summarization, automatic
genre detection and information retrieval and ex-
traction (Pevzner and Hearst, 2002). In recent
work, researchers have applied these techniques
to corpora such as newswire feeds, transcripts of
radio broadcasts, and spoken dialogues, in order
to facilitate browsing, information retrieval, and
topic detection (Allan et al, 1998; van Mulbregt
et al, 1999; Shriberg et al, 2000; Dharanipragada
et al, 2000; Blei and Moreno, 2001; Christensen
et al, 2005). In this paper, we focus on segmenta-
tion of multiparty dialogues, in particular record-
ings of small group meetings. We compare mod-
els based solely on lexical information, which are
common in approaches to automatic segmentation
of text, with models that combine lexical and con-
versational features. Because tasks as diverse as
browsing, on the one hand, and summarization, on
the other, require different levels of granularity of
segmentation, we explore the performance of our
models for two tasks: hypothesizing where ma-
jor topic changes occur and hypothesizing where
more subtle nested topic shifts occur.
In addition, because we do not wish to make the
assumption that high quality transcripts of meet-
ing records, such as those produced by human
transcribers, will be commonly available, we re-
quire algorithms that operate directly on automatic
speech recognition (ASR) output.
2 Previous Work
Prior research on segmentation of spoken ?docu-
ments? uses approaches that were developed for
text segmentation, and that are based solely on
textual cues. These include algorithms based on
lexical cohesion (Galley et al, 2003; Stokes et
al., 2004), as well as models using annotated fea-
tures (e.g., cue phrases, part-of-speech tags, coref-
erence relations) that have been determined to cor-
relate with segment boundaries (Gavalda et al,
1997; Beeferman et al, 1999). Blei et al (2001)
273
and van Mulbregt et al (1999) use topic lan-
guage models and variants of the hidden Markov
model (HMM) to identify topic segments. Recent
systems achieve good results for predicting topic
boundaries when trained and tested on human
transcriptions. For example, Stokes et al (2004)
report an error rate (Pk) of 0.25 on segmenting
broadcast news stories using unsupervised lexical
cohesion-based approaches. However, topic seg-
mentation of multiparty dialogue seems to be a
considerably harder task. Galley et al (2003) re-
port an error rate (Pk) of 0.319 for the task of pre-
dicting major topic segments in meetings.1
Although recordings of multiparty dialogue
lack the distinct segmentation cues commonly
found in text (e.g., headings, paragraph breaks,
and other typographic cues) or news story segmen-
tation (e.g., the distinction between anchor and
interview segments), they contain conversation-
based features that may be of use for automatic
segmentation. These include silence, overlap rate,
speaker activity change (Galley et al, 2003), and
cross-speaker linking information, such as adja-
cency pairs (Zechner and Waibel, 2000). Many
of these features can be expected to be compli-
mentary. For segmenting spontaneous multiparty
dialogue into major topic segments, Galley et
al. (2003) have shown that a model integrating lex-
ical and conversation-based features outperforms
one based on solely lexical cohesion information.
However, the automatic segmentation models
in prior work were developed for predicting top-
level topic segments. In addition, compared to
read speech and two-party dialogue, multi-party
dialogues typically exhibit a considerably higher
word error rate (WER) (Morgan et al, 2003).
We expect that incorrectly recognized words will
impair the robustness of lexical cohesion-based
approaches and extraction of conversation-based
discourse cues and other features. Past research
on broadcast news story segmentation using ASR
transcription has shown performance degradation
from 5% to 38% using different evaluation metrics
(van Mulbregt et al, 1999; Shriberg et al, 2000;
Blei and Moreno, 2001). However, no prior study
has reported directly on the extent of this degra-
dation on the performance of a more subtle topic
segmentation task and in spontaneous multiparty
dialogue. In this paper, we extend prior work by
1For the definition of Pk and Wd, please refer to section
3.4.1
investigating the effect of using ASR output on the
models that have previously been proposed. In ad-
dition, we aim to find useful features and models
for the subtopic prediction task.
3 Method
3.1 Data
In this study, we used the ICSI meeting corpus
(LDC2004S02). Seventy-five natural meetings of
ICSI research groups were recorded using close-
talking far field head-mounted microphones and
four desktop PZM microphones. The corpus in-
cludes human transcriptions of all meetings. We
added ASR transcriptions of all 75 meetings which
were produced by Hain (2005), with an average
WER of roughly 30%.
The ASR system used a vocabulary of 50,000
words, together with a trigram language model
trained on a combination of in-domain meeting
data, related texts found by web search, conver-
sational telephone speech (CTS) transcripts and
broadcast news transcripts (about 109 words in to-
tal), resulting in a test-set perplexity of about 80.
The acoustic models comprised a set of context-
dependent hidden Markov models, using gaussian
mixture model output distributions. These were
initially trained on CTS acoustic training data, and
were adapted to the ICSI meetings domain using
maximum a posteriori (MAP) adaptation. Further
adaptation to individual speakers was achieved us-
ing vocal tract length normalization and maximum
likelihood linear regression. A four-fold cross-
validation technique was employed: four recog-
nizers were trained, with each employing 75% of
the ICSI meetings as acoustic and language model
training data, and then used to recognize the re-
maining 25% of the meetings.
3.2 Fine-grained and coarse-grained topics
We characterize a dialogue as a sequence of top-
ical segments that may be further divided into
subtopic segments. For example, the 60 minute
meeting Bed003, whose theme is the planning of
a research project on automatic speech recognition
can be described by 4 major topics, from ?open-
ing? to ?general discourse features for higher lay-
ers? to ?how to proceed? to ?closing?. Depending
on the complexity, each topic can be further di-
vided into a number of subtopics. For example,
?how to proceed? can be subdivided to 4 subtopic
segments, ?segmenting off regions of features?,
274
?ad-hoc probabilities?, ?data collection? and ?ex-
perimental setup?.
Three human annotators at our site used a tai-
lored tool to perform topic segmentation in which
they could choose to decompose a topic into
subtopics, with at most three levels in the resulting
hierarchy. Topics are described to the annotators
as what people in a meeting were talking about.
Annotators were asked to provide a free text la-
bel for each topic segment; they were encour-
aged to use keywords drawn from the transcrip-
tion in these labels, and we provided some stan-
dard labels for non-content topics, such as ?open-
ing? and ?chitchat?, to impose consistency. For
our initial experiments with automatic segmenta-
tion at different levels of granularity, we flattened
the subtopic structure and consider only two levels
of segmentation?top-level topics and all subtopics.
To establish reliability of our annotation proce-
dure, we calculated kappa statistics between the
annotations of each pair of coders. Our analy-
sis indicates human annotators achieve ? = 0.79
agreement on top-level segment boundaries and
? = 0.73 agreement on subtopic boundaries. The
level of agreement confirms good replicability of
the annotation procedure.
3.3 Probabilistic models
Our goal is to investigate the impact of ASR er-
rors on the selection of features and the choice of
models for segmenting topics at different levels of
granularity. We compare two segmentation mod-
els: (1) an unsupervised lexical cohesion-based
model (LM) using solely lexical cohesion infor-
mation, and (2) feature-based combined models
(CM) that are trained on a combination of lexical
cohesion and conversational features.
3.3.1 Lexical cohesion-based model
In this study, we use Galley et al?s (2003)
LCSeg algorithm, a variant of TextTiling (Hearst,
1997). LCSeg hypothesizes that a major topic
shift is likely to occur where strong term repeti-
tions start and end. The algorithm works with two
adjacent analysis windows, each of a fixed size
which is empirically determined. For each utter-
ance boundary, LCSeg calculates a lexical cohe-
sion score by computing the cosine similarity at
the transition between the two windows. Low sim-
ilarity indicates low lexical cohesion, and a sharp
change in lexical cohesion score indicates a high
probability of an actual topic boundary. The prin-
cipal difference between LCSeg and TextTiling is
that LCSeg measures similarity in terms of lexical
chains (i.e., term repetitions), whereas TextTiling
computes similarity using word counts.
3.3.2 Integrating lexical and
conversation-based features
We also used machine learning approaches that
integrate features into a combined model, cast-
ing topic segmentation as a binary classification
task. Under this supervised learning scheme, a
training set in which each potential topic bound-
ary2 is labelled as either positive (POS) or neg-
ative (NEG) is used to train a classifier to pre-
dict whether each unseen example in the test set
belongs to the class POS or NEG. Our objective
here is to determine whether the advantage of in-
tegrating lexical and conversational features also
improves automatic topic segmentation at the finer
granularity of subtopic levels, as well as when
ASR transcriptions are used.
For this study, we trained decision trees (c4.5)
to learn the best indicators of topic boundaries.
We first used features extracted with the optimal
window size reported to perform best in Galley et
al. (2003) for segmenting meeting transcripts into
major topical units. In particular, this study uses
the following features: (1) lexical cohesion fea-
tures: the raw lexical cohesion score and proba-
bility of topic shift indicated by the sharpness of
change in lexical cohesion score, and (2) conver-
sational features: the number of cue phrases in
an analysis window of 5 seconds preceding and
following the potential boundary, and other inter-
actional features, including similarity of speaker
activity (measured as a change in probability dis-
tribution of number of words spoken by each
speaker) within 5 seconds preceding and follow-
ing each potential boundary, the amount of over-
lapping speech within 30 seconds following each
potential boundary, and the amount of silence be-
tween speaker turns within 30 seconds preceding
each potential boundary.
3.4 Evaluation
To compare to prior work, we perform a 25-
fold leave-one-out cross validation on the set of
25 ICSI meetings that were used in Galley et
2In this study, the end of each speaker turn is a potential
segment boundary. If there is a pause of more than 1 second
within a single speaker turn, the turn is divided at the begin-
ning of the pause creating a potential segment boundary.
275
al. (2003). We repeated the procedure to eval-
uate the accuracy using the lexical cohesion and
combined models on both human and ASR tran-
scriptions. In each evaluation, we trained the au-
tomatic segmentation models for two tasks: pre-
dicting subtopic boundaries (SUB) and predicting
only top-level boundaries (TOP).
3.4.1 Evaluation metrics
In order to be able to compare our results di-
rectly with previous work, we first report our re-
sults using the standard error rate metrics of Pk
and Wd. Pk (Beeferman et al, 1999) is the prob-
ability that two utterances drawn randomly from a
document (in our case, a meeting transcript) are in-
correctly identified as belonging to the same topic
segment. WindowDiff (Wd) (Pevzner and Hearst,
2002) calculates the error rate by moving a sliding
window across the meeting transcript counting the
number of times the hypothesized and reference
segment boundaries are different.
3.4.2 Baseline
To compute a baseline, we follow Kan (2003)
and Hearst (1997) in using Monte Carlo simu-
lated segments. For the corpus used as training
data in the experiments, the probability of a poten-
tial topic boundary being an actual one is approxi-
mately 2.2% for all subtopic segments, and 0.69%
for top-level topic segments. Therefore, the Monte
Carlo simulation algorithm predicts that a speaker
turn is a segment boundary with these probabilities
for the two different segmentation tasks. We exe-
cuted the algorithm 10,000 times on each meeting
and averaged the scores to form the baseline for
our experiments.
3.4.3 Topline
For the 24 meetings that were used in training,
we have top-level topic boundaries annotated by
coders at Columbia University (Col) and in our lab
at Edinburgh (Edi). We take the majority opinion
on each segment boundary from the Col annota-
tors as reference segments. For the Edi annota-
tions of top-level topic segments, where multiple
annotations exist, we choose one randomly. The
topline is then computed as the Pk score compar-
ing the Col majority annotation to the Edi annota-
tion.
4 Results
4.1 Experiment 1: Predicting top-level and
subtopic segment boundaries
The meetings in the ICSI corpus last approxi-
mately 1 hour and have an average of 8-10 top-
level topic segments. In order to facilitate meet-
ing browsing and question-answering, we believe
it is useful to include subtopic boundaries in or-
der to narrow in more accurately on the portion
of the meeting that contains the information the
user needs. Therefore, we performed experiments
aimed at analysing how the LM and CM seg-
mentation models behave in predicting segment
boundaries at the two different levels of granular-
ity.
All of the results are reported on the test set.
Table 1 shows the performance of the lexical co-
hesion model (LM) and the combined model (CM)
integrating the lexical cohesion and conversational
features discussed in Section 3.3.2.3 For the task
of predicting top-level topic boundaries from hu-
man transcripts, CM outperforms LM. LM tends
to over-predict on the top-level, resulting in a
higher false alarm rate. However, for the task of
predicting subtopic shifts, LM alone is consider-
ably better than CM.
Error Rate Transcript ASR
Models Pk Wd Pk Wd
LM SUB 32.31% 38.18% 32.91% 37.13%
(LCSeg) TOP 36.50% 46.57% 38.02% 48.18%
CM SUB 36.90% 38.68% 38.19% n/a
(C4.5) TOP 28.35% 29.52% 28.38% n/a
Table 1: Performance comparison of probabilistic
segmentation models.
In order to support browsing during the meeting
or shortly thereafter, automatic topic segmentation
will have to operate on the transcriptions produced
by ASR. First note from Table 1 that the prefer-
ence of models for segmentation at the two differ-
ent levels of granularity is the same for ASR and
human transcriptions. CM is better for predicting
top-level boundaries and LM is better for predict-
ing subtopic boundaries. This suggests that these
3We do not report Wd scores for the combined model
(CM) on ASR output because this model predicted 0 segment
boundaries when operating on ASR output. In our experi-
ence, CM routinely underpredicted the number of segment
boundaries, and due to the nature of the Wd metric, it should
not be used when there are 0 hypothesized topic boundaries.
276
are two distinct tasks, regardless of whether the
system operates on human produced transcription
or ASR output. Subtopics are better characterized
by lexical cohesion, whereas top-level topic shifts
are signalled by conversational features as well as
lexical-cohesion based features.
4.1.1 Effect of feature combinations:
predicting from human transcripts
Next, we wish to determine which features in
the combined model are most effective for predict-
ing topic segments at the two levels of granularity.
Table 2 gives the average Pk for all 25 meetings
in the test set, using the features described in Sec-
tion 3.3.2. We group the features into four classes:
(1) lexical cohesion-based features (LF): including
lexical cohesion value (LCV) and estimated pos-
terior probability (LCP); (2) interaction features
(IF): the amount of overlapping speech (OVR),
the amount of silence between speaker segments
(GAP), similarity of speaker activity (ACT); (3)
cue phrase feature (CUE); and (4) all available fea-
tures (ALL). For comparison we also report the
baseline (see Section 3.4.2) generated by Monte
Carlo algorithm (MC-B). All of the models us-
ing one or more features from these classes out-
perform the baseline model. A one-way ANOVA
revealed this reliable effect on the top-level seg-
mentation (F (7, 192) = 17.46, p < 0.01) as well
as on the subtopic segmentation task (F (7, 192) =
5.862, p < 0.01).
TRANSCRIPT Error Rate(Pk)
Feature set SUB TOP
MC-B 46.61% 48.43%
LF(LCV+LCP) 38.13% 29.92%
IF(ACT+OVR+GAP) 38.87% 30.11%
IF+CUE 38.87% 30.11%
LF+ACT 38.70% 30.10%
LF+OVR 38.56% 29.48%
LF+GAP 38.50% 29.87%
LF+IF 38.11% 29.61%
LF+CUE 37.46% 29.18%
ALL(LF+IF+CUE) 36.90% 28.35%
Table 2: Effect of different feature combinations
for predicting topic boundaries from human tran-
scripts. MC-B is the randomly generated baseline.
As shown in Table 2, the best performing model
for predicting top-level segments is the one us-
ing all of the features (ALL). This is not surpris-
ing, because these were the features that Galley
et al (2003) found to be most effective for pre-
dicting top-level segment boundaries in their com-
bined model. Looking at the results in more de-
tail, we see that when we begin with LF features
alone and add other features one by one, the only
model (other than ALL) that achieves significant4
improvement (p < 0.05) over LF is LF+CUE,
the model that combines lexical cohesion features
with cue phrases.
When we look at the results for predicting
subtopic boundaries, we again see that the best
performing model is the one using all features
(ALL). Models using lexical-cohesion features
alone (LF) and lexical cohesion features with cue
phrases (LF+CUE) both yield significantly better
results than using interactional features (IF) alone
(p < 0.01), or using them with cue phrase features
(IF+CUE) (p < 0.01). Again, none of the interac-
tional features used in combination with LF sig-
nificantly improves performance. Indeed, adding
speaker activity change (LF+ACT) degrades the
performance (p < 0.05).
Therefore, we conclude that for predicting both
top-level and subtopic boundaries from human
transcriptions, the most important features are the
lexical cohesion based features (LF), followed
by cue phrases (CUE), with interactional features
contributing to improved performance only when
used in combination with LF and CUE.
However, a closer look at the Pk scores in Ta-
ble 2, adds further evidence to our hypothesis that
predicting subtopics may be a different task from
predicting top-level topics. Subtopic shifts oc-
cur more frequently, and often without clear con-
versational cues. This is suggested by the fact
that absolute performance on subtopic prediction
degrades when any of the interactional features
are combined with the lexical cohesion features.
In contrast, the interactional features slightly im-
prove performance when predicting top-level seg-
ments. Moreover, the fact that the feature OVR
has a positive impact on the model for predicting
top-level topic boundaries, but does not improve
the model for predicting subtopic boundaries re-
veals that having less overlapping speech is a more
prominent phenomenon in major topic shifts than
4Because we do not wish to make assumptions about the
underlying distribution of error rates, and error rates are not
measured on an interval level, we use a non-parametric sign
test throughout these experiments to compute statistical sig-
nificance.
277
in subtopic shifts.
4.1.2 Effect of feature combinations:
predicting from ASR output
Features extracted from ASR transcripts are dis-
tinct from those extracted from human transcripts
in at least three ways: (1) incorrectly recognized
words incur erroneous lexical cohesion features
(LF), (2) incorrectly recognized words incur erro-
neous cue phrase features (CUE), and (3) the ASR
system recognizes less overlapping speech (OVR).
In contrast to the finding that integrating conver-
sational features with lexical cohesion features is
useful for prediction from human transcripts, Ta-
ble 3 shows that when operating on ASR output,
neither adding interactional nor cue phrase fea-
tures improves the performance of the model using
only lexical cohesion features. In fact, the model
using all features (ALL) is significantly worse than
the model using only lexical cohesion based fea-
tures (LF). This suggests that we must explore new
features that can lessen the perplexity introduced
by ASR outputs in order to train a better model.
ASR Error Rate(Pk)
Feature set SUB TOP
MC-B 43.41% 45.22%
LF(LCV+LCP) 36.83% 25.27%
IF(ACT+OVR+GAP) 36.83% 25.27%
IF+CUE 36.83% 25.27%
LF+GAP 36.67% 24.62%
LF+IF 36.83% 28.24%
LF+CUE 37.42% 25.27%
ALL(LF+IF+CUE) 38.19% 28.38%
Table 3: Effect of different feature combinations
for predicting topic boundaries from ASR output.
4.2 Experiment 2: Statistically learned cue
phrases
In prior work, Galley et al (2003) empirically
identified cue phrases that are indicators of seg-
ment boundaries, and then eliminated all cues that
had not previously been identified as cue phrases
in the literature. Here, we conduct an experiment
to explore how different ways of identifying cue
phrases can help identify useful new features for
the two boundary prediction tasks.
In each fold of the 25-fold leave-one-out cross
validation, we use a modified5 Chi-square test to
5In order to satisfy the mathematical assumptions under-
calculate statistics for each word (unigram) and
word pair (bi-gram) that occurred in the 24 train-
ing meetings. We then rank unigrams and bigrams
according to their Chi-square scores, filtering out
those with values under 6.64, the threshold for the
Chi-square statistic at the 0.01 significance level.
The unigrams and bigrams in this ranked list are
the learned cue phrases. We then use the occur-
rence counts of cue phrases in an analysis window
around each potential topic boundary in the test
meeting as a feature.
Table 4 shows the performance of models that
use statistically learned cue phrases in their feature
sets compared with models using no cue phrase
features and Galley?s model, which only uses cue
phrases that correspond to those identified in the
literature (Col-cue). We see that for predicting
subtopics, models using the cue word features
(1gram) and the combination of cue words and bi-
grams (1+2gram) yield a 15% and 8.24% improve-
ment over models using no cue features (NOCUE)
(p < 0.01) respectively, while models using only
cue phrases found in the literature (Col-cue) im-
prove performance by just 3.18%. In contrast, for
predicting top-level topics, the model using cue
phrases from the literature (Col-cue) achieves a
4.2% improvement, and this is the only model that
produces statistically significantly better results
than the model using no cue phrases (NOCUE).
The superior performance of models using statis-
tically learned cue phrases as features for predict-
ing subtopic boundaries suggests there may exist a
different set of cue phrases that serve as segmen-
tation cues for subtopic boundaries.
5 Discussion
As observed in the corpus of meetings, the lack
of macro-level segment units (e.g., story breaks,
paragraph breaks) makes the task of segmenting
spontaneous multiparty dialogue, such as meet-
ings, different from segmenting text or broadcast
news. Compared to the task of segmenting expos-
itory texts reported in Hearst (1997) with a 39.1%
chance of each paragraph end being a target topic
boundary, the chance of each speaker turn be-
ing a top-level or sub-topic boundary in our ICSI
corpus is just 2.2% and 0.69%. The imbalanced
class distribution has a negative effect on the per-
lying the test, we removed cases with an expected value that
is under a threshold (in this study, we use 1), and we apply
Yate?s correction, (|ObservedV alue?ExpectedV alue| ?
0.5)2/ExpectedV alue.
278
NOCUE Col-cue 1gram 2gram 1+2gram MC-B Topline
SUB 38.11% 36.90% 32.39% 36.86% 34.97% 46.61% n/a
TOP 29.61% 28.35% 28.95% 29.20% 29.27% 48.43% 13.48%
Table 4: Performance of models trained with cue phrases from the literature (Col-cue) and cue phrases
learned from statistical tests, including cue words (1gram), cue word pairs (2gram), and cue phrases
composed of both words and word pairs (1+2gram). NOCUE is the model using no cue phrase features.
The Topline is the agreement of human annotators on top-level segments.
0 10 20 30 40 50 60 70 80
0.26
0.28
0.3
0.32
0.34
0.36
0.38
0.4
0.42
0.44
Training Set Size (In meetings)
E
rr
or
 R
at
e 
(P
k)
TRAN?ALL
TRAN?TOP
ASR?ALL
ASR?TOP
Figure 1: Performance of the combined model
over the increase of the training set size.
formance of machine learning approaches. In a
pilot study, we investigated sampling techniques
that rebalance the class distribution in the train-
ing set. We found that sampling techniques pre-
viously reported in Liu et al(2004) as useful for
dealing with an imbalanced class distribution in
the task of disfluency detection and sentence seg-
mentation do not work for this particular data set.
The implicit assumption of some classifiers (such
as pruned decision trees) that the class distribution
of the test set matches that of the training set, and
that the costs of false positives and false negatives
are equivalent, may account for the failure of these
sampling techniques to yield improvements in per-
formance, when measured using Pk and Wd.
Another approach that copes with the im-
balanced class prediction problem but does not
change the natural class distribution is to increase
the size of the training set. We conducted an ex-
periment in which we incrementally increased the
training set size by randomly choosing ten meet-
ings each time until all meetings were selected.
We executed the process three times and averaged
the scores to obtain the results shown in Figure 1.
However, increasing training set size adds to the
perplexity in the training phase. We see that in-
creasing the size of the training set only improves
the accuracy of segment boundary prediction for
predicting top-level topics on ASR output. The
figure also indicates that training a model to pre-
dict top-level boundaries requires no more than fif-
teen meetings in the training set to reach a reason-
able level of performance.
6 Conclusions
Discovering major topic shifts and finding nested
subtopics are essential for the success of speech
document browsing and retrieval. Meeting records
contain rich information, in both content and con-
versation behavioral form, that enable automatic
topic segmentation at different levels of granular-
ity. The current study demonstrates that the two
tasks ? predicting top-level and subtopic bound-
aries ? are distinct in many ways: (1) for pre-
dicting subtopic boundaries, the lexical cohesion-
based approach achieves results that are com-
petitive with the machine learning approach that
combines lexical and conversational features; (2)
for predicting top-level boundaries, the machine
learning approach performs the best; and (3) many
conversational cues, such as overlapping speech
and cue phrases discussed in the literature, are
better indicators for top-level topic shifts than
for subtopic shifts, but new features such as cue
phrases can be learned statistically for the subtopic
prediction task. Even in the presence of a rela-
tively higher word error rate, using ASR output
makes no difference to the preference of model for
the two tasks. The conversational features also did
not help improve the performance for predicting
from ASR output.
In order to further identify useful features for
automatic segmentation of meetings at different
levels of granularity, we will explore the use of
279
multimodal, i.e., acoustic and visual, cues. In ad-
dition, in the current study, we only extracted fea-
tures from within the analysis windows immedi-
ately preceding and following each potential topic
boundary; we will explore models that take into
account features of longer range dependencies.
7 Acknowledgements
Many thanks to Jean Carletta for her invaluable
help in managing the data, and for advice and
comments on the work reported in this paper.
Thanks also to the AMI ASR group for produc-
ing the ASR transcriptions, and to the anonymous
reviewers for their helpful comments. This work
was supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811).
References
J. Allan, J.G. Carbonell, G. Doddington, J. Yamron,
and Y. Yang. 1998. Topic detection and tracking pi-
lot study: Final report. In Proceedings of the DARPA
Broadcast News Transcription and Understanding
Workshop.
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statis-
tical models for text segmentation. Machine Learn-
ing, 34:177?210.
D. M. Blei and P. J. Moreno. 2001. Topic segmentation
with an aspect hidden Markov model. In Proceed-
ings of the 24th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval. ACM Press.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2005. Maximum entropy segmentation of broad-
cast news. In Proceedings of the IEEE International
Conference on Acoustic, Speech, and Signal Pro-
cessing, Philadelphia, USA.
S. Dharanipragada, M. Franz, J.S. McCarley, K. Pap-
ineni, S. Roukos, T. Ward, and W. J. Zhu. 2000.
Statistical methods for topic segmentation. In Pro-
ceedings of the International Conference on Spoken
Language Processing, pages 516?519.
M. Galley, K. McKeown, E. Fosler-Lussier, and
H. Jing. 2003. Discourse segmentation of multi-
party conversation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
M. Gavalda, K. Zechner, and G. Aist. 1997. High per-
formance segmentation of spontaneous speech using
part of speech and trigger word information. In Pro-
ceedings of the Fifth ANLP Conference, pages 12?
15.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, and S. Renals. 2005. Tran-
scription of conference room meetings: an investi-
gation. In Proceedings of Interspeech.
M. Hearst. 1997. Texttiling: Segmenting text into mul-
tiparagraph subtopic passages. Computational Lin-
guistics, 25(3):527?571.
M. Kan. 2003. Automatic text summarization as
applied to information retrieval: Using indicative
and informative summaries. Ph.D. thesis, Columbia
University, New York USA.
Y. Liu, E. Shriberg, A. Stolcke, and M. Harper. 2004.
Using machine learning to cope with imbalanced
classes in natural sppech: Evidence from sentence
boundary and disfluency detection. In Proceedings
of the Intl. Conf. Spoken Language Processing.
N. Morgan, D. Baron, S. Bhagat, H. Carvey,
R. Dhillon, J. Edwards, D. Gelbart, A. Janin,
A. Krupski, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, , and C. Wooters. 2003. Meetings about meet-
ings: research at icsi on speech in multiparty conver-
sations. In Proceedings of the IEEE International
Conference on Acoustic, Speech, and Signal Pro-
cessing.
L. Pevzner and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19?36.
E. Shriberg, A. Stolcke, D. Hakkani-tur, and G. Tur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Commu-
nications, 31(1-2):127?254.
N. Stokes, J. Carthy, and A.F. Smeaton. 2004. Se-
lect: a lexical cohesion based news story segmenta-
tion system. AI Communications, 17(1):3?12, Jan-
uary.
P. van Mulbregt, J. Carp, L. Gillick, S. Lowe, and
J. Yamron. 1999. Segmentation of automatically
transcribed broadcast news text. In Proceedings of
the DARPA Broadcast News Workshop, pages 77?
80. Morgan Kaufman Publishers.
Klaus Zechner and Alex Waibel. 2000. DIASUMM:
Flexible summarization of spontaneous dialogues in
unrestricted domains. In Proceedings of COLING-
2000, pages 968?974.
280
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 367?374,
New York, June 2006. c?2006 Association for Computational Linguistics
Incorporating Speaker and Discourse Features into Speech
Summarization
Gabriel Murray, Steve Renals,
Jean Carletta, Johanna Moore
University of Edinburgh, School of Informatics
Edinburgh EH8 9LW, Scotland
gabriel.murray@ed.ac.uk, s.renals@ed.ac.uk,
jeanc@inf.ed.ac.uk, j.moore@ed.ac.uk
Abstract
We have explored the usefulness of incorporat-
ing speech and discourse features in an automatic
speech summarization system applied to meeting
recordings from the ICSI Meetings corpus. By an-
alyzing speaker activity, turn-taking and discourse
cues, we hypothesize that such a system can out-
perform solely text-based methods inherited from
the field of text summarization. The summariza-
tion methods are described, two evaluation meth-
ods are applied and compared, and the results
clearly show that utilizing such features is advanta-
geous and efficient. Even simple methods relying
on discourse cues and speaker activity can outper-
form text summarization approaches.
1. Introduction
The task of summarizing spontaneous spoken di-
alogue from meetings presents many challenges:
information is sparse; speech is disfluent and frag-
mented; automatic speech recognition is imper-
fect. However, there are numerous speech-specific
characteristics to be explored and taken advantage
of. Previous research on summarizing speech has
concentrated on utilizing prosodic features [1, 2].
We have examined the usefulness of additional
speech-specific characteristics such as discourse
cues, speaker activity, and listener feedback. This
speech features approach is contrasted with a sec-
ond summarization approach using only textual
features?a centroid method [3] using a latent se-
mantic representation of utterances. These indi-
vidual approaches are compared to a combined ap-
proach as well as random baseline summaries.
This paper also introduces a new evalua-
tion scheme for automatic summaries of meeting
recordings, using a weighted precision score based
on multiple human annotations of each meeting
transcript. This evaluation scheme is described
in detail below and is motivated by previous find-
ings [4] suggesting that n-gram based metrics like
ROUGE [5] do not correlate well in this domain.
2. Previous Work
In the field of speech summarization in general, re-
search investigating speech-specific characteristics
has focused largely on prosodic features such as F0
mean and standard deviation, pause information,
syllable duration and energy. Koumpis and Re-
nals [1] investigated prosodic features for summa-
rizing voicemail messages in order to send voice-
mail summaries to mobile devices. Hori et al [6]
have developed an integrated speech summariza-
tion approach, based on finite state transducers, in
which the recognition and summarization compo-
nents are composed into a single finite state trans-
ducer, reporting results on a lecture summariza-
tion task. In the Broadcast News domain, Maskey
and Hirschberg [7] found that the best summariza-
tion results utilized prosodic, lexical, and structural
features, while Ohtake et al [8] explored using
only prosodic features for summarization. Maskey
and Hirschberg similarly found that prosodic fea-
tures alone resulted in good quality summaries of
367
Broadcast News.
In the meetings domain (using the ICSI cor-
pus), Murray et al [2] compared text summariza-
tion approaches with feature-based approaches us-
ing prosodic features, with human judges favoring
the feature-based approaches. Zechner [9] inves-
tigated summarizing several genres of speech, in-
cluding spontaneous meeting speech. Though rel-
evance detection in his work relied largely on tf.idf
scores, Zechner also explored cross-speaker infor-
mation linking and question/answer detection, so
that utterances could be extracted not only accord-
ing to high tf.idf scores, but also if they were linked
to other informative utterances.
Similarly, this work aims to detect important
utterances that may not be detectable according
to lexical features or prosodic prominence, but
are nonetheless linked to high speaker activity,
decision-making, or meeting structure.
3. Summarization Approaches
The following subsections give detailed descrip-
tions of our two summarization systems, one of
which focuses on speech and discourse features
while the other utilizes text summarization tech-
niques and latent semantic analysis.
3.1. Speech and Discourse Features
In previous summarization work on the ICSI cor-
pus [2, 4], Murray et al explored multiple ways
of applying latent semantic analysis (LSA) to a
term/document matrix of weighted term frequen-
cies from a given meeting, a development of the
method in [10]. A central insight to the present
work is that additional features beyond simple term
frequencies can be included in the matrix before
singular value decomposition (SVD) is carried out.
We can use SVD to project this matrix of features
to a lower dimensionality space, subsequently ap-
plying the same methods as used in [2] for extract-
ing sentences.
The features used in these experiments in-
cluded features of speaker activity, discourse cues,
listener feedback, simple keyword spotting, meet-
ing location and dialogue act length (in words).
For each dialogue act, there are features indi-
cating which speaker spoke the dialogue act and
whether the same speaker spoke the preceding and
succeeding dialogue acts. Another set of features
indicates how many speakers are active on either
side of a given dialogue act: specifically, how
many speakers were active in the preceding and
succeeding five dialogue acts. To further gauge
speaker activity, we located areas of high speaker
interaction and indicated whether or not a given
dialogue act immediately preceded this region of
activity, with the motivation being that informa-
tive utterances are often provocative in eliciting re-
sponses and interaction. Additionally, we included
a feature indicating which speakers most often ut-
tered dialogue acts that preceded high levels of
speaker interaction, as one way of gauging speaker
status in the meeting. Another feature relating to
speaker activity gives each dialogue act a score ac-
cording to how active the speaker is in the meeting
as a whole, based on the intuition that the most ac-
tive speakers will tend to utter the most important
dialogue acts.
The features for discourse cues, listener feed-
back, and keyword spotting were deliberately su-
perficial, all based simply on detecting informative
words. The feature for discourse cues indicates the
presence or absence of words such as decide, dis-
cuss, conclude, agree, and fragments such as we
should indicating a planned course of action. Lis-
tener feedback was based on the presence or ab-
sence of positive feedback cues following a given
dialogue act; these include responses such as right,
exactly and yeah. Keyword spotting was based
on frequent words minus stopwords, indicating the
presence or absence of any of the top twenty non-
stopword frequent words. The discourse cues of
interest were derived from a manual corpus analy-
sis rather than being automatically detected.
A structural feature scored dialogue acts ac-
cording to their position in the meeting, with di-
alogue acts from the middle to later portion of the
meeting scoring higher and dialogue acts at the be-
ginning and very end scoring lower. This is a fea-
ture that is well-matched to the relatively unstruc-
tured ICSI meetings, as many meetings would be
expected to have informative proposals and agen-
das at the beginning and perhaps summary state-
ments and conclusions at the end.
Finally, we include a dialogue act length fea-
ture motivated by the fact that informative utter-
ances will tend to be longer than others.
The extraction method follows [11] by rank-
ing sentences using an LSA sentence score. The
368
matrix of features is decomposed as follows:
A = USV T
where U is an m?n matrix of left-singular vectors,
S is an n ? n diagonal matrix of singular values,
and V is the n?n matrix of right-singular vectors.
Using sub-matrices S and V T , the LSA sentence
scores are obtained using:
ScLSAi =
?
?
?
?
n
?
k=1
v(i, k)2 ? ?(k)2 ,
where v(i, k) is the kth element of the ith sen-
tence vector and ?(k) is the corresponding singular
value.
Experiments on a development set of 55 ICSI
meetings showed that reduction to between 5?15
dimension was optimal. These development ex-
periments also showed that weighting some fea-
tures slightly higher than others resulted in much
improved results; specifically, the discourse cues
and listener feedback cues were weighted slightly
higher.
3.2. LSA Centroid
The second summarization method is a textual ap-
proach incorporating LSA into a centroid-based
system [3]. The centroid is a pseudo-document
representing the important aspects of the docu-
ment as a whole; in the work of [3], this pseudo-
document consists of keywords and their modi-
fied tf.idf scores. In the present research, we take
a different approach to constructing the centroid
and to representing sentences in the document.
First, tf.idf scores are calculated for all words in
the meeting. Using these scores, we find the top
twenty keywords and choose these as the basis for
our centroid. We then perform LSA on a very large
corpus of Broadcast News and ICSI data, using the
Infomap tool1. Infomap provides a query language
with which we can retrieve word vectors for our
twenty keywords, and the centroid is thus repre-
sented as the average of its constituent keyword
vectors [12] [13].
Dialogue acts from the meetings are repre-
sented in much the same fashion. For each dia-
logue act, the vectors of its constituent words are
1http://infomap.stanford.edu
retrieved, and the dialogue act as a whole is the av-
erage of its word vectors. Extraction then proceeds
by finding the dialogue act with the highest cosine
similarity with the centroid, adding this to the sum-
mary, then continuing until the desired summary
length is reached.
3.3. Combined
The third summarization method is simply a com-
bination of the first two. Each system produces a
ranking and a master ranking is derived from these
two rankings. The hypothesis is that the strength
of one system will differ from the other and that
the two will complement each other and produce
a good overall ranking. The first system would be
expected to locate areas of high activity, decision-
making, and planning, while the second would lo-
cate information-rich utterances. This exempli-
fies one of the challenges of summarizing meeting
recordings: namely, that utterances can be impor-
tant in much different ways. A comprehensive sys-
tem that relies on more than one idea of importance
is ideal.
4. Experimental Setup
All summaries were 350 words in length, much
shorter than the compression rate used in [2] (10%
of dialogue acts). The ICSI meetings themselves
average around 10,000 words in length. The rea-
sons for choosing a shorter length for summaries
are that shorter summaries are more likely to be
useful to a user wanting to quickly overview and
browse a meeting, they present a greater summa-
rization challenge in that the summarizer must be
more exact in pinpointing the important aspects of
the meeting, and shorter summaries make it more
feasible to enlist human evaluators to judge the nu-
merous summaries on various criteria in the future.
Summaries were created on both manual tran-
scripts and speech recognizer output. The unit of
extraction for these summaries was the dialogue
act, and these experiments used human segmented
and labeled dialogue acts rather than try to detect
them automatically. In future work, we intend to
incorporate dialogue act detection and labeling as
part of one complete automatic summarization sys-
tem.
369
4.1. Corpus Description
The ICSI Meetings corpus consists of 75 meetings,
lasting approximately one hour each. Our test set
consists of six meetings, each with multiple hu-
man annotations. Annotators were given access
to a graphical user interface (GUI) for browsing
an individual meeting that included earlier human
annotations: an orthographic transcription time-
synchronized with the audio, and a topic segmen-
tation based on a shallow hierarchical decompo-
sition with keyword-based text labels describing
each topic segment. The annotators were told to
construct a textual summary of the meeting aimed
at someone who is interested in the research being
carried out, such as a researcher who does similar
work elsewhere, using four headings:
? general abstract: ?why are they meeting and
what do they talk about??;
? decisions made by the group;
? progress and achievements;
? problems described
The annotators were given a 200 word limit for
each heading, and told that there must be text for
the general abstract, but that the other headings
may have null annotations for some meetings. An-
notators who were new to the data were encour-
aged to listen to a meeting straight through before
beginning to author the summary.
Immediately after authoring a textual sum-
mary, annotators were asked to create an extractive
summary, using a different GUI. This GUI showed
both their textual summary and the orthographic
transcription, without topic segmentation but with
one line per dialogue act based on the pre-existing
MRDA coding [14]. Annotators were told to ex-
tract dialogue acts that together would convey the
information in the textual summary, and could be
used to support the correctness of that summary.
They were given no specific instructions about the
number or percentage of acts to extract or about
redundant dialogue acts. For each dialogue act ex-
tracted, they were then required in a second pass
to choose the sentences from the textual summary
supported by the dialogue act, creating a many-
to-many mapping between the recording and the
textual summary. Although the expectation was
that each extracted dialogue act and each summary
sentence would be linked to something in the op-
posing resource, we told the annotators that under
some circumstances dialogue acts and summary
sentences could stand alone.
We created summaries using both manual tran-
scripts as well as automatic speech recognition
(ASR) output. The AMI-ASR system [15] is de-
scribed in more detail in [4] and the average word
error rate (WER) for the corpus is 29.5%.
4.2. Evaluation Frameworks
The many-to-many mapping of dialogue acts to
summary sentences described in the previous sec-
tion allows us to evaluate our extractive summaries
according to how often each annotator linked a
given extracted dialogue act to a summary sen-
tence. This is somewhat analogous to Pyramid
weighting [16], but with dialogue acts as the SCUs.
In fact, we can calculate weighted precision, recall
and f-score using these annotations, but because
the summaries created are so short, we focus on
weighted precision as our central metric. For each
dialogue act that the summarizer extracts, we count
the number of times that each annotator links that
dialogue act to a summary sentence. For a given
dialogue act, it may be that one annotator links it
0 times, one annotator links it 1 time, and the third
annotator links it two times, resulting in an aver-
age score of 1 for that dialogue act. The scores for
all of the summary dialogue acts can be calculated
and averaged to create an overall summary score.
ROUGE scores, based on n-gram overlap be-
tween human abstracts and automatic extracts,
were also calculated for comparison [5]. ROUGE-
2, based on bigram overlap, is considered the most
stable as far as correlating with human judgments,
and this was therefore our ROUGE metric of inter-
est. ROUGE-SU4, which evaluates bigrams with
intervening material between the two elements of
the bigram, has recently been shown in the con-
text of the Document Understanding Conference
(DUC)2 to bring no significant additional informa-
tion as compared with ROUGE-2. Results from
[4] and from DUC 2005 also show that ROUGE
does not always correlate well with human judg-
ments. It is therefore included in this research in
the hope of further determining how reliable the
2http://duc.nist.gov
370
ROUGE metric is for our domain of meeting sum-
marization.
5. Results
The experimental results are shown in figure 1
(weighted precision) and figure 2 (ROUGE-2) and
are discussed below.
5.1. Weighted Precision Results
For weighted precision, the speech features ap-
proach was easily the best and scored significantly
better than the centroid and random approaches
(ANOVA,p<0.05), attaining an averaged weighted
precision of 0.52. The combined approach did
not improve upon the speech features approach
but was not significantly worse either. The ran-
domly created summaries scored much lower than
all three systems.
The superior performance of the speech fea-
tures approach compared to the LSA centroid
method closely mirrors results on the ICSI devel-
opment set, where the centroid method scored 0.23
and the speech features approach scored 0.42. For
the speech features approach on the test set, the
best feature by far was dialogue act length. Re-
moving this feature resulted in the precision score
being nearly halved. This mirrors results from
Maskey and Hirschberg [7], who found that the
length of a sentence in seconds and its length in
words were the two best features for predicting
summary sentences. Both the simple keyword
spotting and the discourse cue detection features
caused a lesser decline in precision when removed,
while other features of speaker activity had a neg-
ligible impact on the test results.
Interestingly, the weighted precision scores on
ASR were not significantly worse for any of the
summarization approaches. In fact, the centroid
approach scored very slightly higher on ASR out-
put than on manual transcripts. In [17] and [2] it
was similarly found that summarizing with ASR
output did not cause great deterioration in the qual-
ity of the summaries. It is not especially surpris-
ing that the speech features approach performed
similarly on both manual and ASR transcripts, as
many of its features based on speaker exchanges
and speaker activity would be unaffected by ASR
errors. The speech features approach is still signif-
icantly better than the random and centroid sum-
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
CombinedSpeechFeatsCentroidRandom
Summarization Approaches
PRECISION-MAN
PRECISION-ASR
Figure 1: Weighted Precision Results on Test Set
maries, and is not significantly better than the com-
bined approach on ASR.
5.2. ROUGE Results
The ROUGE results greatly differed from the
weighted precision results in several ways. First,
the centroid method was considered to be the best,
with a ROUGE-2 score of 0.047 compared with
0.041 for the speech features approach. Second,
there were not as great of differences between the
four systems according to ROUGE as there were
according to weighted precision. In fact, the ran-
dom summaries of manual transcripts are not sig-
nificantly worse than the other approaches, accord-
ing to ROUGE-2. Neither the combined approach
nor the speech features approach is significantly
worse than the centroid system, with the combined
approach generally scoring on par with the cen-
troid scores.
The third difference relates to summarization
on ASR output. ROUGE-2 has the random system
and the combined system showing sharp declines
when applied to ASR transcripts. The speech fea-
tures and centroid approaches do not show de-
clines. Random summaries are significantly worse
than both the centroid summaries (p<0.1) and
speech features summaries (p<0.05). Though the
combined approach declines on ASR output, it is
not significantly worse than the other systems.
To get an idea of a ROUGE-2 upper bound, for
each meeting in the test set we left one human ab-
stract out and compared it with the remaining ab-
stracts. The result was an average ROUGE-2 score
of .086.
371
 0.02
 0.04
 0.06
 0.08
 0.1
CombinedSpeechFeatsCentroidRandom
Summarization Approaches
ROUGE2-MAN
ROUGE2-ASR
UPPER BOUND
Figure 2: ROUGE-2 Results on Test Set
ROUGE-1 and ROUGE-SU4 show no signif-
icant differences between the centroid and speech
features approaches.
5.3. Correlations
There is no significant correlation between
macroaveraged ROUGE and weighted precision
scores across the meeting set, on both ASR and
manual transcripts. The Pearson correlation is
0.562 with a significance of p < 0.147. The Spear-
man correlation is 0.282 with a significance of p <
0.498. The correlation of scores across each test
meeting is worse yet, with a Pearson correlation
of 0.185 (p<0.208) and a Spearman correlation of
0.181 (p<0.271).
5.4. Sample Summary
The following is the text of a summary of meeting
Bed004 using the speech features approach:
-so its possible that we could do something like a summary
node of some sort that
-and then the question would be if if those are the things that you
care about uh can you make a relatively compact way of getting from
the various inputs to the things you care about
-this is sort of th the second version and i i i look at this maybe just
as a you know a a whatever uml diagram or you know as just a uh
screen shot not really as a bayes net as john johno said
-and um this is about as much as we can do if we dont w if we want
to avoid uh uh a huge combinatorial explosion where we specify ok if
its this and this but that is not the case and so forth it just gets really
really messy
-also it strikes me that we we m may want to approach the point
where we can sort of try to find a uh a specification for some interface
here that um takes the normal m three l looks at it
-so what youre trying to get out of this deep co cognitive linguistics is
the fact that w if you know about source source paths and goals and
nnn all this sort of stuff that a lot of this is the same for different tasks
-what youd really like of course is the same thing youd always like
which is that you have um a kind of intermediate representation
which looks the same o over a bunch of inputs and a bunch of outputs
-and pushing it one step further when you get to construction
grammar and stuff what youd like to be able to do is say you have
this parser which is much fancier than the parser that comes with uh
smartkom
-in independent of whether it about what is this or where is it or
something that you could tell from the construction you could pull
out deep semantic information which youre gonna use in a general
way
6. Discussion
Though the speech features approach was consid-
ered the best system, it is unclear why the com-
bined approach did not yield improvement. One
possibility relates to the extreme brevity of the
summaries: because the summaries are only 350
words in length, it is possible to have two sum-
maries of the same meeting which are equally
good but completely non-overlapping in content.
In other words, they both extract informative dia-
logue acts, but not the same ones. Combining the
rankings of two such systems might create a third
system which is comparable but not any better than
either of the first two systems alone. However, it
is still possible that the combined system will be
better in terms of balancing the two types of im-
portance discussed above: utterances that contain a
lot of informative content and keywords and utter-
ances that relate to decision-making and meeting
structure.
ROUGE did not correlate well with the
weighted precision scores, a result that adds to the
previous evidence that this metric may not be reli-
able in the domain of meeting summarization.
It is very encouraging that the summarization
approaches in general seem immune to the WER
of the ASR output. This confirms previous find-
ings such as [17] and [2], and the speech and
structural features used herein are particularly un-
affected by a moderately high WER. The reason
for the random summarizaton system not suffering
372
a sharp decline when applied to ASR may be due
to the fact that its scores were already so low that
it couldn?t deteriorate any further.
7. Future Work
The above results show that even a relatively small
set of speech, discourse, and structural features can
outperform a text summarization approach on this
data, and there are many additional features to be
explored. Of particular interest to us are features
relating to speaker status, i.e. features that help us
determine who is leading the meeting and who it is
that others are deferring to. We would also like to
more closely investigate the relationship between
areas of high speaker activity and informative ut-
terances.
In the immediate future, we will incorporate
these features into a machine-learning framework,
building support vector models trained on the ex-
tracted and non-extracted classes of the training
set.
Finally, we will apply these methods to the
AMI corpus [18] and create summaries of compa-
rable length for that meeting set. There are likely
to be differences regarding usefulness of certain
features due to the ICSI meetings being relatively
unstructured and informal and the AMI hub meet-
ings being more structured with a higher informa-
tion density.
8. Conclusion
The results presented above show that using fea-
tures related to speaker activity, listener feedback,
discourse cues and dialogue act length can outper-
form the lexical methods of text summarization ap-
proaches. More specifically, the fact that there are
multiple types of important utterances requires that
we use multiple methods of detecting importance.
Lexical methods and prosodic features are not nec-
essarily going to detect utterances that are relevant
to agreement, decision-making or speaker activity.
This research also provides further evidence that
ROUGE does not correlate well with human judg-
ments in this domain. Finally, it has been demon-
strated that high WER for ASR output does not
significantly decrease summarization quality.
9. Acknowledgements
Thanks to Thomas Hain and the AMI-ASR group
for speech recognition output. This work was
partly supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811, publication AMI-
150).
10. References
[1] K. Koumpis and S. Renals, ?Automatic sum-
marization of voicemail messages using lex-
ical and prosodic features,? ACM Transac-
tions on Speech and Language Processing,
vol. 2, pp. 1?24, 2005.
[2] G. Murray, S. Renals, and J. Carletta, ?Ex-
tractive summarization of meeting record-
ings,? in Proceedings of the 9th European
Conference on Speech Communication and
Technology, Lisbon, Portugal, September
2005.
[3] D. Radev, S. Blair-Goldensohn, and
Z. Zhang, ?Experiments in single and multi-
document summarization using mead,? in
The Proceedings of the First Document
Understanding Conference, New Orleans,
LA, September 2001.
[4] G. Murray, S. Renals, J. Carletta, and
J. Moore, ?Evaluating automatic summaries
of meeting recordings,? in Proceedings of
the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Work-
shop on Machine Translation and Summa-
rization Evaluation (MTSE), Ann Arbor, MI,
USA, June 2005.
[5] C.-Y. Lin and E. H. Hovy, ?Automatic
evaluation of summaries using n-gram co-
occurrence statistics,? in Proceedings of
HLT-NAACL 2003, Edmonton, Calgary,
Canada, May 2003.
[6] T. Hori, C. Hori, and Y. Minami, ?Speech
summarization using weighted finite-state
transducers,? in Proceedings of the 8th Eu-
ropean Conference on Speech Communica-
tion and Technology, Geneva, Switzerland,
September 2003.
373
[7] S. Maskey and J. Hirschberg, ?Compar-
ing lexial, acoustic/prosodic, discourse and
structural features for speech summariza-
tion,? in Proceedings of the 9th European
Conference on Speech Communication and
Technology, Lisbon, Portugal, September
2005.
[8] K. Ohtake, K. Yamamoto, Y. Toma, S. Sado,
S. Masuyama, and S. Nakagawa, ?Newscast
speech summarization via sentence shorten-
ing based on prosodic features,? in Proceed-
ings of the ISCA and IEEE Workshop on
Spontaneous Speech Processing and Recog-
nition, Tokyo, Japan, April 2003,.
[9] K. Zechner, ?Automatic summarization of
open-domain multiparty dialogues in diverse
genres,? Computational Linguistics, vol. 28,
no. 4, pp. 447?485, 2002.
[10] Y. Gong and X. Liu, ?Generic text sum-
marization using relevance measure and la-
tent semantic analysis,? in Proceedings of
the 24th Annual International ACM SI-
GIR Conference on Research and Develop-
ment in Information Retrieval, New Orleans,
Louisiana, USA, September 2001, pp. 19?25.
[11] J. Steinberger and K. Jez?ek, ?Using latent
semantic analysis in text summarization and
summary evaluation,? in Proceedings of ISIM
2004, Roznov pod Radhostem, Czech Repub-
lic, April 2004, pp. 93?100.
[12] P. Foltz, W. Kintsch, and T. Landauer, ?The
measurement of textual coherence with la-
tent semantic analysis,? Discourse Processes,
vol. 25, 1998.
[13] B. Hachey, G. Murray, and D. Reitter, ?The
embra system at duc 2005: Query-oriented
multi-document summarization with a very
large latent semantic space,? in Proceedings
of the Document Understanding Conference
(DUC) 2005, Vancouver, BC, Canada, Octo-
ber 2005.
[14] E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, ,
and H. Carvey, ?The ICSI meeting recorder
dialog act (MRDA) corpus,? in Proceedings
of the 5th SIGdial Workshop on Discourse
and Dialogue, Cambridge, MA, USA, April-
May 2004, pp. 97?100.
[15] T. Hain, J. Dines, G. Garau, M. Karafiat,
D. Moore, V. Wan, R. Ordelman,
I.Mc.Cowan, J.Vepa, and S.Renals, ?An
investigation into transcription of conference
room meetings,? Proceedings of the 9th
European Conference on Speech Commu-
nication and Technology, Lisbon, Portugal,
September 2005.
[16] A. Nenkova and B. Passonneau, ?Evaluat-
ing content selection in summarization: The
pyramid method,? in Proceedings of HLT-
NAACL 2004, Boston, MA, USA, May 2004.
[17] R. Valenza, T. Robinson, M. Hickey, and
R. Tucker, ?Summarization of spoken audio
through information extraction,? in Proceed-
ings of the ESCA Workshop on Accessing In-
formation in Spoken Audio, Cambridge UK,
April 1999, pp. 111?116.
[18] J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, W. Kraaij, M. Kronen-
thal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and
P. Wellner, ?The AMI meeting corpus:
A pre-announcement,? in Proceedings of
MLMI 2005, Edinburgh, UK, June 2005.
374
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 121?124,
New York, June 2006. c?2006 Association for Computational Linguistics
Computational Modelling of Structural Priming in Dialogue
David Reitter, Frank Keller, Johanna D. Moore
dreitter | keller | jmoore @ inf.ed.ac.uk
School of Informatics
University of Edinburgh
United Kingdom
Abstract
Syntactic priming effects, modelled as in-
crease in repetition probability shortly af-
ter a use of a syntactic rule, have the
potential to improve language processing
components. We model priming of syn-
tactic rules in annotated corpora of spo-
ken dialogue, extending previous work
that was confined to selected construc-
tions. We find that speakers are more re-
ceptive to priming from their interlocutor
in task-oriented dialogue than in spona-
neous conversation. Low-frequency rules
are more likely to show priming.
1 Introduction
Current dialogue systems overlook an interesting
fact of language-based communication. Speakers
tend to repeat their linguistic decisions rather than
making them from scratch, creating entrainment
over time. Repetition is evident not just on the ob-
vious lexical level: syntactic choices depend on pre-
ceding ones in a way that can be modelled and, ul-
timately, be leveraged in parsing and language gen-
eration. The statistical analysis in this paper aims to
make headway towards such a model.
Recently, priming phenomena1 have been ex-
ploited to aid automated processing, for instance in
automatic speech recognition using cache models,
but only recently have attempts been made at using
1The term priming refers to a process that influences lin-
guistic decision-making. An instance of priming occurs when a
syntactic structure or lexical item giving evidence of a linguistic
choice (prime) influences the recipient to make the same deci-
sion, i.e. re-use the structure, at a later choice-point (target).
them in parsing (Charniak and Johnson, 2005). In
natural language generation, repetition can be used
to increase the alignment of human and computers.
A surface-level approach is possible by biasing the
n-gram language model used to select the output
string from a variety of possible utterances (Brock-
mann et al, 2005).
Priming effects are common and well known. For
instance, speakers access lexical items more quickly
after a semantically or phonologically similar prime.
Recent work demonstrates large effects for partic-
ular synonymous alternations (e.g., active vs. pas-
sive voice) using traditional laboratory experiments
with human subjects (Bock, 1986; Branigan et al,
2000). In this study, we look at the effect from a
computational perspective, that is, we assume some
form of parsing and syntax-driven generation com-
ponents. While previous studies singled out syntac-
tic phenomena, we assume a phrase-structure gram-
mar where all syntactic rules may receive priming.
We use large-scale corpora, which reflect the reali-
ties of natural interaction, where limited control ex-
ists over syntax and the semantics of the utterances.
Thus, we quantify priming for the general case in
the realistic setting provided by corpus based exper-
iments. As a first hypothesis, we predict that after a a
syntactic rule occurs, it is more likely to be repeated
shortly than a long time afterwards.
From a theoretical perspective, priming opens a
peephole into the architecture of the human lan-
guage faculty. By identifying units in which prim-
ing occurs, we can pinpoint the structures used in
processing. Also, priming may help explain the ease
with which humans engange in conversations.
This study is interested in the differences relevant
to systems implementing language-based human-
121
computer interaction. Often, HCI is a means for
user and system to jointly plan or carry out a task.
Thus, we look at repetition effects in task-oriented
dialogue. A recent psychological perspective mod-
els Interactive Alignment between speakers (Picker-
ing and Garrod, 2004), where mutual understand-
ing about task and situation depends on lower-level
priming effects. Under the model, we expect prim-
ing effects to be stronger when a task requires high-
level alignment of situation models.
2 Method
2.1 Dialogue types
We examined two corpora. Switchboard con-
tains 80,000 utterances of spontaneous spoken con-
versations over the telephone among randomly
paired, North American speakers, syntactically an-
notated with phrase-structure grammar (Marcus
et al, 1994). The HCRC Map Task corpus comprises
more than 110 dialogues with a total of 20, 400 ut-
terances (Anderson et al, 1991). Like Switchboard,
HCRC Map Task is a corpus of spoken, two-person
dialogue in English. However, Map Task contains
task-oriented dialogue: interlocutors work together
to achieve a task as quickly and efficiently as pos-
sible. Subjects were asked to give each other direc-
tions with the help of a map. The interlocutors are in
the same room, but have separate, slightly different
maps and are unable to see each other?s maps.
2.2 Syntactic repetitions
Both corpora are annotated with phrase structure
trees. Each tree was converted into the set of phrase
structure productions that license it. This allows us
to identify the repeated use of rules. Structural prim-
ing would predict that a rule (target) occurs more
often shortly after a potential prime of the same rule
than long afterwards ? any repetition at great dis-
tance is seen as coincidental. Therefore, we can cor-
relate the probability of repetition with the elapsed
time (DIST) between prime and target.
We considered very pair of two equal syntactic
rules up to a predefined maximal distance to be a
potential case of priming-enhanced production. If
we consider priming at distances 1 . . . n, each rule
instance produces up to n data points. Our binary
response variable indicates whether there is a prime
for the target between n ? 0.5 and n + 0.5 seconds
before the target. As a prime, we see the invocation
of the same rule. Syntactic repetitions resulting from
lexical repetition and repetitions of unary rules are
excluded. We looked for repetitions within windows
(DIST) of n = 15 seconds (Section 3.1).
Without priming, one would expect that there is a
constant probability of syntactic repetition, no mat-
ter the distance between prime and target. The anal-
ysis tries to reject this null hypothesis and show a
correlation of the effect size with the type of corpus
used. We expect to see the syntactic priming effect
found experimentally should translate to more cases
for shorter repetition distances, since priming effects
usually decay rapidly (Branigan et al, 1999).
The target utterance is included as a random fac-
tor in our model, grouping all 15 measurements of
all rules of an utterance as repeated measurements,
since they depend on the same target rule occurrence
or at least on other other rules in the utterance, and
are, thus, partially inter-dependent.
We distinguish production-production priming
within (PP) and comprehension-production priming
between speakers (CP), encoded in the factor ROLE.
Models were estimated on joint data sets derived
from both corpora, with a factor SOURCE included
to discriminate the two dialogue types.
Additionally, we build a model estimating the ef-
fect of the raw frequency of a particular syntactic
rule on the priming effect (FREQ). This is of par-
ticular interest for priming in applications, where a
statistical model will, all other things equal, prefer
the more frequent linguistic choice; recall for com-
peting low-frequency rules will be low.
2.3 Generalized Linear Mixed Effect
Regression
In this study, we built generalized linear mixed ef-
fects regression models (GLMM). In all cases, a rule
instance target is counted as a repetition at distance
d iff there is an utterance prime which contains the
same rule, and prime and target are d units apart.
GLMMs with a logit-link function are a form of lo-
gistic regression.2
2We trained our models using Penalized Quasi-Likelihood
(Venables and Ripley, 2002). We will not generally give classi-
calR2 figures, as this metric is not appropriate to such GLMMs.
The below experiments were conducted on a sample of 250,000
122
SWBD PP MT PP MT CP
?0
.1
0
?0
.0
5
0.
00
0.
05
0.
10
0.
15
0.
20
Switchboard Map Task
PP PP CPCP
*
*
*
-
-
-
-
0 5 10 15
0.0
10
0.0
12
0.0
14
0.0
16
0.0
18
0.0
20
distance: Temporal Distance between prime and target (seconds)
p(p
rim
e=t
arg
et|t
arg
et,d
ista
nce
)
Map Task: 
production-production
Switchboard: 
production-production
Map Task: 
comprehension-production
Switchboard: 
comprehension-production
Figure 1: Left: Estimated priming strength (repetition probability decay rate) for Switchboard and Map
Task, for within-speaker (PP) and between-speaker (CP) priming. Right: Fitted model for the development
of repetition probability (y axis) over time (x axis, in seconds). Here, decay (slope) is the relevant factor for
priming strength, as shown on the left. These are derived from models without FREQ.
Regression allows us not only to show that prim-
ing exists, but it allows us to predict the decline of
repetition probability with increasing distance be-
tween prime and target and depending on other vari-
ables. If we see priming as a form of pre-activation
of syntactic nodes, it indicates the decay rate of pre-
activation. Our method quantifies priming and cor-
relates the effect with secondary factors.
3 Results
3.1 Task-oriented and spontaneous dialogue
Structural repetition between speakers occured in
both corpora and its probability decreases logarith-
mically with the distance between prime and target.
Figure 1 provides the model for the influence
of the four factorial combinations of ROLE and
SOURCE on priming (left) and the development of
repetition probability at increasing distance (right).
SOURCE=Map Task has an interaction effect on the
priming decay ln(DIST), both for PP priming (? =
?0.024, t = ?2.0, p < 0.05) and for CP priming
(? = ?0.059, t = ?4.0, p < 0.0005). (Lower coef-
ficients indicate more decay, hence more priming.)
data points per corpus.
In both corpora, we find positive priming effects.
However, PP priming is stronger, and CP priming is
much stronger in Map Task.
The choice of corpus exhibits a marked interac-
tion with priming effect. Spontaneous conversation
shows significantly less priming than task-oriented
dialogue. We believe this is not a side-effect of vary-
ing grammar size or a different syntactic entropy in
the two types of dialogue, since we examine the de-
cay of repetition probability with increasing distance
(interactions with DIST), and not the overall proba-
bility of chance repetition (intercepts / main effects
except DIST).
3.2 Frequency effects
An additional model was built which included
ln(FREQ) as a predictor that may interact with the
effect coefficient for ln(DIST).
ln(FREQ) is inversely correlated with
the priming effect (Paraphrase: ?lnDist =
?1.05, ?lnDist:lnFreq = 0.54, Map Task:
?lnDist = ?2.18, ?lnDist:lnFreq = 0.35, all
p < 0.001). Priming weakens with higher
(logarithmic) frequency of a syntactic rule.
123
4 Discussion
Evidence from Wizard-of-Oz experiments (with sys-
tems simulated by human operators) have shown
that users of dialogue systems strongly align their
syntax with that of a (simulated) computer (Brani-
gan et al, 2003). Such an effect can be leveraged
in an application, provided there is a priming model
interfacing syntactic processing.
We found evidence of priming in general, that is,
when we assume priming of each phrase structure
rule. The priming effects decay quickly and non-
linearly, which means that a dialogue system would
best only take a relatively short preceding context
into account, e.g., the previous few utterances.
An important consideration in the context of di-
alogue systems is whether user and system collab-
orate on solving a task, such as booking tickets or
retrieving information. Here, syntactic priming be-
tween human speakers is strong, so a system should
implement it. In other situations, systems do not
have to use a unified syntactic architecture for pars-
ing and generation, but bias their output on previous
system utterances, and possibly improve parsing by
looking at previously recognized inputs.
The fact that priming is more pronounced within
(PP) a speaker suggests that optimizing parsing and
generation separately is the most promising avenue
in either type of dialogue system.
One explanation for this lies in a reduced cog-
nitive load of spontaneous, everyday conversation.
Consequently, the more accessible, highly-frequent
rules prime less.
In task-oriented dialogue, speakers need to pro-
duce a common situation model. Interactive Align-
ment Model argues that this process is aided by syn-
tactic priming. In support of this model, we find
more priming in task-oriented dialogue.3
5 Conclusions
Syntactic priming effects are reliably present in di-
alogue even in computational models where the full
range of syntactic rules is considered instead of se-
lected constructions with known strong priming.
This is good news for dialogue systems, which
tend to be task-oriented. Linguistically motivated
3For a more detailed analysis from the perspective of inter-
active alignment, see Reitter et al (2006).
systems can possibly exploit the user?s tendency to
repeat syntactic structures by anticipating repetition.
Future systems may also align their output with their
recognition capabilities and actively align with the
user to signal understanding. Parsers and realizers in
natural language generation modules may make the
most of priming if they respect important factors that
influence priming effects, such as task-orientation of
the dialogue and frequency of the syntactic rule.
Acknowledgements
The authors would like to thank Amit Dubey, Roger Levy and
Martin Pickering. The first author?s work is supported by a grant
from the Edinburgh Stanford Link.
References
A. Anderson, M. Bader, E. Bard, E. Boyle, G. M. Doherty,
S. Garrod, S. Isard, J. Kowtko, J. McAllister, J. Miller,
C. Sotillo, H. Thompson, and R. Weinert. 1991. The HCRC
Map Task corpus. Language and Speech, 34(4):351?366.
J. Kathryn Bock. 1986. Syntactic persistence in language pro-
duction. Cognitive Psychology, 18:355?387.
Holly P. Branigan, Martin J. Pickering, and Alexandra A. Cle-
land. 1999. Syntactic priming in language production: Ev-
idence for rapid decay. Psychonomic Bulletin and Review,
6(4):635?640.
Holly P. Branigan, Martin J. Pickering, and Alexandra A. Cle-
land. 2000. Syntactic co-ordination in dialogue. Cognition,
75:B13?25.
Holly P. Branigan, Martin J. Pickering, Jamie Pearson, Janet F.
McLean, and Clifford Nass. 2003. Syntactic alignment be-
tween computers and people: the role of belief about mental
states. In Proceedings of the Twenty-fifth Annual Conference
of the Cognitive Science Society.
Carsten Brockmann, Amy Isard, Jon Oberlander, and Michael
White. 2005. Modelling alignment for affective dialogue. In
Workshop on Adapting the Interaction Style to Affective Fac-
tors at the 10th International Conference on User Modeling
(UM-05). Edinburgh, UK.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Proc.
43th ACL.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies,
M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn
treebank: Annotating predicate argument structure. In Proc.
ARPA Human Language Technology Workshop.
Martin J. Pickering and Simon Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and Brain Sci-
ences, 27:169?225.
David Reitter, Johanna D. Moore, and Frank Keller. 2006. Prim-
ing of syntactic rules in task-oriented dialogue and sponta-
neous conversation. In Proceedings of the 28th Annual Con-
ference of the Cognitive Science Society.
William N. Venables and Brian D. Ripley. 2002. Modern Ap-
plied Statistics with S. Fourth Edition. Springer.
124
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 173?176,
New York, June 2006. c?2006 Association for Computational Linguistics
Evolving optimal inspectable strategies for spoken dialogue systems
Dave Toney
School of Informatics
Edinburgh University
2 Buccleuch Place
Edinburgh EH8 9LW
dave@cstr.ed.ac.uk
Johanna Moore
School of Informatics
Edinburgh University
2 Buccleuch Place
Edinburgh EH8 9LW
jmoore@inf.ed.ac.uk
Oliver Lemon
School of Informatics
Edinburgh University
2 Buccleuch Place
Edinburgh EH8 9LW
olemon@inf.ed.ac.uk
Abstract
We report on a novel approach to gener-
ating strategies for spoken dialogue sys-
tems. We present a series of experiments
that illustrate how an evolutionary rein-
forcement learning algorithm can produce
strategies that are both optimal and easily
inspectable by human developers. Our ex-
perimental strategies achieve a mean per-
formance of 98.9% with respect to a pre-
defined evaluation metric. Our approach
also produces a dramatic reduction in
strategy size when compared with conven-
tional reinforcement learning techniques
(87% in one experiment). We conclude
that this algorithm can be used to evolve
optimal inspectable dialogue strategies.
1 Introduction
Developing a dialogue management strategy for a
spoken dialogue system is often a complex and time-
consuming task. This is because the number of
unique conversations that can occur between a user
and the system is almost unlimited. Consequently,
a system developer may spend a lot of time antic-
ipating how potential users might interact with the
system before deciding on the appropriate system re-
sponse.
Recent research has focused on generating dia-
logue strategies automatically. This work is based
on modelling dialogue as a markov decision process,
formalised by a finite state space S, a finite action
set A, a set of transition probabilities T and a re-
ward function R. Using this model an optimal dia-
logue strategy pi? is represented by a mapping be-
tween the state space and the action set. That is, for
each state s ? S this mapping defines its optimal ac-
tion a?s . How is this mapping constructed? Previous
approaches have employed reinforcement learning
(RL) algorithms to estimate an optimal value func-
tion Q? (Levin et al, 2000; Frampton and Lemon,
2005). For each state this function predicts the fu-
ture reward associated with each action available in
that state. This function makes it easy to extract the
optimal strategy (policy in the RL literature).
Progress has been made with this approach but
some important challenges remain. For instance,
very little success has been achieved with the large
state spaces that are typical of real-life systems.
Similarly, work on summarising learned strategies
for interpretation by human developers has so far
only been applied to tasks where each state-action
pair is explicitly represented (Lec?uche, 2001).
This tabular representation severely limits the size
of the state space.
We propose an alternative approach to finding op-
timal dialogue policies. We make use of XCS, an
evolutionary reinforcement learning algorithm that
seeks to represent a policy as a compact set of state-
action rules (Wilson, 1995). We suggest that this al-
gorithm could overcome both the challenge of large
state spaces and the desire for strategy inspectability.
In this paper, we focus on the issue of inspectabil-
ity. We present a series of experiments that illustrate
how XCS can be used to evolve dialogue strategies
that are both optimal and easily inspectable.
173
2 Learning Classifier Systems and XCS
Learning Classifier Systems were introduced by
John Holland in the 1970s as a framework for learn-
ing rule-based knowledge representations (Holland,
1976). In this model, a rule base consists of a popu-
lation of N state-action rules known as classifiers.
The state part of a classifier is represented by a
ternary string from the set {0,1,#} while the action
part is composed from {0,1}. The # symbol acts as
a wildcard allowing a classifier to aggregate states;
for example, the state string 1#1 matches the states
111 and 101. Classifier systems have been applied
to a number of learning tasks, including data mining,
optimisation and control (Bull, 2004).
Classifier systems combine two machine learning
techniques to find the optimal rule set. A genetic
algorithm is used to evaluate and modify the popu-
lation of rules while reinforcement learning is used
to assign rewards to existing rules. The search for
better rules is guided by the strength parameter as-
sociated with each classifier. This parameter serves
as a fitness score for the genetic algorithm and as a
predictor of future reward (payoff ) for the RL algo-
rithm. This evolutionary learning process searches
the space of possible rule sets to find an optimal pol-
icy as defined by the reward function.
XCS (X Classifier System) incorporates a num-
ber of modifications to Holland?s original frame-
work (Wilson, 1995). In this system, a classifier?s
fitness is based on the accuracy of its payoff predic-
tion instead of the prediction itself. Furthermore, the
genetic algorithm operates on actions instead of the
population as a whole. These aspects of XCS result
in a more complete map of the state-action space
than would be the case with strength-based classi-
fier systems. Consequently, XCS often outperforms
strength-based systems in sequential decision prob-
lems (Kovacs, 2000).
3 Experimental Methodology
In this section we present a simple slot-filling sys-
tem based on the hotel booking domain. The goal of
the system is to acquire the values for three slots: the
check-in date, the number of nights the user wishes
to stay and the type of room required (single, twin
etc.). In slot-filling dialogues, an optimal strategy is
one that interacts with the user in a satisfactory way
while trying to minimise the length of the dialogue.
A fundamental component of user satisfaction is the
system?s prevention and repair of any miscommuni-
cation between it and the user. Consequently, our
hotel booking system focuses on evolving essential
slot confirmation strategies.
We devised an experimental framework for mod-
elling the hotel system as a sequential decision task
and used XCS to evolve three behaviours. Firstly,
the system should execute its dialogue acts in a log-
ical sequence. In other words, the system should
greet the user, ask for the slot information, present
the query results and then finish the dialogue, in that
order (Experiment 1). Secondly, the system should
try to acquire the slot values as quickly as possible
while taking account of the possibility of misrecog-
nition (Experiments 2a and 2b). Thirdly, to increase
the likelihood of acquiring the slot values correctly,
each one should be confirmed at least once (Experi-
ments 3 and 4).
The reward function for Experiments 1, 2a and
2b was the same. During a dialogue, each non-
terminal system action received a reward value of
zero. At the end of each dialogue, the final reward
comprised three parts: (i) -1000 for each system
turn; (ii) 100,000 if all slots were filled; (iii) 100,000
if the first system act was a greeting. In Experiments
3 and 4, an additional reward of 100,000 was as-
signed if all slots were confirmed.
The transition probabilities were modelled using
two versions of a handcoded simulated user. A very
large number of test dialogues are usually required
for learning optimal dialogue strategies; simulated
users are a practical alternative to employing human
test users (Scheffler and Young, 2000; Lopez-Cozar
et al, 2002). Simulated user A represented a fully
cooperative user, always giving the slot information
that was asked. User B was less cooperative, giving
no response 20% of the time. This allowed us to
perform a two-fold cross validation of the evolved
strategies.
For each experiment we allowed the system?s
strategy to evolve over 100,000 dialogues with each
simulated user. Dialogues were limited to a maxi-
mum of 30 system turns. We then tested each strat-
egy with a further 10,000 dialogues. We logged the
total reward (payoff) for each test dialogue. Each
experiment was repeated ten times.
174
In each experiment, the presentation of the query
results and closure of the dialogue were combined
into a single dialogue act. Therefore, the dialogue
acts available to the system for the first experi-
ment were: Greeting, Query+Goodbye, Ask(Date),
Ask(Duration) and Ask(RoomType). Four boolean
variables were used to represent the state of the di-
alogue: GreetingFirst, DateFilled, DurationFilled,
RoomFilled.
Experiment 2 added a new dialogue act: Ask(All).
The goal here was to ask for all three slot values
if the probability of getting the slot values was rea-
sonably high. If the probability was low, the sys-
tem should ask for the slots one at a time as be-
fore. This information was modelled in the sim-
ulated users by 2 variables: Prob1SlotCorrect and
Prob3SlotsCorrect. The values for these variables
in Experiments 2a and 2b respectively were: 0.9 and
0.729 (=0.93); 0.5 and 0.125 (=0.53).
Experiment 3 added three new dialogue acts: Ex-
plicit Confirm(Date), Explicit Confirm(Duration),
Explicit Confirm(RoomType) and three new state
variables: DateConfirmed, DurationConfirmed,
RoomConfirmed. The goal here was for the sys-
tem to learn to confirm each of the slot val-
ues after the user has first given them. Experi-
ment 4 sought to reduce the dialogue length fur-
ther by allowing the system to confirm one slot
value while asking for another. Two new di-
alogue acts were available in this last experi-
ment: Implicit Confirm(Date)+Ask(Duration) and
Implicit Confirm(Duration)+Ask(RoomType).
4 Experimental Results
Table 1 lists the total reward (payoff) averaged over
the 10 cross-validated test trials for each experiment,
expressed as a percentage of the maximum payoff.
In these experiments, the maximum payoff repre-
sents the shortest possible successful dialogue. For
example, the maximum payoff for Experiment 1 is
195,000: 100,000 for filling the slots plus 100,000
for greeting the user at the start of the dialogue mi-
nus 5000 for the minimum number of turns (five)
taken to complete the dialogue successfully. The av-
erage payoff for the 10 trials trained on simulated
user A and tested on user B was 193,877 ? approxi-
mately 99.4% of the maximum possible. In light of
Exp. Training/Test Users Payoff (%)
1 A, B 99.4B, A 99.8
2a A, B 99.1B, A 99.4
2b A, B 96.8B, A 97.2
3 A, B 98.8B, A 99.3
4 A, B 99.3B, A 99.7
Table 1: Payoff results for the evolved strategies.
these results and the stochastic user responses, we
suggest that these evolved strategies would compare
favourably with any handcoded strategies.
It is instructive to compare the rate of convergence
for different strategies. Figure 1 shows the average
payoff for the 100,000 dialogues trained with sim-
ulated user A in Experiments 3 and 4. It shows
that Experiment 3 approached the optimal policy
after approximately 20,000 dialogues whereas Ex-
periment 4 converged after approximately 5000 dia-
logues. This is encouraging because it suggests that
XCS remains focused on finding the shortest suc-
cessful dialogue even when the number of available
actions increases.
0 25,000 50,000 75,000 100,000
0
0.5
1
1.5
2
2.5
3
x 105
Dialogues
Av
er
ag
e 
Pa
yo
ff
Exp. 3 
Exp. 4 
Figure 1: Convergence towards optimality during
training in Experiments 3 and 4 (simulated user A).
Finally, we look at how to represent an optimal
strategy. From the logs of the test dialogues we ex-
tracted the state-action rules (classifiers) that were
executed. For example, in Experiment 4, the op-
175
State Action
Gre
etin
gFi
rst
Da
teF
ille
d
Du
rat
ion
Fill
ed
Roo
mF
ille
d
Da
teC
onfir
med
Du
rat
ion
Con
firm
ed
Roo
mC
onfir
med
0 0 # # # # # Greeting
1 0 0 0 # # # Ask(Date)
1 1 # # 0 # # Implicit Confirm(Date) + Ask(Duration)
1 1 1 # 1 0 0 Implicit Confirm(Duration) + Ask(RoomType)
1 1 1 1 1 1 0 Explicit Confirm(RoomType)
1 1 1 1 1 1 1 Query + Goodbye
Table 2: A summary of the optimal strategy for Experiment 4.
timal strategy is represented by 17 classifiers. By
comparison, a purely RL-based strategy would de-
fine an optimal action for every theoretically pos-
sible state (i.e. 128). In this example, the evolu-
tionary approach has reduced the number of rules
from 128 to 17 (a reduction of 87%) and is therefore
much more easily inspectable. In fact, the size of the
optimal strategy can be reduced further by select-
ing the most general classifier for each action (Table
2). These rules are sufficient since they cover the 60
states that could actually occur while following the
optimal strategy.
5 Conclusions and future work
We have presented a novel approach to generating
spoken dialogue strategies that are both optimal and
easily inspectable. The generalizing ability of the
evolutionary reinforcement learning (RL) algorithm,
XCS, can dramatically reduce the size of the opti-
mal strategy when compared with conventional RL
techniques. In future work, we intend to exploit this
generalization feature further by developing systems
that require much larger state representations. We
also plan to investigate other approaches to strategy
summarisation. Finally, we will evaluate our ap-
proach against purely RL-based methods.
References
Larry Bull, editor. 2004. Applications of Learning Clas-
sifier Systems. Springer.
Matthew Frampton and Oliver Lemon. 2005. Reinforce-
ment learning of dialogue strategies using the user?s
last dialogue act. In IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Edin-
burgh, UK, July.
John Holland. 1976. Adaptation. In Rosen R.
and F. Snell, editors, Progress in theoretical biology.
Plenum, New York.
Tim Kovacs. 2000. Strength or accuracy? Fitness cal-
culation in learning classifier systems. In Pier Luca
Lanzi, Wolfgang Stolzmann, and Stewart Wilson, edi-
tors, Learning Classifier Systems. From Foundations to
Applications, Lecture Notes in Artificial Intelligence
1813, pages 143?160. Springer-Verlag.
Renaud Lec?uche. 2001. Learning optimal dialogue
management rules by using reinforcement learning
and inductive logic programming. In 2nd Meeting
of the North American Chapter of the Association of
Computational Linguistics, Pittsburgh, USA, June.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine inter-
action for learning dialogue strategies. IEEE Transac-
tions on Speech and Audio Processing, 8(1):11?23.
R. Lopez-Cozar, A. De la Torre, J. Segura, A. Rubio, and
V. Sa?nchez. 2002. Testing dialogue systems by means
of automatic generation of conversations. Interacting
with Computers, 14(5):521?546.
Konrad Scheffler and Steve Young. 2000. Probabilis-
tic simulation of human-machine dialogues. In Inter-
national Conference on Acoustics, Speech and Signal
Processing, pages 1217?1220, Istanbul, Turkey, June.
Stewart Wilson. 1995. Classifier fitness based on accu-
racy. Evolutionary Computation, 3(2):149?175.
176
Proceedings of NAACL HLT 2007, pages 25?32,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
What Decisions Have You Made: Automatic Decision Detection in
Conversational Speech
Pei-Yun Hsueh
School of Informatics
University of Edinburgh
Edinburgh EH9 8WL, UK
p.hsueh@ed.ac.uk
Johanna Moore
School of Informatics
University of Edinburgh
Edinburgh EH9 8WL, UK
J.Moore@ed.ac.uk
Abstract
This study addresses the problem of au-
tomatically detecting decisions in conver-
sational speech. We formulate the prob-
lem as classifying decision-making units
at two levels of granularity: dialogue acts
and topic segments. We conduct an em-
pirical analysis to determine the charac-
teristic features of decision-making dia-
logue acts, and train MaxEnt models using
these features for the classification tasks.
We find that models that combine lexi-
cal, prosodic, contextual and topical fea-
tures yield the best results on both tasks,
achieving 72% and 86% precision, respec-
tively. The study also provides a quantita-
tive analysis of the relative importance of
the feature types.
1 Introduction
Making decisions is an important aspect of conver-
sations in collaborative work. In the context of meet-
ings, the proposed argumentative models, e.g., in
Pallotta et al (2005) and Rienks et al (2005), have
specified decisions as an essential outcome of meet-
ings. Whittaker et al (2005) have also described
how reviewing decisions is critical to the re-use of
meeting recordings. For example, a new engineer
who just get assigned to a project will need to know
what major decisions have been made in previous
meetings. Unless all decisions are recorded in meet-
ing minutes or annotated in the speech recordings, it
is difficult to locate the decision points by the brows-
ing and playback utilities alone.
Banerjee and Rudnicky (2005) have shown that
it is easier for users to retrieve the information
they seek if the meeting record includes information
about topic segmentation, speaker role, and meet-
ing state (e.g., discussion, presentation, briefing). To
assist users in identifying or revisiting decisions in
meeting archives, our goal is to automatically iden-
tify the dialogue acts and segments where decisions
are made. Because reviewing decisions is indis-
pensable in collaborative work, automatic decision
detection is expected to lend support to computer-
assisted meeting tracking and understanding (e.g.,
assisting in the fulfilment of the decisions made in
the meetings) and the development of group infor-
mation management applications (e.g., constructing
group memory).
2 Related Work
Spontaneous face-to-face dialogues in meetings vi-
olate many assumptions made by techniques pre-
viously developed for broadcast news (e.g., TDT
and TRECVID), telephone conversations (e.g.,
Switchboard), and human-computer dialogues (e.g.,
DARPA Communicator). In order to develop
techniques for understanding multiparty dialogues,
smart meeting rooms have been built at several insti-
tutes to record large corpora of meetings in natural
contexts, including CMU (Waibel et al, 2001), LDC
(Cieri et al, 2002), NIST (Garofolo et al, 2004),
ICSI (Janin et al, 2003), and in the context of the
IM2/M4 project (Marchand-Mailet, 2003). More
recently, scenario-based meetings, in which partic-
25
ipants are assigned to different roles and given spe-
cific tasks, have been recorded in the context of
the CALO project (the Y2 Scenario Data) (CALO,
2003) and the AMI project (Carletta et al, 2005).
The availability of meeting corpora has enabled
researchers to begin to develop descriptive models
of meeting discussions. Some researchers are mod-
elling the dynamics of the meeting, exploiting dia-
logue models previously proposed for dialogue man-
agement. For example, Niekrasz et al (2005) use
the Issue-Based Information System (IBIS) model
(Kunz and Ritte, 1970) to incorporate the history
of dialogue moves into the Multi-Modal Discourse
(MMD) ontology. Other researchers are modelling
the content of the meeting using the type of struc-
tures proposed in work on argumentation. For ex-
ample, Rienks et al (2005) have developed an ar-
gument diagramming scheme to visualize the rela-
tions (e.g., positive, negative, uncertain) between ut-
terances (e.g., statement, open issue), and Marchand
et al (2003) propose a schema to model different ar-
gumentation acts (e.g., accept, request, reject) and
their organization and synchronization. Decisions
are often seen as a by-product of these models.
Automatically extracting these argument mod-
els is a challenging task. However, researchers
have begun to make progress towards this goal.
For example, Gatica et al (2005) and Wrede and
Shriberg (2003) automatically identify the level of
emotion in meeting spurts (e.g., group level of in-
terest, hot spots). Other researchers have developed
models for detecting agreement and disagreement
in meetings, using models that combine lexical fea-
tures with prosodic features (e.g., pause, duration,
F0, speech rate) (Hillard et al, 2003) and struc-
tural information (e.g., the previous and following
speaker) (Galley et al, 2004). More recently, Purver
et al (2006) have tackled the problem of detecting
one type of decision, namely action items, which
embody the transfer of group responsibility. How-
ever, no prior work has addressed the problem of au-
tomatically identifying decision-making units more
generally in multiparty meetings. Moreover, no pre-
vious research has provided a quantitative account
of the effects of different feature types on the task of
automatic decision detection.
3 Research Goal
Our aim is to develop models for automatically de-
tecting segments of conversation that contain deci-
sions directly from the audio recordings and tran-
scripts of the meetings, and to identify the feature
combinations that are most effective for this task.
Meetings can be viewed at different levels of
granularity. In this study, we first consider how to
detect the dialogue acts that contain decision-related
information (DM DAs). Since it is often difficult
to interpret a decision without knowing the current
topic of discussion, we are also interested in detect-
ing decision-making segments at a coarser level of
granularity: topic segments. The task of automatic
decision detection can therefore be divided into two
subtasks: detecting DM DAs and detecting decision-
making topic segments (DM Segments).
In this study we propose to first empirically
identify the features that are most characteristic of
decision-making dialogue acts and then computa-
tionally integrate the characteristic features to locate
the DM DAs in meeting archives. For the latter task,
previous research on automatic meeting understand-
ing and tracking has commonly utilized a classifica-
tion framework, in which variants of generative and
conditional models are computed directly from data.
In this study, we use a Maximum Entropy (MaxEnt)
classifier to combine the decision characteristic fea-
tures to predict DM DAs and DM Segments.
4 Data
4.1 Decision Annotation
In this study, we use a set of 50 scenario-driven
meetings (approximately 37,400 dialogue acts) that
have been segmented into dialogue acts and anno-
tated with decision information in the AMI meet-
ing corpus. These meetings are driven by a sce-
nario, wherein four participants play the role of
Project Manager, Marketing Expert, Industrial De-
signer, and User Interface Designer in a design team
in a series of four meetings. Each series of meet-
ing recordings uses four distinctive speakers differ-
ent from other series. The corpus includes manual
transcripts for all meetings. It also comes with in-
dividual sound files recorded by close-talking head-
mounted microphones and cross-talking sound files
recorded by desktop microphones.
26
4.1.1 Decision-Making Dialogue Acts
In fact, it is difficult to determine whether a di-
alogue act contains information relevant to any de-
cision point without knowing what decisions have
been made in the meeting. Therefore, in this study
DM DAs are annotated in a two-phase process:
First, annotators are asked to browse through the
meeting record and write an abstractive summary
directed to the project manager about the decisions
that have been made in the meeting. Next, another
group of three annotators are asked to produce ex-
tractive summaries by selecting a subset (around
10%) of dialogue acts which form a summary of this
meeting for the absent manager to understand what
has transpired in the meeting.
Finally, this group of annotators are asked to go
through the extractive dialogue acts one by one and
judge whether they support any of the sentences in
the decision section of the abstractive summary; if a
dialogue act is related to any sentence in the decision
section, a ?decision link? from the dialogue act to
the decision sentence is added. For those extracted
dialogue acts that do not have any closely related
sentence, the annotators are not obligated to specify
a link. We then label the dialogue acts that have one
or more decision links as DM DAs.
In the 50 meetings we used for the experiments,
the annotators have on average found four decisions
per meeting and specified around two decision links
to each sentence in the decision summary section.
Overall, 554 out of 37,400 dialogue acts have been
annotated as DM DAs, accounting for 1.4% of all di-
alogue acts in the data set and 12.7% of the orginal
extractive summary (which is consisted of the ex-
tracted dialogue acts). An earlier analysis has es-
tablished the intercoder reliability of the two-phase
process at the level of kappa ranging from 0.5 to
0.8. In this round of experiment, for each meeting
in the 50-meeting dataset we randomly choose the
DM DA annotation of one annotator as the sourec of
its ground truth data.
4.1.2 Decision-Making Topic Segments
Topic segmentation has also been annotated for
the AMI meeting corpus. Annotators had the free-
dom to mark a topic as subordinated (down to two
levels) wherever appropriate. As the AMI meetings
are scenario-driven, annotators are expected to find
that most topics recur. Therefore, they are given a
standard set of topic descriptions that can be used
as labels for each identified topic segment. Annota-
tors will only add a new label if they cannot find a
match in the standard set. The AMI scenario meet-
ings contain around 14 topic segments per meeting.
Each segment lasts on average 44 dialogue acts long
and contains two DM DAs.
DM Segments are operationalized as topic seg-
ments that contain one or more DM DAs. Over-
all, 198 out of 623 (31.78%) topic segments in the
50-meeting dataset are DM Segments. As the meet-
ings we use are driven by a predetermined agenda,
we expect to find that interlocutors are more likely
to reach decisions when certain topics are brought
up. Analysis shows that some topics are indeed more
likely to contain decisions than others. For example,
80% of the segments labelled as Costing and 58%
of those labelled Budget are DM Segments, whereas
only 7% of the Existing Product segments and none
of the Trend-Watching segments are DM Segments.
Functional segments, such as Chitchat, Opening and
Closing, almost never include decisions.
4.2 Features Used
To provide a qualitative account of the effect of dif-
ferent feature types on the task of automatic decision
detection, we have conducted empirical analysis on
four major types of features: lexical, prosodic, con-
textual and topical features.
4.2.1 Lexical Features
Previous research has studied lexical differences
(i.e., occurrence counts of N-grams) between var-
ious aspects of speech, such as topics (Hsueh and
Moore, 2006), speaker gender (Boulis and Osten-
dorf, 2005), and story-telling conversation (Gordon
and Ganesan, 2005). As we expect that lexical dif-
ferences also exist in DM conversations, we gener-
ated language models from the DM Dialogue Acts in
the corpus. The comparison of the language models
generated from the DM dialogue Acts and the rest of
the conversations shows that some differences exist
between the two models: (1) decision making con-
versations are more likely to contain we than I and
You; (2) in decision-making conversations there are
more explicit mentions of topical words, such as ad-
vanced chips and functional design; (3) in decision-
27
Type Feature
Duration Number of words spoken in current, previous and next subdialogue
Duration (in seconds) of current, previous and next subdialogue
Pause Amount of silence (in seconds) preceding a subdialogue
Amount of silence (in seconds) following a subdialogue
Speech rate Number of words spoken per second in current, previous and next subdialogue
Number of syllables per second in current, previous and next subdialogue
Energy Overall energy level
Average energy level in the first, second, third, and fourth quarter of a subdialogue
Pitch Maximum and minimum F0, overall slope and variance
Slope and variance at the first 100 and 200 ms and last 100 and 200 ms,
at the first and second half, and at each quarter of the subdialogue
Table 1: Prosodic features used in this study.
making conversations, there are fewer negative ex-
pressions, such as I don?t think and I don?t know.
In an exploratory study using unigrams, as well as
bigrams and trigrams, we found that using bigrams
and trigrams does not improve the accuracy of clas-
sifying DM DAs, and therefore we include only uni-
grams in the set of lexical features in the experiments
reported in Section 6.
4.2.2 Prosodic Features
Functionally, prosodic features, i.e., energy, and
fundamental frequency (F0), are indicative of seg-
mentation and saliency. In this study, we follow
Shriberg and Stolcke?s (2001) direct modelling ap-
proach to manifest prosodic features as duration,
pause, speech rate, pitch contour, and energy level.
We utilize the individual sound files provided in the
AMI corpus. To extract prosodic features from the
sound files, we use the Snack Sound Toolkit to com-
pute a list of pitch and energy values delimited by
frames of 10 ms, using the normalized cross correla-
tion function. Then we apply a piecewise linearisa-
tion procedure to remove the outliers and average the
linearised values of the units within the time frame
of a word. Pitch contour of a dialogue act is ap-
proximated by measuring the pitch slope at multi-
ple points within the dialogue act, e.g., the first and
last 100 and 200 ms. The rate of speech is calcu-
lated as both the number of words spoken per sec-
ond and the number of syllables per second. We
use Festival?s speech synthesis front-end to return
phonemes and syllabification information. An ex-
ploratory study has shown the benefits of including
immediate prosodic contexts, and thus we also in-
clude prosodic features of the immediately preced-
ing and following dialogue acts. Table 1 contains
a list of automatically generated prosodic features
used in this study.
4.2.3 Contextual Features
From our qualitative analysis, we expect that con-
textual features specific to the AMI corpus, such as
the speaker role (i.e., PM, ME, ID, UID) and meet-
ing type (i.e., kick-off, conceptual design, functional
design, detailed design) to be characteristic of the
DM DAs. Analysis shows that (1) participants as-
signed to the role of PM produce 42.5% of the DM
DAs, and (2) participants make relatively fewer de-
cisions in the kick-off meetings. Analysis has also
demonstrated a difference in the type, the reflexiv-
ity1 and the number of addressees, between the DM
DAs and the non-DM DAs. For example, dialogue
acts of type inform, suggest, elicit assessment and
elicit inform are more likely to be DM DAs.
We have also found that immediately preceding
and following dialogue acts are important for iden-
tifying DM DAs. For example, stalls and frag-
ments preceding and fragments following a DM
DA are more likely than for non-DM DAs.2 In
1According to the annotation guideline, the reflexivity re-
flects on how the group is carrying on the task. In this case, the
interlocutors pause to evaluate the group performance less often
when it comes to decision making.
2STALL is where people start talking before they are ready,
or keep speaking when they haven?t figured out what to say;
FRAGMENT is the segment which is not really speech or is
unclear enough to be transcribed, or where the speaker did not
28
contrast, there is a lower chance of seeing sug-
gest and elicit-type DAs (i.e., elicit-inform, elicit-
suggestion, elicit-assessment) in the preceding and
following DM DAs.
4.2.4 Topical Features
As reported in Section 4.1.2, we find that inter-
locutors are more likely to reach decisions when cer-
tain topics are brought up. Also, we expect decision-
making conversations to take place towards the end
of a topic segment. Therefore, in this study we in-
clude the following features: the label of the current
topic segment, the position of the DA in a topic seg-
ment (measured in words, in seconds, and in %), the
distance to the previous topic shift (both at the top-
level and sub-topic level)(measured in seconds), the
duration of the current topic segment (both at the
top-level and sub-topic level)(measured in seconds).
5 Experiment
5.1 Classifying DM DAs
Detecting DM DAs is the first step of automatic de-
cision detection. For this purpose, we trained Max-
Ent models to classify each unseen sample as ei-
ther DM DA (POS) or non-DM DA (NEG). We per-
formed a 5-fold cross validation on the set of 50
meetings. In each fold, we trained MaxEnt mod-
els from the feature combinations in the training
set, wherein each of the extracted dialogue acts has
been labelled as either POS or NEG. Then, the
models were used to classify unseen instances in
the test set as either POS or NEG. In Section 4.2,
we described the four major types of features used
in this study: unigrams (LX1), prosodic (PROS),
contextual (CONT), and topical (TOPIC) features.
For comparison, we report the naive baseline ob-
tained by training the models on the prosodic fea-
tures alone, since the prosodic features can be gen-
erated fully automatically. The different combina-
tions of features we used for training models can
be divided into the following four groups: (A) us-
ing prosodic features alone (BASELINE), (B) us-
ing lexical, contextual and topical features alone
(LX1, CONT, TOPIC); (C) using all available fea-
tures except one of the four types of features (ALL-
LX1, ALL-PROS, ALL-CONT, ALL-TOPIC); and
get far enough to express the intention.
(D) using all available features (ALL).
6 Results
6.1 Classifying DM Segments
Detecting DM segments is necessary for interpret-
ing decisions, as it provides information about the
current topic of discussion. Here we combine the
predictions of the DM DAs to classify each unseen
topic segment in the test set as either DM Segment
(POS) or non-DM Segment (NEG). Recall that we
defined a DM Segment as a segment that contains
one or more hypothesized DM DAs. The task of de-
tecting DM Segments can thus be viewed as that of
detecting DM Dialogue Acts in a wider window.
6.2 EXP1: Classifying DM DAs
Table 2 reports the performance on the test set. The
results show that models trained with all features
(ALL), including lexical, prosodic, contextual and
topical features, yield substantially better perfor-
mance than the baseline on the task of detecting DM
DAs. We carried out a one-way ANOVA to exam-
ine the effect of different feature combinations on
overall accuracy (F1). The ANOVA suggests a reli-
able effect of feature type (F (9, 286) = 3.44; p <
0.001). Rows 2-4 in Table 2 report the performance
of models in Group B that are trained with a sin-
gle type of feature. Lexical features are the most
predictive features when used alone. We performed
sign tests to determine whether there are statistical
differences among these models and the baseline.
We find that when used alone, only lexical features
(LX1) can train a better model than the baseline
(p < 0.001). However, none of these models yields
a comparable performance to the ALL model.
To study the relative effect of the different fea-
ture types, Rows 5-8 in the table report the perfor-
mance of models in Group C, which are trained with
all available features except LX1, PROS, CONT and
TOPIC features respectively. The amount of degra-
dation in the overall accuracy (F1) of each of the
models in relation to that of the ALL model indi-
cates the contribution of the feature type that has
been left out of the model. We performed sign tests
to examine the differences among these models and
the ALL model. We find that the ALL model out-
performs all of these models (p < 0.001) except
29
Exact Match Lenient Match
Accuracy Precision Recall F1 Precision Recall F1
BASELINE(PROS) 0.32 0.06 0.1 0.32 0.1 0.15
LX1 0.53 0.3 0.38 0.6 0.43 0.5
CONT 0 0 0 0 0 0
TOPIC 0.49 0.11 0.17 0.57 0.11 0.17
ALL-PROS 0.63 0.47 0.54 0.71 0.57 0.63
ALL-LX1 0.61 0.34 0.44 0.65 0.43 0.52
ALL-CONT 0.66 0.62 0.64 0.69 0.68 0.69
ALL-TOPIC 0.72 0.54 0.62 0.7 0.52 0.59
ALL 0.72 0.54 0.62 0.76 0.64 0.7
Table 2: Effects of different combinations of features on detecting DM DAs.
the model trained by leaving out contextual features
(ALL-CONT). A closer investigation of the preci-
sion and recall of the ALL-CONT model shows that
the contextual features are detrimental to recall but
beneficial for precision. The mixed result is due to
the fact that models trained with contextual features
are tailored to recognize particular types of DM di-
alogue acts. Therefore, using these contextual fea-
tures improves the precision for these types of DM
DAs but reduces the overall recognition accuracy.
The last three columns of Table 2 are the results
obtained using a lenient match measure, allowing a
window of 10 seconds preceding and following a hy-
pothesized DM DA for recognition. The better re-
sults show that there is room for ambiguity in the
assessment of the exact timing of DM DAs.
6.3 EXP2: Classifying DM Segments
As expected, the results in Table 3 are better than
those reported in Table 2, achieving at best 83%
overall accuracy.The model that combines all fea-
tures (ALL) yields significantly better results than
the baseline. The ANOVA shows a reliable effect of
different feature types on the task of detecting DM
Segments (F (11, 284) = 2.33; p <= 0.01). Rows
2-4 suggest that lexical features are the most pre-
dictive in terms of overall accuracy. Sign tests con-
firm the advantage of using lexical features (LX1)
over the baseline (PROS) (p < 0.05). Interest-
ingly, the model that is trained with topical features
alone (TOPIC) yields substantially better precision
(p < 0.001). The increase from 49% precision for
the task of detecting DM DAs (in Table 2) to 91%
for that of detecting DM Segments stems from the
fact that decisions are more likely to occur in certain
types of topic segments. In turn, training models
with topical features helps eliminate incorrect pre-
dictions of DM DAs in these types of topic seg-
ments. However, the accuracy gain of the TOPIC
model on detecting certain types of DM Segments
does not extend to all types of DM Segments. This is
shown by the significantly lower recall of the TOPIC
model over the baseline (p < 0.001).
Finally, Rows 5-8 report the performance of the
models in Group (C) on the task of detecting DM
Segments. Sign tests again show that the model that
is trained with all available features (ALL) outper-
forms the models that leave out lexical, prosodic,
or topical features (p < 0.05). However, the ALL
model does not outperform the model that leaves out
contextual features. In addition, the contextual fea-
tures degrade the recall but improve the precision
on the task of detecting DM Segments. Calculat-
ing how much the overall accuracy of the models in
Group C degrades from the ALL model shows that
the most predictive features are the lexical features,
followed by the topical and prosodic features.
7 Discussion
As suggested by the mixed results obtained by the
model that is trained without the contextual features,
the two-phase decision annotation procedure (as de-
scribed in Section 4.1) may have caused annota-
tors to select dialogue acts that serve different func-
tional roles in a decision-making process in the set
of DM DAs. For example, in the dialogue shown
30
Exact Match
Accuracy Precision Recall F1
BASELINE(PROS) 0.67 0.39 0.49
LX1 0.69 0.69 0.69
CONT 0 0 0
TOPIC 0.91 0.17 0.29
ALL-PROS 0.82 0.76 0.79
ALL-LX1 0.79 0.64 0.7
ALL-CONT 0.79 0.86 0.83
ALL-TOPIC 0.75 0.73 0.74
ALL 0.86 0.8 0.82
Table 3: Effects of different combinations of features
on detecting DM Segments.
in Figure 1, the annotators have marked dialogue
act (1), (5), (8), and (11) as the DM DAs related
to this decision: ?There will be no feature to help
find the remote when it is misplaced?. Among the
four DM DAs, (1) describes the topic of what this
decision is about; (5) and (8) describe the arguments
that support the decision-making process; (11) in-
dicates the level of agreement or disagreement for
this decision. Yet these DM DAs which play dif-
ferent functional roles in the DM process may each
have their own characteristic features. Training one
model to recognize DM DAs of all functional roles
may have degraded the performance on the classifi-
cation tasks. Developing models for detecting DM
DAs that play different functional roles requires a
larger scale study to discover the anatomy of gen-
eral decision-making discussions.
8 Conclusions and Future Work
This is the first study that aimed to detect segments
of the conversation that contain decisions. We have
(1) empirically analyzed the characteristic features
of DM dialogue acts, and (2) computational devel-
oped models to detect DM dialogue acts and DM
topic segments, given the set of characteristic fea-
tures. Empirical analysis has provided a qualitative
account of the DM-characteristic features, whereas
training the computational models on different fea-
ture combinations has provided a quantitative ac-
count of the effect of different feature types on
the task of automatic decision detection. Empiri-
cal analysis has exhibited demonstrable differences
(1) A: but um the feature that we considered for it
not getting lost.
(2) B: Right. Well
(3) B: were talking about that a little bit
(4) B: when we got that email
(5) B: and we think that each of these are so
distinctive, that it it?s not just like another piece of
technology around your house.
(6) B: It?s gonna be somewhere that it can be seen.
(7) A: Mm-hmm.
(8) B: So we?re we?re not thinking that it?s gonna
be as critical to have the loss
(9) D: But if it?s like under covers or like in a couch
you still can?t see it.
. . .
(10) A: Okay , that?s a fair evaluation.
(11) A: Um we so we do we?ve decided not to
worry about that for now.
Figure 1: Example decision-making discussion
in the words (e.g., we), the contextual features (e.g.,
meeting type, speaker role, dialogue act type), and
the topical features. The experimental results have
suggested that (1) the model combining all the avail-
able features performs substantially better, achiev-
ing 62% and 82% overall accuracy on the task of
detecting DM DAs and that of detecting DM Seg-
ments, respectively, (2) lexical features are the best
indicators for both the task of detecting DM DAs and
that of detecting DM Segments, and (3) combining
topical features is important for improving the pre-
cision for the task of detecting DM Segments.
Many of the features used in this study require hu-
man intervention, such as manual transcriptions, an-
notated dialogue act segmentations and labels, anno-
tated topic segmentations and labels, and other types
of meeting-specific features. Our ultimate goal is to
identify decisions using automatically induced fea-
tures. Therefore, studying the performance degra-
dation when using the automatically generated ver-
sions of these features (e.g., ASR words) is essen-
tial for developing a fully automated component on
detecting decisions immediately after a meeting or
even for when a meeting is still in progress. An-
other problem that has been pointed out in Section 6
and in Section 7 is the different functional roles of
DM dialogue acts in current annotations. Purver et
al. (2006) have suggested a hierarchical annotation
scheme to accommodate the different aspects of ac-
tion items. The same technique may be applicable
31
in a more general decision detection task.
9 Acknowledgement
This work was supported by the European Union In-
tegrated Project AMI (Augmented Multi-party Inter-
action, FP6-506811, publication AMI-204).
References
S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The
necessity of a meeting recording and playback system,
and the benefit of topic-level annotations to meeting
browsing. In Proceedings of the Tenth International
Conference on Human-Computer Interaction.
C. Boulis and M. Ostendorf. 2005. A quantitative anal-
ysis of lexical differences between genders in tele-
phone conversation. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics. ACM Press.
CALO. 2003. http://www.ai.sri.com/project/calo.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Wellner.
2005. The ami meeting corpus: A pre-announcement.
In Proceedings of 2nd Joint Workshop on Multi-
modal Interaction and Related Machine Learning Al-
gorithms.
C. Cieri, D. Miller, and K. Walker. 2002. Research
methodologies, observations and outcomes in conver-
sational speech data collection. In Proceedings of the
Human Language Technologies Conference (HLT).
M. Galley, J. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use of bayesian networks to model
pragmatic dependencies. In Proceedings of the 42nd
Annual Meeting of the ACL.
J. S. Garofolo, C. D. Laprun, M. Michel, V.M. Stanford,
and E. Tabassi. 2004. The nist meeting room pilot
corpus. In Proceedings of LREC04.
D. Gatica-Perez, I. McCowan, D. Zhang, and S. Bengio.
2005. Detecting group interest level in meetings. In
IEEE Int. Conf. on Acoustics, Speech, and Signal Pro-
cessing (ICASSP).
Andrew S. Gordon and Kavita Ganesan. 2005. Auto-
mated story extraction from conversational speech. In
Proceedings of the Third International Conference on
Knowledge Capture (K-CAP 05).
D. Hillard, M. Ostendorf, and E. Shriberg. 2003. Detec-
tion of agreement vs. disagreement in meetings: Train-
ing with unlabeled data. In Proc. HLT-NAACL.
P. Hsueh and J. Moore. 2006. Automatic topic segmen-
tation and lablelling in multiparty dialogue. In the first
IEEE/ACM workshop on Spoken Language Technol-
ogy (SLT). IEEE/ACM.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The icsi meeting corpus.
In Proceedings of ICASSP-2003, Hong Kong.
W. Kunz and H. W. J. Ritte. 1970. Issue as elements
of information system. Technical Report Working Pa-
per 131, Institute of Urban and Regional Development
Research, University of California, Berkeley.
S. Marchand-Mailet. 2003. Meeting record modeling for
enhanced browsing. Technical report, Computer Vi-
sion and Multimedia Lab, Computer Centre, Univer-
sity of Geneva, Switzerland.
J. Niekrasz, M. Purver, J. Dowding, and S. Peters. 2005.
Ontology-based discourse understanding for a persis-
tent meeting assistant. In Proc. of the AAAI Spring
Symposium.
V. Pallotta, J. Niekrasz, and M. Purver. 2005. Collab-
orative and argumentative models of meeting discus-
sions. In Proceeding of CMNA-05 workshop on Com-
putational Models of Natural Arguments in IJCAI 05.
M. Purver, P. Ehlen, and J. Niekrasz. 2006. Shallow
discourse structure for action item detection. In the
Workshop of HLT-NAACL: Analyzing Conversations in
Text and Speech. ACM Press.
R. J. Rienks, D. Heylen, and E. van der Weijden. 2005.
Argument diagramming of meeting conversations. In
Multimodal Multiparty Meeting Processing Workshop
at the ICMI.
E. Shriberg and A. Stolcke. 2001. Direct modeling of
prosody: An overview of applications in automatic
speech processing.
A. Waibel, M. Bett, F. Metze, K. Ries, T. Schaaf amd
T. Schultz, H. Soltau, H. Yu, and K. Zechner. 2001.
Advances in automatic meeting record creation and ac-
cess. In Proceedings of ICASSP.
S. Whittaker, R. Laban, and S. Tucker. 2005. Analysing
meeting records: An ethnographic study and techno-
logical implications. In Proceedings of MLMI 2005.
B. Wrede and E. Shriberg. 2003. Spotting hot spots in
meetings: Human judgements and prosodic cues. In
Proceedings of EUROSPEECH 2003.
32
NAACL HLT Demonstration Program, pages 5?6,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Adaptive Tutorial Dialogue Systems Using Deep NLP Techniques
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow,
Manuel Marques-Pita, Colin Matheson and Johanna D. Moore
ICCS-HCRC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, United Kingdom
(mdzikovs,ccallawa,efarrow,mmpita,colin,jmoore)@inf.ed.ac.uk ?
Abstract
We present tutorial dialogue systems in
two different domains that demonstrate
the use of dialogue management and deep
natural language processing techniques.
Generation techniques are used to produce
natural sounding feedback adapted to stu-
dent performance and the dialogue his-
tory, and context is used to interpret ten-
tative answers phrased as questions.
1 Introduction
Intelligent tutoring systems help students improve
learning compared to reading textbooks, though not
quite as much as human tutors (Anderson et al,
1995). The specific properties of human-human di-
alogue that help students learn are still being stud-
ied, but the proposed features important for learn-
ing include allowing students to explain their actions
(Chi et al, 1994), adapting tutorial feedback to the
learner?s level, and engagement/affect. Some tuto-
rial dialogue systems use NLP techniques to analyze
student responses to ?why? questions. (Aleven et al,
2001; Jordan et al, 2006). However, for remediation
they revert to scripted dialogue, relying on short-
answer questions and canned feedback. The result-
ing dialogue may be redundant in ways detrimental
to student understanding (Jordan et al, 2005) and
allows for only limited adaptivity (Jordan, 2004).
?This work was supported under the 6th Framework Pro-
gramme of the European Commission, Ref. IST-507826, and
by a grant from The Office of Naval Research N000149910165.
We demonstrate two tutorial dialogue systems
that use techniques from task-oriented dialogue sys-
tems to improve the interaction. The systems are
built using the Information State Update approach
(Larsson and Traum, 2000) for dialogue manage-
ment and generic components for deep natural lan-
guage understanding and generation. Tutorial feed-
back is generated adaptively based on the student
model, and the interpretation is used to process
explanations and to differentiate between student
queries and hedged answers phrased as questions.
The systems are intended for testing hypotheses
about tutoring. By comparing student learning gains
between versions of the same system using different
tutoring strategies, as well as between the systems
and human tutors, we can test hypotheses about the
role of factors such as free natural language input,
adaptivity and student affect.
2 The BEEDIFF Tutor
The BEEDIFF tutor helps students solve symbolic
differentiation problems, a procedural task. Solu-
tion graphs generated by a domain reasoner are used
to interpret student actions and to generate feed-
back.1 Student input is relatively limited and con-
sists mostly of mathematical formulas, but the sys-
tem generates adaptive feedback based on the notion
of student performance and on the dialogue history.
For example, if an average student asks for a hint
on differentiating sin(x2), the first level of feedback
may be ?Think about which rule to apply?, which
1Solution graphs are generated automatically for arbitrary
expressions, with no limit on the complexity of expressions ex-
cept for possible efficiency considerations.
5
can then be specialized to ?Use the chain rule? and
then to giving away the complete answer. For stu-
dents with low performance, more specific feed-
back can be given from the start. The same strat-
egy (based on an initial corpus analysis) is used in
producing feedback after incorrect answers, and we
intend to use the system to evaluate its effectiveness.
The feedback is generated automatically from a
single diagnosis and generation techniques are used
to produce appropriate discourse cues. For example,
when a student repeats the same mistake, the feed-
back may be ?You?ve differentiated the inner layer
correctly, but you?re still missing the minus sign?.
The two clauses are joined by a contrast relationship,
and the second indicates that an error was repeated
by using the adverbial ?still?.
3 The BEETLE Tutor
The BEETLE tutor is designed to teach students ba-
sic electricity and electronics concepts. Unlike the
BEEDIFF tutor, the BEETLE tutor is built around
a pre-planned course where the students alternate
reading with exercises involving answering ?why?
questions and interacting with a circuit simulator.
Since this is a conceptual domain, for most exer-
cises there is no structured sequence of steps that the
students should follow, but students need to name a
correct set of objects and relationships in their re-
sponse. We model the process of building an answer
to an exercise as co-constructing a solution, where
the student and tutor may contribute parts of the an-
swer. For example, consider the question ?For each
circuit, which components are in a closed path?.
The solution can be built up gradually, with the stu-
dent naming different components, and the system
providing feedback until the list is complete. This
generic process of gradually building up a solution is
also applied to giving explanations. For example, in
answer to the question ?What is required for a light
bulb to light? the student may say ?The bulb must be
in a closed path?, which is correct but not complete.
The system may then say ?Correct, but is that every-
thing?? to prompt the student towards mentioning
the battery as well. The diagnosis of the student an-
swer is represented as a set of correctly given objects
or relationships, incorrect parts, and objects and re-
lationships that have yet to be mentioned, and the
system uses the same dialogue strategy of eliciting
the missing parts for all types of questions.
Students often phrase their answers tentatively,
for example ?Is the bulb in a closed path??. In the
context of a tutor question the interpretation process
treats yes-no questions from the student as poten-
tially hedged answers. The dialogue manager at-
tempts to match the objects and relationships in the
student input with those in the question. If a close
match can be found, then the student utterance is
interpreted as giving an answer rather than a true
query. In contrast, if the student said ?Is the bulb
connected to the battery??, this would be interpreted
as a proper query and the system would attempt to
answer it.
Conclusion We demonstrate two tutorial dialogue
systems in different domains built by adapting di-
alogue techniques from task-oriented dialogue sys-
tems. Improved interpretation and generation help
support adaptivity and a wider range of inputs than
possible in scripted dialogue.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In Proc. AI-ED 2001.
J. R. Anderson, A. T. Corbett, K. R. Koedinger, and
R. Pelletier. 1995. Cognitive tutors: Lessons learned.
The Journal of the Learning Sciences, 4(2):167?207.
M. T. H. Chi, N. de Leeuw, M.-H. Chiu, and C. La-
Vancher. 1994. Eliciting self-explanations improves
understanding. Cognitive Science, 18(3):439?477.
P. Jordan, P. Albacete, and K. VanLehn. 2005. Taking
control of redundancy in scripted tutorial dialogue. In
Proc. of AIED2005, pages 314?321.
P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,
and P. Albacete. 2006. A natural language tutorial
dialogue system for physics. In Proc. of FLAIRS-06.
P. W. Jordan. 2004. Using student explanations as mod-
els for adapting tutorial dialogue. In V. Barr and
Z. Markov, editors, FLAIRS Conference. AAAI Press.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI Dialogue Move
Engine Toolkit. Natural Language Engineering, 6(3-
4):323?340.
6
NAACL HLT Demonstration Program, pages 9?10,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Automatic Segmentation and Summarization of Meeting Speech
Gabriel Murray, Pei-Yun Hsueh, Simon Tucker
Jonathan Kilgour, Jean Carletta, Johanna Moore, Steve Renals
University of Edinburgh
Edinburgh, Scotland
fgabriel.murray,p.hsuehg@ed.ac.uk
1 Introduction
AMI Meeting Facilitator is a system that per-
forms topic segmentation and extractive sum-
marisation. It consists of three components: (1)
a segmenter that divides a meeting into a num-
ber of locally coherent segments, (2) a summa-
rizer that selects the most important utterances
from the meeting transcripts. and (3) a com-
pression component that removes the less im-
portant words from each utterance based on the
degree of compression the user specied. The
goal of the AMI Meeting Facilitator is two-fold:
rst, we want to provide sucient visual aids for
users to interpret what is going on in a recorded
meeting; second, we want to support the devel-
opment of downstream information retrieval and
information extraction modules with the infor-
mation about the topics and summaries in meet-
ing segments.
2 Component Description
2.1 Segmentation
The AMI Meeting Segmenter is trained using a
set of 50 meetings that are seperate from the in-
put meeting. We rst extract features from the
audio and video recording of the input meeting
in order to train the Maximum Entropy (Max-
Ent) models for classifying topic boundaries and
non-topic boundaries. Then we test each utter-
ance in the input meeting on the Segmenter to
see if it is a topic boundary or not. The features
we use include the following ve categories: (1)
Conversational Feature: These include a set
of seven conversational features, including the
amount of overlapping speech, the amount of
silence between speaker segments, the level of
similarity of speaker activity, the number of cue
words, and the predictions of LCSEG (i.e., the
lexical cohesion statistics, the estimated poste-
rior probability, the predicted class). (2) Lex-
ical Feature: Each spurt is represented as a
vector space of uni-grams, wherein a vector is 1
or 0 depending on whether the cue word appears
in the spurt. (3) Prosodic Feature: These
include dialogue-act (DA) rate-of-speech, max-
imum F0 of the DA, mean energy of the DA,
amount of silence in the DA, precedent and sub-
sequent pauses, and duration of the DA. (4)
Motion Feature: These include the average
magnitude of speaker movements, which is mea-
sured by the number of pixels changed, over the
frames of 40 ms within the spurt. (5) Contex-
tual Feature: These include the dialogue act
types and the speaker role (e.g., project man-
ager, marketing expert). In the dialogue act an-
notations, each dialogue act is classied as one
of the 15 types.
2.2 Summarization
The AMI summarizer is trained using a set of
98 scenario meetings. We train a support vec-
tor machine (SVM) on these meetings, using 26
features relating to the following categories: (1)
Prosodic Features: These include dialogue-
act (DA) rate-of-speech, maximum F0 of the
DA, mean energy of the DA, amount of silence
in the DA, precedent and subsequent pauses,
9
and duration of the DA. (2) Speaker Fea-
tures: These features relate to how dominant
the speaker is in the meeting as a whole, and
they include percentage of the total dialogue
acts which each speaker utters, percentage of
total words which speaker utters, and amount
of time in meeting that each person is speak-
ing. (3) Structural Features: These features
include the DA position in the meeting, and the
DA position in the speaker's turn. (4) Term
Weighting Features: We use two types of
term weighting: tf.idf, which is based on words
that are frequent in the meeting but rare across
a set of other meetings or documents, and a sec-
ond weighting feature which relates to how word
usage varies between the four meeting partici-
pants.
After training the SVM, we test on each meet-
ing of the 20 meeting test set in turn, ranking
the dialogue acts from most probable to least
probable in terms of being extract-worthy. Such
a ranking allows the user to create a summary
of whatever length she desires.
2.3 Compression
Each dialogue act has its constituent words
scored using tf.idf, and as the user compresses
the meeting to a greater degree the browser
gradually removes the less important words from
each dialogue act, leaving only the most infor-
mative material of the meeting.
3 Related Work
Previous work has explored the eect of lexi-
cal cohesion and conversational features on char-
acterizing topic boundaries, following Galley et
al.(2003). In previous work, we have also studied
the problem of predicting topic boundaries at
dierent levels of granularity and showed that a
supervised classication approach performs bet-
ter on predicting a coarser level of topic segmen-
tation (Hsueh et al, 2006).
The amount of work being done on speech
summarization has accelerated in recent years.
Maskey and Hirschberg(September 2005) have
explored speech summarization in the domain
of Broadcast News data, nding that combin-
ing prosodic, lexical and structural features yield
the best results. On the ICSI meeting corpus,
Murray et al(September 2005) compared apply-
ing text summarization approaches to feature-
based approaches including prosodic features,
while Galley(2006) used skip-chain Conditional
Random Fields to model pragmatic dependen-
cies between meeting utterances, and ranked
meeting dialogue acts using a combination or
prosodic, lexical, discourse and structural fea-
tures.
4 acknowledgement
This work was supported by the European
Union 6th FWP IST Integrated Project AMI
(Augmented Multi- party Interaction, FP6-
506811)
References
M. Galley, K. McKeown, E. Fosler-Lussier, and
H. Jing. 2003. Discourse segmentation of multi-
party conversation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
M. Galley. 2006. A skip-chain conditional ran-
dom eld for ranking meeting utterances by im-
portance. In Proceedings of EMNLP-06, Sydney,
Australia.
P. Hsueh, J. Moore, and S. Renals. 2006. Automatic
segmentation of multiparty dialogue. In the Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
S. Maskey and J. Hirschberg. September 2005. Com-
paring lexial, acoustic/prosodic, discourse and
structural features for speech summarization. In
Proceedings of the 9th European Conference on
Speech Communication and Technology, Lisbon,
Portugal.
G. Murray, S. Renals, and J. Carletta. Septem-
ber 2005. Extractive summarization of meeting
recordings. In Proceedings of the 9th European
Conference on Speech Communication and Tech-
nology, Lisbon, Portugal.
10
An Empirical Study of the Influence of Argument Conciseness on 
Argument Effectiveness 
Giuseppe Carenini  
Intelligent Systems Program 
University of Pittsburgh,  
Pittsburgh, PA 15260, USA 
carenini@cs.pitt.edu 
Johanna D. Moore 
The Human Communication Research Centre, 
University of Edinburgh, 
2 Buccleuch Place, Edinburgh EH8 9LW, UK.  
jmoore@cogsci.ed.ac.uk 
  
Abstract  
We have developed a system that generates 
evaluative arguments that are tailored to the 
user, properly arranged and concise. We have 
also developed an evaluation framework in 
which the effectiveness of evaluative arguments 
can be measured with real users. This paper 
presents the results of a formal experiment we 
have performed in our framework to verify the 
influence of argument conciseness on argument 
effectiveness 
1 Introduction 
Empirical methods are critical to gauge the 
scalability and robustness of proposed 
approaches, to assess progress and to stimulate 
new research questions. In the field of natural 
language generation, empirical evaluation has 
only recently become a top research priority 
(Dale, Eugenio et al 1998). Some empirical 
work has been done to evaluate models for 
generating descriptions of objects and processes 
from a knowledge base (Lester and Porter March 
1997), text summaries of quantitative data 
(Robin and McKeown 1996), descriptions of 
plans (Young to appear) and concise causal 
arguments (McConachy, Korb et al 1998). 
However, little attention has been paid to the 
evaluation of systems generating evaluative 
arguments, communicative acts that attempt to 
affect the addressee?s attitudes (i.e. evaluative 
tendencies typically phrased in terms of like and 
dislike or favor and disfavor). 
The ability to generate evaluative arguments is 
critical in an increasing number of online 
systems that serve as personal assistants, 
advisors, or shopping assistants1. For instance, a 
shopping assistant may need to compare two 
similar products and argue why its current user 
should like one more than the other. 
                                                   
1
 See for instance www.activebuyersguide.com 
In the remainder of the paper, we first describe a 
computational framework for generating 
evaluative arguments at different levels of 
conciseness. Then, we present an evaluation 
framework in which the effectiveness of 
evaluative arguments can be measured with real 
users. Next, we describe the design of an 
experiment we ran within the framework to 
verify the influence of argument conciseness on 
argument effectiveness. We conclude with a 
discussion of the experiment?s results. 
2 Generating concise evaluative 
arguments 
Often an argument cannot mention all the 
available evidence, usually for the sake of 
brevity. According to argumentation theory, the 
selection of what evidence to mention in an 
argument should be based on a measure of the 
evidence strength of support (or opposition) to 
the main claim of the argument (Mayberry and 
Golden 1996). Furthermore, argumentation 
theory suggests that for evaluative arguments the 
measure of evidence strength should be based on 
a model of the intended reader?s values and 
preferences. 
Following argumentation theory, we have 
designed an argumentative strategy for 
generating evaluative arguments that are 
properly arranged and concise (Carenini and 
Moore 2000).  In our strategy, we assume that 
the reader?s values and preferences are 
represented as an additive multiattribute value 
function (AMVF), a conceptualization based on 
multiattribute utility theory (MAUT)(Clemen 
1996). This allows us to adopt and extend a 
measure of evidence strength proposed in 
previous work on explaining decision theoretic 
advice based on an AMVF (Klein1994).
Figure 1 Sample additive multiattribute value function (AMVF) 
The argumentation strategy has been 
implemented as part of a complete argument 
generator. Other modules of the generator 
include a microplanner, which performs 
aggregation, pronominalization and makes 
decisions about cue phrases and scalar 
adjectives, along with a sentence realizer, which 
extends previous work on realizing evaluative 
statements (Elhadad 1995). 
2.1 Background on AMVF 
An AMVF is a model of a person?s values and 
preferences with respect to entities in a certain 
class. It comprises a value tree and a set of 
component value functions, one for each 
primitive attribute of the entity. A value tree is a 
decomposition of the value of an entity into a 
hierarchy of aspects of the entity2, in which the 
leaves correspond to the entity primitive 
attributes (see Figure 1 for a simple value tree in 
the real estate domain). The arcs of the tree are 
weighted to represent the importance of the 
value of an objective in contributing to the value 
of its parent in the tree (e.g., in Figure 1 location 
is more than twice as important as size in 
determining the value of a house). Note that the 
sum of the weights at each level is equal to 1. A 
component value function for an attribute 
expresses the preferability of each attribute 
value as a number in the [0,1] interval. For 
instance, in Figure 1 neighborhood n2 has 
preferability 0.3, and a distance-from-park of 1 
mile has preferability (1 - (1/5 * 1))=0.8). 
                                                   
2
 In decision theory these aspects are called 
objectives. For consistency with previous work, we 
will follow this terminology in the remainder of the 
paper. 
Formally, an AMVF predicts the value )(ev of 
an entity e as follows: 
v(e) = v(x1,?,xn) = ?wi vi(xi), where 
- (x1,?,xn) is the vector of attribute values for an 
entity e 
- ?attribute i, vi is the component value function, 
which maps the least preferable xi to 0, the most 
preferable to 1, and the other xi to values in [0,1] 
- wi is the weight for attribute i, with 0? wi ?1 
and ?wi =1 
- wi is equal to the product of all the weights 
from the root of the value tree to the attribute i 
 
A function vo(e) can also be defined for each 
objective. When applied to an entity, this 
function returns the value of the entity with 
respect to that objective. For instance, assuming 
the value tree shown in Figure 1, we have: 
  
))(6.0())(4.0(
)(
evev
ev
parkfromDistodNeighborho
Location
??
?+?=
=
 
Thus, given someone?s AMVF, it is possible to 
compute how valuable an entity is to that 
individual. Furthermore, it is possible to 
compute how valuable any objective (i.e., any 
aspect of that entity) is for that person. All of 
these values are expressed as a number in the 
interval [0,1]. 
2.2 A measure of evidence strength 
Given an AMVF for a user applied to an entity 
(e.g., a house), it is possible to define a precise 
measure of an objective strength in determining 
the evaluation of its parent objective for that 
entity. This measure is proportional to two 
factors: (A) the weight of the objective 
? +???
k =1
k = -1
k = 0
compellingness
Figure 2 Sample population of objectives 
represented by dots and ordered by their 
compellingness 
 
(which is by itself a measure of importance), (B) 
a factor that increases equally for high and low 
values of the objective, because an objective can 
be important either because it is liked a lot or 
because it is disliked a lot. We call this measure 
s-compellingness and provide the following 
definition: 
s-compellingness(o, e, refo) = (A)? (B) = 
      = w(o,refo)? max[[vo(e)]; [1 ? vo(e)]],  where 
? o is an objective, e is an entity, refo is an 
ancestor of o in the value tree 
? w(o,refo) is the product of the weights of all 
the links from o to refo 
? vo is the component value function for leaf 
objectives (i.e., attributes), and it is the 
recursive evaluation over children(o) for 
nonleaf objectives 
Given a measure of an objective's strength, a 
predicate indicating whether an objective should 
be included in an argument (i.e., worth 
mentioning) can be defined as follows:  
s-notably-compelling?(o,opop,e, refo) ? 
?s-compellingness(o, e, refo)?>?x+k?x , where 
? o, e, and refo are defined as in the previous 
Def; opop is an objective population (e.g., 
siblings(o)), and ?opop?>2 
? p? opop; x?X = ?s-compellingness(p, e, 
refo)? 
? ?x is the mean of X, ?x  is the standard 
deviation and k is a user-defined constant 
Similar measures for the comparison of two 
entities are defined and extensively discussed in 
(Klein 1994). 
2.3 The constant k 
In the definition of s-notably-compelling?, the 
constant k determines the lower bound of s-
compellingness for an objective to be included 
in an argument. As shown in Figure 2, for k=0 
only objectives with s-compellingness greater  
Figure 3 Arguments about the same house, 
tailored to the same subject but with k ranging 
from 1 to ?1 
 
than the average s-compellingness in a 
population are included in the argument (4 in the 
sample population). For higher positive values 
of k less objectives are included (only 2, when 
k=1), and the opposite happens for negative 
values (8 objectives are included, when k=-1). 
Therefore, by setting the constant k to different 
values, it is possible to control in a principled 
way how many objectives (i.e., pieces of 
evidence) are included in an argument, thus 
controlling the degree of conciseness of the 
generated arguments. 
Figure 3 clearly illustrates this point by showing 
seven arguments generated by our argument 
generator in the real-estate domain. These 
arguments are about the same house, tailored to 
the same subject, for k ranging from 1 to ?1. 
3 The evaluation framework 
In order to evaluate different aspects of the 
argument generator, we have developed an 
evaluation framework based on the task efficacy 
evaluation method. This method allows 
Figure 4 The evaluation framework architecture
the experimenter to evaluate a generation model 
by measuring the effects of its output on user?s 
behaviors, beliefs and attitudes in the context of 
a task. 
Aiming at general results, we chose a rather 
basic and frequent task that has been extensively 
studied in decision analysis: the selection of a 
subset of preferred objects (e.g., houses) out of a 
set of possible alternatives. In the evaluation 
framework that we have developed, the user 
performs this task by using a computer 
environment (shown in Figure 5) that supports 
interactive data exploration and analysis (IDEA) 
(Roth, Chuah et al 1997). The IDEA 
environment provides the user with a set of 
powerful visualization and direct manipulation 
techniques that facilitate the user?s autonomous 
exploration of the set of alternatives and the 
selection of the preferred alternatives. 
Let?s examine now how an argument generator 
can be evaluated in the context of the selection 
task, by going through the architecture of the 
evaluation framework. 
3.1 The evaluation framework architecture 
Figure 4 shows the architecture of the evaluation 
framework. The framework consists of three 
main sub-systems: the IDEA system, a User 
Model Refiner and the Argument Generator. The 
framework assumes that a model of the user?s 
preferences (an AMVF) has been previously 
acquired from the user, to assure a reliable initial 
model. 
At the onset, the user is assigned the task to 
select from the dataset the four most preferred 
alternatives and to place them in a Hot List (see 
Figure 5, upper right corner) ordered by 
preference. The IDEA system supports the user 
in this task (Figure 4 (1)). As the interaction 
unfolds, all user actions are monitored and 
collected in the User?s Action History (Figure 4 
(2a)). Whenever the user feels that the task is 
accomplished, the ordered list of preferred 
alternatives is saved as her Preliminary Decision 
(Figure 4 (2b)). After that, this list, the User?s 
Action History and the initial Model of User?s 
Preferences are analysed by the User Model 
Refiner (Figure 4 (3)) to produce a Refined 
Model of the User?s Preferences (Figure 4 (4)).  
At this point, the stage is set for argument 
generation. Given the Refined Model of the 
User?s Preferences, the Argument Generator 
produces an evaluative argument tailored to the 
model (Figure 4 (5-6)), which is presented to the 
user by the IDEA system (Figure 4 (7)).The 
argument goal is to introduce a new alternative 
(not included in the dataset initially presented to 
the user) and to persuade the user that the 
alternative is worth being considered. The new 
alternative is designed on the fly to be preferable 
for the user given her preference model.  
 3-26
HotList
NewHouse 3-26
Figure 5 The IDEA environment display at the end of the interaction
All the information about the new alternative is 
also presented graphically. Once the argument is 
presented, the user may (a) decide immediately 
to introduce the new alternative in her Hot List, 
or (b) decide to further explore the dataset, 
possibly making changes to the Hot List adding 
the new instance to the Hot List, or (c) do 
nothing. Figure 5 shows the display at the end of 
the interaction, when the user, after reading the 
argument, has decided to introduce the new 
alternative in the Hot List first position (Figure 
5, top right). 
Whenever the user decides to stop exploring and 
is satisfied with her final selections, measures 
related to argument?s effectiveness can be 
assessed (Figure 4  (8)). These measures are 
obtained either from the record of the user 
interaction with the system or from user self-
reports in a final questionnaire (see Figure 6 for 
an example of self-report) and include: 
- Measures of behavioral intentions and attitude 
change: (a) whether or not the user adopts the 
new proposed alternative, (b) in which position 
in the Hot List she places it and (c) how much 
she likes the new alternative and the other 
objects in the Hot List.  
- A measure of the user?s confidence that she has 
selected the best for her in the set of alternatives.  
- A measure of argument effectiveness derived 
by explicitly questioning the user at the end of 
the interaction about the rationale for her 
decision (Olso and Zanna 1991). This can 
provide valuable information on what aspects of 
the argument were more influential (i.e., better 
understood and accepted by the user). 
- An additional measure of argument 
effectiveness is to explicitly ask the user at the 
end of the interaction to judge the argument with 
respect to several dimensions of quality, such as 
content, organization, writing style and 
convincigness. However, evaluations based on 
Figure 6 Self -report on user?s satisfaction with 
houses in the HotList 
Figure 7 Hypotheses on experiment outcomes 
judgements along these dimensions are clearly 
weaker than evaluations measuring actual 
behavioural and attitudinal changes (Olso and 
Zanna 1991).  
To summarize, the evaluation framework just 
described supports users in performing a 
realistic task at their own pace by interacting 
with an IDEA system. In the context of this task, 
an evaluative argument is generated and 
measurements related to its effectiveness can be 
performed. 
We now discuss an experiment that we have 
performed within the evaluation framework 
4 The Experiment 
The argument generator has been designed to 
facilitate testing the effectiveness of different 
aspects of the generation process.  The 
experimenter can easily control whether the 
generator tailors the argument to the current 
user, the degree of conciseness of the argument 
(by varying k as explained in Section 2.3), and 
what microplanning tasks the generator 
performs. In the experiment described here, we 
focused on studying the influence of argument 
conciseness on argument effectiveness. A 
parallel experiment about the influence of 
tailoring is described elsewhere. 
We followed a between-subjects design with 
three experimental conditions: 
No-Argument - subjects are simply informed that 
a new house came on the market.  
Tailored-Concise - subjects are presented with 
an evaluation of the new house tailored to their 
preferences and at a level of conciseness that we 
hypothesize to be optimal. To start our 
investigation, we assume that an effective 
argument (in our domain) should contain 
slightly more than half of the available evidence. 
By running the generator with different values 
for k on the user models of the pilot subjects, we 
found that this corresponds to k=-0.3. In fact, 
with k=-0.3 the arguments contained on average 
10 pieces of evidence out of the 19 available. 
Tailored-Verbose - subjects are presented with 
an evaluation of the new house tailored to their 
preferences, but at a level of conciseness that we 
hypothesize to be too low (k=-1, which 
corresponds on average, in our analysis of the 
pilot subjects, to 16 pieces of evidence out of the 
possible 19).  
In the three conditions, all the information about 
the new house is also presented graphically, so 
that no information is hidden from the subject. 
Our hypotheses on the outcomes of the 
experiment are summarized in Figure 7. We 
expect arguments generated for the Tailored-
Concise condition to be more effective than 
arguments generated for the Tailored-Verbose 
condition. We also expect the Tailored-Concise 
condition to be somewhat better than the No-
Argument condition, but to a lesser extent, 
because subjects, in the absence of any 
argument, may spend more time further 
exploring the dataset, thus reaching a more 
informed and balanced decision. Finally, we do 
not have strong hypotheses on comparisons of 
argument effectiveness between the No-
Argument and Tailored-Verbose conditions. 
The experiment is organized in two phases. In 
the first phase, the subject fills out a 
questionnaire on the Web. The questionnaire 
implements a method form decision theory to 
acquire an AMVF model of the subject?s 
preferences (Edwards and Barron 1994). In the 
second phase of the experiment, to control for 
possible confounding variables (including 
subject?s argumentativeness (Infante and Rancer 
1982), need for cognition (Cacioppo, Petty et al 
1983), intelligence and self-esteem), the subject 
Tailored
Concise
Tailored
Verbose
No-Argument
>
>> ?
 
a) How would you judge the houses in your Hot List? 
The more you like the house the closer you should  
        put a cross to ?good choice? 
 1st house
 
bad choice  : __:__:__:__ :__:__:__:__:__: good choice 
2nd house
 
bad choice  : __:__:__:__ :__:__:__:__:__: good choice 
3rd house 
 
bad choice  : __:__:__:__ :__:__:__:__:__: good choice 
4th house
 
bad choice  : __:__:__:__ :__:__:__:__:__: good choice 
 
 
Figure 8 Sample filled-out self-report on user?s 
satisfaction with houses in the Hot List3 
is randomly assigned to one of the three 
conditions. 
Then, the subject interacts with the evaluation 
framework and at the end of the interaction 
measures of the argument effectiveness are 
collected, as described in Section 3.1.  
After running the experiment with 8 pilot 
subjects to refine and improve the experimental 
procedure, we ran a formal experiment involving 
30 subjects, 10 in each experimental condition. 
5 Experiment Results 
5.1 A precise measure of satisfaction 
According to literature on persuasion, the most 
important measures of arguments effectiveness 
are the ones of behavioral intentions and attitude 
change. As explained in Section 3.1, in our 
framework such measures include (a) whether or 
not the user adopts the new proposed alternative, 
(b) in which position in the Hot List she places 
it, (c) how much she likes the proposed new 
alternative and  the other objects in the Hot List. 
Measures (a) and (b) are obtained from the 
record of the user interaction with the system, 
whereas measures in (c) are obtained from user 
self-reports. 
A closer analysis of the above measures 
indicates that the measures in (c) are simply a 
more precise version of measures (a) and (b). In 
fact, not only they assess the same information 
as measures (a) and (b), namely a preference 
ranking among the new alternative and the 
objects in the Hot List, but they also offer two 
additional critical advantages: 
                                                   
3
 If the subject does not adopt the new house, she is 
asked to express her satisfaction with the new house 
in an additional self-report.  
(i) Self-reports allow a subject to express 
differences in satisfaction more precisely than 
by ranking. For instance, in the self-report 
shown in Figure 8, the subject was able to 
specify that the first house in the Hot List was 
only one space (unit of satisfaction) better then 
the house preceding it in the ranking, while the 
third house was two spaces better than the house 
preceding it.  
(ii) Self-reports do not force subjects to express 
a total order between the houses. For instance, in 
Figure 8 the subject was allowed to express that 
the second and the third house in the Hot List 
were equally good for her. 
Furthermore, measures of satisfaction obtained 
through self-reports can be combined in a single, 
statistically sound measure that concisely 
express how much the subject liked the new 
house with respect to the other houses in the Hot 
List. This measure is the z-score of the subject?s 
self-reported satisfaction with the new house, 
with respect to the self-reported satisfaction with 
the houses in the Hot List. A z-score is a 
normalized distance in standard deviation units 
of a measure xi from the mean of a population X. 
Formally:  
xi? X; z-score( xi ,X) = [xi - ? (X)] / ?(X) 
For instance, the satisfaction z-score for the new 
instance, given the sample self-reports shown in 
Figure 8, would be:  
[7 - ? ({8,7,7,5})] /  ?({8,7,7,5}) = 0.2 
The satisfaction z-score precisely and concisely 
integrates all the measures of behavioral 
intentions and attitude change. We have used 
satisfaction z-scores as our primary measure of 
argument effectiveness. 
5.2 Results 
As shown in Figure 9, the satisfaction z-scores 
obtained in the experiment confirmed our 
hypotheses. Arguments generated for the 
Tailored-Concise condition were significantly 
more effective than arguments generated for 
Tailored-Verbose condition. The Tailored-
Concise condition was also significantly better 
than the No-Argument condition, but to a lesser 
extent. Logs of the interactions suggest that this 
happened because subjects in the No-Argument 
condition spent significantly more time further 
exploring the dataset. Finally, there was no 
significant difference in argument effectiveness  
a)
    How would you judge the houses in your Hot List? 
The more you like the house the closer you should  
        put a cross to ?good choice? 
 1st house
 
bad choice  : __:__:__:__ :__:__:__:X :__: good choice 
2nd house(New house)
 
bad choice  : __:__:__:__ :__:__:X :__:__: good choice 
3rd house 
 
bad choice  : __:__:__:__ :__:__:X :__:__: good choice 
4th house
 
bad choice  : __:__:__:__ :X :__:__:__:__: good choice 
 
 
 Figure 9
 
Results for satisfaction z-scores. The 
average z-scores for the three conditions are 
shown in the grey boxes and the p-values are 
reported beside the links 
 
between the No-Argument and Tailored-
Verbose conditions. 
With respect to the other measures of argument 
effectiveness mentioned in Section 3.1, we have 
not found any significant differences among the 
experimental conditions. 
6 Conclusions and Future Work 
Argumentation theory indicates that effective 
arguments should be concise, presenting only 
pertinent and cogent information. However, 
argumentation theory does not tell us what is the 
most effective degree of conciseness. As a 
preliminary attempt to answer this question for 
evaluative arguments, we have compared in a 
formal experiment the effectiveness of 
arguments generated by our argument generator 
at two different levels of conciseness. The 
experiment results show that arguments 
generated at the more concise level are 
significantly better than arguments generated at 
the more verbose level. However, further 
experiments are needed to determine what is the 
optimal level of conciseness.  
Acknowledgements 
Our thanks go to the members of the Autobrief 
project: S. Roth, N. Green, S. Kerpedjiev and J. 
Mattis. We also thank C. Conati for comments 
on drafts of this paper. This work was supported 
by grant number DAA-1593K0005 from the 
Advanced Research Projects Agency (ARPA).  
 
References 
Cacioppo, J. T., R. E. Petty, et al (1983). ?Effects of 
Need for Cognition on Message Evaluation, Recall, 
and Persuasion.? Journal of Personality and Social 
Psychology 45(4): 805-818. 
Carenini, G. and J. Moore (2000). A Strategy for 
Generating Evaluative Arguments. International 
Conference on Natural Language Generation, 
Mitzpe Ramon, Israel. 
Clemen, R. T. (1996). Making Hard Decisions: an 
introduction to decision analysis. Belmont, 
California, Duxbury Press. 
Dale, R., B. d. Eugenio, et al (1998). ?Introduction to 
the Special Issue on Natural Language 
Generation.? Computational Linguistics 24(3): 
345-353. 
Edwards, W. and F. H. Barron (1994). ?SMARTS 
and SMARTER: Improved Simple Methods for 
Multi-attribute Utility Measurements.? 
Organizational Behavior and Human Decision 
Processes 60: 306-325. 
Elhadad, M. (1995). ?Using argumentation in text 
generation.? Journal of Pragmatics 24: 189-220. 
Infante, D. A. and A. S. Rancer (1982). ?A 
Conceptualization and Measure of 
Argumentativeness.? Journal of Personality 
Assessment 46: 72-80. 
Klein, D. (1994). Decision Analytic Intelligent 
Systems: Automated Explanation and Knowledge   
Acquisition, Lawrence Erlbaum Associates. 
Lester, J. C. and B. W. Porter (March 1997). 
?Developing and Empirically Evaluating Robust 
Explanation Generators: The KNIGHT 
Experiments.? Computational Linguistics 23(1): 
65-101. 
Mayberry, K. J. and R. E. Golden (1996). For 
Argument's Sake: A Guide to Writing Effective 
Arguments, Harper Collins, College Publisher. 
McConachy, R., K. B. Korb, et al (1998). Deciding 
What Not to Say: An Attentional-Probabilistic 
Approach to Argument Presentation. Cognitive 
Science Conference. 
Olso, J. M. and M. P. Zanna (1991). Attitudes and 
beliefs ; Attitude change and attitude-behavior 
consistency. Social Psychology. R. M. Baron and 
W. G. Graziano. 
Robin, J. and K. McKeown (1996). ?Empirically 
Designing and Evaluating a New Revision-Based 
Model for Summary Generation.? Artificial 
Intelligence journal 85: 135-179. 
Roth, S. F., M. C. Chuah, et al (1997). Towards an 
Information Visualization Workspace: Combining 
Multiple Means of   Expression. Human-Computer 
Interaction Journal. 
Young, M. R. ?Using Grice's Maxim of Quantity to 
Select the Content of Plan Descriptions.? Artificial 
Intelligence Journal, to appear. 
Tailored
Concise
Tailored
Verbose
No-Argument
>
?
 0.88
 0.05
 0.25
 0.02
>>
 0.03
 0.31
 
Proceedings of the 43rd Annual Meeting of the ACL, pages 239?246,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Implications for Generating Clarification Requests in Task-oriented
Dialogues
Verena Rieser
Department of Computational Linguistics
Saarland University
Saarbru?cken, D-66041
vrieser@coli.uni-sb.de
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
J.Moore@ed.ac.uk
Abstract
Clarification requests (CRs) in conversa-
tion ensure and maintain mutual under-
standing and thus play a crucial role in
robust dialogue interaction. In this pa-
per, we describe a corpus study of CRs
in task-oriented dialogue and compare our
findings to those reported in two prior
studies. We find that CR behavior in
task-oriented dialogue differs significantly
from that in everyday conversation in a
number of ways. Moreover, the dialogue
type, the modality and the channel qual-
ity all influence the decision of when to
clarify and at which level of the ground-
ing process. Finally we identify form-
function correlations which can inform the
generation of CRs.
1 Introduction
Clarification requests in conversation ensure and
maintain mutual understanding and thus play a sig-
nificant role in robust and efficient dialogue interac-
tion. From a theoretical perspective, the model of
grounding explains how mutual understanding is es-
tablished. According to Clark (1996), speakers and
listeners ground mutual understanding on four lev-
els of coordination in an action ladder, as shown in
Table 1.
Several current research dialogue systems can de-
tect errors on different levels of grounding (Paek
and Horvitz, 2000; Larsson, 2002; Purver, 2004;
Level Speaker S Listener L
Convers. S is proposing activity
?
L is considering pro-
posal ?
Intention S is signalling that p L is recognizing that p
Signal S is presenting signal ? L is identifying signal
?
Channel S is executing behavior
?
L is attending to behav-
ior ?
Table 1: Four levels of grounding
Schlangen, 2004). However, only the work of
Purver (2004) addresses the question of how the
source of the error affects the form the CR takes.
In this paper, we investigate the use of form-
function mappings derived from human-human di-
alogues to inform the generation of CRs. We iden-
tify the factors that determine which function a CR
should take and identify function-form correlations
that can be used to guide the automatic generation
of CRs.
In Section 2, we discuss the classification
schemes used in two recent corpus studies of CRs
in human-human dialogue, and assess their applica-
bility to the problem of generating CRs. Section 3
describes the results we obtained by applying the
classification scheme of Rodriguez and Schlangen
(2004) to the Communicator Corpus (Bennett and
Rudnicky, 2002). Section 4 draws general conclu-
sions for generating CRs by comparing our results
to those of (Purver et al, 2003) and (Rodriguez and
Schlangen, 2004). Section 5 describes the correla-
tions between function and form features that are
present in the corpus and their implications for gen-
erating CRs.
239
Attr. Value Category Example
form non Non-Reprise ?What did you say??
wot Conventional ?Sorry??
frg Reprise Fragment ?Edinburgh??
lit Literal Reprise ?You want a flight to Edinburgh??
slu Reprise Sluice ?Where??
sub Wh-substituted Reprise ?You want a flight where??
gap Gap ?You want a flight to...??
fil Gap Filler ?...Edinburgh??
other Other x
readings cla Clausal ?Are you asking/asserting that X??
con Constituent ?What do you mean by X??
lex Lexical ?Did you utter X??
corr Correction ?Did you intend to utter X instead??
other Other x
Table 2: CR classification scheme by PGH
2 CR Classification Schemes
We now discuss two recently proposed classifica-
tion schemes for CRs, and assess their usefulness for
generating CRs in a spoken dialogue system (SDS).
2.1 Purver, Ginzburg and Healey (PGH)
Purver, Ginzburg and Healey (2003) investigated
CRs in the British National Corpus (BNC) (Burnard,
2000). In their annotation scheme, a CR can take
seven distinct surface forms and four readings, as
shown in Table 2. The examples for the form feature
are possible CRs following the statement ?I want a
flight to Edinburgh?. The focus of this classification
scheme is to map semantic readings to syntactic sur-
face forms. The form feature is defined by its rela-
tion to the problematic utterance, i.e., whether a CR
reprises the antecedent utterance and to what extent.
CRs may take the three different readings as defined
by Ginzburg and Cooper (2001), as well as a fourth
reading which indicates a correction.
Although PGH report good coverage of the
scheme on their subcorpus of the BNC (99%), we
found their classification scheme to to be too coarse-
grained to prescribe the form that a CR should take.
As shown in example 1, Reprise Fragments (RFs),
which make up one third of the BNC, are ambigu-
ous in their readings and may also take several sur-
face forms.
(1) I would like to book a flight on Monday.
(a) Monday?
frg, con/cla
(b) Which Monday?
frg, con
(c) Monday the first?
frg, con
(d) The first of May?
frg, con
(e) Monday the first or Monday the eighth?
frg, (exclusive) con
RFs endorse literal repetitions of part of the prob-
lematic utterance (1.a); repetitions with an addi-
tional question word (1.b); repetition with further
specification (1.c); reformulations (1.d); and alter-
native questions (1.e)1.
In addition to being too general to describe such
differences, the classification scheme also fails to
describe similarities. As noted by (Rodriguez and
Schlangen, 2004), PGH provide no feature to de-
scribe the extent to which an RF repeats the prob-
lematic utterance.
Finally, some phenomena cannot be described at
all by the four readings. For example, the readings
do not account for non-understanding on the prag-
matic level. Furthermore the readings may have sev-
eral problem sources: the clausal reading may be
appropriate where the CR initiator failed to recog-
nise the word acoustically as well as when he failed
to resolve the reference. Since we are interested in
generating CRs that indicate the source of the error,
we need a classification scheme that represents such
information.
2.2 Rodriguez and Schlangen (R&S)
Rodriguez and Schlangen (2004) devised a multi-
dimensional classification scheme where form and
1Alternative questions would be interpreted as asking a polar
question with an exclusive reading.
240
function are meta-features taking sub-features as at-
tributes. The function feature breaks down into
the sub-features source, severity, extent, reply and
satisfaction. The sources that might have caused
the problem map to the levels as defined by Clark
(1996). These sources can also be of different
severity. The severity can be interpreted as de-
scribing the set of possible referents: asking for
repetition indicates that no interpretation is avail-
able (cont-rep); asking for confirmation means
that the CR initiator has some kind of hypothesis
(cont-conf). The extent of a problem describes
whether the CR points out a problematic element in
the problem utterance. The reply represents the an-
swer the addressee gives to the CR. The satisfaction
of the CR-initiator is indicated by whether he renews
the request for clarification or not.
The meta-feature form describes how the CR is
lingustically realised. It describes the sentence?s
mood, whether it is grammatically complete, the re-
lation to the antecedent, and the boundary tone. Ac-
cording to R&S?s classification scheme our illustra-
tive example would be annotated as follows2:
(2) I would like to book a flight on Monday.
(a) Monday?
mood: decl
completeness: partial
rel-antecedent: repet
source: acous/np-ref
severity: cont-repet
extent: yes
(b) Which Monday?
mood: wh-question
completeness: partial
rel-antecedent: addition
source: np-ref
severity: cont-repet
extent: yes
(c) Monday the first?
mood: decl
completeness: partial
rel-antecedent: addition
source: np-ref
severity: cont-conf
extent: yes
(d) The first of May?
mood: decl
completeness: partial
2The source features answer and satisfaction are ignored as
they depend on how the dialogue continues. The interpretation
of the source is dependent on the reply to the CR. Therefore all
possible interpretations are listed.
rel-antecedent: reformul
source: np-ref
severity: cont-conf
extent: yes
(d) Monday the first or Monday the eighth?
mood: alt-q
completeness: partial
rel-antecedent: addition
source: np-ref
severity: cont-repet
extent: yes
In R&S?s classification scheme, ambiguities
about CRs having different sources cannot be re-
solved entirely as example (2.a) shows. However,
in contrast to PGH, the overall approach is a differ-
ent one: instead of explaining causes of CRs within
a theoretic-semantic model (as the three different
readings of Ginzburg and Cooper (2001) do), they
infer the interpretation of the CR from the context.
Ambiguities get resolved by the reply of the ad-
dressee and the satisfaction of the CR initiator in-
dicates the ?mutually agreed interpretation? .
R&S?s multi-dimensional CR description allows
the fine-grained distinctions needed to generate nat-
ural CRs to be made. For example, PGH?s general
category of RFs can be made more specific via the
values for the feature relation to antecedent. In ad-
dition, the form feature is not restricted to syntax; it
includes features such as intonation and coherence,
which are useful for generating the surface form of
CRs. Furthermore, the multi-dimensional function
feature allows us to describe information relevant to
generating CRs that is typically available in dialogue
systems, such as the level of confidence in the hy-
pothesis and the problem source.
3 CRs in the Communicator Corpus
3.1 Material and Method
Material: We annotated the human-human travel
reservation dialogues available as part of the
Carnegie Mellon Communicator Corpus (Bennett
and Rudnicky, 2002) because we were interested
in studying naturally occurring CRs in task-oriented
dialogue. In these dialogues, an experienced travel
agent is making reservations for trips that people in
the Carnegie Mellon Speech Group were taking in
the upcoming months. The corpus comprises 31 di-
alogues of transcribed telephone speech, with 2098
dialogue turns and 19395 words.
241
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
form:
?
?
?
?
?
?
?
distance-src:
{
1 | 2 | 3 | 4 | 5 | more
}
mood:
{
none | decl | polar-q | wh-q | alt-q | imp | other
}
form:
{
none | particle | partial | complete
}
relation-antecedent:
{
none | add | repet | repet-add | reformul | indep
}
boundary-tone:
{
none | rising | falling | no-appl
}
?
?
?
?
?
?
?
function:
?
?
?
?
?
?
?
?
?
source:
{
none | acous | lex | parsing | np-ref | deitic-ref | act-ref |
int+eval | relevance | belief | ambiguity | scr-several
}
extent:
{
none | fragment | whole
}
severity:
{
none | cont-conf | cont-rep | cont-disamb | no-react
}
answer:
{
none | ans-repet | ans-y/n | ans-reformul | ans-elab |
ans-w-defin | no-react
}
satisfaction:
{
none | happy-yes | happy-no | happy-ambig
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: CR classification scheme
Annotation Scheme: Our annotation scheme,
shown in Figure 1, is an extention of the R&S
scheme described in the previous section. R&S?s
scheme was devised for and tested on the Bielefeld
Corpus of German task-oriented dialogues about
joint problem solving.3 To annotate the Commu-
nicator Corpus we extended the scheme in the fol-
lowing ways. First, we found the need to distin-
guish CRs that consist only of newly added infor-
mation, as in example 3, from those that add in-
formation while also repeating part of the utterance
to be clarified, as in 4. We augmented the scheme
to allow two distinct values for the form feature
relation-antecedent, add for cases like 3
and repet-add for cases like 4.
(3) Cust: What is the last flight I could come back on?
Agent: On the 29th of March?
(4) Cust: I?ll be returning on Thursday the fifth.
Agent: The fifth of February?
To the function feature source we added the val-
ues belief to cover CRs like 5 and ambiguity
refinement to cover CRs like 6.
(5) Agent: You need a visa.
Cust: I do need one?
Agent: Yes you do.
(6) Agent: Okay I have two options . . . with Hertz . . . if not
they do have a lower rate with Budget and that is
fifty one dollars.
Cust: Per day?
Agent: Per day um mm.
Finally, following Gabsdil (2003) we introduced
an additional value for severity, cont-disamb, to
3http://sfb360.uni-bielefeld.de
cover CRs that request disambiguation when more
than one interpretation is available.
Method: We first identified turns containing CRs,
and then annotated them with form and function fea-
tures. It is not always possible to identify CRs from
the utterance alone. Frequently, context (e.g., the
reaction of the addressee) or intonation is required
to distinguish a CR from other feedback strategies,
such as positive feedback. See (Rieser, 2004) for a
detailed discussion. The annotation was only per-
formed once. The coding scheme is a slight varia-
tion of R&S, which has been shown relaiable with
Kappa of 0.7 for identifying source.
3.2 Forms and Functions of CRs in the
Communicator Corpus
The human-human dialogues in the Communica-
tor Corpus contain 98 CRs in 2098 dialogue turns
(4.6%).
Forms: The frequencies for the values of the
individual form features are shown in Table 3.
The most frequent type of CRs were partial
declarative questions, which combine the mood
value declarative and the completeness value
partial.4 These account for 53.1% of the CRs
in the corpus. Moreover, four of the five most
frequent surface forms of CRs in the Communi-
cator Corpus differ only in the value for the fea-
ture relation-antecedent. They are partial
declaratives with rising boundary tone, that either re-
formulate (7.1%) the problematic utterance, repeat
4Declarative questions cover ?all cases of non-interrogative
word-order, i.e., both declarative sentences and fragments? (Ro-
driguez and Schlangen, 2004).
242
Feature Value Freq. (%)
Mood declarative 65
polar 21
wh-question 7
other 7
Completeness partial 58
complete 38
other 4
Relation antecedent rep-add 27
independent 21
reformulation 19
repetition 18
addition 10
other 5
Boundary tone rising 74
falling 22
other 4
Table 3: Distribution of values for the form features
the problematic constituent (11.2%), add only new
information (7.1%), or repeat the problematic con-
stituent and add new information (10.2%). The fifth
most frequent type is conventional CRs (10.2%).5
Functions: The distributions of the function fea-
tures are given in Figure 4. The most frequent source
of problems was np-reference. Next most frequent
were acoustic problems, possibly due to the poor
channel quality. Third were CRs that enquire about
intention. As indicated by the feature extent, al-
most 80% of CRs point out a specific element of
the problematic utterance. The features severity and
answer illustrate that most of the time CRs request
confirmation of an hypothesis (73.5%) with a yes-
no-answer (64.3%). The majority of the provided
answers were satisfying, which means that the ad-
dressee tends to interpret the CR correctly and an-
swers collaboratively. Only 6.1% of CRs failed to
elicit a response.
4 CRs in Task-oriented Dialogue
4.1 Comparison
In order to determine whether there are differences
as regards CRs between task-oriented dialogues and
everyday conversations, we compared our results to
those of PGH?s study on the BNC and those of R&S
5Conventional forms are ?Excuse me??, ?Pardon??, etc.
Feature Value Freq. (%)
Source np-reference 40
acoustic 31
intention 8
belief 6
ambiguity 4
contact 4
others 3
relevance 2
several 2
Extent yes 80
no 20
Severity confirmation 73
repetition 20
other 7
Answer y/n answer 64
other 15
elaboration 13
no reaction 6
Table 4: Distribution of values for the function fea-
tures
on the Bielefeld Corpus. The BNC contains a 10
million word sub-corpus of English dialogue tran-
scriptions about topics of general interest. PGH
analysed a portion consisting of ca. 10,600 turns,
ca. 150,000 words. R&S annotated 22 dialogues
from the Bielefeld Corpus, consisting of ca. 3962
turns, ca. 36,000 words.
The major differences in the feature distributions
are listed in Table 5. We found that there are no
significant differences between the feature distri-
butions for the Communicator and Bielefeld cor-
pora, but that the differences between Communica-
tor and BNC, and Bielefeld and BNC are significant
at the levels indicated in Table 5 using Pearson?s
?2. The differences between dialogues of differ-
ent types suggest that there is a different grounding
strategy. In task-oriented dialogues we see a trade-
off between avoiding misunderstanding and keeping
the conversation as efficient as possible. The hy-
pothesis that grounding in task-oriented dialogues is
more cautious is supported by the following facts (as
shown by the figures in Table 5):
? CRs are more frequent in task-oriented dia-
logues.
? The overwhelming majority of CRs directly
follow the problematic utterance.
243
Corpus
Feature Communicator Bielefeld BNC
CRs 98 230 418
frequency 4.6% 5.8%*** 3.9%
distance-src=1 92.8%* 94.8%*** 84.4%
no-react 6.1%* 8.7%** 17.0%
cont-conf 73.5%*** 61.7%*** 46.6%
partial 58.2%** 76.5%*** 42.4%
independent 21.4%*** 9.6%*** 44.2%
cont-rep 19.8%*** 14.8%*** 39.5%
y/n-answer 64.3% 44.8% n/a
Table 5: Comparison of CR forms in everyday vs. task-
oriented corpora (* denotes p < .05, ** is p < .01, *** is
p < .005.)
? CRs in everyday conversation fail to elicit a re-
sponse nearly three times as often.6
? Even though dialogue participants seem to
have strong hypotheses, they frequently con-
firm them.
Although grounding is more cautious in task-
oriented dialogues, the dialogue participants try to
keep the dialogue as efficient as possible:
? Most CRs are partial in form.
? Most of the CRs point out one specific element
(with only a minority being independent as
shown in Table 5). Therefore, in task-oriented
dialogues, CRs locate the understanding prob-
lem directly and give partial credit for what was
understood.
? In task-oriented dialogues, the CR-initiator
asks to confirm an hypothesis about what he
understood rather than asking the other dia-
logue participant to repeat her utterance.
? The addressee prefers to give a short y/n answer
in most cases.
Comparing error sources in the two task-oriented
corpora, we found a number of differences as shown
in Table 6. In particular:
6Another factor that might account for these differences is
that the BNC contains multi-party conversations, and questions
in multi-party conversations may be less likely to receive re-
sponses. Furthermore, due to the poor recording quality of the
BNC, many utterances are marked as ?not interpretable?, which
could also lower the response rate.
Corpus
Feature Communicator Bielefeld Significance
contact 4.1% 0 inst n/a
acoustic 30.6% 11.7% ***
lexical 1 inst 1 inst n/a
parsing 1 inst 0 inst n/a
np-ref 39.8% 24.4% **
deict-ref 1 inst 27.4% ***
ambiguity 4.1% not eval. n/a
belief 6.1% not eval. n/a
relevance 2.1% not eval. n/a
intention 8.2% 22.2% **
several 2.0% 14.3% ***
Table 6: Comparison of CR problem sources in task-oriented
corpora
? Dialogue type: Belief and ambiguity refine-
ment do not seem to be a source of problems
in joint problem solving dialogues, as R&S did
not include them in their annotation scheme.
For CRs in information seeking these features
need to be added to explain quite frequent phe-
nomena. As shown in Table 6, 10.2% of CRs
were in one of these two classes.
? Modality: Deictic reference resolution causes
many more understanding difficulties in dia-
logues where people have a shared point of
view than in telephone communication (Biele-
feld: most frequent problem source; Communi-
cator: one instance detected). Furthermore, in
the Bielefeld Corpus, people tend to formulate
more fragmentary sentences. In environments
where people have a shared point of view, com-
plete sentences can be avoided by using non-
verbal communication channels. Finally, we
see that establishing contact is more of a prob-
lem when speech is the only modality available.
? Channel quality: Acoustic problems are much
more likely in the Communicator Corpus.
These results indicate that the decision process for
grounding needs to consider the modality, the do-
main, and the communication channel. Similar ex-
tensions to the grounding model are suggested by
(Traum, 1999).
244
4.2 Consequences for Generation
The similarities and differences detected can be
used to give recommendations for generating CRs.
In terms of when to initiate a CR, we can state
that clarification should not be postponed, and im-
mediate, local management of uncertainty is criti-
cal. This view is also supported by observations of
how non-native speakers handle non-understanding
(Paek, 2003).
Furthermore, for task-oriented dialogues the sys-
tem should present an hypothesis to be confirmed,
rather than ask for repetition. Our data suggests that,
when they are confronted with uncertainty, humans
tend to build up hypotheses from the dialogue his-
tory and from their world knowledge. For example,
when the customer specified a date without a month,
the travel agent would propose the most reasonable
hypothesis instead of asking a wh-question. It is in-
teresting to note that Skantze (2003) found that users
are more satisfied if the system ?hides? its recog-
nition problem by asking a task-related question to
help to confirm the hypothesis, rather than explicitly
indicating non-understanding.
5 Correlations between Function and
Form: How to say it?
Once the dialogue system has decided on the func-
tion features, it must find a corresponding surface
form to be generated. Many forms are indeed re-
lated to the function as shown in Table 7, where we
present a significance analysis using Pearson?s ?2
(with Yates correction).
Source: We found that the relation to the an-
tecedent seems to distinguish fairly reliably be-
tween CRs clarifying reference and those clarify-
ing acoustic understanding. In the Communicator
Corpus, for acoustic problems the CR-initiator tends
to repeat the problematic part literally, while refer-
ence problems trigger a reformulation or a repeti-
tion with addition. For both problem sources, par-
tial declarative questions are preferred. These find-
ings are also supported by R&S. For the first level
of non-understanding, the inability to establish con-
tact, complete polar questions with no relation to the
antecedent are formulated, e.g., ?Are you there??.
Severity: The severity indicates how much was
understood, i.e., whether the CR initiator asks to
confirm an hypothesis or to repeat the antecedent
utterance. The severity of an error strongly cor-
relates with the sentence mood. Declarative and
polar questions, which take up material from the
problematic utterance, ask to confirm an hypothe-
sis. Wh-questions, which are independent, refor-
mulations or repetitions with additions (e.g., wh-
substituted reprises) of the problematic utterance
usually prompt for repetition, as do imperatives. Al-
ternative questions prompt the addressee to disam-
biguate the hypothesis.
Answer: By definition, certain types of question
prompt for certain answers. Therefore, the feature
answer is closely linked to the sentence mood of
the CR. As polar questions and declarative ques-
tions generally enquire about a proposition, i.e., an
hypothesis or belief, they tend to receive yes/no
answers, but repetitions are also possible. Wh-
questions, alternative questions and imperatives tend
to get answers providing additional information (i.e.,
reformulations and elaborations).
Extent: The function feature extent is logically in-
dependent from the form feature completeness, al-
though they are strongly correlated. Extent is a bi-
nary feature indicating whether the CR points out
a specific element or concerns the whole utterance.
Most fragmentary declarative questions and frag-
mentary polar questions point out a specific element,
especially when they are not independent but stand
in some relation to the antecedent utterance. In-
dependent complete imperatives address the whole
previous utterance.
The correlations found in the Communicator Cor-
pus are fairly consistent with those found in the
Bielefeld Corpus, and thus we believe that the guide-
lines for generating CRs in task-oriented dialogues
may be language independent, at least for German
and English.
6 Summary and Future Work
In this paper we presented the results of a corpus
study of naturally occurring CRs in task-oriented di-
alogue. Comparing our results to two other stud-
ies, one of a task-oriented corpus and one of a cor-
245
Function
Form source severity extent answer
mood ?
2(24) = 112.20
p < 0.001
?2(5) = 30.34
p < 0.001
?2(5) = 24.25
df = p < 0.005
?2(5) = 25.19
p < 0.001
bound-tone indep. indep. indep. indep.
rel-antec ?
2(24) = 108.23
p < 0.001
?2(4) = 11.69
p < 0.005
?2(4) = 42.58
p < 0.001
indep.
complete ?
2(7) = 27.39
p < 0.005
indep. ?
2(1) = 27.39
p < 0.001
indep.
Table 7: Significance analysis for form/function correlations.
pus of everyday conversation, we found no signif-
icant differences in frequency of CRs and distribu-
tion of forms in the two task-oriented corpora, but
many significant differences between CRs in task-
oriented dialogue and everyday conversation. Our
findings suggest that in task-oriented dialogues, hu-
mans use a cautious, but efficient strategy for clar-
ification, preferring to present an hypothesis rather
than ask the user to repeat or rephrase the problem-
atic utterance. We also identified correlations be-
tween function and form features that can serve as
a basis for generating more natural sounding CRs,
which indicate a specific problem with understand-
ing. In current work, we are studying data collected
in a wizard-of-oz study in a multi-modal setting, in
order to study clarification behavior in multi-modal
dialogue.
Acknowledgements
The authors would like thank Kepa Rodriguez, Oliver Lemon,
and David Reitter for help and discussion.
References
Christina L. Bennett and Alexander I. Rudnicky. 2002.
The Carnegie Mellon Communicator Corpus. In Pro-
ceedings of the International Conference of Spoken
Language Processing (ICSLP02).
Lou Burnard. 2000. The British National Corpus Users
Reference Guide. Technical report, Oxford Universiry
Computing Services.
Herbert Clark. 1996. Using Language. Cambridge Uni-
versity Press.
Malte Gabsdil. 2003. Clarification in Spoken Dialogue
Systems. Proceedings of the 2003 AAAI Spring Sym-
posium. Workshop on Natural Language Generation in
Spoken and Written Dialogue.
Jonathan Ginzburg and Robin Cooper. 2001. Resolving
Ellipsis in Clarification. In Proceedings of the 39th
meeting of the Association for Computational Linguis-
tics.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, Goteborg University.
Tim Paek and Eric Horvitz. 2000. Conversation as Ac-
tion Under Uncertainty. In Proceedings of the Six-
teenth Conference on Uncertainty in Artificial Intelli-
gence.
Tim Paek. 2003. Toward a Taxonomy of Communica-
tion Errors. In ISCA Tutorial and Research Workshop
on Error Handling in Spoken Dialogue Systems.
Matthew Purver, Jonathan Ginzburg, and Patrick Healey.
2003. On the Means for Clarification in Dialogue. In
R. Smith and J. van Kuppevelt, editors, Current and
New Directions in Discourse and Dialogue.
Matthew Purver. 2004. CLARIE: The Clarification En-
gine. In Proceedings of the Eighth Workshop on For-
mal Semantics and Dialogue.
Verena Rieser. 2004. Fragmentary Clarifications on Sev-
eral Levels for Robust Dialogue Systems. Master?s
thesis, School of Informatics, University of Edinburgh.
Kepa J. Rodriguez and David Schlangen. 2004. Form,
Intonation and Function of Clarification Requests in
German Task-orientaded Spoken Dialogues. In Pro-
ceedings of the Eighth Workshop on Formal Semantics
and Dialogue.
David Schlangen. 2004. Causes and Strategies for Re-
question Clarification in Dialogue. Proceedings of the
5th SIGdial Workshop on Discourse and Dialogue.
Gabriel Skantze. 2003. Exploring Human Error Han-
dling Strategies: Implications for Spoken Dialogue
Systems. In ISCA Tutorial and Research Workshop
on Error Handling in Spoken Dialogue Systems.
David R. Traum. 1999. Computational Models of
Grounding in Collaborative Systems. In Proceedings
of the AAAI Fall Symposium on Psychological Models
of Communication.
246
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 857?864,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Event Extraction in a Plot Advice Agent
Harry Halpin
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
H.Halpin@ed.ac.uk
Johanna D. Moore
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
J.Moore@ed.ac.uk
Abstract
In this paper we present how the auto-
matic extraction of events from text can
be used to both classify narrative texts ac-
cording to plot quality and produce advice
in an interactive learning environment in-
tended to help students with story writing.
We focus on the story rewriting task, in
which an exemplar story is read to the stu-
dents and the students rewrite the story in
their own words. The system automati-
cally extracts events from the raw text, for-
malized as a sequence of temporally or-
dered predicate-arguments. These events
are given to a machine-learner that pro-
duces a coarse-grained rating of the story.
The results of the machine-learner and the
extracted events are then used to generate
fine-grained advice for the students.
1 Introduction
In this paper we investigate how features of a text
discovered via automatic event extraction can be
used in both natural language understanding and
advice generation in the domain of narrative in-
struction. The background application is a fully
automated plot analysis agent to improve the writ-
ing of students could be used by current nar-
rative tutoring systems (Robertson and Wiemer-
Hastings, 2002). As shown by participatory de-
sign studies, teachers are interested in a plot anal-
ysis agent that can give online natural language
advice and many students enjoy feedback from an
automated agent (Robertson and Cross, 2003). We
use automatic event extraction to create a story-
independent automated agent that can both ana-
lyze the plot of a story and generate appropriate
advice.
1.1 The Story Rewriting Task
A task used in schools is the story rewriting task,
where a story, the exemplar story, is read to the
students, and afterwards the story is rewritten by
each student, providing a corpus of rewritten sto-
ries. This task tests the students ability to both
listen and write, while removing from the student
the cognitive load needed to generate a new plot.
This task is reminiscent of the well-known ?War
of the Ghosts? experiment used in psychology for
studying memory (Bartlett, 1932) and related to
work in fields such as summarization (Lemaire et
al., 2005) and narration (Halpin et al, 2004).
1.2 Agent Design
The goal of the agent is to classify each of the
rewritten stories for overall plot quality. This
rating can be used to give ?coarse-grained? gen-
eral advice. The agent should then provide ?fine-
grained? specific advice to the student on how their
plot could be improved. The agent should be able
to detect if the story should be re-read or a human
teacher summoned to help the student.
To accomplish this task, we extract events that
represent the entities and their actions in the plot
from both the exemplar and the rewritten stories.
A plot comparison algorithm checks for the pres-
ence or absence of events from the exemplar story
in each rewritten story. The results of this algo-
rithm will be used by a machine-learner to clas-
sify each story for overall plot quality and provide
general ?canned? advice to the student. The fea-
tures statistically shared by ?excellent? stories rep-
resent the important events of the exemplar story.
The results of a search for these important events
in a rewritten story provides the input needed by
templates to generate specific advice for a student.
857
2 Corpus
In order to train our agent, we collected a corpus
of 290 stories from primary schools based on two
different exemplar stories. The first is an episode
of ?The Wonderful Adventures of Nils? by Selma
Lagerloff (160 stories) and the second a re-telling
of ?The Treasure Thief? by Herodotus (130 sto-
ries). These will be referred to as the ?Adventure?
and ?Thief? corpora.
2.1 Rating
An experienced teacher, Rater A, designed a rating
scheme equivalent to those used in schools. The
scheme rates the stories as follows:
1. Excellent: An excellent story shows that
the student has ?read beyond the lines? and
demonstrates a deep understanding of the
story, using inference to grasp points that
may not have been explicit in the story. The
student should be able to retrieve all the im-
portant links, and not all the details, but the
right details.
2. Good: A good story shows that the student
understood the story and has ?read between
the lines.? The student recalls the main events
and links in the plot. However, the student
shows no deep understanding of the plot and
does not make use of inference. This can of-
ten be detected by the student leaving out an
important link or emphasizing the wrong de-
tails.
3. Fair: A fair story shows that student has
listened to the story but not understood the
story, and so is only trying to repeat what they
have heard. This is shown by the fact that the
fair story is missing multiple important links
in the story, including a possibly vital part of
the story.
4. Poor: A poor story shows the student has had
trouble listening to the story. The poor story
is missing a substantial amount of the plot,
with characters left out and events confused.
The student has trouble connecting the parts
of the story.
To check the reliability of the rating scheme,
two other teachers (Rater B and Rater C) rated
subsets (82 and 68 respectively) of each of the cor-
pora. While their absolute agreement with Rater A
Class Adventure Thief
1 (Excellent) .231 .146
2 (Good) .300 .377
3 (Fair) .156 .292
4 (Poor) .313 .185
Table 1: Probability Distribution of Ratings
makes the task appear subjective (58% for B and
53% for C), their relative agreement was high, as
almost all disagreements were by one level in the
rating scheme. Therefore we use Cronbach?s ?
and ?b instead of Cohen?s or Fleiss? ? to take into
account the fact that our scale is ordinal. Between
Rater A and B there was a Cronbach?s ? statistic
of .90 and a Kendall?s ?b statistic of .74. Between
Rater B and C there was a Cronbach?s ? statis-
tic of .87 and Kendall?s ?b statistic of .67. These
statistics show the rating scheme to be reliable and
the distribution of plot ratings are given in Table 1.
2.2 Linguistic Issues
One challenge facing this task is the ungrammati-
cal and highly irregular text produced by the stu-
dents. Many stories consist of one long run-on
sentence. This leads a traditional parsing system
with a direct mapping from the parse tree to a se-
mantic representation to fail to achieve a parse on
35% percent of the stories, and as such could not
be used (Bos et al, 2004). The stories exhibit fre-
quent use of reported speech and the switching
from first-person to third-person within a single
sentence. Lastly, the use of incorrect spelling e.g.,
?stalk? for ?stork? appearing in multiple stories
in the corpus, the consistent usage of homonyms
such as ?there? for ?their,? and the invention of
words (?torlix?), all prove to be frequent.
3 Plot Analysis
To automatically rate student writing many tutor-
ing systems use Latent Semantic Analysis, a vari-
ation on the ?bag-of-words? technique that uses
dimensionality reduction (Graesser et al, 2000).
We hypothesize that better results can be achieved
using a ?representational? account that explicitly
represents each event in the plot. These semantic
relationships are important in stories, e.g., ?The
thief jumped on the donkey? being distinctly dif-
ferent from ?The donkey jumped on the thief.?
What characters participate in an action matter,
since ?The king stole the treasure? reveals a major
858
misunderstanding while ?The thief stole the trea-
sure? shows a correct interpretation by the student.
3.1 Stories as Events
We represent a story as a sequence of events,
p1...ph, represented as a list of predicate-
arguments, similar to the event calculus (Mueller,
2003). Our predicate-argument structure is a mini-
mal subset of first-order logic (no quantifiers), and
so is compatible with case-frame and dependency
representations. Every event has a predicate (func-
tion) p that has one or more arguments, n1...na.
In the tradition of Discourse Representation The-
ory (Kamp and Reyle, 1993), our current predi-
cate argument structure could be converted auto-
matically to first order logic by using a default
existential quantification over the predicates and
joining them conjunctively. Predicate names are
often verbs, while their arguments are usually, al-
though not exclusively, nouns or adjectives. When
describing a set of events in the story, a superscript
is used to keep the arguments in an event distinct,
as n25 is argument 2 in event 5. The same argument
name may appear in multiple events. The plot of
any given story is formalized as an event structure
composed of h events in a partial order, with the
partial order denoting their temporal order:
p1(n11, n21, ...na1), ...., ph(n2h, n4h...nch)
An example from the ?Thief? exemplar story is
?The Queen nagged the king to build a treasure
chamber. The king decided to have a treasure
chamber.? This can be represented by an event
structure as:
nag(king, queen)
build(chamber)
decide(king)
have(chamber)
Note due the ungrammatical corpus we cannot at
this time extract neo-Davidsonian events. A sen-
tence maps onto one, multiple, or no events. A
unique name and closed-world assumption is en-
forced, although for purposes of comparing event
we compare membership of argument and predi-
cate names in WordNet synsets in addition to exact
name matches (Fellbaum, 1998).
4 Extracting Events
Paralleling work in summarization, it is hypothe-
sized that the quality of a rewritten story can be
defined by the presence or absence of ?seman-
tic content units? that are crucial details of the
text that may have a variety of syntactic forms
(Nenkova and Passonneau, 2004). We further hy-
pothesize these can be found in chunks of the
text automatically identified by a chunker, and we
can represent these units as predicate-arguments in
our event structure. The event structure of each
story is automatically extracted using an XML-
based pipeline composed of NLP processing mod-
ules, and unlike other story systems, extract full
events instead of filling in a frame of a story script
(Riloff, 1999). Using the latest version of the
Language Technology Text Tokenization Toolkit
(Grover et al, 2000), words are tokenized and sen-
tence boundaries detected. Words are given part-
of-speech tags by a maximum entropy tagger from
the toolkit. We do not attempt to obtain a full parse
of the sentence due to the highly irregular nature
of the sentences. Pronouns are resolved using a
rule-based reimplementation of the CogNIAC al-
gorithm (Baldwin, 1997) and sentences are lem-
matized and chunked using the Cass Chunker (Ab-
ney, 1995). It was felt the chunking method would
be the only feasible way to retrieve portions of the
sentences that may contain complete ?semantic
content units? from the ungrammatical and irregu-
lar text. The application of a series of rules, mainly
mapping verbs to predicate names and nouns to
arguments, to the results of the chunker produces
events from chunks as described in our previous
work (McNeill et al, 2006). The accuracy of our
rule-set was developed by using the grammatical
exemplar stories as a testbed, and a blind judge
found they produced 68% interpretable or ?sen-
sible? events given the ungrammatical text. Stu-
dents usually use the present or past tense exclu-
sively throughout the story and events are usually
presented in order of occurrence. An inspection
of our corpus showed 3% of stories in our corpus
seemed to get the order of events wrong (Hick-
mann, 2003).
4.1 Comparing Stories
Since the student is rewriting the story using their
own words, a certain variance from the plot of the
exemplar story should be expected and even re-
warded. Extra statements that may be true, but
are not explicitly stated in the story, can be in-
ferred by the students. Statements that are true
but are not highly relevant to the course of the
859
plot can likewise be left out. Word similarity
must be taken into account, so that ?The king is
protecting his gold? can be recognized as ?The
pharaoh guarded the treasure.? Characters change
in context, as one character that is described as
the ?younger brother? is from the viewpoint of his
mother ?the younger son.? So, building a model
from the events of two stories and simply check-
ing equivalence can not be used for comparison,
since a wide variety of partial equivalence must be
taken into account.
Instead of using absolute measures of equiva-
lence based on model checking or measures based
on word distribution, we compare each story on
the basis of the presence or absence of events. This
approach takes advantage of WordNet to define
synonym matching and uses the relational struc-
ture of the events to allow partial matching of
predicate functions and arguments. The events
of the exemplar story are assumed to be correct,
and they are searched for in the rewritten story in
the order in which they occur in the exemplar. If
an event is matched (including using WordNet),
then in turn each of the arguments attempts to be
matched.
This algorithm is given more formally in Fig-
ure 1. The complete event structure from the ex-
emplar story, E, and the complete event structure
from the rewritten story R, with each individual
event predicate name labelled as e and r respec-
tively, and their arguments labelled as n in either
Ne and Nr. SYN(x) is the synset of the term x,
including hypernyms and hyponyms except upper
ontology ones. The results of the algorithm are
stored in binary vector F with index i. 1 denotes
an exact match or WordNet synset match, and 0 a
failure to find any match.
4.2 Results
As a baseline system LSA produces a similar-
ity score for each rewritten story by comparing it
to the exemplar, this score is used as a distance
metric for a k-Nearest Neighbor classifier (Deer-
wester et al, 1990). The parameters for LSA were
empirically determined to be a dimensionality of
200 over the semantic space given by the rec-
ommended reading list for American 6th graders
(Landauer and Dumais, 1997). These parameters
resulted in the LSA similarity score having a Pear-
son?s correlation of -.520 with Rater A. k was
found to be optimal at 9.
Algorithm 4.1: PLOTCOMPARE(E,R)
i? 0
f ? ?
for e ? E
do for r ? R
do
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if e = SYN(r)
then fi ? 1
else fi ? 0
for ne ? Ne
do
?
?
?
?
?
?
?
?
?
?
?
for nr ? Nr
do
?
?
?
?
?
if ne = SYN(nr)
then fi ? 1
else fi ? 0
i = i + 1
Figure 1: Plot Comparison Algorithm
Classifier Corpus Features % Correct
k-NN Adventure LSA 47.5
Naive Bayes Adventure PLOT 55.6
k-NN Thief LSA 41.2
Naive Bayes Thief PLOT 45.4
Table 2: Machine-Learning Results
The results of the plot comparison algorithm
were given as features to machine-learners, with
results produced using ten-fold cross-validation.
A Naive Bayes learner discovers the different sta-
tistical distributions of events for each rating. The
results for both the ?Adventure? and ?Thief? sto-
ries are displayed in Table 2. ?PLOT? means the
results of the Plot Comparison Algorithm were
used as features for the machine-learner while
?LSA? means the similarity scores for Latent Se-
mantic Analysis were used instead. Note that the
same machine-learner could not be used to judge
the effect of LSA and PLOT since LSA scores are
real numbers and PLOT a set of features encoded
as binary vectors.
The results do not seem remarkable at first
glance. However, recall that the human raters had
an average of 56% agreement on story ratings, and
in that light the Naive Bayes learner approaches
the performance of human raters. Surprisingly,
when the LSA score is used as a feature in addition
to the results of the plot comparison algorithm for
the Naive Bayes learners, there is no further im-
provement. This shows features given by the event
860
Class 1 2 3 4
1 (Excellent) 14 22 0 1
2 (Good) 5 36 0 7
3 (Fair) 3 20 0 2
4 (Poor) 0 11 0 39
Table 3: Naive Bayes Confusion Matrix: ?Ad-
venture?
Class Precision Recall
Excellent .64 .38
Good .40 .75
Fair .00 .00
Poor .80 .78
Table 4: Naive Bayes Results: ?Adventure?
structure better characterize plot structure than the
word distribution. Unlike previous work, the use
of both the plot comparison results and LSA did
not improve performance for Naive Bayes, so the
results of using Naive Bayes with both are not re-
ported (Halpin et al, 2004).
The results for the ?Adventure? corpus are in
general better than the results for the ?Thief? cor-
pus. However, this is due to the ?Thief? corpus
being smaller and having an infrequent number of
?Excellent? and ?Poor? stories, as shown in Table
1. In the ?Thief? corpus the learner simply col-
lapses most stories into ?Good,? resulting in very
poor performance. Another factor may be that the
?Thief? story was more complex than the ?Adven-
ture? story, featuring 9 characters over 5 scenes, as
opposed to the ?Adventure? corpus that featured 4
characters over 2 scenes.
For the ?Adventure? corpus, the Naive Bayes
classifier produces the best results, as detailed in
Table 4 and the confusion matrix in Figure 3. A
close inspection of the results shows that in the
?Adventure Corpus? the ?Poor? and ?Good? sto-
ries are classified in general fairly well by the
Naive Bayes learner, while some of the ?Excel-
lent? stories are classified as correctly. A signifi-
cant number of both ?Excellent? and most ?Fair?
stories are classified as ?Good.? The ?Fair? cate-
gory, due to its small size in the training corpus,
has disappeared. No ?Poor? stories are classified
as ?Excellent,? and no ?Excellent? stories are clas-
sified as ?Poor.? The increased difficulty in distin-
guishing ?Excellent? stories from ?Good? stories
is likely due to the use of inference by ?Excellent?
stories, which our system does not use. An inspec-
tion of the rating scale?s wording reveals the sim-
ilarity in wording between the ?Fair? and ?Good?
ratings. This may explain the lack of ?Fair? sto-
ries in the corpus and therefore the inability of
machine-learners to recognize them. As given by
a survey of five teachers experienced in using the
story rewriting task in schools, this level of perfor-
mance is not ideal but acceptable to teachers.
Our technique is also shown to be easily
portable over different domains where a teacher
can annotate around one hundred sample stories
using our scale, although performance seems to
suffer the more complex a story is. Since the Naive
Bayes classifier is fast (able to classify stories in
only a few seconds) and the entire algorithm from
training to advice generation (as detailed below)
is fully automatic once a small training corpus has
been produced, this technique can be used in real-
life tutoring systems and easily ported to other sto-
ries.
5 Automated Advice
The plot analysis agent is not meant to give the
students grades for their stories, but instead use
the automatic ratings as an intermediate step to
produce advice, like other hybrid tutoring systems
(Rose et al, 2002). The advice that the agent can
generate from the automatic rating classification
is limited to coarse-grained general advice. How-
ever, by inspecting the results of the plot com-
parison algorithm, our agent is capable of giving
detailed fine-grained specific advice from the re-
lationships of the events in the story. One tutor-
ing system resembling ours is the WRITE sys-
tem, but we differ from it by using event struc-
ture to represent the information in the system,
instead of using rhetorical features (Burstein et
al., 2003). In this regards it more closely resem-
bles the physics tutoring system WHY-ATLAS, al-
though we deal with narrative stories of a longer
length than physics essays. The WHY-ATLAS
physics tutor identifies missing information in the
explanations of students using theorem-proving
(Rose et al, 2002).
5.1 Advice Generation Algorithm
Different types of stories need different amounts
of advice. An ?Excellent? story needs less ad-
vice than a ?Good? story. One advice statement is
?general,? while the rest are specific. The system
861
produces a total of seven advice statements for a
?Poor? story, and two less statements for each rat-
ing level above ?Poor.?
With the aid of a teacher, a number of ?canned?
text statements offering general advice were cre-
ated for each rating class. These include state-
ments such as ?It?s very good! I only have a few
pointers? for a ?Good? story and ?Let?s get help
from the teacher? for ?Poor? story. The advice
generation begins by randomly selecting a state-
ment suitable for the rating of the story. Those
students whose stories are rated ?Poor? are asked
if they would like to re-read the story and ask a
teacher for help.
The generation of specific advice uses the re-
sults of the plot-comparison algorithm to produce
specific advice. A number of advice templates
were produced, and the results of the Advice Gen-
eration Algorithm fill in the needed values of the
template. The ? most frequent events in ?Excel-
lent? stories are called the Important Event Struc-
ture, which represents the ?important? events in
the story in temporal order. Empirical experiments
led us ? = 10 for the ?Adventure? story, but for
longer stories like the ?Thief? story a larger ?
would be appropriate. These events correspond to
the ones given the highest weights by the Naive
Bayes algorithm. For each event in the event struc-
ture of a rewritten story, a search for a match in
the important event structure is taken. If a pred-
icate name match is found in the important event
structure, the search continues to attempt to match
the arguments. If the event and the arguments do
not match, advice is generated using the structure
of the ?important? event that it cannot find in the
rewritten story.
This advice may use both the predicate name
and its arguments, such as ?Did the stork fly??
from fly(stork). If an argument is missing, the ad-
vice may be about only the argument(s), like ?Can
you tell me more about the stork?? If the event is
out of order, advice is given to the student to cor-
rect the order, as in ?I think something with the
stork happened earlier in the story.?
This algorithm is formalized in Figure 2, with
all variables being the same as in the Plot Anal-
ysis Algorithm, except that W is the Important
Event Structure composed of events w with the
set of arguments Nw. M is a binary vector used
to store the success of a match with index i. The
ADV function, given an event, generates one ad-
Algorithm
5.1: ADVICEGENERATE(W,R)
for w ?W
do
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M = ?
i = 0
for r ? R
do
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if w = r or SY N(r)
then mi = 1
else mi = 0
i = i + 1
for nw ? Nw
do
?
?
?
?
?
?
?
?
?
?
?
for nr ? Nr
do
?
?
?
?
?
?
?
?
?
if nw = SYN(nr) or nr
then mi ? 1
else mi ? 0
i = i + 1
ADV (w,M)
Figure 2: Advice Generation Algorithm
vice statement to be given to the student.
An element of randomization was used to gen-
erate a diversity of types of answers. An ad-
vice generation function (ADV ) takes an impor-
tant event (w) and its binary matching vector (M )
and generates an advice statement for w. Per im-
portant event this advice generation function is pa-
rameterized so that it has a 10% chance of deliver-
ing advice based on the entire event, 20% chance
of producing advice that dealt with temporal or-
der (these being parameters being found ideal af-
ter testing the algorithm), and otherwise produces
advice based on the arguments.
5.2 Advice Evaluation
The plot advice algorithm is run using a randomly
selected corpus of 20 stories, 5 from each plot rat-
ing level using the ?Adventure Corpus.? This pro-
duced matching advice for each story, for a total
of 80 advice statements.
5.3 Advice Rating
An advice rating scheme was developed to rate the
advice produced in consultation with a teacher.
1. Excellent: The advice was suitable for the
story, and helped the student gain insight into
the story.
2. Good: The advice was suitable for the story,
862
Rating % Given
Excellent 0
Good 35
Fair 60
Poor 5
Table 5: Advice Rating Results
and would help the student.
3. Fair: The advice was suitable, but should
have been phrased differently.
4. Poor: The advice really didn?t make sense
and would only confuse the student further.
Before testing the system on students, it was de-
cided to have teachers evaluate how well the ad-
vice given by the system corresponded to the ad-
vice they would give in response to a story. A
teacher read each story and the advice. They then
rated the advice using the advice rating scheme.
Each story was rated for its overall advice quality,
and then each advice statement was given com-
ments by the teacher, such that we could derive
how each individual piece of advice contributed
to the global rating. Some of the general ?coarse-
grained? advice was ?Good! You got all the main
parts of the story? for an ?Excellent? story, ?Let?s
make it even better!? for a ?Good? story, and
?Reading the story again with a teacher would be
help!? for a ?Poor? story. Sometimes the ad-
vice generation algorithm was remarkably accu-
rate. In one story the connection between a curse
being lifted by the possession of a coin by the
character Nils was left out by a student. The ad-
vice generation algorithm produced the following
useful advice statement: ?Tell me more about the
curse and Nils.? Occasionally an automatically ex-
tracted event that is difficult to interpret by a hu-
man or simply incorrectly is extracted. This in turn
can cause advice that does not make any sense
can be produced, such as ?Tell me more about a
spot??. Qualitative analysis showed that ?missing
important advice? to be the most significant prob-
lem, followed by ?nonsensical advice.?
5.4 Results
The results are given in Table 5. The majority of
the advice was rated overall as ?fair.? Only one
story was given ?poor? advice, and a few were
given ?good? advice. However, most advice rated
as ?good? was the advice generated by ?excel-
lent? stories, which generate less advice than other
types of stories. ?Poor? stories were given almost
entirely ?fair? advice, although once ?poor? ad-
vice was generated. In general, the teacher found
?coarse-grained? advice to be very useful, and was
very pleased that the agent could detect when the
student needed to re-read the story and when a stu-
dent did not need to write any more. In some cases
the specific advice was shown to help provide a
?crucial detail? and help ?elicit a fact.? The advice
was often ?repetitive? and ?badly phrased.? The
specific advice came under criticism for often not
?being directed enough? and for being ?too literal?
and not ?inferential enough.? The rater noticed
that ?The program can not differentiate between
an unfinished story...and one that is confused.? and
that ?Some why, where and how questions could
be used? in the advice.
6 Conclusion and Future Work
Since the task involved a fine-grained analysis of
the rewritten story, the use of events that take plot
structure into account made sense regardless of
its performance. The use of events as structured
features in a machine-learning classifier outper-
formed a classifier that relied on a unstructured
?bag-of-words? as features. The system achieved
close to human performance on rating the stories.
Since each of the events used as a feature in the
machine-learner corresponds to a particular event
in the story, the features are easily interpretable by
other components in the system and interpretable
by humans. This allows these events to be used
in a template-driven system to generate advice for
students based on the structure of their plot.
Extracting events from text is fraught with er-
ror, particularly in the ungrammatical and infor-
mal domain used in this experiment. This is often
a failure of our system to detect semantic content
units through either not including them in chunks
or only partially including a single unit in a chunk.
Chunking also has difficulty dealing with preposi-
tions, embedded speech, semantic role labels, and
complex sentences correctly. Improvement in our
ability to retrieve semantics would help both story
classification and advice generation.
Advice generation was impaired by the abil-
ity to produce directed questions from the events
using templates. This is because while our sys-
tem could detect important events and their or-
863
der, it could not make explicit their connection
through inference. Given the lack of a large-scale
open-source accessible ?common-sense? knowl-
edge base and the difficulty in extracting infer-
ential statements from raw text, further progress
using inference will be difficult. Progress in ei-
ther making it easier for a teacher to make explicit
the important inferences in the text or improved
methodology to learn inferential knowledge from
the text would allow further progress. Tantaliz-
ingly, this ability for a reader to use ?inference to
grasp points that may not have been explicit in the
story? is given as the hallmark of truly understand-
ing a story by teachers.
References
Steven Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax. In
Jennifer Cole, Georgia Green, and Jerry Morgan,
editors, Computational Linguistics and the Founda-
tions of Linguistic Theory, pages 145?164.
Breck Baldwin. 1997. CogNIAC : A High Precision
Pronoun Resolution Engine.
F.C. Bartlett. 1932. Remembering. Cambridge Uni-
versity Press, Cambridge.
Johan Bos, Stephen Clark, Mark Steedman, James Cur-
ran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING ?04). Geneva,
Switzerland.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the WRITE Stuff: Automatic Identification
of Discourse Structure in Student Essays. IEEE In-
telligent Systems, pages 32?39.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R Harshman. 1990. Indexing by Latent
Semantic Analysis. Journal of the American Society
For Information Science, (41):391?407.
Christine Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
A. Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings,
D. Harter, and N. Person. 2000. Using latent se-
mantic analysis to evaluate the contributions of stu-
dents in autotutor. Interactive Learning Environ-
ments, 8:149?169.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. LT TTT - A Flexible Tokenisa-
tion Tool. In Proceedings of the Second Language
Resources and Evaluation Conference.
Harry Halpin, Johanna Moore, and Judy Robertson.
2004. Automatic analysis of plot for story rewriting.
In In Proceedings of Empirical Methods in Natural
Language Processing, Barcelona, Spain.
Maya Hickmann. 2003. Children?s Discourse: per-
son, space and time across language. Cambridge
University Press, Cambridge, UK.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer Academic.
Thomas. Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The Latent Semantic Anal-
ysis theory of the acquisition, induction, and repre-
sentation of knowledge. Psychological Review.
B. Lemaire, S. Mandin, P. Dessus, and G. Denhire.
2005. Computational cognitive models of summa-
rization assessment skills. In In Proceedings of the
27th Annual Meeting of the Cognitive Science Soci-
ety, Stressa, Italy.
Fiona McNeill, Harry Halpin, Ewan Klein, and Alan
Bundy. 2006. Merging stories with shallow seman-
tics. In Proceedings of the Knowledge Representa-
tion and Reasoning for Language Processing Work-
shop at the European Association for Computational
Linguistics, Genoa, Italy.
Erik T. Mueller. 2003. Story understanding through
multi-representation model construction. In Graeme
Hirst and Sergei Nirenburg, editors, Text Meaning:
Proceedings of the HLT-NAACL 2003 Workshop,
pages 46?53, East Stroudsburg, PA. Association for
Computational Linguistics.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In In Proceedings of the Joint Con-
ference of the North American Association for Com-
putational Linguistics and Human Language Tech-
nologies. Boston, USA.
E. Riloff. 1999. Information extraction as a step-
ping stone toward story understanding. In Ash-
win Ram and Kenneth Moorman, editors, Computa-
tional Models of Reading and Understanding. MIT
Press.
Judy Robertson and Beth Cross. 2003. Children?s
perceptions about writing with their teacher and the
StoryStation learning environment. Narrative and
Interactive Learning Environments: Special Issue
of International Journal of Continuing Engineering
Education and Life-long Learning.
Judy Robertson and Peter Wiemer-Hastings. 2002.
Feedback on children?s stories via multiple interface
agents. In International Conference on Intelligent
Tutoring Systems, Biarritz, France.
C. Rose, D. Bhembe, A. Roque, S. Siler, R. Srivas-
tava, and K. VanLehn. 2002. A hybrid language
understanding approach for robust selection of tutor-
ing goals. In International Conference on Intelligent
Tutoring Systems, Biarritz, France.
864
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 808?815,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Predicting Success in Dialogue
David Reitter and Johanna D. Moore
dreitter | jmoore @ inf.ed.ac.uk
School of Informatics
University of Edinburgh
United Kingdom
Abstract
Task-solving in dialogue depends on the lin-
guistic alignment of the interlocutors, which
Pickering & Garrod (2004) have suggested
to be based on mechanistic repetition ef-
fects. In this paper, we seek confirmation
of this hypothesis by looking at repetition
in corpora, and whether repetition is cor-
related with task success. We show that
the relevant repetition tendency is based on
slow adaptation rather than short-term prim-
ing and demonstrate that lexical and syntac-
tic repetition is a reliable predictor of task
success given the first five minutes of a task-
oriented dialogue.
1 Introduction
While humans are remarkably efficient, flexible and
reliable communicators, we are far from perfect.
Our dialogues differ in how successfully informa-
tion is conveyed. In task-oriented dialogue, where
the interlocutors are communicating to solve a prob-
lem, task success is a crucial indicator of the success
of the communication.
An automatic measure of task success would be
useful for evaluating conversations among humans,
e.g., for evaluating agents in a call center. In human-
computer dialogues, predicting the task success after
just a first few turns of the conversation could avoid
disappointment: if the conversation isn?t going well,
a caller may be passed on to a human operator, or
the system may switch dialogue strategies. As a first
step, we focus on human-human dialogue, since cur-
rent spoken dialogue systems do not yet yield long,
syntactically complex conversations.
In this paper, we use syntactic and lexical features
to predict task success in an environment where we
assume no speaker model, no semantic information
and no information typical for a human-computer
dialogue system, e.g., ASR confidence. The fea-
tures we use are based on a psychological theory,
linking alignment between dialogue participants to
low-level syntactic priming. An examination of this
priming reveals differences between short-term and
long-term effects.
1.1 Repetition supports dialogue
In their Interactive Alignment Model (IAM), Pick-
ering and Garrod (2004) suggest that dialogue be-
tween humans is greatly aided by aligning repre-
sentations on several linguistic and conceptual lev-
els. This effect is assumed to be driven by a cas-
cade of linguistic priming effects, where interlocu-
tors tend to re-use lexical, syntactic and other lin-
guistic structures after their introduction. Such re-
use leads speakers to agree on a common situa-
tion model. Several studies have shown that speak-
ers copy their interlocutor?s syntax (Branigan et al,
1999). This effect is usually referred to as structural
(or: syntactic) priming. These persistence effects
are inter-related, as lexical repetition implies pref-
erences for syntactic choices, and syntactic choices
lead to preferred semantic interpretations. Without
demanding additional cognitive resources, the ef-
fects form a causal chain that will benefit the inter-
locutor?s purposes. Or, at the very least, it will be
easier for them to repeat linguistic choices than to
808
actively discuss their terminology and keep track of
each other?s current knowledge of the situation in
order to come to a mutual understanding.
1.2 Structural priming
The repetition effect at the center of this paper, prim-
ing, is defined as a tendency to repeat linguistic de-
cisions. Priming has been shown to affect language
production and, to a lesser extent, comprehension, at
different levels of linguistic analysis. This tendency
may show up in various ways, for instance in the
case of lexical priming as a shorter response time in
lexical decision making tasks, or as a preference for
one syntactic construction over an alternative one in
syntactic priming (Bock, 1986). In an experimental
study (Branigan et al, 1999), subjects were primed
by completing either sentence (1a) or (1b):
1a. The racing driver showed the torn overall...
1b. The racing driver showed the helpful mechanic...
Sentence (1a) was to be completed with a prepo-
sitional object (?to the helpful mechanic?), while
(1b) required a double object construction (?the torn
overall?). Subsequently, subjects were allowed to
freely complete a sentence such as the following
one, describing a picture they were shown:
2. The patient showed ...
Subjects were more likely to complete (2) with a
double-object construction when primed with (1b),
and with a prepositional object construction when
primed with (1a).
In a previous corpus-study, using transcriptions
of spontaneous, task-oriented and non-task-oriented
dialogue, utterances were annotated with syntactic
trees, which we then used to determine the phrase-
structure rules that licensed production (and com-
prehension) of the utterances (Reitter et al, 2006b).
For each rule, the time of its occurrence was noted,
e.g. we noted
3. 117.9s NP ? AT AP NN a fenced meadow
4. 125.5s NP ? AT AP NN the abandoned cottage
In this study, we then found that the re-occurrence
of a rule (as in 4) was correlated with the temporal
distance to the first occurrence (3), e.g., 7.6 seconds.
The shorter the distance between prime (3) and tar-
get (4), the more likely were rules to re-occur.
In a conversation, priming may lead a speaker
to choose a verb over a synonym because their in-
terlocutor has used it a few seconds before. This,
in turn, will increase the likelihood of the struc-
tural form of the arguments in the governed verbal
phrase?simply because lexical items have their pref-
erences for particular syntactic structures, but also
because structural priming may be stronger if lexi-
cal items are repeated (lexical boost, Pickering and
Branigan (1998)). Additionally, the structural prim-
ing effects introduced above will make a previously
observed or produced syntactic structure more likely
to be re-used. This chain reaction leads interlocu-
tors in dialogue to reach a common situation model.
Note that the IAM, in which interlocutors automati-
cally and cheaply build a common representation of
common knowledge, is at odds with views that af-
ford each dialogue participant an explicit and sepa-
rate representation of their interlocutor?s knowledge.
The connection between linguistic persistence or
priming effects and the success of dialogue is cru-
cial for the IAM. The predictions arising from this,
however, have eluded testing so far. In our previous
study (Reitter et al, 2006b), we found more syn-
tactic priming in the task-oriented dialogues of the
Map Task corpus than in the spontaneous conversa-
tion collected in the Switchboard corpus. However,
we compared priming effects across two datasets,
where participants and conversation topics differed
greatly. Switchboard contains spontaneous conver-
sation over the telephone, while the task-oriented
Map Task corpus was recorded with interlocutors
co-present. While the result (more priming in
task-oriented dialogue) supported the predictions of
IAM, cognitive load effects could not be distin-
guished from priming. In the current study, we ex-
amine structural repetition in task-oriented dialogue
only and focus on an extrinsic measure, namely task
success.
2 Related Work
Prior work on predicting task success has been
done in the context of human-computer spoken di-
alogue systems. Features such as recognition er-
ror rates, natural language understanding confidence
and context shifts, confirmations and re-prompts (di-
alogue management) have been used classify dia-
809
logues into successful and problematic ones (Walker
et al, 2000). With these automatically obtainable
features, an accuracy of 79% can be achieved given
the first two turns of ?How may I help you?? di-
alogues, where callers are supposed to be routed
given a short statement from them about what they
would like to do. From the whole interaction (very
rarely more than five turns), 87% accuracy can be
achieved (36% of dialogues had been hand-labeled
?problematic?). However, the most predictive fea-
tures, which related to automatic speech recognition
errors, are neither available in the human-human di-
alogue we are concerned with, nor are they likely to
be the cause of communication problems there.
Moreover, failures in the Map Task dialogues are
due to the actual goings-on when two interlocutors
engage in collaborative problem-solving to jointly
reach an understanding. In such dialogues, inter-
locutors work over a period of about half an hour.
To predict their degree of success, we will leverage
the phenomenon of persistence, or priming.
In previous work, two paradigms have seen exten-
sive use to measure repetition and priming effects.
Experimental studies expose subjects to a particular
syntactic construction, either by having them pro-
duce the construction by completing a sample sen-
tence, or by having an experimenter or confederate
interlocutor use the construction. Then, subjects are
asked to describe a picture or continue with a given
task, eliciting the target construction or a compet-
ing, semantically equivalent alternative. The analy-
sis then shows an effect of the controlled condition
on the subject?s use of the target construction.
Observational studies use naturalistic data, such
as text and dialogue found in corpora. Here, the
prime construction is not controlled?but again, a
correlation between primes and targets is sought.
Specific competing constructions such as ac-
tive/passive, verbal particle placement or that-
deletion in English are often the object of study
(Szmrecsanyi, 2005; Gries, 2005; Dubey et al,
2005; Ja?ger, 2006), but the effect can also be gen-
eralized to syntactic phrase-structure rules or com-
binatorial categories (Reitter et al, 2006a).
Church (2000) proposes adaptive language mod-
els to account for lexical adaptation. Each document
is split into prime and target halves. Then, for se-
lected words w, the model estimates
P (+adapt) = P (w ? target|w ? prime)
P (+adapt) is higher than Pprior = P (w ?
target), which is not surprising, since texts are usu-
ally about a limited number of topics.
This method looks at repetition over whole doc-
ument halves, independently of decay. In this pa-
per, we apply the same technique to syntactic rules,
where we expect to estimate syntactic priming ef-
fects of the long-term variety.
3 Repetition-based Success Prediction
3.1 The Success Prediction Task
In the following, we define two variants of the task
and then describe a model that uses repetition effects
to predict success.
Task 1: Success is estimated when an entire di-
alogue is given. All linguistic and non-linguistic
information available may be used. This task re-
flects post-hoc analysis applications, where dia-
logues need to be evaluated without the actual suc-
cess measure being available for each dialogue. This
covers cases where, e.g., it is unclear whether a call
center agent or an automated system actually re-
sponded to the call satisfactorily.
Task 2: Success is predicted when just the initial
5-minute portion of the dialogue is available. A dia-
logue system?s or a call center agent?s strategy may
be influenced depending on such a prediction.
3.2 Method
To address the tasks described in the previous Sec-
tion, we train support vector machines (SVM) to
predict the task success score of a dialogue from
lexical and syntactic repetition information accumu-
lated up to a specified point in time in the dialogue.
Data
The HCRC Map Task corpus (Anderson et al,
1991) contains 128 dialogues between subjects, who
were given two slightly different maps depicting the
same (imaginary) landscape. One subject gives di-
rections for a predefined route to another subject,
who follows them and draws a route on their map.
The spoken interactions were recorded, tran-
scribed and syntactically annotated with phrase-
structure grammar.
810
The Map Task provides us with a precise measure
of success, namely the deviation of the predefined
and followed route. Success can be quantified by
computing the inverse deviation between subjects?
paths. Both subjects in each trial were asked to draw
?their? respective route on the map that they were
given. The deviation between the respective paths
drawn by interlocutors was then determined as the
area covered in between the paths (PATHDEV).
Features
Repetition is measured on a lexical and a syntactic
level. To do so, we identify all constituents in the
utterances as per phrase-structure analysis. [Go [to
[the [[white house] [on [the right]]]]]] would yield
11 constituents. Each constituent is licensed by a
syntactic rule, for instance VP ? V PP for the top-
most constituent in the above example.
For each constituent, we check whether it is a lex-
ical or syntactic repetition, i.e. if the same words
occurred before, or if the licensing rule has occurred
before in the same dialogue. If so, we increment
counters for lexical and/or syntactic repetitions, and
increase a further counter for string repetition by the
length of the phrase (in characters). The latter vari-
able accounts for the repetition of long phrases.
We include a data point for each 10-second inter-
val of the dialogue, with features reporting the lexi-
cal (LEXREP), syntactic (SYNREP) and character-
based (CHARREP) repetitions up to that point in
time. A time stamp and the total numbers of con-
stituents and characters are also included (LENGTH).
This way, the model may work with repetition pro-
portions rather than the absolute counts.
We train a support vector machine for regression
with a radial basis function kernel (? = 5), using the
features as described above and the PATHDEV score
as output.
3.3 Evaluation
We cast the task as a regression problem. To pre-
dict a dialogue?s score, we apply the SVM to its data
points. The mean outcome is the estimated score.
A suitable evaluation measure, the classical r2,
indicates the proportion of the variance in the ac-
tual task success score that can be predicted by
the model. All results reported here are produced
from 10-fold cross-validated 90% training / 10% test
Task 1 Task 2
ALL Features 0.17 0.14
ALL w/o SYNREP 0.15 0.06
ALL w/o LEX/CHARREP 0.09 0.07
LENGTH ONLY 0.09 n/a
Baseline 0.01 0.01
Table 1: Portion of variance explained (r2)
splits of the dialogues. No full dialogue was in-
cluded in both test and training sets.
Task 1 was evaluated with all data, the Task 2
model was trained and tested on data points sampled
from the first 5 minutes of the dialogue.
For Task 1 (full dialogues), the results (Table 1)
indicate that ALL repetition features together with
the LENGTH of the conversation, account for about
17% of the total score variance. The repetition fea-
tures improve on the performance achieved from di-
alogue length alone (about 9%).
For the more difficult Task 2, ALL features to-
gether achieve 14% of the variance. (Note that
LENGTH is not available.) When the syntactic repe-
tition feature is taken out and only lexical (LEXREP)
and character repetition (CHARREP) are used, we
achieve 6% in explained variance.
The baseline is implemented as a model that al-
ways estimates the mean score. It should, theoreti-
cally, be close to 0.
3.4 Discussion
Obviously, linguistic information alone will not ex-
plain the majority of the task-solving abilities. Apart
from subject-related factors, communicative strate-
gies will play a role.
However, linguistic repetition serves as a good
predictor of how well interlocutors will complete
their joint task. The features used are relatively sim-
ple: provided there is some syntactic annotation,
rule repetition can easily be detected. Even with-
out syntactic information, lexical repetition already
goes a long way.
But what kind of repetition is it that plays a role in
task-oriented dialogue? Leaving out features is not
an ideal method to quantify their influence?in par-
ticular, where features inter-correlate. The contribu-
tion of syntactic repetition is still unclear from the
811
present results: it acts as a useful predictor only over
the course of the whole dialogues, but not within a
5-minute time span, where the SVM cannot incor-
porate its informational content.
We will therefore turn to a more detailed analysis
of structural repetition, which should help us draw
conclusions relating to the psycholinguistics of dia-
logue.
4 Long term and short term priming
In the following, we will examine syntactic (struc-
tural) priming as one of the driving forces behind
alignment. We choose syntactic over lexical priming
for two reasons. Lexical repetition due to priming is
difficult to distinguish from repetition that is due to
interlocutors attending to a particular topic of con-
versation, which, in coherent dialogue, means that
topics are clustered. Lexical choice reflects those
topics, hence we expect clusters of particular termi-
nology. Secondly: the maps used to collect the dia-
logues in the Map Task corpus contained landmarks
with labels. It is only natural (even if by means
to cross-modal priming) that speakers will identify
landmarks using the labels and show little variability
in lexical choice. We will measure repetition of syn-
tactic rules, whereby word-by-word repetition (topi-
cality effects, parroting) is explicitly excluded.
For syntactic priming1, two repetition effects
have been identified. Classical priming effects are
strong: around 10% for syntactic rules (Reitter et al,
2006b). However, they decay quickly (Branigan
et al, 1999) and reach a low plateau after a few sec-
onds, which likens to the effect to semantic (similar-
ity) priming. What complicates matters is that there
is also a different, long-term adaptation effect that is
also commonly called (repetition) priming.
Adaptation has been shown to last longer, from
minutes (Bock and Griffin, 2000) to several days.
Lexical boost interactions, where the lexical rep-
etition of material within the repeated structure
strengthens structural priming, have been observed
for short-term priming, but not for long-term prim-
ing trials where material intervened between prime
and target utterances (Konopka and Bock, 2005).
Thus, short- and long-term adaptation effects may
1in production and comprehension, which we will not dis-
tinguish further for space reasons. Our data are (off-line) pro-
duction data.
well be due to separate cognitive processes, as re-
cently argued by (Ferreira and Bock, 2006). Section
5 deals with decay-based short-term priming, Sec-
tion 6 with long-term adaptation.
Pickering and Garrod (2004) do not make the type
of priming supporting alignment explicit. Should
we find differences in the way task success interacts
with different kinds of repetition effects, then this
would be a good indication about what effect sup-
ports IAM. More concretely, we could say whether
alignment is due to the automatic, classical priming
effect, or whether it is based on a long-term effect
that is possibly closer to implicit learning (Chang
et al, 2006).
5 Short-term priming
In this section, we attempt to detect differences in
the strength of short-term priming in successful and
less successful dialogues. To do so, we use the mea-
sure of priming strength established by Reitter et al
(2006b), which then allows us to test whether prim-
ing interacts with task success. Under the assump-
tions of IAM we would expect successful dialogues
to show more priming than unsuccessful ones.
Obviously, difficulties with the task at hand may
be due to a range of problems that the subjects may
have, linguistic and otherwise. But given that the di-
alogues contain variable levels of syntactic priming,
one would expect that this has at least some influ-
ence on the outcome of the task.
5.1 Method: Logistic Regression
We used mixed-effects regression models that pre-
dict a binary outcome (repetition) using a number of
discrete and continuous factors.2
As a first step, our modeling effort tries to estab-
lish a priming effect. To do so, we can make use
of the fact that the priming effect decays over time.
How strong that decay is gives us an indication of
how much repetition probability we see shortly after
the stimulus (prime) compared to the probability of
chance repetition?without ever explicitly calculating
such a prior.
Thus we define the strength of priming as the de-
cay rate of repetition probability, from shortly after
2We use Generalized Linear Mixed Effects models fitted us-
ing GlmmPQL in the MASS R library.
812
the prime to 15 seconds afterward (predictor: DIST).
Thus, we take several samples at varying distances
(d), looking at cases of structural repetition, and
cases where structure has not been repeated.
In the syntactic context, syntactic rules such as VP
? VP PP reflect syntactic decisions. Priming of a
syntactic construction shows up in the tendency to
repeat such rules in different lexical contexts. Thus,
we examine whether syntactic rules have been re-
peated at a distance d. For each syntactic rule that
occurs at time t1, we check a one-second time pe-
riod [t1 ? d ? 0.5, t1 ? d + 0.5] for an occurrence
of the same rule, which would constitute a prime.
Thus, the model will be able to implicitly estimate
the probability of repetition.
Generalized Linear Regression Models (GLMs)
can then model the decay by estimating the rela-
tionship between d and the probability of rule repe-
tition. The model is designed to predict whether rep-
etition will occur, or, more precisely, whether there
is a prime for a given target (priming). Under a no-
priming null-hypothesis, we would assume that the
priming probability is independent of d. If there is
priming, however, increasing d will negatively influ-
ence the priming probability (decay). So, we expect
a model parameter (DIST) for d that is reliably neg-
ative, and lower, if there is more priming.
With this method, we draw multiple samples from
the same utterance?for different d, but also for dif-
ferent syntactic rules occurring in those utterances.
Because these samples are inter-dependent, we use
a grouping variable indicating the source utterance.
Because the dataset is sparse with respect to PRIME,
balanced sampling is needed to ensure an equal
number of data points of priming and non-priming
cases (PRIME) is included.
This method has been previously used to confirm
priming effects for the general case of syntactic rules
by Reitter et al (2006b). Additionally, the GLM can
take into account categorical and continuous covari-
ates that may interact with the priming effect. In
the present experiment, we use an interaction term
to model the effect of task success.3 The crucial in-
teraction, in our case, is task success: PATHDEV is
the deviation of the paths that the interlocutors drew,
3We use the A?B operator in the model formulas to indicate
the inclusion of main effects of the features A and B and their
interactions A : B.
normalized to the range [0,1]. The core model is
thus PRIME ? log(DIST) ? PATHDEV.
If IAM is correct, we would expect that the devia-
tion of paths, which indicates negative task success,
will negatively correlate with the priming effect.
5.2 Results
Short-term priming reliably correlated (negatively)
with the distance, hence we see a decay and priming
effect (DIST, b = ?0.151, p < 0.0001, as shown in
previous work).
Notably, path deviation and short-term priming
did not correlate. The model showed was no such
interaction (DIST:PATHDEV, p = 0.91).
We also tested for an interaction with an ad-
ditional factor indicating whether prime and tar-
get were uttered by the same or a different
speaker (comprehension-production vs. production-
production priming). No such interaction ap-
proached reliability (log(DIST):PATHDEV:ROLE,
p = 0.60).
We also tested whether priming changes over time
over the course of each dialogue. It does not. There
were no reliable interaction effects of centered
prime/target times (log(DIST):log(STARTTIME),
p = 0.75, log(DIST):PATHDEV:log(STARTTIME),
p = 0.63). Reducing the model by removing
unreliable interactions did not yield any reliable
effects.
5.3 Discussion
We have shown that while there is a clear priming
effect in the short term, the size of this priming effect
does not correlate with task success. There is no
reliable interaction with success.
Does this indicate that there is no strong func-
tional component to priming in the dialogue con-
text? There may still be an influence of cognitive
load due to speakers working on the task, or an over-
all disposition for higher priming in task-oriented di-
alogue: Reitter et al (2006b) point at stronger prim-
ing in such situations. But our results here are diffi-
cult to reconcile with the model suggested by Picker-
ing and Garrod (2004), if we take short-term priming
as the driving force behind IAM.
Short-term priming decays within a few seconds.
Thus, to what extent could syntactic priming help in-
terlocutors align their situation models? In the Map
813
Task experiments, interlocutors need to refer to land-
marks regularly?but not every few seconds. It would
be sensible to expect longer-term adaptation (within
minutes) to drive dialogue success.
6 Long-term adapation
Long-term adaptation is a form of priming that
occurs over minutes and could, therefore, support
linguistic and situation model alignment in task-
oriented dialogue. IAM and the success of the
SVM based method could be based on such an ef-
fect instead of short-term priming. Analogous to the
the previous experiment, we hypothesize that more
adaptation relates to more task success.
6.1 Method
After the initial few seconds, structural repetition
shows little decay, but can be demonstrated even
minutes or longer after the stimulus. To measure this
type of adapation, we need a different strategy to es-
timate the size of this effect.
While short-term priming can be pin-pointed us-
ing the characteristic decay, for long-term priming
we need to inspect whole dialogues and construct
and contrast dialogues where priming is possible and
ones where it is not. Factor SAMEDOC distinguishes
the two situations: 1) Priming can happen in con-
tiguous dialogues. We treat the first half of the dia-
logue as priming period, and the rule instances in the
second half as targets. 2) The control case is when
priming cannot have taken place, i.e., between unre-
lated dialogues. Prime period and targets stem from
separate randomly sampled dialogue halves that al-
ways come from different dialogues.
Thus, our model (PRIME ? SAMEDOC ?
PATHDEV) estimates the influence of priming on
rule us. From a Bayesian perspective, we would
say that the second kind of data (non-priming) al-
low the model to estimate a prior for rule repetitions.
The goal is now to establish a correlation between
SAMEDOC and the existence of repetition. If and
only if there is long-term adapation would we ex-
pect such a correlation.
Analogous to the short-term priming model, we
define repetition as the occurrence of a prime within
the first document half (PRIME), and sample rule in-
stances from the second document half. To exclude
?
?
?
?
?
?
?
?
? ?
?
?
0.0 0.5 1.0
0.8
2
0.8
4
0.8
6
0.8
8
0.9
0
0.9
2
log path deviation (inverse success)
rela
tive
 re
pet
itio
n (
log
?o
dds
)
more less
success
l
e
s
s
m
o
r
e
s
y
n
.
 
a
d
a
p
t
a
t
i
o
n
Figure 1: Relative rule repetition probability
(chance repetition exluded) over (neg.) task success.
short-term priming effects, we drop a 10-second por-
tion in the middle of the dialogues.
Task success is inverse path deviation PATHDEV
as before, which should, under IAM assumptions,
interact with the effect estimated for SAMEDOC.
6.2 Results
Long-term repetition showed a positive priming ef-
fect (SAMEDOC, b = 3.303, p < 0.0001). This
generalizes previous experimental priming results in
long-term priming.
Long-term-repetition did not inter-
act with (normalized) rule frequency
(SAMEDOC:log(RULEFREQ, b = ?0.044, p =
0.35). The interaction was removed for all other
parameters reported.4
The effect interacted reliably with the path
deviation scores (SAMEDOC:PATHDEV, b =
?0.624, p < 0.05). We find a reliable correlation
of task success and syntactic priming. Stronger path
deviations relate to weaker priming.
6.3 Discussion
The more priming we see, the better subjects per-
form at synchronizing their routes on the maps. This
is exactly what one would expect under the assump-
4Such an interaction also could not be found in a reduced
model with only SAMEDOC and RULEFREQ.
814
tion of IAM. Also, there is no evidence for stronger
long-term adaptation of rare rules, which may point
out a qualitative difference to short-term priming.
Of course, this correlation does not necessarily in-
dicate a causal relationship. However, participants
in Map Task did not receive an explicit indication
about whether they were on the ?right track?. Mis-
takes, such as passing a landmark on its East and
not on the West side, were made and went unno-
ticed. Thus, it is not very likely that task success
caused alignment to improve at large. We suspect
such a possibility, however, for very unsuccessful
dialogues. A closer look at the correlation (Figure
1) reveals that while adaptation indeed decreases as
task success decreases, adaptation increased again
for some of the least successful dialogues. It is pos-
sible that here, miscoordination became apparent to
the participants, who then tried to switch strategies.
Or, simply put: too much alignment (and too little
risk-taking) is unhelpful. Further, qualitative, work
needs to be done to investigate this hypothesis.
From an applied perspective, the correlation
shows that of the repetition effects included in our
task-success prediction model, it is long-term syn-
tactic adaptation as opposed to the more automatic
short-term priming effect that contributes to predic-
tion accuracy. We take this as an indication to in-
clude adaptation rather than just priming in a model
of alignment in dialogue.
7 Conclusion
Task success in human-human dialogue is
predictable?the more successfully speakers collab-
orate, the more they show linguistic adaptation.
This confirms our initial hypothesis of IAM. In the
applied model, knowledge of lexical and syntactic
repetition helps to determine task success even after
just a few minutes of the conversation.
We suggested two application-oriented tasks (es-
timating and predicting task success) and an ap-
proach to address them. They now provide an op-
portunity to explore and exploit other linguistic and
extra-linguistic parameters.
The second contribution is a closer inspection of
structural repetition, which showed that it is long-
term adaptation that varies with task success, while
short-term priming appears largely autonomous.
Long-term adaptation may thus be a strategy that
aids dialogue partners in aligning their language and
their situation models.
Acknowledgments
The authors would like to thank Frank Keller and the reviewers.
The first author is supported by the Edinburgh-Stanford Link.
References
A. Anderson, M. Bader, E. Bard, E. Boyle, G. M. Doherty,
S. Garrod, S. Isard, J. Kowtko, J. McAllister, J. Miller,
C. Sotillo, H. Thompson, and R. Weinert. 1991. The HCRC
Map Task corpus. Language and Speech, 34(4):351?366.
J. Kathryn Bock. 1986. Syntactic persistence in language pro-
duction. Cognitive Psychology, 18:355?387.
J. Kathryn Bock and Zenzi Griffin. 2000. The persistence of
structural priming: transient activation or implicit learning?
J of Experimental Psychology: General, 129:177?192.
H. P. Branigan, M. J. Pickering, and A. A. Cleland. 1999. Syn-
tactic priming in language production: evidence for rapid
decay. Psychonomic Bulletin and Review, 6(4):635?640.
F. Chang, G. Dell, and K. Bock. 2006. Becoming syntactic.
Psychological Review, 113(2):234?272.
Kenneth W. Church. 2000. Empirial estimates of adaptation:
The chance of two noriegas is closer to p/2 than p2. In
Coling-2000, Saarbru?cken, Germany.
A. Dubey, F. Keller, and P. Sturt. 2005. Parallelism in coordi-
nation as an instance of syntactic priming: Evidence from
corpus-based modeling. In Proc. HLT/EMNLP-2005, pp.
827?834. Vancouver, Canada.
Vic Ferreira and Kathryn Bock. 2006. The functions of struc-
tural priming. Language and Cognitive Processes, 21(7-8).
Stefan Th. Gries. 2005. Syntactic priming: A corpus-based ap-
proach. J of Psycholinguistic Research, 34(4):365?399.
T. Florian Ja?ger. 2006. Redundancy and Syntactic Reduction in
Spontaneous Speech. Ph.D. thesis, Stanford University.
Agnieszka Konopka and J. Kathryn Bock. 2005. Helping syntax
out: What do words do? In Proc. 18th CUNY. Tucson, AZ.
Martin J. Pickering and Holly P. Branigan. 1998. The represen-
tation of verbs: Evidence from syntactic priming in language
production. Journal of Memory and Language, 39:633?651.
Martin J. Pickering and Simon Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and Brain Sci-
ences, 27:169?225.
D. Reitter, J. Hockenmaier, and F. Keller. 2006a. Prim-
ing effects in Combinatory Categorial Grammar. In Proc.
EMNLP-2006, pp.308?316. Sydney, Australia.
D. Reitter, J. D. Moore, and F. Keller. 2006b. Priming of syn-
tactic rules in task-oriented dialogue and spontaneous con-
versation. In Proc. CogSci-2006, pp. 685?690. Vancouver,
Canada.
Benedikt Szmrecsanyi. 2005. Creatures of habit: A corpus-
linguistic analysis of persistence in spoken english. Corpus
Linguistics and Linguistic Theory, 1(1):113?149.
M. Walker, I. Langkilde, J. Wright, A. Gorin, and D. Litman.
2000. Learning to predict problematic situations in a spoken
dialogue system: experiments with How may I help you? In
Proc. NAACL-2000, pp. 210?217. San Francisco, CA.
815
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1016?1023,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Combining Multiple Knowledge Sources for Dialogue Segmentation in
Multimedia Archives
Pei-Yun Hsueh
School of Informatics
University of Edinburgh
Edinburgh, UK EH8 9WL
p.hsueh@ed.ac.uk
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, UK EH8 9WL
J.Moore@ed.ac.uk
Abstract
Automatic segmentation is important for
making multimedia archives comprehensi-
ble, and for developing downstream infor-
mation retrieval and extraction modules. In
this study, we explore approaches that can
segment multiparty conversational speech
by integrating various knowledge sources
(e.g., words, audio and video recordings,
speaker intention and context). In particu-
lar, we evaluate the performance of a Max-
imum Entropy approach, and examine the
effectiveness of multimodal features on the
task of dialogue segmentation. We also pro-
vide a quantitative account of the effect of
using ASR transcription as opposed to hu-
man transcripts.
1 Introduction
Recent advances in multimedia technologies have
led to huge archives of audio-video recordings of
multiparty conversations in a wide range of areas
including clinical use, online video sharing ser-
vices, and meeting capture and analysis. While it
is straightforward to replay such recordings, find-
ing information from the often lengthy archives is a
more challenging task. Annotating implicit seman-
tics to enhance browsing and searching of recorded
conversational speech has therefore posed new chal-
lenges to the field of multimedia information re-
trieval.
One critical problem is how to divide unstructured
conversational speech into a number of locally co-
herent segments. The problem is important for two
reasons: First, empirical analysis has shown that an-
notating transcripts with semantic information (e.g.,
topics) enables users to browse and find information
from multimedia archives more efficiently (Baner-
jee et al, 2005). Second, because the automatically
generated segments make up for the lack of explicit
orthographic cues (e.g., story and paragraph breaks)
in conversational speech, dialogue segmentation
is useful in many spoken language understanding
tasks, including anaphora resolution (Grosz and Sid-
ner, 1986), information retrieval (e.g., as input for
the TREC Spoken Document Retrieval (SDR) task),
and summarization (Zechner and Waibel, 2000).
This study therefore aims to explore whether a
Maximum Entropy (MaxEnt) classifier can inte-
grate multiple knowledge sources for segmenting
recorded speech. In this paper, we first evaluate the
effectiveness of features that have been proposed in
previous work, with a focus on features that can be
extracted automatically. Second, we examine other
knowledge sources that have not been studied sys-
tematically in previous work, but which we expect
to be good predictors of dialogue segments. In ad-
dition, as our ultimate goal is to develop an infor-
mation retrieval module that can be operated in a
fully automatic fashion, we also investigate the im-
pact of automatic speech recognition (ASR) errors
on the task of dialogue segmentation.
2 Previous Work
In previous work, the problem of automatic dia-
logue segmentation is often considered as similar to
the problem of topic segmentation. Therefore, re-
search has adopted techniques previously developed
1016
to segment topics in text (Kozima, 1993; Hearst,
1997; Reynar, 1998) and in read speech (e.g., broad-
cast news) (Ponte and Croft, 1997; Allan et al,
1998). For example, lexical cohesion-based algo-
rithms, such as LCSEG (Galley et al, 2003), or its
word frequency-based predecessor TextTile (Hearst,
1997) capture topic shifts by modeling the similarity
of word repetition in adjacent windows.
However, recent work has shown that LCSEG is
less successful in identifying ?agenda-based conver-
sation segments? (e.g., presentation, group discus-
sion) that are typically signalled by differences in
group activity (Hsueh and Moore, 2006). This is
not surprising since LCSEG considers only lexical
cohesion. Previous work has shown that training a
segmentation model with features that are extracted
from knowledge sources other than words, such as
speaker interaction (e.g., overlap rate, pause, and
speaker change) (Galley et al, 2003), or partici-
pant behaviors, e.g., note taking cues (Banerjee and
Rudnicky, 2006), can outperform LCSEG on similar
tasks.
In many other fields of research, a variety of fea-
tures have been identified as indicative of segment
boundaries in different types of recorded speech.
For example, Brown et al (1980) have shown that
a discourse segment often starts with relatively high
pitched sounds and ends with sounds of pitch within
a more compressed range. Passonneau and Lit-
man (1993) identified that topic shifts often occur
after a pause of relatively long duration. Other
prosodic cues (e.g., pitch contour, energy) have been
studied for their correlation with story segments in
read speech (Tur et al, 2001; Levow, 2004; Chris-
tensen et al, 2005) and with theory-based discourse
segments in spontaneous speech (e.g., direction-
given monologue) (Hirschberg and Nakatani, 1996).
In addition, head and hand/forearm movements are
used to detect group-action based segments (Mc-
Cowan et al, 2005; Al-Hames et al, 2005).
However, many other features that we expect to
signal segment boundaries have not been studied
systematically. For instance, speaker intention (i.e.,
dialogue act types) and conversational context (e.g.,
speaker role). In addition, although these features
are expected to be complementary to one another,
few of the previous studies have looked at the ques-
tion how to use conditional approaches to model the
correlation among features.
3 Methodology
3.1 Meeting Corpus
This study aims to explore approaches that can in-
tegrate multimodal information to discover implicit
semantics from conversation archives. As our goal
is to identify multimodal cues of segmentation in
face-to-face conversation, we use the AMI meeting
corpus (Carletta et al, 2006), which includes audio-
video recordings, to test our approach. In particu-
lar, we are using 50 scenario-based meetings from
the AMI corpus, in which participants are assigned
to different roles and given specific tasks related to
designing a remote control. On average, AMI meet-
ings last 26 minutes, with over 4,700 words tran-
spired. This corpus includes annotation for dialogue
segmentation and topic labels. In the annotation pro-
cess, annotators were given the freedom to subdi-
vide a segment into subsegments to indicate when
the group was discussing a subtopic. Annotators
were also given a set of segment descriptions to be
used as labels. Annotators were instructed to add a
new label only if they could not find a match in the
standard set. The set of segment descriptions can
be divided to three categories: activity-based (e.g.,
presentation, discussion), issue-based (e.g., budget,
usability), and functional segments (e.g., chitchat,
opening, closing).
3.2 Preprocessing
The first step is to break a recorded meeting into
minimal units, which can vary from sentence chunks
to blocks of sentences. In this study, we use spurts,
that is, consecutive speech with no pause longer than
0.5 seconds, as minimal units.
Then, to examine the difference between the set
of features that are characteristic of segmentation at
both coarse and fine levels of granularity, this study
characterizes a dialogue as a sequence of segments
that may be further divided into sub-segments. We
take the theory-free dialogue segmentation annota-
tions in the corpus and flatten the sub-segment struc-
ture and consider only two levels of segmentation:
top-level segments and all sub-level segments.1 We
1We take the spurts which the annotators choose as the be-
ginning of a segment as the topic boundaries. On average,
1017
observed that annotators tended to annotate activity-
based segments only at the top level, whereas they
often included sub-topics when segmenting issue-
based segments. For example, a top-level interface
specialist presentation segment can be divided into
agenda/equipment issues, user requirements, exist-
ing products, and look and usability sub-level seg-
ments.
3.3 Intercoder Agreement
To measure intercoder agreement, we employ three
different metrics: the kappa coefficient, PK, and
WD. Kappa values measure how well a pair of an-
notators agree on where the segments break. PK is
the probability that two spurts drawn randomly from
a document are incorrectly identified as belonging
to the same segment. WindowDiff (WD) calculates
the error rate by moving a sliding window across the
transcript counting the number of times the hypoth-
esized and reference segment boundaries are differ-
ent. While not uncontroversial, the use of these met-
rics is widespread. Table 1 shows the intercoder
agreement of the top-level and sub-level segmenta-
tion respectively.
It is unclear whether the kappa values shown here
indicate reliable intercoder agreement.2 But given
the low disagreement rate among codings in terms
of the PK and WD scores, we will argue for the reli-
ability of the annotation procedure used in this study.
Also, to our knowledge the reported degree of agree-
ment is the best in the field of meeting dialogue seg-
mentation.3
Intercoder Kappa PK WD
TOP 0.66 0.11 0.17
SUB 0.59 0.23 0.28
Table 1: Intercoder agreement of annotations at the
top-level (TOP) and sub-level (SUB) segments.
the annotators marked 8.7 top-level segments and 14.6 sub-
segments per meeting.
2In computational linguistics, kappa values over 0.67
point to reliable intercoder agreement. But Di Eugenio and
Glass (2004) have found that this interpretation does not hold
true for all tasks.
3For example, Gruenstein et al(2005) report kappa
(PK/WD) of 0.41(0.28/0.34) for determining the top-level and
0.45(0.27/0.35) for the sub-level segments in the ICSI meeting
corpus.
3.4 Feature Extraction
As reported in Section 2, there is a wide range of
features that are potentially characteristic of segment
boundaries, and we expect to find some of them use-
ful for automatic recognition of segment boundaries.
The features we explore can be divided into the fol-
lowing five classes:
Conversational Features: We follow Galley et
al. (2003) and extracted a set of conversational fea-
tures, including the amount of overlapping speech,
the amount of silence between speaker segments,
speaker activity change, the number of cue words,
and the predictions of LCSEG (i.e., the lexical co-
hesion statistics, the estimated posterior probability,
the predicted class).
Lexical Features: We compile the list of words
that occur more than once in the spurts that have
been marked as a top-level or sub-segment boundary
in the training set. Each spurt is then represented as
a vector space of unigrams from this list.
Prosodic Features: We use the direct modelling
approach proposed in Shriberg and Stolcke (2001)
and include maximum F0 and energy of the spurt,
mean F0 and energy of the spurt, pitch contour (i.e.,
slope) and energy at multiple points (e.g., the first
and last 100 and 200 ms, the first and last quarter,
the first and second half) of a spurt. We also include
rate of speech, in-spurt silence, preceding and sub-
sequent pauses, and duration. The rate of speech is
calculated as both the number of words and the num-
ber of syllables spoken per second.
Motion Features: We measure the magnitude
of relevant movements in the meeting room using
methods that detect movements directly from video
recordings in frames of 40 ms. Of special interest are
the frontal shots as recorded by the close up cameras,
the hand movements as recorded by the overview
cameras, and shots of the areas of the room where
presentations are made. We then average the magni-
tude of movements over the frames within a spurt as
its feature value.
Contextual Features: These include dialogue act
type4 and speaker role (e.g., project manager, mar-
4In the annotations, each dialogue act is classified as one
of 15 types, including acts about information exchange (e.g.,
Inform), acts about possible actions (e.g., Suggest), acts whose
primary purpose is to smooth the social functioning (e.g., Be-
positive), acts that are commenting on previous discussion (e.g.,
1018
keting expert). As each spurt may consist of multiple
dialogue acts, we represent each spurt as a vector of
dialogue act types, wherein a component is 1 or 0
depending on whether the type occurs in the spurt.
3.5 Multimodal Integration Using Maximum
Entropy Models
Previous work has used MaxEnt models for sentence
and topic segmentation and shown that conditional
approaches can yield competitive results on these
tasks (Christensen et al, 2005; Hsueh and Moore,
2006). In this study, we also use a MaxEnt clas-
sifier5 for dialogue segmentation under the typical
supervised learning scheme, that is, to train the clas-
sifier to maximize the conditional likelihood over
the training data and then to use the trained model
to predict whether an unseen spurt in the test set
is a segment boundary or not. Because continuous
features have to be discretized for MaxEnt, we ap-
plied a histogram binning approach, which divides
the value range intoN intervals that contain an equal
number of counts as specified in the histogram, to
discretize the data.
4 Experimental Results
4.1 Probabilistic Models
The first question we want to address is whether
the different types of characteristic multimodal fea-
tures can be integrated, using the conditional Max-
Ent model, to automatically detect segment bound-
aries. In this study, we use a set of 50 meet-
ings, which consists of 17,977 spurts. Among these
spurts, only 1.7% and 3.3% are top-level and sub-
segment boundaries. For our experiments we use
10-fold cross validation. The baseline is the re-
sult obtained by using LCSEG, an unsupervised ap-
proach exploiting only lexical cohesion statistics.
Table 2 shows the results obtained by using the
same set of conversational (CONV) features used
in previous work (Galley et al, 2003; Hsueh and
Moore, 2006), and results obtained by using all the
available features (ALL). The evaluation metrics PK
and WD are conventional measures of error rates in
segmentation (see Section 3.3). In Row 2, we see
Elicit-Assessment), and acts that allow complete segmentation
(e.g., Stall).
5The parameters of the MaxEnt classifier are optimized us-
ing Limited-Memory Variable Metrics.
TOP SUB
Error Rate PK WD PK WD
BASELINE(LCSEG) 0.40 0.49 0.40 0.47
MAXENT(CONV) 0.34 0.34 0.37 0.37
MAXENT(ALL) 0.30 0.33 0.34 0.36
Table 2: Compare the result of MaxEnt models
trained with only conversational features (CONV)
and with all available features (ALL).
that using a MaxEnt classifier trained on the conver-
sational features (CONV) alone improves over the
LCSEG baseline by 15.3% for top-level segments
and 6.8% for sub-level segements. Row 3 shows
that combining additional knowledge sources, in-
cluding lexical features (LX1) and the non-verbal
features, prosody (PROS), motion (MOT), and con-
text (CTXT), yields a further improvement (of 8.8%
for top-level segmentation and 5.4% for sub-level
segmentation) over the model trained on conversa-
tional features.
4.2 Feature Effects
The second question we want to address is which
knowledge sources (and combinations) are good
predictors for segment boundaries. In this round of
experiments, we evaluate the performance of differ-
ent feature combinations. Table 3 further illustrates
the impact of each feature class on the error rate
metrics (PK/WD). In addition, as the PK and WD
score do not reflect the magnitude of over- or under-
prediction, we also report on the average number of
hypothesized segment boundaries (Hyp). The num-
ber of reference segments in the annotations is 8.7 at
the top-level and 14.6 at the sub-level.
Rows 2-6 in Table 3 show the results of models
trained with each individual feature class. We per-
formed a one-way ANOVA to examine the effect
of different feature classes. The ANOVA suggests
a reliable effect of feature class (F (5, 54) = 36.1;
p < .001). We performed post-hoc tests (Tukey
HSD) to test for significant differences. Analysis
shows that the model that is trained with lexical
features alone (LX1) performs significantly worse
than the LCSEG baseline (p < .001). This is
due to the fact that cue words, such as okay and
now, learned from the training data to signal seg-
1019
TOP SUB
Hyp PK WD Hyp PK WD
BASELINE 17.6 0.40 0.49 17.6 0.40 0.47
(LCSEG)
LX1 61.2 0.53 0.72 65.1 0.49 0.66
CONV 3.1 0.34 0.34 2.9 0.37 0.37
PROS 2.3 0.35 0.35 2.5 0.37 0.37
MOT 96.2 0.36 0.40 96.2 0.38 0.41
CTXT 2.6 0.34 0.34 2.2 0.37 0.37
ALL 7.7 0.29 0.33 7.6 0.35 0.38
Table 3: Effects of individual feature classes and
their combination on detecting segment boundaries.
ment boundaries, are often used for non-discourse
purposes, such as making a semantic contribution to
an utterance.6 Thus, we hypothesize that these am-
biguous cue words have led the LX1 model to over-
predict. Row 7 further shows that when all avail-
able features (including LX1) are used, the com-
bined model (ALL) yields performance that is sig-
nificantly better than that obtained with individual
feature classes (F (5, 54) = 32.2; p < .001).
TOP SUB
Hyp PK WD Hyp PK WD
ALL 7.7 0.29 0.33 7.6 0.35 0.38
ALL-LX1 3.9 0.35 0.35 3.5 0.37 0.38
ALL-CONV 6.6 0.30 0.34 6.8 0.35 0.37
ALL-PROS 5.6 0.29 0.31 7.4 0.33 0.35
ALL-MOTION 7.5 0.30 0.35 7.3 0.35 0.37
ALL-CTXT 7.2 0.29 0.33 6.7 0.36 0.38
Table 4: Performance change of taking out each
individual feature class from the ALL model.
Table 4 illustrates the error rate change (i.e., in-
creased or decreased PK and WD score)7 that is
incurred by leaving out one feature class from the
ALL model. Results show that CONV, PROS, MO-
TION and CTXT can be taken out from the ALL
model individually without increasing the error rate
significantly.8 Morevoer, the combined models al-
6Hirschberg and Litman (1987) have proposed to discrimi-
nate the different uses intonationally.
7Note that the increase in error rate indicates performance
degradation, and vice versa.
8Sign tests were used to test for significant differences be-
tween means in each fold of cross validation.
ways perform better than the LX1 model (p < .01),
cf. Table 3.
This suggests that the non-lexical feature classes
are complementary to LX1, and thus it is essential
to incorporate some, but not necessarily all, of the
non-lexical classes into the model.
TOP SUB
Hyp PK WD Hyp PK WD
LX1 61.2 0.53 0.72 65.1 0.49 0.66
MOT 96.2 0.36 0.40 96.2 0.38 0.41
LX1+CONV 5.3 0.27 0.30 6.9 0.32 0.35
LX1+PROS 6.2 0.30 0.33 7.3 0.36 0.38
LX1+MOT 20.2 0.39 0.49 24.8 0.39 0.47
LX1+CTXT 6.3 0.28 0.31 7.2 0.33 0.35
MOT+PROS 62.0 0.34 0.34 62.1 0.37 0.37
MOT+CTXT 2.7 0.33 0.33 2.3 0.37 0.37
Table 5: Effects of combining complementary fea-
tures on detecting segment boundaries.
Table 5 further illustrates the performance of dif-
ferent feature combinations on detecting segment
boundaries. By subtracting the PK or WD score in
Row 1, the LX1 model, from that in Rows 3-6, we
can tell how essential each of the non-lexical classes
is to be combined with LX1 into one model. Results
show that CONV is the most essential, followed by
CTXT, PROS and MOT. The advantage of incorpo-
rating the non-lexical feature classes is also shown
in the noticeably reduced number of overpredictions
as compared to that of the LX1 model.
To analyze whether there is a significant interac-
tion between feature classes, we performed another
round of ANOVA tests to examine the effect of LX1
and each of the non-lexical feature classes on de-
tecting segment boundaries. This analysis shows
that there is a significant interaction effect on de-
tecting both top-level and sub-level segment bound-
aries (p < .01), suggesting that the performance of
LX1 is significantly improved when combined with
any non-lexical feature class. Also, among the non-
lexical feature classes, combining prosodic features
significantly improves the performance of the model
in which the motion features are combined to detect
top-level segment boundaries (p < .05).
1020
4.3 Degradation Using ASR
The third question we want to address here is
whether using the output of ASR will cause sig-
nificant degradation to the performance of the seg-
mentation approaches. The ASR transcripts used in
this experiment are obtained using standard technol-
ogy including HMM based acoustic modeling and
N-gram based language models (Hain et al, 2005).
The average word error rates (WER) are 39.1%. We
also applied a word alignment algorithm to ensure
that the number of words in the ASR transcripts is
the same as that in the human-produced transcripts.
In this way we can compare the PK and WD metrics
obtained on the ASR outputs directly with that on
the human transcripts.
In this study, we again use a set of 50 meetings
and 10-fold cross validation. We compare the per-
formance of the reference models, which are trained
on human transcripts and tested on human tran-
scripts, with that of the ASR models, which are
trained on ASR transcripts and tested on ASR tran-
scripts. Table 6 shows that despite the word recogni-
tion errors, none of the LCSEG, the MaxEnt models
trained with conversational features, and the Max-
Ent models trained with all available features per-
form significantly worse on ASR transcripts than on
reference transcripts. One possible explanation for
this, which we have observed in our corpus, is that
the ASR system is likely to mis-recognize different
occurences of words in the same way, and thus the
lexical cohesion statistic, which captures the similar-
ity of word repetition between two adjacency win-
dows, is also likely to remain unchanged. In addi-
tion, when the models are trained with other features
that are not affected by the recognition errors, such
as pause and overlap, the negative impacts of recog-
nition errors are further reduced to an insignificant
level.
5 Discussion
The results in Section 4 show the benefits of includ-
ing additional knowledge sources for recognizing
segment boundaries. The next question to be ad-
dressed is what features in these sources are most
useful for recognition. To provide a qualitative ac-
count of the segmentation cues, we performed an
analysis to determine whether each proposed feature
TOP SUB
Error Rate PK WD PK WD
LCSEG(REF) 0.45 0.57 0.42 0.47
LCSEG(ASR) 0.45 0.58 0.40 0.47
MAXENT-CONV(REF) 0.34 0.34 0.37 0.37
MAXENT-CONV(ASR) 0.34 0.33 0.38 0.38
MAXENT-ALL(REF) 0.30 0.33 0.34 0.36
MAXENT-ALL(ASR) 0.31 0.34 0.34 0.37
Table 6: Effects of word recognition errors on de-
tecting segments boundaries.
discriminates the class of segment boundaries. Pre-
vious work has identified statistical measures (e.g.,
Log Likelihood ratio) that are useful for determin-
ing the statistical association strength (relevance) of
the occurrence of an n-gram feature to target class
(Hsueh and Moore, 2006). Here we extend that
study to calculate the LogLikelihood relevance of all
of the features used in the experiments, and use the
statistics to rank the features.
Our analysis shows that people do speak and be-
have differently near segment boundaries. Some
of the identified segmentation cues match previous
findings. For example, a segment is likely to start
with higher pitched sounds (Brown et al, 1980; Ay-
ers, 1994) and a lower rate of speech (Lehiste, 1980).
Also, interlocutors pause longer than usual to make
sure that everyone is ready to move on to a new dis-
cussion (Brown et al, 1980; Passonneau and Lit-
man, 1993) and use some conventional expressions
(e.g., now, okay, let?s, um, so).
Our analysis also identified segmentation cues
that have not been mentioned in previous research.
For example, interlocutors do not move around a lot
when a new discussion is brought up; interlocutors
mention agenda items (e.g., presentation, meeting)
or content words more often when initiating a new
discussion. Also, from the analysis of current di-
alogue act types and their immediate contexts, we
also observe that at segment boundaries interlocu-
tors do the following more often than usual: start
speaking before they are ready (Stall), give infor-
mation (Inform), elicit an assessment of what has
been said so far (Elicit-assessment), or act to smooth
social functioning and make the group happier (Be-
positive).
1021
6 Conclusions and Future Work
This study explores the use of features from mul-
tiple knowledge sources (i.e., words, prosody, mo-
tion, interaction cues, speaker intention and role) for
developing an automatic segmentation component
in spontaneous, multiparty conversational speech.
In particular, we addressed the following questions:
(1) Can a MaxEnt classifier integrate the potentially
characteristic multimodal features for automatic di-
alogue segmentation? (2) What are the most dis-
criminative knowledge sources for detecting seg-
ment boundaries? (3) Does the use of ASR tran-
scription significantly degrade the performance of a
segmentation model?
First of all, our results show that a well perform-
ing MaxEnt model can be trained with available
knowledge sources. Our results improve on previous
work, which uses only conversational features, by
8.8% for top-level segmentation and 5.4% for sub-
level segmentation. Analysis of the effectiveness of
the various features shows that lexical features (i.e.,
cue words) are the most essential feature class to
be combined into the segmentation model. How-
ever, lexical features must be combined with other
features, in particular, conversational features (i.e.,
lexical cohesion, overlap, pause, speaker change), to
train well performing models.
In addition, many of the non-lexical feature
classes, including those that have been identified as
indicative of segment boundaries in previous work
(e.g., prosody) and those that we hypothesized as
good predictors of segment boundaries (e.g., mo-
tion, context), are not beneficial for recognizing
boundaries when used in isolation. However, these
non-lexical features are useful when combined with
lexical features, as the presence of the non-lexical
features can balance the tendency of models trained
with lexical cues alone to overpredict.
Experiments also show that it is possible to seg-
ment conversational speech directly on the ASR out-
puts. These results encouragingly show that we
can segment conversational speech using features
extracted from different knowledge sources, and in
turn, facilitate the development of a fully automatic
segmentation component for multimedia archives.
With the segmentation models developed and dis-
criminative knowledge sources identified, a remain-
ing question is whether it is possible to automat-
ically select the discriminative features for recog-
nition. This is particularly important for prosodic
features, because the direct modelling approach we
adopted resulted in a large number of features. We
expect that by applying feature selection methods
we can further improve the performance of auto-
matic segmentation models. In the field of machine
learning and pattern analysis, many methods and se-
lection criteria have been proposed. Our next step
will be to examine the effectiveness of these meth-
ods for the task of automatic segmentation. Also, we
will further explore how to choose the best perform-
ing ensemble of knowledge sources so as to facili-
tate automatic selection of knowledge sources to be
included.
Acknowledgement
This work was supported by the EU 6th FWP IST In-
tegrated Project AMI (Augmented Multi-party Inter-
action, FP6-506811). Our special thanks to Wessel
Kraaij, Stephan Raaijmakers, Steve Renals, Gabriel
Murray, Jean Carletta, and the anonymous review-
ers for valuable comments. Thanks also to the AMI
ASR group for producing the ASR transcriptions,
and to our research partners in TNO for generating
motion features.
References
M. Al-Hames, A. Dielmann, D. GaticaPerez, S. Reiter,
S. Renals, and D. Zhang. 2005. Multimodal integra-
tion for meeting group action segmentation and recog-
nition. In Proc. of MLMI 2005.
J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic detection and tracking pilot
study: Final report. In Proc. of the DARPA Broadcast
News Transcription and Understanding Workshop.
G. M. Ayers. 1994. Discourse functions of pitch range in
spontaneous and read speech. In Jennifer J. Venditti,
editor, OSU Working Papers in Linguistics, volume 44,
pages 1?49.
S. Banerjee and A. Rudnicky. 2006. Segmenting meet-
ings into agenda items by extracting implicit supervi-
sion from human note-taking. In Proc. of IUI 2006.
S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The
necessity of a meeting recording and playback system,
and the benefit of topic-level annotations to meeting
1022
browsing. In Proc. of the Tenth International Confer-
ence on Human-Computer Interaction.
G. Brown, K. L. Currie, and J. Kenworthe. 1980. Ques-
tions of Intonation. University Park Press.
J. Carletta et al 2006. The AMI meeting corpus: A pre-
announcement. In Steve Renals and Samy Bengio, ed-
itors, Springer-Verlag Lecture Notes in Computer Sci-
ence, volume 3869. Springer-Verlag.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2005. Maximum entropy segmentation of broadcast
news. In Proc. of ICASP, Philadelphia USA.
B. Di Eugenio and M. G. Glass. 2004. The kappa
statistic: A second look. Computational Linguistics,
30(1):95?101.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proc. of ACL 2003.
B. Grosz and C. Sidner. 1986. Attention, intentions, and
the structure of discourse. Computational Linguistics,
12(3).
A. Gruenstein, J. Niekrasz, and M. Purver. 2005. Meet-
ing structure annotation: Data and tools. In Proc. of
the SIGdial Workshop on Discourse and Dialogue.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, and S. Renals. 2005. Tran-
scription of conference room meetings: An investiga-
tion. In Proc. of Interspeech 2005.
M. Hearst. 1997. TextTiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguis-
tics, 25(3):527?571.
J. Hirschberg and D. Litman. 1987. Now let?s talk about
now: identifying cue phrases intonationally. In Proc.
of ACL 1987.
J. Hirschberg andC. H. Nakatani. 1996. A prosodic anal-
ysis of discourse segments in direction-giving mono-
logues. In Proc. of ACL 1996.
P. Hsueh and J.D. Moore. 2006. Automatic topic seg-
mentation and lablelling in multiparty dialogue. In the
first IEEE/ACM workshop on Spoken Language Tech-
nology (SLT) 2006.
H. Kozima. 1993. Text segmentation based on similarity
between words. In Proc. of ACL 1993.
I. Lehiste. 1980. Phonetic characteristics of discourse.
In the Meeting of the Committee on Speech Research,
Acoustical Society of Japan.
G. Levow. 2004. Prosody-based topic segmentation for
mandarin broadcast news. In Proc. of HLT 2004.
I. McCowan, D. Gatica-Perez, S. Bengio, G. Lathoud,
M. Barnard, and D. Zhang. 2005. Automatic analysis
of multimodal group actions in meetings. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
(PAMI), 27(3):305?317.
R. Passonneau and D. Litman. 1993. Intention-based
segmentation: Human reliability and correlation with
linguistic cues. In Proc. of ACL 1993.
J. Ponte and W. Croft. 1997. Text segmentation by topic.
In Proc. of the Conference on Research and Advanced
Technology for Digital Libraries 1997.
J. Reynar. 1998. Topic Segmentation: Algorithms and
Applications. Ph.D. thesis, UPenn, PA USA.
E. Shriberg and A. Stolcke. 2001. Direct modeling of
prosody: An overview of applications in automatic
speech processing. In Proc. International Conference
on Speech Prosody 2004.
G. Tur, D. Hakkani-Tur, A. Stolcke, and E. Shriberg.
2001. Integrating prosodic and lexical cues for auto-
matic topic segmentation. Computational Linguistics,
27(1):31?57.
K. Zechner and A. Waibel. 2000. DIASUMM: Flexi-
ble summarization of spontaneous dialogues in unre-
stricted domains. In Proc. of COLING-2000.
1023
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 49?52,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Simulating the Behaviour of Older versus Younger Users
when Interacting with Spoken Dialogue Systems
Kallirroi Georgila, Maria Wolters and Johanna D. Moore
Human Communication Research Centre
University of Edinburgh
kgeorgil|mwolters|jmoore@inf.ed.ac.uk
Abstract
In this paper we build user simulations of
older and younger adults using a corpus of
interactions with a Wizard-of-Oz appointment
scheduling system. We measure the quality of
these models with standard metrics proposed
in the literature. Our results agree with predic-
tions based on statistical analysis of the cor-
pus and previous findings about the diversity
of older people?s behaviour. Furthermore, our
results show that these metrics can be a good
predictor of the behaviour of different types of
users, which provides evidence for the validity
of current user simulation evaluation metrics.
1 Introduction
Using machine learning to induce dialogue man-
agement policies requires large amounts of training
data, and thus it is typically not feasible to build
such models solely with data from real users. In-
stead, data from real users is used to build simulated
users (SUs), who then interact with the system as
often as needed. In order to learn good policies, the
behaviour of the SUs needs to cover the range of
variation seen in real users (Schatzmann et al, 2005;
Georgila et al, 2006). Furthermore, SUs are critical
for evaluating candidate dialogue policies.
To date, several techniques for building SUs have
been investigated and metrics for evaluating their
quality have been proposed (Schatzmann et al,
2005; Georgila et al, 2006). However, to our knowl-
edge, no one has tried to build user simulations
for different populations of real users and measure
whether results from evaluating the quality of those
simulations agree with what is known about those
particular types of real users, extracted from other
studies of those populations. This is presumably due
to the lack of corpora for different types of users.
In this paper we focus on the behaviour of older
vs. younger adults. Most of the work to date on di-
alogue systems focuses on young users. However,
as average life expectancy increases, it becomes in-
creasingly important to design dialogue systems in
such a way that they can accommodate older peo-
ple?s behaviour. Older people are a user group with
distinct needs and abilities (Czaja and Lee, 2007)
that present challenges for user modelling. To our
knowledge no one so far has built statistical user
simulation models for older people. The only sta-
tistical spoken dialogue system for older people we
are aware of is Nursebot, an early application of sta-
tistical methods (POMDPs) within the context of a
medication reminder system (Roy et al, 2000).
In this study, we build SUs for both younger and
older adults using n-grams. Our data comes from a
fully annotated corpus of 447 interactions of older
and younger users with a Wizard-of-Oz (WoZ) ap-
pointment scheduling system (Georgila et al, 2008).
We then evaluate these models using standard met-
rics (Schatzmann et al, 2005; Georgila et al, 2006)
and compare our findings with the results of statisti-
cal corpus analysis.
The novelty of our work lies in two areas. First,
to the best of our knowledge this is the first time that
statistical SUs have been built for the increasingly
important population of older users.
Secondly, a general (but as yet untested) assump-
tion in this field is that current SUs are ?enough like?
real users for training good policies, and that testing
system performance in simulated dialogues is an ac-
curate indication of how a system will perform with
human users. The validity of these assumptions is
49
a critically important open research question. Cur-
rently one of the standard methods for evaluating
the quality of a SU is to run a user simulation on
a real corpus and measure how often the action gen-
erated by the SU agrees with the action observed in
the corpus (Schatzmann et al, 2005; Georgila et al,
2006). This method can certainly give us some in-
sight into how strongly a SU resembles a real user,
but the validity of the metrics used remains an open
research problem. In this paper, we take this a step
further. We measure the quality of user simulation
models for both older and younger users, and show
that these metrics are a good predictor of the be-
haviour of those two user types.
The structure of the paper is as follows: In sec-
tion 2 we describe our data set. In section 3 we
discuss the differences between older and younger
users as measured in our corpus using standard sta-
tistical techniques. Then in section 4 we present our
user simulations. Finally in section 5 we present our
conclusions and propose future work.
2 The Corpus
The dialogue corpus which our simulations are
based on was collected during a controlled experi-
ment where we systematically varied: (1) the num-
ber of options that users were presented with (one
option, two options, four options); (2) the confirma-
tion strategy employed (explicit confirmation, im-
plicit confirmation, no confirmation). The combina-
tion of these 3? 3 design choices yielded 9 different
dialogue systems.
Participants were asked to schedule a health care
appointment with each of the 9 systems, yielding a
total of 9 dialogues per participant. System utter-
ances were generated using a simple template-based
algorithm and synthesised using the speech synthe-
sis system Cerevoice (Aylett et al, 2006), which has
been shown to be intelligible to older users (Wolters
et al, 2007). The human wizard took over the func-
tion of the speech recognition, language understand-
ing, and dialogue management components.
Each dialogue corresponded to a fixed schema:
First, users arranged to see a specific health care pro-
fessional, then they arranged a specific half-day, and
finally, a specific half-hour time slot on that half-day
was agreed. In a final step, the wizard confirmed the
appointment.
The full corpus consists of 447 dialogues; 3 di-
alogues were not recorded. A total of 50 partici-
pants were recruited, of which 26 were older (50?
85) and 24 were younger (20?30). The older users
contributed 232 dialogues, the younger ones 215.
Older and younger users were matched for level of
education and gender.
All dialogues were transcribed orthographically
and annotated with dialogue acts and dialogue con-
text information. Using a unique mapping, we as-
sociate each dialogue act with a ?speech act, task?
pair, where the speech act is task independent and
the task corresponds to the slot in focus (health pro-
fessional, half-day or time slot). For each dialogue,
five measures of dialogue quality were recorded: ob-
jective task completion, perceived task completion,
appointment recall, length (in turns), and detailed
user satisfaction ratings. A detailed description of
the corpus design, statistics, and annotation scheme
is provided in (Georgila et al, 2008).
Our analysis of the corpus shows that there are
clear differences in the way users interact with the
systems. Since it is these differences that good user
simulations need to capture, the most relevant find-
ings for the present study are summarised in the next
section.
3 Older vs. Younger Users
Since the user simulations (see section 4) are based
mainly on dialogue act annotations, we will use
speech act statistics to illustrate some key differ-
ences in behaviour between older and younger users.
User speech acts were grouped into four categories
that are relevant to dialogue management: speech
acts that result in grounding (ground), speech acts
that result in confirmations (confirm) (note, this
category overlaps with ground and occurs after the
system has explicitly or implicitly attempted to con-
firm the user?s response), speech acts that indicate
user initiative (init), and speech acts that indi-
cate social interaction with the system (social).
We also computed the average number of different
speech act types used, the average number of speech
act tokens, and the average token/type ratio per user.
Results are given in Table 1.
There are 28 distinct user speech acts (Georgila et
al., 2008). Older users not only produce more indi-
vidual speech acts, they also use a far richer variety
of speech acts, on average 14 out of 28 as opposed to
9 out of 28. The token/type ratio remains the same,
however. Although the absolute frequency of confir-
mation and grounding speech acts is approximately
50
Variable Older Younger Sig.
# speech act types 14 9 ***
# speech act tokens 126 73 ***
Sp. act tokens/types 8.7 8.5 n.s.
# Confirm 31 30 n.s.
% Confirm 28.3 41.5 ***
# Ground 33 30 n.s.
% Ground 29.4 41.7 ***
# Social 26 5 ***
% Social 17.9 5.3 ***
# Init 15 3 ***
% Init 9.0 3.4 **
Table 1: Behaviour of older vs. younger users. Numbers
are summed over all dialogues and divided by the num-
ber of users. *: p<0.01, **: p<0.005, ***: p<0.001 or
better.
the same for younger and older users, the relative
frequency of these types of speech acts is far lower
for older than for younger users, because older users
are far more likely to take initiative by providing ad-
ditional information to the system and speech acts
indicating social interaction. Based on this analysis
alone, we would predict that user simulations trained
on younger users only will not fare well when tested
on older users, because the behaviour of older users
is richer and more complex.
But do older and younger users constitute two
separate groups, or are there older users that be-
have like younger ones? In the first case, we can-
not use data from older people to create simulations
of younger users? behaviour. In the second case,
data from older users might be sufficient to approx-
imately cover the full range of behaviour we see in
the data. The boxplots given in Fig. 1 indicate that
the latter is in fact true. Even though the means
differ considerably between the two groups, older
users? behaviour shows much greater variation than
that of younger users. For example, for user initia-
tive, the main range of values seen for older users
includes the majority of values observed for younger
users.
4 User Simulations
We performed 5-fold cross validation ensuring that
there was no overlap in speakers between different
folds. Each user utterance corresponds to a user ac-
tion annotated as a list of ?speech act, task? pairs.
For example, the utterance ?I?d like to see the di-
abetes nurse on Thursday morning? could be an-
notated as [(accept info, hp), (provide info, half-
Figure 1: Relative frequency of (a) grounding and (b)
user initiative.
day)] or similarly, depending on the previous sys-
tem prompt. There are 389 distinct actions for older
people and 125 for younger people. The actions of
the younger people are a subset of the actions of the
older people.
We built n-grams of system and user actions with
n varying from 2 to 5. Given a history of system and
user actions (n-1 actions) the SU generates an action
based on a probability distribution learned from the
training data (Georgila et al, 2006). We tested four
values of n, 2, 3, 4, and 5. For reasons of space, we
only report results from 3-grams because they suffer
less from data sparsity than 4- and 5-grams and take
into account larger contexts than 2-grams. However,
results are similar for all values of n.
The actions generated by our SUs were compared
to the actions observed in the corpus using five met-
rics proposed in the literature (Schatzmann et al,
2005; Georgila et al, 2006): perplexity (PP), preci-
sion, recall, expected precision and expected recall.
While precision and recall are calculated based on
the most likely action at a given state, expected pre-
cision and expected recall take into account all pos-
sible user actions at a given state. Details are given
in (Georgila et al, 2006). In our cross-validation
experiments, we used three different sources for the
training and test sets: data from older users (O), data
51
PP Prec Rec ExpPrec ExpRec
O-O 18.1 42.8 39.8 56.0 49.4
Y-O 19.6 34.2 25.1 53.4 40.7
A-O 18.7 41.1 35.9 58.9 49.0
O-Y 5.7 44.8 60.6 66.3 73.4
Y-Y 3.7 50.5 54.1 73.1 70.4
A-Y 3.8 45.8 58.5 70.5 73.0
O-A 10.3 43.7 47.2 60.3 58.0
Y-A 9.3 40.3 33.3 62.0 51.5
A-A 9.3 43.2 43.4 63.9 57.9
Table 2: Results for 3-grams and different combinations
of training and test data. O: older users, Y: younger users,
A: all users.
from younger users (Y), and data from all users (A).
Our results are summarised in Table 2.
We find that models trained on younger users, but
tested on older users (Y-O) perform worse than mod-
els trained on older users / all users and tested on
older users (O-O, A-O). Thus, models of the be-
haviour of younger users cannot be used to simulate
older users. In addition, models which are trained
on older users tend to generalise better to the whole
data set (O-A) than models trained only on younger
users (Y-A). These results are in line with our sta-
tistical analysis, which showed that the behaviour
of younger users appears to be a subset of the be-
haviour of older users. All results are statistically
significant at p<0.05 or better.
5 Conclusions
In this paper we built user simulations for older
and younger adults and evaluated them using stan-
dard metrics. Our results suggest that SUs trained
on older people may also cover the behaviour of
younger users, but not vice versa. This finding
supports the principle of ?inclusive design? (Keates
and Clarkson, 2004): designers should consider a
wide range of users when developing a product for
general use. Furthermore, our results agree with
predictions based on statistical analysis of our cor-
pus. They are also in line with findings of tests of
deployed Interactive Voice Response systems with
younger and older users (Dulude, 2002), which
show the diversity of older people?s behaviour.
Therefore, we have shown that standard metrics for
evaluating SUs are a good predictor of the behaviour
of our two user types. Overall, the metrics we used
yielded a clear and consistent picture. Although our
result needs to be verified on similar corpora, it has
an important implication for corpus design. In order
to yield realistic models of user behaviour, we need
to gather less data from students, and more data from
older and middle-aged users.
In our future work, we will perform more detailed
statistical analyses of user behaviour. In particular,
we will analyse the effect of dialogue strategies on
behaviour, experiment with different Bayesian net-
work structures, and use the resulting user simula-
tions to learn dialogue strategies for both older and
younger users as another way for testing the accu-
racy of our user models and validating our results.
Acknowledgements
This research was supported by the Wellcome Trust VIP
grant and the Scottish Funding Council grant MATCH
(HR04016). We would like to thank Robert Logie and
Sarah MacPherson for contributing to the design of the
original experiment, Neil Mayo and Joe Eddy for coding
the Wizard-of-Oz interface, Vasilis Karaiskos and Matt
Watson for collecting the data, and Melissa Kronenthal
for transcribing the dialogues.
References
M. Aylett, C. Pidcock, and M.E. Fraser. 2006. The
Cerevoice Blizzard Entry 2006: A prototype database
unit selection engine. In Proc. BLIZZARD Challenge.
S. Czaja and C. Lee. 2007. The impact of aging on ac-
cess to technology. Universal Access in the Informa-
tion Society (UAIS), 5:341?349.
L. Dulude. 2002. Automated telephone answering sys-
tems and aging. Behaviour Information Technology,
21:171?184.
K. Georgila, J. Henderson, and O. Lemon. 2006. User
simulation for spoken dialogue systems: Learning and
evaluation. In Proc. Interspeech/ICSLP.
K. Georgila, M. Wolters, V. Karaiskos, M. Kronenthal,
R. Logie, N. Mayo, J. Moore, and M. Watson. 2008.
A fully annotated corpus for studying the effect of cog-
nitive ageing on users? interactions with spoken dia-
logue systems. In Proc. LREC.
S. Keates and J. Clarkson. 2004. Inclusive Design.
Springer, London.
N. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialog
management for robots. In Proc. ACL.
J. Schatzmann, K. Georgila, and S. Young. 2005. Quan-
titative evaluation of user simulation techniques for
spoken dialogue systems. In Proc. SIGdial.
M. Wolters, P. Campbell, C. DePlacido, A. Liddell, and
D. Owens. 2007. Making synthetic speech accessi-
ble to older people. In Proc. Sixth ISCA Workshop on
Speech Synthesis, Bonn, Germany.
52
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 301?304,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Validating the web-based evaluation of NLG systems
Alexander Koller
Saarland U.
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Donna Byron
Northeastern U.
dbyron@ccs.neu.edu
Justine Cassell
Northwestern U.
justine@northwestern.edu
Robert Dale
Macquarie U.
Robert.Dale@mq.edu.au
Sara Dalzel-Job
U. of Edinburgh
S.Dalzel-Job@sms.ed.ac.uk
Jon Oberlander
U. of Edinburgh
Johanna Moore
U. of Edinburgh
{J.Oberlander|J.Moore}@ed.ac.uk
Abstract
The GIVE Challenge is a recent shared
task in which NLG systems are evaluated
over the Internet. In this paper, we validate
this novel NLG evaluation methodology by
comparing the Internet-based results with
results we collected in a lab experiment.
We find that the results delivered by both
methods are consistent, but the Internet-
based approach offers the statistical power
necessary for more fine-grained evaluations
and is cheaper to carry out.
1 Introduction
Recently, there has been an increased interest in
evaluating and comparing natural language gener-
ation (NLG) systems on shared tasks (Belz, 2009;
Dale and White, 2007; Gatt et al, 2008). However,
this is a notoriously hard problem (Scott and Moore,
2007): Task-based evaluations with human experi-
mental subjects are time-consuming and expensive,
and corpus-based evaluations of NLG systems are
problematic because a mismatch between human-
generated output and system-generated output does
not necessarily mean that the system?s output is
inferior (Belz and Gatt, 2008). This lack of evalua-
tion methods which are both effective and efficient
is a serious obstacle to progress in NLG research.
The GIVE Challenge (Byron et al, 2009) is a
recent shared task which takes a third approach to
NLG evaluation: By connecting NLG systems to
experimental subjects over the Internet, it achieves
a true task-based evaluation at a much lower cost.
Indeed, the first GIVE Challenge acquired data
from over 1100 experimental subjects online. How-
ever, it still remains to be shown that the results
that can be obtained in this way are in fact com-
parable to more established task-based evaluation
efforts, which are based on a carefully selected sub-
ject pool and carried out in a controlled laboratory
environment. By accepting connections from arbi-
trary subjects over the Internet, the evaluator gives
up control over the subjects? behavior, level of lan-
guage proficiency, cooperativeness, etc.; there is
also an issue of whether demographic factors such
as gender might skew the results.
In this paper, we provide the missing link by
repeating the GIVE evaluation in a laboratory en-
vironment and comparing the results. It turns out
that where the two experiments both find a signif-
icant difference between two NLG systems with
respect to a given evaluation measure, they always
agree. However, the Internet-based experiment
finds considerably more such differences, perhaps
because of the higher number of experimental sub-
jects (n = 374 vs. n = 91), and offers other oppor-
tunities for more fine-grained analysis as well. We
take this as an empirical validation of the Internet-
based evaluation of GIVE, and propose that it can
be applied to NLG more generally. Our findings
are in line with studies from psychology that indi-
cate that the results of web-based experiments are
typically consistent with the results of traditional
experiments (Gosling et al, 2004). Nevertheless,
we do find and discuss some effects of the uncon-
trolled subject pool that should be addressed in
future Internet-based NLG challenges.
2 The GIVE Challenge
In the GIVE scenario (Byron et al, 2009), users
try to solve a treasure hunt in a virtual 3D world
that they have not seen before. The computer has
complete information about the virtual world. The
challenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
From the perspective of the users, GIVE con-
sists in playing a 3D game which they start from
a website. The game displays a virtual world and
allows the user to move around in the world and
manipulate objects; it also displays the generated
301
instructions. The first room in each game is a tuto-
rial room in which users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Players
can either finish a game successfully, lose it by
triggering an alarm, or cancel the game at any time.
When a user starts the game, they are randomly
connected to one of the three worlds and one of the
NLG systems. The GIVE-1 Challenge evaluated
five NLG systems, which we abbreviate as A, M,
T, U, and W below. A running GIVE NLG system
has access to the current state of the world and to
an automatically computed plan that tells it what
actions the user should perform to solve the task. It
is notified whenever the user performs some action,
and can generate an instruction and send it to the
client for display at any time.
3 The experiments
The web experiment. For the GIVE-1 challenge,
1143 valid games were collected over the Internet
over the course of three months. These were dis-
tributed over three evaluation worlds (World 1: 374,
World 2: 369, World 3: 400). A game was consid-
ered valid if the game client didn?t crash, the game
wasn?t marked as a test run by the developers, and
the player completed the tutorial.
Of these games, 80% were played by males and
10% by females (the remaining 10% of the partic-
ipants did not specify their gender). The players
were widely distributed over countries: 37% con-
nected from IP addresses in the US, 33% from
Germany, and 17% from China; the rest connected
from 45 further countries. About 34% of the par-
ticipants self-reported as native English speakers,
and 62% specified a language proficiency level of
at least ?expert? (3 on a 5-point scale).
The lab experiment. We repeated the GIVE-1
evaluation in a traditional laboratory setting with
91 participants recruited from a college campus.
In the lab, each participant played the GIVE game
once with each of the five NLG systems. To avoid
learning effects, we only used the first game run
from each subject in the comparison with the web
experiment; as a consequence, subjects were dis-
tributed evenly over the NLG systems. To accom-
modate for the much lower number of participants,
the laboratory experiment only used a single game
world ? World 1, which was known from the online
version to be the easiest world.
Among this group of subjects, 93% self-rated
their English proficiency as ?expert? or better; 81%
were native speakers. In contrast to the online ex-
periment, 31% of participants were male and 65%
were female (4% did not specify their gender).
Results: Objective measures. The GIVE soft-
ware automatically recorded data for five objec-
tive measures: the percentage of successfully com-
pleted games and, for the successfully completed
games, the number of instructions generated by
the NLG system, of actions performed by the user
(such as pushing buttons), of steps taken by the
user (i.e., actions plus movements), and the task
completion time (in seconds).
Fig. 1 shows the results for the objective mea-
sures collected in both experiments. To make the
results comparable, the table for the Internet ex-
periment only includes data for World 1. The task
success rate is only evaluated on games that were
completed successfully or lost, not cancelled, as
laboratory subjects were asked not to cancel. This
brings the number of Internet subjects to 322 for
the success rate, and to 227 (only successful games)
for the other measures.
Task success is the percentage of successfully
completed games; the other measures are reported
as means. The chart assigns systems to groups A
through C or D for each evaluation measure. Sys-
tems in group A are better than systems in group
B, and so on; if two systems have no letter in com-
mon, the difference between them is significant
with p < 0.05. Significance was tested using a ?
2
-
test for task success and ANOVAs for instructions,
steps, actions, and seconds. These were followed
by post hoc tests (pairwise ?
2
and Tukey) to com-
pare the NLG systems pairwise.
Results: Subjective measures. Users were
asked to fill in a questionnaire collecting subjec-
tive ratings of various aspects of the instructions.
For example, users were asked to rate the overall
quality of the direction giving system (on a 7-point
scale), the choice of words and the referring ex-
pressions (on 5-point scales), and they were asked
whether they thought the instructions came at the
right time. Overall, there were twelve subjective
measures (see (Byron et al, 2009)), of which we
only present four typical ones for space reasons.
For each question, the user could choose not to
answer. On the Internet, subjects made consider-
able use of this option: for instance, 32% of users
302
Objective Measures Subjective Measures
task
success
instructions steps actions seconds overall
choice
of words
referring
expressions
timing
A 91% A 83.4 B 99.8 A 9.4 A 123.9 A 4.7 A 4.7 A 4.7 A 81% A
M 76% B 68.1 A 145.1 B 10.0 AB 195.4 BC 3.8 AB 3.8 B 4.0 B 70% ABC
T 85% AB 97.8 C 142.1 B 9.7 AB 174.4 B 4.4 B 4.4 AB 4.3 AB 73% AB
U 93% AB 99.8 C 142.6 B 10.3 B 194.0 BC 4.0 B 4.0 B 4.0 B 51% C
W 24% C 159.7 D 256.0 C 9.6 AB 234.1 C 3.8 AB 3.8 B 4.2 AB 50% BC
A 100% A 78.2 AB 93.4 A 9.9 A 143.9 A 5.7 A 4.7 A 4.8 A 92% A B
M 95% A 66.3 A 141.8 B 10.5 A 211.8 B 5.4 A 3.8 B 4.3 A 95% A B
T 93% A 107.2 CD 134.6 B 9.6 A 205.6 B 4.9 A 4.5 A B 4.4 A 64% A B
U 100% A 88.8 BC 128.8 B 9.8 A 195.1 AB 5.7 A 4.7 A 4.3 A 100% A
W 17% B 134.5 D 213.5 C 10.0 A 252.5 B 5.0 A 4.5 A B 4.0 A 100% B
Figure 1: Objective and selected subjective measures on the web (top) and in the lab (bottom).
didn?t fill in the ?overall evaluation? field of the
questionnaire. In the laboratory experiment, the
subjects were asked to fill in the complete question-
naire and the response rate is close to 100%.
The results for the four selected subjective mea-
sures are summarized in Fig. 1 in the same way as
the objective measures. Also as above, the table
is based only on successfully completed games in
World 1. We will justify this latter choice below.
4 Discussion
The primary question that interests us in a compar-
ative evaluation is which NLG systems performed
significantly better or worse on any given evalua-
tion measure. In the experiments above, we find
that of the 170 possible significant differences (=
17 measures ? 10 pairs of NLG systems), the labo-
ratory experiment only found six that the Internet-
based experiment didn?t find. Conversely, there
are 26 significant differences that only the Internet-
based experiment found. But even more impor-
tantly, all pairwise rankings are consistent across
the two evaluations: Where both systems found a
significant difference between two systems, they al-
ways ranked them in the same order. We conclude
that the Internet experiment provides significance
judgments that are comparable to, and in fact more
precise than, the laboratory experiment.
Nevertheless, there are important differences be-
tween the laboratory and Internet-based results. For
instance, the success rates in the laboratory tend
to be higher, but so are the completion times. We
believe that these differences can be attributed to
the demographic characteristics of the participants.
To substantiate this claim, we looked in some detail
at differences in gender, language proficiency, and
questionnaire response rates.
First, the gender distribution differed greatly be-
Web
games reported mean
success 227 = 61% 93% 4.9
lost 92 = 24% 48% 3.4
cancelled 55 = 15% 16% 3.3
Lab
# games reported mean
success 73 = 80% 100% 5.4
lost 18 = 20% 94% 3.3
cancelled 0 ? ?
Figure 2: Skewed results for ?overall evaluation?.
tween the Internet experiment (10% female) and
the laboratory experiment (65% female). This is
relevant because gender had a significant effect
on task completion time (women took longer) and
on six subjective measures including ?overall eval-
uation? in the laboratory. We speculate that the
difference in task completion time may be related
to well-known gender differences in processing
navigation instructions (Moffat et al, 1998).
Second, the two experiments collected data from
subjects of different language proficiencies. While
93% of the participants in the laboratory experi-
ment self-rated their English proficiency as ?expert?
or better, only 62% of the Internet participants did.
This partially explains the lower task success rates
on the Internet, as Internet subjects with English
proficiencies of 3?5 performed significantly better
on ?task success? than the group with proficiencies
1?2. If we only look at the results of high-English-
proficiency subjects on the Internet, the success
rates for all NLG systems except W rise to at least
86%, and are thus close to the laboratory results.
Finally, the Internet data are skewed by the ten-
dency of unsuccessful participants to not fill in the
questionnaire. Fig. 2 summarizes some data about
the ?overall evaluation? question. Users who didn?t
complete the task successfully tended to judge the
303
systems much lower than successful users, but at
the same time tended not to answer the question
at all. This skew causes the mean subjective judg-
ments across all Internet subjects to be artificially
high. To avoid differences between the laboratory
and the Internet experiment due to this skew, Fig. 1
includes only judgments from successful games.
In summary, we find that while the two experi-
ments made consistent significance judgments, and
the Internet-based evaluation methodology thus
produces meaningful results, the absolute values
they find for the individual evaluation measures
differ due to the demographic characteristics of the
participants in the two studies. This could be taken
as a possible deficit of the Internet-based evalua-
tion. However, we believe that the opposite is true.
In many ways, an online user is in a much more
natural communicative situation than a laboratory
subject who is being discouraged from cancelling
a frustrating task. In addition, every experiment ?
whether in the laboratory or on the Internet ? suf-
fers from some skew in the subject population due
to sampling bias; for instance, one could argue that
an evaluation that is based almost exclusively on na-
tive speakers in universities leads to overly benign
judgments about the quality of NLG systems.
One advantage of the Internet-based approach
to data collection over the laboratory-based one is
that, due to the sheer number of subjects, we can de-
tect such skews and deal with them appropriately.
For instance, we might decide that we are only
interested in the results from proficient English
speakers and ignore the rest of the data; but we
retain the option to run the analysis over all partici-
pants, and to analyze how much each system relies
on the user?s language proficiency. The amount
of data also means that we can obtain much more
fine-grained comparisons between NLG systems.
For instance, the second and third evaluation world
specifically exercised an NLG system?s abilities to
generate referring expressions and navigation in-
structions, respectively, and there were significant
differences in the performance of some systems
across different worlds. Such data, which is highly
valuable for pinpointing specific weaknesses of a
system, would have been prohibitively costly and
time-consuming to collect with laboratory subjects.
5 Conclusion
In this paper, we have argued that carrying out task-
based evaluations of NLG systems over the Internet
is a valid alternative to more traditional laboratory-
based evaluations. Specifically, we have shown
that an Internet-based evaluation of systems in the
GIVE Challenge finds consistent significant differ-
ences as a lab-based evaluation. While the Internet-
based evaluation suffers from certain skews caused
by the lack of control over the subject pool, it does
find more differences than the lab-based evaluation
because much more data is available. The increased
amount of data also makes it possible to compare
the quality of NLG systems across different evalua-
tion worlds and users? language proficiency levels.
We believe that this type of evaluation effort
can be applied to other NLG and dialogue tasks
beyond GIVE. Nevertheless, our results also show
that an Internet-based evaluation risks certain kinds
of skew in the data. It is an interesting question for
the future how this skew can be reduced.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
A. Belz. 2009. That?s nice ... what can you do with it?
Computational Linguistics, 35(1):111?118.
D. Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale,
J. Moore, and J. Oberlander. 2009. Report on the
First NLG Challenge on Generating Instructions in
Virtual Environments (GIVE). In Proceedings of the
12th European Workshop on Natural Language Gen-
eration (Special session on Generation Challenges).
R. Dale and M. White, editors. 2007. Proceedings
of the NSF/SIGGEN Workshop for Shared Tasks and
Comparative Evaluation in NLG, Arlington, VA.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA
challenge 2008: Overview and evaluation results.
In Proceedings of the 5th International Natural
Language Generation Conference (INLG?08), pages
198?206.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
S. Moffat, E. Hampson, and M. Hatzipantelis. 1998.
Navigation in a ?virtual? maze: Sex differences and
correlation with psychometric measures of spatial
ability in humans. Evolution and Human Behavior,
19(2):73?87.
D. Scott and J. Moore. 2007. An NLG evaluation com-
petition? Eight reasons to be cautious. In (Dale and
White, 2007).
304
A Strategy for Generating Evaluative Arguments .. 
Giuseppe Carenini 
Intelligent Systems Program 
University of Pittsburgh, 
Pittsburgh, PA 15260, USA 
carenini@cs.pitt.edu 
Abstract 
We propose an argumentation strategy for 
generating evaluative arguments that can be 
applied in systems erving as personal assistants 
or advisors. By following guidelines from 
argumentation theory and by employing a 
quantitative model of the user's preferences, the 
strategy generates arguments hat are tailored to 
the user, properly arranged and concise. Our 
proposal extends the scope of previous 
approaches both in terms of types of arguments 
generated, and in terms of compliance with 
principles from argumentation theory. 
Introduction 
Arguing involves an intentional communicative 
act that attempts to create, change or reinforce 
the beliefs and attitudes of another person. 
Factual and causal arguments attempt to affect 
beliefs (i.e. assessments that something is or is 
not the case), whereas evaluative arguments 
attempt to affect attitudes (i.e., evaluative 
tendencies typically phrased in terms of like and 
dislike or favor and disfavor). 
With the ever growing use of the Web, an 
increasing number of systems that serve as 
personal assistants, advisors, or sales assistants 
are becoming available online ~. These systems 
frequently need to generate evaluative 
arguments for domain entities. For instance, a 
real-estate assistant may need to compare two 
houses, arguing that one would be a better 
choice than the other for its user. 
Argumentation theory (Mayberry and Golden 
1996; Miller and Levine 1996; Corbett and 
Connors 1999) indicates that effective 
arguments should be constructed tbllowing three 
Johanna D. Moore 
The Human Communication Research Centre, 
University of Edinburgh, 
2 Buccleuch Place, Edinburgh EH8 9LW, UK. 
jmoore@cogsci.ed.ac.uk 
general principles. First, arguments hould be 
constructed considering the dispositions of the 
audience towards the information presented. 
Second, sub-arguments supporting or opposing 
the main argument claim should be carefully 
arranged by considering their strength of support 
or opposition. Third, effective arguments should 
be concise, presenting only pertinent and cogent 
information. 
In this paper, we propose an argumentation 
strategy for generating evaluative arguments hat 
can be applied in systems erving as personal 
assistants or advisors. By following principles 
and guidelines from argumentation theory and 
by employing a quantitative model of the user's 
preference, our strategy generates evaluative 
arguments hat are tailored to the user, properly 
arranged and concise. 
Although a preliminary version of our 
argumentative strategy was cursorily described 
in a previous short paper (Carenini and Moore 
1999), this paper includes several additional 
contributions. First, we discuss how the strategy 
is grounded in the argumentation literature. 
Then, we provide details on the measures of 
argument strength and importance used in 
selecting and ordering argument support. Next, 
we generalize the argumentative strategy and 
correct some errors in its preliminary version. 
Finally, we discuss how our strategy extends the 
scope of previous approaches to generating 
evaluative arguments in terms of coverage (i.e., 
types of arguments), and in terms of compliance 
with principles from argumentation theory. 
Because of  space limitations, we only discuss' 
previous work on generating evaluative 
arguments, rather than previous work on 
generating arguments in general. 
See llbr instance www.activebuyersguide.com 
47 
1 Guidelines from Argumentation Theory 
An argumentation strategy specifies what 
content should be included in the argument and 
how it should be arranged. This comprises 
several decisions: what represents supporting (or 
opposing) evidence for the main claim, where to 
position the main claim of the argument; what 
supporting (or opposing) evidence to include 
andhow to order it, and.how to order supp6rfing 
and opposing evidence with respect to each 
other. 
Argumentation theory has developed guidelines 
specifying how these decisions can be 
effectively made (see (Mayberry and Golden 
1996; Miller and Levine 1996; Corbett and 
Connors 1999; McGuire 1968) for details; see 
also (Marcu 1996) for an alternative discussion 
of some of the same guidelines). 
(a) What represents supporting (or opposing) 
evidence for  a claim - Guidelines for this 
decision vary depending on the argument type. 
Limiting our analysis to evaluative arguments, 
argumentation theory indicates that supporting 
and opposing evidence should be identified 
according to a model of the reader's values and 
preferences. For instance, the risk involved in a 
game can be used as evidence for why your 
reader should like the game, only if the reader 
likes risky situations. 
(b) Posit ioning the main claim - Claims are 
often presented up front, usually for the sake of 
clarity. Placing the claim early helps readers 
follow the line of reasoning. However, delaying 
the claim until the end of the argument can be 
effective, particularly when readers are likely to 
find the claim objectionable or emotionally 
shattering. 
(c) Selecting supporting (and opposing) 
evidence - Often an argument cannot mention all 
the available evidence, usually for the sake of 
brevity. Only strong evidence should, be 
presented in detail, whereas weak evidence 
should be either briefly mentioned or omitted 
entirely. 
(d) Arranging/Ordering~supporiing evicleiTce - 
Typically the strongest support should be 
presented first, in order to get at least provisional 
agreement from the reader early on. If at all 
possible, at least one very effective piece of 
supporting evidence should be saved for the end 
of the argument, inorder to leave the reader with 
a final impression of the argument's strength. 
This guideline proposed in (Mayberry and 
Golden 1996) is a compromise between the 
climax and the anti-climax approaches discussed 
in (McGuire 1968). 
(e) Addressing and ordering the 
counterarguments (opposing evidence) - There 
........ ? ~ar~ . ~three,.~options. ~ .for, :Ihis~ :.ateeision: not ~to . . . .  
mention any counterarguments, to acknowledge 
them without directly refuting them, to 
acknowledge them and directly refuting them. 
Weak counterarguments may be omitted. 
Stronger counterarguments should be briefly 
acknowledged, because that shows the reader 
that you are aware of the issue's complexity; and 
it also contributes to the impression that you are 
reasonable and broad-minded. You may need to 
refute a counterargument once you have 
acknowledged it, if the reader agrees with a 
position substantially different from yours. 
Counterarguments should be ordered to 
minimize their effectiveness: strong ones should 
be placed in the middle, weak ones upfront and 
at the end. 
(09 Ordering supporting and opposing evidence 
- A preferred ordering between supporting and 
opposing evidence appears to depend on 
whether the reader is aware of the opposing 
evidence. If so, the preferred ordering is 
opposing before supporting, and the reverse 
otherwise. 
Although these guidelines provide useful 
information on the types of content to include in 
an evaluative argument and how to arrange it, 
the design of a computational rgumentative 
strategy based on these guidelines requires that 
the concepts mentioned in the guidelines be 
formalized in a coherent computational 
framework. This includes: explicitly 
representing the reader's values and preferences 
(used in guideline a); operationally defining the 
term "objectionable claim v (used in guideline b) 
through a measure of the discrepancy between 
the readerrs-initial positionand-the argument's 
main claim2; providing a measure of evidence 
strength (needed in guidelines c, d, and e); and 
3 An operational definition for "emotionally 
shattering" isoutside the scope of this paper. 
48 
House 
Value 
OBJECTIVES ~OMPONENT VALUE FUNCTIONS 
ATTRIBUTES 
Location ?.y 
.Size 0.8 
0.2 
~.  bTeighborhoo d 
Distance-from- park 
"---- t-of-room 
Storage-space 
xl=nl  0 
xl=n2 0.3 
xl=n3 1 
0=<:x2<:5 1-(1/5" X2) 
X~5 0 
Figure 1 Sample additive multiattribute value function (AMVF) 
representing whether the reader is or is not 
aware of  certain facts (needed in guideline tO. 
2 From Guidelines to the Argumentation 
Strategy 
We assume that the reader's values and 
preferences are represented as an additive 
multiattribute value function (AMVF), a 
conceptualization based on multiattribute utility 
theory (MAUT)(Clemen 1996). Besides being 
widely used in decision theory (where they were 
originally developed), conceptualizations based 
on MAUT have recently become a common 
choice in the field of  user modelling (Jameson, 
Schafer et al 1995). Similar models are also 
used in Psychology, in the study of consumer 
behaviour (Solomon 1998). 
2.1 Background on AMVF 
An AMVF is a model of a person's values and 
preferences with respect o entities in a certain 
class. It comprises a value tree and a set of 
component  value funct ions,  one for each 
attribute of the entity. A value tree is a 
decomposition of the value of an entity into a 
hierarchy of aspects of the entity 3, in which the 
leaves correspond to the entity primitive 
a~ributes (see Figure 1 for a simple value tree in 
the real estate domain). The arcs of the tree are 
weighted to represent he importance of the 
value of  an objective in contributing to the value 
3 In decision theory these aspects are called 
objectives. For consistency with previous work, we 
will follow this terminology in the remainder of the 
paper. 
of its parent in the tree (e.g., in Figure 1 location 
is more than twice as important as size in 
determining the value of a house). Note that the 
sum of the weights at each level is equal to 1. A 
component value function for an attribute 
expresses the preferability of each attribute 
value as a number in the \[0,1\] interval. For 
instance, in Figure 1, neighborhood n2 has 
preferability 0.3, and a distance-from-park of 1 
mile has preferability (1 - (1/5" 1))=0.8. 
Formally, an AMVF predicts the value v(e) of an 
entity e as follows: 
v(e) = v(xl ..... x,) = Y~w, v /x9,  where 
- (x/ ..... x,,) is the vector of attribute values for 
an entity e 
- Vattribute i, v, is the component value 
function, which maps the least preferable x, 
to 0, the most preferable to I, and the other 
x, to values in \[0,1\] 
- w, is the weight for attribute i, with 0_< w, _<1 
and Zw, =1 
- w, is equal to the product of all the weights 
from the root of the value tree to the 
attribute i 
A function vo(e) can also be defined for each 
objective. When applied to an entity, this 
? - function "returns ~the value o f  the entity with 
respect o that objective. For instance, assuming 
the value tree shown in Figure 1, we have: 
v,. . . . . . . . .  (e )  = 
= (0.4 * V~,,h~orhooa (e)) + (0.6 * vl~,~,_/,~,,,_r~rk (e)) 
Thus, given someone's AMVF, it is possible to 
compute how valuable an entity is to that 
49 
individual. Furthermore, it is possible to 
compute how valuable any objective (i.e., any 
aspect of that entity) is for that person. All of  
these values are expressed as a number in the 
interval \[0, i \]. 
2.2 Computational Definition of Concepts 
Mentioned in Guidelines 
Presenting an evaluative argument is an attempt 
to persuade the reader that a value judgment 
applies to a subject. The value judgement, also 
called the argumentative intent, can either be 
positive (in favour of  the subject), or negative 
(against the subject) 4. The subject can be a 
single entity (e.g., "This book is very good"), the 
difference between two entities (e.g., "City-a is 
somewhat better than city-b'), or any other form 
of comparison among entities in a set (e.g., 
"This city is the best in North America"). 
Guideline (a) - Given the reader's AMVF, it is 
straightforward to establish what represent 
supporting or opposing evidence for an 
argument with a given argumentative intent and 
a given subject. In fact, if the argumentative 
intent is positive, objectives for which the 
subject has positive value can be used as 
supporting evidence, whereas objectives for 
which the subject has a negative value can be 
used as opposing evidence (the opposite holds 
when the argumentative intent is negative). The 
value of different subjects is measured as 
follows. If the subject is a single entity e, the 
value of the subject for an objective o is vo(e), 
and it is positive when it is greater than 0.5, the 
midpoint of \[0,1\] (negative otherwise). In 
contrast, if the subject is a comparison between 
two entities (e.g., v(ed > v(e_,)), the value of the 
subject for an objective o is \[vo(e9 - Vo(e,)\], and 
it is positive when it is greater than 0 (negative 
otherwise). 
Guidelines (b) - Since argumentative intent is a 
value judgment, we canreasonab\[y assume that 
instead of  being simply positive or negative, it 
may be specified more precisely as a number in 
the interval \[0,1\] (or as a specification that can 
be normalized in this interval), Then, the term 
4 Arguments can also be neutral. However, in this 
paper we do not discuss arguments with a neutral 
argumentative intent. 
"objectionable claim" can be operationally 
defined. If we introduce a measure-of- 
discrepancy(MD) as the absolute value of the 
difference between the argumentative intent and 
the reader's expected value of the subject before 
the argument is presented (based on her AMVF), 
a claim becomes more and more "objectionable!' 
for a reader as MD moves from 0 to 1. 
,~,. ,:,_.~.uidelin~;,(c) ~(d), (e). ~,:~The,,~strength o? the .... 
evidence in support of (or opposition to) the 
main argument claim is critical in selecting and 
organizing the argument content. To define a 
measure of the strength of support (or 
opposition), we adopt and extend previous work 
on explaining decision theoretic advice based on 
an AMVF. (Klein 1994) presents explanation 
strategies (not based on argumentation theory) to 
justify the preference of one alternative from a 
pair. In these strategies, the compellingness of an 
objective measures the objective's strength in 
determining the overall value difference between 
the two alternatives, other things being equal. 
And an objective is notably-compell ing? (i.e., 
worth mentioning) if it is an outlier in a 
population of objectives with respect to 
compeilingness. The formal definitions are: 
compellingness(o, al a2, refo) = 
= w(o, refo)\[vo(at) - Vo(a2)\], where 
- o is an objective, a /and a2 are alternatives, 
refo is an ancestor of o in the value tree 
- w(o, refo) is the product of the weights of all 
the links from o to refo 
- vo is the component value function for leaf 
objectives (i.e., attributes), and it is the 
recursive evaluation over children(o) for 
nonleaf objectives 
notably-compelling?(o, opop. al, a2, refo) - 
\[ compellingness(o, al a2, refo) \[ >px+ko'x, where 
- o, al, a2 and refo are defined as in the 
previous Def; opop is an objective 
population (e.g., siblings(o)), and I opopl >2 
- pe  opop; xeX = \[compellingness(p, al, a_~, 
refo) l 
- gx is the mean of X, ~x is the standard 
deviation and k is a user-defined constant 
We have defined similar measures for arguing 
the value of a single entity and we named them 
s-compellingness and s-notably-compell ing?. 
50 
An objective can be s-compelling either because 
of its strength or because of its weakness in 
contributing to the value of an alternative. So, if 
m~ measures how much the value of an objective 
contributes to the overall value difference of an 
alternative from the worst possible case 5and m2 
measures how much the value of an objective 
contributes to the overall value difference of the 
is either a single entity or a pair of entities in the 
domain of interest. Root can be any objective in 
the value tree for the evaluation (e.g., the overall 
value of a house, its location, its amenities). 
ArgInt is the argumentative intent of the 
argument, a number in \[0,1 \]. The constant k, part 
of the definitions of notably-compelling? and s- 
notably-compelling?, determines the degree of 
:, .,,alternative ,from., th~_b~st:,possible:~ease,:.~e-: :,~ eoneisenessofithe;argument,,, The~Express-Value 
define s-compellingness a  the greatest of the 
two quantities m~ and m2. Following the 
terminology introduced in the two previous 
Equations we have: 
s-compellingness(o, a, refo) = 
= w(o, refo)\[max\[vo(a) - 0\],'\[1 - vo(a)\]\] 
We give to s-notably-compelling? a definition 
analogous to the one for notably-compelling? 
s-notably-compelling? (o,opop, a, refo) - 
\] s-compellingness(o,a, refo) \[ >~+k~x, 
Guideline 09 - An AMVF does not represent 
whether the reader is or is not aware of certain 
facts. We assume this information is represented 
separately. 
2.3 The Argumentation Strategy 
We have applied the formal definitions 
described in the previous ection to develop the 
argumentative strategy shown in Figure 2. The 
strategy is designed for generating honest and 
balanced arguments, which present an 
evaluation of the subject equivalent to the one 
you would expect he reader to hold according to 
her model of preferences (i.e., the argumentative 
intent is equal to the expected value, so MD=0) 6. 
We now examine the strategy in detail, after 
introducing necessary, terminology. The subject 
5 a,.or~, is an alternative such that Vo v~,(a,,,,r~,)=O, 
whereas abL., is an alternative suchthat Vo vo(abe.?~)=l 
6 An alternative strategy, for generating arguments 
whose argumentative intent was-greater (or lower) 
than the expected value, could also be defined in our 
framework. However, this strategy should boost the 
evaluation of supporting evidence and include only 
weak counterarguments, or hide them overall (the 
opposite if the target value was lower than the 
expected value) 
function, used at the end of the strategy, 
indicates that the objective applied to the subject 
must be realized in natural language with a 
certain argumentative intent. 
In the first part of the strategy, depending on the 
nature of the subject, an appropriate measure of 
evidence strength is assigned, along with the 
appropriate predicate that determines whether a 
piece of evidence is worth mentioning. After 
that, only evidence that is worth mentioning is 
assigned as supporting or opposing evidence by 
comparing its value to the argument intent. In 
the second part, ordering constraints from 
argumentation theory are applied 7. Notice that 
we assume a predicate Aware that is true when 
the user is aware of a certain fact, false 
otherwise. Finally, in the third part of the 
strategy, the argument claim is expressed in 
natural language. The opposing evidence (i.e., 
ContrastingSubObjectives), that must be 
considered, but not in detail, is also expressed in
natural language. In contrast, supporting 
evidence is presented in detail, by recursively 
calling the strategy on each supporting piece of 
evidence. 
2.4 Implementation and Application 
The argumentation strategy has been 
implemented as a set of plan operators. Using 
these operators the Longbow discourse planner 
(Young and Moore 1994) selects and arranges 
the content of the argument. We have applied 
our strategy in a system that serves as a real- 
estate personal assistant (Carenini 2000a). The 
system presents information about houses 
available on the market in graphical format. The 
user explores this information by means of 
interactive techniques, and can request a natural 
7 The steps in the strategy are marked with the 
guideline they are based on. 
51 
Argue(subject, Root, Argint, k ) 
;; ass ignments  and content  select ion 
I f  subject = single-entity = e then SVo, = Vol (e) 
Measure-of-strength = s-compel!ingness 
" Worth-mention? = s-notably-compelling? 
Else I f  subject = e~,e 2 then SVo, = \[%, (e,) - vo, (e2)\] 
Measure-of-strength = compellingness 
Worth-mention? = notably-compelling? 
Eliminate all objectives oil ~ Worth-mention? (o,, siblings(o,), subject, Root) ;guideline(c) 
AllEvidence ~- ehildren(RooO 
AlllnFavor~-- all o \] o e AllEvidence/x (SVo ..~ArglnO ;guideline(a) 
SecondBestObjlnFavor~-second most compelling objective o lo E AlllnFavor 
RemainingObjectiveslnFavor ~- AlllnFavor - SecondBestObjlnFavor 
ContrastingObjectives ~- AllEvidence - AlllnFavor ;guideline(a) 
;; ordering the selected content 
AddOrdering(Root -~AllEvidence) ;; we assume MD=0, so claim is not objectionable ;guideline(b) 
I f  Aware(User, ContrastingObjectives) then ;guideline(f) 
AddOrdering( ContrastingObjectives -~ AlllnFavor) 
Else AddOrdering(ContrastingObjectives ~- A lllnFavor ); 
A ddOrdering( RemainingObjectiveslnFavor -~ SecondBestObjlnFavor ) ;guideline(d) 
Sort(RemainingObjectiveslnFavor," decreasing order according to Measure-of-strength) ;guideline(d) 
Sort(ContrastingObjectives," strong ones in the middle, weak ones upfront and at the end) ;guideline(e) 
;; steps for expressing or further argue the content  
Express-Value(subject, Root, Arglnt) 
For all o ~ AlllnFavor, I f  ~leaffo) then Argue(subject, o SVo, k) 
Else Express-Value(subject, o, SVo) 
For all o E ContrastingObjectives, Express-Value(subject, o, SVo) ;guideline(e) 
Legend: (a -~ b) ~ a preceeds b 
(v~ ~- v 2) ~ vl and v 2 are both positive or negative values 
(see Section O for what this means for d~erent subjects) 
-, . -= Figure 2 The,Argumentation strategy 
52 
language evaluation of any house just by 
dragging the graphical representation of the 
house to a query button. The evaluative 
arguments generated by the system are concise, 
properly arranged and tailored to the user's 
preferences s. For sample arguments generated 
by our strategy see (Carenini 2000b) in this 
proceedings. 
(Elzer, Chu-Carroli et al 1994; Chu-Carroll and 
Carberry 1998) studied the generation of 
evaluative arguments in the context of 
collaborative planning dialogues. Although they 
also adopt a qualitative measure of evidence 
strength, when an evaluation is needed this 
measure is mapped into numerical values so that 
preferences can be compared and combined 
. . . . .  ...= :- . . . . . . .  :,, ~- .-: :-;.~ ~,xnore:.:e:ffeeti~ely:,Rl~ve.~t?,~ittr,,respeet =-~tO:our 
3 Previous Work 
Although considerable research has been 
devoted to study the generation of evaluative 
arguments, all approaches proposed so far are 
limited in the type of evaluative arguments 
generated, and in the extent to which they 
comply with guidelines from argumentation 
literature. 
(Elhadad 1992) investigated a general 
computational framework that covers all aspects 
of generating evaluative arguments of single 
entities, from content selection and structuring to 
fine-grained realization decisions. However, his 
work concentrates on the linguistic aspects. His 
approach to content selection and structuring 
does not provide a measure of evidence strength, 
which is necessary to implement several of the 
guidelines from argumentation literature we 
have examined. 
Other studies have focused more on the process 
of content selection and structuring. However, 
with respect o our proposal, they still suffer 
from some limitations. (Morik 1989) describes a 
system that uses a measure of evidence strength 
to tailor evaluations of hotel rooms to its users. 
However, her system adopts a qualitative 
measure of evidence strength (an ordinal scale 
that appears to range from very-important tonot- 
important). This limits the ability of the system 
to select and arrange argument evidence, 
because qualitative measures only support 
approximate comparisons and are ~ notoriously 
difficult to combine (e.g., how many 
"somewhat-important" pieces of evidence are 
equivalent to. :an #important" .:.piece of.. 
evidence?). 
s The generation of fluent English also required the 
development of microplanning and realization 
components. For lack of space, we do not discuss 
them in this paper. 
approach, this work makes two strong 
simplifying assumptions. It only considers the 
decomposition of the preference for an entity 
into preferences for its primitive attributes (not 
considering that complex preferences frequently 
have a hierarchical structure). Additionally, it 
assumes that the same dialogue turn cannot 
provide both supporting and opposing evidence. 
(Kolln 1995) proposes a framework for 
generating evaluative arguments which is based 
on a quantitative measure of evidence strength. 
Evidence strength is computed on a ~zzy 
hierarchical representation of user preferences. 
Although this fuzzy representation may 
represent a viable alternative to the AMVF we 
have discussed in this paper, Kolln's proposal is 
rather sketchy in describing how his measure of 
strength can be used to select and arrange the 
argument content. 
Finally, (Klein 1994) is the previous work most 
relevant to our proposal. Klein developed a 
framework for generating explanations to justify 
the preference of an entity out of a pair. These 
strategies were not based on argumentation 
theory. As described in Section 2.2, from this 
work, we have adapted a measure of evidence 
strength (i.e., compellingness), and a measure 
that defines when a piece of evidence is worth 
mentioning (i.e., notably-compelling?). 
Conclusions and Future Work  
In this paper, we propose.an argumentation 
strategy that extends? previous research on 
generating evaluative arguments in two ways. 
Our .  strategy -covers ~ the: <generation. : :of 
evaluations of a single entity, as well as 
comparisons between two entities. Furthermore, 
our strategy generates arguments, which are 
concise, properly arranged and tailored to a 
hierarchical model of user's preferences, by 
53 
following a comprehensive set of guidelines 
from argumentation theory. 
Several issues require further investigation. 
First, we plan to generalize our approach to 
more complex models of user preferences. 
Second, although our strategy is based on 
insights from argumentation theory, the ultimate 
arbiter for effectiveness is empirical evaluation. 
Clemen, R. T. (1'996). Making Hard Decisions: an 
introduction to decision analysis. Duxbury Press 
Corbett, E. P. J. and R. J. Connors (1999). Classical 
Rhetoric for the Modern Student, Oxford 
University Press. 
Elhadad, M. (1992). Using Argumentation toControl 
Lexical Choice: A Functional Unification 
Implementation. PhD Thesis, CS. Columbia. NY. 
Therefore, we have~..developed~an+~v.atuation ......... Elzer,.S.,..I_Giatt.-.Carrolk..et.al.(.1994).Recogn&ing 
environment o verify whether arguments 
generated by our strategy actually affect user 
attitudes in the intended irection (Carenini 
2000b). A third area for future work is the 
exploration of techniques to improve the 
coherence of arguments generated by our 
strategy. In the short term, we intend to integrate 
the ordering heuristics uggested in (Reed and 
Long 1997). In the long term, by modelling user 
attention and retention, we intend to enable our 
strategy to assess in a principled way when 
repeating the same information can strengthen 
argument force. Finally, we plan to extend our 
strategy to evaluative arguments for 
comparisons between mixtures of entities and 
set of entities. 
Acknowledgements 
Our thanks go to the members of the Autobrief 
project: S. Roth, N. Green, S. Kerpedjiev and J. 
Mattis. We also thank C. Conati for comments 
on drafts of this paper. This work was supported 
by grant number DAA-1593K0005 from the 
Advanced Research Projects Agency (ARPA). 
Its contents are solely responsibility of the 
authors. 
References 
Carenini, G. (2000a). Evaluating Multimedia 
Interactive Arguments in the Context of Data 
Exploration Tasks. PhD Thesis, Intelligent System 
Program, University of Pittsburgh. 
Carenini, G. (2000b). A Framework to Evaluate 
Evaluative Arguments. Int. Conference on Natural 
Language-Generations. Mitzpe~,Ramon, Israel. 
Carenini, G. and J. Moore (1999). Tailoring 
Evaluative Arguments to User's Preferences. User 
Modelling, Banff; Canada : 299-301. 
Chu-Carroll, J. and S, Carberry (1998). Collaborative 
Response Generation in Planning Dialogues. 
Computational Linguistics 24(2): 355-400. 
and Utilizing User Preferences in Collaborative 
Consultation Dialogues. Proceedings of Fourth 
Int. Conf. of User Modeling. Hyannis, MA: 19-24. 
Jameson, A., R. Schafer, et al (1995). Adaptive 
provision of Evaluation-Oriented Information: 
Tasks and techniques. Proc. of 14th IJCAI. 
Montreal, Canada. 
Klein, D. (1994). Decision Analytic Intelligent 
Systems: Automated Explanation and Knowledge 
Acquisition, Lawrence Erlbaum Associates. 
Kolln, M. E. (1995). Employing User Attitudes in 
Text Planning. 5th European Workshop on Natural 
Language Generation, Leiden, The Netherlands. 
Marcu, D. (1996). The Conceptual and Linguistic 
Facets of Persuasive Arguments. ECAI workshop - 
Gaps and Bridges: New Directions in Planning and 
Natural Language Generation. 
Mayberry, K. J. and R. E. Golden (1996). For 
Argument's Sake: A Guide to Writing Effective 
Arguments, Harper Collins, College Publisher. 
McGuire, W. J. (1968). The Nature of Attitudes and 
Attitudes Change. The Handbook of Social 
Psychology. G. Lindzey and E. Aronson, Addison- 
Wesley. 3: 136-314. 
Miller, M. D. and T. R. Levine (1996). Persuasion. 
An Integrated Approach to Communication Theot T
and Research. M. B. Salwen and D. W. Stack. 
Mahwah, New Jersey: 261-276. 
Morik, K. (1989). User Models and Conversational 
Settings: Modeling the User's Wants. User Models 
in Dialog Systems. A. Kobsa and W. Wahlster, 
Springer-Verlag: 364-385. 
Reed, C. and D. Long (1997). Content Ordering in 
the Generation of Persuasive Discourse. Proc, of 
the 15th IJCAI, Nagoya; Japan. 
Solomon, M. R. (1998). Consumer Behavior: Bzo,ing, 
Having. and Being. ~ Prentice Hall. 
Young, M. R. and J. D. Moore (1994). Does 
Discourse Planning Require a Special-Purpose 
Planner? Proc. of the AAAI-94 Workshop on 
planning for lnteragent Communication. Seattle, 
WA. 
54 
 	
	Robustness versus Fidelity in Natural Language Understanding
Mark G. Core and Johanna D. Moore
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
[markc,jmoore]@inf.ed.ac.uk
Abstract
A number of issues arise when trying to scale-
up natural language understanding (NLU) tools
designed for relatively simple domains (e.g.,
flight information) to domains such as medical
advising or tutoring where deep understanding
of user utterances is necessary. Because the
subject matter is richer, the range of vocabu-
lary and grammatical structures is larger mean-
ing NLU tools are more likely to encounter
out-of-vocabulary words or extra-grammatical
utterances. This is especially true in med-
ical advising and tutoring where users may
not know the correct vocabulary and use com-
mon sense terms or descriptions instead. Tech-
niques designed to improve robustness (e.g.,
skipping unknown words, relaxing grammat-
ical constraints, mapping unknown words to
known words) are effective at increasing the
number of utterances for which an NLU sub-
system can produce a semantic interpretation.
However, such techniques introduce additional
ambiguity and can lead to a loss of fidelity (i.e.,
a mismatch between the semantic interpreta-
tion and what the language producer meant).
To control this trade-off, we propose semantic
interpretation confidence scores akin to speech
recognition confidence scores, and describe our
initial attempt to compute such a score in a
modularized NLU sub-system.
1 Introduction
Applications such as automated medical advice and tu-
toring use rich knowledge representations, and natural
language input to dialogue systems in these domains
contains a wide range of vocabulary and grammatical
structures. Natural language understanding (NLU) tools
may fail when encountering out-of-vocabulary words or
extra-grammatical utterances. Using robustness features
such as skipping unknown words and mapping unknown
words to known words can allow an NLU sub-system to
recover fully from failure, or at least to extract pieces of
meaning that can be used for a directed clarification ques-
tion (see (Gabsdil, 2003; Rose?, 1997) for more details).
Such robustness is especially critical for the domain of
tutoring. Educators are interested in using dialogue to
address conceptual errors instead of focusing on termi-
nology errors, and students want ?partial credit? from tu-
tors if they have the right general idea. Thus, NLU sub-
systems for tutoring must attempt to extract correct ele-
ments from student answers that they do not fully under-
stand.
The problem with such robustness features is that they
introduce additional ambiguity and can lead to a loss of
fidelity (i.e., a mismatch between the semantic interpre-
tation and what the language producer meant) if no clar-
ification or confirmation request is made. We argue that
semantic interpretation confidence scores (akin to speech
recognition confidence scores) are necessary to properly
manage this robustness versus fidelity trade-off. It is not
enough to ask for clarification of missing information;
the dialogue system needs confidence scores so it can de-
cide what information present in the logical form (if any)
should be clarified or confirmed.
In addition to avoiding misunderstandings, dialogue
systems should be sensitive to the lexical and syntactic
choices made by users. Pickering and Garrod (under re-
vision) argue that conversational agents track each other?s
use of referring expressions. In experimental contexts
such as games involving mazes, participants often im-
plicitly agree on a conventional way of referring to en-
tities such as places in the maze. If dialogue systems do
not mimic this alignment process, users can easily be-
come confused if they make a reference such as ?the two
o?clock flight? but the system talks about ?BA 112? in-
stead. In some domains (e.g., tutoring, medical advising),
the process is more complex as users may use common
sense terms or descriptions instead of correct terminol-
ogy. The system should not use incorrect terminology,
but instead teach the correct terms. To enable dialogue
systems to align or correct user-referring-expressions, the
NLU sub-system must provide pointers from the seman-
tic interpretation back to the words and syntax produced
by the user.
In the body of this paper, we present the architecture of
our NLU sub-system. Features of our architecture such as
a packed parse tree representation and a two-stage seman-
tic interpretation process provide efficiency and portabil-
ity advantages. However, they complicate the calculation
of confidence scores and maintenance of links between
the words and syntax produced by the user and the output
of NLU. The goal of this paper is to highlight the archi-
tectural trade-offs involved in controlling fidelity.
2 Our Architecture
Our NLU sub-system, NUBEE, is part of a tutorial dia-
logue system, BEETLE (Basic Electricity and Electron-
ics Tutorial Learning Environment) (Zinn et al, forth-
coming). BEETLE users are given tasks to perform in
the circuit simulator pictured in figure 1. Users manipu-
late objects in this simulation as well as conversing with
the system through typed input. The typed input is sent
to NUBEE which queries the domain reasoner, BEER,
and dialogue history to help build a logical form which it
sends to the dialogue manager of the system, the central
component of BEETLE?s response generation.
NUBEE uses an application-specific logical frame-
work which closely resembles minimal recursion seman-
tics (MRS) (Copestake et al, 1999). An example logical
form is shown in figure 2. The identifiers in square brack-
ets define the handles for each of the three elementary
predications (EPs). Handles are used by one EP to ref-
erence another EP. The first EP, connect
   
, takes the han-
dles of two EPs as arguments (the handles for battery    
and wire
    ). Arguments can be handles or atomic val-
ues. Note, we differentiate the definition of a handle from
a handle reference by marking the former with square
brackets. The two arguments to battery
   
are the atomic
values defNP and singular. To simplify processing we
simply pass on these syntactic features, defNP and sin-
gular, for later processing rather than defining quantifiers
such as the.
In section 5, we describe our two stage interpretation
process. Predicates output by the first stage are marked
with a prime (e.g., connect   ) and predicates output by the
second stage (such as those in figure 2) are marked with
a double prime.
Ideally each EP and each of its individual atomic ele-
ments would have a confidence score (reflecting the sub-
(1) connect the battery to the wire
(*and*
(connect?? [id1] id2 id3)
(battery?? [id2] defNP singular)
(wire?? [id3] defNP singular))
Figure 2: Example logical form
system?s confidence that it captures the speaker?s mean-
ing), and a link back to the syntactic structures corre-
sponding to the predicate or atomic element. Such a
fine-grained representation would ensure that a dialogue
system could separate low fidelity elements from high fi-
delity ones, and that all the speaker?s lexical and gram-
matical choices were captured.
Building such a representation is difficult because the
NLU process consists of a series of tasks: preprocess-
ing (in a typed system, this consists of spelling correc-
tion and unknown word handling), syntactic and semantic
analysis, and reference resolution. Backward links must
be built across these processing steps and each step in-
troduces ambiguity (e.g., the parser will output multiple
possibilities). In section 7, we see that the system?s parser
uses a packed representation for ambiguity. Such a rep-
resentation is efficient but the connection between indi-
vidual syntactic structures and semantic structures is lost
meaning these links must be recreated post-hoc.
NUBEE?s architecture is shown in figure 3. The
spelling correction, parsing, and post-processing com-
ponents were built using the Carmel workbench (Rose?,
2000; Rose? et al, 2003). In our architecture for typed
NLU, speech recognition is replaced by unknown word
handling and spelling correction which we discuss in sec-
tions 3 and 4. In these modules, it is relatively easy to
calculate confidence scores and record the transforma-
tions made to the user input. These modules can make
dramatic changes to the user input so it is unclear why
current NLU sub-systems do not track these transforma-
tions.
In section 5, we discuss the parsing and post-
processing modules (parts of the Carmel workbench). We
highlight why it is difficult to assign confidence scores
to Carmel?s output and maintain links between logical
predicates and the corresponding words typed by the user.
Section 6 discusses our reference resolution module and
how it calculates confidence scores and records the trans-
formations that it makes (from logical predicates to sim-
ulated objects in the domain reasoner). In section 7, we
discuss how we calculate global confidence scores and
link references to simulated objects back to the user?s re-
ferring expression.
NUBEE
Domain Reasoner
(BEER)
Dialogue
History
Response Generation
Figure 1: NLU-centric diagram of BEETLE
3 Spelling Correction
NUBEE?s spelling corrector (Elmi and Evens, 1998) and
robust parser are both part of the Carmel workbench and
the interface between the two is predefined. The spelling
corrector uses the parser?s lexicon as its dictionary and at-
tempts to fix spelling and typing errors in known words.
Since the parser?s lexicon is typically much smaller than
a lexical database such as WordNet (Miller, 1990), there
is a reduction in token ambiguity (i.e., the number of pos-
sible replacements to consider) but the spelling of un-
known words will not be corrected. The simplification
is also made that known words are never misspelled ver-
sions of other known words (e.g., typing ?their? instead
of ?there?).
The spelling corrector uses string transformations to
attempt to repair spelling/typing errors. Because repeated
transformations will map any input string to a word in
the dictionary, transformations are given penalty scores
and a threshold defines an allowable spelling correction.
However, the spelling corrector?s decisions are final; re-
placements whose penalty scores are below the threshold
are entered directly into the parser?s chart but the penalty
scores are not passed on.
To produce a record of spelling corrector transforma-
tions, we create a word transformation table for every
new utterance. Each transformation is recorded in a ta-
ble entry consisting of the original word, the transformed
word (after spelling correction), and an associated confi-
dence score. We have modified the spelling corrector to
return a confidence score based on the number of alterna-
tives that it proposes. We show below the transformation
table recording the spelling corrector?s output for the mis-
spelled word ?socet?:
Taggerunknown
word
known word
Reference
Resolution
Spelling
Processing
POS
Corrector Lookup
WordNet
known known synonym
Still
Unknown Robust
Parser
Post?
Figure 3: NUBEE architecture
socet -> socket, 0.5
-> set, 0.5
In future work, we plan to modify the spelling correc-
tor so that it outputs its penalty score.
4 Unknown Word Handling
Carmel?s robust parser can skip words while attempt-
ing to find an analysis for the user input. True un-
known words (not spelling errors) will be skipped be-
cause Carmel will have no information about their syn-
tactic or semantic features. For some unknown words,
this is the best solution because they represent concepts
not having an obvious link to knowledge modeled by the
system (users should be alerted to the system?s limita-
tion). However, we are aware of no work on attempting
to recognize novel ways of referring to entities modeled
by the system.
Our approach is to find known synonyms (i.e., in
NUBEE?s lexicon) of unknown words using the Word-
Net lexical database (Miller, 1990). In WordNet, words
are connected to one or more synsets each corresponding
to a distinct concept. Each synset will have a set of hy-
ponymy (all the subtypes of the synset) and hypernymy
(the supertype of the synset) links.
Currently, we use a very simple search process to
look for a known word whose meaning approximates the
meaning of the unknown word. We assign a part-of-
speech (POS) tag to the unknown word, and search the
appropriate WordNet taxonomy. We retrieve the synsets
associated with the word and run the search procedure
SEARCH-WORD stopping when a known word is found.
procedure SEARCH-WORD (SYNSETS)
1. SEARCH-DOWN (SYNSETS)
2. if height threshold not reached then
SEARCH-WORD (hypernyms for SYNSETS)
procedure SEARCH-DOWN (SYNSETS)
1. search all words having a
synset in SYNSETS
2. SEARCH-DOWN (all hyponyms of SYNSETS)
Nodes in the WordNet taxonomy close to its root
have relatively general meanings (e.g., social-relation,
physical-object) so we define a limit (height threshold)
to how far the search can progress up the taxonomy.
To make a record of unknown-word-handling transfor-
mations, we add additional entries to the word transfor-
mation table output by spelling correction. We use the
size of the search space to calculate confidence scores,
treating the set of replacement words retrieved in each
step of the search process as equally likely. Consider
the example of the unknown word ?cable?. Step one
of SEARCH-DOWN returns: ?telegraph?, ?wire?, and
?fasten-with-a-cable?. ?wire? is a known word, and ?ca-
ble? is replaced by ?wire? with a confidence of 0.33:
cable -> wire, 0.33
5 Carmel Workbench
We use the Carmel workbench (Rose?, 2000; Rose? et al,
2003) for parsing and post-processing. In Carmel?s AU-
TOSEM framework:
?semantic interpretation [operates] in parallel
with syntactic interpretation at parse time in a
lexicon driven fashion. ... [Semantic] knowl-
edge is encoded declaratively within a mean-
ing representation specification. Semantic con-
structor functions are compiled automatically
from this specification and then linked into lex-
ical entries? (Rose?, 2000, p. 311).
Carmel comes with a wide-coverage English grammar
that is compatible with the wide-coverage COMLEX lex-
icon (Grishman et al, 1994). For each COMLEX entry
that we wanted to add into NUBEE?s lexicon, we speci-
fied its meaning as shown below for the words ?connect?,
?battery?, and ?wire?.
connect: connect?, subject->agent,
object->theme,
modifier->destination
battery: battery?
wire: wire?
This simplified example of the meaning specification
assigns a predicate to each word, and in the case of a
verb such as ?connect? assigns a mapping from the syn-
tactic roles of subject, object, and modifier to the seman-
tic roles of agent, theme, and destination. This repre-
sentation is domain-independent and reusable; it will al-
ways be the case that the subject of ?connect? realizes the
(2a)
connect the battery to the cable (wire)
(*and*
(frame connect?)
(theme (*and* (*and* (frame battery?)
(number singular))
(def defNP)))
(destination (*and*
(*and* (frame wire?)
(number singular))
(def defNP))))
Figure 4: Example semantic feature value
(2b)
(*and*
(connect?? [id1] id2 id3)
(battery?? [id2] defNP singular)
(wire?? [id3] defNP singular))
Figure 5: Example logical form from section 1
agent, the object realizes the theme, and that the destina-
tion (if present) will be realized as a modifier.
Figure 4 shows a simplified version of the parser?s out-
put given the utterance, ?connect the battery to the ca-
ble?. Recall from section 4 that the unknown word han-
dler will replace ?cable? with ?wire? leading to the wire
 
predicates in figure 4.
The two occurrences of the definite article ?the? trig-
ger the feature values (def defNP), marking that battery  
and wire
 
occurred in definite NPs. The nouns, ?battery?
and ?wire?, have the associated syntactic feature of being
singular, and the features theme and destination mark the
semantic roles of battery
 
and wire
 
.
Dialogue systems use domain-specific reasoners to
process the output of NLU sub-systems (e.g., to answer a
user question or execute a user command, or to judge the
correctness of a student answer). Such domain reasoners
generally expect input in a predefined, domain-specific
format necessitating a second stage of processing to con-
vert the parser?s output into the correct format.
Our domain reasoner?s representation for the connect
action is a predicate, connect
   
, taking as arguments, the
two objects to be connected. Carmel provides support for
building such predicates from the parser?s output based
on a declarative specification. Based on our specification
for connect
   
, Carmel?s predicate mapper will produce the
logical form shown in figure 5 (a copy of figure 2).
During the parsing and post-processing stages, the
string returned from pre-processing (spelling correction
and unknown word handling) is transformed into a series
of predicates. We currently do not keep track of all the
connections between the predicates and the words that
formed them. The predicate mapping stage is difficult to
(2c) (*or* (connect?? [id4] "|I|BATT1"
"|I|WIRE-1461")
(connect?? [id5] "|I|BATT1"
"|I|WIRE-1441")
(connect?? [id6] "|I|BATT1"
"|I|WIRE-1451")
(connect?? [id7] "|I|BATT1"
"|I|REDLEAD1")
(connect?? [id8] "|I|BATT1"
"|I|BLACKLEAD1"))
Figure 6: Example logical form after reference resolution
unravel; the mapping rules operate on semantic feature
values (such as those shown in figure 4). There is no di-
rect link between pieces of semantic feature values and
the words that triggered them so it is difficult to associate
the output of predicate mapping with words. See section
7 for more details and our interim solution.
6 Reference Resolution
For each predicate corresponding to a physical entity,
the reference resolution module must decide whether the
predicate refers to: a concept (a generic reading), a novel
object (indefinite reference), or an existing object (defi-
nite reference). If the predicate refers to an existing ob-
ject, the predicate (e.g., wire     ) may match several objects
in the domain reasoner but the speaker may only be refer-
ring to a subset of these objects.
Figure 6 shows the example from figure 5 after refer-
ence resolution. The predicate battery
   
is replaced by
the name of the one battery present in figure 1, but wire
   
could refer to any of the five wires in the circuit leading
to the ambiguity depicted.
NUBEE can query the dialogue system?s history list
to assign salience and calculate confidence scores for
the transformations it makes. These transformations are
stored in a table such as the one below (assume that
"|I|WIRE-1461" had been mentioned previously but
the other wires had not):
(battery?? [id2] def singular) ->
"|I|BATT1" (1.0)
(wire?? [id3] def singular) ->
"|I|WIRE-1461" (0.6) "|I|WIRE-1441" (0.1)
"|I|WIRE-1451" (0.1) "|I|REDLEAD1" (0.1)
"|I|BLACKLEAD1" (0.1)
In the next section, we will see how these table entries
are matched with words from the input.
7 Improving Fidelity
We are currently focusing on improving fidelity for re-
ferring expressions: assigning confidence scores to the
objects retrieved during reference resolution and linking
referenced objects to the words used to refer to them. We
make the assumption that all referring expressions are
NPs and build a table of predicates (formed from NPs)
and the words associated with those NPs. A sample entry
is shown below.
(wire?? [id2] def singular)
<= ??the wire??
We run the following procedure on each NP in the one
parse tree node covering the input (in the parser?s packed
representation there will only be one such node with the
category utterance).
procedure PROCESS-NP (NP)
1. run the predicate mapper just on
NP to get its associated predicate
2. follow the children of NP to find
the words associated with it
The next step is consulting the spelling corrector and
unknown word handler. In section 3, we introduced a
table of word substitutions with associated confidence
scores. This table can be used to replace the words found
in the chart with the words actually typed by the user and
to compute a global confidence score by multiplying the
confidence scores of the individual words with the confi-
dence score assigned during reference resolution.
In section 4, we discussed unknown word handling for
?the cable?; combining this result with the reference table
computed above gives us:
"|I|WIRE-1461",
(wire?? [id3] def singular),
0.198, ??the cable??
"|I|WIRE-1441",
(wire?? [id3] def singular),
0.033, ??the cable??
"|I|WIRE-1451",
(wire?? [id3] def singular),
0.033, ??the cable??
"|I|REDLEAD1",
(wire?? [id3] def singular),
0.033, ??the cable??
"|I|BLACKLEAD1",
(wire?? [id3] def singular),
0.033, ??the cable??
One complication is that NPs can be associated with
an ambiguous set of words. Consider a nonsense word
in our domain, ?waters?. The spelling corrector will pro-
pose ?watts? and ?meters? as possible replacements. In
the parser?s packed representation, the nouns ?watts? and
?meters? share the same node. A disjunctive set of fea-
tures represents the ambiguity.
(*or*
(*and* (frame watts?)
(number plural)
(root watt))
(*and* (frame meters?)
(number plural)
(root meter)))
This ambiguity is propagated to the NP node but the
connection between the word stems (the root feature) and
the two NP meanings is lost.
(*or*
(*and* (frame watts?)
(number plural)
(def indef))
(*and* (frame meters?)
(number plural)
(def indef)))
We modified PROCESS-NP to deal with such cases by
adding an additional step:
3. for each meaning, M of the NP
3.1 try to match M with one or
more of the words associated
with the NP (i.e., run the
predicate mapper just on
the word and see if it matches
one of the meanings of the NP)
8 Discussion
Although there has been work on controlling the fidelity
of individual components of the pipeline shown in figure
3, there has been little work considering the NLU sub-
system as a whole. Gabsdil and Bos (2003) incorporate
(speech recognizer) confidence scores into their logical
form for elements that correspond directly to words in the
input (rather than larger structures built through compo-
sition). Consider the example of the word ?manager? and
assume it has a speech recognition confidence score of
0.65. Gabsdil and Bos? parser will assign ?manager? the
semantic value of
 
:MANAGER(  ) where   is a handle
and  a variable. This semantic value is given a con-
fidence score of 0.65 the same as ?manager?. To com-
pute confidence scores for larger constituents they sug-
gest to ?combine confidence scores for sub-formulas re-
cursively? (Gabsdil and Bos, 2003, p. 149).
We have taken this idea further and explored the is-
sues involved in computing confidence scores for larger
constituents. Some of these issues are linked to our two-
stage semantic analysis. However, Carmel?s two-stage
interpretation process (i.e., a domain-independent parsing
stage and a domain-dependent predicate mapping stage)
is not idiosyncratic to the Carmel workbench. Dzikovska
et al (2002) adopt such a two stage approach because
their NLU sub-system is used in multiple domains (e.g.,
transportation planning, medication advice) necessitating
reuse of resources wherever possible. Milward (2000)
uses a two stage approach because it increases robust-
ness. When the parser is not able to build a parse tree
covering the entire input, there will still be a semantic
chart composed of partial parses and their associated se-
mantic feature values. For the domain of airline flight
information, Milward defines post-processing rules that
scan this semantic chart looking for information such as
departure times. Our goal in this paper was to highlight
the architectural trade-offs of such features on controlling
fidelity.
9 Future Work
Our search process for unknown word handling is rudi-
mentary. Each step of the search procedures described
above returns a set of replacement candidates which are
treated as equally likely. In future work, we plan to re-
vise this search process to use a distance metric such as
one of those discussed in (Budanitsky and Hirst, 2001).
Such distance metrics take into account factors such as
the overall depth of the WordNet taxonomy and the fre-
quency of synsets in a corpus, and will allow us to better
control the search process.
Although we know of no work on using WordNet
to handle unknown words during interpretation, there is
work on using WordNet for lexical variation during gen-
eration. Jing (1998) presents an algorithm for converting
WordNet into a domain-specific taxonomy of replaceable
words. First, words and synsets are removed that do not
appear in a corpus representative of the domain.
The senses of verb arguments in the corpus are disam-
biguated based on the intuition that words appearing as
the same argument to the same verb should have senses
close to each other in WordNet. Consider an example
from Jing?s domain of generating basketball news re-
ports. The verb ?add? takes words such as ?rebound?,
?throw?, and ?shot? as objects. Jing states that ?re-
bound? and ?throw? have senses that are members of the
synset accomplishment-achievement; their other senses
do not align in this manner and are pruned from WordNet.
?shot? has a sense that is a hyponym of accomplishment-
achievement forming a small taxonomy. This process
can be used in reverse for words not in WordNet such
as ?layup?. ?layup? often occurs as an object to ?hit? as
do the known words ?jumper? and ?baskets?. Based on
this information, ?layup? is added to WordNet under the
synset accomplishment-achievement, the synset Jing as-
signs to ?jumper? and ?baskets?.
In future work, we will use such a pruning algorithm
on WordNet, and in addition to such static pruning (done
off-line), we want to try dynamic pruning; e.g., to process
the unknown word ?X?, in the example, ?connect the X
to tab 4?, we should only consider ?connectible? entities
as defined by the system?s ontology.
Another area for future work are the parsing and post-
processing steps (section 5); these steps do not maintain
confidence scores nor input-output links making it dif-
ficult to compute global confidence scores and maintain
links between the words and syntax produced by a user
and the resulting output from NUBEE.
The work discussed in section 7 focuses on referring
expressions realized as NPs. An incremental approach to
improving this work would involve supporting other syn-
tactic categories such as verbs, and modifying NUBEE?s
confidence scores to penalize transformations done dur-
ing parsing and post-processing (i.e., skipping words, use
of disambiguation heuristics).
A more general approach would require changes to
the Carmel workbench. Currently the packed represen-
tation represents ambiguity as a disjunction of semantic
feature values (e.g., figure 4). Each of these feature val-
ues could have an associated confidence score and a list
of the words associated with those semantic feature val-
ues. With such a representation, the post-hoc analysis
discussed in section 7 would not be necessary. We could
compute confidence scores as follows. Typically in sta-
tistical parsing, the probability of a parse-tree node is the
product of the probability of the rule forming the node,
and the probabilities of its children. We could propa-
gate confidence scores for individual words in the same
fashion (making the assumption that all rules are equally
likely).
However, the added complexity and associated in-
creased processing load may not be worth the ability
to associate each element of NUBEE?s output with a
confidence score and syntactic information. We plan
to perform a detailed evaluation to investigate the ef-
fect of tracking fidelity on the generation of clarification
and confirmation requests, and alignment of the dialogue
system with the user (and where necessary, correction).
Evaluation will take two forms: building a test suite from
human-human dialogues in this domain, and analysis of
user interactions with the system.
Acknowledgments
The research presented in this paper is supported by Grant
#N000149910165 from the Office of Naval Research,
Cognitive and Neural Sciences Division. Thanks to Car-
olyn Rose? for releasing the CARMEL Workbench for
public use and for her continual support of this software.
Thanks to Myroslava Dzikovska, Claus Zinn, Carolyn
Rose?, Johan Bos, and our anonymous reviewers for their
comments.
References
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in WordNet: An experimental, application-
oriented evaluation of five measures. In Proc. of the
NAACL-01 Workshop on WordNet and Other Lexical
Resources.
Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pol-
lard. 1999. Minimal Recursion Semantics: An intro-
duction. Draft.
Myroslava O. Dzikovska, James F. Allen, and Mary D.
Swift. 2002. Finding the balance between generic
and domain-specific knowledge: a parser customiza-
tion strategy. In Proc. of the LREC 2002 Workshop on
Customizing Knowledge for NLP Applications.
Mohammad A. Elmi and Martha W. Evens. 1998.
Spelling correction using context. In Proc. of ACL-
98/COLING-98, pages 360?364.
Malte Gabsdil and Johan Bos. 2003. Combining acous-
tic confidence scores with deep semantic analysis for
clarification dialogues. In Proc. of the Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 137?150.
Malte Gabsdil. 2003. Clarification in spoken dialogue
systems. In Proc. of the AAAI Spring Symposium on
Natural Language Generation in Spoken and Written
Dialogue.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
1994. COMLEX syntax: Building a computational
lexicon. In Proc. of the 15    International Conference
on Computational Linguistics (COLING-94).
Hongyan Jing. 1998. Usage of WordNet in natural lan-
guage generation. In Proc. of the COLING-ACL-98
Workshop on Usage of WordNet in Natural Language
Processing Systems.
George Miller. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography, 3(4).
David Milward. 2000. Distributing representation for ro-
bust interpretation of dialogue utterances. In Proc. of
the 38
 

Annual Meeting of the Association for Com-
putational Linguistics (ACL-00).
Martin J. Pickering and Simon Garrod. under revision.
Toward a mechanistic psychology of dialogue. Behav-
ioral and Brain Sciences.
Carolyn P. Rose?, Andy Gaydos, Brian S. Hall, Antonio
Roque, and Kurt VanLehn. 2003. Overcoming the
knowledge engineering bottleneck for understanding
student language input. In Proc. of the 11
 

Interna-
tional Conference on Artificial Intelligence in Educa-
tion (AIED ?03).
Carolyn P. Rose?. 1997. Robust Interactive Dialogue In-
terpretation. Ph.D. thesis, School of Computer Sci-
ence, Carnegie Mellon University.
Carolyn P. Rose?. 2000. A framework for robust semantic
interpretation. In Proc. of the 1    Annual Meeting of
the North American Chapter of the ACL (NAACL-00),
Seatle, pages 311?318.
Claus Zinn, Johanna D. Moore, and Mark G. Core. forth-
coming. Intelligent information presentation for tutor-
ing systems. In Oliviero Stock and Massimo Zanca-
naro, editors, Intelligent Multimodal Information Pre-
sentation. Kluwer.
Automatic Analysis of Plot for Story Rewriting
Harry Halpin
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
H.Halpin@ed.ac.uk
Johanna D. Moore
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
J.Moore@ed.ac.uk
Judy Robertson
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
Scotland, UK
judyr@inf.ed.ac.uk
Abstract
A method for automatic plot analysis of narrative
texts that uses components of both traditional sym-
bolic analysis of natural language and statistical
machine-learning is presented for the story rewrit-
ing task. In the story rewriting task, an exemplar
story is read to the pupils and the pupils rewrite the
story in their own words. This allows them to prac-
tice language skills such as spelling, diction, and
grammar without being stymied by content creation.
Often the pupil improperly recalls the story. Our
method of automatic plot analysis enables the tu-
toring system to automatically analyze the student?s
story for both general coherence and specific miss-
ing events.
1 Introduction
StoryStation is an intelligent tutoring system cre-
ated to provide personalized attention and detailed
feedback to children ages 10-12 on their writing
(Roberston and Wiemar-Hastings, 2002). Writing
is viewed as a skill-based task, with skills being
elements of writing such as spelling, diction, and
plot development. Each writing skill is associated
with an animated agent that provides online help.
Evaluations of StoryStation show that children en-
joy the personalized encouragement and construc-
tive comments that StoryStation provides (Robert-
son and Cross, 2003). StoryStation was designed
by researchers in conjunction with two teachers and
a group of students. However, both students and
teachers indicated StoryStation would be signifi-
cantly improved if it were enhanced with an agent
that could give feedback about the plot of a story.
Here we describe how techniques from symbolic
natural language processing and statistical machine-
learning were used to tackle the problem of auto-
mated plot analysis for StoryStation.
2 The Story Rewriting Task
In the story rewriting task, pupils rewrite a story in
their own words, allowing them to focus on their
writing ability instead of plot formulation. This task
is currently used in Scottish schools and thus it was
chosen to be the first feature of the plot analysis
agent. We collected a corpus of 103 stories rewritten
by children from classes at primary schools in Scot-
land. Pupils were told a story, an exemplar story,
by a storyteller and were asked to rewrite the story
in their own words.1 The automated plot analysis
program must be able to give a general rating of the
quality of the rewritten story?s plot and be able to
determine missing or incorrect events. The general
rating can be used by the teacher to find out which
pupils are in need of attention, while the more spe-
cific details can be used by an animated agent in
StoryStation to remind the student of specific events
and characters they have forgotten or misused.
3 Plot Ratings
The stories were rated for plot by three different
raters. A story-teller (Rater B) ranked all of the sto-
ries. Two others (Rater A, a teacher, and Rater C)
ranked the stories as well, although Rater A ranked
only half. The following scale, devised by a teacher
with over forty years of experience, was used.
1. Excellent: An excellent story shows that the
reader understands the ?point? of the story and
should demonstrate some deep understanding
of the plot. The pupil should be able to retrieve
all the important links and, not all the details,
but the right details.
2. Good: A good story shows that the pupil was
listening to the story, and can recall the main
1The exemplar story used in our corpus was ?Nils? Ad-
venture,? a story from ?The Wonderful Adventures of Nils?
(Lagerloff, 1907).
Class Probability Number of Class
1 (Excellent) 0.175 18
2 (Good) 0.320 33
3 (Fair) 0.184 19
4 (Poor) 0.320 33
Table 1: Distribution of Story Ratings
events and links in the plot. However, the
pupil shows no deeper understanding of the
plot, which can often be detected by the pupil
leaving out an important link or emphasizing
the wrong details.
3. Fair: A fair story shows that the pupil is miss-
ing more than one link or chunk of the story,
and not only lacks an understanding of the
?point? but also lacks recall of vital parts of
the story. A fair story does not really flow.
4. Poor: A poor story has definite problems with
recall of events, and is missing substantial
amount of the plot. Characters will be misiden-
tified and events confused. Often the child
writes on the wrong subject or starts off recit-
ing only the beginning of the story.
Rater B and Rater A had an agreement of 39%
while Rater B and Rater C had an agreement of
77%. However, these numbers are misleading as the
rating scale is ordinal and almost all the disagree-
ments were the result of grading a story either one
rank better or worse. In particular Rater A usually
marked incomplete stories as poor while the other
raters assigned partial credit. To evaluate the relia-
bility of the grades both Cronbach?s ? and Kendall?s
?b were used, since these statistics take into account
ordinal scales and inter-rater reliability. Between
Rater A and B there was a Cronbach?s ? statistic
of .86 and a Kendall?s ?b statistic of .72. Between
Rater B and C there was a Cronbach?s ? statistic
of .93 and Kendall?s ?b statistic of .82. These statis-
tics show our rating scheme to be fairly reliable. As
the most qualified expert to rate all the stories, Rater
B?s ratings were used as the gold standard. The dis-
tribution of plot ratings are given in Table 1.
4 A Minimal Event Calculus
The most similar discourse analysis program to the
one needed by StoryStation is the essay-grading
component of ?Criterion? by ETS technologies
(Burstein et al, 2003), which is designed to anno-
tate parts of an essay according to categories such
as ?Thesis, ?Main Points,? ?Support,? and ?Con-
clusion.? Burstein et. al. (2003) uses Rhetorical
Structure Theory to parse the text into discourse re-
lations based on satellites and nuclei connected by
rhetorical relations. Moore and Pollack (1992) note
that Rhetorical Structure Theory conflates the infor-
mational (the information being conveyed) and in-
tentional (the effects on the reader?s beliefs or atti-
tudes) levels of discourse. Narratives are primarily
informational, and so tend to degenerate to long se-
quences of elaboration or sequence relations. Since
in the story rewriting task the students are attempt-
ing to convey information about the narrative, un-
like the primarily persuasive task of an essay, our
system focuses on the informational level as embod-
ied by a simplified event calculus. Another tutoring
system similar to ours is the WHY physics tutoring
system (Rose et al, 2002).
We formulate only three categories to describe
stories: events, event names, and entities. This for-
mulation keeps the categories from being arbitrary
or exploding in number. Entities are both animate
characters, such as ?elves? and ?storks,? and inani-
mate objects like ?sand? and ?weather.? Nouns are
the most common type of entities. Events are com-
posed of the relationships among entities, such as
?the boy becomes an elf,? which is composed of a
?boy? and ?elf? interacting via ?becoming,? which
we call the event name. This is because the use
of such verbs is an indicator of the presence of an
event in the story. In this manner events are relation-
ships labeled with an event name, and entities are
arguments to these relationships as in propositional
logic. Together these can form events such as be-
come(boy,elf), and this formulation maps partially
onto Shanahan?s event calculus which has been
used in other story-understanding models (Mueller,
2003). The key difference between an event calcu-
lus and a collection of propositions is that time is
explicitly represented in the event calculus.
Each story consists of a group of events that are
present in the story, e1...eh. Each event consists of
an event name, a time variable t, and a set of enti-
ties arranged in an ordered set n1...na. An event
must contain one and only one event name. The
event names are usually verbs, while the entities
tend to be, but are not exclusively, nouns. Time is
made explicit through a variable t. Normally, the
Shanahan event calculus has a series of predicates
to deal with relations of achievements, accomplish-
ments, and other types of temporal relations (Shana-
han, 1997), however our calculus does not use these
since it is difficult to extract these from ungrammati-
cal raw text automatically. A story?s temporal order
is a partial ordering of events as denoted by their
time variable t. When incorporating a set of entities
into an event, a superscript is used to keep the enti-
ties distinct, as n13 is entity 1 in event 3. An entity
may appear in multiple events, such as entity 1 ap-
pearing in event 3 (n13) and in event 5 (n15). The plot
of a story can then be considered an event structure
of the following form if it has h events:
e1(t1, (n11, n21, ...na1)), ...., eh(th, (n2h, n4h...nch))
Where time t1 ? t2 ? ...th. An example from a
rewritten story is ?Nils found a coin and he walked
round a sandy beach. He talked to the stork. Asked
a question.? This is represented by an event struc-
ture as:
find(t = 1(Nils, coin)),
walk(t = 1, (Nils, sand, beach)),
talk(t = 2, (stork, Nils)),
ask(t = 3, (question))
Note that the rewritten stories are often ungram-
matical. A sentence may map onto one, multiple, or
no events. Two stories match if they are composed
of the same ordering of events.
5 Extracting the Event Calculus
The event calculus can be extracted from raw text
by layering NLP modules using an XML-based
pipeline. Our main constraint was that the text of the
pupil was rarely grammatical, restricting our choice
of NLP components to those that did not require a
correct parse or were in any other ways dependent
on grammatical sentences. At each level of process-
ing, an XML-enabled natural language processing
component can add mark-up to the text, and use any
mark-up that the previous components made. All
layers in the pipeline are fully automatic. For our
pipeline we used LT-TTT (Language Technology
Text Tokenization Toolkit) (Grover et al, 2000).
Once words are tokenized and sentence boundaries
detected by LT-TTT, LT-POS tags the words using
the Penn Treebank tag-set without parsing the sen-
tences. While a full parse could be generated by a
statistical parser, such parses would likely be incor-
rect for the ungrammatical sentences often gener-
ated by the pupils (Charniak, 2000). Pronouns are
resolved using a cascading rule-based approach di-
rectly inspired by the CogNIAC algorithm (Bald-
win, 1997) with two variations. First, it resolves in
distinct cascades for singular and then plural pro-
nouns. Second, it resolves using only the Cog-
NIAC rules that can be determined using Penn Tree-
bank tags. The words are lemmatized using an aug-
mented version of the SCOL Toolset and sentences
are chunked using the Cass Chunker (Abney, 1995).
There is a trade-off between this chunking approach
that works on ungrammatical sentences and one that
requires a full parse such as those using dependency
grammars. The Cass Chunker is highly precise,
but often inaccurate and misses relations and enti-
ties that are not in a chunk. In its favor, those tu-
ples in chunks that it does identify are usually cor-
rect. SCOL extracts tuples from the chunks to deter-
mine the presence of events, and the remaining ele-
ments in the chunk are inspected via rules for enti-
ties. Time is explicitly identified using a variation of
the ?now point? algorithm (Allen, 1987). We map
each event?s time variable to a time-line, assuming
that events occur in the order in which they appear
in the text. While temporal ordering of events is
hard (Mani and Wilson, 2003), given that children
of this age tend to use a single tense throughout the
narrative and that in narratives events are presented
in order (Hickmann, 2003), this simple algorithm
should suffice for ordering in the domain of chil-
dren?s stories.
6 Plot Comparison Algorithm
Since the story rewriting task involves imperfect re-
call, story events will likely be changed or left out
by the pupil. The story rewriting task involves the
students choosing their own diction and expressing
their own unique mastery of language, so variation
in how the fundamental elements of the story are
rewritten is to be expected. To deal with these is-
sues, an algorithm had to be devised that takes the
event structure of the rewritten story and compares
it to the event structure of the exemplar story, while
disregarding the particularities of diction and gram-
mar. The problem is one of credit allocation for the
similarity of rewritten events to the exemplar event.
The words used in the events of the two story mod-
els may differ. The exemplar story model might
use the event see(Nils,stork), but a rewritten story
may use the word ?bird? instead of the more precise
word ?stork.? However, since the ?bird? is refer-
ring to the stork in the exemplar story, partial credit
should be assigned. A plot comparison algorithm
was created that uses abstract event calculus repre-
sentations of plot and the text of the rewritten story,
taking into account temporal order and word simi-
larity. The exemplar story?s event structure is cre-
ated by applying the event extraction pipeline to the
storyteller?s transcript.
The Plot Comparison Algorithm is given in Fig-
ure 1. In the pseudo-code, E of size h and R of size
j are the event structures of the exemplar story and
rewritten story respectively, with the names of each
of their events denoted as e and r. The set of entities
of each event are denoted as Ne and Nr respectively.
T is the lemmatized tokens of the rewritten story?s
raw text. WordNet(x) denotes the synset of x. The
?now point? of the rewritten story is t, and feature
set is f , which has an index of i. The index i is
incremented every time f is assigned a value. 1 de-
notes an exact match, 2 a WordNet synset match, 3
a match in the text, and 0 a failure to find any match.
The Plot Comparison Algorithm essentially iter-
ates through the exemplar story looking for matches
of the events in the rewritten story. To find if two
events are in or out of order the rewritten story has
a ?now point? that serves as the beginning of its it-
eration. Each event of the event structure of the ex-
emplar story is matched against each event of the
rewritten story starting at the ?now point? and us-
ing the exact text of the event name. If that match
fails a looser match is attempted by giving the event
names of the rewritten story to WordNet and see-
ing if a match to the resultant synset succeeds (Fell-
baum, 1998). If either match attempt succeeds, the
algorithm attempts to match entities in the same
fashion and the ?now point? of the rewritten story
is incremented. Thus the algorithm does not looks
back in the rewritten story for a match. If the event
match fails, one last attempt is made by checking
the event name or entity against every lemmatized
token in the entire rewritten text. If this fails, a fail-
ure is recorded. The results of the algorithm are can
be used as a feature set for machine-learning. The
event calculus extraction pipeline and the Plot Com-
parison Algorithm can produce event calculus rep-
resentations of any English text and compare them.
They have been tested on other stories that do not
have a significant corpus of rewritten stories. The
number of events for an average rewritten story in
our corpus was 26, with each event having an aver-
age of 1 entity.
Included in Figure 2 is sample output from our
algorithm given the exemplar story model ea and a
rewritten story rb whose text is as follows: Nils took
the coin and tossed it away, cause it was worthless.
A city appeared and so he walked in. Everywhere
was gold and the merchant said Buy this Only one
coin Nils has no coin. So he went to get the coin he
threw away but the city vanished just like that right
behind him. Nils asked the bird Hey where the city
go? Let?s go home.
Due to space limitations, we only display selected
events from the transcript and their most likely
match from the rewritten story in Figure 2. The out-
put of the feature set would be the concatenation in
order of every value of fe.
Algorithm
6.1: PLOTCOMPARE(E, R, T )
t? 1
i? 0
for ex ? e1 to eh
do for ry ? rt to rj
do
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if ex = ry
then fi ? 1 and t? t + 1
else if ex ? WORDNET(ry)
then fi ? 2 and t? t + 1
if fi = 1 or 2
then
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
for each n ? Ne
if n ? N r
then fi ? 1
else if n ? WORDNET(Nr)
then fi ? 2
else if n ? T
then fi ? 3
else fi ? 0
else if ex ? T
then fi ? 3
else fi ? 0
Figure 1: Plot Comparison Algorithm
ea rb fe
throw(Nils, coin) toss(coin) 2, 3, 1
see(Nils, city) appear(city) 0, 3, 3
enter(Nils, city) walk(Nils) 0, 3, 3
ask(Nils, merchant) say(merchant) 0, 3, 3
say(Nils) say(merchant) 1, 3
leave(Nils) go(Nils) 2, 1
disappear(city) vanish(city) 2, 1
inquire(Nils, stork) ask(Nils, bird) 2, 1, 2
fly(stork) go(home) 0, 3
Figure 2: Example of Plot Algorithm
7 Learning the Significance of Events
Machine-learning is crucial to our experiment, as it
will allow our model to discriminate what events
and words in a rewritten story are good predictors
of plot quality as rated by a human expert. We
have restricted our feature set to the results of the
Plot Comparison Algorithm and LSA scores, as we
describe below. Other possible features, such as
the grammatical correctness and the number of con-
junctives, are dealt with by other agents in StoryS-
tation. We are focusing on plot recall quality as
opposed to general writing quality. Two different
machine-learning algorithms with differing assump-
tions were used. These are by no means exhaus-
tive of the options, and extensive tests have been
done with other algorithms. Further experiments
are needed to understand the precise nature of the
relations between the feature set and machine learn-
ing algorithms. All results were created by ten-fold
cross validation over the rated stories, which is es-
pecially important given our small corpus size.
7.1 Nearest Neighbors using LSA
We can classify the stories without using the re-
sults of the Plot Comparison Algorithm, and instead
use only their statistical attributes. Latent Semantic
Analysis (LSA) provides an approximation of ?se-
mantic? similarity based on the hypothesis that the
semantics of a word can be deduced from its context
in an entire document, leading to useful coherency
scores when whole documents are compared (Foltz
et al, 1998). LSA compares the text of each rewrit-
ten story in the corpus for similarity to the transcript
of the exemplar story in a subspace produced by
reducing the dimensionality of the TASA 12 grade
USA reading-level to 200. This dimensionality was
discovered through experimentation to be our prob-
lem?s optimal parameters for LSA given the range
of choices originally used by Landauer (1997). The
stories can be easily classified by grouping them to-
gether based on LSA similarity scores alone, and
this technique is embodied in the simple K-Nearest
Neighbors (K-NN) learner. K-NN makes no para-
metric assumptions about the data and uses no for-
mal symbolic features other than an LSA similarity
score. For K-NN k = 4 gave the best results over
a large range of k, and we expect this k would be
ideal for stories of similar length.
As shown in Table 2, despite its simplicity this al-
gorithm performs fairly well. It is not surprising that
features based primarily on word distributions such
as LSA could correctly discriminate the non-poor
from the poor rewritten stories. Some good rewrit-
ten stories closely resemble the exemplar story al-
most word for word, and so share the same word
distribution with the exemplar story. Poor rewritten
stories usually have little resemblance to the exem-
plar story, and so have a drastically different word
distribution. The high spread of error in classifying
stories is shown in the confusion matrix in Table 3.
This leads to unacceptable errors such as excellent
stories being classified as poor stories.
7.2 Hybrid Model with Naive Bayes
By using both LSA scores and event structures as
features for a statistical machine learner, a hybrid
model of plot rating can be created. In hybrid mod-
Class Precision Recall F-score
1 (Excellent) 0.11 0.17 0.13
2 (Good) 0.42 0.46 0.44
3 (Fair) 0.30 0.16 0.21
4 (Poor) 0.83 0.76 0.79
Table 2: K-Nearest Neighbors Precision and Recall
Class 1 2 3 4
1 (Excellent) 3 10 4 1
2 (Good) 13 15 2 3
3 (Fair) 9 6 3 1
4 (Poor) 2 5 1 25
Table 3: K-Nearest Neighbors: Confusion Matrix
els a formal symbolic model (the event calculus-
based results of a Plot Comparison Algorithm) en-
ters a mutually beneficial relationship with a statis-
tical model of the data (LSA), mediated by a ma-
chine learner (Naive Bayes). One way to combine
LSA similarity scores and the results of the event
structure is by using the Naive Bayes (NB) ma-
chine learner. NB makes the assumptions of both
parametrization and Conditional Independence.
The recall and precision per rank is given in Ta-
ble 4, and it is clear that while no stories are clas-
sified as excellent at all, the majority of good and
poor stories are identified correctly. As shown by
the confusion matrix in Table 5, NB does not de-
tect excellent stories and it collapses the distinction
between good and excellent stories. Compared to
K-NN with LSA, NB shows less spread in its er-
rors, although it does confuse some poor stories as
good and one excellent story as fair. Even though
it mistakenly classifies some poor stories as good,
for many teachers this is better than misidentifying
a good story as a poor story.
The raw accuracy results over all classes of the
machine learning algorithms are summarized in Ta-
ble 6. Note that average human rater agreement
is the average agreement between Rater A and C
(whose agreement ranged from 39% to 77%), since
Rater B?s ratings were used as the gold standard.
This average also assumes Rater A would have con-
tinued marking at the same accuracy for the com-
Class Precision Recall F-Score
1 (Excellent) 0.00 0.00 0.00
2 (Good) 0.43 0.88 0.58
3 (Fair) 0.45 0.26 0.33
4 (Poor) 0.92 0.67 0.77
Table 4: Naive Bayes Precision and Recall
Class 1 2 3 4
1 (Excellent) 0 17 1 0
2 (Good) 1 29 2 1
3 (Fair) 0 13 5 1
4 (Poor) 0 8 3 22
Table 5: Naive Bayes Confusion Matrix
Machine Learner Percentage Correct
K-NN (LSA) 44.66%
ID3 DT (Events) 40.78%
NB (LSA + Events) 54.37%
Rater Agreement 58.37%
Table 6: Machine Learner Comparison
plete corpus. DT refers to an ID3 Decision Tree
algorithm that creates a purely symbolic machine-
learner whose feature set was only the results of the
Plot Comparison Algorithm (Quinlan, 1986). It per-
formed worse than K-NN and thus the details are
not reported any further. Using NB and combining
the LSA scores with the results of the Plot Com-
parison Algorithm produces better raw performance
than K-NN. Recall of 54% for NB may seem dis-
appointing, but given that the raters only have an
average agreement of 58%, the performance of the
machine learner is reasonable. So if the machine-
learner had a recall of 75% it would be suspect.
Statistics to compare the results given the ordinal
nature of our rating scheme are shown in Table 7.
8 Discussion
From these experiments as shown in Table 6 we
see that the type of machine learner and the par-
ticular features are important to correctly classify
children?s stories. Inspection of the results shows
that separating good and excellent stories from poor
stories is best performed by Naive Bayes. For our
application, teachers have indicated that the classi-
fication of an excellent or good story as a poor one
is considered worse than the classifying of a fair
or even poor story as good. Moreover, it uses the
event-based results of the Plot Comparison Algo-
rithm so that the agent in StoryStation may use these
results to inform the student what precise events and
entities are missing or misused. NB is fast enough to
provide possible feedback in real time and its abil-
ity to separate poor stories from good and excellent
stories would allow it to be used in classrooms. It
also has comparable raw accuracy to average human
agreement as shown in Table 6, although it makes
more errors than humans in classifying a story off
by more than one class off as shown by the statistics
Machine Learner Cronbach?s ? Kendall?s ?b
NB to Rater B .78 .59
Rater A to Rater B .86 .72
Rater C to Rater B .93 .82
Table 7: Statistical Comparison
in Table 7. The results most in its favor are shown
highlighted in Table 5. It separates with few errors
both excellent and good stories from the majority of
poor stories.
While the event calculus captures some of the rel-
evant defining characteristics of stories, it does not
capture all of them. The types of stories that give the
machine learners the most difficulty are those which
are excellent and fair. One reason is that these sto-
ries are less frequent in the training data than poor
and good stories. Another reason is that there are
features particular to these stories that are not ac-
counted for by an event structure or LSA. Both ex-
cellent stories and fair stories rely on very subtle
features to distinguish them from good and poor sto-
ries. Good stories were characterized in the rating
criteria as ?parroting off of the main events,? and
the event calculus naturally is good at identifying
this. Poor stories have ?definite problems with the
recall of events,? and so are also easily identified.
However, fair stories show both a lack of ?under-
standing of the point? and ?do not really flow? while
the excellent story shows an ?understanding of the
point.? These characteristics involve relations such
as the ?point? of the story and connections between
events. These ideas of ?flow? and ?point? are much
more difficult to analyze automatically.
9 Conclusion
Due to its practical focus, the plot analysis of our
system is very limited in nature, focusing on just
the story rewriting task. Traditionally ?deep? rep-
resentation systems have attempted to be powerful
general-purpose story understanding or generation
systems. A general plot analysis agent would be
more useful than our current system, which is suc-
cessful by virtue of the story rewriting task being
less complex than full story understanding. How-
ever, our system fulfills an immediate need in the
StoryStation application, in contrast to more tra-
ditional story-understanding and story-generation
systems, which are usually used as testing grounds
for theoretical ideas in artificial intelligence. The
system was tested and developed using a small man-
ually collected corpus of a single rewritten story.
While previous researchers who worked on this
problem felt that the small size of the corpus made
machine-learning unusable, the results shows that
with careful feature selection and relatively simple
algorithms empirical methods can be made to work.
We expect that our technique can be generalized to
larger corpora of diverse types.
Our hybrid system uses both LSA and event
structures to classify plot quality. The use of event
structures in classifying stories allows us to de-
tect whether particular crucial characters and events
have been left out of the rewritten story. Separating
the students who have written good plots from those
who have done so poorly is a boon to the teachers,
since often it is the students who have the most dif-
ficulty with plot that are least likely to ask a teacher
for help. StoryStation is now being used in two
schools as part of their classroom writing instruc-
tion over the course of the next year. Results from
this study will be instrumental in shaping the future
of the plot analysis system in StoryStation and the
expansion of the current system into a general pur-
pose plot analysis system for other writing tasks.
References
Steven Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax.
In Jennifer Cole, Georgia Green, and Jerry Mor-
gan, editors, Computational Linguistics and the
Foundations of Linguistic Theory, pages 145?
164.
James Allen. 1987. Natural Language Understand-
ing. Menlo Park, CA, Benjamin/Cummings Pub-
lishing.
Breck Baldwin. 1997. CogNIAC : A High Preci-
sion Pronoun Resolution Engine.
Jill Burstein, Daniel Marcu, and Kevin Knight.
2003. Finding the WRITE Stuff: Automatic
Identification of Discourse Structure in Student
Essays. IEEE Intelligent Systems, pages 32?39.
Eugene Charniak. 2000. A Maximum-Entropy In-
spired Parser. In Proceedings of the North Amer-
ican Association for Computational Linguistics.
Christine Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence
with Latent Semantic Analysis. Discourse Pro-
cesses, 25(2&3):285?307.
Claire Grover, Colin Matheson, Andrei Mikheev,
and Marc Moens. 2000. LT TTT - A Flexible
Tokenisation Tool. In Proceedings of the Second
Language Resources and Evaluation Conference.
Maya Hickmann. 2003. Children?s Discourse: per-
son, space and time across language. Cambridge
University Press, Cambridge, UK.
Selma Lagerloff. 1907. The Wonderful Adventures
of Nils. Doubleday, Page, and Company, Garden
City, New York.
Thomas. Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The Latent Semantic
Analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Re-
view.
I. Mani and G. Wilson. 2003. Robust temporal pro-
cessing of the news. In In Proceedings of Associ-
ation for Computational Linguistics.
Johanna D. Moore and Martha Pollack. 1992.
A problem for RST: The need for multi-level
discourse analysis. Computational Linguistics,
18(4):537?544.
Erik T. Mueller. 2003. Story understanding
through multi-representation model construction.
In Graeme Hirst and Sergei Nirenburg, editors,
Text Meaning: Proceedings of the HLT-NAACL
2003 Workshop, pages 46?53, East Stroudsburg,
PA. Association for Computational Linguistics.
Ross Quinlan. 1986. Induction of decision trees. In
Machine Learning, volume 1. Kluwer Academic
Press.
Judy Roberston and Peter Wiemar-Hastings. 2002.
Feedback on children?s stories via multiple inter-
face agents. In International Conference on In-
telligent Tutoring Systems, Biarritz, France.
Judy Robertson and Beth Cross. 2003. Children?s
perceptions about writing with their teacher and
the StoryStation learning environment. Narrative
and Interactive Learning Environments: Special
Issue of International Journal of Continuing En-
gineering Education and Life-long Learning.
C. Rose, D. Bhembe, A. Roque, S. Siler, R. Srivas-
tava, and K. VanLehn. 2002. A hybrid language
understanding approach for robust selection of tu-
toring goals. In International Conference on In-
telligent Tutoring Systems, Biarritz, France.
Murray Shanahan. 1997. Solving the Frame Prob-
lem. MIT Press, Cambridge, MA.
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 33?40, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Evaluating Automatic Summaries of Meeting Recordings
Gabriel Murray
Centre for Speech Technology Research
University of Edinburgh
Edinburgh, United Kingdom
Steve Renals
Centre for Speech Technology Research
University of Edinburgh
Edinburgh, United Kingdom
Jean Carletta
Human Communication Research Centre
University of Edinburgh
Edinburgh, United Kingdom
Johanna Moore
Human Communication Research Centre
University of Edinburgh
Edinburgh, United Kingdom
Abstract
The research below explores schemes for
evaluating automatic summaries of busi-
ness meetings, using the ICSI Meeting
Corpus (Janin et al, 2003). Both au-
tomatic and subjective evaluations were
carried out, with a central interest be-
ing whether or not the two types of eval-
uations correlate with each other. The
evaluation metrics were used to compare
and contrast differing approaches to au-
tomatic summarization, the deterioration
of summary quality on ASR output ver-
sus manual transcripts, and to determine
whether manual extracts are rated signifi-
cantly higher than automatic extracts.
1 Introduction
In the field of automatic summarization, it is widely
agreed upon that more attention needs to be paid
to the development of standardized approaches to
summarization evaluation. For example, the cur-
rent incarnation of the Document Understanding
Conference is putting its main focus on the de-
velopment of evaluation schemes, including semi-
automatic approaches to evaluation. One semi-
automatic approach to evaluation is ROUGE (Lin
and Hovy, 2003), which is primarily based on n-
gram co-occurrence between automatic and human
summaries. A key question of the research con-
tained herein is how well ROUGE correlates with
human judgments of summaries within the domain
of meeting speech. If it is determined that the two
types of evaluations correlate strongly, then ROUGE
will likely be a valuable and robust evaluation tool in
the development stage of a summarization system,
when the cost of frequent human evaluations would
be prohibitive.
Three basic approaches to summarization are
evaluated and compared below: Maximal Marginal
Relevance, Latent Semantic Analysis, and feature-
based classification. The other major comparisons
in this paper are between summaries on ASR ver-
sus manual transcripts, and between manual and au-
tomatic extracts. For example, regarding the for-
mer, it might be expected that summaries on ASR
transcripts would be rated lower than summaries on
manual transcripts, due to speech recognition errors.
Regarding the comparison of manual and automatic
extracts, the manual extracts can be thought of as
a gold standard for the extraction task, represent-
ing the performance ceiling that the automatic ap-
proaches are aiming for.
More detailed descriptions of the summarization
approaches and experimental setup can be found in
(Murray et al, 2005). That work relied solely on
ROUGE as an evaluation metric, and this paper pro-
ceeds to investigate whether ROUGE alone is a reli-
able metric for our summarization domain, by com-
paring the automatic scores with recently-gathered
human evaluations. Also, it should be noted that
while we are at the moment only utilizing intrinsic
evaluation methods, our ultimate plan is to evalu-
ate these meeting summaries extrinsically within the
context of a meeting browser (Wellner et al, 2005).
33
2 Description of the Summarization
Approaches
2.1 Maximal Marginal Relevance (MMR)
MMR (Carbonell and Goldstein, 1998) uses the
vector-space model of text retrieval and is particu-
larly applicable to query-based and multi-document
summarization. The MMR algorithm chooses
sentences via a weighted combination of query-
relevance and redundancy scores, both derived using
cosine similarity. The MMR score ScMMR(i)for a
given sentence Si in the document is given by
ScMMR(i) =
?(Sim(Si, D))? (1? ?)(Sim(Si, Summ)) ,
where D is the average document vector, Summ
is the average vector from the set of sentences al-
ready selected, and ? trades off between relevance
and redundancy. Sim is the cosine similarity be-
tween two documents.
This implementation of MMR uses lambda an-
nealing so that relevance is emphasized while the
summary is still short and minimizing redundancy is
prioritized more highly as the summary lengthens.
2.2 Latent Semantic Analysis (LSA)
LSA is a vector-space approach which involves pro-
jecting the original term-document matrix to a re-
duced dimension representation. It is based on the
singular value decomposition (SVD) of an m ? n
term-document matrix A, whose elements Aij rep-
resent the weighted term frequency of term i in doc-
ument j. In SVD, the term-document matrix is de-
composed as follows:
A = USV T
where U is an m?n matrix of left-singular vectors,
S is an n ? n diagonal matrix of singular values,
and V is the n ? n matrix of right-singular vectors.
The rows of V T may be regarded as defining top-
ics, with the columns representing sentences from
the document. Following Gong and Liu (Gong and
Liu, 2001), summarization proceeds by choosing,
for each row in V T , the sentence with the highest
value. This process continues until the desired sum-
mary length is reached.
Two drawbacks of this method are that dimen-
sionality is tied to summary length and that good
sentence candidates may not be chosen if they do
not ?win? in any dimension (Steinberger and Jez?ek,
2004). The authors in (Steinberger and Jez?ek, 2004)
found one solution, by extracting a single LSA-
based sentence score, with variable dimensionality
reduction.
We address the same concerns, following the
Gong and Liu approach, but rather than extracting
the best sentence for each topic, the n best sentences
are extracted, with n determined by the correspond-
ing singular values from matrix S. The number of
sentences in the summary that will come from the
first topic is determined by the percentage that the
largest singular value represents out of the sum of all
singular values, and so on for each topic. Thus, di-
mensionality reduction is no longer tied to summary
length and more than one sentence per topic can be
chosen. Using this method, the level of dimension-
ality reduction is essentially learned from the data.
2.3 Feature-Based Approaches
Feature-based classification approaches have been
widely used in text and speech summarization, with
positive results (Kupiec et al, 1995). In this work
we combined textual and prosodic features, using
Gaussian mixture models for the extracted and non-
extracted classes. The prosodic features were the
mean and standard deviation of F0, energy, and du-
ration, all estimated and normalized at the word-
level, then averaged over the utterance. The two lex-
ical features were both TFIDF-based: the average
and the maximum TFIDF score for the utterance.
For our second feature-based approach, we de-
rived single LSA-based sentence scores (Steinberger
and Jez?ek, 2004) to complement the six features de-
scribed above, to determine whether such an LSA
sentence score is beneficial in determining sentence
importance. We reduced the original term-document
matrix to 300 dimensions; however, Steinberger and
Jez?ek found the greatest success in their work by re-
ducing to a single dimension (Steinberger, personal
communication). The LSA sentence score was ob-
tained using:
ScLSAi =
?
?
?
?
n?
k=1
v(i, k)2 ? ?(k)2 ,
34
where v(i, k) is the kth element of the ith sentence
vector and ?(k) is the corresponding singular value.
3 Experimental Setup
We used human summaries of the ICSI Meeting cor-
pus for evaluation and for training the feature-based
approaches. An evaluation set of six meetings was
defined and multiple human summaries were created
for these meetings, with each test meeting having ei-
ther three or four manual summaries. The remaining
meetings were regarded as training data and a single
human summary was created for these. Our sum-
maries were created as follows.
Annotators were given access to a graphical user
interface (GUI) for browsing an individual meeting
that included earlier human annotations: an ortho-
graphic transcription time-synchronized with the au-
dio, and a topic segmentation based on a shallow hi-
erarchical decomposition with keyword-based text
labels describing each topic segment. The annota-
tors were told to construct a textual summary of the
meeting aimed at someone who is interested in the
research being carried out, such as a researcher who
does similar work elsewhere, using four headings:
? general abstract: ?why are they meeting and
what do they talk about??;
? decisions made by the group;
? progress and achievements;
? problems described
The annotators were given a 200 word limit for each
heading, and told that there must be text for the gen-
eral abstract, but that the other headings may have
null annotations for some meetings.
Immediately after authoring a textual summary,
annotators were asked to create an extractive sum-
mary, using a different GUI. This GUI showed
both their textual summary and the orthographic
transcription, without topic segmentation but with
one line per dialogue act based on the pre-existing
MRDA coding (Shriberg et al, 2004) (The dialogue
act categories themselves were not displayed, just
the segmentation). Annotators were told to extract
dialogue acts that together would convey the infor-
mation in the textual summary, and could be used to
support the correctness of that summary. They were
given no specific instructions about the number or
percentage of acts to extract or about redundant dia-
logue act. For each dialogue act extracted, they were
then required in a second pass to choose the sen-
tences from the textual summary supported by the
dialogue act, creating a many-to-many mapping be-
tween the recording and the textual summary.
The MMR and LSA approaches are both unsuper-
vised and do not require labelled training data. For
both feature-based approaches, the GMM classifiers
were trained on a subset of the training data repre-
senting approximately 20 hours of meetings.
We performed summarization using both the hu-
man transcripts and speech recognizer output. The
speech recognizer output was created using base-
line acoustic models created using a training set
consisting of 300 hours of conversational telephone
speech from the Switchboard and Callhome cor-
pora. The resultant models (cross-word triphones
trained on conversational side based cepstral mean
normalised PLP features) were then MAP adapted
to the meeting domain using the ICSI corpus (Hain
et al, 2005). A trigram language model was em-
ployed. Fair recognition output for the whole corpus
was obtained by dividing the corpus into four parts,
and employing a leave one out procedure (training
the acoustic and language models on three parts of
the corpus and testing on the fourth, rotating to ob-
tain recognition results for the full corpus). This
resulted in an average word error rate (WER) of
29.5%. Automatic segmentation into dialogue acts
or sentence boundaries was not performed: the dia-
logue act boundaries for the manual transcripts were
mapped on to the speech recognition output.
3.1 Description of the Evaluation Schemes
A particular interest in our research is how automatic
measures of informativeness correlate with human
judgments on the same criteria. During the devel-
opment stage of a summarization system it is not
feasible to employ many hours of manual evalua-
tions, and so a critical issue is whether or not soft-
ware packages such as ROUGE are able to measure
informativeness in a way that correlates with subjec-
tive summarization evaluations.
35
3.1.1 ROUGE
Gauging informativeness has been the focus
of automatic summarization evaluation research.
We used the ROUGE evaluation approach (Lin
and Hovy, 2003), which is based on n-gram co-
occurrence between machine summaries and ?ideal?
human summaries. ROUGE is currently the stan-
dard objective evaluation measure for the Document
Understanding Conference 1; ROUGE does not as-
sume that there is a single ?gold standard? summary.
Instead it operates by matching the target summary
against a set of reference summaries. ROUGE-1
through ROUGE-4 are simple n-gram co-occurrence
measures, which check whether each n-gram in the
reference summary is contained in the machine sum-
mary. ROUGE-L and ROUGE-W are measures of
common subsequences shared between two sum-
maries, with ROUGE-W favoring contiguous com-
mon subsequences. Lin (Lin and Hovy, 2003) has
found that ROUGE-1 and ROUGE-2 correlate well
with human judgments.
3.1.2 Human Evalautions
The subjective evaluation portion of our research
utilized 5 judges who had little or no familiarity with
the content of the ICSI meetings. Each judge eval-
uated 10 summaries per meeting, for a total of sixty
summaries. In order to familiarize themselves with
a given meeting, they were provided with a human
abstract of the meeting and the full transcript of the
meeting with links to the audio. The human judges
were instructed to read the abstract, and to consult
the full transcript and audio as needed, with the en-
tire familiarization stage not to exceed 20 minutes.
The judges were presented with 12 questions at
the end of each summary, and were instructed that
upon beginning the questionnaire they should not re-
consult the summary itself. 6 of the questions re-
garded informativeness and 6 involved readability
and coherence, though our current research concen-
trates on the informativeness evaluations. The eval-
uations used a Likert scale based on agreement or
disagreement with statements, such as the following
Informativeness statements:
1. The important points of the meeting are repre-
sented in the summary.
1http://duc.nist.gov/
2. The summary avoids redundancy.
3. The summary sentences on average seem rele-
vant.
4. The relationship between the importance of
each topic and the amount of summary space
given to that topic seems appropriate.
5. The summary is repetitive.
6. The summary contains unnecessary informa-
tion.
Statements such as 2 and 5 above are measuring
the same impressions, with the polarity of the state-
ments merely reversed, in order to better gauge the
reliability of the answers. The readability/coherence
portion consisted of the following statements:
1. It is generally easy to tell whom or what is be-
ing referred to in the summary.
2. The summary has good continuity, i.e. the sen-
tences seem to join smoothly from one to an-
other.
3. The individual sentences on average are clear
and well-formed.
4. The summary seems disjointed.
5. The summary is incoherent.
6. On average, individual sentences are poorly
constructed.
It was not possible in this paper to gauge how
responses to these readability statements correlate
with automatic metrics, for the reason that auto-
matic metrics of readability and coherence have not
been widely discussed in the field of summariza-
tion. Though subjective evaluations of summaries
are often divided into informativeness and readabil-
ity questions, only automatic metrics of informative-
ness have been investigated in-depth by the summa-
rization community. We believe that the develop-
ment of automatic metrics for coherence and read-
ability should be a high priority for researchers in
summarization evaluation and plan on pursuing this
avenue of research. For example, work on coher-
ence in NLG (Lapata, 2003) could potentially in-
form summarization evaluation. Mani (Mani et al,
36
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
ROUGE-1-MANROUGE-2-MANROUGE-L-MANROUGE-1-ASRROUGE-2-ASRROUGE-L-ASR
Figure 1: ROUGE Scores for the Summarization Ap-
proaches
1999) is one of the few papers to have discussed
measuring summary readability automatically.
4 Results
The results of these experiments can be analyzed
in various ways: significant differences of ROUGE
results across summarization approaches, deterio-
ration of ROUGE results on ASR versus manual
transcripts, significant differences of human eval-
uations across summarization approaches, deterio-
ration of human evaluations on ASR versus man-
ual transcripts, and finally, the correlation between
ROUGE and human evaluations.
4.1 ROUGE results across summarization
approaches
All of the machine summaries were 10% of the orig-
inal document length, in terms of the number of di-
alogue acts contained. Of the four approaches to
summarization used herein, the latent semantic anal-
ysis method performed the best on every meeting
tested for every ROUGE measure with the excep-
tion of ROUGE-3 and ROUGE-4. This approach
was significantly better than either feature-based ap-
proach (p<0.05), but was not a significant improve-
ment over MMR. For ROUGE-3 and ROUGE-4,
none of the summarization approaches were signifi-
cantly different from each other, owing to data spar-
sity. Figure 1 gives the ROUGE-1, ROUGE-2 and
ROUGE-L results for each of the summarization ap-
proaches, on both manual and ASR transcripts.
4.1.1 ASR versus Manual
The results of the four summarization approaches
on ASR output were much the same, with LSA and
MMR being comparable to each other, and each of
them outperforming the feature-based approaches.
On ASR output, LSA again consistently performed
the best.
Interestingly, though the LSA approach scored
higher when using manual transcripts than when
using ASR transcripts, the difference was small and
insignificant despite the nearly 30% WER of the
ASR. All of the summarization approaches showed
minimal deterioration when used on ASR output
as compared to manual transcripts, but the LSA
approach seemed particularly resilient, as evidenced
by Figure 1. One reason for the relatively small
impact of ASR output on summarization results is
that for each of the 6 meetings, the WER of the
summaries was lower than the WER of the meeting
as a whole. Similarly, Valenza et al(Valenza et
al., 1999) and Zechner and Waibel (Zechner and
Waibel, 2000) both observed that the WER of
extracted summaries was significantly lower than
the overall WER in the case of broadcast news. The
table below demonstrates the discrepancy between
summary WER and meeting WER for the six
meetings used in this research.
Meeting Summary WER Meeting WER
Bed004 27.0 35.7
Bed009 28.3 39.8
Bed016 39.6 49.8
Bmr005 23.9 36.1
Bmr019 28.0 36.5
Bro018 25.9 35.6
WER% for Summaries and Meetings
There was no improvement in the second feature-
based approach (adding an LSA sentence score) as
compared with the first feature-based approach. The
sentence score used here relied on a reduction to 300
dimensions, which may not have been ideal for this
data.
The similarity between the MMR and LSA ap-
proaches here mirrors Gong and Liu?s findings, giv-
ing credence to the claim that LSA maximizes rele-
vance and minimizes redundancy, in a different and
more opaque manner then MMR, but with similar
37
STATEMENT FB1 LSA MMR FB2
IMPORT. POINTS 5.03 4.53 4.67 4.83
NO REDUN. 4.33 2.60 3.00 3.77
RELEVANT 4.83 4.07 4.33 4.53
TOPIC SPACE 4.43 3.83 3.87 4.30
REPETITIVE 3.37 4.70 4.60 3.83
UNNEC. INFO. 4.70 6.00 5.83 5.00
Table 1: Human Scores for 4 Approaches on Manual
Transcripts
results. Regardless of whether or not the singular
vectors of V T can rightly be thought of as topics or
concepts (a seemingly strong claim), the LSA ap-
proach was as successful as the more popular MMR
algorithm.
4.2 Human results across summarization
approaches
Table 1 presents average ratings for the six state-
ments across four summarization approaches on
manual transcripts. Interestingly, the first feature-
based approach is given the highest marks on each
criterion. For statements 2, 5 and 6 FB1 is signif-
icantly better than the other approaches. It is par-
ticularly surprising that FB1 would score well on
statement 2, which concerns redundancy, given that
MMR and LSA explicitly aim to reduce redundancy
while the feature-based approaches are merely clas-
sifying utterances as relevant or not. The second
feature-based approach was not significantly worse
than the first on this score.
Considering the difficult task of evaluating ten ex-
tractive summaries per meeting, we are quite satis-
fied with the consistency of the human judges. For
example, statements that were merely reworded ver-
sions of other statements were given consistent rat-
ings. It was also the case that, with the exception
of evaluating the sixth statement, judges were able
to tell that the manual extracts were superior to the
automatic approaches.
4.2.1 ASR versus Manual
Table 2 presents average ratings for the six state-
ments across four summarization approaches on
ASR transcripts. The LSA and MMR approaches
performed better in terms of having less deteri-
STATEMENT FB1 LSA MMR FB2
IMPORT. POINTS 3.53 4.13 3.73 3.50
NO REDUN. 3.40 2.97 2.63 3.57
RELEVANT 3.47 3.57 3.00 3.47
TOPIC SPACE 3.27 3.33 3.00 3.20
REPETITIVE 4.43 4.73 4.70 4.20
UNNEC. INFO. 5.37 6.00 6.00 5.33
Table 2: Human Scores for 4 Approaches on ASR
Transcripts
 0
 1
 2
 3
 4
 5
 6
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-1-MANHUMAN-1-ASR
Figure 2: INFORMATIVENESS-1 Scores for the
Summarization Approaches
oration of scores when used on ASR output in-
stead of manual transcripts. LSA-ASR was not
significantly worse than LSA on any of the 6 rat-
ings. MMR-ASR was significantly worse than
MMR on only 3 of the 6. In contrast, FB1-
ASR was significantly worse than FB1 for 5 of
the 6 approaches, reinforcing the point that MMR
and LSA seem to favor extracting utterances with
fewer errors. Figures 2, 3 and 4 depict the
how the ASR and manual approaches affect the
INFORMATIVENESS-1, INFORMATIVENESS-4
and INFORMATIVENESS-6 ratings, respectively.
Note that for Figure 6, a higher score is a worse rat-
ing.
4.3 ROUGE and Human correlations
According to (Lin and Hovy, 2003), ROUGE-
1 correlates particularly well with human judg-
ments of informativeness. In the human eval-
uation survey discussed here, the first statement
(INFORMATIVENESS-1) would be expected to
correlate most highly with ROUGE-1, as it is ask-
38
 0
 1
 2
 3
 4
 5
 6
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-4-MANHUMAN-4-ASR
Figure 3: INFORMATIVENESS-4 Scores for the
Summarization Approaches
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-6-MANHUMAN-6-ASR
Figure 4: INFORMATIVENESS-6 Scores for the
Summarization Approaches
ing whether the summary contains the important
points of the meeting. As could be guessed from the
discussion above, there is no significant correlation
between ROUGE-1 and human evaluations when
analyzing only the 4 summarization approaches
on manual transcripts. However, when looking
at the 4 approaches on ASR output, ROUGE-1
and INFORMATIVENESS-1 have a moderate and
significant positive correlation (Spearman?s rho =
0.500, p < 0.05). This correlation on ASR out-
put is strong enough that when ROUGE-1 and
INFORMATIVENESS-1 scores are tested for corre-
lation across all 8 summarization approaches, there
is a significant positive correlation (Spearman?s rho
= 0.388, p < 0.05).
The other significant correlations for ROUGE-
1 across all 8 summarization approaches are with
INFORMATIVENESS-2, INFORMATIVENESS-5
and INFORMATIVENESS-6. However, these are
negative correlations. For example, with regard to
INFORMATIVENESS-2, summaries that are rated
as having a high level of redundancy are given high
ROUGE-1 scores, and summaries with little redun-
dancy are given low ROUGE-1 scores. Similary,
with regard to INFORMATIVENESS-6, summaries
that are said to have a great deal of unnecessary in-
formation are given high ROUGE-1 scores. It is
difficult to interpret some of these negative correla-
tions, as ROUGE does not measure redundancy and
would not necessarily be expected to correlate with
redundancy evaluations.
5 Discussion
In general, ROUGE did not correlate well with the
human evaluations for this data. The MMR and
LSA approaches were deemed to be significantly
better than the feature-based approaches according
to ROUGE, while these findings were reversed ac-
cording to the human evaluations. An area of agree-
ment, however, is that the LSA-ASR and MMR-
ASR approaches have a small and insignificant de-
cline in scores compared with the decline of scores
for the feature-based approaches. One of the most
interesting findings of this research is that MMR and
LSA approaches used on ASR tend to select utter-
ances with fewer ASR errors.
ROUGE has been shown to correlate well with
human evaluations in DUC, when used on news cor-
pora, but the summarization task here ? using con-
versational speech from meetings ? is quite different
from summarizing news articles. ROUGE may sim-
ply be less applicable to this domain.
6 Future Work
It remains to be determined through further ex-
perimentation by researchers using various corpora
whether or not ROUGE truly correlates well with
human judgments. The results presented above are
mixed in nature, but do not present ROUGE as being
sufficient in itself to robustly evaluate a summariza-
tion system under development.
We are also interested in developing automatic
metrics of coherence and readability. We now have
human evaluations of these criteria and are ready to
39
begin testing for correlations between these subjec-
tive judgments and potential automatic metrics.
7 Acknowledgements
Thanks to Thomas Hain and the AMI-ASR group
for the speech recognition output. This work was
partly supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811, publication).
References
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. ACM SIGIR,
pages 335?336.
Y. Gong and X. Liu. 2001. Generic text summarization
using relevance measure and latent semantic analysis.
In Proc. ACM SIGIR, pages 19?25.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, I.Mc.Cowan, J.Vepa, and
S.Renals. 2005. An investigation into transcription of
conference room meetings. Submitted to Eurospeech.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proc. IEEE ICASSP.
J. Kupiec, J. Pederson, and F. Chen. 1995. A trainable
document summarizer. In ACM SIGIR ?95, pages 68?
73.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In ACL, pages 545?
552.
C.-Y. Lin and E. H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proc. HLT-NAACL.
Inderjeet Mani, Barbara Gates, and Eric Bloedorn. 1999.
Improving summaries by revising them. In Proceed-
ings of the 37th conference on Association for Compu-
tational Linguistics, pages 558?565, Morristown, NJ,
USA. Association for Computational Linguistics.
G. Murray, S. Renals, and J. Carletta. 2005. Extractive
summarization of meeting recordings. Submitted to
Eurospeech.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, , and H. Car-
vey. 2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Proc. 5th SIGdial Workshop on
Discourse and Dialogue, pages 97?100.
J. Steinberger and K. Jez?ek. 2004. Using latent semantic
analysis in text summarization and summary evalua-
tion. In Proc. ISIM ?04, pages 93?100.
R. Valenza, T. Robinson, M. Hickey, and R. Tucker.
1999. Summarization of spoken audio through infor-
mation extraction. In Proc. ESCA Workshop on Ac-
cessing Information in Spoken Audio, pages 111?116.
Pierre Wellner, Mike Flynn, Simon Tucker, and Steve
Whittaker. 2005. A meeting browser evaluation test.
In CHI ?05: CHI ?05 extended abstracts on Human
factors in computing systems, pages 2021?2024, New
York, NY, USA. ACM Press.
K. Zechner and A. Waibel. 2000. Minimizing word error
rate in textual summaries of spoken language. In Proc.
NAACL-2000.
40
Proceedings of the 12th European Workshop on Natural Language Generation, pages 114?117,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
The effect of linguistic devices in information presentation messages on
comprehension and recall
Martin I. Tietze and Andi Winterboer and Johanna D. Moore
University of Edinburgh, Edinburgh, United Kingdom
mtietze@inf.ed.ac.uk, A.Winterboer@ed.ac.uk, J.Moore@ed.ac.uk
Abstract
In this paper we examine the effect of
linguistic devices on recall and compre-
hension in information presentation using
both recall and eye-tracking data. In ad-
dition, the results were validated via an
experiment using Amazon?s Mechanical
Turk micro-task environment.
1 Introduction
In this paper, we present two experiments de-
signed to examine the impact of linguistic devices,
such as discourse cues and connectives, on com-
prehension and recall in information presentation
for natural language generation (NLG) as used in
spoken dialogue systems (SDS).
Spoken dialogue systems have traditionally
used simple templates to present options (e.g.,
flights, restaurants) and their attributes to users
(Walker et al, 2004). Recently, however, re-
searchers have proposed approaches to informa-
tion presentation that use linguistic devices (e.g.,
but, however, moreover, only, just, also etc.) in
order to highlight specific properties of and rela-
tions between items presented to the user, e.g. as-
sociations (Polifroni and Walker, 2006) and con-
trasts (Winterboer and Moore, 2007). Previous
research indicates that linguistic devices such as
connectives facilitate comprehension (see Ben-
Anath, 2005, for a review). However, to our
knowledge, no empirical validation has been per-
formed to test whether using linguistic devices has
an effect on comprehension and recall of the infor-
mation presentated.
2 Experiment 1: Recall of written
materials
In order to test whether there are differences in
recall, we performed a within-participants read-
ing experiment comparing recall for experiment
material presented with or without linguistic de-
vices1 A total of 24 participants, native English
speakers and mostly students of the University of
Edinburgh, were paid to participate in the study.
They were naive to the purpose of the experi-
ment but were told that they were about to be pre-
sented with a number of consumer products and
that they were supposed to answer questions about
these. Each participant read 14 short texts describ-
ing consumer products from 14 domains, see Ta-
ble 1 and Table 2 for examples. The texts are the
type of presentation typically produced by spoken
dialogue systems designed to help users select an
entity from a set of available options. Participants?
eye-movements during reading were recorded as
described in section 3.


 

Messina?s price is ?22. It has very good food
quality, attentive service, and decent de?cor.
Ray?s price is ?34. It has very good food qual-
ity, excellent service, and impressive de?cor.
Alhambra?s price is ?16. It has good food
quality, bad service, and plain de?cor.
Figure 1: Experiment material without discourse
cues


 

Messina?s price is ?22. It has very good food
quality, attentive service, and decent de?cor.
Ray?s price is ?34. It has also very good food
quality, but excellent service, and moreover
impressive de?cor.
Alhambra?s price is only ?16. It has good
food quality, but bad service, and only plain
de?cor.
Figure 2: Experiment material with discourse cues
There were two types of messages, one con-
taining linguistic devices to point out similari-
1This experiment has been presented as an one-page ab-
stract, (Winterboer et al, 2008)
114
ties and differences among the options, and one
without these linguistic markers. Each participant
read seven texts of each type, alternating between
types. Ordering of both the domains and the text
type was controlled for. We took particular care
to add discourse devices without modifying the
propositions in any other way. After each mes-
sage, the participant had to answer three questions
testing different levels of recall. Examples of each
type of question are given in figure 3.


 

1. Verbatim questions: Which restaurant?s
price is ?34?
2. Comparison questions: Which restau-
rant is the cheapest?
3. Evaluation questions: Which restaurant
would you like to go to and why?
Figure 3: The three types of evaluation questions
with examples
2.1 Experimental procedure
In each trial, participants read a text presented
for up to 45 seconds on the screen. Users could
press Enter on the keyboard when they were fin-
ished reading. They were then presented with the
questions, which they had to answer one after the
other. After a question was presented, the partic-
ipant pressed Enter to be prompted to type in an
answer.
2.2 Results
Overall, we found a consistent numerical trend
indicating that items in messages containing lin-
guistic devices could be recalled more easily (see
Table 2.2). In particular, answers to compari-
son questions were correctly recalled significantly
more often when linguistic markers were present.
Verb. Q. Comp. Q. Eval. Q.
w/o cues 0.79 0.68* 0.73
with cues 0.82 0.79* 0.81
Figure 4: Average recall on a scale from 0 to 1 for
the 3 questions. t-test, ?*? indicates a significant
difference with p < 0.5.
3 Comprehension of written materials
In this experiment we used an eye-tracker in or-
der to measure reading times, because reading
times are considered to be sensitive to people?s on-
going discourse processing/comprehension (Hav-
iland and Clark, 1974). We found that read-
ing the presentation messages containing linguis-
tic devices took generally slightly longer, with par-
ticipants reading messages containing discourse
cues taking 37.93 seconds per message on aver-
age, and messages without discourse cues taking
35.28 seconds on average to read. The question,
however, was whether this difference could be at-
tributed exclusively to the number of additional
words or whether readers also spent more time to
build a mental representation of the presentation?s
content by reading the parts marked by discourse
cues more carefully. Alternatively, sentence com-
plexity might also increase with the introduction
of linguistic cues, which in turn increases read-
ing times. In order to answer this question, we
compared the reading times of interest areas (IA)
located directly (one word) after the (potential) lo-
cation of the discourse marker. In total, we deter-
mined 46 IAs within the 14 messages, each one
consisting of two words or around nine characters
on average.
3.1 Results
The results of the different reading time mea-
sures, established with linear-mixed effects model
(LME) analyses in R2 (see Table 1), do not reveal
any significant differences between the two con-
ditions, although, surprisingly, IAs had a numer-
ically shorter reading time when linguistic mark-
ers were used. In this repeated measures de-
sign experiment, participant, IA, and item were
random-effect factors and the fixed-effect factor
was whether the presentation contained linguis-
tic devices. We compared first pass and remain-
ing pass reading times per IA, the total number of
passes, and regressions in and out of the IA.
Although sentences containing linguistic de-
vices are more complex and thus should incur
longer reading times, our analyses do not any dif-
ferences in reading times for the words directly
following the linguistic devices. The differences
in the overall reading times noted above are there-
fore due to the additional words (the linguistic de-
vices) and not caused by differences in sentence
complexity or increased effort towards the marked
parts of the text.
2www.r-project.org
115
RT FPRT NoP RegrIn RegrOut
with cues 473.83 1055.56 3.639 0.430 0.322
w/o cues 510.24 1150.70 3.567 0.494 0.350
t = -1.511 t = -0.820 t = 0.625 t = -1.002 t = -0.519
p = 0.131 p = 0.412 p = 0.5321 p = 0.3164 p =0.6039
Table 1: Eye-tracking data per IA (first pass reading times, remaining time reading times, number of
passes, regressions out and in) for messages with and without discourse cues
4 Experiment 2: Web-based recall of
written materials
We carried out a web-based user study on Ama-
zon?s Mechanical Turk3 (MT) platform both in or-
der to verify the results obtained in the previous
recall experiment and in order to test whether re-
sults obtained from casual website users are com-
parable to those obtained from laboratory partici-
pants who focus exclusively on performing the ex-
periment in the lab. We recruited native English
speakers online to carry out the same experiment
previously conducted in the lab. MT is a web-
based micro-task platform that allows researchers
and developers to put small tasks requiring human
intelligence on the web. Deploying MT is advan-
tageous because it attracts many visitors due to its
affiliation with the well established Amazon web-
site and thus eases recruitment of new participants
especially from outside the usual student popula-
tion. In addition, conducting experiments online
significantly reduces the effort involved in data
collection for the experimenter. Moreover, the
website allows for convenient payment for both
participants and the experimenter. For these rea-
sons, MT has recently been used in a number of
language experiments (e.g., Kaisser et al, 2008;
Kittur et al, 2008).
4.1 Participants
We had 60 participants reading the same mate-
rials that were used in experiment 1. MT does
allow to place restrictions on participant location
(only users from the US were allowed to partic-
ipate to ensure English language skills), for in-
stance, or the number of trials (each participant
was only allowed to participate once). However,
one cannot balance gender of participants or con-
trol for age and literacy reliably, as user provided
data cannot be verified. Also, one does not know
whether participants are conducting another task
3https://www.mturk.com/mturk/
simultaneously, or are otherwise distracted. We
paid $ 2.50 for participation, which was, given
that we expected the experiment to last less than
30 minutes, considerably more than participants
would receive for most other tasks available. We
hoped that the higher reward would encourage par-
ticipants to take the task more seriously.
4.2 Experimental setup and procedure
In order to resemble the interface that was used in
the previous experiment as closely as possible in
terms of the general ?look and feel?, a web-based
interface was implemented using Adobe?s Flash
format. We chose the widely used Flash format be-
cause it can be integrated into the MT environment
easily and allows for tighter user control in com-
parison with standard HTML pages. For example,
we made it impossible for users to reread the pre-
sented information once they read the correspond-
ing question. With standard HMTL users would
have been able to use their browser?s back button
to do just that. The experiment was then made
available to the users on Amazon?s MT website.
The procedure was otherwise exactly the same as
in experiment 1.
4.3 Results
The first thing we noticed when evaluating the data
was that it took only a couple of hours from mak-
ing the tasks available on the MT website to re-
ceiving the results. In addition, we learnt from the
submitted answers that the general answer qual-
ity was comparable to answers obtained in the
lab-based experiment. Average recall rate was
nearly identical with 0.76 (web-based) and 0.77
(lab-based). In addition, the average answer time
was also almost identical 23 minutes (web-based)
and 26 minutes (lab-based) per participant. How-
ever, the results from three of the 60 participants
had to be excluded from the analysis (and payment
withheld), as they answered less than 50% of the
questions while performing the task in less than
116
half of the average time.
We did not find an effect on the comparison
questions. Instead, this time the difference be-
tween the two conditions was significant in terms
of correct answers to the evaluation question.
Thus, we again found that using linguistic mark-
ers facilitates recall of information.
Verb. Q. Comp. Q. Eval. Q.
w/o cues 0.83 0.62 0.83*
with cues 0.80 0.65 0.88*
Figure 5: Average recall on a scale from 0 to 1
for the 3 questions in the web-based experiment.
t-test, ?*? indicates a significant difference with
p < 0.5.
5 Discussion and outlook
Taken together, we found a small but significant
effect of discourse cues on recall. The combi-
nation of eye-tracking and recall data seems to
provide a relatively clear picture: Although sen-
tences with linguistic devices took more time to
read, this is exclusively due to the additional words
and not caused by a differences in the construction
of the internal representation. While these find-
ings are in line with results from psycholinguistics
which demonstrated that linguistic devices may
improve comprehension and recall (Ben-Anath,
2005), given the small effect, it does not fully ex-
plain the improvements in terms of task effective-
ness found in information presentation for SDS
(Winterboer and Moore, 2007).
We additionally validated the results using par-
ticipants recruited online. The similar results show
that this method is applicable to the evaluation
of written language materials and adds further
strength to its establishment as an alternative to
lab-based experiments.
Nonetheless, in real-world SDSs users are pre-
sented with information about different options
auditorily. Listening to auditory stimuli should
be more difficult than reading the same stimuli,
because readers can always re-read a problematic
word or sentence, whereas auditory stimuli are
presented sequentially and are transient. However,
research on the differences between reading and
listening comprehension seems to suggest that the
findings found in reading can also be applied to
spoken stimuli due to the commonality of process-
ing between the two modalities (Sinatra, 1990).
However, to confirm this, we are repeating the ex-
periments in order to examine whether linguistic
devices also facilitate recall and comprehension in
auditorily presented messages, using stimuli cre-
ated with a speech synthesiser. We plan to use the
auditory moving window paradigm (Ferreira et al,
1996) to assess the impact of lingustic devices in
this modality in more detail.
References
D. Ben-Anath. 2005. The Role of Connectives in Text
Comprehension. Working Papers in TESOL and Ap-
plied Linguistics, 5(2):1?27.
F. Ferreira, JM Henderson, MD Anes, PA Weeks,
and DK McFarlane. 1996. Effects of Lexical
Frequency and Syntactic complexity in Spoken-
Language Comprehension: Evidence From the Au-
ditory Moving-Window Technique. Journal of ex-
perimental psychology. Learning, memory, and cog-
nition, 22(2):324?335.
S.E. Haviland and H.H. Clark. 1974. What?s new?
acquiring new information as a process in compre-
hension. Journal of Verbal Learning and Verbal Be-
haviour, 13:512?521.
Michael Kaisser, Marti Hearst, and John Lowe. 2008.
Improving Search Result Quality by Customizing
Summary Lengths. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk.
In Proceeding of the twenty-sixth annual SIGCHI
conference on Human factors in computing systems.
J. Polifroni and M. Walker. 2006. Learning database
content for spoken dialogue system design. In 5th
International Conference on Language Resources
and Evaluation (LREC).
G.M. Sinatra. 1990. Convergence of listening and
reading processing. Reading Research Quarterly,
25:115?130.
Marilyn A. Walker, Steve Whittaker, Amanda Stent,
Preetam Maloor, Johanna D. Moore, Michael John-
ston, and Gunaranjan Vasireddy. 2004. Generation
and evaluation of user tailored responses in multi-
modal dialogue. Cognitive Science, 28:811?840.
Andi Winterboer and Johanna D. Moore. 2007. Evalu-
ating information presentation strategies for spoken
recommendations. In Proceedings of the ACM con-
ference on Recommender Systems (RecSys ?07).
Andi Winterboer, Johanna D. Moore, and Fernanda
Ferreira. 2008. Do discourse cues facilitate recall
in information presentation messages? In Proceed-
ings of the 9th International Conference on Spoken
Language Processing.
117
Proceedings of the 12th European Workshop on Natural Language Generation, pages 165?173,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Report on the First NLG Challenge on
Generating Instructions in Virtual Environments (GIVE)
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the first installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE), a new
shared task for the NLG community. We
motivate the design of the challenge, de-
scribe how we carried it out, and discuss
the results of the system evaluation.
1 Introduction
This paper reports on the methodology and results
of the First Challenge on Generating Instructions
in Virtual Environments (GIVE-1), which we ran
from March 2008 to February 2009. GIVE is a
new shared task for the NLG community. It pro-
vides an end-to-end evaluation methodology for
NLG systems that generate instructions which are
meant to help a user solve a treasure-hunt task in a
virtual 3D world. The most innovative aspect from
an NLG evaluation perspective is that the NLG
system and the user are connected over the Inter-
net. This makes it possible to cheaply collect large
amounts of evaluation data.
Five NLG systems were evaluated in GIVE-
1 over a period of three months from November
2008 to February 2009. During this time, we
collected 1143 games that were played by users
from 48 countries. As far as we know, this makes
GIVE-1 the largest evaluation effort in terms of
experimental subjects ever. We have evaluated the
five systems both on objective measures (success
rate, completion time, etc.) and subjective mea-
sures which were collected by asking the users to
fill in a questionnaire.
GIVE-1 was intended as a pilot experiment in
order to establish the validity of the evaluation
methodology and understand the challenges in-
volved in the instruction-giving task. We believe
that we have achieved these purposes. At the same
time, we provide evaluation results for the five
NLG systems which will help their developers im-
prove them for participation in a future challenge,
GIVE-2. GIVE-2 will retain the successful aspects
of GIVE-1, while refining the task to emphasize
aspects that we found to be challenging. We invite
the ENLG community to participate in designing
GIVE-2.
Plan of the paper. The paper is structured as
follows. In Section 2, we will describe and moti-
vate the GIVE Challenge. In Section 3, we will
then describe the evaluation method and infras-
tructure for the challenge. Section 4 reports on
the evaluation results. Finally, we conclude and
discuss future work in Section 5.
2 The GIVE Challenge
In the GIVE scenario, subjects try to solve a trea-
sure hunt in a virtual 3D world that they have not
seen before. The computer has a complete sym-
bolic representation of the virtual world. The chal-
lenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
Complete maps of the game worlds used in the
evaluation are shown in Figs. 3?5: In these worlds,
players must pick up a trophy, which is in a wall
safe behind a picture. In order to access the tro-
165
Figure 1: What the user sees when playing with
the GIVE Challenge.
phy, they must first push a button to move the pic-
ture to the side, and then push another sequence of
buttons to open the safe. One floor tile is alarmed,
and players lose the game if they step on this tile
without deactivating the alarm first. There are also
a number of distractor buttons which either do
nothing when pressed or set off an alarm. These
distractor buttons are intended to make the game
harder and, more importantly, to require appropri-
ate reference to objects in the game world. Finally,
game worlds contained a number of objects such
as chairs and flowers that did not bear on the task,
but were available for use as landmarks in spatial
descriptions generated by the NLG systems.
2.1 Why a new NLG evaluation paradigm?
The GIVE Challenge addresses a need for a new
evaluation paradigm for natural language gener-
ation (NLG). NLG systems are notoriously hard
to evaluate. On the one hand, simply compar-
ing system outputs to a gold standard using auto-
matic comparison algorithms has limited value be-
cause there can be multiple generated outputs that
are equally good. Finding metrics that account
for this variability and produce results consistent
with human judgments and task performance mea-
sures is difficult (Belz and Gatt, 2008; Stent et
al., 2005; Foster, 2008). Human assessments of
system outputs are preferred, but lab-based eval-
uations that allow human subjects to assess each
aspect of the system?s functionality are expensive
and time-consuming, thereby favoring larger labs
with adequate resources to conduct human sub-
jects studies. Human assessment studies are also
difficult to replicate across sites, so system devel-
opers that are geographically separated find it dif-
ficult to compare different approaches to the same
problem, which in turn leads to an overall diffi-
culty in measuring progress in the field.
The GIVE-1 evaluation was conducted via a
client/server architecture which allows any user
with an Internet connection to provide system
evaluation data. Internet-based studies have been
shown to provide generous amounts of data in
other areas of AI (von Ahn and Dabbish, 2004;
Orkin and Roy, 2007). Our implementation allows
smaller teams to develop a system that will partici-
pate in the challenge, without taking on the burden
of running the human evaluation experiment, and
it provides a direct comparison of all participating
systems on the same evaluation data.
2.2 Why study instruction-giving?
Next to the Internet-based data collection method,
GIVE also differs from other NLG challenges by
its emphasis on generating instructions in a vir-
tual environment and in real time. This focus on
instruction giving is motivated by a growing in-
terest in dialogue-based agents for situated tasks
such as navigation and 3D animations. Due to its
appeal to younger students, the task can also be
used as a pedagogical exercise to stimulate interest
among secondary-school students in the research
challenges found in NLG or Computational Lin-
guistics more broadly.
Embedding the NLG task in a virtual world en-
courages the participating research teams to con-
sider communication in a situated setting. This
makes the NLG task quite different than in other
NLG challenges. For example, experiments have
shown that human instruction givers make the in-
struction follower move to a different location in
order to use a simpler referring expression (RE)
(Stoia et al, 2006). That is, RE generation be-
comes a very different problem than the classi-
cal non-situated Dale & Reiter style RE genera-
tion, which focuses on generating REs that are sin-
gle noun phrases in the context of an unchanging
world.
On the other hand, because the virtual environ-
ments scenario is so open-ended, it ? and specif-
ically the instruction-giving task ? can potentially
be of interest to a wide range of NLG researchers.
This is most obvious for research in sentence plan-
ning (GRE, aggregation, lexical choice) and real-
ization (the real-time nature of the task imposes
high demands on the system?s efficiency). But if
166
extended to two-way dialog, the task can also in-
volve issues of prosody generation (i.e., research
on text/concept-to-speech generation), discourse
generation, and human-robot interaction. Finally,
the game world can be scaled to focus on specific
issues in NLG, such as the generation of REs or
the generation of navigation instructions.
3 Evaluation Method and Logistics
Now we describe the method we applied to obtain
experimental data, and sketch the software infras-
tructure we developed for this purpose.
3.1 Software architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components (shown in Fig. 2):
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide. When a user starts the
client, it connects to the Matchmaker and is ran-
domly assigned an NLG server and a game world.
The client and NLG server then communicate over
the course of one game. At the end of the game,
the client displays a questionnaire to the user, and
the game log and questionnaire data are uploaded
to the Matchmaker and stored in a database. Note
that this division allows the challenge to be con-
ducted without making any assumptions about the
internal structure of an NLG system.
The GIVE software is implemented in Java and
available as an open-source Google Code project.
For more details about the software, see (Koller et
al., 2009).
3.2 Subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the internet.
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 2: The GIVE architecture.
Collecting data from anonymous users over the
Internet presents a variety of issues that a lab-
based experiment does not. An Internet-based
evaluation skews the demographic of the subject
pool toward people who use the Internet, but prob-
ably no more so than if recruiting on a college
campus. More worrisome is that, without a face-
to-face meeting, the researcher has less confidence
in the veracity of self-reported demographic data
collected from the subject. For the purposes of
NLG software, the most important demographic
question is the subject?s fluency in English. Play-
ers of the GIVE 2009 challenge were asked to self-
report their command of English, age, and com-
puter experience. English proficiency did interact
with task completion, which leads us to conclude
that users were honest about their level of English
proficiency. See section 4.4 below for a discus-
sion of this interaction. All-in-all, we feel that the
advantage gained from the large increase in the
size of the subject pool offsets any disadvantage
accrued from the lack of accurate demographic in-
formation.
3.3 Materials
Figs. 3?5 show the layout of the three evaluation
worlds. The worlds were intended to provide vary-
ing levels of difficulty for the direction-giving sys-
tems and to focus on different aspects of the prob-
lem. World 1 is very similar to the development
world that the research teams were given to test
their system on. World 2 was intended to focus
on object descriptions - the world has only one
room which is full of objects and buttons, many of
which cannot be distinguished by simple descrip-
tions. World 3, on the other hand, puts more em-
phasis on navigation directions as the world has
many interconnected rooms and hallways.
The difference between the worlds clearly bears
out in the task completion rates reported below.
167
plant
chair
alarm
lamp
tutorial room
couch
safe
Figure 3: World 1
lamp
plant
chair
alarm
tutorial room
safe
Figure 4: World 2
plant
chair
lamp
safe
tutorial room
alarm
Figure 5: World 3
3.4 Timeline
After the GIVE Challenge was publicized in
March 2008, eight research teams signed up for
participation. We distributed an initial version of
the GIVE software and a development world to
these teams. In the end, four teams submitted
NLG systems. These were connected to a cen-
tral Matchmaker instance that ran for about three
months, from 7 November 2008 to 5 February
2009. During this time, we advertised participa-
tion in the GIVE Challenge to the public in order
to obtain experimental subjects.
3.5 NLG systems
Five NLG systems were evaluated in GIVE-1:
1. one system from the University of Texas at
Austin (?Austin? in the graphics below);
2. one system from Union College in Schenec-
tady, NY (?Union?);
3. one system from the Universidad Com-
plutense de Madrid (?Madrid?);
4. two systems from the University of Twente:
one serious contribution (?Twente?) and one
more playful one (?Warm-Cold?).
Of these systems, ?Austin? can serve as a base-
line: It computes a plan consisting of the actions
the user should take to achieve the goal, and at
each point in the game, it realizes the first step
in this plan as a single instruction. The ?Warm-
Cold? system generates very vague instructions
that only tell the user if they are getting closer
(?warmer?) to their next objective or if they are
moving away from it (?colder?). We included this
system in the evaluation to verify whether the eval-
uation methodology would be able to distinguish
such an obviously suboptimal instruction-giving
strategy from the others.
Detailed descriptions of these systems
as well as each team?s own analysis of
the evaluation results can be found at
http://www.give-challenge.org/
research/give-1.
4 Results
We now report on the results of GIVE-1. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures.
Notice that some of our evaluation measures are
in tension with each other: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?), such as the ?Austin? baseline, will lead the
user to completing the task in a minimum number
of steps; but it will require more instructions than
a system that aggregates these. This is intentional,
and emphasizes both the pilot experiment char-
acter of GIVE-1 and our desire to make GIVE a
friendly comparative challenge rather than a com-
petition with a clear winner.
4.1 Demographics
Over the course of three months, we collected
1143 valid games. A game counted as valid if the
game client didn?t crash, the game wasn?t marked
as a test game by the developers, and the player
completed the tutorial.
Of these games, 80.1% were played by males
and 9.9% by females; a further 10% didn?t specify
their gender. The players were widely distributed
over countries: 37% connected from an IP address
in the US, 33% from an IP address in Germany,
and 17% from China; Canada, the UK, and Aus-
tria also accounted for more than 2% of the partic-
168
037,5
75,0
112,5
150,0
N
o
v
 
7
D
e
c
 
1
J
a
n
 
1
F
e
b
 
1
F
e
b
 
5
# games per day
German
press release
US
press release
posted to
SIGGEN list
covered by
Chinese blog
Figure 6: Histogram of the connections per day.
ipants each, and the remaining 2% of participants
connected from 42 further countries. This imbal-
ance stems from very successful press releases that
were issued in Germany and the US and which
were further picked up by blogs, including one
in China. Nevertheless, over 90% of the partici-
pants who answered this question self-rated their
English proficiency as ?good? or better. About
75% of users connected with a client running on
Windows, with the rest split about evenly among
Linux and Mac OS X.
The effect of the press releases is also plainly
visible if we look at the distribution of the valid
games over the days from November 7 to Febru-
ary 5 (Fig. 6). There are huge peaks at the
very beginning of the evaluation period, coincid-
ing with press releases through Saarland Univer-
sity in Germany and Northwestern University in
the US, which were picked up by science and tech-
nology blogs on the Web. The US peak contains
a smaller peak of connections from China, which
were sparked by coverage in a Chinese blog.
4.2 Objective measures
We then extracted objective and subjective mea-
surements from the valid games. The objective
measures are summarized in Fig. 7. For each sys-
tem and game world, we measured the percent-
age of games which the users completed success-
fully. Furthermore, we counted the numbers of in-
structions the system sent to the user, measured
the time until task completion, and counted the
number of low-level steps executed by the user
(any key press, to either move or manipulate an
object) as well as the number of task-relevant ac-
tions (such as pushing a button to open a door).
? task success (Did the player get the trophy?)
? instructions (Number of instructions pro-
duced by the NLG system.?)
? steps (Number of all player actions.?)
? actions (Number of object manipulation
action.?)
? second (Time in seconds.?)
?
Measured from the end of the tutorial until the
end of the game.
Figure 7: Objective measurements
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
success
40% 71% 35% 73% 18%
A A
B B
C
instructions
83.2 58.3 121.2 80.3 190.0
A
B B
C
D
steps
103.6 124.3 160.9 117.5 307.4
A A
B B
C
D
actions
11.2 8.7 14.3 9.0 14.3
A A
B
C C
seconds
129.3 174.8 207.0 175.2 312.2
A
B B
C
D
Figure 8: Objective measures by system. Task
success is reported as the percentage of suc-
cessfully completed games. The other measures
are reported as the mean number of instruc-
tions/steps/actions/seconds, respectively. Letters
group indistinguishable systems; systems that
don?t share a letter were found to be significantly
different with p < 0.05.
169
To ensure comparability, we only counted success-
fully completed games for all these measures, and
only started counting when the user left the tutorial
room. Crucially, all objective measures were col-
lected completely unobtrusively, without requiring
any action on the user?s part.
Fig. 8 shows the results of these objective mea-
sures. This figure assigns systems to groups A,
B, etc. for each evaluation measure. Systems in
group A are better than systems in group B, etc.;
if two systems don?t share the same letter, the dif-
ference between these two systems is significant
with p < 0.05. Significance was tested using a
?2-test for task success and ANOVAs for instruc-
tions, steps, actions, and seconds. These were fol-
lowed by post-hoc tests (pairwise ?2 and Tukey)
to compare the NLG systems pairwise.
Overall, there is a top group consisting of
the Austin, Madrid, and Union systems: While
Madrid and Union outperform Austin on task suc-
cess (with 70 to 80% of successfully completed
games, depending on the world), Austin signifi-
cantly outperforms all other systems in terms of
task completion time. As expected, the Warm-
Cold system performs significantly worse than all
others in almost all categories. This confirms the
ability of the GIVE evaluation method to distin-
guish between systems of very different qualities.
4.3 Subjective measures
The subjective measures, which were obtained by
asking the users to fill in a questionnaire after each
game, are shown in Fig. 9. Most of the questions
were answered on 5-point Likert scales (?overall?
on a 7-point scale); the ?informativity? and ?tim-
ing? questions had nominal answers. For each
question, the user could choose not to answer.
The results of the subjective measurements are
summarized in Fig. 10, in the same format as
above. We ran ?2-tests for the nominal variables
informativity and timing, and ANOVAs for the
scale data. Again, we used post-hoc pairwise ?2-
and Tukey-tests to compare the NLG systems to
each other one by one.
Here there are fewer significant differences be-
tween different groups than for the objective mea-
sures: For the ?play again? category, there is
no significant difference at all. Nevertheless,
?Austin? is shown to be particularly good at navi-
gation instructions and timing, whereas ?Madrid?
outperforms the rest of the field in ?informativ-
7-point scale items:
overall: What is your overall evaluation of the quality of the
direction-giving system? (very bad 1 . . . 7 very good)
5-point scale items:
task difficulty: How easy or difficult was the task for you to
solve? (very difficult 1 2 3 4 5 very easy)
goal clarity: How easy was it to understand what you were
supposed to do? (very difficult 1 2 3 4 5 very easy)
play again: Would you want to play this game again? (no
way! 1 2 3 4 5 yes please!)
instruction clarity: How clear were the directions? (totally
unclear 1 2 3 4 5 very clear)
instruction helpfulness: How effective were the directions at
helping you complete the task? (not effective 1 2 3 4 5
very effective)
choice of words: How easy to understand was the system?s
choice of wording in its directions to you? (totally un-
clear 1 2 3 4 5 very clear)
referring expressions: How easy was it to pick out which ob-
ject in the world the system was referring to? (very hard
1 2 3 4 5 very easy)
navigation instructions: How easy was it to navigate to a par-
ticular spot, based on the system?s directions? (very
hard 1 2 3 4 5 very easy)
friendliness: How would you rate the friendliness of the sys-
tem? (very unfriendly 1 2 3 4 5 very friendly)
Nominal items:
informativity: Did you feel the amount of information you
were given was: too little / just right / too much
timing: Did the directions come ... too early / just at the right
time / too late
Figure 9: Questionnaire items
ity?. In the overall subjective evaluation, the ear-
lier top group of Austin, Madrid, and Union is
confirmed, although the difference between Union
and Twente is not significant. However, ?Warm-
Cold? again performs significantly worse than all
other systems in most measures. Furthermore, al-
though most systems perform similarly on ?infor-
mativity? and ?timing? in terms of the number of
users who judged them as ?just right?, there are
differences in the tendencies: Twente and Union
tend to be overinformative, whereas Austin and
Warm-Cold tend to be underinformative; Twente
and Union tend to give their instructions too late,
whereas Madrid and Warm-Cold tend to give them
too early.
170
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
difficulty
4.3 4.3 4.0 4.3 3.5
A A A A
B
goal clarity
4.0 3.7 3.9 3.7 3.3
A A A A
B
play again
2.8 2.6 2.4 2.9 2.5
A A A A A
instruction
clarity
4.0 3.6 3.8 3.6 3.0
A A A
B B B
C
instruction
helpfulness
3.8 3.9 3.6 3.7 2.9
A A A A
B
informativity
46% 68% 51% 56% 51%
A
B B B B
overall
4.9 4.9 4.3 4.6 3.6
A A A
B B
C
choice of
words
4.2 3.8 4.1 3.7 3.5
A A
B B
C C C
referring
expressions
3.4 3.9 3.7 3.7 3.5
A A A
B B B B
navigation
instructions
4.6 4.0 4.0 3.7 3.2
A
B B B
C
timing
78% 62% 60% 62% 49%
A
B B B
C C
friendliness
3.4 3.8 3.1 3.6 3.1
A A A
B B B
Figure 10: Subjective measures by system. Infor-
mativity and timing are reported as the percentage
of successfully completed games. The other mea-
sures are reported as the mean rating received by
the players. Letters group indistinguishable sys-
tems; systems that don?t share a letter were found
to be significantly different with p < 0.05.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, there may be other factors which also influ-
ence the outcome of our objective and subjective
measures. We tested the following five factors:
evaluation world, gender, age, computer expertise,
and English proficiency (as reported by the users
on the questionnaire). We found that there is a sig-
nificant difference in task success rate for different
evaluation worlds and between users with different
levels of English proficiency.
The interaction graphs in Figs. 11 and 12 also
suggest that the NLG systems differ in their ro-
bustness with respect to these factors. ?2-tests
that compare the success rate of each system in
the three evaluation worlds show that while the
instructions of Union and Madrid seem to work
equally well in all three worlds, the performance
of the other three systems differs dramatically be-
tween the different worlds. Especially World 2
was challenging for some systems as it required
relational object descriptions, such as the blue but-
ton on the left of another blue button.
The players? English skills also affected the sys-
tems in different ways. While Austin, Madrid and
Warm Cold don?t manage to lead players with only
basic English skills to success as often as other
players, Union?s and Twente?s success rates do not
depend on the players? English skills (?2-tests do
not find significant differences in success rate be-
tween players with different levels of English pro-
ficiency for these two systems). However, if we
remove the players with the lowest level of En-
glish proficiency, language skills do not have an
effect on the task success rate anymore for any of
the systems.
5 Conclusion
In this document, we have described the first in-
stallment of the GIVE Challenge, our experimen-
tal methodology, and the results. Altogether, we
collected 1143 valid games for five NLG systems
over a period of three months. Given that this was
the first time we organized the challenge, that it
was meant as a pilot experiment from the begin-
ning, and that the number of games was sufficient
to get significant differences between systems on
a number of measures, we feel that GIVE-1 was a
success. We are in the process of preparing sev-
eral diagnostic utilities, such as heat maps and a
tool that lets the system developer replay an indi-
171
Figure 11: Effect of the evaluation worlds on the
success rate of the NLG systems.
vidual game, which will help the participants gain
further insight into their NLG systems.
Nevertheless, there are a number of improve-
ments we will make to GIVE for future install-
ments. For one thing, the timing of the challenge
was not optimal: A number of colleagues would
have been interested in participating, but the call
for participation came too late for them to acquire
funding or interest students in time for summer
projects or MSc theses. Secondly, although the
software performed very well in handling thou-
sands of user connections, there were still game-
invalidating issues with the 3D graphics and the
networking code that were individually rare, but
probably cost us several hundred games. These
should be fixed for GIVE-2. At the same time,
we are investigating ways in which the networking
and matchmaking core of GIVE can be factored
out into a separate, challenge-independent system
on which other Internet-based challenges can be
built. Among other things, it would be straightfor-
ward to use the GIVE platform to connect two hu-
man users and observe their dialogue while solv-
ing a problem. Judicious variation of parameters
(such as the familiarity of users or the visibility of
an instruction giving avatar) would allow the con-
struction of new dialogue corpora along such lines.
Finally, GIVE-1 focused on the generation of
navigation instructions and referring expressions,
in a relatively simple world, without giving the
Figure 12: Effect of the players? English skills on
the success rate of the NLG systems.
user a chance to talk back. The high success rate
of some systems in this challenge suggests that
we need to widen the focus for a future GIVE-
2 ? by allowing dialogue, by making the world
more complex (e.g., allowing continuous rather
than discrete movements and turns), by making the
communication multi-modal, etc. Such extensions
would require only rather limited changes to the
GIVE software infrastructure. We plan to come to
a decision about such future directions for GIVE
soon, and are looking forward to many fruitful dis-
cussions about this at ENLG.
Acknowledgments. We are grateful to the par-
ticipants of the 2007 NSF/SIGGEN Workshop on
Shared Tasks and Evaluation in NLG and many
other colleagues for fruitful discussions while we
were designing the GIVE Challenge, and to the
organizers of Generation Challenges 2009 and
ENLG 2009 for their support and the opportunity
to present the results at ENLG. We also thank the
four participating research teams for their contri-
butions and their patience while we were working
out bugs in the GIVE software. The creation of
the GIVE infrastructure was supported in part by
a Small Projects grant from the University of Ed-
inburgh.
172
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. Stoia, D. M. Shockley, D. K. Byron, and E. Fosler-
Lussier. 2006. Noun phrase generation for situated
dialogs. In Proceedings of INLG, Sydney.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
173
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 1?10,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Evaluating the Effectiveness of Information Presentation
in a Full End-To-End Dialogue System
Taghi Paksima
Enterprise Search Group
Microsoft
D-81669 Munich, Germany
taghi.paksima@microsoft.com
Kallirroi Georgila
Institute for Creative Technologies
University of Southern California
Marina del Rey, CA 90292, USA
kgeorgila@ict.usc.edu
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
j.moore@ed.ac.uk
Abstract
Recent work on information presenta-
tion in dialogue systems combines user
modelling (UM) and stepwise refine-
ment through clustering and summarisa-
tion (SR) in the UMSR approach. An eval-
uation in which participants rated dialogue
transcripts showed that UMSR presents
complex trade-offs understandably, pro-
vides users with a good overview of their
options, and increases users? confidence
that all relevant options have been pre-
sented (Demberg and Moore, 2006). In
this paper, we evaluate the effectiveness
of the UMSR approach in a more realis-
tic setting, by incorporating this informa-
tion presentation technique into a full end-
to-end dialogue system in the city infor-
mation domain, and comparing it with the
traditional approach of presenting infor-
mation sequentially. Our results suggest
that despite complications associated with
a real dialogue system setting, the UMSR
model retains its advantages.
1 Introduction
Spoken dialogue systems (SDS) that help users
find a desired option (e.g., flight, restaurant,
movie) from the set of options satisfying their con-
straints typically present options sequentially, or-
dered along a default dimension (e.g., by price or
departure time). An example is shown in Fig. 1.
The user can then navigate through the options
and refine them by offering new constraints until
a suitable option has been found. However, when
the number of available options is large, this pro-
cess can be painstaking, leading to long dialogues
There are six restaurant options matching your query.
Number 1: Voujon offers a bright, airy and contempo-
rary dining area, with simple floral displays and leather
seating. It serves Indian cuisine. It is located in the city
centre. The average price is ?24 per person.
Number 2: Saffrani?s decor is modern, the dining room
wee, though the menu is enormous, and the atmosphere
charming. It offers new Indian dishes never before seen
in Edinburgh. It serves Indian, seafood cuisine. It is
located in the city centre. The average price is ?28 per
person.
Number 3: Britannia Spice . . .
Figure 1: Example of sequential information pre-
sentation in the city information domain (modi-
fied version of the TownInfo system (Lemon et al,
2006)).
and reduced user satisfaction. Thus a major chal-
lenge in the development of SDS is to improve
information presentation algorithms. This is im-
portant for several reasons: (1) to avoid overbur-
dening the user?s memory by presenting too many
options; (2) to ensure that the user is given an
overview of the available option space so that the
optimal option can be found; and (3) to minimise
the number of dialogue turns (hence dialogue du-
ration) required for the user to find an acceptable
option. As Walker et al (2001) showed, failing to
meet this third goal may reduce overall user satis-
faction.
Recently several approaches have been pro-
posed to overcome the shortcomings of the se-
quential enumeration strategy (Polifroni et al,
2003; Chung, 2004; Demberg and Moore, 2006;
Polifroni and Walker, 2008). Because of the com-
plexity of building a complete end-to-end SDS,
these approaches have been evaluated using an
?overhearer? methodology in which dialogues are
either hand-crafted or simulated and then pre-
sented to subjects, either as textual transcripts
1
(Demberg and Moore, 2006; Polifroni and Walker,
2008) or audio recordings (Walker et al, 2004),
for rating. The general consensus from these stud-
ies is that users significantly prefer approaches
that take their preferences into account. How-
ever, because users were not interacting with these
SDS, the evaluation criteria were limited to users?
perceptions (e.g., informativeness, good overview
of options, confidence in choice, etc.), and met-
rics such as effectiveness (i.e., actual or perceived
task completion) and efficiency (i.e., length of di-
alogue) could not be assessed. To address this
issue, Winterboer and Moore (2007) carried out
a Wizard-of-Oz (WOz) study in which users par-
ticipated in dialogues controlled by two different
information presentation algorithms. They found
that not only did users prefer presentations based
on a user model, dialogues employing the ?user-
model based summarise and refine? (UMSR) ap-
proach led to greater task success and dialogue ef-
ficiency.
In this paper, we take this one step further, and
evaluate the effectiveness of the UMSR approach
in a more realistic setting, incorporating this con-
tent selection and presentation strategy into a full
end-to-end dialogue system, and comparing it to
the traditional sequential enumeration approach.
Our results suggest that despite complications as-
sociated with a real dialogue system setting, the
UMSR model retains its advantages. Our results
also verify the hypothesis that the UMSR model
presents complex trade-offs in a concise, yet un-
derstandable way. Furthermore, as in the WOz
study, the UMSR approach leads to a significant
reduction in the number of dialogue turns.
The structure of the paper is as follows: In
Sec. 2, we discuss related work. In Sec. 3 we
present the full end-to-end SDS used for com-
parison between the standard sequential enumer-
ation approach and the UMSR approach. In Sec. 4
we describe how we implemented the UMSR ap-
proach. Then in Sec. 5 we provide an example. In
Sec. 6 we describe our experimental design and in
Sec. 7 our results. Finally in Sec. 8, we present our
conclusions.
2 Previous Approaches
As noted above, a number of approaches to in-
formation presentation in SDS have recently been
proposed. The user-model based (UM) approach
employs a model of the users preferences and de-
cision theory techniques to identify and present a
small number of options that best match the user?s
preferences (Carenini and Moore, 2001; Walker et
al., 2004; Moore et al, 2004). Fig. 2 shows a sam-
ple presentation generated using the UM approach
for a student user who cares most about price and
flying direct.
There?s a direct flight on BMI with a good price. It
arrives at four ten p.m. and costs a hundred and twelve
pounds. The cheapest flight is on Ryanair. It arrives at
twelve forty-five p.m. and costs just fifty pounds, but it
requires a connection in Dublin.
Figure 2: Example of information presentation
based on the UM approach, from (Moore et al,
2004).
Although the UM approach enables a concise
presentation of a small number of options, high-
lighting the ways in which these options satisfy
user preferences, it does not scale up to presenting
a large number of options. When there are hun-
dreds of potentially relevant options to consider
(e.g., all Italian restaurants in New York City),
there may be a large number of options that fit
the user model, making it difficult to narrow down
the number of options. In addition, users may
not be able to provide constraints until they are
presented with more details concerning the option
space. Moreover, the system does not provide an
overview of the available options, which may lead
to the user?s actual or perceived missing out on
possibly better alternatives.
An alternative approach, which we call the sum-
marise and refine approach (SR) (Polifroni et al,
2003; Chung, 2004) structures large datasets for
summarisation and successive refinement. It sup-
ports the user in narrowing in on a suitable option
by grouping the options in the database into clus-
ters of options with similar features. The system
can then summarise the common attributes of the
options and thus suggest further refinement con-
straints to the user. In this content structuring ap-
proach, the prompts presented to the user and the
order in which they appear, are determined at run-
time based on an algorithm that computes the most
useful set of attributes, as dictated by the set of op-
tions that satisfy the user query. For large datasets,
attributes that partition the data into the minimal
number of clusters are chosen, so that a concise
summary can be presented to the user to refine
(Fig. 3).
In addition to its many advantages, the SR ap-
proach also has several limitations. First, depend-
ing on the size and complexity of the domain,
many turns may be required during the refinement
2
M: What restaurants do you have?
S: I have found 983 restaurants. Most of them are
located in Boston and Cambridge. There are 32
choices for cuisine. I also have information about
price range.
M: Okay tell me about the ones in Boston.
S: I have found 401 restaurants in Boston. There are
29 choices for cuisine.
M: Do you have any that serve seafood?
S: I have found 19 seafood restaurants. They are
predominantly in Back Bay, the North End, South
Boston and the South End.
Figure 3: Sample dialogue between simulator (M)
and SR system (S), from (Polifroni et al, 2003).
process. Because the user?s preferences are not
taken into account, the clusters may contain many
irrelevant entities, which must be filtered out suc-
cessively with each refinement step. Second, if
there is no optimal solution, exploration of trade-
offs between options can be difficult. Finally, the
chosen clusters may be based on attributes that are
irrelevant for the specific user.
In an attempt to combine the benefits of the UM
and SR approaches, Demberg & Moore (2006)
devised the user-model based summarise and re-
fine (UMSR) approach to information presenta-
tion. This approach first clusters the values of each
attribute in order to group them so that the op-
tions can be summarised more easily later, and la-
bels like ?cheap?, ?moderate?, ?expensive? can be
assigned to values of continuous categories such
as ?price?. The system then structures options
into an option tree based on the ranking of at-
tributes in the user model, the options returned
from the database, and the attribute-value clus-
tering. The resulting option tree determines how
different options relate to one another, and which
ones are most attractive for the user. After the tree
structure is built, it is pruned to decide which op-
tions are compelling to the user according to the
user model. This allows the system to save time
by omitting options that are not of any potential
interest to the user. Once pruning is complete,
each branch of the tree describes a possible refine-
ment path, and thus can be used to direct dialogue
flow. Trade-offs between alternative options are
presented explicitly in order to provide the user
with a better overview of the option space. In ad-
dition, to give users confidence that they are being
presented with all of the relevant options, a brief
account of all the remaining (irrelevant) options is
also provided. For a more detailed discussion of
the UMSR approach, see (Demberg and Moore,
2006). In Sec. 4 we describe how we employed
the UMSR approach in our system.
3 The TownInfo System
The TownInfo SDS was developed as part of the
EC project TALK (Lemon et al, 2006). Users
can search for hotels, bars and restaurants in
an artificial town. The system supports two di-
alogue strategies, one hand-crafted and another
learnt using Reinforcement Learning (Henderson
et al, 2008). For the current experiment we used
the hand-crafted strategy. Natural language un-
derstanding is performed using a keyword-based
parser and natural language generation is based
on templates. The information presentation is se-
quential. An example is given in Fig. 1, taken
from the modified version of TownInfo for the cur-
rent experiment. Although the original TownInfo
system supported speech input and speech output,
here we use text input/output to make sure that our
results are not influenced by poor recognition ac-
curacy or intelligibility due to poor speech syn-
thesis. Of course, as we mention in Sec. 8, the
next step would be to perform an experiment with
speech input/output.
For our current experiment we focussed on
restaurant recommendations and the TownInfo
database had to be extended to include a much
wider range of options to provide more realistic
information presentation scenarios. The database
used in our experiments contains a total of 80
restaurants in Edinburgh, UK.
4 The UMSR Algorithm
This section briefly describes our implementation
of the UMSR algorithm; for more details see
(Demberg and Moore, 2006). Sec. 5 provides an
example for clarity.
4.1 The User Model
The user model contains the user?s ranking and
preferred values for the relevant attributes in the
restaurant domain: price, distance, star
rating, service rating, and cuisine
type. Table 1 shows a sample user model. The
Rank field indicates the relative importance of the
attributes for the user, with 1 being most impor-
tant. The Value field indicates the user?s preferred
value for each attribute.1
1If two attributes in a user model have identical ranks, the
order of the preferences is used to decide which has a higher
priority.
3
UserID Attribute Value Rank
1 Price Cheap 1.00
1 Distance Near 2.00
1 Star High 3.00
1 Cuisine Indian 4.00
1 Service Don?t Care 5.00
Table 1: Sample user model for a student.
According to Elzer et al (1994), some prefer-
ences are enough to reject options outright (and
therefore are more like goals) whereas others are
more purely like preferences (to be weighed and
ranked). Here we do not make such a distinction.
4.2 Adapting to Changes to the User Model
In the original design, the user model was cre-
ated at the outset and not modified during the dia-
logue. However, during initial piloting of the sys-
tem, we found that this design did not support ?sit-
uational preferences?. For example, consider the
user model for the student in Table 1. This user
normally prefers to have Indian food if she has the
option to (a ?dispositional preference?). If, how-
ever, in the current situation she is entertaining a
friend from out of town who wishes to try Scottish
food, the user may decide to explore options for
Scottish cuisine (a ?situational preference?). Here,
the user changes her original query for the situa-
tion, thus redefining her preferences. When this
occurs, we must perform a new database query
and rebuild the option tree. To take these dynamic
changes into account during the course of the dia-
logue, at each dialogue turn the user query is com-
pared against the user model, and if any difference
is noted, the user model is updated to reflect the
current preferences, the tree is rebuilt using the
new user model, and the dialogue continues with
a summary of the available options based on this
new tree.
Note that for individual models, i.e. user models
that are designed for individual people and not for
classes of users (student or business person), some
queries could justify situational changes and some
could indicate permanent (or at least less tempo-
rary) changes to the user model (e.g., ?Are there
any nicer restaurants? I got a new job?). In our
experiment we use only class models and we do
not allow permanent changes to the user model.
4.3 The Clustering Algorithm
Following (Polifroni et al, 2003) and (Demberg
and Moore, 2006), we used agglomerative group-
average clustering to automatically group values
for each attribute. The algorithm begins by assign-
ing each unique attribute value to its own bin, and
successively merging bins whose means are most
similar until a stopping criterion (a target of no
more than three clusters, in our implementation)
is met. The bins are then assigned predefined la-
bels, e.g., ?cheap?, ?moderately priced? and ?ex-
pensive? for price. Clustering attribute values
with this algorithm allows for database-dependent
labelling. Therefore, a restaurant with a price of
?35 might be considered as expensive for Edin-
burgh, but inexpensive for London.
4.4 Building the Option Tree
The tree building algorithm is recursive. It begins
at the root node, which contains all entities in the
retrieved dataset, and builds up the tree level by
level based on the ranking of attributes in the user
model. At each node of the tree, it retrieves the
next attribute preference from the user model and
then invokes the clustering algorithm for this at-
tribute?s values. Once the current dataset has been
clustered, the algorithm then adds the resultant
clusters as the children of the current node. Af-
ter each cluster is added, the algorithm is invoked
recursively on the newly created children of the
current node.
As the tree is being constructed, the algorithm
arranges the nodes in the tree such that the children
of each node are ordered from left to right in de-
creasing order of desirability. For example, if the
particular user prefers restaurants that are far from
the city centre, the clusters based on distance
would be ordered such that ?far? is the leftmost
child and ?near? is the rightmost child. Fig. 5 de-
picts an option tree structure for the user model of
Table 1, in the context of the example of Sec. 5.
The numbers in the nodes indicate how many op-
tions are represented by the node.
Given an option tree ordered in this way, to find
the best available options, the system traverses the
tree in a depth-first fashion starting from the root
and selecting the leftmost branch at each node.
4.5 Pruning the Option Tree
The goal of the UMSR algorithm is to present an
overview of the available options, that are most
relevant to the user?s preferences, concisely and
understandably. To determine the relevance of op-
tions, we use the notion of ?dominance? defined
in Demberg & Moore (2006). Dominant options
are those for which there is no other option in the
dataset that is better on all attributes. A domi-
4
nated option is in all respects equal to or worse
than some other option in the relevant subset of
the database; it should not be of interest for any
rational user.
The pruning algorithm follows Demberg &
Moore (2006), and thus we summarise it only
briefly here. The algorithm operates directly on
the ordered option tree, using the tree structure so
that it can efficiently determine dominance rela-
tions without having to compare each pair of op-
tions. The algorithm traverses the tree in depth-
first order, generating constraints during this pro-
cess. These constraints encode the properties that
other options would need to satisfy in order not to
be dominated by the options which have already
been deemed to be dominant. A node must ful-
fil the constraints that apply to it, otherwise it is
pruned from the tree. If an option (or a cluster of
options) satisfies a constraint, the property that sat-
isfied the constraint is marked as the options? jus-
tification. If some, but not all, of the constraints
can be satisfied by an option, the constraints are
propagated to the other nodes (see Fig. 5).
4.6 Natural Language Generation
Once a pruned option tree has been constructed,
the system can generate a presentation to the user.
The natural language generation (NLG) algorithm
includes three steps described below.
4.6.1 Identifying Trade-offs
To identify the trade-offs, the algorithm tra-
verses the tree looking for constraints that were
generated during the pruning process. For each
node that generated a constraint, the algorithm
finds the best sibling, which satisfies the con-
straint. It does this by first checking the siblings
of the current node, and if none satisfy the con-
straint, it moves up the tree and recursively tra-
verses siblings of the ancestor node. Once a trade-
off node is found, it is recorded in the option tree
at that point, and the algorithm then searches up-
ward in the tree to find the lowest common parent
of the trade-off nodes. This is the ?Differentiating
Parent? for the trade-off; the attribute that deter-
mines the children of this node indicates the trade-
off the user has to make. In the example shown
in Fig. 5, the attribute that the user must trade-
off is distance. The system then generates a
natural language description of the trade-off using
templates, e.g., ?For a restaurant that serves Indian
cuisine you would have to go farther from the city
centre?. If a constraint cannot be satisfied, the sys-
tem generates a sentence describing this situation,
e.g., ?However, there are no inexpensive restau-
rants that match your preferences?.
4.6.2 Summarisation
The summarisation process aims to present the
user with an understandable and concise overview
of the available options, so that she can make
informed decisions on how to proceed with her
query. In a SDS, it is important not to present too
much information in a single turn in order to keep
the memory load on the user manageable. Thus,
in our implementation, the system provides a sum-
mary of the first three attributes (corresponding to
the first three levels of the tree), e.g., ?I found 12
restaurants which are inexpensive. Only 7 of them
are near the city centre, out of which only 4 have
a high star rating?. If there are any trade-offs over
the first three attributes, they are also mentioned to
the user, using the algorithm described above.
4.6.3 Turn Generation
The turn generation algorithm first determines
whether the current user?s query is consistent with
the preferences defined in the user model. Since
the option tree, and consequently the generation so
far, have primarily been based on the user model,
if the algorithm notices an inconsistency it will
give priority to the most recent user query as a sit-
uational adjustment (see Sec. 4.2), and then update
the user model to reflect the adjustment. If the user
model is modified, the algorithm rebuilds the op-
tion tree, and generates the next turn using the new
user model and the most recent user query.
To ensure the user that the system has under-
stood her correctly, and to increase her confidence,
the algorithm includes a summary of all attribute
values that are common to the cluster in the option
tree that is being used to generate the summary.
For example, for the second turn in the dialogue of
Fig. 4, although the user?s most recent query was
about cuisine, the system includes a summary
of all the attributes already mentioned to the user:
?All of them are cheap, are near the city centre,
and have a high star rating?.
5 Example
To clarify things further, here we present an exam-
ple. A dialogue between the UMSR system and
a student (user model of Table 1) is depicted in
Fig. 4.
Fig. 5 depicts the tree structure for the stu-
dent user of our example. The user begins by
5
U: Tell me about the restaurants you have.
S: I found 12 restaurants which are inexpensive. Only
7 of them are near the city centre, out of which only
4 have a high star rating. I also have information
about cuisine and service quality.
U: Tell me about the cuisine.
S: There is only 1 restaurant that serves Scottish food,
and 3 restaurants that serve French food. They all
provide good service quality. All of them are cheap,
are near the city centre, and have a high star rating.
For a restaurant that serves Indian cuisine you
would have to go farther from the city centre.
U: Okay, tell me about the Scottish one.
S: The Jam House?s menu includes Scottish dishes
sparked with international flavour, with a decent
choice of vegetarian dishes. It is located in the
centre. The average price is ?22 per person.
U: What about Indian restaurants?
S: I found 2 restaurants that serve Indian food.
Both of them are cheap, within average distance
from the city centre, and have a high star rating.
Only one of them provides good service quality.
U: Okay, tell me about this one.
S: The Raj on the Shore?s theme extends from the
soldier figurines behind the exotic blue bar to the
predominantly Bangladeshi, Indian menu. It serves
Indian and Bangladeshi cuisine. It is located
in the north. The average price is ?20 per person.
Figure 4: Example dialogue with our UMSR ap-
proach (student user of Table 1).
asking generally for a restaurant recommendation,
and the system provides a summary based on the
three top-ranking attributes for this user: price,
distance and star rating, which corre-
spond to the first three levels of the option tree.
Next the user asks about cuisine type, which
generates a trade-off since there are no Indian
restaurants (user?s preference) that are cheap, near
the city centre, and of high star rating. The user
then asks about the Scottish option, before switch-
ing back to her preferred cuisine type (Indian). Be-
cause Indian cuisine was in the user?s initial pref-
erence model, a constraint of cuisine=Indian
was generated when traversing the leftmost branch
of the tree, and this justified not pruning the un-
shaded nodes in the right subtree of Fig. 5, in or-
der to generate the trade-off. However, if the user
had asked about expensive restaurants, then a new
database query would have been made and a new
option tree would have been built. A more com-
plex example is given in the Appendix.
6 Experimental Design
In total 18 subjects interacted with our two sys-
tems. Each participant interacted three times with
the modified TownInfo system, and another three
times with the system that supported our imple-
mentation of the UMSR model (108 dialogues in
Figure 5: A sample option tree structure for the
student user of Table 1. Pruned nodes are shown
as shaded.
total). The order of the dialogues was randomised
among the subjects. Each experiment took be-
tween 40 and 50 minutes on average.
For each task, subjects were provided with the
user profile and the actual scenario for the spe-
cific task in hand. The tasks were carefully con-
structed so that half of them could be solved with-
out making any trade-offs and the other half re-
quired a trade-off to be made. At the end of each
task the subjects had to fill out a questionnaire with
10 questions on a 7-point Likert scale. They were
also asked if they had been able to accomplish the
given task (perceived task completion), i.e., to find
a suitable restaurant for the scenario and user pro-
file in hand. Finally, after each task they had to
provide the name(s) of the restaurants they chose
for the task. The name(s) stated for this task were
then used to compare perceived task completion
with actual task completion. At the end of each
task with the UMSR system, the profiles were re-
set to the default attribute values and ranks.
Both systems had identical software configura-
tions, i.e., they only differed in the information
presentation component. Yet another important
feature was that the UMSR based model did not
accept multiple attributes in a single query. So
for instance the user could not ask ?I am look-
ing for a moderately priced restaurant near the city
centre that serves Italian food?. This seemed to
be a major shortcoming of the UMSR based sys-
tem compared to the TownInfo system with se-
quential information presentation. However, as we
will see in the following, even with this shortcom-
6
System U CC CF A E
UMSR-all 5.04 4.65 3.22 3.66 4.69
TownInfo-all 4.87 4.04 2.93 3.20 3.59
UMSR-with TO 4.74 4.59 2.67 3.26 4.15
TownInfo-with TO 4.59 3.41 2.74 2.33 2.70
UMSR-no TO 5.33 4.70 3.78 4.08 5.22
TownInfo-no TO 5.15 4.67 3.11 4.07 4.48
Table 2: Average scores of the question-
naires for all dialogues, dialogues with trade-
offs (with TO) and dialogues without trade-offs
(no TO) (U=understandability, CC=conciseness,
CF=confidence, A=accessibility, E=efficiency).
ing the UMSR approach retained its advantages
and proved more successful than the traditional se-
quential enumeration approach.
7 Results
The perceived task completion (PTC) for the
UMSR system and the TownInfo system was
90.74% and 85.19% respectively, and the actual
task completion (ATC) 74.07% and 62.96%. Thus
the UMSR approach led to a relatively better user
confidence in having achieved the task.
The average number of turns was 9.24 for
UMSR compared to 17.78 for TownInfo, which
denotes a significant reduction in the number of
dialogue turns required to accomplish a given
task. This reduction becomes even more promi-
nent when there is a trade-off involved. With such
dialogues, the average number of turns for UMSR
remained almost constant at 9.41, whereas Town-
Info showed an increase reaching up to 24.19.
This huge difference is obviously a significant
improvement in system efficiency and user sat-
isfaction. It also supports our hypothesis that
the UMSR approach can present trade-offs under-
standably. For dialogues without a trade-off the
number of turns was 9.07 for UMSR and 11.37
for TownInfo.
Dialogue duration also showed a great improve-
ment in UMSR over TownInfo (4:49 (m:s) vs.
6:11). The duration however was almost the same
for the two systems when a trade-off existed (4:40
vs. 4:49). This could mean that although the num-
ber of turns in this case is smaller for UMSR, the
length of the generated output is longer, and re-
quires more attention to understand. Yet again in
dialogues without a trade-off, UMSR had a con-
siderably shorter duration than TownInfo (4:57 vs.
7:34).
Average scores of the questionnaires are given
in Table 2.
In response to the question ?I thought the way
the system provided information to me was easy
to understand? the average score over all 108 di-
alogues was 5.04 for UMSR and 4.87 for Town-
Info. The preference for UMSR exists for dia-
logues both with and without a trade-off. How-
ever, for all three cases the differences were not
significant (p > 0.05).
Conciseness is the quality of providing a con-
cise overview of all the available options to the
user. The UMSR system was preferred at 4.65
over 4.04 for TownInfo (p = 0.034). The differ-
ence between the two systems is very significant
for dialogues with a trade-off (p < 0.003). How-
ever, for dialogues without a trade-off p = 0.92.
This was predictable as the main innovation in
UMSR is the ability to present trade-offs in a con-
cise and understandable way, hence the significant
difference for the dialogues with trade-offs.
To evaluate their confidence in having heard all
the relevant options, the subjects were asked to
rate the statement ?I thought there were better op-
tions for my request than what the system gave
me?. Because of the negative nature of the ques-
tion, the Likert scale was inverted before analysis.
The average score was 3.22 and 2.93 for UMSR
and TownInfo respectively. This indicates that
the users have slightly more confidence in hav-
ing heard all the relevant options with the UMSR
system, although this difference is not significant
(p > 0.05). For dialogues with a trade-off, the
average confidence score was slightly better for
TownInfo (2.74 vs. 2.67), but not significant (p =
0.8). However, there is a significant difference for
dialogues without a trade-off (p < 0.03). An-
other notable issue is the overall low scores for the
cases with a trade-off. This signifies that perhaps
more information needs to be given to the user
for dialogue turns describing a trade-off. A care-
ful balance needs to be drawn between concise-
ness and comprehensiveness in these cases. This
however, will obviously increase dialogue dura-
tion, and might affect understandability.
By accessibility, we mean ease of use and
communication with the system. The scores for
UMSR and TownInfo were 3.66 and 3.20 respec-
tively (p = 0.18). A more significant difference
in accessibility was noted for dialogues with a
trade-off (p = 0.008). Again it seemed that users
preferred UMSR when it came down to dealing
with trade-offs. However, the accessibility scores
for dialogues without a trade-off were almost the
same (p = 0.92).
7
Efficiency is the quality of enabling users to
find the optimal option quickly. The statement
?In this task, the system allowed me to find the
optimal restaurant quickly?, resulted in an aver-
age score of 4.69 for UMSR vs. 3.59 for Town-
Info (p = 0.002). Once again, a significant dif-
ference was noted for dialogues with a trade-off,
with 4.15 and 2.70 for UMSR and TownInfo re-
spectively (p = 0.004). However, the difference
for dialogues without a trade-off was not signifi-
cant (p = 0.12).
8 Conclusions and Future Work
In this paper, we evaluated the effectiveness of the
UMSR approach in information presentation in a
full end-to-end dialogue system. The UMSR ap-
proach was compared with the traditional sequen-
tial enumeration of options. Our results verified
our hypothesis that the UMSR approach presents a
better overview of the trade-offs within the option
space, and improves user experience and confi-
dence in the system. Furthermore, with the UMSR
approach there is a significant reduction in the
number of dialogue turns required to complete the
task. The results also showed that UMSR specifi-
cally outperforms TownInfo when there is a trade-
off involved. The UMSR results presented statisti-
cally significant improvement for conciseness, ac-
cessibility, and efficiency. Overall, subjects were
more satisfied with the UMSR system. When they
were asked if they would use the system again as
a deployed product the score was 4.74 for UMSR
and 3.70 for TownInfo (p = 0.002), further veri-
fying that the users preferred the UMSR approach
over the sequential enumeration of TownInfo.
In future work we intend to make a number of
improvements. For example in the turn genera-
tion algorithm, we will optimise the generated out-
put in an effort to strike a balance between un-
derstandability and complexity. Another impor-
tant issue is to modify the UMSR algorithm so that
it can accept multiple attributes in a single query.
Moreover, we will perform experiments with both
speech input and output. Finally, we will com-
pare the UMSR approach with the UM and SR ap-
proaches in the same setting, i.e., a full end-to-end
SDS.
Acknowledgements
This paper is based on a research experiment con-
ducted at the University of Edinburgh. Paksima
was funded by the European Commission Eras-
mus Mundus scholarship program. Georgila was
partially funded by the Wellcome Trust VIP Award
and is currently funded by the U.S. Army Re-
search, Development, and Engineering Command
(RDECOM). The content does not necessarily re-
flect the position or the policy of the U.S. Gov-
ernment, and no official endorsement should be
inferred. The authors thank the three anonymous
reviewers.
References
G. Carenini and J.D. Moore. 2001. An empirical study
of the influence of user tailoring on evaluative argu-
ment effectiveness. In Proc. of IJCAI.
G. Chung. 2004. Developing a flexible spoken dialog
system using simulation. In Proc. of ACL.
V. Demberg and J.D. Moore. 2006. Information pre-
sentation in spoken dialogue systems. In Proc. of
EACL.
S. Elzer, J. Chu-Carroll, and S. Carberry. 1994. Recog-
nizing and utilizing user preferences in collaborative
consultation dialogues. In Proc. of UM ?94.
J. Henderson, O. Lemon, and K. Georgila. 2008. Hy-
brid reinforcement/supervised learning of dialogue
policies from fixed datasets. Computational Lin-
guistics, 34(4):487?511.
O. Lemon, K. Georgila, J. Henderson, and M. Stut-
tle. 2006. An isu dialogue system exhibiting re-
inforcement learning of dialogue policies: generic
slot-filling in the talk in-car system. In Proc. of
EACL.
J.D. Moore, M.E. Foster, O. Lemon, and M. White.
2004. Generating tailored, comparative descriptions
in spoken dialogue. In Proc. of FLAIRS.
J. Polifroni and M. Walker. 2008. Intensional sum-
maries as cooperative responses in dialogue: Au-
tomation and evaluation. In Proc. of ACL?HLT.
J. Polifroni, G. Chung, and M. Walker. 2003. To-
wards automatic generation of mixed-initiative di-
alogue systems from web content. In Proc. of Eu-
rospeech.
M.A. Walker, R.J. Passonneau, and J.E. Boland. 2001.
Quantitative and qualitative evaluation of darpa
communicator spoken dialogue systems. In Proc.
of ACL.
M. Walker, S. Whittaker, A. Stent, P. Maloor, J.D.
Moore, M. Johnston, and G. Vasireddy. 2004. Gen-
eration and evaluation of user tailored responses in
dialogue. Cognitive Science, 28:811?840.
A. Winterboer and J.D. Moore. 2007. Evaluating in-
formation presentation strategies for spoken recom-
mendations. In RecSys ?07: Proc. of the 2007 ACM
conference on Recommender systems.
8
Appendix
Option Tree A for User Model A is built.
U: I?d like to get some food tonight.
S: I found 17 restaurants with a high star rating. Only 8 of them provide good service quality, out of which
only 3 serve Scottish food. I also have information about distance and price.
U: Are they close to the city centre?
S: I found 3 restaurants that are located far from the city centre. 1 of them is cheap and 2 are expensive.
They all have a high star rating, provide good service quality, and serve Scottish food. For a restaurant near
the city centre you would have to go for average service quality.
U: How about French cuisine?
The System cannot answer the query because the ?French?cuisine branches of Option Tree A have been
pruned and their nodes are shown as shaded.
User Model A changes to User Model B (situational query).
Option Tree B for User Model B is built.
S: I found 10 restaurants that serve French food. Only 5 of them have a high star rating, out of which only 3
provide good service quality. I also have information about distance and price.
U: Tell me about the distance from the city centre.
S: Only 1 of them is located near the city centre and is inexpensive. It serves French food, has a high star rating,
and provides good service quality.
U: Tell me about it.
S: La Bagatelle?s dishes include a starter of roast pigeon with canelloni of shiitake mushroom, and a main course
of panfried veal with prunes.
U: That?s great. I?ll take it.
. . .
Figure 6: Example dialogue with our UMSR approach for a business person, User Model A (Table 3)
and User Model B (Table 4), Option Tree A (Fig. 7) and Option Tree B (Fig. 8).
UserID Attribute Value Rank
2 Star High 1.00
2 Service Good 2.00
2 Cuisine Scottish 3.00
2 Distance Near 4.00
2 Price Don?t Care 5.00
Table 3: Sample user model for a business person (User Model A).
UserID Attribute Value Rank
2 Cuisine French 0.95
2 Star High 1.00
2 Service Good 2.00
2 Distance Near 4.00
2 Price Don?t Care 5.00
Table 4: Modified user model for a business person after the situational query ?I?d like a French restau-
rant? (User Model B).
9
Figure 7: Option tree structure (Option Tree A) corresponding to the User Model A of Table 3. Pruned
nodes are shown as shaded.
Figure 8: Option tree structure (Option Tree B) corresponding to the User Model B of Table 4. Pruned
nodes are shown as shaded.
10
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 38?45,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Dealing with Interpretation Errors in Tutorial Dialogue
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow, Johanna D. Moore
School of Informatics
University of Edinburgh, Edinburgh, United Kingdom
mdzikovs,ccallawa,efarrow,jmoore@inf.ed.ac.uk
Natalie Steinhauser, Gwendolyn Campbell
Naval Air Warfare Training Systems Division
Orlando, Florida, USA
Abstract
We describe an approach to dealing with
interpretation errors in a tutorial dialogue
system. Allowing students to provide ex-
planations and generate contentful talk can
be helpful for learning, but the language
that can be understood by a computer sys-
tem is limited by the current technology.
Techniques for dealing with understanding
problems have been developed primarily for
spoken dialogue systems in information-
seeking domains, and are not always appro-
priate for tutorial dialogue. We present a
classification of interpretation errors and our
approach for dealing with them within an
implemented tutorial dialogue system.
1 Introduction
Error detection and recovery is a known problem in
the spoken dialogue community, with much research
devoted to determining the best strategies, and learn-
ing how to choose an appropriate strategy from data.
Most existing research is focused on dealing with
problems in an interaction resulting from speech
recognition errors. This focus is justified, since the
majority of understanding problems observed in cur-
rent spoken dialogue systems (SDS) are indeed due
to speech recognition errors.
Recovery strategies, therefore, are sometimes de-
vised specifically to target speech recognition prob-
lems - for example, asking the user to repeat the ut-
terance, or to speak more softly, which only makes
sense if speech recognition is the source of trouble.
However, errors can occur at all levels of process-
ing, including parsing, semantic interpretation, in-
tention recognition, etc. As speech recognition im-
proves and more sophisticated systems are devel-
oped, strategies for dealing with errors coming from
higher (and potentially more complex) levels of pro-
cessing will have to be developed.
This paper presents a classification of non-
understandings, defined as the errors where the sys-
tem fails to arrive at an interpretation of the user?s
utterance (Bohus and Rudnicky, 2005), and a set of
strategies for dealing with them in an implemented
tutorial dialogue system. Our system differs from
many existing systems in two ways. First, all di-
alogue is typed. This was done in part to avoid
speech recognition issues and allow for more com-
plex language input than would otherwise be pos-
sible. But it is also a valid modality for tutoring -
there are now many GUI-based tutoring systems in
existence, and as distance and online learning have
become more popular, students are increasingly fa-
miliar with typed dialogue in chat rooms and discus-
sion boards. Second, different genres impose dif-
ferent constraints on the set of applicable recovery
strategies - as we discuss in Section 2, certain help
strategies developed for task-oriented dialogue sys-
tems are not suitable for tutorial dialogue, because
tutoring systems should not give away the answer.
We propose a targeted help approach for dealing
with interpretation problems in tutorial dialogue by
providing help messages that target errors at differ-
ent points in the pipeline. In our system they are
combined with hints as a way to lead the student
to an answer that can be understood. While some
38
parts of the system response are specific to tutorial
dialogue, the targeted help messages themselves can
serve as a starting point for developing appropriate
recovery strategies in other systems where errors at
higher levels of interpretation are a problem.
The rest of this paper is organized as follows. In
Section 2, we motivate the need for error handling
strategies in tutorial dialogue. In Section 3 we de-
scribe the design of our system. Section 4 discusses
a classification of interpretation problems and our
targeted help strategy. Section 5 provides a prelim-
inary evaluation based on a set of system tests con-
ducted to date. Finally, we discuss how the approach
taken by our system compares to other systems.
2 Background and Motivation
Tutorial dialogue systems aim to improve learning
by engaging students in contentful dialogue. There
is a mounting body of evidence that dialogue which
encourages students to explain their actions (Aleven
and Koedinger, 2000), or to generate contentful talk
(Purandare and Litman, 2008), results in improved
learning. However, the systems? ability to under-
stand student language, and therefore to encourage
contentful talk, is limited by the state of current lan-
guage technology. Moreover, student language may
be particularly difficult to interpret since students
are often unaware of proper terminology, and may
phrase their answers in unexpected ways. For exam-
ple, a recent error analysis for a domain-independent
diagnoser trained on a large corpus showed that a
high proportion of errors were due to unexpected
paraphrases (Nielsen et al, 2008).
In small domains, domain-specific grammars and
lexicons can cover most common phrasings used
by students to ensure robust interpretation (Aleven,
2003; Glass, 2000). However, as the size of the
domain and the range of possible questions and an-
swers grows, achieving complete coverage becomes
more difficult. For essays in large domains, sta-
tistical methods can be used to identify problems
with the answer (Jordan et al, 2006; Graesser et
al., 1999), but these approaches do not perform well
on relatively short single-sentence explanations, and
such systems often revert to short-answer questions
during remediation to ensure robustness.
To the best of our knowledge, none of these tu-
torial systems use sophisticated error handling tech-
niques. They rely on the small size of the domain
or simplicity of expected answers to limit the range
of student input. They reject utterances they cannot
interpret, asking the user to repeat or rephrase, or
tolerate the possibility that interpretation problems
will lead to repetitive or confusing feedback.
We are developing a tutorial dialogue system that
behaves more like human tutors by supporting open-
ended questions, as well as remediations that allow
for open-ended answers, and gives students detailed
feedback on their answers, similar to what we ob-
served with human tutors. This paper takes the first
step towards addressing the problem of handling er-
rors in tutorial dialogue by developing a set of non-
understanding recovery strategies - i.e. strategies
used where the system cannot find an interpretation
for an utterance.
In early pilot experiments we observed that if the
system simply rejects a problematic student utter-
ance, saying that it was not understood, then stu-
dents are unable to determine the reason for this
rejection. They either resubmit their answer mak-
ing only minimal changes, or else they rephrase the
sentence in a progressively more complicated fash-
ion, causing even more interpretation errors. Even
after interacting with the system for over an hour,
our students did not have an accurate picture as to
which phrasings are well understood by the system
and which should be avoided. Previous research also
shows that users are rarely able to perceive the true
causes of ASR errors, and tend to form incorrect the-
ories about the types of input a system is able to ac-
cept (Karsenty, 2001).
A common approach for dealing with these is-
sues in spoken dialogue systems is to either change
to system initiative with short-answer questions (?Is
your destination London??), or provide targeted help
(?You can say plane, car or hotel?). Neither of these
is suitable for our system. The expected utterances
in our system are often more complex (e.g., ?The
bulb must be in a closed path with the battery?), and
therefore suggesting an utterance may be equivalent
to giving away the entire answer. Giving students
short-answer questions such as ?Are the terminals
connected or not connected?? is a valid tutoring
strategy sometimes used by the tutors. However,
it changes the nature of the question from a recall
39
task to a recognition task, which may affect the stu-
dent?s ability to remember the correct solution in-
dependently. Therefore, we decided to implement
strategies that give the student information about the
nature of the mistake without directly giving infor-
mation about the expected answer, and encourage
them to rephrase their answers in ways that can be
understood by the system.
We currently focus on strategies for dealing
with non-understanding rather than misunderstand-
ing strategies (i.e. cases where the system finds an
interpretation, but an incorrect one). It is less clear
in tutorial dialogue what it means for a misunder-
standing to be corrected. In task-oriented dialogue,
if the system gets a slot value different from what
the user intended, it should make immediate correc-
tions at the user?s request. In tutoring, however, it
is the system which knows the expected correct an-
swer. So if the student gives an answer that does not
match the expected answer, when they try to correct
it later, it may not always be obvious whether the
correction is due to a true misunderstanding, or due
to the student arriving at a better understanding of
the question. Obviously, true misunderstandings can
and will still occur - for example, when the system
resolves a pronoun incorrectly. Dealing with such
situations is planned as part of future work.
3 System Architecture
Our target application is a system for tutoring ba-
sic electricity and electronics. The students read
some introductory material, and interact with a sim-
ulator where they can build circuits using batteries,
bulbs and switches, and measure voltage and cur-
rent. They are then asked two types of questions:
factual questions, like ?If the switch is open, will
bulb A be on or off??, and explanation questions.
The explanation questions ask the student to explain
what they observed in a circuit simulation, for exam-
ple, ?Explain why you got the voltage of 1.5 here?,
or define generic concepts, such as ?What is volt-
age??. The expected answers are fairly short, one or
two sentences, but they involve complex linguistic
phenomena, including conjunction, negation, rela-
tive clauses, anaphora and ellipsis.
The system is connected to a knowledge base
which serves as a model for the domain and a rea-
soning engine. It represents the objects and rela-
tionships the system can reason about, and is used
to compute answers to factual questions.1 The stu-
dent answers are processed using a standard NLP
pipeline. All utterances are parsed to obtain syntac-
tic analyses.2 The lexical-semantic interpreter takes
analyses from the parser and maps them to seman-
tic representations using concepts from the domain
model. A reference resolution algorithm similar to
(Byron, 2002) is used to find referents for named ob-
jects such as ?bulb A? and for pronouns.
Once an interpretation of a student utterance has
been obtained, it is checked in two ways. First, its
internal consistency is verified. For example, if the
student says ?Bulb A will be on because it is in a
closed path?, we first must ensure that their answer
is consistent with what is on the screen - that bulb A
is indeed in a closed path. Otherwise the student
probably has a problem either with understanding
the diagrams or with understanding concepts such as
?closed path?. These problems indicate lack of basic
background knowledge, and need to be remediated
using a separate tutorial strategy.
Assuming that the utterance is consistent with the
state of the world, the explanation is then checked
for correctness. Even though the student utterance
may be factually correct (Bulb A is indeed in a
closed path), it may still be incomplete or irrelevant.
In the example above, the full answer is ?Bulb A
is in a closed path with the battery?, hence the stu-
dent explanation is factually correct but incomplete,
missing the mention of the battery.
In the current version of our system, we are partic-
ularly concerned about avoiding misunderstandings,
since they can result in misleading tutorial feedback.
Consider an example of what can happen if there is
a misunderstanding due to a lexical coverage gap.
The student sentence ?the path is broken? should be
interpreted as ?the path is no longer closed?, corre-
sponding to the is-open relation. However, the
1Answers to explanation questions are hand-coded by tutors
because they are not always required to be logically complete
(Dzikovska et al, 2008). However, they are checked for consis-
tency as described later, so they have to be expressed in terms
that the knowledge base can reason about.
2We are using a deep parser that produces semantic analyses
of student?s input (Allen et al, 2007). However, these have to
undergo further lexical interpretation, so we are treating them
as syntactic analyses for purposes of this paper.
40
most frequent sense of ?broken? is is-damaged,
as in ?the bulb is broken?. Ideally, the system lex-
icon would define ?broken? as ambiguous between
those two senses. If only the ?damaged? sense is
defined, the system will arrive at an incorrect inter-
pretation (misunderstanding), which is false by defi-
nition, as the is-damaged relation applies only to
bulbs in our domain. Thus the system will say ?you
said that the path is damaged, but that?s not true?.
Since the students who used this phrasing were un-
aware of the proper terminology in the first instance,
they dismissed such feedback as a system error. A
more helpful feedback message is to say that the sys-
tem does not know about damaged paths, and the
sentence needs to be rephrased.3
Obviously, frequent non-understanding messages
can also lead to communication breakdowns and im-
pair tutoring. Thus we aim to balance the need to
avoid misunderstandings with the need to avoid stu-
dent frustration due to a large number of sentences
which are not understood. We approach this by us-
ing robust parsing and interpretation tools, but bal-
ancing them with a set of checks that indicate poten-
tial problems. These include checking that the stu-
dent answer fits with the sortal constraints encoded
in the domain model, that it can be interpreted un-
ambiguously, and that pronouns can be resolved.
4 Error Handling Policies
All interpretation problems in our system are han-
dled with a unified tutorial policy. Each message to
the user consists of three parts: a social response,
the explanation of the problem, and the tutorial re-
sponse. The social response is currently a simple
apology, as in ?I?m sorry, I?m having trouble under-
standing.? Research on spoken dialogue shows that
users are less frustrated if systems apologize for er-
rors (Bulyko et al, 2005).
The explanation of the problem depends on the
problem itself, and is discussed in more detail below.
The tutorial response depends on the general tu-
torial situation. If this is the first misunderstanding,
the student will be asked to rephrase/try again. If
3This was a real coverage problem we encountered early on.
While we extended the coverage of the lexical interpreter based
on corpus data, other gaps in coverage may remain. We discuss
the issues related to the treatment of vague or incorrect termi-
nology in Section 4.
they continue to phrase things in a way that is mis-
understood, they will be given up to two different
hints (a less specific hint followed by a more spe-
cific hint); and finally the system will bottom out
with a correct answer. Correct answers produced by
the generator are guaranteed to be parsed and under-
stood by the interpretation module, so they can serve
as templates for future student answers.
The tutorial policy is also adjusted depending
on the interaction history. For example, if a non-
understanding comes after a few incorrect answers,
the system may decide to bottom out immediately in
order to avoid student frustration due to multiple er-
rors. At present we are using a heuristic policy based
on the total number of incorrect or uninterpretable
answers. In the future, such policy could be learned
from data, using, for example, reinforcement learn-
ing (Williams and Young, 2007).
In the rest of this section we discuss the explana-
tions used for different problems. For brevity, we
omit the tutorial response from our examples.
4.1 Parse Failures
An utterance that cannot be parsed represents the
worst possible outcome for the system, since detect-
ing the reason for a syntactic parse failure isn?t pos-
sible for complex parsers and grammars. Thus, in
this instance the system does not give any descrip-
tion of the problem at all, saying simply ?I?m sorry,
I didn?t understand.?
Since we are unable to explain the source of the
problem, we try hard to avoid such failures. We use
a spelling corrector and a robust parser that outputs
a set of fragments covering the student?s input when
a full parse cannot be found. The downstream com-
ponents are designed to merge interpretations of the
fragments into a single representation that is sent to
the reasoning components.
Our policy is to allow the system to use such frag-
mentary parses when handling explanation ques-
tions, where students tend to use complex language.
However, we require full parses for factual ques-
tions, such as ?Which bulbs will be off?? We found
that for those simpler questions students are able to
easily phrase an acceptable answer, and the lack of
a full parse signals some unusually complex lan-
guage that downstream components are likely to
have problems with as well.
41
One risk associated with using fragmentary parses
is that relationships between objects from different
fragments would be missed by the parser. Our cur-
rent policy is to confirm the correct part of the stu-
dent?s answer, and prompt for the missing parts, e.g.,
? Right. The battery is contained in a closed path.
And then?? We can do this because we use a diag-
noser that explicitly identifies the correct objects and
relationships in the answer (Dzikovska et al, 2008),
and we are using a deep generation system that can
take those relationships and automatically generate
a rephrasing of the correct portion of the content.
4.2 Lexical Interpretation Errors
Errors in lexical interpretation typically come from
three main sources: unknown words which the lex-
ical interpreter cannot map into domain concepts,
unexpected word combinations, and incorrect uses
of terminology that violate the sortal constraints en-
coded in the domain model.
Unknown words are the simplest to deal with in
the context of our lexical interpretation policy. We
do not require that every single word of an utter-
ance should be interpreted, because we want the
system to be able to skip over irrelevant asides.
However, we require that if a predicate is inter-
preted, all its arguments should be interpreted as
well. To illustrate, in our system the interpretation of
?the bulb is still lit? is (LightBulb Bulb-1-1)
(is-lit Bulb-1-1 true). The adverbial
?still? is not interpreted because the system is un-
able to reason about time.4 But since all arguments
of the is-lit predicate are defined, we consider
the interpretation complete.
In contrast, in the sentence ?voltage is the mea-
surement of the power available in a battery?, ?mea-
surement? is known to the system. Thus, its argu-
ment ?power? should also be interpreted. However,
the reading material in the lessons never talks about
power (the expected answer is ?Voltage is a mea-
surement of the difference in electrical states be-
tween two terminals?). Therefore the unknown word
detector marks ?power? as an unknown word, and
tells the student ?I?m sorry, I?m having a problem
understanding. I don?t know the word power.?
4The lexical interpretation algorithm makes sure that fre-
quency and negation adverbs are accounted for.
The system can still have trouble interpreting sen-
tences with words which are known to the lexical
interpreter, but which appear in unexpected combi-
nations. This involves two possible scenarios. First,
unambiguous words could be used in a way that
contradicts the system?s domain model. For exam-
ple, the students often mention ?closed circuit? in-
stead of the correct term ?closed path?. The former
is valid in colloquial usage, but is not well defined
for parallel circuits which can contain many differ-
ent paths, and therefore cannot be represented in a
consistent knowledge base. Thus, the system con-
sults its knowledge base to tell the student about the
appropriate arguments for a relation with which the
failure occurred. In this instance, the feedback will
be ?I?m sorry, I?m having a problem understanding.
I don?t understand it when you say that circuits can
be closed. Only paths and switches can be closed.?5
The second case arises when a highly ambiguous
word is used in an unexpected combination. The
knowledge base uses a number of fine-grained rela-
tions, and therefore some words can map to a large
number of relations. For example, the word ?has?
means circuit-component in ?The circuit has
2 bulbs?, terminals-of in ?The bulb has ter-
minals? and voltage-property in ?The bat-
tery has voltage?. The last relation only applies to
batteries, but not to other components. These dis-
tinctions are common for knowledge representation
and reasoning systems, since they improve reason-
ing efficiency, but this adds to the difficulty of lex-
ical interpretation. If a student says ?Bulb A has a
voltage of 0.5?, we cannot determine the concept to
which the word ?has? corresponds. It could be either
terminals-of or voltage-property, since
each of those relations uses one possible argument
from the student?s utterance. Thus, we cannot sug-
gest appropriate argument types and instead we in-
dicate the problematic word combination, for exam-
ple, ?I?m sorry, I?m having trouble understanding. I
didn?t understand bulb has voltage.?
Finally, certain syntactic constructions involving
comparatives or ellipsis are known to be difficult
5Note that these error messages are based strictly on the fact
that sortal constraints from the knowledge base for the relation
that the student used were violated. In the future, we may also
want to adjust the recovery strategy depending on whether the
problematic relation is relevant to the expected answer.
42
open problems for interpretation. While we are
working on interpretation algorithms to be included
in future system versions, the system currently de-
tects these special relations, and produces a mes-
sage telling the student to rephrase without the prob-
lematic construction, e.g., ?I?m sorry. I?m having a
problem understanding. I do not understand same
as. Please try rephrasing without the word as.?
4.3 Reference Errors
Reference errors arise when a student uses an am-
biguous pronoun, and the system cannot find a suit-
able object in the knowledge base to match, or on
certain occasions when an attachment error in a
parse causes an incorrect interpretation. We use a
generic message that indicates the type of the ob-
ject the system perceived, and the actual word used,
for example, ?I?m sorry. I don?t know which switch
you?re referring to with it.?
To some extent, reference errors are instances of
misunderstandings rather than non-understandings.
There are actually 2 underlying cases for reference
failure: either the system cannot find any referent at
all, or it is finding too many referents. In the future
a better policy would be to ask the student which of
the ambiguous referents was intended. We expect to
pilot this policy in one of our future system tests.
5 Evaluation
So far, we have run 13 pilot sessions with our sys-
tem. Each pilot consisted of a student going through
1 or 2 lessons with the system. Each lesson lasts
about 2 hours and has 100-150 student utterances
(additional time is taken with building circuits and
reading material). Both the coverage of the interpre-
tation component and the specificity of error mes-
sages were improved between each set of pilots, thus
it does not make sense to aggregate the data from
them. However, over time we observed the trend
that students are more likely to change their behav-
ior when the system issues more specific messages.
Examples of successful and unsuccessful interac-
tions are shown in Figure 1. In (a), the student used
incorrect terminology, and a reminder about how the
word ?complete? is interpreted was enough to get
the conversation back on track.
The dialogue fragment in (b) shows how mes-
sages which are not specific enough can cause a
breakdown in conversation. The system used an in-
sufficiently specific message at the beginning (omit-
ting the part that says that only switches and paths
can be closed). This led the student away from an
answer which was nearly correct with slightly im-
perfect terminology to an answer which was insuffi-
cient (it?s not enough for the components to be con-
nected, they have to be in a closed path), and then
to rephrase it in a more complicated way that was
impossible for the system to understand (consistent
with findings of Bulyko et al (2005)).
The next step would be to conduct a formal evalu-
ation of our policy. We are planning to do this using
recovery rate after different strategies as our evalu-
ation measure (Bohus and Rudnicky, 2005), which
is the percentage of times when the utterance imme-
diately following a non-understanding was success-
fully interpreted by the system.
6 Discussion
Our error handling policy attempts to balance ro-
bustness in interpretation with the need for pre-
cise tutorial feedback. Human tutors accept a wide
range of variation in terminology and phrasing.
However, there is a known trade-off between non-
understandings and misunderstandings. We decided
that misunderstandings are likely to cause more seri-
ous problems with respect to tutoring. Therefore we
set strict understanding requirements and focused on
developing a policy that would encourage students
to use constructions that the system can interpret re-
liably. This is also supported by our observations in
the human-human data ? tutors believe that teaching
correct terminology is important, and we observed
them saying things like ?It?s good to put it into your
own words like that. Just for the sake of the lesson,
could you also use the term closed path??.
Our approach is similar to the TargetedHelp ap-
proach of (Hockey et al, 2003), where in the case of
an interpretation failure the task assistant produces
a report of what the recognizer heard, a description
of the problem, and a possible in-domain utterance
as an alternative. In a user evaluation such targeted
help messages resulted in significantly lower task
completion times compared to a system that pro-
duced a generic ?I didn?t understand? message, and
43
(a)
S: the open switch makes the path incomplete
T: I?m sorry, I?m having a problem understanding. Paths can?t be complete. Only circuits can be
complete. Please try again.
S: the open switch makes the path open
T: Right. An open switch creates a gap.
(b)
T: What are the conditions that are required to make a bulb light up?
S: The lightbulb has to be in a closed circuit with a battery
T: I?m sorry, I?m having a problem understanding. I don?t understand when you say that circuits
can be closed. Please try again.
S: The lightbulb has to be connected to a battery
T: Yes. You correctly mentioned a battery and a bulb. Keep trying.
S: Both the positive and negative connectors have to be connected to
T: I?m sorry, I?m having a problem understanding. Try rephrasing your answer.
S: The lightbulb has to be connected to the positive and negative connectors of the battery
T: I?m sorry, I didn?t understand. Here?s the answer. The battery and the bulb must be contained
in the same closed path.
Figure 1: Sample student-system interactions. S: is student, T: is tutor (system). (a) A successful interaction
where the student changes their language; (b) a failed interaction where system feedback was unhelpful.
subjects gradually learned how to talk to the sys-
tem, reducing the number of misunderstandings over
time. This gives us reason to believe that our sys-
tem can achieve similar effects in tutorial dialogue.
While we don?t suggest alternative domain utter-
ances due to the tutoring reasons described earlier,
the progressively more specific hints serve a simi-
lar function. To what extent this impacts learning
and interaction with the system will have to be de-
termined in future evaluations.
The error handling in our system is significantly
different from systems that analyze user essays be-
cause it needs to focus on a single sentence at a time.
In a system that does essay analysis, such as AUTO-
TUTOR (Graesser et al, 1999) or Why2-Atlas (Jor-
dan et al, 2006) a single essay can have many flaws.
So it doesn?t matter if some sentences are not fully
understood as long as the essay is understood well
enough to identify at least one flaw. Then that par-
ticular flaw can be remediated, and the student can
resubmit the essay. However, this can also cause stu-
dent frustration and potentially affect learning if the
student is asked to re-write an essay many times due
to interpretation errors.
Previous systems in the circuit domain focused on
troubleshooting rather than conceptual knowledge.
The SHERLOCK tutor (Katz et al, 1998) used only
menu-based input, limiting possible dialogue. Cir-
cuit Fix-It Shop (Smith and Gordon, 1997) was a
task-oriented system which allowed for speech in-
put, but with very limited vocabulary. Our system?s
larger vocabulary and complex input result in differ-
ent types of non-understandings that cannot be re-
solved with simple confirmation messages.
A number of researchers have developed er-
ror taxonomies for spoken dialogue systems (Paek,
2003; Mo?ller et al, 2007). Our classification does
not have speech recognition errors (since we are us-
ing typed dialogue), and we have a more complex
interpretation stack than the domain-specific pars-
ing utilized by many SDSs. However, some types
of errors are shared, in particular, our ?no parse?,
?unknown word? and ?unknown attachment? errors
correspond to command-level errors, and our sor-
tal constraint and reference errors correspond to
concept-level errors in the taxonomy of Mo?ller et al
(2007). This correspondence is not perfect because
of the nature of the task - there are no commands in
a tutoring system. However, the underlying causes
are very similar, and so research on the best way
44
to communicate about system failures would benefit
both tutoring and task-oriented dialogue systems. In
the long run, we would like to reconcile these differ-
ent taxonomies, leading to a unified classification of
system errors and recovery strategies.
7 Conclusion
In this paper we described our approach to handling
non-understanding errors in a tutorial dialogue sys-
tem. Explaining the source of errors, without giving
away the full answer, is crucial to establishing ef-
fective communication between the system and the
student. We described a classification of common
problems and our approach to dealing with different
classes of errors. Our experience with pilot studies,
as well as evidence from spoken dialogue systems,
indicates that our approach can help improve dia-
logue efficiency. We will be evaluating its impact on
both student learning and on dialogue efficiency in
the future.
8 Acknowledgments
This work has been supported in part by Office of
Naval Research grant N000140810043.
References
V. A. Aleven and K. R. Koedinger. 2000. The need for
tutorial dialog to support self-explanation. In Proc. of
AAAI Fall Symposion on Building Dialogue Systems
for Tutorial Applications.
O. P. V. Aleven. 2003. A knowledge-based approach
to understanding students? explanations. In School of
Information Technologies, University of Sydney.
J. Allen, M. Dzikovska, M. Manshadi, and M. Swift.
2007. Deep linguistic processing for spoken dialogue
systems. In Proceedings of the ACL-07 Workshop on
Deep Linguistic Processing.
D. Bohus and A. Rudnicky. 2005. Sorry, i didn?t catch
that! - an investigation of non-understanding errors
and recovery strategies. In Proceedings of SIGdial-
2005, Lisbon, Portugal.
I. Bulyko, K. Kirchhoff, M. Ostendorf, and J. Goldberg.
2005. Error-correction detection and response gener-
ation in a spoken dialogue system. Speech Communi-
cation, 45(3):271?288.
D. K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
M. O. Dzikovska, G. E. Campbell, C. B. Callaway, N. B.
Steinhauser, E. Farrow, J. D. Moore, L. A. Butler, and
C. Matheson. 2008. Diagnosing natural language an-
swers to support adaptive tutoring. In Proceedings
21st International FLAIRS Conference.
M. Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Proc.
of the AAAI Fall Symposium on Building Dialogue Sys-
tems for Tutorial Applications.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-Hastings,
and R. Kreuz. 1999. Autotutor: A simulation of a
human tutor. Cognitive Systems Research, 1:35?51.
B. A. Hockey, O. Lemon, E. Campana, L. Hiatt, G. Aist,
J. Hieronymus, A. Gruenstein, and J. Dowding. 2003.
Targeted help for spoken dialogue systems: intelligent
feedback improves naive users? performance. In Pro-
ceedings of EACL.
P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,
and P. Albacete. 2006. A natural language tuto-
rial dialogue system for physics. In Proceedings of
FLAIRS?06.
L. Karsenty. 2001. Adapting verbal protocol methods to
investigate speech systems use. Applied Ergonomics,
32:15?22.
S. Katz, A. Lesgold, E. Hughes, D. Peters, G. Eggan,
M. Gordin, and L. Greenberg. 1998. Sherlock 2: An
intelligent tutoring system built on the lrdc framework.
In C. Bloom and R. Loftin, editors, Facilitating the
development and use of interactive learning environ-
ments. ERLBAUM.
S. Mo?ller, K.-P. Engelbrecht, and A. Oulasvirta. 2007.
Analysis of communication failures for spoken dia-
logue systems. In Proceedings of Interspeech.
R. D. Nielsen, W. Ward, and J. H. Martin. 2008. Clas-
sification errors in a domain-independent assessment
system. In Proc. of the Third Workshop on Innovative
Use of NLP for Building Educational Applications.
T. Paek. 2003. Toward a taxonomy of communication
errors. In Proceedings of ISCA Workshop on Error
Handling in Spoken Dialogue Systems.
A. Purandare and D. Litman. 2008. Content-learning
correlations in spoken tutoring dialogs at word, turn
and discourse levels. In Proc.of FLAIRS.
R. W. Smith and S. A. Gordon. 1997. Effects of variable
initiative on linguistic behavior in human-computer
spoken natural language dialogue. Computational
Linguistics.
J. D. Williams and S. Young. 2007. Scaling POMDPs for
spoken dialog management. IEEE Trans. on Audio,
Speech, and Language Processing, 15(7):2116?2129.
45
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 54?61,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Participant Subjectivity and Involvement as a Basis for Discourse
Segmentation
John Niekrasz and Johanna Moore
Human Communication Research Centre
School of Informatics
University of Edinburgh
{jniekras,jmoore}@inf.ed.ac.uk
Abstract
We propose a framework for analyzing
episodic conversational activities in terms
of expressed relationships between the
participants and utterance content. We
test the hypothesis that linguistic features
which express such properties, e.g. tense,
aspect, and person deixis, are a useful ba-
sis for automatic intentional discourse seg-
mentation. We present a novel algorithm
and test our hypothesis on a set of inten-
tionally segmented conversational mono-
logues. Our algorithm performs better
than a simple baseline and as well as or
better than well-known lexical-semantic
segmentation methods.
1 Introduction
This paper concerns the analysis of conversations
in terms of communicative activities. Examples of
the kinds of activities we are interested in include
relating a personal experience, making a group de-
cision, committing to future action, and giving in-
structions. The reason we are interested in these
kinds of events is that they are part of partici-
pants? common-sense notion of the goals and ac-
complishments of a dialogue. They are part of par-
ticipants? subjective experience of what happened
and show up in summaries of conversations such
as meeting minutes. We therefore consider them
an ideal target for the practical, common-sense de-
scription of conversations.
Activities like these commonly occur as cohe-
sive episodes of multiple turns within a conver-
sation (Korolija, 1998). They represent an inter-
mediate level of dialogue structure ? greater than
a single speech act but still small enough to have
a potentially well-defined singular purpose. They
have a temporal granularity of anywhere from a
few seconds to several minutes.
Ultimately, it would be useful to use descrip-
tions of such activities in automatic summariza-
tion technologies for conversational genres. This
would provide an activity-oriented summary de-
scribing what ?happened? that would complement
one based on information content or what the con-
versation was ?about?. Part of our research goal is
thus to identify a set of discourse features for seg-
menting, classifying, and describing conversations
in this way.
1.1 Participant subjectivity and involvement
The approach we take to this problem is founded
upon two basic ideas. The first is that the activities
we are interested in represent a coarse level of the
intentional structure of dialogue (Grosz and Sid-
ner, 1986). In other words, each activity is unified
by a common purpose that is shared between the
participants. This suggests there may be linguis-
tic properties which are shared amongst the utter-
ances of a given activity episode.
The second idea concerns the properties which
distinguish different activity types. We propose
that activity types may be usefully distinguished
according to two complex properties of utterances,
both of which concern relationships between the
participants and the utterance: participant sub-
jectivity and participant involvement. Participant
subjectivity concerns attitudinal and perspectival
relationships toward the dialogue content. This
includes properties such as whether the utterance
expresses the private mental state of the speaker,
or the participants? temporal relationship to a de-
scribed event. Participant involvement concerns
the roles participants play within the dialogue con-
54
tent, e.g., as the agent of a described event.
1.2 Intentional segmentation
The hypothesis we test in this paper is that the
linguistic phenomena which express participant-
relational properties may be used as an effective
means of intentional discourse segmentation. This
is based on the idea that if adjacent discourse seg-
ments have different activity types, then they are
distinguishable by participant-relational features.
If we can reliably extract such features, then this
would allow segmentation of the dialogue accord-
ingly.
We test our hypothesis by constructing an algo-
rithm and examining its performance on an exist-
ing set of intentionally segmented conversational
monologues (i.e., one person speaks while another
listens) (Passonneau and Litman, 1997, henceforth
P&L). While our long term goal is to apply our
techniques to multi-party conversations (and to
a somewhat coarser-grained analysis), using this
dataset is a stepping-stone toward that end which
allows us to compare our results with existing in-
tentional segmentation algorithms.
An example dialogue extract from the dataset
is shown in Dialogue 1. Two horizontal lines in-
dicate a segment boundary which was identified
by at least 3 of 7 annotators. A single horizon-
tal line indicates a segment boundary which was
identified by 2 or fewer annotators. In the exam-
PearStories-09 (Chafe, 1980)
21.2 okay.
22.1 Meanwhile,
22.2 there are three little boys,
22.3 up on the road a little bit,
22.4 and they see this little accident.
23.1 And u-h they come over,
23.2 and they help him,
23.3 and you know,
23.4 help him pick up the pears and everything.
24.1 A-nd the one thing that struck me about the- three
little boys that were there,
24.2 is that one had ay uh I don?t know what you call
them,
24.3 but it?s a paddle,
24.4 and a ball-,
24.5 is attached to the paddle,
24.6 and you know you bounce it?
25.1 And that sound was really prominent.
26.1 Well anyway,
26.2 so- u-m tsk all the pears are picked up,
26.3 and he?s on his way again,
Dialogue 1: An example dialogue extract showing
intentional segment boundaries.
ple, there are three basic types of discourse activity
distinguishable according to the properties of par-
ticipant subjectivity and participant involvement.
The segments beginning at 22.1 and 26.2 share the
use of the historical present tense ? a type of par-
ticipant subjectivity ? in a narrative activity type.
Utterances 24.1 and 25.1, on the other hand, are
about the prior perceptions of the speaker, a type
of participant involvement in a past event. The
segment beginning at 24.2 is a type of generic de-
scription activity, exhibiting its own distinct con-
figuration of participant relational features, such
as the generic you and present tense.
We structure the rest of the paper as follows.
First, we begin by describing related and support-
ing theoretical work. This is followed by a test of
our main hypothesis. We then follow this with a
similar experiment which contextualizes our work
both theoretically and in practical terms with re-
spect to the most commonly studied segmentation
task: topic segmentation. We finish with a general
discussion of the implications of our experiments.
2 Background and Related Work
The influential work of Grosz and Sidner (1986)
provides a helpful starting point for understand-
ing our approach. Their theory suggests that in-
tentions (which equate to the goals and purposes
of a dialogue) are a foundation for the structure of
discourse. The individual discourse purposes that
emerge in a dialogue relate directly to the natural
aggregation of utterances into discourse segments.
The attentional state of the dialogue, which con-
tains salient objects and relations and allows for
the efficient generation and interpretation of utter-
ances, is then dependent upon this interrelated in-
tentional and linguistic structure in the emerging
dialogue.
Grosz and Sidner?s theory suggests that atten-
tional state is parasitic upon the underlying inten-
tional structure. This implication has informed
many approaches which relate referring expres-
sions (an attentional phenomenon) to discourse
structure. One example is Centering theory (Grosz
et al, 1995), which concerns the relationship of
referring expressions to discourse coherence. An-
other is P&L, who demonstrated that co-reference
and inferred relations between noun phrases are
a useful basis for automatic intentional segmen-
tation.
Our approach expands on this by highlighting
55
the fact that objects that are in focus within the
attentional state have an important quality which
may be exploited: they are focused upon by the
participants from particular points of view. In ad-
dition, the objects may in fact be the participants
themselves. We would expect the linguistic fea-
tures which express such relationships (e.g., as-
pect, subjectivity, modality, and person deixis) to
therefore correlate with intentional structure, and
to do so in a way which is important to partici-
pants? subjective experience of the dialogue.
This approach is supported by a theory put forth
by Chafe (1994), who describes how speakers can
express ideas from alternative perspectives. For
example, a subject who is recounting the events of
a movie of a man picking pears might say ?the man
was picking pears?, ?the man picks some pears?,
or ?you see a man picking pears.? Each variant is
an expression of the same idea but reflects a dif-
ferent perspective toward, or manner of participa-
tion in, the described event. The linguistic vari-
ation one sees in this example is in the proper-
ties of tense and aspect in the main clause (and in
the last variant, a perspectival superordinate clause
which uses the generic you). We have observed
that discourse coheres in these perspectival terms,
with shifts of perspective usually occurring at in-
tentional boundaries.
Wiebe (1994; 1995) has investigated a phe-
nomenon closely related to this: point-of-view
and subjectivity in fictional narrative. She notes
that paragraph-level blocks of text often share a
common objective or subjective context. That
is, sentences may or may not be conveyed from
the point-of-view of individuals, e.g., the author
or the characters within the narrative. Sentences
continue, resume, or initiate such contexts, and
she develops automatic methods for determining
when the contexts shift and whose point-of-view
is being taken. Her algorithm provides a de-
tailed method for analyzing written fiction, but
has not been developed for conversational or non-
narrative genres.
Smith?s (2003) analysis of texts, however,
draws a more general set of connections between
the content of sentences and types of discourse
segments. She does this by analyzing texts at
the level of short passages and determines a non-
exhaustive list of five basic ?discourse modes? oc-
curring at that level: narrative, description, report,
information, and argument. The mode of a pas-
sage is determined by the type of situations de-
scribed in the text (e.g., event, state, general sta-
tive, etc.) and the temporal progression of the sit-
uations in the discourse. Situation types are in
turn organized according to the perspectival prop-
erties of aspect and temporal location. A narrative
passage, for example, relates principally specific
events and states, with dynamic temporal advance-
ment of narrative time between sentences. On the
other hand, an information passage relates primar-
ily general statives with atemporal progression.
3 Automatic Segmentation Experiment
The analysis described in the previous sections
suggests that participant-relational features corre-
late with the intentional structure of discourse. In
this section we describe an experiment which tests
the hypothesis that a small set of such features, i.e.,
tense, aspect, and first- and second-person pro-
nouns, are a useful basis for intentional segmen-
tation.
3.1 Data
Our experiment uses the same dataset as P&L, a
corpus of 20 spoken narrative monologues known
as the Pear Stories (Chafe, 1980). Chafe asked
subjects to view a silent movie and then sum-
marize it for a second person. Their speech
was then manually transcribed and segmented into
prosodic phrases. This resulted in a mean 100
phrases per narrative and a mean 6.7 words per
phrase. P&L later had each narrative segmented
by seven annotators according to an informal defi-
nition of communicative intention. Each prosodic
phrase boundary was a possible discourse segment
boundary. Using Cochran?s Q test, they concluded
that an appropriate gold standard could be pro-
duced by using the set of boundaries assigned by
at least three of the seven annotators. This is the
gold standard we use in this paper. It assigns a
boundary at a mean 16.9% (? = 4.5%) of the pos-
sible boundary sites in each narrative. The result is
a mean discourse segment length of 5.9 prosodic
phrases, (? = 1.4 across the means of each narra-
tive).
3.2 Algorithm
The basic idea behind our algorithm is to distin-
guish utterances according to the type of activ-
ity in which they occur. To do this, we iden-
tify a set of utterance properties relating to par-
56
ticipant subjectivity and participant involvement,
according to which activity types may be distin-
guished. We then develop a routine for automati-
cally extracting the linguistic features which indi-
cate such properties. Finally, the dialogue is seg-
mented at locations of high discontinuity in that
feature space. The algorithm works in four phases:
pre-processing, feature extraction, similarity mea-
surement, and boundary assignment.
3.2.1 Pre-processing
For pre-processing, disfluencies are removed by
deleting repeated strings of words and incomplete
words. The transcript is then parsed (Klein and
Manning, 2002), and a collection of typed gram-
matical dependencies are generated (de Marneffe
et al, 2006). The TTT2 chunker (Grover and To-
bin, 2006) is then used to perform tense and aspect
tagging.
3.2.2 Feature extraction
Feature extraction is the most important and
novel part of our algorithm. Each prosodic phrase
(the corpus uses prosodic phrases as sentence-like
units, see Data section) is assigned values for five
binary features. The extracted features correspond
to a set of utterance properties which were iden-
tified manually through corpus analysis. The first
four relate directly to individual activity types and
are therefore mutually exclusive properties.
first-person participation [1P] ? helps to distin-
guish meta-discussion between the speaker
and hearer (e.g., ?Did I tell you that??)
generic second-person [2P-GEN] ? helps to dis-
tinguish narration told from the perspective
of a generic participant (e.g., ?You see a man
picking pears?)
third-person stative/progressive [3P-STAT]
? helps to distinguish narrative activities
related to ?setting the scene? (e.g., ?[There is
a man | a man is] picking pears?)
third-person event [3P-EVENT] ? helps to dis-
tinguish event-driven third-person narrative
activities (e.g. ?The man drops the pears?)
past/non-past [PAST] ? helps to distinguish nar-
rative activities by temporal orientation (e.g.
?The man drops the pears? vs. ?The man
dropped the pears?)
Feature extraction works by identifying the lin-
guistic elements that indicate each utterance prop-
erty. First, prosodic phrases containing a first- or
second-person pronoun in grammatical subject or
object relation to any clause are identified (com-
mon fillers like you know, I think, and I don?t know
are ignored). Of the identified phrases, those with
first-person pronouns are marked for 1P, while the
others are marked for 2P-GEN. For the remain-
ing prosodic phrases, those with a matrix clause
are identified. Of those identified, if either its
head verb is be or have, it is tagged by TTT2 as
having progressive aspect, or the prosodic phrase
contains an existential there, then it is marked for
3P-STAT. The others are marked for 3P-EVENT.
Finally, if the matrix clause was tagged as past
tense, the phrase is marked for PAST. In cases
where no participant-relational features are iden-
tified (e.g., no matrix clause, no pronouns), the
prosodic phrase is assigned the same features as
the preceding one, effectively marking a continua-
tion of the current activity type.
3.2.3 Similarity measurement
Similarity measurement is calculated according
to the cosine similarity cos(vi, ci) between the fea-
ture vector vi of each prosodic phrase i and a
weighted sum ci of the feature vectors in the pre-
ceding context. The algorithm requires a parame-
ter l to be set for the desired mean segment length.
This determines the window w = floor(l/2) of
preceding utterances to be used. The weighted
sum representing the preceding context is com-
puted as ci =
?w
j=1((1 + w ? j)/w)vi?j , which
gives increasingly greater weight to more recent
phrases.
3.2.4 Boundary assignment
In the final step, the algorithm assigns bound-
aries where the similarity score is lowest, namely
prior to prosodic phrases where cos is less than the
first 1/l quantile for that discourse.
3.3 Experimental Method and Evaluation
Our experiment compares the performance of
our novel algorithm (which we call NM09) with
a naive baseline and a well-known alternative
method ? P&L?s co-reference based NP algorithm.
To our knowledge, P&L is the only existing publi-
cation describing algorithms designed specifically
for intentional segmentation of dialogue. Their
NP algorithm exploits annotations of direct and
57
inferred relations between noun phrases in adja-
cent units. Inspired by Centering theory (Grosz
et al, 1995), these annotations are used in a com-
putational account of discourse focus to measure
coherence. Although adding pause-based features
improved results slightly, the NP method was the
clear winner amongst those using a single feature
type and produced very good results.
The NP algorithm requires co-reference anno-
tations as input, so to create a fully-automatic
version (NP-AUTO) we have employed a state-of-
the-art co-reference resolution system (Poesio and
Kabadjov, 2004) to generate the required input.
We also include results based on P&L?s original
human co-reference annotations (NP-HUMAN).
For reference, we include a baseline that ran-
domly assigns boundaries at the same mean fre-
quency as the gold-standard annotations, i.e., a se-
quence drawn from the Bernoulli distribution with
success probability p = 0.169 (this probability de-
termines the value of the target segment length pa-
rameter l in our own algorithm). As a top-line ref-
erence, we calculate the mean of the seven anno-
tators? scores with respect to the three-annotator
gold standard.
For evaluation we employ two types of mea-
sure. On one hand, we use P (k) (Beeferman et al,
1999) as an error measure designed to accommo-
date near-miss boundary assignments. It is useful
because it estimates the probability that two ran-
domly drawn points will be assigned incorrectly
to either the same or different segments. On the
other hand, we use Cohen?s Kappa (?) to evalu-
ate the precise placement of boundaries such that
each potential boundary site is considered a binary
classification. While ? is typically used to evalu-
ate inter-annotator agreement, it is a useful mea-
sure of classification accuracy in our experiment
for two reasons. First, it accounts for the strong
class bias in our data. Second, it allows a direct
and intuitive comparison with our inter-annotator
top-line reference. We also provide results for the
commonly-used IR measures F1, recall, and pre-
cision. These are useful for comparing with pre-
vious results in the literature and provide a more
widely-understood measure of the accuracy of the
results. Precision and recall are also helpful in re-
vealing the effects of any classification bias the al-
gorithms may have.
The results are calculated for 18 of the 20 narra-
tives, as manual feature development involved the
Table 1: Mean results for the 18 test narratives.
P (k) ? F1 Rec. Prec.
Human .21 .58 .65 .64 .69
NP-HUMAN .35 .38 .40 .52 .46
NM09 .44 .11 .24 .23 .28
NP-AUTO .52 .03 .27 .71 .17
Random .50 .00 .15 .14 .17
use of two randomly selected narratives as devel-
opment data. The one exception is NP-HUMAN,
which is evaluated on the 10 narratives for which
there are manual co-reference annotations.
3.4 Results
The mean results for the 18 narratives, calculated
in comparison to the three-annotator gold stan-
dard, are shown in Table 1. NP-HUMAN and NM09
are both superior to the random baseline for all
measures (p?0.05). NP-AUTO, however, is only
superior in terms of recall and F1 (p?0.05).
3.5 Discussion
The results indicate that the simple set of features
we have chosen can be used for intentional seg-
mentation. While the results are not near human
performance, it is encouraging that such a simple
set of easily extractable features achieves results
that are 19% (?), 24% (P (k)), and 18% (F1) of
human performance, relative to the random base-
line.
The other notable result is the very high recall
score of NP-AUTO, which helps to produce a re-
spectable F1 score. However, a low ? reveals that
when accounting for class bias, this system is ac-
tually not far from the performance of a high recall
random classifier.
Error analysis showed that the reason for the
problems with NP-AUTO was the lack of reference
chains produced by the automatic co-reference
system. While the system seems to have per-
formed well for direct co-reference, it did not do
well with bridging reference. Inferred relations
were an important part of the reference chains pro-
duced by P&L, and it is now clear that these play
a significant role in the performance of the NP al-
gorithm. Our algorithm is not dependent on this
difficult processing problem, which typically re-
quires world knowledge in the form of training on
large datasets or the use of large lexical resources.
58
4 Topic vs. Intentional Segmentation
It is important to place our experiment on inten-
tional segmentation in context with the most com-
monly studied automatic segmentation task: topic-
based segmentation. While the two tasks are dis-
tinct, the literature has drawn connections between
them which can at times be confusing. In this sec-
tion, we attempt to clarify those connections by
pointing out some of their differences and similar-
ities. We also conduct an experiment comparing
our algorithm to well-known topic-segmentation
algorithms and discuss the results.
4.1 Automatic segmentation in the literature
One of the most widely-cited discourse segmen-
tation algorithms is TextTiling (Hearst, 1997).
Designed to segment texts into multi-paragraph
subtopics, it works by operationalizing the notion
of lexical cohesion (Halliday and Hasan, 1976).
TextTiling and related algorithms exploit the col-
location of semantically related lexemes to mea-
sure coherence. Recent improvements to this
method include the use of alternative lexical sim-
ilarity metrics like LSA (Choi et al, 2001) and
alternative segmentation methods like the mini-
mum cut model (Malioutov and Barzilay, 2006)
and ranking and clustering (Choi, 2000). Re-
cently, Bayesian approaches which model top-
ics as a lexical generative process have been em-
ployed (Purver et al, 2006; Eisenstein and Barzi-
lay, 2008). What these algorithms all share is a
focus on the semantic content of the discourse.
Passonneau and Litman (1997) is another of the
most widely-cited articles on discourse segmenta-
tion. Their overall approach combines an investi-
gation of prosodic features, cue words, and entity
reference. As described above, their approach to
using entity reference is motivated by Centering
theory (Grosz et al, 1995) and the hypothesis that
intentional structure is exhibited in the attentional
relationships between discourse referents.
Hearst and P&L try to achieve different goals,
but their tasks are nonetheless related. One might
reasonably hypothesize, for example, that either
lexical similarity or co-reference could be use-
ful to either type of segmentation on the grounds
that the two phenomena are clearly related. How-
ever, there are also clear differences of intent be-
tween the two studies. While there is an ob-
vious difference in the dataset (written exposi-
tory text vs. spoken narrative monologue), the an-
notation instructions reflect the difference most
clearly. Hearst instructed naive annotators to mark
paragraph boundaries ?where the topics seem to
change,? whereas P&L asked naive annotators to
mark prosodic phrases where the speaker had be-
gun a new communicative task.
The results indicate that there is a difference
in granularity between the two tasks, with inten-
tional segmentation relating to finer-grained struc-
ture. Hearst?s segments have a mean of about 200
words to P&L?s 40. Also, two hierarchical topic
segmentations of meetings (Hsueh, 2008; Gruen-
stein et al, 2008) have averages above 400 words
for the smallest level of segment.
To our knowledge, P&L is the only existing
study of automatic intention-based segmentation.
However, their work has been frequently cited as a
study of topic-oriented segmentation, e.g., (Galley
et al, 2003; Eisenstein and Barzilay, 2008). Also,
recent research in conversational genres (Galley et
al., 2003; Hsueh and Moore, 2007) analyze events
like discussing an agenda or giving a presentation,
which resemble more intentional categories. Inter-
estingly, these algorithms demonstrate the bene-
fit of including non-lexical, non-semantic features.
The results imply that further analysis is needed to
understand the links between different types of co-
herence and different types of segmentation.
4.2 Experiment 2
We have extended the above experiment to com-
pare the results of our novel algorithm with ex-
isting topic segmentation methods. We employ
Choi?s implementations of C99 (Choi, 2000) and
TEXTTILING (Hearst, 1997) as examples of well-
known topic-oriented methods. While we ac-
knowledge that there are newer algorithms which
improve upon this work, these were selected for
being well studied and easy to apply out-of-the-
box. Our method and evaluation is the same as in
the previous experiment.
The mean results for the 18 narratives are shown
in Table 2, with the human and baseline score re-
produced from the previous table. All three auto-
matic algorithms are superior to the random base-
line in terms of P (k), ?, and F1 (p?0.05). The
only statistically significant difference (p?0.05)
between the three automatic methods is between
NM09 and TEXTTILING in terms of F1. The ob-
served difference between NM09 and TEXTTIL-
ING in terms of ? is only moderately significant
59
Table 2: Results comparing our method to topic-
oriented segmentation methods.
NP-auto P (k) ? F1 Rec. Prec.
Human .21 .58 .65 .64 .69
NM09 .44 .11 .24 .24 .28
C99 .44 .08 .22 .20 .24
TEXTTILING .41 .05 .18 .16 .21
Random .50 .00 .15 .14 .17
(p?0.08). The observed differences between be-
tween NM09 and C99 are minimally significant
(p?0.24) .
4.3 Discussion
The comparable performance achieved by our
simple perspective-based approach in comparison
to lexical-semantic approaches suggests two main
points. First, it validates our novel approach in
practical applied terms. It shows that perspective-
oriented features, being simple to extract and ap-
plicable to a variety of genres, are potentially very
useful for automatic discourse segmentation sys-
tems.
Second, the results show that the teasing apart
of topic-oriented and intentional structure may be
quite difficult. Studies of coherence at the level of
short passages or episodes (Korolija, 1998) sug-
gest that coherence is established through a com-
plex interaction of topical, intentional, and other
contextual factors. In this experiment, the major
portion of the dialogues are oriented toward the
basic narrative activity which is the premise of the
Pear Stories dataset. This means that there are
many times when the activity type does not change
at intentional boundaries. At other times, the ac-
tivity type changes but neither the topic nor the set
of referents is significantly changed. The differ-
ent types of algorithms we have tried (i.e., topical,
referential, and perspectival) seem to be operating
on somewhat orthogonal bases, though it is dif-
ficult to say quantitatively how this relates to the
types of ?communicative task? transitions occur-
ring at the boundaries. In a sense, we have pro-
posed an algorithm for performing ?activity type
cohesion? which mimics the methods of lexical
cohesion but is based upon a different dimension
of the discourse. The results indicate that these are
both related to intentional structure.
5 General Discussion and Future Work
Future work in intentional segmentation is needed.
Our ultimate goal is to extend this work to more
conversational domains (e.g., multi-party planning
meetings) and to define the richer set of perspec-
tives and related deictic features that would be
needed for them. For example, we hypothesize
that the different uses of second-person pronouns
in conversations (Gupta et al, 2007) are likely to
reflect alternative activity types. Our feature set
and extraction methods will therefore need to be
further developed to capture this complexity.
The other question we would like to address is
the relationship between various types of coher-
ence (e.g., topical, referential, perspectival, etc.)
and different types (and levels) of discourse struc-
ture. Our current approach uses a feature space
that is orthogonal to most existing segmentation
methods. This has allowed us to gain a deeper
understanding of the relationship between certain
linguistic features and the underlying intentional
structure, but more work is needed.
In terms of practical motivations, we also plan
to address the open question of how to effectively
combine our feature set with other feature sets
which have also been demonstrated to contribute
to discourse structuring and segmentation.
References
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177?210.
Wallace L. Chafe, editor. 1980. The Pear Stories:
Cognitive, Cultural, and Linguistic Aspects of Nar-
rative Production, volume 3 of Advances in Dis-
course Processes. Ablex, Norwood, NJ.
Wallace L. Chafe. 1994. Discourse, Consciousness,
and Time: The Flow and Displacement of Conscious
Experience in Speaking and Writing. University of
Chicago Press, Chicago.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Jo-
hanna Moore. 2001. Latent semantic analysis for
text segmentation. In Proc. EMNLP, pages 109?
117.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proc. NAACL,
pages 26?33.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. LREC, pages 562?569.
60
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proc. EMNLP,
pages 334?343.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proc.
ACL, pages 562?569.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the
local coherence of discourse. Computational Lin-
guistics, 21(2):203?225.
Claire Grover and Richard Tobin. 2006. Rule-based
chunking and reusability. In Proc. LREC.
Alexander Gruenstein, John Niekrasz, and Matthew
Purver. 2008. Meeting structure annotation: Anno-
tations collected with a general purpose toolkit. In
L. Dybkjaer and W. Minker, editors, Recent Trends
in Discourse and Dialogue, pages 247?274.
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007. Resolving ?you? in multi-
party dialog. In Proc. SIGdial, pages 227?230.
M. A. K. Halliday and Ruqayia Hasan. 1976. Cohe-
sion in English. Longman, New York.
Marti Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Pei-Yun Hsueh and Johanna D. Moore. 2007. Com-
bining multiple knowledge sources for dialogue seg-
mentation in multimedia archives. In Proc. ACL,
pages 1016?1023.
Pei-Yun Hsueh. 2008. Meeting Decision Detection:
Multimodal Information Fusion for Multi-Party Di-
alogue Understanding. Ph.D. thesis, School of In-
formatics, University of Edinburgh.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In NIPS 15.
Natascha Korolija. 1998. Episodes in talk: Construct-
ing coherence in multiparty conversation. Ph.D. the-
sis, Link?ping University, The Tema Institute, De-
partment of Communications Studies.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
COLING-ACL, pages 25?32.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse segmentation by human and automated
means. Computational Linguistics, 23(1):103?139.
Massimo Poesio and Mijail A. Kabadjov. 2004. A
general-purpose, off-the-shelf anaphora resolution
module: Implementation and preliminary evalua-
tion. In Proc. LREC.
Matthew Purver, Konrad K?rding, Thomas Griffiths,
and Joshua Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In
Proc. COLING-ACL, pages 17?24.
Carlota S. Smith. 2003. Modes of Discourse. Camb-
drige University Press, Cambridge.
Janyce M. Wiebe. 1994. Tracking point of view in nar-
rative. Computational Linguistics, 20(2):233?287.
Janyce M. Wiebe. 1995. References in narrative text.
In Judy Duchan, Gail Bruder, and Lynne Hewitt, ed-
itors, Deixis in Narrative: A Cognitive Science Per-
spective, pages 263?286.
61
Proceedings of the EACL 2009 Demonstrations Session, pages 33?36,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
The Software Architecture for the
First Challenge on Generating Instructions in Virtual Environments
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Kristina Striegnitz
Union College
striegnk@union.edu
Abstract
The GIVE Challenge is a new Internet-
based evaluation effort for natural lan-
guage generation systems. In this paper,
we motivate and describe the software in-
frastructure that we developed to support
this challenge.
1 Introduction
Natural language generation (NLG) systems are
notoriously hard to evaluate. On the one hand,
simply comparing system outputs to a gold stan-
dard is not appropriate because there can be mul-
tiple generated outputs that are equally good, and
finding metrics that account for this variability and
produce results consistent with human judgments
and task performance measures is difficult (Belz
and Gatt, 2008; Stent et al, 2005; Foster, 2008).
On the other hand, lab-based evaluations with hu-
man subjects to assess each aspect of the system?s
functionality are expensive and time-consuming.
These characteristics make it hard to compare dif-
ferent systems and measure progress.
GIVE (?Generating Instructions in Virtual En-
vironments?) (Koller et al, 2007) is a research
challenge for the NLG community designed to
provide a new approach to NLG system evalua-
tion. In the GIVE scenario, users try to solve
a treasure hunt in a virtual 3D world that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual envi-
ronment. The challenge for the NLG system is
to generate, in real time, natural-language instruc-
tions that will guide the users to the successful
completion of their task (see Fig. 1). One cru-
cial advantage of this generation task is that the
NLG system and the user can be physically sepa-
rated. This makes it possible to carry out a task-
based evaluation over the Internet ? an approach
that has been shown to provide generous amounts
Figure 1: The GIVE Challenge.
of data in earlier studies (von Ahn and Dabbish,
2004; Orkin and Roy, 2007).
In this paper, we describe the software archi-
tecture underlying the GIVE Challenge. The soft-
ware connects each player in a 3D game world
with an NLG system over the Internet. It is imple-
mented and open source, and can be a used online
during EACL at www.give-challenge.org.
In Section 2, we give an introduction to the GIVE
evaluation methodology by describing the experi-
ence of a user participating in the evaluation, the
nature of the data we collect, and our scientific
goals. Then we explain the software architecture
behind the scenes and sketch the API that concrete
NLG systems must implement in Section 3. In
Section 4, we present some preliminary evaluation
results, before we conclude in Section 5.
2 Evaluation method
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
33
b2 b3b4 b5
b6
b7
b1
player
b8b9
b10
b11 b14b13b12
safe 
door
b1 opens doorto room 3
b9 moves picture to
b8: part of safe sequencereveal safe
? to win you have to retrieve the trophy from the safe in room 1? use button b9 to move the picture (and get access to the safe)
? if the alarm sounds, the game is over and you have lost
? press buttons b8, b6, b13, b13, b10 (in this order) to open the safe;if a button is pressed in the wrong order, the whole sequence is reset
b14 makes alarm soundb10, b13: part of safe sequence door to room 2b7 opens/closesstepping on this tiletriggers alarm
alarm
room 3
b2 turns off alarm tileb3 opens/closes door to room 2
b6: part of safe sequence
room 1
b5 makes alarm sound
room 2
door
door
lampcouch
chair
flower
pictu
retrophy
Figure 2: The map of a virtual world.
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system.
The map of one of the game worlds is shown in
Fig. 2: In this world, players must pick up a trophy,
which is in a wall safe behind a picture. In order
to access the trophy, they must first push a button
to move the picture to the side, and then push an-
other sequence of buttons to open the safe. One
floor tile is alarmed, and players lose the game
if they step on this tile without deactivating the
alarm first. There are also a number of distrac-
tor buttons which either do nothing when pressed
or set off an alarm. These distractor buttons are in-
tended to make the game harder and, more impor-
tantly, to require appropriate reference to objects
in the game world. Finally, game worlds can con-
tain a number of objects such as chairs and flowers
which are irrelevant for the task, but can be used
as landmarks by a generation system.
Users are asked to fill out a before- and after-
game questionnaire that collects some demo-
graphic data and asks the user to rate various as-
pects of the instructions they received. Every ac-
tion that players take in a game world, and every
instruction that a generation system generates for
them, is recorded in a database. In addition to the
questionnaire data, we are thus able to compute a
number of objective measures such as:
? the percentage of users each system leads to
a successful completion of the task;
? the average time, the average number of in-
structions, and the average number of in-
game actions that this success requires;
? the percentage of generated referring expres-
sions that the user resolves correctly; and
? average reaction times to instructions.
It is important to note that we have designed
the GIVE Challenge not as a competition, but as
a friendly evaluation effort where people try to
learn from each other?s successes. This is reflected
in the evaluation measures above, which are in
tension with one another: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?) will enjoy short reaction times, but it will re-
quire more instructions than a system that aggre-
gates these. To further emphasize this perspective,
we will also provide a number of diagnostic tools,
such as heat maps that show how much time users
spent on each tile, or a playback function which
displays an entire game run in real time.
In summary, the GIVE Challenge is a novel
evaluation effort for NLG systems. It is motivated
by real applications (such as pedestrian navigation
and the generation of task instructions), makes
no assumptions about the internal structure of an
NLG system, and emphasizes the situated genera-
tion of discourse in a simulated physical environ-
ment. The game world is scalable; it can be made
more complex and it can be adapted to focus on
specific issues in natural language generation.
3 Architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components:
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide.
34
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 3: The GIVE architecture.
When a user starts the client, it connects over
the Internet to the Matchmaker. The Matchmaker
then selects a game world and an NLG server at
random, and requests the NLG server to spawn
a new server instance. It then sends the game
world to the client and the server instance and dis-
connects from them, ready to handle new connec-
tions from other clients. The client and the server
instance play one game together: Whenever the
user does something, the client sends a message
about this to the server instance, and the server in-
stance can also send a message back to the client
at any time, which will then be displayed as an in-
struction. When the game ends, the client and the
server instance disconnect from each other. The
server instance sends a log of all game events to
the Matchmaker, and the client sends the ques-
tionnaire results to the Matchmaker; these then are
stored in the database for later analysis.
All of these components are implemented in
Java. This allows the client to be portable across
all major operating systems, and to be started di-
rectly from the website via Java Web Start without
the need for software installation. We felt it was
important to make startup of the client as effort-
less as possible, in order to maximize the num-
ber of users willing to play the game. Unsurpris-
ingly, we had to spend the majority of the pro-
gramming time on the 3D graphics (based on the
free jMonkeyEngine library) and the networking
code. We could have reduced the effort required
for these programming tasks by building upon an
existing virtual 3D world system such as Second
Life. However, we judged that the effort needed to
adapt such a system to our needs would have been
at least as high (in particular, we would have had
to ensure that the user could only move according
to the rules of the GIVE game and to instrument
the virtual world to obtain real-time updates about
events), and the result would have been less exten-
abstract class NlgSystem:
void connectionEstablished();
void connectionDisconnected();
void handleStatusInformation(Position playerPosition,
Orientation playerOrientation,
List?String? visibleObjects);
void handleAction(Atom actionInstance,
List?Formula? updates);
void handleDidNotUnderstand();
void handleMoveTurnAction(Direction direction);
. . .
Figure 4: The interface of an NLG system.
sible to future installments of the challenge.
Since we provided all the 3D, networking, and
database code, the research teams being evaluated
were able to concentrate on the development of
their NLG systems. Our only requirement was
that they implement a concrete subclass of the
class NlgSystem, shown in Fig. 4. This involves
overriding the six abstract callback methods in
this class with concrete implementations in
which the NLG system reacts to specific events.
The methods connectionEstablished
and connectionDisconnected are called
when users enter the game world and when
they disconnect from the game. The method
handleAction gets called whenever the user
performs some physical action, such as pushing a
button, and specifies what changed in the world
due to this action; handleMoveTurnAction
gets called whenever the user moves;
handleDidNotUnderstand gets called
whenever users press the H key to signal that
they didn?t understand the previous instruction;
and handleStatusInformation gets called
once per second and after each user action to
inform the server of the player?s position and
orientation and the visible objects. Ultimately,
each of these method calls gets triggered by a
message that the client sends over the network
in reaction to some event; but this is completely
hidden from the NLG system developer.
The NLG system can use the method send to
send a string to the client to be displayed. It also
has access to various methods querying the state of
the game world and to an interface to an external
planner which can compute a sequence of actions
leading to the goal.
4 First results
For this first installment of the GIVE Challenge,
four research teams from the US, the Netherlands,
35
and Spain provided generation systems, and a
number of other research groups expressed their
interest in participating, but weren?t able to partic-
ipate due to time constraints. Given that this was
the first time we organized this task, we find this
a very encouraging number. All four of the teams
consisted primarily of students who implemented
the NLG systems over the Northern-hemisphere
summer. This is in line with our goal of tak-
ing this first iteration as a ?dry run? in which we
could fine-tune the software, learn about the easy
and hard aspects of the challenge, and validate the
evaluation methodology.
Public involvement in the GIVE Challenge was
launched with a press release in early Novem-
ber 2008; the Matchmaker and the NLG servers
were then kept running until late January 2009.
During this time, online users played over 1100
games, which translates into roughly 75 game runs
for each experimental condition (i.e., five differ-
ent NLG systems paired with three different game
worlds). To our knowledge, this makes GIVE the
largest NLG evaluation effort yet in terms of ex-
perimental subjects.
While we have not yet carried out the detailed
evaluation, the preliminary results look promising:
a casual inspection shows that there are consider-
able differences in task success rate among the dif-
ferent systems.
While there is growing evidence from differ-
ent research areas that the results of Internet-based
evaluations are consistent with more traditional
lab-based experiments (e.g., (Keller et al, 2008;
Gosling et al, 2004)), the issue is not yet set-
tled. Therefore, we are currently conducting a lab-
based evaluation of the GIVE NLG systems, and
will compare those results to the qualitative and
quantitative data provided by the online subjects.
5 Conclusion
In this paper, we have sketched the GIVE Chal-
lenge and the software infrastructure we have de-
veloped for it. The GIVE Challenge is, to the
best of our knowledge, the largest-scale NLG eval-
uation effort with human experimental subjects.
This is made possible by connecting users and
NLG systems over the Internet; we collect eval-
uation data automatically and unobtrusively while
the user simply plays a 3D game. While we will
report on the results of the evaluation in more de-
tail at a later time, first results seem encouraging
in that the performance of different NLG systems
differs considerably.
In the future, we will extend the GIVE Chal-
lenge to harder tasks. Possibilities includ mak-
ing GIVE into a dialogue challenge by allowing
the user to speak as well as act in the world; run-
ning the challenge in a continuous world rather
than a world that only allows discrete movements;
or making it multimodal by allowing the NLG
system to generate arrows or virtual human ges-
tures. All these changes would only require lim-
ited changes to the GIVE software architecture.
However, the exact nature of future directions re-
mains to be discussed with the community.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
F. Keller, S. Gunasekharan, N. Mayo, and M. Corley.
2008. Timing accuracy of web experiments: A case
study using the WebExp software package. Behav-
ior Research Methods, to appear.
A. Koller, J. Moore, B. di Eugenio, J. Lester, L. Stoia,
D. Byron, J. Oberlander, and K. Striegnitz. 2007.
Shared task proposal: Instruction giving in virtual
worlds. In M. White and R. Dale, editors, Work-
ing group reports of the Workshop on Shared Tasks
and Comparative Evaluation in Natural Language
Generation. Available at http://www.ling.
ohio-state.edu/nlgeval07/report.html.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
36
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 471?481,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Evaluating language understanding accuracy with respect to objective
outcomes in a dialogue system
Myroslava O. Dzikovska and Peter Bell and Amy Isard and Johanna D. Moore
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh, United Kingdom
{m.dzikovska,peter.bell,amy.isard,j.moore}@ed.ac.uk
Abstract
It is not always clear how the differences
in intrinsic evaluation metrics for a parser
or classifier will affect the performance of
the system that uses it. We investigate the
relationship between the intrinsic evalua-
tion scores of an interpretation component
in a tutorial dialogue system and the learn-
ing outcomes in an experiment with human
users. Following the PARADISE method-
ology, we use multiple linear regression to
build predictive models of learning gain,
an important objective outcome metric in
tutorial dialogue. We show that standard
intrinsic metrics such as F-score alone do
not predict the outcomes well. However,
we can build predictive performance func-
tions that account for up to 50% of the vari-
ance in learning gain by combining fea-
tures based on standard evaluation scores
and on the confusion matrix entries. We
argue that building such predictive mod-
els can help us better evaluate performance
of NLP components that cannot be distin-
guished based on F-score alone, and illus-
trate our approach by comparing the cur-
rent interpretation component in the system
to a new classifier trained on the evaluation
data.
1 Introduction
Much of the work in natural language processing
relies on intrinsic evaluation: computing standard
evaluation metrics such as precision, recall and F-
score on the same data set to compare the perfor-
mance of different approaches to the same NLP
problem. However, once a component, such as
a parser, is included in a larger system, it is not
always clear that improvements in intrinsic eval-
uation scores will translate into improved over-
all system performance. Therefore, extrinsic or
task-based evaluation can be used to complement
intrinsic evaluations. For example, NLP com-
ponents such as parsers and co-reference resolu-
tion algorithms could be compared in terms of
how much they contribute to the performance of
a textual entailment (RTE) system (Sammons et
al., 2010; Yuret et al 2010); parser performance
could be evaluated by how well it contributes to
an information retrieval task (Miyao et al 2008).
However, task-based evaluation can be difficult
and expensive for interactive applications. Specif-
ically, task-based evaluation for dialogue systems
typically involves collecting data from a number
of people interacting with the system, which is
time-consuming and labor-intensive. Thus, it is
desirable to develop an off-line evaluation pro-
cedure that relates intrinsic evaluation metrics to
predicted interaction outcomes, reducing the need
to conduct experiments with human participants.
This problem can be addressed via the use of
the PARADISE evaluation methodology for spo-
ken dialogue systems (Walker et al 2000). In a
PARADISE study, after an initial data collection
with users, a performance function is created to
predict an outcome metric (e.g., user satisfaction)
which can normally only be measured through
user surveys. Typically, a multiple linear regres-
sion is used to fit a predictive model of the desired
metric based on the values of interaction param-
eters that can be derived from system logs with-
out additional user studies (e.g., dialogue length,
word error rate, number of misunderstandings).
PARADISE models have been used extensively
in task-oriented spoken dialogue systems to estab-
lish which components of the system most need
improvement, with user satisfaction as the out-
come metric (Mo?ller et al 2007; Mo?ller et al
2008; Walker et al 2000; Larsen, 2003). In tu-
torial dialogue, PARADISE studies investigated
471
which manually annotated features predict learn-
ing outcomes, to justify new features needed in
the system (Forbes-Riley et al 2007; Rotaru and
Litman, 2006; Forbes-Riley and Litman, 2006).
We adapt the PARADISE methodology to eval-
uating individual NLP components, linking com-
monly used intrinsic evaluation scores with ex-
trinsic outcome metrics. We describe an evalua-
tion of an interpretation component of a tutorial
dialogue system, with student learning gain as the
target outcome measure. We first describe the
evaluation setup, which uses standard classifica-
tion accuracy metrics for system evaluation (Sec-
tion 2). We discuss the results of the intrinsic sys-
tem evaluation in Section 3. We then show that
standard evaluation metrics do not serve as good
predictors of system performance for the system
we evaluated. However, adding confusion matrix
features improves the predictive model (Section
4). We argue that in practical applications such
predictive metrics should be used alongside stan-
dard metrics for component evaluations, to bet-
ter predict how different components will perform
in the context of a specific task. We demonstrate
how this technique can help differentiate the out-
put quality between a majority class baseline, the
system?s output, and the output of a new classifier
we trained on our data (Section 5). Finally, we
discuss some limitations and possible extensions
to this approach (Section 6).
2 Evaluation Procedure
2.1 Data Collection
We collected transcripts of students interacting
with BEETLE II (Dzikovska et al 2010b), a tu-
torial dialogue system for teaching conceptual
knowledge in the basic electricity and electron-
ics domain. The system is a learning environment
with a self-contained curriculum targeted at stu-
dents with no knowledge of high school physics.
When interacting with the system, students spend
3-5 hours going through pre-prepared reading ma-
terial, building and observing circuits in a simula-
tor, and talking with a dialogue-based computer
tutor via a text-based chat interface.
During the interaction, students can be asked
two types of questions. Factual questions require
them to name a set of objects or a simple prop-
erty, e.g., ?Which components in circuit 1 are in
a closed path?? or ?Are bulbs A and B wired
in series or in parallel?. Explanation and defi-
nition questions require longer answers that con-
sist of 1-2 sentences, e.g., ?Why was bulb A on
when switch Z was open?? (expected answer ?Be-
cause it was still in a closed path with the bat-
tery?) or ?What is voltage?? (expected answer
?Voltage is the difference in states between two
terminals?). We focus on the performance of the
system on these long-answer questions, since re-
acting to them appropriately requires processing
more complex input than factual questions.
We collected a corpus of 35 dialogues from
paid undergraduate volunteers interacting with the
system as part of a formative system evaluation.
Each student completed a multiple-choice test as-
sessing their knowledge of the material before and
after the session. In addition, system logs con-
tained information about how each student?s utter-
ance was interpreted. The resulting data set con-
tains 3426 student answers grouped into 35 sub-
sets, paired with test results. The answers were
then manually annotated to create a gold standard
evaluation corpus.
2.2 BEETLE II Interpretation Output
The interpretation component of BEETLE II uses
a syntactic parser and a set of hand-authored rules
to extract the domain-specific semantic represen-
tations of student utterances from the text. The
student answer is first classified with respect to its
domain-specific speech act, as follows:
? Answer: a contentful expression to which
the system responds with a tutoring action,
either accepting it as correct or remediating
the problems as discussed in (Dzikovska et
al., 2010a).
? Help request: any expression indicating that
the student does not know the answer and
without domain content.
? Social: any expression such as ?sorry? which
appears to relate to social interaction and has
no recognizable domain content.
? Uninterpretable: the system could not arrive
at any interpretation of the utterance. It will
respond by identifying the likely source of
error, if possible (e.g., a word it does not un-
derstand) and asking the student to rephrase
their utterance (Dzikovska et al 2009).
472
If the student utterance was determined to be an
answer, it is further diagnosed for correctness as
discussed in (Dzikovska et al 2010b), using a do-
main reasoner together with semantic representa-
tions of expected correct answers supplied by hu-
man tutors. The resulting diagnosis contains the
following information:
? Consistency: whether the student statement
correctly describes the facts mentioned in
the question and the simulation environment:
e.g., student saying ?Switch X is closed? is
labeled inconsistent if the question stipulated
that this switch is open.
? Diagnosis: an analysis of how well the stu-
dent?s explanation matches the expected an-
swer. It consists of 4 parts
? Matched: parts of the student utterance
that matched the expected answer
? Contradictory: parts of the student ut-
terance that contradict the expected an-
swer
? Extra: parts of the student utterance that
do not appear in the expected answer
? Not-mentioned: parts of the expected
answer missing from the student utter-
ance.
The speech act and the diagnosis are passed to
the tutorial planner which makes decisions about
feedback. They constitute the output of the inter-
pretation component, and its quality is likely to
affect the learning outcomes, therefore we need
an effective way to evaluate it. In future work,
performance of individual pipeline components
could also be evaluated in a similar fashion.
2.3 Data Annotation
The general idea of breaking down the student an-
swer into correct, incorrect and missing parts is
common in tutorial dialogue systems (Nielsen et
al., 2008; Dzikovska et al 2010b; Jordan et al
2006). However, representation details are highly
system specific, and difficult and time-consuming
to annotate. Therefore we implemented a simpli-
fied annotation scheme which classifies whole an-
swers as correct, partially correct but incomplete,
or contradictory, without explicitly identifying the
correct and incorrect parts. This makes it easier to
create the gold standard and still retains useful in-
formation, because tutoring systems often choose
the tutoring strategy based on the general answer
class (correct, incomplete, or contradictory). In
addition, this allows us to cast the problem in
terms of classifier evaluation, and to use standard
classifier evaluation metrics. If more detailed an-
notations were available, this approach could eas-
ily be extended, as discussed in Section 6.
We employed a hierarchical annotation scheme
shown in Figure 1, which is a simplification of
the DeMAND coding scheme (Campbell et al
2009). Student utterances were first annotated
as either related to domain content, or not con-
taining any domain content, but expressing the
student?s metacognitive state or attitudes. Utter-
ances expressing domain content were then coded
with respect to their correctness, as being fully
correct, partially correct but incomplete, contain-
ing some errors (rather than just omissions) or
irrelevant1. The ?irrelevant? category was used
for utterances which were correct in general but
which did not directly answer the question. Inter-
annotator agreement for this annotation scheme
on the corpus was ? = 0.69.
The speech acts and diagnoses logged by the
system can be automatically mapped into our an-
notation labels. Help requests and social acts
are assigned the ?non-content? label; answers
are assigned a label based on which diagnosis
fields were filled: ?contradictory? for those an-
swers labeled as either inconsistent, or contain-
ing something in the contradictory field; ?incom-
plete? if there is something not mentioned, but
something matched as well, and ?irrelevant? if
nothing matched (i.e., the entire expected answer
is in not-mentioned). Finally, uninterpretable ut-
terances are treated as unclassified, analogous to a
situation where a statistical classifier does not out-
put a label for an input because the classification
probability is below its confidence threshold.
This mapping was then compared against the
manually annotated labels to compute the intrin-
sic evaluation scores for the BEETLE II interpreter
described in Section 3.
3 Intrinsic Evaluation Results
The interpretation component of BEETLE II was
developed based on the transcripts of 8 sessions
1Several different subcategories of non-content utter-
ances, and of contradictory utterances, were recorded. How-
ever, they resulting classes were too small and so were col-
lapsed into a single category for purposes of this study.
473
Category Subcategory Description
Non-content Metacognitive and social expressions without domain content, e.g., ?I
don?t know?, ?I need help?, ?you are stupid?
Content The utterance includes domain content.
correct The student answer is fully correct
pc incomplete The student said something correct, but incomplete, with some parts of
the expected answer missing
contradictory The student?s answer contains something incorrect or contradicting the
expected answer, rather than just an omission
irrelevant The student?s statement is correct in general, but it does not answer the
question.
Figure 1: Annotation scheme used in creating the gold standard
Label Count Frequency
correct 1438 0.43
pc incomplete 796 0.24
contradictory 808 0.24
irrelevant 105 0.03
non content 232 0.07
Table 1: Distribution of annotated labels in the evalu-
ation corpus
of students interacting with earlier versions of the
system. These sessions were completed prior to
the beginning of the experiment during which our
evaluation corpus was collected, and are not in-
cluded in the corpus. Thus, the corpus constitutes
unseen testing data for the BEETLE II interpreter.
Table 1 shows the distribution of codes in
the annotated data. The distribution is unbal-
anced, and therefore in our evaluation results we
use two different ways to average over per-class
evaluation scores. Macro-average combines per-
class scores disregarding the class sizes; micro-
average weighs the per-class scores by class size.
The overall classification accuracy (defined as the
number of correctly classified instances out of all
instances) is mathematically equivalent to micro-
averaged recall; however, macro-averaging better
reflects performance on small classes, and is com-
monly used for unbalanced classification prob-
lems (see, e.g., (Lewis, 1991)).
The detailed evaluation results are presented
in Table 2. We will focus on two metrics: the
overall classification accuracy (listed as ?micro-
averaged recall? as discussed above), and the
macro-averaged F score.
The majority class baseline is to assign ?cor-
rect? to every instance. Its overall accuracy is
43%, the same as BEETLE II. However, this is
obviously not a good choice for a tutoring sys-
tem, since students who make mistakes will never
get tutoring feedback. This is reflected in a much
lower value of the F score (0.12 macroaverage F
score for baseline vs. 0.44 for BEETLE II). Note
also that there is a large difference in the micro-
and macro- averaged scores. It is not immediately
clear which of these metrics is the most important,
and how they relate to actual system performance.
We discuss machine learning models to help an-
swer this question in the next section.
4 Linking Evaluation Measures to
Outcome Measures
Although the intrinsic evaluation shows that the
BEETLE II interpreter performs better than the
baseline on the F score, ultimately system devel-
opers are not interested in improving interpreta-
tion for its own sake: they want to know whether
the time spent on improvements, and the compli-
cations in system design which may accompany
them, are worth the effort. Specifically, do such
changes translate into improvement in overall sys-
tem performance?
To answer this question without running expen-
sive user studies we can build a model which pre-
dicts likely outcomes based on the data observed
so far, and then use the model?s predictions as an
additional evaluation metric. We chose a multiple
linear regression model for this task, linking the
classification scores with learning gain as mea-
sured during the data collection. This approach
follows the general PARADISE approach (Walker
et al 2000), but while PARADISE is typically
used to determine which system components need
474
Label baseline BEETLE II
prec. recall F1 prec. recall F1
correct 0.43 1.00 0.60 0.93 0.52 0.67
pc incomplete 0.00 0.00 0.00 0.42 0.53 0.47
contradictory 0.00 0.00 0.00 0.57 0.22 0.31
irrelevant 0.00 0.00 0.00 0.17 0.15 0.16
non-content 0.00 0.00 0.00 0.91 0.41 0.57
macroaverage 0.09 0.20 0.12 0.60 0.37 0.44
microaverage 0.18 0.43 0.25 0.70 0.43 0.51
Table 2: Intrinsic Evaluation Results for the BEETLE II and a majority class baseline
the most improvement, we focus on finding a bet-
ter performance metric for a single component
(interpretation), using standard evaluation scores
as features.
Recall from Section 2.1 that each participant
in our data collection was given a pre-test and
a post-test, measuring their knowledge of course
material. The test score was equal to the propor-
tion of correctly answered questions. The normal-
ized learning gain, post?pre1?pre is a metric typically
used to assess system quality in intelligent tutor-
ing, and this is the metric we are trying to model.
Thus, the training data for our model consists of
35 instances, each corresponding to a single dia-
logue and the learning gain associated with it. We
can compute intrinsic evaluation scores for each
dialogue, in order to build a model that predicts
that student?s learning gain based on these scores.
If the model?s predictions are sufficiently reliable,
we can also use them for predicting the learning
gain that a student could achieve when interacting
with a new version of the interpretation compo-
nent for the system, not yet tested with users. We
can then use the predicted score to compare dif-
ferent implementations and choose the one with
the highest predicted learning gain.
4.1 Features
Table 4 lists the feature sets we used. We tried two
basic types of features. First, we used the eval-
uation scores reported in the previous section as
features. Second, we hypothesized that some er-
rors that the system makes are likely to be worse
than others from a tutoring perspective. For ex-
ample, if the student gives a contradictory answer,
accepting it as correct may lead to student miscon-
ceptions; on the other hand, calling an irrelevant
answer ?partially correct but incomplete? may be
less of a problem. Therefore, we computed sepa-
rate confusion matrices for each student. We nor-
malized each confusion matrix cell by the total
number of incorrect classifications for that stu-
dent. We then added features based on confusion
frequencies to our feature set.2
Ideally, we should add 20 different features to
our model, corresponding to every possible con-
fusion. However, we are facing a sparse data
problem, illustrated by the overall confusion ma-
trix for the corpus in Table 3. For example,
we only observed 25 instances where a contra-
dictory utterance was miscategorized as correct
(compared to 200 ?contradictory?pc incomplete?
confusions), and so for many students this mis-
classification was never observed, and predictions
based on this feature are not likely to be reliable.
Therefore, we limited our features to those mis-
classifications that occurred at least twice for each
student (i.e., at least 70 times in the entire cor-
pus). The list of resulting features is shown in the
?conf? row of Table 4. Since only a small num-
ber of features was included, this limits the appli-
cability of the model we derived from this data
set to the systems which make similar types of
confusions. However, it is still interesting to in-
vestigate whether confusion probabilities provide
additional information compared to standard eval-
uation metrics. We discuss how better coverage
could be obtained in Section 6.
4.2 Regression Models
Table 5 shows the regression models we obtained
using different feature sets. All models were ob-
tained using stepwise linear regression, using the
Akaike information criterion (AIC) for variable
2We also experimented with using % unclassified as an
additional feature, since % of rejections is known to be a
problem for spoken dialogue systems. However, it did not
improve the models, and we do not report it here for brevity.
475
Actual
Predicted contradictory correct irrelevant non-content pc incomplete
contradictory 175 86 3 0 43
correct 25 752 1 4 26
irrelevant 31 12 16 4 29
non-content 1 3 2 95 3
pc incomplete 200 317 40 28 419
Table 3: Confusion matrix for BEETLE II. System predicted values are in rows; actual values in columns.
selection implemented in the R stepwise regres-
sion library. As measures of model quality, we re-
port R2, the percentage of variance accounted for
by the models (a typical measure of fit in regres-
sion modeling), and mean squared error (MSE).
These were estimated using leave-one-out cross-
validation, since our data set is small.
We used feature ablation to evaluate the contri-
bution of different features. First, we investigated
models using precision, recall or F-score alone.
As can be seen from the table, precision is not pre-
dictive of learning gain, while F-score and recall
perform similarly to one another, withR2 = 0.12.
In comparison, the model using only confusion
frequencies has substantially higher estimated R2
and a lower MSE.3 In addition, out of the 3 con-
fusion features, only one is selected as predictive.
This supports our hypothesis that different types
of errors may have different importance within a
practical system.
The confusion frequency feature chosen by
the stepwise model (?predicted-pc incomplete-
actual-contradictory?) has a reasonable theoret-
ical justification. Previous research shows that
students who give more correct or partially cor-
rect answers, either in human-human or human-
computer dialogue, exhibit higher learning gains,
and this has been established for different sys-
tems and tutoring domains (Litman et al 2009).
Consequently, % of contradictory answers is neg-
atively predictive of learning gain. It is reasonable
to suppose, as predicted by our model, that sys-
tems that do not identify such answers well, and
therefore do not remediate them correctly, will do
worse in terms of learning outcomes.
Based on this initial finding, we investigated
the models that combined either F scores or the
3The decrease in MSE is not statistically significant, pos-
sibly because of the small data set. However, since we ob-
serve the same pattern of results across our models, it is still
useful to examine.
full set of intrinsic evaluation scores with confu-
sion frequencies. Note that if the full set of met-
rics (precision, recall, F score) is used, the model
derives a more complex formula which covers
about 33% of the variance. Our best models,
however, combine the averaged scores with con-
fusion frequencies, resulting in a higher R2 and
a lower MSE (22% relative decrease between the
?scores.f? and ?conf+scores.f? models in the ta-
ble). This shows that these features have comple-
mentary information, and that combining them in
an application-specific way may help to predict
how the components will behave in practice.
5 Using prediction models in evaluation
The models from Table 5 can be used to compare
different possible implementations of the inter-
pretation component, under the assumption that
the component with a higher predicted learning
gain score is more appropriate to use in an ITS.
To show how our predictive models can be used
in making implementation decisions, we compare
three possible choices for an interpretation com-
ponent: the original BEETLE II interpreter, the
baseline classifier described earlier, and a new de-
cision tree classifier trained on our data.
We built a decision tree classifier using the
Weka implementation of C4.5 pruned decision
trees, with default parameters. As features, we
used lexical similarity scores computed by the
Text::Similarity package4. We computed
8 features: the similarity between student answer
and either the expected answer text or the question
text, using 4 different scores: raw number of over-
lapping words, F1 score, lesk score and cosine
score. Its intrinsic evaluation scores are shown in
Table 6, estimated using 10-fold cross-validation.
We can compare BEETLE II and baseline clas-
sifier using the ?scores.all? model. The predicted
4http://search.cpan.org/dist/Text-Similarity/
476
Name Variables
scores.fm fmeasure.microaverage, fmeasure.macroaverage, fmeasure.correct,
fmeasure.contradictory, fmeasure.pc incomplete,fmeasure.non-content,
fmeasure.irrelevant
scores.precision precision.microaverage, precision.macroaverage, precision.correct,
precision.contradictory, precision.pc incomplete,precision.non-content,
precision.irrelevant
scores.recall recall.microaverage, recall.macroaverage, recall.correct, recall.contradictory,
recall.pc incomplete,recall.non-content, recall.irrelevant
scores.all scores.fm + scores.precision + scores.recall
conf Freq.predicted.contradictory.actual.correct,
Freq.predicted.pc incomplete.actual.correct,
Freq.predicted.pc incomplete.actual.contradictory
Table 4: Feature sets for regression models
Variables Cross-
validation
R2
Cross-
validation
MSE
Formula
scores.f 0.12
(0.02)
0.0232
(0.0302)
0.32
+ 0.56 ? fmeasure.microaverage
scores.precision 0.00
(0.00)
0.0242
(0.0370)
0.61
scores.recall 0.12
(0.02)
0.0232
(0.0310)
0.37
+ 0.56 ? recall.microaverage
conf 0.25
(0.03)
0.0197
(0.0262)
0.74
? 0.56 ?
Freq.predicted.pc incomplete.actual.contradictory
scores.all 0.33
(0.03)
0.0218
(0.0264)
0.63
+ 4.20 ? fmeasure.microaverage
? 1.30 ? precision.microaverage
? 2.79 ? recall.microaverage
? 0.07 ? recall.non? content
conf+scores.f 0.36
(0.03)
0.0179
(0.0281)
0.52
? 0.66 ?
Freq.predicted.pc incomplete.actual.contradictory
+ 0.42 ? fmeasure.correct
? 0.07 ? fmeasure.non? content
full
(conf+scores.all)
0.49
(0.02)
0.0189
(0.0248)
0.88
? 0.68 ?
Freq.predicted.pc incomplete.actual.contradictory
? 0.06 ? precision.non domain
+ 0.28 ? recall.correct
? 0.79 ? precision.microaverage
+ 0.65 ? fmeasure.microaverage
Table 5: Regression models for learning gain. R2 and MSE estimated with leave-one-out cross-validation.
Standard deviation in parentheses.
477
score for BEETLE II is 0.66. The predicted
score for the baseline is 0.28. We cannot use
the models based on confusion scores (?conf?,
?conf+scores.f? or ?full?) for evaluating the base-
line, because the confusions it makes are always
to predict that the answer is correct when the
actual label is ?incomplete? or ?contradictory?.
Such situations were too rare in our training data,
and therefore were not included in the models (as
discussed in Section 4.1). Additional data will
need to be collected before this model can rea-
sonably predict baseline behavior.
Compared to our new classifier, BEETLE II has
lower overall accuracy (0.43 vs. 0.53), but per-
forms micro- and macro- averaged scores. BEE-
TLE II precision is higher than that of the classi-
fier. This is not unexpected given how the system
was designed: since misunderstandings caused
dialogue breakdown in pilot tests, the interpreter
was built to prefer rejecting utterances as uninter-
pretable rather than assigning them to an incorrect
class, leading to high precision but lower recall.
However, we can use all our predictive models
to evaluate the classifier. We checked the the con-
fusion matrix (not shown here due to space lim-
itations), and saw that the classifier made some
of the same types of confusions that BEETLE II
interpreter made. On the ?scores.all? model, the
predicted learning gain score for the classifier is
0.63, also very close to BEETLE II. But with the
?conf+scores.all? model, the predicted score is
0.89, compared to 0.59 for BEETLE II, indicating
that we should prefer the newly built classifier.
Looking at individual class performance, the
classifier performs better than the BEETLE II in-
terpreter on identifying ?correct? and ?contradic-
tory? answers, but does not do as well for par-
tially correct but incomplete, and for irrelevant an-
swers. Using our predictive performance metric
highlights the differences between the classifiers
and effectively helps determine which confusion
types are the most important.
One limitation of this prediction, however, is
that the original system?s output is considerably
more complex: the BEETLE II interpreter explic-
itly identifies correct, incorrect and missing parts
of the student answer which are then used by the
system to formulate adaptive feedback. This is
an important feature of the system because it al-
lows for implementation of strategies such as ac-
knowledging and restating correct parts of the an-
Label prec. recall F1
correct 0.66 0.76 0.71
pc incomplete 0.38 0.34 0.36
contradictory 0.40 0.35 0.37
irrelevant 0.07 0.04 0.05
non-content 0.62 0.76 0.68
macroaverage 0.43 0.45 0.43
microaverage 0.51 0.53 0.52
Table 6: Intrinsic evaluation scores for our newly built
classifier.
swer. However, we could still use a classifier to
?double-check? the interpreter?s output. If the
predictions made by the original interpreter and
the classifier differ, and in particular when the
classifier assigns the ?contradictory? label to an
answer, BEETLE II may choose to use a generic
strategy for contradictory utterances, e.g. telling
the student that their answer is incorrect without
specifying the exact problem, or asking them to
re-read portions of the material.
6 Discussion and Future Work
In this paper, we proposed an approach for cost-
sensitive evaluation of language interpretation
within practical applications. Our approach is
based on the PARADISE methodology for dia-
logue system evaluation (Walker et al 2000).
We followed the typical pattern of a PARADISE
study, but instead of relying on a variety of fea-
tures that characterize the interaction, we used
scores that reflect only the performance of the
interpretation component. For BEETLE II we
could build regression models that account for
nearly 50% variance in the desired outcomes, on
par with models reported in earlier PARADISE
studies (Mo?ller et al 2007; Mo?ller et al 2008;
Walker et al 2000; Larsen, 2003). More impor-
tantly, we demonstrated that combining averaged
scores with features based on confusion frequen-
cies improves prediction quality and allows us to
see differences between systems which are not ob-
vious from the scores alone.
Previous work on task-based evaluation of NLP
components used RTE or information extraction
as target tasks (Sammons et al 2010; Yuret et al
2010; Miyao et al 2008), based on standard cor-
pora. We specifically targeted applications which
involve human-computer interaction, where run-
ning task-based evaluations is particularly expen-
478
sive, and building a predictive model of system
performance can simplify system development.
Our evaluation data limited the set of features
that we could use in our models. For most con-
fusion features, there were not enough instances
in the data to build a model that would reliably
predict learning gain for those cases. One way
to solve this problem would be to conduct a user
study in which the system simulates random er-
rors appearing some of the time. This could pro-
vide the data needed for more accurate models.
The general pattern we observed in our data
is that a model based on F-scores alone predicts
only a small proportion of the variance. If a full
set of metrics (including F-score, precision and
recall) is used, linear regression derives a more
complex equation, with different weights for pre-
cision and recall. Instead of the linear model, we
may consider using a model based on F? score,
F? = (1 + ?2) PR?2P+R , and fitting it to the data to
derive the ? weight rather than using the standard
F1 score. We plan to investigate this in the future.
Our method would apply to a wide range of
systems. It can be used straightforwardly with
many current spoken dialogue systems which rely
on classifiers to support language understanding
in domains such as call routing and technical sup-
port (Gupta et al 2006; Acomb et al 2007).
We applied it to a system that outputs more com-
plex logical forms, but we showed that we could
simplify its output to a set of labels which still
allowed us to make informed decisions. Simi-
lar simplifications could be derived for other sys-
tems based on domain-specific dialogue acts typ-
ically used in dialogue management. For slot-
based systems, it may be useful to consider con-
cept accuracy for recognizing individual slot val-
ues. Finally, for tutoring systems it is possible
to annotate the answers on a more fine-grained
level. Nielsen et al(2008) proposed an annota-
tion scheme based on the output of a dependency
parser, and trained a classifier to identify individ-
ual dependencies as ?expressed?, ?contradicted?
or ?unaddressed?. Their system could be evalu-
ated using the same approach.
The specific formulas we derived are not likely
to be highly generalizable. It is a well-known
limitation of PARADISE evaluations that models
built based on one system often do not perform
well when applied to different systems (Mo?ller et
al., 2008). But using them to compare implemen-
tation variants during the system development,
without re-running user evaluations, can provide
important information, as we illustrated with an
example of evaluating a new classifier we built for
our interpretation task. Moreover, the confusion
frequency feature that our models picked is con-
sistent with earlier results from a different tutor-
ing domain (see Section 4.2). Thus, these models
could provide a starting point when making sys-
tem development choices, which can then be con-
firmed by user evaluations in new domains.
The models we built do not fully account for
the variance in the training data. This is expected,
since interpretation performance is not the only
factor influencing the objective outcome: other
factors, such choosing the the appropriate tutor-
ing strategy, are also important. Similar models
could be built for other system components to ac-
count for their contribution to the variance. Fi-
nally, we could consider using different learning
algorithms. Mo?ller et al(2008) examined deci-
sion trees and neural networks in addition to mul-
tiple linear regression for predicting user satisfac-
tion in spoken dialogue. They found that neural
networks had the best prediction performance for
their task. We plan to explore other learning algo-
rithms for this task as part of our future work.
7 Conclusion
In this paper, we described an evaluation of an
interpretation component of a tutorial dialogue
system using predictive models that link intrin-
sic evaluation scores with learning outcomes. We
showed that adding features based on confusion
frequencies for individual classes significantly
improves the prediction. This approach can be
used to compare different implementations of lan-
guage interpretation components, and to decide
which option to use, based on the predicted im-
provement in a task-specific target outcome met-
ric trained on previous evaluation data.
Acknowledgments
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine, Leanne Taylor,
Katherine Harrison and Jonathan Kilgour for help
with data collection and preparation; and Christo-
pher Brew for helpful comments and discussion.
This work has been supported in part by the US
ONR award N000141010085.
479
References
Kate Acomb, Jonathan Bloom, Krishna Dayanidhi,
Phillip Hunter, Peter Krogh, Esther Levin, and
Roberto Pieraccini. 2007. Technical support dia-
log systems: Issues, problems, and solutions. In
Proceedings of the Workshop on Bridging the Gap:
Academic and Industrial Research in Dialog Tech-
nologies, pages 25?31, Rochester, NY, April.
Gwendolyn C. Campbell, Natalie B. Steinhauser,
Myroslava O. Dzikovska, Johanna D. Moore,
Charles B. Callaway, and Elaine Farrow. 2009. The
DeMAND coding scheme: A ?common language?
for representing and analyzing student discourse. In
Proceedings of 14th International Conference on
Artificial Intelligence in Education (AIED), poster
session, Brighton, UK, July.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn E. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of the SIGDIAL 2009 Conference, pages
38?45, London, UK, September.
Myroslava Dzikovska, Diana Bental, Johanna D.
Moore, Natalie B. Steinhauser, Gwendolyn E.
Campbell, Elaine Farrow, and Charles B. Callaway.
2010a. Intelligent tutoring with natural language
support in the Beetle II system. In Sustaining TEL:
From Innovation to Learning and Practice - 5th Eu-
ropean Conference on Technology Enhanced Learn-
ing, (EC-TEL 2010), Barcelona, Spain, October.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a sys-
tem for tutoring and computational linguistics ex-
perimentation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2010) demo session, Uppsala, Swe-
den, July.
Kate Forbes-Riley and Diane J. Litman. 2006. Mod-
elling user satisfaction and student learning in a
spoken dialogue tutoring system with generic, tu-
toring, and user affect parameters. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT-NAACL
?06), pages 264?271, Stroudsburg, PA, USA.
Kate Forbes-Riley, Diane Litman, Amruta Purandare,
Mihai Rotaru, and Joel Tetreault. 2007. Compar-
ing linguistic features for modeling learning in com-
puter tutoring. In Proceedings of the 2007 confer-
ence on Artificial Intelligence in Education: Build-
ing Technology Rich Learning Contexts That Work,
pages 270?277, Amsterdam, The Netherlands. IOS
Press.
Narendra K. Gupta, Go?khan Tu?r, Dilek Hakkani-Tu?r,
Srinivas Bangalore, Giuseppe Riccardi, and Mazin
Gilbert. 2006. The AT&T spoken language un-
derstanding system. IEEE Transactions on Audio,
Speech & Language Processing, 14(1):213?222.
Pamela W. Jordan, Maxim Makatchev, and Umarani
Pappuswamy. 2006. Understanding complex nat-
ural language explanations in tutorial applications.
In Proceedings of the Third Workshop on Scalable
Natural Language Understanding, ScaNaLU ?06,
pages 17?24.
Lars Bo Larsen. 2003. Issues in the evaluation of spo-
ken dialogue systems using objective and subjective
measures. In Proceedings of the 2003 IEEE Work-
shop on Automatic Speech Recognition and Under-
standing, pages 209?214.
David D. Lewis. 1991. Evaluating text categorization.
In Proceedings of the workshop on Speech and Nat-
ural Language, HLT ?91, pages 312?318, Strouds-
burg, PA, USA.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural lan-
guage processing to analyze tutorial dialogue cor-
pora across domains and modalities. In Proceed-
ings of 14th International Conference on Artificial
Intelligence in Education (AIED), Brighton, UK,
July.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of ACL-08: HLT, pages 46?
54, Columbus, Ohio, June.
Sebastian Mo?ller, Paula Smeele, Heleen Boland, and
Jan Krebber. 2007. Evaluating spoken dialogue
systems according to de-facto standards: A case
study. Computer Speech & Language, 21(1):26 ?
53.
Sebastian Mo?ller, Klaus-Peter Engelbrecht, and
Robert Schleicher. 2008. Predicting the quality and
usability of spoken dialogue services. Speech Com-
munication, pages 730?744.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Mihai Rotaru and Diane J. Litman. 2006. Exploit-
ing discourse structure for spoken dialogue perfor-
mance analysis. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?06, pages 85?93, Strouds-
burg, PA, USA.
Mark Sammons, V.G.Vinod Vydiswaran, and Dan
Roth. 2010. ?Ask not what textual entailment can
do for you...?. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1199?1208, Uppsala, Sweden, July.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards Developing General Mod-
els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
480
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
SemEval-2010 task 12: Parser evaluation using tex-
tual entailments. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
51?56, Uppsala, Sweden, July.
481
Generating Tailored, Comparative
Descriptions with Contextually
Appropriate Intonation
Michael White?
The Ohio State University
Robert A. J. Clark??
University of Edinburgh
Johanna D. Moore?
University of Edinburgh
Generating responses that take user preferences into account requires adaptation at all levels of
the generation process. This article describes a multi-level approach to presenting user-tailored
information in spoken dialogues which brings together for the first time multi-attribute decision
models, strategic content planning, surface realization that incorporates prosody prediction, and
unit selection synthesis that takes the resulting prosodic structure into account. The system
selects the most important options to mention and the attributes that are most relevant to
choosing between them, based on the user model. Multiple options are selected when each offers a
compelling trade-off. To convey these trade-offs, the system employs a novel presentation strategy
which straightforwardly lends itself to the determination of information structure, as well as the
contents of referring expressions. During surface realization, the prosodic structure is derived
from the information structure using Combinatory Categorial Grammar in a way that allows
phrase boundaries to be determined in a flexible, data-driven fashion. This approach to choosing
pitch accents and edge tones is shown to yield prosodic structures with significantly higher
acceptability than baseline prosody prediction models in an expert evaluation. These prosodic
structures are then shown to enable perceptibly more natural synthesis using a unit selection
voice that aims to produce the target tunes, in comparison to two baseline synthetic voices. An
expert evaluation and f0 analysis confirm the superiority of the generator-driven intonation and
its contribution to listeners? ratings.
1. Introduction
In an evaluation of nine spoken dialogue information systems, developed as part of the
DARPA Communicator program, the information presentation phase of the dialogues
? 1712 Neil Ave., Columbus, OH 43210, USA. Web: http://www.ling.ohio-state.edu/~mwhite/.
?? 10 Crichton Street, Edinburgh, Scotland EH8 1AB, UK. Web:
http://www.cstr.ed.ac.uk/ssi/people/robert.html.
? 10 Crichton Street, Edinburgh, Scotland EH8 1AB, UK. Web: http://www.hcrc.ed.ac.uk/~jmoore/.
Submission received: 31 January 2008; revised submission received: 19 May 2009; accepted for publication:
24 September 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 2
Figure 1
Typical information presentation phase of a Communicator dialogue.
was found to be the primary contributor to dialogue duration (Walker, Passonneau,
and Boland 2001). During this phase, the typical system sequentially presents the set of
options that match the user?s constraints, as shown in Figure 1. The user can then navi-
gate through these options and refine them by offering new constraints. When multiple
options are returned, this process can be exacting, leading to reduced user satisfaction.
As Walker et al (2004) observe, having to access the set of available options sequen-
tially makes it hard for the user to remember information relevant to making a decision.
To reduce user memory load, we need alternative strategies for sequential presentation.
In particular, we require better algorithms for:
1. selecting the most relevant subset of options to mention, as well as the
attributes that are most relevant to choosing among them; and
2. determining how to organize and express the descriptions of the selected
options and attributes, in ways that are both easy to understand and
memorable.1
In this article, we describe how we have addressed these points in the FLIGHTS2
system, reviewing and extending the description given in Moore et al (2004). FLIGHTS
follows previous work (Carberry, Chu-Carroll, and Elzer 1999; Carenini and Moore
2000; Walker et al 2002) in applying decision-theoretic models of user preferences to
the generation of tailored descriptions of the most relevant available options. Multi-
attribute decision theory provides a detailed account of howmodels of user preferences
can be used in decision making (Edwards and Barron 1994). Such preference models
have been shown to enable systems to present information in ways that are concise and
1 An issue we do not address in this article is whether a multimodal system would be more effective than a
voice-only one. We believe that these needs, and in particular the need to express information with
contextually appropriate prosody, are also highly relevant for multimodal systems. We also note that
there is still strong demand for voice-oriented systems for eyes-busy use and for the blind.
2 FLIGHTS stands for Fancy Linguistically Informed Generation of Highly Tailored Speech.
160
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
tailored to the user?s interests (Carenini and Moore 2001; Walker et al 2004; Carenini
and Moore 2006). Decision-theoretic models have also been commercially deployed in
Web systems.3
To present multiple options, we introduce a novel strategy where the best option
(with respect to the user model) is presented first, followed by the most compelling
remaining options, in terms of trade-offs between attributes that are important to the
user. (Multiple options are selected only when each offers a compelling trade-off.) An
important property of this strategy is that it naturally lends itself to the determination
of information structure, as well as the contents of referring expressions. Thus, to help
make the trade-offs among the selected options clear to the user, FLIGHTS (1) groups
attributes that are positively and negatively valued for the user, (2) chooses referring ex-
pressions that highlight the salient distinguishing attributes, (3) determines information
structure and prosodic structure that express contrasts intelligibly, and (4) synthesizes
utterances with a unit selection voice that takes the prosodic structure into account.
As such, FLIGHTS goes beyond previous systems in adapting its output according to
user preferences at all levels of the generation process, not just at the levels of content
selection and text planning.
Our approach to generating contextually appropriate intonation follows Prevost
(1995) and Steedman (2000a) in using Combinatory Categorial Grammar (CCG) to
convey the information structure of sentences via pitch accents and edge tones. To
adapt Prevost and Steedman?s approach to FLIGHTS, we operationalize the information
structural notion of theme to correspond to implicit questions that necessarily arise
in presenting the trade-offs among options. We also refine the way in which prosodic
structure is derived from information structure by allowing for a more flexible, one-to-
many mapping between themes or rhemes and intonational phrases, where the final
choice of the type and placement of edge tones is determined by n-gram models. To
investigate the impact of information structural grammatical constraints in our hybrid
rule-based, data-driven approach, we compare realizer outputs with those of baseline
n-gram models, and show that the realizer yields target prosodic structures with signif-
icantly higher acceptability than the baseline models in an expert evaluation.
The prosodic structure derived during surface realization is passed as prosodic
markup to the speech synthesizer. The synthesizer uses this prosodic markup in the
text analysis phase of synthesis in place of the structures that it would otherwise have
to predict from the text. The synthesizer then uses the context provided by the markup
to enforce the selection of suitable units from the database. To verify that the prosodic
markup yields improvements in the quality of synthetic speech, we present an exper-
iment which shows that listeners perceive a unit selection voice that aims to produce
the target prosodic structures as significantly more natural than either of two baseline
unit selection voices that do not use the markup. We also present an expert evaluation
and f0 analysis which confirm the superiority of the generator-driven intonation and its
contribution to listeners? ratings.
The remainder of this article is structured as follows. Section 2 presents our ap-
proach to natural language generation (NLG) in the information presentation phase
of a FLIGHTS dialogue, including how multi-attribute decision models are used in
content selection; how rhetorical and information structure are determined during dis-
course planning; how lexical choice and referring expressions are handled in sentence
planning; how prosodic structures are derived in surface realization; and how these
3 See http://www.cogentex.com/solutions/recommender/index.shtml, for example.
161
Computational Linguistics Volume 36, Number 2
Figure 2
Tailored descriptions of the available flights for three different user models.
prosodic structures compare to those predicted by baseline n-gram models in an expert
evaluation. Section 3 describes how the prosodic structures are used in the unit selection
voice employed in the present study, and compares this voice to the baseline ones
used in our perception experiment. Section 4 provides the methods and results of the
perception experiment itself, along with the expert prosody evaluation and f0 analysis.
Section 5 compares our approach to related work. Finally, Section 6 concludes with a
summary and discussion of remaining issues.
2. NLG in FLIGHTS
2.1 Tailoring Flight Descriptions
To illustrate how decision-theoretic models of user preferences can be used to tailor
descriptions of the available options at many points in the generation process, let us
consider the following three hypothetical users of the FLIGHTS system:
Student (S) A student who cares most about price, all else being equal.
Frequent Flyer (FF) A business traveler who prefers business class, but cares most
about building up frequent-flyer miles on KLM.
Business Class (BC) Another business traveler who prefers KLM, but wants, above all,
to travel in business class.
Suppose that each user is interested in flying from Edinburgh to Brussels on a
certain day, and would like to arrive by five o?clock in the afternoon. FLIGHTS begins
the dialogue by gathering the details necessary to query the database for possible flights.
Next, it uses the preferences encoded in the usermodel to select the highest ranked flight
for each user, as well as those flights that offer interesting trade-offs. These flights are
then described to the user, as shown in Figure 2.4
For the student (S), the BMI flight is rated most highly, because it is a fairly in-
expensive, direct flight that arrives near the desired time. The Ryanair flight is also
4 The set of available flights was obtained by ?screen scraping? data from several online sources. The
hypothetical user preferences were chosen with an eye towards making the example interesting. Actual
user preferences are specified as part of registering to use the FLIGHTS system.
162
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
mentioned as a possibility, as it has the best price; it ends up ranked lower overall than
the BMI flight though, because it requires a connection and arrives well in advance of
the desired arrival time. For the KLM frequent flyer (FF), life is a bit more complicated:
A KLM flight with a good arrival time is offered as the top choice, even though it is a
connecting flight with no availability in business class. As alternatives, the direct flight
on BMI (with no business class availability) and the British Airways flight with seats
available in business class (but requiring a connection) are described. Finally, for the
must-have-business-class traveler (BC), the British Airways flight with business class
available is presented first, despite its requiring a connection; the direct flight on BMI is
offered as another possibility.
Although user preferences have an immediately apparent impact on content se-
lection and ordering, they also have more subtle effects on many aspects of how the
selected content is organized and expressed, as explained subsequently.
Referring expressions: Rather than always referring to the available flights in the same
way, flights of interest are instead described using the attributes most relevant to
the user: for example, direct flight, cheapest flight, KLM flight.
Aggregation: For conciseness, multiple attributes may be given in a single sentence,
subject to the constraint that attributes whose values are positive (or negative)
for the user should be kept together. For example, in There?s a KLM flight arriving
Brussels at four fifty p.m., but business class is not available and you?d need to connect in
Amsterdam, the values of the attributes airline and arrival-time are considered
good, and thus are grouped together to contrast with the values of the attributes
fare-class and number-of-legs, which are considered bad.
Scalar terms: Scalar modifiers like good, as in good price, and just, as in just fifty pounds,
are chosen to characterize an attribute?s value to the user relative to values of the
same attribute for other options.
Discourse cues: Attributes with negative values for the user are acknowledged using
discourse cues, such as but and though. Interesting trade-offs can also be signaled
using cues such as if -conditionals.
Information structure and prosody: Compelling trade-offs are always indicated via
prosodic phrasing and emphasis, as in (There ARE seats in business class)theme (on
the British Airways flight)rheme (that arrives at four twenty p.m.)rheme, where the di-
vision of the sentence into theme and rheme phrases is shown informally using
parentheses, and contrastive emphasis (on ARE) is shown using small caps. (See
Section 2.6.2 for details of how emphasis and phrasing are realized by pitch
accents and edge tones.)
2.2 Architecture
The architecture of the FLIGHTS generator appears in Figure 3. OAA (Martin, Cheyer,
and Moran 1999) serves as a communications hub, with the following agents responsi-
ble for specific tasks: DIPPER (Bos et al 2003) for dialogue management; a Java agent
that implements an additivemulti-attribute value function (AMVF), a decision-theoretic
model of the user?s preferences (Carenini and Moore 2000, 2006), for user modeling;
OPlan (Currie and Tate 1991) for content planning; Xalan XSLT5 and OpenCCG (White
5 http://xml.apache.org/xalan-j/.
163
Computational Linguistics Volume 36, Number 2
Figure 3
FLIGHTS generation architecture.
2004, 2006a, 2006b) for sentence planning and surface realization; and Festival (Taylor,
Black, and Caley 1998) for speech synthesis. The user modeling, content planning, sen-
tence planning, and surface realization agents are described in the ensuing subsections.
FLIGHTS follows a typical pipeline architecture (Reiter and Dale 2000) for NLG.
The NLG subsystem takes as input an abstract communicative goal from the dialogue
manager. In the information presentation phase of the dialogue, this goal is to describe
the available flights that best meet the user?s constraints and preferences. Given a
communicative goal, the content planner selects and arranges the information to convey
by applying the plan operators that implement its presentation strategy. In so doing,
it makes use of three further knowledge sources: the user model, the domain model,
and the dialogue history. Next, the content plan is sent to the sentence planner, which
uses XSLT templates to perform aggregation, lexicalization, and referring expression
generation. The output of sentence planning is a sequence of logical forms (LFs). The
use of LF templates represents a practical and flexible way to deal with the interaction
of decisions made at the sentence planning level, and further blurs the traditional
distinction between template-based and ?real? NLG that van Deemter, Krahmer, and
Theune (2005) have called into question. Each LF is realized as a sentence using a CCG
lexico-grammar (Steedman 2000a, 2000b). Note that in contrast to the generation archi-
tectures of, for example, Pan, McKeown, and Hirschberg (2002) and Walker, Rambow,
and Rogati (2002), the prosodic structure of the sentence is determined as an integral
part of surface realization, rather than in a separate prosody prediction component.
The prosodic structure is passed to the Festival speech synthesizer using Affective
Presentation Markup Language (de Carolis et al 2004; Steedman 2004), or APML, an
XML markup language for the annotation of affect, information structure, and prosody.
Festival uses the Tones and Break Indices (Silverman et al 1992), or ToBI,6 pitch
accents and edge tones?specified as APML annotations?in determining utterance
phrasing and intonation, and employs a custom synthetic voice to produce the system
utterances.
6 See http://www.ling.ohio-state.edu/?tobi/ for an introduction to ToBI and links to on-line
resources.
164
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
2.3 User Modeling
FLIGHTS uses an additive multi-attribute value function (AMVF) to represent the
user?s preferences, as in the GEA real estate recommendation system (Carenini and
Moore 2000, 2006) and the MATCH restaurant recommendation system (Walker et al
2004). Decision-theoretic models of this kind are based on the notion that, if anything
is valued, it is valued for multiple reasons, where the relative importance of different
reasons may vary among users.
The first step is to identify good flights for a particular origin, destination, and
arrival or departure time. The following attributes contribute to this objective: arrival-
time, departure-time, number-of-legs, total-travel-time, price, airline, fare-
class, and layover-airport. As in MATCH, these attributes are arranged into a
one-level tree.
The second step is to define a value function for each attribute. A value function
maps from the features of a flight to a number between 0 and 1, representing the value
of that flight for that attribute, where 0 is the worst and 1 is the best. For example,
the function for total-travel-time computes the difference in minutes between the
flight?s arrival and departure times, and then multiplies the result by a scaling factor to
obtain an evaluation between 0 and 1. The functions for the airline, layover-airport,
and fare-class attributes make use of user-specified preferred or dispreferred values
for that attribute. In the current version of these functions, a preferred value is given a
score of 0.8, a dispreferred value 0.2, and all other values 0.5.7
The structure and weights of the user model represent a user?s dispositional biases
about flight selection. Situational features are incorporated in two ways. The requested
origin and destination are used as a filter when selecting the set of available options
by querying the database. In contrast, the requested arrival or departure time?if
specified?is used in the corresponding attribute?s evaluation function to give a higher
score to flights that are closer to the specified time. If an arrival or departure time is not
specified, the corresponding attribute is disabled in the user model.
As in previous work, the overall evaluation of an option is computed as the
weighted sum of its evaluation on each attribute. That is, if f represents the option being
evaluated,N is the total number of attributes, and wi and vi are, respectively, the weight
and the value for attribute i, then the evaluation v( f ) of option f is computed as follows:
v( f ) =
N
?
i=1
wivi( f )
To create a user model for a specific user, two types of information are required. The
user must rank the attributes in order of importance, and he or she must also specify
any preferred or dispreferred attribute values for the airline, layover-airport, and
fare-class attributes. In FLIGHTS, we also allow users to specify a partial ordering of
the rankings, so that several attributes can be given equal importance when registering
to use the system. Figure 4 shows the user models for the student (S), frequent-flyer (FF),
and business-class (BC) users discussed earlier; because no departure time is specified
in the sample query, departure-time is not included in these examples.
7 In informal experiments, we did not find the system to be particularly sensitive to the exact values used
in the attribute functions when selecting content.
165
Computational Linguistics Volume 36, Number 2
Figure 4
Sample user models.
Based on the user?s ranking of the attributes, weights are assigned to each attribute.
As in previous work, we use Rank Order Centroid (ROC) weights (Edwards and Barron
1994). This allows weights to be assigned based on rankings, guaranteeing that the sum
will be 1. The nth ROC weight wRn of N total weights is computed as follows:
wRn =
1
N
N
?
i=n
1
i
We extend these initial weights to the partial-ordering case as follows. If attributes
i . . . j all have the same ranking, then the weight of each will be the mean of the relevant
ROC weights; that is,
(
j
?
k=i
wRk )/( j? i+ 1)
As a concrete example, if there is a single highest-ranked attribute followed by a three-
way tie for second, then w1 = w
R
1 , and w2 = w3 = w4 =
1
3 (w
R
2 + w
R
3 + w
R
4 ).
2.4 Content Planning
2.4.1 Content Selection. Once a specific user model has been created, the AMVF can be
used to select a set of flights to describe for that user, and to determine the features of
those flights that should be included in the descriptions. We use a novel strategy that
combines features of the Compare and Recommend strategies of Walker et al (2004),
refining them with an enhanced method of selecting options to mention. In brief, the
idea behind the strategy is to select the top-ranked flight, along with any other highly
ranked flights that offer a compelling trade-off?that is, a better value (for the user)
on one of its attributes. As we shall see in Section 2.4.2, by guaranteeing that any
option beyond the first one offers such a trade-off, our strategy lends itself naturally
to the determination of information structure and the contents of referring expressions
identifying the option. By contrast, Walker et al?s Recommend strategy only presents
a single option, and their Compare strategy does not present options in a ranked order
that facilitates making trade-offs salient. In addition, we may observe that with our
strategy, the user model need only contain a rough approximation of the user?s true
166
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 5
Algorithm for selecting the options to describe.
preferences in order for it to do its job of helping to identify good flights for the user
to consider.8 A similar observation underlies the candidate/critique model of Linden,
Hanks, and Lesh?s (1997) Web-based system.
Selecting the Options to Describe. In determining whether an option is worth mention-
ing, we make use of two measures. Firstly, we use the z-score of each option; this mea-
sures how far the evaluation v( f ) of an option f is from the mean evaluation. Formally,
it is defined using the mean (?V) and standard deviation (?V) of all evaluations, as
follows:
z( f ) = (v( f )? ?V )/?V
We also make use of the compellingnessmeasure described by Carenini andMoore
(2000, 2006), who provide a formal definition. Informally, the compellingness of an
attribute measures its strength in contributing to the overall difference between the
evaluation of two options, all other things being equal. For options f, g, and threshold
value kc, we define the set comp( f, g, kc) as the set of attributes that have a higher score
for f than for g, and for which the compellingness is above kc.
The set Sel of options to describe is constructed as follows. First, we include the top-
ranked option. Next, for all of the other options whose z-score is above a threshold kz,
we check whether there is an attribute of that option that offers a compelling trade-off
over the already selected options; if so, we add that option to the set.9 This algorithm is
presented in Figure 5.
For the BC user model, for example, this algorithm proceeds as follows. First, it
selects the top-ranked flight: a connecting flight on British Airways with availability
in business class. The next-highest-ranked flight is a morning flight, which does not
have any attributes that are compellingly better than those of the top choice, and is
therefore skipped. However, the third option presents an interesting trade-off: even
though business class is not available, it is a direct flight, so it is also included. None
of the other options above the threshold present any interesting trade-offs, so only those
two flights are included.
8 Of course, if the user model could be relied upon to contain perfect information, the system could always
just recommend a single best flight; however, because we do not expect our models to capture a user?s
preferences perfectly, we have designed the system to let the user weigh the alternatives when there
appear to be interesting trade-offs among the available options.
9 The requirement that an option offer a compelling trade-off is similar to the exclusion of dominated
solutions in Linden, Hanks, and Lesh (1997).
167
Computational Linguistics Volume 36, Number 2
The selected flights for the other sample user models show similar trade-offs, as
mentioned in the discussion of Figure 2. For FF, the selected flights are a connecting,
economy-class flight on the preferred airline; a direct, economy-class flight on a neutral
airline; and a connecting, business-class flight on a neutral airline. For S, the top choices
are a reasonably cheap direct flight that arrives near the desired time, and an even
cheaper, connecting flight that arrives much earlier in the day.
Selecting the Attributes to Include. When selecting the attributes to include in the de-
scription, we make use of an additional measure, s-compellingness. Informally, the
s-compellingness of an attribute represents the contribution of that attribute to the
evaluation of a single option; again, the formal definition is given by Carenini and
Moore (2000, 2006). Note that an attribute may be s-compelling in either a positive or a
negative way. For an option f and threshold kc, we define the set s-comp( f, kc) as the set
of attributes whose s-compellingness for f is greater than kc.
The set Atts of attributes is constructed in two steps. First, we add the most com-
pelling attributes of the top choice. Next, we add all attributes that represent a trade-off
between any two of the selected options; that is, attributes that are compellingly better
for one option than for another. The algorithm appears in Figure 6.
For the BC user model, the s-compelling attributes of the top choice are arrival-
time and fare-class; the latter is also a compelling advantage of this flight over the
second option. The advantage of the second option over the first is that it is direct, so
number-of-legs is also included. A similar process on the other user models results
in price, arrival-time, and number-of-legs being selected for S, and arrival-time,
fare-class, airline, and number-of-legs for FF.
2.4.2 Planning Texts with Information Structure. Based on the information returned by the
content selection process, together with further information from the user model and
the current dialogue context, the content planning agent develops a plan for presenting
the available options. A distinguishing feature of the resulting content plans is that
they contain specifications of the information structure of sentences (Steedman 2000a),
including sentence theme (roughly, the topic the sentence addresses) and sentence
rheme (roughly, the new contribution on a topic).
Steedman (2000a) characterizes the notions of theme and rheme more formally by
stating that a theme presupposes a rheme alternative set, in the sense of Rooth (1992),
while a rheme restricts this set. Because Steedman does not fully formalize the discourse
update semantics of sentence themes, we have chosen to operationalize themes in
FLIGHTS in one particular way, namely, as corresponding to implicit questions that
arise in the context of describing the available options. Our reasoning is as follows.
First, we note that a set of alternative answers corresponds formally to the meaning of a
question. Next, we observe that whenever a flight option presents a compelling trade-off
Figure 6
Algorithm for selecting the attributes to include.
168
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
for a particular attribute, it at least partially addresses the question of whether there are
any flights that have a desirable value for that attribute; moreover, whenever a flight
is presented that has a less-than-optimal value for an attribute, its mention implicitly
raises the question of whether any flights are available with a better value for that
attribute. Finally, we conclude that by specifying and realizing content as thematic, the
system can help the user understand why a flight option is being presented, since the
theme (by virtue of its presupposition) identifies what implicit question?that is, what
trade-off?the option is addressing.
In all, the content planner?s presentation strategy performs the following functions:
 marking the status of items as definite/indefinite and predications as
theme/rheme for information structure;
 determining contrast between options and attributes, or between groups of
options and attributes;
 grouping and ordering of similar options and attributes (e.g., presenting the
top scoring option first vs. last);
 choosing the contents of referring expressions (e.g., referring to a particular
option by airline); and
 decomposing strategies into basic dialogue acts and hierarchically organized
rhetorical speech acts.
The presentation strategy is specified via a small set (about 40) of content planning
operators. These operators present the selected flights as an ordered sequence of op-
tions, starting with the best one. Each flight is suggested and then further described. As
part of suggesting a flight, it is identified by its most compelling attribute, according to
the user model. (Recall that any selected flight options beyond the highest ranked one
must offer a compelling trade-off.) Flights are additionally identified by their airline,
which we deemed sufficiently significant to warrant special treatment (otherwise the
strategy is domain-independent).
The information for the first flight is presented as all rheme, as no direct link is
made to the preceding query. Each subsequent, alternative flight is presented with its
most compelling attribute in the theme, and the remaining attributes in the rheme. For
instance, consider the student example (S) in Figure 2. After presenting the BMI flight?
which has a good price, is direct, and arrives near the desired time?the question arises
whether there are any cheaper alternatives. Because the second option has price as its
most compelling attribute?and given that it is the least expensive flight available?it is
identified as the CHEAPEST flight, with this phrase forming the theme of the utterance.
As another illustration, consider the business-class traveler example (BC) in Figure 2.
After presenting the British Airways flight, which has availability in business class
but is not direct, the question arises whether there are any direct flights available. The
presentation of the second option addresses this implicit question, introducing the BMI
flight with the theme phrase There is a DIRECT flight.
The output of the content planner is derived from the hierarchical goal structure
produced during planning. Figure 7 shows the resulting content plan for the student
example. Note that, as part of suggesting the second flight option in the sequence (f2),
subgoals are introduced that identify the option by informing the user that the option
has type flight and that the option has the attribute cheapest, where this attribute is
the most compelling one for the user. Both subgoals are marked as part of the theme
169
Computational Linguistics Volume 36, Number 2
Figure 7
Content plan for student example (S).
(rheme marking is the default); the subgoal for the option type is also marked for
definiteness (indefinite is the default), as the cheapest attribute uniquely identifies the
flight. The remaining information for the second flight is presented in terms of a contrast
between its positive and negative attributes, as determined by the user model.
The way in which our presentation strategy uses theme phrases to connect alterna-
tive flight suggestions to implicit questions is related to Prevost?s (1995) use of theme
phrases to link system answers to immediately preceding user questions. It is also
170
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 8
Semantic dependency graph produced by the sentence planner for (The CHEAPEST flight)theme (is
on RYANAIR)rheme.
related to Kruijff-Korbayova? et al?s (2003) use of theme phrases to link utterances with
questions under discussion (Ginzburg 1996; Roberts 1996) in an information-state based
dialogue system. An interesting challenge that remains for future work is to determine
to what extent our presentation strategy can be generalized to handle theme/rheme
partitioning, both for explicit questions across turns as well for implicit questions within
turns.
2.5 Sentence Planning
The sentence planning agent uses the Xalan XSLT processor to transform the output of
the content planner into a sequence of LFs that can be realized by the OpenCCG agent.
It is intended to be a relatively straightforward component, as the content planner has
been designed to implement the most important high-level generation choices. Its pri-
mary responsibility is to lexicalize the basic speech acts in the content plan?which may
appear in referring expressions?alongwith the rhetorical speech acts that connect them
together. When alternative lexicalizations are available, all possibilities are included in
a packed structure (Foster and White 2004; White 2006a). The sentence planner is also
responsible for adding discourse markers such as also and but, adding pronouns, and
choosing sentence boundaries. It additionally implements a small number of rhetorical
restructuring operations for enhanced fluency.
The sentence planner makes use of approximately 50 XSLT templates to recursively
transform content plans into logical forms. An example logical form that results from
applying these templates to the content plan shown in Figure 7 appears in Figure 8 (with
alternative lexicalizations suppressed). As described further in Section 2.6.1, the logical
forms produced by the sentence planner are semantic dependency structures,10 which
make use of an info feature to encode theme/rheme partitioning, and a kon feature to
implement Steedman?s (2006) notion of kontrast (Vallduv?? and Vilkuna 1998). Following
Steedman, kontrast is assigned to the interpretations of words which contribute to
distinguishing the theme or rheme of the utterance from other alternatives that the
context makes available.
In order to trigger the inclusion of context-sensitive discourse markers such as also
and either, the sentence planner compares the inform acts for consecutive flight options
10 The nodes of the graph are typically labelled by lexical predicates. An exception here is the has-rel
predicate, which allows the predicative complement on Ryanair to introduce the ?AIRLINE? role in a way
that is similar to the dependency structure for the Ryanair flight.
171
Computational Linguistics Volume 36, Number 2
to see whether acts with the same type have the same value.11 These same checks can
also trigger de-accenting. For example, when two consecutive flights have no seats in
business class, the second one can be described using the phrase it has NO AVAILABILITY
in business class either, where business class has been de-accented.
The development of the XSLT templates wasmade considerably easier by the ability
to invoke OpenCCG to parse a target sentence and then use the resulting logical form
as the basis of a template. (See Section 3 for a description of how the target sentences in
the FLIGHTS voice script were developed.) Using LF templates, rather than templates
at the string level, makes it simpler to uniformly handle discourse markers such as
also and either, which have different preferred positions within a clause, depending in
part on which verb they modify. LF templates also simplify the treatment of subject?
verb agreement. Additionally, by employing LF templates with a single theme/rheme
partition, it becomes possible to underspecify whether the theme and rheme will be
realized by one intonational phrase each or by multiple phrases (see Section 2.6.2). At
the same time, certain aspects of the LF templates can be left alone, when there is no
need for further analysis and generalization.
As noted earlier, the use of LF templates further blurs the traditional distinction
between template-based and ?real? NLG that vanDeemter, Krahmer, and Theune (2005)
have called into question. In the case of referring expressions especially, LF templates in
FLIGHTS represent a practical and flexible way to deal with the interaction of decisions
made at the sentence planning level, as the speech acts identifying flight options are
considered together with the other basic and rhetorical speech acts in the applicability
conditions for the templates that structure clauses. In this way, options can be identified
not only in definite NPs, such as the CHEAPEST flight, but also in there-existentials and
conditionals, such as there is a DIRECT flight on BMI that . . . or if you prefer to fly DIRECT,
there?s a BMI flight that . . . . We may further observe that the traditional approach to gen-
erating referring expressions (Reiter and Dale 2000), where a distinguishing description
for an entity is constructed during sentence planning without regard to a user model,
would not fit in our architecture, where the user model drives the selection of a referring
expression?s content at the content planning level.
Although the use of LF templates in XSLT represents a practical approach to
handling sentence planning tasks that were not the focus of our research, it is not
one that promotes reuse, and thus it is worth noting which aspects of our sentence
planner would pose challenges for a more declarative and general treatment. The bulk
of the templates concern domain-specific lexicalization and are straightforward; given
the way these were developed from the results of OpenCCG parsing, it is conceiv-
able that this process could be largely automated from example input?output pairs.
The templates for adding pronouns and discourse markers require more expertise
but remain reasonably straightforward; the templates for rhetorical restructuring and
choosing sentence boundaries, in contrast, are fairly intricate. In principle, satisfactory
results might be obtained using a more general set of options for handling pronouns,
discourse markers, and sentence boundaries, together with an overgenerate-and-rank
methodology; we leave this possibility as a topic for future research.
2.6 Surface Realization
2.6.1 Chart Realization with OpenCCG. For surface realization, we use the OpenCCG open
source realizer (White 2004, 2006a, 2006b). A distinguishing feature of OpenCCG is that
11 In future work, these checks could be extended to apply across turns as well.
172
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
it implements a hybrid symbolic-statistical chart realization algorithm that combines
(1) a theoretically grounded approach to syntax and semantic composition, with (2) the
use of integrated language models for making choices among the options left open by
the grammar. In so doing, it brings together the symbolic chart realization (Kay 1996;
Shemtov 1997; Carroll et al 1999; Moore 2002) and statistical realization (Knight and
Hatzivassiloglou 1995; Langkilde 2000; Bangalore and Rambow 2000; Langkilde-Geary
2002; Oh and Rudnicky 2002; Ratnaparkhi 2002) traditions. Another recent approach to
combining these traditions appears in Carroll and Oepen (2005), where parse selection
techniques are incorporated into an HPSG realizer.
Like other realizers, the OpenCCG realizer is partially responsible for determining
word order and inflection. For example, the realizer determines that also should prefer-
ably follow the verb in There is also a very cheap flight on Air France, whereas in other cases
it typically precedes the verb, as in I also have a flight that leaves London at 3:45 p.m. It also
enforces subject?verb agreement, for example, between is and flight, or between are and
seats. Less typically, in FLIGHTS and in the COMIC12 system, the OpenCCG realizer
additionally determines the prosodic structure, in terms of the type and placement of
pitch accents and edge tones, based on the information structure of its input logical
forms. Although OpenCCG?s algorithmic details have been described in the works cited
above, details of how prosodic structure can be determined from information structure
in OpenCCG appear for the first time in this article.
The grammars used in the FLIGHTS and COMIC systems have been manually
written with the aim of achieving very high quality. However, to streamline grammar
development, the grammar has been allowed to overgenerate in areas where rules are
difficult to write and where n-gram models can be reliable; in particular, it does not
sufficiently constrain modifier order, which in the case of adverb placement especially
can lead to a large number of possible orderings. Additionally, it allows for a one-to-
many mapping from themes or rhemes to edge tones, yielding many variants that differ
only in boundary type or placement. As will be explained subsequently, we consider
this more flexible, data-driven approach to phrasing to be better suited to the needs of
generation than would be a more direct implementation of Steedman?s (2000a) theory,
which would require all phrasing choices to be made at the sentence planning level.
We have built a language model for the system from the FLIGHTS speech corpus
described in Section 3.1. To enhance generalization, named entities and scalar adjectives
have been replacedwith the names of their semantic classes (such as TIME, DATE, CITY,
AIRLINE, etc.), as is often done in limited domain systems. Note that in the corpus and
the model, pitch accents are treated as integral parts of words, whereas edge tones and
punctuation marks appear as separate words. The language model is a 4-gram back-off
model built with the SRI language modeling toolkit (Stolcke 2002), keeping all 1-counts
and using Ristad?s (1995) natural discounting law for smoothing. OpenCCG has its own
implementation for run-time scoring; in FLIGHTS, the model additionally incorporates
an a/an-filter, which assigns a score of zero to sequences containing a followed by a
vowel, or an followed by a consonant, subject to exceptions culled from bigram counts.
2.6.2 Deriving Prosody. CCG is a unification-based categorial framework that is both
linguistically and computationally attractive. We provide here a brief overview of CCG;
an extensive introduction appears in Steedman (2000b).
12 http://www.hcrc.ed.ac.uk/comic/.
173
Computational Linguistics Volume 36, Number 2
Figure 9
A simple CCG derivation.
Figure 10
A CCG derivation with non-standard constituents.
A grammar in CCG is defined almost entirely in terms of the entries in the lexicon,
which are (possibly complex) categories bearing standard feature information (such as
verb form, agreement, etc.) and subcategorization information. CCG has a small set of
rules which can be used to combine categories in derivations. The two most basic rules
are forward (>) and backward (<) function application. These rules are illustrated in
Figure 9, which shows the derivation of a simple copular sentence (with no prosodic
information).13 In the figure, the noun and proper name receive atomic categories with
labels n and np, respectively. The remaining words receive functional categories, such as
sdcl\np/(sadj\np) for the verb is; this category seeks a predicative adjective (sadj\np) to its
right and an np to its left, and returns the category sdcl, for a declarative sentence. Note
that dcl and adj are values for the form feature; other features, such as those for number
and case, have been suppressed in the figure (as has the feature label, form).
CCG also employs further rules based on the composition (B), type raising (T),
and substitution (S) combinators of combinatory logic. These rules add an element
of associativity to the grammar, making possible multiple derivations with the same
semantics. They are crucial for building the ?non-standard? constituents that are the
hallmark of categorial grammars, and which are essential for CCG?s handling of coor-
dination, extraction, intonation, and other phenomena. For example, Figure 10 shows
how the category for there can be type-raised and composed with the category for is
in order to derive the constituent there is a direct flight (sdcl/(sadj\np)), which cuts across
the VP in traditional syntax. Because there is a direct flight corresponds to the theme
phrase, and on BMI to the rheme phrase, in the first clause of the second sentence in
13 These derivations are shown from the parsing perspective, starting with an ordered sequence of words at
the top. During realization, it is the semantics rather than the word sequence which is given; the word
sequence is determined during the search for a derivation that covers the input semantics. See the
discussion of Figure 11 (later in this section) for a discussion of the semantic representations used in
OpenCCG.
174
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 11
A derivation of theme and rheme phrases.
the business-class (BC) example in Figure 2, the derivation in Figure 10 also shows how
CCG?s flexible notion of constituency allows the intonation structure and information
structure to coincide, where the intonation coincides with surface structure, and the
information structure is built compositionally from the constituent analysis.
In Steedman (2000a), theme and rheme tunes are characterized by distinct patterns
of pitch accents and edge tones. The notation for pitch accents and edge tones is taken
from Pierrehumbert (1980) and ToBI (Silverman et al 1992). We have implemented a re-
duced version of Steedman?s theory in OpenCCG, where theme tunes generally consist
of one or more L+H* pitch accents followed by a L-H% compound edge tone at the end
of the phrase,14 and rheme tunes consist of one or more H* pitch accents followed by
a L- or L-L% boundary at phrase end. Additionally, yes/no questions typically receive
a H-H% final boundary, and L-H% boundaries are often used as continuation rises to
mark the end of a non-final conjoined phrase.
The key implementation idea in Steedman?s (2000a) approach is to use features on
the syntactic categories to enforce information structural phrasing constraints?that is,
to ensure that the intonational phrases are consistent with the theme/rheme partition
in the semantics. For example, if there is a direct flight corresponds to the theme and on
BMI the rheme, then the syntactic features ensure that the intonation brackets the clause
as (there is a direct flight) (on BMI), rather than (there) (is a direct flight on BMI) or (there
is) (a direct flight on BMI), etc. To illustrate, Figure 11 shows, again from the parsing
perspective, how theme and rheme phrases are derived in OpenCCG for the subject
NP and VP of Figure 9, respectively. (To save space, pitch accents and edge tones will
henceforth be written using subscripts and string elements, rather than appearing on a
separate tonal tier.) In the figure, the category for cheapest has the info feature on each
14 To reduce ambiguity in the grammar, unmarked themes?that is, constituents which appear to be
thematic but which are not grouped intonationally into a separate phrase?are assumed to be
incorporated as background parts of the rheme, as suggested in Calhoun et al (2005).
175
Computational Linguistics Volume 36, Number 2
atomic category set to th(eme), and Ryanair has its info feature set to rh(eme), due to
the presence of the L+H* and H* accents, respectively. The remaining words have no
accents, and thus their categories have a variable (M, for EME) as the value of the info
feature on each atomic category.15 All the words also have a variable (O) as the value of
the owner feature, discussed subsequently, on each atomic category. As words combine
into phrases, the info variables serve to propagate the theme or rheme status of a
phrase; thus, the phrase the cheapestL+H? flight has category npth,O, whereas is on RyanairH?
ends up with category sdcl,rh,O\nprh,O. Because these two phrases have incompatible info
values, they cannot combine before being ?promoted? to intonational phrases by their
respective edge tones. In this way, the constraint that phrasing choices respect the
theme/rheme partition is enforced.
The edge tones allow complete intermediate or intonational phrases to combine by
changing the value of the info feature to phr(ase) in the result category.16 Note that
the argument categories of the edge tones do not select for a particular value of the
info feature; instead, L-H% has sh$1 as its argument category, whereas L-L% has ss$1,
where h(earer) and s(peaker) are the respective values of the owner feature.17 Combination
with an edge tone unifies the owner features throughout the phrase. In a prototypical
theme phrase, the hearer is the owner, whereas in a rheme phrase, the speaker is the
owner.
Like other compositional grammatical frameworks, CCG allows logical forms to
be built in parallel with the derivational process. Traditionally, the ?-calculus has been
used to express semantic interpretations, but OpenCCG instead makes use of a more
flexible representational framework, Hybrid Logic Dependency Semantics (Baldridge
and Kruijff 2002; Kruijff 2003), or HLDS. In HLDS, hybrid logic (Blackburn 2000) terms
are used to describe semantic dependency graphs, such as the one seen earlier in
Figure 8. As discussed in White (2006b), HLDS is well suited to the realization task, in
that it enables an approach to semantic construction that ensures semantic monotonicity,
simplifies equality tests, and avoids copying in coordinate constructions.
To illustrate, four lexical items from Figure 11 appear in Example (1). The three
words are derived by a lexical rule which adds pitch accents and information structure
features to the base forms. The format of the entries is lexeme  category, where the
category is itself a pair in the format syntax : logical form. The logical form is a conjunction
of elementary predications (EPs), which come in three varieties: lexical predications,
such as @Ebe; semantic features, such as @E?TENSE?pres; and dependency relations,
such as @E?ARG?X.
(1) a. cheapestL+H?  nX,th,O/nX,th,O : @X?HASPROP?P ?@Pcheapest ?
@P?INFO?th ?@P?OWNER?O ?@P?KON?+
b. RyanairH?  npX,rh,O : @XRyanair ?
@X?INFO?rh ?@X?OWNER?O ?@X?KON?+
15 In practice, variables likeM are replaced with ?fresh? variables (such asM 1) during lexical lookup, so
that theM variables are distinct for different words.
16 The $ variables range over a (possibly empty) stack of arguments, allowing the edge tones to employ a
single s$1\s$1 category to combine with s/(s\np), s\np, and so on. As indicated in the figure, the new
value of the info feature (phr) is automatically distributed throughout the arguments of the result
category.
17 See Steedman (2000a) for a discussion of this notion of ?ownership,? or responsibility.
176
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
c. is  sE,dcl,M,O\npX,M,O/(sP,adj,M,O\npX,M,O) : @Ebe ?
@E?TENSE?pres ?@E?ARG?X ?@E?PROP?P ?
@E?INFO?M ?@E?OWNER?O ?@E?KON??
d. L-H%  sphr$1\sh$1
In these entries, the indices (or nominals, in hybrid logic terms) in the logical forms,
such as X, P, and E, correspond to nodes in the semantic graph structure, and are
linked to the syntactic categories via the index feature. Similarly, the syntactic features
info and owner have associated ?INFO? and ?OWNER? features in the semantics. As
discussed previously, in derivations the values of the info and owner features are
propagated throughout an intonational phrase, which has the effect of propagating the
values of the ?INFO? and ?OWNER? semantic features to every node in the dependency
graph corresponding to the phrase. In this way, a distributed representation of the
theme/rheme partition is encoded, in a fashion reminiscent of Kruijff?s (2003) approach
to representing topic?focus articulation using hybrid logic. By contrast, the ?KON? fea-
ture (cf. Section 2.5) is a purely local one, and thus appears only in the semantics. Note
that because the edge tones do not add any elementary predications, one or more edge
tones?and thus one or more intonational phrases?may be used to derive the same
theme or rheme in the logical form.
To simplify the semantic representations the sentence planner must produce, Open-
CCG includes default rules that (where applicable) propagate the value of the ?INFO?
feature to subtrees in the logical form, set the ?OWNER? feature to its prototypical value,
and set the value of the ?KON? feature to false. When applied to the logical form for
the semantic dependency graph in Figure 8, the rules yield the HLDS term in Exam-
ple (2). After this logical form is flattened to a conjunction of EPs, lexical instantiation
looks up relevant lexical entries, such as those in Example (1), and instantiates the
variables to match those in the EPs. The chart-based search for complete realizations
proceeds from these instantiated lexical entries.
(2) @e(be ? ?TENSE?pres ? ?INFO?rh ? ?OWNER?s ? ?KON?? ?
?ARG?( f ? flight) ? ?DET?the ? ?NUM?sg ? ?INFO?th ? ?OWNER?h ? ?KON?? ?
?HASPROP?(c ? cheapest ? ?INFO?th ? ?OWNER?h ? ?KON?+)) ?
?PROP?(p ? has-rel ? ?INFO?rh ? ?OWNER?s ? ?KON?? ?
?OF?f ?
?AIRLINE?(r ? Ryanair ? ?INFO?rh ? ?OWNER?s ? ?KON?+)))
Given our focus in FLIGHTS on using intonation to express contrasts intelligibly,
we have chosen to employ hard constraints in the OpenCCG grammar on the choice
of pitch accents and on the use of edge tones to separate theme and rheme phrases.
However, the grammar only partially constrains the type of edge tone (as explained
previously), and allows the theme and rheme to be expressed by one or more intona-
tional phrases each; consequently, the final choice of the type and placement of edge
tones is determined by the n-gram model. To illustrate, consider Example (3), which
shows how the frequent flyer sentence seen in Figure 2 is divided into four intonational
phrases. Other possibilities (among many) allowed by the grammar include leaving out
the L-L% boundary between flight and arriving, which would yield a phrase that?s likely
to be perceived as too long, or adding a L- or L-L% boundary between there?s and a,
which would yield two unnecessarily short phrases.
(3) There?s a KLMH? flight L-L% arriving BrusselsH? at fourH? fiftyH? p.m.H? L-L%,
but businessH? classH? is notH? availableH? L-H% and you?d need to connectH? in
AmsterdamH? L-L%.
177
Computational Linguistics Volume 36, Number 2
To allow for this flexible mapping between themes and rhemes and one or more into-
national phrases, we take advantage of our distributed approach to representing the
theme/rheme partition, where edge tones mark the ends of intonational phrases with-
out introducing their own elementary predications. As an alternative, we could have
associated a theme or rheme predication with the edge tones, which would be more
in line with Steedman?s (2000a) approach. However, doing so would make it necessary
to include one such predication per phrase in the logical forms, thereby anticipating
the desired number of output theme and rheme phrases in the realizer?s input. Given
that the naturalness of intonational phrasing decisions can depend on surface features
like phrase length, we consider the distributed approach to representing theme/rheme
status to be better suited to the needs of generation.
2.7 Comparison to Baseline Prosody Prediction Models
As noted in Section 2.2, spoken language dialogue systems often include a separate
prosody prediction component (Pan,McKeown, andHirschberg 2002;Walker, Rambow,
and Rogati 2002), rather than determining prosodic structure as an integral part of
surface realization, as we do here. Although it is beyond the scope of this article to
compare our approach to a full-blown, machine learning?based prosody prediction
model, we do present in this section an expert evaluation that shows that our approach
outperforms strong baseline n-gram models. In particular, we show that the informa-
tion structural constraints in the grammar play an important role in producing target
prosodic boundaries, and that these boundary choices are preferred to those made by
an n-gram model in isolation.
According to Pan, McKeown, and Hirschberg (2002, page 472), word-based n-gram
models can be surpringly good: ?The word itself also proves to be a good predictor for
both accent and break index prediction. . . . Since the performance of this [word] model
is the best among all the [single-feature] accent prediction models investigated, it seems
to suggest that for a CTS [Concept-to-Speech] application created for a specific domain,
features like word can be quite effective in prosody prediction.? Indeed, although their
best accent prediction model exceeded the word-based one, the difference did not reach
statistical significance (page 485). Additionally, word predictability, measured by the
log probability of bigrams and trigrams, was found to significantly correlate with pitch
accent decisions (pages 482?483), and contributed to their best machine-learned models
for accent presence and boundary presence.18
As baseline n-gram models, we trained 1- to 4-gram models for predicting accents
and boundaries using the FLIGHTS speech corpus (Section 3.1), the same corpus used to
train the realizer?s language model. The baseline n-gram models are factored language
models (Bilmes and Kirchhoff 2003), with words, accents, and boundaries as factors.
The accent models have accent as the child variable and 1?4 words as parent variables,
startingwith the current word, and including up to three previous words. The boundary
models are analogous, with boundary as the child variable. With these models, each
maximum likelihood prediction of pitch accent or edge tone is independent of all other
choices, so there is no need to perform a best-path search. The majority baseline predicts
no accent and no edge tone for each word.
18 Note that in our setting, it would be impossible to exactly replicate Pan et al?s models, in which syntactic
boundaries play an important role, as CCG does not have a rigid notion of syntactic boundary.
178
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Table 1
Baseline pitch accent and boundary prediction accuracy against target tunes.
Accuracy
Majority 1-gram 2-gram 3-gram 4-gram N
Accent Presence 73.3% 98.0% 98.6% 97.4% 97.4% 344
Accent Type 73.3% 96.5% 97.4% 96.2% 96.8% 344
Boundary Presence 81.1% 91.9% 94.5% 95.4% 95.1% 344
Boundary Type 81.1% 91.0% 92.7% 93.0% 93.9% 344
We tested the realizer and the baseline n-gram models on the 31 sentences used to
synthesize the stimuli in the perception experiment described in Section 4 (see Figure 13
for an example mini-dialogue). None of the test sentences appear verbatim in the
FLIGHTS speech corpus. The test sentences contain target prosodic structures intended
to be appropriate for the discourse context. Given these structures, we can quantify
the accuracy with which the realizer is able to reproduce the pitch accent and edge
tone choices in the target sentences, and compare it to the accuracy with which n-gram
models predict these choices using maximum likelihood. Note that the target prosodic
structures may not represent the only natural choices, motivating the need for the expert
evaluation described further subsequently.
Although the realizer is capable of generating the target prosodic structure of each
test sentence exactly, the test sentence (with its target prosody) is not always the top-
ranked realization of the corresponding logical form, which may differ in word order
or choice of function words. Thus, to compare the realizer?s choices against the target
accents and boundaries, we generated n-best lists of realizations that included the target
realization, and compared this realization to others in the n-best list with the same
words in the same order (ignoring pitch accents and edge tones). In each case, the target
realization was ranked higher than all other realizations with the same word sequence,
and so we may conclude that the realizer reproduces the target accent and boundary
choices in the test sentences with 100% accuracy.
The accuracy with which the baseline n-gram models reproduce the target tunes
is shown in Table 1. As the test sentences are very similar to those in the FLIGHTS
speech corpus, the accent model performs remarkably well, with the bigram model
reproducing the exact accent type (including no accent) in 97.4% of the cases, and
agreeing on the choice of whether to accent the word at all in 98.6% of the cases. The
boundary model also performs well, though substantially worse than the realizer, with
the 4-gram model reproducing the boundary type (including no boundary) in 93.9% of
the cases, and agreeing on boundary presence in 95.1% of the cases.19
Inspired byMarsi?s (2004) work on evaluating optionality in prosody prediction, we
asked an expert ToBI annotator, who was unfamiliar with the experimental hypotheses
under investigation, to indicate for each test sentence the range of contextually appro-
priate tunes by providing all the pitch accents and edge tones that would be acceptable
19 Because the boundary models sometimes failed to include a boundary before a comma or full stop,
default L-L% boundaries were added in these cases.
179
Computational Linguistics Volume 36, Number 2
Table 2
Examples comparing target tunes to baseline prosody prediction models, with expert corrections
(Items 07-2 and 06-1).
(a) target the only directL+H? flight L-H% leaves at 5:10H? L-L% .
edits none
n-grams the only directH? flight leaves at 5:10H? L-L% .
edits the only directL+H? flight L- leaves at 5:10H? L-L% .
(b) target there ?s a directH? flight on British AirwaysH? with a goodH? price L-L% .
edits there ?s a directH? flight on British AirwaysH? L- with a goodH? price L-L% .
n-grams there ?s a directH? flight on British AirwaysH? L-L% with a goodL+H?
price L-L% .
edits none
for each word.20 However, our annotator found this task to be too difficult, in part
because of the difficulty of coming up with all possible acceptable tunes, and in part
because of dependencies between the choices made for each word. For this reason,
we instead chose to follow the approach taken with the Human Translation Error
Rate (HTER) post-edit metric in MT (Snover et al 2006), and asked our annotator to
indicate, for the target tune and the n-gram baseline tune, which accents and boundaries
would need to change in order to yield a tune appropriate for the context. For the
n-gram baseline tune, we used the choices of the bigram accent model and the 4-gram
boundary model.
Examples of how the target tunes (and realizer choices) compare to those of the
baseline accent and boundary prediction models appear in Table 2, along with the
corrections provided by our expert ToBI annotator.21 In Table 2(a), the target tune,
which contains the theme phrase the only directL+H? flight L-H%, was considered fully
acceptable. By contrast, with the tune of the n-gram models, the H* accent on directwas
not considered acceptable for the context, and at least a minor phrase boundary was
considered necessary to delimit the theme phrase. In Table 2(b), we have an example
where the target tunewas not considered fully acceptable: Although the target consisted
of a single, all-rheme intonational phrase, with no intermediate phrases, our annotator
indicated that at least a minor phrase break was necessary after British Airways. The
n-gram models assigned a major phrase break at this point, which was also considered
acceptable. Note also that the n-gram models had a L+H* accent on good, in contrast to
the target tune?s H*, but both choices were considered acceptable.
Table 3 shows the number of accent and boundary corrections for all 31 test sen-
tences at different levels of specificity. Overall, there were just 10 accent or boundary
corrections for the target tunes, versus 24 for those of the baseline models, out of 688
total accent and boundary choices, a significant difference (p = 0.01, Fisher?s Exact Test
[FET], 1-tailed). With the accents, there were fewer corrections for the target tunes,
but not many in either case, and the difference was not significant. Of course, with a
20 We thank one of the anonymous reviewers for this suggestion. Note that in Marsi?s study, having one
annotator indicate optionality was found to be a reasonable approximation of deriving optionality from
multiple annotations.
21 During realization, multiwords are treated as single words, such as British Airways and 5:10 (five ten a.m.).
Accents on multiwords are distributed to the individual words before the output is sent to the speech
synthesizer.
180
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Table 3
Number of prosodic corrections of different types in all utterances and theme utterances for the
target tunes and the ones selected by n-gram models. Items in bold are significantly different at
p = 0.05 or less by a one-tailed Fisher?s Exact Test.
All Utts Theme Utts
Target n-grams Target n-grams
Total corrections 10 24 3 11
Accents 4 7 2 5
Presence 3 4 2 3
Boundaries 6 17 1 6
Presence 2 13 0 4
Major 0 9 0 3
larger sample size, or with test sentences that are less similar to those in the corpus,
a significant difference could arise. With the boundaries, out of 344 choices, the target
tunes had six corrections, only two of which involved the presence of a boundary, and
none of which involved a missing major boundary; by contrast, the n-gram baseline
had 17, 13, and 9 such corrections, respectively, a significant difference in each case
(p = 0.02, p = 0.003, p = 0.002, respectively, FET). In the subset of 12 sentences involving
theme phrases, where the intonation is more marked than in the all-rheme sentences,
the target tunes again had significantly fewer corrections overall (3 vs. 11 corrections
out of 220 total choices; p = 0.03, FET), and the difference in boundary and boundary
presence corrections approached significance (p = 0.06 in each case, FET).
Having shown that the information structural constraints in the grammar play an
important role in producing target realizations with contextually appropriate prosodic
structures?in particular, in making acceptable boundary choices, where the choice is
not deterministically rule-governed?we now briefly demonstrate that the realizer?s
n-grammodel (see Section 2.6.1) has an important role to play as well. Table 4 compares
the realizer?s output on the test sentences against its output with the language model
disabled, in which case an arbitrary choice is effectively made among those outputs
allowed by the grammar. Not surprisingly, given that the grammar has been allowed
to overgenerate, the realizer produces far more exact matches and far higher BLEU
(Papineni et al 2001) scores with its language model than without. Looking at the dif-
ferences between the realizer?s highest scoring outputs and the target realizations, the
differences largely appear to represent cases of acceptable variation. The most frequent
difference concerns whether an auxiliary or copular verb is contracted or not, where
either choice seems reasonable. Most of the other differences represent minor variations
in word order, such as directH? Air FranceH? flight vs. directH? flight on Air FranceH?. By
Table 4
Impact of language model on realizer choice.
Exact Match BLEU
Realizer 61.3% 0.9505
No LM 0.0% 0.6293
181
Computational Linguistics Volume 36, Number 2
contrast, many of the outputs chosen with no language model scoring contain unde-
sirable variation in word order or phrasing; for example: Air FranceH? directH? flight
instead of directH? flight on Air FranceH?, and you L- though would need L- to connectH? in
AmsterdamH? L- instead of you?d need to connectH? in AmsterdamH? though L-L%.
2.8 Interim Summary
In this section, we have introduced a presentation strategy for highlighting the most
compelling trade-offs for the user that straightforwardly lends itself to the determina-
tion of information structure, which then drives the choice of prosodic structure during
surface realization with CCG. We have also shown how phrase boundaries can be
determined in a flexible, data-driven fashion, while respecting the constraints imposed
by the grammar. An expert evaluation demonstrated that the approach yields prosodic
structures with significantly higher acceptability than strong n-gram baseline prosody
prediction models. In the next two sections, we show how the generator-driven prosody
can be used to produce perceptibly more natural synthetic speech.
3. Unit Selection Synthesis with Prosodic Markup
In this section we describe the unit selection voices that we employed in our perception
experiment (Section 4). Three voices in total were used in the evaluation: GEN, ALL,
and APML. Each was a state-of-the-art unit selection voice using the Festival Multisyn
speech synthesis engine (Clark, Richmond, and King 2007). The GEN voice, used as
a baseline, is a general-purpose unit selection voice. The ALL voice is a voice built
using the same data as the GEN voice but with the addition of the data from the
FLIGHTS speech corpus described in Section 3.1. The APML voice augments the in-
domain data in the ALL voice with prosodic markup, which is then used at run-time by
the synthesizer in conjunction with marked-up input to guide the selection of units.
To provide in-domain data for the ALL and APML voices, we needed to record
a suitable data set from the original speaker used for the GEN voice. We describe the
process of constructing this speech corpus for FLIGHTS next, in Section 3.1. Then, in
Section 3.2, we describe the unit selection voices in detail, alongwith how the in-domain
data was used in building them.
3.1 The FLIGHTS Speech Corpus
The FLIGHTS speech corpus was intended to have a version of each word that needs to
be spoken by the system, recorded in the context in which it will be spoken. This context
can be thought of as a three-word window centered around the given word, together
with the word?s target pitch accent and edge tone, if any. This would theoretically
provide more than sufficient speech data for a full limited domain voice, and a voice
using this data would be guaranteed to have a unit sequence for any sentence generated
by FLIGHTS where each individual unit is a tight context match for the target unit.
Because the system was still limited in its generation capabilities at the time of
recording the voice data for FLIGHTS, we developed a recording script by combining
the generated data that was available with additional utterances that we anticipated
the finished system would generate. To do so, we assembled a set of around 50 utter-
ance templates that describe flight availability?with slots to be filled by times, dates,
amounts, airlines, airports, cities, and flight details?to which we added approximately
two hundred individual or single-slot utterances, which account for the introductions,
182
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
questions, confirmations, and other responses that the systemmakes. We thenmade use
of an algorithm designed by Baker (2003) to iterate through the filler combinations for
each utterance template to provide a suitable recording script.22
To illustrate, Example (4) shows an example utterance template, along with two
utterances in the recording script created from this template. The example demonstrates
that having multiple slots in a template makes it possible to reduce the number of sen-
tences that need to be recorded to cover all possible combinations of fillers. With (b) and
(c) recorded as illustrated to fill the template in (a), we would then have the recorded
data to mix and match the slot fillers with two different values each to synthesize eight
different sentences. It is also often possible to use the fillers for a slot in one utterance
as the fillers for a slot in another utterance, if the phrase structure is sufficiently similar.
A more complex case is shown in Example (5), involving adjacent slots. In this case, it
is not sufficient to just record one example of each possible filler, as the context around
that filler is changed by the presence of other slots; instead, combinations of fillers must
be considered (Baker 2003).
(4) a. It arrives at ?TIME?H? L-H% and costs just ?AMOUNT?H? L-L%, but it requires
a connectionH? in ?CITY?H? L-L%.
b. It arrives at sixH? p.m.H? L-H% and costs just fiftyH? poundsH? L-L%, but it
requires a connectionH? in ParisH? L-L%.
c. It arrives at nineH? a.m.H? L-H% and costs just eightyH? poundsH? L-L%, but it
requires a connectionH? in PisaH? L-L%.
(5) a. There are ?NUM?H? ?MOD?H? flights L-L% from ?CITY?H? to ?CITY?H?
today L-L%.
b. There are twoH? earlierH? flights L-L% from BordeauxH? to AmsterdamH?
today L-L%.
The recording script presented similar utterances in blocks. It made use of small
caps to indicate intended word-level emphasis and punctuation to partially indicate
desired phrasing, as the speaker was not familiar with ToBI annotation. The intended
dialogue context for the utterances was discussed with our speaker, but the recordings
did not consist of complete dialogues, as using complete dialogues would have been too
time consuming. The recording took place during two afternoon sessions and yielded
approximately two hours of recorded speech. The resulting speech database consisted
of 1,237 utterances, the bulk of which was derived from the utterances that describe
flight availability.
The recordings were automatically split into sentence-length utterances, each of
which had a generated APML file with the pitch accents and edge tones predicted for
the utterance. The prosodic structures were based on intuition, as there was no corpus
of human?human dialogues sufficiently similar to what the system produces that could
have been used to inform prosodic choice.
The recordings were then manually checked against the predicted APML, by listen-
ing to the audio and looking at the f0 contours to see if the pitch accents and edge
tones matched the predicted ones. In the interest of consistency, changes were only
22 The target prosody for each utterance template was based on the developers? intuitions, with input from
Mark Steedman. As one of the anonymous reviewers observed, an alternative approach might have been
to conduct an elicitation study in a Wizard-of-Oz setup to determine target tunes. This strategy may have
yielded more natural prosodic variation, but perhaps at the expense of employing less distinctive tunes.
For this project, it was not practical to take the extra time required to conduct such an elicitation study.
183
Computational Linguistics Volume 36, Number 2
made to the APML files when there were clear cases of the speaker diverging from the
predicted phrasing or emphasis; nevertheless, the changes did involve mismatches in
both the presence and the type of the accents and boundaries. As an example of the kind
of changes that were made, in Example (4), we had predicted that the speaker would
use a L-H% boundary (a continuation rise) prior to the word but, however she instead
consistently used a low boundary tone in this location. Consequently, we changed all
the APML files for sentences with this pattern to include a L-L% compound edge tone
at that point. Further details of this data cleanup process are given in Rocha (2004).
3.2 The Synthetic Voices
Ideally, we would like to build a general purpose unit selection voice where we can
fully specify the intonation in terms of ToBI accents and boundaries and then have the
synthesizer generate this for us. This, however, is currently not a fully viable option for
the reasons discussed subsequently, so instead we built a number of different voices
using the unit selection technique to investigate how working towards a voice with
full intonation control would function. There are a number of ways in which prosodic
generation for unit selection can be approached (Aylett 2005; van Santen et al 2005;
Clark and King 2006), with most systems designed to produce best general prosody.
This contrasts with the framework chosen here, which is designed to make maximum
use of in-domain data, in an attempt to produce a system that realizes prosody as well
as is possible in that domain. This can be considered an upper bound for what more
general systems could achieve under optimal conditions.
The ideal way to use intonation within the unit selection framework requires three
components: the database of speech, which must be fully annotated for intonation;
the predicted intonation for a target being synthesized; and a target cost that ensures
suitable candidates are found to match the target. Each of these requirements prove to
be difficult to achieve successfully.
Manually labeling a large speech database accurately and consistently with intona-
tion labels is both difficult and expensive, whereas automatic labeling techniques tend
to produce mediocre results at best. Producing accent labeling for the flight information
script is somewhat easier because of the limited number of templates that the script is
based upon, and once the templates are labeled, intonation for individual sentences can
be derived. To build a general purpose unit selection voice for the FLIGHTS domain,
we would want to combine the FLIGHTS data with speech data designed to provide
more general coverage. Providing accent labeling for the extra, non-FLIGHTS, data is
the main problem here.
Predicting intonation for a target utterance is generally a difficult task, and can
only really be done well when the domain of the speech synthesis is constrained. For
example, a number of statistical modeling techniques may be able to provide adequate
accent prediction for a system to read the news or forecast the weather, because the sen-
tence structure is usually limited to a sequence of simple statements. The task becomes
difficult when the sentence structure is more variable, for example in dialogue where a
number of different question forms may exist along with contrastive statements, and so
forth. In the FLIGHTS domain we can side-step the prediction problem by providing a
specification for the intonation of a sentence as part of language generation.
In one sense, defining a target-cost component to direct the search towards finding
suitable intonation is not difficult, as a simple penalty for not matching the target
intonation suffices. However, standard unit selection techniques only ever take into
184
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
account local effects, and no provision is made to ensure a suitable global intonation
contour.
These problems, and a number of technical issues relating to the use of markup,
automatic segment alignment, and the voice building process, prohibit us from building
a general purpose unit selection voice where we can specify the required intonation.
One of the questions that this study is attempting to answer is whether it is worth trying
to resolve these issues to build such a system in the future. To address this question
we present a number of voices designed to investigate the issues of producing natural
speech synthesis in the FLIGHTS domain.
3.2.1 The GEN Voice. The GEN voice uses a database of approximately 2,000 sentences
of read newspaper speech. The Festival Multisyn unit selection engine selects diphone
units from the database by minimizing a combination of a target cost and a join cost.
The target cost scores the linguistic context of the diphone in terms of stress, position
of the diphone in the current word, syllable and phrase, phonetic context, and part of
speech. The join cost scores the continuity between selected units in terms of spectrum,
energy, and f0. There is no explicit modeling of prosody in this system.
The GEN voice was used as a baseline. There is nothing specific to the FLIGHTS
domain associated with any part of this voice, and the quality of the resulting synthesis
is comparable with a typical unit selection speech synthesiser.
3.2.2 The ALL voice. The ALL voice was created by augmenting the GEN voice with
the speech data recorded as part of the FLIGHTS speech corpus described herein. The
motivation here is to attempt to provide a system which would have the best possible
quality that a general purpose unit selection synthesiser could have when working in
this domain. The additional data increases the availability of units in the exact contexts
that would be required by the FLIGHTS system. Having examples of airport and airline
names in the database, for example, increases the likelihood that when these words
are synthesized there are appropriate, often consecutive, units available to synthesize
them. Naturalness is improved both by the better context match and by the need for
fewer joins in constructing these words and the utterances they are used in.
3.2.3 The APML voice. The APML voice is different from the ALL voice in that it is
designed to take APML input from the FLIGHTS system rather than text input. The
APML voice comprises the same speech data as the ALL voice, but also includes
the APML annotation for the FLIGHTS part of the corpora. The target cost for the
synthesizer is augmented with a prosodic component which penalizes any mismatch
between supplied APML input and the APML specification associated with a particular
unit. As the read newspaper component of this voice does not have accompanying
APML markup, and because the voice is required to work for text input (although this
capability is not used here), the target cost penalizes (1) the synthesis of an APML-
specified target with a unit that does not have accompanying APML markup; (2) the
synthesis of a target without APML specification with APML-specified units, and (3)
synthesis where there is an APML mismatch between the target and candidate. It is
important that these prosodic mismatches are only discouraged, rather than forbidden,
to allow the system to synthesize out of the original domain if required.
As work on the FLIGHTS system progressed, we found numerous cases where the
speech corpus failed to anticipate all the possible outputs of the system. For example,
although we included an utterance template for conveying price in a conjoined verb
phrase, as in Example (4), we did not include a template for conveying price in a
185
Computational Linguistics Volume 36, Number 2
single independent clause, as in It costs just fifty pounds. Had the system been further
along in its development when we recorded the speech data, we could have pursued a
strategy of selecting utterances to record from generated outputs, in order to maximize
some coverage criterion.23 In either case, though, we consider it difficult to develop
a recording script that will completely cover all three-word sequences that a system
will ever generate, especially if further development is considered. For this reason, we
believe it to be essential to have a strategy to handle the cases where generation needs
go beyond the original plan. The use of an augmented general purpose unit selection
system?rather than, for example, a strictly limited domain system?means that there
is no difficulty in synthesizing extra material as needed, although there is the risk that it
will not sound quite as good as that which is closer in context to the original in-domain
recordings. For an APML voice where the input is ?new? APML, if there are no suitable
units within the APML-annotated section of the corpus, units will be chosen from the
main portion of the corpus. There will be a penalty in terms of target cost for doing so,
but the best sequence will still be found.
4. Synthesis Evaluation
In this section, we describe a perceptual experiment which was carried out to deter-
mine whether the prosodic structures generated by the FLlGHTS system actually result
in improved naturalness in speech synthesis. We also describe an evaluation of the
prosody in the synthesized speech that makes use of an expert annotator?s assessments
of the acceptability of the perceived tunes for the given context, and an evaluation that
examines objective differences in the f0 contours of the theme phrases in the synthesized
speech. The three types of evaluation (subject ratings, expert annotation, and objective
measures) all show the APML voice outperforming the ALL voice, both on the complete
set of test utterances as well as on the subset containing theme phrases. Additionally, the
three types of evaluation support each other in that differences in ratings of particular
items correspond to differences in acceptability annotations and to differences in the
objective measures, as will be explained in the following.
4.1 Perception Experiment
4.1.1 Methodology. Our experimental hypothesis was that listeners would prefer the
APML voice, used with contextually appropriate intonation markup, over the ALL and
GEN control voices. We further hypothesized that the preference would be larger for
the utterances containing theme phrases, where the intonation is more marked than it
is in the all-rheme utterances.
Subjects were presented with mini-dialogues consisting of a summary of the user?s
request in text and three versions of the system?s response, one for each of the three
voices, as shown in Figure 12. System utterances were presented side-by-side, with
each system turn comprising two to four utterances, and each voice labeled as A, B,
or C. The label assignments were balanced across the mini-dialogues so that each voice
appeared an equal number of times labeled as A, B, or C, and the mini-dialogues were
presented in an individually randomized order. Subjects were asked to assign ratings
to each version of each utterance on a 1?7 scale, with 7 corresponding to ?completely
natural? and 1 corresponding to ?completely unnatural.? Ratings were gathered on-line
23 For example, in developing a custom voice for the COMIC system, we selected from generated utterances
in order to maximize bigram coverage with high priority named entities.
186
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 12
Screenshot of webexp2 interface for gathering listener ratings.
using webexp2.24 Subjects were allowed to play the sound files any number of times in
any order, but were required to assign ratings to all the utterances before proceeding to
the next screen. In assigning their ratings, subjects were instructed to pay attention to
the context given by the summary of the user?s request, keeping the following questions
in mind:
 Does the utterance make it clear how well the flight (or flights) in question
meet the user?s needs?
 Are words emphasised in a way that highlights the trade-offs among the
different options?
 For the second and subsequent utterances, is emphasis used in a way that
makes sense given the previous system utterances?
 Is the utterance clear and easy to understand, or garbled and difficult to
understand?
Twelve mini-dialogues were used as stimuli, comprising 31 utterances in total. The
dialogues were constructed so as to contain a representative range of theme phrases,
with each mini-dialogue containing one utterance with a theme phrase. An example
dialogue, with target tunes for the APML voice, appears in Figure 13; the second system
utterance contains a theme phrase. The complete set of stimuli, including sound files, is
available on-line25 from the first author?s web page.
Fourteen native English speaking subjects participated in the study. The subjects
were all from the UK or USA and had no known hearing deficits. For participating in
the study, subjects were entered into a prize drawing.
24 http://www.webexp.info/.
25 http://www.ling.ohio-state.edu/?mwhite/flights-stimuli/.
187
Computational Linguistics Volume 36, Number 2
Figure 13
Example mini-dialogue (Dialogue 03).
4.1.2 Results. As shown in Figure 14, the APML voice received higher ratings than the
ALL voice, and both the APML and ALL voices scored much higher than the GEN
voice. Overall, the APML voice?s average rating surpassed that of the ALL voice by
0.77; its score of 5.83 was close to 6 on the rating scale, corresponding to ?mostly
natural,? while the ALL voice?s score was 5.06, just above 5 on the scale, corresponding
to ?somewhat natural.? The difference between the two voices was highly significant
(paired t-test, t433 = 10.20, p < 0.001). On the theme utterances, the difference between
the APML and ALL voices was even larger, at 0.91. With the APML voice, there was
no significant difference between the average ratings of the theme utterances and those
without theme phrases. In contrast, the ALL voice showed a trend towards the theme
utterances scoring worse than the remaining ones (t-test, 1-tailed, t432 = ?1.39, p =
0.08), with the average of the theme utterances 0.19 lower than that of the all-rheme
utterances. The GEN voice did considerably worse (0.48) on the theme utterances
(t-test, 1-tailed, t432 = ?3.06, p = 0.001).
4.2 Expert Prosody Evaluation
4.2.1 Methodology. To more directly examine whether the APML voice yielded more
contextually appropriate prosody than the ALL voice, we asked an expert ToBI anno-
tator, who was unfamiliar with the experimental hypotheses under investigation, to
annotate the perceived tunes in the synthesized stimuli for these two voices. In cases
of uncertainty, multiple annotations were given. The stimuli from the ALL voice were
always presented first, so that listening to the tune from the APML item would not bias
our annotator against the corresponding ALL item. Because there is not necessarily a
unique tune that is acceptable for the context, we also asked our annotator to indicate
the closest acceptable tune by indicating which accents and boundaries would need to
Figure 14
Mean ratings for each voice. Theme utterances are the subset containing a theme phrase. (Error
bars show standard errors.)
188
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 15
Example annotation and f0 showing tune corrections (Item 05-2).
change in order to yield a tune appropriate for the context, in much the same fashion
as in our prosody prediction evaluation (Section 2.7).26 We then counted the number of
accent or boundary corrections at different levels of specificity.
An example annotation with tune corrections appears in Figure 15. This utterance
is the second one in Dialogue 05, where the user prefers to fly BMI; accordingly, the
utterance contains the theme phrase (with target tune) the only BMIL+H? flight L-H%.
With the APML version of the utterance, the annotator perceived a L+H* accent on BMI
and a L-H% boundary on de-accented flight, as desired. Additionally, the annotator
perceived a H* accent on only, which was not part of the target tune. The tune was
nevertheless considered completely acceptable, as the Edits tier is identical to the Tones
tier. By contrast, with the ALL version of the utterance, BMI was less prominent than
only and flight, and accordingly it received no pitch accent, whereas only and flight
received L+H* and H* accents, respectively; in addition, a H- boundary was annotated
on only, and a boundary that was uncertain between L- and L-L% was annotated on
26 The target tunes were not presented together with the synthesized stimuli, to avoid influencing the tunes
perceived by our expert.
189
Computational Linguistics Volume 36, Number 2
Table 5
Number of prosodic corrections of different types in all utterances and theme utterances for the
two voices. Items in bold are significantly different at p = 0.05 or less by a one-tailed Fisher?s
Exact Test.
All Utts Theme Utts
APML ALL APML ALL
Total corrections 22 49 11 26
Accents 10 33 4 16
Presence 6 20 2 8
L+H* 4 13 2 8
Boundaries 12 16 7 10
Presence 7 6 3 4
flight. This tune for the theme phrase was not considered acceptable for the context: As
the Edits tier shows, the lack of an accent on BMI, and the presence of an accent on
flight, was corrected by our annotator, as was the H- minor phrase boundary on only.
This choice makes sense for the context, as BMI is what distinguishes this option from
the one suggested in the first utterance, while flight is given information at this point in
the dialogue. Indeed, it is difficult to come up with an interpretation of onlyL+H? BMI,
with no accent on BMI, though this tune might make sense if the question of whether
the flight was code-shared with another airline was a salient one in the context.
4.2.2 Results. The results of the expert prosody evaluation appear in Table 5. Across all 31
utterances, there were 49 accent or boundary corrections for the ALL voice, versus just
22 for the APML voice, out of 688 total accent and boundary choices, a highly significant
difference (p < 0.001, Fisher?s Exact Test, 1-tailed). With the 12 theme utterances, there
were 26 corrections for the ALL voice, versus 11 for the APML voice, out of 220 total
choices (p = 0.008, FET). Looking at the pitch accents, the difference in the number
of accent corrections was significant in each case (p < 0.001, FET, and p = 0.004, FET,
respectively), as was the number of corrections involving the presence or absence of a
pitch accent (p = 0.004, FET, and p = 0.05, FET, respectively) and the number involving
L+H* accents (p = 0.02, FET, and p = 0.05, FET, respectively).27 Looking at the bound-
aries, we may observe that with the ALL voice, the majority of corrections were with
accents, whereas with the APML voice, the majority were with boundaries. Although
the APML voice had fewer boundary corrections, the difference was not significant.
As noted in the discussion of Figure 15, the perceived tunes with the APML voice
did not always exactly match the target tunes, sometimes in ways that our expert
annotator considered acceptable. More commonly, however, mismatches between the
perceived and target tunes were not judged to be acceptable. In fact, of the 22 correc-
tions for the APML voice, 19 would have been acceptable had the target tune been
successfully achieved; that is, 19 of the 22 corrections changed a perceived accent or
boundary to the one in the target tune. In 12 of these 19 cases, our annotator perceived
an accent or boundary where the target tune had none. There were also five other cases
27 A correction was counted as involving the presence or absence of a pitch accent if the word was
perceived as having an accent when it was deemed that it should have none, or was perceived as having
no accent when it was deemed that it should have one; L+H* accent and boundary presence corrections
were counted in the same fashion.
190
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 16
Mean ratings of items with 0?4 prosodic corrections for the APML and ALL voices combined.
(Error bars show standard errors.)
where the target was a L-L% boundary, but a rising boundary was perceived instead.
The remaining cases involved accent type mismatches.
To examine the relationship between the number of corrections indicated by our
expert ToBI annotator and the ratings in the perception experiment, we grouped the
APML and ALL utterances by the number of corrections and calculated the mean
ratings for each group. The results appear in Figure 16, which shows the expected
relationship between mean ratings and the number of corrections, with ratings go-
ing down as the number of corrections go up. The pattern is less clear with two to
four corrections, where there are fewer tokens. One-tailed t-tests show that utterances
with zero corrections were rated significantly higher than utterances with one to four
corrections (t39 = 2.96, t35 = 5.14, t30 = 3.03, t28 = 5.35, respectively; p < 0.01 in each
case); utterances with one correction were also significantly higher than those with four
corrections (t17 = 2.30, p = 0.02).
We also examined the relationship between the listener ratings and the number
of errors noted by our expert annotator through a multiple regression analysis. The
regressors were the number of accent or boundary presence errors and the number of
remaining accent or boundary errors, involving only a difference in type. The regression
equation was Rating = 5.85? 0.41? PresenceErrors? 0.28? TypeOnlyErrors, which ac-
counted for 32% of the variance and was highly significant (F2,59 = 15.3, p < 0.001). As
expected, ratings were negatively related to accent and boundary errors, with presence
errors showing a greater impact than type-only errors. Both the effect of presence errors
(t59 = ?4.34, p < 0.001) and the effect of type-only errors (t59 = ?2.65, p = 0.01) were
significant. Because both kinds of errors had a significant impact on ratings, the results
suggest that it is worthwhile to make use of fine-grained ToBI categories where feasible,
as we discuss further in Section 6.
4.3 Objective Measures
4.3.1 Methodology. In addition to the expert prosody evaluation, we also measured the
f0 values and durations of the adjectives and nouns in the theme phrases, to see whether
they differed significantly between the APML and ALL voices. We also measured the
drop in f0 between the adjective, which should receive a contrastive pitch accent, and
the noun, which should be deaccented. Note that the f0 drop is a potentially more fine-
grained measure than pitch accent corrections, because the accents could be considered
acceptable even though the tune is less distinct in some cases.
An example in which this occurs appears in Figure 17, which shows the second
utterance of Dialogue 03, seen earlier in Figure 13, for the APML and ALL voices. Here,
191
Computational Linguistics Volume 36, Number 2
Figure 17
Example annotation and f0 showing difference in f0 drop in the theme phrases (Item 03-2).
the APML version was annotated with the target accents, namely L+H* for Lufthansa
and none for flight, whereas the ALL version was annotated with a L+H* !H* pattern,
whichwas also considered acceptable. However, with the APML version the pitch drops
from an f0 value of 293 Hz to 177 Hz, whereas with the ALL version the pitch only drops
from 260 Hz to 209 Hz. As a result, the theme tune is much less distinctive in the ALL
version, in a way that could impact the ease with which the theme phrase?s discourse
function is identified.
4.3.2 Results. Figure 18 shows the mean f0 values across all 12 theme phrases for the
adjective, which should receive contrastive emphasis, and the head noun, which should
be reduced. As the figure shows, the pattern seen in Figure 17 was borne out on the
whole in the stimuli with theme phrases. In particular, the theme phrases for the APML
voice had an f0 drop of 90 Hz on average from the adjective to the noun, whereas the
ALL voice only had an f0 drop of 50 Hz, a significant difference (t-test, 1-tailed, t22 =
2.60, p = 0.008). The durations of the adjectives and nouns, however, did not show a
significant difference. In addition to the f0 drop, a significant difference was also found
with the f0 max on the adjective and the f0 min on the noun (t-tests, 1-tailed, t28 = 1.70,
p = 0.05 and t22 = ?1.86, p = 0.04, respectively). Finally, a relatively high correlation (r =
0.78) was also found between the difference in f0 drop and the difference in the ratings
of the theme utterances for the two voices (t10 = 3.94, p = 0.001), suggesting that the less
192
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
distinctive theme tunes produced by the ALL voice were perceived as less natural than
the ones produced by the APML voice.
4.4 Discussion
The perception experiment confirmed our hypothesis that listeners would prefer the
APML voice, used with contextually appropriate intonation markup, over the ALL
and GEN control voices. Both on the complete set of utterances, as well as the subset
containing theme phrases, the APML voice was rated substantially higher on average
than the ALL voice, and much higher than the GEN voice. Note that the ALL voice was
also rated much higher on average than the GEN voice?which is not surprising, given
that it has access to the same limited domain data as the APML voice?showing that
the ALL voice serves as a tough baseline to beat. The expert prosody evaluation and the
objective measures also confirmed the superiority of the APML voice, in particular on
the theme utterances.
We also found evidence in favor of our hypothesis that the preference for the APML
voice would be larger for the utterances containing theme phrases?where the intona-
tion is more marked than it is in the all-rheme utterances?as the difference between
the mean ratings of the APML and ALL voices was larger on the theme utterances than
on the remaining ones. Additionally, with the APML voice, there was no significant
difference between the mean ratings of the theme utterances and those without theme
phrases, whereas the ALL voice showed a trend towards the theme utterances scoring
worse than the all-rheme ones, and the GEN voice clearly did considerably worse on
the theme utterances. One small surprise was that with the ALL voice, the difference
between the mean ratings of the theme and all-rheme utterances did not reach the
standard level of statistical significance. Of course, it could be that with a larger sample
size, a more significant difference would be found. However, it is undoubtedly the case
that the ALL voice dropped off less on the theme utterances than did the GEN voice,
and the reason is almost certainly that the limited domain data has good coverage of
the theme phrases, and thus the ALL voice often does reasonably well on the theme
utterances even without explicit prosodic control. What is perhaps more remarkable
is that the ALL voice did not do better on the all-rheme utterances, as can be seen in
the number of expert corrections listed in Table 5 for all the utterances, which go well
beyond those in the theme utterance subset. That the ALL voice had more than double
Figure 18
Mean f0 values for emphasized and reduced words in theme phrases for the APML and ALL
voices. (Error bars show standard errors.)
193
Computational Linguistics Volume 36, Number 2
the number of corrections as the APML voice on both the complete set of utterances
and the subset containing theme phrases shows that the prosodic specifications were
important throughout.
Finally, we may observe that although the APML voice had fewer boundary correc-
tions than did the ALL voice, the difference did not reach significance, suggesting that
there is room for improvement in how boundaries are handled in the APML voice. In
particular, this result suggests that edge tones, and intermediate phrase boundaries in
particular, should affect the selection of units non-locally, as theoretically their effect on
the pitch contour spreads back to the last pitch accent. In fact, it maywell be that because
the speech synthesis system only models prosodic effects locally, essentially at the
syllable level, and does not take utterance-level structures such as tune into account, a
ceiling has been reached for both accent and phrasing performance. Representing global
prosodic structures to ensure prosodic coherence will be one of the major challenges for
future generations of speech synthesis systems.
5. Related Work
The FLIGHTS system combines and extends earlier approaches to user-tailored genera-
tion in spoken dialogue. A distinguishing feature of FLIGHTS is that it adapts its output
according to user preferences at all levels of the generation process, from the selection
of content to linguistic realization and the prosody targeted in speech synthesis.
Themost similar system to ours is MATCH (Walker et al 2004), which employs sim-
pler content planning strategies and does not explicitly point out the trade-offs among
options. MATCH also uses simple templates for realization, and does not attempt to
control intonation. Carenini and Moore?s (2000, 2006) system is also closely related, but
it does not make comparisons, and generates text rather than speech. Carberry et al?s
(1999) system likewise employs additive decision models in recommending courses,
though their focus is on dynamically acquiring amodel of the student?s preferences, and
the system is limited to recommending a single option considered better than the user?s
current one. In addition, the system only addresses the problem of selecting positive
attributes to justify the recommendation, and does not plan and prosodically realize the
positive and negative attributes of multiple suggested options.
These systems all employ a user model to select a small set of good options, and to
identify the attributes that justify their desirability, in order to present a summary, com-
parison, or recommendation to the user. Evaluation showed that tailoring recommen-
dations and comparisons to the user increased argument effectiveness and improved
user satisfaction (Walker et al 2004). Thus, the user-model (UM-) based approach is an
appropriate strategy for spoken dialogue systems when there are a small number of
options to present, either because the number of options is limited or because users can
supply sufficient constraints to winnow down a large set before querying the database
of options.
Other researchers have argued that it is important to allow users to browse the data
for a number of reasons: (1) if there are many options that share attribute values, they
will be very close in score when ranked using the UM-based approach; (2) users may
not be able to provide constraints until they hear more information about the space of
options; and (3) the UM-based approach does not give users an overview of the option
space, and this may reduce their confidence that they have been told about the best
option(s) (Demberg and Moore 2006).
Polifroni, Chung, and Seneff (2003) proposed a ?summarize and refine? (SR) ap-
proach, in which the system structures a large number of options into a small number
194
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
of clusters that share attributes. The system then summarizes the clusters based on their
attributes, implicitly prompting the user to provide additional constraints. The system
produces summaries such as
I have found 983 restaurants. Most of them are located in Boston and Cambridge. There are 32
choices for cuisine. I also have information about price range.
which help the user get an overview of the option space. Chung (2004) extended this
approach by proposing a constraint relaxation strategy for coping with queries that
are too restrictive to be satisfied by any option. Pon-Barry, Weng, and Varges (2006)
found that fewer dialogue turns were necessary when the system proactively suggested
refinements and relaxations.
However, as argued in Demberg and Moore (2006), there are several limitations
to the SR approach. First, many turns may be required during the refinement process.
Second, if there is no optimal solution, exploration of trade-offs is difficult. Finally, the
attributes on which the data has been clustered may be irrelevant for the specific user.
Demberg and Moore (2006) subsequently developed the user-model-based summarize
and refine approach (UMSR) to combine the benefits of the UM and SR approaches, by
integrating user modeling with automated clustering. When there are more than a small
number of relevant options, the UMSR approach builds a cluster-based tree structure
which orders the options to allow for stepwise refinement. The effectiveness of the tree
structure, which directs the dialogue flow, is optimized by taking the user?s preferences
into account. Trade-offs between alternative options are presented explicitly to give
the user a better overview of the option space and lead the user to a more informed
choice. To give the user confidence that they are being presented with all relevant
options, a brief account of the remaining (irrelevant) options is also provided. Results of
a laboratory experiment comparing the SR andUMSR approaches demonstrated that (1)
participants preferredUMSR, (2) UMSR presentations are as easy to understand as those
of SR, (3) UMSR increases overall user satisfaction, (4) UMSR significantly improves the
user?s overview of the available options, and (5) UMSR increases users? confidence in
having heard about all relevant options. Although the UMSR approach has not been
implemented in the FLIGHTS system, it could be used when there are a large number
of available options to winnow them down to a handful of relevant ones, which would
then be compared following the approach described in this article.
As regards our work on intonation, as stated in the introduction, Prevost?s (1995)
generator has directly informed our approach to information structure and prosody; his
system does not make use of quantitative user models though, and only describes single
options. Theune (2002) likewise follows Prevost?s approach in her system, refining the
way contrast is determined in assigning pitch accents. Theune et al (2001) show that a
system employing syntactic templates and a rule-based prosody assignment algorithm
leads to more natural synthesis (of Dutch); unlike FLIGHTS though, their D2S system
does not employ a user preference model or a notion of theme phrase, and does not
distinguish different types of pitch accents. Also closely related is Kruijff-Korbayova?
et al?s (2003) information-state based dialogue system, in which the authors explore a
similar approach to using information structure across dialogue turns; however, their
system does not make use of a user model, and employs template-based realization
with much simpler sentence structures. Kruijff-Korbayova? et al likewise present an
evaluation indicating that the contextual appropriateness of spoken output (in Ger-
man) improves when intonation is assigned on the basis of information structure. In
comparison with our evaluation, theirs examines improvements over a general purpose
195
Computational Linguistics Volume 36, Number 2
text-to-speech voice with default intonation, rather than a limited domain voice, which
provides a much higher baseline in terms of the naturalness of the resulting intonation.
Less closely related is most work on machine-learning approaches to prosody
prediction in text-to-speech (TTS) systems (Hirschberg 1993; Hirschberg and Prieto
1996; Taylor and Black 1998; Dusterhoff, Black, and Taylor 1999; Brenier et al 2006)
and concept-to-speech (CTS) systems (Hitzeman et al 1998, 1999; Pan, McKeown, and
Hirschberg 2002). These approaches have typically aimed to develop generic models
of prosody prediction, by training classifiers for accents and boundaries that make
use of a considerable variety of features. For example, in predicting accent placement
(but not type), Hitzeman et al?s CTS system makes use of rhetorical relations such as
list and contrast, along with the reference type of NPs and whether they represent
first mentions of an entity in the discourse. In Pan et al?s more comprehensive study,
their system predicts accent placement (but not type), break indices, and edge tones
based on features extracted from the SURGE realizer (Elhadad 1993), deep semantic
and discourse features, including semantic type, semantic abnormality and given/new
status, and surface features, such as part of speech andword informativeness. However,
neither of these CTS approaches makes use of the theme/rheme distinction, or the
notion of kontrast that stems from Rooth?s (1992) work on alternative sets, both of
which are crucial to Steedman?s (2000a) theory of how information structure constrains
prosodic choices. More recently, Brenier et al have shown that the ratio of accented to
unaccented tokens of a word in spontaneous speech is a surprisingly effective feature in
predicting pitch accents; they also argued that using information status and contrast is
unlikely to improve upon prominence prediction based only on surface features, since
thesemanually labeled features did not yield substantial improvements in their decision
tree models. Again, however, their approach does not make use of the theme/rheme
distinction, and does not attempt to predict pitch accent type or edge tones; in addition,
they report frequent errors on auxiliaries and negatives (e.g., no), which we have found
to be important for highlighting trade-offs prosodically.
In contrast to these approaches, we have emphasized the generation and synthesis
of sharply distinctive theme and rheme tunes in the limited domain of a dialogue
system, using a hybrid rule-based and data-driven approach. In particular, whereas
Hitzeman et al (1998, 1999) and Pan, McKeown, and Hirschberg (2002) make use of
individual classifiers for prosodic realization decisions?with no means of tying the
decisions of these classifiers together?our approach instead uses rules and constraints
in the grammar to specify a space of possible realizations, through which the realizer
searches to find a sequence of words, pitch accents, and edge tones that maximizes the
probability assigned by an n-gram model for the domain.
In an approach that is more similar in spirit to ours, Bulyko and Ostendorf (2002)
likewise aim to reproduce distinctive intonational patterns in a limited domain. How-
ever, unlike our approach, theirs makes use of simple templates for generating para-
phrases, as their focus is on how deferring the final choice of wording and prosodic
realization to their synthesizer enables them to achieve more natural sounding synthetic
speech. Following on the work described in this article, Nakatsu and White (2006)
present a discriminative approach to realization ranking based on predicted synthesis
quality that is directly compatible with the FLIGHTS system.
Turning to our synthesis evaluation, we note that debate over the standardization of
speech synthesis evaluation continues, with the Blizzard Challenge (Black and Tokuda
2005; Fraser and King 2007) proving to be a useful forum for discussing and performing
evaluation across different synthesis platforms. Mayo, Clark, and King (2005) have pro-
posed to evaluate speech synthesis evaluation from a perceptual viewpoint to discover
196
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
exactly what subjects pay attention to, in order to ensure that evaluation actually is
evaluating what we think it is. The authors found through the use of multidimensional
scaling (MDS) techniques that, when asked to make judgments on the naturalness of
synthetic speech, subjects made judgments relating to a number of different dimensions,
including both segmental quality and prosody. Subsequently, Clark et al (2007) found
correlations between results in MDS spaces and standard mean opinion score (MOS)
tests, but as the MOS tests did not correspond to single dimensions in the MDS space,
they suggested that it may be possible to design more informative evaluations by
asking subjects to specifically rate each factor of interest (e.g., prosody), where each
factor relates to one dimension in the MDS space. However, as no specific method is
suggested to guarantee reliable prosodic judgments from naive listeners, we have left
this question for future research, opting instead to augment the listener ratings gathered
in our perception experiment with an expert prosody evaluation and an f0 analysis of
the theme phrases.
6. Conclusions and Discussion
In this article, we have described an approach to presenting user-tailored information
in spoken dialogues which for the first time brings together multi-attribute decision
models, strategic content planning, surface realization which incorporates prosodic fea-
tures, and unit selection synthesis that takes the resulting prosodicmarkup into account.
Based on the user model, the system selects the most important subset of the available
options to mention and the attributes that are most relevant to choosing between
them. To convey these trade-offs, the system employs a novel presentation strategy
which makes it straightforward to determine information structure and the contents of
referring expressions. During surface realization with OpenCCG, the prosodic structure
is derived from the information structure in a way that allows phrase boundaries to be
determined in a flexible, data-driven fashion, andwith significantly higher acceptability
than baseline prosody prediction models in an expert evaluation. We hypothesize that
the resulting descriptions are both memorable and easy for users to understand. As a
step towards verifying this hypothesis, we have presented an experiment which shows
that listeners perceive a unit selection voice that makes use of the prosodic markup
as significantly more natural than either of two baseline synthetic voices. Through
an expert evaluation and f0 analysis, we have also confirmed the superiority of the
generator-driven intonation and its contribution to listeners? ratings. In future work, we
intend to examine the impact of our generation and synthesis methods on memorability
or other task-oriented measures.
The present study provides evidence that it is worthwhile to investigate methods
of developing general purpose synthesizers that accept prosodic specifications in their
input. The main reason that we did not use such a synthesizer in our evaluation is that
in order to build a general purpose Festival APML voice, suitable APMLmarkupwould
be required for the 2,000?3,000 utterances that make up the core unit selection database.
As these utterances are outside of the FLIGHTS domain (and thus not generated by
the NLG system), it would not be possible with current technology to provide accurate
APML markup for these utterances. Given the difficulty of automatically annotating
general texts with APML, it may be worth considering a simplified version of the
markup for the database. For example, a system which marks the location of pri-
mary phrasal stress, other pitch accents, and a simple categorization of overall tune
(wh-question, yes/no-question, statement, etc.) could be used to annotate the speech
database. This could be achieved with an accent detector and a very simple parser to
197
Computational Linguistics Volume 36, Number 2
determine tune type. The APML specification on the input could easily be mapped to
information equivalent to the database annotation by simple rules. Reasonable quality
synthesis could then be achieved without the database being fully parsed and anno-
tated. Additionally, if there is a portion of in-domain data in the database where full
annotation is available, it could be used directly when those units are searched.
An interesting unresolved question is the extent to which more generic prosody
prediction models, along the lines of Hitzeman et al (1999) and Pan, McKeown, and
Hirschberg (2002)?whichmake no use of such information structural notions as theme,
rheme, and kontrast (cf. Section 2.5)?could be trained to produce tunes as distinctive
as those we have targeted. We suspect that such models would have trouble doing so,
given data sparsity issues and the fact that machine-learned classification models tend
to discover general trends, rather than aiming to reproduce aspects of specific examples,
which may contain important but rare events. At the same time, it remains for us to
investigate whether our hybrid rule-based and data-driven approach can be generalized
to be as flexible andwidely applicable as these machine-learnedmodels, while retaining
its ability to express contrasts intelligibly. In so doing, we expect information structural
constraints in the grammar to continue to play an important role.
Acknowledgments
We thank Rachel Baker, Steve Conway, Mary
Ellen Foster, Kallirroi Georgila, Oliver
Lemon, Colin Matheson, Neide Franca
Rocha, and Mark Steedman for their
contributions to the FLIGHTS system; Eric
Fosler-Lussier and Craige Roberts for helpful
discussion; and Julie McGory for providing
the expert ToBI annotations. We also thank
the anonymous reviewers for helpful
comments and suggestions. This work was
supported in part by EPSRC grant
GR/R02450/01 and an Arts & Humanities
Innovation grant from The Ohio State
University.
References
Aylett, Matthew. 2005. Merging data driven
and rule based prosodic models for unit
selection TTS. In 5th ISCA Speech Synthesis
Workshop, pages 55?59, Pittsburg, PA.
Baker, Rachel Elizabeth. 2003. Using unit
selection to synthesise contextually
appropriate intonation in limited domain
synthesis. Master?s thesis, Department of
Linguistics, University of Edinburgh.
Baldridge, Jason and Geert-Jan Kruijff. 2002.
Coupling CCG and Hybrid Logic
Dependency Semantics. In Proceedings of
40th Annual Meeting of the Association for
Computational Linguistics, pages 319?326,
Philadelphia, PA.
Bangalore, Srinivas and Owen Rambow.
2000. Exploiting a probabilistic hierarchical
model for generation. In Proceedings of
COLING-00, pages 42?48, Saarbrucken,
Germany.
Bilmes, Jeff and Katrin Kirchhoff. 2003.
Factored language models and general
parallelized backoff. In Proceedings of
HLT-03, pages 4?6, Edmonton, Canada.
Black, Alan W. and Keiichi Tokuda. 2005.
The blizzard challenge?2005: Evaluating
corpus-based speech synthesis on
common datasets. In Interspeech 2005,
pages 77?80, Lisbon.
Blackburn, Patrick. 2000. Representation,
reasoning, and relational structures: A
hybrid logic manifesto. Logic Journal of the
IGPL, 8(3):339?625.
Bos, Johan, Ewan Klein, Oliver Lemon,
and Tetsushi Oka. 2003. DIPPER:
Description and formalisation of an
information-state update dialogue system
architecture. In 4th SIGdial Workshop on
Discourse and Dialogue, pages 115?124,
Sapporo.
Brenier, Jason, Ani Nenkova, Anubha
Kothari, Laura Whitton, David Beaver,
and Dan Jurafsky. 2006. The (non)utility
of linguistic features for predicting
prominence in spontaneous speech.
IEEE/ACL 2006 Workshop on Spoken
Language Technology, pages 54?57, Palm
Beach, Aruba.
Bulyko, Ivan and Mari Ostendorf. 2002.
Efficient integrated response generation
from multiple targets using weighted finite
state transducers. Computer Speech and
Language, 16:533?550.
Calhoun, Sasha, Malvina Nissim, Mark
Steedman, and Jason Brenier. 2005. A
framework for annotating information
structure in discourse. Proceedings of the
ACL-05 Workshop on Frontiers in Corpus
198
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Annotation II: Pie in the Sky, pages 45?52,
Ann Arbor, Michigan.
Carberry, Sandra, Jennifer Chu-Carroll, and
Stephanie Elzer. 1999. Constructing and
utilizing a model of user preferences in
collaborative consultation dialogues.
Computational Intelligence Journal,
15(3):185?217.
Carenini, Giuseppe and Johanna D. Moore.
2000. A strategy for generating evaluative
arguments. In Proceedings of INLG-00,
pages 47?54, Mitzpe Ramon.
Carenini, Giuseppe and Johanna D. Moore.
2001. An empirical study of the influence
of user tailoring on evaluative argument
effectiveness. In Proceedings of IJCAI-01,
pages 1307?1314, Seattle, WA.
Carenini, Giuseppe and Johanna D. Moore.
2006. Generating and evaluating
evaluative arguments. Artificial Intelligence,
170:925?952.
Carroll, John, Ann Copestake, Dan
Flickinger, and Victor Poznan?ski. 1999.
An efficient chart generator for (semi-)
lexicalist grammars. In Proceedings of
EWNLG-99, pages 86?95, Toulouse, France.
Carroll, John and Stefan Oepen. 2005. High
efficiency realization for a wide-coverage
unification grammar. In Proceedings of
IJCNLP-05, pages 165?176, Jeju Island,
Korea.
Chung, Grace. 2004. Developing a flexible
spoken dialog system using simulation.
In Proceedings of ACL ?04, pages 63?70,
Barcelona.
Clark, Robert A. J. and Simon King. 2006.
Joint prosodic and segmental unit selection
speech synthesis. In Proceedings of
Interspeech 2006, pages 1312?1315,
Pittsburgh, PA.
Clark, Robert A. J., Monika Podsiadlo, Mark
Fraser, Catherine Mayo, and Simon King.
2007. Statistical analysis of the Blizzard
Challenge 2007 listening test results. In
Proceedings of Blizzard Workshop (in
Proceedings of the 6th ISCA Workshop on
Speech Synthesis), pages 1?6, Bonn,
Germany.
Clark, Robert A. J., Korin Richmond, and
Simon King. 2007. Multisyn: Open-domain
unit selection for the Festival speech
synthesis system. Speech Communication,
49(4):317?330.
Currie, K. and A. Tate. 1991. O-Plan: The
open planning architecture. Artificial
Intelligence, 52:49?86.
de Carolis, Bernadina, Catherine Pelachaud,
Isabella Poggi, and Mark Steedman. 2004.
APML, a Mark-up Language for
Believable Behavior Generation. In
H. Prendinger and M. Ishizuka, editors,
Life-like Characters. Tools, Affective Functions
and Applications. Springer, Berlin,
pages 65?85.
Demberg, Vera and Johanna D. Moore. 2006.
Information presentation in spoken
dialogue systems. In Proceedings of the 11th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL-06), pages 5?72, Trento, Italy.
Dusterhoff, Kurt E., Alan W. Black, and
Paul A. Taylor. 1999. Using decision trees
within the tilt intonation model to
predict f0 contours. In Eurospeech-99,
pages 1627?1630, Budapest, Hungary.
Edwards, W. and F. H. Barron. 1994.
SMARTS and SMARTER: Improved
simple methods for multiattribute utility
measurement. Organizational Behavior and
Human Decision Processes, 60:306?325.
Elhadad, Michael. 1993. Using Argumentation
to Control Lexical Choice: A Functional
Unification Implementation. Ph.D. thesis,
Columbia University.
Foster, Mary Ellen and Michael White. 2004.
Techniques for Text Planning with XSLT.
In Proceedings of the 4th NLPXML Workshop,
pages 1?8, Barcelona, Spain.
Fraser, Mark and Simon King. 2007. The
Blizzard Challenge 2007. In Proceedings of
Blizzard Workshop (in Proceedings of the
6th ISCA Workshop on Speech Synthesis),
pages 7?12, Bonn, Germany.
Ginzburg, Jonathan. 1996. Interrogatives:
Questions, facts and dialogue. In
Shalom Lappin, editor, The Handbook of
Contemporary Semantic Theory. Blackwell,
Oxford, pages 385?422.
Hirschberg, Julia. 1993. Pitch accent in
context: Predicting intonational
prominence from text. Artificial Intelligence,
63:305?340.
Hirschberg, Julia and Pilar Prieto. 1996.
Training intonational phrasing rules
automatically for English and Spanish
text-to-speech. Speech Communication,
18:281?290.
Hitzeman, Janet, Alan W. Black, Chris
Mellish, Jon Oberlander, Massimo Poesio,
and Paul Taylor. 1999. An annotation
scheme for concept-to-speech synthesis.
In Proceedings of EWNLG-99, pages 59?66,
Toulouse.
Hitzeman, Janet, Alan W. Black, Chris
Mellish, Jon Oberlander, and Paul Taylor.
1998. On the use of automatically
generated discourse-level information in a
concept-to-speech synthesis system. In
199
Computational Linguistics Volume 36, Number 2
Proceedings of ICSLP-98, pages 2763?2768,
Sydney, Australia.
Kay, Martin. 1996. Chart generation. In
Proceedings of ACL-96, pages 200?204,
Santa Cruz, USA.
Knight, Kevin and Vasileios
Hatzivassiloglou. 1995. Two-level,
many-paths generation. In Proceedings of
ACL-95, pages 252?260, Cambridge, MA.
Kruijff, Geert-Jan M. 2003. Binding across
boundaries. In Geert-Jan M. Kruijff and
Richard T. Oehrle, editors, Resource-
Sensitivity in Binding and Anaphora. Kluwer
Academic Publishers, pages 123?158,
Dordrecht, The Netherlands.
Kruijff-Korbayova?, Ivana, Stina Ericsson,
Kepa J. Rodr??guez, and Elena Karagjosova.
2003. Producing contextually appropriate
intonation in an information-state based
dialogue system. In Proceedings of EACL-93,
pages 227?234, Budapest, Hungary.
Langkilde, Irene. 2000. Forest-based
statistical sentence generation. In
Proceedings of NAACL-00, pages 170?177,
Seattle, Washington.
Langkilde-Geary, Irene. 2002. An empirical
verification of coverage and correctness
for a general-purpose sentence generator.
In Proceedings of INLG-02, pages 17?24,
New York, NY.
Linden, Greg, Steve Hanks, and Neal Lesh.
1997. Interactive assessment of user
preference models: The automated
travel assistant. In Proceedings of User
Modeling ?97, pages 67?78, Chia Laguna,
Sardinia, Italy.
Marsi, Erwin. 2004. Optionality in evaluating
prosody prediction. In Proceedings of the
5th ISCA Speech Synthesis Workshop,
pages 13?18, Pittsburg, PA.
Martin, D. L., A. J. Cheyer, and D. B. Moran.
1999. The Open Agent Architecture: A
framework for building distributed
software systems. Applied Artificial
Intelligence, 13(1):91?128.
Mayo, C., R. A. J. Clark, and S. King. 2005.
Multidimensional scaling of listener
responses to synthetic speech. In
Interspeech 2005, pages 1725?1728, Lisbon.
Moore, Johanna, Mary Ellen Foster, Oliver
Lemon, and Michael White. 2004.
Generating tailored, comparative
descriptions in spoken dialogue. In
Proceedings of FLAIRS-04, pages 917?922,
Miami Beach, USA.
Moore, Robert C. 2002. A complete, efficient
sentence-realization algorithm for
unification grammar. In Proceedings of
INLG-02, pages 41?48, New York, NY.
Nakatsu, Crystal and Michael White. 2006.
Learning to say it well: Reranking
realizations by predicted synthesis quality.
In Proceedings of COLING-ACL ?06,
pages 1113?1120, Sydney, Australia.
Oh, Alice H. and Alexander I. Rudnicky.
2002. Stochastic natural language
generation for spoken dialog systems.
Computer, Speech & Language,
16(3/4):387?407.
Pan, Shimei, Kathleen McKeown, and Julia
Hirschberg. 2002. Exploring features from
natural language generation for prosody
modeling. Computer Speech and Language,
16:457?490.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. Bleu: A
method for Automatic Evaluation of
Machine Translation. Technical Report
RC22176, IBM.
Pierrehumbert, Janet. 1980. The Phonology
and Phonetics of English Intonation. Ph.D.
thesis, MIT.
Polifroni, Joseph, Grace Chung, and
Stephanie Seneff. 2003. Towards
automatic generation of mixed-initiative
dialogue systems fromWeb content.
In Proceedings of Eurospeech ?03,
pages 193?196, Geneva.
Pon-Barry, Heather, Fuliang Weng, and
Sebastian Varges. 2006. Evaluation of
content presentation strategies for an
in-car spoken dialogue system. In
Proceedings of Interspeech 2006,
pages 1930?1933, Pittsburgh, PA.
Prevost, Scott. 1995. A Semantics of Contrast
and Information Structure for Specifying
Intonation in Spoken Language Generation.
Ph.D. thesis, University of Pennsylvania.
Ratnaparkhi, Adwait. 2002. Trainable
approaches to surface natural language
generation and their application to
conversational dialog systems. Computer,
Speech & Language, 16(3/4):435?455.
Reiter, Ehud and Robert Dale. 2000. Building
Natural Language Generation Systems.
Cambridge University Press, Cambridge.
Ristad, Eric S.?1995. A Natural Law of
Succession. Technical Report
CS-TR-495-95, Princeton University.
Roberts, Craige. 1996. Information
structure: Towards an integrated
formal theory of pragmatics. Ohio State
University Working Papers in Linguistics,
49:91?136.
Rocha, Neide Franca. 2004. Evaluating
prosodic markup in a spoken dialogue
system. Master?s thesis, Department of
Linguistics, University of Edinburgh.
200
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Rooth, Mats. 1992. A theory of focus
interpretation. Natural Language Semantics,
1:75?116.
Shemtov, Hadar. 1997. Ambiguity Management
in Natural Language Generation. Ph.D.
thesis, Stanford University.
Silverman, K., M. Beckman, J. Pitrelli,
M. Ostendorf, C. Wightman, P. Price,
J. Pierrehumbert, and J. Hirschberg. 1992.
ToBI: A standard for labeling English
prosody. Proceedings of ICSLP92, 2:867?870,
Banff, Canada.
Snover, Matthew, Bonnie Dorr, Richard
Schwartz, Linnea Micciulla, and John
Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In
Proceedings of the Association for Machine
Translation in the Americas (AMTA-06),
pages 223?231, Cambridge, MA.
Steedman, Mark. 2000a. Information
structure and the syntax-phonology
interface. Linguistic Inquiry,
31(4):649?689.
Steedman, Mark. 2000b. The Syntactic Process.
MIT Press, Cambridge, MA.
Steedman, Mark. 2004. Using APML to
specify intonation. Magicster Project
Deliverable 2.5. University of Edinburgh.
Available at http://www.ltg.ed.ac.uk/
magicster/deliverables/annex2.5/
apml-howto.pdf
Steedman, Mark. 2006. Information-
structural semantics for English
intonation. In Chungmin Lee, Matt
Gordon, and Daniel Bu?ring, editors,
Topic and Focus: Cross-Linguistic
Perspectives on Meaning and Intonation.
Springer, Dordrecht, pages 245?264.
Stolcke, Andreas. 2002. SRILM? An
extensible language modeling toolkit. In
Proceedings of ICSLP-02, pages 901?904,
Denver, Colorado.
Taylor, Paul and Alan Black. 1998. Assigning
phrase breaks from part-of-speech
sequences. Computer Speech and Language,
12:99?117.
Taylor, P., A. Black, and R. Caley. 1998. The
architecture of the the Festival speech
synthesis system. In Third International
Workshop on Speech Synthesis,
pages 147?151, Sydney.
Theune, Marie?t. 2002. Contrast in
concept-to-speech generation. Computer
Speech and Language, 16:491?531.
Theune, Marie?t, Esther Klabbers, Jan-Roelof
de Pijper, Emiel Krahmer, and Jan Odijk.
2001. From data to speech: A general
approach. Natural Language Engineering,
7(1):47?86.
Vallduv??, Enric and Maria Vilkuna. 1998. On
rheme and kontrast. In Peter Culicover
and Louise McNally, editors, Syntax and
Semantics, Vol. 29: The Limits of Syntax.
Academic Press, San Diego, CA,
pages 79?108.
van Deemter, Kees, Emiel Krahmer, and
Marie?t Theune. 2005. Real versus
template-based natural language
generation: A false opposition?
Computational Linguistics, 31(1):15?23.
van Santen, Jan, Alexander Kain, Esther
Klabbers, and Taniya Mishra. 2005.
Synthesis of prosody using multi-level
unit sequences. Speech Communication,
46(3?4):365?375.
Walker, M. A., S. Whittaker, A. Stent,
P. Maloor, J. D. Moore, M. Johnston, and
G. Vasireddy. 2002. Speech-plans:
Generating evaluative responses in spoken
dialogue. In Proceedings of INLG ?02,
pages 73?80, New York, NY.
Walker, M. A., S. J. Whittaker, A. Stent,
P. Maloor, J. D. Moore, M. Johnston, and
G. Vasireddy. 2004. Generation and
evaluation of user-tailored responses in
multimodal dialogue. Cognitive Science,
28:811?840.
Walker, Marilyn A., Rebecca Passonneau,
and Julie E. Boland. 2001. Quantitative
and qualitative evaluation of DARPA
Communicator spoken dialogue systems.
In Proceedings of ACL-01, pages 515?522,
Toulouse, France.
Walker, Marilyn A., Owen C. Rambow, and
Monica Rogati. 2002. Training a sentence
planner for spoken dialogue using
boosting. Computer Speech and Language,
16:409?433.
White, Michael. 2004. Reining in CCG Chart
Realization. In Proceedings of INLG-04,
pages 182?191, Brockenhurst, UK.
White, Michael. 2006a. CCG chart realization
from disjunctive inputs. In Proceedings of
INLG-06, pages 12?19, Sydney, Australia.
White, Michael. 2006b. Efficient realization of
coordinate structures in Combinatory
Categorial Grammar. Research on Language
& Computation, 4(1):39?75.
201

Proceedings of the ACL 2010 Conference Short Papers, pages 43?48,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The impact of interpretation problems on tutorial dialogue
Myroslava O. Dzikovska and Johanna D. Moore
School of Informatics, University of Edinburgh, Edinburgh, United Kingdom
{m.dzikovska,j.moore}@ed.ac.uk
Natalie Steinhauser and Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division, Orlando, FL, USA
{natalie.steihauser,gwendolyn.campbell}@navy.mil
Abstract
Supporting natural language input may
improve learning in intelligent tutoring
systems. However, interpretation errors
are unavoidable and require an effective
recovery policy. We describe an evaluation
of an error recovery policy in the BEE-
TLE II tutorial dialogue system and dis-
cuss how different types of interpretation
problems affect learning gain and user sat-
isfaction. In particular, the problems aris-
ing from student use of non-standard ter-
minology appear to have negative conse-
quences. We argue that existing strategies
for dealing with terminology problems are
insufficient and that improving such strate-
gies is important in future ITS research.
1 Introduction
There is a mounting body of evidence that student
self-explanation and contentful talk in human-
human tutorial dialogue are correlated with in-
creased learning gain (Chi et al, 1994; Purandare
and Litman, 2008; Litman et al, 2009). Thus,
computer tutors that understand student explana-
tions have the potential to improve student learn-
ing (Graesser et al, 1999; Jordan et al, 2006;
Aleven et al, 2001; Dzikovska et al, 2008). How-
ever, understanding and correctly assessing the
student?s contributions is a difficult problem due
to the wide range of variation observed in student
input, and especially due to students? sometimes
vague and incorrect use of domain terminology.
Many tutorial dialogue systems limit the range
of student input by asking short-answer questions.
This provides a measure of robustness, and previ-
ous evaluations of ASR in spoken tutorial dialogue
systems indicate that neither word error rate nor
concept error rate in such systems affect learning
gain (Litman and Forbes-Riley, 2005; Pon-Barry
et al, 2004). However, limiting the range of pos-
sible input limits the contentful talk that the stu-
dents are expected to produce, and therefore may
limit the overall effectiveness of the system.
Most of the existing tutoring systems that accept
unrestricted language input use classifiers based
on statistical text similarity measures to match
student answers to open-ended questions with
pre-authored anticipated answers (Graesser et al,
1999; Jordan et al, 2004; McCarthy et al, 2008).
While such systems are robust to unexpected ter-
minology, they provide only a very coarse-grained
assessment of student answers. Recent research
aims to develop methods that produce detailed
analyses of student input, including correct, in-
correct and missing parts (Nielsen et al, 2008;
Dzikovska et al, 2008), because the more detailed
assessments can help tailor tutoring to the needs of
individual students.
While the detailed assessments of answers to
open-ended questions are intended to improve po-
tential learning, they also increase the probabil-
ity of misunderstandings, which negatively impact
tutoring and therefore negatively impact student
learning (Jordan et al, 2009). Thus, appropri-
ate error recovery strategies are crucially impor-
tant for tutorial dialogue applications. We describe
an evaluation of an implemented tutorial dialogue
system which aims to accept unrestricted student
input and limit misunderstandings by rejecting low
confidence interpretations and employing a range
of error recovery strategies depending on the cause
of interpretation failure.
By comparing two different system policies, we
demonstrate that with less restricted language in-
put the rate of non-understanding errors impacts
both learning gain and user satisfaction, and that
problems arising from incorrect use of terminol-
ogy have a particularly negative impact. A more
detailed analysis of the results indicates that, even
though we based our policy on an approach ef-
43
fective in task-oriented dialogue (Hockey et al,
2003), many of our strategies were not success-
ful in improving learning gain. At the same time,
students appear to be aware that the system does
not fully understand them even if it accepts their
input without indicating that it is having interpre-
tation problems, and this is reflected in decreased
user satisfaction. We argue that this indicates that
we need better strategies for dealing with termi-
nology problems, and that accepting non-standard
terminology without explicitly addressing the dif-
ference in acceptable phrasing may not be suffi-
cient for effective tutoring.
In Section 2 we describe our tutoring system,
and the two tutoring policies implemented for the
experiment. In Section 3 we present experimen-
tal results and an analysis of correlations between
different types of interpretation problems, learning
gain and user satisfaction. Finally, in Section 4 we
discuss the implications of our results for error re-
covery policies in tutorial dialogue systems.
2 Tutorial Dialogue System and Error
Recovery Policies
This work is based on evaluation of BEETLE II
(Dzikovska et al, 2010), a tutorial dialogue sys-
tem which provides tutoring in basic electricity
and electronics. Students read pre-authored mate-
rials, experiment with a circuit simulator, and then
are asked to explain their observations. BEETLE II
uses a deep parser together with a domain-specific
diagnoser to process student input, and a deep gen-
erator to produce tutorial feedback automatically
depending on the current tutorial policy. It also
implements an error recovery policy to deal with
interpretation problems.
Students currently communicate with the sys-
tem via a typed chat interface. While typing
removes the uncertainty and errors involved in
speech recognition, expected student answers are
considerably more complex and varied than in
a typical spoken dialogue system. Therefore, a
significant number of interpretation errors arise,
primarily during the semantic interpretation pro-
cess. These errors can lead to non-understandings,
when the system cannot produce a syntactic parse
(or a reasonable fragmentary parse), or when it
does not know how to interpret an out-of-domain
word; and misunderstandings, where a system ar-
rives at an incorrect interpretation, due to either
an incorrect attachment in the parse, an incorrect
word sense assigned to an ambiguous word, or an
incorrectly resolved referential expression.
Our approach to selecting an error recovery pol-
icy is to prefer non-understandings to misunder-
standings. There is a known trade-off in spoken di-
alogue systems between allowing misunderstand-
ings, i.e., cases in which a system accepts and
acts on an incorrect interpretation of an utterance,
and non-understandings, i.e., cases in which a sys-
tem rejects an utterance as uninterpretable (Bo-
hus and Rudnicky, 2005). Since misunderstand-
ings on the part of a computer tutor are known
to negatively impact student learning, and since
in human-human tutorial dialogue the majority of
student responses using unexpected terminology
are classified as incorrect (Jordan et al, 2009),
it would be a reasonable approach for a tutorial
dialogue system to deal with potential interpreta-
tion problems by treating low-confidence interpre-
tations as non-understandings and focusing on an
effective non-understanding recovery policy.1
We implemented two different policies for com-
parison. Our baseline policy does not attempt any
remediation or error recovery. All student utter-
ances are passed through the standard interpreta-
tion pipeline, so that the results can be analyzed
later. However, the system does not attempt to ad-
dress the student content. Instead, regardless of
the answer analysis, the system always uses a neu-
tral acceptance and bottom out strategy, giving the
student the correct answer every time, e.g., ?OK.
One way to phrase the correct answer is: the open
switch creates a gap in the circuit?. Thus, the stu-
dents are never given any indication of whether
they have been understood or not.
The full policy acts differently depending on the
analysis of the student answer. For correct an-
swers, it acknowledges the answer as correct and
optionally restates it (see (Dzikovska et al, 2008)
for details). For incorrect answers, it restates the
correct portion of the answer (if any) and provides
a hint to guide the student towards the completely
correct answer. If the student?s utterance cannot be
interpreted, the system responds with a help mes-
sage indicating the cause of the problem together
with a hint. In both cases, after 3 unsuccessful at-
tempts to address the problem the system uses the
bottom out strategy and gives away the answer.
1While there is no confidence score from a speech recog-
nizer, our system uses a combination of a parse quality score
assigned by the parser and a set of consistency checks to de-
termine whether an interpretation is sufficiently reliable.
44
The content of the bottom out is the same as in
the baseline, except that the full system indicates
clearly that the answer was incorrect or was not
understood, e.g., ?Not quite. Here is the answer:
the open switch creates a gap in the circuit?.
The help messages are based on the Targeted-
Help approach successfully used in spoken dia-
logue (Hockey et al, 2003), together with the error
classification we developed for tutorial dialogue
(Dzikovska et al, 2009). There are 9 different er-
ror types, each associated with a different targeted
help message. The goal of the help messages is to
give the student as much information as possible
as to why the system failed to understand them but
without giving away the answer.
In comparing the two policies, we would expect
that the students in both conditions would learn
something, but that the learning gain and user sat-
isfaction would be affected by the difference in
policies. We hypothesized that students who re-
ceive feedback on their errors in the full condition
would learn more compared to those in the base-
line condition.
3 Evaluation
We collected data from 76 subjects interacting
with the system. The subjects were randomly as-
signed to either the baseline (BASE) or the full
(FULL) policy condition. Each subject took a pre-
test, then worked through a lesson with the system,
and then took a post-test and filled in a user satis-
faction survey. Each session lasted approximately
4 hours, with 232 student language turns in FULL
(SD = 25.6) and 156 in BASE (SD = 2.02). Ad-
ditional time was taken by reading and interact-
ing with the simulation environment. The students
had little prior knowledge of the domain. The sur-
vey consisted of 63 questions on the 5-point Lik-
ert scale covering the lesson content, the graphical
user interface, and tutor?s understanding and feed-
back. For purposes of this study, we are using an
averaged tutor score.
The average learning gain was 0.57 (SD =
0.23) in FULL, and 0.63 (SD = 0.26) in BASE.
There was no significant difference in learning
gain between conditions. Students liked BASE bet-
ter: the average tutor evaluation score for FULL
was 2.56 out of 5 (SD = 0.65), compared to 3.32
(SD = 0.65) in BASE. These results are signif-
icantly different (t-test, p < 0.05). In informal
comments after the session many students said that
they were frustrated when the system said that it
did not understand them. However, some students
in BASE also mentioned that they sometimes were
not sure if the system?s answer was correcting a
problem with their answer, or simply phrasing it
in a different way.
We used mean frequency of non-interpretable
utterances (out of all student utterances in
each session) to evaluate the effectiveness of
the two different policies. On average, 14%
of utterances in both conditions resulted in
non-understandings.2 The frequency of non-
understandings was negatively correlated with
learning gain in FULL: r = ?0.47, p < 0.005,
but not significantly correlated with learning gain
in BASE: r = ?0.09, p = 0.59. However, in both
conditions the frequency of non-understandings
was negatively correlated with user satisfaction:
FULL r = ?0.36, p = 0.03, BASE r = ?0.4, p =
0.01. Thus, even though in BASE the system
did not indicate non-understanding, students were
negatively affected. That is, they were not satis-
fied with the policy that did not directly address
the interpretation problems. We discuss possible
reasons for this below.
We investigated the effect of different types of
interpretation errors using two criteria. First, we
checked whether the mean frequency of errors was
reduced between BASE and FULL for each individ-
ual strategy. The reduced frequency means that
the recovery strategy for this particular error type
is effective in reducing the error frequency. Sec-
ond, we looked for the cases where the frequency
of a given error type is negatively correlated with
either learning gain or user satisfaction. This is
provides evidence that such errors are negatively
impacting the learning process, and therefore im-
proving recovery strategies for those error types is
likely to improve overall system effectiveness,
The results, shown in Table 1, indicate that the
majority of interpretation problems are not sig-
nificantly correlated with learning gain. How-
ever, several types of problems appear to be
particularly significant, and are all related to
improper use of domain terminology. These
were irrelevant answer, no appr terms, selec-
tional restriction failure and program error.
An irrelevant answer error occurs when the stu-
dent makes a statement that uses domain termi-
2We do not know the percentage of misunderstandings or
concept error rate as yet. We are currently annotating the data
with the goal to evaluate interpretation correctness.
45
full baseline
error type
mean freq.
(std. dev)
satisfac-
tion r
gain
r
mean freq
(std. dev)
satisfac-
tion r
gain
r
irrelevant answer 0.008 (0.01) -0.08 -0.19 0.012 (0.01) -0.07 -0.47**
no appr terms 0.005 (0.01) -0.57** -0.42** 0.003 (0.01) -0.38** -0.01
selectional restr failure 0.032 (0.02) -0.12 -0.55** 0.040 (0.03) 0.13 0.26*
program error 0.002 (0.003) 0.02 0.26 0.003 (0.003) 0 -0.35**
unknown word 0.023 (0.01) 0.05 -0.21 0.024 (0.02) -0.15 -0.09
disambiguation failure 0.013 (0.01) -0.04 0.02 0.007 (0.01) -0.18 0.19
no parse 0.019 (0.01) -0.14 -0.08 0.022(0.02) -0.3* 0.01
partial interpretation 0.004 (0.004) -0.11 -0.01 0.004 (0.005) -0.19 0.22
reference failure 0.012 (0.02) -0.31* -0.09 0.017 (0.01) -0.15 -0.23
Overall 0.134 (0.05) -0.36** -0.47** 0.139 (0.04) -0.4** -0.09
Table 1: Correlations between frequency of different error types and student learning gain and satisfac-
tion. ** - correlation is significant with p < 0.05, * - with p <= 0.1.
nology but does not appear to answer the system?s
question directly. For example, the expected an-
swer to ?In circuit 1, which components are in a
closed path?? is ?the bulb?. Some students mis-
read the question and say ?Circuit 1 is closed.? If
that happens, in FULL the system says ?Sorry, this
isn?t the form of answer that I expected. I am look-
ing for a component?, pointing out to the student
the kind of information it is looking for. The BASE
system for this error, and for all other errors dis-
cussed below, gives away the correct answer with-
out indicating that there was a problem with in-
terpreting the student?s utterance, e.g., ?OK, the
correct answer is the bulb.?
The no appr terms error happens when the stu-
dent is using terminology inappropriate for the les-
son in general. Students are expected to learn to
explain everything in terms of connections and ter-
minal states. For example, the expected answer to
?What is voltage?? is ?the difference in states be-
tween two terminals?. If instead the student says
?Voltage is electricity?, FULL responds with ?I am
sorry, I am having trouble understanding. I see no
domain concepts in your answer. Here?s a hint:
your answer should mention a terminal.? The mo-
tivation behind this strategy is that in general, it is
very difficult to reason about vaguely used domain
terminology. We had hoped that by telling the stu-
dent that the content of their utterance is outside
the domain as understood by the system, and hint-
ing at the correct terms to use, the system would
guide students towards a better answer.
Selectional restr failure errors are typically due
to incorrect terminology, when the students
phrased answers in a way that contradicted the sys-
tem?s domain knowledge. For example, the sys-
tem can reason about damaged bulbs and batter-
ies, and open and closed paths. So if the stu-
dent says ?The path is damaged?, the FULL sys-
tem would respond with ?I am sorry, I am having
trouble understanding. Paths cannot be damaged.
Only bulbs and batteries can be damaged.?
Program error were caused by faults in the un-
derlying network software, but usually occurred
when the student was using extremely long and
complicated utterances.
Out of the four important error types described
above, only the strategy for irrelevant answer was
effective: the frequency of irrelevant answer er-
rors is significantly higher in BASE (t-test, p <
0.05), and it is negatively correlated with learning
gain in BASE. The frequencies of other error types
did not significantly differ between conditions.
However, one other finding is particularly in-
teresting: the frequency of no appr terms errors
is negatively correlated with user satisfaction in
BASE. This indicates that simply accepting the stu-
dent?s answer when they are using incorrect termi-
nology and exposing them to the correct answer is
not the best strategy, possibly because the students
are noticing the unexplained lack of alignment be-
tween their utterance and the system?s answer.
4 Discussion and Future Work
As discussed in Section 1, previous studies of
short-answer tutorial dialogue systems produced a
counter-intuitive result: measures of interpretation
accuracy were not correlated with learning gain.
With less restricted language, misunderstandings
46
negatively affected learning. Our study provides
further evidence that interpretation quality signif-
icantly affects learning gain in tutorial dialogue.
Moreover, while it has long been known that user
satisfaction is negatively correlated with interpre-
tation error rates in spoken dialogue, this is the
first attempt to evaluate the impact of different
types of interpretation errors on task success and
usability of a tutoring system.
Our results demonstrate that different types of
errors may matter to a different degree. In our
system, all of the error types negatively correlated
with learning gain stem from the same underlying
problem: the use of incorrect or vague terminol-
ogy by the student. With the exception of the ir-
relevant answer strategy, the targeted help strate-
gies we implemented were not effective in reduc-
ing error frequency or improving learning gain.
Additional research is needed to understand why.
One possibility is that irrelevant answer was eas-
ier to remediate compared to other error types. It
usually happened in situations where there was a
clear expectation of the answer type (e.g., a list of
component names, a yes/no answer). Therefore,
it was easier to design an effective prompt. Help
messages for other error types were more frequent
when the expected answer was a complex sen-
tence, and multiple possible ways of phrasing the
correct answer were acceptable. Therefore, it was
more difficult to formulate a prompt that would
clearly describe the problem in all contexts.
One way to improve the help messages may be
to have the system indicate more clearly when user
terminology is a problem. Our system apologized
each time there was a non-understanding, leading
students to believe that they may be answering cor-
rectly but the answer is not being understood. A
different approach would be to say something like
?I am sorry, you are not using the correct termi-
nology in your answer. Here?s a hint: your answer
should mention a terminal?. Together with an ap-
propriate mechanism to detect paraphrases of cor-
rect answers (as opposed to vague answers whose
correctness is difficult to determine), this approach
could be more beneficial in helping students learn.
We are considering implementing and evaluating
this as part of our future work.
Some of the errors, in particular instances of
no appr terms and selectional restr failure, also
stemmed from unrecognized paraphrases with
non-standard terminology. Those answers could
conceivably be accepted by a system using seman-
tic similarity as a metric (e.g., using LSA with pre-
authored answers). However, our results also indi-
cate that simply accepting the incorrect terminol-
ogy may not be the best strategy. Users appear to
be sensitive when the system?s language does not
align with their terminology, as reflected in the de-
creased satisfaction ratings associated with higher
rates of incorrect terminology problems in BASE.
Moreover, prior analysis of human-human data
indicates that tutors use different restate strate-
gies depending on the ?quality? of the student an-
swers, even if they are accepting them as correct
(Dzikovska et al, 2008). Together, these point at
an important unaddressed issue: existing systems
are often built on the assumption that only incor-
rect and missing parts of the student answer should
be remediated, and a wide range of terminology
should be accepted (Graesser et al, 1999; Jordan
et al, 2006). While it is obviously important for
the system to accept a range of different phrasings,
our analysis indicates that this may not be suffi-
cient by itself, and students could potentially ben-
efit from addressing the terminology issues with a
specifically devised strategy.
Finally, it could also be possible that some
differences between strategy effectiveness were
caused by incorrect error type classification. Man-
ual examination of several dialogues suggests that
most of the errors are assigned to the appropri-
ate type, though in some cases incorrect syntac-
tic parses resulted in unexpected interpretation er-
rors, causing the system to give a confusing help
message. These misclassifications appear to be
evenly split between different error types, though
a more formal evaluation is planned in the fu-
ture. However from our initial examination, we
believe that the differences in strategy effective-
ness that we observed are due to the actual differ-
ences in the help messages. Therefore, designing
better prompts would be the key factor in improv-
ing learning and user satisfaction.
Acknowledgments
This work has been supported in part by US Office
of Naval Research grants N000140810043 and
N0001410WX20278. We thank Katherine Harri-
son, Leanne Taylor, Charles Callaway, and Elaine
Farrow for help with setting up the system and
running the evaluation. We would like to thank
anonymous reviewers for their detailed feedback.
47
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
Dan Bohus and Alexander Rudnicky. 2005. Sorry,
I didn?t catch that! - An investigation of non-
understanding errors and recovery strategies. In
Proceedings of SIGdial-2005, Lisbon, Portugal.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting
self-explanations improves understanding. Cogni-
tive Science, 18(3):439?477.
Myroslava O. Dzikovska, Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008. Diagnosing natural lan-
guage answers to support adaptive tutoring. In
Proceedings 21st International FLAIRS Conference,
Coconut Grove, Florida, May.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn C. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of SIGDIAL-09, London, UK, Sep.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010. Beetle II: a sys-
tem for tutoring and computational linguistics ex-
perimentation. In Proceedings of ACL-2010 demo
session.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simula-
tion of a human tutor. Cognitive Systems Research,
1:35?51.
Beth Ann Hockey, Oliver Lemon, Ellen Campana,
Laura Hiatt, Gregory Aist, James Hieronymus,
Alexander Gruenstein, and John Dowding. 2003.
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users? performance.
In Proceedings of the tenth conference on European
chapter of the Association for Computational Lin-
guistics, pages 147?154, Morristown, NJ, USA.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language under-
standing approaches in an intelligent tutoring sys-
tem. In James C. Lester, Rosa Maria Vicari, and
Fa?bio Paraguac?u, editors, Intelligent Tutoring Sys-
tems, volume 3220 of Lecture Notes in Computer
Science, pages 346?357. Springer.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Pamela Jordan, Diane Litman, Michael Lipschultz, and
Joanna Drummond. 2009. Evidence of misunder-
standings in tutorial dialogue and their impact on
learning. In Proceedings of the 14th International
Conference on Artificial Intelligence in Education
(AIED), Brighton, UK, July.
Diane Litman and Kate Forbes-Riley. 2005. Speech
recognition performance and learning in spoken di-
alogue tutoring. In Proceedings of EUROSPEECH-
2005, page 1427.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Generalizing tutorial dia-
logue results. In Proceedings of 14th International
Conference on Artificial Intelligence in Education
(AIED), Brighton, UK, July.
Philip M. McCarthy, Vasile Rus, Scott Crossley,
Arthur C. Graesser, and Danielle S. McNamara.
2008. Assessing forward-, reverse-, and average-
entailment indeces on natural language input from
the intelligent tutoring system, iSTART. In Proceed-
ings of the 21st International FLAIRS conference,
pages 165?170.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Heather Pon-Barry, Brady Clark, Elizabeth Owen
Bratt, Karl Schultz, and Stanley Peters. 2004. Eval-
uating the effectiveness of SCoT: A spoken conver-
sational tutor. In J. Mostow and P. Tedesco, editors,
Proceedings of the ITS 2004 Workshop on Dialog-
based Intelligent Tutoring Systems, pages 23?32.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings 21st
International FLAIRS Conference, Coconut Grove,
Florida, May.
48
Proceedings of the ACL 2010 System Demonstrations, pages 13?18,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
BEETLE II: a system for tutoring and computational linguistics
experimentation
Myroslava O. Dzikovska and Johanna D. Moore
School of Informatics, University of Edinburgh, Edinburgh, United Kingdom
{m.dzikovska,j.moore}@ed.ac.uk
Natalie Steinhauser and Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division, Orlando, FL, USA
{gwendolyn.campbell,natalie.steihauser}@navy.mil
Elaine Farrow
Heriot-Watt University
Edinburgh, United Kingdom
e.farrow@hw.ac.uk
Charles B. Callaway
University of Haifa
Mount Carmel, Haifa, Israel
ccallawa@gmail.com
Abstract
We present BEETLE II, a tutorial dia-
logue system designed to accept unre-
stricted language input and support exper-
imentation with different tutorial planning
and dialogue strategies. Our first system
evaluation used two different tutorial poli-
cies and demonstrated that the system can
be successfully used to study the impact
of different approaches to tutoring. In the
future, the system can also be used to ex-
periment with a variety of natural language
interpretation and generation techniques.
1 Introduction
Over the last decade there has been a lot of inter-
est in developing tutorial dialogue systems that un-
derstand student explanations (Jordan et al, 2006;
Graesser et al, 1999; Aleven et al, 2001; Buckley
and Wolska, 2007; Nielsen et al, 2008; VanLehn
et al, 2007), because high percentages of self-
explanation and student contentful talk are known
to be correlated with better learning in human-
human tutoring (Chi et al, 1994; Litman et al,
2009; Purandare and Litman, 2008; Steinhauser et
al., 2007). However, most existing systems use
pre-authored tutor responses for addressing stu-
dent errors. The advantage of this approach is that
tutors can devise remediation dialogues that are
highly tailored to specific misconceptions many
students share, providing step-by-step scaffolding
and potentially suggesting additional problems.
The disadvantage is a lack of adaptivity and gen-
erality: students often get the same remediation
for the same error regardless of their past perfor-
mance or dialogue context, as it is infeasible to
author a different remediation dialogue for every
possible dialogue state. It also becomes more dif-
ficult to experiment with different tutorial policies
within the system due to the inherent completixites
in applying tutoring strategies consistently across
a large number of individual hand-authored reme-
diations.
The BEETLE II system architecture is designed
to overcome these limitations (Callaway et al,
2007). It uses a deep parser and generator, to-
gether with a domain reasoner and a diagnoser,
to produce detailed analyses of student utterances
and generate feedback automatically. This allows
the system to consistently apply the same tutorial
policy across a range of questions. To some extent,
this comes at the expense of being able to address
individual student misconceptions. However, the
system?s modular setup and extensibility make it
a suitable testbed for both computational linguis-
tics algorithms and more general questions about
theories of learning.
A distinguishing feature of the system is that it
is based on an introductory electricity and elec-
tronics course developed by experienced instruc-
tional designers. The course was first created for
use in a human-human tutoring study, without tak-
ing into account possible limitations of computer
tutoring. The exercises were then transferred into
a computer system with only minor adjustments
(e.g., breaking down compound questions into in-
dividual questions). This resulted in a realistic tu-
toring setup, which presents interesting challenges
to language processing components, involving a
wide variety of language phenomena.
We demonstrate a version of the system that
has undergone a successful user evaluation in
13
2009. The evaluation results indicate that addi-
tional improvements to remediation strategies, and
especially to strategies dealing with interpretation
problems, are necessary for effective tutoring. At
the same time, the successful large-scale evalua-
tion shows that BEETLE II can be used as a plat-
form for future experimentation.
The rest of this paper discusses the BEETLE II
system architecture (Section 2), system evaluation
(Section 3), and the range of computational lin-
guistics problems that can be investigated using
BEETLE II (Section 4).
2 System Architecture
The BEETLE II system delivers basic electricity
and electronics tutoring to students with no prior
knowledge of the subject. A screenshot of the sys-
tem is shown in Figure 1. The student interface in-
cludes an area to display reading material, a circuit
simulator, and a dialogue history window. All in-
teractions with the system are typed. Students read
pre-authored curriculum slides and carry out exer-
cises which involve experimenting with the circuit
simulator and explaining the observed behavior.
The system also asks some high-level questions,
such as ?What is voltage??.
The system architecture is shown in Figure 2.
The system uses a standard interpretation pipeline,
with domain-independent parsing and generation
components supported by domain specific reason-
ers for decision making. The architecture is dis-
cussed in detail in the rest of this section.
2.1 Interpretation Components
We use the TRIPS dialogue parser (Allen et al,
2007) to parse the utterances. The parser provides
a domain-independent semantic representation in-
cluding high-level word senses and semantic role
labels. The contextual interpreter then uses a refer-
ence resolution approach similar to Byron (2002),
and an ontology mapping mechanism (Dzikovska
et al, 2008a) to produce a domain-specific seman-
tic representation of the student?s output. Utter-
ance content is represented as a set of extracted
objects and relations between them. Negation is
supported, together with a heuristic scoping algo-
rithm. The interpreter also performs basic ellipsis
resolution. For example, it can determine that in
the answer to the question ?Which bulbs will be
on and which bulbs will be off in this diagram??,
?off? can be taken to mean ?all bulbs in the di-
agram will be off.? The resulting output is then
passed on to the domain reasoning and diagnosis
components.
2.2 Domain Reasoning and Diagnosis
The system uses a knowledge base implemented in
the KM representation language (Clark and Porter,
1999; Dzikovska et al, 2006) to represent the state
of the world. At present, the knowledge base rep-
resents 14 object types and supports the curricu-
lum containing over 200 questions and 40 differ-
ent circuits.
Student explanations are checked on two levels,
verifying factual and explanation correctness. For
example, for a question ?Why is bulb A lit??, if
the student says ?it is in a closed path?, the system
checks two things: a) is the bulb indeed in a closed
path? and b) is being in a closed path a reason-
able explanation for the bulb being lit? Different
remediation strategies need to be used depending
on whether the student made a factual error (i.e.,
they misread the diagram and the bulb is not in a
closed path) or produced an incorrect explanation
(i.e., the bulb is indeed in a closed path, but they
failed to mention that a battery needs to be in the
same closed path for the bulb to light).
The knowledge base is used to check the fac-
tual correctness of the answers first, and then a di-
agnoser checks the explanation correctness. The
diagnoser, based on Dzikovska et al (2008b), out-
puts a diagnosis which consists of lists of correct,
contradictory and non-mentioned objects and re-
lations from the student?s answer. At present, the
system uses a heuristic matching algorithm to clas-
sify relations into the appropriate category, though
in the future we may consider a classifier similar
to Nielsen et al (2008).
2.3 Tutorial Planner
The tutorial planner implements a set of generic
tutoring strategies, as well as a policy to choose
an appropriate strategy at each point of the inter-
action. It is designed so that different policies can
be defined for the system. The currently imple-
mented strategies are: acknowledging the correct
part of the answer; suggesting a slide to read with
background material; prompting for missing parts
of the answer; hinting (low- and high- specificity);
and giving away the answer. Two or more strate-
gies can be used together if necessary.
The hint selection mechanism generates hints
automatically. For a low specificity hint it selects
14
Figure 1: Screenshot of the BEETLE II system
Dialogue ManagerParserContextualInterpreter
Interpretation
CurriculumPlanner
KnowledgeBase
Content Planner & Generator
TutorialPlanner
Tutoring
GUI
Diagnoser
Figure 2: System architecture diagram
15
an as-yet unmentioned object and hints at it, for
example, ?Here?s a hint: Your answer should men-
tion a battery.? For high-specificity, it attempts to
hint at a two-place relation, for example, ?Here?s
a hint: the battery is connected to something.?
The tutorial policy makes a high-level decision
as to which strategy to use (for example, ?ac-
knowledge the correct part and give a high speci-
ficity hint?) based on the answer analysis and di-
alogue context. At present, the system takes into
consideration the number of incorrect answers re-
ceived in response to the current question and the
number of uninterpretable answers.1
In addition to a remediation policy, the tuto-
rial planner implements an error recovery policy
(Dzikovska et al, 2009). Since the system ac-
cepts unrestricted input, interpretation errors are
unavoidable. Our recovery policy is modeled on
the TargetedHelp (Hockey et al, 2003) policy used
in task-oriented dialogue. If the system cannot
find an interpretation for an utterance, it attempts
to produce a message that describes the problem
but without giving away the answer, for example,
?I?m sorry, I?m having a problem understanding. I
don?t know the word power.? The help message is
accompanied with a hint at the appropriate level,
also depending on the number of previous incor-
rect and non-interpretable answers.
2.4 Generation
The strategy decision made by the tutorial plan-
ner, together with relevant semantic content from
the student?s answer (e.g., part of the answer to
confirm), is passed to content planning and gen-
eration. The system uses a domain-specific con-
tent planner to produce input to the surface realizer
based on the strategy decision, and a FUF/SURGE
(Elhadad and Robin, 1992) generation system to
produce the appropriate text. Templates are used
to generate some stock phrases such as ?When you
are ready, go on to the next slide.?
2.5 Dialogue Management
Interaction between components is coordinated by
the dialogue manager which uses the information-
state approach (Larsson and Traum, 2000). The
dialogue state is represented by a cumulative an-
swer analysis which tracks, over multiple turns,
the correct, incorrect, and not-yet-mentioned parts
1Other factors such as student confidence could be con-
sidered as well (Callaway et al, 2007).
of the answer. Once the complete answer has been
accumulated, the system accepts it and moves on.
Tutor hints can contribute parts of the answer to
the cumulative state as well, allowing the system
to jointly construct the solution with the student.
3 Evaluation
The first experimental evaluation involving 81 par-
ticipants (undergraduates recruited from a South-
eastern University in the USA) was completed in
2009. Participants had little or no prior knowledge
of the domain. Each participant took a pre-test,
worked through a lesson with the system, took a
post-test, and completed a user satisfaction survey.
Each session lasted approximately 4 hours.
We implemented two different tutoring policies
in the system for this evaluation. The baseline
policy used an ?accept and bottom out? strategy
for all student answers, regardless of their con-
tent. The students were always given the correct
answer, but the system made no attempt at reme-
diation, and never indicated whether the student
was understood (or correct). In comparison, the
full adaptive policy attempted to select an appro-
priate strategy based on student answer analysis
and dialogue context, as described above. Stu-
dents were randomly assigned to a condition. On
average, each student session contained 232 stu-
dent language turns in the fully adaptive condition
(SD = 25.6) and 156 turns in the baseline condi-
tion (SD = 2.02). Additional time was taken by
reading and interacting with the simulation envi-
ronment.
Out of 81 students, 76 successfully completed
the evaluation (data from 5 participants had to be
discarded due to system crashes). There was no
significant difference in learning gain between the
two conditions (baseline: M = 0.63, SD = 0.26,
full: M = 0.57, SD = 0.23, t-test p = 0.32),
and students gave higher satisfaction scores to the
baseline condition (baseline: M = 3.32, SD =
0.65, full: M = 2.56, SD = 0.65, t-test, p <
0.01). We are currently analyzing the data to in-
vestigate the reasons for this outcome. However,
the evaluation demonstrated that the system can be
successfully deployed in experiments with naive
users and therefore can be used to study the im-
pact of different natural language processing algo-
rithms on learning gain and user satisfaction.
An example dialogue with the system, taken
from the evaluation corpus, is shown in Figure 3.
16
Tutor: Why was bulb A on when switch Y
was open and switch Z was closed?
Student: because it had a closed path
Tutor: Right. There is a closed path. Keep
going.
Student: there was a closed path not effected by
the switch
Tutor: I?m sorry, I?m having a problem un-
derstanding. I don?t understand the
word ?effect? here. Here?s a hint. Your
answer should mention a battery.
Student: it has a closed path with the battery
Tutor: You?re on the right track. Here?s the
answer. Bulb A is still contained in a
closed path with the battery.
Figure 3: Example interaction with the system
from our corpus
It shows three key system properties: after the stu-
dent?s first turn, the system rephrases its under-
standing of the correct part of the student answer
and prompts the student to supply the missing in-
formation. In the second turn, the student utter-
ance could not be interpreted and the system re-
sponds with a targeted help message and a hint
about the object that needs to be mentioned. Fi-
nally, in the last turn the system combines the in-
formation from the tutor?s hint and the student?s
answers and restates the complete answer since the
current answer was completed over multiple turns.
4 Conclusions and Future Work
The BEETLE II system we present was built to
serve as a platform for research in computational
linguistics and tutoring, and can be used for task-
based evaluation of algorithms developed for other
domains. We are currently developing an annota-
tion scheme for the data we collected to identify
student paraphrases of correct answers. The an-
notated data will be used to evaluate the accuracy
of existing paraphrasing and textual entailment ap-
proaches and to investigate how to combine such
algorithms with the current deep linguistic analy-
sis to improve system robustness. We also plan
to annotate the data we collected for evidence of
misunderstandings, i.e., situations where the sys-
tem arrived at an incorrect interpretation of a stu-
dent utterance and took action on it. Such annota-
tion can provide useful input for statistical learn-
ing algorithms to detect and recover from misun-
derstandings.
In dialogue management and generation, the
key issue we are planning to investigate is that of
linguistic alignment. The analysis of the data we
have collected indicates that student satisfaction
may be affected if the system rephrases student
answers using different words (for example, using
better terminology) but doesn?t explicitly explain
the reason why different terminology is needed
(Dzikovska et al, 2010). Results from other sys-
tems show that measures of semantic coherence
between a student and a system were positively as-
sociated with higher learning gain (Ward and Lit-
man, 2006). Using a deep generator to automati-
cally generate system feedback gives us a level of
control over the output and will allow us to devise
experiments to study those issues in more detail.
From the point of view of tutoring research,
we are planning to use the system to answer
questions about the effectiveness of different ap-
proaches to tutoring, and the differences between
human-human and human-computer tutoring. Pre-
vious comparisons of human-human and human-
computer dialogue were limited to systems that
asked short-answer questions (Litman et al, 2006;
Rose? and Torrey, 2005). Having a system that al-
lows more unrestricted language input will pro-
vide a more balanced comparison. We are also
planning experiments that will allow us to eval-
uate the effectiveness of individual strategies im-
plemented in the system by comparing system ver-
sions using different tutoring policies.
Acknowledgments
This work has been supported in part by US Office
of Naval Research grants N000140810043 and
N0001410WX20278. We thank Katherine Harri-
son and Leanne Taylor for their help running the
evaluation.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
James Allen, Myroslava Dzikovska, Mehdi Manshadi,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In Proceedings of the
ACL-07 Workshop on Deep Linguistic Processing.
17
Mark Buckley and Magdalena Wolska. 2007. To-
wards modelling and using common ground in tu-
torial dialogue. In Proceedings of DECALOG, the
2007 Workshop on the Semantics and Pragmatics of
Dialogue, pages 41?48.
Donna K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
Charles B. Callaway, Myroslava Dzikovska, Elaine
Farrow, Manuel Marques-Pita, Colin Matheson, and
Johanna D. Moore. 2007. The Beetle and BeeD-
iff tutoring systems. In Proceedings of SLaTE?07
(Speech and Language Technology in Education).
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting
self-explanations improves understanding. Cogni-
tive Science, 18(3):439?477.
Peter Clark and Bruce Porter, 1999. KM (1.4): Users
Manual. http://www.cs.utexas.edu/users/mfkb/km.
Myroslava O. Dzikovska, Charles B. Callaway, and
Elaine Farrow. 2006. Interpretation and generation
in a knowledge-based tutorial system. In Proceed-
ings of EACL-06 workshop on knowledge and rea-
soning for language processing, Trento, Italy, April.
Myroslava O. Dzikovska, James F. Allen, and Mary D.
Swift. 2008a. Linking semantic and knowledge
representations in a multi-domain dialogue system.
Journal of Logic and Computation, 18(3):405?430.
Myroslava O. Dzikovska, Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008b. Diagnosing natural lan-
guage answers to support adaptive tutoring. In
Proceedings 21st International FLAIRS Conference,
Coconut Grove, Florida, May.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn C. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of SIGDIAL-09, London, UK, Sep.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010. The
impact of interpretation problems on tutorial dia-
logue. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics(ACL-
2010).
Michael Elhadad and Jacques Robin. 1992. Control-
ling content realization with functional unification
grammars. In R. Dale, E. Hovy, D. Ro?sner, and
O. Stock, editors, Proceedings of the Sixth Interna-
tional Workshop on Natural Language Generation,
pages 89?104, Berlin, April. Springer-Verlag.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simula-
tion of a human tutor. Cognitive Systems Research,
1:35?51.
Beth Ann Hockey, Oliver Lemon, Ellen Campana,
Laura Hiatt, Gregory Aist, James Hieronymus,
Alexander Gruenstein, and John Dowding. 2003.
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users? performance.
In Proceedings of the tenth conference on European
chapter of the Association for Computational Lin-
guistics, pages 147?154, Morristown, NJ, USA.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI Dia-
logue Move Engine Toolkit. Natural Language En-
gineering, 6(3-4):323?340.
Diane Litman, Carolyn P. Rose?, Kate Forbes-Riley,
Kurt VanLehn, Dumisizwe Bhembe, and Scott Sil-
liman. 2006. Spoken versus typed human and com-
puter dialogue tutoring. International Journal of Ar-
tificial Intelligence in Education, 16:145?170.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Generalizing tutorial dia-
logue results. In Proceedings of 14th International
Conference on Artificial Intelligence in Education
(AIED), Brighton, UK, July.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings 21st
International FLAIRS Conference, Coconut Grove,
Florida, May.
C.P. Rose? and C. Torrey. 2005. Interactivity versus ex-
pectation: Eliciting learning oriented behavior with
tutorial dialogue systems. In Proceedings of Inter-
act?05.
N. B. Steinhauser, L. A. Butler, and G. E. Campbell.
2007. Simulated tutors in immersive learning envi-
ronments: Empirically-derived design principles. In
Proceedings of the 2007 Interservice/Industry Train-
ing, Simulation and Education Conference, Orlando,
FL.
Kurt VanLehn, Pamela Jordan, and Diane Litman.
2007. Developing pedagogically effective tutorial
dialogue tactics: Experiments and a testbed. In Pro-
ceedings of SLaTE Workshop on Speech and Lan-
guage Technology in Education, Farmington, PA,
October.
Arthur Ward and Diane Litman. 2006. Cohesion and
learning in a tutorial spoken dialog system. In Pro-
ceedings of 19th International FLAIRS (Florida Ar-
tificial Intelligence Research Society) Conference,
Melbourne Beach, FL.
18
Creation of a New Domain and Evaluation of Comparison Generation in a
Natural Language Generation System
Matthew Marge
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
mrmarge@cs.cmu.edu
Amy Isard
ICCS/HCRC
School of Informatics
University of Edinburgh
Amy.Isard@ed.ac.uk
Johanna Moore
ICCS/HCRC
School of Informatics
University of Edinburgh
J.Moore@ed.ac.uk
Abstract
We describe the creation of a new domain for
the Methodius Natural Language Generation
System, and an evaluation of Methodius? pa-
rameterized comparison generation algorithm.
The new domain was based around music and
performers, and texts about the domain were
generated using Methodius. Our evaluation
showed that test subjects learned more from
texts that contained comparisons than from
those that did not. We also established that the
comparison generation algorithm could gener-
alize to the music domain.
1 Introduction
There has been research into tailoring natural lan-
guage to a user?s previous browsing history in a va-
riety of domains such as medicine, museum col-
lections, and animal descriptions (McKeown, 1985;
Milosavljevic, 1997; Dale et al, 1998; O?Donnell
et al, 2001). Another domain in which this could
be applied is automated disc jockeys (DJs) that ac-
company a music stream such as Pandora1 and dis-
cuss interesting trivia or facts about music tracks re-
cently played to the user. User modeling could make
these texts much more natural and less repetitive,
and comparisons and contrasts between music artists
or tracks could also provide users with a novel way
to explore their music collection.
The Methodius system (Isard, 2007) continues
in a line of research which began with ILEX
(O?Donnell et al, 2001) and continued with M-
PIRO (Isard et al, 2003) and now also NaturalOWL
1http://www.pandora.com
(Galanis and Androutsopoulos, 2007). Like these
other systems, Methodius creates customizable de-
scriptions of objects from an database, but it features
a novel algorithm for generating comparisons be-
tween a new object and objects that have previously
been encountered, which stands out from previous
research in this area because it uses several explicit
parameters to choose the most relevant and interest-
ing comparisons given the context (Isard, 2007).
There have been previous evaluations of some of
these systems, including (Cox et al, 1999; Karasi-
mos and Isard, 2004). Karasimos and Isard con-
ducted an evaluation of comparisons and aggrega-
tion in the M-PIRO system. The results showed that
participants learned more and perceived that they
learned more from texts that contained comparisons
and aggregations than they did from texts that did
not. In this study, we investigate whether these re-
sults generalize to our new domain, and we isolate
the effect of comparisons from that of aggregation.
2 Knowledge Base Construction
2.1 Corpus Collection
We collected a small corpus to investigate the type
of facts disc jockeys tend to say about music. We se-
lected two genres where music descriptions between
pieces were common, jazz and classical music. The
programmes we used were broadcast on BBC Ra-
dio Three2. We transcribed sixty-four discussions;
to maintain uniformity, we followed the Linguistic
Data Consortium?s transcription guidelines3. This
2http://www.bbc.co.uk/radio3
3http://projects.ldc.upenn.edu/Transcription/quick-trans
169
was not a thorough corpus collection; the purpose of
collecting examples was to gain a sense of what disc
jockeys tend to discuss and compare.
2.2 Ontology Design
Based on the transcribed examples, we selected and
hand-wrote twelve database entries for music tracks,
using the authoring tool developed by the M-PIRO
project (Androutsopoulos et al, 2007). We trans-
formed the output of this tool into files suitable for
Methodius using an ad-hoc collection of Perl and
XSLT scripts, which also added the necessary infor-
mation to the OpenCCG grammars (White, 2006)
used by Methodius. We discuss future plans in this
area in Section 5.
We created a single-inheritance ontology for a
knowledge base of music pieces. First, we listed
the high-level entity types in the music domain, such
as ?person?, ?instrument?, ?classical music period?,
and ?jazz music period?. We then added attributes
commonly found in our disc jockey transcriptions.
For each entity type, we defined a set of fields. For
example, the classical-period field must contain an
entity which expresses a classical music piece?s time
period. We also specified a microplanning expres-
sion for each field, which provides detail on how the
field?s information should be generated at the sen-
tence level. We then added all the lexical items nec-
essary for the music domain.
2.3 Ontology Population
We populated our domain with six classical music
pieces and six jazz music pieces from the allmu-
sic.com database4. The songs were selected to yield
at least two interesting comparisons when placed in
a specific order. We also added entities linked to the
twelve songs, for example, each song?s album, per-
former, and composer, and information about these
entities. One challenge inherent in selecting these
entities from a publicly available database was to
eliminate as much common knowledge as possi-
ble about the music. In order to decrease back-
ground knowledge as a potential factor in our ex-
periment, we selected songs that primarily did not
contain popular performers, composers, and con-
ductors. We were able to gauge the popularity of
4http://www.allmusic.com
"Avatar" was written by Gary Husband and it was
performed by Billy Cobham, who was influenced
by Miles Davis. Billy Cobham originated from
Panama City, Panama and he played the drums; he
was active from the 1970s to the 1990s and he
participated in the Mahavishnu Orchestra. He
was influenced by Miles Davis. "Avatar" was
written during the Fusion period.
Figure 1: A generated description without comparisons.
Unlike "Fracture" and "A Mystery in Town",
which were written by Eddie "Lockjaw" Davis and
were performed by Fats Navarro, "Avatar" was
written by Gary Husband and it was performed by
Billy Cobham. Cobham originated from Panama
City, Panama and he played the drums; he was
active from the 1970s to the 1990s and he
participated in the Mahavishnu Orchestra. He
was influenced by Miles Davis. "Avatar" was
written during the Fusion period.
Figure 2: A generated description with comparisons to
previously described songs.
artists by their ?popularity rank? in the allmusic.com
database. However, we had to maintain a careful
balance between obscure artists and the ability to
generate interesting comparisons. Obscure artists
had less detailed information in the allmusic.com
database than popular music artists, so were forced
to select a few popular music artists for our exper-
iment, as their music pieces had multiple possible
interesting comparisons.
3 Experiment
We tried to maintain as many conditions from the
previous, similar study (Karasimos and Isard, 2004)
as possible to allow us to directly compare our re-
sults to theirs. The previous study established that
people learned more and perceived that they learned
more from text enriched with comparisons and ag-
gregations of facts than from texts that contained
neither. Our experimental design was similar to
theirs but all conditions of our experiment contained
text generated with aggregations of facts; our aim
was to isolate the effects of comparisons from those
of sentence aggregation.
For jazz texts, comparisons between songs involv-
ing performers, albums, composers, and time peri-
ods were possible. Classical texts could produce
all four of these types of comparisons. In addi-
tion, classical texts could also include comparisons
of conductors. Although the potential similarities
170
for classical and jazz texts were not equal, we de-
cided to include the conductor as a potential com-
parison for classical music. This is because across
both text types, we maintained the same number of
generated comparisons for each text type by limit-
ing Methodius to generating only one comparison
or contrast per paragraph of text. We present exam-
ples of a paragraph of text generated by Methodius
without (Figure 1) and with (Figure 2) comparisons.
In both cases, we assume that the user has already
seen texts about the songs ?Fracture? and ?A Mys-
tery in Town?, which expressed the facts about these
previous songs which are used in the comparisons in
Figure 2; the comparison text does not contain more
new information.
3.1 Evaluation Design
For our user study, we created a web interface using
WebExp2 Experiment Design software5 that con-
tained text generated by Methodius from our music
fact knowledge base. Forty fluent English speak-
ers were recruited and directed to a web page that
gave detailed instructions. After providing some ba-
sic personal information including their name, age,
gender, occupation and native languages, subjects
started with a test page, where they read a sample
paragraph and responded to one factual question, to
make sure that they had understood the interface,
and they then proceeded to the main experiment.
Participants read 6 paragraphs about either jazz
or classical music, and answered 15 factual recall
questions. They then read a further 6 paragraphs
about the other type of music, followed by 15 fac-
tual recall questions on the second set of texts. Fi-
nally they completed a post-experimental survey of
12 Likert Scale questions (Likert, 1932). We used
a within-subjects design, where each subject saw
two sets of texts, one classical and one jazz, one
with and one without comparisons, and the order
in which text sets were presented was controlled.
The multiple choice questions did not change given
the condition; so every participant saw the same
two sets of 15 multiple-choice questions in random-
ized orders. Seven multiple-choice questions of each
fifteen-question set dealt with facts that may be rein-
forced by comparisons. The remaining eight ques-
5http://www.webexp.info
Group Texts with com-
parisons
Texts without
comparisons
A 4.15 (1.814) 3.35 (1.872)
B 4.45 (1.638) 3.10 (1.651)
All 4.30 (1.713) 3.23 (1.747)
Table 1: Mean multiple choice scores with standard devi-
ation in brackets.
tions in each section served as a control for this ex-
periment.
On each page, the interface presented an image of
a paragraph of text generated by Methodius. The
users proceeded to the next paragraph when they
were ready by pressing the ?Next song? or ?Next
piece? button, depending on whether the music type
was jazz or classical. The texts were presented as
images for two reasons: so that the presentation of
stimuli would remain consistent across the differ-
ent computers and to prevent the text from being
selected by the participant, thus discouraging them
from copying the text and placing it into another
window as a reference to answer the factual recall
questions asked later.
4 Results
A summary of the participants? multiple choice
scores are shown in Table 1. Group A read classi-
cal texts with comparisons and jazz texts without,
and Group B read jazz texts with comparisons and
classical texts without.
We performed a 2-way repeated measures
ANOVA on our data and found that participants per-
formed significantly better on questions about the
texts which had comparisons (F (1, 36) = 11.131,
p < .01). There were no ordering or grouping
effects?the performance of participants did not de-
pend on which type of texts they saw first, or on
which type of texts contained comparisons.
In general, the Likert scores showed no signifi-
cant differences between the texts which had com-
parisons and those which did not. Karasimos and
Isard (2004) did find significant differences, but in
their case, texts had either comparisons and sen-
tence aggregations, or neither. In our study, all the
texts had sentence aggregations, so it may be this
factor which contributed to their higher Likert re-
171
sults on questions such as ?I enjoyed reading about
these songs? and the binary ?Which text (quality,
fluency) did you like more? question, for which we
also found no significant difference. Details of re-
sults and statistics can be found in (Marge, 2007).
5 Conclusions and Future Work
We have shown that the Methodius comparison gen-
eration algorithm does generalize to new domains,
and that it is possible to quickly author a new domain
and generate fluent and readable text, using an ap-
propriate authoring tool. We have also confirmed the
findings of previous studies, and showed that the use
of comparisons in texts does significantly improve
participants? recall of the facts which they have read.
In future work, we would like to use the cur-
rent text generation in an automatic DJ system with
streaming music, and perform further user studies in
order to make the texts as interesting and relevant
as possible. We would also like to perform a study
in which we compare the output of the comparison
algorithm using different parameter settings, to see
whether users express a preference.
Since this work was carried out, Methodius has
been adapted to accept ontologies and sentence
plans written in OWL/RDF. These can be created
using the Prote?ge? editor6 with an NLG plugin de-
veloped at the Athens University of Economics and
Business as part of the NaturalOWL generation sys-
tem (Galanis and Androutsopoulos, 2007), which is
available as an open source package7. A more prin-
cipled method for the OpenCCG conversion process
than the one described in Section 2.2 is in develop-
ment, and we hope to publish a paper on this subject.
Acknowledgements
The authors would like to acknowledge the help
and advice given by Colin Matheson, Ellen Bard,
Keith Edwards, Ray Carrick, Frank Keller, and Neil
Mayo and the comments of the anonymous review-
ers. This work was funded in part by a grant from
the Edinburgh-Stanford Link and by the Saint An-
drew?s Society of the State of New York. The music
data in this study was used with the permission of
the All Music Guide.
6http://www.protege.stanford.edu
7http://www.aueb.gr/users/ion/software/NaturalOWL.tar.gz
References
I. Androutsopoulos, J. Oberlander, and V. Karkaletsis.
2007. Source authoring for multilingual generation
of personalised object descriptions. Natural Language
Engineering, 13:191?233.
R. Cox, M. O?Donnell, and J. Oberlander. 1999. Dy-
namic versus static hypermedia in museum education:
an evaluation of ILEX, the intelligent labelling ex-
plorer. In Proceedings of the Artificial Intelligence in
Education conference, Le Mans.
R. Dale, J. Green, M. Milosavljevic, C. Paris, C. Ver-
spoor, and S. Williams. 1998. The realities of gener-
ating natural language from databases. In Proceedings
of the 11th Australian Joint Conference on Artificial
Intelligence, Brisbane, Australia.
D. Galanis and I. Androutsopoulos. 2007. Generating
multilingual descriptions from linguistically annotated
OWL ontologies: the NaturalOWL system. In Pro-
ceedings of ENLG 2007.
A. Isard, J. Oberlander, I. Androutsopoulos, and C. Math-
eson. 2003. Speaking the users? languages. IEEE In-
telligent Systems, 18(1):40?45. Special Issue on Ad-
vances in Natural Language Processing.
A. Isard. 2007. Choosing the best comparison under
the circumstances. In Proceedings of the International
Workshop on Personalization Enhanced Access to Cul-
tural Heritage, Corfu, Greece.
A. Karasimos and A. Isard. 2004. Multi-lingual eval-
uation of a natural language generation system. In
Proceedings of the Fourth International Conference on
Language Resources and Evaluation, Lisbon, Portu-
gal.
R. Likert. 1932. A technique for the measurement of
attitudes. Archives of Psychology, 22(140):1?55.
M. Marge. 2007. An evaluation of comparison genera-
tion in the methodius natural language generation sys-
tem. Master?s thesis, University of Edinburgh.
K. McKeown. 1985. Text Generation: Using Discourse
Strategies and Focus Constraints to Generate Natu-
ral Language Text. Cambridge University Press, New
York, NY, USA.
M. Milosavljevic. 1997. Content selection in compari-
son generation. In 6th European Workshop on Natural
Language Generation), Duisburg, Germany.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: An architecture for a dynamic hypertext
generation system. Natural Language Engineering,
7:225?250.
M.White. 2006. Efficient realization of coordinate struc-
tures in combinatory categorial grammar. Research on
Language and Computation, 4(1):39?75.
172
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 256?264,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Annotating Participant Reference in English Spoken Conversation
John Niekrasz and Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
{jniekras,jmoore}@inf.ed.ac.uk
Abstract
In conversational language, references to
people (especially to the conversation par-
ticipants, e.g., I, you, and we) are an es-
sential part of many expressed meanings.
In most conversational settings, however,
many such expressions have numerous po-
tential meanings, are frequently vague,
and are highly dependent on social and sit-
uational context. This is a significant chal-
lenge to conversational language under-
standing systems ? one which has seen
little attention in annotation studies. In this
paper, we present a method for annotat-
ing verbal reference to people in conver-
sational speech, with a focus on reference
to conversation participants. Our goal is
to provide a resource that tackles the is-
sues of vagueness, ambiguity, and contex-
tual dependency in a nuanced yet reliable
way, with the ultimate aim of supporting
work on summarization and information
extraction for conversation.
1 Introduction
Spoken conversation ? the face-to-face verbal in-
teraction we have every day with colleagues, fam-
ily, and friends ? is the most natural setting for
language use. It is how we learn to use language
and is universal to the world?s societies. This
makes it an ideal subject for research on the ba-
sic nature of language and an essential subject for
the development of technologies supporting natu-
ral communication. In this paper, we describe our
research on designing and applying an annotation
procedure for a problem of particular relevance to
conversational language ? person reference.
The procedure is a coreference annotation of all
references to people, and the focus of our scheme
is on distinguishing different types of participant
reference (references to the conversation?s partic-
ipants), the predominant type of person reference
in face-to-face multi-party conversation. Partici-
pant reference is exemplified by the use of proper
names such as James or most commonly by the
pronouns I, you, and we.
Participant reference plays an essential role in
many of the most important types of expressed
meanings and actions in conversation, including
subjective language, inter-personal agreements,
commitments, narrative story-telling, establishing
social relationships, and meta-discourse. In fact,
some person-referring words are the most frequent
words in conversation.1
Perhaps contrary to intuition, however, in-
terpreting person-referring expressions can be
rather complex. Person-reference interpretation
is strongly dependent on social, situational, and
discourse context. The words you and we
are especially problematic. Either can be used
for generic, plural, or singular reference, as
addressee-inclusive or addressee-exclusive, in ref-
erence to hypothetical individuals or non-human
entities, or even metonymically in reference to ob-
jects connected to individuals (Mu?hlha?usler and
Harre?, 1990; Wales, 1996). In addition, these and
many other issues are not simply occasional prob-
lems but arise regularly.
Consider the following utterance from the AMI
corpus of remote control design meetings, which
is typical of the corpus in terms of complexity of
person-reference.
1The words I and you are the most frequently used nom-
inals in several conversational corpora, including Switch-
board (Godfrey et al, 1992) and the AMI Meeting Cor-
pus (McCowan et al, 2005). In the British National Corpus
they are the two most common of any words in the demo-
graphic (i.e., conversational) subcorpus (Burnard, 2007), and
Google?s Web 1T 5-gram statistics (Brants and Franz, 2006)
list I and you as more frequent even than the word it. The
word we falls within the top 10 most frequent words in all of
these corpora.
256
?Current remote controls do not match well with
the operating behaviour of the user overall. For
example, you can see below there, seventy five
percent of users zap a lot, so you?ve got your
person sunk back in the sofa channel-hopping.?
As this example demonstrates, person-referring
expressions have many potential meanings and are
often vague or non-specific. In this case, ?the
user? refers to a non-specific representative of a
hypothetical group, which is referred to itself as
?users.? The first use of ?you? refers to the ad-
dressees, but the second use has a more ?generic?
meaning whilst retaining an addressee-oriented
meaning as well. The phrase ?your person? refers
to a specific hypothetical example of the ?users?
referred to previously.
1.1 Purpose of the Annotations
The annotation research we describe here aims at
addressing the fact that if conversational language
applications are to be useful and effective (our
interest is primarily with abstractive summariza-
tion), then accurate interpretation of reference to
the conversation?s participants is of critical impor-
tance. Our work looks at language as a means for
action (Clark, 1996), and our focus is on those ac-
tions that the participants themselves consider as
relevant and salient, such as the events occurring
in a meeting that might appear in the minutes of
the meeting. For our system to identify, distin-
guish, or describe such events, it is essential for
it to understand the participants? roles and rela-
tionships to those events through interpreting their
linguistic expression within the dialogue. This in-
cludes understanding direct reference to partici-
pants and recognizing discourse structure through
evidence of referential coherence.
Another aim of our research is to increase un-
derstanding of the nature of participant reference
through presenting a nuanced yet reliable set of
type and property distinctions. We propose novel
distinctions concerning three main issues. The
first distinction concerns vagueness and indetermi-
nacy, which is often exploited by speakers when
using words such as you, they, and we. Our aim
is to provide a reliable basis for making an ex-
plicit distinction between specific and vague uses,
motivated by usefulness to the aforementioned ap-
plications. The second distinction concerns an
issue faced frequently in informal conversation,
where words typically used to do person-referring
are also commonly used in non-person-referring
ways. A principal goal is thus establishing reliable
person/non-person and referential/non-referential
distinctions for these words. The third issue con-
cerns addressing roles (i.e., speaker, addressee,
and non-addressee), which we propose can be a
useful means for further distinguishing between
different types of underspecified and generic refer-
ences, beyond the specific/underspecified/generic
distinctions made in schemes such as ACE (Lin-
guistic Data Consortium, 2008).
1.2 Summary and Scope of Contributions
The work described in this paper includes the de-
sign of an annotation procedure and a statistical
analysis of a corpus of annotations and their re-
liability. The procedure we propose (Section 3)
is based on a simple non-anaphoric coreference-
like scheme, modest in comparison to much pre-
vious work. The produced dataset (Section 4) in-
cludes annotations of 11,000 occasions of person-
referring in recorded workplace meetings. Our
analysis of the dataset includes a statistical sum-
mary of interesting results (Section 4.1) and an
analysis of inter-coder agreement (with discussion
of specific disagreements) for the introduced dis-
tinctions (Section 4.2).
Though our annotation procedure is designed
primarily for multi-party spoken conversation,
some of the central issues that concern us, such
as addressee inclusion and vagueness, arise in
textual and non-conversational settings as well.
Our scheme therefore has relevance to general
work on reference annotation, though principally
to settings where social relationships between
the participants (i.e., speakers/authors and ad-
dressees/readers) are important.
2 Related Annotation Schemes
Previous work on reference annotation has cov-
ered a wide range of issues surrounding reference
generally. It is useful to categorize this work ac-
cording to the natural language processing tasks
the annotations are designed to support.
2.1 Schemes for anaphora and generation
Several schemes have been designed with the goal
of testing linguistic theoretical models of dis-
course structure or for use in the study of discourse
processing problems like anaphora resolution and
reference generation. These schemes have been
applied to both text and dialogue and label dis-
257
course references with a rich set of syntactic, se-
mantic, and pragmatic properties. For example,
the DRAMA scheme (Passonneau, 1997) and the
GNOME scheme (Poesio, 2000; Poesio, 2004) in-
clude labels for features such as bridging relation
type and NP type in addition to a rich representa-
tion of referent semantics. Other schemes label an-
imacy, prosody, and information structure to study
their relationship to the organization and salience
of discourse reference (Nissim et al, 2004; Cal-
houn et al, 2005). Recent developments include
the explicit handling of anaphoric ambiguity and
discourse deixis (Poesio and Artstein, 2008).
Despite the depth and detail of these schemes,
participant reference has not been their main con-
cern. The annotations by Poesio et al (2000;
2004) include dialogue source material, but the
rather constrained interactional situations do not
elicit a rich set of references to participants. The
scheme thus employs simple default labels for
words like I and you. The work by Nissim et
al., (2004) is an annotation of the Switchboard cor-
pus (Godfrey et al, 1992), which contains only
two participants who are neither co-present nor
socially connected. Participant reference is thus
rather constrained. Other than labeling corefer-
entiality, the Nissim scheme includes only a sin-
gle distinction between referential and generic in-
stances of the word you.
2.2 Schemes for information extraction
In contrast to the schemes described above, which
are mainly driven toward investigating linguistic
theories of discourse processing, some reference
annotation projects are motivated instead by infor-
mation extraction applications. For these projects
(which includes our own), a priority is placed on
entity semantics and coreference to known entities
in the world. For example, the objective of the Au-
tomatic Content Extraction (ACE) program (Dod-
dington et al, 2004) is to recognize and extract
entities, events, and relations between them, di-
rectly from written and spoken sources, mostly
from broadcast news. The schemes thus focus
on identifying and labeling the properties of en-
tities in the real world, and then marking expres-
sions as referring to these entities. Recent work
in the ACE project has expanded the scope of
this task to include cross-document recognition
and resolution (Strassel et al, 2008). In the ACE
scheme (Linguistic Data Consortium, 2008), per-
son reference is a central component, and in the
broadcast conversation component of the corpus
there is an extensive inventory of participant refer-
ences. The annotation scheme contains a distinc-
tion between specific, underspecified, and general
entities, as well as a distinction between persons
and organizations.
Another closely related set of studies are four
recent investigations of second-person reference
resolution (Gupta et al, 2007a; Gupta et al,
2007b; Frampton et al, 2009; Purver et al,
2009). These studies are based upon a common
set of annotations of the word you in source mate-
rial from the Switchboard and ICSI Meeting cor-
pora. The purpose for the annotations was to
support learning of classifiers for two main prob-
lems: disambiguation of the generic/referential
distinction, and reference resolution for referential
cases. In addition to the generic/referential dis-
tinction and an addressing-based reference anno-
tation, the scheme employed special classes for re-
ported speech and fillers and allowed annotators to
indicate vague or difficult cases. Our work builds
directly upon this work by extending the annota-
tion scheme to all person-referring expressions.
3 Annotation Method
Our person-reference annotation method consists
of two main phases: a preliminary phase where
the first names of the conversation participants are
identified, and a subsequent person reference la-
beling process. The first phase is not of central
concern in this paper, though we provide a brief
summary below (Section 3.2). The primary focus
of this paper is the second phase (Section 3.3), dur-
ing which every instance of person-referring oc-
curring in a given meeting is labelled. We pro-
vide more detail concerning the most novel and
challenging aspects of the person-referring label-
ing process in Section 3.4 and present a brief sum-
mary of the annotation tool in Section 3.5.
3.1 Source Material
The source material is drawn from two source
corpora: the AMI corpus (McCowan et al,
2005), which contains experimentally-controlled
scenario-driven design meetings, and the ICSI cor-
pus (Janin et al, 2003), which contains naturally
occurring workplace meetings. All the meetings
have at least four participants and have an average
duration of about 45 minutes. In the AMI corpus,
258
the participants are experimental subjects who are
assigned institutional roles, e.g. project manager
and industrial designer. This helps to establish
controlled social relationships within the group,
but generally limits the types of person referring.
The ICSI meetings are naturally occurring and ex-
hibit complex pre-existing social relationships be-
tween the participants. Person referring in this cor-
pus is quite complex and often includes other in-
dividuals from the larger institution and beyond.
3.2 Labeling Participant Names
The first phase of annotation consists of identify-
ing the names of the participants. We perform this
task for every participant in every meeting in the
AMI and ICSI source corpora, which totals 275
unique participants in 246 meetings. Despite the
fact that the participants? are given anonymized
identifiers by the corpus creators, determining par-
ticipants? names is possible because name men-
tions are not excised from the speech transcript.
This allows identification of the names of any par-
ticipants who are referred to by name in the dia-
logue, as long as the referent is disambiguated by
contextual clues such as addressing.
To extract name information, the list of capi-
talized words in the speech transcript is scanned
manually for likely person names. This was done
manually due to the difficulty of training a suffi-
ciently robust named-entity recognizer for these
corpora. Proceeding through each meeting for
which any participant names are yet unidentified,
and taking each potential name token in order
of frequency of occurrence in that meeting, short
segments of the recording surrounding the occur-
rences were replayed. In most cases, the name was
used in reference to a participant and it was clear
from discourse context which participant was the
intended referent. In the AMI meetings, 158 of
223 (71%) of the participants? first names were
identified. In the ICSI meetings, 36 of 52 (69%)
were identified. While these numbers may seem
low, failure to determine a name was generally as-
sociated with a low level of participation of the
individual either in terms of amount of speech or
number of meetings attended. As such, the propor-
tion of utterances across both corpora for which
the speaker?s name is identified is actually 91%.
3.3 Person-reference Annotation
The second, principal phase of annotation con-
sists of annotating person-referring ? instances
of verbal reference to people. The recognition
of person-referring requires the annotator to si-
multaneously identify whether a referring event
has occurred, and whether the referent is a per-
son. In practice, this is divided into four an-
notation steps: markable identification, referent
identification, functional category labeling, and
co-reference linking. For non-specific references,
there is an additional step of labeling addressing
properties. For each meeting, annotators label ev-
ery instance of person-referring in every utterance
in the meeting, performing the steps in sequence
for each utterance. Section 4 describes the set of
meetings annotated. The UML diagram in Fig-
ure 1 depicts the formal data structure produced
by the procedure.2
The first step is markable identification, which
involves recognizing person-referring expres-
sions in the transcript. Only expressions that are
noun phrases are considered, and only the head
noun is actually labeled by the annotator ? the
extent of the expression is not labeled. These iden-
tified head nouns are called markables. Note,
however, that before human annotation begins, an
automatic process identifies occurrences of words
that are likely to be head nouns in person-referring
expressions. The list of words includes all per-
sonal pronouns except it, them, and they (these
are more likely to be non-person-referring in our
dataset) and the wh-pronouns (not labeled in our
scheme). It also includes any occurrences of
the previously identified proper names. Some of
the automatically identified words might not be
person-referring. Also, there may be instances of
person-referring that are not automatically iden-
tified. Annotators do not unmark any of the au-
tomatically identified words, even if they are not
person-referring. The resulting set of manually
and automatically identified words, which may or
may not be person-referring, constitute the com-
plete set of markables.
The second step is the labeling of person refer-
ents. Any people or groups of people that are re-
ferred to specifically and unambiguously (see Sec-
tion 3.4.3 for details) are added by the annotator
to a conversation referent list. The list is auto-
matically populated with each of the conversation
participants.
2The diagram may also be viewed informally as loosely
reflecting a decision tree for the main annotation steps. A
complete coding manual is available from the author?s web
site.
259
ATTR-QUANTIFIED-SUPERSET: Boolean
category: Enum: {
  FUNC-PREF-VOCATIVE,
  FUNC-PREF-INTRODUCTION, 
  FUNC-PREF-TROUBLE, 
  FUNC-PREF-DEFAULT }
Person-Referring Markable
1..*
1
Referent List
*
1
Transcript
*
1
category: Enum: {
  FUNC-FILLER, 
  FUNC-NON-PREF }
Non-Referring Markable
Person Referent
ATTR-SPEAKER-INCL: Boolean
ATTR-ADDRESSEE-INCL: Boolean
ATTR-OTHER-INCL: Boolean
Underspecified Referent (PERSON-OTHER)
id: String
NICKNAME: String
Specific Real Referent
PERSON-SINGLE PERSON-MULTIPLE
2..*
0..*
members
speaker: Participant
word: String
startTime: Double
Word
Conversation
Markable Word Non-Markable Word
Figure 1: A UML diagram depicting the data structure used to represent and store the annotations.
The third step consists of labeling markables
with a functional category (FUNC-*). The func-
tional categories serve two main purposes. They
are used to distinguish person-referring markables
from all others (corresponding to the two main
boxes in the diagram), and they are used to distin-
guish between specific dialogue purposes (the cat-
egories listed within the boxes, see Section 3.4.4).
The final step is to link the markables that were
labeled as person-referring to the appropriate ref-
erent in the referent list. This is only done for
specific and unambiguous referring. Otherwise,
the referent is said to be underspecified, and in-
stead of linking the markable to a referent, it is la-
beled with three binary addressing inclusion at-
tributes. Inclusion attributes label whether the
speaker, addressee, or any other individuals are in-
cluded in the set of people being referred to, given
the social, situational, and discourse context (de-
tails in Section 3.4.5).
3.4 Special Issues
3.4.1 Defining ?person? and ?referring?
To be person-referring, an expression must sat-
isfy two conditions. First, the expression?s pri-
mary contribution to the speaker?s intended mean-
ing or purpose must be either to identify, label,
describe, specify, or address. These are the ba-
sic types of referring. Second, the referent being
identified, labeled, etc., must be a person, which
we define to include any of the following: a dis-
tinct person in the real world; a fictitious or hypo-
thetical person; a human agent, perceiver, or par-
ticipant in a described event, scene, or fact; a class,
type, or kind of person, or representative thereof;
a specification or description of a person or set of
people; a (possibly vaguely defined) group or col-
lection of any of the above; the human race as a
whole, or a representative thereof.
If a noun phrase is used to do person-referring
as defined, the associated markable is labeled with
one of the four person-referring functional cat-
egories (FUNC-PREF-*). If a markable is not
person-referring (either non-referring or referring
to a non-person referent), it is labeled with the
functional category FUNC-NON-PREF. The one
exception to this is the use of a pre-defined list of
common discourse fillers such as you know and I
mean. When used as fillers, these are labeled with
the non-referential FUNC-FILLER category.
260
3.4.2 Joint action and referring ?trouble?
Annotators are asked to consider occasions of re-
ferring to be joint actions between the speaker and
the addressee(s) of the utterance. The annotator
assumes the role of an overhearer and considers
as referring any case where the speaker?s intended
purpose is to refer. If the instance of referring
is not successfully negotiated between the partic-
ipants (i.e., common ground is not achieved), but
the speaker?s intended purpose is to refer, then the
annotator marks this as FUNC-PREF-TROUBLE.
This is used to identify problematic cases for fu-
ture study.
3.4.3 Specific, Unambiguous Referring
Only the referents of specific, unambiguous re-
ferring to a person in the real world (PERSON-
SINGLE) are included in the conversation referent
list and made the subject of coreference annota-
tion. References to more than one such individual
can qualify (PERSON-MULTIPLE), but only if the
members are precisely enumerable and qualify in-
dividually. The motivation for this distinction is to
distinguish references that would be directly use-
ful to applications. Coreference for underspecified
references is not labeled.
3.4.4 Special Functional Categories
Two functional categories are used to distinguish
special uses of person-referring for subsequent
use in speaker name induction (the task of auto-
matically learning participants? names). The two
categories are FUNC-PREF-INTRODUCTION and
FUNC-PREF-VOCATIVE, which specify personal
introductions such as ?Hi, I?m John,? and vocative
addressing such as ?What do you think, Jane??
These categories are used only for proper names.
3.4.5 Addressing-based Inclusion Attributes
A major novelty in our annotation scheme is the
use of addressing-based distinctions for under-
specified referents. Rather than using the labels
?generic? or ?indeterminate?, we employ three bi-
nary attributes (ATTR-*-INCL) that label whether
the speaker, addressee or any other real individuals
are members of the set of people referred to.
The use of this distinction is informed by the no-
tion that addressing distinctions are of central im-
portance to the recognition of joint activity type,
structure, and participation roles. A generic pro-
noun, for example, will often have all three cat-
egories labeled positively. But as an example
of where this scheme creates a novel distinction,
consider the phrase ?You really take a beating
out there on the pitch!?, where the speaker is a
football player describing the nature of play to
someone who has never played the game. This
?generic? use of you, used in an activity of autobi-
ographical description, is intuitively interpreted as
not including the addressee (ATTR-ADDRESSEE-
INCL=FALSE) but including the speaker and others
(ATTR-{SPEAKER,OTHER}-INCL=TRUE). These
distinctions are hard to motivate linguistically yet
critical to identifying useful properties relating to
participation in the communicative activity.
3.4.6 Special or Difficult Cases
In some cases, an annotator can determine that a
reference is specific and unambiguous for the par-
ticipants but the annotator himself is unable to de-
termine the identity of the referent. This is gener-
ally due to a lack of contextual awareness such as
not having adequate video. In such cases, the an-
notator assigns a special REF-UNKNOWN referent.
Other difficult aspects of our annotation proce-
dure are covered in the annotation manual, includ-
ing handling of disfluencies, quantification, and
identifying lexical heads.
3.5 Annotation Tool
The annotations were collected using a software
tool we have designed for discrete event-based an-
notation of multi-modal corpora. The tool uses a
simple, low-latency text-based interface that dis-
plays multiple streams of discrete events in tempo-
ral order across the screen. In our case, the events
are time-synchronized words that are distributed
to different rows according to speaker. The inter-
face allows keyboard input only and is synchro-
nized with the MPlayer playback engine.
4 Results and Analysis
4.1 Statistical summary
The dataset consists of approximately 11,000 in-
dividually annotated referring expressions in 16
experimentally-controlled, scenario-driven design
meetings from the AMI corpus (McCowan et al,
2005) and 3 natural workplace meetings from
the ICSI corpus (Janin et al, 2003). Figure 2
shows, for each grammatical type of referring ex-
pression, the frequency of occurrence of the five
principal markable types, which are defined to
consist of the two non-person-referring functional
261
OTHER
QUANT
3PP
3PS
2P
1PP
1PS
FUNC?PREF / PERSON?SINGLE
FUNC?PREF / PERSON?MULTIPLE
FUNC?PREF / PERSON?OTHER
FUNC?NON?PREF
FUNC?FILLER
0 500 1000 2000 3000
Figure 2: Frequency of occurrence of referring
types for the whole corpus, by grammatical type
of the referring expression.
categories (FUNC-NON-PREF and FUNC-FILLER),
and a breakdown of person-referring according
to the type of person referent: a specific indi-
vidual (PERSON-SINGLE), multiple specific indi-
viduals (PERSON-MULTIPLE), or underspecified
(PERSON-OTHER). The grammatical types in-
clude a grouping of the personal pronouns by
grammatical person and number (1PS, 1PP, 2P,
3PS, 3PP), the quantified pronouns (QUANT), and
a group including all other expressions (OTHER).
Table 1 shows the relative frequency for the gram-
matical types and the most frequent expressions.
As is usually found in conversation, first-
person and second-person pronouns are the most
frequent, collectively comprising 82.0% of all
person-referring expressions. Of particular inter-
est, due to their high frequency and multiple possi-
ble referential meanings, are the 1PP and 2P cate-
gories (e.g., we and you), comprising respectively
24.6% and 23.7% of all person-referring expres-
Gram. Freq. Ent. Freq. words
(%) (bits)
1PS 33.7 .57 I, my, me
1PP 24.6 .67 we, our, us
2P 23.7 1.78 you, your, yours
3PS .9 .66 he, his, she
3PP 7.2 1.25 they, them, their
QUANT 1.0 1.14 everyone, everybody
OTHER 8.9 1.57 people, guys, user
Table 1: A statistical summary of all the mark-
ables in the dataset by grammatical type (gram.),
showing their frequency relative to all markables
(freq.), the entropy of the referring type given the
grammatical type (ent.), and a list of the most fre-
quent examples (freq. words).
sions. In Table 1, we show the information en-
tropy of the referring type, given the grammati-
cal category. This measures the uncertainty one
has about the type, given knowledge of only the
grammatical type of the expression. The analysis
reveals that second-person pronouns are a partic-
ularly challenging reference resolution problem,
with a broad and relatively even distribution across
referring types.
4.2 Reliability and Error Analysis
To show that our annotations are credible and suit-
able for empirical testing, we must establish that
the subjective distinctions defined in our scheme
may be applied by individuals other than the
scheme developers. To do this, we assess inter-
coder agreement between two independent anno-
tators on four meetings from the AMI corpus, us-
ing Cohen?s Kappa (Cohen, 1960). Each of the
decisions in the annotation procedure are assessed
separately: markable identification, labeling ref-
erentiality, labeling specificity of person refer-
ents, and labeling addressing inclusion attributes.
Because each decision depends on the previous,
we employ a hierarchical assessment procedure
that considers only instances where the annota-
tors have agreed on previous decisions. This kind
of multi-level assessment corresponds to that de-
scribed and used in Carletta et al, (1997).
Markables The first annotation decision of in-
terest is the identification of markables. Markables
are either automatically identified occurrences of
a pre-defined list of pronouns, or they are identi-
262
fied manually by the annotators. Agreement on
this task, assessed only for manually identified
words, was very good (?=.94). Error analysis
shows that the main issue with this decision was
not determining lexical heads, but rather deter-
mining whether phrases such as ?all age groups,?
?the older generation,? and ?the business market?
should be considered as referring to people or not.
Person referentiality The next annotation deci-
sion is between person-referring and non-person-
referring markables. For assessment of this
choice, we measure agreement on a three-way
categorization of the agreed markables as either
FUNC-NON-PREF, FUNC-FILLER, or one of the
FUNC-PREF-* categories. Agreement on this task
was good (?=.77). The only errors occurred on
first- and second-person pronouns and between the
FUNC-NON-PREF and FUNC-PREF-* categories.
Error analysis suggests confusion tends to occur
when pronouns are used with semantically light
verbs like go, get, and have, for example in phrases
such as ?there we go? and ?you?ve got the main
things on the front.? As in the latter example,
some of the difficult choices appear to involve de-
scriptions of states, which the speaker can choose
to express either from various participants? points
of view, as above, or alternatively without ex-
plicit subjectivity, e.g., ?the main things are on the
front.?
Specificity and cardinality The next choice we
assess is the decision between referring specif-
ically to a single person (PERSON-SINGLE), to
multiple people (PERSON-MULTIPLE), or as un-
derspecified (also referred to as PERSON-OTHER).
Agreement on this choice was very good (?=.91),
though considering only the difficult 1PP and 2P
grammatical categories (e.g., we and you), agree-
ment was less strong (?=.75). Note that due to the
hierarchical nature of the scheme, evaluation con-
sidered only cases where both annotators labeled
a word as person-referring. Errors on this decision
often involved ambiguities in addressing, where
one annotator believed a particular individual was
being addressed by you and the other thought the
whole group was being addressed. Another com-
mon disagreement was on cases such as ?we want
it to be original,? where we was interpreted by one
annotator as referring to the present group of par-
ticipants, but by the other as (presumably) refer-
ring to the organization to which the participants
belong.
Addressing inclusion attributes For the three
inclusion attributes for underspecified referents
(ATTR-*-INCL), agreement is calculated three
times, once for each of the binary attributes.
Agreement was good, though slightly problematic
for addressee inclusion (speaker ?=.72; addressee
?=.50; other ?=.66). Disagreements were mainly
for occurrences of you like the example of autobi-
ography in Section 3.4.5. For example, ?it?s your
best friend? was used to explain why a dog is the
speaker?s favorite animal, and the annotators dis-
agreed on whether the addressee was included.
5 Conclusion
We have presented an annotation scheme and a
set of annotations that address participant refer-
ence ? a conversational language problem that
has seen little previous annotation work. Our fo-
cus has been on eliciting novel distinctions that we
hypothesize will help us to distinguish, label, and
summarize conversational activities. We also ad-
dress the issues of vagueness, ambiguity, and con-
textual dependency in participant referring.
Based on analysis of inter-annotator agreement,
the major distinctions proposed by the scheme ap-
pear to be reliably codable. In addition, our sta-
tistical analysis shows that our dataset contains a
wide variety of participant references and should
be a useful resource for several reference resolu-
tion problems for conversation. Our novel method
for distinguishing specific reference to real indi-
viduals appears to be very reliably codable. Our
novel addressing-based distinctions for underspec-
ified reference are less reliable but adequate as a
resource for some dialogue structuring tasks.
Further work proposed for this task includes
labeling a variety of conversational and non-
conversation genres. Our immediate concern is to
apply our annotations in the training and/or test-
ing of machine learning approaches to discourse
segmentation and abstractive summarization.
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram, Version 1. Linguistic Data Consortium. Cat-
alog ID: LDC2006T13.
Lou Burnard, 2007. Reference Guide for the British
National Corpus (XML Edition). Research Tech-
nologies Service at Oxford University Computing
Services.
263
Sasha Calhoun, Malvina Nissim, Mark Steedman, and
Jason Brenier. 2005. A framework for annotating
information structure in discourse. In Proceedings
of the ACL Workshop on Frontiers in Corpus Anno-
tation II: Pie in the Sky.
Jean Carletta, Stephen Isard, Anne H. Anderson,
Gwyneth Doherty-Sneddon, Amy Isard, and Jacque-
line C. Kowtko. 1997. The reliability of a dialogue
structure coding scheme. Computational Linguis-
tics, 23(1):13?31.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37?46.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program: Tasks, data, and evaluation. In
Proc. LREC.
Matthew Frampton, Raquel Fernndez, Patrick Ehlen,
Mario Christoudias, Trevor Darrell, and Stanley Pe-
ters. 2009. Who is ?you?? Combining linguis-
tic and gaze features to resolve second-person ref-
erences in dialogue. In Proc. EACL.
John J. Godfrey, Edward Holliman, and J. McDaniel.
1992. SWITCHBOARD: Telephone speech corpus
for research and development. In Proc. ICASSP,
pages 517?520, San Francisco, CA.
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007a. Resolving ?you? in multi-
party dialog. In Proc. SIGdial, pages 227?230.
Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.
2007b. Disambiguating between generic and refer-
ential ?you? in dialog. In Proc. ACL.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting cor-
pus. In Proc. ICASSP, volume 1, pages 364?367.
Linguistic Data Consortium, 2008. ACE (Au-
tomatic Content Extraction) English Annotation
Guidelines for Entities, Version 6.5. Down-
loaded from http://projects.ldc.upenn.
edu/ace/annotation/.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-
coln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Cor-
pus. In Proceedings of Measuring Behavior 2005,
the 5th International Conference on Methods and
Techniques in Behavioral Research, Wageningen,
Netherlands.
Peter Mu?hlha?usler and Rom Harre?. 1990. Pronouns
and People: The Linguistic Construction of Social
and Personal Identity. Blackwell, Oxford.
Malvina Nissim, Shipra Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for
information status in dialogue. In Proc. LREC.
R. Passonneau, 1997. Instructions for applying dis-
course reference annotation for multiple applica-
tions (DRAMA).
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proc. LREC.
Massimo Poesio, 2000. The GNOME Annotation
Scheme Manual, Version 4. University of Edin-
burgh, HCRC and Informatics.
Massimo Poesio. 2004. Discourse annotation and se-
mantic annotation in the GNOME corpus. In Pro-
ceedings of the ACL 2004 Workshop on Discourse
Annotation, pages 72?79.
Matthew Purver, Raquel Fernndez, Matthew Framp-
ton, and Stanley Peters. 2009. Cascaded lexicalised
classifiers for second-person reference resolution.
In Proc. SIGdial, pages 306?309.
Stephanie Strassel, Mark Przybocki, Kay Peterson,
Zhiyi Song, and Kazuaki Maeda1. 2008. Linguistic
resources and evaluation techniques for evaluation
of cross-document automatic content extraction. In
Proc. LREC.
Katie Wales. 1996. Personal pronouns in present-day
English. Cambridge University Press, Cambridge.
264
Report on the Second NLG Challenge on
Generating Instructions in Virtual Environments (GIVE-2)
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Andrew Gargett
Saarland University
gargett@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the second installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE-2), a shared
task for the NLG community which took
place in 2009-10. We evaluated seven
NLG systems by connecting them to 1825
users over the Internet, and report the re-
sults of this evaluation in terms of objec-
tive and subjective measures.
1 Introduction
This paper reports on the methodology and results
of the Second Challenge on Generating Instruc-
tions in Virtual Environments (GIVE-2), which
we ran from August 2009 to May 2010. GIVE
is a shared task for the NLG community which
we ran for the first time in 2008-09 (Koller et al,
2010). An NLG system in this task must generate
instructions which guide a human user in solving
a treasure-hunt task in a virtual 3D world, in real
time. For the evaluation, we connect these NLG
systems to users over the Internet, which makes
it possible to collect large amounts of evaluation
data cheaply.
While the GIVE-1 challenge was a success, in
that it evaluated five NLG systems on data from
1143 game runs in the virtual environments, it
was limited in that users could only move and
turn in discrete steps in the virtual environments.
This made the NLG task easier than intended; one
of the best-performing GIVE-1 systems generated
instructions of the form ?move three steps for-
ward?. The primary change in GIVE-2 compared
to GIVE-1 is that users could now move and turn
freely, which makes expressions like ?three steps?
meaningless, and makes it hard to predict the pre-
cise effect of instructing a user to ?turn left?.
We evaluated seven NLG systems from six in-
stitutions in GIVE-2 over a period of three months
from February to May 2010. During this time,
we collected 1825 games that were played by
users from 39 countries, which is an increase of
over 50% over the data we collected in GIVE-
1. We evaluated each system both on objec-
tive measures (success rate, completion time, etc.)
and subjective measures which were collected by
asking the users to fill in a questionnaire. We
completely revised the questionnaire for the sec-
ond challenge, which now consists of relatively
fine-grained questions that can be combined into
more high-level groups for reporting. We also in-
troduced several new objective measures, includ-
ing the point in the game in which users lost
or cancelled, and an experimental ?back-to-base?
task intended to measure how much users learned
about the virtual world while interacting with the
NLG system.
Plan of the paper. The paper is structured as fol-
lows. In Section 2, we describe and motivate the
GIVE-2 Challenge. In section 3, we describe the
evaluation method and infrastructure. Section 4
reports on the evaluation results. Finally, we con-
clude and discuss future work in Section 5.
2 The GIVE Challenge
GIVE-2 is the second installment of the GIVE
Challenge (?Generating Instructions in Virtual En-
vironments?), which we ran for the first time in
2008-09. In the GIVE scenario, subjects try to
solve a treasure hunt in a virtual 3Dworld that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual world.
The challenge for the NLG system is to gener-
ate, in real time, natural-language instructions that
will guide the users to the successful completion
of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
Figure 1: What the user sees when playing with
the GIVE Challenge.
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
In each game world we used in GIVE-2, players
must pick up a trophy, which is in a wall safe be-
hind a picture. In order to access the trophy, they
must first push a button to move the picture to the
side, and then push another sequence of buttons to
open the safe. One floor tile is alarmed, and play-
ers lose the game if they step on this tile without
deactivating the alarm first. There are also a num-
ber of distractor buttons which either do nothing
when pressed or set off an alarm. These distractor
buttons are intended to make the game harder and,
more importantly, to require appropriate reference
to objects in the game world. Finally, game worlds
contained a number of objects such as chairs and
flowers that did not bear on the task, but were
available for use as landmarks in spatial descrip-
tions generated by the NLG systems.
The crucial difference between this task and
the (very similar) GIVE-1 task was that in GIVE-
2, players could move and turn freely in the vir-
tual world. This is in contrast to GIVE-1, where
players could only turn by 90 degree increments,
and jump forward and backward by discrete steps.
This feature of the way the game controls were set
up made it possible for some systems to do very
well in GIVE-1 with only minimal intelligence,
using exclusively instructions such as ?turn right?
and ?move three steps forward?. Such instructions
are unrealistic ? they could not be carried over to
instruction-giving in the real world ?, and our aim
was to make GIVE harder for systems that relied
on them.
3 Method
Following the approach from the GIVE-1 Chal-
lenge (Koller et al, 2010), we connected the NLG
systems to users over the Internet. In each game
run, one user and one NLG system were paired up,
with the system trying to guide the user to success
in a specific game world.
3.1 Software infrastructure
We adapted the GIVE-1 software to the GIVE-2
setting. The GIVE software infrastructure (Koller
et al, 2009a) consists of three different mod-
ules: The client, which is the program which the
user runs on their machine to interact with the
virtual world (see Fig. 1); a collection of NLG
servers, which generate instructions in real-time
and send them to the client; and a matchmaker,
which chooses a random NLG server and virtual
world for each incoming connection from a client
and stores the game results in a database.
The most visible change compared to GIVE-1
was to modify the client so it permitted free move-
ment in the virtual world. This change further ne-
cessitated a number of modifications to the inter-
nal representation of the world. To support the de-
velopment of virtual worlds for GIVE, we changed
the file format for world descriptions to be much
more readable, and provided an automatic tool
for displaying virtual worlds graphically (see the
screenshots in Fig. 2).
3.2 Recruiting subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the Internet
and in traditional newspapers. We further adver-
tised GIVE at the Cebit computer expo as part of
the Saarland University booth. Recruiting anony-
mous experimental subjects over the Internet car-
ries known risks (Gosling et al, 2004), but we
showed in GIVE-1 that the results obtained for
the GIVE Challenge are comparable and more in-
formative than those obtained from a laboratory-
World 1 World 2 World 3
Figure 2: The three GIVE-2 evaluation worlds.
based experiment (Koller et al, 2009b).
We also tried to leverage social networks for re-
cruiting participants by implementing and adver-
tising a Facebook application. Because of a soft-
ware bug, only about 50 participants could be re-
cruited in this way. Thus tapping the true poten-
tial of social networks for recruiting participants
remains a task for the next installment of GIVE.
3.3 Evaluation worlds
Fig. 2 shows the three virtual worlds we used in the
GIVE-2 evaluation. Overall, the worlds were more
difficult than the worlds used in GIVE-1, where
some NLG-systems had success rates around 80%
in some of the worlds. As for GIVE-1, the three
worlds were designed to pose different challenges
to the NLG systems. World 1 was intended to be
more similar to the development world and last
year?s worlds. It did have rooms with more than
one button of the same color, however, these but-
tons were not located close together. World 2 con-
tained several situations which required more so-
phisticated referring expressions, such as rooms
with several buttons of the same color (some of
them close together) and a grid of buttons. Fi-
nally, World 3 was designed to exercise the sys-
tems? navigation instructions: one room contained
a ?maze? of alarm tiles, and another room two
long rows of buttons hidden in ?booths? so that
they were not all visible at the same time.
3.4 Timeline
After the GIVE-2 Challenge was publicized in
June 2009, fifteen researchers and research teams
declared their interest in participating. We dis-
tributed a first version of the software to these
teams in August 2009. In the end, six teams sub-
mitted NLG systems (two more than in GIVE-1);
one team submitted two independent NLG sys-
tems, bringing the total number of NLG systems
up to seven (two more than in GIVE-1). These
were connected to a central matchmaker that ran
for a bit under three months, from 23 February to
17 May 2010.
3.5 NLG systems
Seven NLG systems were evaluated in GIVE-2:
? one system from the Dublin Institute of Tech-
nology (?D? in the discussion below);
? one system from Trinity College Dublin
(?T?);
? one system from the Universidad Com-
plutense de Madrid (?M?);
? one system from the University of Heidelberg
(?H?);
? one system from Saarland University (?S?);
? and two systems from INRIA Grand-Est in
Nancy (?NA? and ?NM?).
Detailed descriptions of these systems as well
as each team?s own analysis of the evalua-
tion results can be found at http://www.
give-challenge.org/research.
4 Results
We now report the results of GIVE-2. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures. The
data for the objective measures are extracted from
the logs of the interactions; whereas the data for
the subjective measures are obtained from a ques-
tionnaire which asked subjects to rate various as-
pects of the NLG system they interacted with.
Notice that some of our evaluation measures are
in tension with each other: For instance, a sys-
tem which gives very low-level instructions may
allow the user to complete the task more quickly
(there is less chance of user errors), but it will re-
quire more instructions than a system that aggre-
gates these. This is intentional, and emphasizes
our desire to make GIVE a friendly comparative
challenge rather than a competition with a clear
winner.
4.1 Demographics
Over the course of three months, we collected
1825 valid games. This is an increase of almost
60% over the number of valid games we collected
in GIVE-1. A game counted as valid if the game
client did not crash, the game was not marked as a
test game by the developers, and the player com-
pleted the tutorial.
Of these games, 79.0% were played by males
and 9.6% by females; a further 11.4% did not
specify their gender. These numbers are compa-
rable to GIVE-1. About 42% of users connected
from an IP address in Germany; 12% from the US,
8% from France, 6% from Great Britain, and the
rest from 35 further countries. About 91% of the
participants who answered the question self-rated
their English language proficiency as ?good? or
better. About 65% of users connected from vari-
ous versions of Windows, the rest were split about
evenly between Linux and MacOS.
4.2 Objective measures
The objective measures are summarize in Fig. 3.
In addition to calculating the percentage of games
users completed successfully when being guided
by the different systems, we measured the time
until task completion, the distance traveled until
task completion, and the number of actions (such
as pushing a button to open a door) executed. Fur-
thermore, we counted howmany instructions users
received from each system, and how many words
these instructions contained on average. All objec-
tive measures were collected completely unobtru-
sively, without requiring any action on the user?s
part. To ensure comparability, we only counted
successfully completed games.
task success: Did the player get the trophy?
duration: Time in seconds from the end of the tu-
torial until the retrieval of the trophy.
distance: Distance traveled (measured in distance
units of the virtual environment).
actions: Number of object manipulation actions.
instructions: Number of instructions produced
by the NLG system.
words per instruction: Average number of
words the NLG system used per instruction.
Figure 3: Objective measures.
Fig. 4 shows the results of these objective mea-
sures. Task success is reported as the percent-
age of successfully completed games. The other
measures are reported as the mean number of sec-
onds/distance units/actions/instructions/words per
instruction, respectively. The figure also assigns
systems to groups A, B, etc. for each evaluation
measure. For example, users interacting with sys-
tems in group A had a higher task success rate,
needed less time, etc. than users interacting with
systems in group B. If two systems do not share
the same letter, the difference between these two
systems is significant with p < 0.05. Significance
was tested using a ?2-test for task success and
ANOVAs for the other objective measures. These
were followed by post-hoc tests (pairwise ?2 and
Tukey) to compare the NLG systems pairwise.
In terms of task success, the systems fall pretty
neatly into four groups. Note that systems D and
T had very low task success rates. That means
that, for these systems, the results for the other ob-
jective measures may not be reliable because they
are based on just a handful of games. Another
aspect in which systems clearly differed is how
many words they used per instruction. Interest-
ingly, the three systems with the best task success
rates also produced the most succinct instructions.
The distinctions between systems in terms of the
other measures is less clear.
4.3 Subjective measures
The subjective measures were obtained from re-
sponses to a questionnaire that was presented to
users after each game. The questionnaire asked
users to rate different statements about the NLG
D H M NA NM S T
task
success
9% 11% 13% 47% 30% 40% 3%
A A
B
C C C
D D
duration
888 470 407 344 435 467 266
A A A A A
B B B B B
C
distance
231 164 126 162 167 150 89
A A A A A A
B B B B B
actions
25 22 17 17 18 17 14
A A A A A A A
instructions
349 209 463 224 244 244 78
A A A A A A
B B
words per
instruction
15 11 16 6 10 6 18
A A
B
C
D
E E
Figure 4: Results for the objective measures.
system using a continuous slider. The slider posi-
tion was translated to a number between -100 and
100. Figs. 7 and 6 show the statements that users
were asked to rate as well as the results. These
results are based on all games, independent of the
success. We report the mean rating for each item,
and, as before, systems that do not share a letter,
were found to be significantly different (p< 0.05).
We used ANOVAs and post-hoc Tukey tests to test
for significance. Note that some items make a pos-
itive statement about the NLG system (e.g., Q1)
and some make a negative statement (e.g., Q2).
For negative statements, we report the reversed
scores, so that in Figs. 7 and 6 greater numbers are
always better, and systems in group A are always
better than systems in group B.
In addition to the items Q1?Q22, the ques-
tionnaire contained a statement about the over-
all instruction quality: ?Overall, the system gave
me good directions.? Furthermore notice that the
other items fall into two categories: items that as-
sess the quality of the instructions (Q1?Q15) and
items that assess the emotional affect of the in-
teraction (Q16?Q22). The ratings in these cate-
D H M NA NM S T
overall
quality
question
-33 -18 -12 36 18 19 -25
A
B B
C C C C
quality
measures
(summed)
-183 -148 -18 373 239 206 -44
A A A
B B B B
emotional
affect
measures
(summed)
-130 -103 -90 20 -5 0 -88
A A A A
B B B B B
C C C C C
Figure 5: Results for item assessing overall in-
struction quality and the aggregated quality and
emotional affect measures.
gories can be aggregated into just two ratings by
summing over them. Fig. 5 shows the results for
the overall question and the aggregated ratings for
quality measures and emotional affect measures.
The three systems with the highest task success
rate get rated highest for overall instruction qual-
ity. The aggregated quality measure also singles
out the same group of three systems.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, some other factors also influence the out-
comes of our objective and subjective measures.
As in GIVE-1, we find that there is a significant
difference in task success rate for different evalua-
tion worlds and between users with different levels
of English proficiency. Fig. 8 illustrates the effect
of the different evaluation worlds on the task suc-
cess rate for different systems, and Fig. 9 shows
the effect that a player?s English skills have on the
task success rate. As in GIVE-1, some systems
seem to be more robust than others with respect to
changes in these factors.
None of the other factors we looked at (gender,
age, and computer expertise) have a significant ef-
fect on the task success rate. With a few excep-
tions the other objective measures were not influ-
enced by these demographic factors either. How-
ever, we do find a significant effect of age on the
time and number of actions a player needs to re-
trieve the trophy: younger players are faster and
need fewer actions. And we find that women travel
a significantly shorter distance than men on their
way to the trophy. Interestingly, we do not find
D H M NA NM S T
Q1: The system used words and phrases
that were easy to understand.
45 26 41 62 54 58 46
A A A A
B B B B
C C C
Q2: I had to re-read instructions to under-
stand what I needed to do.
-26 -9 3 40 8 19 0
A
B B B B
C C C
D D
Q3: The system gave me useful feedback
about my progress.
-17 -30 -31 9 11 -13 -27
A A
B B B B
C C C C
Q4: I was confused about what to do next.
-35 -27 -18 29 9 5 -31
A
B B
C C C C
Q5: I was confused about which direction
to go in.
-32 -20 -16 21 8 3 -25
A A
B B
C C C C
Q6: I had no difficulty with identifying
the objects the system described for me.
-21 -11 -5 18 13 20 -21
A A A
B B
C C C C
Q7: The system gave me a lot of unnec-
essary information.
-22 -9 6 15 10 10 -6
A A A A
B B B B
C C C
D D D
D H M NA NM S T
Q8: The system gave me too much infor-
mation all at once.
-28 -8 9 31 8 21 15
A A A
B B B B
C C
Q9: The system immediately offered help
when I was in trouble.
-15 -13 -13 32 3 -5 -23
A
B B B B B
C C C C
Q10: The system sent instructions too
late.
15 15 9 38 39 14 8
A A
B B B B B
Q11: The system?s instructions were de-
livered too early.
15 5 21 39 12 30 28
A A A
B B B B
C C C C
D D D D
Q12: The system?s instructions were vis-
ible long enough for me to read them.
-67 -21 -19 6 -14 0 -18
A A
B B B
C C C C
D
Q13: The system?s instructions were
clearly worded.
-20 -9 1 32 23 26 6
A A A
B B B
C C C
D D
Q14: The system?s instructions sounded
robotic.
16 -6 8 -4 -1 5 1
A A A A A A
B B B B B B
Q15: The system?s instructions were
repetitive.
-28 -26 -11 -31 -28 -26 -23
A A A A A
B B B B B B
Figure 7: Results for the subjective measures assessing the quality of the instructions.
D H M NA NM S T
Q16: I really wanted to find that trophy.
-10 -13 -9 -11 -8 -7 -12
A A A A A A A
Q17: I lost track of time while solving the
overall task.
-13 -18 -21 -16 -18 -11 -20
A A A A A A A
Q18: I enjoyed solving the overall task.
-21 -23 -20 -8 -4 -5 -21
A A A A A A
B B B B B
Q19: Interacting with the system was re-
ally annoying.
-14 -20 -12 8 -2 -2 -14
A A A
B B B B B
C C C C
Q20: I would recommend this game to a
friend.
-36 -39 -31 -30 -25 -24 -31
A A A A A A A
Q21: The system was very friendly.
0 -1 5 30 20 19 5
A A A
B B B B
C C C C
D D D D
Q22: I felt I could trust the system?s in-
structions.
-21 -6 -3 37 23 21 -13
A A A
B B B B
Figure 6: Results for the subjective measures as-
sessing the emotional affect of the instructions.
Figure 8: Effect of the evaluation worlds on the
success rate of the NLG systems.
Figure 9: Effect of the players? English skills on
the success rate of the NLG systems.
a significant effect of gender on the time players
need to retrieve the trophy as in GIVE-1 (although
the mean duration is somewhat higher for female
than for male players; 481 vs. 438 seconds).
5 Conclusion
In this paper, we have described the setup and re-
sults of the Second GIVE Challenge. Altogether,
we collected 1825 valid games for seven NLG sys-
tems over a period of three months. Given that this
is a 50% increase over GIVE-1, we feel that this
further justifies our basic experimental methodol-
ogy. As we are writing this, we are preparing de-
tailed results and analyses for each participating
team, which we hope will help them understand
and improve the performance of their systems.
The success rate is substantially worse in GIVE-
2 than in GIVE-1. This is probably due to the
Figure 10: Points at which players lose/cancel.
harder task (free movement) explained in Sec-
tion 2 and to the more complex evaluation worlds
(see Section 3.3). It was our intention to make
GIVE-2 more difficult, although we did not antic-
ipate such a dramatic drop in performance. GIVE-
2.5 next year will use the same task as GIVE-2 and
we hope to see an increase in task success as the
participating research teams learn from this year?s
results.
It is also noticeable that players gave mostly
negative ratings in response to statements about
immersion and engagement (Q16-Q20). We dis-
cussed last year how to make the task more engag-
ing on the one hand and how to manage expecta-
tions on the other hand, but none of the suggested
solutions ended up being implemented. It seems
that we need to revisit this issue.
Another indication that the task may not be able
to capture participants is that the vast majority of
cancelled and lost games end in the very begin-
ning. To analyze at what point players lose or give
up, we divide the game into phases demarcated
by manipulations of buttons that belong to the 6-
button safe sequence. Fig. 10 illustrates in which
phase of the game players lose or cancel.
We are currently preparing the GIVE-2.5 Chal-
lenge, which will take place in 2010-11. GIVE-2.5
will be very similar to GIVE-2, so that GIVE-2
systems will be able to participate with only mi-
nor changes. In order to support the development
of GIVE-2.5 systems, we have collected a multi-
lingual corpus of written English and German in-
structions in the GIVE-2 environment (Gargett et
al., 2010). We expect that GIVE-3 will then extend
the GIVE task substantially, perhaps in the direc-
tion of full dialogue or of multimodal interaction.
Acknowledgments. GIVE-2 was only possible
through the support and hard work of a number of
colleagues, especially Konstantina Garoufi (who
handled the website and other publicity-related is-
sues), Ielka van der Sluis (who contributed to the
design of the GIVE-2 questionnaire), and several
student assistants who programmed parts of the
GIVE-2 system. We thank the press offices of
Saarland University, the University of Edinburgh,
and Macquarie University for their helpful press
releases. We also thank the organizers of Gener-
ation Challenges 2010 and INLG 2010 for their
support and the opportunity to present our results,
and the seven participating research teams for their
contributions.
References
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The GIVE-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), Malta.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009a. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jo-
hanna Moore, and Jon Oberlander. 2009b. Validat-
ing the web-based evaluation of NLG systems. In
Proceedings of ACL-IJCNLP 2009 (Short Papers),
Singapore.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2010. The first challenge on
generating instructions in virtual environments. In
E. Krahmer and M. Theune, editors, Empirical
Methods in Natural Language Generation, volume
5790 of LNCS, pages 337?361. Springer.
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 103?106,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Learning Dialogue Strategies from Older and Younger Simulated Users
Kallirroi Georgila
Institute for Creative Technologies
University of Southern California
Playa Vista, USA
kgeorgila@ict.usc.edu
Maria K. Wolters
School of Informatics
University of Edinburgh
Edinburgh, UK
maria.wolters@ed.ac.uk
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, UK
J.Moore@ed.ac.uk
Abstract
Older adults are a challenging user group
because their behaviour can be highly vari-
able. To the best of our knowledge, this
is the first study where dialogue strategies
are learned and evaluated with both sim-
ulated younger users and simulated older
users. The simulated users were derived
from a corpus of interactions with a strict
system-initiative spoken dialogue system
(SDS). Learning from simulated younger
users leads to a policy which is close to
one of the dialogue strategies of the under-
lying SDS, while the simulated older users
allow us to learn more flexible dialogue
strategies that accommodate mixed initia-
tive. We conclude that simulated users are
a useful technique for modelling the be-
haviour of new user groups.
1 Introduction
State-of-the-art statistical approaches to dia-
logue management (Frampton and Lemon, 2006;
Williams and Young, 2007) rely on having ade-
quate training data. Dialogue strategies are typ-
ically inferred from data using Reinforcement
Learning (RL), which requires on the order of
thousands of dialogues to achieve good perfor-
mance. Therefore, it is no longer feasible to rely
on data collected with real users. Instead, training
data is generated through interactions of the sys-
tem with simulated users (SUs) (Georgila et al,
2006). In order to learn good policies, the be-
haviour of the SUs needs to cover the range of
variation seen in real users (Georgila et al, 2006;
Schatzmann et al, 2006). Furthermore, SUs are
critical for evaluating candidate dialogue policies.
To date, SUs have been used to learn dialogue
strategies for specific domains such as flight reser-
vation, restaurant recommendation, etc., and to
learn both how to collect information from the
user (Frampton and Lemon, 2006) as well as how
to present information to the user (Rieser and
Lemon, 2009; Janarthanam and Lemon, 2009).
In addition to covering different domains, SUs
should also be able to model relevant user at-
tributes (Schatzmann et al, 2006), such as coop-
erativeness vs. non-cooperativeness (Lo?pez-Co?zar
et al, 2006; Jung et al, 2009), or age (Georgila et
al., 2008). In this paper, we focus on user age.
As the proportion of older people in the popu-
lation increases, it becomes essential to make spo-
ken dialogue systems (SDS) easy to use for this
group of people. Only very few spoken dialogue
systems have been developed for older people (e.g.
Nursebot (Roy et al, 2000)), and we are aware of
no work on learning specific dialogue policies for
older people using SUs and RL.
Older people present special challenges for di-
alogue systems. While cognitive and perceptual
abilities generally decline with age, the spread of
ability in older people is far larger than in any
other segment of the population (Rabbitt and An-
derson, 2005). Older users may also use differ-
ent strategies for interacting with SDS. In our pre-
vious work on studying the interactions between
older and younger users and a simulated appoint-
ment scheduling SDS (Wolters et al, 2009b), we
found that some older users were very ?social?,
treating the system like a human, and failing to
adapt to the SDS?s system-initiative dialogue strat-
egy. A third of the older users, however, tended
to be more ?factual?, using short commands and
conforming to the system?s dialogue strategy. In
that, they were very similar to the younger users
(Wolters et al, 2009b).
In previous work (Georgila et al, 2008), we
successfully built SUs for both older and younger
103
adults from the corpus used by (Wolters et al,
2009b) and documented in (Georgila et al, 2010).
When we evaluated the SUs using metrics such as
precision and recall (Georgila et al, 2006; Schatz-
mann et al, 2006), we found that SUs trained on
older users? data can cover behaviour patterns typ-
ical of younger users, but not the opposite. The
behaviour of older people is too diverse to be cap-
tured by a SU trained on younger users? data. This
result agrees with the findings of (Wolters et al,
2009b; Georgila et al, 2010).
In this study, we take our work one step
further?we use the SUs developed in (Georgila
et al, 2008) to learn dialogue policies and evalu-
ate the resulting policies with data from both older
and younger users. Our work is important for two
reasons. First, to the best of our knowledge this
is the first time that people have used SUs and
RL to learn dialogue strategies for the increas-
ingly important population of older users. Sec-
ond, despite the fact that SUs are used for learn-
ing dialogue strategies it is not clear whether they
can learn policies that are appropriate for different
user populations. We show that SUs can be suc-
cessfully used to learn policies for older users that
are adapted to their specific patterns of behaviour,
even though these patterns are far more varied than
the behaviour patterns of younger users. This pro-
vides evidence for the validity of the user simula-
tion methodology for learning and evaluating dia-
logue strategies for different user populations.
The structure of the paper is as follows: In sec-
tion 2 we describe our data set, discuss the dif-
ferences between older and younger users as seen
in our corpus, and describe our user simulations.
In section 3, we present the results of our experi-
ments. Finally, in section 4 we present our conclu-
sions and propose future work.
2 The Corpus
In the original dialogue corpus, people were asked
to schedule health care appointments with 9 dif-
ferent simulated SDS in a Wizard-of-Oz setting.
The systems varied in the number of options pre-
sented at each stage of the dialogue (1, 2, 4),
and in the confirmation strategies used (explicit
confirmation, implicit confirmation, no confirma-
tion). System utterances were generated using
a simple template-based algorithm and synthe-
sised using a female Scottish English unit selec-
tion voice. The human Wizard took over the func-
tion of speech recognition (ASR), language under-
standing (NLU), and dialogue management com-
ponents. No ASR or NLU errors were simulated,
because having to deal with ASR and/or NLU er-
rors in addition to task completion would have in-
creased cognitive load (Wolters et al, 2009a).
The system (Wizard) followed a strict policy
which resulted in dialogues with a fixed schema:
First, users arranged to see a specific health care
professional, then they arranged a specific half-
day, and finally, a specific half-hour time slot on
that half-day was agreed. Users were not allowed
to skip any stage of the dialogue. This design en-
sured that all users were presented with the rele-
vant number of options and the relevant confirma-
tion strategy at least three times per dialogue. In a
final step, the Wizard confirmed the appointment.
The full corpus consists of 447 dialogues; 3 di-
alogues were not recorded. A total of 50 partici-
pants were recruited, of which 26 were older, aged
between 50 and 85 years, and 24 were younger,
aged between 18 and 30 years. The older users
contributed 232 dialogues, the younger ones 215.
Older and younger users were matched for level
of education and gender. All dialogues were tran-
scribed orthographically and annotated with dia-
logue acts and dialogue context information. Us-
ing a unique mapping, we associate each dialogue
act with a ?speech act, task? pair, where the speech
act is task independent and the task corresponds to
the slot in focus (health professional, half-day or
time slot). For example, ?confirm pos, hp? cor-
responds to positive explicit confirmation of the
health professional slot. For each dialogue, de-
tailed measures of dialogue quality were recorded:
objective task completion, perceived task comple-
tion, appointment recall, length (in turns), and ex-
tensive user satisfaction ratings. For a detailed dis-
cussion of the corpus, see (Georgila et al, 2010).
The choice of dialogue strategy did not affect
task completion and appointment recall, but had
significant effects on efficiency (Wolters et al,
2009a). Task completion and appointment recall
were the same for older and younger users, but
older users took more turns to complete the task
(Wolters et al, 2009a). Clear differences between
the two user groups emerge when we look at in-
teraction patterns in more detail (Wolters et al,
2009b; Georgila et al, 2010). Older people tend
to ?ground? information (using repetitions) and
take the initiative more than younger people. In
our corpus it was very common that the older per-
son would provide information about the half-day
and the time slot of the appointment before hav-
ing been asked by the system. However, due to the
104
Experiment 1 Experiment 2
slot filled +50 +50
appointment confirmed +200 +200
dialogue length -5 per turn -5 per turn
slot confirmed +100 not used
wrong order -500 not used
Table 1: Reward functions for the experiments.
strict policy of the Wizard, this information would
be ignored and the system would later ask for the
information that had already been provided.
In our SUs, each user utterance corresponds to a
user action described by a list of ?speech act, task?
pairs. There are 31 distinct system actions and 389
distinct actions for older users. Younger people
used a subset of 125 of the older users? actions.
Our SUs do not simulate ASR or NLU errors since
such errors were not simulated in the collection of
the corpus.
We built n-grams of system and user actions
with n varying from 2 to 5. Given a history of n-1
actions from system and user, the SU generates an
action based on a probability distribution learned
from the training data (Georgila et al, 2006). In
the present study, n was set to 3, which means that
each user action is predicted based on the previous
user action and the previous system action.
3 Learning Dialogue Strategies
We performed two experiments. In Experiment 1,
our goal was to learn the policy of the Wizard, i.e.
the strict system-initiative policy of requesting and
confirming information for each slot before mov-
ing to the next slot, in the following order: health
professional, half-day, time slot. In Experiment
2, our goal was to learn a more flexible policy that
could accommodate some degree of user initiative.
The reward functions for both experiments are
specified in Table 1; they are similar to the reward
functions used in the literature, e.g. (Frampton and
Lemon, 2006). Slots that have been filled success-
fully and confirmed appointments are rewarded,
while long dialogues are penalised. For Experi-
ment 1, policies were rewarded that filled slots in
the correct order and that confirmed each slot af-
ter it had been filled. A large penalty was imposed
when the policy deviated from the strict slot order
(health professional, half-day, time slot). For Ex-
periment 2, these constraints were removed. Slots
could be filled in any order. Confirmations were
not required because there was no speech act in
the corpus for confirming more than one slot at a
time.
In both experiments we used the SARSA-? al-
gorithm (Sutton and Barto, 1998) for RL. 30,000
iterations were used for learning the final pol-
icy for each condition. For each experiment,
we learned two policies, Policy-Old, which was
based on simulated older users, and Policy-Young,
which was based on simulated younger users.
The resulting policies were then tested on simu-
lated older users (Test-Old) and simulated younger
users (Test-Young). To have comparable results
between Experiment 1 and Experiment 2, dur-
ing testing we score our policies using the reward
function of Experiment 2. The best possible score
is 190, i.e. the user fills all the slots in one turn
and then confirms the appointment. (Note that +50
points are given when a slot is only filled, not con-
firmed too.) For each test condition, we gener-
ated 10,000 simulated dialogues. Overall scores
for each combination of policy and SU were es-
tablished using 5-fold cross-validation.
Our results are summarised in Figure 1. While
average rewards were not affected by policy
type (ANOVA, F (1, 68)=1, p=0.3) or training
data set (F (1, 185)=3, p=0.09), we found a very
strong interaction between policy type and data
set (F (1, 3098)=51, p=0.000). Learning with
simulated younger users yields better strict poli-
cies than learning with older users (Tukey?s Hon-
est Significant Difference Test, ?=20, 95% CI
= [11, 30], p=0.000), while learning with simu-
lated older users yields better flexible policies than
learning with younger users (?=15, 95% CI =
[6, 24], p=0.001). This is what we would expect
from our corpus analysis, since the interaction be-
haviour of older users is far more variable than that
of younger users (Wolters et al, 2009b; Georgila
et al, 2010).
The strict policy that was learned from sim-
ulated younger users was as follows, with only
slight variations: first request the type of health
professional, then implicitly confirm the health
professional and request the half-day slot, then im-
plicitly confirm the half-day slot and request the
time slot, and then confirm the appointment. The
strict policy learned from simulated older users
was similar, but less successful, because most
older users do not readily conform to the fixed
structure.
The flexible policy learned from simulated older
users takes into account initiative from the user
and does not always confirm. The score for the
flexible policy learned from simulated younger
users was relatively low, even though the resulting
105
Score
140150
160170
180190
Test?Old Test?Young
Policy?OldReward?Flex
Test?Old Test?Young
Policy?YoungReward?Flex
Policy?OldReward?Strict
140150
160170
180190
Policy?YoungReward?Strict
Figure 1: Mean scores for each combination of
reward function, training set, and test set (5-fold
cross-validation).
policy was very similar to the strict policy learned
from younger users (i.e. a sequence of informa-
tion requests and implicit confirmations), and even
though the behaviour of younger users is far more
predictable than the behaviour of older users. It
appears that the explicit penalty for violating the
order of slots is crucial for fully exploiting the pat-
terns in younger users? behaviour.
4 Conclusions
We have shown that SUs can be used to learn ap-
propriate policies for older adults, even though
their interaction behaviour is more complex and
diverse than that of younger adults. Crucially, sim-
ulated older users allowed us to learn a more flex-
ible version of the strict system-initiative dialogue
strategies that were used for creating the original
corpus of interactions. These results are consis-
tent with previous analyses of the original corpus
(Wolters et al, 2009b; Georgila et al, 2010) and
support the validity of the user simulation method-
ology for learning and evaluating dialogue strate-
gies.
In our future work, we will experiment with
more complex SUs, e.g. linear feature combina-
tion models (Georgila et al, 2006), and see if they
can be used to learn similar policies. We also plan
to study the effect of training and testing with dif-
ferent user simulation techniques, such as n-grams
versus linear feature combination models.
Acknowledgements
This research was partially supported by the MATCH project
(SHEFC-HR04016, http://www.match-project.
org.uk). Georgila is supported by the U.S. Army Research,
Development, and Engineering Command (RDECOM). The
content does not necessarily reflect the position or the policy
of the U.S. Government, and no official endorsement should
be inferred.
References
M. Frampton and O. Lemon. 2006. Learning more effective
dialogue strategies using limited dialogue move features.
In Proc. ACL.
K. Georgila, J. Henderson, and O. Lemon. 2006. User simu-
lation for spoken dialogue systems: Learning and evalua-
tion. In Proc. Interspeech.
K. Georgila, M. Wolters, and J. Moore. 2008. Simulating the
behaviour of older versus younger users. In Proc. ACL.
K. Georgila, Maria Wolters, J.D. Moore, and R.H. Logie.
2010. The MATCH corpus: A corpus of older and
younger users? interactions with spoken dialogue systems.
Language Resources and Evaluation, 44(3):221?261.
S. Janarthanam and O. Lemon. 2009. A two-tier user simula-
tion model for reinforcement learning of adaptive referring
expression generation policies. In Proc. SIGdial.
S. Jung, C. Lee, K. Kim, and G.G. Lee. 2009. Hybrid ap-
proach to user intention modeling for dialog simulation.
In Proc. ACL.
R. Lo?pez-Co?zar, Z. Callejas, and M. McTear. 2006. Testing
the performance of spoken dialogue systems by means of
an artificially simulated user. Artificial Intelligence Re-
view, 26(4):291?323.
P. Rabbitt and M.M. Anderson. 2005. The lacunae of
loss? Aging and the differentiation of human abilities.
In F.I. Craik and E. Bialystok, editors, Lifespan Cogni-
tion: Mechanisms of Change, chapter 23. Oxford Univer-
sity Press, New York, NY.
V. Rieser and O. Lemon. 2009. Natural language gener-
ation as planning under uncertainty for spoken dialogue
systems. In Proc. EACL.
N. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialog man-
agement for robots. In Proc. ACL.
J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young.
2006. A survey of statistical user simulation tech-
niques for reinforcement-learning of dialogue manage-
ment strategies. Knowlege Engineering Review, 21(2):97?
126.
R.S. Sutton and A.G. Barto. 1998. Reinforcement Learning:
An Introduction. MIT Press.
J. Williams and S. Young. 2007. Partially observable Markov
decision processes for spoken dialog systems. Computer
Speech and Language, 21(2):393?422.
M. Wolters, K. Georgila, J.D. Moore, R.H. Logie, S.E.
MacPherson, and M. Watson. 2009a. Reducing work-
ing memory load in spoken dialogue systems. Interacting
with Computers, 21(4):276?287.
M. Wolters, K. Georgila, J.D. Moore, and S.E. MacPherson.
2009b. Being old doesn?t mean acting old: How older
users interact with spoken dialog systems. ACM Trans.
Accessible Computing, 2(1).
106
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 162?172,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Exploring User Satisfaction in a Tutorial Dialogue System
Myroslava O. Dzikovska, Johanna D. Moore
School of Informatics, University of Edinburgh
Edinburgh, United Kingdom
m.dzikovska,j.moore@ed.ac.uk
Natalie Steinhauser, Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division
Orlando, Florida, USA
gwendolyn.campbell,natalie.steinhauser@navy.mil
Abstract
User satisfaction is a common evaluation met-
ric in task-oriented dialogue systems, whereas
tutorial dialogue systems are often evaluated
in terms of student learning gain. However,
user satisfaction is also important for such
systems, since it may predict technology ac-
ceptance. We present a detailed satisfaction
questionnaire used in evaluating the BEETLE
II system (REVU-NL), and explore the un-
derlying components of user satisfaction us-
ing factor analysis. We demonstrate interest-
ing patterns of interaction between interpreta-
tion quality, satisfaction and the dialogue pol-
icy, highlighting the importance of more fine-
grained evaluation of user satisfaction.
1 Introduction
User satisfaction is one of the primary evaluation
measures for task-oriented spoken dialogue systems
(SDS): the goal of an SDS is to accomplish the task,
and to keep the user satisfied, so that they will want
to continue using the system. Typically, the PAR-
ADISE methodology (Walker et al, 2000) is used to
establish a performance function which relates user
satisfaction measured through questionnaires to in-
teraction parameters that can be derived from sys-
tem logs. This function can then be used to better
understand which properties of the interaction have
the most impact on the users, and to compare differ-
ent system versions.
In contrast, tutorial dialogue systems are typically
evaluated in terms of student learning gain, by com-
paring student scores on standardized tests before
and after interacting with the system. This is clearly
an important evaluation metric, since it directly as-
sesses the benefit students obtain from using the sys-
tem. However, it is also important to evaluate user
satisfaction, since it can influence students? willing-
ness to use computer tutors in a long run. Thus,
recent studies have looked at factors that could in-
fluence user satisfaction in tutorial dialogue, such as
different tutoring policies (Forbes-Riley and Litman,
2011), quality of speech output (Forbes-Riley et al,
2006), and students? prior attitudes towards technol-
ogy (Jackson et al, 2009).
Assessing user satisfaction, however, is not a
straightforward task. As we discuss in more detail in
Section 2, user satisfaction is known to be a complex
multi-dimensional construct, composed of largely
independent factors such as perceived ease of use
and perceived usefulness. Therefore, questionnaires
used for assessing satisfaction need to be validated
through user studies, and different satisfaction di-
mensions should be assessed independently. There-
fore, SDS researchers are now starting to use tech-
niques from psychometrics for this purpose (Hone
and Graham, 2000; Mo?ller et al, 2007). However,
user satisfaction studies tutorial dialogue currently
rely on simple questionnaires adapted from either
task-oriented SDS or non-dialogue intelligent tutor-
ing systems (Michael et al, 2003; Forbes-Riley et
al., 2006; Forbes-Riley and Litman, 2011; Jackson
et al, 2009), and these questionnaires have not been
validated for tutorial dialogue systems.
In this paper, we make the first step towards de-
veloping a better user satisfaction questionnaire for
tutorial dialogue systems. We present a user satis-
162
faction evaluation of the BEETLE II tutorial dialogue
system. Starting with a detailed user satisfaction
questionnaire, we employ exploratory factor analy-
sis to discover a set of dimensions for the students?
satisfaction with a dialogue-based tutor. We then
use the factors we derived to compare user satisfac-
tion between two versions of our computer tutor that
use different policies for generating the tutor?s feed-
back. We investigate the relationships between the
subjective satisfaction dimensions and the objective
learning gain metric for the two systems. Finally, we
carry out a more detailed investigation of our prior
results on the relationship between user satisfaction
and interpretation quality in tutorial dialogue. Our
analysis also provides insights for further improving
the questionnaire we developed and gives an exam-
ple of how user satisfaction metrics developed for
task-oriented dialogue can be adapted to different
dialogue applications. It also opens new questions
about how different properties of the interaction af-
fect user satisfaction in tutorial dialogue, which can
be investigated in future work.
The rest of the paper is organized as follows. We
discuss the approaches for assessing user satisfac-
tion with SDS in Section 2. In Section 3 we describe
the BEETLE II tutorial dialogue system used in this
evaluation. We describe our questionnaire design in
Section 4, and describe its use in BEETLE II evalu-
ation in Section 5. We conclude by discussing the
implication of our analysis for tutorial dialogue sys-
tem evaluation in Section 6.
2 Background
A typical approach to assessing user satisfaction in
dialogue systems is collecting user survey data by
asking users to rate their agreement with statements
such as ?the system was easy to use?. In the simplest
case of early PARADISE studies, the questionnaires
contained 5 items assessing different dimensions of
satisfaction, which were then summed to produce a
total satisfaction score.
However, using simple questionnaires has draw-
backs now recognized by the SDS community. First,
if individual questions are expected to assess differ-
ent dimensions of user satisfaction, they need to be
validated first, or else they may be ambiguous and
mean different things to different users. Second,
summing or averaging over questions measuring dif-
ferent satisfaction components may not be the best
approach, since it may conflate unrelated judgments
(Hone and Graham, 2000).
To address this problem, SDS researchers have
started using more complex questionnaires, where
each underlying dimension of user satisfaction is as-
sessed through multiple questions. Factor analysis is
then used to determine which questions are related
to one another (and therefore are likely to be assess-
ing the same underlying satisfaction dimension), and
to discard possibly ambiguous questions. Then, the
PARADISE methodology can be used to relate dif-
ferent interaction parameters to individual compo-
nents of user satisfaction.
Several such studies have been conducted recently
(Hone and Graham, 2000; Larsen, 2003; Mo?ller et
al., 2007; Wolters et al, 2009), covering command-
and-control and information-seeking dialogue. The
questionnaires in those studies contained 25 to 50
items, and factor analyses typically resulted in 6- or
7-factor solutions, with dimensions such as accept-
ability, affect, system response accuracy and cogni-
tive demand. The underlying factors found by those
analyses tend to match up well, but not to over-
lap perfectly. In comparison, all user satisfaction
questionnaires for tutorial dialogue systems that we
are aware of contain 10-15 items which are either
summed up for PARADISE studies, or compared
individually to track system improvement (Michael
et al, 2003; Forbes-Riley et al, 2006; Forbes-Riley
and Litman, 2011; Jackson et al, 2009).
In this paper, we apply the more sophisticated
SDS evaluation methodology to the BEETLE II tu-
torial dialogue system. We devise a more sophis-
ticated user satisfaction questionnaire using SDS
questionnaires for guidance and then apply factor
analysis to investigate the underlying dimensions.
We compare our results to analyses from two pre-
vious studies: SASSI (Hone and Graham, 2000),
which is a validated questionnaire intended for use
with a variety of task-oriented dialogue systems,
and a more recent ?modified SASSI? questionnaire
which is a version of SASSI adapted for use with the
INSPIRE home control system (Mo?ller et al, 2007).
Henceforth we will refer to this as INSPIRE.
163
3 BEETLE II Tutorial Dialogue System
The goal of BEETLE II (Dzikovska et al, 2010c)
is to teach students conceptual knowledge in the do-
main of basic electricity and electronics. The system
is built on the premise that encouraging students to
explain their answers and to talk about the domain
will lead to improved learning, a finding consistent
with analyses of human-human tutoring in several
domains (Purandare and Litman, 2008; Litman et
al., 2009). BEETLE II has been engineered to test
this hypothesis by eliciting contentful talk through
explanation questions.
The BEETLE II learning material consists of two
self-contained lessons suitable for college-level stu-
dents with no prior knowledge of basic electricity
and electronics. The lessons take 4 to 5 hours to
complete, and consist of reading materials and inter-
active exercises. During the exercises, the students
interact with a circuit simulator, building electrical
circuits containing bulbs, batteries and switches, and
using a multimeter to measure voltage. Then the
tutor asks students to explain circuit behavior, for
example, ?Why was bulb A on when switch Y was
open and switch Z was closed?? In addition, at dif-
ferent points in the lesson the tutor asks ?summary?
questions, asking students to define concepts such
as voltage, and verbalize general patterns such as
?What are the conditions that are required for a bulb
to light??. At present, students use a typed chat in-
terface to communicate with the system.1
We built and evaluated two versions of the sys-
tem (Dzikovska et al, 2010a). The baseline non-
adaptive tutor (BASE) requires students to produce
answers, but does not provide any remediation and
immediately states the correct answer. The fully
adaptive version (FULL) engages in dialogue with
the student, and tailors its feedback to the student?s
answer by confirming its correct parts and giving
hints in order to help students fix missing or incor-
rect parts. The FULL system generates feedback au-
tomatically based on a detailed analysis of the stu-
dent?s input, and is capable of giving hints at differ-
ent levels of specificity depending on the student?s
previous performance.
1A speech interface is being developed, but typed communi-
cation is common in online and distance learning, and therefore
is an acceptable choice for tutorial dialogue as well.
These two system versions were designed to eval-
uate the impact of adaptive feedback (within the lim-
itations of current language interpretation technol-
ogy) on student learning and satisfaction. Our initial
data analysis focused on the differences in student
language depending on the condition (Dzikovska et
al., 2010a), and on the impact of different types of
interpretation errors on learning gain and user sat-
isfaction (Dzikovska et al, 2010b). However, these
initial results were based on an aggregate satisfac-
tion score obtained by averaging over scores for all
questions in our user satisfaction questionnaire. In
this analysis, we take a more detailed look at the dif-
ferent factors that contribute to students satisfaction
with the system, and their relationship with learning
gain and interpretation quality.
4 Data Collection
4.1 Questionnaire Design
To support user satisfaction evaluation we developed
a satisfaction questionnaire, REVU-IT (Report on
the Enjoyment, Value, and Usability of an Intelli-
gent Tutor). It consists of 63 items which cover all
aspects of interaction with the tutoring system: the
clarity and usefulness of the reading material; the
graphical user interface to the circuit simulator; in-
teraction with the dialogue tutor; and the overall im-
pression of the BEETLE II system as a whole. The
reading material, graphical user interface and inter-
action with the tutor sections are complementary,
because they cover separate parts of the BEETLE II
interface. We expect that all of these three compo-
nents contribute to the overall impression score. For
purposes of this paper, we will focus on the part of
the questionnaire that relates to the natural language
interaction with the tutor (REVU-NL), and its re-
lationship to the overall impression score (REVU-
OVERALL).
The REVU-IT questionnaire was developed by
experienced cognitive psychologists (two of the au-
thors of this paper). The REVU-NL section con-
sists of 35 items shown in Appendix A. Its design
was guided by questionnaires used in previous re-
search, including INSPIRE and a questionnaire used
to evaluate the ITSPOKE tutorial dialogue system
(Forbes-Riley et al, 2006). REVU-NL contains a
number of items from these, but omits items that are
164
not relevant to the BEETLE II domain (e.g, ?Domes-
tic devices can be operated efficiently with the sys-
tem? or ?The tutor responded effectively after I was
uncertain?), and adds extra questions related to tu-
toring (e.g., ?Our dialogues quickly led to me hav-
ing a deeper understanding of the material?), based
on the authors? previous experience in human factors
research. We also slightly rephrased all questions to
refer to ?the tutor? rather than ?the system?.
The REVU-OVERALL section of REVU-IT
consists of 5 items assessing the student?s satis-
faction with their learning as a whole. The ques-
tions are: ?Overall, I am satisfied with my experi-
ence learning about electricity from this system.?;
?Working in this learning environment was just like
working one-on-one with a human tutor?; ?I would
have preferred to learn about electricity in a different
way.?; ?I would use this system again in the future to
continue to learn about electricity.?; ?I would like to
be able to use a system like this to learn about other
topics in the future.?. We use the averaged score over
these 5 items to represent the student?s overall satis-
faction with the learning environment, referring to it
as ?overall satisfaction?.
Adding new questions to the REVU-NL ques-
tionnaire on top of already existing questions is the
initial step in addressing the issues discussed in Sec-
tion 2: validating the individual questions and dis-
covering the underlying dimensions of user satis-
faction. Having a large number of questions ask-
ing about the same aspects of the interaction will
allow us to group related questions together into di-
mensions (?factors?), and also to discover ambigu-
ous questions that will need to be improved in future
studies. The detailed discussion of the technique and
issues involved is presented in Hone and Graham
(2000).
4.2 Participants
We used REVU-IT as part of a controlled experi-
ment comparing the BASE and FULL versions of the
system. We recruited 87 participants from a uni-
versity in the Southern US, paid for participation.
Participants had little knowledge of the domain.
Each participant signed consent forms and com-
pleted a pre-test, then worked through both lessons
(with breaks), and then completed a post-test and a
REVU-IT questionnaire. Each session lasted 3.5
hours on average.
Out of 87 participants that completed the study, 13
had an inordinate amount of trouble with interface:
they typed utterances that could not be interpreted
by the tutor (defined as having more than 3 standard
deviations in interpretation errors compared to the
rest), did not follow tutor?s instructions or experi-
enced system crashes. In addition, two participants
were learning gain outliers (again, more than 3 stan-
dard deviations from average). These participants
were removed from the analysis. The questionnaires
from the remaining 72 participants are used in our
data analysis.
5 Analysis
5.1 Underlying satisfaction dimensions
Each item in the REVU-NL questionnaire used a
5-point Likert scale, from ?completely disagree? (1)
to ?fully agree? (5). Most of the items were phrased
so that the agreement with the statement meant a
positive evaluation of the system. For a few items,
however, the polarity was reversed (e.g., ?The tutor
was not helpful?). Those items were reverse-coded,
with 1 meaning ?fully agree? and 5 ?completely dis-
agree?, to ensure that a lower score on all questions
corresponds to a negative assessment.
Following Hone and Graham (2000), we used
exploratory factor analysis to group questionnaire
items into clusters representing different dimen-
sions. One of the standard approaches in determin-
ing how many factors (?question clusters?) to use
is the scree test which checks the number of eigen-
values in the question covariance matrix which are
greater than 1. These typically correspond to prin-
cipal components which reflect the underlying ques-
tionnaire structure. The scree test showed 7 eigen-
values greater than 1, resulting in the 7-factor solu-
tion presented in Table 1.
The loadings in the table are the correlation coef-
ficients between the individual question scores and
the variables representing the factors. Most of the
correlations are quite high, indicating that the ques-
tions are strongly correlated both among themselves
and the underlying factor. However, the last two fac-
tors contain only non-loading questions according to
the criteria in (Hone and Graham, 2000), i.e., ques-
tions for which the correlations are too weak to be
165
# Question Load-
ing
1 t29: Knew what to say at each point 0.82
1 t22: Easy to interact with the tutor. 0.79
1 t9: Not sure what was expected. 0.73
1 t18: Knew what to say to the tutor. 0.70
1 t14: The tutor was too inflexible. 0.69
1 t19: Able to recover easily from errors 0.69
1 t24: Easy to learn to speak to tutor. 0.69
1 t16: Tutor didn?t do what I wanted. 0.65
1 t3: Tutor understood me well. 0.65
1 t15: Working as easy as with a human. 0.64
1 t13: Had to concentrate when talking. 0.62
2 t31 Tutor was an efficient way to learn. 0.79
2 t32: Easy to learn from the tutor. 0.78
2 t34: Tutor was worthwhile 0.72
3 t28: Tutor was irritating. 0.76
3 t10: Tutor was fun. 0.74
3 t7: Enjoyed talking with tutor. 0.72
3 t30: Dialogues were boring. 0.66
4 t2: Tutor took too long to respond 0.84
4 t33: Tutor responded quickly 0.84
5 t26: Didn?t always understand tutor 0.89
6 (t3: The tutor understood me well) 0.4
7 (t25: Comfortable talking with tutor) 0.59
Table 1: Factors derived from the REVU-NL question-
naire, with question loadings for the factor to which each
question was assigned. Question text shortened due to
space limitations, full text presented in the appendix.
Non-loading questions in parentheses.
reliable. In addition, factors 4 and 5 had fewer than
3 questions. Since the number of subjects in our data
set is small, such factors may not be reliable. There-
fore, we focus our remaining analysis on the top 3
factors from the questionnaire, each of which con-
tains 3 or more questions.
Twelve questions in REVU-NL were ?cross-
loading? according to criteria in Hone and Graham
(2000), that is, their two top loadings differed by
less than 0.2. This indicates questions that are likely
to be ambiguous, since they are strongly correlated
with two (theoretically independent) variables. Such
questions should be refined and re-designed in future
surveys. These were questions t1, t4, t6, t11, t12,
t17, t20, t21, t23, t25, t27, t35 from the appendix.
We removed them from our solution, and discuss the
implications for survey design in Section 6.
The first component in our analysis lines up well
with the Transparency and Cognitive load factors
from INSPIRE, and Response accuracy, Cognitive
demand and Habitability from SASSI, though it was
not split into individual factors as in those analyses.
We will refer to this factor as Transparency. The
second component contains questions specific to tu-
toring. However, it is similar to the Acceptability
dimension from INSPIRE (the original SASSI ques-
tionnaire did not include similar questions), which
asked users to rate statements such as ?domestic de-
vices can be operated efficiently with the system?.
Thus, we will refer to it as Acceptability. Finally,
our third dimension lines up best with the Affect and
Annoyance items from SASSI.2 We will refer to it as
Affect.
Although the correspondences between our fac-
tors and those derived from SASSI and INSPIRE
are not perfect, the fact that similar underlying fac-
tors are derived from different user groups and sys-
tems indicates that they are likely to be measuring
the same underlying constructs.
5.2 Comparing satisfaction in different systems
Recall that in this study we combined the data from
two systems: FULL, where the system provided stu-
dents with adaptive feedback and hints, and BASE,
where the system simply acknowledged the stu-
dent?s answers and then provided a correct answer
without engaging in dialogue. Table 2 separates out
the average factor scores for these two conditions,
where a factor score is computed by averaging over
scores of all questions assigned to that factor.
When comparing learning gain and overall satis-
faction between the two systems (which is the over-
all impression of the system behavior as a whole,
including circuit simulation and lesson design), the
difference is not statistically significant (learning
gain t(69) = ?0.95, p = 0.35, overall satisfac-
tion t(69) = ?1.52, p = 0.13). In contrast, on
individual dimensions related to tutoring the scores
for BASE is significantly higher than the score for
FULL (Transparency, t(69) = ?7.19, p < 0.0001;
Acceptability: t(69) = ?3.24, p < 0.01; Affect:
2The acceptability dimension from INSPIRE is split be-
tween our factors 2 and 3, but most of the questions correspond
to our factor 2 questions.
166
FULL BASE
Transparency 2.15 (0.56) 3.36 (0.81)
Acceptability 3.11 (1.02) 3.80 (0.77)
Affect 2.43 (0.80) 2.86 (0.996)
Overall 3.39 (0.88) 3.70 (0.83)
Learning gain 0.61 (0.15) 0.65 (0.22)
Table 2: Average scores for different satisfaction dimen-
sions in FULL and BASE (standard deviation in parenthe-
ses)
t(69) = ?1.97, p = 0.05). Comparing the means,
the biggest difference in student ratings shows on the
Transparency scale, while the affective reaction for
the two systems is more similar (though still rated
higher for BASE).
It is somewhat unexpected to see that the students
were equally satisfied overall with both systems but
rated the tutor in BASE more highly than in FULL,
since the tutor behavior was the only thing different
between conditions. We are at present investigating
the reasons for this result. One possibility is that
when students did not get much feedback from the
tutor (as in BASE), other factors became more im-
portant to overall satisfaction, such as course design
and quality of user simulation.
5.3 Relationships between subjective and
objective outcome measures
We investigated the correlations between learning
gain and different user satisfaction factors for the
two system versions. Results are presented in Table
3. As can be seen from the table, learning gain and
user satisfaction are only significantly correlated in
FULL, and only for the acceptability and overall sat-
isfaction factors. None of the factors in the BASE
system correlate with learning gain. This indicates
that the student?s affective reaction to the system is
not necessarily linked directly to its objective bene-
fits. We discuss these results further in Section 6
5.4 Impact of interpretation quality on user
satisfaction
It is generally known in SDS research that measures
of interpretation quality such as word error rate and
concept accuracy are strongly correlated with user
FULL BASE
Transparency 0.32 (0.07) 0.06 (0.69)
Acceptability 0.38 (0.03) 0.23 (0.16)
Affect 0.29 (0.08) -0.10 (0.53)
Overall 0.38 (0.02) 0.18 (0.28)
Table 3: Correlations between satisfaction factors and
learning gain for two dialogue policies. Significance level
in parentheses. Bold indicates significance at p < 0.05
level.
satisfaction (e.g., (Walker et al, 2000; Mo?ller et al,
2007)). Our system uses typed input and produces
complex logical representations (rather than sim-
ple slot-value pairs), thus, these measures cannot be
computed directly. However, in an earlier study we
showed that another measure of interpretation qual-
ity, namely, percentage of utterances that could not
be interpreted by the system (?uninterpretable utter-
ances?) is negatively correlated with learning gain
and user satisfaction (Dzikovska et al, 2010b).3
That study revealed an unexpected pattern. Al-
though the system recorded the number of utter-
ances it could not interpret in both FULL and BASE,
students in BASE were never informed of any in-
terpretation problems. Nevertheless, the proportion
of such uninterpretable utterances was still signifi-
cantly negatively correlated with user satisfaction in
BASE. After analyzing correlations between differ-
ent types of errors and user satisfaction, we hypoth-
esized that this can be explained by the lack of align-
ment between the system and the student, in partic-
ular when students used terminology different from
that used by the system (Dzikovska et al, 2010b).
We can now analyze this relationship in more de-
tail, looking at correlations between interpretation
problems and different components of user satisfac-
tion. The results are presented in Table 4.
As can be seen from the table, the proportion
of uninterpretable answers is significantly correlated
with Acceptability in FULL, but not in BASE. This
is not surprising, indicating that students who were
told that they were not understood perceived the
system as less useful for them. More surprisingly,
Transparency, which is related to perceived ease of
3In that study, we computed user satisfaction with the tutor
by averaging over the entire 35 questions in our questionnaire
as an initial approximation.
167
FULL BASE
Transparency -0.28 (0.1) -0.25 (0.10)
Acceptability -0.58 (< 0.001) -0.29 (0.07)
Affect -0.35 (0.04) -0.34 (0.04)
Overall -0.38 (0.03) -0.27 (0.11)
Learning gain -0.38 (0.03) -0.09(0.60)
Table 4: Correlations between satisfaction factors and un-
interpretable utterances for two different policies. Signif-
icance level in parentheses.
use for the system, was not correlated with uninter-
pretable utterances. Finally, the proportion of unin-
terpretable utterances is significantly correlated with
Affect for both systems. Moreover, the unexpected
negative correlation we observed in the earlier study
between satisfaction with the tutor and interpretation
problems in BASE can be primarily attributed to the
negative correlation with the Affect score.
6 Discussion
In this study, we attempted to apply insights from
studies of user satisfaction in spoken dialogue sys-
tems to a different type of dialogue application: tu-
torial dialogue. We were looking to develop a better
user satisfaction questionnaire for evaluating tutorial
dialogue systems, and to implement an evaluation
methodology which takes into account different un-
derlying dimensions of user satisfaction.
The three dimensions we obtained based on ex-
ploratory factor analysis of REVU-NL align well
with the dimensions reported in the SDS litera-
ture, which provides some evidence of their valid-
ity. However, the results are preliminary because
of the small number of participants involved, and
need to be replicated with additional participants and
different tutoring systems. Regardless, our analysis
highlighted important issues in designing satisfac-
tion surveys for different dialogue genres.
When choosing which questions to include in a
satisfaction questionnaire for a new system type,
SASSI is a very attractive starting point, because
it was validated across multiple SDS in two gen-
res (command and control and information seeking).
This also means that SASSI items are phrased very
generally and therefore easier to adapt. In contrast,
INSPIRE contains a number of questions specific to
the command and control domain, asking whether
the user thinks the system is useful in achieving their
goals (i.e., operating the domestic devices). SASSI
includes only one similar item, ?The system was
useful?. It was classed as Affect, most likely be-
cause there were no other similar items. However,
we think that such questions represent an important
separate dimension, namely the ?perceived useful-
ness? factor known to predict technology acceptance
(Adams et al, 1989). Therefore we included sev-
eral items in REVU-NL with similar intent, asking
whether users thought the system was beneficial to
their goal (i.e., learning the material). These items
were clustered into a separate dimension by factor
analysis, indicating that they should be included in
other satisfaction surveys.
Moreover, some of the questions that appeared
genre-independent to us proved to be cross-loading
in our analysis, which is an indicator of ambiguity.
Apparently, some of the items from task-oriented di-
alogue questionnaires did not transfer well. For ex-
ample, statements like ?The system didn?t always do
what I expected? are unambiguous for task-oriented
dialogue, where the user is supposed to be in control
of the interaction, and therefore has clear expecta-
tions of what the system should do. In contrast, in
tutorial dialogue the tutor has control over the learn-
ing material. Thus, it may be more ambiguous as
to what, if anything, students are expecting from the
interaction.
Overall, our experience shows that it may not
be possible, or indeed useful, to create completely
generic surveys. However, we believe that question-
naires can be phrased generally enough to apply to a
range of systems with similar goals, and REVU-NL
in particular is useful starting point for comparing
dialogue-based tutoring systems. We believe that the
18 questions that we retained as unambiguous in our
analysis provide adequate assessment of user satis-
faction, and are grouped into factors consistent with
results of previous research. However, the question-
naire could be further improved by revisiting the
cross-loading items we rejected as ambiguous, and
seeing if their wording could be improved. We are
also intending to use REVU-IT in evaluating a spo-
ken version of BEETLE II, thus providing additional
validation data on a different version of the interface.
With respect to evaluation methodology, our re-
sults highlight the need to look at different satis-
168
faction dimensions separately. We used our fac-
tors to further investigate a pattern that we discov-
ered in previous research, namely, that students who
speak in a way that is difficult for the system to in-
terpret tend to be less satisfied with the tutor, even
when they are not told of the interpretation prob-
lems. Looking at correlations with individual di-
mensions shows that this relationship is primarily
explained by the Affect dimension. Our working hy-
pothesis is that the lack of alignment between in-
correct student answers and the answers supplied by
the system caused students to perceive the system as
a less likeable or cooperative conversational partner.
We also observed that Acceptability, but no other
dimensions, were correlated with learning gain in
FULL. One possible explanation is that students who
are learning more believe that the system is help-
ing them reach their goals (our definition of Accept-
ability). The FULL condition provides students with
more explicit feedback as to their learning; whereas
in BASE students may have a less accurate estimate
of how well they are doing, and hence no satisfaction
dimensions are correlated with learning gain.
It is worth noting that an earlier study investigat-
ing the relationship between user satisfaction and
learning in two different tutorial dialogue systems
(Forbes-Riley and Litman, 2009) found little corre-
lation between the answers to individual questions
on their satisfaction questionnaire and learning gain.
Only one correlation, with the question ?The tutor
helped me to concentrate?, reached significance in
only one of the 4 conditions they investigated. This
adds further evidence that the relationship between
learning gain and satisfaction is not straightforward.
However, our results are difficult to compare since
the questionnaires used are different, and Forbes-
Riley and Litman (2009) are studying correlations
with individual questions rather than grouping re-
lated questions together. Developing better validated
questionnaires will make such results easier to com-
pare and interpret, and we believe that REVU-NL
makes a significant step in that direction.
7 Conclusion and Future Work
In this paper, we proposed an improved question-
naire (REVU-NL) for evaluating user satisfaction
in tutorial dialogue systems, which is an important
evaluation metric alongside learning gain. We used
the methodology from SDS evaluations to investi-
gate different dimensions of user satisfaction, and
their relationship to learning gain and different in-
teraction properties. Next, we are planning to use
the PARADISE methodology to establish predictive
models that relate satisfaction dimensions to mea-
surable interaction properties, so that we can de-
termine development priorities, and make it eas-
ier to compare different system versions. We are
also planning to collect additional questionnaire data
with a speech-enabled version of the system, and
verify our analyses on this extended data set.
Acknowledgments
This work has been supported in part by US Of-
fice of Naval Research grants N000141010085 and
N0001410WX20278. We would like to thank our
sponsors from the Office of Naval Research, Dr. Su-
san Chipman and Dr. Ray Perez, and the Research
Associates who worked on this project, Kather-
ine Harrison, Leanne Taylor, Charles Scott, Simon
Caine, Elaine Farrow and Charles Callaway for their
contribution to this effort.
References
Dennis A. Adams, R. Ryan Nelson, and Peter A. Todd.
1989. Perceived usefulness, ease of use, and usage of
information technology. MIS Quarterly., 13:319?339.
Myroslava Dzikovska, Natalie B. Steinhauser, Jo-
hanna D. Moore, Gwendolyn E. Campbell, Kather-
ine M. Harrison, and Leanne S. Taylor. 2010a. Con-
tent, social, and metacognitive statements: An em-
pirical study comparing human-human and human-
computer tutorial dialogue. In Sustaining TEL: From
Innovation to Learning and Practice - 5th European
Conference on Technology Enhanced Learning (EC-
TEL 2010), pages 93?108, Barcelona, Spain, October.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010b. The
impact of interpretation problems on tutorial dialogue.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics(ACL-2010),
Uppsala, Sweden, July.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010c. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proceedings of the 48th Annual Meeting of
169
the Association for Computational Linguistics (ACL-
2010) demo session, Uppsala, Sweden, July.
Katherine Forbes-Riley and Diane J. Litman. 2009.
Adapting to student uncertainty improves tutoring dia-
logues. In Artificial Intelligence in Education: Build-
ing Learning Systems that Care: From Knowledge
Representation to Affective Modelling, Proceedings
of the 14th International Conference on Artificial In-
telligence in Education (AIED 2009), pages 33?40,
Brighton, UK, July.
Katherine Forbes-Riley and Diane J. Litman. 2011.
Designing and evaluating a wizarded uncertainty-
adaptive spoken dialogue tutoring system. Computer
Speech & Language, 25(1):105?126.
Katherine Forbes-Riley, Diane J. Litman, Scott Silliman,
and Joel R. Tetreault. 2006. Comparing synthesized
versus pre-recorded tutor speech in an intelligent tu-
toring spoken dialogue system. In Proceedings of
the Nineteenth International Florida Artificial Intelli-
gence Research Society Conference, pages 509?514,
Melbourne Beach, Florida, USA, May.
Kate S. Hone and Robert Graham. 2000. Towards a
tool for the subjective assessment of speech system
interfaces (SASSI). Natural Language Engineering,
6(3&4):287?303.
G. Tanner Jackson, Arthur C. Graesser, and Danielle S.
McNamara. 2009. What students expect may have
more impact than what they know or feel. In Proceed-
ings 14th International Conference on Artificial Intel-
ligence in Education (AIED), Brighton, UK.
Lars Bo Larsen. 2003. Issues in the evaluation of spo-
ken dialogue systems using objective and subjective
measures. In Proceedings of 2003 IEEE Workshop
on Automatic Speech Recognition and Understanding
(ASRU?03), pages 209 ? 214, December.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural language pro-
cessing to analyze tutorial dialogue corpora across do-
mains and modalities. In Proceedings of 14th Interna-
tional Conference on Artificial Intelligence in Educa-
tion (AIED), Brighton, UK, July.
Joel Michael, Allen Rovick, Michael Glass, Yujian Zhou,
and Martha Evens. 2003. Learning from a computer
tutor with natural language capabilities. Interactive
Learning Environments, 11:233?262(30).
Sebastian Mo?ller, Paula Smeele, Heleen Boland, and Jan
Krebber. 2007. Evaluating spoken dialogue systems
according to de-facto standards: A case study. Com-
puter Speech & Language, 21(1):26 ? 53.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings of
the 21st International FLAIRS Conference, Coconut
Grove, Florida, May.
Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-
man. 2000. Towards Developing General Models of
Usability with PARADISE. Natural Language Engi-
neering, 6(3).
Maria Wolters, Kallirroi Georgila, Robert Logie, Sarah
MacPherson, Johanna Moore, and Matt Watson. 2009.
Reducing working memory load in spoken dialogue
systems. Interacting with Computers, 21(4):276?287.
170
A REVU-NL Questions
t1 I felt in control of my conversations with the tutor.
t2 It took the tutor too long to respond to my statements.
t3 I felt that the tutor understood me well.
t4 The tutor didn?t always do what I expected.
t5 The information that the tutor provided to me was incomplete.
t6 It was easy for me to become confused during our dialogue.
t7 I enjoyed talking with the tutor.
t8 The tutor interfered with my understanding of the topics in electricity and circuits.
t9 I was not always sure what the tutor expected of me.
t10 Conversing with the tutor was fun.
t11 It was easy to understand the things that the tutor said.
t12 The dialogue between me and the tutor was very repetitive.
t13 I had to really concentrate when I was talking with the tutor.
t14 The tutor was too inflexible.
t15 Working through the lessons with the computer tutor was as easy as working through the lessons
with a human tutor.
t16 The tutor didn?t always do what I wanted.
t17 I felt confident when talking with the tutor.
t18 I always knew what to say to the tutor.
t19 I was able to recover easily from errors during our dialogues.
t20 Talking with the tutor was frustrating.
t21 The information provided by the tutor was clear.
t22 It was easy to interact with the tutor.
t23 The tutor?s dialogue was clumsy and unnatural.
t24 It was easy to learn how to speak to the tutor in a way that the tutor understood.
t25 I felt comfortable talking with the tutor.
t26 I didn?t always understand what the tutor meant.
t27 The tutor was not helpful.
t28 I found conversing with the tutor to be irritating.
t29 I knew what I could say or do at each point in the conversation with the tutor.
t30 I found our dialogues to be boring.
t31 Having the tutor help me with the material was an efficient way to learn.
t32 It was easy to learn from the tutor.
t33 The tutor responded quickly.
t34 Having the tutor was worthwhile
t35 Our dialogues quickly led to me having a deeper understanding of the material.
B REVU-OVERALL questions
o1 Overall, I am satisfied with my experience learning about electricity from this system.
o2 Working in this learning environment was just like working one-on-one with a human tutor.
o3 I would have preferred to learn about electricity in a different way.
o4 I would use this system again in the future to continue to learn about electricity.
o5 I would like to be able to use a system like this to learn about other topics in the future.
171
C REVU-IT questions related to GUI and reading material (mentioned but not analyzed
in the paper)
sl1 It was easy to navigate through the slides.
sl2 It took a long time for each new slide to be displayed.
sl3 The material on the slides was easy to understand.
sl4 The material on the slides was poorly written.
sl5 I would have benefited from more instrucion on how to move through the slides.
sl6 The material on the slides was interesting.
sl7 The slide navigation buttons didn?t always work the way I expected them to.
sl8 The slides were annoying.
sl9 The material on the slides was written at a level far beneath my abilities.
sl10 I would prefer reading a text book over reading these slides.
e1 I found it difficult to learn how to build circuits and take measurements in the workspace.
e2 Completing exercises in the workspace was fun.
e3 Before beginning the lesson, I received the right amount of instruction on how to build circuits in
the workspace and take measurements.
e4 The exercises were well designed to illustrate the important lesson concepts.
e5 Sometimes I didn?t understand what I was supposed to do for an exercise.
e6 The method for connecting components with wires was counter-intuitive.
e7 Having to build all those circuits was annoying.
e8 I always knew exactly what to build and/or measure in the workspace, and how to do it.
e9 Circuits loaded quickly.
e10 Even if I didn?t predict the outcome correctly ahead of time, once I completed an exercise, I
always understood the point.
e11 It was easy to use the meter.
e12 There were more exercises than necessary to cover the lesson topics.
e13 I would have learned more if I had been able to build circuits with actual light bulbs and batteries.
172
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 338?340,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
BEETLE II: an adaptable tutorial dialogue system
Myroslava O. Dzikovska and Amy Isard and Peter Bell and Johanna D. Moore
School of Informatics, University of Edinburgh, Edinburgh, United Kingdom
{m.dzikovska,j.moore,amy.isard,peter.bell}@ed.ac.uk
Natalie Steinhauser and Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division, Orlando, FL, USA
{gwendolyn.campbell,natalie.steihauser}@navy.mil
Abstract
We present BEETLE II, a tutorial dialogue sys-
tem which accepts unrestricted language in-
put and supports experimentation with differ-
ent dialogue strategies. Our first system eval-
uation compared two dialogue policies. The
resulting corpus was used to study the impact
of different tutoring and error recovery strate-
gies on user satisfaction and student interac-
tion style. It can also be used in the future to
study a wide range of research issues in dia-
logue systems.
1 Introduction
There has recently been much interest in develop-
ing tutorial dialogue systems that understand student
explanations (Graesser et al, 1999; Aleven et al,
2001; Nielsen et al, 2008; VanLehn et al, 2007),
because it has been shown that high percentages of
self-explanation and student contentful talk are cor-
related with better learning in human-human tutor-
ing (Chi et al, 1994; Litman et al, 2009). How-
ever, most existing systems use pre-authored tutor
responses for addressing student errors. The advan-
tage of this approach is that tutors can devise reme-
diation dialogues that are highly tailored to specific
misconceptions, providing step-by-step scaffolding
and potentially suggesting additional exercises. The
disadvantage is a lack of adaptivity and generality:
students often get the same remediation for the same
error regardless of their past performance or dia-
logue context. It also becomes more difficult to ex-
periment with different dialogue policies (including
error recovery and tutorial policies determining the
most appropriate feedback), due to the complexities
in applying tutoring strategies consistently in a large
number of hand-authored remediations.
The BEETLE II system architecture is designed to
overcome these limitations (Callaway et al, 2007).
It uses a deep parser and generator, together with
a domain reasoner and a diagnoser, to produce de-
tailed analyses of student utterances and to generate
feedback automatically. This allows the system to
consistently apply the same tutorial policy across a
range of questions. The system?s modular setup and
extensibility also make it a suitable testbed for both
computational linguistics algorithms and more gen-
eral questions about theories of learning.
The system is based on an introductory electric-
ity and electronics course developed by experienced
instructional designers, originally created for use in
a human-human tutoring study. The exercises were
then transferred into a computer system with only
minor adjustments (e.g., breaking down compound
questions into individual questions). This resulted
in a realistic tutoring setup, which presents interest-
ing challenges to language processing components,
involving a wide variety of language phenomena.
We demonstrate a version of the system that un-
derwent a user evaluation in 2009, which found sig-
nificant learning gains for students interacting with
the system. The experimental data collection com-
pared two different dialogue policies implemented
in the system, and resulted in a corpus supporting
research into a variety of questions about human-
computer dialogue interaction (Dzikovska et al,
2010a).
338
Figure 1: Screenshot of the BEETLE II system
2 Example Interaction
The BEETLE II system delivers basic electricity and
electronics tutoring to students with no prior knowl-
edge of the subject. A screenshot is shown in Figure
1. The student interface includes an area to display
reading material, a circuit simulator, and a dialogue
history window. Currently all interactions with the
system are typed. Students read pre-authored cur-
riculum slides and carry out exercises which in-
volve experimenting with the circuit simulator and
explaining the observed behaviour. The system also
asks some high-level questions, such as ?What is
voltage??.
An example dialogue with the system, taken from
the evaluation corpus, is shown in Figure 2. It shows
three key system properties: after the student?s first
turn, which was correct but incomplete, the system
rephrases the correct part of the student answer and
prompts for the missing information. In the second
turn, the system cannot interpret the student utter-
ance, so it responds with a targeted help message
and a hint about the object that needs to be men-
tioned. Finally, in the last turn the system com-
bines the information from the tutor?s hint and the
student?s answers and restates the complete answer
since the current answer was completed over multi-
ple turns.
3 Data Analysis and Future Work
The data collected with the BEETLE II system has
been used to investigate several research questions
regarding discourse and dialogue: the effectiveness
of different error recovery strategies (Dzikovska et
al., 2010b); the underlying dimensions of user sat-
isfaction and their relationship with learning gain
(Dzikovska et al, 2011); the relationship between
(student) alignment in dialogue and learning gain
(Steinhauser et al, 2011); and the differences be-
tween students? social and metacognitive statements
depending on the interaction style (Dzikovska et al,
2010a). We are currently annotating the data with
additional interaction parameters, including correct-
ness of student answers and appropriateness of sys-
tem hints. This will allow us to apply PARADISE
339
Tutor: Why was bulb A on when switch Y
was open and switch Z was closed?
Student: because it had a closed path
Tutor: Right. There is a closed path. Keep
going.
Student: there was a closed path not effected by
the switch
Tutor: I?m sorry, I?m having a problem un-
derstanding. I don?t understand the
word ?effect? here. Here?s a hint. Your
answer should mention a battery.
Student: it has a closed path with the battery
Tutor: You?re on the right track. Here?s the
answer. Bulb A is still contained in a
closed path with the battery.
Figure 2: Example interaction with the system from our
corpus
methodology to get additional insight into which pa-
rameters of tutorial dialogue affect learning gain and
user satisfaction.
We are also adding a speech interface to the sys-
tem, which will open new and interesting research
questions. Students often fail to use domain termi-
nology correctly, and in the tutoring domain it is
important to teach students to use proper terminol-
ogy, so incorrect usage must be detected and cor-
rected. This means that grammar-based language
models are not appropriate for the language mod-
elling, and opens new questions about robust ASR
and language interpretation in such domains.
Acknowledgements
This work has been supported in part by US Of-
fice of Naval Research grants N000141010085 and
N0001410WX20278. We thank Katherine Harrison,
Leanne Taylor, Charles Scott, Simon Caine, Charles
Callaway and Elaine Farrow for their contributions
to this effort.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
Charles B. Callaway, Myroslava Dzikovska, Elaine Far-
row, Manuel Marques-Pita, Colin Matheson, and Jo-
hanna D. Moore. 2007. The Beetle and BeeDiff tutor-
ing systems. In Proceedings of SLaTE?07 (Speech and
Language Technology in Education).
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439?477.
Myroslava Dzikovska, Natalie B. Steinhauser, Jo-
hanna D. Moore, Gwendolyn E. Campbell, Kather-
ine M. Harrison, and Leanne S. Taylor. 2010a. Con-
tent, social, and metacognitive statements: An em-
pirical study comparing human-human and human-
computer tutorial dialogue. In Proceedings of ECTEL-
2010, pages 93?108.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010b. The
impact of interpretation problems on tutorial dialogue.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics(ACL-2010).
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2011. Ex-
ploring user satisfaction in a tutorial dialogue system.
In Proceedings of the 12th annual SIGdial Meeting on
Discourse and Dialogue.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural language pro-
cessing to analyze tutorial dialogue corpora across do-
mains and modalities. In Proc. of 14th International
Conference on Artificial Intelligence in Education.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Learning to assess low-level conceptual under-
standing. In Proceedings 21st International FLAIRS
Conference, Coconut Grove, Florida, May.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava O. Dzikovska, and Johanna D. Moore. 2011.
Talk like an electrician: Student dialogue mimicking
behavior in an intelligent tutoring system. In Proceed-
ings of the 15th International Conference on Artificial
Intelligence in Education (AIED-2011).
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proceedings of
SLaTE Workshop on Speech and Language Technol-
ogy in Education, Farmington, PA, October.
340
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 293?299,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Improving interpretation robustness in a tutorial dialogue system
Myroslava O. Dzikovska and Elaine Farrow and Johanna D. Moore
School of Informatics, University of Edinburgh
Edinburgh, EH8 9AB, United Kingdom
{m.dzikovska,elaine.farrow,j.moore}@ed.ac.uk
Abstract
We present an experiment aimed at improv-
ing interpretation robustness of a tutorial dia-
logue system that relies on detailed semantic
interpretation and dynamic natural language
feedback generation. We show that we can
improve overall interpretation quality by com-
bining the output of a semantic interpreter
with that of a statistical classifier trained on
the subset of student utterances where seman-
tic interpretation fails. This improves on a pre-
vious result which used a similar approach but
trained the classifier on a substantially larger
data set containing all student utterances. Fi-
nally, we discuss how the labels from the sta-
tistical classifier can be integrated effectively
with the dialogue system?s existing error re-
covery policies.
1 Introduction
Giving students formative feedback as they inter-
act with educational applications, such as simu-
lated training environments, problem-solving tutors,
serious games, and exploratory learning environ-
ments, is known to be important for effective learn-
ing (Shute, 2008). Suitable feedback can include
context-appropriate confirmations, hints, and sug-
gestions to help students refine their answers and
increase their understanding of the subject. Pro-
viding this type of feedback automatically, in nat-
ural language, is the goal of tutorial dialogue sys-
tems (Aleven et al, 2002; Dzikovska et al, 2010b;
Graesser et al, 1999; Jordan et al, 2006; Litman and
Silliman, 2004; Khuwaja et al, 1994; Pon-Barry et
al., 2004; VanLehn et al, 2007).
Much work in NLP for educational applications
has focused on automated answer grading (Leacock
and Chodorow, 2003; Pulman and Sukkarieh, 2005;
Mohler et al, 2011). Automated answer assess-
ment systems are commonly trained on large text
corpora. They compare the text of a student answer
with the text of one or more reference answers sup-
plied by human instructors and calculate a score re-
flecting the quality of the match. Automated grad-
ing methods are integrated into intelligent tutoring
systems (ITS) by having system developers antic-
ipate both correct and incorrect responses to each
question, with the system choosing the best match
(Graesser et al, 1999; Jordan et al, 2006; Litman
and Silliman, 2004; VanLehn et al, 2007). Such
systems have wide domain coverage and are robust
to ill-formed input. However, as matching relies on
shallow features and does not provide semantic rep-
resentations of student answers, this approach is less
suitable for dynamically generating adaptive natural
language feedback (Dzikovska et al, 2013).
Real-time simulations and serious games are
commonly used in STEM learning environments
to increase student engagement and support ex-
ploratory learning (Rutten et al, 2012; Mayo, 2007).
Natural language dialogue can help improve learn-
ing in such systems by asking students to explain
their reasoning, either directly during interaction, or
during post-problem reflection (Aleven et al, 2002;
Pon-Barry et al, 2004; Dzikovska et al, 2010b).
Interpretation of student answers in such systems
needs to be grounded in the current state of a dynam-
ically changing environment, and feedback may also
be generated dynamically to reflect the changing
system state. This is typically achieved by employ-
ing hand-crafted parsers and semantic interpreters to
produce structured semantic representations of stu-
dent input, which are then used to instantiate ab-
293
stract tutorial strategies with the help of a natural
language generation system (Freedman, 2000; Clark
et al, 2005; Dzikovska et al, 2010b).
Rule-based semantic interpreters are known to
suffer from robustness and coverage problems, fail-
ing to interpret out-of-grammar student utterances.
In the event of an interpretation failure, most sys-
tems have little information on which to base a feed-
back decision and typically respond by asking the
student to rephrase, or simply give away the answer
(though more sophisticated strategies are sometimes
possible, see Section 4). While statistical scoring ap-
proaches are more robust, they may still suffer from
coverage issues when system designers fail to antic-
ipate the full range of expected student answers. In
one study of a statistical system, a human judge la-
beled 33% of student utterances as not matching any
of the anticipated responses, meaning that the sys-
tem had no information to use as a basis for choos-
ing the next action and fell back on a single strategy,
giving away the answer (Jordan et al, 2009).
Recently, Dzikovska et al (2012b) developed an
annotated corpus of student responses (henceforth,
the SRA corpus) with the goal of facilitating dy-
namic generation of tutorial feedback.1 Student re-
sponses are assigned to one of 5 domain- and task-
independent classes that correspond to typical flaws
found in student answers. These classes can be used
to help a system choose a feedback strategy based
only on the student answer and a single reference
answer. Dzikovska et al (2013) showed that a sta-
tistical classifier trained on this data set can be used
in combination with a semantic interpreter to sig-
nificantly improve the overall quality of natural lan-
guage interpretation in a dialogue-based ITS. The
best results were obtained by using the classifier
to label the utterances that the semantic interpreter
failed to process.
In this paper we further extend this result by
showing that we can obtain similar results by train-
ing the classifier directly on the subset of utterances
that cannot be processed by the interpreter. The
distribution of labels across the classes is differ-
ent in this subset compared to the rest of the cor-
pus. Therefore we can train a subset-specific classi-
1http://www.cs.york.ac.uk/semeval-2013/
task7/index.php?id=data
fier, reducing the amount of annotated training data
needed without compromising performance of the
combined system.
The rest of the paper is organized as follows. In
Section 2 we describe an architecture for combining
semantic interpretation and classification in a sys-
tem with dynamic natural language feedback gener-
ation. In Section 3 we describe an experiment to im-
prove combined system performance using a classi-
fier trained only on non-interpretable utterances. We
discuss future improvements in Section 4.
2 Background
The SRA corpus is made up of two subsets: (1)
the SciEntsBank subset, consisting of written re-
sponses to assessment questions (Nielsen et al,
2008b), and (2) the Beetle subset consisting of ut-
terances collected from student interactions with the
BEETLE II tutorial dialogue system (Dzikovska et
al., 2010b). The SRA corpus annotation scheme
defines 5 classes of student answers (?correct?,
?partially-correct-incomplete?, ?contradictory?, ?ir-
relevant? and ?non-domain?). Each utterance is as-
signed to one of the 5 classes based on pre-existing
manual annotations (Dzikovska et al, 2012b).
We focus on the Beetle subset because the Beetle
data comes from an implemented system, meaning
that we also have access to the semantic interpreta-
tions of student utterances produced by the BEETLE
II interpretation component. The system uses fine-
grained semantic analysis to produce detailed diag-
noses of student answers in terms of correct, incor-
rect, missing and irrelevant parts. We developed a
set of rules to map these diagnoses onto the SRA
corpus 5-class annotation scheme to support system
evaluation (Dzikovska et al, 2012a).
In our previous work (Dzikovska et al, 2013), we
used this mapping as the basis for combining the
output of the BEETLE II semantic interpreter with
the output of a statistical classifier, using a rule-
based policy to determine which label to use for
each instance. If the label from the semantic in-
terpreter is chosen, then the full range of detailed
feedback strategies can be used, based on the corre-
sponding semantic representation. If the classifier?s
label is chosen, then the system can fall back to us-
ing content-free prompts, choosing an appropriate
294
prompt based on the SRA corpus label.
We evaluated 3 rule-based combination policies,
chosen to reduce the effects of the errors that the
semantic interpreter makes, and taking into account
tutoring goals such as reducing student frustration.
The best performing policy takes the classifier?s out-
put if and only if the semantic interpreter is unable
to process the utterance.2 This allows the system to
choose from a wider set of content-free prompts in-
stead of always telling the student that the utterance
was not understood.
As discussed earlier, non-interpretable utterances
present a problem for both rule-based and statistical
approaches. Therefore, we carried out an additional
set of experiments, focusing on the performance of
system combinations that use policies designed to
address non-interpretable utterances. We discuss our
results and future directions in the rest of the paper.
3 Improving Interpretation Robustness
3.1 Experimental Setup
The Beetle portion of the SRA corpus contains 3941
unique student answers to 47 different explanation
questions. Each question is associated with one or
more reference answers provided by expert tutors,
and each student answer is manually annotated with
the label assigned by the BEETLE II interpreter and
a gold-standard correctness label.
In our experiments, we follow the procedure de-
scribed in (Dzikovska et al, 2013), using 10-fold
cross-validation to evaluate the performance of the
various stand-alone and combined systems. We re-
port the per-class F1 scores as evaluation metrics,
using the macro-averaged F1 score as the primary
evaluation metric.
Dzikovska et al (2013) used a statistical classi-
fier based on lexical overlap, taken from (Dzikovska
et al, 2012a), and evaluated 3 different rule-based
policies for combining its output with that of the se-
mantic interpreter. In two of those policies the inter-
preter?s output is always used if it is available, and
the classifier?s label is used for a (subset of) non-
interpretable utterances:
1. NoReject: the classifier?s label is used in all
cases where semantic interpretation fails, thus
2We will refer to such utterances as ?non-interpretable? fol-
lowing (Bohus and Rudnicky, 2005).
creating a system that never rejects student in-
put as non-interpretable
2. NoRejectCorrect: the classifier?s label is
used for non-interpretable utterances which are
labeled as ?correct? by the classifier. This more
conservative policy aims to ensure that correct
student answers are always accepted, but incor-
rect answers may still be rejected with a request
to rephrase.
We conducted a new experiment to evaluate these
two policies together with an enhanced classifier,
discussed in the next section.
3.2 Classifier
For this paper, we extended the classifier from the
previous study (Dzikovska et al, 2013), which we
will call Sim8, with additional features to improve
handling of lexical variability and negation.
Sim8 uses the Weka 3.6.2 implementation of
C4.5 pruned decision trees, with default parameters.
It uses 8 features based on lexical overlap similarity
metrics provided by Perl?s Text::Similarity
package v.0.09: 4 metrics measuring overlap be-
tween the student answer and the expected answer,
and the same 4 metrics applied to the student?s an-
swer and the question text.
In our enhanced classifier, Sim20, we extended
the baseline feature set with 12 additional features.
8 of these are direct analogs of the baseline features,
this time computed on the stemmed text to reduce
the impact of syntactic variation, using the Porter
stemmer from the Lingua::Stem package.3 In
addition, 4 features were added to improve negation
handling and thus detection of contradictions. These
are:
? QuestionNeg, AnswerNeg: features in-
dicating the presence of a negation marker
in the question and the student?s answer re-
spectively, detected using a regular expression.
We distinguish three cases: a negation marker
3We also experimented with features that involve removing
stop words before computing similarity scores, and with using
SVMs for classification, but failed to obtain better performance.
We continue to investigate different SVM kernels and alterna-
tive classification algorithms such as random forests for our fu-
ture work.
295
Standalone Sem. Interp. + Sim20 Sem. Interp. + Sim20NI
Sem. Interp. Sim8 Sim20 no rej no rej corr no rej no rej corr
correct 0.66 0.71 0.71 0.70 0.70 0.70 0.70
pc inc 0.48 0.38 0.40 0.51 0.48 0.50 0.48
contra 0.27 0.40 0.45 0.47 0.27 0.51 0.27
irrlvnt 0.21 0.05 0.08 0.22 0.21 0.22 0.21
nondom 0.65 0.73 0.78 0.83 0.65 0.83 0.65
macro avg 0.45 0.45 0.48 0.55 0.46 0.55 0.46
Table 1: F1 scores for three stand-alone systems, and for combination systems using the Sim20 and Sim20NI
classifiers together with the semantic interpreter. Stand-alone performance for Sim20NI is not shown since it was
trained only on the non-interpretable data subset and is therefore not applicable for the complete data set.
likely to be associated with domain content
(e.g., ?not connected?); a negation marker more
likely to be associated with general expressions
of confusion (such as ?don?t know?); and no
negation marker present.
? BestOverlapNeg: true if the reference an-
swer that has the highest F1 overlap with the
student answer includes a negation marker.
? BestOverlapPolarityMatch: a flag
computed from the values of AnswerNeg and
BestOverlapNeg. Again, we distinguish
three cases: they have the same polarity (both
the student answer and the reference answer
contain negation markers, or both have no
negation markers); they have opposite polar-
ity; or the student answer contains a negation
marker associated with an expression of confu-
sion, as described above.
3.3 Evaluation
Evaluation results are shown in Table 1. Unless
otherwise specified, all performance differences dis-
cussed in the text are significant on an approximate
randomization significance test with 10,000 itera-
tions (Yeh, 2000).
Adding the new features to create the Sim20
classifier resulted in a performance improvement
compared to the Sim8 classifier, raising macro-
averaged F1 from 0.45 to 0.48, with an improvement
in contradiction detection as intended. But these im-
provements did not translate into improvements in
the combined systems. Combinations using Sim20
performed exactly the same as the combinations us-
ing Sim8 (not shown due to space limitations, see
(Dzikovska et al, 2013)). Clearly, more sophisti-
cated features are needed to obtain further perfor-
mance gains in the combined systems.
However, we noted that the subset of non-
interpretable utterances in the corpus has a differ-
ent distribution of labels compared to the full data
set. In the complete data set, 1665 utterances (42%)
are labeled as correct and 1049 (27%) as contradic-
tory. Among the 1416 utterances considered non-
interpretable by the semantic interpreter, 371 (26%)
belong to the ?correct? class, and 598 (42%) to ?con-
tradictory? (other classes have similar distributions
in both subsets). We therefore hypothesized that a
combination system that uses the classifier output
only if an utterance is non-interpretable, may ben-
efit from employing a classifier trained specifically
on this subset rather than on the whole data set.
If our hypothesis is true, it offers an interesting
possibility for combining rule-based and statistical
classifiers in similar setups: if the classifier can be
trained using only the examples that are problematic
for the rule-based system, it can provide improved
robustness at a significantly lower annotation cost.
We therefore trained another classifier,
Sim20NI, using the same feature set as Sim20,
but this time using only the instances rejected
as non-interpretable by the semantic interpreter
in each cross-validation fold (1416 utterances,
36% of all data instances). We again used the
NoReject and NoRejectCorrect policies to
combine the output of Sim20NI with that of the
semantic interpreter. Evaluation results confirmed
our hypothesis. The system combinations that
use Sim20 and Sim20NI perform identically on
296
macro-averaged F1, with NoReject being the best
combination policy in both cases and significantly
outperforming the semantic interpreter alone. How-
ever, the Sim20NI classifier has the advantage of
needing significantly less annotated data to achieve
this performance.
4 Discussion and Future Work
Our research focuses on combining deep and shal-
low processing by supplementing fine-grained se-
mantic interpretations from a rule-based system
with more coarse-grained classification labels. Al-
ternatively, we could try to learn structured se-
mantic representations from annotated text (Zettle-
moyer and Collins, 2005; Wong and Mooney, 2007;
Kwiatkowski et al, 2010), or to learn more fine-
grained assessment labels (Nielsen et al, 2008a).
However, such approaches require substantially
larger annotation effort. Therefore, we believe it is
worth exploring the use of the simpler 5-label anno-
tation scheme from the SRA corpus. We previously
showed that it is possible to improve system perfor-
mance by combining the output of a symbolic inter-
preter with that of a statistical classifier (Dzikovska
et al, 2013). The best combination policy used the
statistical classifier to label utterances rejected as
non-interpretable by the rule-based interpreter.
In this paper, we showed that similar results can
be achieved by training the classifier only on non-
interpretable utterances, rather than on the whole la-
beled corpus. The student answers that the inter-
preter has difficulty with have a distinct distribution,
which is effectively utilized by training a classifier
only on this subset. This reduces the amount of an-
notated training data needed, reducing the amount of
manual labor required.
In future, we will further investigate the best com-
bination of parsing and statistical classification in
systems that offer sophisticated error recovery poli-
cies for non-understandings. Our top-performing
policy, NoReject, uses deep parsing and semantic
interpretation to produce a detailed semantic analy-
sis for the majority of utterances, and falls back on a
shallower statistical classifier for utterances that are
difficult for the interpreter. This policy assumes that
it is always better to use a content-free prompt than
to reject a non-interpretable student utterance. How-
ever, interpretation problems can arise from incor-
rect uses of terminology, and learning to speak in
the language of the domain has been positively cor-
related with learning outcomes (Steinhauser et al,
2011). Therefore, rejecting some non-interpretable
answers as incorrect could be a valid tutoring strat-
egy (Sagae et al, 2010; Dzikovska et al, 2010a).
The BEETLE II system offers several error re-
covery strategies intended to help students phrase
their answers in more acceptable ways by giving a
targeted help message, e.g., ?I am sorry, I?m hav-
ing trouble understanding. Paths cannot be broken,
only components can be broken? (Dzikovska et al,
2010a). Therefore, it may be worthwhile to con-
sider other combination policies. We evaluated the
NoRejectCorrect policy, which uses the statis-
tical classifier to identify correct answers rejected
by the semantic interpreter and asks for rephrasings
in other cases. Using this policy resulted in only a
small improvement in system performance. A dif-
ferent classifier geared towards more accurate iden-
tification of correct answers may help, and we are
planning to investigate this option in the future.
Alternatively, we could consider a combination
policy which looks for rejected answers that the
classifier identifies as contradictory and changes the
wording of the targeted help message to indicate that
the student may have made a mistake, instead of
apologizing for the misunderstanding. This has the
potential to help students learn correct terminology
rather than presenting the issue as strictly an inter-
pretation failure.
Ultimately, all combination policies must be
tested with users to ensure that improved robust-
ness translates into improved system effectiveness.
We have previously studied the effectiveness of our
targeted help strategies with respect to improving
learning outcomes (Dzikovska et al, 2010a). A sim-
ilar study is required to evaluate our combination
strategies.
Acknowledgments
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine and Sarah Denhe
for help with data collection and preparation. The
research reported here was supported by the US
ONR award N000141010085.
297
References
Vincent Aleven, Octav Popescu, and Kenneth R.
Koedinger. 2002. Pilot-testing a tutorial dialogue sys-
tem that supports self-explanation. In Proc. of ITS-02
conference, pages 344?354.
Dan Bohus and Alexander Rudnicky. 2005. Sorry,
I didn?t catch that! - An investigation of non-
understanding errors and recovery strategies. In Pro-
ceedings of SIGdial-2005, Lisbon, Portugal.
Brady Clark, Oliver Lemon, Alexander Gruenstein, Eliz-
abethOwen Bratt, John Fry, Stanley Peters, Heather
Pon-Barry, Karl Schultz, Zack Thomsen-Gray, and
Pucktada Treeratpituk. 2005. A general purpose ar-
chitecture for intelligent tutoring systems. In JanC.J.
Kuppevelt, Laila Dybkjr, and NielsOle Bernsen, edi-
tors, Advances in Natural Multimodal Dialogue Sys-
tems, volume 30 of Text, Speech and Language Tech-
nology, pages 287?305. Springer Netherlands.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010a. The
impact of interpretation problems on tutorial dialogue.
In Proc. of ACL 2010 Conference Short Papers, pages
43?48.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Peter Bell, Amy Isard, and Jo-
hanna D. Moore. 2012a. Evaluating language under-
standing accuracy with respect to objective outcomes
in a dialogue system. In Proc. of EACL-12 Confer-
ence, pages 471?481.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012b. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Myroslava O. Dzikovska, Elaine Farrow, and Johanna D.
Moore. 2013. Combining semantic interpretation and
statistical classification for improved explanation pro-
cessing in a tutorial dialogue system. In In Proceed-
ings of the The 16th International Conference on Ar-
tificial Intelligence in Education (AIED 2013), Mem-
phis, TN, USA, July.
Reva Freedman. 2000. Using a reactive planner as the
basis for a dialogue agent. In Proceedings of the Thir-
teenth Florida Artificial Intelligence Research Sympo-
sium (FLAIRS 2000), pages 203?208.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Pamela Jordan, Diane Litman, Michael Lipschultz, and
Joanna Drummond. 2009. Evidence of misunder-
standings in tutorial dialogue and their impact on
learning. In Proc. of 14th International Conference on
Artificial Intelligence in Education, pages 125?132.
Ramzan A. Khuwaja, Martha W. Evens, Joel A. Michael,
and Allen A. Rovick. 1994. Architecture of
CIRCSIM-tutor (v.3): A smart cardiovascular physi-
ology tutor. In Proc. of 7th Annual IEEE Computer-
Based Medical Systems Symposium.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proc. of EMNLP-2010 Conference,
pages 1223?1233.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Diane J. Litman and Scott Silliman. 2004. ITSPOKE:
an intelligent tutoring spoken dialogue system. In
Demonstration Papers at HLT-NAACL 2004, pages 5?
8, Boston, Massachusetts.
Merrilea J. Mayo. 2007. Games for science and engi-
neering education. Commun. ACM, 50(7):30?35, July.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
298
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Nico Rutten, Wouter R. van Joolingen, and Jan T. van der
Veen. 2012. The learning effects of computer simula-
tions in science education. Computers and Education,
58(1):136 ? 153.
Alicia Sagae, W. Lewis Johnson, and Stephen Bodnar.
2010. Validation of a dialog system for language
learners. In Proceedings of the 11th Annual Meeting of
the Special Interest Group on Discourse and Dialogue,
SIGDIAL ?10, pages 241?244, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Valerie J Shute. 2008. Focus on formative feedback.
Review of educational research, 78(1):153?189.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava O. Dzikovska, and Johanna D. Moore. 2011.
Talk like an electrician: Student dialogue mimicking
behavior in an intelligent tutoring system. In Proc. of
15th international conference on Artificial Intelligence
in Education, pages 361?368.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2007), Prague, Czech Republic, June.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to Map Sentences to Logical Form: Structured
Classification with Probabilistic Categorial Grammars.
In Proceedings of the 21th Annual Conference on
Uncertainty in Artificial Intelligence (UAI-05), pages
658?666, Arlington, Virginia. AUAI Press.
299
