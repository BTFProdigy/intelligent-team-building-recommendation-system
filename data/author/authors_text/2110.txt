Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 123?130,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Speech to speech machine translation:
Biblical chatter from Finnish to English
David Ellis
Brown University
Providence, RI 02912
Mathias Creutz Timo Honkela
Helsinki University of Technology
FIN-02015 TKK, Finland
Mikko Kurimo
Abstract
Speech-to-speech machine translation is in
some ways the peak of natural language pro-
cessing, in that it deals directly with our
original, oral mode of communication (as
opposed to derived written language). As
such, it presents challenges that are not to be
taken lightly. Although existing technology
covers each of the steps in the process, from
speech recognition to synthesis, deriving a
model of translation that is effective in the
domain of spoken language is an interesting
and challenging task. If we could teach our
algorithms to learn as children acquire lan-
guage, the result would be useful both for
language technology and cognitive science.
We propose several potential approaches, an
implementation of a multi-path model that
translates recognized morphemes alongside
words, and a web-interface to test our speech
translation tool as trained for Finnish to En-
glish. We also discuss current approaches to
machine translation and the problems they
face in adapting simultaneously to morpho-
logically rich languages and to the spoken
modality.
1 Introduction
Effective and fluent machine translation poses many
challenges, and often requires a variety of resources.
Some are language-specific, some domain-specific,
and others manage to be relatively independent (one
might even say context-free), and thus generally ap-
plicable in a wide variety of circumstances. There
are still untapped resources, however, that might
benefit machine translation systems. Most statistical
approaches do not take into account any similarities
in word forms, so words that share a common root,
(like ?blanche? and ?bianca?, meaning ?white? in
French and Italian respectively) are no more likely to
be aligned than others (like ?vache? and ?guardare?,
meaning ?cow? and ?to watch? respectively). Such
a root is sometimes subject to vowel shift and conso-
nant gradation, and may not be reflected in orthog-
raphy, since it is often purely phonetic.
This means we are not taking advantage of every-
thing that normally benefits human speakers, hear-
ers and translators. It may be that a more natural
approach to translation would first involve under-
standing of the input, stored in some mental rep-
resentation (an interlingua), and then generation of
an equivalent phrase in the target language, directly
from the knowledge sources.
In order to allow for more dramatic differences
in grammar like agglutinativity, it seems that the
statistical machine translation (SMT) system must
be more aware of sub-word units (morphemes) and
features (phonetic similarity). This general sort of
morphological approach could potentially benefit
any language pair, but might be crucial for a sys-
tem that handles Finnish, Turkish, Hungarian, Es-
tonian or any other highly inflectional language. In
the following section we discuss the confounds pre-
sented by agglutinative languages, and how aware-
ness of morphemes might improve the situation.
This is followed by a brief foray into semantics
and natural language generation as a component of
123
SMT. Capturing phonetic features is most applicable
to speech-to-speech translation, which will be dis-
cussed in the penultimate section. A description of
the Bible conversation experiment and some of its
results can be found in the final section.
2 Agglutinative Confounds
Traditional n-gram language models and phrase-
based translation models do not work terribly well
for Finnish because each lexical item can appear in
dozens of inflected or declined forms. If an SMT
system is presented with ?taloosi? (to your house), it
will not know if that is another form of a word it saw
in training (like ?taloissaan?, in their houses). Align-
ment data are thus unnaturally sparse and test sen-
tences often contain several unknown items, which
share their stems with trained words. It has been
assumed that morphological analysis would be es-
sential for handling agglutinative languages. How-
ever, although several effective segmenters and an-
alyzers for specific languages exist, and even unsu-
pervised language-neutral versions such as Morfes-
sor (Creutz and Lagus, 2007), only recently have
similar approaches been successfully used in the
context of machine translation to improve the BLEU
score (Oflazer and El-Kahlout, 2007), and none yet
in Finnish.
In our experience, building a translation model
through stemmed (truncated) word-alignment out-
performs full-form alignment, or any morph-
segmented alignment. But once one has generated
such a translation model, including phrase tables
where stemmed forms (keys in source language)
are associated with full forms (values in target lan-
guage), is there anything to be gained from induction
of morphology? Our research in this area has yet to
reveal any positive results, but we are still working
on it. It is also worth considering the effectiveness of
the evaluation metrics. Does BLEU accurately cap-
ture the accuracy of a translation, particularly in an
agglutinative language? Unfortunately not.
We think the word segmentation in the BLEU
metric is biased against progress in morpheme-level
translation. Some other metrics have been set forth,
but none is widely accepted, in part due to inertia,
but also because translation cannot be objectively
evaluated, unless both the communicative intent and
its effectiveness can be quantified. The same prob-
lem occurs for teachers grading essays ? what was
the student intending to convey, was the phrasing
correct, the argument sound, and where does all this
diverge from the underlying power of words, written
or well said, to transmit information? Translation is
an art, and maybe in addition to human evaluation
by linguists and native speakers of the language, we
should consider the equivalent of an art or literary
critic. On the other hand, that might only be worth-
while for poetry, wherein automated translation is
perhaps not the best approach.
One might think that the stemmed model de-
scribed above would lose track of closed-class func-
tion items (like prepositions), particularly when they
are represented as inflectional morphemes in one
language but as separate words in the other. How-
ever, it seems that the language model for the target
takes care of that quite well in most cases. There
are some languages (like Japanese) with underspec-
ified noun phrases, in which efforts to preserve def-
initeness (i.e., the book, kirjan; a book, kirjaa) seem
futile, but given the abundance of monolingual data
to train LM?s on, these are contextually inferred and
corrected at the tail end of the production line. Ag-
glutinative confounds are thus very closely related to
other issues found throughout machine translation,
and perhaps an integrated solution (including a new
evaluation metric) is necessary.
3 Knowledge-Based Approaches
Incorporating statistical natural language generation
into a machine translation system involves some
modifications to the above. First, the source lan-
guage is translated or parsed into ontological rep-
resentations. This is similar to sentence parsing
techniques that can be used to induce a context-free
grammar for a language (Charniak, 1997), and could
in fact be considered one of their more useful appli-
cations. The parsing generally depends on a proba-
bilistic model trained on sentences aligned with their
syntactic and semantic representations, often in a
tree that could be generated by a context-free gram-
mar. The resulting semantic representation can then
be used as the source of a target-language generation
process.
The algorithm that generates such a representa-
124
tion from raw input could be trained on a tree-
bank, and an annotated form of the same corpus
(where the derivations in the generation space are
associated with counts for each decision made) can
be used to train the output component to generate
language. (Belz, 2005) To incorporate the statisti-
cal component, which allows for robust generaliza-
tion, per (Knight and Hatzivassiloglou, 1995), the
NLG on the target side is filtered through a language
model (described above). This helps address many
of the knowledge gap problems introduced by lin-
guistic differences or in a component of the system
- the analyzer or generator.
This approach does have significant advantages,
particularly in that it is more focused on semantics
(as opposed to statistical cooccurrence), so it may
be less likely to distort meaning. On the other hand,
it could misinterpret or miscommunicate (or both),
just like a human translator. Perhaps the crucial dif-
ference is that, while machine learning often has lit-
tle to do with our understanding of cognitive pro-
cesses, this sort of machine translation has greater
potential for illuminating mysterious areas of the hu-
man process. It is not an ersatz brain, nor neural
network, but in many ways it has more in common
with those technologies (particularly in that they
model cognition) than many natural language pro-
cessing algorithms. That is because, if we can get
a semantically-aware machine translation system to
work, it may more closely mirror human cognition.
Humans certainly do not ignore meaning when they
translate, but today?s statistical machine translation
has no awareness of it at all.
Potential disadvantages of the system include its
dependence on more resources. However, this is
less of a problem with WordNet(Miller, 1995) and
other such semantic webs. It is also worth men-
tioning again that humans always have an incred-
ible amount of information at their disposal when
translating. Not only all of their past experience and
word-knowledge, but their interlocutor?s demeanor,
manner, intonation, facial expressions, gestures, and
so on. There are often things that would be obvi-
ous in the context of a conversation, but are missing
from the transcribed text. For instance, the referent
of many pronouns is ambiguous, but usually there is
a unique individual or item picked out by the speak-
ers? shared information. This is true for simple sen-
tences like ?He hit him,? which are normally dis-
ambiguated by conversational context, but a purely
statistical, pseudo-syntactic interpretation would get
little of the meaning a human would glean from that
utterance.
4 Spoken Features
Speech-to-speech machine translation is in some
ways the peak of natural language processing, in that
it deals directly with our (humans?) original, oral
mode of communication (as opposed to derived writ-
ten language). As such, it presents challenges that
are not to be taken lightly. Much of the pipeline in-
volved is at least relatively straightforward: acoustic
modeling and language modeling on the input side
can take advantage of the latest advances without
extensive adaptation; similarly, speech synthesis on
the output can be directly connected with the system
(i.e., not work with text output, but a richer repre-
sentation).
Although such a system might seem quite com-
plicated, it could better take advantage of all the
available data. Natural language understanding and
generation could even be incorporated to an extent,
perhaps to add further confidence measures based
on semantic equivalence. Designing it in this way
also allows for a variety of methods to be tried with
ease, in a modular fashion. It may be that yet an-
other source of information can be found to improve
the translation by adding features to the translation
model ? perhaps leveraging multilingual corpora in
other languages, segmenting into morphemes earlier
in the process, or even incorporating intonation in
some fashion. Weights for all such features could
be learned during training, such that no language-
specific tuning would be necessary. This framework
would certainly not make speech-to-speech transla-
tion simple, but its flexibility might make research
and improvement in this area more tractable.
Efficiency is crucial in online translation of con-
versation, so a word alignment model with collapsed
Gibbs sampling, rather than EM, at its core is worth
experimenting with. We have written up a bare-
bones IBM Model 1 in both C++ and Python, us-
ing the standard EM approach and a Gibbs sampling
one. The latter allows for optimizations using lin-
ear algebra, and although it does not quite match the
125
perplexity or log-likelihood achieved by EM, it is
significantly faster, particularly on longer sentences.
Since morpheme segmentation is at least somewhat
helpful in speech recognition (Creutz, 2006; Creutz
et al, 2007), it should still be considered a potential
component in speech-to-speech translation. In terms
of incorporating the knowledge-based approach into
such a system, we think it may yet be too early,
but if existing understanding-and-generation frame-
works for machine translation could be adapted to
this use, it could be very fruitful, in particular since
spoken language generation might be more effective
from a knowledge base, since it would know what
it was trying to say, instead of relying on statistics
alone, hoping the phonemes end up in a meaningful
order.
The critical step of SST is, of course, transla-
tion. In an integrated system, as described above,
the translation model could be trained on a parallel
spoken corpus (perhaps tokenized into phonemes, or
segmented into morphemes), since there might be
advantages to limiting the intermediate steps in the
process. The Bible is a massively multilingual publi-
cation, and as it happens, its text is available aligned
between Finnish and English, and it is possible to
find corresponding recordings in both languages.
So, this corpus would enable a direct approach
to speech-to-speech translation. Alternatively, one
could treat the speech recognition and synthesis as
distinct from the translation, in which case text cor-
pora corresponding to the style and genre of speech
would be necessary. This would be particularly fea-
sible when, for instance, translating UN parliamen-
tary proceedings from a recording, for which trans-
lated transcripts are readily available. For a more
general and robust solution, we might advocate a
combined approach, in the hope that some potential
weaknesses of one might be avoided or compensated
for by using whatever limited resources are available
to add features from the other. Thus, a direct trans-
lation from speech to speech could be informed, in a
sense, by a derived translation from the recognized
text.
5 Biblical Chatter
Here, we present a system for translating Finnish to
English speech, in a restricted and ancient domain:
the Bible.
5.1 Introduction
Speech to speech translation attacks a variety of
problems at once, from speech recognition to syn-
thesis, and can similarly be used for several pur-
poses. If a system is efficient enough to be used
without introducing significant delay, it can trans-
late conversational speech online, acting as an in-
terpreter in place of (or in cooperation with) a hu-
man professional. On the other hand, a slow speech
translation system is still useful because it can make
news broadcasts (radio or television) accessible to
wider audiences through offline multilingual dub-
bing, allowing international viewers to enjoy a de-
layed broadcast.
5.2 System Description
The domain selected for our experiments was heav-
ily influenced by the available data. We needed a
bilingual (Finnish and English) and bimodal (text
and speech) corpus, and unfortunately none is read-
ily available, but we put one together using the
Bible. Both Old and New Testaments were used,
with one book from each left out for testing pur-
poses. We used multiple editions of the Bible to
train the translation model: the American Standard
Version (first published in 1901, updated 1997),
and Finnish translations (from 1992 and 1933,38).
The spoken recordings used were the World English
Bible (1997) and Finnish Bible (Raamattu) readings
(recorded at TKK 2004).
Our approach was to use existing components,
and try weaving them together in an optimal way.
First, there is the open vocabulary automatic speech
recognition (ASR) task, where the goal is to de-
tect phonemes in an acoustic signal and map them
to words. Here, we use an ?unlimited vocabu-
lary? continuous speech recognizer (Hirsima?ki et al,
2006), trained on a multi-speaker Finnish acoustic
model with a varigram (Siivola et al, 2007) lan-
guage model that includes Bible n-grams. Then,
for translation, Moses (Koehn et al, 2007) is trained
on words and morphemes (derived from Morfessor
Baseline (Creutz and Lagus, 2005)). For speech syn-
thesis, we used Festival (Taylor, 1999), including the
built-in English voice and a Finnish voice developed
at Helsinki University.
126
5.3 Results
The following is an example fragment, taken from
the test corpus.
Niin Daavid meni David slept with his
lepoon isiensa? luo, fathers, and was
ja ha?nethaudattiin buried in the
Daavidin kaupunkiin. city of David. The days
Nelja?kymmenta? vuotta that David reigned
ha?n oli ollut over Israel were
Israelin kuninkaana. forty years; seven
Hebronissa ha?n years reigned he
hallitsi seitsema?n in Hebron, and
vuotta, Jerusalemissa thirty-three years
kolmenkymmenenkolmen reigned he
vuoden ajan. in Jerusalem.
Salomo nousi Solomon sat on
isa?nsa? Daavidin the throne of David
valtaistuimelle,ja his father; and
ha?nen kuninkuutensa his kingdom was
vahvistui lujaksi. established greatly.
A translation of the reference text skips recogni-
tion, and runs the system from translation to synthe-
sis. The following shows how the sample text was
translated by our system (BLEU = 0.735):
Niin Daavid meni so david slept with his
lepoon isiensa? luo, fathers and was
ja ha?net haudattiin buried in the
Daavidin kaupunkiin. city of david
Nelja?kymmenta? vuotta forty years he
ha?n oli ollut was king over
Israelin kuninkaana. israel and in
Hebronissa ha?n hebron he reigned
hallitsi seitsema?n seven years
vuotta, Jerusalemissa in jerusalem
kolmenkymmenenkolmen thirty and three
vuoden ajan. years solomon
Salomo nousi went up to
isa?nsa? Daavidin the throne of
valtaistuimelle, ja david his father
ha?nen kuninkuutensa and his kingdom
vahvistui lujaksi. was strong for luja
The following recognized translation (BLEU =
0.541) represents a complete run of the system. The
recognition (on the left) had a LER of 12.9% and a
WER of 56.8%.
niintaa meni niintaa went
lepoon isiensa?lla isiensa?lla rest and was
ja ha?net haudattiin buried in the
daavidin kaupunkiin city of david the king
nelja?kymmenta? of israel was
vuotta ha?n oli ollut forty years he was
israelin kuninkaan in hebron he
hebronissa ha?n reigned seven years
hallitsi seitsema?n in jerusalem
vuotta jerusalemissa kymmenenkolmen
kolmen kymmenenkolmen three years
vuoden ajan after the new
salomon uusi on the throne of david
isa?nsa? daavidin and solomon
valtaistuimelle ja his father
ha?nenkuninkuutensa ha?nenkuninkuutensa
valmistulujaksi valmistulujaksi
Here we have an alternative path through the sys-
tem, which uses Morfessor on the recognized text,
and then translates using a model trained on the
morpheme-segmented corpus. This results in a re-
duced score (BLEU = 0.456), but fewer unknown
words.
n iin taa# meni# iin behind went to
lepo on# isi ensa? lla# the sabbath that
ja# ha?n et# hauda ttiin# is with ensa? and he
daavid in# kaupunki in# shall not at the grave of abner
nelja?kymmenta?# vuotta# was forty years of the
ha?n# oli# ollut# city of david and
israeli n kuninkaan# he was israeli to
hebron issa# ha?n# the king of hebron
hallitsi# seitsema?n# and he reigned
vuotta# jerusalem seven years in
issa# kolmen# jerusalem three tenth
kymmenen kolmen# three years of
vuoden# ajan# the new solomon his
salomo n# uusi# isa? istuim to david
nsa?# daavid in# my father of the
valta istuim elle# kingdoms of
ja# ha?n en kun ink ink and he
uutensa# valmistu luja ksi# uutensa valmistu to luja
The morphemes might have been more effective
in translation if they had been derived through rule-
based morphological analysis. Or, they could still be
statistical, but optimized for the translation phase by
minimizing perplexity during word alignment.
A significant barrier to thorough and concrete
evaluation of our system involves segmentation of
the speech stream into sentences (or verses) to match
the text. In the above examples, we manually
clipped the audio files. Evaluating performance on
the entire test set reduced the BLEU score if the
data were streamed through each component unseg-
mented. When the recognizer was set to insert a pe-
riod for detected pauses of a certain length, or at sen-
tence boundaries identified by its language model,
127
input to the translation phase became considerably
more problematic. In particular, the lattice input
ought to be split into sentences, but there would usu-
ally be a period in every time slice (but with low
probability).
5.4 Discussion
There were significant difficulties in the process,
particularly in the English to Finnish direction.
Whereas Finnish speech recognition is relatively
straightforward, since its orthography is consistent,
English speech recognition is more dependent on
a pronunciation dictionary. Although many such
dictionaries are available, and the pronunciation of
novel words can be estimated, neither of these re-
sources is terribly effective within the Bible domain,
where there are many archaic words and names. In
the second step, translation into Finnish is demon-
strably difficult from any source language, and re-
sults in consistently lower BLEU scores (Virpioja
et al, 2007). And although using morphemes can
reduce the frequency of unknown words, it also re-
duces the BLEU score.
It might improve translation quality if we use the
recognizer lattice as translator input, since acous-
tically improbable segments may lead to the most
fluent translation. Having access to many possibili-
ties might help the translation model, but then again,
second-guessing the recognizer might not be help-
ful. There were some difficulties with the Moses in-
tegration, in part because the word-sausage format
varies from SRILM?s. Also, the recognizer output
indicates word boundaries as <w>, not string-final
hash-marks (#). This is problematic since the for-
mer are separate symbols, occupying a node in the
lattice, whereas the latter are appended to another
symbol (e.g., ?<w> morph eme </w>?, 4 nodes,
versus ?morph eme#?, 2 nodes). Using the lattice,
final output from Moses tends to be more fluent,
but less on-topic, and often truncated. Although we
have no improvements thus far, it is likely that with
further parameter tuning, we could achieve better re-
sults. On the other hand, we seek a general, robust,
domain-independent solution, so focusing on Bible
translation may not be worthwhile.
Our speech-to-speech translation system is
accessible through a web interface.
http://www.cis.hut.fi/projects/speech/
translate/
It accepts a sound file, with recorded Finnish
bible-style chatter, an optional reference text and
translation, and within a half hour (usually much
less) sends a detailed report, including a sound file
with the synthesized English.
Ideas for future research include online speech-
to-speech translation, which must be efficient, light-
weight and robust. A potential barrier to this and
other applications is the lack of spoken language
training texts. It might be possible to adaptively train
to new speakers and contexts, perhaps taking advan-
tage of an efficient alternative to EM in word align-
ment (see discussion of Gibbs sampling). As men-
tioned elsewhere, it might be worth using prosodic
features captured during recognition as factors in
translation. Adapting existing resources to new lan-
guage pairs is particularly essential in an area where
so much is necessary, and so little available.
6 Conclusion
We cannot yet say for sure whether linguistic or
statistically optimized morphemes derived from text
corpora could be useful somehow in machine trans-
lation, but it has been demonstrated helpful in
speech recognition. Awareness of sub-word units
could benefit a speech-to-speech translation system,
and it may in fact help to maintain information
from the speech recognizer about morpheme seg-
mentation throughout the translation process, even
in speech generation. Incorporating natural lan-
guage understanding may also be fruitful, but for
compact, efficient systems (like a handheld transla-
tor) might not have access to the necessary resources
or computational power to support that. On the other
hand, it is our duty as researchers to stay ahead of the
technology and push its limits.
We are by no means the first to imagine this, but
perhaps we will soon be speaking into wrist watches
that understand our query, seemingly instantly shift
through more information than Google has currently
indexed, and reply in fluent English, Finnish, or Pun-
jabi with as much detail as could be hoped for after
hours of painstaking research with current technol-
ogy. In this case (and computational linguists must
always be optimistic), knowledge-based natural lan-
guage processing certainly has a crucial place.
128
Morphemes and agglutinative languages do pose
unique problems for computational linguists, but
many of the general techniques developed for lan-
guages like Arabic and Chinese, which are equally
far from English in grammar (and even orthogra-
phy), might surmount those problems without any
manual adaptation. Discriminative training of fea-
tures used in the translation model allows for such
solutions to be molded automatically to whatever
language pair (and set of corpora) they are being
used for. There is, as always, much more to be done
in this area, and we hope our research into efficient,
online Bible-conversational translation ? a modern
Babelfish in an ancient genre? is fruitful, and helps
to shed light on lemmatization.
Acknowledgments
Many thanks to Teemu Hirsima?ki, Antti Puurula,
Sami-Virpioja and Jaakko J. Va?yrynen for their
help with components of the system and for their
thoughts and comments at various stages of the
project.
References
Anja Belz. 2005. Statistical generation: Three methods
compared and evaluated. In Proceedings of the 10th
European Workshop on Natural Language Generation
(ENLG05), pages 15?23.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
AAAI/IAAI, pages 598?603.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Technical Re-
port A81, Publications in Computer and Information
Science, Helsinki University of Technology.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1), January.
Mathias Creutz, Teemu Hirsim?ki, Mikko Kurimo, Antti
Puurula, Janne Pylkknen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Saraclar, and Andreas
Stolcke. 2007. Analysis of morph-based speech
recognition and the modeling of out-of-vocabulary
words across languages. In Proceedings of Hu-
man Language Technologies / The Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL 2007),
Rochester, NY, USA.
Mathias Creutz. 2006. Morfessor in the morpho chal-
lenge. In Mikko Kurimo, Mathias Creutz, and Krista
Lagus, editors, Proceedings of the PASCAL Challenge
Workshop on Unsupervised Segmentation of Words
into Morphemes, Venice, Italy.
T. Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-
pioja, and J. Pylkko?nen. 2006. Unlimited vocabu-
lary speech recognition with morph language models
applied to Finnish. Computer Speech and Language,
20(4):515?541.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings of
the 33rd annual meeting on Association for Compu-
tational Linguistics, pages 252?260, Morristown, NJ,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Ondrej Bojar, Alexandra Constantin, and Evan
Herb. 2007. Moses: Open source toolkit for statistical
machine translation. In Proceedings of the ACL 2007
Demo and Poster Sessions, pages 177?180.
George A. Miller. 1995. Wordnet: a lexical database for
English. Commun. ACM, 38(11):39?41.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proceedings
of the ACL 2007 Demo and Poster Sessions, pages 25?
32.
Vesa Siivola, Teemu Hirsima?ki, and Sami Virpioja. 2007.
On growing and pruning Kneser-Ney smoothed n-
gram models. IEEE Transactions on Audio, Speech
and Language Processing, 15(5):1617?1624.
Paul Taylor. 1999. The Festival Speech Architecture.
Web page.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology?aware statis-
tical machine translation based on morphs induced in
an unsupervised manner. In Proceedings of the Ma-
chine Translation Summit XI, Copenhagen, Denmark.
To appear.
129
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
130
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 168?175,
New York, June 2006. c?2006 Association for Computational Linguistics
Multilevel Coarse-to-fine PCFG Parsing
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil,
David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths,
Jeremy Moore, Michael Pozar, and Theresa Vu
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
ec@cs.brown.edu
Abstract
We present a PCFG parsing algorithm
that uses a multilevel coarse-to-fine
(mlctf) scheme to improve the effi-
ciency of search for the best parse.
Our approach requires the user to spec-
ify a sequence of nested partitions or
equivalence classes of the PCFG non-
terminals. We define a sequence of
PCFGs corresponding to each parti-
tion, where the nonterminals of each
PCFG are clusters of nonterminals of
the original source PCFG. We use the
results of parsing at a coarser level
(i.e., grammar defined in terms of a
coarser partition) to prune the next
finer level. We present experiments
showing that with our algorithm the
work load (as measured by the total
number of constituents processed) is
decreased by a factor of ten with no de-
crease in parsing accuracy compared to
standard CKY parsing with the origi-
nal PCFG. We suggest that the search
space over mlctf algorithms is almost
totally unexplored so that future work
should be able to improve significantly
on these results.
1 Introduction
Reasonably accurate constituent-based parsing
is fairly quick these days, if fairly quick means
about a second per sentence. Unfortunately, this
is still too slow for many applications. In some
cases researchers need large quantities of parsed
data and do not have the hundreds of machines
necessary to parse gigaword corpora in a week
or two. More pressingly, in real-time applica-
tions such as speech recognition, a parser would
be only a part of a much larger system, and
the system builders are not keen on giving the
parser one of the ten seconds available to pro-
cess, say, a thirty-word sentence. Even worse,
some applications require the parsing of multi-
ple candidate strings per sentence (Johnson and
Charniak, 2004) or parsing from a lattice (Hall
and Johnson, 2004), and in these applications
parsing efficiency is even more important.
We present here a multilevel coarse-to-fine
(mlctf) PCFG parsing algorithm that reduces
the complexity of the search involved in find-
ing the best parse. It defines a sequence of in-
creasingly more complex PCFGs, and uses the
parse forest produced by one PCFG to prune
the search of the next more complex PCFG.
We currently use four levels of grammars in our
mlctf algorithm. The simplest PCFG, which we
call the level-0 grammar, contains only one non-
trivial nonterminal and is so simple that min-
imal time is needed to parse a sentence using
it. Nonetheless, we demonstrate that it identi-
fies the locations of correct constituents of the
parse tree (the ?gold constituents?) with high
recall. Our level-1 grammar distinguishes only
argument from modifier phrases (i.e., it has two
nontrivial nonterminals), while our level-2 gram-
mar distinguishes the four major phrasal cate-
gories (verbal, nominal, adjectival and preposi-
tional phrases), and level 3 distinguishes all of
the standard categories of the Penn treebank.
168
The nonterminal categories in these grammars
can be regarded as clusters or equivalence classes
of the original Penn treebank nonterminal cat-
egories. (In fact, we obtain these grammars by
relabeling the node labels in the treebank and
extracting a PCFG from this relabelled treebank
in the standard fashion, but we discuss other ap-
proaches below.) We require that the partition
of the nonterminals defined by the equivalence
classes at level l + 1 be a refinement of the par-
tition defined at level l. This means that each
nonterminal category at level l+1 is mapped to a
unique nonterminal category at level l (although
in general the mapping is many to one, i.e., each
nonterminal category at level l corresponds to
several nonterminal categories at level l + 1).
We use the correspondence between categories
at different levels to prune possible constituents.
A constituent is considered at level l + 1 only
if the corresponding constituent at level l has
a probability exceeding some threshold.. Thus
parsing a sentence proceeds as follows. We first
parse the sentence with the level-0 grammar to
produce a parse forest using the CKY parsing
algorithm. Then for each level l + 1 we reparse
the sentence with the level l + 1 grammar us-
ing the level l parse forest to prune as described
above. As we demonstrate, this leads to consid-
erable efficiency improvements.
The paper proceeds as follows. We next dis-
cuss previous work (Section 2). Section 3 out-
lines the algorithm in more detail. Section
4 presents some experiments showing that the
work load (as measured by the total number of
constituents processed) is decreased by a fac-
tor of ten over standard CKY parsing at the
final level. We also discuss some fine points of
the results therein. Finally in section 5 we sug-
gest that because the search space of mlctf al-
gorithms is, at this point, almost totally unex-
plored, future work should be able to improve
significantly on these results.
2 Previous Research
Coarse-to-fine search is an idea that has ap-
peared several times in the literature of com-
putational linguistics and related areas. The
first appearance of this idea we are aware of is
in Maxwell and Kaplan (1993), where a cover-
ing CFG is automatically extracted from a more
detailed unification grammar and used to iden-
tify the possible locations of constituents in the
more detailed parses of the sentence. Maxwell
and Kaplan use their covering CFG to prune the
search of their unification grammar parser in es-
sentially the same manner as we do here, and
demonstrate significant performance improve-
ments by using their coarse-to-fine approach.
The basic theory of coarse-to-fine approxima-
tions and dynamic programming in a stochastic
framework is laid out in Geman and Kochanek
(2001). This paper describes the multilevel
dynamic programming algorithm needed for
coarse-to-fine analysis (which they apply to de-
coding rather than parsing), and show how
to perform exact coarse-to-fine computation,
rather than the heuristic search we perform here.
A paper closely related to ours is Goodman
(1997). In our terminology, Goodman?s parser
is a two-stage ctf parser. The second stage is a
standard tree-bank parser while the first stage is
a regular-expression approximation of the gram-
mar. Again, the second stage is constrained by
the parses found in the first stage. Neither stage
is smoothed. The parser of Charniak (2000)
is also a two-stage ctf model, where the first
stage is a smoothed Markov grammar (it uses
up to three previous constituents as context),
and the second stage is a lexicalized Markov
grammar with extra annotations about parents
and grandparents. The second stage explores
all of the constituents not pruned out after the
first stage. Related approaches are used in Hall
(2004) and Charniak and Johnson (2005).
A quite different approach to parsing effi-
ciency is taken in Caraballo and Charniak (1998)
(and refined in Charniak et al (1998)). Here
efficiency is gained by using a standard chart-
parsing algorithm and pulling constituents off
the agenda according to (an estimate of) their
probability given the sentence. This probability
is computed by estimating Equation 1:
p(nki,j | s) =
?(nki,j)?(nki,j)
p(s) . (1)
169
It must be estimated because during the
bottom-up chart-parsing algorithm, the true
outside probability cannot be computed. The
results cited in Caraballo and Charniak (1998)
cannot be compared directly to ours, but are
roughly in the same equivalence class. Those
presented in Charniak et al (1998) are superior,
but in Section 5 below we suggest that a com-
bination of the techniques could yield better re-
sults still.
Klein and Manning (2003a) describe efficient
A? for the most likely parse, where pruning is
accomplished by using Equation 1 and a true
upper bound on the outside probability. While
their maximum is a looser estimate of the out-
side probability, it is an admissible heuristic and
together with an A? search is guaranteed to find
the best parse first. One question is if the guar-
antee is worth the extra search required by the
looser estimate of the true outside probability.
Tsuruoka and Tsujii (2004) explore the frame-
work developed in Klein and Manning (2003a),
and seek ways to minimize the time required
by the heap manipulations necessary in this
scheme. They describe an iterative deepening
algorithm that does not require a heap. They
also speed computation by precomputing more
accurate upper bounds on the outside proba-
bilities of various kinds of constituents. They
are able to reduce by half the number of con-
stituents required to find the best parse (com-
pared to CKY).
Most recently, McDonald et al (2005) have
implemented a dependency parser with good
accuracy (it is almost as good at dependency
parsing as Charniak (2000)) and very impres-
sive speed (it is about ten times faster than
Collins (1997) and four times faster than Char-
niak (2000)). It achieves its speed in part be-
cause it uses the Eisner and Satta (1999) algo-
rithm for n3 bilexical parsing, but also because
dependency parsing has a much lower grammar
constant than does standard PCFG parsing ?
after all, there are no phrasal constituents to
consider. The current paper can be thought of
as a way to take the sting out of the grammar
constant for PCFGs by parsing first with very
few phrasal constituents and adding them only
Level: 0 1 2 3
S1
{
S1
{
S1
{
S1
P
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
HP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
S
?
?
?
?
?
?
?
?
?
?
?
?
?
S
VP
UCP
SQ
SBAR
SBARQ
SINV
N
?
?
?
?
?
?
?
?
?
?
?
?
?
NP
NAC
NX
LST
X
UCP
FRAG
MP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
A
?
?
?
?
?
?
?
?
?
?
?
?
?
ADJP
QP
CONJP
ADVP
INTJ
PRN
PRT
P
?
?
?
?
?
?
?
?
?
?
?
?
?
PP
PRT
RRC
WHADJP
WHADVP
WHNP
WHPP
Figure 1: The levels of nonterminal labels
after most constituents have been pruned away.
3 Multilevel Course-to-fine Parsing
We use as the underlying parsing algorithm a
reasonably standard CKY parser, modified to
allow unary branching rules.
The complete nonterminal clustering is given
in Figure 1. We do not cluster preterminals.
These remain fixed at all levels to the standard
Penn-tree-bank set Marcus et al (1993).
Level-0 makes two distinctions, the root node
and everybody else. At level 1 we make one
further distinction, between phrases that tend
to be heads of constituents (NPs, VPs, and Ss)
and those that tend to be modifiers (ADJPs,
PPs, etc.). Level-2 has a total of five categories:
root, things that are typically headed by nouns,
those headed by verbs, things headed by prepo-
sitions, and things headed by classical modifiers
(adjectives, adverbs, etc.). Finally, level 3 is the
170
S1
P
P
PRP
He
P
VBD
ate
P
IN
at
P
DT
the
NN
mall
.
.
S1
HP
HP
PRP
He
HP
VBD
ate
MP
IN
at
HP
DT
the
NN
mall
.
.
S1
S_
N_
PRP
He
S_
VBD
ate
P_
IN
at
N_
DT
the
NN
mall
.
.
S1
S
NP
PRP
He
VP
VBD
ate
PP
IN
at
NP
DT
the
NN
mall
.
.
Figure 2: A tree represented at levels 0 to 3
classical tree-bank set. As an example, Figure 2
shows the parse for the sentence ?He ate at the
mall.? at levels 0 to 3.
During training we create four grammars, one
for each level of granularity. So, for example, at
level 1 the tree-bank rule
S ?NP VP .
would be translated into the rule
HP ?HP HP .
That is, each constituent type found in ?S ?NP
VP .? is mapped into its generalization at level 1.
The probabilities of all rules are computed us-
ing maximum likelihood for constituents at that
level.
The grammar used by the parser can best be
described as being influenced by four compo-
nents:
1. the nonterminals defined at that level of
parsing,
2. the binarization scheme,
3. the generalizations defined over the bina-
rization, and
4. extra annotation to improve parsing accu-
racy.
The first of these has already been covered. We
discuss the other three in turn.
In anticipation of eventually lexicalizing the
grammar we binarize from the head out. For
example, consider the rule
A ?a b c d e
where c is the head constituent. We binarize
this as follows:
A ?A1 e
A1 ?A2 d
A2 ?a A3
A3 ?b c
Grammars induced in this way tend to be
too specific, as the binarization introduce a very
large number of very specialized phrasal cat-
egories (the Ai). Following common practice
Johnson (1998; Klein and Manning (2003b) we
Markovize by replacing these nonterminals with
ones that remember less of the immediate rule
context. In our version we keep track of only the
parent, the head constituent and the constituent
immediately to the right or left, depending on
which side of the constituent we are processing.
With this scheme the above rules now look like
this:
A ?Ad,c e
Ad,c ?Aa,c d
Aa,c ?a Ab,c
Ab,c ?b c
So, for example, the rule ?A ?Ad,c e? would
have a high probability if constituents of type
A, with c as their head, often have d followed
by e at their end.
Lastly, we add parent annotation to phrasal
categories to improve parsing accuracy. If we
assume that in this case we are expanding a rule
for an A used as a child of Q, and a, b, c, d, and
e are all phrasal categories, then the above rules
become:
AQ ?Ad,c eA
Ad,c ?Aa,c dA
Aa,c ?aA Ab,c
Ab,c ?bA cA
171
10?8 10?7 10?6 10?5 10?4 10?3
0.0001
0.001
0.01
0.1
 
 
Level 0
Level 1
Level 2
Level 3
Figure 3: Probability of a gold constituent being
pruned as a function of pruning thresholds for
the first 100 sentences of the development corpus
Once we have parsed at a level, we evaluate
the probability of a constituent p(nki,j | s) ac-
cording to the standard inside-outside formula
of Equation 1. In this equation nki,j is a con-
stituent of type k spanning the words i to j, and
?(?) and ?(?) are the outside and inside proba-
bilities of the constituent, respectively. Because
we prune at the end each granularity level, we
can evaluate the equation exactly; no approxi-
mations are needed (as in, e.g., Charniak et al
(1998)).
During parsing, instead of building each con-
stituent allowed by the grammar, we first test
if the probability of the corresponding coarser
constituent (which we have from Equation 1 in
the previous round of parsing) is greater than
a threshold. (The threshold is set empirically
based upon the development data.) If it is below
the threshold, we do not put the constituent in
the chart. For example, before we can use a NP
and a VP to create a S (using the rule above),
we would first need to check that the probability
in the coarser grammar of using the same span
HP and HP to create a HP is above the thresh-
old. We use the standard inside-outside for-
mula to calculate this probability (Equation 1).
The empirical results below justify our conjec-
ture that there are thresholds that allow signifi-
cant pruning while leaving the gold constituents
untouched.
10?8 10?7 10?6 10?5 10?4 10?3
0.001
0.01
0.1
1
 
 
Level 0
Level 1
Level 2
Level 3
Figure 4: Fraction of incorrect constituents kept
as a function of pruning thresholds for the first
100 sentences of the development corpus
4 Results
In all experiments the system is trained on the
Penn tree-bank sections 2-21. Section 23 is used
for testing and section 24 for development. The
input to the parser are the gold-standard parts
of speech, not the words.
The point of parsing at multiple levels of gran-
ularity is to prune the results of rough levels be-
fore going on to finer levels. In particular, it is
necessary for any pruning scheme to retain the
true (gold-standard WSJ) constituents in the
face of the pruning. To gain an idea of what
is possible, consider Figure 3. According to the
graph, at the zeroth level of parsing and a the
pruning level 10?4 the probability that a gold
constituent is deleted due to pruning is slightly
more than 0.001 (or 0.1%). At level three it is
slightly more that 0.01 (or 1.0%).
The companion figure, Figure 4 shows the
retention rate of the non-gold (incorrect) con-
stituents. Again, at pruning level 10?4 and pars-
ing level 0 we retain about .3 (30%) of the bad
constituents (so we pruned 70%), whereas at
level 3 we retain about .004 (0.4%). Note that
in the current paper we do not actually prune
at level 3, instead return the Viterbi parse. We
include pruning results here in anticipation of
future work in which level 3 would be a precur-
sor to still more fine-grained parsing.
As noted in Section 2, there is some (implicit)
172
Level Constits Constits % Pruned
Produced Pruned
?106 ?106
0 8.82 7.55 86.5
1 9.18 6.51 70.8
2 11.2 9.48 84.4
3 11,8 0 0.0
total 40.4 ? ?
3-only 392.0 0 0
Figure 5: Total constituents pruned at all levels
for WSJ section 23, sentences of length ? 100
debate in the literature on using estimates of
the outside probability in Equation 1, or instead
computing the exact upper bound. The idea is
that an exact upper bound gives one an admis-
sible search heuristic but at a cost, since it is a
less accurate estimator of the true outside prob-
ability. (Note that even the upper bound does
not, in general, keep all of the gold constituents,
since a non-perfect model will assign some of
them low probability.) As is clear from Figure
3, the estimate works very well indeed.
On the basis of this graph, we set the lowest
allowable constituent probability at ? 5 ? 10?4,
? 10?5, and ? 10?4 for levels 0,1, and 2, re-
spectively. No pruning is done at level 3, since
there is no level 4. After setting the pruning
parameters on the development set we proceed
to parse the test set (WSJ section 23). Figure 5
shows the resulting pruning statistics. The to-
tal number of constituents created at level 0, for
all sentences combined, is 8.82 ? 106. Of those
7.55 ? 106 (or 86.5%) are pruned before going on
to level 1. At level 1, the 1.3 million left over
from level 0 expanded to a total of 9.18 ? 106.
70.8% of these in turn are pruned, and so forth.
The percent pruned at, e.g., level 1 in Figure 3
is much higher than that shown here because it
considers all of the possible level-1 constituents,
not just those left unpruned after level 0.
There is no pruning at level 3. There we sim-
ply return the Viterbi parse. We also show that
with pruning we generate a total of 40.4 ? 106
constituents. For comparison exhaustively pars-
ing using the tree-bank grammar yields a total
of 392 ? 106 constituents. This is the factor-of-10
Level Time for Level Running Total
0 1598 1598
1 2570 4168
2 4303 8471
3 1527 9998
3-only 114654 ?
Figure 6: Running times in seconds on WSJ sec-
tion 23, with and without pruning
workload reduction mentioned in Section 1.
There are two points of interest. The first is
that each level of pruning is worthwhile. We do
not get most of the effect from one or the other
level. The second point is that we get signif-
icant pruning at level 0. The reader may re-
member that level 0 distinguishes only between
the root node and the rest. We initially ex-
pected that it would be too coarse to distinguish
good from bad constituents at this level, but it
proved as useful as the other levels. The expla-
nation is that this level does use the full tree-
bank preterminal tags, and in many cases these
alone are sufficient to make certain constituents
very unlikely. For example, what is the proba-
bility of any constituent of length two or greater
ending in a preposition? The answer is: very
low. Similarly for constituents of length two or
greater ending in modal verbs, and determiners.
Not quite so improbable, but nevertheless less
likely than most, would be constituents ending
in verbs, or ending just short of the end of the
sentence.
Figure 6 shows how much time is spent at each
level of the algorithm, along with a running to-
tal of the time spent to that point. (This is for
all sentences in the test set, length ? 100.) The
number for the unpruned parser is again about
ten times that for the pruned version, but the
number for the standard CKY version is prob-
ably too high. Because our CKY implementa-
tion is quite slow, we ran the unpruned version
on many machines and summed the results. In
all likelihood at least some of these machines
were overloaded, a fact that our local job dis-
tributer would not notice. We suspect that the
real number is significantly lower, though still
173
No pruning 77.9
With pruning 77.9
Klein and Manning (2003b) 77.4
Figure 7: Labeled precision/recall f-measure,
WSJ section 23, all sentences of length ? 100
much higher than the pruned version.
Finally Figure 7 shows that our pruning is ac-
complished without loss of accuracy. The results
with pruning include four sentences that did not
receive any parses at all. These sentences re-
ceived zeros for both precision and recall and
presumably lowered the results somewhat. We
allowed ourselves to look at the first of these,
which turned out to contain the phrase:
(NP ... (INTJ (UH oh) (UH yes)) ...)
The training data does not include interjections
consisting of two ?UH?s, and thus a gold parse
cannot be constructed. Note that a different
binarization scheme (e.g. the one used in Klein
and Manning (2003b) would have smoothed over
this problem. In our case the unpruned version
is able to patch together a lot of very unlikely
constituents to produce a parse, but not a very
good one. Thus we attribute the problem not to
pruning, but to binarization.
We also show the results for the most similar
Klein and Manning (2003b) experiment. Our
results are slightly better. We attribute the dif-
ference to the fact that we have the gold tags
and they do not, but their binarization scheme
does not run into the problems that we encoun-
tered.
5 Conclusion and Future Research
We have presented a novel parsing algorithm
based upon the coarse-to-fine processing model.
Several aspects of the method recommend it.
First, unlike methods that depend on best-first
search, the method is ?holistic? in its evalua-
tion of constituents. For example, consider the
impact of parent labeling. It has been repeat-
edly shown to improve parsing accuracy (John-
son, 1998; Charniak, 2000; Klein and Manning,
2003b), but it is difficult if not impossible to
integrate with best-first search in bottom-up
chart-parsing (as in Charniak et al (1998)). The
reason is that when working bottom up it is diffi-
cult to determine if, say, ssbar is any more or less
likely than ss, as the evidence, working bottom
up, is negligible. Since our method computes
the exact outside probability of constituents (al-
beit at a coarser level) all of the top down in-
formation is available to the system. Or again,
another very useful feature in English parsing
is the knowledge that a constituent ends at the
right boundary (minus punctuation) of a string.
This can be included only in an ad-hoc way when
working bottom up, but could be easily added
here.
Many aspects of the current implementation
that are far from optimal. It seems clear to
us that extracting the maximum benefit from
our pruning would involve taking the unpruned
constituents and specifying them in all possible
ways allowed by the next level of granularity.
What we actually did is to propose all possi-
ble constituents at the next level, and immedi-
ately rule out those lacking a corresponding con-
stituent remaining at the previous level. This
was dictated by ease of implementation. Before
using mlctf parsing in a production parser, the
other method should be evaluated to see if our
intuitions of greater efficiency are correct.
It is also possible to combine mlctf parsing
with queue reordering methods. The best-first
search method of Charniak et al (1998) esti-
mates Equation 1. Working bottom up, estimat-
ing the inside probability is easy (we just sum
the probability of all the trees found to build
this constituent). All the cleverness goes into
estimating the outside probability. Quite clearly
the current method could be used to provide a
more accurate estimate of the outside probabil-
ity, namely the outside probability at the coarser
level of granularity.
There is one more future-research topic to add
before we stop, possibly the most interesting of
all. The particular tree of coarser to finer con-
stituents that governs our mlctf algorithm (Fig-
ure 1) was created by hand after about 15 min-
utes of reflection and survives, except for typos,
with only two modifications. There is no rea-
174
son to think it is anywhere close to optimal. It
should be possible to define ?optimal? formally
and search for the best mlctf constituent tree.
This would be a clustering problem, and, for-
tunately, one thing statistical NLP researchers
know how to do is cluster.
Acknowledgments
This paper is the class project for Computer
Science 241 at Brown University in fall 2005.
The faculty involved were supported in part
by DARPA GALE contract HR0011-06-2-0001.
The graduate students were mostly supported
by Brown University fellowships. The under-
graduates were mostly supported by their par-
ents. Our thanks to all.
References
Sharon Caraballo and Eugene Charniak. 1998. Fig-
ures of merit for best-first probabalistic parsing.
Computational Linguistics, 24(2):275?298.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 2005 Meeting of
the Association for Computational Linguistics.
Eugene Charniak, Sharon Goldwater, and Mark
Johnson. 1998. Edge-based best-first chart pars-
ing. In Proceedings of the Sixth Workshop on
Very Large Corpora, pages 127?133. Morgan Kauf-
mann.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 132?139.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, San Francisco. Mor-
gan Kaufmann.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 457?464.
Stuart Geman and Kevin Kochanek. 2001. Dy-
namic programming and the representation of
soft-decodable codes. IEEE Transactions on In-
formation Theory, 47:549?568.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 1997).
Keith Hall and Mark Johnson. 2004. Attention shift-
ing for parsing speech. In The Proceedings of the
42th Annual Meeting of the Association for Com-
putational Linguistics, pages 40?46.
Keith Hall. 2004. Best-first Word-lattice Pars-
ing: Techniques for Integrated Syntactic Language
Modeling. Ph.D. thesis, Brown University.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 33?
39.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003a. A* parsing:
Fast exact viterbi parse selection. In Proceedings
of HLT-NAACL?03.
Dan Klein and Christopher Manning. 2003b. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics.
Michell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?
330.
John T. Maxwell and Ronald M. Kaplan. 1993.
The interface between phrasal and functional con-
straints. Computational Linguistics, 19(4):571?
590.
Ryan McDonald, Toby Crammer, and Fernando
Pereira. 2005. Online large margin training of
dependency parsers. In Proceedings of the 43rd
Meeting of the Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2004. It-
erative cky parsing for probabilistic context-free
grammars. In International Joint Conference on
Natural-Language Processing.
175
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 12?19,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
A Bayesian model of natural language phonology:
generating alternations from underlying forms
David Ellis
de@cs.brown.edu
Brown University
Providence, RI 02912
Abstract
A stochastic approach to learning phonology.
The model presented captures 7-15% more
phonologically plausible underlying forms
than a simple majority solution, because it
prefers ?pure? alternations. It could be use-
ful in cases where an approximate solution is
needed, or as a seed for more complex mod-
els. A similar process could be involved in
some stages of child language acquisition; in
particular, early learning of phonotactics.
1 Introduction
Sound changes in natural language, such as stem
variation in inflected forms, can be described as
phonological processes. These are governed by a
constraint hierarchy as in Optimality Theory (OT),
or by a set of ordered rules. Both rely on a sin-
gle lexical representation of each morpheme (i.e., its
underlying form), and context-sensitive transforma-
tions to surface forms. Phonological changes often
affect segments near morpheme boundaries, but can
also apply over an entire prosodic word, as in vowel
harmony.
It does not seem straightforward to incorporate
context into a Bayesian model of phonology, al-
though a clever solution may yet be found. A
standard way of incorporating conditioning envi-
ronments is to treat them as factors in a Gibbs
model (Liang and Klein, 2007), but such models
require an explicit calculation of the partition func-
tion. Unless the rule contexts possess some kind of
locality, we don?t know how to compute this par-
tition function efficiently. Some context could be
captured by generating underlying phonemes from
an n-gram model, or by annotating surface forms
with neighborhood features. However, the effects of
autosegmental phonology and other long-range de-
pendencies (like vowel harmony) cannot be easily
Bayesianized.
1.1 Related Work
In the last decade, finite-state approaches to phonol-
ogy (Gildea and Jurafsky, 1996; Beesley and Kart-
tunen, 2000) have effectively brought theoretical lin-
guistic work on rewrite rules into the computational
realm. A finite-state approximation of optimality
theory (Karttunen, 1998) was later refined into a
compact treatment of gradient constraints (Gerde-
mann and van Noord, 2000).
Recent work on Bayesian models of morpholog-
ical segmentation (Johnson et al, 2007) could be
combined with phonological rule induction (Gold-
water and Johnson, 2004) in a variety of ways,
some of which will be explored in our discussion
of future work. Also, the Hierarchical Bayes Com-
piler (Daume III, 2007) could be used to generate a
model similar to the one presented here, but less con-
strained1 which makes correspondingly more ran-
dom, less accurate predictions.
1.2 Dataset
As we describe the model and its implementation in
this and subsequent sections, we will refer to a sam-
1Recent updates to HBC, inspired by discussions with the
author, have addressed some of these limitations.
12
ple dataset (in Figure 1), consisting of a paradigm2
of verb stems and person/number suffixes. The
head of each row or column is an /underlying/ form,
which in 3rd person singular is a phonologically null
segment (represented as /?/). In [surface] forms, the
realization of each morpheme is affected by phono-
logical processes. For example, in the combination
of /tieta?/ + /vat/, the result is [tieta?+va?t], where the
3rd person plural /a/ becomes [a?] due to vowel har-
mony.
1.3 Bayesian Approach
As a baseline model, we select the most frequently
occurring allophone as the underlying form. Our
goal is to outperform this baseline using a Bayesian
model. In other words, what patterns in phonologi-
cal processes can be inferred with such a statistical
model? This simple framework begins learning with
the assumption that the underlying forms are faithful
to the surface (i.e., without considering markedness
or phonotactics).
We model the generation of surface forms from
underlying ones on the segmental (character) level.
Input is an inflectional paradigm, with tokens of the
form stem+suffix. Morphology is limited to a
single suffix (no agglutination), and is already iden-
tified. Each character of an underlying stem or suf-
fix (ui) generates surface characters (sij) in an entire
row or column of the input.
To capture the phonology of a variety of lan-
guages with a single model, we need constraints
from linguistically plausible priors (universal gram-
mar). We prefer that underlying characters be pre-
served in surface forms, especially when there is no
alternation. It is also reasonable that there be fewer
underlying forms (phonemes) than surface forms
(phones, phonetic inventory), to account for allo-
phones. We expect to be able to capture a signifi-
cant subset of phonological processes using a simple
model (only faithfulness constraints).
1.4 Pure Generators
Our model has an advantage over the baseline in its
preference for ?purity? in underlying forms. Each
underlying segment should generate as few distinct
2The paradigm format lends itself to analysis of word types,
but if supplemented with surface counts, can also handle tokens.
surface segments as possible: if it generates non-
alternating (identical) segments, it will be less likely
to generate an alternation in addition. This means
that when two segments alternate, the underlying
form should be the one that appears less frequently
in other contexts, irrespective of the majority within
the alternation.
In the first stem of our Finnish verb conjugation
(Figure 1), we see a [t,d] alternation (a case of con-
sonant gradation), as well as unalternating [t]. If we
isolate three of the surface forms where /tieta?/ is in-
flected (1st person singular, and 3rd person singular
and plural), and consider only the dental segments in
the stem of each, we have two underlying segments.
Here, we use question marks to indicate unknown
underlying segments.
/??/ [dt] [tt] [tt]
In this subset of the data, the reasonable candidate
underlying forms are /t/ and /d/. These two compete
to explain the observed data (surface forms). The na-
ture of the prior probability distribution determines
whether the majority is hypothesized for each under-
lying form, so /t/ produces both alternating and unal-
ternating surface segments, or /d/ is hypothesized as
the source of the alternation (and /t/ remains ?pure?).
In a Bayesian setting, we impose a sparse prior over
underlying forms conditioned on the surface forms
they generate.
If u2 is hypothesized to be /t/, the posterior prob-
ability of u1 being /t/ goes down:
P (u1 = /t/|u2 = /t/) < P (u1 = /t/)
The probability of u1 being the competitor, /d/, cor-
respondingly increases:
P (u1 = /d/|u2 = /t/) > P (u1 = /d/)
Even though the majority in this case would be /t/,
the favored candidate for the alternating form was
/d/. This happened because of how we defined the
model?s prior, in combination with the evidence that
/t/ (assigned to u2) generated the sequence of [t]. So
selection bias prefers /d/ as the source of an ambigu-
ous segment, leaving /t/ to always generate itself.
A similar effect can occur if there are both unal-
ternating [t]?s and [d]?s on the surface, in addition to
the [t,d] alternation. The candidate (/t/ or /d/) that is
13
aaaaaa
StemSuffix /n/ (1s) /t/ (2s) /?/ (3s) /mme/ (1p) /tte/ (2p) /vat/ (3p)
/tieta?/ [tieda?+n] [tieda?+t] [tieta?+a?] [tieda?+mme] [tieda?+tte] [tieta?+va?t]
/aiko/ [ai?o+n] [ai?o+t] [aiko+o] [ai?o+mme] [ai?o+tte] [aiko+vat]
/luke/ [lu?e+n] [lu?e+t] [luke+e] [lu?e+mme] [lu?e+tte] [luke+vat]
/puhu/ [puhu+n] [puhu+t] [puhu+u] [puhu+mme] [puhu+tte] [puhu+vat]
/saa/ [saa+n] [saa+t] [saa+?] [saa+mme] [saa+tte] [saa+vat]
/tule/ [tule+n] [tule+t] [tule+e] [tule+mme] [tule+tte] [tule+vat]
/pelka?a?/ [pelka?a?+n] [pelka?a?+t] [pelka?a?+?] [pelka?a?+mme] [pelka?a?+tte] [pelka?a?+va?t]
Figure 1: Sample dataset (constructed by hand): Finnish verbs, with inflection for person and number.
generating fewer unalternating segments is preferred
to explain the alternation. For example, if there were
1000 cases of [t], 500 [d] and 500 [t,d], we would ex-
pect the following hypotheses: /t/ ? [t], /d/ ? [d]
and /d/ ? [t, d]. This is because one of the two
candidates must be responsible for both unalternat-
ing and alternating segments, but we prefer to have
as much ??purity? as possible, to minimize ambigu-
ity.
With this solution, we still have 1000 pure /t/ ?
[t], and only the 500 /d/ ? [d] are now indistinct
from /d/ ? [t, d]. If we had selected /t/ as the
source of the alternation, there would be only 500
remaining ?pure? (/d/) segments, and 1500 ambigu-
ous /t/. Our Bayesian model should prefer the less
ambiguous (?purer?) solution, given an appropriate
prior.
2 Model
We will use boldface to indicate vectors, and sub-
scripts to identify an element from a vector or ma-
trix. The variable N(u) is a vector of observed
counts with the current underlying form hypothe-
ses. The notation we use for a vector u with one
element i removed is u?i, so we can exclude the
counts associated with a particular underlying form
by indicating that in the parenthesized variable (i.e.,
N(u?4) is all the counts except those associated with
the fourth underlying form). Ni(u) is the number of
times character i is used as an underlying form, and
Nij(u) is the number of times character i generated
surface character j.
The priors over surface s and underlying u seg-
ments in Figure 2 are captured by Dirichlet priors
? and ?, which generate the multinomial distribu-
tions ? and ?, respectively (see Figure 3). The
prior over underlying form encourages sparse solu-
tions, so ?u < 1 for all u. The prior over surface
form given underlying encourages identity mapping,
/x/ ? [x], so ?xx > 1, and discourages different
segments, /x/ ? [y], so ?xy < 1 for all x 6= y.
nc
?
?
nu
mnu
s
u
?
?
Figure 2: Bayesian network: ? and ? are vectors of hy-
perparameters, and ?i (for i ? {1, . . . , nc}) and ? are
distributions. u is a vector of underlying forms, generated
from ?, and si (for i ? nu) is a set of observed surface
forms generated from the hidden variable ui according to
?i
Phones and phonemes are drawn from a set of
characters (e.g., IPA, unicode) C used to represent
them. ?i is the probability of a character (Ci for
i ? nc) being an underlying form, irrespective of
current alignments or its position in the paradigm.
?ij is the conditional probability of a surface char-
14
?c | ? ? DIR(?), c = 1, . . . , nc
? | ? ? DIR(?)
ui | ?i ? MULTI(?i), i = 1, . . . , nu
sij | ui,?ui ? MULTI(?ui), i = 1, . . . , nu,
j = 1, . . . ,mi
Figure 3: Model parameters: nc is # different segments,
nu is # underlying segments
acter (skn = Cj for j ? nc, n ? mk) given the
underlying character it is generated from (uk = Ci
for i ? nc, k ? nu), which is determined by its po-
sition in the paradigm.
In our Finnish example (Figure 1), if k = 1, we
are looking at the first underlying character, which
is /t/ (from /tieta?/), so assuming our character set is
the Finnish alphabet, of which ?t? is the 20th char-
acter, u1 = C20 = t. It generates the first character
of each inflected form (1st, 2nd, 3rd person, singu-
lar and plural) of that stem, so m1 = 6, and since
there is no alternation s1n = t (for n ? {1, . . . , 6}).
Given the phonologically plausible (gold) underly-
ing forms, the probability of /t/ is ?20 = 7/41.
On the other hand, k = 33 identifies the 3rd per-
son singular /?/, which inflects each of the seven
stems, so m33 = 7. Since we need our alpha-
bet to identify a null character, we?ll give it in-
dex zero (i.e., u33 = C0 = ?). For each of the
(underlying, surface) alignments in this alternation
(caused by vowel gemination), we can identify the
probability in ?. For 3rd person singular [tieta?+a?],
where s33,1 = C28 = a?, the conditional probability
?0,28 = 1/7.
The prior hyperparameters can be understood as
follows. As ?i gets smaller, an underlying form uk
is less likely to be Ci. As ?ij gets smaller, an un-
derlying uk = Ci is less likely to generate a surface
segment skn = Cj ?n ? mk. In our experiments,
we will vary ?i=j (prior over identity map from un-
derlying to surface) and ?i6=j .
Our implementation of this model uses Gibbs
sampling (c.f., (Bishop, 2006), pp 542-8), an algo-
rithm that produces samples from the posterior dis-
tribution. Each sample is an assignment of the hid-
den variables, u (i.e., a set of hypothesized underly-
ing forms). Our sampler initializes u from a uniform
distribution over segments in the training data, and
resamples underlying forms in a fixed order, as in-
put in the paradigm. Rather than reestimate ? and
? at each iteration before sampling from u, we can
marginalize these intermediate probability distribu-
tions in order to ease implementation and speed con-
vergence.
Our search procedure tries to sample from the
posterior probability, according to Bayes? rule.
posterior ? likelihood ? prior
P (u, s|?,?) ? P (u|?)P (s, u|?)
Each of these probabilities is drawn from a Dirichlet
distribution, which is defined in terms of the multi-
variate Beta function, C . The prior ? added to un-
derlying counts N(u) forms the posterior Dirichlet
corresponding to P (u|?). In P (s|u,?), each ?i
vector is supplemented by the observed counts of
(underlying, surface) pairs N(si).
P (u, s|?,?) = C(? + N(u))C(?)
nc
?
c=1
C(?c +
?
i:ui=c N(si))
C(?)
The collapsed update procedure consists of re-
sampling each underlying form, u, incorporating the
prior hyperparameters ?,? and counts N over the
rest of the dataset. The relevant counts for a can-
didate k being the underlying form ui are Nk(u?i)
and Nksij (u?i) for j ? mi. P (ui = k|u?i,?,?) is
proportional to the probability of generating ui = k,
given the other u?i and all sij (for j ? mi), given
s?i and u?i.
P (ui = c|u?i,?,?) ? Nc(u?i) + ?cn? 1 + ??
C(? + ?i? 6=i:ui?=c N(s
?
i) + N(si))
C(? + ?i? 6=i:ui?=c N(s
?
i))
Suppose we were updating this sampler running
on the Finnish verb inflections. If we had all seg-
ments as in Figure 1, but wanted to resample u31 (1st
person singular /n/), we would consider the counts
N excluding that form (i.e., under u?31). The prior
for /n/, ?14, is fixed, and there are no other occur-
rences, so N14(u?31) = 0. Another potential un-
derlying form, like /t/, would have higher uncondi-
tioned posterior probability, because of the counts
15
(7, in this case) added to its prior from ?. Then, we
have to multiply by the probability of each generated
surface segment (all are [n], so 7 ? P ([n]|c,?) for a
given hypothesis u31 = c).
We select a given character c ? C for u31 from a
distribution at random. Depending on the prior, /n/
will be the most likely choice, but other values are
still possible with smaller probability. The counts
used for the next resampling, N(u?31), are affected
by this choice, because the new identity of u31 has
contributed to the posterior distribution. After un-
bounded iterations, Gibbs sampling is guaranteed to
converge and produce samples from the true poste-
rior (Geman and Geman, 1984).
3 Evaluation
This model provides a language agnostic solution to
a subset of phonological problems. We will first
examine performance on the sample Finnish data
(from Figure 1), and then look more closely at the is-
sue of convergence. Finally, we present results from
larger corpora 3.
3.1 Finnish
Output from a trial run on Finnish verbs (from Fig-
ure 1) follows, with hyperparameters ?ij{100 ??
i = j, 0.05 ?? i 6= j} and ?i = {0.1}.
In the paradigm (a sample after 1000 iterations),
each [sur+face] form is followed by its hypothesized
/under/ + /lying/ morphemes.
[tieda?+n] : /tieda?/ + /n/
[tieda?+t] : /tieda?/ + /t/
[tieta?+a?] : /tieda?/ + /a?/
[tieda?+mme] : /tieda?/ + /mme/
[tieda?+tte] : /tieda?/ + /tte/
[tieta?+va?t] : /tieda?/ + /va?t/
[ai?o+n] : /ai?o/ + /n/
...
[pelka?a?+va?t] : /pelka?a?/ + /vat/
With strong enough priors (faithfulness con-
straints), our sampler often selects the most com-
mon surface form aligned with an underlying seg-
ment. Although [vat] is more common than [va?t],
we choose the latter as the purer underlying form.
So /a/ is always [a], but /a?/ can be either [a?] or [a].
32.8 million word types from Morphochallenge2007 (Ku-
rimo et al, 2007)
3.2 Convergence
Testing convergence, we run again on the sample
data (Figure 1), using ?ij = 0.1 when i 6= j and
10 when i = j and ? = 0.1, starting from different
initializations, we get the same solution.
0 10 20 30 40 50 60 70 80 90 100
2.97
2.98
2.99
3
3.01
3.02
3.03
3.04
3.05 x 10
6
Iteration
?
lo
g 
Li
ke
lih
oo
d
Figure 4: Posterior likelihood at each of the first 100 iter-
ations, from 4 runs (with different random seeds) on 10%
of the Morphochallenge dataset (?i6=j = 0.001, ?i=j =
100, ? = 0.1), indicating convergence within the first 15
iterations.
To confirm that the sampler has converged, we
output and plot trace statistics at each iteration, in-
cluding marginal probability, log likelihood, and
changes in underlying forms (i.e., variables resam-
pled). If the sampler has converged, there should no
longer be a trend (consistent slope) in any of these
statistics (as in Figure 4).
Examining the posterior probability of each se-
lected underlying form reveals interesting patterns
that also help explain the variation. In the above run,
the ambiguous segments (with surface alternations)
were drawn from the distributions (with improbable
segments elided) in Figure 5.
We expect this model to maximize the probabil-
ity of either the ?majority? solution or a solution
demonstrating selection bias. We compare likeli-
hood of the posterior sample with that of a ?phono-
logically plausible? solution (in which underlying
forms are determined by referring to formal lin-
guistic accounts of phonological derivation) and a
?majority solution? (see Figure 6 for a log-log plot,
where lower is more likely).
The posterior sample has optimal likelihood with
each parameter setting, as expected. The majority
parse is selected with ?i6=j = 0.5 With lower val-
ues of ?i6=j , the ?phonologically plausible? parse is
16
u4=/d/ s4=[d,d,t,d,d,t]
P (ui = c) ?
d 0.99968
t 0.00014
u8=/k/ s8=[?,?,k,?,?,k]
(same behavior as u12)
P (ui = c) ?
? 0.642
k 0.124
u33=/e/ s33=[a?,o,e,u,?,e,?]
P (ui = c) ?
a?,o,u 0.0029
? 0.215
a 0.0003
e 0.297
Figure 5: Resampling probabilities for alternations, after
1000 iterations.
10?2 10?1 100
104.24
104.25
104.26
alpha
?
lo
g 
lik
el
ih
oo
d
 
 
posterior sample
majority solution
phonologically plausible
Figure 6: Parse likelihood
more likely than the majority. However, the sam-
pler does not converge to this solution, because in
this [t,d] alternation, the ?phonologically plausible?
solution identifies /t/, but neither selection bias nor
majority rules would lead to that with the given data.
3.3 Morphologically segmented corpora
In our search for appropriate data for additional,
larger-scale experiments, we found several vi-
able alternatives. The correct morphological seg-
mentations for Finnish data used in Morphochal-
lenge2007 (Kurimo et al, 2007) provide a rich and
varied set of words, and are readily analyzable by
our sampler. Rather than associating each surface
form with a position in the paradigm, we use the an-
Majority Bayesian
types 50.84 69.53
tokens 65.23 72.11
Figure 7: Accuracy of underlying segment hypotheses.
notated morphemes.
For example, the word ajavalle is listed in the cor-
pus as follows:
ajavalle aja:ajaa|V va:PCP1 lle:ALL The
word is segmented into a verb stem, ?aja? (drive),
a present participle marker ?va?, and the allative suf-
fix (?for?). Each surface realization of a given mor-
pheme is identified by the same tag (e.g., PCP1).
However, in this corpus, insertion and deletion are
not explicitly marked (as they were in the paradigm,
by ?). Rather than introduce another component
to determine which segments in the form were
dropped, we ignore these cases.
The sampling algorithm proceeds as described in
section 2. To run on tokens (as opposed to types), we
incorporate another input file that contains counts
from the original text (ajavalle appeared 8 times).
The counts of each morpheme?s surface forms then
reflect the number of times that form appeared in any
word in the corpus.
3.3.1 Type or Token
In Finnish verb conjugation, 3rd person (esp. sin-
gular) forms have high frequency and tend to be un-
marked (i.e., closer to underlying). In types, un-
marked is a minority (one third), but incorporat-
ing token frequency shifts that balance, benefiting
the ?majority learner.? Among noun inflections, un-
marked has higher frequency in speech, but marked
tokens may still dominate in text. We might expect
that it is easier to learn from tokens than types, in
part because more data is often helpful.
Testing on half of the Morphochallenge 2007
Finnish data (1M word types, 5M morph types,
17.5M word tokens, 48M morph tokens), we ran
both our Bayesian model and a majority solver on
the morphological analyses, and compared against
phonologically plausible (gold) underlying forms.
Results are reported in Figure 7.
The Bayesian estimate consistently outperformed
the majority solution, and cases where the two differ
could often be ascribed to the preference for ?pure?
17
analyses.
4 Conclusion
We have described a model where surface forms
are generated from underlying representations seg-
ment by segment. Taking this approach allowed us
to investigate the properties of a Bayesian statistical
learner, and how these can be useful in the context
of sound systems, a basic component of language.
Experiments with our implementation of a collapsed
sampler have produced results largely confirming
our hypotheses.
Without context, we can often learn about 60 to 80
percent of the mapping from underlying phonemes
to surface phones. Especially with lower values of
?i6=j , closer to 0, our model does prefer pure alter-
nations. Gibbs sampling tends to select the major-
ity underlying form, particularly with ?i6=j relatively
high, closer to 1. So, a sparser prior leads us further
from the baseline, and often closer to a phonologi-
cally plausible solution.
4.1 Directions
In future research, we hope to integrate morpholog-
ical analysis into this sort of a treatment of phonol-
ogy. This is a natural approach for children learn-
ing their first language. They intuitively discover
phonotactics, and how it affects the prosodic shape
of each word, as they learn meaningful units and
compose them together. It is clear that many lay-
ers of linguistic information interact in the early
stages of child language acquisition (Demuth and
Ellis, 2005 in press), so they should also interact
in our models. As discussed above, the present
model should be applicable to analysis of language-
learners? speech errors, and this connection should
be explored in greater depth.
It might be interesting to predispose the sampler
to select underlying forms from open syllables. That
is, set ? to increase the probability of matching
one of the surface segments if its context (feature
annotations) includes a vocalic segment or a word
boundary immediately following. The probability
of phonological processes like assimilation could be
similarly modeled, with the prior higher for choos-
ing a segment that appears on the surface in a con-
trastive context (where it shares few features with
neighboring segments).
If we define a MaxEnt distribution over Optimal-
ity Theoretic constraints, we might use that to in-
form our selection of underlying forms. In (Gold-
water and Johnson, 2003), the learning algorithm
was given a set of candidate surface forms asso-
ciated with an underlying form, and tried to opti-
mize the constraint weights. In addition to the con-
straint weights, we must also optimize the underly-
ing form, since our goal is to take as input only ob-
servable data. Sampling from this type of complex
distribution is quite difficult, but some approaches
(e.g., (Murray et al, 2006)) may help reduce the in-
tractability.
References
Kenneth R. Beesley and Lauri Karttunen. 2000. Finite-
state non-concatenative morphotactics. In Lauri Kart-
tunen, Jason Eisner, and Alain The?riault, editors, SIG-
PHON2000, August 6 2000. Proceedings of the Fifth
Workshop of the ACL Special Interest Group in Com-
putational Phonology., pages 1?12.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer, August.
Hal Daume III. 2007. Hbc: Hierarchical bayes compiler.
Katherine Demuth and David Ellis, 2005 (in press). Re-
visiting the acquisition of Sesotho noun class prefixes.
Lawrence Erlbaum.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, gibbs distributions, and the bayesian restora-
tion of images. IEEE Trans. Pattern Anal. Machine
Intell., 6(6):721?741, Nov.
Dale Gerdemann and Gertjan van Noord. 2000. Approx-
imation and exactness in finite state optimality theory.
Daniel Gildea and Daniel Jurafsky. 1996. Learning bias
and phonological-rule induction. Computational Lin-
guistics, 22(4):497?530.
Sharon Goldwater and Mark Johnson. 2003. Learning ot
constraint rankings using a maximum entropy model.
Sharon Goldwater and Mark Johnson. 2004. Priors in
Bayesian learning of phonological rules. In Proceed-
ings of the Seventh Meeting of the ACL Special Inter-
est Group in Computational Phonology, pages 35?42,
Barcelona, Spain, July. Association for Computational
Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641?648. MIT Press, Cambridge, MA.
18
Lauri Karttunen. 1998. The proper treatment of optimal-
ity in computational phonology. In Lauri Karttunen,
editor, FSMNLP?98: International Workshop on Fi-
nite State Methods in Natural Language Processing,
pages 1?12. Association for Computational Linguis-
tics, Somerset, New Jersey.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of morpho challenge in clef 2007.
In Working Notes for the CLEF 2007 Workshop, Bu-
dapest, Hungary.
Percy Liang and Dan Klein. 2007. Tutorial 1: Bayesian
nonparametric structured models, June.
Iain Murray, Zoubin Ghahramani, and David MacKay.
2006. MCMC for doubly-intractable distributions. In
UAI. AUAI Press.
19
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 1?4,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Social (distributed) language modeling, clustering and dialectometry
David Ellis
Facebook
Palo Alto, CA
dellis@facebook.com
Abstract
We present ongoing work in a scalable,
distributed implementation of over 200
million individual language models, each
capturing a single user?s dialect in a given
language (multilingual users have several
models). These have a variety of prac-
tical applications, ranging from spam de-
tection to speech recognition, and dialec-
tometrical methods on the social graph.
Users should be able to view any content
in their language (even if it is spoken by
a small population), and to browse our site
with appropriately translated interface (au-
tomatically generated, for locales with lit-
tle crowd-sourced community effort).
1 Introduction
We approach several key questions from a data-
driven (statistical) perspective, drawing on large,
dynamic annotated corpora:
1. What social factors affect language change
(and evolution)? How?
2. How do individuals adjust their speech or
writing depending on context and audience?
(e.g., register, formality, humor, reference)
3. What are the minimum requirements for a
language (or dialect)?
(e.g., number of speakers, corpus size)
4. Is a common language necessary for commu-
nication?
Can a pidgin be predicted from its speaker-population?
To this end, we describe a framework for lan-
guage modeling on the social graph, which incor-
porates similarity clustering and lays the ground-
work for personalized (and multimodal) machine
translation.
2 Related Work
Research on large scale language model-
ing (Brants et al, 2007) has addressed sharding,
smoothing and integration with a machine transla-
tion pipeline. Our work takes a similar approach,
using Hadoop (Borthakur, 2007) and Hive to
query and process distributed data. Social annota-
tions enhanced smoothing for language modeling
in the context of information retrieval (Xu et
al., 2007), and hierarchical Bayesian networks
were used (Zhou et al, 2008) to incorporate user
domain interest in such models. Language models
are often used to detect spam, including in social
bookmarking (Bogers and van den Bosch, 2008).
Proposed scoring models for social
search (Schenkel et al, 2008) use friendship
strengths and an extension of term frequency
1
.
These could benefit from a deeper integration with
friends? language models, perhaps to approximate
a user-specific inverse document frequency, rather
than treat each tag by a user as equally relevant to
all his friends of a given (relationship) strength.
Strehl et al (2000) found that similarity clustering
perform best using weighted graph partitioning.
3 Language Model
An individual?s language model is a mixture of
their locale (or another language they speak) and
token frequencies from the content they produce
(write) and consume (read). Since we have hun-
dreds of milliions of users, each of whose lan-
guage model can depend on a variety of data
sources, it is essential to distribute these counts
(and other figures derived from them) in a way that
optimizes the efficiency of our access patterns
2
.
We also tried clustering users, and represent-
ing the language of each as deviations from its
neighbors (or the norm of the cluster). However,
1
Called ?socially-enhanced tag frequency?.
2
See Section 5 for discussion of a variety of use cases.
1
there are significantly more edges than nodes in
our graph (more friendships than people), so this
alternative is less efficient.
An individual?s language use varies greatly de-
pending on his interlocutor or audience
3
. Mes-
sages I send (privately) to a friend differ in style
from comments I make on a public photo of my
nephew, which in turn differ from my writing style
as realized in an academic or industry paper or ar-
ticle.
An obvious optimization is to describe a min-
imum spanning tree (MST) on the graph, where
each edge is weighted according to the similarity
of dialects associated with the nodes (individuals,
groups or other entities) it connects. Then, lan-
guage models of nodes connected by the MST can
depend on each other?s counts. Singletons default
to the general language model from their locale.
3.1 Detecting Deviations
People who aren?t friends (and have no mutual
friends or other evident connection) may yet use
more similar language than siblings. This exam-
ple seems highly improbable or unnatural, and in
fact serves as a good heuristic for detecting com-
promised, spam-sending accounts (even if not or-
ganized in a botnet).
If a user sends a message with high perplexity:
1. Their account is compromised, and being
used to spam (or phish) their friends.
2. They are using a different language than
usual. Users are often bilingual (sometimes
multi)-, so we may not yet have realized they
are proficient in a given language.
3. There may be a problem with the language
model:
(a) large vocabulary (tends to inflate per-
plexity)
(b) genre mix (user interface v. user com-
munication)
3.2 Locale Induction
A regional cluster of personal language models
can be combined to create a new locale. A crowd-
sourced translation process (Ellis, 2009) can thus
3
This is not novel in or of itself, but the scale of our data
and experiments should lead to finer-grained understanding,
both of issues peculiar to a single language or its family, and
of language universals (or.patterns; priors likely intuitively
encoded).
be bootstrapped by indirect community contribu-
tions.
4 Machine Translation
For an English-sepaking user, in order to opti-
mize the probability of the target (translated) sen-
tence given its source (Foreign), we follow Och
and Ney?s (2004) optimization of a set of feature
functions:
?
e = argmax
e
M
?
m=1
?
m
h
m
(e,f)
It is thus easy for us to aggregate scores from
multiple language models (e.g., from individuals
comprising your network of friends or others you
interact with).
Our distributed, individual language models can
be a component of personalized machine transla-
tion, where the target language may be a penpal?s.
Either the decoder incorporates the counts from
user communications by supplementing the lan-
guage model used in its n-best candidate search,
or it uses the locale?s general language model and
factors in individual variance in a rescoring step.
We plan to offer inline statistical machine trans-
lation (SMT) of user-generated content, where the
translation model combines features from:
1. Our (interface) translations corpus for the
language pair
2. Related langauges or dialects
4
3. Linguistic rules (Ellis, 2009), in some com-
bination of:
(a) Explicitly encoded
(b) Induced from training corpora
(c) Borrowed from related languages (esp.
for relatively minor or resource-poor)
4.1 Sparse Data
Data sparseness is clearly an issue for modeling
with this degree of specificity, so we explore a
range of possible smoothing techniques, as well
as methods for leveraging resources from related
languages (Genzel, 2005). If a user signed up for
Facebook last week, (s)he may not yet have con-
nected with many friends or shared much content
(which exacerbates the problem).
4
e.g. Spanish (Argentina, Spain), Chinese (Mandarin,
Cantonese (Hong Kong, Taiwan)), or Finnish and its neigh-
bors: inc. Estonian, S?ami, Komi
2
Domain adaptation is also important, since the
base corpus is for a user interface: usually more
formal, less varied than conversation. Ideally, we
would like to capture not only language change
(diversion, creolization) but an individual?s lin-
guistic evolution in a variety of contexts:
? She learns a language, practices its use, be-
comes increasingly accustomed to its twists
and turns (syntactic, lexical, morphological,
etc.)
? His mood shifts, he moves into a new apart-
ment or city, let alne grander (potentially
dynamic) features of context
? A startup company is suddently more visible
(e.g., resulting from press coverage, or a tech
blogger?s reference), and so an image (and
website design, copy) revamp is in order.
? Afflicted with post-traumatic stress, after
sensory deprivation, or in cases of neurologi-
cal disorders or brain damage.
5 Similarity
We use a pipeline to cluster strings (to suggest
translations) and users (based on language use):
1. Preprocessing
? normalization (lowercasing)
? {segment,{lemmat,token}iz}ation
2. Similarity (pick one)
? fuzzy (hash) similarity
5
? string edit distance
? phonetic (or phonological) edit distance
? language model perplexity
? KL-divergence (btn. language models)
3. Clustering (modular: select-an-algo)
? hierarchical (agglomerative or divisive)
? K-means (partitioning)
? graph-theoretic methods (cover as op-
posed to cluster)
This is architected for ease of experimentation
and modification, testing and tuning, so any com-
bination of the above should be functional. Some
applications of similarity require high accuracy
but can be processed offline, whereas others need
to be computed in less than ten milliseconds in re-
sponse to a live query.
5
i.e., Jaccard coefficient (Wikipedia, 2008)
Figure 1: Visualization of a user?s friends, where
the extent of each type of relationship or commu-
nication is indicated by saturation (shade of blue)
of the connection.
6 Evaluation
Although the components we use can be (and in
most cases, have been) thoroughly evaluated in
relative isolation, it is important to understand the
consequences of their use in concert. Improve-
ments to spam detection should be evident both in
tests on annotated
6
data and in decreased reports
or complaints from users.
User-generated metadata, in some cases a sim-
ple report of offensive content or a friend?s com-
promised account, is a natural source of both la-
beled test data and training data. Our customer
service processes are thus tightly integrated with
machine learning efforts. See Figure 1 for commu-
nications in a small segment of the social graph.
7 Conclusion
Preliminary experiments with user-initiated ma-
chine translation of friend-generated content sug-
gest it will soon be valuable. It is crucial to design
this in a scalable way, such that it extends to arbi-
trarily many languages
7
, both draws on and sup-
6
Either a binary classification (spam or non-spam) or a
gradient scale, possibly incorporating dimensions of phishi-
ness, spamminess, or other types of solicitousness.
7
Including underrepresented ones like Oshindonga.
3
ports our internationalization efforts, and should
be useful on mobile devices (including in the spo-
ken modality).
Our introductory questions (from Section 1) are
far from fully answered, but we hope this work
might help to address them.
1. The number and strength of connections,
speed and frequency of communication, and
diversity of languages individuals are ex-
posed to all have strong influences on lan-
guage change.
2. Stylistic variations in an individual?s lan-
guage are evident in that it can be more accu-
rately captured as a mixture of models, each
of which is suited to a specific situation, style,
or set of interlocutors.
3. Two speakers is sufficient for a language. A
small model can adequately describe a lan-
guage, if each data point is a deviation from
another language.
4. A common language is far from necessary for
communication
8
. A set of arbitrary individu-
als? language models can be combined (and
pruned, evolved) to derive the pidgin they
might speak.
7.1 Future Work
Social natural language processing is (in a sense)
in its infancy. We hope to capture aspects of its
evolution, just as the field comes to better describe
and understand ongoing changes in human lan-
guages. We have not yet satisfactorily answered
our second question, but expect more fine-grained
analyses to follow, using our framework to com-
pare and contrast a variety of languages (from
Bantu to Balinese) and phenomena (inside jokes,
cross-linguistic usage of l33t and txt msg terms).
We hope to facilitate this by providing an API
to allow researchers access to anonymized
9
, ag-
gregated data.
Acknowledgments
This technology is developed with support from
i18n team (engineers, language managers and oth-
ers) at Facebook, and all our international users.
8
Photos, emoticons and tone of voice (for example) go
a long way. We hope personalized (including speech-to-
speech) translation will continue to bridge the language di-
vide.
9
Also honoring users? privacy settings.
Thanks to our data scientists for the visualization
of a user?s friends, and the extent of communica-
tion connecting them.
References
Toine Bogers and Antal van den Bosch. 2008. Using
language models for spam detection in social book-
marking. In Proceedings of the ECML/PKDD Dis-
covery Challenge.
Dhruba Borthakur, 2007. The Hadoop Distributed File
System: Architecture and Design. The Apache Soft-
ware Foundation.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858?867.
David Ellis. 2009. A case study in community-
driven translation of a fast-changing website. In
Proceedings of the 13th International Conference on
Human-Computer Interaction HCII (to appear), San
Diego, California, USA.
Dmitriy Genzel. 2005. Creating Algorithms for
Parsers and Taggers for Resource-Poor Languages
Using a Related Resource-Rich Language. Ph.D.
thesis, Brown University.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449.
Ralf Schenkel, Tom Crecelius, Mouna Kacimi, Thomas
Neumann, Josiane Parreira, Marc Spaniol, and Ger-
hard Weikum. 2008. Social wisdom for search and
recommendation, June.
Er Strehl, Joydeep Ghosh, and Raymond Mooney.
2000. Impact of similarity measures on web-page
clustering. In In Workshop on Artificial Intelligence
for Web Search (AAAI 2000, pages 58?64. AAAI.
Wikipedia. 2008. Jaccard?s similarity coefficient.
Shengliang Xu, Shenghua Bao, Yunbo Cao, and Yong
Yu. 2007. Using social annotations to improve lan-
guage model for information retrieval. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, pages
1003?1006. CIKM.
Ding Zhou, Jiang Bian, Shuyi Zheng, Hongyuan Zha,
and Lee C. Giles. 2008. Exploring social an-
notations for information retrieval. In WWW ?08:
Proceeding of the 17th international conference on
World Wide Web, pages 715?724, New York, NY,
USA. ACM.
4
