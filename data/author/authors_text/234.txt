Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 117?121,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A Machine Learning-Based Coreference Detection System For OntoNotes
Yaqin Yang
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
yaqin@brandeis.edu
Nianwen Xue
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
xuen@brandeis.edu
Peter Anick
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
peter anick@yahoo.com
Abstract
In this paper, we describe the algorithms and
experimental results of Brandeis University
in the participation of the CoNLL Task 2011
closed track. We report the features used in
our system, and describe a novel cluster-based
chaining algorithm to improve performance of
coreference identification. We evaluate the
system using the OntoNotes data set and de-
scribe our results.
1 Introduction
This paper describes the algorithms designed and
experiments finished in the participation of the
CoNLL Task 2011. The goal of the Task is to design
efficient algorithms for detecting entity candidates
and identifying coreferences. Coreference identifi-
cation is an important technical problem. Its impor-
tance in NLP applications has been observed in pre-
vious work, such as that of Raghunathan et al, Prad-
han et al, Bergsma et al, Haghighi et al, and Ng et
al.. While most of the existing work has evaluated
their systems using the ACE data set, in this work
we present our experimental results based on the
OntoNotes data set used in the CoNLL 2011 Shared
Task. We detail a number of linguistic features
that are used during the experiments, and highlight
their contribution in improving coreference identi-
fication performance over the OntoNotes data set.
We also describe a cluster-based approach to multi-
entity chaining. Finally, we report experimental re-
sults and summarize our work.
2 Data Preparation
We divide the CoNLL Task into three steps. First,
we detect entities from both the training data and
the development data. Second, we group related en-
tities into entity-pairs. Finally, we use the gener-
ated entity-pairs in the machine learning-based clas-
sifier to identify coreferences. In this section, we
describe how we extract the entities and group them
into pairs.
2.1 Generating Entity Candidates
We use the syntactic parse tree to extract four types
of entities, including noun phrase, pronoun, pre-
modifier and verb (Pradhan et al, 2007). This
method achieves 94.0% (Recall) of detection accu-
racy for gold standard trees in the development data.
When using the automatic parses, not surprisingly,
the detection accuracy becomes lower, with a per-
formance drop of 5.3% (Recall) compared with that
of using the gold standard trees. Nevertheless, this
method can still cover 88.7% of all entities existing
in the development data, thus we used it in our algo-
rithm.
2.2 Generating Entity-Pairs From Individual
Entities
In the annotated training documents, an entity has
been marked in a coreference chain that includes all
coreferential entities. In our algorithm, we only de-
tect the closest antecedent for each entity, instead
of all coreferences, of each entity. Specifically, we
define each training and testing instance as a pair
of entities. During the training process, for each
entity encountered by the system, we create a pos-
itive instance by pairing an entity with its closest
antecedent (Soon et al, 2001). In addition, a set
of negative instances are also created by pairing the
entity with any preceding entities that exist between
its closest antecedent and the entity itself (note that
the antecedent must be a coreference of the current
entity, whereas preceding entities may not be coref-
erential). For example, in the entity sequence ?A,
B, C, D, E?, let us assume that ?A? is the closest
antecedent of ?D?. Then, for entity ?D?, ?A-D? is
considered a positive instance, whereas ?B-D? and
?C-D? are two negative instances.
To generate testing data, every entity-pair within
the same sentence is considered to form positive or
negative instances, which are then used to form test-
ing data. Since occasionally the distance between an
entity and its closest antecedent can be far apart, we
handle considerably distant coreferences by consid-
117
ering each entity-pair that exists within the adjacent
N sentences. During our experiments, we observed
that the distance between an entity and its closest an-
tecedent could be as far as 23 sentences. Therefore,
in the classification process, we empirically set N as
23.
3 Machine Learning-Based Classification
After labeling entity pairs, we formalize the corefer-
ence identification problem as a binary classification
problem. We derive a number of linguistic features
based on each entity-pair, i and j, where i is the po-
tential antecedent and j the anaphor in the pair (Soon
et al, 2001). Generally, we select a set of features
that have been proved to be useful for the corefer-
ence classification tasks in previous work, includ-
ing gender, number, distance between the antecedent
and the anaphor, and WordNet (WordNet, 2010). In
addition, we design additional features that could
be obtained from the OntoNotes data, such as the
speaker or author information that is mainly avail-
able in Broadcast Conversation and Web Log data
(Pradhan et al, 2007). Moreover, we extract appo-
sition and copular structures and used them as fea-
tures. The features we used in the system are de-
tailed below.
? Independent feature: 1) if a noun phrase is defi-
nite; 2) if a noun phrase is demonstrative; 3) gender
information of each entity; 4) number information
of each entity; 5) the entity type of a noun phrase;
6) if an entity is a subject; 7) if an entity is an object;
8) if an noun phrase is a coordination, the number
of entities it has; 9) if a pronoun is preceded by a
preposition; 10) if a pronoun is ?you? or ?me?; 11)
if a pronoun is ?you? and it is followed by the word
?know?.
? Name entity feature: 1) i-j-same-entity-type-
etype=True, if i and j have the same entity type;
2) i-j-same-etype-subphrase=True, if i and j have
the same entity type and one is the subphrase of the
other.
? Syntactic feature: 1) i-j-both-subject=True, if i
and j are both subjects; 2) if i and j are in the same
sentence, record the syntactic path between i and j,
e.g. i-j-syn-path=PRP?NP!PRP; 3) i-j-same-sent-
diff-clause=True, if i and j are in the same sentence
but in different clauses.
? Gender and number feature: 1) i-j-same-
gender=True/False, by comparing if i and j have the
same gender; 2) i-j-same-num=True/False, by com-
paring if i and j have the same number; 3) i-j-same-
num-modifier=True/False, by comparing if i and j
have the same number modifier, e.g. ?two coun-
tries? and ?they both? have the same number mod-
ifier; 4) i-j-same-family=True/False, we designed
seven different families for pronouns, e.g. ?it?, ?its?
and ?itself? are in one family while ?he?, ?him?,
?his? and ?himself? are in another one.
? Distance feature: 1) i-j-sent-dist, if the sentence
distance between i and j is smaller than three, use
their sentence distance as a feature; 2) i-j-sent-
dist=medium/far: if the sentence distance is larger
than or equal to three, set the value of i-j-sent-dist
to ?medium?, otherwise set it to ?far? combined
with the part-of-speech of the head word in j.
? String and head word match feature: 1) i-j-
same-string=True, if i and j have the same string;
2) i-j-same-string-prp=True, if i and j are the
same string and they are both pronouns; 3) i-j-sub-
string=True, if one is the sub string of the other,
and neither is a pronoun; 4) i-j-same-head=True,
if i and j have the same head word; 5) i-j-prefix-
head=True, if the head word of i or j is the pre-
fix of the head word of the other; 6) i-j-loose-head,
the same as i-j-prefix-head, but comparing only the
first four letters of the head word.
? Apposition and copular feature: for each noun
phrase, if it has an apposition or is followed by
a copular verb, then the apposition or the subject
complement is used as an attribute of that noun
phrase. We also built up a dictionary where the
key is the noun phrase and the value is its apposi-
tion or the subject?s complement to define features.
1) i-appo-j-same-head=True, if i?s apposition and
j have the same head word; 2) i-j-appo-same-
head=True, if j?s apposition has the same head word
as i; we define the similar head match features for
the noun phrase and its complement; Also, if an i
or j is a key in the defined dictionary, we get the
head word of the corresponding value for that key
and compare it to the head word of the other entity.
? Alias feature: i-j-alias=True, if one entity is a
proper noun, then we extract the first letter of each
word in the other entity. ( The extraction process
skips the first word if it?s a determiner and also skips
the last one if it is a possessive case). If the proper
noun is the same as the first-letter string, it is the
alias of the other entity.
? Wordnet feature: for each entity, we used Wordnet
to generate all synsets for its head word, and for
each synset, we get al hypernyms and hyponyms.
1) if i is a hypernym of j, then i-hyper-j=True; 2)
if i is a hyponym of j, then i-hypo-j=True.
? Speaker information features: In a conversation,
a speaker usually uses ?I? to refer to himself/herself,
and most likely uses ?you? to refer to the next
speaker. Since speaker or author name informa-
tion is given in Broadcast Conversation and Web
Log data, we use such information to design fea-
tures that represent relations between pronouns and
118
speakers. 1) i-PRP1-j-PRP2-same-speaker=True,
if both i and j are pronouns, and they have the same
speaker; 2) i-I-j-I-same-speaker=True, if both i and
j are ?I?, and they have the same speaker; 3) i-I-j-
you-same-speaker=True, if i is ?I? and j is ?you?,
and they have the same speaker; 4) if i is ?I?, j
is ?you? and the speaker of j is right after that of
i, then we have feature i-I-j-you&itarget=jspeaker;
5) if i is ?you?, j is ?I? and the speaker of j is
right after that of i, then we have feature i-you-
j-I-itarget=jspeaker; 6) if both i and j are ?you?,
and they followed by the same speaker, we consider
?you? as a general term, and this information is used
as a negative feature.
? Other feature: i-j-both-prp=True, if both i and j
are pronouns.
4 Chaining by Using Clusters
After the classifier detects coreferential entities,
coreference detection systems usually need to chain
multiple coreferential entity-pairs together, forming
a coreference chain. A conventional approach is
to chain all entities in multiple coreferential entity-
pairs if they share the same entities. For example, if
?A-B?, ?B-C?, and ?C-D? are coreferential entity-
pairs, then A, B, C, and D would be chained to-
gether, forming a coreference chain ?A-B-C-D?.
One significant disadvantage of this approach is
that it is likely to put different coreference chains to-
gether in the case of erroneous classifications. For
example, suppose in the previous case, ?B-C? is ac-
tually a wrong coreference detection, then the coref-
erence chain created above will cause A and D to be
mistakenly linked together. This error can propagate
as coreference chains become larger.
To mitigate this issue, we design a cluster-based
chaining approach. This approach is based on the
observation that some linguistic rules are capable of
detecting coreferential entities with high detection
precision. This allows us to leverage these rules to
double-check the coreference identifications, and re-
ject chaining entities that are incompatible with rule-
based results.
To be specific, we design two lightweight yet ef-
ficient rules to cluster entities.
? Rule One. For the first noun phrase (NP) encoun-
tered by the system, if 1) this NP has a name entity
on its head word position or 2) it has a name en-
tity inside and the span of this entity includes the
head word position, a cluster is created for this NP.
The name entity of this NP is also recorded. For
each following NP with a name entity on its head
word position, if there is a cluster that has the same
name entity, this NP is considered as a coreference
to other NPs in that cluster, and is put into that clus-
ter. If the system cannot find such a cluster, a new
cluster is created for the current NP.
? Rule Two. In Broadcast Conversation or Web Log
data, a speaker or author would most likely use ?I?
to refer to himself/herself. Therefore, we used it
as the other rule to cluster all ?I? pronouns and the
same speaker information together.
Given the labeled entity pairs, we then link them in
different coreference chains by using the cluster in-
formation. As the Maximum Entropy classifier not
only labels each entity-pair but also returns a con-
fidence score of that label, we sort all positive pairs
using their possibilities. For each positive entity-pair
in the sorted list, if the two entities are in different
clusters, we consider this to be a conflict, and with-
draw this positive entity-pair; if one entity belongs to
one cluster whereas the other does not belong to any
cluster, the two entities will be both included in that
cluster. This process is repeated until no more enti-
ties can be included in a cluster. Finally, we chain
the rest of entity pairs together.
5 Results and Discussion
To evaluate the features and the chaining approach
described in this paper, we design experiments de-
scribed as follows. Since there are five different
data types in the provided OntoNotes coreference
data set, we create five different classifiers to pro-
cess each of the data types. We used the features
described in Section 3 to train the classifiers, and
did the experiments using a Maximum Entropy clas-
sifier trained with the Mallet package (McCallum,
2002). We use the gold-standard data in the training
set to train the five classifiers and test the classifiers
on both gold and automatically-parsed data in the
development data set. The MUC metric provided by
the Task is used to evaluate the results.
5.1 Performance without Clustering
First, we evaluate the system by turning the clus-
tering technique off during the process of creating
coreference chains. For entity detection, we ob-
serve that for all five data types, i.e. Broadcast
(BC), Broad news (BN), Newswire (NW), Magazine
(MZ), and Web blog (WB), the NW and WB data
types achieve relatively lower F1-scores, whereas
the BC, BN, and MZ data types achieve higher per-
119
BC BN NW MZ WB
Without Clustering
Gold 57.40 (64.92/51.44) 59.45 (63.53/55.86) 52.01 (59.71/46.07) 55.59 (62.90/49.80) 49.53 (61.16/41.62)
Auto 54.00 (61.28/48.26) 55.40 (59.05/52.17) 48.44 (55.32/43.09) 52.21 (59.78/46.33) 47.02 (58.33/39.39)
With Clustering
Gold 57.44 (64.12/52.03) 56.56 (58.10/55.09) 51.37 (56.64/46.99) 54.26 (60.07/49.47) 49.00 (60.09/41.36)
Auto 54.19 (60.82/48.87) 52.69 (54.07/51.37) 48.01 (52.74/44.05) 50.82 (56.76/46.01) 46.86 (57.49/39.55)
Table 1: Performance comparison of coreference identification between using and without using the clustering tech-
nique in chaining. Note that the results are listed in sequence of F1-scores (Recalls/Precisions). The results shown are
based on MUC.
formance. Due to limited space, the performance
table of entity detection is not included in this paper.
For coreference identification, as shown in Ta-
ble 1, we observe pretty similar performance gaps
among different data types. The NW and WB data
types achieve the lowest F1-scores (i.e. 52.01%
and 49.53% for gold standard data, and 48.44% and
47.02% for automatically-parsed data) among all the
five data types. This can be explained by seeing that
the entity detection performance of these two data
types are also relatively low. The other three types
achieves more than 55% and 52% F1-scores for gold
and auto data, respectively.
These experiments that are done without using
clustering techniques tend to indicate that the perfor-
mance of entity detection has a positive correlation
with that of coreference identification. Therefore, in
the other set of experiments, we enable the cluster-
ing technique to improve coreference identification
performance by increasing entity detection accuracy.
Metric Recall Precision F1
MUC 59.94 45.38 51.65
BCUBED 72.07 53.65 61.51
CEAF (M) 45.67 45.67 45.67
CEAF (E) 29.43 42.54 34.79
BLANC 70.86 60.55 63.37
Table 2: Official results of our system in the CoNLL Task
2011. Official score is 49.32. ((MUC + BCUBED +
CEAF (E))/3)
5.2 Performance with Clustering
After enabling the clustering technique, we observe
an improvement in entity detection performance.
This improvement occurs mainly in the cases of the
NW and WB data types, which show low entity
detection performance when not using the cluster-
ing technique. To be specific, the performance of
the NW type on both the gold standard and auto-
matic data improves by about 0.5%, and the perfor-
mance of the WB type on the automatic data im-
proves about 0.1%. In addition, the performance of
the BC type on both the gold standard and automatic
data also increases about 0.2% to 0.6%.
Although the clustering technique succeeds in im-
proving entity detection performance for multiple
data types, there is no obvious improvement gained
with respect to coreference identification. This is
quite incompatible with our observation in the ex-
periments that do not utilize the clustering tech-
nique. Currently, we attribute this issue to the low
accuracy rates of the clustering operation. For ex-
ample, ?H. D. Ye.? and ?Ye? can be estimated cor-
rectly to be coreferential by the Maxtent classifier,
but the clustering algorithm puts them into different
clusters since ?H. D. Ye.? is a PERSON type name
entity while ?Ye? is a ORG type name entity. There-
fore, the system erroneously considers them to be a
conflict and rejects them. We plan to investigate this
issue further in our future work.
The official results of our system in the CoNLL
Task 2011 are summarized in Table 2.
6 Conclusion
In this paper, we described the algorithm design and
experimental results of Brandeis University in the
CoNLL Task 2011. We show that several linguistic
features perform well in the OntoNotes data set.
References
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
120
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. International Conference on Semantic
Computing (ICSC 2007), pages 446?453, September.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
WordNet. 2010. Princeton University ?About
WordNet.? WordNet. Princeton University. 2010.
http://wordnet.princeton.edu.
121
Proceedings of SADAATL 2014, pages 31?39,
Dublin, Ireland, August 24, 2014.
Extracting Aspects and Polarity from Patents 
 Peter Anick, Marc Verhagen and James Pustejovsky 
Computer Science Department 
Brandeis University 
Waltham, MA, United States 
Peter_anick@yahoo.com, marc@cs.brandeis.edu, 
jamesp@cs.brandeis.edu 
 
Abstract 
We describe an approach to terminology extraction from patent corpora that follows from a view of pa-
tents as ?positive reviews? of inventions.  As in aspect-based sentiment analysis, we focus on identify-
ing not only the components of products but also the attributes and tasks which, in the case of patents, 
serve to justify an invention?s utility.  These semantic roles (component, task, attribute) can serve as a 
high level ontology for categorizing domain terminology, within which the positive/negative polarity of 
attributes serves to identify technical goals and obstacles.  We show that bootstrapping using a very 
small set of domain-independent lexico-syntactic features may be sufficient for constructing domain-
specific classifiers capable of assigning semantic roles and polarity to terms in domains as diverse as 
computer science and health. 
1 Introduction 
Automated data mining of patents has had a long history of research, driven by the large volume of 
patents produced each year and the many tasks to which they are put to use, including prior art inves-
tigation, competitive analysis, and trend detection and forecasting (Tseng, 2007).  Much of this work 
has concentrated on bibliographic methods such as citation analysis, but text mining has also been 
widely explored as a way to assist analysts to characterize patents, discover relationships, and facilitate 
patent searches.  One of the indicators of new technology emergence is the coinage, adoption and 
spread of new terms; hence the identification and tracking of technical terminology over time is of par-
ticular interest to researchers designing tools to support analysts engaged in technology forecasting 
(e.g., Woon, 2009; deMiranda, 2006) 
For the most part, research into terminology extraction has either (1) focused on the identification of 
keywords within individual patents or corpora without regard to the roles played by the keywords 
within the text (e.g., Sheremetyeva, 2009) or, (2) engaged in fine-grained analysis of the semantics of 
narrow domains (e.g., Yang, 2008).  In this paper we strive towards a middle ground, using a high-
level classification suitable for all domains, inspired in part by recent work on sentiment analysis (Liu, 
2012). In aspect-based sentiment analysis, natural language reviews of specific target entities, such as 
restaurants or cameras, are analyzed to extract aspects, i.e., features of the target entities, along with 
the sentiment expressed toward those features.  In the restaurant domain, for example, aspects might 
include the breadth of the menu, quality of the service, preparation of the food, and cost. Aspects thus 
tend to capture the tasks that the entity is expected to perform and various dimensions and components 
related to those tasks.  Sentiment reflects the reviewer?s assessment of these aspects on a scale from 
negative to positive.   
A patent application is required by definition to do three things: describe an invention, argue for its 
novelty, and justify its utility.  The utility of a patent is typically defined by the accomplishment of a 
new task or an improvement to some existing task along one or more dimensions.  Thus, a patent can 
be thought of as a positive review of a product with respect to specific aspects of its task(s).  Indeed, 
the most commonly occurring verbs in patents include those indicative of components (?comprise?, 
?include?), attributes (?increase?, ?reduce?), and tasks (?achieve?, ?perform?).   Organizing keywords 
along these high-level distinctions, then, would allow patent analysts to explore terminological infor-
This work is licensed under a Creative Commons Attribution 4.0 International License.  Page numbers and proceedings foot-
er are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
31
mation from several different relevant perspectives.  Furthermore, given the interpretation of a patent 
as a positive review, it should be possible to identify the default polarity of measurable aspects in the 
context of a domain.  For example, if a patent makes a reference to increasing network bandwidth, 
then this should lend support to the notion that network bandwidth is not only a relevant attribute with-
in the patent?s domain but also a positive one.  Likewise, if a patent refers to reducing power con-
sumption, then we might interpret power consumption as an aspect with negative polarity.  For ana-
lysts trying to assess trends within a technology domain, tracking the occurrences of terms signifying 
tasks and attributes, along with their polarity, could help them characterize the changing goals and ob-
stacles for inventors over time. 
The US patent office receives over half a million patent applications a year.1  These are classified by 
subject matter within several standardized hierarchical schemes, which permits dividing up the corpus 
of patents both by application date and subfield (e.g., computer science, health, chemistry).  Since our 
goal is to support analysts across all domains, it is highly desirable to extract domain-specific aspects 
through semi-supervised machine learning rather than incur the cost of domain-specific knowledge 
engineering.  To this end, we employed a bootstrapping approach in which a small number of domain 
independent features was used to generate a much larger number of domain dependent features for 
classification.  We then applied na?ve Bayes classification in a two-step classification process: first 
distinguishing attributes, components and tasks; and then classifying the extracted attribute terms by 
their polarity. 
The paper is structured as follows.  In section 2, we describe the system architecture.  Section 3 
shows results for two domains (computer science and health).  In section 4, we present an evaluation 
of results and discuss issues and shortcomings of the current implementation.  In section 5, we present 
related research and in section 6, our conclusions and directions for future work. 
2 System architecture 
2.1 Corpus processing 
Our patent collection is a set of 7,101,711 US patents in XML-markup form from Lexis-Nexis.  We 
divided the collection into subcorpora by application year and high-level domain using the patents? 
classification within the USPTO hierarchy.  The XML markup was then used to extract the relevant 
portions of patents for further analysis.  These sections included title, abstract, background, summary, 
description and claims. References, other than those embedded in the sections above, were omitted, as 
they contain many entity types (people, publications, and organizations) that are not particularly useful 
for our current task.  The text of each section was extracted and broken into sentences by the Stanford 
tagger (Toutanova, 2003) which also tokenized and tagged each token with a part of speech tag.   
We then chunked adjacent tokens into simple noun phrase chunks of the form (ADJECTIVE)? 
(NOUN)* NOUN.2  We will hereafter refer to these chunks as terms.  The majority of these patent 
terms fall into one of three major categories:  
Components: the physical constituents or processes that make up an invention, as well as the ob-
jects impacted, produced by or used in the invention.   
Tasks: the activities which inventions, their components or beneficiaries perform or undergo.   
Attributes: the measureable dimensions of tasks and components mentioned in the patent. 
 
To generate features suitable for machine learning of these semantic categories, we used a small set 
of lexico-syntactic relationships, each defined with respect to the location of the term in a sentence: 
prev_V: the closest token tagged as a verb appearing to the left of the term, along with any preposi-
tions or particles in between.  (cached_in, prioritizing, deal_with) 
prev_VNpr: a construction of the form <verb><NP><prep> appearing to the left of the term.  Only 
the head noun in the NP is retained (inform|user|of, provides|list|of, causes|increase|in) 
prev_Npr: a construction of the form <noun><prep> appearing to the left of the term. (re-
striction_on, applicability_of, time_with) 
                                                 
1 http://www.uspto.gov/web/offices/ac/ido/oeip/taf/us_stat.htm 
2 We blocked a set of 246 general adjectival modifiers (e.g., other, suitable, preferred, entire, initial,?) from participating in 
terms. 
32
prev_Jpr: a construction of form <adjective> <prep> appearing to the left of the term. (free_from, 
desirable_in, unfamiliar_with) 
prev_J: a construction of form <adjective> <prep> appearing to the left of the term. (excessive, con-
siderable, easy) 
 
These features were designed to capture specific dependency relations between the term and its pre-
modifiers and dominant verbs, nouns, and adjective phrases.  We extracted the features using localized 
rules rather than create a full dependency parse.3  One additional feature internal to the term itself was 
also included: last_word.  This simply captured the head term of the noun phrase, which often carries 
generalizable semantic information about the phrase.  Each feature instance was represented as a string 
comprising a prefix (the feature type) and its value (a token or concatenation of tokens).   
 
2.2 Classification 
 
For each term appearing in a subcorpus, the collection of co-occurring features across all documents 
was assembled into a single weighted feature vector in which the weight captured the number of doc-
uments for which the feature occurred in conjunction with the given term.  We also calculated the 
document frequency for each term, as well as its ?domain specificity score?, a metric reflecting the 
relative frequency of the term in specialized vs. randomized corpora (see section 3).   
In order to avoid the need to create manually labeled training data for each patent domain, we em-
ployed bootstrapping, a form of semi-supervised learning in which a small number of labeled features 
or seed terms are used in an iterative fashion to automaticaly identify other likely diagnostic features 
or category exemplars.  Bootstrapping approaches have previously shown considerable promise in the 
construction of semantic lexicons (Riloff, 1999; Thelen, 2002, Ziering, 2013).  By surveying common 
prev_V features in a domain-independent patent subcorpus, we selected a small set of domain-
independent diagnostic lexico-syntactic features (?seed features?) that we felt were strong indicators 
for each of the three semantic categories.  The set of seed features for each category is shown below.  
Semantically equivalent inflectional variants were also included as features.   
 
Attribute: improve, optimize, increase, decrease, reduce 
Component: comprise, contain, encompass, incorporate, use, utilize, consist_of, assembled_of, com-
posed_of 
Task: accomplish, achieve, enhance, facilitate, assisting_in, employed_in, encounter_in, perform, 
used_for, utilized_for 
 
We then utilized these manually labeled generic features to bootstrap larger feature sets F for do-
main-specific subcorpora.  For each term t in a domain-specific subcorpus, we extracted all the manu-
ally labeled features that the term co-ocurred with. Any term which co-occurred with at least two la-
beled feature instances and for which all of its labeled features were of the same class was itself la-
beled with that class for subsequent use as a seed term s for estimating the parameters of a multinomial 
na?ve Bayes classifier (Manning et al, 2008).  Each seed term so selected was represented as a bag of 
its co-occurring features.   
 
The prior probability of each class and conditional probabilities of each feature given the class were 
estimated as follows, using Laplace ?add one? smoothing to eliminate 0 probabilities: 
 
 ?(  )   
      
     
 
 
 ?(   )   
     (   )   
     ( )       
 
                                                 
3 The compute time required to produce dependency parses for the quantity of data to be analyzed led to the choice of a 
?leaner? feature extraction method. 
33
 
where    is the set of seed terms with class label j, S is the set of all seed terms, count(f,c) is the count 
of co-occurrences of feature f with seed terms in class c, count(c) is the total number of feature co-
occurrences with seed terms in class c, and F is the set of all features (used for Laplace smoothing).  
Using the na?ve Bayes conditional independence assumption, the class of each term in a subcorpus 
was then computed by maximizing the product of the prior probability for a class and the product of 
the conditional probabilities of the term?s features: 
         
    
 ( ) ? (   )
   
 
 
Terms for which no diagnostic features existed were labeled as ?unknown?.   
Once the terms in a subcorpus were categorized as attribute, component, or task, the terms identi-
fied as attributes were selected as input to a second round of classification.4  We used the same boot-
strapping process as described for the first round, choosing a small set of features highly diagnostic of 
the polarity of attributes.  For positive polarity, the seed features were: increase, raise, maximize. For 
negative polarity: avoid, lower, decrease, deal_with, eliminate, minimize, reduce, resulting_from, 
caused_by.  Based on co-occurrence with these features, a set of terms was produced from which pa-
rameters for a larger set of features could be estimated, as described above.  We then used na?ve Bayes 
classification to label the full set of attribute terms. 
3 Results 
We present results from two domains, health and computer science, using a corpus consisting of all 
US patent applications submitted in the year 2002. The health subcorpus consisted of 19,800 docu-
ments, while the computer science subcorpus contained 51,058 documents.  A ?generic? corpus com-
posed of 38,482 patents randomly selected from all domains was also constructed for the year for use 
in computing a ?domain specificity score?.  This score was designed to measure the degree to which a 
term could be considered part of a specific domain?s vocabulary and was computed as the 
log(probability of term in domain corpus / probability of term in generic corpus).  For example, in 
computer science, the term encryption technology earned a domain specificity score of 4.132, while 
speed earned .783 and color garnered .022.  Using a combination of term frequency (# of documents a 
term occurs in within a domain) thresholds and domain specificity, one can extract subsets of terms 
with varying degrees of relevance within a collection.5 
 
3.1 Attribute/Component/Task (ACT) Classification 
The bootstrapping process generated 1,644 features for use in the health domain and 3,200 in com-
puter science. Kullback-Leibler divergence is a commonly used metric for comparing the difference 
between two probability distributions (Kullback and Leibler, 1951).  By computing Kullback-Leibler 
divergence    (    ) between the distribution P of classes predicted by each feature (i.e., the proba-
bility of the class given the feature alone based on the term seed set labels) and the prior class distribu-
tion Q, we could estimate the impact of individual features in the model.  Table 1 shows some of the 
domain-specific features in the health and computer science domains, along with the category each 
tended to select for.6   
Using the features generated by bootstrapping, the classifier was able to label 61% of the 1,335,240 
terms in health and 81% of the 1,391,402 terms in computer science.  The majority of unlabeled terms 
were extremely low frequency (typically 1).  Higher frequency unlabeled terms were typically from 
categories other than those under consideration here (e.g., john wiley, j. biochem, 2nd edition).  The 
distribution of category labels for the health and computer domains is shown in Table 2. 
                                                 
4 We found relatively little evidence of explicit sentiment targeted at component and task aspects in patents and therefore 
focused our polarity analysis on attributes.  
5 Similar to Velardi?s use of ?domain relevance? and ?consensus? (Velardi, 2001). 
6 Although it is possible to use KL-Divergence for feature selection, it is applied here solely for diagnostic purposes to verify 
that feature distributions match our intuitions with respect to the classification scheme. 
34
 
 
Table 1.  Features highly associated with classes (a[ttribute], c[omponent], t[ask]) in the health and com-
puter science domains, along with an example of a term co-occurring with each feature in some patent. 
Health                                                                              Computer Science 
Feature Class Term Feature Class Term 
prev_V=performed_during                  t biopsy prev_V=automates                t retrieval 
prev_V=undergone                     t angioplasty last_word=translation            t axis translation 
prev_V=suffer                            a hypertension prev_Npr=reduction_in         a power usage 
prev_Npr=monitoring_of           a alertness Prev_Npr=degradation_in                  a audio quality 
prev_V=binds_to                        c cytokines prev_V=displayed_on           c oscillograph 
prev_Npr=salts_of                      c saccharin last_word=information          c customer infor-
mation 
 
Table 2. Number and percentage of category labels for health and computer domains (2002) 
Category Health Computer Science 
attribute 88,860   (10.8 %) 56,389   (6.5%) 
component 680,034  (83.2%) 716,688  (83.2%) 
task 48,002  (5.8 %) 88,786   (10.3%) 
 
Tables 3a and 3b show examples of machine-labeled terms for the health and computer science do-
mains.  When terms were ranked by frequency, given a relatively relaxed domain specificity threshold 
(e.g., .05 for health), the top terms tended to capture broad semantic types relevant to the domain.   As 
this threshold was increased (e.g., to 1.0 for health), the terms increased in specialization within each 
class.7 As the table entries show, while the classification is not perfect, most terms fit the definitions of 
their respective classes.  Note that in the health domain in particular, many of the ?components? reflect 
objects acted upon by the invention, not just constituents of inventions themselves.  Symptoms and 
diseases are interpreted as attributes because they are often measured according to severity and are 
targets for reduction.  
 
Table 3a. Examples  of ACT category results for health domain at two levels of domain specificity (ds). 
Component 
(ds .05) 
  
(ds 1.0) 
Attribute 
(ds .05) 
  
(ds 1.0) 
Task 
(ds .05) 
  
(ds 1.0) 
patients, 
tissue, 
blood, 
diseases, 
drugs, 
skin, 
catheter, 
brain, 
tablets, 
organs 
mitral valve, 
arterial blood, 
small incisions, 
pulmonary 
veins, 
anterior cham-
ber, 
intraocular 
lens, 
ultrasound sys-
tem, 
ultrasound en-
ergy, 
adenosine tri-
phosphate, 
bone fragments 
disease, 
infection, 
symptoms, 
pain, 
efficacy, 
side effects, 
inflammation, 
severity, 
death, 
blood flow 
cosmetic prop-
erties, 
cardiac activity, 
urination, 
tissue tempera-
ture, 
gastric empty-
ing, 
arousal 
neurotransmitter 
release, 
atrial arrhyth-
mias, 
thrombogenicity 
ventricular pac-
ing 
treatment, 
administration, 
therapy, 
surgery, 
diagnosis, 
oral admin-
istration, 
implantation, 
stimulation, 
parenteral 
administration, 
surgical pro-
cedures 
invasive proce-
dure, 
ultrasound imag-
ing, 
systole, 
anastomosis, 
spinal fusion, 
tissue ablation, 
image, recon-
struction, 
cardiac pacing, 
mass analysis, 
spinal surgery 
 
  
                                                 
7 The domain specificity thresholds chosen here differ between domains in order to compensate for the influence of the size 
of each domain?s subcorpus on the terminology mix in the ?generic? domain corpus against which domain specificity is 
measured.   In the future, we plan to compensate directly for these size disparities in the score computation.  
35
Table 3b. Examples of ACT category results for computer domain at two levels of domain specificity. 
Component 
(ds 1.5) 
 
(ds 3.0) 
Attribute 
(ds 1.5) 
  
(ds 3.0) 
Task 
(ds 1.5) 
  
(ds 3.0) 
data, 
information, 
network, 
computer, 
users, 
memory, 
internet, 
software, 
program, 
processor 
web applica-
tions, 
object access 
protocol, 
loans, 
memory sub-
system, 
function call, 
obligations, 
source file, 
file formats, 
lender 
centralized 
database 
errors, 
security, 
real time, 
traffic, 
overhead, 
delays, 
latency, 
burden, 
sales, 
copyright, 
protection 
interest rate, 
resource utiliza-
tion, 
resource con-
sumption, 
temporal locali-
ty, 
system errors, 
transport layer 
security, 
performance 
bottleneck, 
processor ca-
pacity, 
cpu utilization, 
shannon limit 
access, 
communication, 
execution, 
implementation, 
communications, 
management, 
task, 
tasks, 
stores, 
collection 
network envi-
ronments, 
business activi-
ties, 
database access, 
server process, 
search operation, 
client 's request, 
backup opera-
tion, 
project man-
agement, 
program devel-
opment, 
document man-
agement 
 
3.2 Polarity Classification 
For the polarity classification task, the system assigned positive or negative polarity to 80,870 
health and 73,289 computer science attributes. While not all the system labeled attributes merited their 
designation as attributes, the large quantity so labeled in each domain illustrates the vast number of 
conditions and dimensions for which inventions are striving to ?move the needle? one way or the oth-
er, relative to attributes in the domain.  Examples of the system?s polarity decisions are shown in Ta-
ble 4.  The system?s labels suggest that the default polarity of attributes in both domains is nearly 
evenly split. 
 
Table 4.  Examples of (pos)itive and (neg)ative polarity terms in health and computer science domains 
Domain # attributes % of total Examples 
health   
          pos 
43807 54% ambulation, hemodynamic performance, atrial rate, antico-
agulant activity, coaptation, blood oxygen saturation 
 
          neg 
37063 46% bronchospasm, thrombogenicity, ventricular pacing, with-
drawal symptoms, fibrin formation, cardiac dysfunction 
computer 
science                     
          pos 
32291 44% transport layer security, processor capacity, cpu utilization, 
routability, network speeds, microprocessor performance 
 
          neg 
40998 56% identity theft, deadlocks, system overhead, memory frag-
mentation, risk exposure, bus contention, software devel-
opment costs, network latencies, data entry errors 
 
4 Evaluation and discussion 
In order to evaluate the classification output, we first selected a subset of terms within each domain 
as candidates for evaluation based on the twin criteria of document frequency and domain specificity.  
That is, we wished to concentrate on terms with sufficient presence in the corpus as well as terms that 
were likely to express concepts of particular relevance to the domain.  Using a frequency threshold of 
10 this yielded 19,088 terms for the health corpus and 35,220 for computer science with domain speci-
ficity scores above .05 and 1.5 respectively.  For each domain, two judges annotated approximately 
150 random term instances with ACT judgments and approximately 100 machine-labeled attributes for 
polarity. The annotation tool displayed each term along with five random sentences from the corpus 
that contained the term, and asked the judge to choose the best label, given the contexts provided.  An 
36
?other? option was available if the term fit none of the target categories.  For the polarity task, the 
?other? label included cases where the attribute was neutral, could not be assigned a polarity, or was 
improperly assigned the category ?attribute?.   An adjudicated gold standard was compared to system 
labels to measure precision and recall, as shown in table 5.  
 
Table 5a. Health domain: precision, recall and F-score for ACT and polarity classification tasks 
Task      Category Precision Recall F-score 
ACT attribute .70 .44 .54 
 component .76 1.0 .86 
 task .86 .29 .43 
Polarity  positive  .53 .85 .65 
                 negative .77 .93 .84 
 
 Table 5b. Computer domain: precision, recall and F-score for ACT and polarity classification tasks 
Task      Category Precision Recall F-score 
ACT attribute .80 .62 .70 
 component .86 .96 .90 
 task .43 .33 .38 
Polarity  positive  .67 .88 .76 
                 negative .75 .86 .80 
 
 Although the size of the evaluation set is small, we can make some observations from this sample. 
Precision in most cases is strong, which is important for the intended use of this data to characterize 
trends along each dimension using terminology statistics over time.  The lower scores for tasks within 
the ACT classification may reflect the fact that the distinction between component and task is not al-
ways clear cut.  The term ?antivirus protection?, for example, describes a task but it is classified by the 
system as a component because it occurs with features like ?prev_V=distribute? and 
?prev_V=provided_with?, which outweigh the contribution of the feature ?last_word=protection? to 
select for the type task.  To capture such cases of role ambiguity, it may be reasonable to assign some 
terms to multiple classes when the conditional probabilities for the two most probable classes are very 
close (as they are in this case).  It may also be possible to integrate other forms of evidence, such as 
syntactic coordination patterns (Zierning, 2013) to refine system decisions. 
 One shortcoming of the current polarity classifier is that it does not attempt to identify attributes for 
which the polarity is neutral or dependent upon further context within the domain.  For example, the 
attribute ?body weight gain? is labeled as a negative.  However, in the context of premature birth or 
cancer recovery, it may be actually be a positive attribute.  Testing whether an attribute co-occurs with 
conflicting features (e.g., prev_V=increase and prev_V=decrease) could help spot such cases. 
5 Related work 
Text mining from patents has focused on identifying domain keywords and terminology for analyt-
ics (Tseng, 2007).  Velardi?s (2001) approach, using statistics to determine domain relevance and con-
sensus is very similar to that adopted here. We have also drawn inspiration from sentiment analysis, 
proposing an ontology for patents that reflects their review-like qualities (Liu, 2012).  Most relevant is 
the work on discovering aspects and opinions relating to a particular subject such as a camera or res-
taurant (Kobayashi, 2007).  There are many subtleties that have been studied in opinion mining re-
search that we have finessed in our research here, such as detecting implicit sentiment and attributes 
not expressed as noun phrases.  Wilson et al (2005, 2009) addressed the larger problem of determining 
contextual polarity for subjective expressions in general, putting considerable effort into the compila-
tion of subjectivity clues and annotations.  In contrast, our aim was to test whether we could substan-
tially reduce the annotation effort when the task is focused on polarity labeling of attributes within pa-
tents.  We hypothesized that the specialized role of patents might permit a more lightweight approach 
amenable to bootstrapping from a very small set of annotations and feature types.   
37
Bootstrapping has been successfully applied to developing semantic lexicons containing a variety of 
concept types (Riloff, 1999; Thelen, 2002).  It is often applied iteratively to learn new discriminative 
features after a set of high probability categorized terms are identified during an earlier round.  While 
this increases recall, it also runs the risk of semantic drift if some terms are erroneously labeled.  Giv-
en that the majority of unlabeled terms after a single round in our system are either extremely low fre-
quency or not relevant to our ontology, we have not felt a need to run multiple iterations.  Zierning 
(2013) used bootstrapping to identify instances of the classes substance and disease in patents, exploit-
ing the tendency of syntactic coordination to relate noun phrases of the same semantic type.  Given the 
general nature of coordination, a similar approach could be used to find corroborating evidence for the 
classifications that our system produces. 
  
6 Conclusion 
We have described an approach to text data mining from patents that strikes a middle ground be-
tween undifferentiated keywords and rich, domain specific ontologies.  Motivated by the interpretation 
of patents as ?positive reviews?, we have made use of generic lexico-syntactic features common 
across patent domains to bootstrap domain-specific classifiers capable of organizing terms according 
to their roles as components, tasks and attributes with polarity.  Although the majority of keywords in 
a domain are categorized as components, the ontology puts tasks and attributes on an equal footing 
with components, thereby shifting the emphasis from devices and processes to the goals, obstacles and 
targets of inventions, information which could be valuable for analysts attempting to detect trends and 
make forecasts. In addition to more rigorous evaluation and tuning, future research directions include 
testing the approach across a wider range of technology domains, incorporation into time series analy-
sis for forecasting, and mining relationships between terms from different categories to provide an 
even richer terminological landscape for analysts to work with. 
Acknowledgements 
This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via 
Department of Interior National Business Center (DoI/NBC) contract number D11PC20154. The U.S. 
Government is authorized to reproduce and distribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Disclaimer: The views and conclusions contained herein 
are those of the authors and should not be interpreted as necessarily representing the official policies 
or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government. 
References 
de Miranda, G. M. Coelho, Dos, and L. F. Filho. (2006) Text mining as a valuable tool in foresight exercises: A 
study on nanotechnology.  Technological Forecasting and Social Change, 73(8):1013?1027. 
Kobayashi, N., Inui, K. and Matsumoto, Y. (2007) Extracting aspect-evaluation and aspect-of relations in opin-
ion mining, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing 
and Computational Natural Language Learning, Prague, Czech Republic, pp. 1065?1074. 
Kullback, S. and Leibler, R. (1951). "On Information and Sufficiency". Annals of Mathematical Statistics 22 (1): 
79?86. 
Liu, B. (2012): Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies, 
Morgan & Claypool Publishers. 
Manning, C., Raghavan, P. and Sch?tze, H. (2008) Introduction to Information Retrieval.  Cambridge University 
Press. 
Riloff, E. and Jones, R. (1999) Learning dictionaries for information extraction by multi-level bootstrapping. In 
Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications 
of Artificial Intelligence Conference, pp. 474?479. 
Riloff, E. and Shepherd, J. (1997) A corpus-based approach for building semantic lexicons. In Proceedings of the 
Second Conference on Empirical Methods in Natural Language Processing, pp. 117?124. 
38
Shih, M.J., Liu, D.R., and Hsu, M.L. (2008) Mining Changes in Patent Trends for Competitive Intelligence. 
PAKDD 2008: 999-1005. 
Sheremetyeva S. 2009. An Efficient Patent Keyword Extractor As Translation Resource Proceedings of the 3rd 
Workshop on Patent Translation in conjunction with MT-Summit XII Ottawa, Canada. 
Thelen, M. and Riloff, E.  (2002) A bootstrapping method for learning semantic lexicons using extraction pattern 
contexts. In Proceedings of the Conference on Empirical Methods in Natural Language. 
Toutanova, K., Klein, D., Manning, C. and Singer, Y. (2003) Feature-Rich Part-of-Speech Tagging with a Cyclic 
Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259. 
Tseng, Y.-H., Lin, C.-J., and Lin, Y.-I. (2007). Text mining techniques for patent analysis. Information Pro-
cessing & Management, 43(5):1216 ? 1247. 
Velardi, P., Fabriani, P. and Missikoff, M. (2001) FOIS '01 Proceedings of the international conference on For-
mal Ontology in Information Systems - Volume 2001, pp. 270-284. 
Wilson, T., Wiebe, J and Hoffmann, P. (2005). Recognizing Contextual Polarity in Phrase-Level Sentiment 
Analysis. Joint Human Language Technology Conference and the Conference on Empirical Methods in Natu-
ral Language Processing (HLT-EMNLP-2005). 
 Wilson, T., Wiebe, J and Hoffmann, P. (2009). Recognizing Contextual Polarity: an exploration of features for 
phrase-level sentiment analysis. Computational Linguistics 35(3). 
Woon, W. L., Henschel, A., and Madnick, S. (2009) A Framework for Technology Forecasting and Visualiza-
tion.  Working Paper CISL# 2009-11 , Massachusetts Institute of Technology.  
Yang, S.Y., Lin, S.Y., Lin, S. N., Lee, C. F., Cheng, S. L., and Soo, V. W. (2008) Automatic extraction of se-
mantic relations from patent claims.  International Journal of Electronic Business Management, Vol. 6, No. 1, 
pp. 45-54 (2008) 45. 
Ziering, P., van der Plas, L. and Sch?tze, H. (2013) Bootstrapping Semantic Lexicons for Technical Domains. In 
Proceedings of the Sixth International Joint Conference on Natural Language Processing, pp. 844?848, Nago-
ya, Japan, October 2013. Asian Federation of Natural Language Processing. 
 
 
 
39
