Proceedings of the Workshop on Statistical Machine Translation, pages 86?93,
New York City, June 2006. c?2006 Association for Computational Linguistics
Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation 
 
Karolina Owczarzak Declan Groves Josef Van Genabith Andy Way 
National Centre for Language Technology 
School of Computing 
Dublin City University 
Dublin 9, Ireland 
{owczarzak,dgroves,josef,away}@computing.dcu.ie 
 
 
Abstract 
In this paper we present a novel method 
for deriving paraphrases during automatic 
MT evaluation using only the source and 
reference texts, which are necessary for 
the evaluation, and word and phrase 
alignment software. Using target language 
paraphrases produced through word and 
phrase alignment a number of alternative 
reference sentences are constructed auto-
matically for each candidate translation. 
The method produces lexical and low-
level syntactic paraphrases that are rele-
vant to the domain in hand, does not use 
external knowledge resources, and can be 
combined with a variety of automatic MT 
evaluation system. 
1 Introduction 
Since their appearance, BLEU (Papineni et al, 
2002) and NIST (Doddington, 2002) have been the 
standard tools used for evaluating the quality of 
machine translation. They both score candidate 
translations on the basis of the number of n-grams 
it shares with one or more reference translations 
provided. Such automatic measures are indispen-
sable in the development of machine translation 
systems, because they allow the developers to con-
duct frequent, cost-effective, and fast evaluations 
of their evolving models.  
These advantages come at a price, though: an 
automatic comparison of n-grams measures only 
the string similarity of the candidate translation to 
one or more reference strings, and will penalize 
any divergence from them. In effect, a candidate 
translation expressing the source meaning accu-
rately and fluently will be given a low score if the 
lexical choices and syntactic structure it contains, 
even though perfectly legitimate, are not present in 
at least one of the references. Necessarily, this 
score would not reflect a much more favourable 
human judgment that such a translation would re-
ceive. 
The limitations of string comparison are the 
reason why it is advisable to provide multiple ref-
erences for a candidate translation in the BLEU- or 
NIST-based evaluation in the first place. While 
(Zhang and Vogel, 2004) argue that increasing the 
size of the test set gives even more reliable system 
scores than multiple references, this still does not 
solve the inadequacy of BLEU and NIST for sen-
tence-level or small set evaluation. On the other 
hand, in practice even a number of references do 
not capture the whole potential variability of the 
translation. Moreover, often it is the case that mul-
tiple references are not available or are too difficult 
and expensive to produce: when designing a statis-
tical machine translation system, the need for large 
amounts of training data limits the researcher to 
collections of parallel corpora like Europarl 
(Koehn, 2005), which provides only one reference, 
namely the target text; and the cost of creating ad-
ditional reference translations of the test set, usu-
ally a few thousand sentences long, often exceeds 
the resources available. Therefore, it would be de-
sirable to find a way to automatically generate le-
gitimate translation alternatives not present in the 
reference(s) already available. 
86
In this paper, we present a novel method that 
automatically derives paraphrases using only the 
source and reference texts involved in for the 
evaluation of French-to-English Europarl transla-
tions produced by two MT systems: statistical 
phrase-based Pharaoh (Koehn, 2004) and rule-
based Logomedia.1 In using what is in fact a minia-
ture bilingual corpus our approach differs from the 
mainstream paraphrase generation based on mono-
lingual resources. We show that paraphrases pro-
duced in this way are more relevant to the task of 
evaluating machine translation than the use of ex-
ternal lexical knowledge resources like thesauri or 
WordNet2, in that our paraphrases contain both 
lexical equivalents and low-level syntactic vari-
ants, and in that, as a side-effect, evaluation bitext-
derived paraphrasing naturally yields domain-
specific paraphrases. The paraphrases generated 
from the evaluation bitext are added to the existing 
reference sentences, in effect creating multiple ref-
erences and resulting in a higher score for the can-
didate translation. Our hypothesis, confirmed by 
the experiments in this paper, is that the scores 
raised by additional references produced in this 
way will correlate better with human judgment 
than the original scores. 
The remainder of this paper is organized as fol-
lows: Section 2 describes related work; Section 3 
describes our method and presents examples of 
derived paraphrases; Section 4 presents the results 
of the comparison between the BLUE and NIST 
scores for a single-reference translation and the 
same translation using the paraphrases automati-
cally generated from the bitext, as well as the cor-
relations between the scores and human judgment; 
Section 5 discusses ongoing work; Section 6 con-
cludes. 
2 
2.1 
                                                          
Related work 
Word and phrase alignment 
Several researchers noted that the word and 
phrase alignment used in training translation mod-
els in Statistical MT can be used for other purposes 
as well. (Diab and Resnik, 2002) use second lan-
guage alignments to tag word senses. Working on 
an assumption that separate senses of a L1 word 
2.2 
1 http://www.lec.com/ 
2 http://wordnet.princeton.edu/ 
can be distinguished by its different translations in 
L2, they also note that a set of possible L2 transla-
tions for a L1 word may contain many synonyms. 
(Bannard and Callison-Burch, 2005), on the other 
hand, conduct an experiment to show that para-
phrases derived from such alignments can be se-
mantically correct in more than 70% of the cases. 
Automatic MT evaluation 
The insensitivity of BLEU and NIST to per-
fectly legitimate variation has been raised, among 
others, in (Callison-Burch et al, 2006), but the 
criticism is widespread. Even the creators of BLEU 
point out that it may not correlate particularly well 
with human judgment at the sentence level (Pap-
ineni et al, 2002), a problem also noted by (Och et 
al., 2003) and (Russo-Lassner et al, 2005). A side 
effect of this phenomenon is that BLEU is less re-
liable for smaller data sets, so the advantage it pro-
vides in the speed of evaluation is to some extent 
counterbalanced by the time spent by developers 
on producing a sufficiently large test data set in 
order to obtain a reliable score for their system.  
Recently a number of attempts to remedy these 
shortcomings have led to the development of other 
automatic machine translation metrics. Some of 
them concentrate mainly on the word reordering 
aspect, like Maximum Matching String (Turian et 
al., 2003) or Translation Error Rate (Snover et al, 
2005). Others try to accommodate both syntactic 
and lexical differences between the candidate 
translation and the reference, like CDER (Leusch 
et al, 2006), which employs a version of edit dis-
tance for word substitution and reordering; 
METEOR (Banerjee and Lavie, 2005), which uses 
stemming and WordNet synonymy; and a linear 
regression model developed by (Russo-Lassner et 
al., 2005), which makes use of stemming, Word-
Net synonymy, verb class synonymy, matching 
noun phrase heads, and proper name matching. 
A closer examination of these metrics suggests 
that the accommodation of lexical equivalence is 
as difficult as the appropriate treatment of syntactic 
variation, in that it requires considerable external 
knowledge resources like WordNet, verb class da-
tabases, and extensive text preparation: stemming, 
tagging, etc. The advantage of our method is that it 
produces relevant paraphrases with nothing more 
than the evaluation bitext and a widely available 
word and phrase alignment software, and therefore 
can be used with any existing evaluation metric. 
87
3 Contextual bitext-derived paraphrases 
The method presented in this paper rests on a 
combination of two simple ideas. First, the compo-
nents necessary for automatic MT evaluation like 
BLEU or NIST, a source text and a reference text, 
constitute a miniature parallel corpus, from which 
word and phrase alignments can be extracted 
automatically, much like during the training for a 
statistical machine translation system. Second, tar-
get language words ei1, ?,  ein aligned as the likely 
translations to a source language word fi are often 
synonyms or near-synonyms of each other. This 
also holds for phrases: target language phrases epi1, 
?, epin aligned with a source language phrase fpi 
are often paraphrases of each other. For example, 
in our experiment, for the French word question 
the most probable automatically aligned English 
translations are question, matter, and issue, which 
in English are practically synonyms. Section 3.2 
presents more examples of such equivalent expres-
sions.  
3.1 
3.2 
                                                          
Experimental design 
For our experiment, we used two test sets, 
each consisting of 2000 sentences, drawn ran-
domly from the test section of the Europarl parallel 
corpus. The source language was French and the 
target language was English. One of the test sets 
was translated by Pharaoh trained on 156,000 
French-English sentence pairs. The other test set 
was translated by Logomedia, a commercially 
available rule-based MT system. Each test set con-
sisted therefore of three files: the French source 
file, the English translation file, and the English 
reference file. 
Each translation was evaluated by the BLEU 
and NIST metrics first with the single reference, 
then with the multiple references for each sentence 
using the paraphrases automatically generated 
from the source-reference mini corpus. A subset of 
a 100 sentences was randomly extracted from each 
test set and evaluated by two independent human 
judges with respect to accuracy and fluency; the 
human scores were then compared to the BLEU 
and NIST scores for the single-reference and the 
automatically generated multiple-reference files. 
Word alignment and phrase extraction 
We used the GIZA++ word alignment soft-
ware3 to produce initial word alignments for our 
miniature bilingual corpus consisting of the source 
French file and the English reference file, and the 
refined word alignment strategy of (Och and Ney, 
2003; Koehn et al, 2003; Tiedemann, 2004) to 
obtain improved word and phrase alignments. 
For each source word or phrase fi that is 
aligned with more than one target words or 
phrases, its possible translations ei1, ..., ein were 
placed in a list as equivalent expressions (i.e. 
synonyms, near-synonyms, or paraphrases of each 
other). A few examples are given in (1). 
 
(1) agreement - accordance 
adopted - implemented 
matter - lot - case 
funds - money 
arms - weapons 
area - aspect  
question ? issue ? matter 
we would expect - we cer-
tainly expect 
bear on - are centred 
around 
 
Alignment divides target words and 
phrases into equivalence sets; each set corresponds 
to one source word/phrase that was originally 
aligned with the target elements. For example, for 
the French word citoyens three English words were 
deemed to be the most appropriate translations: 
people, public, and citizens; therefore these three 
words constitute an equivalence set. Another 
French word population was aligned with two 
English translations: population and people; so the 
word people appears in two equivalence set (this 
gives rise to the question of equivalence transitiv-
ity, which will be discussed in Section 3.3). From 
the 2000-sentence evaluation bitext we derived 769 
equivalence sets, containing in total 1658 words or 
phrases. Each set contained on average two or 
three elements. In effect, we produced at least one 
equivalent expression for 1658 English words or 
phrases. 
An advantage of our method is that the tar-
get paraphrases and words come ordered with re-
3 http://www.fjoch.com/GIZA++ 
88
spect to their likelihood of being the translation of 
the source word or phrase ? each of them is as-
signed a probability expressing this likelihood, so 
we are able to choose only the most likely transla-
tions, according to some experimentally estab-
lished threshold. The experiment reported here was 
conducted without such a threshold, since the word 
and phrase alignment was of a very high quality. 
3.3 
3.4 
3.5 
Domain-specific lexical and syntactic 
paraphrases 
It is important to notice here how the para-
phrases produced are more appropriate to the task 
at hand than synonyms extracted from a general-
purpose thesaurus or WordNet. First, our para-
phrases are contextual - they are restricted to only 
those relevant to the domain of the text, since they 
are derived from the text itself. Given the context 
provided by our evaluation bitext, the word area in 
(1) turns out to be only synonymous with aspect, 
and not with land, territory, neighbourhood, divi-
sion, or other synonyms a general-purpose thesau-
rus or WordNet would give for this entry. This 
allows us to limit our multiple references only to 
those that are likely to be useful in the context pro-
vided by the source text. Second, the phrase align-
ment captures something neither a thesaurus nor 
WordNet will be able to provide: a certain amount 
of syntactic variation of paraphrases. Therefore, we 
know that a string such as we would expect in (1), 
with the sequence noun-aux-verb, might be para-
phrased by we certainly expect, a sequence of 
noun-adv-verb. 
Open and closed class items 
One important conclusion we draw from 
analysing the synonyms obtained through word 
alignment is that equivalence is limited mainly to 
words that belong to open word classes, i.e. nouns, 
verbs, adjectives, adverbs, but is unlikely to extend 
to closed word classes like prepositions or pro-
nouns. For instance, while the French preposition ? 
can be translated in English as to, in, or at, depend-
ing on the context, it is not the case that these three 
prepositions are synonymous in English. The divi-
sion is not that clear-cut, however: within the class 
of pronouns, he, she, and you are definitely not 
synonymous, but the demonstrative pronouns this 
and that might be considered equivalent for some 
purposes. Therefore, in our experiment we exclude 
prepositions and in future work we plan to examine 
the word alignments more closely to decide 
whether to exclude any other words. 
Creating multiple references 
After the list of synonyms and paraphrases is 
extracted from the evaluation bitext, for each 
reference sentence a string search replaces every 
eligible word or phrase with its equivalent(s) from 
the paraphrase list, one at a time, and the resulting 
string is added to the array of references. The 
original string is added to the array as well. This 
process results in a different number of reference 
sentences for every test sentence, depending on 
whether there was anything to replace in the refer-
ence and how many paraphrases we have available 
for the original substring. One example of this 
process is shown in (2). 
 
(2) Original reference: 
i admire the answer mrs parly 
gave this morning but we have 
turned a blind eye to that 
Paraphrase 1: 
i admire the reply mrs parly 
gave this morning but we have 
turned a blind eye to that 
Paraphrase 2: 
i admire the answer mrs parly 
gave this morning however we 
have turned a blind eye to 
that  
Paraphrase 3: 
i admire the answer mrs parly 
gave this morning but we have 
turned a blind eye to it 
 
Transitivity 
As mentioned before, an interesting question 
that arises here is the potential transitivity of our 
automatically derived synonyms/paraphrases. It 
could be argued that if the word people is equiva-
lent to public according to one set from our list, 
and to the word population according to another 
set, then public can be thought of as equivalent to 
population. In this case, the equivalence is not con-
troversial. However, consider the following rela-
tion: if sure in one of the equivalence sets is 
synonymous to certain, and certain in a different 
89
set is listed as equivalent to some, then treating 
sure and some as synonyms is a mistake. In our 
experiment we do not allow synonym transitivity; 
we only use the paraphrases from equivalence sets 
containing the word/phrase we want to replace.  
Multiple simultaneous substitution 
Note that at the moment the references we are 
producing do not contain multiple simultaneous 
substitutions of equivalent expressions; for exam-
ple, in (2) we currently do not produce the follow-
ing versions: 
 
(3) Paraphrase 4:  
i admire the reply mrs parly 
gave this morning however we 
have turned a blind eye to 
that 
Paraphrase 5: 
i admire the answer mrs parly 
gave this morning however we 
have turned a blind eye to it 
Paraphrase 6: 
i admire the reply mrs parly 
gave this morning but we have 
turned a blind eye to it 
 
This can potentially prevent higher n-grams being 
successfully matched if two or more equivalent 
expressions find themselves within the range of n-
grams being tested by BLEU and NIST. To avoid 
combinatorial problems, implementing multiple 
simultaneous substitutions could be done using a 
lattice, much like in (Pang et al, 2003). 
4 Results 
As expected, the use of multiple references 
produced by our method raises both the BLEU and 
NIST scores for translations produced by Pharaoh 
(test set PH) and Logomedia (test set LM). The 
results are presented in Table 1. 
 
 BLEU NIST 
PH single ref 0.2131 6.1625 
PH multi ref 0.2407 7.0068 
LM single ref 0.1782 5.5406 
LM multi ref 0.2043 6.3834 
 
Table 1. Comparison of single-reference and multi-
reference scores for test set PH and test set LM 
 
The hypothesis that the multiple-reference 
scores reflect better human judgment is also con-
firmed. For 100-sentence subsets (Subset PH and 
Subset LM) randomly extracted from our test sets 
PH and LM, we calculated Pearson?s correlation 
between the average accuracy and fluency scores 
that the translations in this subset received from 
two human judges (for each subset) and the single-
reference and multiple-reference sentence-level 
BLEU and NIST scores.  
There are two issues that need to be noted at 
this point. First, BLEU scored many of the sen-
tences as zero, artificially leveling many of the 
weaker translations.4 This explains the low, al-
though still statistically significant (p value < 
0.015) correlation with BLEU for both single and 
multiple reference translations. Using a version of 
BLEU with add-one smoothing we obtain consid-
erably higher correlations. Table 2 shows Pear-
son?s correlation coefficient for BLEU, BLEU 
with add-one smoothing, NIST, and human judg-
ments for Subsets PH. Multiple paraphrase refer-
ences produced by our method consistently lead to 
a higher correlation with human judgment for 
every metric.6 
 
                           Subset PH 
Metric  
single 
ref 
multi 
ref 
H & BLEU 0.297 0.307 
H & BLEU smoothed 0.396 0.404 
H & NIST  0.323 0.355 
 
Table 2. Pearson?s correlation between human 
judgment and single-reference and multiple-
reference BLEU, smoothed BLEU, and NIST for 
subset PH (of test set PH)  
 
The second issue that requires explanation is 
the lower general scores Logomedia?s translation 
received on the full set of 2000 sentences, and the 
extremely low correlation of its automatic evalua-
tion with human judgment, irrespective of the 
number of references. It has been noticed (Calli-
                                                          
4 BLEU uses a geometric average while calculating the sen-
tence-level score and will score a sentence as 0 if it does not 
have at least one 4-gram.  
5 A critical value for Pearson?s correlation coefficient for the 
sample size between 90 and 100 is 0.267, with p < 0.01. 
6 The significance of the rise in scores was confirmed in a 
resampling/bootstrapping test, with p < 0.0001. 
90
son-Burch et al, 2006) that BLEU and NIST fa-
vour n-gram based MT models such as Pharaoh, so 
the translation produced by Logomedia scored 
lower on the automatic evaluation, even though the 
human judges rated Logomedia output higher than 
Pharaoh?s translation. Both human judges consis-
tently gave very high scores to most sentences in 
subset LM (Logomedia), and as a consequence 
there was not enough variation in the scores as-
signed by them to create a good correlation with 
the BLEU and NIST scores. The average human 
scores for the subsets PH and LM and the coeffi-
cients of variation are presented in Table 3. It is 
easy to see that Logomedia?s translation received a 
higher mean score (on a scale 0 to 5) from the hu-
man judges and with less variance than Pharaoh. 
 
 Mean score  Variation 
Subset PH 3.815 19.1% 
Subset LM 4.005 16.25% 
 
Table 3. Human judgment mean scores and coeffi-
cients of variation for Subset PH and Subset LM 
 
As a result of the consistently high human scores 
for Logomedia, none of the Pearson?s correlations 
computed for Subset LM is high enough to be sig-
nificant. The values are lower than the critical 
value 0.164 corresponding to p < 0.10. 
 
                          Subset LM 
Metric  
single 
ref 
multi 
ref 
H & BLEU 0.046* 0.067* 
H & BLEU smoothed 0.163* 0.151* 
H & NIST  0.078* 0.116* 
 
Table 4. Pearson?s correlation between human 
judgment and single-reference and multiple-
reference BLEU, smoothed BLEU, and NIST for 
subset LM (of test set LM). * denotes values with p >  
0.10. 
5 Current and future work 
We would like to experiment with the way in 
which the list of equivalent expressions is pro-
duced. One possible development would be to de-
rive the expressions from a very large training 
corpus used by a statistical machine translation 
system, following (Bannard and Callison-Burch, 
2005), for instance, and use it as an external wider-
purpose knowledge resource (rather than a current 
domain-tailored resource as in our experiment), 
which would be nevertheless improve on a thesau-
rus in that it would also include phrase equivalents 
with some syntactic variation. According to (Ban-
nard and Callison-Burch, 2005), who derived their 
paraphrases automatically from a corpus of over a 
million German-English Europarl sentences, the 
baseline syntactic and semantic accuracy of the 
best paraphrases (those with the highest probabil-
ity) reaches 48.9% and 64.5%, respectively. That 
is, by replacing a phrase with its one most likely 
paraphrase the sentence remained syntactically 
well-formed in 48.9% of the cases and retained its 
meaning in 65% of the cases. 
In a similar experiment we generated para-
phrases from a French-English Europarl corpus of 
700,000 sentences. The data contained a consid-
erably higher level of noise than our previous ex-
periment on the 2000-sentence test set, even 
though we excluded any non-word entities from 
the results. Like (Bannard and Callison-Burch, 
2005), we used the product of probabilities p(fi|ei1) 
and p(ei2|fi) to determine the best paraphrase for a 
given English word ei1. We then compared the ac-
curacy across four samples of data. Each sample 
contained 50 randomly drawn words/phrases and 
their paraphrases. For the first two samples, the 
paraphrases were derived from the initial 2000-
sentence corpus; for the second two, the para-
phrases were derived from the 700,000-sentence 
corpus. For each corpus, one of the two samples 
contained only one best paraphrase for each entry, 
while the other listed all possible paraphrases. We 
then evaluated the quality of each paraphrase with 
respect to its syntactic and semantic accuracy. In 
terms of syntax, we considered the paraphrase ac-
curate either if it had the same category as the 
original word/phrase; in terms of semantics, we 
relied on human judgment of similarity. Tables 5 
and 6 summarize the syntactic and semantic accu-
racy levels in the samples. 
 
                       Paraphrases 
Derived from 
Best All 
2000-sent. corpus 59% 60% 
700,000-sent. corpus 70% 48% 
 
Table 5. Syntactic accuracy of paraphrases 
 
 
91
                       Paraphrases 
Derived from 
Best All 
2000-sent. corpus 83% 74% 
700,000-sent. corpus 76% 68% 
 
Table 6. Semantic accuracy of paraphrases 
 
Although it has to be kept in mind that these 
percentages were taken from relatively small sam-
ples, an interesting pattern emerges from compar-
ing the results. It seems that the average syntactic 
accuracy of all paraphrases decreases with in-
creased corpus size, but the syntactic accuracy of 
the one best paraphrase improves. This reflects the 
idea behind word alignment: the bigger the corpus, 
the more potential alignments there are for a given 
word, but at the same time the better their order in 
terms of probability and the likelihood to obtain 
the correct translation. Interestingly, the same pat-
tern is not repeated for semantic accuracy, but 
again, these samples are quite small. In order to 
address this issue, we plan to repeat the experiment 
with more data. 
Additionally, it should be noted that certain 
expressions, although not completely correct syn-
tactically, could be retained in the paraphrase lists 
for the purposes of machine translation evaluation. 
Consider the case where our equivalence set looks 
like this: 
 
(4) abandon ? abandoning ? 
abandoned 
 
The words in (4) are all inflected forms of the verb 
abandon, and although they would produce rather 
ungrammatical paraphrases, those ungrammatical 
paraphrases still allow us to score our translation 
higher in terms of BLEU or NIST if it contains one 
of the forms of abandon than when it contains 
some unrelated word like piano instead. This is 
exactly what other scoring metrics mentioned in 
Section 2 attempt to obtain with the use of stem-
ming or prefix matching. 
6 Conclusions 
In this paper we present a novel combination 
of existing ideas from statistical machine transla-
tion and paraphrase generation that leads to the 
creation of multiple references for automatic MT 
evaluation, using only the source and reference 
files that are required for the evaluation. The 
method uses simple word and phrase alignment 
software to find possible synonyms and para-
phrases for words and phrases of the target text, 
and uses them to produce multiple reference sen-
tences for each test sentence, raising the BLEU and 
NIST evaluation scores and reflecting human 
judgment better. The advantage of this method 
over other ways to generate paraphrases is that (1) 
unlike other methods, it does not require extensive 
parallel monolingual paraphrase corpora, but it 
extracts equivalent expressions from the miniature 
bilingual corpus of the source and reference 
evaluation files; (2) unlike other ways to accom-
modate synonymy in automatic evaluation, it does 
not require external lexical knowledge sources like 
thesauri or WordNet; (3) it extracts only synonyms 
that are relevant to the domain in hand; and (4) the 
equivalent expressions it produces include a certain 
amount of syntactic paraphrases.  
The method is general and it can be used with 
any automatic evaluation metric that supports mul-
tiple references. In our future work, we plan to ap-
ply it to newly developed evaluation metrics like 
CDER and TER that aim to allow for syntactic 
variation between the candidate and the reference, 
therefore bringing together solutions for the two 
shortcomings of automatic evaluation systems: 
insensitivity to allowable lexical differences and 
syntactic variation. 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. Proceed-
ings of the ACL 2005 Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or Sum-
marization: 65-73. 
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proceed-
ings of the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL 2005): 597-604. 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn. 2006. Re-evaluating the role of BLEU in 
Machine Translation Research. To appear in Pro-
ceedings of EACL-2006. 
Mona Diab and Philip Resnik. 2002. An unsupervised 
Method for Word Sense Tagging using Parallel Cor-
pora. Proceedings of the 40th Annual Meeting of the 
92
Association for Computational Linguistics, Philadel-
phia, PA. 
George Doddington. 2002. Automatic Evaluation of MT 
Quality using N-gram Co-occurrence Statistics. Pro-
ceedings of Human Language Technology Confer-
ence 2002: 138?145. 
Philipp Koehn, Franz Och and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. Proceedings of 
the  Human Language Technology Conference (HLT-
NAACL 2003): 48-54. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation mod-
els. Machine translation: From real users to re-
search. 6th Conference of the Association for 
Machine Translation in the Americas (AMTA 2004): 
115-124. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. Proceedings of MT 
Summit 2005: 79-86. 
Gregor Leusch, Nicola Ueffing and Hermann Ney. 
2006. CDER: Efficient MT Evaluation Using Block 
Movements. To appear in Proceedings of the 11th 
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL 2006). 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Modes. 
Computational Linguistics, 29:19?51. 
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, 
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar 
Kumar, Libin Shen, David Smith, Katherine Eng, 
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. 
Syntax for statistical machine translation. Technical 
report, Center for Language and Speech Processing, 
John Hopkins University, Baltimore, MD.  
Bo Pang, Kevin Knight and Daniel Marcu. 2003. Syn-
tax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences. 
Proceedings of Human Language Technology-North 
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL) 2003: 181?188. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
ACL: 311-318. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-based Approach to Machine 
Translation Evaluation. Technical Report LAMP-
TR-125/CS-TR-4754/UMIACS-TR-2005-57, Uni-
versity of Maryland, College Park, MD. 
Mathew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciula and Ralph Weischedel. 
2005. A Study of Translation Error Rate with Tar-
geted Human Annotation. Technical Report LAMP-
TR-126, CS-TR-4755, UMIACS-TR-2005-58, Uni-
versity of Maryland, College Park. MD. 
J?rg Tiedemann. 2004. Word to word alignment strate-
gies. Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING 2004): 
212-218. 
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine translation and Its 
Evaluation. Proceedings of MT Summit 2003: 386-
393. 
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalua-
tion metrics. TMI-2004: Proceedings of the 10th 
Conference on Theoretical and Methodological Is-
sues in Machine Translation: 85-94. 
93
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 80?87,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Dependency-Based Automatic Evaluation for Machine Translation 
Karolina Owczarzak Josef van Genabith Andy Way  
National Centre for Language Technology  
School of Computing, Dublin City University  
Dublin 9, Ireland  
{owczarzak,josef,away}@computing.dcu.ie  
    
 
Abstract 
We present a novel method for evaluating 
the output of Machine Translation (MT), 
based on comparing the dependency 
structures of the translation and reference 
rather than their surface string forms. Our 
method uses a treebank-based, wide-
coverage, probabilistic Lexical-Functional 
Grammar (LFG) parser to produce a set of 
structural dependencies for each 
translation-reference sentence pair, and 
then calculates the precision and recall for 
these dependencies. Our dependency-
based evaluation, in contrast to most 
popular string-based evaluation metrics, 
will not unfairly penalize perfectly valid 
syntactic variations in the translation. In 
addition to allowing for legitimate 
syntactic differences, we use paraphrases 
in the evaluation process to account for 
lexical variation. In comparison with 
other metrics on 16,800 sentences of 
Chinese-English newswire text, our 
method reaches high correlation with 
human scores. An experiment with two 
translations of 4,000 sentences from 
Spanish-English Europarl shows that, in 
contrast to most other metrics, our method 
does not display a high bias towards 
statistical models of translation. 
1 Introduction 
Since their appearance, string-based evaluation 
metrics such as BLEU (Papineni et al, 2002) and 
NIST (Doddington, 2002) have been the standard 
tools used for evaluating MT quality. Both score a 
candidate translation on the basis of the number of 
n-grams shared with one or more reference 
translations. Automatic measures are indispensable 
in the development of MT systems, because they 
allow MT developers to conduct frequent, cost-
effective, and fast evaluations of their evolving 
models.  
These advantages come at a price, though: an 
automatic comparison of n-grams measures only 
the string similarity of the candidate translation to 
one or more reference strings, and will penalize 
any divergence from them. In effect, a candidate 
translation expressing the source meaning 
accurately and fluently will be given a low score if 
the lexical and syntactic choices it contains, even 
though perfectly legitimate, are not present in at 
least one of the references. Necessarily, this score 
would differ from a much more favourable human 
judgement that such a translation would receive. 
The limitations of string comparison are the 
reason why it is advisable to provide multiple 
references for a candidate translation in BLEU- or 
NIST-based evaluations. While Zhang and Vogel 
(2004) argue that increasing the size of the test set 
gives even more reliable system scores than 
multiple references, this still does not solve the 
inadequacy of BLEU and NIST for sentence-level 
or small set evaluation. In addition, in practice 
even a number of references do not capture the 
whole potential variability of the translation. 
Moreover, when designing a statistical MT system, 
the need for large amounts of training data limits 
the researcher to collections of parallel corpora 
such as Europarl (Koehn, 2005), which provides 
only one reference, namely the target text; and the 
cost of creating additional reference translations of 
the test set, usually a few thousand sentences long, 
is often prohibitive. Therefore, it would be 
desirable to find an evaluation method that accepts 
legitimate syntactic and lexical differences 
80
between the translation and the reference, thus 
better mirroring human assessment. 
In this paper, we present a novel method that 
automatically evaluates the quality of translation 
based on the dependency structure of the sentence, 
rather than its surface form. Dependencies abstract 
away from the particulars of the surface string (and 
CFG tree) realization and provide a ?normalized? 
representation of (some) syntactic variants of a 
given sentence. The translation and reference files 
are analyzed by a treebank-based, probabilistic 
Lexical-Functional Grammar (LFG) parser (Cahill 
et al, 2004), which produces a set of dependency 
triples for each input. The translation set is 
compared to the reference set, and the number of 
matches is calculated, giving the precision, recall, 
and f-score for that particular translation.   
In addition, to allow for the possibility of valid 
lexical differences between the translation and the 
references, we follow Kauchak and Barzilay 
(2006) and Owczarzak et al (2006) in adding a 
number of paraphrases in the process of evaluation 
to raise the number of matches between the 
translation and the reference, leading to a higher 
score. 
Comparing the LFG-based evaluation method 
with other popular metrics: BLEU, NIST, General 
Text Matcher (GTM) (Turian et al, 2003), 
Translation Error Rate (TER) (Snover et al, 
2006)1, and METEOR (Banerjee and Lavie, 2005), 
we show that combining dependency 
representations with paraphrases leads to a more 
accurate evaluation that correlates better with 
human judgment. 
The remainder of this paper is organized as 
follows: Section 2 gives a basic introduction to 
LFG; Section 3 describes related work; Section 4 
describes our method and gives results of two 
experiments on different sets of data: 4,000 
sentences from Spanish-English Europarl and 
16,800 sentences of Chinese-English newswire text 
from the Linguistic Data Consortium?s (LDC) 
Multiple Translation project; Section 5 discusses 
ongoing work; Section 6 concludes. 
                                                 
1 As we focus on purely automatic metrics, we omit 
HTER (Human-Targeted Translation Error Rate) here. 
2 Lexical-Functional Grammar 
In Lexical-Functional Grammar (Bresnan, 2001) 
sentence structure is represented in terms of 
c(onstituent)-structure and f(unctional)-structure. 
C-structure represents the surface string word order 
and the hierarchical organisation of phrases in 
terms of CFG trees. F-structures are recursive 
feature (or attribute-value) structures, representing 
abstract grammatical relations, such as subj(ect), 
obj(ect), obl(ique), adj(unct), approximating to 
predicate-argument structure or simple logical 
forms. C-structure and f-structure are related in 
terms of functional annotations (attribute-value 
structure equations) in c-structure trees, describing 
f-structures.  
While c-structure is sensitive to surface word 
order, f-structure is not. The sentences John 
resigned yesterday and Yesterday, John resigned 
will receive different tree representations, but 
identical f-structures, shown in (1). 
 
(1) C-structure:                         F-structure: 
 
              S 
                  
      
 NP                      VP 
   |                     
John       
              V               NP-TMP 
               |                      | 
       resigned       yesterday 
                         
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
                     S 
                  
      
    NP       NP       VP 
      |                 |            | 
Yesterday  John        V              
                                    | 
                            resigned                             
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
 
Notice that if these two sentences were a 
translation-reference pair, they would receive a 
less-than-perfect score from string-based metrics. 
For example, BLEU with add-one smoothing2 
gives this pair a score of barely 0.3781. 
The f-structure can also be described as a flat 
set of triples. In triples format, the f-structure in (1) 
could be represented as follows: {subj(resign, 
john), pers(john, 3), num(john, sg), tense(resign, 
                                                 
2 We use smoothing because the original BLEU gives 
zero points to sentences with fewer than one four-gram. 
81
past), adj(resign, yesterday), pers(yesterday, 3), 
num(yesterday, sg)}. 
Cahill et al (2004) presents Penn-II Treebank-
based LFG parsing resources. Her approach 
distinguishes 32 types of dependencies, including 
grammatical functions and morphological 
information. This set can be divided into two major 
groups: a group of predicate-only dependencies 
and non-predicate dependencies. Predicate-only 
dependencies are those whose path ends in a 
predicate-value pair, describing grammatical 
relations. For example, for the f-structure in (1), 
predicate-only dependencies would include: 
{subj(resign, john), adj(resign, yesterday)}.3  
In parser evaluation, the quality of the f-
structures produced automatically can be checked 
against a set of gold standard sentences annotated 
with f-structures by a linguist. The evaluation is 
conducted by calculating the precision and recall 
between the set of dependencies produced by the 
parser, and the set of dependencies derived from 
the human-created f-structure. Usually, two 
versions of f-score are calculated: one for all the 
dependencies for a given input, and a separate one 
for the subset of predicate-only dependencies. 
In this paper, we use the parser developed by 
Cahill et al (2004), which automatically annotates 
input text with c-structure trees and f-structure 
dependencies, reaching high precision and recall 
rates. 4  
3 Related work 
The insensitivity of BLEU and NIST to perfectly 
legitimate syntactic and lexical variation has been 
raised, among others, in Callison-Burch et al 
(2006), but the criticism is widespread. Even the 
creators of BLEU point out that it may not 
correlate particularly well with human judgment at 
the sentence level (Papineni et al, 2002). A side 
                                                 
3 Other predicate-only dependencies include: 
apposition,  complement, open complement, 
coordination, determiner, object, second object, 
oblique, second oblique, oblique agent, possessive, 
quantifier, relative clause, topic, relative clause 
pronoun. The remaining non-predicate dependencies 
are: adjectival degree, coordination surface form, focus, 
complementizer forms: if, whether, and that, modal, 
number, verbal particle, participle, passive, person, 
pronoun surface form, tense, infinitival clause. 
4 http://lfg-demo.computing.dcu.ie/lfgparser.html 
effect of this phenomenon is that BLEU is less 
reliable for smaller data sets, so the advantage it 
provides in the speed of evaluation is to some 
extent counterbalanced by the time spent by 
developers on producing a sufficiently large test 
set in order to obtain a reliable score for their 
system.  
Recently a number of attempts to remedy these 
shortcomings have led to the development of other 
automatic MT evaluation metrics. Some of them 
concentrate mainly on word order, like General 
Text Matcher (Turian et al, 2003), which 
calculates precision and recall for translation-
reference pairs, weighting contiguous matches 
more than non-sequential matches, or Translation 
Error Rate (Snover et al, 2005), which computes 
the number of substitutions, inserts, deletions, and 
shifts necessary to transform the translation text to 
match the reference. Others try to accommodate 
both syntactic and lexical differences between the 
candidate translation and the reference, like CDER 
(Leusch et al, 2006), which employs a version of 
edit distance for word substitution and reordering; 
or METEOR (Banerjee and Lavie, 2005), which 
uses stemming and WordNet synonymy. Kauchak 
and Barzilay (2006) and Owczarzak et al (2006) 
use paraphrases during BLEU and NIST evaluation 
to increase the number of matches between the 
translation and the reference; the paraphrases are 
either taken from WordNet5 in Kauchak and 
Barzilay (2006) or derived from the test set itself 
through automatic word and phrase alignment in 
Owczarzak et al (2006). Another metric making 
use of synonyms is the linear regression model 
developed by Russo-Lassner et al (2005), which 
makes use of stemming, WordNet synonymy, verb 
class synonymy, matching noun phrase heads, and 
proper name matching. Kulesza and Schieber 
(2004), on the other hand, train a Support Vector 
Machine using features like proportion of n-gram 
matches and word error rate to judge a given 
translation?s distance from human-level quality. 
Nevertheless, these metrics use only string-
based comparisons, even while taking into 
consideration reordering. By contrast, our 
dependency-based method concentrates on 
utilizing linguistic structure to establish a 
comparison between translated sentences and their 
reference.  
                                                 
5 http://wordnet.princeton.edu/ 
82
4 LFG f-structure in MT evaluation 
The process underlying the evaluation of f-
structure quality against a gold standard can be 
used in automatic MT evaluation as well: we parse 
the translation and the reference, and then, for each 
sentence, we check the set of translation 
dependencies against the set of reference 
dependencies, counting the number of matches. As 
a result, we obtain the precision and recall scores 
for the translation, and we calculate the f-score for 
the given pair. Because we are comparing two 
outputs that were produced automatically, there is 
a possibility that the result will not be noise-free. 
To assess the amount of noise that the parser 
may introduce we conducted an experiment where 
100 English Europarl sentences were modified by 
hand in such a way that the position of adjuncts 
was changed, but the sentence remained 
grammatical and the meaning was not changed. 
This way, an ideal parser should give both the 
source and the modified sentence the same f-
structure, similarly to the case presented in (1). The 
modified sentences were treated like a translation 
file, and the original sentences played the part of 
the reference. Each set was run through the parser. 
We evaluated the dependency triples obtained from 
the ?translation? against the dependency triples for 
the ?reference?, calculating the f-score, and applied 
other metrics (TER, METEOR, BLEU, NIST, and 
GTM) to the set in order to compare scores. The 
results, inluding the distinction between f-scores 
for all dependencies and predicate-only 
dependencies, appear in Table 1. 
 
 baseline modified 
TER 0.0 6.417 
METEOR   1.0 0.9970 
BLEU 1.0000 0.8725 
NIST 11.5232 11.1704 (96.94%) 
GTM 100 99.18 
dep f-score  100 96.56 
dep_preds f-score 100 94.13 
Table 1. Scores for sentences with reordered adjuncts 
 
The baseline column shows the upper bound for a 
given metric: the score which a perfect translation, 
word-for-word identical to the reference, would 
obtain.6 In the other column we list the scores that 
the metrics gave to the ?translation? containing 
reordered adjunct. As can be seen, the dependency 
and predicate-only dependency scores are lower 
than the perfect 100, reflecting the noise 
introduced by the parser.  
To show the difference between the scoring 
based on LFG dependencies and other metrics in 
an ideal situation, we created another set of a 
hundred sentences with reordered adjuncts, but this 
time selecting only those reordered sentences that 
were given the same set of dependencies by the 
parser (in other words, we simulated having the 
ideal parser). As can be seen in Table 2, other 
metrics are still unable to tolerate legitimate 
variation in the position of adjuncts, because the 
sentence surface form differs from the reference; 
however, it is not treated as an error by the parser. 
 
 baseline modified 
TER 0.0 7.841 
METEOR   1.0 0.9956 
BLEU 1.0000 0.8485 
NIST 11.1690 10.7422 (96.18%) 
GTM 100 99.35 
dep f-score  100 100 
dep_preds f-score 100 100 
Table 2. Scores for sentences with reordered adjuncts in 
an ideal situation 
4.1 Initial experiment ? Europarl 
In the first experiment, we attempted to determine 
whether the dependency-based measure is biased 
towards statistical MT output, a problem that has 
been observed for n-gram-based metrics like 
BLEU and NIST. Callison-Burch et al (2006) 
report that BLEU and NIST favour n-gram-based 
MT models such as Pharaoh (Koehn, 2004), so the 
translations produced by rule-based systems score 
lower on the automatic evaluation, even though 
human judges consistently rate their output higher 
than Pharaoh?s translation. Others repeatedly 
                                                 
6 Two things have to be noted here: (1) in case of NIST 
the perfect score differs from text to text, which is why 
we provide the percentage points as well, and (2) in case 
of TER the lower the score, the better the translation, so 
the perfect translation will receive 0, and there is no 
upper bound on the score, which makes this particular 
metric extremely difficult to directly compare with 
others. 
83
observed this tendency in previous research as 
well; in one experiment, reported in Owczarzak et 
al. (2006), where the rule-based system 
Logomedia7 was compared with Pharaoh, BLEU 
scored Pharaoh 0.0349 points higher, NIST scored 
Pharaoh 0.6219 points higher, but human judges 
scored Logomedia output 0.19 points higher (on a 
5-point scale).  
4.1.1 Experimental design 
In order to check for the existence of a bias in the 
dependency-based metric, we created a set of 
4,000 sentences drawn randomly from the Spanish-
English subset of Europarl (Koehn, 2005), and we 
produced two translations: one by a rule-based 
system Logomedia, and the other by the standard 
phrase-based statistical decoder Pharaoh, using 
alignments produced by GIZA++8 and the refined 
word alignment strategy of Och and Ney (2003). 
The translations were scored with a range of 
metrics: BLEU, NIST, GTM, TER, METEOR, and 
the dependency-based method. 
4.1.2 Adding synonyms 
Besides the ability to allow syntactic variants as 
valid translations, a good metric should also be 
able to accept legitimate lexical variation. We 
introduced synonyms and paraphrases into the 
process of evaluation, creating new best-matching 
references for the translations using either 
paraphrases derived from the test set itself 
(following Owczarzak et al (2006)) or WordNet 
synonyms (as in Kauchak and Barzilay (2006)). 
 
Bitext-derived paraphrases 
Owczarzak et al (2006) describe a simple way to 
produce a list of paraphrases, which can be useful 
in MT evaluation, by running word alignment 
software on the test set that is being evaluated. 
Paraphrases derived in this way are specific to the 
domain at hand and contain low-level syntactic 
variants in addition to word-level synonymy. 
Using the standard GIZA++ software and the 
refined word alignment strategy of Och and Ney 
(2003) on our test set of 4,000 Spanish-English 
sentences, the method generated paraphrases for 
just over 1100 items. These paraphrases served to 
                                                 
7 http://www.lec.com/ 
8 http://www.fjoch.com/GIZA++ 
create new individual best-matching references for 
the Logomedia and Pharaoh translations. Due to 
the small size of the paraphrase set, only about 
20% of reference sentences were actually modified 
to better reflect the translation. This, in turn, led to 
little difference in scores. 
WordNet synonyms 
To maximize the number of matches between a 
translation and a reference, Kauchak and Barzilay 
(2006) use WordNet synonyms during evaluation. 
In addition, METEOR also has an option of 
including WordNet in the evaluation process. As in 
the case of bitext-derived paraphrases, we used 
WordNet synonyms to create new best-matching 
references for each of the two translations. This 
time, given the extensive database containing 
synonyms for over 150,000 items, around 70% of 
reference sentences were modified: 67% for 
Pharaoh, and 75% for Logomedia. Note that the 
number of substitutions is higher for Logomedia; 
this confirms the intuition that the translation 
produced by Pharaoh, trained on the domain which 
is also the source of the reference text, will need 
fewer lexical replacements than Logomedia, which 
is based on a general non-domain-specific model. 
4.1.3 Results 
Table 3 shows the difference between the scores 
which Pharaoh?s and Logomedia?s translations 
obtained from each metric: a positive number 
shows by how much Pharaoh?s score was higher 
than Logomedia?s, and a negative number reflects 
Logomedia?s higher score (the percentages are 
absolute values). As can be seen, all the metrics 
scored Pharaoh higher, inlcuding METEOR and 
the dependency-based method that were boosted 
with WordNet. The values in the table are sorted in 
descending order, from the largest to the lowest 
advantage of Pharaoh over Logomedia. 
Interestingly, next to METEOR boosted with 
WordNet, it is the dependency-based method, and 
especially the predicates-only version, that shows 
the least bias towards the phrase-based translation. 
In the next step, we selected from this set smaller 
subsets of sentences that were more and more 
similar in terms of translation quality (as 
determined by a sentence?s BLEU score). As the 
similarity of the translation quality increased, most 
metrics lowered their bias, as is shown in Table 4. 
The first column shows the case where the 
sentences chosen differed at the most by 0.05 
84
points BLEU score; in the second column the 
difference was lowered to 0.01; and in the third 
column to 0.005. The numbers following the hash 
signs in the header row indicate the number of 
sentences in a given set.  
 
metric PH score ? LM score 
TER 1.997 
BLEU 7.16% 
NIST 6.58% 
dep 4.93% 
dep+paraphr 4.80% 
GTM 3.89% 
METEOR 3.80% 
dep_preds 3.79% 
dep+paraphr_preds 3.70% 
dep+WordNet 3.55% 
dep+WordNet_preds 2.60% 
METEOR+WordNet 1.56% 
Table 3. Difference between scores assigned to Pharaoh 
and Logomedia. Positive numbers show by how much 
Pharaoh?s score was higher than Logomedia?s. Legend: 
dep = dependency f-score, paraph = paraphrases, _preds = 
predicate-only f-score.  
 
~ 0.05 #1692 ~ 0.01 #567 ~ 0.005 #335 
NIST 2.29% NIST 1.76% NIST 1.48% 
BLEU 0.95% BLEU 0.42% BLEU 0.59% 
GTM 0.94% GTM 0.29% GTM -0.09% 
d+p 0.67% d 0.04% d+p -0.15% 
d 0.61% d+p 0.02% d -0.24% 
d+WN -0.29% d+WN -0.78% d+WN -0.99% 
d+p_pr -0.70% M -0.99% d+p_pr -1.30% 
d_pr -0.75% d_pr -1.37% d_pr -1.43% 
M -1.03% d+p_pr -1.38% M -1.57% 
d+WN_pr -1.43% d+WN_pr -1.97% d+WN_pr -1.94% 
M+WN -2.51% M+WN -2.21% M+WN -2.74% 
TER -1.579 TER -1.228 TER -1.739 
Table 4. Difference between scores assigned to Pharaoh 
and Logomedia for sets of increasing similarity. Positive 
numbers show Pharaoh?s advantage, negative numbers 
show Logomedia?s advantage. Legend: d = dependency f-
score, p = paraphrases, _pr = predicate-only f-score, M = 
METEOR, WN = WordNet.  
 
These results confirm earlier suggestions that 
the predicate-only version of the dependency-
based evaluation is less biased in favour of the 
statistical MT system than the version that includes 
all dependency types. Adding a sufficient number 
of lexical choices reduces the bias even further; 
although again, paraphrases generated from the test 
set only are too few to make a significant 
difference. Similarly to METEOR, the 
dependency-based method shows on the whole 
lower bias than other metrics. However, we cannot 
be certain that the underlying scores vary linearly 
with each other and with human judgements, as we 
have no framework of reference such as human 
segment-level assessment of translation quality in 
this case. Therefore, the correlation with human 
judgement is analysed in our next experiment.   
4.2 Correlation with human judgement ? 
MultiTrans 
To calculate how well the dependency-based 
method correlates with human judgement, and how 
it compares to the correlation shown by other 
metrics, we conducted an experiment on Chinese-
English newswire text.  
4.2.1 Experimental design 
We used the data from the Linguistic Data 
Consortium Multiple Translation Chinese (MTC) 
Parts 2 and 4. The data consists of multiple 
translations of Chinese newswire text, four human-
produced references, and segment-level human 
scores for a subset of the translation-reference 
pairs. Although a single translated segment was 
always evaluated by more than one judge, the 
judges used a different reference every time, which 
is why we treated each translation-reference-
human score triple as a separate segment. In effect, 
the test set created from this data contained 16,800 
segments. As in the previous experiment, the 
translation was scored using BLEU, NIST, GTM, 
TER, METEOR, and the dependency-based 
method. 
4.2.2 Results 
We calculated Pearson?s correlation coefficient for 
segment-level scores that were given by each 
metric and by human judges. The results of the 
correlation are shown in Table 5. Note that the 
correlation for TER is negative, because in TER 
zero is the perfect score, in contrast to other 
metrics where zero is the worst possible score; 
however, this time the absolute values can be 
easily compared to each other. Rows are ordered 
85
by the highest value of the (absolute) correlation 
with the human score. 
First, it seems like none of the metrics is very 
good at reflecting human fluency judgments; the 
correlation values in the first column are 
significantly lower than the correlation with 
accuracy. However, the dependency-based method 
in almost all its versions has decidedly the highest 
correlation in this area. This can be explained by 
the method?s sensitivity to the grammatical 
structure of the sentence: a more grammatical 
translation is also a translation that is more fluent. 
 
H_FL  H_AC  H_AVE  
d+WN 0.168 M+WN 0.294 M+WN 0.255 
d   0.162 M   0.278 d+WN 0.244 
d+WN_pr 0.162 NIST 0.273 M   0.242 
BLEU 0.155 d+WN 0.266 NIST 0.238 
d_pr 0.154 GTM 0.260 d   0.236 
M+WN 0.153 d  0.257 GTM 0.230 
M   0.149 d+WN_pr 0.232 d+WN_pr 0.220 
NIST 0.146 d_pr 0.224 d_pr 0.212 
GTM 0.146 BLEU 0.199 BLEU 0.197 
TER -0.133 TER -0.192 TER -0.182 
Table 5. Pearson?s correlation between human scores and 
evaluation metrics. Legend: d = dependency f-score, _pr = 
predicate-only f-score, M = METEOR, WN = WordNet, 
H_FL = human fluency score, H_AC = human accuracy 
score, H_AVE = human average score.9 
 
Second, and somewhat surprisingly, in this 
detailed examination the relative order of the 
metrics changed. The predicate-only version of the 
dependency-based method appears to be less 
adequate for correlation with human scores than its 
non-restricted versions. As to the correlation with 
human evaluation of translation accuracy, our 
method currently falls short of METEOR and even 
NIST. This is caused by the fact that both 
METEOR and NIST assign relatively little 
importance to the position of a specific word in a 
sentence, therefore rewarding the translation for 
content rather than linguistic form. For our 
dependency-based method, the noise introduced by 
the parser might be the reason for low correlation: 
if even one side of the translation-reference pair 
contains parsing errors, this may lead to a less 
reliable score. An obvious solution to this problem, 
                                                 
9 In general terms, an increase of 0.015 between any two 
scores is significant with a 95% confidence interval. 
which we are examining at the moment, is to 
include a number of best parses for each side of the 
evaluation. 
High correlation with human judgements of 
fluency and lower correlation with accuracy results 
in a high second place for our dependency-based 
method when it comes to the average correlation 
coefficient. The WordNet-boosted dependency-
based method scores only slightly lower than 
METEOR with WordNet. These results are very 
encouraging, especially as we see a number of 
ways the dependency-based method could be 
further developed.  
5 Current and future work 
While the idea of a dependency-based method is a 
natural step in the direction of a deeper linguistic 
analysis for MT evaluation, it does require an LFG 
grammar and parser for the target language. There 
are several obvious areas for improvement with 
respect to the method itself. First, we would also 
like to adapt the process of translation-reference 
dependency comparison to include n-best parsers 
for the input sentences, as well as some basic 
transformations which would allow an even deeper 
logical analysis of input (e.g. passive to active 
voice transformation). 
 Second, we want to repeat both 
experiments using a paraphrase set derived from a 
large parallel corpus, rather than the test set, as 
described in Owczarzak et al (2006). While 
retaining the advantage of having a similar size to 
a corresponding set of WordNet synonyms, this set 
will also capture low-level syntactic variations, 
which can increase the number of matches and the 
correlation with human scores. 
 Finally, we want to take advantage of the 
fact that the score produced by the dependency-
based method is the proportional average of f-
scores for a group of up to 32 (but usually far 
fewer) different dependency types. We plan to 
implement a set of weights, one for each 
dependency type, trained in such a way as to 
maximize the correlation of the final dependency f-
score with human evaluation.  
6 Conclusions 
In this paper we present a novel way of 
evaluating MT output. So far, all metrics relied on 
86
comparing translation and reference on a string 
level. Even given reordering, stemming, and 
synonyms for individual words, current methods 
are still far from reaching human ability to assess 
the quality of translation. Our method compares 
the sentences on the level of their grammatical 
structure, as exemplified by their f-structure 
dependency triples produced by an LFG parser. 
The dependency-based method can be further 
augmented by using paraphrases or WordNet 
synonyms, and is available in full version and 
predicate-only version. In our experiments we 
showed that the dependency-based method 
correlates higher than any other metric with human 
evaluation of translation fluency, and shows high 
correlation with the average human score. The use 
of dependencies in MT evaluation is a rather new 
idea and requires more research to improve it, but 
the method shows potential to become an accurate 
evaluation metric.  
 
Acknowledgements 
This work was partly funded by Microsoft Ireland 
PhD studentship  2006-8  for the first author of the 
paper. We would also like to thank our reviewers 
for their insightful comments. All remaining errors 
are our own. 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments. 
Proceedings of the ACL 2005 Workshop on Intrinsic 
and Extrinsic Evaluation Measures for MT and/or 
Summarization: 65-73. 
Joan Bresnan. 2001. Lexical-Functional Syntax, 
Blackwell, Oxford. 
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef 
van Genabith, and Andy Way. 2004. Long-Distance 
Dependency Resolution in Automatically Acquired 
Wide-Coverage PCFG-Based LFG Approximations, 
In Proceedings of ACL-04: 320-327 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn. 2006. Re-evaluating the role of BLEU in 
Machine Translation Research. Proceedings of  
EACL 2006: 249-256 
George Doddington. 2002. Automatic Evaluation of MT 
Quality using N-gram Co-occurrence Statistics. 
Proceedings of HLT 2002: 138-145. 
David Kauchak and Regina Barzilay. 2006. 
Paraphrasing for Automatic Evaluation. Proceedings 
of HLT-NAACL 2006: 45-462. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. Proceedings of the AMTA 2004 Workshop 
on Machine Translation: From real users to 
research: 115-124. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. Proceedings of MT 
Summit 2005: 79-86. 
Alex Kulesza and Stuart M. Shieber. 2004. A learning 
approach to improving sentence-level MT evaluation. 
In Proceedings of the TMI 2004: 75-84. 
Gregor Leusch, Nicola Ueffing and Hermann Ney. 
2006. CDER: Efficient MT Evaluation Using Block 
Movements. Proceedings of EACL 2006: 241-248. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Modes. 
Computational Linguistics, 29:19-51. 
Karolina Owczarzak, Declan Groves, Josef van 
Genabith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation. 
Proceedings of the HLT-NAACL 2006 Workshop on 
Statistical Machine Translation: 86-93. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
WeiJing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
ACL 2002: 311-318. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-based Approach to Machine 
Translation Evaluation. Technical Report LAMP-TR-
125/CS-TR-4754/UMIACS-TR-2005-57, University 
of Maryland, College Park, MD. 
Mathew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciula. 2006. A Study of 
Translation Error Rate with Targeted Human 
Annotation. Proceedings of AMTA 2006: 223-231. 
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine Translation and Its 
Evaluation. Proceedings of MT Summit 2003: 386-
393. 
Ying Zhang and Stephan Vogel. 2004. Measuring 
confidence intervals for the machine translation 
evaluation metrics. Proceedings of TMI 2004: 85-94. 
87
Proceedings of the Second Workshop on Statistical Machine Translation, pages 104?111,
Prague, June 2007. c?2007 Association for Computational Linguistics
Labelled Dependencies in Machine Translation Evaluation 
Karolina Owczarzak Josef van Genabith Andy Way 
National Centre for Language Technology 
School of Computing, Dublin City University 
Dublin 9, Ireland 
{owczarzak,josef,away}@computing.dcu.ie 
    
 
Abstract 
We present a method for evaluating the 
quality of Machine Translation (MT) 
output, using labelled dependencies 
produced by a Lexical-Functional 
Grammar (LFG) parser. Our dependency-
based method, in contrast to most popular 
string-based evaluation metrics, does not 
unfairly penalize perfectly valid syntactic 
variations in the translation, and the 
addition of WordNet provides a way to 
accommodate lexical variation. In 
comparison with other metrics on 16,800 
sentences of Chinese-English newswire 
text, our method reaches high correlation 
with human scores.  
1 Introduction 
Since the creation of BLEU (Papineni et al, 2002) 
and NIST (Doddington, 2002), the subject of 
automatic evaluation metrics for MT has been 
given quite a lot of attention. Although widely 
popular thanks to their speed and efficiency, both 
BLEU and NIST have been criticized for 
inadequate accuracy of evaluation at the segment 
level (Callison-Burch et al, 2006). As string 
based-metrics, they are limited to superficial 
comparison of word sequences between a 
translated sentence and one or more reference 
sentences, and are unable to accommodate any 
legitimate grammatical variation when it comes to 
lexical choices or syntactic structure of the 
translation, beyond what can be found in the 
multiple references. A natural next step in the field 
of evaluation was to introduce metrics that would 
better reflect our human judgement by accepting 
synonyms in the translated sentence or evaluating 
the translation on the basis of what syntactic 
features it shares with the reference. 
Our method follows and substantially extends 
the earlier work of Liu and Gildea (2005), who use 
syntactic features and unlabelled dependencies to 
evaluate MT quality, outperforming BLEU on 
segment-level correlation with human judgement. 
Dependencies abstract away from the particulars of 
the surface string (and syntactic tree) realization 
and provide a ?normalized? representation of 
(some) syntactic variants of a given sentence.  
While Liu and Gildea (2005) calculate n-gram 
matches on non-labelled head-modifier sequences 
derived by head-extraction rules from syntactic 
trees, we automatically evaluate the quality of 
translation by calculating an f-score on labelled 
dependency structures produced by a Lexical-
Functional Grammar (LFG) parser. These 
dependencies differ from those used by Liu and 
Gildea (2005), in that they are extracted according 
to the rules of the LFG grammar and they are 
labelled with a type of grammatical relation that 
connects the head and the modifier, such as 
subject, determiner, etc. The presence of 
grammatical relation labels adds another layer of 
important linguistic information into the 
comparison and allows us to account for partial 
matches, for example when a lexical item finds 
itself in a correct relation but with an incorrect 
partner. Moreover, we use a number of best parses 
for the translation and the reference, which serves 
to decrease the amount of noise that can be 
introduced by the process of parsing and extracting 
dependency information. 
The translation and reference files are 
analyzed by a treebank-based, probabilistic LFG 
parser (Cahill et al, 2004), which produces a set of 
dependency triples for each input. The translation 
set is compared to the reference set, and the 
number of matches is calculated, giving the 
104
precision, recall, and f-score for each particular 
translation.   
In addition, to allow for the possibility of valid 
lexical differences between the translation and the 
references, we follow Kauchak and Barzilay 
(2006) in adding a number of synonyms in the 
process of evaluation to raise the number of 
matches between the translation and the reference, 
leading to a higher score. 
In an experiment on 16,800 sentences of 
Chinese-English newswire text with segment-level 
human evaluation from the Linguistic Data 
Consortium?s (LDC) Multiple Translation project, 
we compare the LFG-based evaluation method 
with other popular metrics like BLEU, NIST, 
General Text Matcher (GTM) (Turian et al, 2003), 
Translation Error Rate (TER) (Snover et al, 
2006)1, and METEOR (Banerjee and Lavie, 2005), 
and we show that combining dependency 
representations with synonyms leads to a more 
accurate evaluation that correlates better with 
human judgment. Although evaluated on a 
different test set, our method also outperforms the 
correlation with human scores reported in Liu and 
Gildea (2005). 
The remainder of this paper is organized as 
follows: Section 2 gives a basic introduction to 
LFG; Section 3 describes related work; Section 4 
describes our method and gives results of the 
experiment on the Multiple Translation data; 
Section 5 discusses ongoing work; Section 6 
concludes. 
2 Lexical-Functional Grammar 
In Lexical-Functional Grammar (Kaplan and 
Bresnan, 1982; Bresnan, 2001) sentence structure 
is represented in terms of c(onstituent)-structure 
and f(unctional)-structure. C-structure represents 
the word order of the surface string and the 
hierarchical organisation of phrases in terms of 
CFG trees. F-structures are recursive feature (or 
attribute-value) structures, representing abstract 
grammatical relations, such as subj(ect), obj(ect), 
obl(ique), adj(unct), etc., approximating to 
predicate-argument structure or simple logical 
forms. C-structure and f-structure are related in 
                                                 
1 We omit HTER (Human-Targeted Translation Error 
Rate), as it is not fully automatic and requires human 
input. 
terms of functional annotations (attribute-value 
structure equations) in c-structure trees, describing 
f-structures.  
While c-structure is sensitive to surface 
rearrangement of constituents, f-structure abstracts 
away from the particulars of the surface 
realization. The sentences John resigned yesterday 
and Yesterday, John resigned will receive different 
tree representations, but identical f-structures, 
shown in (1). 
 
(1) C-structure:                         F-structure: 
 
              S 
                  
      
 NP                      VP 
   |                     
John       
              V               NP-TMP 
               |                      | 
       resigned       yesterday 
                         
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
                     S 
                  
      
    NP       NP       VP 
      |                 |            | 
Yesterday  John        V              
                                    | 
                            resigned                             
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
 
Note that if these sentences were a translation-
reference pair, they would receive a less-than-
perfect score from string-based metrics. For 
example, BLEU with add-one smoothing2 gives 
this pair a score of barely 0.3781. This is because, 
although all three unigrams from the ?translation? 
(John; resigned; yesterday) are present in the 
reference, which contains four items including the 
comma (Yesterday; ,; John; resigned), the 
?translation? contains only one bigram (John 
resigned) that matches the ?reference? (Yesterday 
,; , John; John resigned), and no matching 
trigrams. 
The f-structure can also be described in terms 
of a flat set of triples. In triples format, the f-
structure in (1) is represented as follows: 
{subj(resign, john), pers(john, 3), num(john, sg), 
tense(resign, past), adj(resign, yesterday), 
pers(yesterday, 3), num(yesterday, sg)}. 
                                                 
2 We use smoothing because the original BLEU metric 
gives zero points to sentences with fewer than one four-
gram. 
105
Cahill et al (2004) presents a set of Penn-II 
Treebank-based LFG parsing resources. Their 
approach distinguishes 32 types of dependencies, 
including grammatical functions and 
morphological information. This set can be divided 
into two major groups: a group of predicate-only 
dependencies and non-predicate dependencies. 
Predicate-only dependencies are those whose path 
ends in a predicate-value pair, describing 
grammatical relations. For example, for the f-
structure in (1), predicate-only dependencies would 
include: {subj(resign, john), adj(resign, 
yesterday)}.  
Other predicate-only dependencies include: 
apposition, complement, open complement, 
coordination, determiner, object, second object, 
oblique, second oblique, oblique agent, possessive, 
quantifier, relative clause, topic, and relative 
clause pronoun. The remaining non-predicate 
dependencies are: adjectival degree, coordination 
surface form, focus, complementizer forms: if, 
whether, and that, modal, number, verbal particle, 
participle, passive, person, pronoun surface form, 
tense, and infinitival clause. 
In parser evaluation, the quality of the f-
structures produced automatically can be checked 
against a set of gold standard sentences annotated 
with f-structures by a linguist. The evaluation is 
conducted by calculating the precision and recall 
between the set of dependencies produced by the 
parser, and the set of dependencies derived from 
the human-created f-structure. Usually, two 
versions of f-score are calculated: one for all the 
dependencies for a given input, and a separate one 
for the subset of predicate-only dependencies. 
In this paper, we use the parser developed by 
Cahill et al (2004), which automatically annotates 
input text with c-structure trees and f-structure 
dependencies, obtaining high precision and recall 
rates. 3  
3 Related work 
3.1 String-based metrics 
The insensitivity of BLEU and NIST to perfectly 
legitimate syntactic and lexical variation has been 
raised, among others, in Callison-Burch et al 
(2006), but the criticism is widespread. Even the 
                                                 
3 A demo of the parser can be found at http://lfg-
demo.computing.dcu.ie/lfgparser.html 
creators of BLEU point out that it may not 
correlate particularly well with human judgment at 
the sentence level (Papineni et al, 2002).  
Recently a number of attempts to remedy these 
shortcomings have led to the development of other 
automatic MT evaluation metrics. Some of them 
concentrate mainly on word order, like General 
Text Matcher (Turian et al, 2003), which 
calculates precision and recall for translation-
reference pairs, weighting contiguous matches 
more than non-sequential matches, or Translation 
Error Rate (Snover et al, 2006), which computes 
the number of substitutions, insertions, deletions, 
and shifts necessary to transform the translation 
text to match the reference. Others try to 
accommodate both syntactic and lexical 
differences between the candidate translation and 
the reference, like CDER (Leusch et al, 2006), 
which employs a version of edit distance for word 
substitution and reordering; or METEOR 
(Banerjee and Lavie, 2005), which uses stemming 
and WordNet synonymy. Kauchak and Barzilay 
(2006) and Owczarzak et al (2006) use 
paraphrases during BLEU and NIST evaluation to 
increase the number of matches between the 
translation and the reference; the paraphrases are 
either taken from WordNet4 in Kauchak and 
Barzilay (2006) or derived from the test set itself 
through automatic word and phrase alignment in 
Owczarzak et al (2006). Another metric making 
use of synonyms is the linear regression model 
developed by Russo-Lassner et al (2005), which 
makes use of stemming, WordNet synonymy, verb 
class synonymy, matching noun phrase heads, and 
proper name matching. Kulesza and Shieber 
(2004), on the other hand, train a Support Vector 
Machine using features such as proportion of n-
gram matches and word error rate to judge a given 
translation?s distance from human-level quality.  
3.2 Dependency-based metric 
The metrics described above use only string-based 
comparisons, even while taking into consideration 
reordering. By contrast, Liu and Gildea (2005) 
present three metrics that use syntactic and 
unlabelled dependency information. Two of these 
metrics are based on matching syntactic subtrees 
between the translation and the reference, and one 
                                                 
4 http://wordnet.princeton.edu/ 
106
is based on matching headword chains, i.e. 
sequences of words that correspond to a path in the 
unlabelled dependency tree of the sentence. 
Dependency trees are created by extracting a 
headword for each node of the syntactic tree, 
according to the rules used by the parser of Collins 
(1999), where every subtree represents the 
modifier information for its root headword. The 
dependency trees for the translation and the 
reference are converted into flat headword chains, 
and the number of overlapping n-grams between 
the translation and the reference chains is 
calculated. Our method, extending this line of 
research with the use of labelled LFG 
dependencies, partial matching, and n-best parses, 
allows us to considerably outperform Liu and 
Gildea?s (2005) highest correlations with human 
judgement (they report 0.144 for the correlation 
with human fluency judgement, 0.202 for the 
correlation with human overall judgement), 
although it has to be kept in mind that such 
comparison is only tentative, as their correlation is 
calculated on a different test set. 
4 LFG f-structure in MT evaluation 
LFG-based automatic MT evaluation reflects the 
same process that underlies the evaluation of 
parser-produced f-structure quality against a gold 
standard: we parse the translation and the 
reference, and then, for each sentence, we check 
the set of labelled translation dependencies against 
the set of labelled reference dependencies, 
counting the number of matches. As a result, we 
obtain the precision and recall scores for the 
translation, and we calculate the f-score for the 
given pair.  
4.1 Determining parser noise 
Because we are comparing two outputs that were 
produced automatically, there is a possibility that 
the result will not be noise-free, even if the parser 
fails to provide a parse only in 0.1% of cases. 
To assess the amount of noise that the parser 
introduces, Owczarzak et al (2006) conducted an 
experiment where 100 English sentences were 
hand-modified so that the position of adjuncts was 
changed, but the sentence remained grammatical 
and the meaning was not influenced. This way, an 
ideal parser should give both the source and the 
modified sentence the same f-structure, similarly to 
the example presented in (1). The modified 
sentences were treated like a translation file, and 
the original sentences played the part of the 
reference. Each set was run through the parser, and 
the dependency triples obtained from the 
?translation? were compared against the 
dependency triples for the ?reference?, calculating 
the f-score. Additionally, the same ?translation-
reference? set was scored with other metrics (TER, 
METEOR, BLEU, NIST, and GTM). The results, 
including the distinction between f-scores for all 
dependencies and predicate-only dependencies, 
appear in Table 1. 
 
 baseline modified 
TER 0.0 6.417 
METEOR   1.0 0.9970 
BLEU 1.0000 0.8725 
NIST 11.5232 11.1704 (96.94%) 
GTM 100 99.18 
dep f-score  100 96.56 
dep_preds f-score 100 94.13 
Table 1. Scores for sentences with reordered adjuncts 
 
The baseline column shows the upper bound for a 
given metric: the score which a perfect translation, 
word-for-word identical to the reference, would 
obtain.5 The other column lists the scores that the 
metrics gave to the ?translation? containing 
reordered adjunct. As can be seen, the dependency 
and predicate-only dependency scores are lower 
than the perfect 100, reflecting the noise 
introduced by the parser. 
 We propose that the problem of parser 
noise can be alleviated by introducing a number of 
best parses into the comparison between the 
translation and the reference. Table 2 shows how 
increasing the number of parses available for 
comparison brings our method closer to an ideal 
noise-free parser.  
 
                                                 
5 Two things have to be noted here: (1) in the case of 
NIST the perfect score differs from text to text, which is 
why the percentage points are provided along the 
numerical score, and (2) in the case of TER the lower 
the score, the better the translation, so the perfect 
translation will receive 0, and there is no upper bound 
on the score, which makes this particular metric 
extremely difficult to directly compare with others. 
107
 dependency f-score 
1 best 96.56 
2 best 97.31 
5 best 97.90 
10 best 98.31 
20 best 98.59 
30 best 98.74 
50 best 98.79 
baseline 100 
Table 2.  Dependency f-scores for sentences with reordered 
adjuncts with n-best parses available 
 
It has to be noted, however, that increasing the 
number of parses beyond a certain threshold does 
little to further improve results, and at the same 
time it considerably decreases the efficiency of the 
method, so it is important to find the right balance 
between these two factors. In our opinion, the 
optimal value would be 10-best parses. 
4.2 Correlation with human judgement ? 
MultiTrans 
4.2.1 Experimental design 
To evaluate the correlation with human 
assessment, we used the data from the Linguistic 
Data Consortium Multiple Translation Chinese 
(MTC) Parts 2 and 4, which consists of multiple 
translations of Chinese newswire text, four human-
produced references, and segment-level human 
scores for a subset of the translation-reference 
pairs. Although a single translated segment was 
always evaluated by more than one judge, the 
judges used a different reference every time, which 
is why we treated each translation-reference-
human score triple as a separate segment. In effect, 
the test set created from this data contained 16,800 
segments. As in the previous experiment, the 
translation was scored using BLEU, NIST, GTM, 
TER, METEOR, and our labelled dependency-
based method. 
4.2.2 Labelled dependency-based method 
We examined a number of modifications of the 
dependency-based method in order to find out 
which one gives the highest correlation with 
human scores. The correlation differences between 
immediate neighbours in the ranking were often 
too small to be statistically significant; however, 
there is a clear overall trend towards improvement.  
Besides the plain version of the dependency f-
score, we also looked at the f-score calculated on 
predicate dependencies only (ignoring ?atomic? 
features such as person, number, tense, etc.), which 
turned out not to correlate well with human 
judgements. 
Another addition was the use of 2-, 10-, or 50-
best parses of the translation and reference 
sentences, which partially neutralized parser noise 
and resulted in increased correlations.  
We also created a version where predicate 
dependencies of the type subj(resign,John) are split 
into two parts, each time replacing one of the 
elements participating in the relation with a 
variable, giving in effect subj(resign,x) and 
subj(y,John). This lets us score partial matches, 
where one correct lexical object happens to find 
itself in the correct relation, but with an incorrect 
?partner?.  
Lastly, we added WordNet synonyms into the 
matching process to accommodate lexical 
variation, and to compare our WordNet-enhanced 
method with the WordNet-enhanced version of 
METEOR.  
4.2.3 Results 
We calculated Pearson?s correlation coefficient for 
segment-level scores that were given by each 
metric and by human judges. The results of the 
correlation are shown in Table 3. Note that the 
correlation for TER is negative, because in TER 
zero is the perfect score, in contrast to other 
metrics where zero is the worst possible score; 
however, this time the absolute values can be 
easily compared to each other. Rows are ordered 
by the highest value of the (absolute) correlation 
with the human score. 
First, it seems like none of the metrics is very 
good at reflecting human fluency judgments; the 
correlation values in the first column are 
significantly lower than the correlation with 
accuracy. This finding has been previously 
reported, among others, in Liu and Gildea (2005). 
However, the dependency-based method in almost 
all its versions has decidedly the highest 
correlation in this area. This can be explained by 
the method?s sensitivity to the grammatical 
structure of the sentence: a more grammatical 
translation is also a translation that is more fluent. 
As to the correlation with human evaluation of 
translation accuracy, our method currently falls 
108
short of METEOR. This is caused by the fact that 
METEOR assign relatively little importance to the 
position of a specific word in a sentence, therefore 
rewarding the translation for content rather than 
linguistic form. Interestingly, while METEOR, 
with or without WordNet, considerably 
outperforms all other metrics when it comes to the 
correlation with human judgements of translation 
accuracy, it falls well behind most versions of our 
dependency-based method in correlation with 
human scores of translation fluency. 
Surprisingly, adding partial matching to the 
dependency-based method resulted in the greatest 
increase in correlation levels, to the extent that the 
partial-match versions consistently outperformed 
versions with a larger number of parses available 
but without the partial match. The most interesting 
effect was that the partial-match versions (even 
those with just a single parse) offered results 
comparable to or higher than the addition of 
WordNet to the matching process when it comes to 
accuracy and overall judgement. 
5 Current and future work 
Fluency and accuracy are two very different 
aspects of translation quality, each with its own set 
of conditions along which the input is evaluated. 
Therefore, it seems unfair to expect a single 
automatic metric to correlate highly with human 
judgements of both at the same time. This pattern 
is very noticeable in Table 3: if a metric is 
(relatively) good at correlating with fluency, its 
accuracy correlation suffers (GTM might serve as 
an example here), and the opposite holds as well 
(see METEOR?s scores). It does not mean that any 
improvement that increases the method?s 
correlation with one aspect will result in a decrease 
in the correlation with the other aspect; but it does 
suggest that a possible way of development would 
be to target these correlations separately, if we 
want our automated metrics to reflect human 
scores better. At the same time, string-based 
metrics might have already exhausted their 
potential when it comes to increasing their 
correlation with human evaluation; as has been 
pointed out before, these metrics can only tell us 
that two strings differ, but they cannot distinguish 
legitimate grammatical variance from 
ungrammatical variance. As the quality of MT  
 
 
Table 3. Pearson?s correlation between human scores and 
evaluation metrics. Legend: d = dependency f-score, _pr = 
predicate-only f-score, 2, 10, 50 = n-best parses; var = 
partial-match version; M = METEOR, WN = WordNet6 
 
improves, the community will need metrics that are 
more sensitive in this respect. After all, the true 
quality of MT depends on producing grammatical 
output which describes the same concept as the 
source utterance, and the string identity with a 
reference is only a very selective approximation of 
this goal.  
                                                 
6 In general terms, an increase of 0.022 or more between 
any two scores in the same column is significant with a 
95% confidence interval. The statistical significance of 
correlation differences was calculated using Fisher?s z? 
transformation and the general formula for confidence 
interval. 
 
fluency  accuracy  average  
d_50+WN 0.177 M+WN 0.294 M+WN 0.255 
d+WN 0.175 M   0.278 d_50_var 0.252 
d_50_var 0.174 d_50_var 0.273 d_50+WN 0.250 
GTM 0.172 NIST 0.273 d_10_var 0.250 
d_10_var 0.172 d_10_var 0.273 d_2_var 0.247 
d_50 0.171 d_2_var 0.270 d+WN 0.244 
d_2_var 0.168 d_50+WN 0.269 d_50 0.243 
d_10 0.168 d_var 0.266 d_var 0.243 
d_var 0.165 d_50 0.262 M   0.242 
d_2 0.164 d_10 0.262 d_10 0.242 
d   0.161 d+WN 0.260 NIST 0.238 
BLEU 0.155 d_2 0.257 d_2 0.237 
M+WN 0.153 d  0.256 d   0.235 
M   0.149 d_pr 0.240 d_pr 0.216 
NIST 0.146 GTM 0.203 GTM 0.208 
d_pr 0.143 BLEU 0.199 BLEU 0.197 
TER -0.133 TER -0.192 TER -0.182 
109
 In order to maximize the correlation with 
human scores of fluency, we plan to look more 
closely at the parser output, and implement some 
basic transformations which would allow an even 
deeper logical analysis of input (e.g. passive to 
active voice transformation). 
  Additionally, we want to take advantage of 
the fact that the score produced by the dependency-
based method is the proportional average of 
matches for a group of up to 32 (but usually far 
fewer) different dependency types. We plan to 
implement a set of weights, one for each 
dependency type, trained in such a way as to 
maximize the correlation of the final dependency f-
score with human evaluation. In a preliminary 
experiment, for example, assigning a low weight to 
the topic dependency increases our correlations 
slightly (this particular case can also be seen as a 
transformation into a more basic logical form by 
removing non-elementary dependency types). 
 In a similar direction, we want to 
experiment more with the f-score calculations. 
Initial check shows that assigning a higher weight 
to recall than to precision improves results. 
 To improve the correlation with accuracy 
judgements, we would like to experiment using a 
paraphrase set derived from a large parallel corpus, 
as described in Owczarzak et al (2006). While 
retaining the advantage of having a similar size to 
a corresponding set of WordNet synonyms, this set 
will also capture low-level syntactic variations, 
which can increase the number of matches.  
6 Conclusions 
In this paper we present a linguistically-
motivated method for automatically evaluating the 
output of Machine Translation. Most currently 
used popular metrics rely on comparing translation 
and reference on a string level. Even given 
reordering, stemming, and synonyms for individual 
words, current methods are still far from reaching 
human ability to assess the quality of translation, 
and there exists a need in the community to 
develop more dependable metrics. Our method 
explores one such direction of development, 
comparing the sentences on the level of their 
grammatical structure, as exemplified by their f-
structure labelled dependency triples produced by 
an LFG parser. In our experiments we showed that 
the dependency-based method correlates higher 
than any other metric with human evaluation of 
translation fluency, and shows high correlation 
with the average human score. The use of 
dependencies in MT evaluation has not been 
extensively researched before (one exception here 
would be Liu and Gildea (2005)), and requires 
more research to improve it, but the method shows 
potential to become an accurate evaluation metric.  
 
Acknowledgements 
This work was partly funded by Microsoft Ireland 
PhD studentship  2006-8  for the first author of the 
paper. We would also like to thank our reviewers 
and Dan Melamed for their insightful comments. 
All remaining errors are our own. 
 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments. 
Proceedings of the Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization at the Association for Computational 
Linguistics Conference 2005: 65-73. Ann Arbor, 
Michigan. 
Joan Bresnan. 2001. Lexical-Functional Syntax, 
Blackwell, Oxford. 
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef 
van Genabith, and Andy Way. 2004. Long-Distance 
Dependency Resolution in Automatically Acquired 
Wide-Coverage PCFG-Based LFG Approximations, 
In Proceedings of Association for Computational 
Linguistics 2004: 320-327. Barcelona, Spain. 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn. 2006. Re-evaluating the role of BLEU in 
Machine Translation Research. Proceedings of the 
European Chapter of the Association for 
Computational Linguistics 2006: 249-256. Oslo, 
Norway. 
Michael J. Collins. 1999. Head-driven Statistical 
Models for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania, Philadelphia. 
George Doddington. 2002. Automatic Evaluation of MT 
Quality using N-gram Co-occurrence Statistics. 
Proceedings of Human Language Technology 
Conference 2002: 138-145. San Diego, California. 
Kaplan, R. M., and J. Bresnan. 1982. Lexical-functional 
Grammar: A Formal System for Grammatical 
110
Representation.  In J. Bresnan (ed.), The Mental 
Representation of Grammatical Relations.  MIT 
Press, Cambridge. 
David Kauchak and Regina Barzilay. 2006. 
Paraphrasing for Automatic Evaluation. Proceedings 
of Human Language Technology ? North American 
Chapter of the Association for Computational 
Linguistics Conference 2006: 45-462. New York, 
New York. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. Proceedings of the Workshop on Machine 
Translation: From real users to research at the 
Association for Machine Translation in the Americas 
Conference 2004: 115-124. Washington, DC. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. Proceedings of MT 
Summit 2005: 79-86. Phuket, Thailand. 
Alex Kulesza and Stuart M. Shieber. 2004. A learning 
approach to improving sentence-level MT evaluation. 
In Proceedings of the Conference on Theoretical and 
Methodological Issues in Machine Translation 2004: 
75-84. Baltimore, Maryland. 
Gregor Leusch, Nicola Ueffing and Hermann Ney. 
2006. CDER: Efficient MT Evaluation Using Block 
Movements. Proceedings of European Chapter of the 
Association for Computational Linguistics 
Conference 2006: 241-248. Trento, Italy. 
Ding Liu and Daniel Gildea. 2005. Syntactic Features 
for Evaluation of Machine Translation. In 
Proceedings of the Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine 
Translation and/or Summarization at the Association 
for Computational Linguistics Conference 2005. Ann 
Arbor, Michigan. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Modes. 
Computational Linguistics, 29:19-51. 
Karolina Owczarzak, Declan Groves, Josef van 
Genabith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation. 
Proceedings of the Workshop on Statistical Machine 
Translation at the Human Language Technology ? 
North American Chapter of the Association for 
Computational Linguistics Conference 2006: 86-93. 
New York, New York. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
WeiJing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
Association for Computational Linguistics 
Conference 2002: 311-318. Philadelphia, 
Pennsylvania. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-based Approach to Machine 
Translation Evaluation. Technical Report LAMP-TR-
125/CS-TR-4754/UMIACS-TR-2005-57, University 
of Maryland, College Park, Maryland. 
Mathew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciula. 2006. A Study of 
Translation Error Rate with Targeted Human 
Annotation. Proceedings of the Association for 
Machine Translation in the Americas Conference 
2006: 223-231. Boston, Massachusetts. 
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine Translation and Its 
Evaluation. Proceedings of MT Summit 2003: 386-
393. New Orleans, Luisiana. 
Ying Zhang and Stephan Vogel. 2004. Measuring 
confidence intervals for the machine translation 
evaluation metrics. Proceedings of Conference on 
Theoretical and Methodological Issues in Machine 
Translation 2004: 85-94. Baltimore, Maryland. 
111
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 190?198,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
DEPEVAL(summ): Dependency-based Evaluation for Automatic
Summaries
Karolina Owczarzak
Information Access Division
National Institute of Standards and Technology
Gaithersburg, MD 20899
karolina.owczarzak@nist.gov
Abstract
This paper presents DEPEVAL(summ),
a dependency-based metric for automatic
evaluation of summaries. Using a rerank-
ing parser and a Lexical-Functional Gram-
mar (LFG) annotation, we produce a
set of dependency triples for each sum-
mary. The dependency set for each
candidate summary is then automatically
compared against dependencies generated
from model summaries. We examine a
number of variations of the method, in-
cluding the addition of WordNet, par-
tial matching, or removing relation la-
bels from the dependencies. In a test
on TAC 2008 and DUC 2007 data, DE-
PEVAL(summ) achieves comparable or
higher correlations with human judg-
ments than the popular evaluation metrics
ROUGE and Basic Elements (BE).
1 Introduction
Evaluation is a crucial component in the area of
automatic summarization; it is used both to rank
multiple participant systems in shared summariza-
tion tasks, such as the Summarization track at Text
Analysis Conference (TAC) 2008 and its Docu-
ment Understanding Conference (DUC) predeces-
sors, and to provide feedback to developers whose
goal is to improve their summarization systems.
However, manual evaluation of a large number
of documents necessary for a relatively unbiased
view is often unfeasible, especially in the contexts
where repeated evaluations are needed. Therefore,
there is a great need for reliable automatic metrics
that can perform evaluation in a fast and consistent
manner.
In this paper, we explore one such evaluation
metric, DEPEVAL(summ), based on the compar-
ison of Lexical-Functional Grammar (LFG) de-
pendencies between a candidate summary and
one or more model (reference) summaries. The
method is similar in nature to Basic Elements
(Hovy et al, 2005), in that it extends beyond
a simple string comparison of word sequences,
reaching instead to a deeper linguistic analysis
of the text. Both methods use hand-written ex-
traction rules to derive dependencies from con-
stituent parses produced by widely available Penn
II Treebank parsers. The difference between
DEPEVAL(summ) and BE is that in DEPE-
VAL(summ) the dependency extraction is accom-
plished through an LFG annotation of Cahill et
al. (2004) applied to the output of the reranking
parser of Charniak and Johnson (2005), whereas
in BE (in the version presented here) dependen-
cies are generated by the Minipar parser (Lin,
1995). Despite relying on a the same concept, our
approach outperforms BE in most comparisons,
and it often achieves higher correlations with hu-
man judgments than the string-matching metric
ROUGE (Lin, 2004).
A more detailed description of BE and ROUGE
is presented in Section 2, which also gives an ac-
count of manual evaluation methods employed at
TAC 2008. Section 3 gives a short introduction to
the LFG annotation. Section 4 describes in more
detail DEPEVAL(summ) and its variants. Sec-
tion 5 presents the experiment in which we com-
pared the perfomance of all three metrics on the
TAC 2008 data (consisting of 5,952 100-words
summaries) and on the DUC 2007 data (1,620
250-word summaries) and discusses the correla-
tions these metrics achieve. Finally, Section 6
presents conclusions and some directions for fu-
ture work.
2 Current practice in summary
evaluation
In the first Text Analysis Conference (TAC 2008),
as well as its predecessor, the Document Under-
standing Conference (DUC) series, the evaluation
190
of summarization tasks was conducted using both
manual and automatic methods. Since manual
evaluation is still the undisputed gold standard,
both at TAC and DUC there was much effort to
evaluate manually as much data as possible.
2.1 Manual evaluation
Manual assessment, performed by human judges,
usually centers around two main aspects of sum-
mary quality: content and form. Similarly to Ma-
chine Translation, where these two aspects are rep-
resented by the categories of Accuracy and Flu-
ency, in automatic summarization evaluation per-
formed at TAC and DUC they surface as (Content)
Responsiveness and Readability. In TAC 2008
(Dang and Owczarzak, 2008), however, Content
Responsiveness was replaced by Overall Respon-
siveness, conflating these two dimensions and re-
flecting the overall quality of the summary: the
degree to which a summary was responding to
the information need contained in the topic state-
ment, as well as its linguistic quality. A sepa-
rate Readability score was still provided, assess-
ing the fluency and structure independently of con-
tent, based on such aspects as grammaticality, non-
redundancy, referential clarity, focus, structure,
and coherence. Both Overall Responsiveness and
Readability were evaluated according to a five-
point scale, ranging from ?Very Poor? to ?Very
Good?.
Content was evaluated manually by NIST asses-
sors using the Pyramid framework (Passonneau et
al., 2005). In the Pyramid evaluation, assessors
first extract all possible ?information nuggets?, or
Summary Content Units (SCUs) from the four
human-crafted model summaries on a given topic.
Each SCU is assigned a weight in proportion to the
number of model summaries in which it appears,
on the assumption that information which appears
in most or all human-produced model summaries
is more essential to the topic. Once all SCUs are
harvested from the model summaries, assessors
determine how many of these SCUs are present
in each of the automatic peer summaries. The
final score for an automatic summary is its total
SCUweight divided by the maximum SCUweight
available to a summary of average length (where
the average length is determined by the mean SCU
count of the model summaries for this topic).
All types of manual assessment are expensive
and time-consuming, which is why it can be rarely
provided for all submitted runs in shared tasks
such as the TAC Summarization track. It is also
not a viable tool for system developers who ide-
ally would like a fast, reliable, and above all au-
tomatic evaluation method that can be used to im-
prove their systems. The creation and testing of
automatic evaluation methods is, therefore, an im-
portant research venue, and the goal is to produce
automatic metrics that will correlate with manual
assessment as closely as possible.
2.2 Automatic evaluation
Automatic metrics, because of their relative speed,
can be applied more widely than manual evalua-
tion. In TAC 2008 Summarization track, all sub-
mitted runs were scored with the ROUGE (Lin,
2004) and Basic Elements (BE) metrics (Hovy et
al., 2005).
ROUGE is a collection of string-comparison
techniques, based on matching n-grams between
a candidate string and a reference string. The
string in question might be a single sentence (as
in the case of translation), or a set of sentences
(as in the case of summaries). The variations of
ROUGE range from matching unigrams (i.e. sin-
gle words) to matching four-grams, with or with-
out lemmatization and stopwords, with the options
of using different weights or skip-n-grams (i.e.
matching n-grams despite intervening words). The
two versions used in TAC 2008 evaluations were
ROUGE-2 and ROUGE-SU4, where ROUGE-2
calculates the proportion of matching bigrams be-
tween the candidate summary and the reference
summaries, and ROUGE-SU4 is a combination of
unigram match and skip-bigram match with skip
distance of 4 words.
BE, on the other hand, employs a certain de-
gree of linguistic analysis in the assessment pro-
cess, as it rests on comparing the ?Basic Elements?
between the candidate and the reference. Basic El-
ements are syntactic in nature, and comprise the
heads of major syntactic constituents in the text
(noun, verb, adjective, etc.) and their modifiers
in a dependency relation, expressed as a triple
(head, modifier, relation type). First, the input text
is parsed with a syntactic parser, then Basic Ele-
ments are extracted from the resulting parse, and
the candidate BEs are matched against the refer-
ence BEs. In TAC 2008 and DUC 2008 evalua-
tions the BEs were extracted with Minipar (Lin,
1995). Since BE, contrary to ROUGE, does not
191
rely solely on the surface sequence of words to de-
termine similarity between summaries, but delves
into what could be called a shallow semantic struc-
ture, comprising thematic roles such as subject and
object, it is likely to notice identity of meaning
where such identity is obscured by variations in
word order. In fact, when it comes to evaluation
of automatic summaries, BE shows higher corre-
lations with human judgments than ROUGE, al-
though the difference is not large enough to be
statistically significant. In the TAC 2008 evalua-
tions, BE-HM (a version of BE where the words
are stemmed and the relation type is ignored) ob-
tained a correlation of 0.911 with human assess-
ment of overall responsiveness and 0.949 with the
Pyramid score, whereas ROUGE-2 showed corre-
lations of 0.894 and 0.946, respectively.
While using dependency information is an im-
portant step towards integrating linguistic knowl-
edge into the evaluation process, there are many
ways in which this could be approached. Since
this type of evaluation processes information in
stages (constituent parser, dependency extraction,
and the method of dependency matching between
a candidate and a reference), there is potential
for variance in performance among dependency-
based evaluation metrics that use different com-
ponents. Therefore, it is interesting to compare
our method, which relies on the Charniak-Johnson
parser and the LFG annotation, with BE, which
uses Minipar to parse the input and produce de-
pendencies.
3 Lexical-Functional Grammar and the
LFG parser
The method discussed in this paper rests on the
assumptions of Lexical-Functional Grammar (Ka-
plan and Bresnan, 1982; Bresnan, 2001) (LFG). In
LFG sentence structure is represented in terms of
c(onstituent)-structure and f(unctional)-structure.
C-structure represents the word order of the sur-
face string and the hierarchical organisation of
phrases in terms of trees. F-structures are re-
cursive feature structures, representing abstract
grammatical relations such as subject, object,
oblique, adjunct, etc., approximating to predicate-
argument structure or simple logical forms. C-
structure and f-structure are related by means of
functional annotations in c-structure trees, which
describe f-structures.
While c-structure is sensitive to surface rear-
rangement of constituents, f-structure abstracts
away from (some of) the particulars of surface re-
alization. The sentences John resigned yesterday
and Yesterday, John resigned will receive differ-
ent tree representations, but identical f-structures.
The f-structure can also be described in terms of a
flat set of triples, or dependencies. In triples for-
mat, the f-structure for these two sentences is rep-
resented in 1.
(1)
subject(resign,john)
person(john,3)
number(john,sg)
tense(resign,past)
adjunct(resign,yesterday)
person(yesterday,3)
number(yesterday,sg)
Cahill et al (2004), in their presentation of
LFG parsing resources, distinguish 32 types of
dependencies, divided into two major groups: a
group of predicate-only dependencies and non-
predicate dependencies. Predicate-only dependen-
cies are those whose path ends in a predicate-
value pair, describing grammatical relations. For
instance, in the sentence John resigned yester-
day, predicate-only dependencies would include:
subject(resign, john) and adjunct(resign, yester-
day), while non-predicate dependencies are per-
son(john,3), number(john,sg), tense(resign,past),
person(yesterday,3), num(yesterday,sg). Other
predicate-only dependencies include: apposition,
complement, open complement, coordination, de-
terminer, object, second object, oblique, second
oblique, oblique agent, possessive, quantifier, rel-
ative clause, topic, and relative clause pronoun.
The remaining non-predicate dependencies are:
adjectival degree, coordination surface form, fo-
cus, complementizer forms: if, whether, and that,
modal, verbal particle, participle, passive, pro-
noun surface form, and infinitival clause.
These 32 dependencies, produced by LFG an-
notation, and the overlap between the set of de-
pendencies derived from the candidate summary
and the reference summaries, form the basis of our
evaluation method, which we present in Section 4.
First, a summary is parsed with the Charniak-
Johnson reranking parser (Charniak and Johnson,
2005) to obtain the phrase-structure tree. Then,
a sequence of scripts annotates the output, trans-
lating the relative phrase position into f-structural
dependencies. The treebank-based LFG annota-
tion used in this paper and developed by Cahill et
al. (2004) obtains high precision and recall rates.
As reported in Cahill et al (2008), the version of
192
the LFG parser which applies the LFG annotation
algorithm to the earlier Charniak?s parser (Char-
niak, 2000) obtains an f-score of 86.97 on the Wall
Street Journal Section 23 test set. The LFG parser
is robust as well, with coverage levels exceeding
99.9%, measured in terms of complete spanning
parse.
4 Dependency-based evaluation
Our dependency-based evaluation method, simi-
larly to BE, compares two unordered sets of de-
pendencies: one bag contains dependencies har-
vested from the candidate summary and the other
contains dependencies from one or more reference
summaries. Overlap between the candidate bag
and the reference bag is calculated in the form
of precision, recall, and the f-measure (with pre-
cision and recall equally weighted). Since for
ROUGE and BE the only reported score is recall,
we present recall results here as well, calculated as
in 2:
(2) DEPEVAL(summ) Recall = |Dcand|?|Dref ||Dref |
where Dcand are the candidate dependencies
and Dref are the reference dependencies.
The dependency-based method using LFG an-
notation has been successfully employed in the
evaluation of Machine Translation (MT). In
Owczarzak (2008), the method achieves equal or
higher correlations with human judgments than
METEOR (Banerjee and Lavie, 2005), one of the
best-performing automatic MT evaluation metrics.
However, it is not clear that the method can be ap-
plied without change to the task of assessing au-
tomatic summaries; after all, the two tasks - of
summarization and translation - produce outputs
that are different in nature. In MT, the unit of
text is a sentence; text is translated, and the trans-
lation evaluated, sentence by sentence. In auto-
matic summarization, the output unit is a sum-
mary with length varying depending on task, but
which most often consists of at least several sen-
tences. This has bearing on the matching pro-
cess: with several sentences on the candidate and
reference side each, there is increased possibility
of trivial matches, such as dependencies contain-
ing function words, which might inflate the sum-
mary score even in the absence of important con-
tent. This is particularly likely if we were to em-
ploy partial matching for dependencies. Partial
matching (indicated in the result tables with the
tag pm) ?splits? each predicate dependency into
two, replacing one or the other element with a
variable, e.g. for the dependency subject(resign,
John) we would obtain two partial dependencies
subject(resign, x) and subject(x, John). This pro-
cess helps circumvent some of the syntactic and
lexical variation between a candidate and a refer-
ence, and it proved very useful in MT evaluation
(Owczarzak, 2008). In summary evaluation, as
will be shown in Section 5, it leads to higher cor-
relations with human judgments only in the case
of human-produced model summaries, because al-
most any variation between two model summaries
is ?legal?, i.e. either a paraphrase or another, but
equally relevant, piece of information. For au-
tomatic summaries, which are of relatively poor
quality, partial matching lowers our method?s abil-
ity to reflect human judgment, because it results in
overly generous matching in situations where the
examined information is neither a paraphrase nor
relevant.
Similarly, evaluating a summary against the
union of all references, as we do in the base-
line version of our method, increases the pool
of possible matches, but may also produce score
inflation through matching repetitive information
across models. To deal with this, we produce a
version of the score (marked in the result tables
with the tag one) that counts only one ?hit? for ev-
ery dependency match, independent of how many
instances of a given dependency are present in the
comparison.
The use of WordNet1 module (Rennie, 2000)
did not provide a great advantage (see results
tagged with wn), and sometimes even lowered our
correlations, especially in evaluation of automatic
systems. This makes sense if we take into consid-
eration that WordNet lists all possible synonyms
for all possible senses of a word, and so, given
a great number of cross-sentence comparisons in
multi-sentence summaries, there is an increased
risk of spurious matches between words which,
despite being potentially synonymous in certain
contexts, are not equivalent in the text.
Another area of concern was the potential noise
introduced by the parser and the annotation pro-
cess. Due to parsing errors, two otherwise equiv-
alent expressions might be encoded as differ-
ing sets of dependencies. In MT evaluation,
the dependency-based method can alleviate parser
1http://wordnet.princeton.edu/
193
noise by comparing n-best parses for the candidate
and the reference (Owczarzak et al, 2007), but this
is not an efficient solution for comparing multi-
sentence summaries. We have therefore attempted
to at least partially counteract this issue by remov-
ing relation labels from the dependencies (i.e. pro-
ducing dependencies of the form (resign, John) in-
stead of subject(resign, John)), which did provide
some improvement (see results tagged with norel).
Finally, we experimented with a predicate-only
version of the evaluation, where only the predi-
cate dependencies participate in the comparison,
excluding dependencies that provide purely gram-
matical information such as person, tense, or num-
ber (tagged in the results table as pred). This
move proved beneficial only in the case of system
summaries, perhaps by decreasing the number of
trivial matches, but decreased the method?s corre-
lation for model summaries, where such detailed
information might be necessary to assess the de-
gree of similarity between two human summaries.
5 Experimental results
The first question we have to ask is: which of
the manual evaluation categories do we want our
metric to imitate? It is unlikely that a single au-
tomatic measure will be able to correctly reflect
both Readability and Content Responsiveness, as
form and content are separate qualities and need
different measures. Content seems to be the more
important aspect, especially given that Readabil-
ity can be partially derived from Responsiveness
(a summary high in content cannot be very low
in readability, although some very readable sum-
maries can have little relevant content). Content
Responsiveness was provided in DUC 2007 data,
but not in TAC 2008, where the extrinsic Pyra-
mid measure was used to evaluate content. It is,
in fact, preferable to compare our metric against
the Pyramid score rather than Content Responsive-
ness, because both the Pyramid and our method
aim to measure the degree of similarity between
a candidate and a model, whereas Content Re-
sponsiveness is a direct assessment of whether the
summary?s content is adequate given a topic and
a source text. The Pyramid is, at the same time,
a costly manual evaluation method, so an auto-
matic metric that successfully emulates it would
be a useful replacement.
Another question is whether we focus on
system-level or summary-level evaluation. The
correlation values at the summary-level are gener-
ally much lower than on the system-level, which
means the metrics are better at evaluating sys-
tem performance than the quality of individual
summaries. System-level evaluations are essen-
tial to shared summarization tasks; summary-level
assessment might be useful to developers who
want to test the effect of particular improvements
in their system. Of course, the ideal evaluation
metric would show high correlations with human
judgment on both levels.
We used the data from the TAC 2008 and
DUC 2007 Summarization tracks. The first set
comprised 58 system submissions and 4 human-
produced model summaries for each of the 96 sub-
topics (there were 48 topics, each of which re-
quired two summaries: a main and an update sum-
mary), as well as human-produced Overall Re-
sponsiveness and Pyramid scores for each sum-
mary. The second set included 32 system submis-
sions and 4 human models for each of the 45 top-
ics. For fair comparison of models and systems,
we used jackknifing: while each model was evalu-
ated against the remaining three models, each sys-
tem summary was evaluated four times, each time
against a different set of three models, and the four
scores were averaged.
5.1 System-level correlations
Table 1 presents system-level Pearson?s cor-
relations between the scores provided by our
dependency-based metric DEPEVAL(summ),
as well as the automatic metrics ROUGE-2,
ROUGE-SU4, and BE-HM used in the TAC
evaluation, and the manual Pyramid scores, which
measured the content quality of the systems.
It also includes correlations with the manual
Overall Responsiveness score, which reflected
both content and linguistic quality. Table 3 shows
the correlations with Content Responsiveness
for DUC 2007 data for ROUGE, BE, and those
few select versions of DEPEVAL(summ) which
achieve optimal results on TAC 2008 data (for
a more detailed discussion of the selection see
Section 6).
The correlations are listed for the following ver-
sions of our method: pm - partial matching for
dependencies; wn - WordNet; pred - matching
predicate-only dependencies; norel - ignoring de-
pendency relation label; one - counting a match
only once irrespective of how many instances of
194
TAC 2008 Pyramid Overall Responsiveness
Metric models systems models systems
DEPEVAL(summ): Variations
base 0.653 0.931 0.883 0.862
pm 0.690 0.811 0.943 0.740
wn 0.687 0.929 0.888 0.860
pred 0.415 0.946 0.706 0.909
norel 0.676 0.929 0.880 0.861
one 0.585 0.958* 0.858 0.900
DEPEVAL(summ): Combinations
pm wn 0.694 0.903 0.952* 0.839
pm pred 0.534 0.880 0.898 0.831
pm norel 0.722 0.907 0.936 0.835
pm one 0.611 0.950 0.876 0.895
wn pred 0.374 0.946 0.716 0.912
wn norel 0.405 0.941 0.752 0.905
wn one 0.611 0.952 0.856 0.897
pred norel 0.415 0.945 0.735 0.905
pred one 0.415 0.953 0.721 0.921*
norel one 0.600 0.958* 0.863 0.900
pm wn pred 0.527 0.870 0.905 0.821
pm wn norel 0.738 0.897 0.931 0.826
pm wn one 0.634 0.936 0.887 0.881
pm pred norel 0.642 0.876 0.946 0.815
pm pred one 0.504 0.948 0.817 0.907
pm norel one 0.725 0.941 0.905 0.880
wn pred norel 0.433 0.944 0.764 0.906
wn pred one 0.385 0.950 0.722 0.919
wn norel one 0.632 0.954 0.872 0.896
pred norel one 0.452 0.955 0.756 0.919
pm wn pred norel 0.643 0.861 0.940 0.800
pm wn pred one 0.486 0.932 0.809 0.890
pm pred norel one 0.711 0.939 0.881 0.891
pm wn norel one 0.743* 0.930 0.902 0.870
wn pred norel one 0.467 0.950 0.767 0.918
pm wn pred norel one 0.712 0.927 0.887 0.880
Other metrics
ROUGE-2 0.277 0.946 0.725 0.894
ROUGE-SU4 0.457 0.928 0.866 0.874
BE-HM 0.423 0.949 0.656 0.911
Table 1: System-level Pearson?s correlation between auto-
matic and manual evaluation metrics for TAC 2008 data.
a particular dependency are present in the candi-
date and reference. For each of the metrics, in-
cluding ROUGE and BE, we present the correla-
tions for recall. The highest result in each category
is marked by an asterisk. The background gradi-
ent indicates whether DEPEVAL(summ) correla-
tion is higher than all three competitors ROUGE-
2, ROUGE-SU4, and BE (darkest grey), two of the
three (medium grey), one of the three (light grey),
or none (white). The 95% confidence intervals are
not included here for reasons of space, but their
comparison suggests that none of the system-level
differences in correlation levels are large enough
to be significant. This is because the intervals
themselves are very wide, due to relatively small
number of summarizers (58 automatic and 8 hu-
man for TAC; 32 automatic and 10 human for
DUC) involved in the comparison.
5.2 Summary-level correlations
Tables 2 and 4 present the same correlations,
but this time on the level of individual sum-
maries. As before, the highest level in each
category is marked by an asterisk. Contrary to
system-level, here some correlations obtained by
DEPEVAL(summ) are significantly higher than
those achieved by the three competing metrics,
ROUGE-2, ROUGE-SU4, and BE-HM, as de-
termined by the confidence intervals. The let-
ters in parenthesis indicate that a given DEPE-
VAL(summ) variant is significantly better at cor-
relating with human judgment than ROUGE-2 (=
R2), ROUGE-SU4 (= R4), or BE-HM (= B).
6 Discussion and future work
It is obvious that none of the versions performs
best across the board; their different character-
istics might render them better suited either for
models or for automatic systems, but not for
both at the same time. This can be explained if
we understand that evaluating human gold stan-
dard summaries and automatically generated sum-
maries of poor-to-medium quality is, in a way, not
the same task. Given that human models are by
default well-formed and relevant, relaxing any re-
straints on matching between them (i.e. allowing
partial dependencies, removing the relation label,
or adding synonyms) serves, in effect, to accept as
correct either (1) the same conceptual information
expressed in different ways (where the difference
might be real or introduced by faulty parsing),
or (2) other information, yet still relevant to the
topic. Accepting information of the former type
as correct will ratchet up the score for the sum-
mary and the correlation with the summary?s Pyra-
mid score, which measures identity of information
across summaries. Accepting the first and second
type of information will raise the score and the
correlation with Responsiveness, which measures
relevance of information to the particular topic.
However, in evaluating system summaries such re-
laxation of matching constraints will result in ac-
cepting irrelevant and ungrammatical information
as correct, driving up the DEPEVAL(summ) score,
but lowering its correlation with both Pyramid and
Responsiveness. In simple words, it is okay to give
a model summary ?the benefit of doubt?, and ac-
cept its content as correct even if it is not match-
ing other model summaries exactly, but the same
strategy applied to a system summary might cause
mass over-estimation of the summary?s quality.
This substantial difference in the nature of
human-generated models and system-produced
summaries has impact on all automatic means of
evaluation, as long as we are limited to methods
that operate on more shallow levels than a full
195
TAC 2008 Pyramid Overall Responsiveness
Metric models systems models systems
DEPEVAL(summ): Variations
base 0.436 (B) 0.595 (R2,R4,B) 0.186 0.373 (R2,B)
pm 0.467 (B) 0.584 (R2,B) 0.183 0.368 (B)
wn 0.448 (B) 0.592 (R2,B) 0.192 0.376 (R2,R4,B)
pred 0.344 0.543 (B) 0.170 0.327
norel 0.437 (B) 0.596* (R2,R4,B) 0.186 0.373 (R2,B)
one 0.396 0.587 (R2,B) 0.171 0.376 (R2,R4,B)
DEPEVAL(summ): Combinations
pm wn 0.474 (B) 0.577 (R2,B) 0.194* 0.371 (R2,B)
pm pred 0.407 0.537 (B) 0.153 0.337
pm norel 0.483 (R2,B) 0.584 (R2,B) 0.168 0.362
pm one 0.402 0.577 (R2,B) 0.167 0.384 (R2,R4,B)
wn pred 0.352 0.537 (B) 0.182 0.328
wn norel 0.364 0.541 (B) 0.187 0.329
wn one 0.411 0.581 (R2,B) 0.182 0.384 (R2,R4,B)
pred norel 0.351 0.547 (B) 0.169 0.327
pred one 0.325 0.542 (B) 0.171 0.347
norel one 0.403 0.589 (R2,B) 0.176 0.377 (R2,R4,B)
pm wn pred 0.415 0.526 (B) 0.167 0.337
pm wn norel 0.488* (R2,R4,B) 0.576 (R2,B) 0.168 0.366 (B)
pm wn one 0.417 0.563 (B) 0.179 0.389* (R2,R4.B)
pm pred norel 0.433 (B) 0.538 (B) 0.124 0.333
pm pred one 0.357 0.545 (B) 0.151 0.381 (R2,R4,B)
pm norel one 0.437 (B) 0.567 (R2,B) 0.174 0.369 (B)
wn pred norel 0.353 0.541 (B) 0.180 0.324
wn pred one 0.328 0.535 (B) 0.179 0.346
wn norel one 0.416 0.584 (R2,B) 0.185 0.385 (R2,R4,B)
pred norel one 0.336 0.549 (B) 0.169 0.351
pm wn pred norel 0.428 (B) 0.524 (B) 0.120 0.334
pm wn pred one 0.363 0.525 (B) 0.164 0.380 (R2,R4,B)
pm pred norel one 0.420 (B) 0.533 (B) 0.154 0.375 (R2,R4,B)
pm wn norel one 0.452 (B) 0.558 (B) 0.179 0.376 (R2,R4,B)
wn pred norel one 0.338 0.544 (B) 0.178 0.349
pm wn pred norel one 0.427 (B) 0.522 (B) 0.153 0.379 (R2,R4,B)
Other metrics
ROUGE-2 0.307 0.527 0.098 0.323
ROUGE-SU4 0.318 0.557 0.153 0.327
BE-HM 0.239 0.456 0.135 0.317
Table 2: Summary-level Pearson?s correlation between automatic and manual
evaluation metrics for TAC 2008 data.
DUC 2007 Content Responsiveness
Metric models systems
DEPEVAL(summ) 0.7341 0.8429
DEPEVAL(summ) wn 0.7355 0.8354
DEPEVAL(summ) norel 0.7394 0.8277
DEPEVAL(summ) one 0.7507 0.8634
ROUGE-2 0.4077 0.8772
ROUGE-SU4 0.2533 0.8297
BE-HM 0.5471 0.8608
Table 3: System-level Pearson?s correlation
between automatic metrics and Content Respon-
siveness for DUC 2007 data. For model sum-
maries, only DEPEVAL correlations are signif-
icant (the 95% confidence interval does not in-
clude zero). None of the differences between
metrics are significant at the 95% level.
DUC 2007 Content Responsiveness
Metric models systems
DEPEVAL(summ) 0.2059 0.4150
DEPEVAL(summ) wn 0.2081 0.4178
DEPEVAL(summ) norel 0.2119 0.4185
DEPEVAL(summ) one 0.1999 0.4101
ROUGE-2 0.1501 0.3875
ROUGE-SU4 0.1397 0.4264
BE-HM 0.1330 0.3722
Table 4: Summary-level Pearson?s correlation
between automatic metrics and Content Respon-
siveness for DUC 2007 data. ROUGE-SU4 and
BE correlations for model summaries are not
statistically significant. None of the differences
between metrics are significant at the 95% level.
semantic and pragmatic analysis against human-
level world knowledge. The problem is twofold:
first, our automatic metrics measure identity rather
than quality. Similarity of content between a can-
didate summary and one or more references is act-
ing as a proxy measure for the quality of the can-
didate summary; yet, we cannot forget that the re-
lation between these two features is not purely lin-
ear. A candidate highly similar to the reference
will be, necessarily, of good quality, but a candi-
date which is dissimilar from a reference is not
necessarily of low quality (vide the case of par-
allel model summaries, which almost always con-
tain some non-overlapping information).
The second problem is the extent to which our
metrics are able to distinguish content through
the veil of differing forms. Synonyms, para-
phrases, or pragmatic features such as the choice
of topic and focus render simple string-matching
techniques ineffective, especially in the area of
summarization where the evaluation happens on
a supra-sentential level. As a result, then, a lot
of effort was put into developing metrics that
can identify similar content despite non-similar
form, which naturally led to the application of
linguistically-oriented approaches that look be-
yond surface word order.
Essentially, though, we are using imperfect
measures of similarity as an imperfect stand-in for
quality, and the accumulated noise often causes
a divergence in our metrics? performance with
model and system summaries. Much like the in-
verse relation of precision and recall, changes and
additions that improve a metric?s correlation with
human scores for model summaries often weaken
the correlation for system summaries, and vice
versa. Admittedly, we could just ignore this prob-
lem and focus on increasing correlations for auto-
matic summaries only; after all, the whole point
of creating evaluation metrics is to score and rank
the output of systems. Such a perspective can be
rather short-sighted, though, given that we expect
continuous improvement from the summarization
systems to, ideally, human levels, so the same is-
sues which now prevent high correlations for mod-
els will start surfacing in evaluation of system-
produced summaries as well. Using metrics that
only perform reliably for low-quality summaries
might prevent us from noticing when those sum-
maries become better. Our goal should be, there-
fore, to develop a metric which obtains high cor-
relations in both categories, with the assumption
that such a metric will be more reliable in evaluat-
ing summaries of varying quality.
196
Since there is no single winner among all 32
variants of DEPEVAL(summ) on TAC 2008 data,
we must decide which of the categories is most im-
portant to a successful automatic evaluation met-
ric. Correlations with Overall Responsiveness are
in general lower than those with the Pyramid score
(except in the case of system-level models). This
makes sense, if we rememeber that Overall Re-
sponsiveness judges content as well as linguistic
quality, which are two different dimensions and so
a single automatic metric is unlikely to reflect it
well, and that it judges content in terms of its rel-
evance to topic, which is also beyond the reach
of contemporary metrics which can at most judge
content similarity to a model. This means that the
Pyramid score makes for a more relevant metric to
emulate.
The last dilemma is whether we choose to focus
on system- or summary-level correlations. This
ties in with the purpose which the evaluation met-
ric should serve. In comparisons of multiple sys-
tems, such as in TAC 2008, the value is placed
in the correct ordering of these systems; while
summary-level assessment can give us important
feedback and insight during the system develop-
ment stage.
The final choice among all DEPEVAL(summ)
versions hinges on all of these factors: we should
prefer a variant which correlates highly with the
Pyramid score rather than with Responsiveness,
which minimizes the gap between model and au-
tomatic peer correlations while retaining relatively
high values for both, and which fulfills these re-
quirements similarly well on both summary- and
system-levels. Three such variants are the base-
line DEPEVAL(summ), the WordNet version DE-
PEVAL(summ) wn, and the version with removed
relation labels DEPEVAL(summ) norel. Both the
baseline and norel versions achieve significant im-
provement over ROUGE and BE in correlations
with the Pyramid score for automatic summaries,
and over BE for models, on the summary level. In
fact, almost in all categories they achieve higher
correlations than ROUGE and BE. The only ex-
ceptions are the correlations with Pyramid for sys-
tems at the system-level, but there the results are
close and none of the differences in that category
are significant. To balance this exception, DE-
PEVAL(summ) achieves much higher correlations
with the Pyramid scores for model summaries than
either ROUGE or BE on the system level.
In order to see whether the DEPEVAL(summ)
advantage holds for other data, we examined the
most optimal versions (baseline, wn, norel, as
well as one, which is the closest counterpart
to label-free BE-HM) on data from DUC 2007.
Because only a portion of the DUC 2007 data
was evaluated with Pyramid, we chose to look
rather at the Content Responsiveness scores. As
can be seen in Tables 3 and 4, the same pat-
terns hold: decided advantage over ROUGE/BE
when it comes to model summaries (especially
at system-level), comparable results for automatic
summaries. Since DUC 2007 data consisted of
fewer summaries (1,620 vs 5,952 at TAC) and
fewer submissions (32 vs 57 at TAC), some results
did not reach statistical significance. In Table 3, in
the models category, only DEPEVAL(summ) cor-
relations are significant. In Table 4, in the model
category, only DEPEVAL(summ) and ROUGE-2
correlations are significant. Note also that these
correlations with Content Responsiveness are gen-
erally lower than those with Pyramid in previous
tables, but in the case of summary-level compari-
son higher than the correlations with Overall Re-
sponsiveness. This is to be expected given our
earlier discussion of the differences in what these
metrics measure.
As mentioned before, the dependency-based
evaluation can be approached from different an-
gles, leading to differences in performance. This
is exemplified in our experiment, where DEPE-
VAL(summ) outperforms BE, even though both
these metrics rest on the same general idea. The
new implementation of BE presented at the TAC
2008 workshop (Tratz and Hovy, 2008) introduces
transformations for dependencies in order to in-
crease the number of matches among elements that
are semantically similar yet differ in terms of syn-
tactic structure and/or lexical choices, and adds
WordNet for synonym matching. Its core modules
were updated as well: Minipar was replaced with
the Charniak-Johnson reranking parser (Charniak
and Johnson, 2005), Named Entity identification
was added, and the BE extraction is conducted us-
ing a set of Tregex rules (Levy and Andrew, 2006).
Since our method, presented in this paper, also
uses the reranking parser, as well as WordNet, it
would be interesting to compare both methods di-
rectly in terms of the performance of the depen-
dency extraction procedure.
197
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL 2005 Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Sum-
marization, pages 65?73, Ann Arbor, MI, USA.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-
distance dependency resolution in automatically ac-
quired wide-coverage PCFG-based LFG approxima-
tions. In Proceedings of the 42th Annual Meeting
of the Association for Computational Linguistics,
pages 320?327, Barcelona, Spain.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-coverage deep statistical parsing using auto-
matic dependency structure annotation. Comput.
Linguist., 34(1):81?124.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL 2005: Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 173?180, Morristown, NJ,
USA. Association for Computational Linguistics.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the 1st Annual Meeting of
the North American Chapter of the Association for
Computational Linguistics, pages 132?139, Seattle,
WA, USA.
Hoa Trang Dang and Karolina Owczarzak. 2008.
Overview of the tac 2008 summarization track: Up-
date task. In to appear in: Proceedings of the 1st
Text Analysis Conference (TAC).
Eduard Hovy, Chin-Yew Lin, and Liang Zhou. 2005.
Evaluating DUC 2005 using Basic Elements. In
Proceedings of the 5th Document Understanding
Conference (DUC).
Ronald M. Kaplan and Joan Bresnan, 1982. The Men-
tal Representation of Grammatical Relations, chap-
ter Lexical-functional Grammar: A Formal System
for Grammatical Representation. MIT Press, Cam-
bridge, MA, USA.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: Tools for querying and manipulating tree data
structures. In Proceedings of the 5th International
Conference on Language Resources and Evaluation.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of the 14th International Joint Conference on Artifi-
cial Intelligence, pages 1420?1427.
Chin-Yew Lin. 2004. ROUGE: A package for au-
tomatic evaluation of summaries. In Proceedings
of the ACL 2004 Workshop: Text Summarization
Branches Out, pages 74?81.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Evaluating Machine Translation with
LFG dependencies. Machine Translation, 21(2):95?
119.
Karolina Owczarzak. 2008. A novel dependency-
based evaluation metric for Machine Translation.
Ph.D. thesis, Dublin City University.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McK-
eown, and Sergey Sigelman. 2005. Applying
the Pyramid method in DUC 2005. In Proceed-
ings of the 5th Document Understanding Conference
(DUC).
Jason Rennie. 2000. Wordnet::querydata: a
Perl module for accessing the WordNet database.
http://people.csail.mit.edu/ jrennie/WordNet.
Stephen Tratz and Eduard Hovy. 2008. Summariza-
tion evaluation using transformed Basic Elements.
In Proceedings of the 1st Text Analysis Conference
(TAC).
198
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 23?30,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Evaluation of automatic summaries: Metrics under varying data
conditions
Karolina Owczarzak and Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
Gaithersburg, MD 20899
karolina.owczarzak@nist.gov hoa.dang@nist.gov
Abstract
In evaluation of automatic summaries, it
is necessary to employ multiple topics and
human-produced models in order for the
assessment to be stable and reliable. How-
ever, providing multiple topics and models
is costly and time-consuming. This paper
examines the relation between the number
of available models and topics and the cor-
relations with human judgment obtained
by automatic metrics ROUGE and BE, as
well as the manual Pyramid method. Test-
ing all these methods on the same data set,
taken from the TAC 2008 Summarization
track, allows us to compare and contrast
the methods under different conditions.
1 Introduction
Appropriate evaluation of results is an important
aspect of any research. In areas such as automatic
summarization, the problem is especially complex
because of the inherent subjectivity in the task it-
self and its evaluation. There is no single objective
standard for a good quality summary; rather, its
value depends on the summary?s purpose, focus,
and particular requirements of the reader (Sp?arck
Jones, 2007). While the purpose and focus can
be set as constant for a specific task, the variabil-
ity of human judgment is more difficult to con-
trol. Therefore, in attempts to produce stable eval-
uations, it has become standard to use multiple
judges, not necessarily for parallel evaluation, but
in such a way that each judge evaluates a differ-
ent subset of the many summaries on which the
final system assessment is based. The incorpora-
tion of multiple points of view is also reflected in
automatic evaluation, where it takes the form of
employing multiple model summaries to which a
candidate summary is compared.
Since these measures to neutralize judgment
variation involve the production of multiple model
summaries, as well as multiple topics, evaluation
can become quite costly. Therefore, it is inter-
esting to examine how many models and topics
are necessary to obtain a relatively stable eval-
uation, and whether this number is different for
manual and automatic metrics. In their exami-
nation of summary evaluations, van Halteren and
Teufel (2003) suggest that it is necessary to use
at least 30 to 40 model summaries for a stable
evaluation; however, Harman and Over (2004) ar-
gue that a stable evaluation can be conducted even
with a single model, as long as there is an ade-
quate number of topics. This view is supported by
Lin (2004a), who concludes that ?correlations to
human judgments were increased by using multi-
ple references but using single reference summary
with enough number of samples was a valid al-
ternative?. Interestingly, similar conclusions were
also reached in the area of Machine Translation
evaluation; in their experiments, Zhang and Vogel
(2004) show that adding an additional reference
translation compensates the effects of removing
10?15% of the testing data, and state that, there-
fore, ?it seems more cost effective to have more
test sentences but fewer reference translations?.
In this paper, we look at how various metrics
behave with respect to a variable number of top-
ics and models used in the evaluation. This lets us
determine the stability of individual metrics, and
helps to illuminate the trade-offs inherent in de-
signing a good evaluation. For our experiments,
we used data from the Summarization track at the
Text Analysis Conference (TAC) 2008, where par-
ticipating systems were assessed on their summa-
rization of 48 topics, and the automatic metrics
ROUGE and BE, as well as the manual Pyramid
evaluation method, had access to 4 human mod-
els. TAC 2008 was the first task of the TAC/DUC
(Document Understanding Conference) series in
which the Pyramid method was used on all evalu-
ated data, making it possible to conduct a full com-
23
parison among the manual and automatic meth-
ods. Despite the lack of full Pyramid evaluation
in DUC 2007, we look at the remaining metrics
applied that year (ROUGE, BE, and Content Re-
sponsiveness), in order to see whether they con-
firm the insights gained from the TAC 2008 data.
2 Summary evaluation
The main evaluation at TAC 2008 was performed
manually, assessing the automatic candidate sum-
maries with respect to Overall Responsiveness,
Overall Readability, and content coverage accord-
ing to the Pyramid framework (Nenkova and Pas-
sonneau, 2004; Passonneau et al, 2005). Task par-
ticipants were asked to produce two summaries for
each of the 48 topics; the first (initial summary)
was a straightforward summary of 10 documents
in response to a topic statement, which is a request
for information about a subject or event; the sec-
ond was an update summary, generated on the ba-
sis of another set of 10 documents, which followed
the first set in temporal order and described further
developments in the given topic. The idea behind
the update summary was to avoid repeating all the
information included in the first set of documents,
on the assumption that the reader is familiar with
that information already.
The participating teams submitted up to three
runs each; however, only the first and second
runs were evaluated manually due to limited re-
sources. For each summary under evaluation, as-
sessors rated the summary from 1 (very poor) to
5 (very good) in terms of Overall Responsiveness,
which measures how well the summary responds
to the need for information expressed in the topic
statement and whether its linguistic quality is ad-
equate. Linguistic qualities such as grammatical-
ity, coreference, and focus were also evaluated as
Overall Readability, also on the scale from 1 to
5. Content coverage of each summary was evalu-
ated using the Pyramid framework, where asses-
sors create a list of information nuggets (called
Summary Content Units, or SCUs) from the set of
human-produced summaries on a given topic, then
decide whether any of these nuggets are present in
the candidate summary. All submitted runs were
evaluated with the automatic metrics: ROUGE
(Lin, 2004b), which calculates the proportion of
n-grams shared between the candidate summary
and the reference summaries, and Basic Elements
(Hovy et al, 2005), which compares the candidate
to the models in terms of head-modifier pairs.
2.1 Manual metrics
Evaluating Overall Responsiveness and Overall
Readability is a rather straightforward procedure,
as most of the complex work is done in the mind
of the human assessor. Each candidate summary
is given a single score, and the final score for
the summarization system is the average of all its
summary-level scores. The only economic factor
here is the number of topics, i.e. summaries per
system, that need to be judged in order to neutral-
ize both intra- and inter-annotator variability and
obtain a reliable assessment of the summarization
system.
When it comes to the Pyramid method, which
measures content coverage of candidate sum-
maries, the need for multiple topics is accompa-
nied by the need for multiple human model sum-
maries. First, independent human assessors pro-
duce summaries for each topic, guided by the topic
statement. Next, in the Pyramid creation stage,
an assessor reads all human-produced summaries
for a given topic and extracts all ?information
nuggets?, called Summary Content Units (SCUs),
which are short, atomic statements of facts con-
tained in the text. Each SCU has a weight which
is directly proportional to the number of model
summaries in which it appears, on the assumption
that the fact?s importance is reflected in how many
human summarizers decide to include it as rele-
vant in their summary. Once all SCUs have been
harvested from the model summaries, an assessor
then examines each candidate summary to see how
many of the SCUs from the list it contains. The fi-
nal Pyramid score for a candidate summary is its
total SCU weight divided by the maximum SCU
weight available to a summary of average length
(where the average length is determined by the
mean SCU count of the model summaries for this
topic). The final score for a summarization system
is the average score of all its summaries. In TAC
2008, the evaluation was conducted with 48 topics
and 4 human models for each topic.
We examined to what extent the number of
models and topics used in the evaluation can in-
fluence the Pyramid score and its stability. The
stability, similarly to the method employed by
Voorhees and Buckley (2002) for Information Re-
trieval, is determined by how well a system rank-
ing based on a small number of models/topics cor-
24
Models Pyramid ROUGE-2 ROUGE-SU4 BE
1 0.8839 0.8032 0.7842 0.7680
2 0.8943 0.8200 0.7957 0.7983
3 0.8974* 0.8258 0.7999* 0.8098
4 (bootstr) 0.8972* 0.8310 0.8023* 0.8152
4 (actual) 0.8997 0.8302 0.8033 0.8171
Table 1: Mean correlations of Responsiveness and other met-
rics using 1, 2, 3, or 4 models for TAC 2008 initial summaries.
Values in each row are significantly different from each other at
95% level.
Models Pyramid ROUGE-2 ROUGE-SU4 BE
1 0.9315 0.8861 0.8874 0.8716
2 0.9432 0.9013 0.8961 0.8978
3 0.9474* 0.9068* 0.8994 0.9076
4 (bootstr) 0.9481* 0.9079* 0.9023 0.9114
4 (actual) 0.9492 0.9103 0.9020 0.9132
Table 2: Mean correlations of Responsiveness and other met-
rics using 1, 2, 3, or 4 models for TAC 2008 update summaries.
Values in each row are significantly different from each other
at 95% level except ROUGE-2 and ROUGE-SU4 in 1-model
category.
Models ROUGE-2 ROUGE-SU4 BE
1 0.8789 0.8671 0.8553
2 0.8972 0.8803 0.8917
3 0.9036 0.8845 0.9048
4 (bootstr) 0.9082 0.8874 0.9107
4 (actual) 0.9077 0.8877 0.9123
Table 3: Mean correlations of 4-model Pyramid score and
other metrics using 1, 2, 3, or 4 models for TAC 2008 initial
summaries. Values in each row are significantly different from
each other at 95% level except ROUGE-2 and BE in 4-model
category.
Models ROUGE-2 ROUGE-SU4 BE
1 0.9179 0.9110 0.9016
2 0.9336 0.9199 0.9284
3 0.9392 0.9233 0.9383
4 (bootstr) 0.9443 0.9277 0.9436
4 (actual) 0.9429 0.9263 0.9446
Table 4: Mean correlations of 4-model Pyramid score and
other metrics using 1, 2, 3, or 4 models for TAC 2008 update
summaries. Values in each row are significantly different from
each other at 95% level except ROUGE-2 and BE in 4-model
category.
relates with the ranking based on another set of
models/topics, where the two sets are randomly
selected and mutually exclusive. This methodol-
ogy allows us to check the correlations based on
up to half of the actual number of models/topics
only (because of the non-overlap requirement), but
it gives an indication of the general tendency. We
also look at the correlation between the Pyramid
score and Overall Responsiveness. We don?t ex-
pect a perfect correlation between Pyramid and
Responsiveness in the best of times, because Pyra-
mid measures content identity between the can-
didate and the model, and Responsiveness mea-
sures content relevance to topic as well as linguis-
tic quality. However, the degree of variation be-
tween the two scores depending on the number of
models/topics used for the Pyramid will give us
a certain indication of the amount of information
lost.
2.2 Automatic metrics
Similarly to the Pyramid method, ROUGE (Lin,
2004b) and Basic Elements (Hovy et al, 2005)
require multiple topics and model summaries to
produce optimal results. ROUGE is a collection
of automatic n-gram matching metrics, ranging
from unigram to four-gram. It also includes mea-
surements of the longest common subsequence,
weighted or unweighted, and the option to com-
pare stemmed versions of words and omit stop-
words. There is also the possibility of accept-
ing skip-n-grams, that is, counting n-grams as
matching even if there are some intervening non-
matching words. The skip-n-grams together with
stemming are the only ways ROUGE can acco-
modate alternative forms of expression and match
concepts even though they might differ in terms of
their syntactic or lexical form.
These methods are necessarily limited, and so
ROUGE relies on using multiple parallel model
summaries which serve as a source of lexi-
cal/syntactic variation in the comparison process.
The fewer models there are, the less reliable the
score. Our question here is not only what this rela-
tion looks like (as it was examined on the basis of
Document Understanding Conference data in Lin
(2004a)), but also how it compares to the reliabil-
ity of other metrics.
Basic Elements (BE), on the other hand, goes
beyond simple string matching and parses the syn-
tactic structure of the candidate and model to ob-
tain a set of head-modifier pairs for each, and then
compares the sets. A head-modifier pair consist of
the head of a syntactic unit (e.g. the noun in a noun
phrase), and the word which modifes the head (i.e.
a determiner in a noun phrase). It is also possible
to include the name of the relation which connects
them (i.e. subject, object, etc.). Since BEs reflect
thematic relations in a sentence rather than surface
word order, it should be possible to accommodate
certain differences of expression that might appear
between a candidate summary and a reference, es-
pecially as the words can be stemmed. This could,
in theory, allow us to use fewer models for the
evaluation. In practice, however, it fails to account
for the total possible variety, and, what is more,
25
the additional step of parsing the text can intro-
duce noise into the comparison.
TAC 2008 and DUC 2007 evaluations used
ROUGE-2 and ROUGE-SU4, which refer to the
recall of bigram and skip-bigram (with up to 4 in-
tervening words) matches on stemmed words, re-
spectively, as well as a BE score calculated on the
basis of stemmed head-modifier pairs without re-
lation labels. Therefore, these are the versions we
use in our comparisons.
3 Number of models
Since Responsiveness score does not depend on
the number of models, it serves as a reference
against which we compare the remaining metrics,
while we calculate their score with only 1, 2, 3, or
all 4 models. Given 48 topics in TAC 2008, and
4-model summaries for each topic, there are 4
48
possible combinations to derive the final score in
the single-model category, so to keep the experi-
ments simple we only selected 1000 random sam-
ples from that space. For 1000 repetitions, each
time we selected a random combination of model
summaries (only one model out of 4 available per
topic), against which we evaluated the candidate
summaries. Then, for each of the 1000 samples,
we calculated the correlation between the result-
ing score and Responsiveness. We then took the
1000 correlations produced in this manner, and
computed their mean. In the same way, we cal-
culated the scores based on 2 and 3 model sum-
maries, randomly selected from the 4 available for
each topic. The correlation means for all metrics
and categories are given in Table 1 for initial sum-
maries and Table 2 for update summaries. We also
ran a one-way analysis of variance (ANOVA) on
these correlations to determine whether the cor-
relation means were significantly different from
each other. For the 4-model category there was
only one possible sample for each metric, so in or-
der to perform ANOVA we bootstrapped this sam-
ple to produce 1000 samples. The actual value of
the 4-model correlation is given in the tables as 4
(actual), and the mean value of the bootstrapped
1000 correlations is given as 4 (bootstr).
Values for initial summaries are significantly
different from their counterparts for update sum-
maries at the 95% level. Pairwise testing of values
for statistically significant differences is shown
with symbols: in each column, the first value
marked with a particular symbol is not signifi-
cantly different from any subsequent value marked
with the same symbol.
We also examined the correlations of the met-
rics with the 4-model Pyramid score. Table 3
presents the correlation means for the initial sum-
maries, and Table 4 shows the correlation means
for the update summaries.
Since the Pyramid, contrary to Responsiveness,
makes use of multiple model summaries, we ex-
amine its stability given a decreased number of
models to rely on. For this purpose, we correlated
the Pyramid score based on randomly selected 2
models (half of the model pool) for each topic with
the score based on the remaining 2 models, and
repeated this 1000 times. We also looked at the
1-model category, where the Pyramid score cal-
culated on the basis of one model per topic was
correlated with the Pyramid score calculated on
the basis on another randomly selected model. In
both case we witness a very high mean correlation:
0.994 and 0.995 for the 2-model category, 0.982
and 0.985 for the 1-model category for TAC initial
and update summaries, respectively. As an illus-
tration, Figure1 shows the variance of correlations
for the initial summaries.
Figure 1: Correlations between Pyramid scores based on 1
or 2 model summaries for TAC 2008 initial summaries.
The variation in correlation levels between
other metrics and Pyramid and Responsiveness,
presented in Tables 3?4, is more visible in the
graph form. Figures 2-3 illustrate the mean
correlation values for TAC 2008 initial sum-
maries. While all the metrics record the steep-
est increase in correlation values with the addi-
tion of the second model, adding the third and
fourth model provides the metrics with smaller
but steady improvement, with the exception of
Pyramid-Responsiveness correlation in Figure 2.
The increase in correlation mean is most dramatic
for BE, which in all cases starts as the lowest-
26
correlating metric in the single-model category,
but by the 4-model point it outperforms one or
both versions of ROUGE. The Pyramid metric
achieves significantly higher correlations than any
other metric, independent of the number of mod-
els, which is perhaps unsurprising given that it is a
manual evaluation method. Of the two ROUGE
versions, ROUGE-2 seems consistently a better
predictor of both Responsiveness and the ?full? 4-
model Pyramid score than ROUGE-SU4.
Figure 2: Responsiveness vs. other metrics with 1, 2, 3, or
4 models for TAC 2008 initial summaries.
Figure 3: 4-model Pyramid vs. other metrics with 1, 2, 3,
or 4 models for TAC 2008 initial summaries.
Similar patterns appear in DUC 2007 data (Ta-
ble 5), despite the fact that the Overall Respon-
siveness of TAC 2008 is replaced with Content Re-
sponsiveness (ignoring linguistic quality), against
which we calculate all the correlations. Although
the increase in correlation means from 1- to 4-
models for the three automatic metrics is smaller
than for TAC 2008, the clearest rise occurs with
the addition of a second model, especially for BE,
and the subsequent additions change little. As in
the case of initial summaries 2008, ROUGE-2 out-
performs the remaining two metrics independently
of the number of models. However, most of the in-
creases are too small to be significant.
This comparison suggests diminishing returns
Models ROUGE-2 ROUGE-SU4 BE
1 0.8681 0.8254 0.8486
2 0.8747* 0.8291* 0.8577*
3 0.8766*? 0.8299*? 0.8599*
4 (bootstr) 0.8761*? 0.8305*? 0.8633
4 (actual) 0.8795 0.8301 0.8609
Table 5: Mean correlations of Content Responsiveness and
other metrics using 1, 2, 3, or 4 models for DUC 2007 sum-
maries. Values in each row are significantly different from
each other at 95% level.
with the addition of more models, as well as dif-
ferent reactions among the metrics to the presence
or absence of additional models. When correlating
with Responsiveness, the manual Pyramid metric
benefits very little from the fourth model, but au-
tomatic BE benefits most from almost every addi-
tion. ROUGE is situated somewhere between the
two, noting small but often significant increases.
On the whole, the use of multiple models (at least
two) seems supported, especially if we use auto-
matic metrics in our evaluation.
4 Number of topics
For the second set of experiments we kept all four
models, but varied the number of topics which
went into the final average system score. To deter-
mine the stability of Responsiveness and Pyramid
we looked at the correlations between the scores
based on smaller sets of topics. For 1000 rep-
etitions, we calculated Pyramid/Responsiveness
score based on a set of 1, 3, 6, 12, or 24 topics ran-
domly chosen from the pool of 48, and compared
the system ranking thus created with the ranking
based on another, equally sized set, such that the
sets did not contain common topics. Table 6 shows
the mean correlation for each case. Although such
comparison was only possible up to 24 topics (half
of the whole available topic pool), the numbers
suggest that at the level of 48 topics both Respon-
siveness and Pyramid are stable enough to serve as
reference for the automatic metrics.
Responsiveness Pyramid
Topics Initial Update Initial Update
1 0.182 0.196 0.333 0.267
3 0.405 0.404 0.439 0.520
6 0.581 0.586 0.608 0.690
12 0.738 0.738 0.761 0.816
24 0.849 0.866 0.851 0.901
Table 6: Mean correlations between Responsive-
ness/Pyramid scores based on 1, 3, 6, 12, and 24 topic sam-
ples for TAC 2008 initial and update summaries.
In a process which mirrored that described in
Section 3, we created 1000 random samples in
each of the n-topics category: 1, 3, 6, 12, 24, 36,
27
Topics Pyramid ROUGE-2 ROUGE-SU4 BE
1 0.4219 0.4276 0.4375 0.3506
3 0.6204 0.5980 0.9016 0.5108
6 0.7274 0.6901 0.6836 0.6233
12 0.8159 0.7618 0.7456 0.7117
24 0.8679 0.8040 0.7809 0.7762
36 0.8890* 0.8208* 0.7951* 0.8017*
39 0.8927*? 0.8231*? 0.7967*? 0.8063*?
42 0.8954*?? 0.8258*?? 0.7958*?? 0.8102*??
45 0.8977*??? 0.8274*??? 0.8008*??? 0.8132???
48 (bootstr) 0.8972*??? 0.8302*??? 0.8046??? 0.8138???
48 (actual) 0.8997 0.8302 0.8033 0.8171
Table 7: Mean correlations of 48 topic Responsiveness and
other metrics using from 1 to 48 topics for TAC 2008 initial
summaries. Values in each row are significantly different from
each other at 95% level except: ROUGE-2, ROUGE-SU4 and
BE in 1-topic category, ROUGE-2 and ROUGE-SU4 in 3- and
6-topic category.
Topics Pyramid ROUGE-2 ROUGE-SU4 BE
1 0.5005 0.4882 0.5609 0.4011
3 0.7053 0.6862 0.7340 0.6097
6 0.8080 0.7850 0.8114 0.7274
12 0.8812 0.8498 0.8596 0.8188
24 0.9250 0.8882 0.8859 0.8774
36 0.9408* 0.9023* 0.8960* 0.8999*
39 0.9433*? 0.9045*? 0.8973*? 0.9037*?
42 0.9455*?? 0.9061*?? 0.8987*?? 0.9068*??
45 0.9474??? 0.9078*??? 0.8996*??? 0.9094???
48 (bootstr) 0.9481??? 0.9101??? 0.9015*??? 0.9111???
48 (actual) 0.9492 0.9103 0.9020 0.9132
Table 8: Mean correlations of 48 topic Responsiveness and
other metrics using from 1 to 48 topics for TAC 2008 update
summaries. Values in each row are significantly different from
each other at 95% level except: Pyramid and ROUGE-2 in 1-
topic category, Pyramid and ROUGE-SU4 in 6-topic category,
ROUGE-2 and BE in 39-, 42-, and 48-topic category.
Topics ROUGE-2 ROUGE-SU4 BE
1 0.4693 0.4856 0.3888
3 0.6575 0.6684 0.5732
6 0.7577 0.7584 0.6960
12 0.8332 0.8245 0.7938
24 0.8805 0.8642 0.8684
36 0.8980* 0.8792* 0.8966*
39 0.9008*? 0.8812*? 0.9017*?
42 0.9033*?? 0.8839*?? 0.9058??
45 0.9052*??? 0.8853*??? 0.9093???
48 (bootstr) 0.9074??? 0.8877??? 0.9107???
48 (actual) 0.9077 0.8877 0.9123
Table 9: Mean correlations of 48 topic Pyramid score and
other metrics using from 1 to 48 topics for TAC 2008 initial
summaries. Values in each row are significantly different from
each other at 95% level except: ROUGE-2 and ROUGE-SU4
in the 6-topic category, ROUGE-2 and BE in 39- and 48-topic
category.
Topics ROUGE-2 ROUGE-SU4 BE
1 0.5026 0.5729 0.4094
3 0.7106 0.7532 0.6276
6 0.8130 0.8335 0.7512
12 0.8806 0.8834 0.8475
24 0.9196 0.9092 0.9063
36 0.9343* 0.9198* 0.9301*
39 0.9367*? 0.9213*? 0.9341*?
42 0.9386*?? 0.9227*?? 0.9376*??
45 0.9402*??? 0.9236*??? 0.9402???
48 (bootstr) 0.9430??? 0.9280? 0.9444??
48 (actual) 0.9429 0.9263 0.9446
Table 10: Mean correlations of 48 topic Pyramid score and
other metrics using from 1 to 48 topics for TAC 2008 update
summaries. Values in each row are significantly different from
each other at 95% level except: ROUGE-2 and ROUGE-SU4
in 12-topic category, ROUGE-2 and BE in 45-topic category.
39, 42, or 45. Within each of these categories, for
a thousand repetitions, we calculated the score for
automatic summarizers by averaging over n topics
randomly selected from the pool of 48 topics avail-
able in the evaluation. Again, we examined the
correlations between the metrics and the ?full? 48-
topic Responsiveness and Pyramid. As previously,
we then used ANOVA to determine whether the
correlation means differed significantly. Because
there was only one possible sample with all 48
topics for each metric, we bootstrapped this sam-
ple to provide 1000 new samples in the 48-topic
category, in order to perfom the ANOVA compari-
son of variance. Tables 7 and 8, as well as Figures
4 and 5, show the metrics? changing correlations
with Responsiveness. Tables 9 and 10, and Fig-
ures 6 and 7, show the correlations with the 48-
topic Pyramid score. Values for initial summaries
are significantly different from their counterparts
for update summaries at the 95% level.
In all cases, it becomes clear that the curves flat-
ten out and the correlations stop increasing almost
completely beyond the 36-topic mark. This means
that the scores for the automatic summarization
systems based on 36 topics will be on average
practically indistiguishable from the scores based
on all 48 topics, showing that beyond a certain
minimally necessary number of topics adding or
removing a few (or even ten) topics will not influ-
ence the system scores much. (However, we can-
not conclude that a further considerable increase in
the number of topics ? well beyond 48 ? would not
bring more improvement in the correlations, per-
haps increasing the stable ?correlation window? as
well.)
Topics ROUGE-2 ROUGE-SU4 BE
1 0.6157 0.6378 0.5756
3 0.7597 0.7511 0.7323
6 0.8168 0.7904 0.7957
12 0.8493 0.8123 0.8306
24 0.8690 0.8249* 0.8517*
36 0.8751* 0.8287*? 0.8580*?
39 0.8761*? 0.8295*?? 0.8592??
42 0.8768*?? 0.8299*??? 0.8602???
45 (bootstr) 0.8761*?? 0.8305??? 0.8627???
45 (actual) 0.8795 0.8301 0.8609
Table 11: Mean correlations of 45 topic Content Respon-
siveness and other metrics using from 1 to 45 topics for DUC
2007 summaries. Values in each row are significantly differ-
ent from each other at 95% level.
An interesting observation is that if we pro-
duce such limited-topic scores for the manual
metrics, Responsiveness and Pyramid, and corre-
late them with their own ?full? versions based on
28
Figure 4: Responsiveness vs. other metrics with 1 to 48 topics
for TAC 2008 initial summaries.
Figure 5: Responsiveness vs. other metrics with 1 to 48 topics
for TAC 2008 update summaries.
Figure 6: 48-topic Pyramid vs. other metrics with 1 to 48
topics for TAC 2008 initial summaries.
Figure 7: 48-topic Pyramid vs. other metrics with 1 to 48
topics for TAC 2008 update summaries.
all 48 topics, it appears that they are less stable
than the automatic metrics, i.e. there is a larger
gap between the worst and best correlations they
achieve.
1
The mean correlation between the ?full?
Responsiveness and that based on 1 topic is 0.443
and 0.448 for the initial and update summaries, re-
spectively; for that based on 3 topics, 0.664 and
0.667. Pyramid based on 1 topic achieves 0.467
for initial and 0.525 for update summaries; Pyra-
mid based on 3 topics obtains 0.690 and 0.742,
respectively. Some of these values, especially
for update summaries, are even lower than those
obtained by ROUGE in the same category, de-
spite the fact that 1- and 3-topic Responsiveness or
Pyramid is a proper subset of the 48-topic Respon-
siveness/Pyramid. On the other hand, ROUGE
achieves considerably worse correlations with Re-
sponsiveness than Pyramid when there are many
topics available. ROUGE-SU4 seems to be more
stable than ROUGE-2; in all cases ROUGE-2
starts with lower correlations than ROUGE-SU4,
but by the 12-topic mark its correlations increase
1
For reasons of space, these values are not included in the
tables, as they offer little insight besides what is mentioned
here.
above it.
Additionally, despite being an automatic metric,
BE seems to follow the same pattern as the manual
metrics. It is seriously affected by the decreasing
number of topics; in fact, if the number of topics
drops below 24, BE is the least reliable indicator
of either Responsiveness or Pyramid. However,
by the 48-topic mark it rises to levels comparable
with ROUGE-2.
As in the case of models, DUC 2007 data shows
mostly the same pattern as TAC 2008. Again, in
this data set, the increase in the correlation mean
with the addition of topics for each metric are
smaller than for either initial or update summaries
in TAC 2008, but the relative rate of increase re-
mains the same: BE gains most from additional
topics (+0.28 in DUC vs. +0.47 and +0.51 in
TAC), ROUGE-SU4 again shows the smallest in-
crease (+0.19 in DUC vs. +0.36 and +0.34 in
TAC), which means it is the most stable of the met-
rics across the variable number of topics.
2
2
The smaller total increase might be due to the smaller
number of available topics (45 in DUC vs. 48 in TAC), but
we have seen the same effect in Section 3 while discussing
models, so it might just be an accidental property of a given
data set.
29
5 Discussion and conclusions
As the popularity of shared tasks increases, task
organizers face an ever growing problem of pro-
viding an adequate evaluation to all participating
teams. Often, evaluation of multiple runs from the
same team is required, as a way to foster research
and development. With more and more system
submissions to judge, and the simultaneous need
for multiple topics and models in order to provide
a stable assessment, difficult decisions of cutting
costs and effort might sometimes be necessary. It
would be useful then to know where such deci-
sions will have the smallest negative impact, or at
least, what might be the trade-offs inherent in such
decisions.
From our experiments, it appears that manual
metrics such as Pyramid gain less from the addi-
tion of more model summaries than the automatic
metrics. A Pyramid score based on any two mod-
els correlates very highly with the score based on
any other two models. For the automatic metrics,
the largest gain is recorded with adding the sec-
ond model; afterwards the returns diminish. BE
seems to be the most sensitive metric to changes in
the number of models and topics; ROUGE-SU4,
on the other hand, is the least sensitive to such
changes and the most stable, but it does not ob-
tain the highest correlations when many models
and topics are available.
Whatever the number of models, manual Pyra-
mid considerably outperforms automatic metrics,
as can be expected, since human understanding is
not hampered by the possible differences in sur-
face expression between a candidate and a model.
But when it comes to decreased number of topics,
the inherent variability of human judgment shows
strongly, to the extent that, in extreme cases of
very few topics, it might be more prudent to use
ROUGE-SU4 than Pyramid or Responsiveness.
Lastly, we observe that, as with models, adding
one or two topics to the evaluation plays a great
role only if we have very few topics to start with.
Our experiments suggest that, as the number of
topics available for evaluation increases, so does
the number of additional topics necessary to make
a difference in the system ranking produced by
a metric. It seems that in the case of evaluation
based on 48 topics, as in the TAC Summarization
track, it would be possible to decrease the number
to about 36 without sacrificing much stability.
References
Donna Harman and Paul Over. 2004. The effects of
human variation in DUC summarization evaluation.
In Proceedings of the ACL-04 Workshop: Text Sum-
marization Branches Out, pages 10?17, Barcelona,
Spain.
Eduard Hovy, Chin-Yew Lin, and Liang Zhou. 2005.
Evaluating DUC 2005 using Basic Elements. In
Proceedings of the 5th Document Understanding
Conference (DUC).
Chin-Yew Lin. 2004a. Looking for a few good met-
rics: Automatic summarization evaluation - how
many samples are enough? In Proceedings of NT-
CIR Workshop 4, Tokyo, Japan.
Chin-Yew Lin. 2004b. ROUGE: A package for au-
tomatic evaluation of summaries. In Proceedings
of the ACL 2004 Workshop: Text Summarization
Branches Out, pages 74?81.
Ani Nenkova and Rebecca J. Passonneau. 2004.
Evaluating content selection in summarization: The
Pyramid method. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 145?152, Boston, MA.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McK-
eown, and Sergey Sigelman. 2005. Applying
the Pyramid method in DUC 2005. In Proceed-
ings of the 5th Document Understanding Conference
(DUC), Vancouver, Canada.
Karen Sp?arck Jones. 2007. Automatic summarising:
The state of the art. Information Processing and
Management, 43(6):1449?1481.
Hans van Halteren and Simone Teufel. 2003. Examin-
ing the consensus between human summaries: Ini-
tial experiments with factoid analysis. In Proceed-
ings of the HLT-NAACL DUCWorkshop 2003, pages
57?64, Edmonton, Canada.
Ellen M. Voorhees and Chris Buckley. 2002. Effect of
topic set size on retrieval experiment error. In Pro-
ceedings of the 25th Annual International ACM SI-
GIR conference on Research and Development in In-
formation Retrieval, pages 317?323, Tampere, Fin-
land.
Ying Zhang and Stephan Vogel. 2004. Measuring
confidence intervals for the Machine Translation
evaluation metrics. In Proceedings of the 10th Con-
ference on Theoretical and Methodological Issues in
Machine Translation, pages 85?94, Baltimore, MD.
30
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359?362,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Assessing the Effect of Inconsistent Assessors on Summarization Evaluation
Karolina Owczarzak
National Institute of Standards and Technology
Gaithersburg, MD 20899
karolina.owczarzak@gmail.com
Peter A. Rankel
University of Maryland
College Park, Maryland
rankel@math.umd.edu
Hoa Trang Dang
National Institute of Standards and Technology
Gaithersburg, MD 20899
hoa.dang@nist.gov
John M. Conroy
IDA/Center for Computing Sciences
Bowie, Maryland
conroy@super.org
Abstract
We investigate the consistency of human as-
sessors involved in summarization evaluation
to understand its effect on system ranking and
automatic evaluation techniques. Using Text
Analysis Conference data, we measure anno-
tator consistency based on human scoring of
summaries for Responsiveness, Readability,
and Pyramid scoring. We identify inconsis-
tencies in the data and measure to what ex-
tent these inconsistencies affect the ranking
of automatic summarization systems. Finally,
we examine the stability of automatic metrics
(ROUGE and CLASSY) with respect to the
inconsistent assessments.
1 Introduction
Automatic summarization of documents is a re-
search area that unfortunately depends on human
feedback. Although attempts have been made at au-
tomating the evaluation of summaries, none is so
good as to remove the need for human assessors.
Human judgment of summaries, however, is not per-
fect either. We investigate two ways of measuring
evaluation consistency in order to see what effect it
has on summarization evaluation and training of au-
tomatic evaluation metrics.
2 Assessor consistency
In the Text Analysis Conference (TAC) Summariza-
tion track, participants are allowed to submit more
than one run (usually two), and this option is of-
ten used to test different settings or versions of the
same summarization system. In cases when the sys-
tem versions are not too divergent, they sometimes
produce identical summaries for a given topic. Sum-
maries are randomized within each topic before they
are evaluated, so the identical copies are usually in-
terspersed with 40-50 other summaries for the same
topic and are not evaluated in a row. Given that each
topic is evaluated by a single assessor, it then be-
comes possible to check assessor consistency, i.e.,
whether the assessor judged the two identical sum-
maries in the same way.
For each summary, assessors conduct content
evaluation according to the Pyramid framework
(Nenkova and Passonneau, 2004) and assign it Re-
sponsiveness and Readability scores1, so assessor
consistency can be checked in these three areas sep-
arately. We found between 230 (in 2009) and 430
(in 2011) pairs of identical summaries for the 2008-
2011 data (given on average 45 topics, 50 runs, and
two summarization conditions: main and update),
giving in effect anywhere from around 30 to 60 in-
stances per assessor per year. Using Krippendorff?s
alpha (Freelon, 2004), we calculated assessor con-
sistency within each year, as well as total consis-
tency over all years? data (for those assessors who
worked multiple years). Table 1 shows rankings of
assessors in 2011, based on their Readability, Re-
sponsiveness, and Pyramid judgments for identical
summary pairs (around 60 pairs per assessor).
Interestingly, consistency values for Readability
are lower overall than those for Responsiveness and
Pyramid, even for the most consistent assessors.
Given that Readability and Responsiveness are eval-
uated in the same way, i.e. by assigning a numeri-
cal score according to detailed guidelines, this sug-
1http://www.nist.gov/tac/2011/Summarization/Guided-
Summ.2011.guidelines.html
359
ID Read ID Resp ID Pyr
G 0.867 G 0.931 G 0.975
D 0.866 D 0.875 D 0.970
A 0.801 H 0.808 H 0.935
H 0.783 A 0.750 A 0.931
F 0.647 F 0.720 E 0.909
C 0.641 E 0.711 C 0.886
E 0.519 C 0.490 F 0.872
Table 1: Annotator consistency in assigning Readability
and Responsiveness scores and in Pyramid evaluation, as
represented by Krippendorff?s alpha for interval values,
on 2011 data.
gests that Readability as a quality of text is inher-
ently more vague and difficult to pinpoint.
On the other hand, Pyramid consistency values
are generally the highest, which can be explained
by how the Pyramid evaluation is designed. Even
if the assessor is inconsistent in selecting Sum-
mary Content Units (SCUs) across different sum-
maries, as long as the total summary weight is sim-
ilar, the summary?s final score will be similar, too.2
Therefore, it would be better to look at whether as-
sessors tend to find the same SCUs (information
?nuggets?) in different summaries on the same topic,
and whether they annotate them consistently. This
can be done using the ?autoannotate? function of
the Pyramid process, where all SCU contributors
(selected text strings) from already annotated sum-
maries are matched against the text of a candidate
(un-annotated) summary. The autoannotate func-
tion works fairly well for matching between extrac-
tive summaries, which tend to repeat verbatim whole
sentences from source documents.
For each summary in 2008-2011 data, we autoan-
notated it using all remaining manually-annotated
summaries from the same topic, and then we com-
pared the resulting ?autoPyramid? score with the
score from the original manual annotation for that
summary. Ideally, the autoPyramid score should
be lower or equal to the manual Pyramid score: it
would mean that in this summary, the assessor se-
lected as relevant all the same strings as s/he found
in the other summaries on the same topic, plus possi-
bly some more information that did not appear any-
2The final score is based on total weight of all SCUs found
in the summary, so the same weight can be obtained by select-
ing a larger number of lower-weight SCUs or a smaller number
of higher-weight SCUs (or the same number of similar-weight
SCUs which nevertheless denote different content).
Figure 1: Annotator consistency in selecting SCUs in
Pyramid evaluation, as represented by the difference be-
tween manual Pyramid and automatic Pyramid scores
(mP-aP), on 2011 data.
where else. If the autoPyramid score is higher than
the manual Pyramid score, it means that either (1)
the assessor missed relevant strings in this summary,
but found them in other summaries; or (2) the strings
selected as relevant elsewhere in the topic were acci-
dental, and as such not repeated in this summary. Ei-
ther way, if we then average out score differences for
all summaries for a given topic, it will give us a good
picture of the annotation consistency in this partic-
ular topic. Higher average autoPyramid scores sug-
gest that the assessor was missing content, or other-
wise making frequent random mistakes in assigning
content. Figure 1 shows the macro-average differ-
ence between manual Pyramid scores and autoPyra-
mid scores for each assessor in 2011.3 For the most
part, it mirrors the consistency ranking from Table
1, confirming that some assessors are less consistent
than others; however, certain differences appear: for
instance, Assessor A is one of the most consistent in
assigning Readability scores, but is not very good at
selecting SCUs consistently. This can be explained
by the fact that the Pyramid evaluation and assigning
Readability scores are different processes and might
require different skills and types of focus.
3 Impact on evaluation
Since human assessment is used to rank participat-
ing summarizers in the TAC Summarization track,
3Due to space constraints, we report figures for only 2011,
but the results for other years are similar.
360
Pearson?s r Spearman?s rho
-1 worst -2 worst -1 worst -2 worst
Readability 0.995 0.993 0.988 0.986
Responsiveness 0.996 0.989 0.986 0.946
Pyramid 0.996 0.992 0.978 0.960
mP-aP 0.996 0.987 0.975 0.943
Table 2: Correlation between the original summarizer
ranking and the ranking after excluding topics by one or
two worst assessors in each category.
we should examine the potential impact of incon-
sistent assessors on the overall evaluation. Because
the final summarizer score is the average over many
topics, and the topics are fairly evenly distributed
among assessors for annotation, excluding noisy
topics/assessors has very little impact on summa-
rizer ranking. As an example, consider the 2011 as-
sessor consistency data in Table 1 and Figure 1. If
we exclude topics by the worst performing assessor
from each of these categories, recalculate the sum-
marizer rankings, and then check the correlation be-
tween the original and newly created rankings, we
obtain results in Table 2.
Although the impact on evaluating automatic
summarizers is small, it could be argued that exclud-
ing topics with inconsistent human scoring will have
an impact on the performance of automatic evalua-
tion metrics, which might be unfairly penalized by
their inability to emulate random human mistakes.
Table 3 shows ROUGE-2 (Lin, 2004), one of the
state-of-the-art automatic metrics used in TAC, and
its correlations with human metrics, before and af-
ter exclusion of noisy topics from 2011 data. The
results are fairly inconclusive: it seems that in most
cases, removing topics does more harm than good,
suggesting that the signal-to-noise ratio is still tipped
in favor of signal. The only exception is Readability,
where ROUGE records a slight increase in correla-
tion; this is unsurprising, given that consistency val-
ues for Readability are the lowest of all categories,
and perhaps here removing noise has more impact.
In the case of Pyramid, there is a small gain when
we exclude the single worst assessor, but excluding
two assessors results in a decreased correlation, per-
haps because we remove too much valid information
at the same time.
A different picture emerges when we examine
how well ROUGE-2 can predict human scores on
the summary level. We pooled together all sum-
Readability Responsiveness Pyramid mP-aP
before 0.705 0.930 0.954 0.954
-1 worst 0.718 0.921 0.961 0.942
-2 worst 0.718 0.904 0.952 0.923
Table 3: Correlation between the summarizer rankings
according to ROUGE-2 and human metrics, before and
after excluding topics by one or two worst assessors in
that category.
Readability Responsiveness Pyramid mP-aP
before 0.579 0.694 0.771 0.771
-1 worst 0.626 0.695 0.828 0.752
-2 worst 0.628 0.721 0.817 0.741
Table 4: Correlation between ROUGE-2 and human met-
rics on a summary level before and after excluding topics
by one or two worst assessors in that category.
maries annotated by each particular assessor and cal-
culated the correlation between ROUGE-2 and this
assessor?s manual scores for individual summaries.
Then we calculated the mean correlation over all
assessors. Unsurprisingly, inconsistent assessors
tend to correlate poorly with automatic (and there-
fore always consistent) metrics, so excluding one
or two worst assessors from each category increases
ROUGE?s average per-assessor summary-level cor-
relation, as can be seen in Table 4. The only ex-
ception here is when we exclude assessors based on
their autoPyramid performance: again, because in-
consistent SCU selection doesn?t necessarily trans-
late into inconsistent final Pyramid scores, exclud-
ing those assessors doesn?t do much for ROUGE-2.
4 Impact on training
Another area where excluding noisy topics might be
useful is in training new automatic evaluation met-
rics. To examine this issue we turned to CLASSY
(Rankel et al, 2011), an automatic evaluation met-
ric submitted to TAC each year from 2009-2011.
CLASSY consists of four different versions, each
aimed at predicting a particular human evaluation
score. Each version of CLASSY is based on one
of three regression methods: robust regression, non-
negative least squares, or canonical correlation. The
regressions are calculated based on a collection of
linguistic and content features, derived from the
summary to be scored.
CLASSY requires two years of marked data to
score summaries in a new year. In order to predict
361
the human metrics in 2011, for example, CLASSY
uses the human ratings from 2009 and 2010. It first
considers each subset of the features in turn, and us-
ing each of the regression methods, fits a model to
the 2009 data. The subset/method combination that
best predicts the 2010 scores is then used to pre-
dict scores for 2011. However, the model is first re-
trained on the 2010 data to calculate the coefficients
to be used in predicting 2011.
First, we trained all four CLASSY versions on
all available 2009-2010 topics, and then trained
again excluding topics by the most inconsistent as-
sessor(s). A different subset of topics was ex-
cluded depending on whether this particular version
of CLASSY was aiming to predict Responsiveness,
Readability, or the Pyramid score. Then we tested
CLASSY?s performance on 2011 data, ranking ei-
ther automatic summarizers (NoModels case) or hu-
man and automatic summarizers together (AllPeers
case), separately for main and update summaries,
and calculated its correlation with the metrics it was
aiming to predict. Table 5 shows the result of this
comparison. For Pyramid, (a) indicates that ex-
cluded topics were selected based on Krippendorff?s
alpha, and (b) indicates that topics were excluded
based on their mean difference between manual and
automatic Pyramid scores.
The results are encouraging; it seems that remov-
ing noisy topics from training data does improve the
correlations with manual metrics in most cases. The
greatest increase takes place in CLASSY?s correla-
tions with Responsiveness for main summaries in
AllPeers case, and for correlations with Readabil-
ity. While none of the changes are large enough
to achieve statistical significance, the pattern of im-
provement is fairly consistent.
5 Conclusions
We investigated the consistency of human assessors
in the area of summarization evaluation. We con-
sidered two ways of measuring assessor consistency,
depending on the metric, and studied the impact of
consistent scoring on ranking summarization sys-
tems and on the performance of automatic evalu-
ation systems. We found that summarization sys-
tem ranking, based on scores for multiple topics,
was surprisingly stable and didn?t change signifi-
NoModels AllPeers
main update main update
Pyramid
CLASSY1 Pyr 0.956 0.898 0.945 0.936
CLASSY1 Pyr new (a) 0.950 0.895 0.932 0.955
CLASSY1 Pyr new (b) 0.960 0.900 0.940 0.955
Responsiveness
CLASSY2 Resp 0.951 0.903 0.948 0.963
CLASSY2 Resp new 0.954 0.907 0.973 0.950
CLASSY4 Resp 0.951 0.927 0.830 0.949
CLASSY4 Resp new 0.943 0.928 0.887 0.946
Readability
CLASSY3 Read 0.768 0.705 0.844 0.907
CLASSY3 Read new 0.793 0.721 0.858 0.906
Table 5: Correlations between CLASSY and human met-
rics on 2011 data (main and update summaries), before
and after excluding most inconsistent topic from 2009-
2010 training data for CLASSY.
cantly when several topics were removed from con-
sideration. However, on a summary level, remov-
ing topics scored by the most inconsistent assessors
helped ROUGE-2 increase its correlation with hu-
man metrics. In the area of training automatic met-
rics, we found some encouraging results; removing
noise from the training data allowed most CLASSY
versions to improve their correlations with the man-
ual metrics that they were aiming to model.
References
Deen G. Freelon. 2010. ReCal: Intercoder Reliability
Calculation as a Web Service. International Journal
of Internet Science, Vol 5(1).
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
78?81. Barcelona, Spain.
Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluat-
ing content selection in summarization: The Pyramid
method. Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, 145?
152. Boston, MA.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-
own, and Sergey Sigelman. 2005. Applying the Pyra-
mid method in DUC 2005. Proceedings of the 5th
Document Understanding Conference (DUC). Van-
couver, Canada.
Peter A. Rankel, John M. Conroy, and Judith D.
Schlesinger. 2012. Better Metrics to Automatically
Predict the Quality of a Text Summary. Proceedings
of the SIAM Data Mining Text Mining Workshop 2012.
362
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 25?32,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Who wrote What Where: Analyzing the content of human and automatic
summaries
Karolina Owczarzak and Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
Gaithersburg, MD 20899
karolina.owczarzak@nist.gov hoa.dang@nist.gov
Abstract
Abstractive summarization has been a long-
standing and long-term goal in automatic sum-
marization, because systems that can generate
abstracts demonstrate a deeper understanding
of language and the meaning of documents
than systems that merely extract sentences
from those documents. Genest (2009) showed
that summaries from the top automatic sum-
marizers are judged as comparable to manual
extractive summaries, and both are judged to
be far less responsive than manual abstracts,
As the state of the art approaches the limits
of extractive summarization, it becomes even
more pressing to advance abstractive summa-
rization. However, abstractive summarization
has been sidetracked by questions of what
qualifies as important information, and how do
we find it? The Guided Summarization task
introduced at the Text Analysis Conference
2010 attempts to neutralize both of these prob-
lems by introducing topic categories and lists
of aspects that a responsive summary should
address. This design results in more similar
human models, giving the automatic summa-
rizers a more focused target to pursue, and also
provides detailed diagnostics of summary con-
tent, which can can help build better meaning-
oriented summarization systems.
1 Introduction
What qualifies as important information and how do
we find it? These questions have been leading re-
search in automatic summarization since its begin-
nings, and we are still nowhere near a definitive
answer. Worse, experiments with humans subjects
suggest a definitive answer might not even exist.
With all their near-perfect language understanding
and world knowledge, two human summarizers will
still produce two different summaries of the same
text, simply because they will disagree on what?s
important. Fortunately, usually some of this infor-
mation will overlap. This is represented by the idea
behind the Pyramid evaluation framework (Nenkova
and Passonneau, 2004; Passonneau et al, 2005),
where different levels of the pyramid represent the
proportion of concepts (?Summary Content Units?,
or SCUs) mentioned by 1 to n summarizers in sum-
maries of the same text. Usually, there are very few
SCUs that are mentioned by all summarizers, a few
more that are mentioned by some of them, and the
greatest proportion are the SCUs that are mentioned
by individual summarizers only.
This variance in what should be a ?gold standard?
makes research in automatic summarization meth-
ods particularly difficult. How can we reach a goal
so vague and under-defined? Using term frequency
to determine important concepts in a text has proven
to be very successful, largely because of its simplic-
ity and universal applicability, but statistical meth-
ods can only provide the most basic level of perfor-
mance. On the other hand, there is no real motiva-
tion to use any deeper meaning-oriented text anal-
ysis if we are not even certain what information to
look for in order to produce a responsive summary.
To address these concerns, the Summarization
track at the 2010 Text Analysis Conference1 (TAC)
introduced a new summarization task ? Guided
Summarization ? in which topics are divided into
1All datasets available at http://www.nist.gov/tac/
25
narrow categories and a list of required aspects is
provided for each category. This serves two pur-
poses: first, it creates a more focused target for au-
tomatic summarizers, neutralizing human variance
and pointing to concrete types of information the
reader requires, and second, it provides a detailed
diagnostic tool to analyze the content of automatic
summaries, which can help build more meaning-
oriented systems. This paper shows how these ob-
jectives were achieved in TAC 2010, looking at the
similarity of human-crafted models, and then using
the category and aspect information to look in depth
at the differences between human and top automatic
summarizers, discovering strengths and weaknesses
of automatic systems and areas for improvement.
2 Topic-specific summarization
The idea that different types of stories might require
different approaches is not new, although the classi-
fication varies from task to task. Topic categories
were present in Document Understanding Confer-
ence2 (DUC) 2001, where topics were divided into:
single-event, single-subject, biographical, multiple
events of same type, and opinion. In their analy-
sis of these results, Nenkova and Louis (2008) find
that summaries of articles in what they call topic-
cohesive categories (single-event, single-subject, bi-
ography) are of higher quality than those in non-
cohesive categories (opinion, multiple event).
In essence, categorizing topics into types is based
on the assumption that stories of the same type fol-
low a specific template and include the same kinds
of facts, and this predictability might be employed
to improve the summarization process, since we at
least know what kinds of information are important
and what to look for. This was shown, among others,
by Bagga (1997), who analyzed source articles used
in the Message Understanding Conference (MUC)
and graphed the distribution of facts in articles on
air vehicle launches, terrorist attacks, joint ventures,
and corporate personnel changes, finding that the
same kinds of facts appeared repeatedly. A nat-
ural conclusion is that Information Extraction (IE)
methods might be helpful here, and in fact, White
et al (2001) presented an IE-based summarization
system for natural disasters, where they first filled
2http://www-nlpir.nist.gov/projects/duc/
an IE template with slots related to date, location,
type of disaster, damage (people, physical effects),
etc. Similarly, Radev and McKeown (1998) used IE
combined with Natural Language Generation (NLG)
in their SUMMON system.
There are two ways to classify stories: according
to their level of cohesiveness (to use the distinction
made by Nenkova and Louis (2008)), and accord-
ing to subject. The first classification could help
us determine which topics would be easier for au-
tomatic summarization, but the difficulty is related
purely to lexical characteristics of the text; as shown
in Louis and Nenkova (2009), source document sim-
ilarity in terms of word overlap is one of the pre-
dictive features of multi-document summary qual-
ity. The second classification, according to subject
matter, is what enables us to utilize more meaning-
oriented approaches such as IE and attempt a deeper
semantic analysis of the source text, and is what we
describe in this paper.
3 Guided summarization at TAC
The new guided summarization task in 2010 was
designed with the second classification in mind,
in order to afford the participants a chance to
explore deeper linguistic methods of text analy-
sis. There were five topic categories: (1) Acci-
dents and Natural Disasters, (2) Attacks (Crimi-
nal/Terrorist), (3) Health and Safety, (4) Endangered
Resources, and (5) Trials and Investigations (Crim-
inal/Legal/Other).3 In contrast to previous topic-
specific summarization tasks, the Guided Summa-
rization task also provided a list of required aspects,
which described the type of information that should
be included in the summary (if such information
could be found in source documents). Summariz-
ers also had the option of including any other in-
formation they deemed important to the topic. The
categories and their aspects, shown in Table 1, were
developed on the basis of past DUC and TAC topics
and model summaries from years 2001-2009.
Each topic came with 20 chronologically ordered
3In the remainder of this paper, the following short forms are
used for names of categories: Accidents = Accidents and Nat-
ural Disasters; Attacks = Attacks; Health = Health and Safety;
Resources = Endangered Resources; Trials = Trials and Inves-
tigations. Full description of the task is available at the TAC
website.
26
Accidents Attacks Health
what what what
when when who affected
where where how
why perpertrators why
who affected why countermeasures
damages who affected
countermeasures damages
countermeasures
Resources Trials
what who
importance who investigating
threats why
countermeasures charges
plead
sentence
Table 1: Categories and aspects in TAC 2010 Guided
Summarization task.
news articles. The initial summaries were to be pro-
duced on the basis of the first 10 documents. As
in TAC 2008 and 2009, the 2010 Summarization
task had an update component: using the second 10
documents, summarizers were to produce an update
summary under the assumption that the user had al-
ready read the first set of source documents. This
means that for the update part, there were two in-
teracting conditions, with the requirement for non-
redundancy taking priority over the requirement to
address all category aspects.
For each topic, four model summaries were writ-
ten by human assessors. All summaries were eval-
uated with respect to linguistic quality (Overall
Readability), content (Pyramid), and general quality
(Overall Responsiveness). Readability and Respon-
siveness were judged by human assessors on a scale
from 1 (very poor) to 5 (very good), while Pyramid
is a score between 0 and 1 (in very rare cases, it
exceeds 1, if the candidate summary contains more
SCUs than the average reference summary).
Since this was the first year of Guided Summa-
rization, only about half of the 43 participating sys-
tems made some use of the provided categories and
aspects, mostly using them and their synonyms as
query terms.
3.1 Model summaries across years
The introduction of categories, which implies tem-
plate story types, and aspects, which further nar-
rows content selection, resulted in the parallel model
summaries being much more similar to each other
than in previous years, as represented by the Pyra-
human automatic
initial update initial update
P
yr
am
id 2008 0.66 0.63 0.26 0.20
2009 0.68 0.60 0.26 0.20
2010 0.78 0.67 0.30 0.20
R
es
po
ns
.
2008 4.62 4.62 2.32 2.02
2009 4.66 4.48 2.32 2.17
2010 4.76 4.71 2.56 2.10
Table 2: Macro-average Pyramid and Responsiveness
scores for initial and update summaries for years 2008-
2010. Responsiveness scores for 2009 were scaled from
a ten-point to a five-point scale.
mid score, which measures information overlap be-
tween a candidate summary and a set of refer-
ence summaries. Table 2 shows the macro-averaged
Pyramid and Responsiveness scores for years 2008-
2010. Both initial and update human summaries
score higher for Pyramid in 2010, and also gain a lit-
tle in Responsiveness. The macro-averages for auto-
matic summarizers, on the other hand, increase only
for initial summaries, which we will discuss further
in Section 3.4. The similarity effect among model
summaries can be more clearly seen in Table 3,
which shows the percentage of Summary Content
Units (SCUs, information ?nuggets?or simple facts)
with different weights in Pyramids across the years
between 2008-2010. The weight of an SCU is sim-
ply the number of model summaries in which this
information unit appears. Pyramids in 2010 have
greater percentage of SCUs with weight > 1, and
their proportion of weight-1 SCUs is below half of
all SCUs. The difference is much more pronounced
for the initial summaries, since the update compo-
nent is restricted by the non-redundancy require-
ment, resulting in more variance in content selection
after the required aspects have been covered.4
3.2 Content coverage in TAC 2010
During the Pyramid creation process, assessors ex-
tracting SCUs from model summaries were asked to
mark the aspect(s) relevant to each SCU. This lets
us examine and compare the distribution of infor-
mation in human and automatic summaries. Table 4
shows macro-average SCU counts in Pyramids com-
4Each summary could be up to 100 words long, and no
incentive was given for writing summaries of shorter length;
therefore, the goal for both human and automatic summarizers
was to fit as much relevant information as possible in the 100-
word limit.
27
SCU
weight 2008 2009 2010
in
it
ia
l 4 9% 12% 22%
3 14% 13% 18%
2 22% 23% 24%
1 55% 52% 36%
up
da
te
4 8% 7% 11%
3 12% 12% 14%
2 21% 20% 26%
1 59% 62% 49%
Table 3: Percentage of SCUs with weights 1?4 in pyra-
mids for initial and update summaries for years 2008-
2010.
posed of four human summaries, and macro-average
counts of matching SCUs in the summaries of the
15 top-performing automatic summarizers (as deter-
mined by their Responsiveness rank on initial sum-
maries).5 Although automatic summaries find only
a small percentage of all available information (as
represented by the number of Pyramid SCUs), the
SCUs they find for the initial summaries are usually
those of the highest weight, i.e. encoding informa-
tion that is the most essential to the topic.
SCU distribution in human summaries is also in-
teresting: Health, Resources, and Trials all have
the expected pyramid shape, with many low-weight
SCUs at the base and few high-weight SCUs on top,
but for Attacks and Accidents, the usual pattern is
broken and we see an hourglass shape instead, re-
flecting the presence of many weight-4 SCUs. The
most likely explanation is that these two categories
are guided by a relatively long list of aspects (cf.
Table 1), many of which have unique answers in the
source text.
This is shown in more detail in Table 5, which
presents aspect coverage by Pyramids and top 15
automatic summarizers in terms of an average num-
ber of SCUs relevant to a given aspect and an aver-
age weight of an aspect-related SCU. Only Attack
and Accidents have aspects that tend to generate the
same answers from almost all human summarizers:
when, where in Accidents and what, when, where,
perpetrators, and who affected in Attacks all have
average weight of around 3. The patterns hold for
update summaries; although all values decrease and
5We chose to use the top 15 out of 43 participating systems
in order to exclude outliers like systems that returned empty
summaries, and to measure the state-of-the-art in the summa-
rization field.
SCU
weight initial update
pyramids automatic pyramids automatic
A
cc
id
en
ts
4 6.4 3.2 1.9 0.5
3 3.7 1 3.43 0.8
2 6.9 1.6 6.1 0.6
1 7.9 0.8 7.6 0.7
total 24.9 7.7 19.1 3.1
A
tt
ac
ks
4 7.7 4.9 3.7 1
3 3.1 0.8 3.7 0.8
2 5 1 5.3 0.8
1 5.6 0.5 9.4 0.7
total 21.4 9.1 22.1 3.9
H
ea
lt
h
4 4.9 1.8 1.6 0.4
3 4.2 0.8 2.6 0.7
2 5.3 0.6 4.9 0.8
1 10.6 0.9 12 0.8
total 25 5 21 3
R
es
ou
rc
es
4 4.2 1.5 1.1 0.6
3 5.1 1.3 2.7 0.5
2 5 1 5.9 1
1 9.5 0.7 12.4 1
total 23.8 5 22.1 3.4
T
ri
al
s
4 4.4 2.6 3.4 1.2
3 5.7 2 3.3 0.5
2 7.8 1.6 5.7 0.6
1 9.2 0.5 8.5 0.6
total 27.1 8.5 20.9 3.3
Table 4: Macro-average SCU counts with weights 1?4 in
pyramids and matching SCU counts in automatic sum-
maries, for initial and update summaries.
there is less overlap between models, answers to
these aspects are the most likely to occur in multi-
ple summaries.
The situation for top 15 automatic summarizers
is even more interesting: while they contain rela-
tively few matching SCUs, the SCUs they do find
are those of high weight, as can be seen by compar-
ing their SCU weight averages. Even for ?other?,
which covers ?all other information important for
the topic? and is therefore more dependent on sum-
mary writer?s subjective judgment and shows more
content diversity, resulting in low-weight SCUs in
the Pyramid, the top automatic summarizers find
those most weighted. It would seem, then, that the
content selection methods are able to identify some
of the most important facts; at the same time, the
density of information in automatic summaries is
much lower than in human summaries, indicating
that the automatic content is either not compressed
adequately, or that it includes non-relevant or re-
peated information.
28
Avg SCU weight (avg SCU count)
initial summaries update summaries
Pyramids automatic Pyramids automatic
A
cc
id
en
ts
what 2.4 (4.4) 3.1 (1.9) 2.5 (2.7) 2.87 (0.6)
when 3.6 (2.1) 3.7 (0.7) 3.7 (0.4) 4 (0.1)
where 3.0 (3.6) 3.2 (1.3) 2.1 (1.1) 2.58 (0.4)
why 2.6 (2.3) 3.1 (0.5) 2.4 (2.0) 3 (0.3)
who aff 2.3 (4.9) 2.8 (1.5) 2.0 (4.1) 2.45 (0.6)
damages 1.8 (2.4) 3.1 (0.5) 1.7 (1.9) 2.05 (0.2)
counterm 2.1 (8.0) 2.7 (1.2) 2.0 (8.1) 2.4 (0.9)
other 1.3 (0.4) 1.9 (0.1) 1.3 (0.6) 1 (0.0)
A
tt
ac
ks
what 2.9 (3.1) 3.7 (1.6) 2.0 (1.4) 2.8 (0.4)
when 3.4 (1.3) 3.8 (0.4) 2.4 (1.4) 2.2 (0.1)
where 2.7 (2.9) 3.7 (1.2) 2.5 (0.9) 3.8 (0.3)
perpetr 2.8 (3.6) 3.4 (1.0) 2.2 (3.0) 3.0 (0.9)
why 2.1 (3.4) 2.8 (0.9) 1.8 (1.3) 1.6 (0.2)
who aff 3.3 (4.0) 3.6 (1.7) 2.0 (2.0) 2.1 (0.3)
damages 2.2 (0.9) 3.0 (0.2) 3.4 (0.7) 4.0 (0.1)
counterm 2.3 (4.3) 2.8 (1.1) 2.1 (10.3) 2.6 (1.1)
other 1.7 (1.3) 2.2 (0.1) 1.6 (2.6) 1.7 (0.2)
H
ea
lt
h
what 2.4 (6.0) 3.1 (1.6) 2.4 (2.9) 3.0 (0.7)
who aff 2.0 (5.6) 2.6 (0.8) 1.8 (2.7) 2.0 (0.3)
how 2.4 (6.6) 3.1 (1.1) 1.6 (2.7) 2.4 (0.3)
why 2.2 (3.9) 2.9 (0.6) 1.7 (2.3) 2.1 (0.4)
counterm 2.0 (6.3) 2.7 (0.8) 1.7 (10.4) 2.2 (1.0)
other 1.1 (0.6) 1.9 (0.1) 1.2 (1.9) 1.6 (0.2)
R
es
ou
rc
es
what 2.3 (3.2) 2.9 (1.3) 1.6 (1.4) 2.6 (0.4)
importan 2.4 (3.1) 2.7 (0.3) 1.8 (1.9) 2.3 (0.2)
threats 2.3 (7.6) 2.8 (1.6) 1.6 (6.8) 2.0 (1.1)
counterm 2.0 (10.1) 2.8 (1.7) 1.7 (12.1) 2.2 (1.4)
other 1.4 (0.7) 2.9 (0.1) 1.8 (1.2) 2.5 (0.1)
T
ri
al
s
who 2.7 (3.5) 3.2 (1.7) 2.7 (2.3) 3.2 (0.4)
who inv 1.9 (5.5) 2.8 (0.8) 1.8 (3.3) 2.6 (0.5)
why 2.6 (6.3) 3.1 (2.2) 1.8 (2.4) 2.3 (0.3)
charges 2.7 (2.4) 3.2 (0.8) 2.4 (1.4) 2.5 (0.3)
plead 2.0 (5.0) 2.9 (0.9) 2.1 (3.5) 3.0 (0.5)
sentence 2.3 (2.7) 3.0 (0.5) 2.6 (6.0) 3.5 (0.8)
other 1.5 (3.2) 2.0 (0.3) 1.7 (4.8) 2.4 (0.6)
Table 5: Aspect coverage for Pyramids and top 15 auto-
matic summarizers in TAC 2010.
3.3 Effect of categories and aspects
Some categories in the Guided Summarization task
are defined in more detail than others, depending
on types of stories they represent. Stories about at-
tacks and accidents (and, to some extent, trials) tend
to follow more predictable and detailed templates,
which results in more similar models and better re-
sults for automatic summarizers. Figure 1 gives a
graphic representation of the macro-average Pyra-
mid and Responsiveness scores for human and top
15 automatic summarizers, with exact scores in Ta-
bles 6 and 7, where the first score marked with a
letter is not statistically significant from any subse-
quent score marked with the same letter, according
to ANOVA (p>0.05). Lack of significant difference
between human Responsiveness scores in Table 6
suggests that, for all categories, human summaries
are highly and equally responsive, but a look at their
Pyramid scores confirms that Attacks and Accidents
models tend to have more overlapping information.
For automatic summaries, their Pyramid and Re-
sponsiveness patterns are parallel. Here Attacks,
Accidents, and Trials contain on average more
matching SCUs than Health and Resources, making
these summaries more responsive. One reason for
these differences might be that many systems rely on
sentence positon in the extraction process, and first
sentences in these template stories often are a short
description of event including date, location, persons
involved, in effect giving systems the unique-answer
aspects mentioned in Section 3.2. Table 5 shows
this distribution of matching information in more de-
tail: for Attacks and Accidents, automatic summa-
rizers match relatively more SCUs for what, where,
when, who affected than for countermeasures, dam-
ages, or other. For Trials, again the easier aspects
are those that tend to appear at the beginning of
documents: who [is under investigation] and why.
Stories in Health and Resources, the weakest cate-
gories overall for automatic summarizers and with
the greatest amount of variance for human summa-
rizers, are non-events, instead being closer to what
in past DUC tasks was described as a ?multi-event?
or ?single subject? story type. Individual documents
within the source set might sometimes follow the
typical event template (e.g. describing individual
instances of coral reef destruction), but in general
these categories require much more abstraction and
render the opening-sentence extraction strategy less
effective.
If the higher averages are really due to the infor-
mation extracted with first sentences, then we would
also expect higher scores from Baseline 1, which
simply selected the opening sentences of the most
recent document, up to the 100-word limit. And in-
deed, as shown in Table 8, the partial Pyramid scores
for Baseline 1 are the highest for exactly these ?con-
crete? categories and aspects, mostly for Attacks and
Accidents, and aspects such aswhere, what, andwho
(the score of 1 for Accidents other is an outlier, since
there was only one SCU relevant for this calcula-
tion and the baseline happened to match it). On the
other hand, its lowest performance is mostly con-
centrated in Health and Resources, and in the more
?vague? aspects, like why, how, importance, coun-
29
Pyramid Responsiveness
in
it
ia
l
Attacks 0.857 A Trials 4.825 A
Accidents 0.812 AB Accidents 4.821 AB
Resources 0.773 AB Attacks 4.786 ABC
Health 0.767 AB Health 4.750 ABCD
Trials 0.751 B Resources 4.650 ABCD
up
da
te
Trials 0.749 A Attack 4.857 A
Attacks 0.745 AB Trials 4.825 AB
Accidents 0.700 AB Accidents 4.714 ABC
Health 0.610 C Health 4.625 ABCD
Resources 0.604 C Resources 4.600 ABCD
Table 6: Macro-average Pyramid and Responsiveness
scores per category for human summaries, comparison
across categories.
Pyramid Responsiveness
in
it
ia
l
Attacks 0.524 A Attacks 3.400 A
Trials 0.446 B Accidents 3.362 AB
Accidents 0.418 B Trials 3.167 ABC
Resources 0.323 C Resources 2.893 CD
Health 0.290 C Health 2.617 D
up
da
te
Resources 0.286 A Resources 2.520 A
Trials 0.261 AB Health 2.417 AB
Attacks 0.251 ABC Trials 2.380 ABC
Health 0.236 BCD Attacks 2.286 ABCD
Accidents 0.228 BCD Accidents 2.248 ABCD
Table 7: Macro-average Pyramid and Responsiveness
scores per category for top 15 automatic summaries, com-
parison across categories.
termeasures, and other. We can conclude that early
sentence position is not a good predictor of such in-
formation, and that automatic summarizers might do
well to diversify their methods of content identifi-
cation based on what type of information they are
looking for.
3.4 Initial and update summaries
While the initial component is only guided by
the categories and aspects, the update component
is placed under an overarching condition of non-
redundancy. Update summaries should not repeat
Highest Lowest
Category Aspect score Category Aspect score
(Accidents Other 1) Resources other 0
Attacks WHERE 0.66 Health other 0
Attacks WHAT 0.66 Attacks COUNTERM 0
Trials WHO 0.6 Attacks other 0
Attacks WHO AFF 0.56 Accidents WHY 0
Accidents WHERE 0.44 Health WHO AFF 0
Accidents WHAT 0.41 Trials SENTENCE 0.06
Trials WHY 0.38 Health WHY 0.06
Attacks PERP 0.34 Accidents DAMAGES 0.07
Trials WHO INV 0.33 Health HOW 0.08
Trials CHARGES 0.33 Resources IMPORTAN 0.09
Table 8: Top Pyramid scores for Baseline 1, per aspect,
for initial summaries.
Figure 1: Macro-average Pyramid and Responsiveness
scores in initial and update summaries, for humans and
top 15 automatic systems. In each group, columns from
left: Accidents, Attacks, Health, Resources, Trials. As-
terisk indicates significant drop from initial score.
any information that can be found in the initial doc-
ument set. This restriction narrows the pool of po-
tential summary elements to choose from. More im-
portantly, since the concrete aspects with unique an-
swers like what, where, and when are likely to be
mentioned in the first set of document (and, by ex-
tension, in the initial summaries), this shifts content
selection to aspects that generate more variance, like
why, countermeasures, or other. As shown in Fig-
ure 1, while Responsiveness remains high for hu-
man summarizers across categories, which means
the content is still relevant to the topic, the Pyramid
scores are lower in the update component, which
means the summarizers differ more in terms of what
information they extract from the source documents.
Note that this is not the case for Trials, where the
human performance for both Responsiveness and
Pyramid is practically identical for initial and up-
date summaries. The time course of trials is gener-
ally longer than those for accidents and attacks, and
many of the later-occurring aspects such as plea and
sentence are well-defined; hence the initial and up-
date human summaries have similar Pyramid scores.
Automatic summarizers, on the other hand, suffer
the greatest drop in those categories in which they
were the most successful before: Attacks, Acci-
dents, and Trials, in effect rendering their perfor-
mance across categories more or less even (cf. Fig-
30
ure 1).
A closer look at the aspect coverage in initial and
update components confirms the differences in as-
pect distribution. Figure 2 gives four columns for
each aspect: the first two columns represent initial
summaries, the second two represent update sum-
maries. Dark columns in each pair are human sum-
marizers, light columns are top 15 automatic sum-
marizers. For almost all aspects, humans find fewer
relevant (and new!) facts in the update documents,
with the exception of sentence in Trials, and coun-
termeasures and other in all categories. Logically,
once all the anchoring information has been given
(date, time, location, event), the only remaining rel-
evant content to focus on are consequences of the
event (countermeasures, sentence), and possibly up-
dates in victims and damages (who affected, dam-
ages) as well as any other information that might be
relevant. A similar (though less consistent) pattern
holds for automatic summarizers.
4 Summary and conclusions
Initial attempts at more complex treatments of any
subject often fail when faced with unrestricted, ?real
world? input. This is why almost all research in
summarization remains centered around relatively
simple extractive methods. Few developers try to
incorporate syntactic parsing to compress summary
sentences, and almost none want to venture into se-
mantic decompositon of source text, since the com-
plexity of these methods is the cause of potential
errors. Also, the tools might not deal particularly
well with different types of stories in the ?newswire?
genre. However, Genest (2009) showed the limits
of purely extractive summarization: their manual,
extractive summarizer (HexTac) performed much
worse than human abstractors, and comparably to
the top automatic summarizers in TAC 2009.
But if we want to see significant progress in ab-
stractive summarization, it?s important to provide a
more controlled environment for such experiments.
TAC 2010 results show that, first of all, by guid-
ing summary creation we end up with more similar
human abstracts than in previous tasks (partly due
to the choice of template-like categories, and partly
due to the further guiding role of aspects). Narrow-
ing down possible summary content, while exclud-
Figure 2: Average number of SCUs per aspect in initial
and update summaries in TAC 2010. Dark grey = Pyra-
mids, light grey = top 15 automatic summarizers. The
first pair of columns for each aspects shows initial sum-
maries, the second pair shows update summaries.
31
ing variance due to subjective opinions among hu-
man writers, creates in effect a more concrete in-
formation model, and a single, unified information
model is an easier goal to emulate than relying on
vague and subjective goals like ?importance?. Out
of five categories, Attacks and Accidents generated
the most similar models, mostly because they re-
quired concrete, unique-answer aspects like where
or when. In Health and Resources, the aspects were
more subjective in nature, and the resulting variance
was greater.
Moreover, the Guided Task provides a very valu-
able and detailed diagnostic tool for system devel-
opers: by looking at the system performance within
each aspect, we can find out which types of infor-
mation it is better able to identify. While the top au-
tomatic summarizers managed to retrieve less than
half of relevant information at the best of times, the
facts they did retrieve were highly-weighted. Their
better performance for certain aspects of Attacks,
Accidents, and Trials could be ascribed to the fact
that most of them rely on sentence position to deter-
mine important information in the source document.
A comparison of covered aspects suggests that sen-
tence position might be a better indicator for some
types of information than others.
Since it was the first year of the Guided Task, only
some of the teams used the provided category/aspect
information; as the task continues, we hope to see
more participants adopting categories and aspects
to guide their summarization. The predictable el-
ements of each category invite the use of differ-
ent techniques depending on the type of informa-
tion sought, perhaps suggesting the use of Infor-
mation Extraction methods. Some categories might
be easier to process than others, but even if the
information-mining approach cannot be extended to
all types of stories, at worst we will end up with
better summarization for event-type stories, like at-
tacks, accidents, or trials, which together comprise a
large part of reported news.
References
Amit Bagga and Alan W. Biermann. 1997. Analyzing
the Complexity of a Domain With Respect To An In-
formation Extraction Task. Proceedings of the tenth
International Conference on Research on Computa-
tional Linguistics (ROCLING X), 175?194.
Pierre-Etienne Genest, Guy Lapalme, and Mehdi Yousfi-
Monod. 2009. HEXTAC: the Creation of a Manual
Extractive Run. Proceedings of the Text Analysis Con-
ference 2009.
Annie Louis and Ani Nenkova. 2009. Performance
confidence estimation for automatic summarization.
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, 541?548. Athens, Greece.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbia?s Newsblaster. Proceed-
ings of the Second International Conference on Hu-
man Language Technology Research, 280?285. San
Diego, California.
Ani Nenkova and Annie Louis. 2008. Can You Summa-
rize This? Identifying Correlates of Input Difficulty
for Multi-Document Summarization. Proceedings of
ACL-08: HLT, 825?833. Columbus, Ohio.
Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluat-
ing content selection in summarization: The Pyramid
method. Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, 145?
152. Boston, MA.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-
own, and Sergey Sigelman. 2005. Applying the Pyra-
mid method in DUC 2005. Proceedings of the 5th
Document Understanding Conference (DUC). Van-
couver, Canada.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):470?500.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. Multidocument
summarization via information extraction. 2001. Pro-
ceedings of the First International Conference on Hu-
man Language Technology Research, 1?7. San Diego,
California.
32
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1?9,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
An Assessment of the Accuracy of Automatic Evaluation in Summarization
Karolina Owczarzak
Information Access Division
National Institute of Standards and Technology
karolina.owczarzak@gmail.com
John M. Conroy
IDA Center for Computing Sciences
conroy@super.org
Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
hoa.dang@nist.gov
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
Automatic evaluation has greatly facilitated
system development in summarization. At the
same time, the use of automatic evaluation
has been viewed with mistrust by many, as its
accuracy and correct application are not well
understood. In this paper we provide an as-
sessment of the automatic evaluations used for
multi-document summarization of news. We
outline our recommendations about how any
evaluation, manual or automatic, should be
used to find statistically significant differences
between summarization systems. We identify
the reference automatic evaluation metrics?
ROUGE 1 and 2?that appear to best emu-
late human pyramid and responsiveness scores
on four years of NIST evaluations. We then
demonstrate the accuracy of these metrics in
reproducing human judgements about the rel-
ative content quality of pairs of systems and
present an empirical assessment of the rela-
tionship between statistically significant dif-
ferences between systems according to man-
ual evaluations, and the difference according
to automatic evaluations. Finally, we present a
case study of how new metrics should be com-
pared to the reference evaluation, as we search
for even more accurate automatic measures.
1 Introduction
Automatic evaluation of content selection in sum-
marization, particularly the ROUGE evaluation
toolkit (Lin and Hovy, 2003), has been enthusias-
tically adopted by researchers since its introduction
in 2003. It is now standardly used to report results in
publications; however we have a poor understanding
of the accuracy of automatic evaluation. How often
do we publish papers where we report an improve-
ment according to automatic evaluation, but never-
theless, a standard manual evaluation would have led
us to different conclusions? In our work we directly
address this question, and hope that our encouraging
findings contribute to a better understanding of the
strengths and shortcomings of automatic evaluation.
The aim of this paper is to give a better assessment
of the automatic evaluation metrics for content se-
lection standardly used in summarization research.
We perform our analyses on data from the 2008-
2011 Text Analysis Conference (TAC)1 organized
by the National Institute of Standards and Technol-
ogy (NIST). We choose these datasets because in
early evaluation initiatives, the protocol for manual
evaluation changed from year to year in search of
stable manual evaluation approaches (Over et al,
2007). Since 2008, however, the same evaluation
protocol has been applied by NIST assessors and we
consider it to be the model that automatic metrics
need to emulate.
We start our discussion by briefly presenting the
manual procedure for comparing systems (Section
2) and how these scores should be best used to iden-
tify significant differences between systems over a
given test set (Section 3). Then, we embark on our
discussion of the accuracy of automatic evaluation
and its ability to reproduce manual scoring.
To begin our analysis, we assess the accuracy of
common variants of ROUGE on the TAC 2008-2011
datasets (Section 4.1). There are two aspects of eval-
uation that we pay special attention to:
Significant difference Ideally, all system compar-
isons should be performed using a test for sta-
1http://www.nist.gov/tac/
1
tistical significance. As both manual metrics
and automatic metrics are noisy, a statistical
hypothesis test is needed to estimate the prob-
ability that the differences observed are what
would be expected if the systems are compa-
rable in their performance. When this proba-
bility is small (by convention 0.05 or less) we
reject the null hypothesis that the systems? per-
formance is comparable.
It is important to know if scoring a system via
an automatic metric will lead to conclusions
about the relative merits of two systems differ-
ent from what one would have concluded on the
basis of manual evaluation. We report very en-
couraging results, showing that automatic met-
rics rarely contradict manual metrics, and some
metrics never lead to contradictions. For com-
pleteness, given that most papers do not report
significance, we also compare the agreement
between manual and automatic metrics without
taking significance into account.
Type of comparison Established manual evalua-
tions have two highly desirable properties: (1)
they can tell apart good automatic systems from
bad automatic systems and (2) they can differ-
entiate automatic summaries from those pro-
duced by humans with high accuracy. Both
properties are essential. Obviously, choosing
the better system in development cycles is key
in eventually improving overall performance.
Being able to distinguish automatic from man-
ual summaries is a general sanity test 2 that any
evaluation adopted for wide use is expected to
pass?it is useless to report system improve-
ments when it appears that automatic methods
are as good as human performance3. As we will
see, there is no single ROUGE variant that has
both of these desirable properties.
Finally, in Section 5, we discuss ways to compare
other automatic evaluation protocols with the refer-
2For now, automatic systems do not have the performance
of humans, thus, the ability to distinguish between human and
automatically generated summaries is an exemplar of the wider
problem of distinguishing high quality summaries from others.
3Such anomalous findings, when using automatic evalua-
tion, have been reported for some summarization genres such
as summarization of meetings (Galley, 2006).
ence ROUGE metrics we have established. We de-
fine standard tests for significance that would iden-
tify evaluations that are significantly more accurate
than the current reference measures, thus warrant-
ing wider adoption for future system development
and reporting of results. As a case study we apply
these to the TAC AESOP (Automatically Evaluating
Summaries of Peers) task which called for the devel-
opment of novel evaluation techniques that are more
accurate than ROUGE evaluations.
2 Manual evaluation
Before automatic evaluation methods are developed,
it is necessary to establish a desirable manual eval-
uation which the automatic methods will need to re-
produce. The type of summarization task must also
be precisely specified?single- or multi-document
summarization, summarization of news, meetings,
academic articles, etc. Saying that an automatic
evaluation correlates highly with human judgement
in general, is disturbingly incomplete, as the same
automatic metric can predict some manual evalu-
ation scores for some summarization tasks well,
while giving poor correlation with other manual
scores for certain tasks (Lin, 2004; Liu and Liu,
2010).
In our work, we compare automatic metrics with
the manual methods used at TAC: Pyramid and Re-
sponsiveness. These manual metrics primarily aim
to assess if the content of the summary is appro-
priately chosen to include only important informa-
tion. They do not deal directly with the linguistic
quality of the summary?how grammatical are the
sentences or how well the information in the sum-
mary is organized. Subsequently, in the experiments
that we present in later sections, we do not address
the assessment of automatic evaluations of linguistic
quality (Pitler et al, 2010), but instead analyze the
performance of ROUGE and other related metrics
that aim to score summary content.
The Pyramid evaluation (Nenkova et al, 2007) re-
lies on multiple human-written gold-standard sum-
maries for the input. Annotators manually identify
shared content across the gold-standards regardless
of the specific phrasing used in each. The pyra-
mid score is based on the ?popularity? of informa-
tion in the gold-standards. Information that is shared
2
across several human gold-standards is given higher
weight when a summary is evaluated relative to the
gold-standard. Each evaluated summary is assigned
a score which indicates what fraction of the most
important information for a given summary size is
expressed in the summary, where importance is de-
termined by the overlap in content across the human
gold-standards.
The Responsiveness metric is defined for query-
focused summarization, where the user?s informa-
tion need is clearly stated in a short paragraph. In
this situation, the human assessors are presented
with the user query and a summary, and are asked
to assign a score that reflects to what extent the sum-
mary satisfies the user?s information need. There are
no human gold-standards, and the linguistic quality
of the summary is to some extent incorporated in the
score, because information that is presented in a con-
fusing manner may not be seen as relevant, while it
could be interpreted by the assessor more easily in
the presence of a human gold-standard. Given that
all standard automatic evaluation procedures com-
pare a summary with a set of human gold-standards,
it is reasonable to expect that they will be more accu-
rate in reproducing results from Pyramid evaluation
than results from Responsiveness judgements.
3 Comparing systems
Evaluation metrics are used to determine the rela-
tive quality of a summarization system in compari-
son to one or more systems, which is either another
automatic summarizer, or a human reference sum-
marizer. Any evaluation procedure assigns a score
to each summary. To identify which of the two sys-
tems is better, we could simply average the scores
of summaries produced by each system in the test
set, and compare these averages. This approach is
straightforward; however, it gives no indication of
the statistical significance of the difference between
the systems. In system development, engineers may
be willing to adopt new changes only if they lead
to significantly better performance that cannot be at-
tributed to chance.
Therefore, in order to define more precisely what
it means for a summarization system to be ?bet-
ter? than another for a given evaluation, we employ
statistical hypothesis testing comparisons of sum-
marization systems on the same set of documents.
Given an evaluation of two summarization systems
A and B we have the following:
Definition 1. We say a summarizer A ?signifi-
cantly outperforms? summarizer B for a given
evaluation score if the null hypothesis of the fol-
lowing paired test is rejected with 95% confidence.
Given two vectors of evaluation scores x and y,
sampled from the corresponding random vari-
ables X and Y, measuring the quality of sum-
marizer A and B, respectively, on the same col-
lection of document sets, with the median of x
greater than the median of y,
H0 : The median of X ? Y is 0.
Ha : The median of X ? Y is not 0.
We apply this test using human evaluation met-
rics, such as pyramid and responsiveness, as well as
automatic metrics. Thus, when comparing two sum-
marization systems we can, for example, say system
A significantly outperforms system B in responsive-
ness if the null hypothesis can be rejected. If the null
hypothesis cannot be rejected, we say system A does
not significantly perform differently than system B.
A complicating factor when the differences be-
tween systems are tested for significance, is that
some inputs are simply much harder to summarize
than others, and there is much variation in scores
that is not due to properties of the summarizers
that produced the summaries but rather properties of
the input text that are summarized (Nenkova, 2005;
Nenkova and Louis, 2008).
Given this variation in the data, the most appropri-
ate approach to assess significance in the difference
between system is to use paired rank tests such as
a paired Wilcoxon rank-sum test, which is equiva-
lent to the Mann-Whitney U test. In these tests, the
scores of the two systems are compared only for the
same input and ranks are used instead of the actual
difference in scores assigned by the evaluation pro-
cedures. Prior studies have shown that paired tests
for significance are indeed able to discover consid-
erably more significant differences between systems
than non-paired tests, in which the noise of input dif-
ficulty obscures the actual difference in system per-
3
formance (Rankel et al, 2011). For this paper, we
perform all testing using the Wilcoxon sign rank test.
4 How do we identify a good metric?
If we treat manual evaluation metrics as our gold
standard, then we require that a good automatic met-
ric mirrors the distinctions made by such a man-
ual metric. An automatic metric for summarization
evaluation should reliably predict how well a sum-
marization system would perform relative to other
summarizers if a human evaluation were performed
on the summaries. An automatic metric would hope
to answer the question:
Would summarizer A significantly outper-
form summarizer B when evaluated by a
human?
We address this question by evaluating how well
an automatic metric agrees with a human metric in
its judgements in the following cases:
? all comparisons between different summariza-
tion systems
? all comparisons between systems and human
summarizers.
Depending on the application, we may record the
counts of agreements and disagreements or we may
normalize these counts to estimate the probability
that an automatic evaluation metric will agree with a
human evaluation metric.
4.1 Which is the best ROUGE variant
In this section, we set out to identify which of the
most widely-used versions of ROUGE have highest
accuracy in reproducing human judgements about
the relative merits of pairs of systems. We exam-
ine ROUGE-1, ROUGE-2 and ROUGE-SU4. For
all experiments we use stemming and for each ver-
sion we test scores produced both with and without
removing stopwords. This corresponds to six differ-
ent versions of ROUGE that we examine in detail.
ROUGE outputs several scores including preci-
sion, recall, and an F-measure. However, the most
informative score appears to be recall as reported
when ROUGE was first introduced (Lin and Hovy,
2003). Given that in the data we work with, sum-
maries are produced for a specified length in word
s (and all summaries are truncated to the predefined
length), recall on the task does not allow for artifi-
cially high scores which would result by producing
a summary of excessive length.
The goal of our analysis is to identify which of the
ROUGE variants is most accurate in correctly pre-
dicting which of two participating systems is the bet-
ter one according to the manual pyramid and respon-
siveness scores. We use the data for topic-focused
summarization from the TAC summarization track
in 2008-20114.
Table 1 gives the overview of the 2008-2011 TAC
Summarization data, including the number of top-
ics and participants. For each topic there were four
reference (model) summaries, written by one of the
eight assessors; as a result, there were eight human
?summarizers,? but each produced summaries only
for half of the topics.
year topics automatic human references/
summarizers summarizers topic
2008 48 58 8 4
2009 44 55 8 4
2010 46 43 8 4
2011 44 50 8 4
Table 1: Data in TAC 2008-2011 Summarization track.
We compare each pair of participating systems
based on the manual evaluation score. For each pair,
we are interested in identifying the system that is
better. We consider both the case when an appropri-
ate test for statistical significance has been applied to
pick out the better system as well as the case where
simply the average scores of systems over the test set
are compared. The latter use of evaluations is most
common in research papers on summarization; how-
ever, in summarization system development, testing
for significance is important because a difference in
summarizer scores that is statistically significant is
much more likely to reflect a true difference in qual-
ity between the two systems.
Therefore, we look at agreement between
ROUGE and manual metrics in two ways:
? agreement about significant differences be-
tween summarizers, according to a paired
4In all these years systems also competed on producing up-
date summaries. We do not report results on this task for the
sake of simplifying the discussion.
4
Auto only Human-Automatic
Pyr Resp Pyr Resp
diff no diff contr diff no diff contr diff no diff contr diff no diff contr
r1m 91 59 0.85 87 51 1.34 91 75 0.06 91 100 0.45
r1ms 90 59 0.83 84 50 3.01 91 75 0.06 90 100 0.45
r2m 91 68 0.19 88 60 0.47 75 75 0.62 75 100 1.02
r2ms 88 72 0 84 62 0.65 73 75 1.56 72 100 1.95
r4m 91 64 0.62 87 56 0.91 82 75 0.43 82 100 0.83
r4ms 90 64 0.04 85 55 1.15 83 75 0.81 83 100 1.20
Table 2: Average percentage agreement between ROUGE and manual metrics about significant differences on TAC
2008-2011 data. r1 = ROUGE-1, r2 = ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; diff =
agreement on significant differences, no diff = agreement on lack of significant differences, contr = contradictions.
Auto only Human-Automatic
Pyr Resp Pyr Resp
metric sig all sig all sig all sig all
r1m 77 87 70 82 90 99 90 99
r1ms 77 88 69 80 90 98 90 98
r2m 81 89 75 83 75 94 75 94
r2ms 81 89 74 81 72 93 72 93
r4m 80 88 73 82 82 96 82 96
r4ms 79 89 71 81 83 96 83 96
Table 3: Average agreement between ROUGE and manual metrics on TAC 2008-2011 data. r1 = ROUGE-1, r2 =
ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; sig = agreement on significant differences, all
= agreement on all differences.
Wilcoxon test. No adjustments for multiple
comparisons are made.
? agreement about any differences between sum-
marizers (whether significant on not).
Agreements occur when the two evaluation met-
rics make the same distinction between System A
and System B: A is significantly better than B, A is
significantly worse than B, or A and B are not sig-
nificantly different from each other. Contradictions
occur when both metrics find a significant difference
between A and B, but in opposite directions; this is
a much more serious case than a mere lack of agree-
ment (i.e., when one metric says A and B are not
significantly different, and the other metric finds a
significant difference).
Table 2 shows the average percentage agreement
between ROUGE and Pyramid/Responsiveness
when it comes to identifying significant differences
or lack thereof. Column diff shows the recall
of significant differences between pairs of systems
(i.e., how many significant differences determined
by Pyramid/Responsiveness are found by ROUGE);
column no diff gives the recall of the cases where
there are no significant differences between two sys-
tems according to Pyramid/Responsiveness.
There are a few instances of contradictions, as
well, but their numbers are fairly small. ?Auto only?
refers to comparisons between automatic summariz-
ers only; ?Human-Automatic? refers to cases when
a human summarizer is compared to an automatic
summarizer. There are fewer human summarizers,
so there are fewer ?Human-Automatic? comparisons
than ?Auto only? ones.
There are a few exceptional cases where the hu-
man summarizer is not significantly better than the
automatic summarizers, even according to the man-
ual evaluation, which accounts for the uniform val-
ues in the ?no difference? column (this is proba-
bly because the comparison is performed for much
fewer test inputs).
Table 3 combines the number of agreements in
the ?difference? and ?no difference? columns from
Table 2 into the sig column, which shows accu-
racy: in checking system pairs for significant differ-
ences, in how many cases does ROUGE make the
same decision as the manual metric (there is/isn?t
a significant difference between A and B). Ta-
ble 3 also gives the number of agreements about
any differences between systems, not only those
that reached statistical significance; in other words,
agreements on system pairwise rankings. In both
5
tables we see that removing stopwords often de-
creases performance of ROUGE, although not al-
ways. Also, there is no clear winner in the ROUGE
comparison: while ROUGE-2 with stemming is the
best at distinguishing among automatic summariz-
ers, ROUGE-1 is the most accurate when it comes
to human?automatic comparisons. To reflect this,
we adopt both ROUGE-1 and ROUGE-2 (with stem-
ming, without removing stopwords) as our reference
automatic metrics for further comparisons.
Reporting pairwise accuracy of automatic evalua-
tion measures has several advantages over reporting
correlations between manual and automatic metrics.
In correlation analysis, we cannot obtain any sense
of how accurate the measure is in identifying statis-
tically significant differences. In addition, pairwise
accuracy is more interpretable than correlations and
gives some provisional indication about how likely
it is that we are drawing a wrong conclusion when
relying on automatic metric to report results.
Table 3 tells us that when statistical significance
is not taken into account, in 89% of cases ROUGE-
2 scores will lead to the same conclusion about the
relative merits of systems as the expensive Pyramid
evaluation. In 83% of cases the conclusions will
agree with the Responsiveness evaluation. The accu-
racy of identifying significant differences is worse,
dropping by about 10% for both Pyramid and Re-
sponsiveness.
Finally, we would like to get empirical estimates
of the relationship between the size of the difference
in ROUGE-2 scores between two systems and the
agreement between manual and ROUGE-2 evalua-
tion. The goal is to check if it is the case that if
one system scores higher than another by x ROUGE
points, then it would be safe to assume that a manual
evaluation would have led to the same conclusion.
Figure 1 shows a histogram of differences in
ROUGE-2 scores. The pairs for which this differ-
ence was significant are given in red and for those
where the difference is not significant are given in
blue. The histogram clearly shows that in general,
the size of improvement cannot be used to replace a
test for significance. Even for small differences in
ROUGE score (up to 0.007) there are about 15 pairs
out of 200 for which the difference is in fact signif-
icant according to Pyramid or Responsiveness. As
the difference in ROUGE-2 scores between the two
systems increases, there are more significant differ-
ences. For differences greater than 0.05, all differ-
ences are significant.
Figure 2 shows the histograms of differences in
ROUGE-2 scores, split into cases where the pairwise
ranking of systems according to ROUGE agrees
with manual evaluation (blue) and disagrees (red).
For score differences smaller than 0.013, about half
of the times ROUGE-2 would be wrong in identify-
ing which system in the pair is the better one accord-
ing to manual evaluations. For larger differences the
number of disagreements drops sharply. For this
dataset, a difference in ROUGE-2 scores of more
than 0.04 always corresponds to an improvement in
the same direction according to the manual metrics.
5 Looking for better metrics
In the preceding sections, we established that
ROUGE-2 is the best ROUGE variant for compar-
ing two automatic systems, and ROUGE-1 is best in
distinguishing between humans and machines. Ob-
viously, it is of great interest to develop even bet-
ter automatic evaluations. In this section, we out-
line a simple procedure for deciding if a new au-
tomatic evaluation is significantly better than a ref-
erence measure. For this purpose, we consider the
automatic metrics from the TAC 2011 AESOP task,
which called for the development of better automatic
metrics for summarization evaluation NIST ( 2011).
For each automatic evaluation metric, we estimate
the probability that it agrees with Pyramid or Re-
sponsiveness. Figure 3 gives the estimated proba-
bility of agreement with Pyramid and Overall Re-
sponsiveness for all AESOP 2011 metrics with an
agreement of 0.6 or more. The metrics are plot-
ted with error bars giving the 95% confidence in-
tervals for the probability of agreement with the
manual evaluations. The red-dashed line is the
performance of the reference automatic evaluation,
which is ROUGE-2 for machine only and ROUGE-
1 for comparing machines and human summariz-
ers. Metrics whose 95% confidence interval is be-
low this line are significantly worse (as measured
by the z-test approximation of a binomial test) than
the baseline. Conversely, those whose 95% con-
fidence interval is above the red line are signifi-
cantly better than the baseline. Thus, just ROUGE-
6
Figure 1: Histogram of the differences in ROUGE-2 score versus significant differences as determined by Pyramid
(left) or Responsiveness (right).
Figure 2: Histogram of the differences in ROUGE-2 score versus differences as determined by Pyramid (left) or
Responsiveness (right).
BE (the MINIPAR variant of ROUGE-BE), one of
NIST?s baselines for AESOP, significantly outper-
formed ROUGE-2 for predicting pyramid compar-
isons; and 4 metrics: ROUGE-BE, DemokritosGR2,
catholicasc1, and CLASSY1, all significantly out-
perform ROUGE-2 for predictiong responsiveness
comparisons. Descriptions of these metrics as well
as the other proposed metrics can be found in the
TAC 2011 proceedings (NIST, 2011).
Similarly, Figure 4 gives the estimated probabil-
ity when the comparison is made between human
and machine summarizers. Here, 10 metrics are sig-
nificantly better than ROUGE-1 in predicting com-
parisons between automatic summarization systems
and human summarizers in both pyramid and re-
sponsiveness. The ROUGE-SU4 and ROUGE-BE
baselines are not shown here but their performance
was approximately 57% and 46% respectively.
If we limit the comparisons to only those where
a significant difference was measured by Pyramid
and also Overall Responsiveness, we get the plots
given in Figure 5 for comparing automatic summa-
rization systems. (The corresponding plot for com-
parisons between machines and humans is omitted
as all differences are significant.) The results show
that there are 6 metrics that are significantly better
than ROUGE-2 for correctly predicting when a sig-
nificant difference in pyramid scores occurs, and 3
metrics that are significantly better than ROUGE-2
for correctly predicting when a significant difference
in responsiveness occurs.
6 Discussion
In this paper we provided a thorough assessment
of automatic evaluation in summarization of news.
We specifically aimed to identify the best variant
of ROUGE on several years of TAC data and dis-
covered that ROUGE-2 recall with stemming and
stopwords not removed, provides the best agreement
with manual evaluations. The results shed positive
light on the automatic evaluation, as we find that
ROUGE-2 agrees with manual evaluation in almost
90% of the case when statistical significance is not
computed, and about 80% when it is. However,
these numbers are computed in a situation where
many very different systems are compared?some
7
Figure 3: Pyramid and Responsiveness Agreement of AESOP 2011 Metrics for automatic summarizers.
Figure 4: Pyramid and Responsiveness Significant Difference Agreement of AESOP 2011 Metrics for all summarizers.
8
Figure 5: Pyramid and Responsiveness Significant Difference Agreement of AESOP 2011 Metrics for automatic
summarizers.
very good, others bad. We examine the size of dif-
ference in ROUGE score and identify that for differ-
ences less than 0.013 a large fraction of the conclu-
sions drawn by automatic evaluation will contradict
the conclusion drawn by a manual evaluation. Fu-
ture studies should be more mindful of these find-
ings when reporting results.
Finally, we compare several alternative automatic
evaluation measures with the reference ROUGE
variants. We discover that many new proposals are
better than ROUGE in distinguishing human sum-
maries from machine summaries, but most are the
same or worse in evaluating systems. The Basic El-
ements evaluation (ROUGE-BE) appears to be the
strongest contender for an automatic evaluation to
augment or replace the current reference.
References
Paul Over and Hoa Dang and Donna Harman. 2007.
DUC in context. Inf. Process. Manage. 43(6), 1506?
1520.
Chin-Yew Lin and Eduard H. Hovy. 2003. Auto-
matic Evaluation of Summaries Using N-gram Co-
occurrence Statistics. Proceeding of HLT-NAACL.
Michel Galley. 2006. A Skip-Chain Conditional Ran-
dom Field for Ranking Meeting Utterances by Impor-
tance. Proceeding of EMNLP, 364?372.
Feifan Liu and Yang Liu. 2010. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. Trans. Audio, Speech and Lang. Proc.,
187?196.
C.Y. Lin. 2004. Looking for a Few Good Metrics: Au-
tomatic Summarization Evaluation - How Many Sam-
ples are Enough? Proceedings of the NTCIR Work-
shop 4.
Ani Nenkova and Rebecca J. Passonneau and Kathleen
McKeown. 2007. The Pyramid Method: Incorporat-
ing human content selection variation in summariza-
tion evaluation. TSLP 4(2).
Emily Pitler and Annie Louis and Ani Nenkova. 2010.
Automatic Evaluation of Linguistic Quality in Multi-
Document Summarization. Proceedings of ACL, 544?
554.
Ani Nenkova. 2005. Automatic Text Summarization of
Newswire: Lessons Learned from the Document Un-
derstanding Conference. AAAI, 1436?1441.
Ani Nenkova and Annie Louis. 2008. Can You Summa-
rize This? Identifying Correlates of Input Difficulty
for Multi-Document Summarization. ACL, 825?833.
Peter Rankel and John M. Conroy and Eric Slud and Di-
anne P. O?Leary. 2011. Ranking Human and Machine
Summarization Systems. Proceedings of EMNLP,
467?473.
National Institute of Standards and Technology.
2011. Text Analysis Workshop Proceedings
http://www.nist.gov/tac/publications/index.html.
9
Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 63?71,
Dublin, Ireland, August 24 2014.
Wordsyoudontknow: Evaluation of lexicon-based decompounding 
with unknown handling 
 
 
Karolina Owczarzak Ferdinand de Haan George Krupka Don Hindle 
 
Oracle Language Technology 
1111 19th Street NW #600, Washington, DC 20036, USA 
{karolina.owczarzak,ferdinand.de.haan,george.krupka, 
don.hindle}@oracle.com 
 
Abstract 
In this paper we present a cross-linguistic evaluation of a lexicon-based decomposition method 
for decompounding, augmented with a ?guesser? for unknown components. Using a gold 
standard test set, for which the correct decompositions are known, we optimize the method?s 
parameters and show correlations between each parameter and the resulting scores. The results 
show that even with optimal parameter settings, the performance on compounds with unknown 
elements is low in terms of matching the expected lemma components, but much higher in 
terms of correct string segmentation. 
1 Introduction 
Compounding is a productive process that creates new words by combining existing words together in 
a single string. It is predominant in Germanic and Scandinavian languages, but is also present in other 
languages, e.g. Finnish, Korean, or Farsi. Many languages that are not usually thought of as 
?compounding? nevertheless display marginal presence of compounds, restricted, for instance, to 
numerical expressions (e.g. Polish czterogodzinny ?four-hour?). Depending on a language, 
compounding can be a very frequent and productive process, in effect making it impossible to list all 
the compound words in the dictionary. This creates serious challenges for Natural Language 
Processing in many areas, including search, Machine Translation, information retrieval and related 
disciplines that rely on matching multiple occurrences of words to the same underlying representation.  
In this paper, we present a cross-linguistic evaluation of a lexicon-based decomposition method 
augmented with a ?guesser? for handling unknown components. We use existing lexicons developed 
at Oracle Language Technology in combination with a string scanner parametrized with language-
specific input/output settings. Our focus is on the evaluation that tries to tease apart string 
segmentation (i.e. finding boundaries between components) and morphological analysis (i.e. matching 
component parts to known lemmas). 
The paper is organized as follows: Section 2 gives an overview of related research; Section 3 
describes the compound analyzer used in our experiments; Section 4 presents experimental results; 
Section 5 contains error analysis and discussion. Section 6 concludes and suggests future research. 
2 Related research 
Current research on compound splitting is predominantly lexicon-based, with a range of selection 
methods to choose the most likely decomposition. The lexicons used to identify components are 
usually collected from large monolingual corpora (Larson et al., 2000; Monz and de Rijke, 2001; 
Alfonseca et al, 2008; Holz and Biemann, 2008; von Huyssteen and von Zaanen, 2004).  
The problem with pure lexicon-based approach without any constraints is that it will produce many 
spurious decompositions, matching small substrings that happen to be legitimate words in the 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
63
language. Therefore, some approaches introduce maximizing component length (or, conversely, 
minimizing the number of components) as one of the selection factors (von Huyssteen and von 
Zaanen, 2004; Holz and Biemann, 2008; Macherey et al., 2011; Larson et al., 2000); others use part of 
speech to eliminate short components which tend to be function words (Koehn and Knight, 2003; 
Monz and de Rijke, 2001). In other cases, Named Entity Recognition is used to filter out proper names 
that should not be decomposed but that can contain frequent short components like ?-berg? or ?-dorf? 
(Alfonseca et al., 2008).  
Even after removing unlikely small component candidates, there is enough ambiguity in 
decomposition to warrant further filtering methods. And so, approaches related to Machine Translation 
use bilingual parallel corpora to find the most likely components by checking whether their 
translations match elements of the whole compound translation (Koehn and Knight, 2003; Macherey 
et al., 2011). Other filtering methods are based on combined frequency of the components (Koehn and 
Knight, 2003; Holz and Biemann, 2008), point-wise mutual information of components, or occurrence 
of components in related locations, such as anchor text (Alfonseca et al., 2008). A very interesting 
lexicon-free approach is presented in Aussems et al. (2013), which uses point-wise mutual information 
to detect likely boundaries between characters that would identify a compound. 
A major issue with the current research is the absence of common training and testing data, 
particularly across multiple languages, which then translates into limited evaluations of presented 
methods. Using pre-annotated frequency lists, we create gold standard test sets for 10 languages: 
Norwegian, Danish, Dutch, Estonian, Finnish, German, Hungarian, Korean, Farsi, Swedish, which 
range from around 600 to 15,000 compounds. This allows a more thorough comparison of the analyser 
performance across different languages.  
3 Lexicon-based analyzer 
Our approach follows the main line of research in that it uses lexicons to identify potential components 
in a compound; however, our lexicons contain lemmas rather than word forms, in contrast to lexicons 
harvested from monolingual corpora. However, the lexicons we use contain as well as partial lemmas 
whose occurrences are restricted to compounds (e.g. German verb forms without the final ?en; for 
example schlie?-). In addition, we use morphological rules to map recognized inflected forms to base 
(lexicon) lemmas. Both the lexicons and the morphological rules have been previously created by 
computational linguists and native speakers for use in a variety of NLP applications at Oracle 
Language Technology.   
On the most basic level, a compound can be explicitly added to the lexicon, with a specific 
decomposition and appropriate part of speech and grammatical features; this option is used when the 
decomposition is irregular or non-obvious, for instance when the component appears in a form that is 
not directly analyzable to its lemma, as in the example below, which shows irregular plurals and 
deletion of consonant: 
 
(1) a. Danish: barn ?child? plural: b?rn 
b?rnebog barn+e+bog [child-connector-book] ?children?s book? 
b. Norwegian Bokm?l: deletion of repeated consonant 
musikkorps musikk+korps [music-band] ?music band? 
 
Lexicalized compounds are treated like any other words, and their inflected forms will be 
recognized. Explicitly adding the compound to the lexicon is also useful when the compound can have 
multiple decompositions, and we want to restrict the output only to the semantically correct analysis. 
In Dutch, for instance, the compound part stem can refer to the noun stem ?voice? or to the root of the 
verb stemmen ?vote?. These readings are distinguished in the lexicon by listing explicit decompositions 
for compounds that contain the part:  
 
(2) Dutch stem N vs. V 
a. stemband stem#band  [voice-cord]  ?vocal cord? (N-N)  
b. stembureau stemmen#bureau [vote-station] ?polling station (V-N) 
 
64
However, adding all compounds to the lexicon is simply unfeasible for many languages where the 
compounding process is highly productive. For this reason, we also use a compound analyser to 
identify components in a dynamic manner, based on available component lemmas in the lexicon. 
Components are found by removing any recognizable inflections from the candidate string, scanning it 
left-to-right, and looking for all matching lemmas, subject to constraints based on part of speech, 
length, number of components, and available features. For speed reasons, we apply greedy matching, 
and prefer decompositions with the longest prefix and the smallest number of components. 
Since our goal is developing language processing systems that are as universal as possible, leaving 
context-dependent decisions to higher-level applications, we are not particularly concerned with 
always selecting the single best decomposition for a compound, since in many cases is will be 
dependent on the domain and application. However, it is useful to filter out decompositions that would 
be highly unlikely in any context, for instance those containing small function words mentioned in 
previous section. For this purpose, we apply constraints described below. 
3.1 Rules for compound sequences 
For each language, we list the possible part of speech sequences that can appear in compounds. These 
rules serve not only to prevent the decompositions that would not appear in the language (for instance, 
noun-verb-particle), but also to restrict sequences that are fairly infrequent, but that would lead to 
considerable over-generation if they were added. For example, in German, there are relatively few 
compounds that end with a verb, unless it is a combination of movable prefix particle (aus, an, ab, ein, 
etc.) and the verb (aus+gehen, auf+stehen, um+steigen, etc.). These verbs are functionally analyzed as 
compounds, i.e. a concatenation of two lemmas. However, since sequences noun/adjective/verb + verb 
are much less productive (spazieren+gehen, auto+fahren), it is more efficient to restrict the verb-final 
compounds to particle-verb only, and add the exceptions to the lexicon. A few examples of compound 
part of speech sequences for different languages are shown in (3). 
 
(3) a. Dutch:  
 cardinal_number + verb e.g., vier+en+delen ?quarter? 
b. Estonian:  
noun+adjective  e.g. silmi+pimestav ?eye-dazzling? 
c. German:  
ordinal_number + adjective e.g. zweit+gr??t ?second largest? 
d. Swedish:  
noun + noun   e.g citron+saft ?lemon juice? 
 
Another issue is compounds of cardinal or ordinal numbers, which can also occur in some 
languages like Italian (cinquecento+sessanta+nove ?five hundred sixty nine?) or Greek (??????????, 
???? + ??????? ?eight hundred?). These number compounds can be very productive and are also 
included in the lists of allowed compound sequences.   
3.2 Connectors 
In many compounding languages, the subparts of a compound can be connected with extra material, a 
connector (or linking element). These are semantically empty elements that have a mainly 
phonological role in connecting the compound parts (Bauer, 2009). In many Germanic languages 
connectors are derived from plural or genitive morphemes (such as ?er or ?s in German), but do not 
have this role any more, as evidenced, among others, by the fact that in certain cases the connector is 
optional and compounds with and without a connector co-exist (4a) or by the fact that there are cases 
where two different connectors co-occur (4b) (Krott et al., 2007): 
 
(4) a. Norwegian Bokm?l: 
rettssak rett + s + sak ?court case? 
rettsak rett + ? + sak 
b. Dutch: 
paddestoel pad + e + stoel ?toadstool? 
paddenstoel pad + en + stoel 
65
 
For each language, we create a set of allowed connectors, a few examples of which can be seen in 
(5).1 Note that it might be useful to restrict certain connectors to appear only in certain sequences (e.g. 
between noun and noun, but not adjective and verb); we plan to implement this restriction in future 
work. 
 
(5) Connector examples 
a. Dutch  s e.g. water+s+nood ?flood? 
b. German zu e.g. to match auf+stehen and auf+zu+stehen ?stand up? 
c. Swedish o e.g. veck+o+slut ?weekend? 
3.3 Decompounding settings 
Another factor in successful dynamic decompounding is restrictions on possible number of 
components, and on length of candidate strings and candidate components. Choosing to allow fewer 
components of longer length helps to prevent spurious over-analysis, where several short words can 
accidentally match the string which is being analyzed. However, setting the limits too high might also 
prevent legitimate decomposition, so this trade-off needs to be carefully balanced. There are four basic 
length settings, as shown in Table 1 below; the values are dependent on language. 
Maximum number of elements: Limits the number of components in a compound. Low values help 
prevent spurious decompositions into many small elements. 
Minimum length of compound: The minimum length of string that should be subject to 
decompounding; short strings are unlikely to be compounds, so for efficiency reasons, they are not 
decompounded. 
Minimum length of component: Specifies the minimum length of potential compound elements; 
shorter substrings are excluded to avoid accidental matching of very short words. 
Minimum length of component with connector: A version of the above setting, it specifies the 
minimum length of potential element when this element is next to a connector; to avoid spurious 
matches of the short word + connector combination (e.g. Dutch paspoort should be decomposed as 
pas+poort, not pa+s+poort).    
 
setting  value 
maximum number of elements 2-4 
minimum length of compound 4-11 
minimum length of component 2-4 
minimum length of component with connector 2-4 
 
Table 1. Length settings for dynamic decompounding. 
 
The values for these settings are established manually and separately for each language, based on 
review of top N most frequent compounds in the lexicon and the general knowledge of that language?s 
grammar and patterns. 
4 Experimental results 
Despite all the constraints and settings described above, decompounding is still an imperfect process: 
there can be multiple competing (i.e. overlapping) decompositions, and many decompositions that are 
technically possible are incorrect due to semantic reasons. This problem becomes even more 
challenging when some of the components are not present in the lexicon. Since lexicons are limited, 
and real world text can contain misspellings, proper names, or obscure words, we need to address the 
issue of decompounding with unknown elements. Therefore, we set out to evaluate the performance of 
our lexicon-based method on a gold standard set of known compounds, and compare it to an 
augmented version that also tries to construct potential components from unknown substrings. 
                                                          
1 Note that for our purposes, particle zu in German is also treated as a connector, to match the movable particle verbs that can 
appear with and without zu: auf + zu + stehen and auf + stehen ?get up?. 
66
4.1 Test set  
For our experiments, we collected compounds from the top 90% frequency lists based on large 
news and Wikipedia corpora. Each compound was annotated with the correct decomposition(s) by a 
linguist who was also a native speaker of the target language according to simple instructions: if the 
meaning of the word is compositional (i.e. can be fully described by the component elements), treat it 
as a compound and provide component lemmas.  
Approximate sizes of source corpora per language are given in Table 2; column ?compounds? 
shows the count of compounds; column ?lexical? shows how many of these are lexicalized compounds 
(i.e. compounds that have been added to the lexicon for reasons of irregularity). While two-part 
compounds are by far the most frequent in all the languages we examined, there is also some 
percentage of compounds with more than two parts; the distribution is shown in the last four columns. 
 
language 
news 
corpus 
MB 
wiki 
corpus 
MB 
compounds lexical 2-part 3-part 4-part 5-part 
Danish 335 154 1,982 1,326 1,856 122 4 0 
Dutch 512 103 3,439 1,909 3,186 245 8 0 
Estonian 204 41 2,343 562 2,166 169 8 0 
Farsi 512 244 648 340 635 13 0 0 
Finnish 512 78 1,868 1,665 1,703 154 11 0 
German 520 227 15,490 5,087 14,544 915 31 0 
Hungarian 512 257 1,841 1,537 1,794 45 2 0 
Korean 826 190 11,398 4,774 10,919 425 39 5 
Norwegian 512 88 3,582 1,106 3,405 175 2 0 
Swedish 512 204 9,677 5,608 8,901 744 31 5 
 
Table 2. Size of corpora per language, count of compounds, distribution of parts. 
 
4.2 Dynamic decompounding with available lemmas 
As mentioned before, it is not feasible to add all (or even the majority) of possible compounds, so we 
need to examine our performance using only dynamic decompounding. For this purpose, we removed 
all lexicalized compounds from the lexicon, and then ran the analyzer on the compound test set 
described above. This means that all the compound analysis was done dynamically, using only the 
available simple lemmas and compound rules and length restrictions. Table 3 shows the results. The 
scores for lexicalized + dynamic decompounding are given only for reference; they are high but less 
interesting, since they reflect the fact that the lexicalized compounds were largely collected from the 
same corpora (among other sources). Our focus is on the dynamic scores, which show performance on 
unknown compounds assuming a nearly ?perfect? lexicon that contains almost all the component 
lemmas. As such, these scores will serve as the upper bound for our next experiment, in which we 
remove at least one of the component lemmas from the lexicon and test the resulting performance.   
As can be seen in Table 3, for most languages recall decreases considerably ? this suggests that 
lexicalized compounds are of the kind that are not covered by the decompounding rules or whose 
correct analysis is blocked by another decomposition.  
4.3 Dynamic decompounding with missing lemmas 
While dynamic decompounding can handle the productive nature of compounds, it is still limited to 
finding components that are already present in the lexicon. However, in the real world compounds will 
contain elements unknown to a lexicon-based analyzer, whether it is because they are domain-specific 
vocabulary, proper names, foreign borrowings, or misspellings. In those cases, it is still useful to 
attempt analysis and return the known parts, with the option of returning the unknown substring as the 
missing lemma. 
67
   lexicalized + dynamic dynamic only 
  prec rec f-score prec rec f-score 
Danish 98.18 99.6 98.88 87.99 66.9 76.01 
Dutch 98.84 100 99.42 84.46 80.49 82.43 
Estonian 98.25 99.83 99.03 95.69 90.27 92.9 
Farsi 92.9 100 96.32 65.75 72.84 69.11 
Finnish 98.74 100 99.37 84.55 68.63 75.76 
German 96.11 99.98 98.01 88.01 89.03 88.52 
Hungarian 90.44 99.84 94.91 77.42 72.19 74.71 
Korean 99.72 100 99.86 95.23 59.49 73.23 
Norwegian 99.6 100 99.8 93.25 86.32 89.65 
Swedish 96.35 99.88 98.08 86.67 75.75 80.84 
 
Table 3. Precision, recall, and f-measure for dynamic decompounding. 
 
To evaluate the performance of our analyzer in case where some component lemmas are unknown, 
we applied a ?compound guesser? function that tries to find known elements of unknown compounds, 
even if a complete decomposition to only known elements is impossible. The guesser has its own 
constraints, independent of the main compound analyzer, which are shown in Table 4. 
 
 
setting  value 
maximum number of elements 2-20 
minimum length of compound 3-20 
minimum length of component 2-5 
minimum length of unknown element 1-5 
minimum percent of string covered 0-100% 
 
Table 4. Settings for dynamic decompounding with unknown elements. 
 
The first three settings are parallel to the settings for regular dynamic decompounding; however, we 
also add restrictions on length for unknown elements (minimum length of unknown element) and total 
string coverage (minimum percent of string covered). Restriction on length of unknown element mean 
that any unknown string shorter than the minimum length will be treated as a potential 
connector/suffix/prefix and will not be returned as a lemma: 
 
(6) German: assuming freundlicher ?friendlier? is unknown: 
umweltfreundlicher -> umwelt + freundlich (! + er) [environment + friendly]  
 
The last setting allows a more fine-grained control over the proportion of known to unknown parts; 
however, since any value less than 100% will restrict the number of produced candidate 
decompositions, resulting in no output if the unknown substring is too long, we do not test the impact 
of this setting.  
For this experiment, we collected all component lemmas from the test compounds, and removed 
from lexicon at least one component lemma per compound. This renders the whole string 
unanalyzable by regular means. Then we ran the compound guesser with each combination of settings 
from Table 4, to find the optimal set of values.  
Table 5 shows results obtained with the optimal guesser settings per language, compared to scores 
from Table 3: a fully functional decomposition that has access to both dynamic decomposition and 
lexicalized compounds, and dynamic decomposition with near-perfect component lexicon. It is clear 
68
that even with optimal settings, the guesser performance falls well below the level of full functionality, 
even when we compare to a system that has no access to lexicalized compounds. The highest score 
achieved by the guesser is 34 for the Hungarian test set, which includes mostly simple two-part 
compounds, and where the lexicon does not provide too many spurious sub-matches.  
 
language lexical + dynamic 
dynamic 
only 
dynamic 
guesser 
dynamic guesser - 
string segmentation 
Danish 98.88 76.01 25.93 51.25 
Dutch 99.42 82.43 27.13 64.01 
Estonian 99.03 92.9 9.56 53.89 
Farsi 96.32 69.11 27.16 78.68 
Finnish 99.37 75.76 19.49 51.6 
German 98.01 88.52 25.1 52.29 
Hungarian 94.91 74.71 34 53.5 
Korean 99.86 73.23 16.81 76.54 
Norwegian 99.8 89.65 22.56 49.74 
Swedish 98.08 80.84 25.56 54.18 
Table 5. Dynamic decomposition with missing lemmas, optimal settings; string segmentation shows 
accuracy score; remaining values are harmonic f-score of precision and recall. 
 
However, a major problem with this evaluation is that output of the regular decompounding process 
produces lemmas in their dictionary form, without inflection, whereas the guesser can only return 
surface strings for the unknown elements which might carry grammatical inflection or stem 
alternations. Therefore, it would be more fair to compare the guesser to dynamic decompounding in 
terms of pure string segmentation ? whether it finds the same boundaries between components, 
without concern for the form of the returned component. This lets us tease apart the impact of finding 
component elements from the impact of morphology. The last column in Table 5 shows accuracy of 
guesser string segmentation as compared to string segmentation performed by regular dynamic 
decompounding; in this respect the guesser?s performance is indeed much better. These results are 
encouraging, showing that we can recover correct components in up to 79% of cases, which is a very 
useful improvement for the purposes of information retrieval and search. While some recall is lost by 
returning strings instead of lemmas, we are planning to add a second step that would employ a lemma 
?guesser?, in order to produce the most likely dictionary form from the recovered unknown string. 
 
language 
max 
elements 
corr. 
with 
score 
min 
length of 
compound 
corr. 
with 
score 
min 
length 
of 
element 
corr. 
with 
score 
min 
length of 
unknown 
element 
corr. 
with 
score 
Danish 2 -0.19 3-7 -0.46 4 0.43 3 -0.09 
Dutch 2 -0.21 8 -0.47 5 0.51 3 -0.06 
Estonian 2 -0.15 3-7 -0.57 4 0.33 3 -0.08 
Farsi 2 -0.17 3-5 -0.51 3 0.11 2 -0.24 
Finnish 2-10 -0.07 3-8 -0.49 5 0.41 3 0 
German 2 -0.26 8 -0.43 5 0.5 3 -0.06 
Hungarian 2-16 0 1-6 -0.58 4 0.39 2 -0.33 
Korean 2-10 -0.07 3 -0.14 2 -0.07 1 -0.11 
Norwegian 2-10 -0.18 3-8 -0.61 5 0.56 3 0.01 
Swedish 2 -0.23 7 -0.45 4 0.49 3 -0.04 
Average   -0.15   -0.47   0.37   -0.1 
Table 6. Optimal guesser settings and their correlations of settings with the guesser score. 
69
 
Finally, Table 6 shows the correlation (Pearson?s r) of guesser settings (or their ranges) and the 
resulting scores. As can be seen, the strongest correlation holds for the minimum length of compound 
(average -0.47) and minimum length of element (0.37). In the former case, the correlation is inverse, 
which means the higher the value, the lower the final score; this is caused by the fact that our test set 
contains only compounds, so returning the whole unsplit string will never be the right result. The 
second correlation reflects the fact that it is safer to exclude very short elements from appearing as 
components, a finding that confirms earlier research.  
5 Error analysis 
A considerable percentage of mismatch errors when guessing the unknown components of 
compounds is caused by the connectors. Our current guesser settings return the whole unknown string, 
without attempting to identify any potential connectors on its edges. This seems like an obvious area 
for improvement, as it would let us return more correct decompositions for cases shown in Table 7 
(unknown strings are enclosed in square brackets and are currently returned whole).  
 
language token dynamic guesser translation 
Norwegian kj?rlighetsbrev kj?rlighet#brev kj?rlighet#[s + brev] love letter 
Danish ungdomshus ungdom#hus ungdom#[s + hus] youth 
German sklavenmoral sklave#moral sklave#[n + moral] slave morality 
Swedish kvinnof?rbund kvinna#f?rbund kvinn#[o + f?rbund] women's alliance 
Table 7. Examples of connector mismatches between dynamic decompounding and the guesser. 
 
As could be expected, most errors are nevertheless caused by the guesser splitting unknown strings 
into smaller known chunks; several typical examples are shown in Table 8. 
 
language token dynamic guesser translation 
Danish popul?rkulturen popul?r#kultur popul?r#kult#uren popular culture 
Dutch kunstschilders kunst#schilder kunst#schil#ders painters 
Finnish rockmuusikot rock#muusiko rock#muusi#kot rock music 
Swedish radioversion radio#version radio#vers#ion radio  version 
Table 8. Examples of incorrect splitting of unknown strings. 
 
6 Conclusion and future work 
In this paper, we have shown a dictionary-based compound analyzer, augmented with the function to 
handle unknown substrings. A cross-linguistic evaluation against the gold standard containing 
component lemmas shows that the correct handling of unknown compound elements is a difficult issue 
especially if we try to match dictionary lemmas; however, a more detailed evaluation of the string 
segmentation and boundary detection shows fairly good results. Being able to decompose unknown 
compounds and match the components to known lemmas to increase recall is crucial to many NLP 
applications, such as information retrieval or Machine Translation. A correct segmentation is of 
fundamental importance, but the question remains how we can match the unknown, possibly inflected, 
substring to known lemmas. In the future, we plan to address this question by (1) adding the option to 
separate out connectors from unknown strings, and (2) build a lemma ?guesser? that would try to 
construct a probable dictionary representation for the unknown string, in effect building a pipeline that 
would more fully mirror the process of regular dynamic decompounding. 
 
Acknowledgements  
We would like to thank the rest of the Oracle Language Technology team, in particular Elena Spivak 
and Rattima Nitisaroj, for their help with compound examples. 
 
70
References 
Alfonseca, Enrique, Slaven Bilac and Stefan Pharies. 2008. German Decompounding in a Difficult Corpus. In 
Computational Linguistics and Intelligent Text Processing, A. Gelbukh (ed.). Springer Verlag, Berlin and 
Heidelberg, 128-139.  
Aussems, Suzanne., Bas Goris., Vincent Lichtenberg, Nanne van Noord, Rick Smetser, and Menno van Zaanen. 
2013. Unsupervised identification of compounds. In Proceedings of the 22nd Belgian-Dutch conference on 
machine learning, A. van den Bosch, T. Heskes, & D. van Leeuwen (Eds.), Nijmegen, 18-25. 
Bauer, Laurie. 2009. Typology of Compounds. In The Oxford Handbook of Compounding, Rochelle Lieber and 
Pavol ?tekauer (eds.). Oxford University Press, Oxford.343-356. 
Holz, Florian and Chris Biemann. 2008. Unsupervised and Knowledge-Free Learning of Compound Splits and 
Periphrases. CICLing'08 Proceedings of the 9th international conference on Computational linguistics and 
intelligent text processing, A. Gelbukh (ed.). Springer Verlag, Berlin and Heidelberg, 117-127.  
Koehn, Philipp and Kevin Knight. 2003. Empirical Methods for Compound Splitting.  Proceedings of the 10th 
conference of the European Chapter of the Association for Computational Linguistics, Vol. 1, 187-193. 
Krott, Andrea, Robert Schreuder, R. Harald Baayen and Wolfgang U. Dressler. 2007 Analogical effects on 
linking elements in German compounds. Language and Cognitive Processes, 22(1):25-57.  
Larson, Martha, Daniel Willett, Joachin K?hler and Gerhard Rigoll. 2000. Compound splitting and lexical unit 
recombination for improved performance of a speech recognition system for German parliamentary speeches 
In INTERSPEECH, 945-948. 
Macherey, Klaus, Andrew M. Dai, David Talbot, Ashok C. Popat and Franz Och. 2011. Language-independent 
compound splitting with Morphological Operations. Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics, 1395-1404. 
Monz, Christof and Maarten de Rijke. 2002. Shallow Morphological Analysis in Monolingual Information 
Retrieval for Dutch, German and Italian. In Evaluation of Cross-Language Information Retrieval Systems. 
Carol Peters, Martin Braschler, Julio Gonzalo and Michael Kluck (eds.). Springer Verlag, Berlin and 
Heidelberg, 262-277. 
van Huyssteen, Gerhard and Menno van Zaanen. 2004.  Learning Compound Boundaries for Afrikaans Spelling 
Checking. In Pre-Proceedings of the Workshop on International Proofing Tools and Language Technologies; 
Patras, Greece. 101?108. 
van Zaanen, Menno, Gerhard van Huyssteen, Suzanne Aussems, Chris Emmery, and Roald Eiselen. 2014. The 
Development of Dutch and Afrikaans Language Resources for Compound Boundary Analysis. In Proceeding 
of LREC 2014. 
 
 
71
