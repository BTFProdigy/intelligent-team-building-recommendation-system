Syntactic features for high precision Word Sense Disambiguation 
 
David Mart?nez, Eneko Agirre  
IxA NLP Group 
University of the Basque Country 
Donostia, Spain 
{jibmaird,eneko}@si.ehu.es 
Llu?s M?rquez 
TALP Research Center 
Polytechnical University of Catalonia 
Barcelona, Spain 
lluism@lsi.upc.es 
 
Abstract 
This paper explores the contribution 
of a broad range of syntactic features 
to WSD: grammatical relations coded 
as the presence of adjuncts/arguments 
in isolation or as subcategorization 
frames, and instantiated grammatical 
relations between words. We have 
tested the performance of syntactic 
features using two different ML 
algorithms (Decision Lists and 
AdaBoost) on the Senseval-2 data. 
Adding syntactic features to a basic 
set of traditional features improves 
performance, especially for AdaBoost. 
In addition, several methods to build 
arbitrarily high accuracy WSD 
systems are also tried, showing that 
syntactic features allow for a precision 
of 86% and a coverage of 26% or 95% 
precision and 8% coverage.  
1. Introduction 
Supervised learning has become the most 
successful paradigm for Word Sense 
Disambiguation (WSD). This kind of algorithms 
follows a two-step process: 
1. Choosing the representation as a set of 
features for the context of occurrence of the 
target word senses.  
2. Applying a Machine Learning (ML) 
algorithm to train on the extracted features 
and tag the target word in the test examples.  
Current WSD systems attain high performances 
for coarse word sense differences (two or three 
senses) if enough training material is available. 
In contrast, the performance for finer-grained 
sense differences (e.g. WordNet senses as used 
in Senseval 2 (Preiss & Yarowsky, 2001)) is far 
from application needs. Nevertheless, recent 
work (Agirre and Martinez, 2001a) shows that it 
is possible to exploit the precision-coverage 
trade-off and build a high precision WSD system 
that tags a limited number of target words with a 
predefined precision.  
This paper explores the contribution of a 
broad set of syntactically motivated features that 
ranges from the presence of complements and 
adjuncts, and the detection of subcategorization 
frames, up to grammatical relations instantiated 
with specific words. The performance of the 
syntactic features is measured in isolation and in 
combination with a basic set of local and topical 
features (as defined in the literature), and using 
two ML algorithms: Decision Lists (Dlist) and 
AdaBoost (Boost). While Dlist does not attempt 
to combine the features, i.e. it takes the strongest 
feature only, Boost tries combinations of 
features and also uses negative evidence, i.e. the 
absence of features. 
Additionally, the role of syntactic features in 
a high-precision WSD system based on the 
precision-coverage trade-off is also investigated.  
The paper is structured as follows. Section 2 
reviews the features previously used in the 
literature. Section 3 defines a basic feature set 
based on the preceding review. Section 4 
presents the syntactic features as defined in our 
work, alongside the parser used. In section 5 the 
two ML algorithms are presented, as well as the 
strategies for the precision-coverage trade-off. 
Section 6 shows the experimental setting and the 
results. Finally section 7 draws the conclusions 
and summarizes further work. 
2. Previous work. 
Yarowsky (1994) defined a basic set of features 
that has been widely used (with some variations) 
by other WSD systems. It consisted on words 
appearing in a window of ?k positions around 
the target and bigrams and trigrams constructed 
with the target word. He used words, lemmas, 
coarse part-of-speech tags and special classes of 
words, such as ?Weekday?. These features have 
been used by other approaches, with variations 
such as the size of the window, the distinction 
between open class/closed class words, or the 
pre-selection of significative words to look up in 
the context of the target word.  
Ng (1996) uses a basic set of features similar 
to those defined by Yarowsky, but they also use 
syntactic information: verb-object and subject-
verb relations. The results obtained by the 
syntactic features are poor, and no analysis of 
the features or any reason for the low 
performance is given. 
Stetina et al (1998) achieve good results with 
syntactic relations as features. They use a 
measure of semantic distance based on WordNet 
to find similar features. The features are 
extracted using a statistical parser (Collins, 
1996), and consist of the head and modifiers of 
each phrase. Unfortunately, they do not provide 
a comparison with a baseline system that would 
only use basic features.  
The Senseval-2 workshop was held in 
Toulouse in July 2001 (Preiss & Yarowsky, 
2001). Most of the supervised systems used only 
a basic set of local and topical features to train 
their ML systems. Regarding syntactic 
information, in the Japanese tasks, several 
groups relied on dependency trees to extract 
features that were used by different models 
(SVM, Bayes, or vector space models). For the 
English tasks, the team from the University of 
Sussex extracted selectional preferences based 
on subject-verb and verb-object relations. The 
John Hopkins team applied syntactic features 
obtained using simple heuristic patterns and 
regular expressions. Finally, WASP-bench used 
finite-state techniques to create a grammatical 
relation database, which was later used in the 
disambiguation process. The papers in the 
proceedings do not provide specific evaluation 
of the syntactic features, and it is difficult to 
derive whether they were really useful or not.  
3. Basic feature set 
We have taken a basic feature set widely used in 
the literature, divided in topical features and 
local features (Agirre & Martinez, 2001b). 
Topical features correspond to open-class 
lemmas that appear in windows of different sizes 
around the target word. In this experiment, we 
used two different window-sizes: 4 lemmas 
around the target (coded as win_lem_4w), and 
the lemmas in the sentence plus the 2 previous 
and 2 following sentences (win_lem_2s). 
Local features include bigrams and trigrams 
(coded as big_, trig_ respectively) that contain 
the target word. An index (+1, -1, 0) is used to 
indicate the position of the target in the bigram 
or trigram, which can be formed by part of 
speech, lemmas or word forms (wf, lem, 
pos). We used TnT (Brants, 2000) for PoS 
tagging.  
For instance, we could extract the following 
features for the target word known from the 
sample sentence below: word form ?whole? 
occurring in a 2 sentence window (win_wf_2s), 
the bigram  ?known widely? where target is the 
last word (big_wf_+1) and the trigram ?RB RB N? 
formed by the two PoS before the target word 
(trig_pos_+1). 
 
?There is nothing in the whole range of human 
experience more widely known and universally ?? 
4. Set of Syntactic Features. 
In order to extract syntactic features from the 
tagged examples, we needed a parser that would 
meet the following requirements: free for 
research, able to provide the whole structure 
with named syntactic relations (in contrast to 
shallow parsers), positively evaluated on well-
established corpora, domain independent, and 
fast enough. 
Three parsers fulfilled all the requirements: 
Link Grammar (Sleator and Temperley, 1993), 
Minipar (Lin, 1993) and (Carroll & Briscoe, 
2001). We installed the first two parsers, and 
performed a set of small experiments (John 
Carroll helped out running his own parser). 
Unfortunately, we did not have a comparative 
evaluation to help choosing the best. We 
performed a little comparative test, and all 
parsers looked similar. At this point we chose 
Minipar mainly because it was fast, easy to 
install and the output could be easily processed. 
The choice of the parser did not condition the 
design of the experiments (cf. section 7). 
From the output of the parser, we extracted 
different sets of features. First, we distinguish 
between direct relations (words linked directly 
in the parse tree) and indirect relations (words 
that are two or more dependencies apart in the 
syntax tree, e.g. heads of prepositional modifiers 
of a verb). For example, from ?Henry was listed 
on the petition as the mayor's attorney? a direct 
verb-object relation is extracted between listed 
and Henry and the indirect relation ?head of a 
modifier prepositional phrase? between listed 
and petition. For each relation we store also its 
inverse. The relations are coded according to the 
Minipar codes (cf. Appendix): 
 
[Henry obj_word listed] 
[listed objI_word Henry] 
[petition mod_Prep_pcomp-n_N_word listed] 
[listed mod_Prep_pcomp-n_NI_word petition] 
 
For instance, in the last relation above, mod_Prep 
indicates that listed has some prepositional 
phrase attached, pcomp-n_N indicates that petition 
is the head of the prepositional phrase, I 
indicates that it is an inverse relation, and word 
that the relation is between words (as opposed to 
relations between lemmas).  
We distinguished two different kinds of 
syntactic relations: instantiated grammatical 
relations (IGR) and grammatical relations (GR). 
4.1. Instantiated Grammatical Relations 
IGRs are coded as [wordsense relation value] 
triples, where the value can be either the word 
form or the lemma. Some examples for the 
target noun ?church? are shown below. In the 
first example, a direct relation is extracted for 
the ?building? sense, and in the second example 
an indirect relation for the ?group of Christians? 
sense. 
 
Example 1: ?...Anglican churches have been 
demolished...? 
[Church#2 obj_lem  demolish] 
 
Example 2: ?...to whip men into a surrender to a 
particular churh...? 
[Church#1 mod_Prep_pcomp-n_N_lem surrender] 
4.2. Grammatical relations 
This kind of features refers to the grammatical 
relation themselves. In this case, we collect 
bigrams [wordsense relation] and also n-grams 
[wordsense relation1 relation2 relation3 ...]. The 
relations can refer to any argument, adjunct or 
modifier. N-grams are similar to verbal 
subcategorization frames. At present, they have 
been used only for verbs. Minipar provides 
simple subcategorization information in the PoS 
itself, e.g. V_N_N for a verb taking two 
arguments. We have defined 3 types of n-grams: 
? Ngram1: The subcategorization information 
included in the PoS data given by Minipar, 
e.g. V_N_N.  
? Ngram2: The subcategorization information 
in ngram1, filtered by the arguments that 
actually occur in the sentence. 
? Ngram3: Which includes all dependencies in 
the parse tree.  
The three types have been explored in order to 
account for the argument/adjunct distinction, 
which Minipar does not always assign correctly. 
In the first case, Minipar?s judgment is taken 
from the PoS. In the second case the PoS and the 
relations deemed as arguments are combined 
(adjuncts are hopefully filtered out, but some 
arguments might be also discarded). In the third, 
all relations (including adjuncts and arguments) 
are considered. 
In the example below, the ngram1 feature 
indicates that the verb has two arguments (i.e. it 
is transitive), which is an error of Minipar 
probably caused by a gap in the lexicon. The 
ngram2 feature indicates simply that it has a 
subject and no object, and the ngram3 feature 
denotes the presence of the adverbial modifier 
?still?. Ngram2 and ngram3 try to repair possible 
gaps in Minipar?s lexicon. 
 
Example: ?His mother was nudging him, but he 
was still falling? 
[Fall#1 ngram1 V_N_N] 
[Fall#1 ngram2 subj] 
[Fall#1 ngram3 amodstill+subj] 
5. ML algorithms. 
In order to measure the contribution of syntactic 
relations, we wanted to test them on several ML 
algorithms. At present we have chosen one 
algorithm which does not combine features 
(Decision Lists) and another which does 
combine features (AdaBoost).  
Despite their simplicity, Decision Lists (Dlist 
for short) as defined in Yarowsky (1994) have 
been shown to be very effective for WSD 
(Kilgarriff & Palmer, 2000). Features are 
weighted with a log-likelihood measure, and 
arranged in an ordered list according to their 
weight. In our case the probabilities have been 
estimated using the maximum likelihood 
estimate, smoothed adding a small constant (0.1) 
when probabilities are zero. Decisions taken 
with negative values were discarded (Agirre & 
Martinez, 2001b).  
AdaBoost (Boost for short) is a general 
method for obtaining a highly accurate 
classification rule by linearly combining many 
weak classifiers, each of which may be only 
moderately accurate (Freund, 1997). In these 
experiments, a generalized version of the Boost 
algorithm has been used, (Schapire, 1999), 
which works with very simple domain 
partitioning weak hypotheses (decision stumps) 
with confidence rated predictions. This 
particular boosting algorithm is able to work 
efficiently in very high dimensional feature 
spaces, and has been applied, with significant 
success, to a number of NLP disambiguation 
tasks, including word sense disambiguation 
(Escudero et al, 2000). Regarding 
parametrization, the smoothing parameter has 
been set to the default value (Schapire, 1999), 
and Boost has been run for a fixed number of 
rounds (200) for each word. No optimization of 
these parameters has been done at a word level. 
When testing, the sense with the highest 
prediction is assigned. 
5.1. Precision vs. coverage trade-off. 
A high-precision WSD system can be obtained 
at the cost of low coverage, preventing the 
system to return an answer in the lowest 
confidence cases. We have tried two methods on 
Dlists, and one method on Boost. 
The first method is based on a decision-
threshold (Dagan and Itai, 1994): the algorithm 
rejects decisions taken when the difference of 
the maximum likelihood among the competing 
senses is not big enough. For this purpose, a 
one-tailed confidence interval was created so we 
could state with confidence 1 - ? that the true 
value of the difference measure was bigger than 
a given threshold (named ?). As in (Dagan and 
Itai, 1994), we adjusted the measure to the 
amount of evidence. Different values of ? were 
tested, using a 60% confidence interval. The 
values of ? range from 0 to 4. For more details 
check (Agirre and Martinez, 2001b). 
The second method is based on feature 
selection (Agirre and Martinez, 2001a). Ten-
fold cross validation on the training data for 
each word was used to measure the precision of 
each feature in isolation. Thus, the ML 
algorithm would be used only on the features 
with precision exceeding a given threshold. This 
method has the advantage of being able to set 
the desired precision of the final system.  
In the case of Boost, there was no 
straightforward way to apply the first method. 
The application of the second method did not 
yield satisfactory results, so we turned to 
directly use the support value returned for each 
decision being made. We first applied a 
threshold directly on this support value, i.e. 
discarding decisions made with low support 
values. A second approximation, which is the 
one reported here, applies a threshold over the 
difference in the support for the winning sense 
and the second winning sense. Still, further work 
is needed in order to investigate how Boost 
could discard less-confident results. 
6. Experimental setting and results. 
We used the Senseval-2 data (73 nouns, verbs 
and adjectives), keeping the original training and 
testing sets. In order to measure the contribution 
of syntactic features the following experiments 
were devised (not all ML algorithms were used 
in all experiments, as specified): contribution of 
IGR-type and GR-type relations (Dlist), 
contribution of syntactic features over a 
combination of local and topical features (Dlist, 
Boost), and contribution of syntactic features in 
a high precision system (Dlist, Boost). 
Performance is measured as precision and 
coverage (following the definitions given in 
Senseval-2). We also consider F11 to compare 
the overall performance as it gives the harmonic 
average between precision and recall (where 
recall is in this case precision times the 
coverage). F1 can be used to select the best 
precision/coverage combination (cf. section 6.3). 
6.1. Results for different sets of syntactic 
features (Dlist). 
Table 1 shows the precision, coverage and F1 
figures for each of the grammatical feature sets 
as used by the decision list algorithm. 
Instantiated Grammatical Relations provide very 
good precision, but low coverage. The only 
exceptions are verbs, which get very similar 
precision for both kinds of syntactic relations. 
Grammatical Relations provide lower precision 
but higher coverage. A combination of both 
attains best F1, and is the feature set used in 
subsequent experiments.  
                                                     
1 F1=2*precision*recall/(precision+recall). In this 
case we use recall=precision*coverage. 
6.2. Results for different combinations of 
features (Dlist, Boost) 
Both ML algorithms were used on syntactic 
features, local features, a combination of 
local+topical features (also called basic), and a 
combination of all features (basic+syntax) in 
turn. Table 2 shows the F1 figures for each 
algorithm, feature set and PoS.  
All in all, Boost is able to outperform Dlist in 
all cases, except for local features. Syntactic 
features get worse results than local features. 
Regarding the contribution of syntactic 
features to the basic set, the last two columns in 
Table 2 show a "+" whenever the difference in 
the precision over the basic feature set is 
significant (McNemar's test). Dlist is able to 
scarcely profit from the additional syntactic 
features (only significant for verbs). Boost 
attains significant improvement, showing that 
basic and syntactic features are complementary.  
The difference 
algorithms could be 
Dlist is a conservati
that it only uses the 
by the first feature tha
(abstaining if none o
using a combination o
single-feature classife
negative evidence) 
positive predictions t
Dlist. Since the feat
covered and given th
accurate, Boost achie
it is a significant
approaching a 100% c
6.3. Precision vs. coverage: high precision 
systems (Dlist, Boost)  
Figure 1 shows the results for the three methods 
to exploit the precision/coverage trade-off in 
order to obtain a high-precision system. For each 
method two sets of features have been used: the 
basic set alne and the combination of both 
basic and syntactic features.  
The figure reveals an interesting behavior for 
different coverage ranges. In the high coverage 
range, Boost on basic+syntactic features attains 
the best performance. In the medium coverage 
area, the feature selection method for Dlist 
obtains the best results, also for basic+syntactic 
features. Finally, in the low coverage and high 
precision area the decision-threshold method for 
Dlist is able to reach precisions in the high 90?s, 
with no profit from syntactic features. 
The two methods to raise precision for Dlists 
are very effective. The decision-threshold 
e in performance 
 coverage. The 
s 86% precision 
ctic features, but 
.  
obtain extremely 
 of low coverage) 
most predictive 
ave had problems 
 algorithm for 
ions. 
r coverage over 
ures consistently 
ing that syntactic 
IGR GR All-syntax 
PoS Prec. Cov. F1 Prec. Cov. F1 Prec. Cov. F1 
A 81,6 21,8 29,2 70,1 65,4 55,4 70,7 68,9 57,7 
N 74,6 36,0 38,5 65,4 57,6 47,8 67,6 62,5 52,0 
V 68,6 32,2 33,4 67,3 41,2 39,2 66,3 52,7 45,4 
Ov. 72,9 31,9 35,2 67,1 52,1 46,0 67,7 59,5 50,4 
Table 1: precision and coverage for different sets of syntactic features (percentage). 
 
  Syntax Local Local+Topical (Basic) Basic + Syntax 
PoS MFS Dlist Boost Dlist Boost Dlist Boost Dlist Boost 
A 59,0 57,7 62,6 66,3 67,5 65,3 66,2 65,4     67,7 
N 57,1 52,0 60,0 63,6 65,3 63,2 67,9 63,3 69,3+ 
V 40,3 45,4 48,5 51,6 50,1 51,0 51,6   51,2+ 53,9+ 
Ov. 48,2 50,4 55,2 59,4 59,3 58,5 60,7 58,7 62,5+ 
Table 2: F1 results (perc.) for different feature sets. ?+? indicates statistical significance over Basic. between the two ML 
explained by the fact that 
ve algorithm in the sense 
positive information given 
t holds in the test example 
f them are applicable). By 
f the predictions of several 
rs (using both positive and 
Boost is able to assign 
o more test examples than 
ure space is more widely 
at the classifiers are quite 
ves better recall levels and 
ly better algorithm for 
method obtains constant increas
up to 93% precision with 7%
feature selection method attain
with 26% coverage using synta
there is no further improvement
In this case Dlist is able to 
good accuracy rates (at the cost
restricting to the use of the 
features. On the contrary, we h
in adjusting the AdaBoost
obtaining high precision predict
The figure also shows, fo
20%, that the syntactic feat
allow for better results, confirmoverage WSD system. features improve the results of the basic set. 
7. Conclusions and further work. 
This paper shows that syntactic features 
effectively contribute to WSD precision. We 
have extracted syntactic relations using the 
Minipar parser, but the results should be also 
applicable to other parsers with similar 
performance. Two kinds of syntactic features are 
defined: Instantiated Grammatical Relations  
(IGR) between words, and Grammatical 
Relations (GR) coded as the presence of 
adjuncts / arguments in isolation or as 
subcategorization frames.  
The experimental results were tried on the 
Senseval-2 data, comparing two different ML 
algorithms (Dlist and Boost) trained both on a 
basic set of widely used features alone, and on a 
combination of basic and syntactic features. The 
main conclusions are the following: 
? IGR get better precision than GR, but the 
best precision/coverage combination 
(measured with F1) is attained by the 
combination of both. 
? Boost is able to profit from the addition of 
syntactic features, obtaining better results 
than Dlist. This proves that syntactic 
features contain information that is not 
present in other traditional features.  
? Overall the improvement is around two 
points for Boost, with highest increase for 
verbs.  
Several methods to exploit the precision-
coverage trade-off where also tried: 
? The results show that syntactic features 
consistently improve the results on all data 
points except in the very low coverage 
range, confirming the contribution of syntax. 
? The results also show that Dlist are suited to 
build a system with high precision: either a 
precision of 86% and a coverage of 26%, or 
95% precision and 8% coverage. 
Regarding future work, a thorough analysis of 
the quality of each of the syntactic relations 
extracted should be performed. In addition, a 
word-by-word analysis would be interesting, as 
some words might profit from specific syntactic 
features, while others might not. A preliminary 
analysis has been performed in (Agirre & 
Martinez, 2001b). 
Other parsers rather than Minipar could be 
used. In particular, we found out that Minipar 
always returns unambiguous trees, often making 
erroneous attachment decisions. A parser 
returning ambiguous output could be more 
desirable. The results of this paper do not 
depend on the parser used, only on the quality of 
the output, which should be at least as good as 
Minipar. 
Concerning the performance of the algorithm 
as compared to other Senseval 2 systems, it is 
not the best. Getting the best results was not the 
objective of this paper, but to show that syntactic 
features are worth including. We plan to 
improve the pre-processing of our systems, the 
detection of multiword lexical entries, etc. which 
could improve greatly the results. In addition 
there can be a number of factors that could 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: prec./cov. curve for three high precision methods on basic and basic+syntactic features. 
0,50
0,55
0,60
0,65
0,70
0,75
0,80
0,85
0,90
0,95
1,00
0 0,2 0,4 0,6 0,8 1coverage
pr
ec
isi
on
dlist threshold basic dlist feat.sel. basic boost basic
dlist threshold basic+synt dlist feat.sel. basic+synt boost basic+synt
diminish or disguise the improvement in the 
results: hand-tagging errors, word senses 
missing from training or testing data, biased 
sense distributions, errors in syntactic relations, 
etc. Factor out this ?noise? could show the real 
extent of the contribution of syntactic features. 
On the other hand, we are using a high 
number of features. It is well known that many 
ML algorithms have problems to scale to high 
dimensional feature spaces, especially when the 
number of training examples is relatively low (as 
it is the case for Senseval-2 word senses). 
Researching on more careful feature selection 
(which is dependent of the ML algorithm) could 
also improve the contribution of syntactic 
features, and WSD results in general. In 
addition, alternative methods to produce a high 
precision method based on Boost need to be 
explored. 
Finally, the results on high precision WSD 
open the avenue for acquiring further examples 
in a bootstrapping framework.  
Acknowledgements 
This research has been partially funded by McyT 
(Hermes project TIC-2000-0335-C03-03). David 
Martinez was funded by the Basque 
Government, grant AE-BFI:01.245). 
References 
Agirre, E. and D. Martinez. 2001a. Decision Lists for 
English and Basque. Proceedings of the 
SENSEVAL-2 Workshop. In conjunction with 
ACL'2001/EACL'2001. Toulouse, France. 
Agirre, E. and D. Martinez. 2001b. Analysis of 
supervised word sense disambiguation systems. Int. 
report LSI 11-2001, available from the authors. 
Brants, T. 2000. TnT - A Statistical Part-of-Speech 
Tagger. In Proc. of the Sixth Applied Natural 
Language Processing Conference, Seattle, WA. 
Carroll, J. and E. Briscoe (2001) `High precision 
extraction of grammatical relations'. In Proceedings 
of the 7th ACL/SIGPARSE International Workshop 
on Parsing Technologies, Beijing, China. 78-89.  
Collins M. 1996. A new statistical parser based on 
bigram lexical dependencies. In Proceedings of the 
34th Annual Meeting of the ACL, pages 184-191. 
Dagan I., and A. Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational Linguistics 
20:4, pp. 563--596. 
Freund Y. and R. E. Schapire. 1997. A Decision-
Theoretic Generalization of On-line Learning and 
an Application to Boosting. Journal of Computer 
and System Sciences, 55(1):119--139. 
Escudero G., L. M?rquez, G. Rigau. 2000. Boosting 
Applied to Word Sense Disambiguation. 
Proceedings of the 12th European Conference on 
Machine Learning, ECML 2000. Barcelona, Spain. 
Kilgarriff, A. and M. Palmer. (eds). 2000. Special 
issue on SENSEVAL. Computer and the 
Humanities, 34 (1-2). 
Lin, D. 1993. Principle Based parsing without 
Overgeneration. In 31st Annual Meeting of the 
Association for Computational Linguistics. 
Columbus, Ohio. pp 112-120.  
Ng, H. T. and H. B. Lee. 1996. Integrating Multiple 
Knowledge Sources to Disambiguate Word Sense: 
An Exemplar-based Approach. Proceedings of the 
34th Annual Meeting of the Association for 
Computational Linguistics. 
Preiss, J. and D. Yarowsky. 2001. Proc. of the 
Second Intl. Workshop on Evaluating Word Sense 
Disambiguation Systems (Senseval 2). In conj. with 
ACL'2001/EACL'2001. Toulouse, France. 
Schapire, R. E. and Y. Singer. 1999. Improved 
Boosting Algorithms Using Confidence-rated 
Predictions. Machine Learning, 37(3):297--336. 
Sleator, D. and D. Temperley. 1993. Parsing English 
with a Link Grammar. Third International 
Workshop on Parsing Technologies. 
Stetina J., S. Kurohashi, M. Nagao. 1998. General 
Word Sense Disambiguation Method Based on a 
Full Sentential Context. In Usage of WordNet in 
Natural Language Processing , Proceedings of 
COLING-ACL Workshop. Montreal (Canada).  
Yarowsky, D. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent 
Restoration in Spanish and French. Proceedings of 
the 32nd Annual Meeting of the Association for 
Computational Linguistics, pp. 88--95.  
Appendix: main Minipar relations. 
Relation Direct Indirect Description 
by-subj X  Subj. with passives 
C  X clausal complement 
Cn  X nominalized clause 
comp1 X  complement (PP, inf/fin clause) of noun 
Desc X  description  
Fc X  finite complement 
I  X see c and fc, dep. between clause and main verb 
Mod X  Modifier 
Obj X  Object 
pcomp-c X  clause of pp 
Pcomp-n X  nominal head of pp 
Pnmod X  postnominal modifier. 
Pred X  predicative (can be A or N) 
Sc X  sentential complement 
Subj X  subject 
Vrel X  passive verb modifier of nouns 
For each relation the acronym, whether it is used as a 
direct relation or to construct indirect relations, and a 
short description are provided. 
One Sense per Collocation and Genre/Topic Variations 
David Martinez 
IXA NLP Group 
University of the Basque Country 
649 pk. 20.080 
Donostia. Spain 
jibmaird@si.ehu.es 
Eneko Agirre 
IXA NLP Group 
University of the Basque Country 
649 pk. 20.080 
Donostia. Spain 
eneko@si.ehu.es 
Abstract 
This paper revisits the one sense per 
collocation hypothesis using fine-grained 
sense distinctions and two different corpora. 
We show that the hypothesis i  weaker for 
fine-grained sense distinctions (70% vs. 
99% reported earlier on 2-way ambiguities). 
We also show that one sense per collocation 
does hold across corpora, but that 
collocations vary from one corpus to the 
other, following genre and topic variations. 
This explains the low results when 
performing word sense disambiguation 
across corpora. In fact, we demonstrate hat 
when two independent corpora share a 
related genre/topic, the word sense 
disambiguation results would be better. 
Future work on word sense disambiguation 
will have to take into account genre and 
topic as important parameters on their 
models. 
Introduction 
In the early nineties two famous papers claimed 
that the behavior of word senses in texts adhered 
to two principles: one sense per discourse (Gale 
et al, 1992) and one sense per collocation 
(Yarowsky, 1993). 
These hypotheses were shown to hold for 
some particular corpora (totaling 380 Mwords) 
on words with 2-way ambiguity. The word 
sense distinctions came from different sources 
(translations into French, homophones, 
homographs, pseudo-words, etc.), but no 
dictionary or lexical resource was linked to 
them. In the case of the one sense per 
collocation paper, several corpora were used, 
but nothing is said on whether the collocations 
hold across corpora. 
Since the papers were published, word sense 
disambiguation has moved to deal with fine- 
grained sense distinctions from widely 
recognized semantic lexical resources; 
ontologies like Sensus, Cyc, EDR, WordNet, 
EuroWordNet, etc. or machine-readable 
dictionaries like OALDC, Webster's, LDOCE, 
etc. This is due, in part, to the availability of 
public hand-tagged material, e.g. SemCor 
(Miller et al, 1993) and the DSO collection (Ng 
& Lee, 1996). We think that the old hypotheses 
should be tested under the conditions of this 
newly available data. This paper focuses on the 
DSO collection, which was tagged with 
WordNet senses (Miller et al 1990) and 
comprises sentences extracted from two 
different corpora: the balanced Brown Corpus 
and the Wall Street Journal corpus. 
Krovetz (1998) has shown that the one sense 
per discourse hypothesis does not hold for fine- 
grained senses in SemCor and DSO. His results 
have been confirmed in our own experiments. 
We will therefore concentrate on the one sense 
per collocation hypothesis, considering these 
two questions: 
? Does the collocation hypothesis hold across 
corpora, that is, across genre and topic 
variations (compared to a single corpus, 
probably with little genre and topic 
variations)? 
? Does the collocation hypothesis hold for free- 
grained sense distinctions (compared to 
homograph level granularity)? 
The experimental tools to test the hypothesis 
will be decision lists based on various kinds of 
collocational information. We will compare the 
performance across several corpora (the Brown 
Corpus and Wall Street Journal parts of the 
DSO collection), and also across different 
sections of the Brown Corpus, selected 
according to the genre and topics covered. We 
will also perform a direct comparison, using 
agreement statistics, of the collocations used 
and of the results obtained. 
207 
This study has special significance at this 
point of word sense disambiguation research. A
recent study (Agirre & Martinez, 2000) 
concludes that, for currently available hand- 
tagged data, the precision is limited to around 
70% when tagging all words in a running text. 
In the course of extending available data, the 
efforts to use corpora tagged by independent 
teams of researchers have been shown to fail 
(Ng et al, 1999), as have failed some tuning 
experiments (Escudero et al, 2000), and an 
attempt to use examples automatically acquired 
from the Internet (Agirre & Martinez, 2000). All 
these studies obviated the fact that the examples 
come from different genre and topics. Future 
work that takes into account he conclusions 
drawn in this paper will perhaps be able to 
automatically extend the number of examples 
available and tackle the acquisition problem. 
The paper is organized as follows. The 
resources used and the experimental settings are 
presented first. Section 3 presents the 
collocations considered and Section 4 explains 
how decision lists have been adapted to n-way 
ambiguities. Sections 5 and 6 show the in- 
corpus and cross-corpora experiments, 
respectively. Section 7 discusses the effect of 
drawing training and testing data from the same 
documents. Section 8 evaluates the impact of 
genre and topic variations, which is fiarther 
discussed in Section 9. Finally, Section 10 
presents some conclusions. 
1 Resources used 
The DSO collection (Ng and Lee, 1996) focuses 
on 191 frequent and polysemous words (nouns 
and verbs), and contains around 1,000 sentences 
per word. Overall, there are 112,800 sentences, 
where 192,874 occurrences of the target words 
were hand-tagged with WordNet senses (Miller 
et al, 1990). 
The DSO collection was built with examples 
from the Wall Street Journal (WSJ) and 
Brown Corpus (BC). The Brown Corpus is 
balanced, and the texts are classified according 
some predefined categories (el. Table 1). The 
examples from the Brown Corpus comprise 
78,080 occurrences of word senses, and the 
examples from the WSJ 114,794 occurrences. 
The sentences in the DSO collection were 
tagged with parts of speech using TnT (Brants, 
2000) trained on the Brown Corpus itself. 
A. Press: Reportage 
B. Press: Editorial 
C. Press: Reviews (theatre, books, music, dance) 
D. Religion 
E. Skills and Hobbies 
F. Popular Lore 
G. Belles Lettres, Biography, Memoirs, etc. 
H. Miscellaneous 
J. Learned 
K. General Fiction 
L. Mystery and Detective Fiction 
M. Science Fiction 
N. Adventure and Western Fiction 
P. Romance and Love Story 
R. Humor 
Table 1: List of categories of texts from the 
Brown Corpus, divided into informative prose 
(top) and imaginative prose (bottom). 
1.1 Categories in the Brown Corpus 
and genre/topic variation 
The Brown Corpus manual (Francis & Kucera, 
1964) does not detail the criteria followed to set 
the categories in Table 1: 
The samples represent a wide range of  styles 
and varieties of  prose... The list of main 
categories and their subdivisions was drawn 
up at a conference held at Brown University 
in February 1963. 
These categories have been previously used in 
genre detection experiments (Karlgrcn & 
Cutting, 1994), where each category was used 
as a genre. We think that the categories not only 
reflect genre variations but also topic variations 
(e.g. the Religion category follows topic 
distinctions rather than genre). Nevertheless we 
are aware that some topics can be covered in 
more than one category. Unfortunately there are 
no topically tagged corpus which also have 
word sense tags. We thus speak of genre and 
topic variation, knowing that further analysis 
would be needed to measure the effect of each 
of them. 
2 Experimental setting 
In order to analyze and compare the behavior of 
several kinds of collocations (cf. Section 3), 
Yarowsky (1993) used a measure of entropy as 
well as the results obtained when tagging held- 
out data with the collocations organized as 
decision lists (el. Section 4) .  As Yarowsky 
shows, both measures correlate closely, so we 
208 
only used the experimental results of  decision Word PoS #Senses #Ex. BC #Ex. WSJ 
lists. Age N 5 243 248 
When comparing the performance on Art N 4 200 194 
decision lists trained on two different corpora Body N 9 296 110 
(or sub-corpora) we always take an equal Car N 5 357 1093 
amount of examples per word from each Child N 6 577 484 
corpora. This is done to discard the amount-of- Cost N 3 317 1143 
data factor. Head N 28 432 434 
As usual, we use 10-fold cross-validation Interest N 8 364 1115 
Line N 28 453 880 
when training and testing on the same corpus. Point N 20 442 249 
No significance tests could be found for our State N 6 757 706 
comparison, as training and test sets differ. Thing N 11 621 805 
Because of the large amount of experiments Work N 6 596 825 
involved, we focused on 21 verbs and nouns (el. Become V 4 763 736 
Table 2), selected from previous works (Agirre Fall V 17 221 1227 
& Martinez, 2000; Escudero et al, 2000). Grow V 8 243 731 
Lose V 10 245 935 
Set V 20 925 355 
Speak V 5 210 307 
Strike V 17 159 95 
Tell V 8 740 744 
3 Collocations considered 
For the sake of this work we take a broad 
definition of collocations, which were classified 
in three subsets: local content word collocations, 
local part-of-speech and function-word 
collocations, and global content-word 
collocations. If a more strict linguistic 
perspective was taken, rather than collocations 
we should speak about co-occurrence r lations. 
In fact, only local content word collocations 
would adhere to this narrower view. 
We only considered those collocations that 
could be easily exlracted form a part of speech 
tagged corpus, like word to left, word to right, 
etc. Local content word collocations comprise 
bigrams (word to left, word to right) and 
trigrams (two words to left, two words to right 
and both words to right and left). At least one of 
those words needs to be a content word. Local 
function-word collocations comprise also all 
kinds of bigrams and trigrams, as before, but the 
words need to be function words. Local PoS 
collocations take the Part of Speech of the 
words in the bigrams and trigrams. Finally 
global content word collocations comprise the 
content words around the target word in two 
different contexts: a window of 4 words around 
the target word, and all the words in the 
sentence. Table 3 summarizes the collocations 
used. These collocations have been used in other 
word sense disambiguation research and are also 
referred to as features (Gale et al, 1993; Ng & 
Lee, 1996; Escudero et al, 2000). 
Compared to Yarowsky (1993), who also 
took into account grammatical relations, we 
only share the content-word-to-left and the 
content-word-to-right collocations. 
Table 2: Data for selected words. Part of 
speech, number of senses and number of 
examples m BC and WSJ are shown. 
Local content word collocations 
Word-to-left Content Word 
Word-to-right Content Word 
Two-words-to-left At least one 
Two-words-to-right Content Word 
Word-to-right-and-left 
Local PoS and function word collocations 
Word-to-left PoS Function Word 
Word-to-right PoS Function Word 
Two-words-to-left PoS Both Function Two-words-to-fight PoS Words Word-to-fight-and-left PoS 
Global content word collocations 
Word in Window of 4 Content Word Word in sentence 
Table 3: Kinds of collocations considered 
We did not lemmatize content words, and we 
therefore do take into account he form of the 
target word. For instance, governing body and 
governing bodies are different collocations for 
the sake of this paper. 
4 Adaptation of decision lists to n-way 
ambiguities 
Decision lists as defined in (Yarowsky, 1993; 
1994) are simple means to solve ambiguity 
problems. They have been successfully applied 
to accent restoration, word" sense disambiguation 
209 
and homograph disambiguation (Yarowsky, 
1994; 1995; 1996). In order to build decision 
lists the training examples are processed to 
extract he features (each feature corresponds to
a kind of collocation), which are weighted with 
a log-likelihood measure. The list of all features 
ordered by log-likelihood values constitutes the 
decision list. We adapted the original formula in 
order to accommodate ambiguities higher than 
two: 
. , Pr(sense i I features)  , weight(sensei ,  feature , )  = ~ogt-  ) 
Pr(sense~ l feature , )  
,i=i 
When testing, the decision list is checked in 
order and the feature with highest weight hat is 
present in the test sentence selects the winning 
word sense. For this work we also considered 
negative weights, which were not possible on 
two-way ambiguities. 
The probabilities have been estimated using 
the maximum likelihood estimate, smoothed 
using a simple method: when the denominator 
in the formula is 0 we replace it with 0.1. It is 
not clear how the smoothing technique proposed 
in (Yarowsky, 1993) could be extended to n- 
way ambiguities. 
More details of the implementation can be 
found in (Agirre & Martinez, 2000). 
5 In-corpus experiments: 
collocations are weak  (80%) 
We extracted the collocations in the Brown 
Corpus section of the DSO corpus and, using 
10-fold cross-validation, tagged the same 
corpus. Training and testing examples were thus 
from the same corpus. The same procedure was 
followed for the WSJ part. The results are 
shown in Tables 4 and 5. We can observe the 
following: 
? The best kinds of collocations are local 
content word collocations, especially if two 
words from the context are taken into 
consideration, but the coverage is low. 
Function words to right and left also attain 
remarkable precision. 
? Collocations are stronger in the WSJ, surely 
due to the fact that the BC is balanced, and 
therefore includes more genres and topics. 
This is a first indicator than genre and topic 
variations have to be taken into account. 
? Collocations for fine-gained word-senses are 
sensibly weaker than those reported by 
Yarowsky (1993) for two-way ambiguous 
words. Yarowsky reports 99% precision, 
N V Overall 
Collocations Pr. Cov. Pr. Cov. Pr. Coy. 
Word-to-righ~ .768.254.529.264 1680.258 
Word-to-left .724.185.867.182.775.184 
Two-words-to-righ1.784.191 .623.113.744.163 
Two-words-to-left. 811 . 160.862.179.830.166 
Word-to-right-and-left.820.169.728.129.793.155 
Word-to-righ1.600.457.527.370.577.426 
Word-to-left .545.609.629.472.570.560 
Two-words-to-righ1.638.133.687.084.650.116 
Two-words-to-left .600.140.657.108.617.128 
Word-to-right-and-left.721.220.694.138.714.191 
PoS-to-righ1.490.993.488.993.489.993 
PoS -to-left .465.991 .584.994.508.992 
Two- PoS -to-righ1.526.918.534.879.529.904 
Two- PoS -to-left .518.822.614.912.555.854 
PoS -to-right-and-left .555.918.634.891 .583.908 
O~daii~ib:~al;P~g,~.Fiifii~ !622 7o6 i64b:i~00 i629:Ii60 
Word in sentence .611 1.00.572 1.00.597 1.00 
Word in Window of 4.627.979.611.975.622.977 
OVERAM.; : i::/::: i:~ .661i,L00,635I'.00.652:11200 
Table 4: Train on WSJ, tag WSJ. 
N V Overall 
Collocations Pr. Coy. Pr. Cov. Pr. Cov. 
Word-to-right,644.203 4 2.230 .562.212 
Word-to-left,626.124 770.139 .681.129 
Two-words-to-right,657.146 500.103 ,613.131 
Two-words-to-left,740.092 ,819.122 ,774.103 
Word-to-right-and-left.647.088 686.114 .663.098 
Word-to-right 480.503 452.406 ,471.468 
Word-to-leA 414.639 572.527 :,464.599 
Two-words-to-right,520.183 624.113 ,547.158 
Two-words-to-left ,420.131 648.173 ,516.146 
Word-to-right-and-leg 549.238 654.160 ,577.210 
PoS4o-righ~ 340.992 356.992 i,346.992 
PoS -to-left,350.994 483.992 ,398.993 
Two- PoS -to-righ' 406.923 422.876 ,412.906 
Two- PoS -to-lef 396.792 539.897 i,452.829 
PoS -to-right-and-lef ,416.921 545.885 ,461.908 
Word in sentence 545 1.00 !.492 1.00 ,526 1.00 
Word in Window of 4 550.972 1.525.951 ,541.964 
Table 5: Train on BC, tag BC. 
while our highest results do not reach 80%. 
It has to be noted that the test and training 
examples come from the same corpus, which 
means that, for some test cases, there are 
training examples from the same document. In 
somesense we can say that one sense  per  
d i scourse  comes into play. This point will be 
further explored in Section 7. 
210 
1. state -- (the group of people comprising the government ofa sovereign) 
2. state, province 
-- (the territory occupied by one of the constituent administrative districts of a nation) 
3. state, nation, country, land, commonwealth, res publica, body politic 
-- (a politically organized body of people under a single government) 
4. state -- (the way something iswith respect o its main attributes) 
5. Department of  State, State Department, State 
-- (the federal department that sets and maintains foreign policies) 
6. country, state, land, nation -- (the territory occupied by a nation) 
F igure  1: Word senses for state in WordNet 1.6 (6 out of  8 are shown) 
In the rest o f  this paper, only the overall 
results for each subset of  the collocations will be 
shown. We will pay special attention to local- 
content collocations, as they are the strongest, 
and also closer to strict definitions of  
collocation. 
As an example of  the learned collocations 
Table 6 shows some strong local content word 
col locat ions for the noun state, and Figure 1 
shows the word senses of  state (6 out of  the 8 
senses are shown as the rest were not present in 
the corpora). 
6 Cross-corpora experiments: 
one sense per col location in doubt. 
In these experiments we train on the Brown 
Corpus and tag the WSJ corpus and vice versa. 
Tables 7 and 8, when compared to Tables 4 and 
5 show a significant drop in performance (both 
precision and coverage) for all kind of  
collocations (we only show the results for each 
subset of  collocations). For instance, Table 7 
shows a drop in .16 in precision for local 
content collocations when compared to Table 4. 
These results confirm those by (Escudero et 
al. 2000) who conclude that the information 
learned in one corpus is not useful to tag the 
other. 
In order to analyze the reason of  this 
performance degradation, we compared the 
local content-word collocations extracted from 
one corpus and the other. Table 9 shows the 
amount of  collocations extracted from each 
corpus, how many of  the collocations are shared 
on average and how many of  the shared 
collocations are in contradiction. The low 
amount of  collocations shared between both 
corpora could explain the poor figures, but fo r  
some words (e.g. point) there is a worrying 
proportion of  contradicting collocations. 
We inspected some of  the contradicting 
collocations and saw that m all the cases they 
were caused by errors (or at least differing 
Senses 
Collocations Log #1 #2 #3 #4 #5 #6 
State government 3.68 - - 4 
six states 3.68 - - 4 
State's largest 3.68 - - 4 
State of emergency 3.68 - 4 
Federal, state 3.68 - - 4 
State, including 3.68 - - 4 
Current state of 3.40 - 3 - 
State aid 3.40 - 3 
State where Farmers 3.40 3 
State of rnind 3.40 3 
Current state 3.40 3 
State thrift 3.40 - 3 
Distributable state aid 3.40 - 3 
State judges 3.40 3 
a state court 3.40 3 - 
said the state 3.40 3 
Several states 3.40 - 3 
State monopolies 3.40 - 3 
State laws 3.40 3 
State aid bonds 3.40 - 3 - 
Distributable state 3.40 - 3 
State and local 2.01 1 1 15 
Federal and state 1.60 1 5 - 
State court 1.38 - 12 3 - 
Other state. 1.38 4 1 - 
State$overnments 1.09 1 3 - 
Table  6: Local content-word collocations for 
State in WSJ  
Collocations Pr. 
Overall ocal content .597 
Overall ocal PoS&Fun .478 
Overall global content .442 
OVERALL .485 
N V \[Overall 
Cov. Pr. Cov. Pr. Cov. 
.338 591 .356 595 .344 
.999 ,491 .997 483 .998 
1.00:455 .999 .447 1.00 
1.00 497 1.00 489 1.00 
Tab le  7: Train on BC, tag WSJ 
N V i Overall 
Collocations Pr. Cov. Pr. Cov. i Pr. Cov. 
Overall ocal content 512 .273 .556 .336 530 .295 
Overall local PoS&Fun 421 1.00 .486 1.00 44.4 1.00 
Overall global content !.392 1.00 .423 1.00 403 1.00 
OVERALL 429 1.00 .483 1.00 448 1.00 
Tab le  8: Train on WSJ, tag BC 
211 
criteria) of the hand-taggers when dealing with 
words with difficult sense: distinctions. For 
instance, Table 10 shows some collocations of 
point which receive contradictory senses in the 
BC and the WSJ. The collocation important 
point, for instance, is assigned the second sense I 
in all 3 occurrences in the 13C, and the fourth 
sense 2in all 2 occurrences in the WSJ. 
We can therefore conclude that the one sense 
per collocation holds across corpora, as the 
contradictions found were due to tagging errors. 
The low amount of collocations in common 
would explain in itself the low figures on cross- 
corpora tagging. 
But yet, we wanted to further study the 
reasons of the low number of collocations in 
common, which causes the low cross-corpora 
performance. We thought of several factors that 
could come into play: 
a) As noted earlier, the training and test 
examples from the in-corpus experiments are 
taken at random, and they could be drawn 
from the same document. This could make 
the results appear better for in-corpora 
experiments. On the contrary, in the cross- 
corpora experiments training and testing 
example come from different documents. 
b) The genre and topic changes caused by the 
shift from one corpus to the other. 
c) Corpora have intrinsic features that carmot 
be captured by sole genre and topic 
variations. 
d) The size of the data, being small, would 
account for the low amount of collocations 
shared. 
We explore a) in Section 7 mad b) in Section 8. 
c) and d) are commented in Section 8. 
7 Drawing training and testing 
examples from the same documents 
affects performance 
In order to test whether drawing training and 
testing examples from the same document or not 
explains the different performance in in-corpora 
and cross-corpora tagging, low cross-corpora 
results, we performed the following experiment. 
Instead of organizing the 10 random subsets for 
cross-validation on the examples, we choose 10 
subsets of the documents (also at random). This 
i The second sense of point is defined as the precise 
location of something; a spatially limited location. 
2 Defined as an isolated fact that is considered 
separately from the whole. 
# Coll. # Coll. % Coil % Coll. Word PoS BC WSJ Shared Contradict. 
Age N 45 60 27 0 
Art N 24 35 34 20 
Body N 12 20 12 0 
Car N 92 99 17 0 
Child N 77 111 40 05 
Cost N 88 88 32 0 
Head N 77 95 07 33 
Interest N 80 141 32 33 
Line N 110 145 20 38 
Point N 44 44 32 86 
State N 196 214 28 48 
Thing N 197 183 66 52 
Work N 112 149 46 63 
Become V 182 225 51 15 
Fall V 36 68 19 60 
Grow V 61 71 36 33 
Lose V 63 56 47 43 
Set V 94 113 54 43 
Speak V 34 38 28 0 
Strike V 12 17 14 0 
Tell V 137 190 45 57 
Table 9: Collocations hared and m 
contradiction between BC and WSJ. 
BC WSJ Collocation #2 #4 Other #2 #4 Other 
important point 3 0 0 0 2 0 
pointofview 1 13 1 19 0 0 
Table 10: Contradictory senses of point 
way, the testing examples and training examples 
are guaranteed to come from different 
documents. We also think that this experiment 
would show more realistic performance figures, 
as a real application can not expect to find 
examples from the documents used for training. 
Unfortunately, there are not any explicit 
document boundaries, neither in the BC nor in 
the WSJ. 
In the BC, we took files as documents, even 
if files might contain more than one excerpt 
from different documents. This guarantees that 
document boundaries are not crossed. It has to 
be noted that following this organization, the 
target examples would share fewer examples 
from the same topic. The 168 files from the BC 
were divided in 10 subsets at random: we took 8 
subsets with 17 files and 2 subsets with 16 files. 
For the WSJ, the only cue was the directory 
organization. In this case we were unsure about 
the meaning of this organization, but hand 
inspection showed that document boundaries 
were not crossing discourse boundaries. The 61 
directories were divided in 9 subsets with 6 
directories and 1 subset with 7. 
212 
Again, 10-fold cross-validation was used, on 
these subsets and the results in Tables 11 and 12 
were obtained. The ,5 column shows the change 
in precision with respect to Tables 5 and 6. 
Table 12 shows that, for the BC, precision 
and coverage, compared to Table 5, are 
degraded significantly. On the contrary results 
for the WSJ are nearly the same (el. Tables 11 
and 4). 
The results for WSJ indicate that drawing 
training and testing data from the same or 
different documents in itself does not affect so 
much the results. On the other hand, the results 
for BC do degrade significantly. This could be 
explained by the greater variation in topic and 
genre between the files in the BC corpus. This 
will be further studied in Section 8. 
Table 13 summarizes the overall results on 
WSJ and BC for each of the different 
experiments performed. The figures show that 
drawing training and testing data from the same 
or different documents would not in any case 
explain the low figures in cross-corpora t gging. 
8 Genre and topic variation affects 
performance 
Trying to shed some light on this issue we 
observed that the category press:reportage, is
related to the genre/topics of the WSJ. We 
therefore designed the following experiment: we 
tagged each category in the BC with the 
decision lists trained on the WSJ, and also with 
the decision lists trained on the rest of the 
categories in the BC. 
Table 14 shows that the local content-word 
collocations trained in the WSJ attain the best 
precision and coverage for press:reportage, 
both compared to the results for the other 
categories, and to the results attained by the rest 
of the BC on press:reportage. That is: 
? From all the categories, the collocations from 
press:reportage are the most similar to those 
of WSJ. 
? WSJ contains collocations which are closer 
to those of press:reportage, than those from 
the rest of the BC. 
In other words, having related genre/topics help 
having common collocations, and therefore, 
warrant better word sense disambiguation 
performance. 
Overall Localcontent 
pr. coy. Apr. pr. cov. Apr. 
N .650 1.00 -.011 .762 .486 -.002 
V .634 1.00 -.001 .697 .494 -.040 
Overall .644 1.00 -.011 .738 .489 -.017 
Table 11: Train on WSJ, tag WSJ, 
crossvalidation according to files 
Overall Local content 
pr. cov. Apr. pr. cov. Apr. 
N .499 1.00 -.078 .573 .307 -.102 
V .543 1.00 -.021 .608 .379 -.027 
Overall .514 1.00 -.058 .587 .333 -.074 
Table 12: Train on BC, tag BC, 
crossvalidation according to files 
Overall (prec.) 
In-corpora In-corpora 
(examples) (files) Cross-corpora 
WSJ .652 .644 .489 
BC .572 .514 .448 
Table 13: Overall results in different 
experiments 
Category 
WSJ Rest of BC 
local content local content 
pr. coy. pr. cov. 
Press: Reportage .625 .330 .541 .285 
Press: Editorial .504 .283 .593 .334 
Press: Reviews .438 .268 .488 .404 
Religion .409 .306 .537 .326 
Skills and Hobbies .569 .296 .571 .302 
Popular Lore .488 .304 .563 .353 
Belles Lettres . . . . .  516 .272 .524 .314 
Miscellaneous .534 .321 .534 .304 
Learned .518 .257 .563 .280 
General Fiction .525 .239 .605 .321 
Mystery and . . . .  523 .243 .618 .369 
Science Fiction .459 .211 .586 .307 
Adventure and . . . .  551 .223 .702 .312 
Romance and . . . .  561 .271 .595 .340 
Humor .516 .321 .524 .337 
Table 14: Tagging different categories in BC. 
Best precision results are shown in bold. 
9 Reasons for cross-corpor a degradation 
The goal of sections 7 and 8 was to explore the 
possible causes for the low number of 
collocations in common between BC and WSJ. 
Section 7 concludes that drawing the examples 
from different files is not the main reason for 
the degradation. This is specially true when the 
corpus has low genre/topic variation (e.g. WSJ). 
Section 8 shows that sharing enre/topic s a key 
factor; as the WSJ corpus attains better esults 
on the press:reportage category than the rest of 
213 
the categories on the BC itself. Texts on the 
same genre/topic share more collocations than 
texts on disparate genre/topics, even if they 
come from different corpora. 
This seems to also rule out explanation c) 
(cf. Section 6), as a good measure of topic/genre 
similarity would help overcome cross-corpora 
problems. 
That only leaves the low amount of data 
available for this study (explanation d). It is true 
that data-scarcity can affect the number of 
collocations hared across corpora. We think 
that larger amounts will make', this number grow, 
especially if the corpus draws texts from 
different genres and topics. Nevertheless, the 
figures in Table 14 indicate that even in those 
conditions genre/topic relatedness would help to 
find common collocations. 
10 -Conclusions 
This paper shows that the one sense per 
collocation hypothesis is weaker for fine- 
grained word sense distinctions (e.g. those in 
WordNet): from the 99% precision mentioned 
for 2-way ambiguities in (Yarowsky, 1993) we 
drop to 70% figures. These figures could 
perhaps be improved using more available data. 
We also show that one sense per collocation 
does hold across corpora, but that collocations 
vary from one corpus to other, following genre 
and topic variations. This explains the low 
results when performing word sense 
disambiguation across corpora. In fact, we 
demonstrated that when two independent 
corpora share a related genre/topic, the word 
sense disambiguation results would be better. 
This has considerable impact in future work 
on word sense disambiguation, as genre and 
topic are shown to be crucial parameters. A 
system trained on a specific genre/topic would 
have difficulties to adapt to new genre/topics. 
Besides, methods that try to extend 
automatically the amount of examples for 
training need also to account for genre and topic 
variations. 
As a side effect, we have shown that the 
results on usual WSD exercises, which mix 
training and test data drawn from the same 
documents, are higher than those from a more 
realistic setting. 
We also discovered several hand-tagging 
errors, which distorted extracted collocations. 
We did not evaluate the extent of these errors, 
but they certainly affected the performance on 
cross-corpora t gging. 
Further work will focus on evaluating the 
separate weight of genre and topic in word sense 
disambiguation performance, and on studying 
the behavior of each particular word and 
features through genre and topic variations. We 
plan to devise ways to integrate genre/topic 
parameters into the word sense disambiguation 
models, and to apply them on a system to 
acquire training examples automatically. 
References 
Agirre, E. and D. Martinez. Exploring automatic 
word sense disambiguation with decision lists and 
the Web. Proceedings of the COLING Workshop 
on Semantic Annotation and Intelligent Content. 
Saarbrticken, Germany. 2000. 
Brants, T. TnT- A Statistical Part-of-Speech Tagger. 
In Proceedings of the Sixth Applied Natural 
Language Processing Conference, Seattle, WA. 
2000. 
Escudero, G. , L. Mhrquez and G. Rigau. On the 
Portability and Tuning of Supervised Word Sense 
Disambiguation Systems. In Proceedings of the 
Joint Sigdat Conference on Empirical Methods in 
Natural Language Processing and Very Large 
Corpora, Hong Kong. 2000. 
Francis, W. M. and H. Kucera. Brown Corpus 
Manual oflnformation. Department ofLinguistics, 
Brown University. Also available at 
http://khnt.hit.uib.no/icame/manuals/brown/. 1964. 
Gale, W., K. W. Church, and D. Yarowsky. A 
Method for Disambiguating Word Senses in a 
Large Corpus, Computers and the Humanities, 26, 
415--439, 1993. 
Ide, N. and J. Veronis. Introduction to the Special 
Issue on Word Sense Disambiguation: The State of 
the Art. Computational Linguistics, 24(1), 1--40, 
1998. 
Karlgren, J. and D. Cutting. Recognizing Text Genres 
with Simple Metrics Using Discriminant Analysis. 
Proceedings of the International Conference on 
Computational Linguistics. 1994 
Krovetz, R. More Than One Sense Per Discourse, 
Proceedings of SENSEVAL and the Lexicography 
Loop Workshop. http://www.itri.brighton.ac.uk/ 
events/senseval/PROCEEDINGS/. 1998 
Leacock, C., M. Chodorow, and G. A. Miller. Using 
Corpus Statistics and WordNet Relations for Sense 
Identification. Computational Linguistics, 24(1), 
147--166, 1998. 
Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, 
and K. Miller. Five Papers on WordNet. Special 
Issue of International Journal of Lexicography, 
3(4), 1990. 
214 
Miller, G. A., C. Leacock, R. Tengi, and R. T. 
Bunker, A Semantic Concordance. Proceedings of 
the ARPA Workshop on Human Language 
Technology, 1993. 
Ng, H. T. and H. B. Lee. Integrating Multiple 
Knowledge Sources to Disambiguate Word Sense: 
An Exemplar-based Approach. Proceedings of the 
34th Annual Meeting of the Association for 
Computational Linguistics. 1996. 
Ng, H. T., C. Y. Lira and S. K. Foo. A Case Study on 
Inter-Annotator Agreement for Word Sense 
Disambiguation. Proceedings of the Siglex-ACL 
Workshop on Standarizing Lexical Resources. 
1999. 
Yarowsky, D. One Sense per Collocation. Proc. of 
the 5th DARPA Speech and Natural Language 
Workshop. 1993 
Yarowsky, D. Decision Lists for Lexical Ambiguity 
Resolution: Application to Accent Restoration in 
Spanish and French. Proceedings of the 32rid 
Annual Meeting of the Association for 
Computational Linguistics, pp. 88--95. 1994. 
Yarowsky, D. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 
Cambridge, MA, pp. 189-196, 1995. 
Yarowsky, D. Homograph Disambiguation in Text- 
to-speech Synthesis. J Hirschburg, R. Sproat and J. 
Van Santen (eds.) Progress in Speech Synthesis, 
Springer-Vorlag, pp. 159-175. 1996. 
215 
Learning class-to-class selectional preferences 
Eneko Agirre 
IXA NLP Group 
University of the Basque Country 
649 pk. 20.080 
 Donostia. Spain. 
eneko@si.ehu.es 
David Martinez 
IXA NLP Group 
University of the Basque Country 
649 pk. 20.080 
 Donostia. Spain. 
jibmaird@si.ehu.es 
Abstract 
Selectional preference learning 
methods have usually focused on word-
to-class relations, e.g., a verb selects as 
its subject a given nominal class. This 
papers extends previous statistical 
models to class-to-class preferences, 
and presents a model that learns 
selectional preferences for classes of 
verbs. The motivation is twofold: 
different senses of a verb may have 
different preferences, and some classes 
of verbs can share preferences. The 
model is tested on a word sense 
disambiguation task which uses 
subject-verb and object-verb 
relationships extracted from a small 
sense-disambiguated corpus.  
1 Introduction 
Previous literature on selectional preference has 
usually learned preferences for words in the 
form of classes, e.g., the object of eat is an 
edible entity. This paper extends previous 
statistical models to classes of verbs, yielding a 
relation between classes in a hierarchy, as 
opposed to a relation between a word and a 
class.  
The model is trained using subject-verb and 
object-verb associations extracted from Semcor, 
a corpus (Miller et al, 1993) tagged with 
WordNet word-senses (Miller et al, 1990). The 
syntactic relations were extracted using the 
Minipar parser (Lin, 1993). A peculiarity of this 
exercise is the use of a small sense-
disambiguated corpus, in contrast to using a 
large corpus of ambiguous words. We think that 
two factors can help alleviate the scarcity of 
data: the fact that using disambiguated words 
provides purer data, and the ability to use classes 
of verbs in the preferences. Nevertheless, the 
approach can be easily extended to larger, non-
disambiguated corpora.  
We have defined a word sense 
disambiguation exercise in order to evaluate the 
extracted preferences, using a sample of words 
and a sample of documents, both from Semcor. 
Following this short introduction, section 2 
reviews selectional restriction acquisition. 
Section 3 explains our approach, which is 
formalized in sections 4 and 5. Next, section 6 
shows the results on the WSD experiment. Some 
of the acquired preferences are analysed in 
section 7. Finally, some conclusions are drawn 
and future work is outlined. 
2 Selectional preference learning 
Selectional preferences try to capture the fact 
that linguistic elements prefer arguments of a 
certain semantic class, e.g. a verb like ?eat? 
prefers as object edible things, and as subject 
animate entities, as in, (1) ?She was eating an 
apple?. Selectional preferences get more 
complex than it might seem: (2) ?The acid ate 
the metal?, (3) ?This car eats a lot of gas?, (4) 
?We ate our savings?, etc.   
Corpus-based approaches for selectional 
preference learning extract a number of  (e.g. 
verb/subject) relations from large corpora and 
use an algorithm to generalize from the set of 
nouns for each verb separately. Usually, nouns 
are generalized using classes (concepts) from a 
lexical knowledge base (e.g. WordNet).  
Resnik (1992, 1997) defines an information-
theoretic measure of the association between a 
verb and nominal WordNet classes: selectional 
association. He uses verb-argument pairs from 
Brown. Evaluation is performed applying 
intuition and WSD. Our measure follows in part 
from his formalization.  
Abe and Li (1995) follow a similar approach, 
but they employ a different information-
theoretic measure (the minimum description 
length principle) to select the set of concepts in a 
hierarchy that generalize best the selectional 
preferences for a verb. The argument pairs are 
extracted from the WSJ corpus, and evaluation 
is performed using intuition and PP-attachment 
resolution.  
Stetina et al (1998) extract word-arg-word 
triples for all possible combinations, and use a 
measure of  ?relational probability? based on 
frequency and similarity. They provide an 
algorithm to disambiguate all words in a 
sentence. It is directly applied to WSD with 
good results.  
3 Our approach 
The model explored in this paper emerges as a 
result of the following observations:  
? Distinguishing verb senses can be useful. 
The examples for eat above are taken from 
WordNet, and each corresponds to a 
different word sense1: example (1) is from 
the ?take in solid food? sense of eat, (2) 
from the ?cause to rust? sense, and 
examples (3) and (4) from the ?use up? 
sense.  
? If the word senses of a set of verbs are 
similar (e.g. word senses of ingestion verbs 
like eat, devour, ingest, etc.) they can have 
related selectional preferences, and we can 
generalize and say that a class of verbs has a 
particular selectional preference.  
Our formalization thus distinguishes among verb 
senses, that is, we treat each verb sense as a 
1                                                           
1 A note is in order to introduce the terminology used in the 
paper. We use concept and class indistinguishably, and 
they refer to the so-called synsets in WordNet. Concepts in 
WordNet are represented as sets of synonyms, e.g. <food, 
nutrient>. A word sense in WordNet is a word-concept 
pairing, e.g. given the concepts a=<chicken, poulet, 
volaille> and b=<wimp, chicken, crybaby> we can say 
that chicken has at least two word senses, the pair chicken-
a and the pair chicken-b. In fact the former is sense 1 of 
chicken, and the later is sense 3 of chicken. For the sake of 
simplicity we also talk about <chicken, poulet, volaille> 
being a word sense of chicken. 
different unit that has a particular selectional 
preference. From the selectional preferences of 
single verb word senses, we also infer 
selectional preferences for classes of verbs. 
Contrary to other methods (e.g. Li and 
Abe?s), we don?t try to find the classes which 
generalize best the selectional preferences. All 
possibilities, even the very low probability ones, 
are stored. 
The method stands as follows: we collect 
[noun-word-sense relation verb-word-sense] 
triples from Semcor, where the relation is either 
subject or object. As word senses refer to 
concepts, we also collect the triple for each 
possible combination of concepts that subsume 
the word senses in the triple. Direct frequencies 
and estimates of frequencies for classes are then 
used to compute probabilities for the triples.  
These probabilities could be used to 
disambiguate either nouns, verbs or both at the 
same time. For the time being, we have chosen 
to disambiguate nouns only, and therefore we 
compute the probability for a nominal concept, 
given that it is the subject/object of a particular 
verb. Note that when disambiguating we ignore 
the particular sense in which the governing verb 
occurs. 
4 Formalization 
As mentioned in the previous sections we are 
interested in modelling the probability of a 
nominal concept given that it is the 
subject/object of a particular verb: 
)|( vrelcnP i  (1) 
Before providing the formalization for our 
approach we present a model based on words 
and a model based on nominal-classes. Our 
class-to-class model is an extension of the 
second2. The estimation of the frequencies of 
classes are presented in the following section. 
1                                                           
2 Notation: v stands for a verb, cn (cv) stand for nominal 
(verbal) concept, cni (cvi ) stands for the concept linked to 
the i-th sense of the given noun (verb), rel could be any 
grammatical relation (in our case object or subject), ? 
stands for the subsumption relation, fr stands for frequency 
and rf? .for the estimation of the frequencies of classes. 
4.1 Word-to-word model: eat chickeni 
At this stage we do not use information of class 
subsumption. The probability of the first sense 
of chicken being an object of eat depends on 
how often does the concept linked to chicken1 
appear as object of the word eat, divided by the 
number of occurrences of eat with an object.  
)(
)()|( vrelfr
vrelcnfrvrelcnP ii =   (2) 
Note that instead of )|( vrelsenseP i  we use 
)|( vrelcnP i , as we count occurrences of 
concepts rather than word senses. This means 
that synonyms also count, e.g. poulet as 
synonyms of the first sense of chicken.  
4.2 word-to-class model:  
eat <food, nutrient> 
The probability of eat chicken1 depends on the 
probabilities of the concepts subsumed by and 
subsuming chicken1 being objects of eat. For 
instance, if chicken1 never appears as an object 
of eat, but other word senses under <food, 
nutrient> do, the probability of chicken1 will not 
be 0.  
Formula (3) shows that for all concepts 
subsuming cni the probability of cni given the 
more general concept times the probability of 
the more general concept being a subject/object 
of the verb is added. The first probability is 
estimated dividing the class frequencies of cni 
with the class frequencies of the more general 
concept. The second probability is estimated as 
in 4.1.  
4.3 class-to-class model:  
<ingest, take in, ?> <food, nutrient> 
The probability of eat chicken1 depends on the 
probabilities of all concepts above chicken1 
being objects of all concepts above the possible 
senses of eat. For instance, if devour never 
appeared on the training corpus, the model could 
infer its selectional preference from that of its 
??
??
?=?=
icncnicncn
vrelfr
vrelcnrf
cnrf
cncnrfvrelcnPcncnPvrelcnP iii )(
)(?
)(?
),(?)|()|()|(  (3) 
? ?
? ?
? ?
? ?
??=
??=
icncn cvcvvsenseofcv
icncn cvcvvsenseofcv
j
ji
j
j
jiji
cvrelfr
cvrelcnrf
cvrf
cvcvrf
cnrf
cncnrf
cvrelcnPcvcvPcncnPvrelcnP
max
max
)(
)(?
)(?
),(?
)(?
),(?
)|()|()|()|(
 (4) 
?
?
?=
cnicn
i
i
cnfrcnclassescnrf )()(
1)(?   (5) 
??
??
? ??
=
?
?
otherwise
cncnifcnfrcnclassescncnrf iij jji cncn
0
)()(
1
),(?  (6) 
?
?
?=
cnicn
vrelcnfrcnclassesvrelcnrf ii
)()(
1)(?  (7) 
? ?
? ?
??=
cnicn cnicv
ii
ii
cvrelcnfrcvclassescnclassescvrelcnrf )()(
1
)(
1)(?  (8) 
superclass <ingest, take in, ...>. As the verb can 
be polysemous, the sense with maximum 
probability is selected.  
Formula (4) shows that the maximum 
probability for the possible senses (cvj) of the 
verb is taken. For each possible verb concept 
(cv) and noun concept (cn) subsuming the target 
concepts (cni,cvj), the probability of the target 
concept given the subsuming concept (this is 
done twice, once for the verb, once for the noun) 
times the probability the nominal concept being 
subject/object of the verbal concept is added.  
5 Estimation of class frequencies 
Frequencies for classes can be counted directly 
from the corpus when the class is linked to a 
word sense that actually appears in the corpus, 
written as fr(cni). Otherwise they have to be 
estimated using the direct counts for all 
subsumed concepts, written as )(? icnrf . Formula 
(5) shows that all the counts for the subsumed 
concepts (cni) are added, but divided by the 
number of classes for which ci is a subclass (that 
is, all ancestors in the hierarchy). This is 
necessary to guarantee the following: 
?
? icncn
cncnP i )|( = 1. 
Formula (6) shows the estimated frequency 
of a concept given another concept. In the case 
of the first concept subsuming the second it is 0, 
otherwise the frequency is estimated as in (5). 
Formula (7) estimates the counts for 
[nominal-concept relation verb] triples for all 
possible nominal-concepts, which is based on 
the counts for the triples that actually occur in 
the corpus. All the counts for subsumed 
concepts are added, divided by the number of 
classes in order to guarantee the following: 
?
cn
vsubjcnP )|( =1 
Finally, formula (8) extends formula (7) to 
[nominal-concept relation verbal-concept] in a 
similar way. 
6 Training and testing on a WSD 
exercise 
For training we used the sense-disambiguated 
part of Brown, Semcor, which comprises around 
250.000 words tagged with WordNet word 
senses. The parser we used is Minipar. For this 
current experiment we only extracted verb-
object and verb-subject pairs. Overall 14.471 
verb-object pairs and 12.242 verb-subject pairs 
w
st
cl
fo
ex
no
co
W
co
T
ch
le
ra
m
of
th
1
3 
M
Noun # sens # occ 
# occ.  
as obj 
# occ.  
as subj 
account 10 27 8 3 
age 5 104 10 9 
church 3 128 19 10 
duty 3 25 8 1 
head 30 179 58 16 
interest 7 140 31 13 
member 5 74 13 11 
people 4 282 41 83 
Overall  67 959 188 146 
Table 1. Data for the selected nouns. 
 
 Prec.  
Obj 
Cov.  Rec Prec.  
Subj 
Cov. Rec. 
Random .192 1.00 .192 .192 1.00 .192 
MFS .690 1.00 .690 .690 1.00 .690 
Word2word .959 .260 .249 .742 .243 .180 
Word2class .669 .867 .580 .562 .834 .468 
Class2class .666 .973 .648 .540 .995 .537 
Table 2. Average results for the 8 nouns. ere extracted. For the sake of efficiency, we 
ored all possible class-to-class relations and 
ass frequencies at this point, as defined in 
rmulas (5) to (8).  
The acquired data was tested on a WSD 
ercise. The goal was to disambiguate all 
uns occurring as subjects and objects, but it 
uld be also used to disambiguate verbs. The 
SD algorithm just gets the frequencies and 
mputes the probabilities as they are needed. 
he word sense with the highest probability is 
osen.  
Two experiments were performed: on the 
xical sample we selected a set of 8 nouns at 
ndom3 and applied 10fold crossvalidation to 
ake use of all available examples. In the case 
 whole documents, they were withdrawn from 
e training corpus and tested in turn.  
                                                           
This set was also used on a previous paper (Agirre & 
artinez, 2000). 
Table 1 shows the data for the set of nouns. 
Note that only 19% (15%) of the occurrences of 
the nouns are objects (subjects) of any verb. 
Table 2 shows the average results using subject 
and object relations for each possible 
formalization. Each column shows respectively, 
the precision, the coverage over the occurrences 
with the given relation, and the recall. Random 
and most frequent baselines are also shown. 
Word-to-word gets the highest precision of all 
three, but it can only be applied on a few 
instances. Word-to-class gets slightly better 
precision than class-to-class, but class-to-class is 
near complete coverage and thus gets the best 
recall of all three. All are well above the random 
baseline, but slightly below the most frequent 
sense.  
On the all-nouns experiment, we 
disambiguated the nouns appearing in four files 
extracted from Semcor. We observed that not 
many nouns were related to a verb as object or 
subject (e.g. in the file br-a01 only 40% (16%) 
of the polisemous nouns were tagged as object 
(subject)). Table 3 illustrates the results on this 
task. Again, word-to-word obtains the best 
precision in all cases, but because of the lack of 
data the recall is low. Class-to-class attains the 
best recall.  
We think that given the small corpus 
available, the results are good. Note that there is 
no smoothing or cut-off value involved, and 
some decisions are taken with very little points 
of data. Sure enough both smoothing and cut-off 
values will allow to improve the precision. On 
the contrary, literature has shown that the most 
frequent sense baseline needs less training data.  
7 Analysis of the acquired selectional 
preferences 
In order to analyze the acquired selectional 
preferences, we wanted a word that did not 
occur too often and which had clearly 
distinguishable senses. The goal is to study the 
preferences that were applied in the 
disambiguation for all occurrences, and check 
what is the difference among each of the 
models.  
The selected word was church, which has three 
senses in WordNet, and occurs 19 times. Figure 
1 shows the three word senses and the 
corresponding subsuming concepts. Table 4 
shows the results of the disambiguation 
algorithm for church.  
Object Subject 
File Rand. MFS word2word word2class class2class Rand. MFS word2word word2class class2class 
br-a01 .286 .746 .138 .447 .542 .313 .884 .312 .640 .749 
br-b20 .233 .776 .093 .418 .487 .292 .780 .354 .580 .677 
br-j09 .254 .645 .071 .429 .399 .256 .761 .200 .500 .499 
br-r05 .269 .639 .126 .394 .577 .294 .720 .144 .601 .710 
Table 3. Average recall for the nouns in the four Semcor files. 
 
Sense 1 
church, Christian church, Christianity 
       => religion, faith 
           => institution, establishment 
               => organization, organisation 
                   => social group 
                       => group, grouping 
 
Sense 2 
church, church building 
       => place of worship, house of prayer,  
            house of God, house of worship 
           => building, edifice 
               => structure, construction 
                   => artifact, artefact 
                       => object, physical object 
                           => entity, something 
 
Sense 3 
church service, church 
       => service, religious service, divine service 
           => religious ceremony, religious ritual
               => ceremony 
                   => activity 
                       => act, human action, human activity 
Figure 1. Word senses and superclasses for church
In the word-to-word model, the model is 
unable to tag any of the examples4 (all the verbs 
related to ?church? were different). For church 
as object, both class-to-class and word-to-class 
have similar recall, but word-to-class has better 
precision. Notice that the majority of the 
examples with church as object were not tagged 
with the most frequent sense in Semcor, and 
therefore the MFS precision is remarkably low 
(21%). For church as subject, the class-to-class 
model has both better precision and coverage.  
We will now study in more detail each of the 
examples.  
7.1 Church as object 
There were 19 examples with church as object 
(15 tagged in Semcor with sense 2 and 4 with 
sense 1). Using the word-to-class model, 12 
were tagged correctly, 5 incorrectly and 2 had 
not enough data to answer. In the class-to-class 
model 12 examples were tagged correctly and 7 
incorrectly. Therefore there was no gain in 
recall.  
First, we will analyze the results of the 
word-to-class model. From the 12 hits, 10 
corresponded to sense 2 and the other 2 to sense 
1. Here we show the 12 verbs and the 
superconcept of the senses of church that gets 
the highest selectional preference probability, 
and thus selects the winning sense, in this case, 
correctly.  
? Tagged with sense 2: 
 look: <buil
 have: <buil
1                                 
4 Note that we applied 10fo
not able to tag anything b
samples do not appear in t
the verbs governing church 
 demolish: <building, edifice> 
 move:  <structure, construction> 
 support:  <structure, construction> 
 build:  <structure, construction> 
 enter:  <structure, construction> 
 sell:  <artifact, artefact> 
 abandon:  <artifact, artefact> 
 see:  <artifact, artefact> 
? Tagged with sense 1 
 strengthen:  <organization, organisation> 
 turn_to:  <organization, organisation> 
The five examples where the model failed 
revealed different types of errors. We will check 
each of the verbs in turn. 
1. Attend (Semcor 2, word-to-class 1)5: We 
quote the whole sentence:  
From many sides come remarks that 
Protestant churches are badly attended and 
the large medieval cathedrals look all but 
empty during services . 
We think that the correct sense should be 3 ( 
?church services? are attended, not the 
buildings). In any case, the class that gets the 
higher weight is <institution, establishment>, 
pointing to sense 1 of church and beating the 
more appropriate class <religious ceremony, 
religious ritual> because of the lack of 
examples in the training.  
2. Join (Semcor 1, word-to-class 2): It seems 
that this verb should be a good clue for sense 1.  
But among the few occurrences of join in the 
training set there were ?join-obj-temple? and 
oth temple and 
 have organization-
et and they were thus 
 under <building, 
                  
se in Semcor (the correct 
igned by the model. 
 #occ OK KO No ansr Prec. Cov. Rec. 
obj MFS 19 4 15 0 .210 1.00 .210 
obj word-to-word 19 0 0 19 .000 .000 .000 
obj word-to-class 19 12 5 2 .705 .894 .631 
obj class-to-class 19 12 7 0 .631 1.00 .631 
subj MFS 10 8 2 0 .800 1.00 .800 
subj word-to-word 10 0 0 10 .000 .000 .000 
subj word-to-class 10 4 3 3 .571 .700 .400 
subj class-to-class 10 6 4 0 .600 1.00 .600 
Table 4: Results disambiguating the word church. ding, edifice> 
ding, edifice> 
                          
ld crossvalidation. The model is 
ecause the verbs in the testing 
he training samples. In fact all  
occur only once. 
?join-obj-synagogue?. B
synagogue have do not
related concepts in WordN
tagged with a concept
1                                         
5 For each verb we list the sen
reference sense) and the sense ass
edifice>. This implies that <place of worship, 
house of prayer, house of God, house of 
worship> gets most credit and the answer is 
sense 2.  
3. Imprison (Semcor 1, word-to-class 3): The 
scarcity of training examples is very evident 
here. There are only 2 examples of imprison 
with an object, one of them wrongly selected by 
Minipar (imprison-obj-trip) that falls under 
<act, human action, human activity> and points 
to sense 3.  
4. Empty (Semcor 2, word-to-class 1): The 
different senses of empty introduce misleading 
examples. The best credit is given to <group, 
grouping> (following an sense of empty which 
is not appropriate here) which selects the sense 1 
of church. The correct sense of empty in this 
context relates with <object, physical object>, 
and would thus select the correct sense, but does 
not have enough credit.  
5. Advance (Semcor 2, word-to-class 3): the 
misleading senses of ?advance? and the low 
number of examples point to sense 3.  
We thus identified 4 sources of error in the 
word-to-class model: 
A. Incorrect Semcor tag 
B. Wrongly extracted verb-object relations 
C. Scarcity of data 
D. Misleading verb senses 
The class-to-class model should help to 
mitigate the effects of errors type C and D. We 
would specially hope for the class-to-class 
model to discard misleading verb senses. We 
now turn to analyze the results of this model.  
From the 12 correct examples tagged using 
word-to-class, we observed that 3 were 
mistagged using class-to-class. The reason was 
that the class-to-class introduces new examples 
from verbs that are superclasses of the target 
verb, and these introduced noise. For example, 
we examined the verb turn_to (tagged in Semcor 
with sense 1):  
1. turn-to (Semcor 1, word-to-class 1): there are 
fewer training examples than in the class-to-
class model and they get more credit. The 
relation ?turn_to-obj-platoon? gives weight to 
the class <organization, organisation>.  
2. turn-to (Semcor 1, class-to-class 2): the 
relations ?take_up-obj-position? and ?call_on-
obj-esprit_de_corps? introduce noise and point 
to the class <artifact, artefact>. As a result, the 
sense 2 is wrongly selected.  
From the 5 mistagged examples in class-to-
class, only ?empty? was tagged correctly using 
classes (in this case the class-to-class model is 
able to select the correct sense of the verb, 
discarding the misleading senses of empty):  
1. Attend, Join, Advance: they had errors of 
type A and B (incorrect Semcor tag/ misleading 
verb-object relations) and we can not expect the 
?class-to-class? model to handle them.  
2. Imprison: still has not enough information to 
make a good choice.  
3. Empty (Semcor 2, class-to-class 2): new 
examples associated to the appropriate sense of 
empty give credit to the classes <place of 
worship, house of prayer, house of God, house 
of worship> and <church, church building>. 
With the weight of these classes the correct 
sense 2 is correctly chosen.  
Finally, the 2 examples that received no 
answer in the ?word-to-class? model were 
tagged correctly:  
1. Flurry (Semcor 2, class-to-class 2): the 
answer is correct although the choice is made 
with few data. The strongest class is <structure, 
construction>. 
2. Rebuild (Semcor 2, class-to-class 2): the new 
information points to the appropriate sense.  
7.2 Church as subject 
The class2class model showed a better behavior 
with the examples in which church appeared as 
subject. There were only 10 examples, 8 tagged 
with sense 1 and 2 with sense 2.  
In this case, the class-to-class model tagged 
in the same way the examples tagged by the 
class-to-word model, but it also tagged the 3 
occurrences that had not been tagged by the 
word-to-class model (2 correctly and 1 
incorrectly).  
8 Conclusions 
We presented a statistical model that extends 
selectional preference to classes of verbs, 
yielding a relation between classes in a 
hierarchy, as opposed to a relation between a 
word and a class. The motivation is twofold: 
different senses of a verb may have different 
preferences, and some classes of verbs can share 
preferences. 
The model is trained using subject-verb and 
object-verb relations extracted from a sense-
disambiguated corpus using Minipar. A 
peculiarity of this exercise is the use of a small 
sense-disambiguated corpus, in contrast to using 
a large corpus of ambiguous words.  
Contrary to other methods we do not try to 
find the classes which generalize best the 
selectional preferences. All possibilities, even 
the ones with very low probability, are stored. 
Evaluation is based on a word sense 
disambiguation exercise for a sample of words 
and a sample of documents from Semcor. The 
proposed model gets similar results on precision 
but significantly better recall than the classical 
word-to-class model.  
We plan to train the model on a large 
untagged corpus, in order to compare the quality 
of the acquired selectional preferences with 
those extracted from this small tagged corpora. 
The model can easily be extended to 
disambiguate other relations and POS. At 
present we are also integrating the model on a 
supervised WSD algorithm that uses decision 
lists.  
References 
Abe, H. & Li, N. 1996. Learning Word Association 
Norms Using Tree Cut Pair Models. In 
Proceedings of the 13th International Conference 
on Machine Learning ICML.  
Agirre E. and Martinez D. 2000. Decision lists and 
automatic word sense disambiguation. COLING 
2000, Workshop on Semantic Annotation and 
Intelligent Content. Luxembourg.  
Lin, D. 1993. Principle Based parsing without 
Overgeneration. In 31st Annual Meeting of the 
Association for Computational Linguistics. 
Columbus, Ohio. pp 112-120.  
Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, 
and K. Miller. 1990. Five Papers on WordNet. 
Special Issue of the International Journal of 
Lexicography, 3(4).  
Miller, G. A., C. Leacock, R. Tengi, and R. T. 
Bunker. 1993. A Semantic Concordance. 
Proceedings of the ARPA Workshop on Human 
Language Technology.  
Resnik, P. 1992. A class-based approach to lexical 
discovery. In Proceedings of the Proceedings of 
the 30th Annual Meeting of the Association for 
Computational Linguists., . 327-329.  
Resnik,P. 1997. Selectional Preference and Sense 
Disambiguation.. In Proceedings of the ANLP 
Workshop ``Tagging Text with Lexical 
Semantics: Why What and How?''., Washington, 
DC.  
Stetina J., Kurohashi S., Nagao M. 1998. General 
Word Sense Disambiguation Method Based on a 
Full Sentential Context. In Usage of WordNet in 
Natural Language Processing , Proceedings of 
COLING-ACL Workshop. Montreal (C Canada).  
 
 
A Multilingual Approach to Disambiguate Prepositions  
and Case Suffixes 
Eneko Agirre, Mikel Lersundi, David Martinez 
 
IxA NLP group 
University of the Basque Country 
649 pk. - 20.080 Donostia (Spain) 
{eneko, jialeaym, jibmaird}@si.ehu.es 
 
Abstract 
This paper presents preliminary 
experiments in the use of translation 
equivalences to disambiguate 
prepositions or case suffixes. The core 
of the method is to find translations of 
the occurrence of the target preposition 
or case suffix, and assign the 
intersection of their set of 
interpretations. Given a table with 
prepositions and their possible 
interpretations, the method is fully 
automatic. We have tested this method 
on the occurrences of the Basque 
instrumental case -z in the definitions of 
a Basque dictionary, looking for the 
translations in the definitions from 3 
Spanish and 3 English dictionaries. The 
results have been that we are able to 
disambiguate with 94.5% accuracy 
2.3% of those occurrences (up to 91). 
The ambiguity is reduced from 7 
readings down to 3.1. The results are 
very encouraging given the simple 
techniques used, and show great 
potential for improvement. 
1 Introduction 
This paper presents some preliminary experiments 
in the use of translation equivalences to 
disambiguate the interpretations of case suffixes in 
Basque. Basque is an agglutinative language, and 
its case suffixes are more or less equivalent to 
prepositions, but are also used to mark the subject 
and objects of verbs. The method is general, and 
could be as easily applied to prepositions in any 
other language. The core of the method is to find a 
preposition in the translation of an occurrence of 
the target case suffix, and select the 
interpretation(s) in the intersection of both as the 
valid interpretation(s). At this point, we have not 
used additional sources for the disambiguation, 
e.g. governing verbs, nouns, etc., but they could 
complement the technique here presented. 
In this particular experiment, the method was 
tested on the definitions of a Basque monolingual 
dictionary, using the -z instrumental as the target 
case suffix. The main reason is that we are in the 
process of building a Lexical Knowledge Base out 
of dictionary definitions, and the disambiguation 
of case suffixes and other semantic dependencies 
is of great interest. 
The method searches for the respective 
definitions in English and Spanish monolingual 
dictionaries and tries to find a preposition that is 
the translation of the target case suffix. Once the 
preposition is found, the intersection of the set of 
interpretations of both the source case suffix and 
the translated preposition is taken, and the 
outcome is stored. 
The resources needed to perform this task are 
the following: lemmatizers, bilingual dictionaries 
and monolingual dictionaries, as well as a table of 
possible interpretations of prepositions and case 
suffixes. In our case, we have used Basque, 
English and Spanish lemmatizers, Basque/English 
and Basque/Spanish bilingual dictionaries, a target 
Basque monolingual dictionary, 3 Spanish and 3 
English monolingual dictionaries.  
The method is fully automatic; the Spanish and 
English monolingual dictionaries are accessed 
from the Internet, and the rest are local, installed 
in our machines. The manual work has been to 
build the table with possible interpretations of the 
prepositions and case suffixes. 
                       July 2002, pp. 1-8.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
The paper is structured as follows. Section 2 
presents the method for disambiguation in detail. 
Section 3 introduces the interpretations for the 
case suffix and the prepositions. The results are 
shown in Section 4, which are further discussed in 
Section 5. Finally, section 6 presents the 
conclusions and future work.  
2 Method for disambiguation 
The goal of the method is to disambiguate 
between the possible interpretations of a case 
suffix appearing in any text. We have taken as the 
target text the definitions from a monolingual 
Basque dictionary Euskal Hiztegia, EH in short 
(Sarasola, 1996). The method consists on five 
steps:  
? Extraction of the definitions in EH where the 
target case suffix occurs.  
? Search of on-line Spanish and English 
dictionaries to obtain the translation 
equivalent of the definitions.  
? Extraction of the target preposition from the 
translation definitions.  
? Disambiguation based on the intersection of 
the interpretations of case suffix and 
prepositions.  
We will explain each step in turn. 
2.1 Extraction of relations from EH 
Given a case suffix, in this step we will search the 
EH dictionary for occurrences of the case suffix. 
We first lemmatize and perform morphological 
analysis of the definitions (Aduriz et. al, 1996). 
The definitions that contain the target case suffix 
in a morphological analysis are extracted, storing 
the following information: the Basque dictionary 
entry of the definition, the lemma that has the case 
suffix, the case suffix, and the following lemma.  
Below we can see a sample definition, its 
lemmatized version, and the two triples extracted 
from this definition. The occurrences of the 
instrumental -z are shown in bold. 
 
Ildo iz. A1 Goldeaz lurra irauliz 
egiten den irekidura luzea1  
                                                     
1 The literal translation of the definition is the 
following : furrow, a long trench produced turning 
 
/<@@lema ildo>/<ID>/ 
/<@@Adiera_string A1.>/<ID>/ 
/<@@Kategoria iz. >/<ID>/ 
"<Goldeaz>" 
  "golde"  IZE ARR DEK INS NUMS MUGM  
"<lurra>" 
  "lur"  IZE ARR DEK ABS NUMS MUGM  
"<irauliz>" 
  "irauli"  ADI SIN AMM PART DEK INS MG  
"<egiten>" 
  "egin"  ADI SIN AMM ADOIN ASP EZBU  
"<den>" 
  "izan"  ADL A1 NOR NR_HU ERL MEN ERLT  
"<irekidura>" 
  "irekidura"  IZE ARR DEK ABS MG  
"<luzea>" 
  "luze"  ADJ IZO DEK ABS NUMS MUGM  
"<$.>" 
  PUNT_PUNT 
 
golde#INS#lur2 
irauli#INS#egin 
 
Extracting lemma-suffix-lemma triples in this 
simple way leads to some errors (cf. section 5.1). 
For instance, the first triple should rather be the 
dependency golde#INS#irauli (plow#with#turn, to 
be read in reverse order). We will see that even in 
this case we will be able to obtain correct 
translations and disambiguate the preposition 
correctly. Nevertheless, in the future we plan to 
use a syntactic parser to identify better the lemmas 
that are related by the case suffix.  
2.2 Search for Spanish/English 
translations 
After we have a list of entries in the Basque 
dictionary that contain the lemma-suffix-lemma 
triple, we search for their equivalent definitions in 
Spanish and English. We first look up the entry in 
the bilingual dictionary, and then retrieve the 
                                                                                  
over the ground with a plow.  
2 The translation of the first triple is plow#with#ground, 
to be read on reverse. The translation of the second is 
turn#NULL#produce, to be also read on reverse. In this 
second triple the instrumental case suffix is not 
translated explicitly by a preposition, but by a syntactic 
construct. 
definitions for each of the possible translations 
from the monolingual dictionaries. 
We use two bilingual and 6 monolingual 
Machine Readable Dictionaries: Morris 
Basque/English dictionary (Morris, 1998) Elhuyar 
Basque/Spanish dictionary (Elhuyar, 1996); 
English monolingual on-line dictionaries are: 
Cambridge (online), Heritage (online), and 
Wordsmyth (online); and Spanish monolingual 
on-line dictionaries are: Colmex (online), Rae 
(online), and Vox (online). The Basque dictionary 
and the bilingual dictionaries are stored in a local 
server, while the monolingual dictionaries are 
accessed from the Internet using a wrapper. 
The incomplete list of the translation of ildo 
(furrow in English, surco in Spanish) is shown 
below. Note that we got two different definitions 
for surco, coming from different Spanish 
dictionaries. 
 
furrow#A long , narrow , shallow 
trench made in the ground by a 
plow  
 
surco#Excavaci?n alargada , angosta y 
poco profunda que se hace 
paralelamente en la tierra con el 
arado , para sembrarla despu?s  
 
surco#Hendedura que se hace en la 
tierra con el arado  
2.3 Extraction of Spanish/English 
equivalent relations 
Given a list of definitions in Spanish and English, 
we search in the definition the translation of the 
Basque triple found in step 2.1, that is, we look for 
a triple of consecutive words where the first word 
is the translation of the last word in the Basque 
triple, the second word is a preposition (which 
corresponds to the Basque suffix) and the third 
word is the translation of the first word in the 
Basque triple. Between the preposition and the last 
word in the triple we allow for the presence of a 
determiner or an adjective in the text. More 
complex patterns could be allowed, up to full 
syntactic analyses, but at this point we follow this 
simple scheme. 
Below we can find the triples for 
golde#INS#lur, obtained from the three definitions 
above. One triple is obtained twice from two 
different definitions. 
 
furrow#ground#by#plow 
surco#tierra#con#arado 
surco#tierra#con#arado 
 
Definitions that do not have a matching triple 
are discarded, leaving Basque triples without 
matching triple ambiguous. For instance we could 
not find triples for irauli#INS#egin(cf. example in 
section 2.1). The instrumental suffix is sometimes 
translated without prepositions (in this case ?? 
made turning ??). 
Looking up the bilingual dictionaries for 
translation requires lemmatization and Part of 
Speech tagging. For English we use the TnT PoS 
tagger (Brants, 2000) and WordNet for 
lemmatization (Miller et al, 1990). For Spanish 
we use (Atserias et al, 1998).  
2.4 Disambiguation 
For each Basque case suffix, Spanish preposition 
and English preposition we have a list of 
interpretations (cf. Table 1). We assign the 
interpretations of the preposition to each 
Spanish/English triple. The intersection of all the 
interpretations is assigned to it. 
Continuing with out example, we can see that 
the intersection between the interpretations of the 
English by preposition (three interpretations) and 
the interpretations of the Spanish con preposition 
(four interpretations) are manner and instrument. 
Therefore, we can say that the Basque 
instrumental case interpretation in this case will 
be manner or instrument. 
 
furrow#ground#by a#plow# 
manner instrument during-time 
surco#tierra#con el#arado# 
manner instrument cause containing 
 
golde#INS#lur#instrument manner 
3 Interpretations for the 
instrumental case suffix and 
equivalent prepositions 
The method explained in the previous section is 
fully automatic, and it only requires the list of 
interpretations for each case suffix and 
preposition. In this work, we want to evaluate if 
the overall approach is feasible, so we selected 
Basque as the target language and a single case 
suffix, -z the instrumental case. Table 1 shows the 
list of possible interpretations and Table 2 and 3 
examples for each interpretation. 
The sources for the interpretations of the 
instrumental case have been a grammar of Basque 
(Euskaltzaindia, 1985) and a bilingual dictionary 
(Elhuyar, 1996). Possible interpretations for 
Spanish and English prepositions have been taken 
from an English dictionary (Cambridge, online), a 
Spanish dictionary (Vox, online) and a Spanish 
grammar (Bosque & Demonte, 1999).  
For this work we have taken a descriptive 
approach, but other more theoretically committed 
approaches are also possible. The overall method 
is independent of the set of interpretations, as it 
only needs a table of possible interpretations in the 
style of Table 1. Section 5.4 further discusses 
other alternatives. 
In order to disambiguate the occurrences of the 
instrumental case suffix we have taken the 
Spanish and English translations for this case 
suffix. The list of possible translations is 
preliminary and covers what we found necessary 
to make this experiment. Table 1 shows the list of 
prepositions and interpretations for Spanish and 
English. Examples of the interpretations can be 
found in Table 2. The Spanish preposition de had 
the same interpretations as the instrumental case 
suffix (cf. Table1), so it was discarded. 
4 Results 
The instrumental case occurs in 4,004 different 
definitions in the EH dictionary. The algorithm in 
Section 2 was applied to all these definitions, 
yielding a result for 125 triples, 3.1% of the total. 
The triples for which we had an answer were 
tagged by hand independently, i.e. not consulting 
the results output by the algorithm. The hand-
tagged set constitutes what we call the gold 
standard. 
A single linguist made the tagging, consulting 
other teammates when in doubt. Apart from 
marking the interpretation, there were some other 
special cases. 
1. In some of the examples, the instrumental 
case was part of a more complex scheme, and 
was tagged accordingly: 
? Part of a postposition (XPOST), e.g. -en 
bidez (by means of) or -en ordez (instead 
of). 
? Part of a conjunction (XLOK), e.g. batez 
ere (specially). 
? Part of a compounded suffix ?zko 
(XZKO), which results from the 
aggregation of the instrumental ?z  with 
the location genitive -ko. 
2. There were three errors in the lemmatization 
process (XLEM), due to lexicalized items, e.g. 
gizonezko (meaning male person).  
3. Finally, the relation in the definition was 
sometimes wrongly retrieved, e.g.  
? The triple would contain the determiner or 
an adjective instead of the dependencies. 
We thought that the algorithm would be 
able to work well even with those cases, 
so we decided to keep them. 
? The triple contains a conjunction (X): 
these were tagged as incorrect. 
Table 4 shows the amount of such cases, 
alongside the frequency of each interpretation. 
The most frequent interpretation is instrument. In 
seven examples, the linguist decided to keep two 
interpretations: instrument and manner. In a single 
example, the linguist was unable to select an 
interpretation, so this example was discarded. 
The output of the algorithm was compared 
with the gold standard, yielding the accuracy 
figures in Table 5. An output was considered 
correct if it yielded at least one interpretation in 
common with the gold standard. The accuracy is 
given for each dictionary in isolation, or merging 
all the results (as mentioned in section 2, when 
two dictionaries propose interpretations for the 
same triple, their intersection is taken). The 
remaining ambiguity is 3.1 overall. 
  Basque English Spanish 
 -z (ins.) of by with in de con a en 
theme x x   x x  x  
during-time x x x   x    
instrument x  x x x x x  x 
manner x  x  x x x x x 
cause x x  x x x x   
containing x x  x x x x   
matter x x    x    
Table 1: interpretations for the instrumental case in Basque and its equivalents in English and Spanish. 
 
 Basque English 
theme Seguru nago horretaz 
Matematikaz asko daki 
I?m sure of that 
He?s an expert in maths 
during-time Arratsaldez lasai egon nahi dut 
Gauez egin dut 
I like to relax of an evening 
I did it by night 
instrument Autobusez etorri naiz 
Belarra segaz moztu 
Euskaraz hitz egin 
I have come by bus 
To cut grass with a scythe 
To speak in Basque 
manner Animali baten hestea betez egindako haragia
 
Ahots ozen batez 
A meat preparation made by filling an 
animal intestine 
In a loud voice 
cause Haren aitzakiez nekatuta nago  
Beldurrez zurbildu 
Kanpoan lan egitea baztertu zuenez, lan-
aukera ederra galdu zuen 
Sick of his excuses 
To turn white of fear 
In refusing to work abroad, she missed an 
excellent job opportunity 
containing Edalontzia ardoz beteta dago 
Txapelaz dagoen gizona 
Ilez estalia 
The glass is full of wine 
The man with the beret on 
Cover in hair 
matter Armairua egurrez egina dago The wardrobe is made of wood 
Table 2: examples in Basque and English for the set of possible interpretations. 
 
 Basque Spanish 
theme Mariaz aritu dira 
Honetaz ziur naiz 
Han mencionado a  Maria 
Estoy seguro de esto 
during-time Gauez egin dut Lo he hecho de noche 
instrument Belarra segaz moztu 
Euskaraz hitz egin 
Hiria harresiz inguratu dute 
Cortar la hierba con la guada?a 
Hablar en vasco 
Han cubierto la ciudad de murallas 
manner Oinez etorri zen 
Ahots ozen batez 
Bere familiaren laguntzaz erosi zuen 
Berdez margotzen ari dira 
Vino a pie 
En voz alta 
Lo compr? con la ayuda de su familia 
Lo estan pintando de verde 
cause Beldurrez zurbildu 
Maitasunez hil 
Con el miedo me qued? p?lido 
Morir de amor 
containing Edalontzia ardoz beteta dago 
Txapelaz dagoen gizona ikusi dut 
El baso esta lleno de vino 
He visto a un hombre con boina 
matter Armairua egurrez egina dago El armario est? hecho de madera 
Table 3: examples in Basque and Spanish for the set of possible interpretations. 
Table 4 also shows the most frequent baseline 
(MF), constructed as follows: for each occurrence 
of the suffix, the three most frequent 
interpretations are chosen. The accuracy of this 
baseline is practically equal to that of the 
algorithm. Note that the frequency is computed on 
the same sample where it is applied, yielding 
better results than it should. 
5 Discussion 
The obtained results show a very good accuracy, 
leaving a remaining ambiguity of 3.1 results per 
example. This means that we were able to discard 
an average of 4 readings for each of the examples, 
introducing only 5.5% of error. The results are 
practically equal to the most frequent baseline, 
which is usually hard to beat using knowledge-
based techniques. 
Coverage of the method is very low, only 
2.3%, but this was not an issue for us, as we plan 
to couple this method with other Machine 
Learning techniques in a bootstrapping 
framework. Nevertheless, we are still interested in 
increasing the coverage, in order to obtain more 
training data. 
Next, we will analyze more in depth the causes 
of the low coverage, the sources of the errors and 
ambiguity and the interpretations of case suffixes 
and prepositions. 
5.1 Sources of low coverage 
As soon as we started devising this method, it was 
clear to us that the coverage will be rather low. 
The main reason is that different dictionaries tend 
to give different details in their definitions, or use 
differing paraphrases. This fact is intrinsic to our 
method, and accounts for the large majority of 
missing answers. 
On the other hand, the simple method used to 
find triples means that a change in the order of the 
complements will cause our method to fail 
looking for a translation triple. Syntactic analysis, 
even shallow parsing methods, will help increase 
the coverage. 
Another source of discarded triples are the 
cases where the suffix is not translated by a 
preposition, e.g. the relation is carried out by a 
subject or direct object. When syntactic analysis is 
performed, we
interpretations o
5.2 Sources
Only five errors
were caused 
especially whe
determiner inste
- xixta/pric
needle 
- luma/feed
a submarine
There errors
parser. Other 
# interpretation 
   8 XPOST 
1 XLOK 
12 XZKO 
   3 XLEM 
   9 X 
1 No interpretation 
34 Total discarded 
  37 instrument 
  35 containing 
   7 instrument manner 
6 manner 
5 theme 
   1 cause 
0 matter 
0 during-time 
Table 4: fre
 
Dictionary 
cambridge 
Am. heritage
wordsmith 
Colmex 
vox_ya 
Rae 
overall  
MF baseline 
Table 5: result  
combination fo also plan to incorporate the 
f the other syntactic relations. 
 of error  
 we made by the algorithm, which 
by the wrong triple pairings, 
n the Basque triple contained a 
ad of the related word. Examples: 
k: punta batez osatua/made by a 
le: odi batez osatua/wake made by 
 
 could be avoided using a syntactic 
wrong pairings were caused by 
91 Total kept 
quency of tags in gold standard. 
total correct accur. ambig.
16 15 0.938 4.0
34 32 0.941 3.2
26 26 1.000 3.7
10 9 0.900 2.6
7 7 1.000 2.8
26 25 0.962 2.8
91 86 0.945 3.1
91 85 0.934 3.0
s for each of the dictionaries, overall
r all and the most frequent baseline. 
errors in the English PoS tagger, or chance made 
the algorithm find an unrelated definition.  
5.3 Remaining ambiguity 
The amount of readings left by our method in this 
experiment is rather high, around 3.1 readings 
compared to 7 possible readings for the 
instrumental. This is a strong reduction but we 
would like to make it even smaller. 
We plan to study which is the source of the 
residual ambiguity. Alternative sets of 
interpretations (cf. Section 5.4) with coarser 
grained differences and smaller ambiguity, could 
yield better results. Another alternative is to 
explore more infrequent translations of the case 
suffixes, which might yield a narrower overlap. 
This is the case for the instrumental case suffix 
being translated with from, up, etc. 
5.4 Interpretations of case suffixes and 
prepositions 
Different authors give differing interpretations for 
prepositions. It has been our choice to take a 
descriptive list of possible interpretations from a 
set of sources, mainly dictionaries and grammar 
books. 
This work covers only the instrumental case 
suffix and its translations to English and Spanish. 
If tables for all case suffixes and prepositions were 
built, the method could be applied to all case 
suffixes and prepositions, yielding disambiguated 
relations in all three languages. 
More theoretically committed lists of 
interpretations (Dorr et al, 1998; Civit et al, 
2000; Sowa, 2000) should also be considered, but 
unfortunately we have not found a full account for 
all prepositions. If such a full table of 
interpretations existed, it could be very easy to 
apply our method, and obtain the outcome in 
terms of these other interpretations. 
6 Conclusion and further work 
This paper presents preliminary experiments in the 
use of translation equivalences to disambiguate 
prepositions or case suffixes. The core of the 
method is to find translations of the occurrence of 
the target preposition or case suffix, and assign the 
intersection of their set of interpretations. The 
method is fully automatic, given a table with 
prepositions and their possible interpretations.  
We have tested this method on the occurrences 
of the Basque instrumental case -z in the 
definitions of a Basque dictionary. We have 
searched the translations in the definitions from 3 
Spanish and 3 English dictionaries.  
The results have been that we are able to 
disambiguate with 94.5% accuracy 2.3% of those 
occurrences (up to 91). The ambiguity is reduced 
from 7 readings down to 3.1. We think that these 
are very good results, especially seeing that there 
is room for improvement.  
More specifically, we plan to apply surface 
syntactic analysis to better extract the dependency 
relations, which is the main source of errors. We 
would like to study other inventories of 
preposition interpretations, both in order to have 
better theoretical foundations as well as to 
investigate whether coarser grained distinctions 
would lead to a reduction in the ambiguity.  
In the future, we plan to explore the possibility 
to feed a Machine Learning algorithm with the 
automatically disambiguated examples, in order to 
construct a full-fledged disambiguation algorithm 
following a bootstrapping approach. On the other 
hand, we would like to apply the method to the set 
of all prepositions and case suffixes, and beyond 
that to all syntactic dependencies. The results will 
be directly loaded in a Lexical Knowledge Base 
extracted from the Basque dictionary (Ansa et al, 
in prep.). 
We also plan to explore whether this method 
can be applied to free running text, removing the 
constraint that the translations have to be 
definitions of the equivalent word. 
Finally, this technique could be coupled with 
techniques that make use of the semantic types of 
the words in the context. 
Overall, we found the results are very 
encouraging given the simple techniques used, 
and we think that it shows great potential for 
improvement and interesting avenues for research. 
Acknowledgments 
Mikel Lersundi and David Martinez were 
supported by Basque Government grants AE-
BFI:98.217 and AE-BFI:01.2485. This work was 
partially funded by the MCYT HERMES project 
(TIC-2000-0335) and the EC MEANING project 
(IST-2001-34460). 
References 
Aduriz I., Aldezabal I., Alegria I., Artola X., 
Ezeiza N., Urizar R., 1996, "EUSLEM: A 
Lemmatiser / Tagger for Basque" Proc. Of 
EURALEX'96, G?teborg (Sweden) Part 1, 17-26. 
Ansa O., Arregi X., Lersundi M., ?A 
Conceptual Schema for a Basque Lexical-
Semantic Framework? (in preparation) 
Bosque, I., Demonte, V., 1999, Gramatica 
descriptiva de la lengua Espa?ola, Espasa, 
Madrid. 
Brants, T. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth 
Applied Natural Language Processing 
Conference, Seattle, WA. 
Cambridge, online. Cambridge 
International Dictionary of English 
http://dictionary.cambridge.org/ 
Civit, M., Castell?n, I., Mart?, M.A. and Taul?, 
M., 2000,  ?LEXPIR: a verb lexicon for Spanish? 
Cuadernos de Filolog?a Inglesa, Vol. 9.1. Corpus-
based Research in English Language and 
Linguistics, University of Granada. 
Colmex, online. Diccionario del espa?ol usual 
en M?xico (Colmex) http://mezcal.colmex.mx 
(also accessible from 
http://www.foreignword.com 
Dorr, Bonnie J., Nizar Habash, and David 
Traum, 1998, ?A Thematic Hierarchy for Efficient 
Generation from Lexical-Conceptual Structure,? 
in Proceedings of the Third Conference of the 
Association for MT in the America's, Langhorne, 
PA, pp. 333--343 
Elhuyar, 1996, Elhuyar Hiztegia, Elhuyar K.E., 
Usurbil. 
Euskaltzaindia, 1985, Euskal Gramatika Lehen 
Urratsak-I (EGLU-I), Euskaltzaindia, Bilbo. 
Heritage, online. The American Heritage? 
Dictionary of the English Language. 
http://www.bartleby.com/61 
J. Atserias, J. Carmona, I. Castellon, S. 
Cervell, M. Civit, L. Marquez, M.A. Marti, L. 
Padro, R.Placer, H. Rodriguez, M. Taule & J. 
Turmo ?Morphosyntactic Analysis and Parsing of 
Unrestricted Spanish Text? First International 
Conference on Language Resources and 
Evaluation (LREC'98). Granada, Spain, 1998. 
Miller, G. A., R. Beckwith, C. Fellbaum, D. 
Gross, and K. Miller. 1990. Five Papers on 
WordNet. Special Issue of International Journal of 
Lexicography, 3(4). 
Morris M., 1998, Morris Student dictionary, 
Klaudio Harluxet Fundazioa, Donostia. 
Rae, online. Diccionario de la Real Academia 
de la Lengua http://buscon.rae.es/drae/drae.htm 
Sarasola, I., 1996, Euskal Hiztegia, 
Gipuzkoako Kutxa, Donostia. 
John F. Sowa, 2000, Knowledge 
Representation: Logical, Philosophical, and 
Computational Foundations, Brooks Cole 
Publishing Co., Pacific Grove, CA 
John F. Sowa, ed. (1992) Knowledge-Based 
Systems, Special Issue on Conceptual Graphs, vol. 
5, no. 3, September 1992 
Vox, online. Diccionario General de la lengua 
espa?ola VOX http://www.vox.es/consultar.html 
Wordsmyth, online. The Wordsmyth 
Educational Dictionary-Thesaurus 
http://www.wordsmyth.net 
The Basque lexical-sample task  
Eneko Agirre, Itziar Aldabe, Mikel Lersundi, David Martinez, Eli Pociello, Larraitz Uria(*) 
IxA NLP group, Basque Country University  
649 pk. 20.080 Donostia, Spain 
eneko@si.ehu.es 
 
Abstract 
In this paper we describe the Senseval 3 
Basque lexical sample task. The task 
comprised 40 words (15 nouns, 15 verbs and 
10 adjectives) selected from the Basque 
WordNet. 10 of the words were chosen in 
coordination with other lexical-sample tasks. 
The examples were taken from newspapers, an 
in-house balanced corpus and Internet texts. 
We additionally included a large set of 
untagged examples, and a lemmatised version 
of the data including lemma, PoS and case 
information. The method used to hand-tag the 
examples produced an inter-tagger agreement 
of 78.2% before arbitration. The eight 
competing systems attained results well above 
the most frequent baseline and the best system 
from Swarthmore College scored 70.4% 
recall. 
1 Introduction 
This paper reviews the Basque lexical-sample task 
organized for Senseval 3. Each participant was 
provided with a relatively small set of labelled 
examples (2/3 of 75+15*senses+7*multiwords) 
and a comparatively large set of unlabelled 
examples (roughly ten times more when possible) 
for around 40 words. The larger number of 
unlabelled data was released with the purpose to 
enable the exploration of semi-supervised systems. 
The test set comprised 1/3 of the tagged examples. 
The sense inventory was taken from the Basque 
WordNet, which is linked to WordNet version 1.6 
(Fellbaum, 1998). The examples came mainly from 
newspaper texts, although we also used a balanced 
in-house corpus and texts from Internet. The words 
selected for this task were coordinated with other 
lexical-sample tasks (such as Catalan, English, 
Italian, Romanian and Spanish) in order to share 
around 10 of the target words.  
The following steps were taken in order to carry 
out the task: 
                                                     
(*) Authors listed in alphabetic order. 
1. set the exercise  
a. choose sense inventory from a pre-existing 
resource 
b. choose target corpora 
c. choose target words  
d. lemmatize the corpus automatically 
e. select examples from the corpus 
2. hand-tagging 
a. define the procedure 
b. revise the sense inventory 
c. tag 
d. analyze the inter-tagger agreement 
e. arbitrate 
This paper is organized as follows: The 
following section presents the setting of the 
exercise. Section 3 reviews the hand-tagging, and 
Section 4 the details of the final release. Section 5 
shows the results of the participant systems. 
Section 6 discusses some main issues and finally, 
Section 7 draws the conclusions. 
2 Setting of the exercise  
In this section we present the setting of the 
Basque lexical-sample exercise. 
2.1 Basque 
As Basque is an agglutinative language, the 
dictionary entry takes each of the elements 
necessary to form the different functions. More 
specifically, the affixes corresponding to the 
determinant, number and declension case are taken 
in this order and independently of each other (deep 
morphological structure). For instance, ?etxekoari 
emaiozu? can be roughly translated as ?[to the one 
in the house] [give it]? where the underlined 
sequence of suffixes in Basque corresponds to ?to 
the one in the?.  
2.2 Sense inventory 
We chose the Basque WordNet, linked to 
WordNet 1.6, for the sense inventory. This way, 
the hand tagging enabled us to check the sense 
coverage and overall quality of the Basque 
WordNet, which is under construction. The Basque 
WordNet is available at http://ixa3.si.ehu.es/ 
wei3.html. 
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2.3 Corpora used 
Being Basque a minority language it is not easy 
to find the required number of occurrences for each 
word. We wanted to have both balanced and 
newspaper examples, but we also had to include 
texts extracted from the web, specially for the 
untagged corpus. The procedure to find examples 
from the web was the following: for each target 
word all possible morphological declensions were 
automatically generated, searched in a search-
engine, documents retrieved, automatically 
lemmatized (Aduriz et al 2000), filtered using 
some heuristics to ensure quality of context, and 
finally filtered for PoS mismatches. Table 1 shows 
the number of examples from each source. 
2.4 Words chosen 
Basically, the words employed in this task are 
the same words used in Senseval 2 (40 words, 15 
nouns, 15 verbs and 10 adjectives), only the sense 
inventory changed. Besides, in Senseval 3 we 
replaced 5 verbs with new ones. The reason for this 
is that in the context of the MEANING project1 we 
are exploring multilingual lexical acquisition, and 
there are ongoing experiments that focus on those 
verbs. (Agirre et al 2004; Atserias et al 2004). 
In fact, 10 words in the English lexical-sample 
have translations in the Basque, Catalan, Italian, 
Romanian and Spanish lexical tasks: channel, 
crown, letter, program, party (nouns), simple 
(adjective), play, win, lose, decide (verbs).  
2.5 Selection of examples from corpora 
The minimum number of examples for each 
word according to the task specifications was 
calculated as follows: 
 
N=75+15*senses+7*multiwords  
 
As the number of senses in WordNet is very high, 
we decided to first estimate the number of senses 
and multiwords that really occur in the corpus. The 
taggers were provided with a sufficient number of 
examples, but they did not have to tag all. After 
they had tagged around 100 examples, they would 
count the number of senses and multiwords that 
had occurred and computed the N according to 
those counts.  
The context is constituted of 5 sentences, 
including the sentence with the target word 
appearing in the middle. Links were kept to the 
source corpus, document, and to the newspaper 
section when applicable.  
The occurrences were split at random in training 
set (2/3 of all occurrences) and test set (1/3).  
                                                     
1 http://www.lsi.upc.es/~nlp/meaning/meaning.html 
 Total (N) (B) (I)
# words 40  
# senses 316  
# number of tagged examples 7362 5695 924 743
# number of untagged examples 62498 - - 62498
# tags  9887  
Table 1: Some figures regarding the task. N, B and I 
correspond to the source of the examples: newspaper, 
balanced corpus and Internet respectively. 
3 Hand tagging 
Three persons, graduate linguistics students, 
took part in the tagging. They are familiar with 
word senses, as they are involved in the 
development of the Basque WordNet. The 
following procedure was defined in the tagging of 
each word. 
? Before tagging, one of the linguists (the editor) 
revised the 40 words in the Basque WordNet. 
She had to delete and add senses to the words, 
specially for adjectives and verbs, and was 
allowed to check the examples in the corpus.  
? The three taggers would meet, read the glosses 
and examples given in the Basque WordNet 
and discuss the meaning of each synset. They 
tried to agree and clarify the meaning 
differences among the synsets. For each word 
two hand-taggers and a referee is assigned by 
chance. 
? The number of senses of a word in the Basque 
WordNet might change during this meeting; 
that is, linguists could agree that one of the 
word?s senses was missing, or that a synset did 
not fit with a word. This was done prior to 
looking at the corpus. Then, the editor would 
update the Basque WordNet according to those 
decisions before giving the taggers the final 
synset list. Overall (including first bullet 
above), 143 senses were deleted and 92 senses 
added, leaving a total of 316 senses. This 
reflects the current situation of the Basque 
WordNet, which is still under construction. 
? Two taggers independently tagged all 
examples for the word. No communication was 
allowed while tagging the word. 
? Multiple synset tags were allowed, as well as 
the following tags: the lemma (in the case of 
multiword terms), U (unassignable), P (proper 
noun), and X (incorrectly lemmatized). Those 
with an X were removed from the final release. 
In the case of proper nouns and multiword 
terms no synset tag was assigned. Sometimes 
the U tag was used for word senses which are 
not in the Basque WordNet. For instance, the 
sense of kanal corresponding to TV channel, 
which is the most frequent sense in the 
examples, is not present in the Basque 
WordNet (it was not included in WordNet 1.6).  
? A program was used to compute agreement 
rates and to output those occurrences where 
there was disagreement. Those occurrences 
were  grouped by the senses assigned. 
? A third tagger, the referee, reviewed the 
disagreements and decided which one was the 
correct sense (or senses).  
The taggers were allowed to return more than one 
sense, and they returned 9887 tags (1.34 per 
occurrence). Overall, the two taggers agreed in at 
least one tag 78.2% of the time. Some words 
attained an agreement rate above 95% (e.g. nouns 
kanal or tentsio), but others like herri ?
town/people/nation? attained only 52% agreement. 
On average, the whole tagging task took 54 
seconds per occurrence for the tagger, and 20 
seconds for the referee. However, this average 
does not include the time the taggers and the 
referee spent in the meetings they did to 
understand the meaning of each synset. The 
comprehension of a word with all its synsets 
required 45.5 minutes on average. 
4 Final release 
Table 1 includes the total amount of hand-tagged 
and untagged examples that were released. In 
addition to the usual release, the training and 
testing data were also provided in a lemmatized 
version (Aduriz et al 2000) which included 
lemma, PoS and case information. The motivation 
was twofold: 
? to make participation of the teams easier, 
considering the deep inflection of Basque. 
? to factor out the impact of different 
lemmatizers and PoS taggers in the system 
comparison.  
5 Participants and Results 
5 teams took part in this task: Swarthmore 
College (swat), Basque Country University 
(BCU), Instituto per la Ricerca Scientifica e 
Tecnologica (IRST), University of Minnesota 
Duluth (Duluth) and University of Maryland 
(UMD). All the teams presented supervised systems 
which only used the tagged training data, and no 
other external resource. In particular, no system 
used the pointers to the full texts, or the additional 
untagged texts. All the systems used the lemma, 
PoS and case information provided, except the 
BCU team, which had additional access to number, 
determiner and ellipsis information directly from 
the analyzer. This extra information was not 
provided publicly because of representation issues.  
 
 Prec. Rec. Attempted
basque-swat_hk-bo 71.1  70.4  99.04 %
BCU_Basque_svm 69.9  69.9  100.00 %
BCU_-_Basque_Comb 69.5  69.5  100.00 %
swat-hk-basque 67.0  67.0  100.00 %
IRST-Kernels-bas 65.5  65.5  100.00 %
swat-basque 64.6  64.6  100.00 %
Duluth-BLSS 60.8  60.8  100.00 %
UMD_SST1 65.6  58.7  89.42 %
MFS 55.8  55.8  100.00 %
Table 2: Results of systems and MFS baseline, ordered 
according to Recall. 
We want to note that due to a bug, a few examples 
were provided without lemmas.  
The results for the fine-grained scoring are 
shown in Table 2, including the Most Frequent 
Sense baseline (MFS). We will briefly describe 
each of the systems presented by each team in 
order of best recall.  
? Swat presented three systems based in the 
same set of features: the best one was based on 
Adaboost, the second on a combination of five 
learners (Adaboost, maximum entropy, 
clustering system based on cosine similarity, 
decision lists, and na?ve bayes, combined by 
majority voting), and the third on a 
combination of three systems (the last three).  
? BCU presented two systems: the first one based 
on Support Vector Machines (SVM) and the 
second on a majority-voting combination of 
SVM, cosine based vectors and na?ve bayes.  
? IRST participated with a kernel-based method. 
? Duluth participated with a system that votes 
among three bagged decision trees. 
? UMD presented a system based on SVM. 
The winning system is the one using Adaboost 
from Swat, followed closely by the BCU system 
using SVM. 
6 Discussion 
These are the main issues we think are 
interesting for further discussion. 
Sense inventory. Using the Basque WordNet 
presented some difficulties to the taggers. The 
Basque WordNet has been built using the 
translation approach, that is, the English synsets 
have been ?translated? into Basque. The taggers 
had some difficulties to comprehend synsets, and 
especially, to realize what makes a synset different 
from another. In some cases the taggers decided to 
group some of the senses, for instance, in herri ?
town/people/nation? they grouped 6 senses. This 
explains the relatively high number of tags per 
occurrence (1.34). The taggers think that the 
tagging would be much more satisfactory if they 
had defined the word senses directly from the 
corpus.  
Basque WordNet quality. There was a 
mismatch between the Basque WordNet and the 
corpus: most of the examples were linked to a 
specific genre, and this resulted in i) having a 
handful of senses in the Basque WordNet that did 
not appear in our corpus and ii) having some 
senses that were not included in the Basque 
WordNet. Fortunately, we already predicted this 
and we had a preparation phase where the editor 
enriched WordNet accordingly. Most of the 
deletions in the preliminary part were due to the 
semi-automatic method to construct the Basque 
WordNet. All in all, we think that tagging corpora 
is the best way to ensure the quality of the 
WordNets and we plan to pursue this extensively 
for the improvement of the Basque WordNet.  
7 Conclusions and future work 
5 teams participated in the Basque lexical-
sample task with 8 systems. All of the participants 
presented supervised systems which used lemma, 
PoS and case information provided, but none used 
the large amount of untagged senses provided by 
the organizers. The winning system attained 70.4 
recall. Regarding the organization of the task, we 
found that the taggers were more comfortable 
grouping some of the senses in the Basque 
WordNet. We also found that tagging word senses 
is essential for enriching and quality checking of 
the Basque WordNet. 
Acknowledgements 
The work has been partially funded by the 
European Commission (MEANING project IST-
2001-34460). Eli Pociello has a PhD grant from 
the Basque Government.  
References  
I. Aduriz, E. Agirre, I. Aldezabal, I. Alegria, X. 
Arregi, J.M. Arriola, X. Artola, K. Gojenola, A. 
Maritxalar, K. Sarasola, M. Urkia. 2000. A 
Word-grammar Based Morphological Analyzer 
for Agglutinative Languages. In Proceedings of 
the International Conference on Computational 
Linguistics (COLING). Saarbrucken, Germany.  
E. Agirre, A. Atutxa, K. Gojenola, K. Sarasola. 
2004. Exploring portability of syntactic 
information from English to Basque. In 
Proceedings of the 4rd International Conference 
on Languages Resources and Evaluations 
(LREC). Lisbon, Portugal. 
J. Atserias, B. Magnini, O. Popescu, E. Agirre, A. 
Atutxa, G. Rigau, J. Carroll and R. Koeling 
2004. Cross-Language Acquisition of Semantic 
Models for Verbal Predicates. In Proceedings of 
the 4rd International Conference on Languages 
Resources and Evaluations (LREC). Lisbon, 
Portugal. 
C. Fellbaum. 1998. WordNet: An electronic 
Lexical Database. The MIT Press, Cambridge, 
Massachusetts.  
The Basque Country University system: English and Basque tasks
Eneko Agirre
IXA NLP Group
Basque Country University
Donostia, Spain
eneko@si.ehu.es
David Martinez
IXA NLP Group
Basque Country University
Donostia, Spain
davidm@si.ehu.es
Abstract
Our group participated in the Basque and En-
glish lexical sample tasks in Senseval-3. A
language-specific feature set was defined for
Basque. Four different learning algorithms were
applied, and also a method that combined their
outputs. Before submission, the performance
of the methods was tested for each task on the
Senseval-3 training data using cross validation.
Finally, two systems were submitted for each
language: the best single algorithm and the best
ensemble.
1 Introduction
Our group (BCU, Basque Country University),
participated in the Basque and English lexical
sample tasks in Senseval-3. We applied 4 differ-
ent learning algorithms (Decision Lists, Naive
Bayes, Vector Space Model, and Support Vector
Machines), and also a method that combined
their outputs. These algorithms were previously
tested and tuned on the Senseval-2 data for En-
glish. Before submission, the performance of
the methods was tested for each task on the
Senseval-3 training data using 10 fold cross val-
idation. Finally, two systems were submitted
for each language, the best single algorithm and
the best ensemble in cross-validation.
The main difference between the Basque and
English systems was the feature set. A rich
set of features was used for English, includ-
ing syntactic dependencies and domain infor-
mation, extracted with different tools, and also
from external resources like WordNet Domains
(Magnini and Cavaglia?, 2000). The features for
Basque were different, as Basque is an agglu-
tinative language, and syntactic information is
given by inflectional suffixes. We tried to rep-
resent this information in local features, relying
on the analysis of a deep morphological analyzer
developed in our group (Aduriz et al, 2000).
In order to improve the performance of the al-
gorithms, different smoothing techniques were
tested on the English Senseval-2 lexical sam-
ple data (Agirre and Martinez, 2004), and ap-
plied to Senseval-3. These methods helped to
obtain better estimations for the features, and
to avoid the problem of 0 counts Decision Lists
and Naive Bayes.
This paper is organized as follows. The learn-
ing algorithms are first introduced in Section 2,
and Section 3 describes the features applied to
each task. In Section 4, we present the exper-
iments performed on training data before sub-
mission; this section also covers the final config-
uration of each algorithm, and the performance
obtained on training data. Finally, the official
results in Senseval-3 are presented and discussed
in Section 5.
2 Learning Algorithms
The algorithms presented in this section rely on
features extracted from the context of the target
word to make their decisions.
The Decision List (DL) algorithm is de-
scribed in (Yarowsky, 1995b). In this algorithm
the sense with the highest weighted feature is se-
lected, as shown below. We can avoid undeter-
mined values by discarding features that have a
0 probability in the divisor. More sophisticated
smoothing techniques have also been tried (cf.
Section 4).
arg max
k
w(s
k
, f
i
) = log(
Pr(s
k
|f
i
)
?
j =k
Pr(s
j
|f
i
)
)
The Naive Bayes (NB) algorithm is based
on the conditional probability of each sense
given the features in the context. It also re-
quires smoothing.
arg max
k
P (s
k
)
?
m
i=1
P (f
i
|s
k
)
For the Vector Space Model (V) algo-
rithm, we represent each occurrence context as
a vector, where each feature will have a 1 or 0
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
value to indicate the occurrence/absence of the
feature. For each sense in training, one cen-
troid vector is obtained. These centroids are
compared with the vectors that represent test-
ing examples, by means of the cosine similarity
function. The closest centroid is used to assign
its sense to the testing example. No smooth-
ing is required to apply this algorithm, but it is
possible to use smoothed values.
Regarding Support Vector Machines
(SVM) we utilized SVM-Light (Joachims,
1999), a public distribution of SVM. Linear ker-
nels were applied, and the soft margin (C) was
estimated per each word (cf. Section 4).
3 Features
3.1 Features for English
We relied on an extensive set of features of
different types, obtained by means of different
tools and resources. The features used can be
grouped in four groups:
Local collocations: bigrams and trigrams
formed with the words around the target. These
features are constituted with lemmas, word-
forms, or PoS tags1. Other local features
are those formed with the previous/posterior
lemma/word-form in the context.
Syntactic dependencies: syntactic depen-
dencies were extracted using heuristic patterns,
and regular expressions defined with the PoS
tags around the target2. The following rela-
tions were used: object, subject, noun-modifier,
preposition, and sibling.
Bag-of-words features: we extract the
lemmas of the content words in the whole con-
text, and in a ?4-word window around the tar-
get. We also obtain salient bigrams in the con-
text, with the methods and the software de-
scribed in (Pedersen, 2001).
Domain features: The WordNet Domains
resource was used to identify the most relevant
domains in the context. Following the relevance
formula presented in (Magnini and Cavaglia?,
2000), we defined 2 feature types: (1) the most
relevant domain, and (2) a list of domains above
a predefined threshold3. Other experiments us-
ing domains from SUMO, the EuroWordNet
1The PoS tagging was performed with the fnTBL
toolkit (Ngai and Florian, 2001).
2This software was kindly provided by David
Yarowsky?s group, from Johns Hopkins University.
3The software to obtain the relevant domains was
kindly provided by Gerard Escudero?s group, from Uni-
versitat Politecnica de Catalunya
top-ontology, and WordNet?s Semantic Fields
were performed, but these features were dis-
carded from the final set.
3.2 Features for Basque
Basque is an agglutinative language, and syn-
tactic information is given by inflectional suf-
fixes. The morphological analysis of the text is
a necessary previous step in order to select in-
formative features. The data provided by the
task organization includes information about
the lemma, declension case, and PoS for the par-
ticipating systems. Our group used directly the
output of the parser (Aduriz et al, 2000), which
includes some additional features: number, de-
terminer mark, ambiguous analyses and elliptic
words. For a few examples, the morphological
analysis was not available, due to parsing errors.
In Basque, the determiner, the number and
the declension case are appended to the last el-
ement of the phrase. When defining our fea-
ture set for Basque, we tried to introduce the
same knowledge that is represented by features
that work well for English. We will describe
our feature set with an example: for the phrase
?elizaren arduradunei? (which means ?to the
directors of the church?) we get the following
analysis from our analyzer:
eliza |-ren |arduradun |-ei
church |of the |director |to the +pl.
The order of the words is the inverse in En-
glish. We extract the following information for
each word:
elizaren:
Lemma: eliza (church)
PoS: noun
Declension Case: genitive (of)
Number: singular
Determiner mark: yes
arduradunei:
Lemma: arduradun (director)
PoS: noun
Declension Case: dative (to)
Number: plural
Determiner mark: yes
We will assume that eliza (church) is the
target word. Words and lemmas are shown
in lowercase and the other information in up-
percase. As local features we defined different
types of unigrams, bigrams, trigrams and a
window of ?4 words. The unigrams were con-
structed combining word forms, lemmas, case,
number, and determiner mark. We defined 4
kinds of unigrams:
Uni wf0 elizaren
Uni wf1 eliza SING+DET
Uni wf2 eliza GENITIVE
Uni wf3 eliza SING+DET GENITIVE
As for English, we defined bigrams based on
word forms, lemmas and parts-of-speech. But
in order to simulate the bigrams and trigrams
used for English, we defined different kinds of
features. For word forms, we distinguished two
cases: using the text string (Big wf0), or using
the tags from the analysis (Big wf1). The word
form bigrams for the example are shown below.
In the case of the feature type ?Big wf1?, the
information is split in three features:
Big wf0 elizaren arduradunei
Big wf1 eliza GENITIVE
Big wf1 GENITIVE arduradun PLUR+DET
Big wf1 arduradun PLUR+DET DATIVE
Similarly, depending on the use of the de-
clension case, we defined three kinds of bigrams
based on lemmas:
Big lem0 eliza arduradun
Big lem1 eliza GENITIVE
Big lem1 GENITIVE arduradun
Big lem1 arduradun DATIVE
Big lem2 eliza GENITIVE
Big lem2 arduradun DATIVE
The bigrams constructed using Part-of-
speech are illustrated below. We included the
declension case as if it was another PoS:
Big pos -1 NOUN GENITIVE
Big pos -1 GENITIVE NOUN
Big pos -1 NOUN DATIVE
Trigrams are built similarly, by combining the
information from three consecutive words. We
also used as local features all the content words
in a window of ?4 words around the target. Fi-
nally, as global features we took all the con-
tent lemmas appearing in the context, which
was constituted by the target sentence and the
two previous and posterior sentences.
One difficult case to model in Basque is the el-
lipsis. For example, the word ?elizakoa? means
?the one from the church?. We were able to
extract this information from our analyzer and
we represented it in the features, using a special
symbol in place of the elliptic word.
4 Experiments on training data
The algorithms that we applied were first tested
on the Senseval-2 lexical sample task for En-
glish. The best versions were then evaluated by
10 fold cross-validation on the Senseval-3 data,
both for Basque and English. We also used the
training data in cross-validation to tune the pa-
rameters, such as the smoothed frequencies, or
the soft margin for SVM. In this section we will
describe first the parameters of each method
(including the smoothing procedure), and then
the cross-validation results on the Senseval-3
training data.
4.1 Methods and Parameters
DL: On Senseval-2 data, we observed that
DL improved significantly its performance with
a smoothing technique based on (Yarowsky,
1995a). For our implementation, the smoothed
probabilities were obtained by grouping the ob-
servations by raw frequencies and feature types.
As this method seems sensitive to the feature
types and the amount of examples, we tested
3 DL versions: DL smooth (using smoothed
probabilities), DL fixed (replacing 0 counts with
0.1), and DL discard (discarding features ap-
pearing with only one sense).
NB: We applied a simple smoothing method
presented in (Ng, 1997), where zero counts are
replaced by the probability of the given sense
divided by the number of examples.
V: The same smoothing method used for NB
was applied for vectors. For Basque, two ver-
sions were tested: as the Basque parser can re-
turn ambiguous analyses, partial weights are as-
signed to the features in the context, and we can
chose to use these partial weights (p), or assign
the full weight to all features (f).
SVM: No smoothing was applied. We esti-
mated the soft margin using a greedy process in
cross-validation on the training data per each
word.
Combination: Single voting was used,
where each system voted for its best ranked
sense, and the most voted sense was chosen.
More sophisticate schemes like ranked voting,
were tried on Senseval-2 data, but the results
did not improve. We tested combinations of
the 4 algorithms, leaving one out, and the two
best. The best results were obtained combining
3 methods (leave one out).
Method Recall
vector 73,9
SVM 73,5
DL smooth 69,4
NB 69,4
DL fixed 65,6
DL discard 65,4
MFS 57,1
Table 1: Single systems (English) in cross-
validation, sorted by recall.
Combination Recall
SVM-vector-DL smooth-NB 73,2
SVM-vector-DL fixed-NB 72,7
SVM-vector-DL smooth 74,0
SVM-vector-DL fixed 73,8
SVM-vector-NB 73,6
SVM-DL smooth-NB 72,4
SVM-DL fixed-NB 71,3
SVM-vector 73,1
Table 2: Combined systems (English) in cross-
validation, best recall in bold.
Method Recall
SVM 71,1
NB 68,5
vector(f) 66,8
DL smooth 65,9
DL fixed 65,2
vector(p) 65,0
DL discard 60,7
MFS 53,0
Table 3: Single systems (Basque) in cross-
validation, sorted by recall.
Combination Recall
SVM-vector-DL smooth-NB 70,6
SVM-vector-DL fixed-NB 71,1
SVM-vector-DL smooth 70,6
SVM-vector-DL fixed 70,8
SVM-vector-NB 71,1
SVM-DL smooth-NB 70,2
SVM-DL fixed-NB 70,5
SVM-vector 69,0
SVM-NB 69,8
Table 4: Combined systems (Basque) in cross-
validation, best recall in bold. Only vector(f)
was used for combination.
4.2 Results on English Training Data
The results using cross-validation on the
Senseval-3 data are shown in Table 1 for single
systems, and in Table 2 for combined methods.
All the algorithms have full-coverage (for En-
glish and Basque), therefore the recall and the
precision are the same. The most frequent sense
(MFS) baseline is also provided, and it is easily
beaten by all the algorithms.
We have to note that these figures are consis-
tent with the performance we observed in the
Senseval-2 data, where the vector method is
the best performing single system, and the best
combination is SVM-vector-DL smooth. There
is a small gain when combining 3 systems, which
we expected would be higher. We submitted the
best single system, and the best combination for
this task.
4.3 Results on Basque Training Data
The performance on the Senseval-3 Basque
training data is given in Table 1 for single sys-
tems, and in Table 2 for combined methods. In
this case, the vector method, and DL smooth
obtain lower performance in relation to other
methods. This can be due to the type of fea-
tures used, which have not been tested as ex-
tensively as for English. In fact, it could hap-
pen that some features contribute mostly noise.
Also, the domain tag of the examples, which
could provide useful information, was not used.
There is no improvement when combining dif-
ferent systems, and the result of the combina-
tion of 4 systems is unusually high in relation
to the English experiments. We also submit-
ted two systems for this task: the best single
method in cross-validation (SVM), and the best
3-method combination (SVM-vector-NB).
5 Results and Conclusions
Table 5 shows the performance obtained by our
systems and the winning system in the Senseval-
3 evaluation. We can see that we are very close
to the best algorithms in both languages.
The recall of our systems is 1.2%-1.9% lower
than cross-validation for every system and task,
which is not surprising when we change the set-
ting. The combination of methods is useful for
English, where we improve the recall in 0.3%,
reaching 72.3%. The difference is statistically
significant according to McNemar?s test.
However, the combination of methods does
not improve the results in the the Basque task,
where the SVM method alone provides better
Task Code Method Rec.
Eng. Senseval-3 Best ? 72,9
Eng. BCU comb
SVM-vector-
DL smooth 72,3
Eng. BCU-english vector 72,0
Basq. Senseval-3 Best ? 70,4
Basq. BCU-basque SVM 69,9
Basq. BCU-Basque comb SVM-vector-
NB
69,5
Table 5: Official results for the English and
Basque lexical tasks (recall).
results (69.9% recall). In this case the difference
is not significant applying McNemar?s test.
Our disambiguation procedure shows a sim-
ilar behavior on the Senseval-2 and Senseval-3
data for English (both in cross-validation and
in the testing part), where the ensemble works
best, followed by the vector model. This did
not apply to the Basque dataset, where some
algorithms seem to perform below the expecta-
tions. For future work, we plan to study better
the Basque feature set and include new features,
such as domain tags.
Overall, the ensemble of algorithms provides
a more robust system for WSD, and is able to
achieve state-of-the-art performance.
6 Acknowledgements
We wish to thank both David Yarowsky?s group,
from Johns Hopkins University, and Gerard Es-
cudero?s group, from Universitat Politecnica de
Catalunya, for providing us software for the ac-
quisition of features. This research has been
partially funded by the European Commission
(MEANING IST-2001-34460).
References
I. Aduriz, E. Agirre, I. Aldezabal, I. Alegria,
X. Arregi, J. Arriola, X. Artola, K. Gojenola,
A. Maritxalar, K. Sarasola, and M. Urkia.
2000. A word-grammar based morphological
analyzer for agglutinative languages. In Pro-
ceedings of the International Conference on
Computational Linguistics COLING, Saar-
brucken, Germany.
Eneko Agirre and David Martinez. 2004.
Smoothing and word sense disambiguation.
(submitted).
T. Joachims. 1999. Making large?scale SVM
learning practical. In Advances in Kernel
Methods ? Support Vector Learning, pages
169?184, Cambridge, MA. MIT Press.
Bernardo Magnini and Gabriela Cavaglia?. 2000.
Integrating subject field codes into WordNet.
In Proceedings of the Second International
LREC Conference, Athens, Greece.
Hwee Tou Ng. 1997. Exemplar-based word
sense disambiguation: Some recent improve-
ments. In Proceedings of the Second EMNLP
Conference. ACL, Somerset, New Jersey.
Grace Ngai and Radu Florian. 2001.
Transformation-based learning in the fast
lane. Proceedings of the Second Conference
of the NAACL, Pittsburgh, PA, USA.
Ted Pedersen. 2001. A decision tree of bi-
grams is an accurate predictor of word sense.
Proceedings of the Second Meeting of the
NAACL, Pittsburgh, PA.
David Yarowsky. 1995a. Three machine learn-
ing algorithms for lexical ambiguity resolu-
tion. In PhD thesis, Department of Com-
puter and Information Sciences, University of
Pennsylvania.
David Yarowsky. 1995b. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 189?196,
Cambridge, MA.
The ?Meaning? System on the English Allwords Task
L. Villarejo   , L. Ma`rquez   , E. Agirre  , D. Mart??nez  , , B. Magnini  ,
C. Strapparava  , D. McCarthy  , A. Montoyo  , and A. Sua?rez 
 
TALP Research Center, Universitat Polite`cnica de Catalunya,  luisv,lluism  @lsi.upc.es
 IXA Group, University of the Basque Country,  eneko,davidm  @si.ehu.es
 ITC-irst (Istituto per la Ricerca Scientifica e Tecnologica),  magnini,strappa  @itc.it
 University of Sussex, dianam@sussex.ac.uk
 LSI, University of Alicante, montoyo@dlsi.ua.es,armando.suarez@ua.es
1 Introduction
The ?Meaning? system has been developed within
the framework of the Meaning European research
project1 . It is a combined system, which integrates
several supervised machine learning word sense
disambiguation modules, and several knowledge?
based (unsupervised) modules. See section 2 for de-
tails. The supervised modules have been trained ex-
clusively on the SemCor corpus, while the unsuper-
vised modules use WordNet-based lexico?semantic
resources integrated in the Multilingual Central
Repository (MCR) of the Meaning project (Atserias
et al, 2004).
The architecture of the system is quite simple.
Raw text is passed through a pipeline of linguis-
tic processors (tokenizers, POS tagging, named en-
tity extraction, and parsing) and then a Feature Ex-
traction module codifies examples with features ex-
tracted from the linguistic annotation and MCR.
The supervised modules have priority over the un-
supervised and they are combined using a weighted
voting scheme. For the words lacking training ex-
amples, the unsupervised modules are applied in a
cascade sorted by decreasing precision. The tuning
of the combination setting has been performed on
the Senseval-2 allwords corpus.
Several research groups have been providers of
resources and tools, namely: IXA group from the
University of the Basque Country, ITC-irst (?Is-
tituto per la Ricerca Scientifica e Tecnologica?),
University of Sussex (UoS), University of Alicante
(UoA), and TALP research center at the Technical
University of Catalonia. The integration was carried
out by the TALP group.
2 The WSD Modules
We have used up to seven supervised learning sys-
tems and five unsupervised WSD modules. Some
of them have also been applied individually to the
1Meaning, Developing Multilingual Web-scale Lan-
guage Technologies (European Project IST-2001-34460):
http://www.lsi.upc.es/  nlp/meaning/meaning.html.
Senseval-3 lexical sample and allwords tasks.
 Naive Bayes (NB) is the well?known Bayesian
algorithm that classifies an example by choos-
ing the class that maximizes the product, over
all features, of the conditional probability of
the class given the feature. The provider of this
module is IXA. Conditional probabilities were
smoothed by Laplace correction.
 Decision List (DL) are lists of weighted clas-
sification rules involving the evaluation of one
single feature. At classification time, the algo-
rithm applies the rule with the highest weight
that matches the test example (Yarowsky,
1994). The provider is IXA and they also ap-
plied smoothing to generate more robust deci-
sion lists.
 In the Vector Space Model method (cosVSM),
each example is treated as a binary-valued fea-
ture vector. For each sense, one centroid vec-
tor is obtained from training. Centroids are
compared with the vectors representing test ex-
amples, using the cosine similarity function,
and the closest centroid is used to classify the
example. No smoothing is required for this
method provided by IXA.
 Support Vector Machines (SVM) find the hy-
perplane (in a high dimensional feature space)
that separates with maximal distance the pos-
itive examples from the negatives, i.e., the
maximal margin hyperplane. Providers are
TALP (SVM  ) and IXA (SVM 	 ) groups. Both
used the freely available implementation by
(Joachims, 1999), linear kernels, and one?vs?
all binarization, but with different parameter
tuning and feature filtering.
 Maximum Entropy (ME) are exponential
conditional models parametrized by a flexible
set of features. When training, an iterative opti-
mization procedure finds the probability distri-
bution over feature coefficients that maximizes
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the entropy on the training data. This system is
provided by UoA.
 AdaBoost (AB) is a method for learning an en-
semble of weak classifiers and combine them
into a strong global classification rule. We
have used the implementation described in
(Schapire and Singer, 1999) with decision trees
of depth fixed to 3. The provider of this system
is TALP.
 Domain Driven Disambiguation (DDD) is an
unsupervised method that makes use of do-
main information in order to solve lexical am-
biguity. The disambiguation of a word in
its context is mainly a process of compari-
son between the domain of the context and
the domains of the word?s senses (Magnini
et al, 2002). ITC-irst provided two variants
of the system DDD   and DDD  , aiming at
maximizing precision and F  score, respec-
tively. The UoA group also provided another
domain?based unsupervised classifier (DOM).
Their approach exploits information contained
in glosses of WordNet Domains and introduces
a new lexical resource ?Relevant Domains? ob-
tained from Association Ratio over glosses of
WordNet Domains.
 Automatic Predominant Sense (autoPS) pro-
vide an unsupervised first sense heuristic for
the polysemous words in WordNet. This
is produced by UoS automatically from the
BNC (McCarthy et al, 2004). The method
uses automatically acquired thesauruses for the
main PoS categories. The nearest neighbors
for each word are related to its WordNet senses
using a WordNet similarity measure.
 We also used a Most Frequent Sense tagger,
according to the WordNet ranking of senses
(MFS).
3 Evaluation of Individual Modules
For simplicity, and also due to time constraints, the
supervised modules were trained exclusively on the
SemCor-1.6 corpus, intentionally avoiding the use
of other sources of potential training examples, e.g,
other corpora, WordNet examples and glosses, sim-
ilar/substitutable examples extracted from the same
Semcor-1.6, etc. An independent training set was
generated for each polysemous word (of a certain
part?of?speech) with 10 or more examples in the
SemCor-1.6 corpus. This makes a total of 2,440 in-
dependent learning problems, on which all super-
vised WSD systems were trained.
The feature representation of the training exam-
ples was shared between all learning modules. It
consists of a rich feature representation obtained
using the Feature Extraction module of the TALP
team in the Senseval-3 English lexical sample task.
The feature set includes the classic window?based
pattern features extracted from a local context and
the ?bag?of?words? type of features taken from a
broader context. It also contains a set of features
representing the syntactic relations involving the
target word, and semantic features of the surround-
ing words extracted from the MCR of the Meaning
project. See (Escudero et al, 2004) for more details
on the set of features used.
The validation corpus for these classifiers was the
Senseval-2 allwords dataset, which contains 2,473
target word occurrences. From those, 2,239 occur-
rences correspond to polysemous words. We will
refer to this subcorpus as S2-pol. Only 1,254 words
from S2-pol were actually covered by the classifiers
trained on the SemCor-1.6 corpus. We will refer to
this subset of words as the S2-pol-sup corpus. The
conversion between WordNet-1.6 synsets (SemCor-
1.6) and WordNet-1.7 (Senseval-2) was performed
on the output of the classifiers by applying an auto-
matically derived mapping provided by TALP2.
Table 1 shows the results (precision and cover-
age) obtained by the individual supervised modules
on the S2-pol-sup subcorpus, and by the unsuper-
vised modules on the S2-pol subcorpus (i.e., we
exclude from evaluation the monosemous words).
Support Vector Machines and AdaBoost are the best
performing methods, though all of them perform in
a small accuracy range from 53.4% to 59.5%.
Regarding the unsupervised methods, DDD is
clearly the best performing method, achieving a re-
markable precision of 61.9% with the DDD   vari-
ant, at a cost of a lower coverage. The DDD  ap-
pears to be the best system for augmenting the cov-
erage of the former. Note that the autoPS heuristic
for ranking senses is a more precise estimator than
the WordNet most?frequent?sense (MFS).
4 Integration of WSD modules
All the individual modules have to be integrated in
order to construct a complete allwords WSD sys-
tem. Following the architecture described in section
1, we decided to apply the unsupervised modules
only to the subset of the corpus not covered by the
training examples. Some efforts on applying the
unsupervised modules jointly with the supervised
failed at improving accuracy. See an example in ta-
ble 3.
2http://www.lsi.upc.es/  nlp/tools/mapping.html
supervised, S2-pol-sup corpus unsupervised, S2-pol corpus
SVM   AB cosVSM SVM  ME NB DL DDD  DDD  autoPS MFS DOM
prec. 59.5 59.1 57.8 57.1 56.3 54.6 53.4 61.9 50.2 45.2 32.5 23.8
cov. 100.0 100.0 100.0 100.0 100.0 100.0 100.0 48.8 99.6 89.6 98.0 49.1
Table 1: Results of individual supervised and unsupervised WSD modules
As a first approach, we devised three baseline
systems (Base-1, Base-2, and Base-3), which use
the best modules available in both subsets. Base-1
applies the SVM  supervised method and the MFS
for the non supervised part. Base-2 applies also the
SVM  supervised method and the cascade DDD   ?
MFS for the non supervised part (MFS is used in the
cases in which DDD   abstains). Base-3 shares the
same approach but uses a third unsupervised mod-
ule: DDD   ?DDD  ?MFS.
The precision results of the baselines systems can
be found in the right hand side of table 3. As it can
be observed, the positive contribution of the DDD  
module is very significant since Base-2 performs
2.2 points higher than Base-1. The addition of the
third unsupervised module (DDD   ) makes Base-3
to gain 0.4 extra precision points.
As simple combination schemes we considered
majority voting and weighted voting. More sophis-
ticated combination schemes are difficult to tune
due to the extreme data sparseness on the valida-
tion set. In the case of unsupervised systems, these
combination schemes degraded accuracy because
the least accurate systems perform much worse that
the best ones. Thus, we simply decided to apply a
cascade of unsupervised modules sorted by preci-
sion on the Senseval-2 corpus.
In the case of the supervised classifiers there is a
chance of improving the global performance, since
there are several modules performing almost as well
as the best. Previous to the experiments, we cal-
culated the agreement rates on the outputs of each
pair of systems (low agreements increase the prob-
ability of uncorrelatedness between errors of differ-
ent systems). We obtained an average agreement of
83.17%, with values between 64.7% (AB vs SVM 	 )
and 88.4% (SVM 	 vs cosVSM).
The ensembles were obtained by incrementally
aggregating, to the best performing classifier, the
classifiers from a list sorted by decreasing accu-
racy. The ranking of classifiers can be performed
by evaluating them at different levels of granular-
ity: from particular words to the overall accuracy
on the whole validation set. The level of granularity
defines a tradeoff between classifier specialization
and risk of overfitting to the tuning corpus. We de-
cided to take an intermediate level of granularity,
and sorted the classifiers according to their perfor-
mance on word sets based on the number of training
examples available3 .
Table 2 contains the results of the ranking exper-
iment, by considering five word-sets of increasing
number of training examples: between 10 and 20,
between 21 and 40, between 41 and 80, etc. At each
cell, the accuracy value is accompanied by the rel-
ative position the system achieves in that particu-
lar subset. Note that the resulting orderings, though
highly correlated, are quite different from the one
derived from the overall results.
(10,20) (21,40) (41,80) (81,160)  160
SVM  60.9-1 59.1-1 64.2-2 61.1-2 56.4-1
AB 60.9-1 56.6-2 60.0-7 64.7-1 56.1-2
c-VSM 59.9-2 56.6-2 62.6-3 57.0-4 55.8-3
SVM  50.8-5 55.1-4 61.6-4 57.4-3 53.1-5
ME 56.7-3 55.3-3 65.3-1 53.3-5 53.8-4
NB 59.9-2 54.6-5 61.1-5 49.2-6 51.5-7
DL 56.4-4 49.9-6 60.5-6 47.2-7 52.5-6
Table 2: Results on frequency?based word sets
Table 3 shows the precision results4 of the Mean-
ing system obtained on the whole Senseval-2 corpus
by combining from 1 to 7 supervised classifiers ac-
cording to the classifier orderings of table 2 for each
subset of words. The unsupervised classifiers are
all applied in a cascade sorted by precision. M-Vot
stands for a majority voting scheme, while W-Vot
refers to the weighted voting scheme. The weights
for the classifiers are simply the accuracy values on
the validation corpus. As an additional example,
the column M-Vot+ shows the results of the vot-
ing scheme when the unsupervised DDD   module
is also included in the ensemble. The table also in-
cludes the baseline results.
Unfortunately, the ensembles of classifiers did
not provide significant improvements on the final
precision. Only in the case of weighted voting a
slight improvement is observed when adding up to
3 classifiers. From the fourth classifier performance
also degrades. The addition of unsupervised sys-
tems to the supervised ensemble systematically de-
graded performance.
As a reference, the best result (67.5% precision
3One of the factors that differentiates between learning al-
gorithms is the amount of training examples needed to learn.
4Coverage of the combined systems is 98% in all cases.
M-Vot W-Vot M-Vot+ Base-1 Base-2 Base-3
1 67.3 67.3 66.4 ? ? ?
2 ? 67.4 66.3 ? ? ?
3 67.2 67.5 67.1 ? ? ?
4 ? 67.1 66.9 ? ? ?
5 66.5 66.5 66.7 ? ? ?
6 ? 66.3 66.3 ? ? ?
7 65.7 65.9 66.0 ? ? ?
best 67.3 67.5 67.1 64.8 67.0 67.4
Table 3: Results of the combination of systems
System prec. recall F 
Meaning-c 61.1% 61.0% 61.05
Meaning-wv 62.5% 62.3% 62.40
Table 4: Results on the Senseval-3 test corpus
and 98.0% coverage) would have put our combined
system in second place in the Senseval-2 allwords
task.
5 Evaluation on the Senseval-3 Corpus
The Senseval-3 test set contains 2,081 target words,
1,851 of them polysemous. The subset covered by
the SemCor-1.6 training contains 1,211 target words
(65.42%, compared to the 56.0% of the Senseval-2
corpus). We submitted the outputs of two different
configurations of the Meaning system: Meaning-
c and Meaning-wv. These systems correspond to
Base-3 and W-Vot (in the best configuration) from
table 3, respectively. The results from the official
evaluation are given in table 4. Again, we applied an
automatic mapping from WordNet-1.6 to WordNet-
1.7.1 synset labels. However, there are senses in
1.7.1 that do not exist in 1.6, thus our system sim-
ply cannot assign them.
It can be observed that, even though on the tun-
ing corpus both variants obtained very similar pre-
cision (67.4 and 67.5), on the test set the weighted
voting scheme is clearly better than the baseline sys-
tem, probably due to the robustness achieved by the
ensemble. The performance decrease observed on
the test set with respect to the Senseval-2 corpus is
very significant (   5 points). Given that the baseline
system performs worse than the voted approach, it
seems unlikely that there is overfitting during the
ensemble tuning. However, we plan to repeat the
tuning experiments directly on the Senseval-3 cor-
pus to see if the same behavior and conclusions
are observed. Probably, the decrease in perfor-
mance is due to the differences between the train-
ing and test corpora. We intend to investigate the
differences between SemCor-1.6, Senseval-2, and
Senseval-3 corpora at different levels of linguistic
information in order to check the appropriateness of
using SemCor-1.6 as the main information source.
6 Acknowledgements
This research has been possible thanks to the sup-
port of European and Spanish research projects:
IST-2001-34460 (Meaning), TIC2000-0335-C03-
02 (Hermes). The authors would like to thank also
Gerard Escudero for letting us use the Feature Ex-
traction module and German Rigau for helpful sug-
gestions and comments.
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and P. Vossen. 2004. The
Meaning multilingual central repository. In Pro-
ceedings of the Second International WordNet
Conference.
G. Escudero, L. Ma`rquez, and G. Rigau. 2004.
TALP system for the english lexical sample task.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
T. Joachims. 1999. Making large?scale SVM learn-
ing practical. In B. Scho?lkopf, C. J. C. Burges,
and A. J. Smola, editors, Advances in Kernel
Methods ? Support Vector Learning, pages 169?
184. MIT Press, Cambridge, MA.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
D. McCarthy, R. Koeling, J. Weeds, and J. Car-
roll. 2004. Using automatically acquired pre-
dominant senses for word sense disambiguation.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
R. Schapire and Y. Singer. 1999. Improved boost-
ing algorithms using confidence?rated predic-
tions. Machine Learning, 37(3):297?336.
David Yarowsky. 1994. Decision lists for lexi-
cal ambiguity resolution: Application to accent
restoration in Spanish and French. In Proceed-
ings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 88?95,
Las Cruces, NM.
Unsupervised WSD based on automatically retrieved examples:
The importance of bias
Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Spain
eneko@si.ehu.es
David Martinez
IXA NLP Group
University of the Basque Country
Donostia, Spain
davidm@si.ehu.es
Abstract
This paper explores the large-scale acquisition of
sense-tagged examples for Word Sense Disam-
biguation (WSD). We have applied the ?WordNet
monosemous relatives? method to construct auto-
matically a web corpus that we have used to train
disambiguation systems. The corpus-building pro-
cess has highlighted important factors, such as the
distribution of senses (bias). The corpus has been
used to train WSD algorithms that include super-
vised methods (combining automatic and manually-
tagged examples), minimally supervised (requiring
sense bias information from hand-tagged corpora),
and fully unsupervised. These methods were tested
on the Senseval-2 lexical sample test set, and com-
pared successfully to other systems with minimum
or no supervision.
1 Introduction
The results of recent WSD exercises, e.g. Senseval-
21 (Edmonds and Cotton, 2001) show clearly that
WSD methods based on hand-tagged examples are
the ones performing best. However, the main draw-
back for supervised WSD is the knowledge acqui-
sition bottleneck: the systems need large amounts
of costly hand-tagged data. The situation is more
dramatic for lesser studied languages. In order to
overcome this problem, different research lines have
been explored: automatic acquisition of training ex-
amples (Mihalcea, 2002), bootstrapping techniques
(Yarowsky, 1995), or active learning (Argamon-
Engelson and Dagan, 1999). In this work, we have
focused on the automatic acquisition of examples.
When supervised systems have no specific train-
ing examples for a target word, they need to rely on
publicly available all-words sense-tagged corpora
like Semcor (Miller et al, 1993), which is tagged
with WordNet word senses. The systems perform-
ing best in the English all-words task in Senseval-2
were basically supervised systems trained on Sem-
cor. Unfortunately, for most of the words, this cor-
1http://www.senseval.org.
pus only provides a handful of tagged examples. In
fact, only a few systems could overcome the Most
Frequent Sense (MFS) baseline, which would tag
each word with the sense occurring most frequently
in Semcor. In our approach, we will also rely on
Semcor as the basic resource, both for training ex-
amples and as an indicator of the distribution of the
senses of the target word.
The goal of our experiment is to evaluate up to
which point we can automatically acquire examples
for word senses and train accurate supervised WSD
systems on them. This is a very promising line of
research, but one which remains relatively under-
studied (cf. Section 2). The method we applied
is based on the monosemous relatives of the target
words (Leacock et al, 1998), and we studied some
parameters that affect the quality of the acquired
corpus, such as the distribution of the number of
training instances per each word sense (bias), and
the type of features used for disambiguation (local
vs. topical).
Basically, we built three systems, one fully su-
pervised (using examples from both Semcor and au-
tomatically acquired examples), one minimally su-
pervised (using the distribution of senses in Semcor
and automatically acquired examples) and another
fully unsupervised (using an automatically acquired
sense rank (McCarthy et al, 2004) and automati-
cally acquired examples).
This paper is structured as follows. First, Section
2 describes previous work on the field. Section 3 in-
troduces the experimental setting for evaluating the
acquired corpus. Section 4 is devoted to the process
of building the corpus, which is evaluated in Section
5. Finally, the conclusions are given in Section 6.
2 Previous work
As we have already mentioned, there is little work
on this very promising area. In (Leacock et al,
1998), the method to obtain sense-tagged examples
using monosemous relatives is presented. In this
work, they retrieve the same number of examples
per each sense, and they give preference to monose-
mous relatives that consist in a multiword contain-
ing the target word. Their experiment is evaluated
on 3 words (a noun, a verb, and an adjective) with
coarse sense-granularity and few senses. The results
showed that the monosemous corpus provided pre-
cision comparable to hand-tagged data.
In another related work, (Mihalcea, 2002) gener-
ated a sense tagged corpus (GenCor) by using a set
of seeds consisting of sense-tagged examples from
four sources: SemCor, WordNet, examples created
using the method above, and hand-tagged examples
from other sources (e.g., the Senseval-2 corpus). By
means of an iterative process, the system obtained
new seeds from the retrieved examples. An exper-
iment in the lexical-sample task showed that the
method was useful for a subset of the Senseval-2
testing words (results for 5 words are provided).
3 Experimental Setting for Evaluation
In this section we will present the Decision List
method, the features used to represent the context,
the two hand-tagged corpora used in the experiment
and the word-set used for evaluation.
3.1 Decision Lists
The learning method used to measure the quality of
the corpus is Decision Lists (DL). This algorithm is
described in (Yarowsky, 1994). In this method, the
sense s
k
with the highest weighted feature f
i
is se-
lected, according to its log-likelihood (see Formula
1). For our implementation, we applied a simple
smoothing method: the cases where the denomina-
tor is zero are smoothed by the constant 0.1 .
weight(s
k
, f
i
) = log(
Pr(s
k
|f
i
)
?
j =k
Pr(s
j
|f
i
)
) (1)
3.2 Features
In order to represent the context, we used a basic set
of features frequently used in the literature for WSD
tasks (Agirre and Martinez, 2000). We distinguish
two types of features:
 Local features: Bigrams and trigrams, formed
by the word-form, lemma, and part-of-speech2
of the surrounding words. Also the content
lemmas in a ?4 word window around the tar-
get.
 Topical features: All the content lemmas in the
context.
2The PoS tagging was performed using TnT (Brants, 2000)
We have analyzed the results using local and top-
ical features separately, and also using both types
together (combination).
3.3 Hand-tagged corpora
Semcor was used as training data for our supervised
system. This corpus offers tagged examples for
many words, and has been widely used for WSD.
It was necessary to use an automatic mapping be-
tween the WordNet 1.6 senses in Semcor and the
WordNet 1.7 senses in testing (Daude et al, 2000).
For evaluation, the test part of the Senseval-2 En-
glish lexical-sample task was chosen. The advan-
tage of this corpus was that we could focus on a
word-set with enough examples for testing. Be-
sides, it is a different corpus, so the evaluation is
more realistic than that made using cross-validation.
The test examples whose senses were multiwords
or phrasal verbs were removed, because they can be
efficiently detected with other methods in a prepro-
cess.
It is important to note that the training part of
Senseval-2 lexical-sample was not used in the con-
struction of the systems, as our goal was to test
the performance we could achieve with minimal re-
sources (i.e. those available for any word). We only
relied on the Senseval-2 training bias in preliminary
experiments on local/topical features (cf. Table 4),
and to serve as a reference for unsupervised perfor-
mance (cf. Table 5).
3.4 Word-set
The experiments were performed on the 29 nouns
available for the Senseval-2 lexical-sample task. We
separated these nouns in 2 sets, depending on the
number of examples they have in Semcor: Set A
contained the 16 nouns with more than 10 examples
in Semcor, and Set B the remaining low-frequency
words.
4 Building the monosemous relatives web
corpus
In order to build this corpus3, we have acquired
1000 Google snippets for each monosemous word
in WordNet 1.7. Then, for each word sense of the
ambiguous words, we gathered the examples of its
monosemous relatives (see below). This method is
inspired in (Leacock et al, 1998), and has shown to
be effective in experiments of topic signature acqui-
sition (Agirre and Lopez, 2004). This last paper also
shows that it is possible to gather examples based on
3The automatically acquired corpus will be referred indis-
tinctly as web-corpus, or monosemous-corpus
monosemous relatives for nearly all noun senses in
WordNet4.
The basic assumption is that for a given word
sense of the target word, if we had a monosemous
synonym of the word sense, then the examples of
the synonym should be very similar to the target
word sense, and could therefore be used to train a
classifier of the target word sense. The same, but
in a lesser extent, can be applied to other monose-
mous relatives, such as direct hyponyms, direct hy-
pernyms, siblings, indirect hyponyms, etc. The ex-
pected reliability decreases with the distance in the
hierarchy from the monosemous relative to the tar-
get word sense.
The monosemous-corpus was built using the sim-
plest technique: we collected examples from the
web for each of the monosemous relatives. The rel-
atives have an associated number (type), which cor-
relates roughly with the distance to the target word,
and indicates their relevance: the higher the type,
the less reliable the relative. A sample of monose-
mous relatives for different senses of church, to-
gether with its sense inventory in WordNet 1.7 is
shown in Figure 1.
Distant hyponyms receive a type number equal
to the distance to the target sense. Note that we
assigned a higher type value to direct hypernyms
than to direct hyponyms, as the latter are more use-
ful for disambiguation. We also decided to include
siblings, but with a high type value (3).
In the following subsections we will describe step
by step the method to construct the corpus. First we
will explain the acquisition of the highest possible
amount of examples per sense; then we will explain
different ways to limit the number of examples per
sense for a better performance; finally we will see
the effect of training on local or topical features on
this kind of corpora.
4.1 Collecting the examples
The examples are collected following these steps
1: We query Google5 with the monosemous rel-
atives for each sense, and we extract the snippets as
returned by the search engine. All snippets returned
by Google are used (up to 1000). The list of snippets
is sorted in reverse order. This is done because the
top hits usually are titles and incomplete sentences
that are not so useful.
2: We extract the sentences (or fragments of sen-
tences) around the target search term. Some of the
4All the examples in this work are publicly available in
http://ixa2.si.ehu.es/pub/sensecorpus
5We use the offline XML interface kindly provided by
Google for research.
Sense 0 1 2 3 Total Semcor
church#1 0 476 524 0 1000 60
church#2 306 100 561 0 967 58
church#3 147 0 20 0 167 10
Overall 453 576 1105 0 2134 128
Table 1: Examples per type (0,1,...) that are ac-
quired from the web for the three senses of church
following the Semcor bias, and total examples in
Semcor.
sentences are discarded, according to the following
criteria: length shorter than 6 words, having more
non-alphanumeric characters than words divided by
two, or having more words in uppercase than in low-
ercase.
3: The automatically acquired examples contain
a monosemous relative of the target word. In or-
der to use these examples to train the classifiers,
the monosemous relative (which can be a multi-
word term) is substituted by the target word. In
the case of the monosemous relative being a mul-
tiword that contains the target word (e.g. Protestant
Church for church) we can choose not to substitute,
because Protestant, for instance, can be a useful fea-
ture for the first sense of church. In these cases, we
decided not to substitute and keep the original sen-
tence, as our preliminary experiments on this corpus
suggested (although the differences were not signif-
icant).
4: For a given word sense, we collect the desired
number of examples (see following section) in or-
der of type: we first retrieve all examples of type
0, then type 1, etc. up to type 3 until the necessary
examples are obtained. We did not collect exam-
ples from type 4 upwards. We did not make any
distinctions between the relatives from each type.
(Leacock et al, 1998) give preference to multiword
relatives containing the target word, which could be
an improvement in future work.
On average, we have acquired roughly 24,000 ex-
amples for each of the target words used in this ex-
periment.
4.2 Number of examples per sense (bias)
Previous work (Agirre and Martinez, 2000) has re-
ported that the distribution of the number of exam-
ples per word sense (bias for short) has a strong
influence in the quality of the results. That is, the
results degrade significantly whenever the training
and testing samples have different distributions of
the senses.
As we are extracting examples automatically, we
have to decide how many examples we will use for
Sense 1
church, Christian church, Christianity -- (a group of Christians; any group professing
Christian doctrine or belief)
Sense 2
church, church building -- (a place for public (especially Christian) worship)
Sense 3
church service, church -- (a service conducted in a church)
Monosemous relatives for different senses of church
Synonyms (Type 0): church building (sense 2), church service (sense 3) ...
Direct hyponyms (Type 1): Protestant Church (sense 1), Coptic Church (sense 1) ...
Direct hypernyms (Type 2): house of prayer (sense 2), religious service (sense 3) ...
Distant hyponyms (Type 2,3,4...): Greek Church (sense 1), Western Church (sense 1)...
Siblings (Type 3): Hebraism (sense 2), synagogue (sense 2) ...
Figure 1: Sense inventory and some monosemous relatives in WordNet 1.7 for church.
Web corpus
Sense
Semcor Web bias Semcor Pr Semcor MR Automatic MR Senseval test
# ex % # ex % # ex % # ex % # ex % # ex %
authority#1 18 60 338 0.5 338 33.7 324 59.9 138 19.3 37 37.4
authority#2 5 16.7 44932 66.4 277 27.6 90 16.6 75 10.5 17 17.2
authority#3 3 10 10798 16 166 16.6 54 10.0 93 13.0 1 1.0
authority#4 2 6.7 886 1.3 111 11.1 36 6.7 67 9.4 0 0
authority#5 1 3.3 6526 9.6 55 5.5 18 3.3 205 28.6 34 34.3
authority#6 1 3.3 71 0.1 55 5.5 18 3.3 71 9.9 10 10.1
authority#7 0 0 4106 6.1 1 0.1 1 0.2 67 9.4 0 0
Overall 30 100 67657 100 1003 100 541 100 716 100 99 100
Table 2: Distribution of examples for the senses of authority in different corpora. Pr (proportional) and MR
(minimum ratio) columns correspond to different ways to apply Semcor bias.
each sense. In order to test the impact of bias, dif-
ferent settings have been tried:
 No bias: we take an equal amount of examples
for each sense.
 Web bias: we take all examples gathered from
the web.
 Automatic ranking: the number of examples
is given by a ranking obtained following the
method described in (McCarthy et al, 2004).
They used a thesaurus automatically created
from the BNC corpus with the method from
(Lin, 1998), coupled with WordNet-based sim-
ilarity measures.
 Semcor bias: we take a number of examples
proportional to the bias of the word senses in
Semcor.
For example, Table 1 shows the number of exam-
ples per type (0,1,...) that are acquired for church
following the Semcor bias. The last column gives
the number of examples in Semcor.
We have to note that the 3 first methods do not
require any hand-labeled data, and that the fourth
relies in Semcor.
The way to apply the bias is not straightforward
in some cases. In our first approach for Semcor-
bias, we assigned 1,000 examples to the major sense
in Semcor, and gave the other senses their propor-
tion of examples (when available). But in some
cases the distribution of the Semcor bias and that
of the actual examples in the web would not fit. The
problem is caused when there are not enough exam-
ples in the web to fill the expectations of a certain
word sense.
We therefore tried another distribution. We com-
puted, for each word, the minimum ratio of exam-
ples that were available for a given target bias and a
given number of examples extracted from the web.
We observed that this last approach would reflect
better the original bias, at the cost of having less ex-
amples.
Table 2 presents the different distributions of
examples for authority. There we can see the
Senseval-testing and Semcor distributions, together
with the total number of examples in the web; the
Semcor proportional distribution (Pr) and minimum
ratio (MR); and the automatic distribution. The
table illustrates how the proportional Semcor bias
produces a corpus where the percentage of some of
Word Web bias Semcor bias Automatic bias
art 15,387 10,656 2,610
authority 67,657 541 716
bar 50,925 16,627 5,329
bum 17,244 2,555 4,745
chair 24,625 8,512 2,111
channel 31,582 3,235 10,015
child 47,619 3,504 791
church 8,704 5,376 6,355
circuit 21,977 3,588 5,095
day 84,448 9,690 3,660
detention 2,650 1,510 511
dyke 4,210 1,367 843
facility 11,049 8,578 1,196
fatigue 6,237 3,438 5,477
feeling 9,601 1,160 945
grip 20,874 2,209 277
hearth 6,682 1,531 2,730
holiday 16,714 1,248 1,846
lady 12,161 2,959 884
material 100,109 7,855 6,385
mouth 648 287 464
nation 608 594 608
nature 32,553 24,746 9,813
post 34,968 4,264 8,005
restraint 33,055 2,152 2,877
sense 10,315 2,059 2,176
spade 5,361 2,458 2,657
stress 10,356 2,175 3,081
yew 10,767 2,000 8,013
Average 24,137 4,719 3,455
Total 699,086 136,874 100,215
Table 3: Number of examples following different
sense distributions. Minimum-ratio is applied for
the Semcor and automatic bias.
the senses is different from that in Semcor, e.g. the
first sense only gets 33.7% of the examples, in con-
trast to the 60% it had in Semcor.
We can also see how the distributions of senses
in Semcor and Senseval-test have important differ-
ences, although the main sense is the same. For the
web and automatic distributions, the first sense is
different; and in the case of the web distribution, the
first hand-tagged sense only accounts for 0.5% of
the examples retrieved from the web. Similar distri-
bution discrepancies can be observed for most of the
words in the test set. The Semcor MR column shows
how using minimum ratio we get a better reflection
of the proportion of examples in Semcor, compared
to the simpler proportional approach (Semcor Pr) .
For the automatic bias we only used the minimum
ratio.
To conclude this section, Table 3 shows the num-
ber of examples acquired automatically following
the web bias, the Semcor bias with minimum ratio,
and the Automatic bias with minimum ratio.
4.3 Local vs. topical features
Previous work on automatic acquisition of examples
(Leacock et al, 1998) has reported lower perfor-
mance when using local collocations formed by PoS
tags or closed-class words. We performed an early
experiment comparing the results using local fea-
tures, topical features, and a combination of both.
In this case we used the web corpus with Senseval
training bias, distributed according to the MR ap-
proach, and always substituting the target word. The
recall (per word and overall) is given in Table 4.
In this setting, we observed that local collocations
achieved the best precision overall, but the combina-
tion of all features obtained the best recall. The table
does not show the precision/coverage figures due to
space constraints, but local features achieve 58.5%
precision for 96.7% coverage overall, while topical
and combination of features have full-coverage.
There were clear differences in the results per
word, showing that estimating the best feature-set
per word would improve the performance. For the
corpus-evaluation experiments, we chose to work
with the combination of all features.
5 Evaluation
In all experiments, the recall of the systems is pre-
sented as evaluation measure. There is total cover-
age (because of the high overlap of topical features)
and the recall and precision are the same6.
In order to evaluate the acquired corpus, our first
task was to analyze the impact of bias. The results
are shown in Table 5. There are 2 figures for each
distribution: (1) simply assign the first ranked sense,
and (2) use the monosemous corpus following the
predetermined bias. As we described in Section 3,
the testing part of the Senseval-2 lexical sample data
was used for evaluation. We also include the results
using Senseval2 bias, which is taken from the train-
ing part. The recall per word for some distributions
can be seen in Table 4.
The results show clearly that when bias informa-
tion from a hand-tagged corpora is used the recall
improves significantly, even when the bias comes
from a corpus -Semcor- different from the target
corpus -Senseval-. The bias is useful by itself, and
we see that the higher the performance of the 1st
ranked sense heuristic, the lower the gain using the
monosemous corpus. We want to note that in fully
unsupervised mode we attain a recall of 43.2% with
the automatic ranking. Using the minimally su-
pervised information of bias, we get 49.8% if we
have the bias from an external corpus (Semcor) and
6Except for the experiment in Section 4.3, where using local
features the coverage is only partial.
Senseval bias Semcor Autom.
Word Loc. Top. Comb. bias bias
art 54.2 45.6 47.0 55.6 45.6
authority 47.8 43.2 46.2 41.8 40.0
bar 52.1 55.9 57.2 51.6 26.4
bum 81.2 87.5 85.0 5.0 57.5
chair 88.7 88.7 88.7 88.7 69.4
channel 39.7 53.7 55.9 16.2 30.9
child 56.5 55.6 56.5 54.0 34.7
church 67.7 51.6 54.8 48.4 49.7
circuit 45.3 54.2 56.1 41.5 49.1
day 59.4 54.7 56.8 48.0 12.5
detention 87.5 87.5 87.5 52.1 87.5
dyke 89.3 89.3 89.3 92.9 80.4
facility 28.6 21.4 21.4 26.8 22.0
fatigue 82.5 82.5 82.5 82.5 75.0
feeling 55.1 60.2 60.2 60.2 42.5
grip 19.0 38.0 39.0 16.0 28.2
hearth 73.4 75.0 75.0 75.0 60.4
holiday 96.3 96.3 96.3 96.3 72.2
lady 80.4 73.9 73.9 80.4 23.9
material 43.2 44.2 43.8 54.2 52.3
mouth 36.8 38.6 39.5 54.4 46.5
nation 80.6 80.6 80.6 80.6 80.6
nature 44.4 39.3 40.7 46.7 34.1
post 43.9 40.5 40.5 34.2 47.4
restraint 29.5 37.5 37.1 27.3 31.4
sense 58.1 37.2 38.4 47.7 41.9
spade 74.2 72.6 74.2 67.7 85.5
stress 53.9 46.1 48.7 2.6 27.6
yew 81.5 81.5 81.5 66.7 77.8
Overall 56.5 56.0 57.0 49.8 43.2
Table 4: Recall for all the nouns using the monose-
mous corpus with Senseval-2 training bias (MR, and
substitution), Semcor bias, and Automatic bias. The
Senseval-2 results are given by feature type.
57.5% if we have access to the bias of the target
corpus (Senseval7). This results show clearly that
the acquired corpus has useful information about the
word senses, and that bias is extremely important.
We will present two further experiments per-
formed with the monosemous corpus resource. The
goal of the first will be to measure the WSD per-
formance that we achieve using Semcor as the only
supervised data source. In our second experiment,
we will compare the performance of our totally un-
supervised approach (monosemous corpus and au-
tomatic bias) with other unsupervised approaches in
the Senseval-2 English lexical task.
5.1 Monosemous corpus and Semcor bias
In this experiment we compared the performance
using the monosemous corpus (with Semcor bias
and minimum ratio), and the examples from Sem-
cor. We noted that there were clear differences
depending on the number of training examples for
7Bias obtained from the training-set.
each word, therefore we studied each word-set de-
scribed in Section 3.4 separately. The results per
word-set are shown in Table 6. The figures cor-
respond to the recall training in Semcor, the web-
corpus, and the combination of both.
If we focus on set B (words with less than 10 ex-
amples in Semcor), we see that the MFS figure is
very low (40.1%). There are some words that do not
have any occurrence in Semcor, and thus the sense
is chosen at random. It made no sense to train the
DL for this set, therefore this result is not in the ta-
ble. For this set, the bias information from Semcor
is also scarce, but the DLs trained on the web-corpus
raise the performance to 47.8%.
For set A, the average number of examples is
higher, and this raises the results for Semcor MFS
(51.9%). We see that the recall for DL training
in Semcor is lower that the MFS baseline (50.5%).
The main reasons for these low results are the dif-
ferences between the training and testing corpora
(Semcor and Senseval). There have been previous
works on portability of hand-tagged corpora that
show how some constraints, like the genre or topic
of the corpus, affect heavily the results (Martinez
and Agirre, 2000). If we train on the web-corpus
the results improve, and the best results are ob-
tained with the combination of both corpora, reach-
ing 51.6%. We need to note, however, that this is
still lower than the Semcor MFS.
Finally, we will examine the results for the whole
set of nouns in the Senseval-2 lexical-sample (last
row in Table 6), where we see that the best approach
relies on the web-corpus. In order to disambiguate
the 29 nouns using only Semcor, we apply MFS
when there are less than 10 examples (set B), and
train the DLs for the rest.
The results in Table 6 show that the web-corpus
raises recall, and the best results are obtained com-
bining the Semcor data and the web examples
(50.3%). As we noted, the web-corpus is specially
useful when there are few examples in Semcor (set
B), therefore we made another test, using the web-
corpus only for set B, and applying MFS for set A.
The recall was slightly better (50.5%), as is shown
in the last column.
5.2 Monosemous corpus and Automatic bias
(unsupervised method)
In this experiment we compared the performance
of our unsupervised system with other approaches.
For this goal, we used the resources available from
the Senseval-2 competition8, where the answers of
the participating systems in the different tasks were
8http://www.senseval.org.
Bias Type 1stsense
Train
exam. Diff.
no bias 18.3 38.0 +19.7
web bias unsuperv. 33.3 39.8 +6.5
autom. ranking 36.1 43.2 +7.1
Semcor bias minimally- 47.8 49.8 +2.0
Senseval2 bias supervised 55.6 57.5 +1.9
Table 5: Performance (recall) on Senseval-2 lexical-
sample, using different bias to create the corpus.
The type column shows the kind of system.
Word-set MFS Semcor Web Semcor
+ Web
MFS &
Web
set A (> 10) 51.9 50.5 50.9 51.6 51.9
set B (< 10) 40.1 - 47.7 47.8 47.8
all words 47.8 47.4 49.8 50.3 50.5
Table 6: Recall training in Semcor, the acquired
web corpus (Semcor bias), and a combination of
both, compared to that of the Semcor MFS.
available. This made possible to compare our re-
sults and those of other systems deemed unsuper-
vised by the organizers on the same test data and set
of nouns.
From the 5 unsupervised systems presented in
the Senseval-2 lexical-sample task as unsupervised,
the WASP-Bench system relied on lexicographers
to hand-code information semi-automatically (Tug-
well and Kilgarriff, 2001). This system does not
use the training data, but as it uses manually coded
knowledge we think it falls clearly in the supervised
category.
The results for the other 4 systems and our own
are shown in Table 7. We show the results for the
totally unsupervised system and the minimally un-
supervised system (Semcor bias). We classified the
UNED system (Fernandez-Amoros et al, 2001) as
minimally supervised. It does not use hand-tagged
examples for training, but some of the heuristics that
are applied by the system rely on the bias informa-
tion available in Semcor. The distribution of senses
is used to discard low-frequency senses, and also to
choose the first sense as a back-off strategy. On the
same conditions, our minimally supervised system
attains 49.8 recall, nearly 5 points more.
The rest of the systems are fully unsupervised,
and they perform significantly worse than our sys-
tem.
6 Conclusions and Future Work
This paper explores the large-scale acquisition of
sense-tagged examples for WSD, which is a very
Method Type Recall
Web corpus (Semcor bias) minimally- 49.8
UNED supervised 45.1
Web corpus (Autom. bias) 43.3
Kenneth Litkowski-clr-ls unsupervised 35.8
Haynes-IIT2 27.9
Haynes-IIT1 26.4
Table 7: Our minimally supervised and fully unsu-
pervised systems compared to the unsupervised sys-
tems (marked in bold) in the 29 noun subset of the
Senseval-2 Lexical Sample.
promising line of research, but remains relatively
under-studied. We have applied the ?monosemous
relatives? method to construct automatically a web
corpus which we have used to train three systems
based on Decision Lists: one fully supervised (ap-
plying examples from Semcor and the web corpus),
one minimally supervised (relying on the distribu-
tion of senses in Semcor and the web corpus) and
another fully unsupervised (using an automatically
acquired sense rank and the web corpus). Those
systems were tested on the Senseval-2 lexical sam-
ple test set.
We have shown that the fully supervised system
combining our web corpus with the examples in
Semcor improves over the same system trained on
Semcor alone. This improvement is specially no-
ticeable in the nouns that have less than 10 examples
in Semcor. Regarding the minimally supervised
and fully unsupervised systems, we have shown
that they perform well better than the other systems
of the same category presented in the Senseval-2
lexical-sample competition.
The system can be trained for all nouns
in WordNet, using the data available at
http://ixa2.si.ehu.es/pub/sensecorpus.
The research also highlights the importance of
bias. Knowing how many examples are to be fed
into the machine learning system is a key issue. We
have explored several possibilities, and shown that
the learning system (DL) is able to learn from the
web corpus in all the cases, beating the respective
heuristic for sense distribution.
We think that this research opens the opportu-
nity for further improvements. We have to note that
the MFS heuristic and the supervised systems based
on the Senseval-2 training data are well ahead of
our results, and our research aims at investigating
ideas to close this gap. Some experiments on the
line of adding automatically retrieved examples to
available hand-tagged data (Semcor and Senseval-
2) have been explored. The preliminary results indi-
cate that this process has to be performed carefully,
taking into account the bias of the senses and apply-
ing a quality-check of the examples before they are
included in the training data.
For the future we also want to test the perfor-
mance of more powerful Machine Learning meth-
ods, explore feature selection methods for each in-
dividual word, and more sophisticated ways to com-
bine the examples from the web corpus with those
of Semcor or Senseval. Now that the monosemous
corpus is available for all nouns, we would also like
to test the system on the all-words task. In addition,
we will give preference to multiwords that contain
the target word when choosing the relatives. Finally,
more sophisticated methods to acquire examples are
now available, like ExRetriever (Fernandez et al,
2004), and they could open the way to better exam-
ples and performance.
7 Acknowledgments
We wish to thank Diana McCarthy, from the Univer-
sity of Sussex, for providing us the sense rank for
the target nouns. This research has been partially
funded by the European Commission (MEANING
IST-2001-34460).
References
E. Agirre and O. Lopez. 2004. Publicly available
topic signatures for all wordnet nominal senses.
In Proceedings of the 4rd International Con-
ference on Language Resources and Evaluation
(LREC), Lisbon, Portugal.
E. Agirre and D. Martinez. 2000. Exploring auto-
matic word sense disambiguation with decision
lists and the web. In Procedings of the COLING
2000 Workshop on Semantic Annotation and In-
telligent Content, Luxembourg.
S. Argamon-Engelson and I. Dagan. 1999.
Committee-based sample selection for proba-
bilistic classifiers. In Journal of Artificial Intel-
ligence Research, volume 11, pages 335?360.
T. Brants. 2000. Tnt - a statistical part-of-speech
tagger. In Proceedings of the Sixth Applied Nat-
ural Language Processing Conference, Seattle,
WA.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping
wordnets using structural information. In 38th
Anual Meeting of the Association for Computa-
tional Linguistics (ACL?2000), Hong Kong.
P. Edmonds and S. Cotton. 2001. Senseval-2:
Overview. In Proceedings of the Second Interna-
tional Workshop on evaluating Word Sense Dis-
ambiguation Systems, Toulouse, France.
D. Fernandez-Amoros, J. Gonzalo, and F. Verdejo.
2001. The uned systems at senseval-2. In Pro-
ceedings of the SENSEVAL-2 Workshop. In con-
junction with ACL, Toulouse, France.
J. Fernandez, M. Castillo, G. Rigau, J. Atserias, and
J. Turmo. 2004. Automatic acquisition of sense
examples using exretriever. In Proceedings of the
4rd International Conference on Language Re-
sources and Evaluation (LREC), Lisbon, Portu-
gal.
C. Leacock, M. Chodorow, and G. A. Miller. 1998.
Using corpus statistics and WordNet relations for
sense identification. In Computational Linguis-
tics, volume 24, pages 147?165.
D. Lin. 1998. Automatic retrieval and clustering
of similar words. In In Proceedings of COLING-
ACL, Montreal, Canada.
D. Martinez and E. Agirre. 2000. One sense per
collocation and genre/topic variations. In Pro-
ceedings of the Joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing
and Very Large Corpora, Hong Kong.
D. McCarthy, R. Koeling, J. Weeds, and J. Car-
roll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL) (to appear), Barcelona,
Spain.
R. Mihalcea. 2002. Bootstrapping large sense
tagged corpora. In Proceedings of the 3rd Inter-
national Conference on Language Resources and
Evaluation (LREC), Las Palmas, Spain.
G. A. Miller, C. Leacock, R. Tengi, and R. Bunker.
1993. A semantic concordance. In Proceedings
of the ARPA Human Language Technology Work-
shop, pages 303?308, Princeton, NJ.
D. Tugwell and A. Kilgarriff. 2001. Wasp-bench:
a lexicographic tool supporting word sense dis-
ambiguation. In Proceedings of the SENSEVAL-2
Workshop. In conjunction with ACL-2001/EACL-
2001, Toulouse, France.
D. Yarowsky. 1994. Decision lists for lexical am-
biguity resolution: Application to accent restora-
tion in spanish and french. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, Las Cruces, NM.
D. Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics
(ACL), Cambridge, MA.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 585?593,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Two graph-based algorithms for state-of-the-art WSD
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle and Aitor Soroa
IXA NLP Group
University of the Basque Country
Donostia, Basque Contry
a.soroa@si.ehu.es
Abstract
This paper explores the use of two graph
algorithms for unsupervised induction and
tagging of nominal word senses based on
corpora. Our main contribution is the op-
timization of the free parameters of those
algorithms and its evaluation against pub-
licly available gold standards. We present
a thorough evaluation comprising super-
vised and unsupervised modes, and both
lexical-sample and all-words tasks. The
results show that, in spite of the infor-
mation loss inherent to mapping the in-
duced senses to the gold-standard, the
optimization of parameters based on a
small sample of nouns carries over to all
nouns, performing close to supervised sys-
tems in the lexical sample task and yield-
ing the second-best WSD systems for the
Senseval-3 all-words task.
1 Introduction
Word sense disambiguation (WSD) is a key
enabling-technology. Supervised WSD tech-
niques are the best performing in public evalu-
ations, but need large amounts of hand-tagged
data. Existing hand-annotated corpora like Sem-
Cor (Miller et al, 1993), which is annotated with
WordNet senses (Fellbaum, 1998) allow for a
small improvement over the simple most frequent
sense heuristic, as attested in the all-words track of
the last Senseval competition (Snyder and Palmer,
2004). In theory, larger amounts of training data
(SemCor has approx. 700K words) would improve
the performance of supervised WSD, but no cur-
rent project exists to provide such an expensive re-
source.
Supervised WSD is based on the ?fixed-list of
senses? paradigm, where the senses for a tar-
get word are a closed list coming from a dic-
tionary or lexicon. Lexicographers and seman-
ticists have long warned about the problems of
such an approach, where senses are listed sepa-
rately as discrete entities, and have argued in fa-
vor of more complex representations, where, for
instance, senses are dense regions in a contin-
uum (Cruse, 2000).
Unsupervised WSD has followed this line of
thinking, and tries to induce word senses directly
from the corpus. Typical unsupervised WSD sys-
tems involve clustering techniques, which group
together similar examples. Given a set of induced
clusters (which represent word uses or senses1),
each new occurrence of the target word will be
compared to the clusters and the most similar clus-
ter will be selected as its sense.
Most of the unsupervised WSD work has been
based on the vector space model, where each
example is represented by a vector of features
(e.g. the words occurring in the context), and
the induced senses are either clusters of ex-
amples (Schu?tze, 1998; Purandare and Peder-
sen, 2004) or clusters of words (Pantel and Lin,
2002). Recently, Ve?ronis (Ve?ronis, 2004) has pro-
posed HyperLex, an application of graph models
to WSD based on the small-world properties of
cooccurrence graphs. Graph-based methods have
gained attention in several areas of NLP, including
knowledge-based WSD (Mihalcea, 2005; Navigli
and Velardi, 2005) and summarization (Erkan and
Radev, 2004; Mihalcea and Tarau, 2004).
The HyperLex algorithm presented in (Ve?ronis,
2004) is entirely corpus-based. It builds a cooccur-
rence graph for all pairs of words cooccurring in
the context of the target word. Ve?ronis shows that
this kind of graph fulfills the properties of small
world graphs, and thus possesses highly connected
1Unsupervised WSD approaches prefer the term ?word
uses? to ?word senses?. In this paper we use them inter-
changeably to refer to both the induced clusters, and to the
word senses from some reference lexicon.
585
components (hubs) in the graph. These hubs even-
tually identify the main word uses (senses) of the
target word, and can be used to perform word
sense disambiguation. These hubs are used as a
representation of the senses induced by the sys-
tem, the same way that clusters of examples are
used to represent senses in clustering approaches
to WSD (Purandare and Pedersen, 2004).
One of the problems of unsupervised systems
is that of managing to do a fair evaluation.
Most of current unsupervised systems are evalu-
ated in-house, with a brief comparison to a re-
implementation of a former system, leading to a
proliferation of unsupervised systems with little
ground to compare among them.
In preliminary work (Agirre et al, 2006), we
have shown that HyperLex compares favorably
to other unsupervised systems. We defined a
semi-supervised setting for optimizing the free-
parameters of HyperLex on the Senseval-2 En-
glish Lexical Sample task (S2LS), which con-
sisted on mapping the induced senses onto the
official sense inventory using the training part of
S2LS. The best parameters were then used on the
Senseval-3 English Lexical Sample task (S3LS),
where a similar semi-supervised method was used
to output the official sense inventory.
This paper extends the previous work in sev-
eral aspects. First of all, we adapted the PageR-
ank graph-based method (Brin and Page, 1998)
for WSD and compared it with HyperLex.
We also extend the previous evaluation scheme,
using measures in the clustering community which
only require a gold standard clustering and no
mapping step. This allows for having a purely
unsupervised WSD system, and at the same time
comparing supervised and unsupervised systems
according to clustering criteria.
We also include the Senseval-3 English All-
words testbed (S3AW), where, in principle, unsu-
pervised and semi-supervised systems have an ad-
vantage over purely supervised systems due to the
scarcity of training data. We show that our sys-
tem is competitive with supervised systems, rank-
ing second.
This paper is structured as follows. We first
present two graph-based algorithms, HyperLex
and PageRank. Section 3 presents the two evalu-
ation frameworks. Section 4 introduces parameter
optimization. Section 5 shows the experimental
setting and results. Section 6 analyzes the results
and presents related work. Finally, we draw the
conclusions and advance future work.
2 A graph algorithm for corpus-based
WSD
The basic steps for our implementation of Hyper-
Lex and its variant using PageRank are common.
We first build the cooccurrence graph, then we se-
lect the hubs that are going to represent the senses
using two different strategies inspired by Hyper-
Lex and PageRank. We are then ready to use the
induced senses to do word sense disambiguation.
2.1 Building cooccurrence graphs
For each word to be disambiguated, a text corpus
is collected, consisting of the paragraphs where
the word occurs. From this corpus, a cooccur-
rence graph for the target word is built. Vertices
in the graph correspond to words2 in the text (ex-
cept the target word itself). Two words appear-
ing in the same paragraph are said to cooccur, and
are connected with edges. Each edge is assigned
a weight which measures the relative frequency of
the two words cooccurring. Specifically, let wij be
the weight of the edge3 connecting nodes i and j,
then wij = 1 ? max[P (i | j), P (j | i)], where
P (i | j) = freqijfreqj and P (j | i) =
freqij
freqi .The weight of an edge measures how tightly
connected the two words are. Words which always
occur together receive a weight of 0. Words rarely
cooccurring receive weights close to 1.
2.2 Selecting hubs: HyperLex vs. PageRank
Once the cooccurrence graph is built, Ve?ronis pro-
poses a simple iterative algorithm to obtain its
hubs. At each step, the algorithm finds the ver-
tex with highest relative frequency4 in the graph,
and, if it meets some criteria, it is selected as a hub.
These criteria are determined by a set of heuristic
parameters, that will be explained later in Section
4. After a vertex is selected to be a hub, its neigh-
bors are no longer eligible as hub candidates. At
any time, if the next vertex candidate has a relative
frequency below a certain threshold, the algorithm
stops.
Another alternative is to use the PageRank algo-
rithm (Brin and Page, 1998) for finding hubs in the
2Following Ve?ronis, we only work on nouns.
3The cooccurrence graph is undirected, i.e. wij = wji
4In cooccurrence graphs, the relative frequency of a vertex
and its degree are linearly related, and it is therefore possible
to avoid the costly computation of the degree.
586
coocurrence graph. PageRank is an iterative algo-
rithm that ranks all the vertices according to their
relative importance within the graph following a
random-walk model. In this model, a link between
vertices v1 and v2 means that v1 recommends v2.
The more vertices recommend v2, the higher the
rank of v2 will be. Furthermore, the rank of a ver-
tex depends not only on how many vertices point
to it, but on the rank of these vertices as well.
Although PageRank was initially designed to
work with directed graphs, and with no weights
in links, the algorithm can be easily extended
to model undirected graphs whose edges are
weighted. Specifically, let G = (V, E) be an undi-
rected graph with the set of vertices V and set of
edges E. For a given vertex vi, let In(vi) be the set
of vertices pointing to it5. The rank of vi is defined
as:
P (vi) = (1? d) + d
?
j?In(vi)
wji
?
k?In(vj) wjk
P (vj)
where wij is the weight of the link between ver-
tices vi and vj , and 0 ? d ? 1. d is called the
damping factor and models the probability of a
web surfer standing at a vertex to follow a link
from this vertex (probability d) or to jump to a ran-
dom vertex in the graph (probability 1 ? d). The
factor is usually set at 0.85.
The algorithm initializes the ranks of the ver-
tices with a fixed value (usually 1N for a graph with
N vertices) and iterates until convergence below a
given threshold is achieved, or, more typically, un-
til a fixed number of iterations are executed. Note
that the convergence of the algorithms doesn?t de-
pend on the initial value of the ranks.
After running the algorithm, the vertices of the
graph are ordered in decreasing order according to
its rank, and a number of them are chosen as the
main hubs of the word. The hubs finally selected
depend again of some heuristics and will be de-
scribed in section 4.
2.3 Using hubs for WSD
Once the hubs that represent the senses of the word
are selected (following any of the methods pre-
sented in the last section), each of them is linked
to the target word with edges weighting 0, and
the Minimum Spanning Tree (MST) of the whole
graph is calculated and stored.
5As G is undirected, the in-degree of a vertex v is equal
to its out-degree.
The MST is then used to perform word sense
disambiguation, in the following way. For every
instance of the target word, the words surrounding
it are examined and looked up in the MST. By con-
struction of the MST, words in it are placed under
exactly one hub. Each word in the context receives
a set of scores s, with one score per hub, where all
scores are 0 except the one corresponding to the
hub where it is placed. If the scores are organized
in a score vector, all values are 0, except, say, the
i-th component, which receives a score d(hi, v),
which is the distance between the hub hi and the
node representing the word v. Thus, d(hi, v) as-
signs a score of 1 to hubs and the score decreases
as the nodes move away from the hub in the tree.
For a given occurrence of the target word, the
score vectors of all the words in the context are
added, and the hub that receives the maximum
score is chosen.
3 Evaluating unsupervised WSD systems
All unsupervised WSD algorithms need some ad-
dition in order to be evaluated. One alternative, as
in (Ve?ronis, 2004), is to manually decide the cor-
rectness of the hubs assigned to each occurrence
of the words. This approach has two main disad-
vantages. First, it is expensive to manually verify
each occurrence of the word, and different runs of
the algorithm need to be evaluated in turn. Sec-
ond, it is not an easy task to manually decide if
an occurrence of a word effectively corresponds
with the use of the word the assigned hub refers
to, specially considering that the person is given a
short list of words linked to the hub. Besides, it is
widely acknowledged that people are leaned not to
contradict the proposed answer.
A second alternative is to evaluate the system
according to some performance in an application,
e.g. information retrieval (Schu?tze, 1998). This is
a very attractive idea, but requires expensive sys-
tem development and it is sometimes difficult to
separate the reasons for the good (or bad) perfor-
mance.
A third alternative would be to devise a method
to map the hubs (clusters) returned by the system
to the senses in a lexicon. Pantel and Lin (2002)
automatically mapped the senses to WordNet, and
then measured the quality of the mapping. More
recently, tagged corpora have been used to map
the induced senses, and then compare the sys-
tems over publicly available benchmarks (Puran-
587
dare and Pedersen, 2004; Niu et al, 2005; Agirre
et al, 2006), which offers the advantage of com-
paring to other systems, but converts the whole
system into semi-supervised. See Section 5 for
more details on these systems. Note that the map-
ping introduces noise and information loss, which
is a disadvantage when comparing to other sys-
tems that rely on the gold-standard senses.
Yet another possibility is to evaluate the induced
senses against a gold standard as a clustering task.
Induced senses are clusters, gold standard senses
are classes, and measures from the clustering lit-
erature like entropy or purity can be used. In this
case the manually tagged corpus is taken to be the
gold standard, where a class is the set of examples
tagged with a sense.
We decided to adopt the last two alternatives,
since they allow for comparison over publicly
available systems of any kind.
3.1 Evaluation of clustering: hubs as clusters
In this setting the selected hubs are treated as
clusters of examples and gold standard senses are
classes. In order to compare the clusters with the
classes, hand annotated corpora are needed (for in-
stance Senseval). The test set is first tagged with
the induced senses. A perfect clustering solution
will be the one where each cluster has exactly the
same examples as one of the classes, and vice
versa. The evaluation is completely unsupervised.
Following standard cluster evaluation prac-
tice (Zhao and Karypis, 2005), we consider three
measures: entropy, purity and Fscore. The entropy
measure considers how the various classes of ob-
jects are distributed within each cluster. In gen-
eral, the smaller the entropy value, the better the
clustering algorithm performs. The purity mea-
sure considers the extent to which each cluster
contained objects from primarily one class. The
larger the values of purity, the better the cluster-
ing algorithm performs. The Fscore is used in a
similar fashion to Information Retrieval exercises,
with precision and recall defined as the percent-
age of correctly ?retrieved? examples for a clus-
ter (divided by total cluster size), and recall as the
percentage of correctly ?retrieved? examples for a
cluster (divided by total class size). For a formal
definition refer to (Zhao and Karypis, 2005). If the
clustering is identical to the original classes in the
datasets, FScore will be equal to one which means
that the higher the FScore, the better the clustering
is.
3.2 Evaluation as supervised WSD: mapping
hubs to senses
(Agirre et al, 2006) presents a straightforward
framework that uses hand-tagged material in or-
der to map the induced senses into the senses used
in a gold standard . The WSD system first tags the
training part of some hand-annotated corpus with
the induced hubs. The hand labels are then used
to construct a matrix relating assigned hubs to ex-
isting senses, simply counting the times an occur-
rence with sense sj has been assigned hub hi. In
the testing step we apply the WSD algorithm over
the test corpus, using the hubs-to-senses matrix to
select the sense with highest weights. See (Agirre
et al, 2006) for further details.
4 Tuning the parameters
The behavior of the original HyperLex algorithm
was influenced by a set of heuristic parameters,
which were set by Ve?ronis following his intuition.
In (Agirre et al, 2006) we tuned the parameters us-
ing the mapping strategy for evaluation. We set a
range for each of the parameters, and evaluated the
algorithm for each combination of the parameters
on a fixed set of words (S2LS), which was differ-
ent from the final test sets (S3LS and S3AW). This
ensures that the chosen parameter set can be used
for any noun, and is not overfitted to a small set of
nouns.
In this paper, we perform the parameter tuning
according to four different criteria, i.e., best su-
pervised performance and best unsupervised en-
tropy/purity/FScore performance. At the end, we
have four sets of parameters (those that obtained
the best results in S2LS for each criterion), and
each set is then selected to be run against the S3LS
and S3AW datasets.
The parameters of the graph-based algorithm
can be divided in two sets: those that affect how
the cooccurrence graph is built (p1?p4 below), and
those that control the way the hubs are extracted
from it (p5?p8 below).
p1 Minimum frequency of edges (occurrences)
p2 Minimum frequency of vertices (words)
p3 Edges with weights above this value are removed
p4 Context containing fewer words are not processed
p5 Minimum number of adjacent vertices in a hub
p6 Max. mean weight of the adjacent vertices of a hub
p7 Minimum frequency of hubs
p8 Number of selected hubs
588
Vr opt Pr fr (p7) and Pr fx (p8)
Vr Range Best Range Best
p1 5 1-3 1 1-3 2
p2 10 2-4 3 2-4 3
p3 .9 .3-.7 .4 .4-.5 .5
p4 4 4 4 4 4
p5 6 1-7 1 ? ?
p6 .8 .6-.95 .95 ? ?
p7 .001 .0009-.003 .001 .0015-.0025 .0016
p8 ? ? ? 50-65 55
Table 1: Parameters of the HyperLex algorithm
Both strategies to select hubs from the coocur-
rence graph (cf. Sect. 2.2) share parameters p1?
p4. The algorithm proposed by Ve?ronis uses p5?
p6 as requirements for hubs, and p7 as the thresh-
old to stop looking for more hubs: candidates with
frequency below p7 are not eligible to be hubs.
Regarding PageRank the original formulation
does not have any provision for determining which
are hubs and which not, it just returns a weighted
list of vertices. We have experimented with two
methods: a threshold for the frequency of the hubs
(as before, p7), and a fixed number of hubs for ev-
ery target word (p8). For a shorthand we use Vr for
Veronis? original formulation with default param-
eters, Vr opt for optimized parameters, and Pr fr
and Pr fx respectively for the two ways of using
PageRank.
Table 1 lists the parameters of the HyperLex al-
gorithm, with the default values proposed for them
in the original work (second column), the ranges
that we explored, and the optimal values according
to the supervised recall evaluation (cf. Sect. 3.1).
For Vr opt we tried 6700 combinations. PageRank
has less parameters, and we also used the previous
optimization of Vr opt to limit the range of p4, so
Pr fr and Pr fx get respectively 180 and 288 com-
binations.
5 Experiment setting and results
To evaluate the HyperLex algorithm in a standard
benchmark, we will first focus on a more exten-
sive evaluation of S3LS and then see the results
in S3AW (cf. Sec. 5.4). Following the design
for evaluation explained in Section 3, we use the
standard train-test split for the supervised evalua-
tion, while the unsupervised evaluation only uses
the test part.
Table 2 shows the results of the 4 variants of
our algorithm. Vr stands for the original Vero-
nis algorithm with default parameters, Vr opt to
our optimized version, and Pr fr and Pr fx to the
Sup. Unsupervised
Rec. Entr. Pur. FS
Vr 59.9 50.3 58.2 44.1
Vr opt 64.6 18.3 78.5 35.0
Pr fr 64.5 18.7 77.2 34.3
Pr fx 62.2 25.4 72.2 33.3
1ex-1hub 40.1 0.0 100.0 14.5
MFS 54.5 53.2 52.8 28.3
S3LS-best 72.9 19.9 67.3 63.8
kNN-all 70.6 21.2 64.0 60.6
kNN-BoW 63.5 22.6 61.1 57.1
Cymfony (10%-S3LS) 57.9 25.0 55.7 52.0
Prob0 (MFS-S3) 54.2 28.8 49.3 46.0
clr04 (MFS-Sc) 48.8 25.8 52.5 46.2
Ciaosenso (MFS-Sc) 48.7 28.0 50.3 48.8
duluth-senserelate 47.5 27.2 51.1 44.9
Table 2: Results for the nouns in S3LS using the 4 meth-
ods (Vr, Vr opt, Pr fr and Pr fx). Each of the methods was
optimized in S2LS using the 4 evaluation criteria (Supervised
recall, Entropy, Purity and Fscore) and evaluated on S3LS ac-
cording to the respective evaluation criteria (in the columns).
Two baselines, plus 3 supervised and 5 unsupervised systems
are also shown. Bold is used for best results in each category.
two variants of PageRank. In the columns we find
the evaluation results according to our 4 criteria.
For supervised evaluation we indicate only recall,
which in our case equals precision, as the cover-
age is 100% in all cases (values returned by the
official Senseval scorer). We also include 2 base-
lines, a system returning a single cluster (that of
the most frequent sense, MFS), and another re-
turning one cluster for each example (1ex-1hub).
The last rows list the results for 3 supervised and
5 unsupervised systems (see Sect. 5.1). We will
comment on the result of this table from different
perspectives.
5.1 Supervised evaluation
In this subsection we will focus in the first four
evaluation rows in Table 2. All variants of the al-
gorithm outperform by an ample margin the MFS
and the 1ex-1hub baselines when evaluated on
S3LS recall. This means that the method is able
to learn useful hubs. Note that we perform this su-
pervised evaluation just for comparison with other
systems, and to prove that we are able to provide
high performance WSD.
The default parameter setting (Vr) gets the
worst results, followed by the fixed-hub imple-
mentation of PageRank (Pr fx). Pagerank with
frequency threshold (Pr fr) and the optimized
Veronis (Vr opt) obtain a 10 point improvement
over the MFS baseline with very similar results
(the difference is not statistically significant ac-
cording to McNemar?s test at 95% confidence
589
level).
Table 2 also shows the results of three super-
vised systems. These results (and those of the
other unsupervised systems in the table) where ob-
tained from the Senseval website, and the only
processing we did was to filter nouns. S3LS-best
stands for the the winner of S3LS (Mihalcea et al,
2004), which is 8.3 points over our method. We
also include the results of two of our in-house sys-
tems. kNN-all is a state-of-the-art system (Agirre
et al, 2005) using wide range of local and top-
ical features, and only 2.3 points below the best
S3LS system. kNN-BoW which is the same super-
vised system, but restricted to bag-of-words fea-
tures only, which are the ones used by our graph-
based systems. The table shows that Vr opt and
Pr fr are one single point from kNN-BoW, which
is an impressive result if we take into account the
information loss of the mapping step and that we
tuned our parameters on a different set of words.
The last 5 rows of Table 2 show several un-
supervised systems, all of which except Cym-
fony (Niu et al, 2005) and (Purandare and Ped-
ersen, 2004) participated in S3LS (check (Mihal-
cea et al, 2004) for further details on the systems).
We classify them according to the amount of ?su-
pervision? they have: some have access to most-
frequent information (MFS-S3 if counted over
S3LS, MFS-Sc if counted over SemCor), some use
10% of the S3LS training part for mapping (10%-
S3LS). Only one system (Duluth) did not use in
any way hand-tagged corpora.
The table shows that Vr opt and Pr fr are more
than 6 points above the other unsupervised sys-
tems, but given the different typology of unsuper-
vised systems, it?s unfair to draw definitive con-
clusions from a raw comparison of results. The
system coming closer to ours is that described in
(Niu et al, 2005). They use hand tagged corpora
which does not need to include the target word to
tune the parameters of a rather complex clustering
method which does use local features. They do use
the S3LS training corpus for mapping. For every
sense of the target word, three of its contexts in
the train corpus are gathered (around 10% of the
training data) and tagged. Each cluster is then re-
lated with its most frequent sense. The mapping
method is similar to ours, but we use all the avail-
able training data and allow for different hubs to
be assigned to the same sense.
Another system similar to ours is (Purandare
and Pedersen, 2004), which unfortunately was
evaluated on Senseval 2 data and is not included
in the table. The authors use first and second or-
der bag-of-word context features to represent each
instance of the corpus. They apply several cluster-
ing algorithms based on the vector space model,
limiting the number of clusters to 7. They also
use all available training data for mapping, but
given their small number of clusters they opt for a
one-to-one mapping which maximizes the assign-
ment and discards the less frequent clusters. They
also discard some difficult cases, like senses and
words with low frequencies (10% of total occur-
rences and 90, respectively). The different test set
and mapping system make the comparison diffi-
cult, but the fact that the best of their combina-
tions beats MFS by 1 point on average (47.6% vs.
46.4%) for the selected nouns and senses make us
think that our results are more robust (nearly 10%
over MFS).
5.2 Clustering evaluation
The three columns corresponding to fully unsu-
pervised evaluation in Table 2 show that all our
3 optimized variants easily outperform the MFS
baseline. The best results are in this case for the
optimized Veronis, followed closely by Pagerank
with frequency threshold.
The comparison with the supervised and unsu-
pervised systems shows that our system gets better
entropy and purity values, but worse FScore. This
can be explained by the bias of entropy and purity
towards smaller and more numerous clusters. In
fact the 1ex-1hub baseline obtains the best entropy
and purity scores. Our graph-based system tends
to induce a large number of senses (with averages
of 60 to 70 senses). On the other hand FScore pe-
nalizes the systems inducing a different number of
clusters. As the supervised and unsupervised sys-
tems were designed to return the same (or similar)
number of senses as in the gold standard, they at-
tain higher FScores. This motivated us to compare
the results of the best parameters across evaluation
methods.
5.3 Comparison across evaluation methods
Table 3 shows all 16 evaluation possibilities for
each variant of the algorithm, depending of the
evaluation criteria used in S2LS (in the rows)
and the evaluation criteria used in S3LS (in the
columns). This table shows that the best results (in
bold for each variant) tend to be in the diagonal,
590
that is, when the same evaluation criterion is used
for optimization and test, but it is not decisive. If
we take the first row (supervised evaluation) as the
most credible criterion, we can see that optimiz-
ing according to entropy and purity get similar and
sometimes better result (Pr fr and Pr fx). On the
contrary the Fscore yields worse results by far.
This indicates that a purely unsupervised sys-
tem evaluated according to the gold standard
(based on entropy or purity) yields optimal param-
eters similar to the supervised (mapped) version.
This is an important result, as it shows that the
quality in performance does not come from the
mapping step, but from the algorithm and opti-
mal parameter setting. The table shows that op-
timization on purity and entropy criteria do corre-
late with good performance in the supervised eval-
uation.
The failure of FScore based optimization, in our
opinion, indicates that our clustering algorithm
prefers smaller and more numerous clusters, com-
pared to the gold standard. FScore prefers cluster-
ing solutions that have a similar number of clusters
to that of the gold standard, but it is unable to drive
the optimization or our algorithm towards good re-
sults in the supervised evaluation.
All in all, the best results are attained with
smaller and more numerous hubs, a kind of micro-
senses. This effect is the same for all three vari-
ants tried and all evaluation criteria, with Fscore
yielding less clusters. At first we were uncom-
fortable with this behavior, so we checked whether
HyperLex was degenerating into a trivial solution.
This was the main reason to include the 1ex-1hub
baseline, which simulates a clustering algorithm
returning one hub per example, and its precision
was 40.1, well below the MFS baseline. We also
realized that our results are in accordance with
some theories of word meaning, e.g. the ?indef-
initely large set of prototypes-within-prototypes?
envisioned in (Cruse, 2000). Ted Pedersen has
also observed a similar behaviour in his vector-
space model clustering experiments (PC). We now
think that the idea of having many micro-senses
is attractive for further exploration, specially if we
are able to organize them into coarser hubs in fu-
ture work.
5.4 S3AW task
In the Senseval-3 all-words task (Snyder and
Palmer, 2004) all words in three document ex-
Sup. Unsupervised
Alg. Opt. Rec. Entr. Pur. FS
Vr Sup 64.6 18.4 77.9 30.0
Ent 64.6 18.3 78.3 29.1
Pur 63.7 19.0 78.5 30.8
Fsc 60.4 38.2 63.5 35.0
Pr fr Sup 64.5 20.8 76.1 28.6
Ent 64.6 18.7 77.7 27.2
Pur 64.7 19.3 77.2 27.6
Fsc 61.2 36.0 65.2 34.3
Pr fx Sup 62.2 28.2 69.3 29.5
Ent 63.1 25.4 72.2 28.4
Pur 63.1 25.4 72.2 28.4
Fsc 54.5 32.9 66.5 33.3
Table 3: Cross-evaluation comparison. In the rows the eval-
uation method for optmizing over S2LS is shown, and in the
columns the result over S3LS according to the different eval-
uation methods.
recall
kuaw 70.9
Pr fr 70.7
Vr opt 70.1
GAMBL 70.1
MFS 69.9
LCCaw 68.6
Table 4: Results for the nouns in S3AW, compared to the
most frequent baseline and the top three supervised systems
cerpts need to be disambiguated. Given the
scarce amount of training data available in Sem-
cor (Miller et al, 1993), supervised systems barely
improve upon the simple most frequent heuris-
tic. In this setting the unsupervised evaluation
schemes are not feasible, as many of the target
words occur only once, so we used the map-
ping strategy with Semcor to produce the required
WordNet senses in the output.
Table 4 shows the results for our systems with
the best parameters according to the supervised
criterion on S2LS, plus the top three S3AW super-
vised systems and the most frequent sense heuris-
tic. In order to focus the comparison, we only kept
noun occurrences of all systems and filtered out
multiwords, target words with two different lem-
mas and unknown tags, leaving a total of 857 oc-
currences of nouns. We can see that Pr fr is only
0.2 from the S3AW winning system, demonstrat-
ing that our unsupervised graph-based systems
that use Semcor for mapping are nearly equivalent
to the most powerful supervised systems to date.
In fact, the differences in performance for the sys-
tems are not statistically significant (McNemar?s
test at 95% significance level).
591
6 Conclusions and further work
This paper has explored the use of two graph algo-
rithms for corpus-based disambiguation of nomi-
nal senses. We have shown that the parameter op-
timization learnt over a small set of nouns signifi-
cantly improves the performance for all nouns, and
produces a system which (1) in a lexical-sample
setting (Senseval 3 dataset) is 10 points over the
Most-Frequent-Sense baseline, 1 point over a su-
pervised system using the same kind of informa-
tion (i.e. bag-of-words features), and 8 points be-
low the best supervised system, and (2) in the all-
words setting is a` la par the best supervised sys-
tem. The performance of PageRank is statistically
the same as that of HyperLex, with the advantage
of PageRank of using less parameters.
In order to compete on the same test set as su-
pervised systems, we do use hand-tagged data, but
only to do the mapping from the induced senses
into the gold standard senses. In fact, we believe
that using our WSD system as a purely unsuper-
vised system (i.e. returning just hubs), the per-
fomance would be higher, as we would avoid the
information loss in the mapping. We would like
to test this on Information Retrieval, perhaps on a
setting similar to that of (Schu?tze, 1998), which
would allow for an indirect evaluation of the qual-
ity and a comparison with supervised WSD system
on the same grounds.
We have also shown that the optimization ac-
cording to purity and entropy values (which does
not need the supervised mapping step) yields very
good parameters, comparable to those obtained in
the supervised optimization strategy. This indi-
cates that we are able to optimize the algorithm
in a completely unsupervised fashion for a small
number of words, and then carry over to tag new
text with the induced senses.
Regarding efficiency, our implementation of
HyperLex is extremely fast. Trying the 6700 com-
binations of parameters takes 5 hours in a 2 AMD
Opteron processors at 2GHz and 3Gb RAM. A
single run (building the MST, mapping and tag-
ging the test sentences) takes only 16 sec. For this
reason, even if an on-line version would be in prin-
ciple desirable, we think that this batch version is
readily usable as a standalone word sense disam-
biguation system.
Both graph-based methods and vector-based
clustering methods rely on local information, typ-
ically obtained by the occurrences of neighbor
words in context. The advantage of graph-
based techniques over over vector-based cluster-
ing might come from the fact that the former are
able to measure the relative importance of a vertex
in the whole graph, and thus combine both local
and global cooccurrence information.
For the future, we would like to look more
closely the micro-senses induced by HyperLex,
and see if we can group them into coarser clus-
ters. We would also like to integrate different
kinds of information, specially the local or syn-
tactic features so successfully used by supervised
systems, but also more heterogeneous information
from knowledge bases.
Graph models have been very successful in
some settings (e.g. the PageRank algorithm of
Google), and have been rediscovered recently
for natural language tasks like knowledge-based
WSD, textual entailment, summarization and de-
pendency parsing. Now that we have set a ro-
bust optimization and evaluation framework we
would like to test other such algorithms (e.g.
HITS (Kleinberg, 1999)) in the same conditions.
Acknowledgements
Oier Lopez de Lacalle enjoys a PhD grant from the
Basque Government. We thank the comments of
the three anonymous reviewers.
References
E. Agirre, O. Lopez de Lacalle, and D. Martinez. 2005.
Exploring feature spaces with svd and unlabeled
data for word sense disambiguation. In Proc. of
RANLP.
E. Agirre, O. Lopez de Lacalle, D. Martinez, and
A. Soroa. 2006. Evaluating and optimizing the pa-
rameters of an unsupervised graph-based wsd algo-
rithm. In Proc. of the NAACL Texgraphs workshop.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1-7).
D. A. Cruse, 2000. Polysemy: Theoretical and Compu-
tational Approaches, chapter Aspects of the Micro-
structure of Word Meanings, pages 31?51. OUP.
G Erkan and D. R. Radev. 2004. Lexrank: Graph-
based centrality as salience in text summarization.
Journal of Artificial Intelligence Research (JAIR).
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
592
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632.
R. Mihalcea and P Tarau. 2004. Textrank: Bringing
order into texts. In Proc. of EMNLP2004.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The senseval-3 english lexical sample task. In
R. Mihalcea and P. Edmonds, editors, Senseval-3
proccedings, pages 25?28. ACL, July.
R. Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proc. of
EMNLP2005.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker.
1993. A semantic concordance. In Proc. of the
ARPA HLT workshop.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: a knowledge-based approach
to word sense disambiguation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
7(27):1063?1074, June.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word in-
dependent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proc. of CoNLL-2004, pages 41?
48.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
J. Ve?ronis. 2004. Hyperlex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252.
Y Zhao and G Karypis. 2005. Hierarchical clustering
algorithms for document datasets. Data Mining and
Knowledge Discovery, 10(2):141?168.
593
Workshop on TextGraphs, at HLT-NAACL 2006, pages 89?96,
New York City, June 2006. c?2006 Association for Computational Linguistics
Evaluating and optimizing the parameters
of an unsupervised graph-based WSD algorithm
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle and Aitor Soroa
IXA NLP Group
University of Basque Country
Donostia, Basque Contry
a.soroa@ehu.es
Abstract
Ve?ronis (2004) has recently proposed
an innovative unsupervised algorithm for
word sense disambiguation based on
small-world graphs called HyperLex. This
paper explores two sides of the algorithm.
First, we extend Ve?ronis? work by opti-
mizing the free parameters (on a set of
words which is different to the target set).
Second, given that the empirical compar-
ison among unsupervised systems (and
with respect to supervised systems) is sel-
dom made, we used hand-tagged corpora
to map the induced senses to a standard
lexicon (WordNet) and a publicly avail-
able gold standard (Senseval 3 English
Lexical Sample). Our results for nouns
show that thanks to the optimization of
parameters and the mapping method, Hy-
perLex obtains results close to supervised
systems using the same kind of bag-of-
words features. Given the information
loss inherent in any mapping step and the
fact that the parameters were tuned for an-
other set of words, these are very interest-
ing results.
1 Introduction
Word sense disambiguation (WSD) is a key en-
abling technology. Supervised WSD techniques are
the best performing in public evaluations, but need
large amounts of hand-tagging data. Existing hand-
annotated corpora like SemCor (Miller et al, 1993),
which is annotated with WordNet senses (Fellbaum,
1998) allow for a small improvement over the simple
most frequent sense heuristic, as attested in the all-
words track of the last Senseval competition (Sny-
der and Palmer, 2004). In theory, larger amounts
of training data (SemCor has approx. 500M words)
would improve the performance of supervised WSD,
but no current project exists to provide such an ex-
pensive resource.
Supervised WSD is based on the ?fixed-list of
senses? paradigm, where the senses for a target word
are a closed list coming from a dictionary or lex-
icon. Lexicographers and semanticists have long
warned about the problems of such an approach,
where senses are listed separately as discrete enti-
ties, and have argued in favor of more complex rep-
resentations, where, for instance, senses are dense
regions in a continuum (Cruse, 2000).
Unsupervised WSD has followed this line of
thinking, and tries to induce word senses directly
from the corpus. Typical unsupervised WSD sys-
tems involve clustering techniques, which group to-
gether similar examples. Given a set of induced
clusters (which represent word uses or senses1),
each new occurrence of the target word will be com-
pared to the clusters and the most similar cluster will
be selected as its sense.
Most of the unsupervised WSD work has been
based on the vector space model (Schu?tze, 1998;
Pantel and Lin, 2002; Purandare and Pedersen,
2004), where each example is represented by a vec-
tor of features (e.g. the words occurring in the
context). Recently, Ve?ronis (Ve?ronis, 2004) has
1Unsupervised WSD approaches prefer the term ?word uses?
to ?word senses?. In this paper we use them interchangeably to
refer to both the induced clusters, and to the word senses from
some reference lexicon.
89
proposed HyperLex, an application of graph mod-
els to WSD based on the small-world properties
of cooccurrence graphs. Hand inspection of the
clusters (called hubs in this setting) by the author
was very positive, with hubs capturing the main
senses of the words. Besides, hand inspection of the
disambiguated occurrences yielded precisions over
95% (compared to a most frequent baseline of 73%)
which is an outstanding figure for WSD systems.
We noticed that HyperLex had some free param-
eters and had not been evaluated against a public
gold standard. Besides, we were struck by the few
works where supervised and unsupervised systems
were evaluated on the same test data. In this pa-
per we use an automatic method to map the induced
senses to WordNet using hand-tagged corpora, en-
abling the automatic evaluation against available
gold standards (Senseval 3 English Lexical Sam-
ple S3LS (Mihalcea et al, 2004)) and the automatic
optimization of the free parameters of the method.
The use of hand-tagged corpora for tagging makes
this algorithm a mixture of unsupervised and super-
vised: the method to induce senses in completely
unsupervised, but the mapping is supervised (albeit
very straightforward).
This paper is structured as follows. We first
present the graph-based algorithm as proposed by
Ve?ronis, reviewing briefly the features of small-
world graphs. Section 3 presents our framework for
mapping and evaluating the induced hubs. Section 4
introduces parameter optimization. Section 5 shows
the experiment setting and results. Section 6 ana-
lyzes the results and presents related work. Finally,
we draw the conclusions and advance future work.
2 HyperLex
Before presenting the HyperLex algorithm itself, we
briefly introduce small-world graphs.
2.1 Small world graphs
The small-world nature of a graph can be explained
in terms of its clustering coefficient and characteris-
tic path length. The clustering coefficient of a graph
shows the extent to which nodes tend to form con-
nected groups that have many edges connecting each
other in the group, and few edges leading out of
the group. On the other side, the characteristic path
length represents ?closeness? in a graph. See (Watts
and Strogatz, 1998) for further details on these char-
acteristics.
Randomly built graphs exhibit low clustering co-
efficients and are believed to represent something
very close to the minimal possible average path
length, at least in expectation. Perfectly ordered
graphs, on the other side, show high clustering coef-
ficients but also high average path length. According
to Watts and Strogatz (1998), small-world graphs lie
between these two extremes: they exhibit high clus-
tering coefficients, but short average path lengths.
Barabasi and Albert (1999) use the term ?scale-
free? to graphs whose degree probability follow a
power-law2. Specifically, scale free graphs follow
the property that the probability P (k) that a vertex
in the graph interacts with k other vertices decays as
a power-law, following P (k) ? k??. It turns out
that in this kind of graphs there exist nodes centrally
located and highly connected, called hubs.
2.2 The HyperLex algorithm for WSD
The HyperLex algorithm builds a cooccurrence
graph for all pairs of words cooccurring in the con-
text of the target word. Ve?ronis shows that this kind
of graph fulfills the properties of small world graphs,
and thus possess highly connected components in
the graph. The centers or prototypes of these com-
ponents, called hubs, eventually identify the main
word uses (senses) of the target word.
We will briefly introduce the algorithm here,
check (Ve?ronis, 2004) for further details. For each
word to be disambiguated, a text corpus is collected,
consisting of the paragraphs where the word occurs.
From this corpus, a cooccurrence graph for the tar-
get word is built. Nodes in the graph correspond to
the words3 in the text (except the target word itself).
Two words appearing in the same paragraph are said
to cooccur, and are connected with edges. Each edge
is assigned with a weight which measures the rela-
tive frequency of the two words cooccurring. Specif-
ically, let wij be the weight of the edge4 connecting
2Although scale-free graphs are not necessarily small
worlds, a lot of real world networks are both scale-free and
small worlds.
3Following Ve?ronis, we only work on nouns for the time
being.
4Note that the cooccurrence graph is undirected, i.e. wij =
wji
90
nodes i and j, then
wij = 1? max[P (i | j), P (j | i)]
P (i | j) =
freqij
freqj
and P (j | i) =
freqij
freqi
The weight of an edge measures how tightly con-
nected the two words are. Words which always oc-
cur together receive a weight of 0. Words rarely
cooccurring receive weights close to 1.
Once the cooccurrence graph is built, a simple it-
erative algorithm is executed to obtain its hubs. At
each step, the algorithm finds the vertex with high-
est relative frequency5 in the graph, and, if it meets
some criteria, it is selected as a hub. These criteria
are determined by a set of heuristic parameters, that
will be explained later in Section 4. After a vertex is
selected to be a hub, its neighbors are no longer eli-
gible as hub candidates. At any time, if the next ver-
tex candidate has a relative frequency below a cer-
tain threshold, the algorithm stops.
Once the hubs are selected, each of them is linked
to the target word with edges weighting 0, and the
Minimum Spanning Tree (MST) of the whole graph
is calculated and stored.
The MST is then used to perform word sense dis-
ambiguation, in the following way. For every in-
stance of the target word, the words surrounding it
are examined and confronted with the MST. By con-
struction of the MST, words in it are placed under
exactly one hub. Each word in the context receives
a set of scores s, with one score per hub, where all
scores are 0 except the one corresponding to the hub
where it is placed. If the scores are organized in a
score vector, all values are 0, except, say, the i-th
component, which receives a score d(hi, v), which
is the distance between the hub hi and the node rep-
resenting the word v. Thus, d(hi, v) assigns a score
of 1 to hubs and the score decreases as the nodes
move away from the hub in the tree.
For a given occurrence of the target word, the
score vectors of all the words in the context are
added, and the hub that receives the maximum score
is chosen.
5In cooccurrence graphs, the relative frequency of a vertex
and its degree are linearly related, and it is therefore possible to
avoid the costly computation of the degree.
Base
corpus
hyperLex_wsd hyperLex_wsd
hyperLex
Evaluator
Tagged
corpus
Test
corpus
Mapping
corpus
MST
matrix
Mapping
Figure 1: Design for the automatic mapping and evaluation
of HyperLex algorithm against a gold standard (test corpora).
3 Evaluating unsupervised WSD systems
All unsupervised WSD algorithms need some addi-
tion in order to be evaluated. One alternative, as in
(Ve?ronis, 2004), is to manually decide the correct-
ness of the hubs assigned to each occurrence of the
words. This approach has two main disadvantages.
First, it is expensive to manually verify each occur-
rence of the word, and different runs of the algo-
rithm need to be evaluated in turn. Second, it is not
an easy task to manually decide if an occurrence of
a word effectively corresponds with the use of the
word the assigned hub refers to, especially consid-
ering that the person is given a short list of words
linked to the hub. We also think that instead of judg-
ing whether the hub returned by the algorithm is cor-
rect, the person should have independently tagged
the occurrence with hubs, which should have been
then compared to the hub returned by the system.
A second alternative is to evaluate the system ac-
cording to some performance in an application, e.g.
information retrieval (Schu?tze, 1998). This is a very
attractive idea, but requires expensive system devel-
opment and it is sometimes difficult to separate the
reasons for the good (or bad) performance.
A third alternative would be to devise a method
to map the hubs (clusters) returned by the system
to the senses in a lexicon. Pantel and Lin (2002)
automatically map the senses to WordNet, and then
measure the quality of the mapping. More recently,
the mapping has been used to test the system on
publicly available benchmarks (Purandare and Ped-
91
Default p180 p1800 p6700
value Range Best Range Best Range Best
p1 5 2-3 2 1-3 2 1-3 1
p2 10 3-4 3 2-4 3 2-4 3
p3 0.9 0.7-0.9 0.7 0.5-0.7 0.5 0.3-0.7 0.4
p4 4 4 4 4 4 4 4
p5 6 6-7 6 3-7 3 1-7 1
p6 0.8 0.5-0.8 0.6 0.4-0.8 0.7 0.6-0.95 0.95
p7 0.001 0.0005-0.001 0.0009 0.0005-0.001 0.0009 0.0009-0.003 0.001
Table 1: Parameters of the HyperLex algorithm
ersen, 2004; Niu et al, 2005). See Section 6 for
more details on these systems.
Yet another possibility is to evaluate the induced
senses against a gold standard as a clustering task.
Induced senses are clusters, gold standard senses are
classes, and measures from the clustering literature
like entropy or purity can be used. As we wanted to
focus on the comparison against a standard data-set,
we decided to leave aside this otherwise interesting
option.
In this section we present a framework for au-
tomatically evaluating unsupervised WSD systems
against publicly available hand-tagged corpora. The
framework uses three data sets, called Base corpus,
Mapping corpus and Test corpus:
? The Base Corpus: a collection of examples of
the target word. The corpus is not annotated.
? The Mapping Corpus: a collection of examples
of the target word, where each corpus has been
manually annotated with its sense.
? The Test Corpus: a separate collection, also an-
notated with senses.
The evaluation framework is depicted in Figure 1.
The first step is to execute the HyperLex algorithm
over the Base corpus in order to obtain the hubs of
a target word, and the generated MST is stored. As
stated before, the Base Corpus is not tagged, so the
building of the MST is completely unsupervised.
In a second step (left part in Figure 1), we assign a
hub score vector to each of the occurrences of target
word in the Mapping corpus, using the MST calcu-
lated in the previous step (following the WSD al-
gorithm in Section 2.2). Using the hand-annotated
sense information, we can compute a mapping ma-
trix M that relates hubs and senses in the following
way. Suppose there are m hubs and n senses for the
target word. Then, M = {mij} 1 ? i ? m, 1 ?
j ? n, and each mij = P (sj |hi), that is, mij is the
probability of a word having sense j given that it has
been assigned hub i. This probability can be com-
puted counting the times an occurrence with sense
sj has been assigned hub hi.
This mapping matrix will be used to transform
any hub score vector h? = (h1, . . . , hm) returned
by the WSD algorithm into a sense score vector
s? = (s1, . . . , sn). It suffices to multiply the score
vector by M , i.e., s? = h?M .
In the last step (right part in Figure 1), we apply
the WSD algorithm over the Test corpus, using again
the MST generated in the first step, and returning a
hub score vector for each occurrence of the target
word in the test corpus. We then run the Evaluator,
which uses the M mapping matrix in order to con-
vert the hub score vector into a sense score vector.
The Evaluator then compares the sense with high-
est weight in the sense score vector to the sense that
was manually assigned, and outputs the precision
figures.
Preliminary experiments showed that, similar to
other unsupervised systems, HyperLex performs
better if it sees the test examples when building the
graph. We therefore decided to include a copy of the
training and test corpora in the base corpus (discard-
ing all hand-tagged sense information, of course).
Given the high efficiency of the algorithm this poses
no practical problem (see efficiency figures in Sec-
tion 6).
4 Tuning the parameters
As stated before, the behavior of the HyperLex algo-
rithm is influenced by a set of heuristic parameters,
that affect the way the cooccurrence graph is built,
the number of induced hubs, and the way they are
extracted from the graph. There are 7 parameters in
total:
p1 Minimum frequency of edges (occurrences)
p2 Minimum frequency of vertices (words)
p3 Edges with weights above this value are removed
p4 Context containing fewer words are not processed
92
word train test MFS default p180 p1800 p6700
argument 221 111 51.4 51.4 51.4 51.4 51.4
arm 266 133 82.0 82.0 80.5 82.0 82.7
atmosphere 161 81 66.7 67.9 70.4 70.4 67.9
audience 200 100 67.0 69.0 71.0 74.0 77.0
bank 262 132 67.4 69.7 75.0 76.5 75.0
degree 256 128 60.9 60.9 60.9 62.5 63.3
difference 226 114 40.4 40.4 41.2 46.5 49.1
difficulty 46 23 17.4 30.4 30.4 39.1 26.1
disc 200 100 38.0 66.0 75.0 70.0 76.0
image 146 74 36.5 63.5 62.2 67.6 64.9
interest 185 93 41.9 49.5 41.9 47.3 51.6
judgment 62 32 28.1 28.1 28.1 53.1 50.0
organization 112 56 73.2 73.2 73.2 71.4 73.2
paper 232 117 25.6 42.7 39.3 47.9 53.8
party 230 116 62.1 67.2 64.7 65.5 67.2
performance 172 87 32.2 44.8 46.0 54.0 59.8
plan 166 84 82.1 81.0 79.8 81.0 83.3
shelter 196 98 44.9 45.9 49.0 48.0 54.1
sort 190 96 65.6 64.6 64.6 65.6 64.6
source 64 32 65.6 59.4 56.2 62.5 62.5
Average: 54.5 59.9 60.3 63.0 64.6
(Over S2LS) 51.9 56.2 57.5 58.7 60.0
Table 2: Precision figures for nouns over the test corpus (S3LS). The second and third columns show the number of occurrences
in the train and test splits. The MFS column corresponds to the most frequent sense. The rest of columns correspond to different
parameter settings: default for the default setting, p180 for the best combination over 180, etc.. The last rows show the micro-
average over the S3LS run, and we also add the results on the S2LS dataset (different sets of nouns) to confirm that the same trends
hold in both datasets.
p5 Minimum number of adjacent vertices a hub must have
p6 Max. mean weight of the adjacent vertices of a hub
p7 Minimum frequency of hubs
Table 1 lists the parameters of the HyperLex al-
gorithm, and the default values proposed for them in
the original work (second column).
Given that we have devised a method to efficiently
evaluate the performance of HyperLex, we are able
to tune the parameters against the gold standard. We
first set a range for each of the parameters, and eval-
uated the algorithm for each combination of the pa-
rameters on a collection of examples of different
words (Senseval 2 English lexical-sample, S2LS).
This ensures that the chosen parameter set is valid
for any noun, and is not overfitted to a small set of
nouns.6 The set of parameters that obtained the best
results in the S2LS run is then selected to be run
against the S3LS dataset.
We first devised ranges for parameters amounting
to 180 possible combinations (p180 column in Ta-
ble 2), and then extended the ranges to amount to
1800 and 6700 combinations (columns p1800 and
p6700).
6In fact, previous experiments showed that optimizing the
parameters for each word did not yield better results.
5 Experiment setting and results
To evaluate the HyperLex algorithm in a standard
benchmark, we applied it to the 20 nouns in S3LS.
We use the standard training-test split. Following
the design in Section 3, we used both the training
and test sets as the Base Corpus (ignoring the sense
tags, of course). The Mapping Corpus comprised
the training split only, and the Test corpus the test
split only. The parameter tuning was done in a simi-
lar fashion, but on the S2LS dataset.
In Table 2 we can see the number of examples
of each word in the different corpus and the results
of the algorithm. We indicate only precision, as the
coverage is 100% in all cases. The left column,
named MFS, shows the precision when always as-
signing the most frequent sense (relative to the train
split). This is the baseline of our algorithm as our
algorithm does see the tags in the mapping step (see
Section 6 for further comments on this issue).
The default column shows the results for the Hy-
perLex algorithm with the default parameters as set
by Ve?ronis, except for the minimum frequency of
the vertices (p2 in Table 1), which according to some
preliminary experiments we set to 3. As we can see,
the algorithm with the default settings outperforms
93
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 0.5  0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1
Pr
ec
is
io
n
Similarity
Parameter space
Best fitting line
Figure 2: Dispersion plot of the parameter space for 6700
combinations. The horizontal axis shows the similarity of a pa-
rameter set w.r.t. the best parameter set using the cosine. The
vertical axis shows the precision in S2LS. The best fitting line
is also depicted.
the MFS baseline by 5.4 points average, and in al-
most all words (except plan, sort and source).
The results for the best of 180 combinations of the
parameters improve the default setting (0.4 overall),
Extending the parameter space to 1800 and 6700 im-
proves the precision up to 63.0 and 64.6, 10.1 over
the MFS (MFS only outperforms HyperLex in the
best setting for two words). The same trend can be
seen on the S2LS dataset, where the gain was more
modest (note that the parameters were optimized for
S2LS).
6 Discussion and related work
We first comment the results, doing some analysis,
and then compare our results to those of Ve?ronis. Fi-
nally we overview some relevant work and review
the results of unsupervised systems on the S3LS
benchmark.
6.1 Comments on the results
The results show clearly that our exploration of the
parameter space was successful, with the widest pa-
rameter space showing the best results.
In order to analyze whether the search in the pa-
rameter space was making any sense, we drew a dis-
persion plot (see Figure 2). In the top right-hand cor-
ner we have the point corresponding to the best per-
forming parameter set. If the parameters were not
conditioning the good results, then we would have
expected a random cloud of points. On the contrary,
we can see that there is a clear tendency for those
default p180 p1800 p6700
hubs defined 9.2 ?3.8 15.3 ?5.7 38.6 ?11.8 77.7?18.7
used 8.4 ?3.5 14.4 ?5.3 30.4 ?9.3 45.2?13.3
senses defined 5.4 ?1.5 5.4 ?1.5 5.4 ?1.5 5.4 ?1.5
used 2.6 ?1.2 2.5 ?1 3.1 ?1.1 3.2?1.2
senses in test 5.1 ?1.3 - - -
Table 3: Average number of hubs and senses (along with the
standard deviation) for three parameter settings. Defined means
the number of hubs induced, and used means the ones actually
returned by HyperLex when disambiguating the test set. The
same applies for senses, that is, defined means total number of
senses (equal for all columns), and used means the senses that
were actually used by HyperLex in the test set. The last row
shows the actual number of senses used by the hand-annotators
in the test set.
parameter sets most similar to the best one to obtain
better results, and in fact the best fitting line shows a
clearly ascending slope.
Regarding efficiency, our implementation of Hy-
perLex is extremely fast. Doing the 1800 combina-
tions takes 2 hours in a 2 AMD Opteron processors
at 2GHz and 3Gb RAM. A single run (building the
MST, mapping and tagging the test sentences) takes
only 16 sec. For this reason, even if an on-line ver-
sion would be in principle desirable, we think that
this batch version is readily usable.
6.2 Comparison to (Ve?ronis, 2004)
Compared to Ve?ronis we are inducing larger num-
bers of hubs (with different parameters), using less
examples to build the graphs and obtaining more
modest results (far from the 90?s). Regarding the lat-
ter, our results are in the range of other S3LS WSD
systems (see below), and the discrepancy can be ex-
plained by the way Ve?ronis performed his evaluation
(see Section 3).
Table 3 shows the average number of hubs for
the four parameter settings. The average number
of hubs for the default setting is larger than that of
Ve?ronis (which ranges between 4 and 9 per word),
but quite close to the average number of senses. The
exploration of the parameter space prefers parame-
ter settings with even larger number of hubs, and the
figures shows that most of them are actually used
for disambiguation. The table also shows that, after
the mapping, less than half of the senses are actu-
ally used, which seems to indicate that the mapping
tends to favor the most frequent senses.
Regarding the actual values of the parameters
used (c.f. Table 1), we had to reduce the value
94
of some parameters (e.g. the minimum frequency
of vertices) due to the smaller number of of exam-
ples (Ve?ronis used from 1900 to 8700 examples per
word). In theory, we could explore larger parame-
ter spaces, but Table 1 shoes that the best setting for
the 6700 combinations has no parameter in a range
boundary (except p5, which cannot be further re-
duced).
All in all, the best results are attained with smaller
and more numerous hubs, a kind of micro-senses.
A possible explanation for this discrepancy with
Ve?ronis could be that he was inspecting by hand
the hubs that he got, and perhaps was biased by the
fact that he wanted the hubs to look more like stan-
dard senses. At first we were uncomfortable with
this behavior, so we checked whether HyperLex was
degenerating into a trivial solution. We simulated
a clustering algorithm returning one hub per exam-
ple, and its precision was 40.1, well below the MFS
baseline. We also realized that our results are in
accordance with some theories of word meaning,
e.g. the ?indefinitely large set of prototypes-within-
prototypes? envisioned in (Cruse, 2000). We now
think that the idea of having many micro-senses is
very attractive for further exploration, especially if
we are able to organize them into coarser hubs.
6.3 Comparison to related work
Table 4 shows the performance of different systems
on the nouns of the S3LS benchmark. When not re-
ported separately, we obtained the results for nouns
running the official scorer program on the filtered
results, as available in the S3LS web page. The sec-
ond column shows the type of system (supervised,
unsupervised).
We include three supervised systems, the winner
of S3LS (Mihalcea et al, 2004), an in-house system
(kNN-all, CITATION OMITTED) which uses opti-
mized kNN, and the same in-house system restricted
to bag-of-words features only (kNN-bow), i.e. dis-
carding other local features like bigrams or trigrams
(which is what most unsupervised systems do). The
table shows that we are one point from the bag-of-
words classifier kNN-bow, which is an impressive
result if we take into account the information loss of
the mapping step and that we tuned our parameters
on a different set of words. The full kNN system is
state-of-the-art, only 4 points below the S3LS win-
System Type Prec. Cov.
S3LS-best Sup. 74.9 0.99
kNN-all Sup. 70.3 1.0
kNN-bow Sup. 65.7 1.0
HyperLex Unsup(S3LS) 64.6 1.0
Cymfony Unsup(10%-S3LS) 57.9 1.0
Prob0 Unsup. (MFS-S3) 55.0 0.98
MFS - 51.5 1.0
Ciaosenso Unsup (MFS-Sc) 53.95 0.90
clr04 Unsup (MFS-Sc) 48.86 1.0
duluth-senserelate Unsup 47.48 1.0
(Purandare and
Pedersen, 2004)
Unsup (S2LS) - -
Table 4: Comparison of HyperLex and MFS baseline to S3LS
systems for nouns. The last system was evaluated on S2LS.
ner.
Table 4 also shows several unsupervised systems,
all of which except Cymfony and (Purandare and
Pedersen, 2004) participated in S3LS (check (Mi-
halcea et al, 2004) for further details on the sys-
tems). We classify them according to the amount of
?supervision? they have: some have have access to
most-frequent information (MFS-S3 if counted over
S3LS, MFS-Sc if counted over SemCor), some use
10% of the S3LS training part for mapping (10%-
S3LS), and some use the full amount of S3LS train-
ing for mapping (S3LS). Only one system (Duluth)
did not use in any way hand-tagged corpora.
Given the different typology of unsupervised sys-
tems, it?s unfair to draw definitive conclusions from
a raw comparison of results. The system coming
closer to ours is that described in (Niu et al, 2005).
They use hand tagged corpora which does not need
to include the target word to tune the parameters of
a rather complex clustering method which does use
local information (an exception to the rule of unsu-
pervised systems). They do use the S3LS training
corpus for mapping. For every sense the target word,
three of its contexts in the train corpus are gathered
(around 10% of the training data) and tagged. Each
cluster is then related with its most frequent sense.
Only one cluster may be related to a specific sense,
so if two or more clusters map to the same sense,
only the largest of them is retained. The mapping
method is similar to ours, but we use all the avail-
able training data and allow for different hubs to be
assigned to the same sense.
Another system similar to ours is (Purandare and
Pedersen, 2004), which unfortunately was evaluated
on Senseval 2 data. The authors use first and second
95
order bag-of-word context features to represent each
instance of the corpus. They apply several clustering
algorithms based on the vector space model, limiting
the number of clusters to 7. They also use all avail-
able training data for mapping, but given their small
number of clusters they opt for a one-to-one map-
ping which maximizes the assignment and discards
the less frequent clusters. They also discard some
difficult cases, like senses and words with low fre-
quencies (10% of total occurrences and 90, respec-
tively). The different test set and mapping system
make the comparison difficult, but the fact that the
best of their combinations beats MFS by 1 point on
average (47.6% vs. 46.4%) for the selected nouns
and senses make us think that our results are more
robust (nearly 10% over MFS).
7 Conclusions and further work
This paper has explored two sides of HyperLex: the
optimization of the free parameters, and the empir-
ical comparison on a standard benchmark against
other WSD systems. We use hand-tagged corpora
to map the induced senses to WordNet senses.
Regarding the optimization of parameters, we
used a another testbed (S2LS) comprising different
words to select the best parameter. We consistently
improve the results of the parameters by Ve?ronis,
which is not perhaps so surprising, but the method
allows to fine-tune the parameters automatically to a
given corpus given a small test set.
Comparing unsupervised systems against super-
vised systems is seldom done. Our results indicate
that HyperLex with the supervised mapping is on
par with a state-of-the-art system which uses bag-
of-words features only. Given the information loss
inherent to any mapping, this is an impressive re-
sult. The comparison to other unsupervised systems
is difficult, as each one uses a different mapping
strategy and a different amount of supervision.
For the future, we would like to look more closely
the micro-senses induced by HyperLex, and see if
we can group them into coarser clusters. We also
plan to apply the parameters to the Senseval 3 all-
words task, which seems well fit for HyperLex: the
best supervised system only outperforms MFS by
a few points in this setting, and the training cor-
pora used (Semcor) is not related to the test corpora
(mainly Wall Street Journal texts).
Graph models have been very successful in some
settings (e.g. the PageRank algorithm of Google),
and have been rediscovered recently for natural lan-
guage tasks like knowledge-based WSD, textual en-
tailment, summarization and dependency parsing.
We would like to test other such algorithms in the
same conditions, and explore their potential to inte-
grate different kinds of information, especially the
local or syntactic features so successfully used by
supervised systems, but also more heterogeneous in-
formation from knowledge bases.
References
A. L. Barabasi and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 286(5439):509?512,
October.
D. A. Cruse, 2000. Polysemy: Theoretical and Com-
putational Approaches, chapter Aspects of the Micro-
structure of Word Meanings. OUP.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
senseval-3 english lexical sample task. In R. Mihal-
cea and P. Edmonds, editors, Senseval-3 proceedings,
pages 25?28. ACL, July.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A semantic concordance. In Proc. of the ARPA HLT
workshop.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word
independent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Proc. of CoNLL-2004.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
J. Ve?ronis. 2004. HyperLex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252.
D. J. Watts and S. H. Strogatz. 1998. Collec-
tive dynamics of ?small-world? networks. Nature,
393(6684):440?442, June.
96
Word Sense Disambiguation Using Automatically Translated
Sense Examples
Xinglong Wang
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh
EH8 9LW, UK
xwang@inf.ed.ac.uk
David Martinez
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
davidm@dcs.shef.ac.uk
Abstract
We present an unsupervised approach to
Word Sense Disambiguation (WSD). We
automatically acquire English sense exam-
ples using an English-Chinese bilingual
dictionary, Chinese monolingual corpora
and Chinese-English machine translation
software. We then train machine learn-
ing classifiers on these sense examples
and test them on two gold standard En-
glish WSD datasets, one for binary and
the other for fine-grained sense identifica-
tion. On binary disambiguation, perfor-
mance of our unsupervised system has ap-
proached that of the state-of-the-art super-
vised ones. On multi-way disambiguation,
it has achieved a very good result that is
competitive to other state-of-the-art unsu-
pervised systems. Given the fact that our
approach does not rely on manually anno-
tated resources, such as sense-tagged data
or parallel corpora, the results are very
promising.
1 Introduction
Results from recent Senseval workshops have
shown that supervised Word Sense Disambigua-
tion (WSD) systems tend to outperform their unsu-
pervised counterparts. However, supervised sys-
tems rely on large amounts of accurately sense-
annotated data to yield good results and such re-
sources are very costly to produce. It is difficult
for supervised WSD systems to perform well and
reliably on words that do not have enough sense-
tagged training data. This is the so-called knowl-
edge acquisition bottleneck.
To overcome this bottleneck, unsupervised
WSD approaches have been proposed. Among
them, systems under the multilingual paradigm
have shown great promise (Gale et al, 1992; Da-
gan and Itai, 1994; Diab and Resnik, 2002; Ng et
al., 2003; Li and Li, 2004; Chan and Ng, 2005;
Wang and Carroll, 2005). The underlying hy-
pothesis is that mappings between word forms
and meanings can be different from language to
language. Much work have been done on ex-
tracting sense examples from parallel corpora for
WSD. For example, Ng et al (2003) proposed
to train a classifier on sense examples acquired
from word-aligned English-Chinese parallel cor-
pora. They grouped senses that share the same
Chinese translation, and then the occurrences of
the word on the English side of the parallel corpora
were considered to have been disambiguated and
?sense tagged? by the appropriate Chinese trans-
lations. Their system was evaluated on the nouns
in Senseval-2 English lexical sample dataset, with
promising results. Their follow-up work (Chan
and Ng, 2005) has successfully scaled up the ap-
proach and achieved very good performance on
the Senseval-2 English all-word task.
Despite the promising results, there are prob-
lems with relying on parallel corpora. For exam-
ple, there is a lack of matching occurrences for
some Chinese translations to English senses. Thus
gathering training examples for them might be dif-
ficult, as reported in (Chan and Ng, 2005). Also,
parallel corpora themselves are rare resources and
not available for many language pairs.
Some researchers seek approaches using mono-
lingual resources in a second language and then
try to map the two languages using bilingual dic-
tionaries. For example, Dagan and Itai (1994) car-
ried out WSD experiments using monolingual cor-
pora, a bilingual lexicon and a parser for the source
language. One problem of this method is that
45
for many languages, accurate parsers do not exist.
Wang and Carroll (2005) proposed to use mono-
lingual corpora and bilingual dictionaries to auto-
matically acquire sense examples. Their system
was unsupervised and achieved very promising
results on the Senseval-2 lexical sample dataset.
Their system also has better portability, i.e., it runs
on any language pair as long as a bilingual dictio-
nary is available. However, sense examples ac-
quired using the dictionary-based word-by-word
translation can only provide ?bag-of-words? fea-
tures. Many other features useful for machine
learning (ML) algorithms, such as the ordering of
words, part-of-speech (POS), bigrams, etc., have
been lost. It could be more interesting to translate
Chinese text snippets using machine translation
(MT) software, which would provide richer con-
textual information that might be useful for WSD
learners. Although MT systems themselves are
expensive to build, once they are available, they
can be used repeatedly to automatically generate
as much data as we want. This is an advantage
over relying on other expensive resources such as
manually sense-tagged data and parallel copora,
which are limited in size and producing additional
data normally involves further costly investments.
We carried out experiments on acquiring sense
examples using both MT software and a bilingual
dictionary. When we had the two sets of sense ex-
amples ready, we trained a ML classifier on them
and then tested them on coarse-grained and fine-
grained gold standard WSD datasets, respectively.
We found that on both test datasets the classi-
fier using MT translated sense examples outper-
formed the one using those translated by a dictio-
nary, given the same amount of training examples
used on each word sense. This confirms our as-
sumption that a richer feature set, although from
a noisy data source, such as machine translated
text, might help ML algorithms. In addition, both
systems performed very well comparing to other
state-of-the-art WSD systems. As we expected,
our system is particularly good on coarse-grained
disambiguation. Being an unsupervised approach,
it achieved a performance competitive to state-of-
the-art supervised systems.
This paper is organised as follows: Section 2
revisits the process of acquiring sense examples
proposed in (Wang and Carroll, 2005) and then
describes our adapted approach. Section 3 out-
lines resources, the ML algorithm and evaluation
metrics that we used. Section 4 and Section 5 de-
tail experiments we carried out on gold standard
datasets. We also report our results and error anal-
ysis. Finally, Section 6 concludes the paper and
draws future directions.
2 Acquisition of Sense Examples
Wang and Carroll (2005) proposed an automatic
approach to acquire sense examples from large
amount of Chinese text and English-Chinese and
Chinese-English dictionaries. The acquisition pro-
cess is summarised as follows:
1. Translate an English ambiguous word   to Chinese,
using an English-Chinese lexicon. Given the assump-
tion that mappings between words and senses are dif-
ferent between English and Chinese, each sense   of
  maps to a distinct Chinese word. At the end of this
step, we have produced a set

, which consists of Chi-
nese words   	 
   
    
    , where   is the translation
corresponding to sense   of   , and  is the number of
senses that   has.
2. Query large Chinese corpora or/and a search engine us-
ing each element in

. For each   in

, we collect the
text snippets retrieved and construct a Chinese corpus.
3. Word-segment these Chinese text snippets.
4. Use an electronic Chinese-English lexicon to translate
the Chinese corpora constructed word by word to En-
glish.
This process can be completely automatic and
unsupervised. However, in order to compare
the performance against other WSD systems, one
needs to map senses in the bilingual dictionary to
those used by gold standard datasets, which are
often from WordNet (Fellbaum, 1998). This step
is inevitable unless we use senses in the bilingual
dictionary as gold standard. Fortunately, the map-
ping process only takes a very short time1, com-
paring to the effort that it would take to manually
sense annotate training examples. At the end of
the acquisition process, for each sense   of an am-
biguous word  , we have a large set of English
contexts. Note that a context is represented by a
bag of words only. We mimicked this process and
built a set of sense examples.
To obtain a richer set of features, we adapted the
above process and carried out another acquisition
experiment. When translating Chinese text snip-
pets to English in the 4th step, we used MT soft-
ware instead of a bilingual dictionary. The intu-
ition is that although machine translated text con-
tains noise, features like word ordering, POS tags
1A similar process took 15 minutes per noun as reported
in (Chan and Ng, 2005), and about an hour for 20 nouns as
reported in (Wang and Carroll, 2005).
46
English ambiguous word w
Sense 1 of w Sense 2 of w
Chinese translation of
sense 2
Chinese translation of
sense 1
English-Chinese
Lexicon
Chinese text snippet 1
Chinese text snippet 2
... ...
Search
Chinese
Corpora
Machine
Translation
Software
Chinese text snippet 1
Chinese text snippet 2
... ...
{English sense example 1
for sense 1 of w}
{English sense example 2
for sense 1 of w}
... ...
{English sense example 1
for sense 2 of w}
{English sense example 2
for sense 2 of w}
... ...
Figure 1:Adapted process of automatic acquisition of sense
examples. For simplicity, assume   has two senses.
and bigrams/trigrams may still be of some use for
ML classifiers. In this approach, the 3rd step can
be omitted, since MT software should be able to
take care of segmentation. Figure 1 illustrates our
adapted acquisition process.
As described above, we prepared two sets of
training examples for each English word sense
to disambiguate: one set was translated word-by-
word by looking up a bilingual dictionary, as pro-
posed in (Wang and Carroll, 2005), and the other
translated using MT software. In detail, we first
mapped senses of ambiguous words, as defined
in the gold-standard TWA (Mihalcea, 2003) and
Senseval-3 lexical sample (Mihalcea et al, 2004)
datasets (which we use for evaluation) onto their
corresponding Chinese translations. We did this
by looking up an English-Chinese dictionary Pow-
erWord 20022. This mapping process involved
human intervention, but it only took an annota-
tor (fluent speaker in both Chinese and English)
4 hours. Since some Chinese translations are
also ambiguous, which may affect WSD perfor-
mance, the annotator was asked to select the Chi-
nese words that are relatively unambiguous (or
ideally monosemous) in Chinese for the target
word senses, when it was possible. Sometimes
multiple senses of an English word can map to
the same Chinese word, according to the English-
Chinese dictionary. In such cases, the annotator
was advised to try to capture the subtle difference
between these English word senses and then to
2PowerWord is a commercial electronic dictio-
nary application. There is a free online version at:
http://cb.kingsoft.com.
select different Chinese translations for them, us-
ing his knowledge on the languages. Then, using
the translations as queries, we retrieved as many
text snippets as possible from the Chinese Giga-
word Corpus. For efficiency purposes, we ran-
domly chose maximumly     text snippets for
each sense, when acquiring data for nouns and
adjectives from Senseval-3 lexical sample dataset.
The length of the snippets was set to    Chinese
characters.
From here we prepared two sets of sense exam-
ples differently. For the approach of dictionary-
based translation, we segmented all text snippets,
using the application ICTCLAS3. After the seg-
mentor marked all word boundaries, the system
automatically translated the text snippets word by
word using the electronic LDC Mandarin-English
Translation Lexicon 3.0. All possible translations
of each word were included. As expected, the lex-
icon does not cover all Chinese words. We simply
discarded those Chinese words that do not have an
entry in this lexicon. We also discarded those Chi-
nese words with multiword English translations.
Finally we got a set of sense examples for each
sense. Note that a sense example produced here is
simply a bag of words without ordering.
We prepared the other set of sense examples by
translating text snippets with the MT software Sys-
tran  
 Standard, where each example contains
much richer features that potentially can be ex-
ploited by ML algorithms.
3 Experimental Settings
3.1 Training
We applied the Vector Space Model (VSM) algo-
rithm on the two different kinds of sense examples
(i.e., dictionary translated ones vs. MT software
translated ones), as it has been shown to perform
well with the features described below (Agirre and
Martinez, 2004a). In VSM, we represent each
context as a vector, where each feature has an 1
or 0 value to indicate its occurrence or absence.
For each sense in training, a centroid vector is ob-
tained, and these centroids are compared to the
vectors that represent test examples, by means of
the cosine similarity function. The closest centroid
assigns its sense to the test example.
For the sense examples translated by MT soft-
ware, we analysed the sentences using different
3See: http://mtgroup.ict.ac.cn/  zhp/ICTCLAS
47
tools and extracted relevant features. We ap-
plied stemming and POS tagging, using the fnTBL
toolkit (Ngai and Florian, 2001), as well as shal-
low parsing4. Then we extracted the following
types of topical and domain features5, which were
then fed to the VSM machine learner:
  Topical features: we extracted lemmas of the
content words in two windows around the tar-
get word: the whole context and a  4 word
window. We also obtained salient bigrams in
the context, with the methods and the soft-
ware described in (Pedersen, 2001). We in-
cluded another feature type, which match the
closest words (for each POS and in both
directions) to the target word (e.g. LEFT
NOUN ?dog? or LEFT VERB ?eat?).
  Domain features: The ?WordNet Domains?
resource was used to identify the most rel-
evant domains in the context. Following
the relevance formula presented in (Magnini
and Cavaglia?, 2000), we defined two feature
types: (1) the most relevant domain, and (2)
a list of domains above a threshold6.
For the dictionary-translated sense examples,
we simply used bags of words as features.
3.2 Evaluation
We evaluated our WSD classifier on both
coarse-grained and fine-grained datasets. For
coarse-grained WSD evaluation, we used TWA
dataset (Mihalcea, 2003), which is a binarily
sense-tagged corpus drawn from the British Na-
tional Corpus (BNC), for 6 nouns. For fine-
grained evaluation, we used Senseval-3 English
lexical sample dataset (Mihalcea et al, 2004),
which comprises 7,860 sense-tagged instances for
training and 3,944 for testing, on 57 words (nouns,
verbs and adjectives). The examples were mainly
drawn from BNC. WordNet      7 was used as
sense inventory for nouns and adjectives, and
Wordsmyth8 for verbs. We only evaluated our
WSD systems on nouns and adjectives.
4This software was kindly provided by David Yarowsky?s
group at Johns Hopkins University.
5Preliminary experiments using local features (bigrams
and trigrams) showed low performance, which was expected
because of noise in the automatically acquired data.
6This software was kindly provided by Gerard Escudero?s
group at Universitat Politecnica de Catalunya. The threshold
was set in previous work.
7http://wordnet.princeton.edu
8http://www.wordsmyth.net
We also used the SemCor corpus (Miller et al,
1993) for tuning our relative-threshold heuristic. It
contains a number of texts, mainly from the Brown
Corpus, comprising about 200,000 words, where
all content words have been manually tagged with
senses from WordNet.
Throughout the paper we will use the concepts
of precision and recall to measure the performance
of WSD systems, where precision refers to the ra-
tio of correct answers to the total number of an-
swers given by the system, and recall indicates the
ratio of correct answers to the total number of in-
stances. Our ML systems attempt every instance
and always give a unique answer, and hence preci-
sion equals to recall. When comparing with other
systems that participated in Senseval-3 in Table 7,
both recall and precision are shown. When POS
and overall averages are given, they are calculated
by micro-averaging the number of examples per
word.
4 Experiments on TWA dataset
First we trained a VSM classifier on the sense
examples translated with the Systran MT soft-
ware (we use notion ?MT-based approach? to re-
fer to this process), and then tested it on the TWA
test dataset. We tried two combinations of fea-
tures: one only used topical features and the other
used the whole feature set (i.e., topical and do-
main features). Table 1 summarises the sizes of
the training/test data, the Most Frequent Sense
(MFS) baseline and performances when apply-
ing the two different feature combinations. We
can see that best results were obtained when us-
ing all the features. It also shows that both our
systems achieved a significant improvement over
the MFS baseline. Therefore, in the subsequent
WSD experiments following the MT-based ap-
proach, we decided to use the entire feature set.
To compare the machine-translated sense exam-
ples with the ones translated word-by-word, we
then trained the same VSM classifier on the ex-
amples translated with a bilingual dictionary (we
use notion ?dictionary-based approach? to refer
to this process) and evaluated it on the same test
dataset. Table 2 shows results of the dictionary-
based approach and the MT-based approach. For
comparison, we include results from another sys-
tem (Mihalcea, 2003), which uses monosemous
relatives to automatically acquire sense examples.
The right-most column shows results of a 10-fold
48
Word Train ex. Test ex. MFS Topical All
bass 3,201 107 90.7 92.5 93.5
crane 3,656 95 74.7 84.2 83.2
motion 2,821 201 70.1 78.6 84.6
palm 1,220 201 71.1 82.6 85.1
plant 4,183 188 54.4 76.6 76.6
tank 3,898 201 62.7 79.1 77.1
Overall 18,979 993 70.6 81.1 82.5
Table 1:Recall(%) of the VSM classifier trained on the MT-
translated sense examples, with different sets of features. The
MFS baseline(%) and the number of training and test exam-
ples are also shown.
(Mihalcea, Dictionary- MT- Hand-
Word 2003) based based tagged
bass 92.5 91.6 93.5 90.7
crane 71.6 74.5 83.2 81.1
motion 75.6 72.6 84.6 93.0
palm 80.6 81.1 85.1 87.6
plant 69.1 51.6 76.6 87.2
tank 63.7 66.7 77.1 84.1
Overall 76.6 71.3 82.5 87.6
Table 2:Recall(%) on TWA dataset for 3 unsupervised sys-
tems and a supervised cross-validation on test data.
cross-validation on the TWA data, which indicates
the score that a supervised system would attain,
taking additional advantage that the examples for
training and test are drawn from the same corpus.
We can see that our MT-based approach has
achieved significantly better recall than the other
two automatic methods. Besides, the results of
our unsupervised system are approaching the per-
formance achieved with hand-tagged data. It is
worth mentioning that Mihalcea (2003) applied a
similar supervised cross-validation method on this
dataset that scored 83.35%, very close to our unsu-
pervised system9. Thus, we can conclude that the
MT-based system is able to reach the best perfor-
mance reported on this dataset for an unsupervised
system.
5 Experiments on Senseval-3
In this section we describe the experiments carried
out on the Senseval-3 lexical sample dataset. First,
we introduce a heuristic method to deal with the
problem of fine-grainedness of WordNet senses.
The remaining two subsections will be devoted
to the experiments of the baseline system and the
contribution of the heuristic to the final system.
9The main difference to our hand-tagged evaluation, apart
from the ML algorithm, is that we did not remove the bias
from the ?one sense per discourse? factor, as she did.
Remove Remove Sn.-Tk.
Threshold Senses Tokens ratio
4 7,669 (40.6) 11,154 (15.9) 2.55
5 9,759 (51.6) 15,516 (22.1) 2.34
6 11,341 (60.0) 18,827 (26.8) 2.24
7 12,569 (66.5) 21,775 (31.0) 2.14
8 13,553 (71.7) 24,224 (34.5) 2.08
9 14,376 (76.0) 27,332 (38.9) 1.95
10 14,914 (78.9) 29,418 (41.9) 1.88
Table 3:Sense filtering by relative-threshold on SemCor. For
each threshold the number of removed senses/tokens and am-
biguity are shown.
5.1 Unsupervised methods on fine-grained
senses
When applying unsupervised WSD algorithms to
fine-grained word senses, senses that rarely occur
in texts often cause problems, as these cases are
difficult to detect without relying on hand-tagged
data. This is why many WSD systems use sense-
tagged corpora such as SemCor to discard or pe-
nalise low-frequency senses.
For our work, we did not want to rely on hand-
tagged corpora, and we devised a method to detect
low-frequency senses and to remove them before
using our translation-based approach. The method
is based on the hypothesis that word senses that
have few close relatives (synonyms, hypernyms,
and hyponyms) tend to have low frequency in cor-
pora. We collected all the close relatives to the
target senses, according to WordNet, and then re-
moved all the senses that did not have a number of
relatives above a given threshold. We used this
method on nouns, as the WordNet hierarchy is
more developed for them.
First, we observed the effect of sense removal
in the SemCor corpus. For all the polysemous
nouns, we applied different thresholds (4-10 rel-
atives) and measured the percentage of senses and
SemCor tokens that were removed. Our goal was
to remove as many senses as we could, while keep-
ing as many tokens as possible. Table 3 shows
the results of the process on all        polysemous
nouns in SemCor for a total of 18,912 senses and
70,238 tokens. The average number of senses per
token initially is      .
For the lowest threshold (4) we can see that
we are able to remove a large number of senses
from consideration (40%), keeping 85% of the to-
kens in SemCor. Higher thresholds can remove
more senses, but it forces us to discard more valid
tokens. In Table 3, the best ratios are given by
lower thresholds, suggesting that conservative ap-
49
proaches would be better. However, we have to
take into account that unsupervised state-of-the-
art WSD methods on fine-grained senses perform
below 50% recall on this dataset10, and therefore
an approach that is more aggressive may be worth
trying.
We applied this heuristic method in our exper-
iments and decided to measure the effect of the
threshold parameter by relying on SemCor and the
Senseval-3 training data. Thus, we tested the MT-
based system for different threshold values, re-
moving the senses for consideration when the rel-
ative number was below the threshold. The results
of the experiments using this technique will be de-
scribed in Section 5.3.
5.2 Baseline system
We performed experiments on Senseval-3 test
data with both MT-based and dictionary-based ap-
proaches. We show the results for nouns and ad-
jectives in Table 4, together with the MFS base-
line (obtained from the Senseval-3 lexical sam-
ple training data). We can see that the results are
similar for nouns, while for adjectives the MT-
based system achieves significantly better recall.
Overall, the performance was much lower than our
previous 2-way disambiguation. The system also
ranks below the MFS baseline.
One of the main reasons for the low perfor-
mance was that senses with few examples in the
test data are over-represented in training. This is
because we trained the classifiers on equal num-
ber of maximumly 200 sense examples for every
sense, no matter how rarely a sense actually oc-
curs in real text. As we explained in the previ-
ous section, this problem could be alleviated for
nouns by using the relative-based heuristics. We
only implemented the MT-based approach for the
rest of the experiments, as it performed better than
the dictionary-based one.
5.3 Relative threshold
In this section we explored the contribution of
the relative-based threshold to the system. We
tested the system only on nouns. In order to
tune the threshold parameter, we first applied the
method on SemCor and the Senseval-3 training
data. We used hand-tagged corpora from two
different sources to see whether the method was
10Best score in Senseval-3 for nouns without SemCor
or hand-tagged data: 47.5% recall (figure obtained from
http://www.senseval.org).
Test Dictionary- MT-
Word Ex. MFS based based
Nouns 1807 54.23 40.07 40.73
Adjs 159 49.69 15.74 23.29
Overall 1966 53.86 38.10 39.32
Table 4:Averaged recall(%) for the dictionary-based and MT-
based methods in Senseval-3 lexical-sample data. The MFS
baseline(%) and the number of testing examples are also
shown.
Avg. test
Threshold ambiguity Senseval-3 SemCor
0 5.80 40.68 30.11
4 3.60 40.15 32.99
5 3.32 39.43 32.82
6 2.76 40.53 34.18
7 2.52 43.89 35.94
8 2.36 46.90 39.15
9 2.08 45.37 38.98
10 1.88 48.62 46.16
11 1.80 48.59 47.68
12 1.68 48.34 43.63
13 1.40 47.23 45.31
14 1.28 44.32 42.05
Table 5:Average ambiguity and recall(%) for the relative-
based threshold on Senseval-3 training data and SemCor (for
nouns only). Best results shown in bold.
generic enough to be applied on unseen test data.
Note also that we used this experiment to define a
general threshold for the heuristic, instead of opti-
mising it for different words. Once the threshold
is fixed, it will be used for all target words.
The results of the MT-based system applying
threshold values from 4 to 14 are given in Table 5.
We can see clearly that the algorithm benefits from
the heuristic, specially when ambiguity is reduced
to around 2 senses in average. Also observe that
the contribution of the threshold is quite similar
for SemCor and Senseval-3 training data. From
this table, we chose 11 as threshold value for the
test data, as it obtained the best performance on
SemCor.
Thus, we performed a single run of the algo-
rithm on the test data applying the chosen thresh-
old. The performance for all nouns is given in
Table 6. We can see that the recall has increased
significantly, and is now closer to the MFS base-
line, which is a very hard baseline for unsuper-
vised systems (McCarthy et al, 2004). Still, the
performance is significantly lower than the score
achieved by supervised systems, which can reach
above 72% recall (Mihalcea et al, 2004). Some of
the reasons for the gap are the following:
  The acquisition process: problems can arise
50
Word Test Ex. MFS Our System
argument 111 51.40 45.90
arm 133 82.00 85.70
atmosphere 81 66.70 35.80
audience 100 67.00 67.00
bank 132 67.40 67.40
degree 128 60.90 60.90
difference 114 40.40 40.40
difficulty 23 17.40 39.10
disc 100 38.00 27.00
image 74 36.50 17.60
interest 93 41.90 11.80
judgment 32 28.10 40.60
organization 56 73.20 19.60
paper 117 25.60 37.60
party 116 62.10 52.60
performance 87 26.40 26.40
plan 84 82.10 82.10
shelter 98 44.90 39.80
sort 96 65.60 65.60
source 32 65.60 65.60
Overall 1807 54.23 48.58
Table 6:Final results(%) for all nouns in Senseval-3 test data.
Together with the number of test examples and MFS base-
line(%).
from ambiguous Chinese words, and the ac-
quired examples can contain noise generated
by the MT software.
  Distribution of fine-grained senses: As we
have seen, it is difficult to detect rare senses
for unsupervised methods, while supervised
systems can simply rely on frequency of
senses.
  Lack of local context: Our system does
not benefit from local bigrams and trigrams,
which for supervised systems are one of the
best sources of knowledge.
5.4 Comparison with Senseval-3
unsupervised systems
Finally, we compared the performance of our
system with other unsupervised systems in the
Senseval-3 lexical-sample competition. We eval-
uated these systems for nouns, using the out-
puts provided by the organisation11 , and focusing
on the systems that are considered unsupervised.
However, we noticed that most of these systems
used the information of SemCor frequency, or
even Senseval-3 examples in their models. Thus,
we classified the systems depending on whether
they used SemCor frequencies (Sc), Senseval-3
examples (S-3), or did not (Unsup.). This is an
11http://www.senseval.org
System Type Prec. Recall
wsdiit S-3 67.96 67.96
Cymfony S-3 57.94 57.94
Prob0 S-3 55.01 54.13
clr04 Sc 48.86 48.75
upv-unige-CIAOSENSO Sc 53.95 48.70
MT-based Unsup. 48.58 48.58
duluth-senserelate Unsup. 47.48 47.48
DFA-Unsup-LS Sc 46.71 46.71
KUNLP.eng.ls Sc 45.10 45.10
DLSI-UA-ls-eng-nosu. Unsup. 20.01 16.05
Table 7:Comparison of unsupervised S3 systems for nouns
(sorted by recall(%)). Our system given in bold.
important distinction, as simply knowing the most
frequent sense in hand-tagged data is a big advan-
tage for unsupervised systems (applying the MFS
heuristic for nouns in Senseval-3 would achieve
54.2% precision, and 53.0% recall when using
SemCor). At this point, we would like to remark
that, unlike other systems using Semcor, we have
applied it to the minimum extent. Its only contri-
bution has been to indirectly set the threshold for
our general heuristic based on WordNet relatives.
We are exploring better ways to integrate the rela-
tive information in the model.
The results of the Senseval-3 systems are given
in Table 7. There are only 2 systems that do not re-
quire any hand-tagged data, and our method is able
to improve both when using the relative-threshold.
The best systems in Senseval-3 benefited from the
training examples from the training data, particu-
larly the top-scoring system, which is clearly su-
pervised. The 2nd ranked system requires 10%
of the training examples in Senseval-3 to map the
clusters that it discovers automatically, and the 3rd
simply applies the MFS heuristic.
The remaining systems introduce bias of the
SemCor distribution in their models, which clearly
helped their performance for each word. Our sys-
tem is able to obtain a similar performance to the
best of those systems without relying on hand-
tagged data. We also evaluated the systems on
the coarse-grained sense groups provided by the
Senseval-3 organisers. The results in Table 8 show
that our system is comparatively better on this
coarse-grained disambiguation task.
6 Conclusions and Future Work
We automatically acquired English sense exam-
ples for WSD using large Chinese corpora and MT
software. We compared our sense examples with
those reported in previous work (Wang and Car-
51
System Type Prec. Recall
wsdiit S-3 75.3 75.3
Cymfony S-3 66.6 66.6
Prob0 S-3 61.9 61.9
MT-based Unsup. 57.9 57.9
clr04 Sc. 57.6 57.6
duluth-senserelate Unsup. 56.1 56.1
KUNLP-eng-ls Sc. 55.6 55.6
upv-unige-CIAOSENSO- Sc. 61.3 55.3
DFA-Unsup-LS Sc. 54.5 54.5
DLSI-UA-ls-eng-nosu. Unsup. 27.6 27.6
Table 8:Coarse-grained evaluation of unsupervised S3 sys-
tems for nouns (sorted by recall(%)). Our system given in
bold.
roll, 2005), by training a ML classifier on them
and then testing the classifiers on both coarse-
grained and fine-grained English gold standard
datasets. On both datasets, our MT-based sense
examples outperformed dictionary-based ones. In
addition, evaluations show our unsupervised WSD
system is competitive to the state-of-the-art super-
vised systems on binary disambiguation, and un-
supervised systems on fine-grained disambigua-
tion.
In the future, we would like to combine our ap-
proach with other systems based on automatic ac-
quisition of sense examples that can provide lo-
cal context (Agirre and Martinez, 2004b). The
goal would be to construct a collection of exam-
ples automatically obtained from different sources
and to apply ML algorithms on them. Each exam-
ple would have a different weight depending on
the acquisition method used.
Regarding the influence of sense distribution
in the training data, we will explore the poten-
tial of using a weighting scheme on the ?relative
threshold? algorithm. Also, we would like to anal-
yse if automatically obtained information on sense
distribution (McCarthy et al, 2004) can improve
WSD performance. We may also try other MT
systems and possibly see if our WSD can in turn
help MT, which can be viewed as a bootstrapping
learning process. Another interesting direction is
automatically selecting the most informative sense
examples as training data for ML classifiers.
References
E. Agirre and D. Martinez. 2004a. The Basque Country Uni-
versity system: English and Basque tasks. In Proceedings
of the 3rd ACL workshop on the Evaluation of Systems
for the Semantic Analysis of Text (SENSEVAL), Barcelona,
Spain.
E. Agirre and D. Martinez. 2004b. Unsupervised wsd
based on automatically retrieved examples: The impor-
tance of bias. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing (EMNLP),
Barcelona, Spain.
Y. S. Chan and H. T. Ng. 2005. Scaling up word sense disam-
biguation via parallel texts. In Proceedings of the 20th Na-
tional Conference on Artificial Intelligence (AAAI 2005),
Pittsburgh, Pennsylvania, USA.
I. Dagan and A. Itai. 1994. Word sense disambiguation using
a second language monolingual corpus. Computational
Linguistics, 20(4):563?596.
M. Diab and P. Resnik. 2002. An unsupervised method for
word sense tagging using parallel corpora. In Proceedings
of the      Anniversary Meeting of the Association for
Computational Linguistics (ACL-02). Philadelphia, USA.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
W. A. Gale, K. W. Church, and D. Yarowsky. 1992. Us-
ing bilingual materials to develop word sense disambigua-
tion methods. In Proceedings of the International Con-
ference on Theoretical and Methodological Issues in Ma-
chine Translation, pages 101?112.
H. Li and C. Li. 2004. Word translation disambiguation us-
ing bilingual bootstrapping. Computational Linguistics,
20(4):563?596.
B. Magnini and G. Cavaglia?. 2000. Integrating subject field
codes into WordNet. In Proceedings of the Second Inter-
national LREC Conference, Athens, Greece.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding Predominant Word Senses in Untagged Text. In
Proceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Barcelona,
Spain.
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004. The
Senseval-3 English lexical sample task. In Proceedings of
the 3rd ACL workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
R. Mihalcea. 2003. The role of non-ambiguous words in
natural language disambiguation. In Proceedings of the
Conference on Recent Advances in Natural Language Pro-
cessing, RANLP.
G. A. Miller, C. Leacock, R. Tengi, and R. Bunker. 1993.
A Semantic Concordance. In Proceedings of the ARPA
Human Language Technology Workshop, pages 303?308,
Princeton, NJ, March. distributed as Human Language
Technology by San Mateo, CA: Morgan Kaufmann Pub-
lishers.
H. T. Ng, B. Wang, and Y. S. Chan. 2003. Exploiting parallel
texts for word sense disambiguation: an empirical study.
In Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics.
G. Ngai and R. Florian. 2001. Transformation-based learn-
ing in the fast lane. Proceedings of the Second Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 40-47, Pittsburgh,
PA, USA.
T. Pedersen. 2001. A decision tree of bigrams is an accu-
rate predictor of word sense. Proceedings of the Second
Meeting of the NAACL, Pittsburgh, PA.
X. Wang and J. Carroll. 2005. Word sense disambiguation
using sense examples automatically acquired from a sec-
ond language. In Proceedings of HLT/EMNLP, Vancou-
ver, Canada.
52
MRD-based Word Sense Disambiguation: Further#2 Extending#1 Lesk
Timothy Baldwin,? Su Nam Kim,? Francis Bond,? Sanae Fujita,?
David Martinez? and Takaaki Tanaka?
? CSSE
University of Melbourne
VIC 3010 Australia
? NICT
3-5 Hikaridai, Seika-cho
Soraku-gun, Kyoto
619-0289 Japan
? NTT CS Labs
2-4 Hikari-dai, Seika-cho
Soraku-gun, Kyoto
619-0237 Japan
Abstract
This paper reconsiders the task of MRD-
based word sense disambiguation, in extend-
ing the basic Lesk algorithm to investigate
the impact onWSD performance of different
tokenisation schemes, scoring mechanisms,
methods of gloss extension and filtering
methods. In experimentation over the Lex-
eed Sensebank and the Japanese Senseval-
2 dictionary task, we demonstrate that char-
acter bigrams with sense-sensitive gloss ex-
tension over hyponyms and hypernyms en-
hances WSD performance.
1 Introduction
The aim of this work is to develop and extend word
sense disambiguation (WSD) techniques to be ap-
plied to all words in a text. The goal of WSD is
to link occurrences of ambiguous words in specific
contexts to their meanings, usually represented by
a machine readable dictionary (MRD) or a similar
lexical repository. For instance, given the following
Japanese input:
(1) ?????
quiet
?
dog
?
ACC
????
want to keep
?(I) want to keep a quiet dog?
we would hope to identify each component word as
occurring with the sense corresponding to the indi-
cated English glosses.
WSD systems can be classified according to the
knowledge sources they use to build their models. A
top-level distinction is made between supervised and
unsupervised systems. The former rely on training
instances that have been hand-tagged, while the lat-
ter rely on other types of knowledge, such as lexical
databases or untagged corpora. The Senseval evalu-
ation tracks have shown that supervised systems per-
form better when sufficient training data is available,
but they do not scale well to all words in context.
This is known as the knowledge acquisition bottle-
neck, and is the main motivation behind research on
unsupervised techniques (Mihalcea and Chklovski,
2003).
In this paper, we aim to exploit an existing lexical
resource to build an all-words Japanese word-sense
disambiguator. The resource in question is the Lex-
eed Sensebank (Tanaka et al, 2006) and consists of
the 28,000 most familiar words of Japanese, each of
which has one or more basic senses. The senses take
the form of a dictionary definition composed from
the closed vocabulary of the 28,000 words contained
in the dictionary, each of which is further manually
sense annotated according to the Lexeed sense in-
ventory. Lexeed also has a semi-automatically con-
structed ontology.
Through the Lexeed sensebank, we investigate a
number of areas of general interest to theWSD com-
munity. First, we test extensions of the Lesk algo-
rithm (Lesk, 1986) over Japanese, focusing specif-
ically on the impact of the overlap metric and seg-
ment representation on WSD performance. Second,
we propose further extensions of the Lesk algorithm
that make use of disambiguated definitions. In this,
we shed light on the relative benefits we can expect
from hand-tagging dictionary definitions, i.e. in in-
troducing ?semi-supervision? to the disambiguation
task. The proposed method is language independent,
and is equally applicable to the Extended WordNet1
for English, for example.
2 Related work
Our work focuses on unsupervised and semi-
supervised methods that target al words and parts
of speech (POS) in context. We use the term
?unsupervised? to refer to systems that do not
use hand-tagged example sets for each word, in
line with the standard usage in the WSD litera-
ture (Agirre and Edmonds, 2006). We blur the su-
pervised/unsupervised boundary somewhat in com-
bining the basic unsupervised methods with hand-
tagged definitions from Lexeed, in order to measure
the improvement we can expect from sense-tagged
data. We qualify our use of hand-tagged definition
1 http://xwn.hlt.utdallas.edu
775
sentences by claiming that this kind of resource is
less costly to produce than sense-annotated open text
because: (1) the effects of discourse are limited, (2)
syntax is relatively simple, (3) there is significant se-
mantic priming relative to the word being defined,
and (4) there is generally explicit meta-tagging of
the domain in technical definitions. In our experi-
ments, we will make clear when hand-tagged sense
information is being used.
Unsupervised methods rely on different knowl-
edge sources to build their models. Primarily
the following types of lexical resources have been
used for WSD: MRDs, lexical ontologies, and un-
tagged corpora (monolingual corpora, second lan-
guage corpora, and parallel corpora). Although
early approaches focused on exploiting a single re-
source (Lesk, 1986), recent trends show the bene-
fits of combining different knowledge sources, such
as hierarchical relations from an ontology and un-
tagged corpora (McCarthy et al, 2004). In this sum-
mary, we will focus on a few representative systems
that make use of different resources, noting that this
is an area of very active research which we cannot
do true justice to within the confines of this paper.
The Lesk method (Lesk, 1986) is an MRD-based
system that relies on counting the overlap between
the words in the target context and the dictionary
definitions of the senses. In spite of its simplicity,
it has been shown to be a hard baseline for unsu-
pervised methods in Senseval, and it is applicable to
all-words with minimal effort. Banerjee and Peder-
sen (2002) extended the Lesk method for WordNet-
based WSD tasks, to include hierarchical data from
the WordNet ontology (Fellbaum, 1998). They ob-
served that the hierarchical relations significantly
enhance the basic model. Both these methods will
be described extensively in Section 3.1, as our ap-
proach is based on them.
Other notable unsupervised and semi-supervised
approaches are those of McCarthy et al (2004), who
combine ontological relations and untagged corpora
to automatically rank word senses in relation to a
corpus, and Leacock et al (1998) who use untagged
data to build sense-tagged data automatically based
on monosemous words. Parallel corpora have also
been used to avoid the need for hand-tagged data,
e.g. by Chan and Ng (2005).
3 Background
As background to our work, we first describe the ba-
sic and extended Lesk algorithms that form the core
of our approach. Then we present the Lexeed lex-
ical resource we have used in our experiments, and
finally we outline aspects of Japanese relevant for
this work.
3.1 Basic and Extended Lesk
The original Lesk algorithm (Lesk, 1986) performs
WSD by calculating the relative word overlap be-
tween the context of usage of a target word, and the
dictionary definition of each of its senses in a given
MRD. The sense with the highest overlap is then se-
lected as the most plausible hypothesis.
An obvious shortcoming of the original Lesk al-
gorithm is that it requires that the exact words used
in the definitions be included in each usage of the
target word. To redress this shortcoming, Banerjee
and Pedersen (2002) extended the basic algorithm
for WordNet-based WSD tasks to include hierarchi-
cal information, i.e. expanding the definitions to in-
clude definitions of hypernyms and hyponyms of the
synset containing a given sense, and assigning the
same weight to the words sourced from the different
definitions.
Both of these methods can be formalised accord-
ing to the following algorithm, which also forms the
basis of our proposed method:
for each word wi in context w = w1w2...wn do
for each sense si,j and definition di,j of wi do
score(si,j) = overlap(w, di,j)
end for
s?i = arg maxj score(si,j)
end for
3.2 The Lexeed Sensebank
All our experimentation is based on the Lexeed
Sensebank (Tanaka et al, 2006). The Lexeed Sense-
bank consists of all Japanese words above a certain
level of familiarity (as defined by Kasahara et al
(2004)), giving rise to 28,000 words in all, with a to-
tal of 46,000 senses which are similarly filtered for
similarity. The sense granularity is relatively coarse
for most words, with the possible exception of light
verbs, making it well suited to open-domain appli-
cations. Definition sentences for these senses were
rewritten to use only the closed vocabulary of the
28,000 familiar words (and some function words).
Additionally, a single example sentence was man-
ually constructed to exemplify each of the 46,000
senses, once again using the closed vocabulary of the
Lexeed dictionary. Both the definition sentences and
example sentences were then manually sense anno-
tated by 5 native speakers of Japanese, from which a
majority sense was extracted.
776
In addition, an ontology was induced from the
Lexeed dictionary, by parsing the first definition sen-
tence for each sense (Nichols et al, 2005). Hy-
pernyms were determined by identifying the highest
scoping real predicate (i.e. the genus). Other rela-
tion types such as synonymy and domain were also
induced based on trigger patterns in the definition
sentences, although these are too few to be useful
in our research. Because each word is sense tagged,
the relations link senses rather than just words.
3.3 Peculiarities of Japanese
The experiments in this paper focus exclusively
on Japanese WSD. Below, we outline aspects of
Japanese which are relevant to the task.
First, Japanese is a non-segmenting language, i.e.
there is no explicit orthographic representation of
word boundaries. The native rendering of (1), e.g., is???????????. Various packages exist to
automatically segment Japanese strings into words,
and the Lexeed data has been pre-segmented using
ChaSen (Matsumoto et al, 2003).
Second, Japanese is made up of 3 basic alpha-
bets: hiragana, katakana (both syllabic in nature)
and kanji (logographic in nature). The relevance of
these first two observations to WSD is that we can
choose to represent the context of a target word by
way of characters or words.
Third, Japanese has relatively free word order,
or strictly speaking, word order within phrases is
largely fixed but the ordering of phrases governed
by a given predicate is relatively free.
4 Proposed Extensions
We propose extensions to the basic Lesk algorithm
in the orthogonal areas of the scoring mechanism,
tokenisation, extended glosses and filtering.
4.1 Scoring Mechanism
In our algorithm, overlap provides the means to
score a given pairing of context w and definition
di,j . In the original Lesk algorithm, overlap was
simply the sum of words in common between the
two, which Banerjee and Pedersen (2002) modified
by squaring the size of each overlapping sub-string.
While squaring is well motivated in terms of prefer-
ring larger substring matches, it makes the algorithm
computationally expensive. We thus adopt a cheaper
scoring mechanism which normalises relative to the
length of w and di,j , but ignores the length of sub-
string matches. Namely, we use the Dice coefficient.
4.2 Tokenisation
Tokenisation is particularly important in Japanese
because it is a non-segmenting language with a lo-
gographic orthography (kanji). As such, we can
chose to either word tokenise via a word splitter
such as ChaSen, or character tokenise. Charac-
ter and word tokenisation have been compared in
the context of Japanese information retrieval (Fujii
and Croft, 1993) and translation retrieval (Baldwin,
2001), and in both cases, characters have been found
to be the superior representation overall.
Orthogonal to the question of whether to tokenise
into words or characters, we adopt an n-gram seg-
ment representation, in the form of simple unigrams
and simple bigrams. In the case of word tokenisa-
tion and simple bigrams, e.g., example (1) would be
represented as {?????? ,?? ,????? }.
4.3 Extended Glosses
The main direction in which Banerjee and Peder-
sen (2002) successfully extended the Lesk algorithm
was in including hierarchically-adjacent glosses (i.e.
hyponyms and hypernyms). We take this a step
further, in using both the Lexeed ontology and the
sense-disambiguated words in the definition sen-
tences.
The basic form of extended glossing is the simple
Lesk method, where we take the simple definitions
for each sense si,j (i.e. without any gloss extension).
Next, we replicate the Banerjee and Pedersen
(2002) method in extending the glosses to include
words from the definitions for the (immediate) hy-
pernyms and/or hyponyms of each sense si,j .
An extension of the Banerjee and Pedersen (2002)
method which makes use of the sense-annotated def-
initions is to include the words in the definition of
each sense-annotated word dk contained in defini-
tion di,j = d1d2...dm of word sense si,j . That is,
rather than traversing the ontology relative to each
word sense candidate si,j for the target word wi,
we represent each word sense via the original def-
inition plus all definitions of word senses contained
in it (weighting each to give the words in the original
definition greater import than those from definitions
of those word senses). We can then optionally adopt
a similar policy to Banerjee and Pedersen (2002) in
expanding each sense-annotated word dk in the orig-
inal definition relative to the ontology, to include the
immediate hypernyms and/or hyponyms.
We further expand the definitions (+extdef) by
adding the full definition for each sense-tagged word
in the original definition. This can be combined
with the Banerjee and Pedersen (2002) method by
777
also expanding each sense-annotated word dk in the
original definition relative to the ontology, to in-
clude the immediate hypernyms (+hyper) and/or hy-
ponyms (+hypo).
4.4 Filtering
Each word sense in the dictionary is marked with a
word class, and the word splitter similarly POS tags
every definition and input to the system. It is nat-
ural to expect that the POS tag of the target word
should match the word class of the word sense, and
this provides a coarse-grained filter for discriminat-
ing homographs with different word classes.
We also experiment with a stop word-based filter
which ignores a closed set of 18 lexicographic mark-
ers commonly found in definitions (e.g. ? [ryaku]
?an abbreviation for ...?), in line with those used by
Nichols et al (2005) in inducing the ontology.
5 Evaluation
We evaluate our various extensions over two
datasets: (1) the example sentences in the Lexeed
sensebank, and (2) the Senseval-2 Japanese dictio-
nary task (Shirai, 2002).
All results below are reported in terms of sim-
ple precision, following the conventions of Senseval
evaluations. For all experiments, precision and re-
call are identical as our systems have full coverage.
For the two datasets, we use two baselines: a ran-
dom baseline and the first-sense baseline. Note that
the first-sense baseline has been shown to be hard
to beat for unsupervised systems (McCarthy et al,
2004), and it is considered supervised when, as in
this case, the first-sense is the most frequent sense
from hand-tagged corpora.
5.1 Lexeed Example Sentences
The goal of these experiments is to tag all the words
that occur in the example sentences in the Lexeed
Sensebank. The first set of experiments over the
Lexeed Sensebank explores three parameters: the
use of characters vs. words, unigrams vs. bigrams,
and original vs. extended definitions. The results of
the experiments and the baselines are presented in
Table 1.
First, characters are in all cases superior to words
as our segment granularity. The introduction of bi-
grams has a uniformly negative impact for both char-
acters and words, due to the effects of data sparse-
ness. This is somewhat surprising for characters,
given that the median word length is 2 characters,
although the difference between character unigrams
and bigrams is slight.
Extended definitions are also shown to be superior
to simple definitions, although the relative increment
in making use of large amounts of sense annotations
is smaller than that of characters vs. words, suggest-
ing that the considerable effort in sense annotating
the definitions is not commensurate with the final
gain for this simple method.
Note that at this stage, our best-performing
method is roughly equivalent to the unsupervised
(random) baseline, but well below the supervised
(first sense) baseline.
Having found that extended definitions improve
results to a small degree, we turn to our next exper-
iment were we investigate whether the introduction
of ontological relations to expand the original def-
initions further enhances our precision. Here, we
persevere with the use of word and characters (all
unigrams), and experiment with the addition of hy-
pernyms and/or hyponyms, with and without the ex-
tended definitions. We also compare our method
directly with that of Banerjee and Pedersen (2002)
over the Lexeed data, and further test the impact
of the sense annotations, in rerunning our experi-
ments with the ontology in a sense-insensitive man-
ner, i.e. by adding in the union of word-level hyper-
nyms and/or hyponyms. The results are described in
Table 2. The results in brackets are reproduced from
earlier tables.
Adding in the ontology makes a significant dif-
ference to our results, in line with the findings of
Banerjee and Pedersen (2002). Hyponyms are better
discriminators than hypernyms (assuming a given
word sense has a hyponym ? the Lexeed ontology
is relatively flat), partly because while a given word
sense will have (at most) one hypernym, it often has
multiple hyponyms (if any at all). Adding in hyper-
nyms or hyponyms, in fact, has a greater impact on
results than simple extended definitions (+extdef),
especially for words. The best overall results are
produced for the (weighted) combination of all on-
tological relations (i.e. extended definitions, hyper-
nyms and hyponyms), achieving a precision level
above both the unsupervised (random) and super-
vised (first-sense) baselines.
In the interests of getting additional insights into
the import of sense annotations in our method, we
ran both the original Banerjee and Pedersen (2002)
method and a sense-insensitive variant of our pro-
posed method over the same data, the results for
which are also included in Table 2. Simple hy-
ponyms (without extended definitions) and word-
based segments returned the best results out of all
the variants tried, at a precision of 0.656. This com-
pares with a precision of 0.683 achieved for the best
778
UNIGRAMS BIGRAMS
ALL WORDS POLYSEMOUS ALL WORDS POLYSEMOUS
Simple Definitions
CHARACTERS 0.523 0.309 0.486 0.262
WORDS 0.469 0.229 0.444 0.201
Extended Definitions
CHARACTERS 0.526 0.313 0.529 0.323
WORDS 0.489 0.258 0.463 0.227
Table 1: Precision over the Lexeed example sentences using simple/extended definitions and word/character
unigrams and bigrams (best-performing method in boldface)
ALL WORDS POLYSEMOUS
UNSUPERVISED BASELINE: 0.527 0.315
SUPERVISED BASELINE: 0.633 0.460
Banerjee and Pedersen (2002) 0.648 0.492
Ontology expansion (sense-sensitive)
simple (0.469) (0.229)
+extdef (0.489) (0.258)
+hypernyms 0.559 0.363
W +hyponyms 0.655 0.503
+def +hyper 0.577 0.386
+def +hypo 0.649 0.490
+def +hyper +hypo 0.683 0.539
simple (0.523) (0.309)
+extdef (0.526) (0.313)
+hypernyms 0.539 0.334
C +hyponyms 0.641 0.481
+def +hyper 0.563 0.365
+def +hypo 0.671 0.522
+def +hyper +hypo 0.671 0.522
Ontology expansion (sense-insensitive)
+hypernyms 0.548 0.348
+hyponyms 0.656 0.503
W +def +hyper 0.551 0.347
+def +hypo 0.649 0.490
+def + hyper +hypo 0.631 0.464
+hypernyms 0.537 0.332
+hyponyms 0.644 0.485
C +def +hyper 0.542 0.335
+def +hypo 0.644 0.484
+def + hyper +hypo 0.628 0.460
Table 2: Precision over the Lexeed exam-
ple sentences using ontology-based gloss extension
(with/without word sense information) and word
(W) and character (C) unigrams (best-performing
method in boldface)
of the sense-sensitive methods, indicating that sense
information enhances WSD performance. This rein-
forces our expectation that richly annotated lexical
resources improve performance. With richer infor-
mation to work with, character based methods uni-
formly give worse results.
While we don?t present the results here due to rea-
sons of space, POS-based filtering had very little im-
pact on results, due to very few POS-differentiated
homographs in Japanese. Stop word filtering leads
ALL
WORDS
POLYSEMOUS
Baselines
Unsupervised (random) 0.310 0.260
Supervised (first-sense) 0.577 0.555
Ontology expansion (sense-sensitive)
W +def +hyper +hypo 0.624 0.605
C +def +hyper +hypo 0.624 0.605
Ontology expansion (sense-insensitive)
W +def +hyper +hypo 0.602 0.581
C +def +hyper +hypo 0.593 0.572
Table 3: Precision over the Senseval-2 data
to a very slight increment in precision across the
board (of the order of 0.001).
5.2 Senseval-2 Japanese Dictionary Task
In our second set of experiments we apply our pro-
posed method to the Senseval-2 Japanese dictionary
task (Shirai, 2002) in order to calibrate our results
against previously published results for Japanese
WSD. Recall that this is a lexical sample task,
and that our evaluation is relative to Lexeed re-
annotations of the same dataset, although the relative
polysemy for the original data and the re-annotated
version are largely the same (Tanaka et al, 2006).
The first sense baselines (i.e. sense skewing) for the
two sets of annotations differ significantly, however,
with a precision of 0.726 reported for the original
task, and 0.577 for the re-annotated Lexeed vari-
ant. System comparison (Senseval-2 systems vs. our
method) will thus be reported in terms of error rate
reduction relative to the respective first sense base-
lines.
In Table 3, we present the results over the
Senseval-2 data for the best-performing systems
from our earlier experiments. As before, we in-
clude results over both words and characters, and
with sense-sensitive and sense-insensitive ontology
expansion.
Our results largely mirror those of Table 2, al-
though here there is very little to separate words
and characters. All methods surpassed both the ran-
dom and first sense baselines, but the relative impact
779
of sense annotations was if anything even less pro-
nounced than for the example sentence task.
Both sense-sensitiveWSDmethods achieve a pre-
cision of 0.624 over all the target words (with one
target word per sentence), an error reduction rate
of 11.1%. This compares favourably with an error
rate reduction of 21.9% for the best of the WSD
systems in the original Senseval-2 task (Kurohashi
and Shirai, 2001), particularly given that our method
is semi-supervised while the Senseval-2 system is a
conventional supervised word sense disambiguator.
6 Conclusion
In our experiments extending the Lesk algorithm
over Japanese data, we have shown that definition
expansion via an ontology produces a significant
performance gain, confirming results by Banerjee
and Pedersen (2002) for English. We also explored
a new expansion of the Lesk method, by measuring
the contribution of sense-tagged definitions to over-
all disambiguation performance. Using sense infor-
mation doubles the error reduction compared to the
supervised baseline, a constant gain that shows the
importance of precise sense information for error re-
duction.
Our WSD system can be applied to all words in
running text, and is able to improve over the first-
sense baseline for two separate WSD tasks, using
only existing Japanese resources. This full-coverage
system opens the way to explore further enhance-
ments, such as the contribution of extra sense-tagged
examples to the expansion, or the combination of
different WSD algorithms.
For future work, we are also studying the in-
tegration of the WSD tool with other applications
that deal with Japanese text, such as a cross-lingual
glossing tool that aids Japanese learners reading text.
Another application we are working on is the inte-
gration of the WSD system with parse selection for
Japanese grammars.
Acknowledgements
This material is supported by the Research Collaboration be-
tween NTT Communication Science Laboratories, Nippon
Telegraph and Telephone Corporation and the University of
Melbourne. We would like to thank members of the NTT Ma-
chine Translation Group and the three anonymous reviewers for
their valuable input on this research.
References
Eneko Agirre and Philip Edmonds, editors. 2006. Word Sense
Disambiguation: Algorithms and Applications. Springer,
Dordrecht, Netherlands.
Timothy Baldwin. 2001. Low-cost, high-performance transla-
tion retrieval: Dumber is better. In Proc. of the 39th Annual
Meeting of the ACL and 10th Conference of the EACL (ACL-
EACL 2001), pages 18?25, Toulouse, France.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk
algorithm for word sense disambiguation using WordNet. In
Proc. of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-2002),
pages 136?45, Mexico City, Mexico.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word
sense disambiguation via parallel texts. In Proc. of the 20th
National Conference on Artificial Intelligence (AAAI 2005),
pages 1037?42, Pittsburgh, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Hideo Fujii and W. Bruce Croft. 1993. A comparison of index-
ing techniques for Japanese text retrieval. In Proc. of 16th
International ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?93), pages 237?
46, Pittsburgh, USA.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lexicon:
Lexeed. In Proc. of SIG NLC-159, Tokyo, Japan.
Sadao Kurohashi and Kiyoaki Shirai. 2001. SENSEVAL-2
Japanese tasks. In IEICE Technical Report NLC 2001-10,
pages 1?8. (in Japanese).
Claudia Leacock, Martin Chodorow, and George A. Miller.
1998. Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics, 24(1):147?
65.
Michael Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone from
an ice cream cone. In Proc. of the 1986 SIGDOC Confer-
ence, pages 24?6, Ontario, Canada.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka
Hirano, Hiroshi Matsuda, Kazuma Takaoka, and Masayuki
Asahara. 2003. Japanese Morphological Analysis System
ChaSen Version 2.3.3 Manual. Technical report, NAIST.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004. Finding predominant senses in untagged text. In
Proc. of the 42nd Annual Meeting of the ACL, pages 280?
7, Barcelona, Spain.
Rada Mihalcea and Timothy Chklovski. 2003. Open Mind
Word Expert: Creating Large Annotated Data Collections
with Web Users? Help. In Proceedings of the EACL
2003 Workshop on Linguistically Annotated Corpora (LINC
2003), pages 53?61, Budapest, Hungary.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictionar-
ies. In Proc. of the 19th International Joint Conference
on Artificial Intelligence (IJCAI-2005), pages 1111?6, Ed-
inburgh, UK.
Kiyoaki Shirai. 2002. Construction of a word sense tagged
corpus for SENSEVAL-2 japanese dictionary task. In Proc.
of the 3rd International Conference on Language Resources
and Evaluation (LREC 2002), pages 605?8, Las Palmas,
Spain.
Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The
Hinoki sensebank ? a large-scale word sense tagged cor-
pus of Japanese ?. In Proc. of the Workshop on Frontiers
in Linguistically Annotated Corpora 2006, pages 62?9, Syd-
ney, Australia.
780
Proceedings of ACL-08: HLT, pages 317?325,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving Parsing and PP attachment Performance with Sense Information
Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Basque Country
e.agirre@ehu.es
Timothy Baldwin
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
tim@csse.unimelb.edu.au
David Martinez
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
davidm@csse.unimelb.edu.au
Abstract
To date, parsers have made limited use of se-
mantic information, but there is evidence to
suggest that semantic features can enhance
parse disambiguation. This paper shows that
semantic classes help to obtain significant im-
provement in both parsing and PP attachment
tasks. We devise a gold-standard sense- and
parse tree-annotated dataset based on the in-
tersection of the Penn Treebank and SemCor,
and experiment with different approaches to
both semantic representation and disambigua-
tion. For the Bikel parser, we achieved a
maximal error reduction rate over the base-
line parser of 6.9% and 20.5%, for parsing and
PP-attachment respectively, using an unsuper-
vised WSD strategy. This demonstrates that
word sense information can indeed enhance
the performance of syntactic disambiguation.
1 Introduction
Traditionally, parse disambiguation has relied on
structural features extracted from syntactic parse
trees, and made only limited use of semantic in-
formation. There is both empirical evidence and
linguistic intuition to indicate that semantic fea-
tures can enhance parse disambiguation perfor-
mance, however. For example, a number of different
parsers have been shown to benefit from lexicalisa-
tion, that is, the conditioning of structural features
on the lexical head of the given constituent (Mager-
man, 1995; Collins, 1996; Charniak, 1997; Char-
niak, 2000; Collins, 2003). As an example of lexi-
calisation, we may observe in our training data that
knife often occurs as the manner adjunct of open in
prepositional phrases headed by with (c.f. open with
a knife), which would provide strong evidence for
with (a) knife attaching to open and not box in open
the box with a knife. It would not, however, pro-
vide any insight into the correct attachment of with
scissors in open the box with scissors, as the disam-
biguation model would not be able to predict that
knife and scissors are semantically similar and thus
likely to have the same attachment preferences.
In order to deal with this limitation, we propose to
integrate directly the semantic classes of words into
the process of training the parser. This is done by
substituting the original words with semantic codes
that reflect semantic classes. For example, in the
above example we could substitute both knife and
scissors with the semantic class TOOL, thus relating
the training and test instances directly. We explore
several models for semantic representation, based
around WordNet (Fellbaum, 1998).
Our approach to exploring the impact of lexical
semantics on parsing performance is to take two
state-of-the-art statistical treebank parsers and pre-
process the inputs variously. This simple method
allows us to incorporate semantic information into
the parser without having to reimplement a full sta-
tistical parser, and also allows for maximum compa-
rability with existing results in the treebank parsing
community. We test the parsers over both a PP at-
tachment and full parsing task.
In experimenting with different semantic repre-
sentations, we require some strategy to disambiguate
the semantic class of polysemous words in context
(e.g. determining for each instance of crane whether
it refers to an animal or a lifting device). We explore
a number of disambiguation strategies, including the
use of hand-annotated (gold-standard) senses, the
317
use of the most frequent sense, and an unsupervised
word sense disambiguation (WSD) system.
This paper shows that semantic classes help to
obtain significant improvements for both PP attach-
ment and parsing. We attain a 20.5% error reduction
for PP attachment, and 6.9% for parsing. These re-
sults are achieved using most frequent sense infor-
mation, which surprisingly outperforms both gold-
standard senses and automatic WSD.
The results are notable in demonstrating that very
simple preprocessing of the parser input facilitates
significant improvements in parser performance. We
provide the first definitive results that word sense
information can enhance Penn Treebank parser per-
formance, building on earlier results of Bikel (2000)
and Xiong et al (2005). Given our simple procedure
for incorporating lexical semantics into the parsing
process, our hope is that this research will open the
door to further gains using more sophisticated pars-
ing models and richer semantic options.
2 Background
This research is focused on applying lexical seman-
tics in parsing and PP attachment tasks. Below, we
outline these tasks.
Parsing
As our baseline parsers, we use two state-of-the-
art lexicalised parsing models, namely the Bikel
parser (Bikel, 2004) and Charniak parser (Charniak,
2000). While a detailed description of the respective
parsing models is beyond the scope of this paper, it
is worth noting that both parsers induce a context
free grammar as well as a generative parsing model
from a training set of parse trees, and use a devel-
opment set to tune internal parameters. Tradition-
ally, the two parsers have been trained and evaluated
over the WSJ portion of the Penn Treebank (PTB:
Marcus et al (1993)). We diverge from this norm in
focusing exclusively on a sense-annotated subset of
the Brown Corpus portion of the Penn Treebank, in
order to investigate the upper bound performance of
the models given gold-standard sense information.
PP attachment in a parsing context
Prepositional phrase attachment (PP attachment)
is the problem of determining the correct attachment
site for a PP, conventionally in the form of the noun
or verb in a V NP PP structure (Ratnaparkhi et al,
1994; Mitchell, 2004). For instance, in I ate a pizza
with anchovies, the PP with anchovies could attach
either to the verb (c.f. ate with anchovies) or to the
noun (c.f. pizza with anchovies), of which the noun
is the correct attachment site. With I ate a pizza with
friends, on the other hand, the verb is the correct at-
tachment site. PP attachment is a structural ambigu-
ity problem, and as such, a subproblem of parsing.
Traditionally the so-called RRR data (Ratna-
parkhi et al, 1994) has been used to evaluate PP
attachment algorithms. RRR consists of 20,081
training and 3,097 test quadruples of the form
(v,n1,p,n2), where the attachment decision is
either v or n1. The best published results over RRR
are those of Stetina and Nagao (1997), who em-
ploy WordNet sense predictions from an unsuper-
vised WSD method within a decision tree classifier.
Their work is particularly inspiring in that it signifi-
cantly outperformed the plethora of lexicalised prob-
abilistic models that had been proposed to that point,
and has not been beaten in later attempts.
In a recent paper, Atterer and Schu?tze (2007) crit-
icised the RRR dataset because it assumes that an
oracle parser provides the two hypothesised struc-
tures to choose between. This is needed to derive the
fact that there are two possible attachment sites, as
well as information about the lexical phrases, which
are typically extracted heuristically from gold stan-
dard parses. Atterer and Schu?tze argue that the only
meaningful setting for PP attachment is within a
parser, and go on to demonstrate that in a parser set-
ting, the Bikel parser is competitive with the best-
performing dedicated PP attachment methods. Any
improvement in PP attachment performance over the
baseline Bikel parser thus represents an advance-
ment in state-of-the-art performance.
That we specifically present results for PP attach-
ment in a parsing context is a combination of us sup-
porting the new research direction for PP attachment
established by Atterer and Schu?tze, and us wishing
to reinforce the findings of Stetina and Nagao that
word sense information significantly enhances PP
attachment performance in this new setting.
Lexical semantics in parsing
There have been a number of attempts to incorpo-
rate word sense information into parsing tasks. The
318
most closely related research is that of Bikel (2000),
who merged the Brown portion of the Penn Tree-
bank with SemCor (similarly to our approach in Sec-
tion 4.1), and used this as the basis for evaluation of
a generative bilexical model for joint WSD and pars-
ing. He evaluated his proposed model in a parsing
context both with and without WordNet-based sense
information, and found that the introduction of sense
information either had no impact or degraded parse
performance.
The only successful applications of word sense in-
formation to parsing that we are aware of are Xiong
et al (2005) and Fujita et al (2007). Xiong et al
(2005) experimented with first-sense and hypernym
features from HowNet and CiLin (both WordNets
for Chinese) in a generative parse model applied
to the Chinese Penn Treebank. The combination
of word sense and first-level hypernyms produced
a significant improvement over their basic model.
Fujita et al (2007) extended this work in imple-
menting a discriminative parse selection model in-
corporating word sense information mapped onto
upper-level ontologies of differing depths. Based
on gold-standard sense information, they achieved
large-scale improvements over a basic parse selec-
tion model in the context of the Hinoki treebank.
Other notable examples of the successful incorpo-
ration of lexical semantics into parsing, not through
word sense information but indirectly via selectional
preferences, are Dowding et al (1994) and Hektoen
(1997). For a broader review of WSD in NLP appli-
cations, see Resnik (2006).
3 Integrating Semantics into Parsing
Our approach to providing the parsers with sense
information is to make available the semantic de-
notation of each word in the form of a semantic
class. This is done simply by substituting the origi-
nal words with semantic codes. For example, in the
earlier example of open with a knife we could sub-
stitute both knife and scissors with the class TOOL,
and thus directly facilitate semantic generalisation
within the parser. There are three main aspects that
we have to consider in this process: (i) the seman-
tic representation, (ii) semantic disambiguation, and
(iii) morphology.
There are many ways to represent semantic re-
lationships between words. In this research we
opt for a class-based representation that will map
semantically-related words into a common semantic
category. Our choice for this work was the WordNet
2.1 lexical database, in which synonyms are grouped
into synsets, which are then linked via an IS-A hi-
erarchy. WordNet contains other types of relations
such as meronymy, but we did not use them in this
research. With any lexical semantic resource, we
have to be careful to choose the appropriate level of
granularity for a given task: if we limit ourselves to
synsets we will not be able to capture broader gen-
eralisations, such as the one between knife and scis-
sors;1 on the other hand by grouping words related at
a higher level in the hierarchy we could find that we
make overly coarse groupings (e.g. mallet, square
and steel-wool pad are also descendants of TOOL in
WordNet, none of which would conventionally be
used as the manner adjunct of cut). We will test dif-
ferent levels of granularity in this work.
The second problem we face is semantic disam-
biguation. The more fine-grained our semantic rep-
resentation, the higher the average polysemy and the
greater the need to distinguish between these senses.
For instance, if we find the word crane in a con-
text such as demolish a house with the crane, the
ability to discern that this corresponds to the DE-
VICE and not ANIMAL sense of word will allow us
to avoid erroneous generalisations. This problem of
identifying the correct sense of a word in context is
known as word sense disambiguation (WSD: Agirre
and Edmonds (2006)). Disambiguating each word
relative to its context of use becomes increasingly
difficult for fine-grained representations (Palmer et
al., 2006). We experiment with different ways of
tackling WSD, using both gold-standard data and
automatic methods.
Finally, when substituting words with semantic
tags we have to decide how to treat different word
forms of a given lemma. In the case of English, this
pertains most notably to verb inflection and noun
number, a distinction which we lose if we opt to
map all word forms onto semantic classes. For our
current purposes we choose to substitute all word
1In WordNet 2.1, knife and scissors are sister synsets, both
of which have TOOL as their 4th hypernym. Only by mapping
them onto their 1st hypernym or higher would we be able to
capture the semantic generalisation alluded to above.
319
forms, but we plan to look at alternative represen-
tations in the future.
4 Experimental setting
We evaluate the performance of our approach in two
settings: (1) full parsing, and (2) PP attachment
within a full parsing context. Below, we outline the
dataset used in this research and the parser evalu-
ation methodology, explain the methodology used
to perform PP attachment, present the different op-
tions for semantic representation, and finally detail
the disambiguation methods.
4.1 Dataset and parser evaluation
One of the main requirements for our dataset is the
availability of gold-standard sense and parse tree an-
notations. The gold-standard sense annotations al-
low us to perform upper bound evaluation of the rel-
ative impact of a given semantic representation on
parsing and PP attachment performance, to contrast
with the performance in more realistic semantic dis-
ambiguation settings. The gold-standard parse tree
annotations are required in order to carry out evalu-
ation of parser and PP attachment performance.
The only publicly-available resource with these
two characteristics at the time of this work was the
subset of the Brown Corpus that is included in both
SemCor (Landes et al, 1998) and the Penn Tree-
bank (PTB).2 This provided the basis of our dataset.
After sentence- and word-aligning the SemCor and
PTB data (discarding sentences where there was a
difference in tokenisation), we were left with a total
of 8,669 sentences containing 151,928 words. Note
that this dataset is smaller than the one described by
Bikel (2000) in a similar exercise, the reason being
our simple and conservative approach taken when
merging the resources.
We relied on this dataset alne for all the exper-
iments in this paper. In order to maximise repro-
ducibility and encourage further experimentation in
the direction pioneered in this research, we parti-
tioned the data into 3 sets: 80% training, 10% devel-
opment and 10% test data. This dataset is available
on request to the research community.
2OntoNotes (Hovy et al, 2006) includes large-scale tree-
bank and (selective) sense data, which we plan to use for future
experiments when it becomes fully available.
We evaluate the parsers via labelled bracketing re-
call (R), precision (P) and F-score (F1). We use
Bikel?s randomized parsing evaluation comparator3
(with p < 0.05 throughout) to test the statistical sig-
nificance of the results using word sense informa-
tion, relative to the respective baseline parser using
only lexical features.
4.2 PP attachment task
Following Atterer and Schu?tze (2007), we wrote
a script that, given a parse tree, identifies in-
stances of PP attachment ambiguity and outputs the
(v,n1,p,n2) quadruple involved and the attach-
ment decision. This extraction system uses Collins?
rules (based on TREEP (Chiang and Bikel, 2002))
to locate the heads of phrases. Over the combined
gold-standard parsing dataset, our script extracted a
total of 2,541 PP attachment quadruples. As with
the parsing data, we partitioned the data into 3 sets:
80% training, 10% development and 10% test data.
Once again, this dataset and the script used to ex-
tract the quadruples are available on request to the
research community.
In order to evaluate the PP attachment perfor-
mance of a parser, we run our extraction script over
the parser output in the same manner as for the gold-
standard data, and compare the extracted quadru-
ples to the gold-standard ones. Note that there is
no guarantee of agreement in the quadruple mem-
bership between the extraction script and the gold
standard, as the parser may have produced a parse
which is incompatible with either attachment possi-
bility. A quadruple is deemed correct if: (1) it exists
in the gold standard, and (2) the attachment deci-
sion is correct. Conversely, it is deemed incorrect if:
(1) it exists in the gold standard, and (2) the attach-
ment decision is incorrect. Quadruples not found in
the gold standard are discarded. Precision was mea-
sured as the number of correct quadruples divided by
the total number of correct and incorrect quadruples
(i.e. all quadruples which are not discarded), and re-
call as the number of correct quadruples divided by
the total number of gold-standard quadruples in the
test set. This evaluation methodology coincides with
that of Atterer and Schu?tze (2007).
Statistical significance was calculated based on
3www.cis.upenn.edu/?dbikel/software.html
320
a modified version of the Bikel comparator (see
above), once again with p < 0.05.
4.3 Semantic representation
We experimented with a range of semantic represen-
tations, all of which are based on WordNet 2.1. As
mentioned above, words in WordNet are organised
into sets of synonyms, called synsets. Each synset
in turn belongs to a unique semantic file (SF). There
are a total of 45 SFs (1 for adverbs, 3 for adjectives,
15 for verbs, and 26 for nouns), based on syntactic
and semantic categories. A selection of SFs is pre-
sented in Table 1 for illustration purposes.
We experiment with both full synsets and SFs as
instances of fine-grained and coarse-grained seman-
tic representation, respectively. As an example of
the difference in these two representations, knife in
its tool sense is in the EDGE TOOL USED AS A CUT-
TING INSTRUMENT singleton synset, and also in the
ARTIFACT SF along with thousands of other words
including cutter. Note that these are the two ex-
tremes of semantic granularity in WordNet, and we
plan to experiment with intermediate representation
levels in future research (c.f. Li and Abe (1998), Mc-
Carthy and Carroll (2003), Xiong et al (2005), Fu-
jita et al (2007)).
As a hybrid representation, we tested the effect
of merging words with their corresponding SF (e.g.
knife+ARTIFACT ). This is a form of semantic spe-
cialisation rather than generalisation, and allows the
parser to discriminate between the different senses
of each word, but not generalise across words.
For each of these three semantic representations,
we experimented with substituting each of: (1) all
open-class POSs (nouns, verbs, adjectives and ad-
verbs), (2) nouns only, and (3) verbs only. There are
thus a total of 9 combinations of representation type
and target POS.
4.4 Disambiguation methods
For a given semantic representation, we need some
form of WSD to determine the semantics of each
token occurrence of a target word. We experimented
with three options:
1. Gold-standard: Gold-standard annotations
from SemCor. This gives us the upper bound
performance of the semantic representation.
SF ID DEFINITION
adj.all all adjective clusters
adj.pert relational adjectives (pertainyms)
adj.ppl participial adjectives
adv.all all adverbs
noun.act nouns denoting acts or actions
noun.animal nouns denoting animals
noun.artifact nouns denoting man-made objects
...
verb.consumption verbs of eating and drinking
verb.emotion verbs of feeling
verb.perception verbs of seeing, hearing, feeling
...
Table 1: A selection of WordNet SFs
2. First Sense (1ST): All token instances of a
given word are tagged with their most fre-
quent sense in WordNet.4 Note that the first
sense predictions are based largely on the same
dataset as we use in our evaluation, such that
the predictions are tuned to our dataset and not
fully unsupervised.
3. Automatic Sense Ranking (ASR): First sense
tagging as for First Sense above, except that an
unsupervised system is used to automatically
predict the most frequent sense for each word
based on an independent corpus. The method
we use to predict the first sense is that of Mc-
Carthy et al (2004), which was obtained us-
ing a thesaurus automatically created from the
British National Corpus (BNC) applying the
method of Lin (1998), coupled with WordNet-
based similarity measures. This method is fully
unsupervised and completely unreliant on any
annotations from our dataset.
In the case of SFs, we perform full synset WSD
based on one of the above options, and then map the
prediction onto the corresponding (unique) SF.
5 Results
We present the results for each disambiguation ap-
proach in turn, analysing the results for parsing and
PP attachment separately.
4There are some differences with the most frequent sense in
SemCor, due to extra corpora used in WordNet development,
and also changes in WordNet from the original version used for
the SemCor tagging.
321
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .857 .808 .832 .837 .845 .841
SF .855 .809 .831 .847? .854? .850?
SFn .860 .808 .833 .847? .853? .850?
SFv .861 .811 .835 .847? .856? .851?
word + SF .865? .814? .839? .837 .846 .842
word + SFn .862 .809 .835 .841? .850? .846?
word + SFv .862 .810 .835 .840 .851 .845
Syn .863? .812 .837 .845? .853? .849?
Synn .860 .807 .832 .841 .849 .845
Synv .863? .813? .837? .843? .851? .847?
Table 2: Parsing results with gold-standard senses (? in-
dicates that the recall or precision is significantly better
than baseline; the best performing method in each col-
umn is shown in bold)
5.1 Gold standard
We disambiguated each token instance in our cor-
pus according to the gold-standard sense data, and
trained both the Charniak and Bikel parsers over
each semantic representation. We evaluated the
parsers in full parsing and PP attachment contexts.
The results for parsing are given in Table 2. The
rows represent the three semantic representations
(including whether we substitute only nouns, only
verbs or all POS). We can see that in almost all
cases the semantically-enriched representations im-
prove over the baseline parsers. These results are
statistically significant in some cases (as indicated
by ?). The SFv representation produces the best re-
sults for Bikel (F-score 0.010 above baseline), while
for Charniak the best performance is obtained with
word+SF (F-score 0.007 above baseline). Compar-
ing the two baseline parsers, Bikel achieves better
precision and Charniak better recall. Overall, Bikel
obtains a superior F-score in all configurations.
The results for the PP attachment experiments us-
ing gold-standard senses are given in Table 3, both
for the Charniak and Bikel parsers. Again, the F-
score for the semantic representations is better than
the baseline in all cases. We see that the improve-
ment is significant for recall in most cases (particu-
larly when using verbs), but not for precision (only
Charniak over Synv and word+SFv for Bikel). For
both parsers the best results are achieved with SFv,
which was also the best configuration for parsing
with Bikel. The performance gain obtained here is
larger than in parsing, which is in accordance with
the findings of Stetina and Nagao that lexical se-
mantics has a considerable effect on PP attachment
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .667 .798 .727 .659 .820 .730
SF .710 .808 .756 .714? .809 .758
SFn .671 .792 .726 .706 .818 .758
SFv .729? .823 .773? .733? .827 .778?
word + SF .710? .801 .753 .706? .837 .766?
word + SFn .698? .813 .751 .706? .829 .763?
word + SFv .714? .805 .757? .706? .837? .766?
Syn .722? .814 .765? .702? .825 .758
Synn .678 .805 .736 .690 .822 .751
Synv .702? .817? .755? .690? .834 .755?
Table 3: PP attachment results with gold-standard senses
(? indicates that the recall or precision is significantly bet-
ter than baseline; the best performing method in each col-
umn is shown in bold)
performance. As in full-parsing, Bikel outperforms
Charniak, but in this case the difference in the base-
lines is not statistically significant.
5.2 First sense (1ST)
For this experiment, we use the first sense data from
WordNet for disambiguation. The results for full
parsing are given in Table 4. Again, the perfor-
mance is significantly better than baseline in most
cases, and surprisingly the results are even better
than gold-standard in some cases. We hypothesise
that this is due to the avoidance of excessive frag-
mentation, as occurs with fine-grained senses. The
results are significantly better for nouns, with SFn
performing best. Verbs seem to suffer from lack of
disambiguation precision, especially for Bikel. Here
again, Charniak trails behind Bikel.
The results for the PP attachment task are shown
in Table 5. The behaviour is slightly different here,
with Charniak obtaining better results than Bikel in
most cases. As was the case for parsing, the per-
formance with 1ST reaches and in many instances
surpasses gold-standard levels, achieving statistical
significance over the baseline in places. Compar-
ing the semantic representations, the best results are
achieved with SFv, as we saw in the gold-standard
PP-attachment case.
5.3 Automatic sense ranking (ASR)
The final option for WSD is automatic sense rank-
ing, which indicates how well our method performs
in a completely unsupervised setting.
The parsing results are given in Table 6. We can
see that the scores are very similar to those from
322
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .857 .807 .832 .837 .845 .841
SF .851 .804 .827 .843 .850 .846
SFn .863? .813 .837? .850? .854? .852?
SFv .857 .808 .832 .843 .853? .848
word + SF .859 .810 .834 .833 .841 .837
word + SFn .862? .811 .836 .844? .851? .848?
word + SFv .857 .808 .832 .831 .839 .835
Syn .857 .810 .833 .837 .844 .840
Synn .863? .812 .837? .844? .851? .848?
Synv .860 .810 .834 .836 .844 .840
Table 4: Parsing results with 1ST (? indicates that the
recall or precision is significantly better than baseline; the
best performing method in each column is shown in bold)
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .667 .798 .727 .659 .820 .730
SF .710 .808 .756 .702 .806 .751
SFn .671 .781 .722 .702 .829 .760
SFv .737? .836? .783? .718? .821 .766?
word + SF .706 .811 .755 .694 .823 .753
word + SFn .690 .815 .747 .667 .810 .731
word + SFv .714? .805 .757? .710? .819 .761?
Syn .725? .833? .776? .698 .828 .757
Synn .698 .828? .757? .667 .817 .734
Synv .722? .811 .763? .706? .818 .758?
Table 5: PP attachment results with 1ST (? indicates that
the recall or precision is significantly better than baseline;
the best performing method in each column is shown in
bold)
1ST, with improvements in some cases, particularly
for Charniak. Again, the results are better for nouns,
except for the case of SFv with Bikel. Bikel outper-
forms Charniak in terms of F-score in all cases.
The PP attachment results are given in Table 7.
The results are similar to 1ST, with significant im-
provements for verbs. In this case, synsets slightly
outperform SF. Charniak performs better than Bikel,
and the results for Synv are higher than the best ob-
tained using gold-standard senses.
6 Discussion
The results of the previous section show that the im-
provements in parsing results are small but signifi-
cant, for all three word sense disambiguation strate-
gies (gold-standard, 1ST and ASR). Table 8 sum-
marises the results, showing that the error reduction
rate (ERR) over the parsing F-score is up to 6.9%,
which is remarkable given the relatively superficial
strategy for incorporating sense information into the
parser. Note also that our baseline results for the
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .857 .807 .832 .837 .845 .841
SF .863 .815? .838 .845? .852 .849
SFn .862 .810 .835 .845? .850 .847?
SFv .859 .810 .833 .846? .856? .851?
word + SF .859 .810 .834 .836 .844 .840
word + SFn .865? .813? .838? .844? .852? .848?
word + SFv .856 .806 .830 .832 .839 .836
Syn .856 .807 .831 .840 .847 .843
Synn .864? .813? .838? .844? .851? .847?
Synv .857 .806 .831 .837 .845 .841
Table 6: Parsing results with ASR (? indicates that the
recall or precision is significantly better than baseline; the
best performing method in each column is shown in bold)
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .667 .798 .727 .659 .820 .730
SF .733? .824 .776? .698 .805 .748
SFn .682 .791 .733 .671 .807 .732
SFv .733? .813 .771? .710? .812 .757?
word + SF .714? .798 .754 .675 .800 .732
word + SFn .690 .807 .744 .659 .804 .724
word + SFv .706? .800 .750 .702? .814 .754?
Syn .733? .827 .778? .694 .805 .745
Synn .686 .810 .743 .667 .806 .730
Synv .714? .816 .762? .714? .816 .762?
Table 7: PP attachment results with ASR (? indicates that
the recall or precision is significantly better than baseline;
the best performance in each column is shown in bold)
dataset are almost the same as previous work pars-
ing the Brown corpus with similar models (Gildea,
2001), which suggests that our dataset is representa-
tive of this corpus.
The improvement in PP attachment was larger
(20.5% ERR), and also statistically significant. The
results for PP attachment are especially important,
as we demonstrate that the sense information has
high utility when embedded within a parser, where
the parser needs to first identify the ambiguity and
heads correctly. Note that Atterer and Schu?tze
(2007) have shown that the Bikel parser performs as
well as the state-of-the-art in PP attachment, which
suggests our method improves over the current state-
of-the-art. The fact that the improvement is larger
for PP attachment than for full parsing is suggestive
of PP attachment being a parsing subtask where lex-
ical semantic information is particularly important,
supporting the findings of Stetina and Nagao (1997)
over a standalone PP attachment task. We also ob-
served that while better PP-attachment usually im-
proves parsing, there is some small variation. This
323
WSD TASK PAR BASE SEM ERR BEST
Pars.
C .832 .839? 4.2% word+SF
Gold- B .841 .851? 6.3% SFv
standard
PP
C .727 .773? 16.9% SFv
B .730 .778? 17.8% SFv
Pars.
C .832 .837? 3.0% SFn, Synn
1ST
B .841 .852? 6.9% SFn
PP
C .727 .783? 20.5% SFv
B .730 .766? 13.3% SFv
Pars.
C .832 .838? 3.6% SF, word+SFn, Synn
ASR
B .841 .851? 6.3% SFv
PP
C .727 .778? 18.7% Syn
B .730 .762? 11.9% Synv
Table 8: Summary of F-score results with error reduc-
tion rates and the best semantic representation(s) for each
setting (C = Charniak, B = Bikel)
means that the best configuration for PP-attachment
does not always produce the best results for parsing
One surprising finding was the strong perfor-
mance of the automatic WSD systems, actually
outperforming the gold-standard annotation overall.
Our interpretation of this result is that the approach
of annotating all occurrences of the same word with
the same sense allows the model to avoid the data
sparseness associated with the gold-standard distinc-
tions, as well as supporting the merging of differ-
ent words into single semantic classes. While the
results for gold-standard senses were intended as
an upper bound for WordNet-based sense informa-
tion, in practice there was very little difference be-
tween gold-standard senses and automatic WSD in
all cases barring the Bikel parser and PP attachment.
Comparing the two parsers, Charniak performs
better than Bikel on PP attachment when automatic
WSD is used, while Bikel performs better on parsing
overall. Regarding the choice of WSD system, the
results for both approaches are very similar, show-
ing that ASR performs well, even if it does not re-
quire sense frequency information.
The analysis of performance according to the se-
mantic representation is not so clear cut. Gener-
alising only verbs to semantic files (SFv) was the
best option in most of the experiments, particularly
for PP-attachment. This could indicate that seman-
tic generalisation is particularly important for verbs,
more so than nouns.
Our hope is that this paper serves as the bridge-
head for a new line of research into the impact of
lexical semantics on parsing. Notably, more could
be done to fine-tune the semantic representation be-
tween the two extremes of full synsets and SFs.
One could also imagine that the appropriate level of
generalisation differs across POS and even the rel-
ative syntactic role, e.g. finer-grained semantics are
needed for the objects than subjects of verbs.
On the other hand, the parsing strategy is very
simple, as we just substitute words by their semantic
class and then train statistical parsers on the trans-
formed input. The semantic class should be an in-
formation source that the parsers take into account in
addition to analysing the actual words used. Tighter
integration of semantics into the parsing models,
possibly in the form of discriminative reranking
models (Collins and Koo, 2005; Charniak and John-
son, 2005; McClosky et al, 2006), is a promising
way forward in this regard.
7 Conclusions
In this work we have trained two state-of-the-art
statistical parsers on semantically-enriched input,
where content words have been substituted with
their semantic classes. This simple method allows
us to incorporate lexical semantic information into
the parser, without having to reimplement a full sta-
tistical parser. We tested the two parsers in both a
full parsing and a PP attachment context.
This paper shows that semantic classes achieve
significant improvement both on full parsing and
PP attachment tasks relative to the baseline parsers.
PP attachment achieves a 20.5% ERR, and parsing
6.9% without requiring hand-tagged data.
The results are highly significant in demonstrating
that a simplistic approach to incorporating lexical
semantics into a parser significantly improves parser
performance. As far as we know, these are the first
results over both WordNet and the Penn Treebank to
show that semantic processing helps parsing.
Acknowledgements
We wish to thank Diana McCarthy for providing us
with the sense rank for the target words. This work
was partially funded by the Education Ministry (project
KNOW TIN2006-15049), the Basque Government (IT-
397-07), and the Australian Research Council (grant no.
DP0663879). Eneko Agirre participated in this research
while visiting the University of Melbourne, based on joint
funding from the Basque Government and HCSNet.
324
References
Eneko Agirre and Philip Edmonds, editors. 2006. Word Sense
Disambiguation: Algorithms and Applications. Springer,
Dordrecht, Netherlands.
Michaela Atterer and Hinrich Schu?tze. 2007. Prepositional
phrase attachment without oracles. Computational Linguis-
tics, 33(4):469?476.
Daniel M. Bikel. 2000. A statistical model for parsing and
word-sense disambiguation. In Proc. of the Joint SIGDAT
Conference on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora (EMNLP/VLC-2000), pages
155?63, Hong Kong, China.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In Proc.
of the 43rd Annual Meeting of the ACL, pages 173?80, Ann
Arbor, USA.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In Proc. of the 15th Annual
Conference on Artificial Intelligence (AAAI-97), pages 598?
603, Stanford, USA.
Eugene Charniak. 2000. A maximum entropy-based parser.
In Proc. of the 1st Annual Meeting of the North Ameri-
can Chapter of Association for Computational Linguistics
(NAACL2000), Seattle, USA.
David Chiang and David M. Bikel. 2002. Recovering latent
information in treebanks. In Proc. of the 19th International
Conference on Computational Linguistics (COLING 2002),
pages 183?9, Taipei, Taiwan.
Michael Collins and Terry Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational Linguistics,
31(1):25?69.
Michael J. Collins. 1996. A new statistical parser based on
lexical dependencies. In Proc. of the 34th Annual Meeting
of the ACL, pages 184?91, Santa Cruz, USA.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):589?637.
John Dowding, Robert Moore, Franc?ois Andry, and Douglas
Moran. 1994. Interleaving syntax and semantics in an effi-
cient bottom-up parser. In Proc. of the 32nd Annual Meeting
of the ACL, pages 110?6, Las Cruces, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki
Tanaka. 2007. Exploiting semantic information for HPSG
parse selection. In Proc. of the ACL 2007 Workshop on Deep
Linguistic Processing, pages 25?32, Prague, Czech Repub-
lic.
Daniel Gildea. 2001. Corpus variation and parser performance.
In Proc. of the 6th Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2001), pages 167?202,
Pittsburgh, USA.
Erik Hektoen. 1997. Probabilistic parse selection based
on semantic cooccurrences. In Proc. of the 5th Inter-
national Workshop on Parsing Technologies (IWPT-1997),
pages 113?122, Boston, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proc. of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume: Short
Papers, pages 57?60, New York City, USA.
Shari Landes, Claudia Leacock, and Randee I. Tengi. 1998.
Building semantic concordances. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database. MIT
Press, Cambridge, USA.
Hang Li and Naoki Abe. 1998. Generalising case frames using
a thesaurus and the MDL principle. Computational Linguis-
tics, 24(2):217?44.
Dekang Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of the 36th Annual Meeting of the
ACL and 17th International Conference on Computational
Linguistics: COLING/ACL-98, pages 768?774, Montreal,
Canada.
David M. Magerman. 1995. Statistical decision-tree models
for parsing. In Proc. of the 33rd Annual Meeting of the ACL,
pages 276?83, Cambridge, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguistics,
19(2):313?30.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computational Linguistics,
29(4):639?654.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004. Finding predominant senses in untagged text. In
Proc. of the 42nd Annual Meeting of the ACL, pages 280?
7, Barcelona, Spain.
David McClosky, Eugene Charniak, and Mark Johnson. 2006.
Effective self-training for parsing. In Proc. of the Hu-
man Language Technology Conference of the NAACL
(NAACL2006), pages 152?159, New York City, USA.
Brian Mitchell. 2004. Prepositional Phrase Attachment using
Machine Learning Algorithms. Ph.D. thesis, University of
Sheffield.
Martha Palmer, Hoa Dang, and Christiane Fellbaum. 2006.
Making fine-grained and coarse-grained sense distinctions,
both manually and automatically. Natural Language Engi-
neering, 13(2):137?63.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994.
A maximum entropy model for prepositional phrase attach-
ment. In HLT ?94: Proceedings of the Workshop on Human
Language Technology, pages 250?255, Plainsboro, USA.
Philip Resnik. 2006. WSD in NLP applications. In Eneko
Agirre and Philip Edmonds, editors, Word Sense Disam-
biguation: Algorithms and Applications, chapter 11, pages
303?40. Springer, Dordrecht, Netherlands.
Jiri Stetina and Makoto Nagao. 1997. Corpus based PP attach-
ment ambiguity resolution with a semantic dictionary. In
Proc. of the 5th Annual Workshop on Very Large Corpora,
pages 66?80, Hong Kong, China.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese Tree-
bank with semantic knowledge. In Proc. of the 2nd Inter-
national Joint Conference on Natural Language Processing
(IJCNLP-05), pages 70?81, Jeju Island, Korea.
325
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 237?240,
Prague, June 2007. c?2007 Association for Computational Linguistics
MELB-MKB: Lexical Substitution System based on Relatives in Context
David Martinez, Su Nam Kim and Timothy Baldwin
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
{davidm,snkim,tim}@csse.unimelb.edu.au
Abstract
In this paper we describe the MELB-MKB
system, as entered in the SemEval-2007 lex-
ical substitution task. The core of our sys-
tem was the ?Relatives in Context? unsuper-
vised approach, which ranked the candidate
substitutes by web-lookup of the word se-
quences built combining the target context
and each substitute. Our system ranked third
in the final evaluation, performing close to
the top-ranked system.
1 Introduction
This paper describes the system we developed for
the SemEval lexical substitution task, a new task in
SemEval-2007. Although we tested different con-
figurations on the trial data, our basic system relied
on WordNet relatives (Fellbaum, 1998) and Google
queries in order to identify the most plausible sub-
stitutes in the context.
The main goal when building our system was to
study the following factors: (i) substitution candi-
date set, (ii) settings of the relative-based algorithm,
and (iii) syntactic filtering. We analysed these fac-
tors over the trial data provided by the organisation,
and used the BEST metric to tune our system. This
metric accepts multiple answers, and averages the
score across the answers. We did not experiment
with the OOT (top 10 answers) and MULTIWORD
metrics.
In the remainder of this paper we briefly intro-
duce the basic Relatives in Context algorithm in Sec-
tion 2. Next we describe our experiments on the trial
data in Section 3. Our final system and its results are
described in Section 4. Finally, our conclusions are
outlined in Section 5.
2 Algorithm
Our basic algorithm is an unsupervised method pre-
sented in Martinez et al (2006). This technique
makes use of the WordNet relatives of the target
word for disambiguation, by way of the following
steps: (i) obtain a set of close relatives from Word-
Net for each sense of the target word; (ii) for each
test instance define all possible word sequences that
include the target word; (iii) for each word sequence,
substitute the target word with each relative, and
then query Google; (iv) rank queries according to
the following factors: length of the query, distance
of the relative to the target word, and number of hits;
and (v) select the relative from the highest ranked
query.1
For the querying step, first we tokenise each tar-
get sentence, and then we apply sliding windows of
different sizes (up to 6 tokens) that include the tar-
get word. For each window and each relative in the
pool, we substitute the target word for the relative,
and query Google. The algorithm stops augment-
ing the window for the relative when one of its sub-
strings returns zero hits. The length of the query is
measured as the number of words, and the distance
of the relative to the target words gives preference
to synonyms over hypernyms, and immediate hyper-
nyms over further ones.
One important parameter in this method is the
candidate set. We performed different experiments
to measure the expected score we could achieve
1In the case of WSD we would use the relative to chose the
sense it relates to.
237
from WordNet relatives, and the contribution of dif-
ferent types of filters (syntactic, frequency-based,
etc.) to the overall result. We also explored other
settings of the algorithm, such as the ranking crite-
ria, and the number of answers to return. These ex-
periments and some other modifications of the basic
algorithm are covered in Section 3.
3 Development on Trial data
In this section we analyse the coverage of WordNet
over the data, the basic parameter exploration pro-
cess, a syntactic filter, and finally the extra experi-
ments we carried out before submission. The trial
data consisted on 300 instances of 34 words with
gold-standard annotations.
3.1 WordNet coverage
The most obvious resource for selecting substitution
candidates was WordNet, due to its size and avail-
ability. We used version 2.0 throughout this work.
In our first experiment, we tried to determine which
kind of relationships to use, and the coverage of the
gold-standard annotations that we could expect from
WordNet relations only. As a basic set of relations,
we used the following: SYNONYMY, SIMILAR-TO,
ENTAILMENT, CAUSE, ALSO-SEE, and INSTANCE.
We created two extended candidate sets using im-
mediate and 2-step hypernyms (hype and hype2, re-
spectively, in Table 1).
Given that we are committed to using Word-
Net, we set out to measure the percentage of gold-
standard substitutes that were ?reachable? using dif-
ferent WordNet relations. Table 1 shows the cov-
erage for the three sets of candidates. Instance-
coverage indicates the percentage of instances that
have at least one of the gold-standard instances cov-
ered from the candidate set. We can see that the per-
centage is surprisingly low.
Any shortcoming in coverage will have a direct
impact on performance, suggesting the need for al-
ternate means to obtain substitution candidates. One
possibility is to extend the candidates from Word-
Net by following links from the relatives (e.g. col-
lect all synonyms of the synonymous words), but
this could add many noisy candidates. We can also
use other lexical repositories built by hand or auto-
matically, such as the distributional theusauri built
Candidate Set Subs. Cov. Inst. Cov.
basic 344/1152 (30%) 197 / 300 (66%)
hype 404/1152 (35%) 229/300 (76%)
hype2 419/1152 (36%) 229/300 (76%)
Table 1: WordNet coverage for different candidate
sets, based on substitute (Subs.) and instance (Inst.)
coverage.
in Lin (1998). A different approach that we are test-
ing for future work is to adapt the algorithm to work
with wildcards instead of explicit candidates. Due to
time constraints, we only relied on WordNet for our
submission.
3.2 Parameter Tuning
In this experiment we tuned different parameters of
the basic algorithm. First, we observed the data in
order to identify the most relevant variables for this
task. We tried to avoid including too many parame-
ters and overfitting the system to the trial dataset. At
this point, we separated the instances by PoS, and
studied the following parameters:
Candidate set: From WordNet, we tested four
possible datasets for each target word: basic-set, 1st-
sense (basic relations from the first sense only), hype
(basic set and immediate hypernyms), and hype2
(basic set and up to two-step hypernyms).
Semcor-based filters: Semcor provides frequency
information for WordNet senses, and can be used
to identify rare senses. As each candidate is ob-
tained via WordNet semantic relations with the tar-
get word, we can filter out those candidates that are
related with unfrequent senses in Semcor. We tested
three configurations: (1) no filter, (2) filter out candi-
dates when the candidate-sense in the relation does
not occur in Semcor, (3) and filter out candidates
when the target-sense in the relation does not oc-
cur in Semcor. The filters can potentially lead to the
removal of all candidates, in which case a back-off
is applied (see below).
Relative-ranking criteria: Our algorithm ranks
relatives according to the length in words of their
context-match. In the case of ties, the number of re-
turned hits from Google is applied. The length can
be different depending on whether we count punc-
tuation marks as separate tokens, and whether the
word-length of substitute multiwords is included.
238
We tested three options: including the target word,
not including the target word (multiwords count as a
single word), and not counting punctuation marks.
Back-off: We need a back-off method in case the
basic algorithm does not find any matches. We
tested the following: sense-ordered synonyms from
WordNet (highest sense first, randomly breaking
ties), and most frequent synonyms from the first sys-
tem (using two corpora: Semcor and BNC).
Number of answers: We also measured the per-
formance for different numbers of system outputs
(1, 2, or 3).
All in all, we performed 324 (4x3x3x3x3) runs
for each PoS, based on the different combinations.
The best scores for each PoS are shown in Table 2,
together with the baselines. We can see that the pre-
cision is above the official WordNet baseline, but is
still very low. The results illustrate the difficulty of
the task. In error analysis, we observed that the per-
formance and settings varied greatly depending on
the PoS of the target word. Adverbs produced the
best performance, followed by nouns. The scores
were very low for adjectives and verbs (the baseline
score for verbs was only 2%).
We will now explain the main conclusions ex-
tracted from the parameter analysis. Regarding the
candidate set, we observed that using synonyms only
was the best approach for all PoS, except for verbs,
where hypernyms helped. The option of limiting the
candidates to the first sense only helped for adjec-
tives, but not for other PoS.
For the Semcor-based filter, our results showed
that the target-sense filter improved the performance
for verbs and adverbs. For nouns and adjectives, the
candidate-sense filter worked best. All in all, apply-
ing the Semcor filters was effective in removing rare
senses and improving performance.
The length criteria did not affect the results signif-
icantly, and only made a difference in some extreme
cases. Not counting the length of the target word
helped slightly for nouns and adverbs, and removing
punctuation improved results for adjectives. Regard-
ing the back-off method, we observed that the count
of frequencies in Semcor was the best approach for
all PoS except verbs, which reached their best per-
formance with BNC frequencies.
PoS Relatives in Context WordNet Baseline
Nouns 18.4 14.9
Verbs 6.7 2.0
Adjectives 9.6 7.5
Adverbs 31.1 29.9
Overall 14.4 10.4
Table 2: Experiments to tune parameters on the trial
data, based on the BEST metric. Scores correspond
to precision (which is the same as recall).
Finally, we observed that the performance for the
BEST score decreased significantly when more than
one answer was returned, probably due to the diffi-
culty of the task.
3.3 Syntactic Filter
After the basic parameter analysis, we studied the
contribution of a syntactic filter to remove those can-
didates that, when substituted, generate an ungram-
matical sentence. Intuitively, we would expect this
to have a high impact for verbs, which vary consid-
erably in their subcategorisation properties. For ex-
ample, in the case of the (reduced) target If we order
our lives well ..., the syntactic filter should ideally
disallow candidates such as If we range our lives
well ...
In order to apply this filter, we require a parser
which has an explicit notion of grammaticality, rul-
ing out the standard treebank parsers. We experi-
mented briefly with RASP, but found that the En-
glish Resource Grammar (ERG: Flickinger (2002)),
combined with the PET run-time engine, was the
best fit for out needs. Unfortunately we could not get
unknown word handling working within the ERG
for our submission, such that we get a meaningful
output for a given input string only in the case that
the ERG has full lexical coverage over that string
(we will never get a spanning parse for an input
where we are missing lexical entries). As such, the
syntactic filter is limited in coverage only to strings
where the ERG has lexical coverage.
Ideally, we would have tested this filter on trial
data, but unfortunately we ran out of time. Thus, we
simply eyeballed a sample of examples, and we de-
cided to include this filter in our final submission. As
we will see in Section 4, its effect was minimal. We
plan to perform a complete evaluation of this module
in the near future.
239
3.4 Extra experiments
One of the limitations of the ?Relatives in Context?
algorithm is that it only relies on the local con-
text. We wanted to explore the contribution of other
words in the context for the task, and we performed
an experiment including the Topical Signatures re-
source (Agirre and Lopez de Lacalle, 2004). We
simply counted the overlapping of words shared be-
tween the context and the different candidates. We
only tested this for nouns, for which the results were
below baseline. We then tried to integrate the topic-
signature scores with the ?Relatives in Context? al-
gorithm, but we did not improve our basic system?s
results on the trial data. Thus, this approach was not
included in our final submission.
Another problem we observed in error analysis
was that the Semcor-based filters were too strict in
some cases, and it was desirable to have a way of
penalising low frequency senses without removing
them completely. Thus, we weighted senses by the
inverse of their sense-rank. As we did not have time
to test this intuition properly, we opted for applying
the sense-weighting only when the candidates had
the same context-match length, instead of using the
number of hits. We will see the effect of this method
in the next section.
4 Final system
The test data consisted of 1,710 instances. For our
final system we applied the best configuration for
each PoS as observed in the development experi-
ments, and the syntactic filter. We also incorpo-
rated the sense-weighting to solve ties. The results
of our system, the best competing system, and the
best baseline (WordNet) are shown in Table 3 for the
BEST metric. Precision and recall are provided for
all the instances, and also for the ?Mode? instances
(those that have a single preferred candidate).
Our method outperforms the baseline in all cases,
and performs very close to the top system, ranking
third out of eight systems. This result is consistent
in the ?further analysis? tables provided by the task
organisers for subsets of data, where our system al-
ways performs close to the top score. The overall
scores are below 13% recall for all systems when
targeting all instances. This illustrates the difficulty
of the task, and the similarity of the top-3 scores sug-
All instances Mode
System P R P R
Best 12.90 12.90 20.65 20.65
Relat. in Context 12.68 12.68 20.41 20.41
WordNet baseline 9.95 9.95 15.28 15.28
Table 3: Official results based on the BEST metric.
gests that similar resources (i.e. WordNet) have been
used in the development of the systems.
After the release of the gold-standard data, we
tested two extra settings to measure the effect of the
syntactic filter and the sense-weighting in the final
score. We observed that our application of the syn-
tactic filter had almost no effect in the performance,
but sense-weighting increased the overall recall by
0.4% (from 12.3% to 12.7%).
5 Conclusions
Although the task was difficult and the scores were
low, we showed that by using WordNet and the lo-
cal context we are able to outperform the baselines
and achieve close to top performance. For future
work, we would like to integrate a parser with un-
known word handling in our system. We also aim to
adapt the algorithm to match the target context with
wildcards, in order to avoid explicitly defining the
candidate set.
Acknowledgments
This research was carried out with support from Australian Re-
search Council grant no. DP0663879.
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly avail-
able topic signatures for all WordNet nominal senses. In
Proc. of the 4rd International Conference on Languages Re-
sources and Evaluations (LREC 2004), pages 1123?6, Lis-
bon, Portugal.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Dan Flickinger. 2002. On building a more efficient grammar by
exploiting types. In Stephan Oepen, Dan Flickinger, Jun?ichi
Tsujii, and Hans Uszkoreit, editors, Collaborative Language
Engineering. CSLI Publications, Stanford, USA.
Dekang Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL, pages 768?74,
Montreal, Canada.
David Martinez, Eneko Agirre, and Xinglong Wang. 2006.
Word relatives in context for word sense disambiguation. In
Proc. of the 2006 Australasian Language Technology Work-
shop, pages 42?50, Sydney, Australia.
240
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 350?353,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-UMB: Combining unsupervised and supervised systems for all-words
WSD
David Martinez,Timothy Baldwin
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
{davidm,tim}@csse.unimelb.edu.au
Eneko Agirre, Oier Lopez de Lacalle
IXA NLP Group
Univ. of the Basque Country
Donostia, Basque Country
{e.agirre,jibloleo}@ehu.es
Abstract
This paper describes the joint submission
of two systems to the all-words WSD sub-
task of SemEval-2007 task 17. The main
goal of this work was to build a competitive
unsupervised system by combining hetero-
geneous algorithms. As a secondary goal,
we explored the integration of unsupervised
predictions into a supervised system by dif-
ferent means.
1 Introduction
This paper describes the joint submission of two sys-
tems to the all-words WSD subtask of SemEval-
2007 task 17. The systems were developed by the
University of the Basque Country (UBC), and the
University of Melbourne (UMB). The main goal of
this work was to build a competitive unsupervised
system by combining heterogeneous algorithms. As
a secondary goal, we explored the integration of
this method into a supervised system by different
means. Thus, this paper describes both the unsu-
pervised system (UBC-UMB-1), and the combined
supervised system (UBC-UMB-2) submitted to the
all-words task.
Our motivation in building unsupervised systems
comes from the difficulty of creating hand-tagged
data for all words and all languages, which is col-
loquially known as the knowledge acquisition bot-
tleneck. There have also been promising results in
recent work on the combination of unsupervised ap-
proaches that suggest the gap with respect to super-
vised systems is narrowing (Brody et al, 2006).
The remainder of the paper is organized as fol-
lows. First we describe the disambiguation algo-
rithms in Section 2. Next, the development exper-
iments are presented in Section 3, and our final sub-
missions and results in Section 4. Finally, we sum-
marize our conclusions in Section 5.
2 Algorithms
In this section, we will describe the standalone algo-
rithms (three unsupervised and one supervised) and
the combination schemes we explored. The unsu-
pervised methods are based on different intuitions
for disambiguation (topical features, local context,
and WordNet relations), which is a desirable charac-
teristic for combining algorithms.
2.1 Topic Signatures (TS)
Topic signatures (Agirre and de Lacalle, 2004) are
lists of words related to a particular sense. They can
be built from a variety of sources, and be used di-
rectly to perform WSD. Cuadros and Rigau (2006)
present a detailed evaluation of topic signatures built
from a variety of knowledge sources. In this work
we built those coming from the following:
? the relations in the Multilingual Central Repos-
itory (TS-MCR)
? the relations in the Extended WordNet (TS-
XWN)
In order to apply this resource for WSD, we sim-
ply measured the word-overlap between the target
context and each of the senses of the target word.
The sense with highest overlap is chosen as the cor-
rect sense.
350
2.2 Relatives in Context (RIC)
This is an unsupervised method presented in Mar-
tinez et al (2006). This algorithm makes use of
the WordNet relatives of the target word for disam-
biguation. The process is carried out in these steps:
(i) obtain a set of close relatives from WordNet for
each sense (the relatives can be polysemous); (ii) for
each test instance define all possible word sequences
that include the target word; (iii) for each word se-
quence, substitute the target word with each relative
and query a web search engine; (iv) rank queries ac-
cording to the following factors: length of the query,
distance of the relative to the target word, and num-
ber of hits; and (v) select the sense associated with
the highest ranked query.
The intuition behind this system is that we can
find related words that can be substituted for the tar-
get word in a given context, which are indicative of
its sense. The close relatives that can form more
common phrases from the target context determine
the target sense.
2.3 Relative Number (RNB)
This heuristic has been motivated as a way of identi-
fying rare senses of a word. An important disadvan-
tage of unsupervised systems is that rare senses can
be over-represented in the models, while supervised
systems are able to discard them because they have
access to token-level word sense distributions.
This simple algorithm relies on the number of
close relatives found in WordNet for each sense of
the word. The senses are ranked according to the
number of synonyms, direct hypernyms, and di-
rect hyponyms they have in WordNet. The highest
ranked sense is taken to be the most important for the
target word, and all occurrences of the target word
are tagged with that sense.
2.4 k-Nearest Neighbours (kNN)
As our supervised system, we relied on kNN. This is
a memory-based learning method where the neigh-
bours are the k most similar contexts, represented by
feature vectors (~ci) of the test vector (~f ). The sim-
ilarity among instances is measured by the cosine
of their vectors. The test instance is labeled with the
sense that obtains the maximum sum of the weighted
votes of the k most similar contexts. Each vote is
weighted depending on its (neighbour) position in
the ordered rank, with the closest being first. Equa-
tion 1 formalizes kNN, where Ci corresponds to the
sense label of the i-th closest neighbour.
arg max
Sj
=
k
?
i=1
{
1
i if Ci = Sj
0 otherwise (1)
The UBC group used a combination of kNN clas-
sifiers trained over a large set of features, and en-
hanced this method using Singular Value Decompo-
sition (SVD) for their supervised submission (UBC-
ALM) to the lexical-sample and all-words subtasks
(Agirre and Lopez de Lacalle, 2007). However, we
only used the basic implementation in this work, due
to time constraints.
2.5 Combination of systems
We explored two approaches to combine the stan-
dalone systems. The first consisted simply of adding
up the normalized weights that each system would
give to each sense. We tested this voting approach
both for the unsupervised and supervised settings.
The second method could only be applied in com-
bination with the supervised kNN system. The
idea was to include the unsupervised predictions as
weighted features for the supervised system. We re-
fer to this method as ?stacking?, and it has been pre-
viously used to integrate heterogeneous knowledge
sources for WSD (Stevenson and Wilks, 2001).
3 Development experiments
We tested the single algorithms and their combina-
tion over both Semcor and the training distribution
of the SemEval-2007 lexical-sample subtask of task
17 (S07LS for short). The goal of these experiments
was to obtain an estimate of the expected perfor-
mance, and submit the most promising configura-
tion. We present first the tests on the unsupervised
setting, and then the supervised setting. It is im-
portant to note that the hand-tagged corpora was not
used to fine-tune the parameters of the unsupervised
algorithms.
3.1 Unsupervised systems
For the first evaluation of our unsupervised systems,
we relied on Semcor, and tagged 43,063 instances
of the 329 word types occurring in SemEval-2007
351
System Recall
RNB 30.6
TS-MCR 57.5
TS-XWN 47.0
TS-MCR & TS-XWN 57.3
RBN & TS-MCR & TS-XWN 53.6
Table 1: Evaluation of standalone and combined
unsupervised systems over 43,063 instances from
Semcor
System Recall
TS-MCR 60.1
TS-XWN 54.3
TS-MCR & TS-XWN 61.1
TS-MCR & TS-XWN & RIC* 61.2
Table 2: Evaluation of standalone and combined
unsupervised systems over 8,518 instances from
S07LS training
all-words. Due to time constraints, we were not able
to test the RIC algorithm on this dataset. The re-
sults are shown in Table 1. We can see that the RNB
heuristic performs poorly, and that the best configu-
ration consists of applying the single TS-MCR algo-
rithm. From this experiment, we decided to remove
the RNB heuristic and focus on the topic signatures
and RIC.
We also used S07LS for extra experiments in
the unsupervised setting. From the training part of
the S07LS dataset, we extracted 8,518 instances of
words also occurring in SemEval-2007 all-words.
As S07LS used senses from OntoNotes, we relied
on the mapping provided by the task organisers to
link them to WordNet senses. We left RNB out of
this experiment due to its low performance in Sem-
cor, and regarding RIC, we only evaluated a sample
of 68 instances. Results are shown in Table 2. The
best scores are achieved when combining both sets
of topic signatures. The few cases that have been
disambiguated with RIC improve the overall perfor-
mance slightly.
3.2 Combined system
We could not rely on Semcor in the supervised set-
ting (we used it for training), and therefore tried to
use as much data as possible from the training com-
ponent of S07LS, wherein all the instances avail-
able (22,281) were disambiguated. We tested first
System Recall
kNN 87.4
kNN & TS-MCR 86.8
kNN & TS-XWN 86.4
kNN & TS-MCR & TS-XWN 86.0
Table 3: Evaluation of voting supervised systems in
22,281 instances from S07LS training
System Recall
kNN 71.7
kNN & TS-MCR & TS-XWN 71.8
Table 4: Evaluation of ?stacking? the unsupervised
systems on kNN over 8,518 instances from S07LS
training
the voting combination by adding the normalized
weights from the output of each system. Due to
time constraints we only evaluated the combination
of kNN with TS-MCR and TS-XWN. Results are
shown in Table 3, where we can see that combin-
ing the unsupervised systems with voting hurts the
performance of the kNN method.
Finally, we applied the second combination ap-
proach, consisting of including the predictions of the
unsupervised systems as features for kNN (?stack-
ing?). We performed this experiment on the training
part of S07LS, but only for the 8,518 instances of
the words occurring on the all-words dataset. The
results of this experiment are given in Table 4. We
observed a slight improvement in this case.
4 Final systems
For our final submissions, we chose the combination
?TS-MCR& TS-XWN&RIC? for the unsupervised
system (UBC-UMB-1), and the combination ?kNN
& TS-MCR & TS-XWN? via ?stacking? for our su-
pervised system (UBC-UMB-2). The results of all
the systems are given in Table 5.
We can see that our unsupervised system ranked
10th. Unfortunately, we do not know at the time of
writing which other systems are unsupervised, and
therefore are unable to compare to other unsuper-
vised systems.
Our ?stacking? supervised system performs
slightly lower than the kNN supervised systems by
UBC-ALM (which ranks 7th), showing that our sys-
tem was not able to profit from information from
352
System Precision Recall
1. 0.537 0.537
2. 0.527 0.527
3. 0.524 0.524
4. 0.522 0.486
5. 0.518 0.518
6. 0.514 0.514
7. 0.493 0.492
8. UBC-UMB-2 0.485 0.484
9. 0.420 0.420
10. UBC-UMB-1 0.362 0.362
11. 0.355 0.355
12. 0.337 0.337
13. 0.298 0.298
14. 0.120 0.118
Table 5: Official results for all systems in task #17
of SemEval-2007. Our systems are shown in bold.
UBC-UMB-1 stands for TS-MCR & TS-XWN &
RIC, and UBC-UMB-2 for kNN & TS-MCR & TS-
XWN.
System Precision Recall
TS-MCR 36.7 36.5
TS-XWN 33.1 32.9
RIC 30.6 30.4
TS-MCR & TS-XWN 37.5 37.3
TS-MCR & TS-XWN & RIC 36.2 36.2
Table 6: Our unsupervised systems in the SemEval-
2007 all words test data
the unsupervised systems. However, we cannot at-
tribute the decrease only to the unsupervised fea-
tures, as the kNN implementations were different
(UBC-ALM relied on SVD).
After the gold-standard data was released, we
were able to test the contribution of each of the un-
supervised systems in the ensemble, as well as two
additional combinations. The results are given in
Table 6. We can see that TS-MCR is the best per-
forming method, confirming our development ex-
periments (cf. Tables 1 and 2). In contrast, in-
cluding RIC decreased the performance by 0.7 per-
cent points, and had we used only TS-MCR and TS-
XWN our results would have been better.
5 Conclusions
In this submission we combined heterogeneous un-
supervised algorithms to obtain competitive perfor-
mance without relying on training data. However,
due to time constraints, we were only able to submit
a preliminary system, and some of the unsupervised
methods were not properly developed and tested.
For future work we plan to properly test these
methods, and deploy other unsupervised algorithms.
We also plan to explore more sophisticated combina-
tion strategies, using meta-learning to try to predict
which features of each word make a certain WSD
system succeed (or fail).
Acknowledgements
The first and second authors were supported by Aus-
tralian Research Council grant no. DP0663879. We
want to thank German Rigau from the University of
the Basque Country for kindly providing access to
the MCR.
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of the 4rd International
Conference on Language Resources and Evaluations
(LREC), pages 1123?6, Lisbon, Portugal.
Eneko Agirre and Oier Lopez de Lacalle. 2007. UBC-
ALM: Lexical-Sample and All-Words tasks. In
Proceedings of SemEval-2007 (forthcoming), Prague,
Czech Republic.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised WSD. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the ACL, pages 97?104, Sydney, Australia.
Montse Cuadros and German Rigau. 2006. Quality as-
sessment of large scale knowledge resources. In Pro-
ceedings of the International Conference on Empirical
Methods in Natural Language Processing (EMNLP-
06), pages 534?41, Sydney, Australia.
David Martinez, Eneko Agirre, and Xinglong Wang.
2006. Word relatives in context for word sense dis-
ambiguation. In Proceedings of the 2006 Australasian
Language Technology Workshop, pages 42?50, Syd-
ney, Australia.
Mark Stevenson and YorickWilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?49.
353
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 80?87,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Knowledge Sources for Word Sense
Disambiguation of Biomedical Text
Mark Stevenson, Yikun Guo
and Robert Gaizauskas
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield, S1 4DP
United Kingdom
{inital.surname}@dcs.shef.ac.uk
David Martinez
Department of Computer Science
& Software Engineering
University of Melbourne
Victoria 3010
Australia
davidm@csse.unimelb.edu.au
Abstract
Like text in other domains, biomedical doc-
uments contain a range of terms with more
than one possible meaning. These ambigu-
ities form a significant obstacle to the auto-
matic processing of biomedical texts. Previ-
ous approaches to resolving this problem have
made use of a variety of knowledge sources in-
cluding linguistic information (from the con-
text in which the ambiguous term is used) and
domain-specific resources (such as UMLS). In
this paper we compare a range of knowledge
sources which have been previously used and
introduce a novel one: MeSH terms. The best
performance is obtained using linguistic fea-
tures in combination with MeSH terms. Re-
sults from our system outperform published
results for previously reported systems on a
standard test set (the NLM-WSD corpus).
1 Introduction
The number of documents discussing biomedical
science is growing at an ever increasing rate, making
it difficult to keep track of recent developments. Au-
tomated methods for cataloging, searching and nav-
igating these documents would be of great benefit
to researchers working in this area, as well as hav-
ing potential benefits to medicine and other branches
of science. Lexical ambiguity, the linguistic phe-
nomena where a word or phrase has more than
one potential meaning, makes the automatic pro-
cessing of text difficult. For example, ?cold? has
six possible meanings in the Unified Medical Lan-
guage System (UMLS) Metathesaurus (Humphreys
et al, 1998) including ?common cold?, ?cold sen-
sation? and ?Chronic Obstructive Airway Disease
(COLD)?. The NLM Indexing Initiative (Aronson et
al., 2000) attempted to automatically index biomedi-
cal journals with concepts from the UMLS Metathe-
saurus and concluded that lexical ambiguity was the
biggest challenge in the automation of the indexing
process. Weeber et al (2001) analysed MEDLINE
abstracts and found that 11.7% of phrases were am-
biguous relative to the UMLS Metathesaurus.
Word Sense Disambiguation (WSD) is the pro-
cess of resolving lexical ambiguities. Previous re-
searchers have used a variety of approaches for
WSD of biomedical text. Some of them have taken
techniques proven to be effective for WSD of gen-
eral text and applied them to ambiguities in the
biomedical domain, while others have created sys-
tems using domain-specific biomedical resources.
However, there has been no direct comparison of
which knowledge sources are the most useful or
whether combining a variety of knowledge sources,
a strategy which has been shown to be successful for
WSD in the general domain (Stevenson and Wilks,
2001), improves results.
This paper compares the effectiveness of a vari-
ety of knowledge sources for WSD in the biomed-
ical domain. These include features which have
been commonly used for WSD of general text as
well as information derived from domain-specific
resources. One of these features is MeSH terms,
which we find to be particularly effective when com-
bined with generic features.
The next section provides an overview of various
approaches to WSD in the biomedical domain. Sec-
80
tion 3 outlines our approach, paying particular atten-
tion to the range of knowledge sources used by our
system. An evaluation of this system is presented
in Section 4. Section 5 summarises this paper and
provides suggestions for future work.
2 Previous Work
WSD has been actively researched since the 1950s
and is regarded as an important part of the process
of understanding natural language texts.
2.1 The NLM-WSD data set
Research on WSD for general text in the last decade
has been driven by the SemEval evaluation frame-
works1 which provide a set of standard evaluation
materials for a variety of semantic evaluation tasks.
At this point there is no specific collection for the
biomedical domain in SemEval, but a test collection
for WSD in biomedicine was developed by Wee-
ber et al (2001), and has been used as a benchmark
by many independent groups. The UMLS Metathe-
saurus was used to provide a set of possible mean-
ings for terms in biomedical text. 50 ambiguous
terms which occur frequently in MEDLINE were
chosen for inclusion in the test set. 100 instances
of each term were selected from citations added to
the MEDLINE database in 1998 and manually dis-
ambiguated by 11 annotators. Twelve terms were
flagged as ?problematic? due to substantial disagree-
ment between the annotators. There are an average
of 2.64 possible meanings per ambiguous term and
the most ambiguous term, ?cold? has five possible
meanings. In addition to the meanings defined in
UMLS, annotators had the option of assigning a spe-
cial tag (?none?) when none of the UMLS meanings
seemed appropriate.
Various researchers have chosen to evaluate their
systems against subsets of this data set. Liu et al
(2004) excluded the 12 terms identified as problem-
atic by Weeber et al (2001) in addition to 16 for
which the majority (most frequent) sense accounted
for more than 90% of the instances, leaving 22 terms
against which their system was evaluated. Leroy and
Rindflesch (2005) used a set of 15 terms for which
the majority sense accounted for less than 65% of
the instances. Joshi et al (2005) evaluated against
1http://www.senseval.org
the set union of those two sets, providing 28 am-
biguous terms. McInnes et al (2007) used the set
intersection of the two sets (dubbed the ?common
subset?) which contained 9 terms. The terms which
form these various subsets are shown in Figure 1.
The 50 terms which form the NLM-WSD data set
represent a range of challenges for WSD systems.
The Most Frequent Sense (MFS) heuristic has be-
come a standard baseline in WSD (McCarthy et al,
2004) and is simply the accuracy which would be
obtained by assigning the most common meaning of
a term to all of its instances in a corpus. Despite its
simplicity, the MFS heuristic is a hard baseline to
beat, particularly for unsupervised systems, because
it uses hand-tagged data to determine which sense
is the most frequent. Analysis of the NLM-WSD
data set showed that the MFS over all 50 ambigu-
ous terms is 78%. The different subsets have lower
MFS, indicating that the terms they contain are more
difficult to disambiguate. The 22 terms used by (Liu
et al, 2004) have a MFS of 69.9% while the set
used by (Leroy and Rindflesch, 2005) has an MFS
of 55.3%. The union and intersection of these sets
have MFS of 66.9% and 54.9% respectively.
adjustment
blood pressure
evaluation
immunosuppression
radiation
sensitivity
degree
growth
man
mosaic
nutrition
cold
depression
discharge
extraction
fat
implantation
association
condition
culture
determination
energy
failure
fit
fluid
frequency
ganglion
glucose
inhibition 
pressure 
resistance
secretion
single
strains
support
surgery
transient
transport
variation
repair
scale
weight
white
japanese
lead
mole
pathology
reduction
sex
ultrasound
NLM-WSD data set
Liu et. al. (2004)
Leroy and Rindflesch (2005)
Figure 1: The NLM-WSD test set and some of its sub-
sets. Note that the test set used by (Joshi et al, 2005)
comprises the set union of the terms used by (Liu et al,
2004) and (Leroy and Rindflesch, 2005) while the ?com-
mon subset? is formed from their intersection.
2.2 WSD of Biomedical Text
A standard approach to WSD is to make use of
supervised machine learning systems which are
trained on examples of ambiguous words in con-
text along with the correct sense for that usage. The
81
models created are then applied to new examples of
that word to determine the sense being used.
Approaches which are adapted from WSD of gen-
eral text include Liu et al (2004). Their technique
uses a supervised learning algorithm with a vari-
ety of features consisting of a range of collocations
of the ambiguous word and all words in the ab-
stract. They compared a variety of supervised ma-
chine learning algorithms and found that a decision
list worked best. Their best system correctly dis-
ambiguated 78% the occurrences of 22 ambiguous
terms in the NLM-WSD data set (see Section 2.1).
Joshi et al (2005) also use collocations as features
and experimented with five supervised learning al-
gorithms: Support Vector Machines, Naive Bayes,
decision trees, decision lists and boosting. The Sup-
port Vector Machine performed scoring 82.5% on
a set of 28 words (see Section 2.1) and 84.9% on
the 22 terms used by Liu et al (2004). Performance
of the Naive Bayes classifier was comparable to the
Support Vector Machine, while the other algorithms
were hampered by the large number of features.
Examples of approaches which have made use of
knowledge sources specific to the biomedical do-
main include Leroy and Rindflesch (2005), who re-
lied on information from the UMLS Metathesaurus
assigned by MetaMap (Aronson, 2001). Their sys-
tem used information about whether the ambigu-
ous word is the head word of a phrase identified by
MetaMap, the ambiguous word?s part of speech, se-
mantic relations between the ambiguous words and
surrounding words from UMLS as well as semantic
types of the ambiguous word and surrounding word.
Naive Bayes was used as a learning algorithm. This
approach correctly disambiguated 65.6% of word in-
stances from a set of 15 terms (see Section 2.1).
Humphrey et al (2006) presented an unsupervised
system that also used semantic types. They con-
structed semantic type vectors for each word from
a large collection of MEDLINE abstracts. This al-
lowed their method to perform disambiguation at a
coarser level, without the need for labeled training
examples. In most cases the semantic types can be
mapped to the UMLS concepts but not for five of the
terms in the NLM-WSD data set. Humphrey et al
(2006) reported 78.6% accuracy over the remaining
45. However, their approach could not be applied
to all instances of ambiguous terms and, in particu-
lar, is unable to model the ?none? tag. Their system
could only assign senses to an average of 54% of the
instances of each ambiguous term.
McInnes et al (2007) made use of Concept
Unique Identifiers (CUIs) from UMLS which are
also assigned by MetaMap. The information con-
tained in CUIs is more specific than in the semantic
types applied by Leroy and Rindflesch (2005). For
example, there are two CUIs for the term ?culture?
in UMLS: ?C0010453: Anthropological Culture?
and ?C0430400: Laboratory Culture?. The seman-
tic type for the first of these is ?Idea or Concept? and
?Laboratory Procedure? for the second. McInnes et
al. (2007) were interested in exploring whether the
more specific information contained in CUIs was
more effective than UMLS semantic types. Their
best result was reported for a system which repre-
sented each sense by all CUIs which occurred at
least twice in the abstract surrounding the ambigu-
ous word. They used a Naive Bayes classifier as the
learning algorithm. McInnes et al (2007) reported
an accuracy of 74.5% on the set of ambiguous terms
tested by Leroy and Rindflesch (2005) and 80.0% on
the set used by Joshi et al (2005). They concluded
that CUIs are more useful for WSD than UMLS se-
mantic types but that they are not as robust as fea-
tures which are known to work in general English,
such as unigrams and bigrams.
3 Approach
Our approach is to adapt a state-of-the-art WSD sys-
tem to the biomedical domain by augmenting it with
additional domain-specific and domain-independent
knowledge sources. Our basic system (Agirre and
Mart??nez, 2004) participated in the Senseval-3 chal-
lenge (Mihalcea et al, 2004) with a performance
close to the best system for the English and Basque
lexical sample tasks. The system is based on a su-
pervised learning approach. The features used by
Agirre and Mart??nez (2004) are derived from text
around the ambiguous word and are domain inde-
pendent. We refer to these as linguistic features.
This feature set has been adapted for the disam-
biguation of biomedical text by adding further lin-
guistic features and two different types of domain-
specific features: CUIs (as used by (McInnes et al,
2007)) and Medical Subject Heading (MeSH) terms.
82
3.1 Features
Our feature set contains a number of parameters
which were set empirically (e.g. threshold for un-
igram frequency in the linguistic features). In addi-
tion, we use the entire abstract as the context of the
ambiguous term for relevant features rather than just
the sentence containing the term. Effects of varying
these parameters are consistent with previous results
(Liu et al, 2004; Joshi et al, 2005; McInnes et al,
2007) and are not reported in this paper.
Linguistic features: The system uses a wide
range of domain-independent features which are
commonly used for WSD.
? Local collocations: A total of 41 features which
extensively describe the context of the am-
biguous word and fall into two main types:
(1) bigrams and trigrams containing the am-
biguous word constructed from lemmas, word
forms or PoS tags2 and (2) preceding/following
lemma/word-form of the content words (adjec-
tive, adverb, noun and verb) in the same sen-
tence with the target word. For example, con-
sider the sentence below with the target word
adjustment.
?Body surface area adjustments of
initial heparin dosing...?
The features would include the following: left-
content-word-lemma ?area adjustment?, right-
function-word-lemma ?adjustment of ?, left-
POS ?NN NNS?, right-POS ?NNS IN?, left-
content-word-form ?area adjustments?, right-
function-word-form ?adjustment of ?, etc.
? Syntactic Dependencies: These features model
longer-distance dependencies of the ambigu-
ous words than can be represented by the lo-
cal collocations. Five relations are extracted:
object, subject, noun-modifier, preposition and
sibling. These are identified using heuristic pat-
terns and regular expressions applied to PoS tag
sequences around the ambiguous word. In the
above example, ?heparin? is noun-modifier fea-
ture of ?adjustment?.
2A maximum-entropy-based part of speech tagger was used
(Ratnaparkhi, 1996) without the adaptation to the biomedical
domain.
? Salient bigrams: Salient bigrams within the ab-
stract with high log-likelihood scores, as de-
scribed by Pedersen (2001).
? Unigrams: Lemmas of unigrams which appear
more frequently than a predefined threshold in
the entire corpus, excluding those in a list of
stopwords. We empirically set the threshold
to 1. This feature was not used by Agirre and
Mart??nez (2004), but Joshi et al (2005) found
them to be useful for this task.
Concept Unique Identifiers (CUIs): We follow
the approach presented by McInnes et al (2007) to
generate features based on UMLS Concept Unique
Identifiers (CUIs). The MetaMap program (Aron-
son, 2001) identifies all words and terms in a
text which could be mapped onto a UMLS CUI.
MetaMap does not disambiguate the senses of the
concepts, instead it enumerates all the possible com-
binations of the concept names found. For exam-
ple, MetaMap will segment the phrase ?Body sur-
face area adjustments of initial heparin dosing ...?
into two chunks: ?Body surface area adjustments?
and ?of initial heparin dosing?. The first chunk
will be mapped onto four CUIs with the concept
name ?Body Surface Area?: ?C0005902: Diag-
nostic Procedure? and ?C1261466: Organism At-
tribute? and a further pair with the name ?Adjust-
ments?: ?C0456081: Health Care Activity? and
?C0871291: Individual Adjustment?. The final re-
sults from MetaMap for the first chunk will be eight
combinations of those concept names, e.g. first four
by second two concept names. CUIs which occur
more than three times in the abstract containing the
ambiguous word are included as features.
Medical Subject Headings (MeSH): The fi-
nal feature is also specific to the biomedical do-
main. Medical Subject Headings (MeSH) (Nelson
et al, 2002) is a controlled vocabulary for index-
ing biomedical and health-related information and
documents. MeSH terms are manually assigned to
abstracts by human indexers. The latest version of
MeSH contains over 24,000 terms organised into an
11 level hierarchy.
The terms assigned to the abstract in which
each ambiguous word occurs are used as fea-
tures. For example, the abstract containing our
example phrase has been assigned 16 MeSH
83
terms including ?M01.060.116.100: Aged?,
?M01.060.116.100.080: Aged, 80 and over?,
?D27.505.954.502.119: Anticoagulants? and
?G09.188.261.560.150: Blood Coagulation?. To
our knowledge MeSH terms have not been pre-
viously used as a feature for WSD of biomedical
documents.
3.2 Learning Algorithms
We compared three machine leaning algorithms
which have previously been shown to be effective
for WSD tasks.
The Vector Space Model is a memory-based
learning algorithm which was used by (Agirre and
Mart??nez, 2004). Each occurrence of an ambiguous
word is represented as a binary vector in which each
position indicates the occurrence/absence of a fea-
ture. A single centroid vector is generated for each
sense during training. These centroids are compared
with the vectors that represent new examples using
the cosine metric to compute similarity. The sense
assigned to a new example is that of the closest cen-
troid.
The Naive Bayes classifier is based on a proba-
bilistic model which assumes conditional indepen-
dence of features given the target classification. It
calculates the posterior probability that an instance
belongs to a particular class given the prior proba-
bilities of the class and the conditional probability
of each feature given the target class.
Support Vector Machines have been widely
used in classification tasks. SVMs map feature vec-
tors onto a high dimensional space and construct a
classifier by searching for the hyperplane that gives
the greatest separation between the classes.
We used our own implementation of the Vector
Space Model and Weka implementations (Witten
and Frank, 2005) of the other two algorithms.
4 Results
This system was applied to the NLM-WSD data set.
Experiments were carried out using each of the three
types of features (linguistic, CUI and MeSH) both
alone and in combination. Ten-fold cross valida-
tion was used, and the figures we report are averaged
across all ten runs.
Results from this experiment are shown in Table
1 which lists the performance using combinations of
learning algorithm and features. The figure shown
for each configuration represents the percentage of
instances of ambiguous terms which are correctly
disambiguated.
These results show that each of the three types
of knowledge (linguistic, CUIs and MeSH) can be
used to create a classifier which achieves a reason-
able level of disambiguation since performance ex-
ceeds the relevant baseline score. This suggests that
each of the knowledge sources can contribute to the
disambiguation of ambiguous terms in biomedical
text.
The best performance is obtained using a combi-
nation of the linguistic and MeSH features, a pattern
observed across all test sets and machine learning
algorithms. Although the increase in performance
gained from using both the linguistic and MeSH
features compared to only the linguistic features is
modest it is statistically significant, as is the differ-
ence between using both linguistic and MeSH fea-
tures compared with using the MeSH features alone
(Wilcoxon Signed Ranks Test, p < 0.01).
Combining MeSH terms with other features gen-
erally improves performance, suggesting that the
information contained in MeSH terms is distinct
from the other knowledge sources. However, the
inclusion of CUIs as features does not always im-
prove performance and, in several cases, causes it to
fall. This is consistent with McInnes et al (2007)
who concluded that CUIs were a useful informa-
tion source for disambiguation of biomedical text
but that they were not as robust as a linguistic knowl-
edge source (unigrams) which they had used for a
previous system. The most likely reason for this is
that our approach relies on automatically assigned
CUIs, provided by MetaMap, while the MeSH terms
are assigned manually. We do not have access to a
reliable assignment of CUIs to text; if we had WSD
would not be necessary. On the other hand, reli-
ably assigned MeSH terms are readily available in
Medline. The CUIs assigned by MetaMap are noisy
while the MeSH terms are more reliable and prove
to be a more useful knowledge source for WSD.
The Vector Space Model learning algorithm per-
forms significantly better than both Support Vector
Machine and Naive Bayes (Wilcoxon Signed Ranks
Test, p < 0.01). This pattern is observed regardless
84
Features
CUI+ Linguistic Linguistic Linguistic+Data sets Linguistic CUI MeSH
MeSH +MeSH +CUI MeSH+CUI
Vector space model
All words 87.2 85.8 81.9 86.9 87.8 87.3 87.6
Joshi subset 82.3 79.6 76.6 81.4 83.3 82.4 82.6
Leroy subset 77.8 74.4 70.4 75.8 79.0 78.0 77.8
Liu subset 84.3 81.3 78.3 83.4 85.1 84.3 84.5
Common subset 79.6 75.1 70.4 76.9 80.8 79.6 79.2
Naive Bayes
All words 86.2 81.2 85.7 81.1 86.4 81.4 81.5
Joshi subset 80.6 73.4 80.1 73.3 80.9 73.7 73.8
Leroy subset 76.4 66.1 74.6 65.9 76.8 66.3 66.3
Liu subset 81.9 75.4 81.7 75.3 82.2 75.5 75.6
Common subset 76.7 66.1 74.7 65.8 77.2 65.9 65.9
Support Vector Machine
All words 85.6 83.5 85.3 84.5 86.1 85.3 85.6
Joshi subset 79.8 76.4 79.5 78.0 80.6 79.1 79.8
Leroy subset 75.1 69.7 72.6 72.0 76.3 74.2 74.9
Liu subset 81.3 78.2 81.0 80.0 82.0 80.6 81.2
Common subset 75.7 69.8 71.6 73.0 76.8 74.7 75.2
Previous Approaches
MFS Liu et. al. Leroy and Joshi et. McInnes et.
baseline (2004) Rindflesch (2005) al. (2005) al. (2007)
All words 78.0 ? ? ? 85.3
Joshi subset 66.9 ? ? 82.5 80.0
Leroy subset 55.3 ? 65.5 77.4 74.5
Liu subset 69.9 78.0 ? 84.9 82.0
Common subset 54.9 ? 68.8 79.8 75.7
Table 1: Results from WSD system applied to various sections of the NLM-WSD data set using a variety of fea-
tures and machine learning algorithms. Results from baseline and previously published approaches are included for
comparison.
of which set of features are used, and it is consis-
tent of the results in Senseval data from (Agirre and
Mart??nez, 2004).
4.1 Per-Word Analysis
Table 2 shows the results of our best performing sys-
tem (combination of linguistic and MeSH features
using the Vector Space Model learning algorithm).
Comparable results for previous supervised systems
are also reported where available.3 The MFS base-
line for each term is shown in the leftmost column.
The performance of Leroy and Rindflesch?s sys-
3It is not possible to directly compare our results with Liu
et al (2004) or Humphrey et al (2006). The first report only
optimal configuration for each term (combination of feature sets
and learning algorithm) while the second do not assign senses
to all of the instances of each ambiguous term (see Section 2).
tem is always lower than the best result for each
word. The systems reported by Joshi et al (2005)
and McInnes et al (2007) are better than, or the
same as, all other systems for 14 and 12 words re-
spectively. The system reported here achieves re-
sults equal to or better than previously reported sys-
tems for 33 terms.
There are seven terms for which the performance
of our approach is actually lower than the MFS base-
line (shown in italics) in Table 2. (In fact, the base-
line outperforms all systems for four of these terms.)
The performance of our system is within 1% of the
baseline for five of these terms. The remaining pair,
?blood pressure? and ?failure?, are included in the
set of problematic words identified by (Weeber et
al., 2001). Examination of the possible senses show
that they include pairs with similar meanings. For
85
MFS Leroy and Joshi et. McInnes et. Reported
baseline Rindflesch (2005) al. (2005) al. (2007) system
adjustment 62 57 71 70 74
association 100 - - 97 100
blood pressure 54 46 53 46 46
cold 86 - 90 89 88
condition 90 - - 89 89
culture 89 - - 94 95
degree 63 68 89 79 95
depression 85 - 86 81 88
determination 79 - - 81 87
discharge 74 - 95 96 95
energy 99 - - 99 98
evaluation 50 57 69 73 81
extraction 82 - 84 86 85
failure 71 - - 73 67
fat 71 - 84 77 84
fit 82 - - 87 88
fluid 100 - - 99 100
frequency 94 - - 94 94
ganglion 93 - - 94 96
glucose 91 - - 90 91
growth 63 62 71 69 68
immunosuppression 59 61 80 75 80
implantation 81 - 94 92 93
inhibition 98 - - 98 98
japanese 73 - 77 76 75
lead 71 - 89 90 94
man 58 80 89 80 90
mole 83 - 95 87 93
mosaic 52 66 87 75 87
nutrition 45 48 52 49 54
pathology 85 - 85 84 85
pressure 96 - - 93 95
radiation 61 72 82 81 84
reduction 89 - 91 92 89
repair 52 81 87 93 88
resistance 97 - - 96 98
scale 65 84 81 83 88
secretion 99 - - 99 99
sensitivity 49 70 88 92 93
sex 80 - 88 87 87
single 99 - - 98 99
strains 92 - - 92 93
support 90 - - 91 89
surgery 98 - - 94 97
transient 99 - - 98 99
transport 93 - - 93 93
ultrasound 84 - 92 85 90
variation 80 - - 91 95
weight 47 68 83 79 81
white 49 62 79 74 76
Table 2: Per-word performance of best reported systems.
example, the two senses which account for 98% of
the instances of ?blood pressure?, which refer to the
blood pressure within an organism and the result ob-
tained from measuring this quantity, are very closely
related semantically.
5 Conclusion
This paper has compared a variety of knowledge
sources for WSD of ambiguous biomedical terms
and reported results which exceed the performance
of previously published approaches. We found that
accurate results can be achieved using a combina-
tion of linguistic features commonly used for WSD
86
of general text and manually assigned MeSH terms.
While CUIs are a useful source of information for
disambiguation, they do not improve the perfor-
mance of other features when used in combination
with them. Our approach uses manually assigned
MeSH terms while the CUIs are obtained automati-
cally using MetaMap.
The linguistic knowledge sources used in this pa-
per comprise a wide variety of features including
n-grams and syntactic dependencies. We have not
explored the effectiveness of these individually and
this is a topic for further work.
In addition, our approach does not make use of
the fact that MeSH terms are organised into a hierar-
chy. It would be interesting to discover whether this
information could be used to improve WSD perfor-
mance. Others have developed techniques to make
use of hierarchical information in WordNet for WSD
(see Budanitsky and Hirst (2006)) which could be
adapted to MeSH.
References
E. Agirre and D. Mart??nez. 2004. The Basque Coun-
try University system: English and Basque tasks. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 44?
48, Barcelona, Spain, July.
A. Aronson, O. Bodenreider, H. Chang, S. Humphrey,
J. Mork, S. Nelson, T. Rindflesch, and W. Wilbur.
2000. The NLM Indexing Initiative. In Proceedings
of the AMIA Symposium.
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: the MetaMap program.
In Proceedings of the American Medical Informatics
Association (AMIA), pages 17?21.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32(1):13?47.
S. Humphrey, W. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T. Rindflesch. 2006. Word Sense Dis-
ambiguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Information
Science and Technology, 57(5):96?113.
L. Humphreys, D. Lindberg, H. Schoolman, and G. Bar-
nett. 1998. The Unified Medical Language System:
An Informatics Research Collaboration. Journal of the
American Medical Informatics Association, 1(5):1?11.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Compara-
tive Study of Support Vector Machines Applied to the
Word Sense Disambiguation Problem for the Medical
Domain. In Proceedings of the Second Indian Confer-
ence on Artificial Intelligence (IICAI-05), pages 3449?
3468, Pune, India.
G. Leroy and T. Rindflesch. 2005. Effects of Information
and Machine Learning algorithms on Word Sense Dis-
ambiguation with small datasets. International Jour-
nal of Medical Informatics, 74(7-8):573?585.
H. Liu, V. Teller, and C. Friedman. 2004. A Multi-aspect
Comparison Study of Supervised Word Sense Disam-
biguation. Journal of the American Medical Informat-
ics Association, 11(4):320?331.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Lingusitics (ACL-2004), pages
280?287, Barcelona, Spain.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the Annual Symposium of the Ameri-
can Medical Informatics Association, pages 533?537,
Chicago, IL.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
Senseval-3 English lexical sample task. In Proceed-
ings of Senseval-3: The Third International Workshop
on the Evaluation of Systems for the Semantic Analysis
of Text, Barcelona, Spain.
S. Nelson, T. Powell, and B. Humphreys. 2002. The
Unified Medical Language System (UMLS) Project.
In Allen Kent and Carolyn M. Hall, editors, Ency-
clopedia of Library and Information Science. Marcel
Dekker, Inc.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), pages 79?86, Pittsburgh, PA., June.
A. Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 133?142.
M. Stevenson and Y. Wilks. 2001. The Interaction of
Knowledge Sources in Word Sense Disambiguation.
Computational Linguistics, 27(3):321?350.
M. Weeber, J. Mork, and A. Aronson. 2001. Developing
a Test Collection for Biomedical Word Sense Disam-
biguation. In Proceedings of AMAI Symposium, pages
746?50, Washington, DC.
I. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
mann, San Francisco.
87
Proceedings of the Workshop on BioNLP, pages 46?54,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extraction of Named Entities from Tables in Gene Mutation Literature
Wern Wong?, David Martinez??, Lawrence Cavedon??
??NICTA Victoria Research Laboratory
?Dept of Computer Science and Software Engineering
The University of Melbourne
{wongwl,davidm,lcavedon}@csse.unimelb.edu.au
Abstract We investigate the challenge of extract-
ing information about genetic mutations from ta-
bles, an important source of information in scien-
tific papers. We use various machine learning algo-
rithms and feature sets, and evaluate performance in
extracting fields associated with an existing hand-
created database of mutations. We then show how
classifying tabular information can be leveraged for
the task of named entity detection for mutations.1
Keywords Information extraction; tables;
biomedical applications.
1 Introduction
We are interested in applying information extraction
and text mining techniques to aiding the construc-
tion of databases of biomedical information, in par-
ticular information about genetic mutations. Such
databases are currently constructed by hand: a long,
involved, time-consuming and human-intensive pro-
cess. Each paper considered for inclusion in the
database must be read, the interesting data identified
and then entered by hand into a database.2
However, the biomedical domain throws up many
new and serious challenges to information extraction
and text mining. Unusual terminology and under-
developed standards for nomenclature present prob-
lems for tokenisation and add complexity to stan-
dard information extraction tasks, such as named en-
tity recognition (NER). A lack of resources (at least
1A short version of this paper was presented at the Aus-
tralasian Document Computing Symposium, 2008. All copy-
rights from that event were retained by the authors.
2Karamis et al(2008) illustrate how even simple tools can
have an impact on improving the database-curation process.
compared to other domains), such as collections of
annotated full-text documents and relevance judge-
ments for various tasks, are a bottleneck to develop-
ing and evaluating the core techniques required.
In this paper, we report on work performed on
extracting information from tables in biomedical
research papers. Tables present a succinct and
information-rich format for providing information,
and are particularly important when reporting re-
sults in biological and medical research papers.
For example, the Human Genome Variation Society
(HGVS), in its general recommendations for muta-
tion nomenclature, recommends making use of tab-
ular listings when several changes are described in
a manuscript.3 A specific premise of our work is
that the highly-structured nature of tabular informa-
tion allows leverage of some techniques that are not
so sensitive to the well-reported problems inherent
in biomedical terminology, which complicate NER
tasks in this domain. In particular, we describe
initial techniques for extending NER performance
through the analysis of tables: columns/rows are
classified as containing items of the entities of inter-
est, thereby allowing those entities to be recognized
as of the target type. Since a significant amount of
such entities may be found in tables in biomedical
scientific papers, this can have positive impact on
the performance of base NER techniques.
NER tools specifically targeted at recognising
mutations have been developed (e.g. (Horn et al,
2004; Baker and Witte, 2006; Caporaso et al, 2007;
Lee et al, 2007)); however, they only detect a sub-
class of mutations, so-called single-point mutations,
3http://www.hgvs.org/mutnomen/recs.html#general
46
i.e. those that affect a single base. MutationFinder
(Caporaso et al, 2007) is the only publicly available
tool, built with around 700 automatically-generated
rules (both for different nomenclatures and natural
language). However, most of the mutations that
we find in our dataset are not point mutations or
do not follow point-mutation nomenclature, limiting
the usefulness of MutationFinder (and related tools)
over our document collection.
In the next section, we describe the setting of our
task, the Mismatch Repair (MMR) Database, and
outline the task of extraction from tables. In Sec-
tion 3, we describe the preparation of our document
collection, and in Section 4, we analyse the amount
of mutation-related information that is in the associ-
ated tables. Section 5 describes the main task, which
is classifying table rows and columns as containing
mutations, and Section 6 leverages this technique to
detect mutations of interest to the MMR Database.
We discuss the results in Section 7.
2 Background
In this section, we discuss the MMR database?the
setting for our task and from which we construct
our document collection?and previous approaches
to table processing.
2.1 The MMR Database
Our extraction task is grounded in the specific con-
text of the Mismatch Repair (MMR) Database com-
piled at the Memorial University of Newfoundland
(Woods et al, 2007)?a database of known genetic
mutations related to hereditary non-polyposis col-
orectal cancer (HNPCC), a hereditary form of bowel
cancer. The MMR Database contains information
on genetic mutations known to be related to HN-
PCC, along with links to the research papers from
which the database has been constructed.4 From the
database and its links to papers, we were able to con-
struct a collection of tables related to HNPCC muta-
tions, and then use the MMR database records them-
selves as a gold standard for evaluating our tech-
niques. As of May 2008, the MMR database con-
tained a total of 5,491 records on mutations that oc-
4I.e. a team of geneticists manually trawled the biomedical
literature for information on HNPCC-related mutation informa-
tion, and added links to any papers relevant to those mutations
in the context of HNPCC.
cur on any one of four genes that have been identi-
fied as related to colon cancer. An example record
from the MMR database is the following:
MLH1 | Exon13 | c.1491delG | Yamamoto et al | 9500462
Respectively, this record contains: gene; exon;
mutation; citation of the paper the information was
sourced from;5 and the paper?s PubMedID. These
fields are important because they contain informa-
tion researchers are directly interested in (gene,
exon, mutation) and the paper said information was
found in. Note that if a gene/mutation pair is refer-
enced in multiple papers, then there are correspond-
ingly multiple entries in the database. Conversely, if
a single paper mentions multiple (relevant) genes,
then that paper is mentioned in multiple database
records.
2.2 Table Processing
An important but less-researched sub-problem in
text mining is information extraction from tables.
This is particularly important in the biomedical do-
main since much important data is present in tabu-
lar form, such as experimental results, relations be-
tween entities, and other information that may not
be contained elsewhere in the text. For example, the
table shown in Figure 1 (taken from an article in our
collection) contains much of the same data that was
present in database records, in a similar format.
Tabular information extraction can be divided into
two broad sub-tasks:
? table detection: identifying tables within docu-
ments;
? table processing: extraction of data from tables.
Several systems have been developed to handle both
tasks, some are designed only to handle table de-
tection, and others focus only on extracting data.
Both machine learning and heuristic / rule-based ap-
proaches have been proposed.
Table detection techniques depend heavily on the
input format. Most work that tackles this problem
tends to assume one homogeneous input format, but
tables generally come in one of two varieties:6
5This field has been abbreviated. We have also omitted fields
such as ?internal id?.
6We don?t consider the possibility of processing bitmaps or
other images from scanned documents.
47
Figure 1: Sample table containing mutation information related to HNPCC
? raw text tables: generally ASCII text in
monospace font, delimited by whitespace
and/or special characters;
? rich text tables: those formatted using LaTeX,
PDF, HTML and other such formats.
Tables in plain text tend to be more difficult to
detect, as the detection system must be sensitive to
whitespace and symbols used to align cells in tables.
Efforts to handle rich text formats generally focus on
HTML-based representations. Raw HTML is easier
to parse than raw LaTeX or PDF, and most formats
are easily converted to HTML. HTML tables can
theoretically be trivially detected using <table>
tags. However, Lerman et al(2004) note that in
HTML files taken from the web, only a fraction of
tabular data was presented using <table> tags, and
those tags were also used to format multi-column
text, images and other non-table applications. Hurst
(2001) attests that less than 30% of HTML tables on
the web contain actual tabular content; for many, the
HTML table tags are often used simply for format-
ting purposes.
Zanibbi et al(2004) present a survey of table
recognition in general. Of greatest relevance to us
here are approaches that adopt a machine learning
approach to detecting and/or extracting table data.
Cohen et al(2002) use features based on HTML
table tags, number of rows and columns of spe-
cific lengths, and ratios of single-square cells to to-
tal number of cells, to perform table detection, and
then form a geometric representation of the data us-
ing algorithms based on table-rendering techniques
implemented by browsers.
Pinto, Wei, and their colleagues have used condi-
tional random fields (CRFs) to both detect and pro-
cess tables simultaneously. Pinto et al(2003) com-
pare the output of their CRF system with a previ-
ous effort using hidden Markov machines (HMMs).
These systems use features such as: presence of
whitespace of varying length (different lengths of
whitespace are used as separate features); domain-
specific lexical features (such as month names, year
strings, specified keywords); separator characters
(e.g. ?+?, ?-?, etc). In subsequent work they develop
a system for performing question answering over ta-
ble data (Wei et al, 2004) by treating each extracted
data cell as a discrete document.
To our knowledge, no previous system has at-
tempted to extract data from tables in biomedical
literature. This is possibly because of a combina-
tion of the lack of resources for this domain (e.g.
48
collections of full-text documents; relevance judge-
ments), as well as the lesser focus on text mining
in general in this area. As will be seen in the next
section, the vagaries of the construction of our col-
lection of tables means we were effectively able to
ignore the issue of table detection and focus directly
on the problem of processing.
3 Experimental Setting
Our experiments were designed to identify mentions
of mutations in the biomedical literature, focusing
on tabular content. In this section, we first describe
our target dataset, built from the hand-curated MMR
database (Woods et al, 2007); we then explain the
table extraction process; finally, we introduce the
task design.
3.1 Mutation Mention Dataset
We relied on the MMR Database and MEDLINE in
order to build our test collection. First we collected
all the information available in the hand-curated
MMR records, obtaining a total of 5,491 mutations
linked to 719 distinct PubMedIDs7.
Our next step was to crawl the full-text articles
from MEDLINE. We used an automatic crawler that
followed the links from the PubMed interface, and
downloaded those papers that had a full-text HTML
version, and which contained at least one content ta-
ble.
The tables were then extracted from the full text
HTML files. It is important to note that the tables
were already present as links to separate HTML files
rather than being presented as inline tables, making
this process easier. Papers that did not contain tables
in HTML format were discarded.
Our final collection consisted of 70 papers out of
the original 719 PubMedIDs. Some of the papers
were not available in full text, and for others our
crawling script failed to extract the full version. Our
approach was conservative, and our collection could
be augmented in the future, but we decided to fo-
cus on this dataset for the experiments presented in
this paper. This set of articles is linked to 717 MMR
records (mutations), which constitutes our gold stan-
dard hand-curated annotation. The collection con-
tains 197 tables in all.
7Data was downloaded from the web interface in May 2008.
3.2 Table extraction
Once scraped, the tables were then pre-processed
into a form that more readily allowed experimenta-
tion. The tables were therefore split into three parts:
column headers, row headers, and data cells. This
was done based on the HTML formatting, which
was consistent throughout the data set as the tables
were automatically generated.
The first step was to deconstruct the HTML ta-
bles into nested lists of cells based on HTML ta-
ble tags. Inconsistencies introduced by colspan and
rowspan attributes were resolved by replicating a
cell?s contents across its spanned lengths. That is, a
cell with colspan=3 would be duplicated across the
three columns, and likewise for cells spanning mul-
tiple rows. Single-cell rows at the top or bottom of a
table were assumed to be captions and discarded.
The remaining HTML was stripped, save for the
following tags which contained important informa-
tion:
? img tags were replaced by their alternate text,
where available. Such images often represent
a mathematical symbol, which is important in-
formation to retain;
? hr tags proved to be an important indicator for
dividing header cells from data cells.
Tables were broken up into row headers, column
headers, and data cells by making use of the hr tags,
denoting horizontal lines, to detect column headers.
Such tags tend to be present as a separator between
column header cells and data cells; in fact, the only
tables in our collection that did not have the separa-
tors did not have column headers either. The hr tags
were subsequently stripped after this use. Detecting
row headers was performed by checking if the top
left cell of the table was blank, a pattern which oc-
curred in all row-major tables. The vast majority of
tables had column headers rather than row headers,
although some had both and a small proportion had
only row headers. We acknowledge that this pro-
cessing may be specific to the vagaries of the specific
format of the HTML generation used by PubMed
(from which we sourced the tables). However, our
whole task is specific to this domain; further, our fo-
cus is on the extraction task rather than the actual
detection of row/column headers.
49
Class Class Freq. Cell Freq.
Gene 64 1,618
Exon 48 1,004
Codon 23 435
Mutation 90 2,174
Statistic 482 8,788
Other 576 14,324
Total 1,283 28,343
Table 1: Frequency per class and number of cells in the
collection.
3.3 Task Design
In order to extract mutation information from
tables, we first performed classification of full
columns/rows into relevant entities. The content of a
column (or row, depending on whether the table was
row- or column-oriented) tends to be homogeneous;
this allowed us to build classifiers that can identify
full vectors of relevant entities in a single step. We
refer to this task as table vector classification.
We identified the following classes as relevant:
Gene, Exon, Mutation, Codon, and Statistic. The
first four were chosen directly from the MMR
Database. We decided to include ?Statistic? after in-
specting the tabular dataset, since we found that this
provides relevant information about the importance
of a given mutation. Of the five classes, Mutation
is the most informative for our final information ex-
traction goal.
The next step was to hand-annotate the headers
of the 197 tables in our collection by using the five
classes and the class ?Other? as the tagset. Some
headers belonged to more that one class, since the
classes were collapsed into a single field of the ta-
ble. The frequency per class and the number of cells,
across the collection of tables, is shown in Table 1.
3.4 Evaluation
We evaluated our systems in two ways:
? Header classification: performance of different
systems on predicting the classes of each col-
umn/row of the tables;
? Mutation extraction: recall of our system over
the subset of the hand-curated MMR database.
Evaluation for the header classification step was
performed using precision, recall and f-score, micro-
averaged amongst the classes. Micro-averaging in-
volves multiplying the score of a class by the number
of instances of the class in the gold standard, and di-
viding by the total number of instances. For the ma-
chine learning algorithms, evaluation was performed
using 10-fold cross-validation. For mutation extrac-
tion we focus on a single class, and produce recall
and a lower-bound on precision.
4 Mutation Mentions in Tables
In order to determine the value of processing tab-
ular data for mutation-mining purposes, we ob-
tained a sample of 100 documents that were hand-
annotated by curators prior to their introduction in
the database?the curators highlighted relevant mu-
tations found in each paper. We found that for 59
of the documents, only the tabular parts of the paper
were selected; 33 of the documents had only textual
parts highlighted; and for 8 documents both tables
and text were selected. This is an indicator of the
importance of tabular data in this context.
Our next step was to measure the amount of in-
formation that we could potentially extract from the
tables in our collection. Since we are interested in
mutations, we extracted all cells from the vectors
that were manually annotated as ?Mutation? in or-
der to compare them to the goldstandard, and mea-
sure the recall. This comparison was not straight-
forward, because mutation mentions have different
nomenclatures. Ideally we would normalise the dif-
ferent references into a standard form, and then per-
form the comparison. However, normalisation is a
complex process in itself, and we resorted to evalu-
ation by hand at this point.
We found that 198 of the 717 goldstandard muta-
tions were present in tables (28%). This is a signif-
icant amount, taking into account that their extrac-
tion should be much easier than parsing the raw text.
We also tested MutationFinder over the full text, and
found that only 6 of the goldstandard mutations were
retrieved (0.8%), which indicates that point mutation
identification is not sufficient for this task.
Finally, we measured the amount of information
that could be extracted by a simple string look-up
system separately over the tabular and textual parts
50
of the articles. We were looking for mutation men-
tions that correspond exactly to the goldstandard
record from each article, which meant that mentions
in different nomenclatures would be missed. We
found that a total of 177 mentions (24.7%) could be
found with the same spelling; of those 142 (80.1%)
were found in tables only, and the remaining 35
(20.9%) were found in both tables and text; i.e., no
mention was found in text only.
These results indicate that we can find relevant in-
formation in tables that is not easy to detect in run-
ning text.
5 Table Vector Classification
We built automatic classifiers to detect relevant en-
tities in tables. Two separate approaches were at-
tempted for vector classification: applying heuristic
rules, and machine learning (ML) techniques. These
are described here, along with an analysis of their
performance.
5.1 Heuristic Baseline
As a baseline method, we approached the task of
classifying headers by matching the header string to
the names of the classes in a case-insensitive man-
ner. When the class name was found as a substring
of the header, the class would be assigned to it. For
example, a header string such as ?Target Mutation?
would be assigned the class ?Mutation?. Some head-
ers had multiple annotations (E.g. ?Gene/Exon?).
For better recall, we also matched synonyms for
the class ?Mutation? (the terms ?Variation? and
?Missense?) and the class ?Statistic? (the terms
?No.?, ?Number? and ?%?). For the remaining
classes we did not identify other obvious synonyms.
The results are shown in Table 2. Precision
was reasonably high for the ?Codon?, ?Exon? and
?Statistic? classes. However, this was not the case
for ?Mutation?, and this illustrates that different
types of information are provided under this head-
ing; illustrative examples include the heading ?Mu-
tation Detected? on a ?Gene? vector, or the heading
?Germline Mutation? referring to ?Statistics?. The
recall was also low for ?Mutation? and most other
classes, showing that more sophisticated approaches
are required in order to exploit the information con-
tained in the tables. Notice also that the micro-
Class Precision Recall FScore
Gene 0.537 0.620 0.575
Exon 0.762 0.615 0.681
Codon 0.850 0.654 0.739
Mutation 0.283 0.301 0.292
Statistic 0.911 0.324 0.478
Other 0.581 0.903 0.707
Micro Avg. 0.693 0.614 0.651
Table 2: Naive Baseline results across the different
classes and micro-averaged
Class Precision Recall FScore
Gene 0.537 0.611 0.571
Exon 0.762 0.615 0.681
Codon 0.850 0.654 0.739
Mutation 0.600 0.452 0.515
Statistic 0.911 0.340 0.495
Other 0.579 0.910 0.708
Micro Avg. 0.715 0.633 0.672
Table 3: Results integrating MutationFinder across the
different classes and micro-averaged
average is highly biased by the classes ?Statistic?
and ?Others?, since they contain most of the test in-
stances.
Our second step was to build a more informed
classifier for the class ?Mutation? using the point
mutation NER system MutationFinder (Caporaso et
al., 2007). We applied this tool to the text in the
table-cells, and identified which table-vectors con-
tained at least one mutation mention. These vectors
were also classified as mutations. The results are
shown in Table 3. This approach caused the ?Muta-
tion? results to improve, but the overall f-score val-
ues are still in the range 50%-70%.
We considered other heuristic rules that could
be applied, such as looking for different kinds of
patterns for each class: for instance, numbers for
?Exon?, or the normalised form c.N[symbol]N for
mutation, or trying to match against term lists (e.g.
using Gene dictionaries). Future work will explore
extending the ML approach below with features
such as these.
51
5.2 Classification Techniques
For the ML experiments we used the Weka (Witten
and Frank, 2005) toolkit, as it contains a wide se-
lection of in-built algorithms. We selected a variety
of well-known approaches in order to obtain a better
picture of the overall performance. As a baseline, we
applied the majority class from the training data to
all test instances. We applied the following ML sys-
tems:8 Naive Bayes (NB); Support Vector Machines
(SVM); Propositional Rule Learner (JRip); and De-
cision Trees (J48). We did not tune the parameters,
and relied on the default settings.
In order to define our feature sets, we used the
text in the headers and cells of the tables, without
tokenisation. Other possible sources of information,
such as captions or the running text referring to the
table were not employed at this stage. We applied
four feature sets:
? Basic (Basic): Four basic features, consisting
of the header string, the average and median
cell lengths, and a binary feature indicating
whether the data in the cells was numeric.
? Cell Bag-of-Words (C bow): Bag of words
over the tokens in the table cells.
? Header Bag-of-Words (H bow): Bag of
words over the tokens in the header strings.
? Header + Cell Bag-of-Words (HC bow):
Combination of bags of words formed by the
tokens in headers and cells, represented as sep-
arate types of features.
The micro-averaged results of the different learn-
ing methods and feature sets are shown in Table 4.
Regarding the feature sets, we can see that the best
performance is obtained by using the headers as bag-
of-words, while the content of the cells seems to be
too sparse to guide the learning methods. SVM is
the best algorithm for this dataset, with JRip and J48
following, and NB performing worst of the four in
most cases.
Overall, the results show that the ML approach
is superior to the baselines when using the header
bag of words feature to classify the relevant entities.
8We applied a number of other ML algorithms as well, but
these showed significantly lesser performance.
Method Feature SetsBasic C bow H bow HC bow
Mj. Cl. 0.288
NB 0.614 0.454 0.678 0.581
SVM 0.717 0.599 0.839 0.816
JRip 0.564 0.493 0.790 0.749
J48 0.288 0.532 0.793 0.782
Table 4: Results for ML Algorithms - Micro-Averaged
FScores. Mj.Cl.: Majority Class. The best results per
column are given in bold.
Class Precision Recall FScore
Gene 0.778 0.737 0.757
Exon 0.786 0.707 0.745
Codon 0.833 0.882 0.857
Mutation 0.656 0.679 0.667
Statistic 0.919 0.853 0.885
Other 0.82 0.884 0.850
Micro Avg 0.839 0.841 0.839
Table 5: Results for SVM and the feature set H bow per
class and micro-averaged.
SVM is able to reach a high f-score of 83.9%, which
has been found to be significantly better than the best
baseline after applying a paired t-test (p-value under
0.0001).
We break down the results per class in Table 5,
using the outputs from SVM and feature-set H bow.
We can see that all classes show an improvement
over the heuristic baselines. There is a big increase
for the classes ?Gene? and ?Statistic?, and all classes
except mutation are above 70% f-score. ?Muta-
tion? is the most difficult class to predict, but it
still reaches 66.7% f-score, which can be helpful for
some tasks, as we explore in the next section.
6 Automatic Mutation Extraction
We applied the results of our classifier to a practi-
cal application, i.e., the detection of mutations in
the literature for the MMR Database project. Ta-
ble vector classification allows us to extract lists of
candidate mutation names from tables to be added
to the database. We would like a system with high
recall that identifies all relevant candidates, but also
acceptable precision so that not all the tables need to
52
System Mut. Found TP % in MMR Rec.
Automatic 1,702 153 9.0 77.3
Gold standard 1,847 198 10.7 100
Table 6: Results for Mutation detection. TP indicates the
number of true positives, ?% in MMR? shows the per-
centage of positives found in the database.
be hand-checked.
In order to test the viability of this approach, we
measured the results of the system in detecting the
existing hand-curated mutations in MMR. We cal-
culated the recall in retrieving those mutations, and
also the rate of false positives; however, note that
we also consider as false positives those valid muta-
tions that were not relevant for MMR, and therefore
the reported precision is artificially low.
Results for the automatic extraction and the gold-
standard annotation are given in Table 6. As ex-
pected, there is a high rate of false positives in the
goldstandard and automatic systems; this shows that
most of the mutations detected are not relevant for
the MMR database. More interestingly, we were
able to retrieve 77.3% of relevant mutation mentions
automatically using the ML approach, which corre-
sponds to 21.3% of all the hand-curated data.
The vector classifier discriminates 1,702 mutation
cells out of a total of 27,700 unique cells in the table
collection, and it effectively identifies 153 out of the
198 relevant mutations present in the tabular data.
This means that we only need to hand-check 6.1%
of the tabular content to retrieve 77.3% of relevant
mutations, saving the curators a significant amount
of time. The classifiers could also be biased towards
higher recall by parameter tuning?this is an area for
further investigation.
Finally, after the evaluation process we observed
that many false mutation candidates could be re-
moved by discarding those that do not contain two
consecutive digits or any of the following n-grams:
?c.?, ?p.?, ?>?, ?del?, ?ins?, ?dup?. This heuristic re-
duces the number of mutation candidates from 1,702
to 989 with no cost in recall.
7 Discussion
While this is early work, our preliminary results on
the task of identifying relevant entities from gene
mutation literature show that targeting tables can be
a fruitful approach for text mining. By relying on
ML methods and simple bag-of-words features, we
were able to achieve good performance over a num-
ber of selected entities, well above header word-
matching baselines. This allowed us to identify lists
of mentions of relevant entities with minimal effort.
An advantage of our approach is that the annotation
of examples for training and evaluation is consider-
ably easier, since many entities can be annotated in
a single step, opening the way to faster annotation of
other entities of interest in the biomedical domain.
The approach of using table vector classification
for the named entity task also has promise. In partic-
ular, the wide variety and non-standard terminology
of biomedical entities (i.e. genes, proteins, muta-
tions) is one of the challenges to NER in this do-
main. However, since a column of homogeneous
information may include representatives of the het-
erogeneous nomenclature schemes, classification of
a whole column or row potentially helps nullify the
effect of the terminological variability.
For future work, we plan to study different types
of features for better representing the entities tar-
geted in this work. Specially for mutation mentions,
we observed that the presence of certain ngrams (e.g.
?del?) can be a strong indicator for this class. An-
other issue we plan to address is that of the normal-
isation of mutation mentions into a standard form,
for which we have started developing a collection
of regular expressions. Another of our goals is to
increase the size of our dataset of articles by im-
proving our web crawler, and by hand-annotating
the retrieved table vectors for further experimenta-
tion. Finally, we also aim to explore the potential of
using tabular data for NER of different entities in the
biomedical domain, such as gene mentions.
Acknowledgements NICTA is funded by the Aus-
tralian Government as represented by the Depart-
ment of Broadband, Communications and the Dig-
ital Economy and the Australian Research Council
through the ICT Centre of Excellence program.
Thanks to Mike Woods and his colleagues at the
Memorial University of Newfoundland for making
the MMR database and their curation data available
to us. Eric Huang wrote several of the scripts men-
tioned in Section 3 for creating the table collection.
53
References
C. J. O. Baker and R. Witte. 2006. Mutation mining?a
prospector?s tale. J. of Information Systems Frontiers,
8(1):45?57.
J. G. Caporaso, W. A. Baumgartner Jr., D. A. Randolph,
K. B. Cohen, and L. Hunter. 2007. Mutationfinder: A
high-performance system for extracting point mutation
mentions from text. Bioinformatics, 23(14):1862?
1865.
W. W. Cohen, M. Hurst, and L. S. Jensen. 2002. A flex-
ible learning system for wrapping tables and lists in
html documents. In WWW ?02: Proc. 11th Int?l Conf.
on World Wide Web, pages 232?241, Honolulu.
F. Horn, A. L. Lau, and F. E. Cohen. 2004. Auto-
mated extraction of mutation data from the literature:
Application of MuteXt to g protein-coupled recep-
tors and nuclear hormone receptors. Bioinformatics,
20(4):557?568.
M. Hurst. 2001. Layout and language: Challenges
for table understanding on the web. Technical report,
WhizBang!Labs.
N. Karamanis, R. Seal, I. Lewin, P. McQuilton, A. Vla-
chos, C. Gasperin, R. Drysdale, and T. Briscoe. 2008.
Natural language processing in aid of flybase curators.
BMC Bioinformatics, 9:193?204.
Lawrence C. Lee, Florence Horn, and Fred E. Cohen.
2007. Automatic extraction of protein point muta-
tions using a graph bigram association. PLoS Com-
putational Biology, 3(2):e16+, February.
K. Lerman, L. Getoor, S. Minton, and C. Knoblock.
2004. Using the structure of web sites for automatic
segmentation of tables. In SIGMOD?04, pages 119?
130, Paris.
D. Pinto, A. McCallum, X. Wei, and W. B. Croft. 2003.
Table extraction using conditional random fields. In
SIGIR ?03, pages 235?242.
X. Wei, W.B. Croft, and D. Pinto. 2004. Question
answering performance on table data. Proceedings
of National Conference on Digital Government Re-
search.
I. H. Witten and E. Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
M.O. Woods, P. Williams, A. Careen, L. Edwards,
S. Bartlett, J. McLaughlin, and H. B. Younghusband.
2007. A new variant database for mismatch repair
genes associated with lynch syndrome. Hum. Mut.,
28:669?673.
R. Zanibbi, D. Bolstein, and J. R. Cordy. 2004. A survey
of table recognition. Int?l J. on Document Analysis
and Recognition, 7(1).
54
Proceedings of the Workshop on BioNLP: Shared Task, pages 77?85,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Biomedical Event Annotation with CRFs and Precision Grammars
Andrew MacKinlay, David Martinez and Timothy Baldwin
NICTA Victoria Research Laboratories
University of Melbourne, VIC 3010, Australia
{amack,davidm,tim}@csse.unimelb.edu.au
Abstract
This work describes a system for the tasks
of identifying events in biomedical text and
marking those that are speculative or negated.
The architecture of the system relies on
both Machine Learning (ML) approaches and
hand-coded precision grammars. We submit-
ted the output of our approach to the event ex-
traction shared task at BioNLP 2009, where
our methods suffered from low recall, al-
though we were one of the few teams to pro-
vide answers for task 3.
1 Introduction
We present in this paper our techniques for the tasks
1 and 3 of the event extraction shared task at BioNLP
2009. We make use of both Machine Learning (ML)
approaches and hand-coded precision grammars in
an architecture that combines multiple dedicated
modules. In the third task on negation/speculation,
we extract extract rich linguistic features resulting
from our HPSG high-precision grammar to train an
ML classifier.
2 Methodology
2.1 Task 1: Shallow Features and CRFs
Our system consists of two main modules, the first
of which is devoted to the detection of event trigger
words, and the second to event?theme analysis.
2.1.1 Trigger-word detection
We developed two separate systems to perform
trigger word detection, and also a hybrid system
which combines their outputs. The first system is
a simple dictionary-based look-up tagger; the sec-
ond system learns a structured model from the train-
ing data using conditional random fields (CRFs).
For pre-processing, we relied on the domain-specific
token and sentence splitter from the JULIE Lab
(Tomanek et al, 2007) and the GENIA tagger for
lemmatisation, POS tagging, chunking, and protein
detection (Tsuruoka et al, 2005).
The look-up tagger operates by counting the oc-
currences in the training data of different event tags
for a given term. Over the development and test data,
each occurrence of a given term is assigned the event
class with the highest prior in the training data. We
experimented with a frequency cut-off that allows us
to explore the precision/recall trade-off.
Our second system relies on CRFs, as imple-
mented in the CRF++ toolkit (Lafferty et al, 2001).
CRFs provide a discriminative framework for build-
ing structured models to segment and label sequence
data. CRFs have the well-known advantage that they
both model sequential effects and support the use of
large numbers of features. In our experiments we
used the following feature types: word-forms, lem-
mas, POS, chunk tags, protein annotation, and gram-
matical dependencies. For dependency annotation,
we used the Bikel parser and GDep as provided by
the organisers. This information was provided as a
feature that expresses the grammatical function of
the token. We explored window sizes of?3 and?4.
Finally, we tested combining the outputs of the
look-up tagger and CRF, by selecting all trigger
words from both outputs.
77
2.1.2 Event-theme construction
We constructed the output for task 1 by differenti-
ating among three types of events, according to their
expected themes: basic events, binding events, and
regulation events. We applied a simple strategy, as-
signing the closest events or proteins within a given
sentence as themes.
For the basic events, we simply assigned the clos-
est protein, an approach that we found to perform
well over the training and development data. For
binding events, we estimated the maximum dis-
tance away from the event word(s) for themes, and
the maximum number of themes. For regulation
events, we had to choose between proteins or events
as themes, and the CAUSE field was also required.
Again, we relied on a maximum distance threshold,
and gave priority to events over proteins as themes.
We removed regulation events as theme candidates,
since our basic approach could not indicate the di-
rection of the regulation. We also tested predicting
the CAUSE by relying on the protein closest to the
regulation event.
2.2 Task 3: Deep Parsing and Maximum
Entropy classification
For task 3 we ran a syntactic parser over the abstracts
and used the outputs to construct feature vectors for
a machine learning algorithm. We built two classi-
fiers (possibly with overlapping sets of feature vec-
tors) for each training run: one to identify specula-
tion and one for negation. We deliberately built a
separate binary classifier for each task instead of a
single four-class classifier, since the problem natu-
rally decomposes this way. Speculation and nega-
tion are independent of one another (informally, not
statistically) and it enables us to focus on feature en-
gineering for each subtask.
2.2.1 Deep Parsing with the ERG
It seemed likely that syntactico-semantic analysis
would be useful for task 3. To identify negation or
speculation with relatively high precision, it is prob-
able that knowledge of the relationships of possibly
distant elements (such as the negation particle not)
to a particular target word would provide valuable
information for classification.
Further to this, it was our intention to evaluate
the utility of deep parsing in such an approach,
rather than a shallower annotation such as the out-
put of a dependency parser. With this in mind,
we selected the English Resource Grammar1 (ERG:
Copestake and Flickinger (2000)), an open-source,
broad-coverage high-precision grammar of English
in the HPSG framework.
While the ERG is relatively robust across dif-
ferent domains, it is a general-purpose resource,
and there are some aspects of the language used in
the biomedical abstracts that cause difficulties; un-
known word handling is especially important given
the nature of terms in the domain. Fortunately we
can make some optimisations to mitigate this. The
GENIA tagger mentioned in Section 2.1.1 provides
both POS and named entity annotations, which we
used to constrain the input to the ERG in two ways:
? Biological named entities identified by the GE-
NIA tagger are flagged as such, and the parser
does not attempt to decompose them.
? POS tags are appended to each input token to
constrain the token to an appropriate category
if it is absent from the ERG lexicon.
With these modifications to the parser, as well as
preprocessing to handle differences in the tokenisa-
tion expected by the ERG to the output of the tagger,
we were able to obtain a spanning parse for 72% of
the training sentences. This still leaves 28% of the
sentences inaccessible ? the need for a fallback strat-
egy is discussed further in Section 4.2.
2.2.2 Feature Extraction from RMRSs
Rather than outputting syntactic parse trees, the
ERG can also produce output in particular semantic
formalisms: Minimal Recursion Semantics (MRS:
Copestake et al (2005)) and the closely related Ro-
bust Minimal Recursion Semantics (RMRS: Copes-
take (2004)). For our feature generation here we
make use of the latter.
Figure 1 shows an example RMRS obtained from
one of the training documents. While there is in-
sufficient space to give a complete treatment here,
we highlight several aspects for expository purposes.
1Specifically the July 2008 version, downloadable from
http://lingo.stanford.edu/ftp/test/
78
l1,
{ l3: thus a 1?62:67?(e5, ARG1: h4),
l16: generic unk nom rel?68:78?(x11, CARG: ?nf- kappa b?),
l6: udef q rel?68:89?(x9, RSTR: h8, BODY: h7),
l10: compound rel?68:89?(e12, ARG1: x9, ARG2: x11),
l13: udef q rel?68:89?(x11, RSTR: h15, BODY: h14),
l101: activation n 1?79:89?(x9),
l17: neg rel?94:97?(e19, ARG1: h18),
l20: require v 1?98:106?(e2, ARG1: u21, ARG2: x9),
l102: parg d rel?98:106?(e22, ARG1: e2, ARG2: x9),
l103: for p?107:110?(e24, ARG1: e2, ARG2: x23),
l34: generic unk nom rel?111:129?(x29,
CARG: ?neuroblastoma cell?),
l25: udef q rel?111:146?(x23, RSTR: h27, BODY: h26),
l28: compound rel?111:146?(e30, ARG1: x23, ARG2: x29),
l31: udef q rel?111:146?(x29, RSTR: h33, BODY: h32),
l104: differentiation n of?130:146?(x23, ARG1: u35) },
{ h4 qeq l17, h8 qeq l10, h15 qeq l16, h18 qeq l20, h27 qeq l28,
h33 qeq l34 },
{ l10 in-g l101, l20 in-g l102, l20 in-g l103, l28 in-g l104 }
Figure 1: RMRS representation of the sentence Thus NF-
kappa B activation requires neuroblastoma cell differ-
entiation showing, in order, elementary predicates, qeq-
constraints, and in-g constraints
The primary component of an RMRS is bag of ele-
mentary predicates, or EPs. Each EP shown has: (a)
a label, such as ?l104?; (b) a predicate name, such as
? differentiation n 1? (where ?n? indicates the part-
of-speech); (c) character indices to the source sen-
tence; and (d) a set of arguments. The first argu-
ment is always ARG0 and is afforded special sta-
tus, generally referring to the variable introduced by
the predicate. Subsequent arguments are labelled ac-
cording to the relation of the argument to the pred-
icate. Arguments can be variables such as ?e30? or
?x23? (where the first letter indicates the nature of
the variable ? ?e? referring to events and ?x? to enti-
ties), or handles such as ?h33?.
These handles are generally used in the qeq con-
straints, which relate a handle to a label, indicating
a particular kind of outscoping relationship between
the handle and the label ? either that the handle and
label are equal or that the handle is equal to the label
except that one or more quantifiers occur between
the two (the name is derived from ?equality mod-
ule quantifiers?). Finally there are in-g constraints
which indicate that labels can be treated as equal.
For our purposes this simply affects which qeq con-
straints they participate in ? for example from the
in-g constraint ?l28 in-g l104? and the qeq constraint
?h27 qeq l28?, we can also infer that ?h27 qeq l104?.
In constructing features, we make use of:
? The outscopes relationship (specifically qeq-
outscopes) ? if EP A has a handle argument
which qeq-outscopes the label of EP B, A is
said to immediately outscope B ; outscopes is
the transitive closure of this.
? The shared-argument relationship, where EPs
C and D refer to the same variable in one
or more of their argument positions. We also
in some cases make further restrictions on the
types of arguments (ARG0 , RSTR , etc) that
may be shared on either end of the relationship.
2.2.3 Feature Sets and Classification
Feature vectors for a given event are constructed
on the basis of the trigger word for the particular
event, which we assume has already been identified;
a natural consequence is that all events with the same
trigger words have identical feature vectors. We use
the term trigger EPs to describe the EP(s) which cor-
respond to that trigger word ? i.e. those whose char-
acter span encompasses the trigger word. We have
a potentially large set of related EPs (with the kinds
of relationships described above), which we filter to
create the various feature sets, as outlined below.
We have several feature sets targeted at identify-
ing negation:
? NEGOUTSCOPE2: If any EPs in the RMRS
have predicate names in { no q, but+not c,
nor c, only a, never a, not+as+yet a,
not+as+yet a, unable a, neg rel}, and that
EP outscopes a trigger EP, set a general feature
as well as a specific one for the particle.
? NEGCONJINDEX: If any EPs in the RMRS
have predicate names in { not c, but+not c,
nor c}, the R-INDEX (RHS of a conjunction)
of that EP is the ARG0 a trigger EP, set a gen-
eral feature as well as a specific one for the par-
ticle ? capturing the notion that these conjunc-
tions are semantically negative for the particle
on the right. This also had a corresponding fea-
ture for the L-INDEX of nor c, corresponding
to the LHS of the neither...nor construction.
79
? ARG0NEGOUTSCOPEESA: For any EPs
which have an argument that matches the
ARG0 of a trigger EP, if they are outscoped
by an EP whose predicate name is in
the list { only a, never a, not+as+yet a,
not+as+yet a, unable a, neg rel}, set a gen-
eral feature to true, as well as features for the
name of the outscoping and outscoped EPs.
This is designed to catch trigger EP which are
nouns, where the verb of which they are subject
or object (or indeed an adjective/preposition to
which they are linked) is semantically negated.
And several targeted at identifying speculation:
? SPECVOBJ2: if a verb is a member of
the set { investigate, study, examine, test,
evaluate, observe} and its ARG2 (which cor-
responds to the verb object) is the ARG0 of a
trigger EP. This has a general feature for if any
of the verbs match, and a feature which is spe-
cific to each verb in the target list.
? SPECVOBJ2+WN: as above, but augment the
list of seed verbs with a list of WordNet sisters
(i.e. any lemmas from any synsets for the verb),
and add a feature which is set for the seed verbs
which gave rise to other sister verbs.
? MODALOUTSCOPE: modal verbs (can, should,
etc) may be strong indicators of specula-
tion; this sets a value when the trigger EP is
outscoped by any predicate corresponding to a
modal, both as a general feature and a specific
feature for the particular modal.
? ANALYSISSA: the ARG0 of the trigger EP is
also an argument of an EP with the predicate
name analysis n. Such constructions involv-
ing the word analysis are relatively frequent in
speculative events in the data.
And some general features, aiming to see if the
learning algorithm could pick up other patterns we
had missed:
? TRIGPREDPROPS: Set a feature value for the
predicate name of each trigger EP, as well as
the POS of each trigger EP.
? TRIGOUTSCOPES: Set a feature value for the
predicate name and POS of each EP that is
outscoped by the trigger EP.
? MODADJ: Set a feature value for any EPs
which have an ARG1 which matches the ARG0
of the trigger EP if their POS is marked as ad-
jective or adverb.
? +CONJ: This is actually a variant on the feature
extraction method, which attempts to abstract
away the effect of conjunctions. If the trigger
EP is a member of a conjunction (i.e. shares
an ARG0 with the L-INDEX or R-INDEX of a
conjunction), also treat the EPs which are con-
junction parents (and their conjunctive parents
if they exist) as trigger EPs in the feature con-
struction.
2.2.4 Implementation
To produce training data to feed into a classifier,
we parsed as many sentences as possible using the
ERG, and used the output RMRSs to create train-
ing data using various combinations of the feature
sets described above. The construction of features,
however, presupposes annotations for the events and
trigger words. For producing training data, we used
the provided trigger annotations. For the test phase,
we simply use the outputs of the classifier we built
in phase 1, selecting the combination with the best
performance over the development set. This pipeline
architecture places limits on annotation performance
? in particular, the recall in task 1 is an upper bound
on task 3 recall. We used a maximum entropy clas-
sification algorithm for the ML component here ? it
has a low level of parameterization and is a solid per-
former in NLP tasks. The implementation we used
was Zhang Le?s Maxent Toolkit.2
3 Development experiments
3.1 Task 1
We devised a set of experiments over the trial, train-
ing, and development data in order to estimate the
parameters for our final submission. Using the trial
data, we performed manual error analysis on the
rules used to construct events. With the training
2http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
80
data, we performed our own evaluation based on
cross-validation to detect trigger words and con-
struct events. For the experiments over the devel-
opment data, we relied on the evaluation interface
provided by the organisation. We focused on testing
the following modules: look-up tagger, CRF, com-
bined system, and event construction.
First, we tuned the parameters of our look-up tag-
ger over the training data. We used a threshold on
the minimum number of term occurrences required
to use the class information for that term from the
training data. We evaluated thresholding on raw fre-
quencies, and also on the percentage of occurrences
of the term that were linked to the majority event. In
cross-validation over the training data, we found that
the raw-frequency threshold worked best, achiev-
ing a maximum F-score of 38.86%, as compared to
30.81% for the percentage approach (the results are
shown in the bottom part of Table 1). We also es-
timated the frequency threshold as ? 25, and ob-
served that most of the terms identified consisted of
a single word, due to data sparseness in the training
set.
Our next experiments are devoted to the CRF sys-
tem, focusing on feature engineering. The results
over the training data for: (a) the full feature set, and
(b) removing one feature type at a time, are shown
in Table 1, for windows of size ?3 and ?4. We can
see that the best F-score is achieved by the?3 word-
window system when removing the syntactic depen-
dencies from Bikel?s parser. These results improved
over the look-up system.
As a final experiment on feature combinations
and window size, we used the development evalu-
ation interface. We submitted the best combinations
shown in the above experiment, and also syntactic
dependencies extracted with GDep. We observed
the same behaviour as in training data, with the ?3
word window obtaining the best F-score, and syn-
tactic dependencies harming performance. These re-
sults are shown in the upper part of Table 2. Our
final CRF system used this configuration (?3 word
window and all feature types except syntactic depen-
dencies).
Our next step was to test the integration of the
look-up tagger and CRF into a single system. We
observed that by combining the outputs directly we
W. size Feats. Rec. Prec. FSc.
?3 All 30.28 64.44 41.20
?3 ?synt. dep. 30.20 65.01 41.24
?3 ?protein NER 28.04 65.73 39.31
?3 ?chunking 30.13 65.16 41.20
?3 ?POS 29.68 65.25 40.80
?3 ?lemma 27.96 62.60 38.66
?3 ?word form 29.98 63.81 40.79
?4 All 28.86 66.15 40.19
?4 ?synt. dep. 29.75 67.06 41.22
?4 ?protein NER 28.11 66.73 39.56
?4 ?chunking 28.56 66.61 39.98
?4 ?POS 28.19 66.67 39.62
?4 ?lemma 26.55 65.20 37.73
?4 ?word form 28.19 65.28 39.38
Look-up (freq.) 52.14 30.97 38.86
Look-up (perc.) 38.20 25.82 30.81
Table 1: Trigger-word detection performance over train-
ing data. Results for the look-up tagger and CRFs with
the full feature set and when removing one feature type
at a time, for 3 and 4 word windows. The best results per
column are shown in bold.
W. size Feats. Rec. Prec. FSc.
?3 All - synt. 17.55 56.17 26.75
?4 All - synt. 17.38 56.75 26.62
?3 All (GDep) 15.48 58.69 24.50
Combined (All) 26.94 27.83 27.38
Combined (Best) 21.24 39.92 27.73
Table 2: Performance of selected feature and window-
size combinations over development data. Best results
per column are given in bold.
could improve over the recall of CRF, and achieve
higher F-score. This approach is referred to as
?Combined (All)? in Table 2. We also tested the
results when choosing either the look-up tagger or
CRF depending on their performance over each
event in the training data. The results of this sys-
tem (?Combined (Best)?) show a slight improve-
ment over the basic combination.
Finally, we analysed the results of the event con-
struction step. We used the gold-standard trigger an-
notation over the trial data and analysed the errors of
our rules. We found out that there were three main
types of error: (1) incorrect assignation of regulation
themes; (2) trigger words having multiple themes;
and (3) themes crossing sentence boundaries. We
plan to address these problems in future work. We
also observed that predicting CAUSE for the regula-
tory events caused the F-score to drop, resulting in
us removing this functionality from the system.
81
N1: NEGOUTSCOPE2+CONJ, NEGCONJINDEX
N2: N1, TRIGPREDPROPS
N3: N1, ARG0NEGOUTSCOPEESA
N4: N3, TRIGPREDPROPS, NEGVOUTSCOPE
N5: N3, NEGVOUTSCOPE
S1: SPECVOBJ2+WN+CONJ, ANALYSISSA
S2: S1, TRIGPREDPROPS
S3: S1, MODADJ, MODALOUTSCOPE
S4: S3, TRIGOUTSCOPES
S5: SPECVOBJ2+WN+CONJ, MODADJ,
MODALOUTSCOPE,TRIGOUTSCOPES
B+y?x: Context window of lemmatized tokens: x preceding and yfollowing.
Table 3: Task 3 feature sets
3.2 Task 3
We evaluated the classification performance of vari-
ous feature sets (including some not described here)
using 10-fold cross-validation over the training data
in the initial stages. We ran various combinations of
the most promising features over the development
data and evaluated their relative performance in an
attempt to avoid overfitting.
To evaluate the performance boost we got in task
3 relative to more naive methods, we also experi-
mented with feature sets based on a bag-of-words
approach with a sliding context window of lemma-
tised tokens on either side. We evaluated all com-
binations of preceding and following context win-
dow sizes from 0 to 3. There are features for tokens
that precede the trigger, follow the trigger, or lie any-
where within the context window, as well as for the
trigger itself. A ?token? here may also be a named
biological entity (protein etc) produced by GENIA
tagger in our preprocessing phase, which would not
be lemmatised. For comparability we only evaluate
these features for sentences which we were able to
parse. For the best performing baseline and RMRS-
based feature sets, we also tested them in combina-
tion to see whether the features produced were com-
plementary.
In Table 4 we present the results over the develop-
ment data, using the provided gold-standard annota-
tions of trigger words, as well as some selected re-
sults for our other task 1 outputs. The gold-standard
figures are unrealistically high compared to what
we would expect to achieve against the test data,
but they are indicative at least of what we could
achieve with a perfect event classifier. Similar to
Task 1 Mod Feats. Rec. Prec. FSc.
Gold Spec B+2?2 23.2 40.0 29.3
Gold Spec B+3?3 22.1 47.7 30.2
Gold Spec S2 15.8 83.3 26.5
Gold Spec S3 18.9 78.3 30.5
Gold Spec S3,B+2?2 21.1 58.8 31.0
Gold Spec S3,B+3?3 23.2 57.9 33.1
Comb(best) Spec S3 4.2 21.0 7.0
Gold Spec S4 17.9 94.4 30.1
Gold Spec S5 17.9 100.0 30.4
Gold Neg B+0?2 14.0 33.3 19.7
Gold Neg B+1?3 15.0 30.2 20.0
Gold Neg N2 19.6 61.8 29.8
Comb(best) Neg N2 0.9 7.7 1.7
Gold Neg N3 15.9 68.0 25.8
Gold Neg N4 19.6 67.7 30.4
Gold Neg N4,B+1?3 22.4 52.2 31.4
Gold Neg N4,B+0?2 24.3 68.4 35.9
Gold Neg N5 16.8 69.2 30.1
Table 4: Results (exact match) over development data
for task 3 using gold-standard event/trigger annotations
and selected other annotations for task 1. Feature sets
described in Table 3
task 1, our system shows reasonable precision but
suffers badly in recall. The substantially poorer per-
formance when using our own annotations for the in-
put events is discussed in more detail in Section 4.2
One area where we could improve is to go after
the 30% of sentences for which we do not have a
spanning parse and resultant RMRS. To reuse ex-
isting infrastructure, we could produce RMRS out-
put from an alternative processing component with
broader coverage but less precision. Several meth-
ods exist to do this ? e.g. producing RMRS out-
put from RASP (Briscoe et al, 2006) is described in
Frank (2004). However there is clearly room for im-
provement in the remaining 70% of sentences which
we can parse ? our results in Table 4 are still well
below the limit of roughly 70% recall.3
Additional lexical resources beyond WordNet,
particularly domain-specific ones, are likely to be
useful in boosting performance since they will help
maximally utilise the training data. Additionally,
we have not yet made use of other event annota-
tions apart from the trigger words ? features based
on characteristics such as the event class or proper-
ties of the event arguments could also be useful.
3We have not performed any analysis to verify whether the
number of events per sentence differs between parseable and
unparseable sentences.
82
System Rec. Prec. FSc.
Combined (Best) 17.44 39.99 24.29
Combined (All) 24.36 30.87 27.23
CRF 12.23 62.24 20.44
CRF (+ synt feats) 12.01 61.91 20.11
Look-Up 22.88 29.67 25.84
Look-Up (freq >= 20) 23.26 26.74 24.88
Look-Up (freq >= 30) 21.37 30.50 25.13
Table 5: Task 1 results with approximate span matching,
recursive evaluation (our final submission is in bold)
4 Results
4.1 Task 1
Our experiments on the training and development set
showed that our CRF++ was biased towards preci-
sion at the cost of recall, and for the look-up system
the best F-score was obtained when aiming for high
recall at the cost of lower precision. The best results
were obtained when combining both approaches,
and this was the composition of the system we sub-
mitted.
For our final submission, the CRF++ approach
had a ?3 word window, and all the features ex-
cept for syntactic dependencies, which were found
to harm performance. Our final look-up system re-
lied on raw frequencies to choose candidate terms,
and those above 24 occurrences in training data were
included in the dictionary. For the combination, we
observed that for most events the look-up system
performed better (although the overall F-score was
lower), and we decided to use the CRF++ output
only for the events that showed better performance
than the look-up system (TRANSCRIPTION, GENE
EXPRESSION, and POSITIVE REGULATION).
The results over the test data for our final submis-
sion and the main variants we explored are shown in
Table 5. We can see that the CRF performed poorly,
with very low recall over the test set, in contrast with
the development results, where the higher recall re-
sulted in a higher F-score than the look-up approach.
The best of our systems was the full combination of
CRF and the look-up tagger, with a 27.23% F-score.
The results for each event separately are given in
Table 6. The system performs much worse on regu-
lation events, due to the difficulty of having to cor-
Event Class Rec. Prec. FSc.
Localization 25.86 65.22 37.04
Binding 17.00 28.92 21.42
Gene-expression 45.71 69.18 55.05
Transcription 34.31 26.26 29.75
Protein-catabolism 42.86 85.71 57.14
Phosphorylation 45.19 64.21 53.04
EVT-TOTAL 35.84 53.15 42.81
Regulation 15.46 13.24 14.26
Positive-regulation 13.84 14.82 14.31
Negative-regulation 12.14 20.44 15.23
REG-TOTAL 13.73 15.31 14.48
ALL-TOTAL 24.36 30.87 27.23
Table 6: Results for the different events from our com-
bined system. Averaged scores for single events, regula-
tions, and all.
rectly identify other events in the near context.
4.2 Task 3
For testing, we repurposed all of the development
data as training data and retrained our classifiers.
The results in Table 7 were somewhat disappointing,
but a drop in recall versus the equivalent run over
the development data using oracle task 1 annotations
was unsurprising and the ratio of this drop is within
the bounds of what we would expect. The substan-
tial drop in precision can similarly be explained by
flow-on effects from our task 1 classification, a nat-
ural consequence of our pipeline architecture. It is
quite possible for our system to identify false pos-
itive events as being modified; in the online eval-
uation system, these classifications of non-existent
events reduce our precision in task 3.
In the feature engineering stage, we primarily
used the oracle data for task 1 to maximise the
amount of training data available. We felt that if we
were to use our task 1 classifications for events and
trigger words, the effectively lower number of train-
ing instances would only hurt performance. How-
ever this possibly led to bias towards features which
were more useful for classifying events that we
couldn?t successfully classify in task 1. The devel-
opment set shows similar performance drops under
these conditions in Table 4.
It is also possible that our features work reason-
ably but that our classification engine trained over
the oracle data simply learnt the wrong parameters
83
Task 1 Mod Fts. Rec. Prec. FSc.
Comb(Best) Spc B+3?3 2.88 12.24 4.67
Comb(Best) Spc S2 4.33 37.50 7.76
Comb(Best) Spc S3 4.81 30.30 8.30
Comb(All) Spc S3 5.29 26.19 8.80
Comb(Best) Spc S3,B+3?3 4.81 14.08 7.17
Comb(Best) Spc S4 3.85 27.59 6.75
Comb(Best) Spe S5 3.85 27.59 6.75
Comb(Best) Neg B+3?1 3.96 25.00 6.84Comb(Best) Neg N2 5.29 34.48 9.17
Comb(All) Neg N2 5.73 30.00 9.62
Comb(Best) Neg N3 5.29 27.78 8.88
Comb(Best) Neg N4 5.29 34.48 9.17
Comb(Best) Neg N4,B+0?2 4.85 28.12 8.27
Comb(All) Neg N4 5.73 27.27 9.47
Comb(Best) Neg N5 5.29 29.41 8.96
Table 7: Results over test data for task 3 using gold-
standard event annotations (approx recursive matching),
showing which set of trigger word classifications from
task 1 was used as input (submitted results in bold). Fea-
ture sets described in table 3
for the events we had identified correctly in task 1.
We could check this by training a classifier using
our task 1 event classifications combined with the
gold-standard trigger annotations. However com-
bining the gold-standard annotations for task 3 with
the classifier outputs of task 1 is non-trivial and was
not attempted due to time constraints. It also would
have been instructive to calculate a ceiling on our
task 3 performance given our performance in task 1
? i.e. how many modifications we could have cor-
rectly identified with a perfect task 3 classifier, but
we were not able to show this for similar reasons.
5 Conclusions
Our analysis of task 1 seemed to indicate that the
scarcity of training instances was the main reason
for the low recall of CRFs. The look-up system con-
tributed to increase the recall, but at the cost of lower
precision. In order to improve this module we plan
to find ways to extend the training data automatically
in a bootstrapping process.
Another limitation of our system is the event-
construction module, which follows simple rules
and performs poorly on regulation events. For this
subtask we plan to extend the rule set and apply op-
timisation techniques, following the lessons learned
in error analysis.
In task 3 we investigated the application of a pre-
cise, general-purpose grammar over this domain,
and were relatively successful. However, while the
parse coverage for task 3 is very respectable for a
precision grammar on comparatively difficult mate-
rial, it is clearly unwise to throw away 30% of sen-
tences, so a method to extract features from these is
desirable. Further sources of data would also be use-
ful, such as data from the event annotations them-
selves, and additional lexical resources tailored to
the biomedical domain.
We have also shown the syntactico-semantic out-
put of a deep parser, in the form of an RMRS, can
be beneficial in such a task compared with a more
naive approach based on bags of words within a
sliding context window. From Table 4, for nega-
tion, the syntactic features provided substantial per-
formance gains over the best set of baseline param-
eters we could find. For speculation the evidence
here is less compelling, with similar scores from
both approaches. Over test data in Table 7, the the
deep methods showed superior performance, albeit
over a smaller number of instances. Regardless, the
RMRS still has some advantages, giving (unsurpris-
ingly) higher precision than the baseline methods.
Combining naive and deep features does tend to give
slightly higher performance than either of the inputs
over the development data (although not over the test
data, perhaps due to the poorer performance of naive
methods), suggesting that the two approaches iden-
tify slightly different kinds of modification.
Our system suffered from the pipeline approach ?
there was no way to recover from an incorrect classi-
fication in task 1, resulting in greatly reduced preci-
sion and recall in task 3. It is possible that a carefully
constructed integrated system could annotate events
for trigger words and argument at the same time as
modification, with features shared between the two,
which may avoid some of these issues.
Acknowledgements
We wish to thank Rebecca Dridan, Dan Flickinger
and Lawrence Cavedon for their advice. NICTA
is funded by the Australian Government as repre-
sented by the Department of Broadband, Communi-
cations and the Digital Economy and the Australian
Research Council through the ICT Centre of Excel-
lence program.
84
References
Edward Briscoe, John Carroll, and Rebecca Watson.
2006. The second release of the RASP system. In Pro-
ceedings of the COLING/ACL 2006 Interactive Poster
System, pages 77?80, Sydney, Australia.
Ann Copestake and Dan Flickinger. 2000. An open
source grammar development environment and broad-
coverage English grammar using HPSG. In Interna-
tional Conference on Language Resources and Evalu-
ation.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: An in-
troduction. Research on Language and Computation,
pages 281?332.
Ann Copestake. 2004. Report on the design of RMRS.
Technical Report D1.1a, University of Cambridge,
Cambridge, UK.
Anette Frank. 2004. Constraint-based RMRS construc-
tion from shallow grammars. In COLING ?04: Pro-
ceedings of the 20th international conference on Com-
putational Linguistics, page 1269, Morristown, NJ,
USA. Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning, pages 282?289.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on condi-
tional random fields. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 49?57, Melbourne, Australia.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics - 10th Panhellenic Conference on Infor-
matics, pages 382?392.
85
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Chart Mining-based Lexical Acquisition with Precision Grammars
Yi Zhang,? Timothy Baldwin,?? Valia Kordoni,? David Martinez? and Jeremy Nicholson??
? DFKI GmbH and Dept of Computational Linguistics, Saarland University, Germany
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? NICTA Victoria Research Laboratory
yzhang@coli.uni-sb.de, tb@ldwin.net, kordoni@dfki.de,
{davidm,jeremymn}@csse.unimelb.edu.au
Abstract
In this paper, we present an innovative chart
mining technique for improving parse cover-
age based on partial parse outputs from preci-
sion grammars. The general approach of min-
ing features from partial analyses is applica-
ble to a range of lexical acquisition tasks, and
is particularly suited to domain-specific lexi-
cal tuning and lexical acquisition using low-
coverage grammars. As an illustration of the
functionality of our proposed technique, we
develop a lexical acquisition model for En-
glish verb particle constructions which oper-
ates over unlexicalised features mined from
a partial parsing chart. The proposed tech-
nique is shown to outperform a state-of-the-art
parser over the target task, despite being based
on relatively simplistic features.
1 Introduction
Parsing with precision grammars is increasingly
achieving broad coverage over open-domain texts
for a range of constraint-based frameworks (e.g.,
TAG, LFG, HPSG and CCG), and is being used in
real-world applications including information ex-
traction, question answering, grammar checking and
machine translation (Uszkoreit, 2002; Oepen et al,
2004; Frank et al, 2006; Zhang and Kordoni, 2008;
MacKinlay et al, 2009). In this context, a ?preci-
sion grammar? is a grammar which has been engi-
neered to model grammaticality, and contrasts with
a treebank-induced grammar, for example.
Inevitably, however, such applications demand
complete parsing outputs, based on the assumption
that the text under investigation will be completely
analysable by the grammar. As precision grammars
generally make strong assumptions about complete
lexical coverage and grammaticality of the input,
their utility is limited over noisy or domain-specific
data. This lack of complete coverage can make
parsing with precision grammars less attractive than
parsing with shallower methods.
One technique that has been successfully applied
to improve parser and grammar coverage over a
given corpus is error mining (van Noord, 2004;
de Kok et al, 2009), whereby n-grams with low
?parsability? are gathered from the large-scale out-
put of a parser as an indication of parser or (pre-
cision) grammar errors. However, error mining is
very much oriented towards grammar engineering:
its results are a mixture of different (mistreated) lin-
guistic phenomena together with engineering errors
for the grammar engineer to work through and act
upon. Additionally, it generally does not provide
any insight into the cause of the parser failure, and it
is difficult to identify specific language phenomena
from the output.
In this paper, we instead propose a chart min-
ing technique that works on intermediate parsing re-
sults from a parsing chart. In essence, the method
analyses the validity of different analyses for words
or constructions based on the ?lifetime? and prob-
ability of each within the chart, combining the con-
straints of the grammar with probabilities to evaluate
the plausibility of each.
For purposes of exemplification of the proposed
technique, we apply chart mining to a deep lexical
acquisition (DLA) task, using a maximum entropy-
based prediction model trained over a seed lexicon
and treebank. The experimental set up is the fol-
lowing: given a set of sentences containing puta-
tive instances of English verb particle constructions,
10
extract a list of non-compositional VPCs optionally
with valence information. For comparison, we parse
the same sentence set using a state-of-the-art statisti-
cal parser, and extract the VPCs from the parser out-
put. Our results show that our chart mining method
produces a model which is superior to the treebank
parser.
To our knowledge, the only other work that has
looked at partial parsing results of precision gram-
mars as a means of linguistic error analysis is that of
Kiefer et al (1999) and Zhang et al (2007a), where
partial parsing models were proposed to select a set
of passive edges that together cover the input se-
quence. Compared to these approaches, our pro-
posed chart mining technique is more general and
can be adapted to specific tasks and domains. While
we experiment exclusively with an HPSG grammar
in this paper, it is important to note that the proposed
method can be applied to any grammar formalism
which is compatible with chart parsing, and where it
is possible to describe an unlexicalised lexical entry
for the different categories of lexical item that are to
be extracted (see Section 3.2 for details).
The remainder of the paper is organised as fol-
lows. Section 2 defines the task of VPC extraction.
Section 3 presents the chart mining technique and
the feature extraction process for the VPC extraction
task. Section 4 evaluates the model performance
with comparison to two competitor models over sev-
eral different measures. Section 5 further discusses
the general applicability of chart mining. Finally,
Section 6 concludes the paper.
2 Verb Particle Constructions
The particular construction type we target for DLA
in this paper is English Verb Particle Constructions
(henceforth VPCs). VPCs consist of a head verb
and one or more obligatory particles, in the form
of intransitive prepositions (e.g., hand in), adjec-
tives (e.g., cut short) or verbs (e.g., let go) (Villav-
icencio and Copestake, 2002; Huddleston and Pul-
lum, 2002; Baldwin and Kim, 2009); for the pur-
poses of our dataset, we assume that all particles are
prepositional?by far the most common and produc-
tive of the three types?and further restrict our atten-
tion to single-particle VPCs (i.e., we ignore VPCs
such as get alng together).
One aspect of VPCs that makes them a partic-
ularly challenging target for lexical acquisition is
that the verb and particle can be non-contiguous (for
instance, hand the paper in and battle right on).
This sets them apart from conventional collocations
and terminology (cf., Manning and Schu?tze (1999),
Smadja (1993) and McKeown and Radev (2000))
in that they cannot be captured effectively using n-
grams, due to their variability in the number and type
of words potentially interceding between the verb
and the particle. Also, while conventional colloca-
tions generally take the form of compound nouns
or adjective?noun combinations with relatively sim-
ple syntactic structure, VPCs occur with a range of
valences. Furthermore, VPCs are highly productive
in English and vary in use across domains, making
them a prime target for lexical acquisition (Dehe?,
2002; Baldwin, 2005; Baldwin and Kim, 2009).
In the VPC dataset we use, there is an addi-
tional distinction between compositional and non-
compositional VPCs. With compositional VPCs,
the semantics of the verb and particle both corre-
spond to the semantics of the respective simplex
words, including the possibility of the semantics be-
ing specific to the VPC construction in the case of
particles. For example, battle on would be clas-
sified as compositional, as the semantics of bat-
tle is identical to that for the simplex verb, and
the semantics of on corresponds to the continua-
tive sense of the word as occurs productively in
VPCs (cf., walk/dance/drive/govern/... on). With
non-compositional VPCs, on the other hand, the
semantics of the VPC is somehow removed from
that of the parts. In the dataset we used for eval-
uation, we are interested in extracting exclusively
non-compositional VPCs, as they require lexicalisa-
tion; compositional VPCs can be captured via lexi-
cal rules and are hence not the target of extraction.
English VPCs can occur with a number of va-
lences, with the two most prevalent and productive
valences being the simple transitive (e.g., hand in
the paper) and intransitive (e.g., back off ). For the
purposes of our target task, we focus exclusively on
these two valence types.
Given the above, we define the English VPC ex-
traction task to be the production of triples of the
form ?v, p, s?, where v is a verb lemma, p is a prepo-
sitional particle, and s ? {intrans , trans} is the va-
11
lence; additionally, each triple has to be semantically
non-compositional. The triples are extracted relative
to a set of putative token instances for each of the
intransitive and transitive valences for a given VPC.
That is, a given triple should be classified as positive
if and only if it is associated with at least one non-
compositional token instance in the provided token-
level data.
The dataset used in this research is the one used
in the LREC 2008 Multiword Expression Workshop
Shared Task (Baldwin, 2008).1 In the dataset, there
is a single file for each of 4,090 candidate VPC
triples, containing up to 50 sentences that have the
given VPC taken from the British National Cor-
pus. When the valence of the VPC is ignored,
the dataset contains 440 unique VPCs among 2,898
VPC candidates. In order to be able to fairly com-
pare our method with a state-of-the-art lexicalised
parser trained over the WSJ training sections of the
Penn Treebank, we remove any VPC types from the
test set which are attested in the WSJ training sec-
tions. This removes 696 VPC types from the test
set, and makes the task even more difficult, as the
remaining testing VPC types are generally less fre-
quent ones. At the same time, it unfortunately means
that our results are not directly comparable to those
for the original shared task.2
3 Chart Mining for Parsing with a Large
Precision Grammar
3.1 The Technique
The chart mining technique we use in this paper
is couched in a constituent-based bottom-up chart
parsing paradigm. A parsing chart is a data struc-
ture that records all the (complete or incomplete) in-
termediate parsing results. Every passive edge on
the parsing chart represents a complete local analy-
sis covering a sub-string of the input, while each ac-
tive edge predicts a potential local analysis. In this
view, a full analysis is merely a passive edge that
spans the whole input and satisfies certain root con-
1Downloadable from http://www.csse.unimelb.
edu.au/research/lt/resources/vpc/vpc.tgz.
2In practice, there was only one team who participated in
the original VPC task (Ramisch et al, 2008), who used a vari-
ety of web- and dictionary-based features suited more to high-
frequency instances in high-density languages, so a simplistic
comparison would not have been meaningful.
ditions. The bottom-up chart parser starts with edges
instantiated from lexical entries corresponding to the
input words. The grammar rules are used to incre-
mentally create longer edges from smaller ones until
no more edges can be added to the chart.
Standardly, the parser returns only outputs that
correspond to passive edges in the parsing chart that
span the full input string. For those inputs without a
full-spanning edge, no output is generated, and the
chart becomes the only source of parsing informa-
tion.
A parsing chart takes the form of a hierarchy of
edges. Where only passive edges are concerned,
each non-lexical edge corresponds to exactly one
grammar rule, and is connected with one or more
daughter edge(s), and zero or more parent edge(s).
Therefore, traversing the chart is relatively straight-
forward.
There are two potential challenges for the chart-
mining technique. First, there is potentially a huge
number of parsing edges in the chart. For in-
stance, when parsing with a large precision gram-
mar like the HPSG English Resource Grammar
(ERG, Flickinger (2002)), it is not unusual for a
20-word sentence to receive over 10,000 passive
edges. In order to achieve high efficiency in pars-
ing (as well as generation), ambiguity packing is
usually used to reduce the number of productive
passive edges on the parsing chart (Tomita, 1985).
For constraint-based grammar frameworks like LFG
and HPSG, subsumption-based packing is used to
achieve a higher packing ratio (Oepen and Carroll,
2000), but this might also potentially lead to an in-
consistent packed parse forest that does not unpack
successfully. For chart mining, this means that not
all passive edges are directly accessible from the
chart. Some of them are packed into others, and the
derivatives of the packed edges are not generated.
Because of the ambiguity packing, zero or more
local analyses may exist for each passive edge on
the chart, and the cross-combination of the packed
daughter edges is not guaranteed to be compatible.
As a result, expensive unification operations must be
reapplied during the unpacking phase. Carroll and
Oepen (2005) and Zhang et al (2007b) have pro-
posed efficient k-best unpacking algorithms that can
selectively extract the most probable readings from
the packed parse forest according to a discrimina-
12
tive parse disambiguation model, by minimising the
number of potential unifications. The algorithm can
be applied to unpack any passive edges. Because
of the dynamic programming used in the algorithm
and the hierarchical structure of the edges, the cost
of the unpacking routine is empirically linear in the
number of desired readings, and O(1) when invoked
more than once on the same edge.
The other challenge concerns the selection of in-
formative and representative pieces of knowledge
from the massive sea of partial analyses in the pars-
ing chart. How to effectively extract the indicative
features for a specific language phenomenon is a
very task-specific question, as we will show in the
context of the VPC extraction task in Section 3.2.
However, general strategies can be applied to gener-
ate parse ranking scores on each passive edge. The
most widely used parse ranking model is the log-
linear model (Abney, 1997; Johnson et al, 1999;
Toutanova et al, 2002). When the model does not
use non-local features, the accumulated score on a
sub-tree under a certain (unpacked) passive edge can
be used to approximate the probability of the partial
analysis conditioned on the sub-string within that
span.3
3.2 The Application: Acquiring Features for
VPC Extraction
As stated above, the target task we use to illustrate
the capabilities of our chart mining method is VPC
extraction.
The grammar we apply our chart mining method
to in this paper is the English Resource Grammar
(ERG, Flickinger (2002)), a large-scale precision
HPSG for English. Note, however, that the method
is equally compatible with any grammar or grammar
formalism which is compatible with chart parsing.
The lexicon of the ERG has been semi-
automatically extended with VPCs extracted
by Baldwin (2005). In order to show the effective-
ness of chart mining in discovering ?unknowns?
and remove any lexical probabilities associated
with pre-existing lexical entries, we block the
3To have a consistent ranking model on any sub-analysis,
one would have to retrain the disambiguation model on every
passive edge. In practice, we find this to be intractable. Also,
the approximation based on full-parse ranking model works rea-
sonably well.
lexical entries for the verb in the candidate VPC
by substituting the input token with a DUMMY-V
token, which is coupled with four candidate lexical
entries of type: (1) intransitive simplex verb (v - e),
(2) transitive simplex verb (v np le), (3) intransitive
VPC (v p le), and (4) transitive VPC (v p-np le),
respectively. These four lexical entries represent the
two VPC valences we wish to distinguish between
in the VPC extraction task, and the competing
simplex verb candidates. Based on these lexical
types, the features we extract with chart mining are
summarised in Table 1. The maximal constituent
(MAXCONS) of a lexical entry is defined to be the
passive edge that is an ancestor of the lexical entry
edge that: (i) must span over the particle, and (ii)
has maximal span length. In the case of a tie,
the edge with the highest disambiguation score is
selected as the MAXCONS. If there is no edge found
on the chart that spans over both the verb and the
particle, the MAXCONS is set to be NULL, with a
MAXSPAN of 0, MAXLEVEL of 0 and MAXCRANK
of 4 (see Table 1). The stem of the particle is also
collected as a feature.
One important characteristic of these features is
that they are completely unlexicalised on the verb.
This not only leads to a fair evaluation with the ERG
by excluding the influence from the lexical coverage
of VPCs in the grammar, but it also demonstrates
that complete grammatical coverage over simplex
verbs is not a prerequisite for chart mining.
To illustrate how our method works, we present
the unpacked parsing chart for the candidate VPC
show off and input sentence The boy shows off his
new toys in Figure 1. The non-terminal edges are
marked with their syntactic categories, i.e., HPSG
rules (e.g., subjh for the subject-head-rule, hadj for
the head-adjunct-rule, etc.), and optionally their dis-
ambiguation scores. By traversing upward through
parent edges from the DUMMY-V edge, all features
can be efficiently extracted (see the third column in
Table 1).
It should be noted that none of these features are
used to deterministically dictate the predicted VPC
category. Instead, the acquired features are used as
inputs to a statistical classifier for predicting the type
of the VPC candidate at the token level (in the con-
text of the given sentence). In our experiment, we
used a maximum entropy-based model to do a 3-
13
Feature Description Examples
LE:MAXCONS
A lexical entry together with the maximal constituent
constructed from it
v - le:subjh, v np le:hadj,
v p le:subjh, v p-np le:subj
LE:MAXSPAN
A lexical entry together with the length of the span of
the maximal constituent constructed from the LE
v - le:7, v np le:5, v p le:4,
v p-np le:7
LE:MAXLEVEL
A lexical entry together with the levels of projections
before it reaches its maximal constituent
v - le:2, v np le:1, v p le:2,
v p-np le:3
LE:MAXCRANK
A lexical entry together with the relative disambigua-
tion score ranking of its maximal constituent among
all MaxCons from different LEs
v - le:4, v np le:3, v p le:1,
v p-np le:2
PARTICLE The stem of the particle in the candidate VPC off
Table 1: Chart mining features used for VPC extraction
his new toysoffshows
PREPPRTL
v_?_le
NP1
VP4?hcomp
NP2
VP5?hcomp
PP?hcomp
0 2 3 4 7
DUMMY?V
S1?subjh(.125)
S3?subjh(.875)
VP1?hadj VP3?hcomp
S2?subjh(.925)
VP2?hadj(.325)
v_p?np_lev_np_le v_p_le
the boy
Figure 1: Example of a parsing chart in chart-mining for VPC extraction with the ERG
category classification: non-VPC, transitive VPC,
or intransitive VPC. For the parameter estimation
of the ME model, we use the TADM open source
toolkit (Malouf, 2002). The token-level predictions
are then combined with a simple majority voting to
derive the type-level prediction for the VPC candi-
date. In the case of a tie, the method backs off to
the na??ve baseline model described in Section 4.2,
which relies on the combined probability of the verb
and particle forming a VPC.
We have also experimented with other ways of de-
riving type-level predictions from token-level classi-
fication results. For instance, we trained a separate
classifier that takes the token-level prediction as in-
put in order to determine the type-level VPC predic-
tion. Our results indicate no significant difference
between these methods and the basic majority vot-
ing approach, so we present results exclusively for
this simplistic approach in this paper.
4 Evaluation
4.1 Experiment Setup
To evaluate the proposed chart mining-based VPC
extraction model, we use the dataset from the LREC
2008 Multiword Expression Workshop shared task
(see Section 2). We use this dataset to perform three
distinct DLA tasks, as detailed in Table 2.
The chart mining feature extraction is imple-
mented as an extension to the PET parser (Callmeier,
14
Task Description
GOLD VPC Determine the valence for a verb?preposition combination which is known to occur
as a non-compositional VPC (i.e. known VPC, with unknown valence(s))
FULL Determine whether each verb?preposition combination is a VPC or not, and further
predict its valence(s) (i.e. unknown if VPC, and unknown valence(s))
VPC Determine whether each verb?preposition combination is a VPC or not ignoring va-
lence (i.e. unknown if VPC, and don?t care about valence)
Table 2: Definitions of the three DLA tasks
2001). We use a slightly modified version of the
ERG in our experiments, based on the nov-06 re-
lease. The modifications include 4 newly-added
dummy lexical entries for the verb DUMMY-V and
the corresponding inflectional rules, and a lexical
type prediction model (Zhang and Kordoni, 2006)
trained on the LOGON Treebank (Oepen et al, 2004)
for unknown word handling. The parse disambigua-
tion model we use is also trained on the LOGON
Treebank. Since the parser has no access to any of
the verbs under investigation (due to the DUMMY-
V substitution), those VPC types attested in the
LOGON Treebank do not directly impact on the
model?s performance. The chart mining feature ex-
traction process took over 10 CPU days, and col-
lected a total of 44K events for 4,090 candidate VPC
triples.4 5-fold cross validation is used to train/test
the model. As stated above (Section 2), the VPC
triples attested in the WSJ training sections of the
Penn Treebank are excluded in each testing fold for
comparison with the Charniak parser-based model
(see Section 4.2).
4.2 Baseline and Benchmark
For comparison, we first built a na??ve baseline model
using the combined probabilities of the verb and par-
ticle being part of a VPC. More specifically, P (c|v)
and P (c|p) are the probabilities of a given verb
v and particle p being part of a VPC candidate
of type s ? {intrans , trans , null}, for transitive
4Not all sentences in the dataset are successfully chart-
mined. Due to the complexity of the precision grammar we
use, the parser is unlikely to complete the parsing chart for ex-
tremely long sentences (over 50 words). Moreover, sentences
which do not receive any spanning edge over the verb and the
particle are not considered as an indicative event. Nevertheless,
the coverage of the chart mining is much higher than the full-
parse coverage of the grammar.
VPC, intransitive VPC, and non-VPC, respectively.
P? (s|v, p) = P (s|v) ? P (s|p) is used to approxi-
mate the joint probability of verb-particle (v, p) be-
ing of type s, and the prediction type is chosen ran-
domly based on this probabilistic distribution. Both
P (s|v) and P (s|p) can be estimated from a list of
VPC candidate types. If v is unseen, P (s|v) is set to
be 1|V |
?
vi?V P (s|vi) estimated over all verbs |V |
seen in the list of VPC candidates. The na??ve base-
line performed poorly, mainly because there is not
enough knowledge about the context of use of VPCs.
This also indicates that the task of VPC extraction
is non-trivial, and that context (evidence from sen-
tences in which the VPC putatively occurs) must be
incorporated in order to make more accurate predic-
tions.
As a benchmark VPC extraction system, we use
the Charniak parser (Charniak, 2000). This sta-
tistical parser induces a context-free grammar and
a generative parsing model from a training set of
gold standard parse trees. Traditionally, it has been
trained over the WSJ component of the Penn Tree-
bank, and for this work we decided to take the same
approach and train over sections 1 to 22, and use sec-
tion 23 for parameter-tuning. After parsing, we sim-
ply search for the VPC triples in each token instance
with tgrep2,5 and decide on the classification of
the candidate by majority voting over all instances,
breaking ties randomly.
5Noting that the Penn POS tagset captures essentially the
compositional vs. non-compositional VPC distinction required
in the extraction task, through the use of the RP (prepositional
particle, for non-compositional VPCs) and RB (adverb, for com-
positional VPCs) tags.
15
4.3 Results
The results of our experiments are summarised in
Table 3. For the na??ve baseline and the chart mining-
based models, the results are averaged over 5-fold
cross validation.
We evaluate the methods in the form of the three
tasks described in Table 2. Formally, GOLD VPC
equates to extracting ?v, p, s? tuples from the sub-
set of gold-standard ?v, p? tuples; FULL equates to
extracting ?v, p, s? tuples for all VPC candidates;
and VPC equates to extracting ?v, p? tuples (ignor-
ing valence) over all VPC candidates. In each case,
we present the precision (P), recall (R) and F-score
(? = 1: F). For multi-category classifications (i.e.
the two tasks where we predict the valence s, indi-
cated as ?All? in Table 3), we micro-average the pre-
cision and recall over the two VPC categories, and
calculate the F-score as their harmonic mean.
From the results, it is obvious that the chart
mining-based model performs best overall, and in-
deed for most of the measures presented. The Char-
niak parser-based extraction method performs rea-
sonably well, especially in the VPC+valence extrac-
tion task over the FULL task, where the recall was
higher than the chart mining method. Although
not reported here, we observe a marked improve-
ment in the results for the Charniak parser when
the VPC types attested in the WSJ are not filtered
from the test set. This indicates that the statisti-
cal parser relies heavily on lexicalised VPC infor-
mation, while the chart mining model is much more
syntax-oriented. In error analysis of the data, we ob-
served that the Charniak parser was noticeably more
accurate at extracting VPCs where the verb was fre-
quent (our method, of course, did not have access
to the base frequency of the simplex verb), under-
lining again the power of lexicalisation. This points
to two possibilities: (1) the potential for our method
to similarly benefit from lexicalisation if we were to
remove the constraint on ignoring any pre-existing
lexical entries for the verb; and (2) the possibility
for hybridising between lexicalised models for fre-
quent verbs and unlexicalised models for infrequent
verbs. Having said this, it is important to reinforce
that lexical acquisition is usually performed in the
absence of lexicalised probabilities, as if we have
prior knowledge of the lexical item, there is no need
to extract it. In this sense, the first set of results in
Table 3 over Gold VPCs are the most informative,
and illustrate the potential of the proposed approach.
From the results of all the models, it would ap-
pear that intransitive VPCs are more difficult to ex-
tract than transitive VPCs. This is partly because the
dataset we use is unbalanced: the number of transi-
tive VPC types is about twice the number of intran-
sitive VPCs. Also, the much lower numbers over
the FULL set compared to the GOLD VPC set are due
to the fact that only 1/8 of the candidates are true
VPCs.
5 Discussion and Future Work
The inventory of features we propose for VPC ex-
traction is just one illustration of how partial parse
results can be used in lexical acquisition tasks.
The general chart mining technique can easily be
adapted to learn other challenging linguistic phe-
nomena, such as the countability of nouns (Bald-
win and Bond, 2003), subcategorization properties
of verbs or nouns (Korhonen, 2002), and general
multiword expression (MWE) extraction (Baldwin
and Kim, 2009). With MWE extraction, e.g., even
though some MWEs are fixed and have no internal
syntactic variability, such as ad hoc, there is a very
large proportion of idioms that allow various de-
grees of internal variability, and with a variable num-
ber of elements. For example, the idiom spill the
beans allows internal modification (spill mountains
of beans), passivisation (The beans were spilled in
the latest edition of the report), topicalisation (The
beans, the opposition spilled), and so forth (Sag et
al., 2002). In general, however, the exact degree of
variability of an idiom is difficult to predict (Riehe-
mann, 2001). The chart mining technique we pro-
pose here, which makes use of partial parse results,
may facilitate the automatic recognition task of even
more flexible idioms, based on the encouraging re-
sults for VPCs.
The main advantage, though, of chart mining is
that parsing with precision grammars does not any
longer have to assume complete coverage, as has
traditionally been the case. As an immediate con-
sequence, the possibility of applying our chart min-
ing technique to evolving medium-sized grammars
makes it especially interesting for lexical acquisi-
16
Task VPC Type Na??ve Baseline Charniak Parser Chart-Mining
P R F P R F P R F
GOLD VPC
Intrans-VPC 0.300 0.018 0.034 0.549 0.753 0.635 0.845 0.621 0.716
Trans-VPC 0.676 0.348 0.459 0.829 0.648 0.728 0.877 0.956 0.915
All 0.576 0.236 0.335 0.691 0.686 0.688 0.875 0.859 0.867
FULL
Intrans-VPC 0.060 0.018 0.028 0.102 0.593 0.174 0.153 0.155 0.154
Trans-VPC 0.083 0.348 0.134 0.179 0.448 0.256 0.179 0.362 0.240
All 0.080 0.236 0.119 0.136 0.500 0.213 0.171 0.298 0.218
VPC 0.123 0.348 0.182 0.173 0.782 0.284 0.259 0.332 0.291
Table 3: Results for the different methods over the three VPC extraction tasks detailed in Table 2
tion over low-density languages, for instance, where
there is a real need for rapid-prototyping of language
resources.
The chart mining approach we propose in this
paper is couched in the bottom-up chart parsing
paradigm, based exclusively on passive edges. As
future work, we would also like to look into the
top-level active edges (those active edges that are
never completed), as an indication of failed assump-
tions. Moreover, it would be interesting to investi-
gate the applicability of the technique in other pars-
ing strategies, e.g., head-corner or left-corner pars-
ing. Finally, it would also be interesting to in-
vestigate whether by using the features we acquire
from chart mining enhanced with information on the
prevalence of certain patterns, we could achieve per-
formance improvements over broader-coverage tree-
bank parsers such as the Charniak parser.
6 Conclusion
We have proposed a chart mining technique for lex-
ical acquisition based on partial parsing with preci-
sion grammars. We applied the proposed method
to the task of extracting English verb particle con-
structions from a prescribed set of corpus instances.
Our results showed that simple unlexicalised fea-
tures mined from the chart can be used to effec-
tively extract VPCs, and that the model outperforms
a probabilistic baseline and the Charniak parser at
VPC extraction.
Acknowledgements
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram. The first was supported by the German Excellence
Cluster of Multimodal Computing and Interaction.
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23:597?618.
Timothy Baldwin and Francis Bond. 2003. Learning
the countability of English nouns from corpus data.
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2003),
pages 463?470, Sapporo, Japan.
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing.
CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer Speech
and Language, Special Issue on Multiword Expres-
sions, 19(4):398?414.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of English verb-particle con-
structions. In Proceedings of the LREC 2008 Work-
shop: Towards a Shared Task for Multiword Expres-
sions (MWE 2008), pages 1?2, Marrakech, Morocco.
Ulrich Callmeier. 2001. Efficient parsing with large-
scale unification grammars. Master?s thesis, Univer-
sita?t des Saarlandes, Saarbru?cken, Germany.
John Carroll and Stephan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In Proceedings of the 2nd International Joint Confer-
ence on Natural LanguageProcessing (IJCNLP 2005),
pages 165?176, Jeju Island, Korea.
Eugene Charniak. 2000. A maximum entropy-based
parser. In Proceedings of the 1st Annual Meeting of
the North American Chapter of Association for Com-
putational Linguistics (NAACL2000), Seattle, USA.
Daniel de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error min-
ing in parsing results. In Proceedings of the ACL2009
Workshop on Grammar Engineering Across Frame-
works (GEAF), Singapore.
17
Nicole Dehe?. 2002. Particle Verbs in English: Syn-
tax, Information, Structure and Intonation. John Ben-
jamins, Amsterdam, Netherlands/Philadelphia, USA.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, edi-
tors, Collaborative Language Engineering, pages 1?
17. CSLI Publications.
Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, Brigitte Jo?rg, and Ul-
rich Scha?fer. 2006. Question answering from struc-
tured knowledge sources. Journal of Applied Logic,
Special Issue on Questions and Answers: Theoretical
and Applied Perspectives., 5(1):20?48.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochas-
tic unifcation-based grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 1999), pages 535?541, Mary-
land, USA.
Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, and
Rob Malouf. 1999. A Bag of Useful Techniques for
Efficient and Robust Parsing. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 473?480, Maryland, USA.
Anna Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Andrew MacKinlay, David Martinez, and Timothy Bald-
win. 2009. Biomedical event annotation with CRFs
and precision grammars. In Proceedings of BioNLP
2009: Shared Task, pages 77?85, Boulder, USA.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th Conferencde on Natural Language
Learning (CoNLL 2002), pages 49?55, Taipei, Taiwan.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Kathleen R. McKeown and Dragomir R. Radev. 2000.
Collocations. In Robert Dale, Hermann Moisl, and
Harold Somers, editors, Handbook of Natural Lan-
guage Processing.
Stephan Oepen and John Carroll. 2000. Ambiguity pack-
ing in constraint-based parsing ? practical results. In
Proceedings of the 1st Annual Meeting of the North
American Chapter of Association for Computational
Linguistics (NAACL 2000), pages 162?169, Seattle,
USA.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik
Velldal, Dorothee Beermann, John Carroll, Dan
Flickinger, Lars Hellan, Janne Bondi Johannessen,
Paul Meurer, Torbj?rn Nordga?rd, and Victoria Rose?n.
2004. Som a? kapp-ete med trollet? Towards MRS-
Based Norwegian?English Machine Translation. In
Proceedings of the 10th International Conference on
Theoretical and Methodological Issues in Machine
Translation, Baltimore, USA.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for the
extraction of multiword expressions. In Proceedings
of the LREC 2008 Workshop: Towards a Shared Task
for Multiword Expressions (MWE 2008), pages 50?53,
Marrakech, Morocco.
Susanne Riehemann. 2001. A Constructional Approach
to Idioms and Word Formation. Ph.D. thesis, Stanford
University, CA, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-
2002), pages 1?15, Mexico City, Mexico.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?178.
Masaru Tomita. 1985. An efficient context-free parsing
algorithm for natural languages. In Proceedings of the
9th International Joint Conference on Artificial Intel-
ligence, pages 756?764, Los Angeles, USA.
Kristina Toutanova, Christoper D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse ranking for a rich HPSG grammar. In Proceed-
ings of the 1st Workshop on Treebanks and Linguistic
Theories (TLT 2002), pages 253?263, Sozopol, Bul-
garia.
Hans Uszkoreit. 2002. New chances for deep linguis-
tic processing. In Proceedings of the 19th interna-
tional conference on computational linguistics (COL-
ING 2002), Taipei, Taiwan.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics), pages 446?453, Barcelona, Spain.
Aline Villavicencio and Ann Copestake. 2002. Verb-
particle constructions in a computational grammar of
English. In Proceedings of the 9th International Con-
ference on Head-Driven Phrase Structure Grammar
(HPSG-2002), Seoul, Korea.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC 2006),
pages 275?280, Genoa, Italy.
Yi Zhang and Valia Kordoni. 2008. Robust parsing
with a large HPSG grammar. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), Marrakech, Morocco.
Yi Zhang, Valia Kordoni, and Erin Fitzgerald. 2007a.
Partial parse selection for robust deep processing. In
Proceedings of ACL 2007 Workshop on Deep Linguis-
tic Processing, pages 128?135, Prague, Czech Repub-
lic.
Yi Zhang, Stephan Oepen, and John Carroll. 2007b. Ef-
ficiency in unification-based N-best parsing. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies (IWPT 2007), pages 48?59, Prague,
Czech.
18
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 15?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Intelligent Linux Information Access by Data Mining: the ILIAD Project
Timothy Baldwin,? David Martinez,? Richard B. Penman,? Su Nam Kim,?
Marco Lui,? Li Wang? and Andrew MacKinlay?
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? NICTA Victoria Research Laboratory
Abstract
We propose an alternative to conventional in-
formation retrieval over Linux forum data,
based on thread-, post- and user-level analysis,
interfaced with an information retrieval engine
via reranking.
1 Introduction
Due to the sheer scale of web data, simple keyword
matching is an effective means of information ac-
cess for many informational web queries. There
still remain significant clusters of information access
needs, however, where keyword matching is less
successful. One such instance is technical web fo-
rums and mailing lists (collectively termed ?forums?
for the purposes of this paper): technical forums
are a rich source of information when troubleshoot-
ing, and it is often possible to resolve technical
queries/problems via web-archived data. The search
facilities provided by forums and web search en-
gines tend to be over-simplistic, however, and there
is a desperate need for more sophisticated search (Xi
et al, 2004; Seo et al, 2009), including: favour-
ing threads which have led to a successful resolu-
tion; reflecting the degree of clarity/reproducibility
of the proposed solution in a given thread; repre-
senting threads via their threaded rather than sim-
ple chronological structure; the ability to highlight
key aspects of the thread, in terms of the problem
description and solution which led to a successful
resolution; and ideally, the ability to represent the
problem and solution in normalised form via infor-
mation extraction.
This paper provides a brief outline of an attempt
to achieve these and other goals in the context of
Linux web user forum data, in the form of the IL-
IAD (Intelligent Linux Information Access by Data
Mining) project. Linux users and developers rely
particularly heavily on web user forums and mail-
ing lists, due to the nature of the community, which
is highly decentralised ? with massive proliferation
of packages and distributions? and notoriously bad
at maintaining up-to-date documentation at a level
suitable for newbie and even intermediate users.
2 Project Outline
Our proposed solution is as follows: (1) crawl data
from a variety of web user forums; (2) analyse each
thread, to identify named entities and generate meta-
data; (3) analyse post-level linkages; (4) predict
user-level features which are expected to impinge on
the quality of search results; and finally (5) draw to-
gether the features from (1) to (4) to enhance the
quality of a traditional ranked IR approach. We
briefly review each step below. Given space limi-
tations, we focus on outlining our interpretation of
the task in this paper. For further details and results,
the reader is referred to the key papers cited herein.
2.1 Crawling
The first step is to crawl data from a variety of fo-
rums and mailing lists, for which we have developed
open-source scraping software in the form of SITE-
SCRAPER.1 SITESCRAPER is designed such that the
user simply copies relevant content from a browser-
rendered version of a given set of pages, which it
interprets as a structured record, and translates into
a generalised XPATH query.
2.2 Thread-level analysis
Next, we perform named entity recognition (NER)
over each thread to identify entities such as package
and distribution names, version numbers and snip-
pets of code; as part of this, we perform version
1http://sitescraper.googlecode.com/
15
anchoring, in identifying what entity each version
number relates to.
To generate thread-level metadata, we classify
each thread for the following three features, based
on an ordinal scale of 1?5 (Baldwin et al, 2007):
Complete: Is the problem description complete?
Solved: Is a solution provided in the thread?
Task Oriented: Is the thread about a specific
problem?
We additionally automatically classify the nature
of the thread content, in terms of, e.g., whether it
contains documentation or installation details, or re-
lates to software, hardware or programming.
Our experiments on thread-level classification are
based on a set of 250 annotated threads from Lin-
uxQuestions and other forums, as well as a dataset
from CNET.
2.3 Post-level analysis
We automatically analyse the post-to-post discourse
structure of each thread, in terms of which (preced-
ing) post(s) each post relates to, and how, building
off the work of Rose? et al (1995) and Wolf and Gib-
son (2005). For example, a given post may refute
the solution proposed in an earlier post, and also
propose a novel solution in response to the initiat-
ing post.
Separately, we are developing techniques for
identifying whether a new post to a given forum
is sufficiently similar to other (ideally resolved)
threads that the author should be prompted to first
check the existing threads for redundancy before a
new thread is initiated.
Our experiments on post-level analysis are, once
again, based on data from LinuxQuestions and
CNET.
2.4 User-level analysis
We are also experimenting with profiling users vari-
ously, based on a 5-point ordinal scale across a range
of user characteristics. Our experiments are based
on data from LinuxQuestions (Lui, 2009).
2.5 IR ranking
The various features are interfaced with an ad hoc
information retrieval (IR) system via a learning-to-
rank approach (Cao et al, 2007). In order to carry
out IR evaluation, we have developed a set of queries
and relevance judgements over a large-scale set of
forum data.
Our experiments to date have been based on com-
bination over three IR engines (LUCENE, ZETTAIR
and LEMUR), and involved thread-level metadata
only, but we have achieved encouraging results, sug-
gesting that thread-level metadata can enhance IR
effectiveness.
3 Conclusions
This paper provides an outline of the ILIAD project,
focusing on the tasks of crawling, thread-level anal-
ysis, post-level analysis, user-level analysis and IR
reranking. We have designed a series of class sets
for the component tasks, and carried out experimen-
tation over a range of data sources, achieving en-
couraging results.
Acknowledgements
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram.
References
T Baldwin, D Martinez, and RB Penman. 2007. Auto-
matic thread classification for Linux user forum infor-
mation access. In Proc of ADCS 2007.
Z Cao, T Qin, TY Liu, MF Tsai, and H Li. 2007. Learn-
ing to rank: from pairwise approach to listwise ap-
proach. In Proc of ICML 2007.
M Lui. 2009. Impact of user characteristics on online fo-
rum classification tasks. Honours thesis, University of
Melbourne. http://repository.unimelb.edu.
au/10187/5745.
CP Rose?, B Di Eugenio, LS Levin, and C Van Ess-
Dykema. 1995. Discourse processing of dialogues
with multiple threads. In Proc of ACL 1995.
J Seo, WB Croft, and DA Smith. 2009. Online commu-
nity search using thread structure. In Proc of CIKM
2009.
F Wolf and E Gibson. 2005. Representing discourse co-
herence: A corpus-based study. Comp Ling, 31(2).
W Xi, J Lind, and E Brill. 2004. Learning effective rank-
ing functions for newsgroup search. In Proc of SIGIR
2004.
16
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35?44,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Extracting Biomedical Events and Modifications Using Subgraph
Matching with Noisy Training Data
Andrew MacKinlay?, David Martinez?, Antonio Jimeno Yepes?,
Haibin Liu?, W. John Wilbur? and Karin Verspoor?
? NICTA Victoria Research Laboratory, University of Melbourne, Australia
{andrew.mackinlay, david.martinez}@nicta.com.au
{antonio.jimeno, karin.verspoor}@nicta.com.au
? National Center for Biotechnology Information, Bethesda, MD, USA
haibin.liu@nih.gov, wilbur@ncbi.nlm.nih.gov
Abstract
The Genia Event (GE) extraction task of
the BioNLP Shared Task addresses the ex-
traction of biomedical events from the nat-
ural language text of the published litera-
ture. In our submission, we modified an
existing system for learning of event pat-
terns via dependency parse subgraphs to
utilise a more accurate parser and signifi-
cantly more, but noisier, training data. We
explore the impact of these two aspects of
the system and conclude that the change in
parser limits recall to an extent that cannot
be offset by the large quantities of training
data. However, our extensions of the sys-
tem to extract modification events shows
promise.
1 Introduction
In this paper, we describe our submission to the
Genia Event (GE) information extraction subtask
of the BioNLP Shared Task. This task requires the
development of systems that are capable of iden-
tifying bio-molecular events as those events are
expressed in full-text publications. The task rep-
resents an important contribution to the broader
problem of converting unstructured information
captured in the biomedical literature into struc-
tured information that can be used to index and
analyse bio-molecular relationships.
This year?s task builds on previous instantia-
tions of this task (Kim et al, 2009; Kim et al,
2012), with only minor changes in the task defini-
tion introduced for 2011. The task organisers pro-
vided full text publications annotated with men-
tions of biological entities including proteins and
genes, and asked participants to provide annota-
tions of simple events including gene expression,
binding, localization, and protein modification, as
well as higher-order regulation events (e.g., pos-
itive regulation of gene expression). In our sub-
mission, we built on a system originally developed
for the BioNLP-ST 2011 (Liu et al, 2011) and ex-
tended in more recent work (Liu et al, 2013a; Liu
et al, 2013b). This system learns to recognise sub-
graphs of syntactic dependency parse graphs that
express a given bio-molecular event, and matches
those subgraphs to new text using an algorithm
called Approximate Subgraph Matching.
Due to the method?s fundamental dependency
on the syntactic dependency parse of the text, in
this work we set out to explore the impact of
substituting the previously employed dependency
parsers with a different parser which has been
demonstrated to achieve higher performance than
other commonly used parsers for full-text biomed-
ical literature (Verspoor et al, 2012).
In addition, we aimed to address the relatively
lower recall of the method through incorporation
of large quantities of external training data, ac-
quired through integration of previously automat-
ically extracted bio-molecular events available in
a web repository of such extracted events, EVEX
(Van Landeghem et al, 2011; Van Landeghem
et al, 2012), and additional bio-molecular events
generated from a large sample of full text pub-
lications using one of the state-of-the-art event
extraction systems, TEES (Bjo?rne and Salakoski,
2011). Since the performance of the subgraph
matching method, as an instance-based learning
strategy (Alpaydin, 2004), is dependent on having
good training examples that express the events in a
range of syntactic structures, the motivation under-
lying this was to increase the amount of training
data available to the system, even if that data was
derived from a less-than-perfect source. The aug-
mentation of training corpora with external unla-
belled data that is automatically processed to gen-
erate additional labels has been explored for re-
training the same system, in an approach known as
self-training. This approach has been shown to be
35
very effective for improving parsing performance
(McClosky et al, 2006; McClosky and Charniak,
2008). Self-training of the TEES system has been
previously explored (Bjorne et al, 2012), with
somewhat mixed results, but with evidence sug-
gesting it could be useful with an appropriate strat-
egy for selecting training examples. Here, rather
than training our system with its own output over
external data, we explore a semi-supervised learn-
ing approach in which we train our system with the
outputs of a different system (TEES) over external
data.
2 Methodology
2.1 Base Event Extraction System
The event extraction algorithm is essentially the
same as the one used in Liu et al (2013b). A fuller
description can be found there, but we summarise
the most important aspects of it here.
2.1.1 Event Extraction with ASM
The principal method used in event extraction is
Approximate Subgraph Matching, or ASM (Liu et
al., 2013a). Broadly, we learn subgraph patterns
from the event structures in the training data, and
then apply them by looking for matches with the
patterns of the learned rules, using ASM to allow
for non-exact matches of the patterns.
The first stage in this is learning the rules which
link subgraphs to associated patterns. The input
is a set of dependency-parsed articles (the setup
is described in ?2.1.2), and a set of gold-standard
annotations of proteins and events in the shared
task format. Using the standoff annotations in the
training data, every protein and trigger is mapped
to one or more nodes in the corresponding depen-
dency graphs. In addition, the textual content of
every protein is replaced with a generic string en-
abling abstraction over individual protein names.
Then, for each event annotation in the training
data, we retrieve the nodes from the graph corre-
sponding to the associated trigger and protein en-
tities. We determine the shortest path (or paths, in
case of a tie) connecting the graph trigger to each
of the event argument nodes. For arguments which
are themselves events (e.g., for regulatory events),
the node corresponding to the trigger of the event
argument is used instead of a protein node. Where
there are multiple arguments, we take the union of
the shortest paths to each individual argument.
This path is then used as the pattern compo-
nent of an event rule. The rule also consists of an
event type, and a mapping from event arguments
to nodes from the pattern graph, or to an event
type/node pair for nested event arguments. Af-
ter processing all training documents, we get on
the order of a few thousand rules; this can be de-
creased slightly by removing rules with subgraphs
that are isomorphic to those of other rules.
In principle, this set of rules could then be di-
rectly applied to the test documents, by searching
for any matching subgraphs. However, in practice
doing so leads to very low recall, since the pat-
terns are not general enough to get a broad range of
matches on new data. We can alleviate this by re-
laxing the strictness of the subgraph matching pro-
cess. Most basically, we relax node matching. In-
stead of requiring an exact match between both the
token and the part-of-speech of the nodes of the
sentence graph and those from the rule subgraph,
we also allow a match on the basis of the lemma
(according to BioLemmatizer (Liu et al, 2012)),
and a coarse-grained POS-tag (where there is only
one POS-tag for nouns, verbs and adjectives).
More importantly, we also relax the require-
ments on how closely the graphs must match, by
using ASM. ASM defines distances measures be-
tween subgraphs, based on structure, edge labels
and edge directions, and uses a set of specified
weights to combine them into an overall subgraph
distance. We have a pre-configured set of distance
thresholds for each event type, and for each sen-
tence/rule pairing, we extract events for any rules
with subgraphs under the given threshold.
The problem with this approximate matching is
that some rules now match too broadly, and pre-
cision is reduced. This is mitigated by adding
an iterative optimisation phase. In each iteration,
we run the event extraction using the current rule
set over some dataset ? usually the training set,
or a subset of it. We check the contribution of
each rule in terms of postulated events and actual
events which match the gold standard. If the ra-
tio of matched to postulated events is too low (for
the work reported here, the threshold is 0.25), the
rule is discarded. This process is repeated until no
more rules are discarded. This can take multiple
iterations since the rules are interdependent due to
the presence of nested event arguments.
The optimisation step is by far the most time-
consuming step of our process, especially for the
large rule sets produced in some configurations.
36
We were able to improve optimisation times some-
what by parallelising the event extraction, and
temporarily removing documents with long ex-
traction times from the optimisation process un-
til as late as possible, but it remained the primary
bottleneck in our experimentation.
2.1.2 Parsing Pipeline
In our parsing pipeline, we first split sentences
using the JULIE Sentence Boundary Detector, or
JSBD (Tomanek et al, 2007). We then parse
using a version of clearnlp1 (Choi and McCal-
lum, 2013), a successor to ClearParser (Choi and
Palmer, 2011), which was shown to have state-
of-the-art performance over the CRAFT corpus
of full-text biomedical articles (Verspoor et al,
2012). We use dependency and POS-tagging mod-
els trained on the CRAFT corpus (except where
noted); these pre-trained models are provided with
clearnlp. Our fork of clearnlp integrates to-
ken span marking into the parsing process, so the
dependency nodes can easily be matched to the
standoff annotations provided with the shared task
data. This pipeline is not dependent on any pre-
annotated data, so can thus be trivially applied to
extra data not provided as part of the shared task.
In addition the parsing is fast, requiring roughly 46
wall-clock seconds (processing serially) to parse
the 5059 sentences from the training and develop-
ment sets of the 2013 GE task ? an average of 9 ms
per sentence. The ability to apply the same pars-
ing configuration to new text was useful for adding
extra training data, as discussed in ?2.2.
The usage of clearnlp as the parser is the pri-
mary point of difference between our system and
that of Liu et al (2013b), who use the Charniak-
Johnson parser with the McClosky biomedical
model (CJM; McClosky and Charniak (2008)), al-
though there are other minor differences in tokeni-
sation and sentence splitting. We expected that the
higher accuracy of clearnlp over biomedical text
would translate into increased accuracy of event
detection in the shared task; we consider this ques-
tion in some detail below.
2.2 Adding Noisy Training Data
One of the limitations of the ASM approach is that
the high precision comes at the cost of lower re-
call. Our hypothesis is that adding extra training
instances, even if some are errors, will raise re-
call and improve overall performance. We utilised
1https://code.google.com/p/clearnlp/
two sources of automatically-annotated data: the
EVEX database, and running an automatic event
annotator over documents from PubMed Central
(PMC) and MEDLINE.
To test our hypothesis, we utilise one of the
best performing automatic event extractors in pre-
vious BioNLP tasks: TEES (Turku Event Extrac-
tion System)2 (Bjo?rne et al, 2011). We expand our
pool of training examples by adding the highest-
confidence events TEES identifies in unlabelled
text. We explored different approaches to ranking
events based on classifier confidence empirically.
TEES relies on multi-class SVMs both for trig-
ger and event classification, and produces confi-
dence scores for each prediction. We explored
ranking events according to: (i) score of the trig-
ger prediction, (ii) score of the event-type predic-
tion, and (iii) sum of trigger and event type predic-
tions. We also compared the performance when
selecting the top-k events overall, versus choos-
ing the top-k events for each event type. We also
tested adding as many instances per event-type as
there were in the manually-annotated dataset, with
different multiplying factors. Finally, we evalu-
ated the effect of using different splits of the data
for the evaluation and optimisation steps of ASM.
This is the full list of parameters that we tested
over held-out data:
? Original confidence scores: we ranked events
according to the three SVM scores mentioned
above: trigger prediction, event-type predic-
tion, and combined.
? Overall top-k: we selected the top 1,000,
5,000, 10,000, 20,000, 30,000, 40,000, and
50,000 for the different experimental runs.
? Top-k per type: for each event type, we se-
lected the top 400, 1,000, and 2,000.
? Training bias per type: we add as many in-
stances from EVEX per type as there are in
the manually annotated data. We experiment
with adding up to 6 times as many as in man-
ually annotated data.
? Training/optimisation split: we combine
manually and automatically annotated data
for training. For optimisation we tested
different options: manually annotated only,
manual + automatic, manual + top-100
events, and manual + top-1000 events.
2http://jbjorne.github.com/TEES/
37
We did not explore all these settings exhaus-
tively due to time constraints, and we report here
the most promising settings. It is worth mention-
ing that most of the configurations contributed to
improve the baseline performance. We only ob-
served drops when using automatically-annotated
data in the optimisation step.
2.2.1 Data from EVEX
Conveniently, the developers of TEES have re-
leased the output of their tool over the full 2009
collection of MEDLINE, consisting of abstracts of
biomedical articles, in a collection known as the
EVEX dataset. We used the full EVEX dataset as
provided by the University of Turku, and explored
different ways of ranking the full list of events as
described above.
2.2.2 Data from TEES
To augment the training data, we annotated two
data sets with TEES based on MEDLINE and
PubMed Central (PMC). The developers of TEES
released a trained model for the GE 2013 training
data that we utilised.
Due to the long pre-processing time of TEES,
which includes gene named entity recognition,
part-of-speech tagging and parsing, we used the
EVEX pre-processed MEDLINE, which required
some adaptation of the EVEX XML to the XML
format accepted by TEES. Once this adaptation
was finished, the files were processed by TEES.
Then, we have selected articles from PMC us-
ing a query containing specific MeSH headings
related to the GE task and limiting the result to
only the Open Access part of PMC. From the al-
most 600k articles from the PMC Open Access set,
we reduced the total number of articles to around
155k. The PMC query is the following:
(Genetic Phenomena[MH] OR Metabolic
Phenomena[MH] OR Cell Physiological
Phenomena[MH] OR Biochemical
Processes[MH]) AND open access[filter]
Furthermore, the articles were split into sections
and specific sections from the full text like Intro-
duction, Background and Methods were removed
to reduce the quantity of text to be annotated by
TEES. The PMC files produced by this filtering
were processed by TEES on the NICTA cluster.
2.3 Modification Detection
To evaluate the utility of ASM for a diverse range
of tasks, we also applied it to the task of detect-
ing modification (SPECULATION or NEGATION)
NEGATION cues
? Basic: not, no, never, nor, only, neither, fail, cease,
stop, terminate, end, lacking, missing, absent, absence,
failure, negative, unlikely, without, lack, unable
? Data-derived: any, prevention, prevent, disrupt, dis-
ruption
SPECULATION cues:
? Basic: analysis, whether, may, should, can, could, un-
certain, questionable, possible, likely, probable, prob-
ably, possibly, conceivable, conceivably, perhaps, ad-
dress, analyze, analyse, assess, ask, compare, consider,
enquire, evaluate, examine, experiment, explore, inves-
tigate, test, research, study, speculate
? Data-derived: measure, measurement, suggest, sug-
gestion, value, quantify, quantification, determine, de-
termination, detect, detection, calculate, calculation
Table 1: Modification cues
of events. In event detection, triggers are explic-
itly annotated, so the linguistic cue which indi-
cates that an event is occurring is easy to identify.
As described in Section 3.2, these triggers are im-
portant for learning event patterns.
The event extraction method is based on paths
between dependency graph nodes, so it is neces-
sary to have at least two relevant graph nodes be-
fore we can determine a path between them. For
learning modification rules, one graph node is the
trigger of the event which is subjec to modifica-
tion. However here we needed a method to deter-
mine another node in the sentence which provided
evidence that NEGATION or SPECULATION was
occurring, and could thus form an endpoint for a
semantically relevant graph pattern. To achieve
this, we specified a set cue lemmas for NEGATION
and SPECULATION. The basic set of cue lemmas
came from a variety of sources. Some were man-
ually specified and some were derived from previ-
ous work on modification detection (Cohen et al,
2011; MacKinlay et al, 2012). We manually ex-
panded this cue list to include obvious derivational
variants. This gave us a basic set of 34 SPECULA-
TION and 21 NEGATION cues.
We also used a data-driven strategy to find ad-
ditional lemmas indicative of modification. We
adapted the method of Rayson and Garside (2000)
which uses log-likelihood for finding words that
characterise differences between corpora. Here,
the ?corpora? are sentences attached to all events
in the training set, and sentences attached to events
which are subject to NEGATION or SPECULATION
(treated separately). We build a frequency distri-
bution over lemmas in each set of sentences, and
calculate the log-likelihood for all lemmas, us-
38
ing the observed frequency from the modification
events and the expected frequency over all events.
Sorting by decreasing log-likelihood, we get a
list of lemmas which are most strongly associated
with NEGATION or SPECULATION. We manually
examined the highest-ranked lemmas from these
two lists and noted lemmas which may occur,
according to human judgment, in phrases which
would denote the relevant modification type. We
found seven extra SPECULATION cues and three
extra NEGATION cues. Expanding with morpho-
logical variants as described above yielded 47
SPECULATION cues and 26 NEGATION cues to-
tal. These cues are shown, divided into basic and
data-derived, in Table 1.
For every node N with a lemma in the appro-
priate set of cue lemmas, we create a rule based
on the shortest path between the cue lemma node
N and the event trigger node. The trigger lem-
mas are replaced with generic lemmas which only
reflect the POS-tag of the trigger, to broaden the
range of possible matches. Each rule thus consists
of the POS-tag of an event trigger, and a subgraph
pattern including the abstracted event trigger node.
At modification detection time, the rules are ap-
plied in a similar way to the event rules. After
detecting events, we look for matches of each ex-
tracted event with every modification rule. A rule
R is considered to match if the event trigger node
POS tag matches the POS tag of the rule, and the
subgraph pattern of the rule matches the graph of
the sentence, including a node corresponding to
the event trigger node. If R is found to match
for a given event and sentence, any events which
have the trigger defined in the rule are marked as
SPECULATION or NEGATION as appropriate. As
in event extraction, we use ASM to allow a looser
match between graphs, but initial experimentation
showed that increasing the match thresholds be-
yond a relatively small distance was detrimental.
We have not yet added an optimisation phase for
modification, which might allow larger ASM dis-
tance threshold to have more benefit.
3 Results
We present our results over development data,
and the official test. We report the Approximate
Span/Approximate Recursive metric in all our ta-
bles, for easy comparison of scores. We describe
the data split used for development, explain our
event extraction results, and finally describe our
performance in modification detection.
3.1 Data division for development
In the data provided by the task organisers, the
split of data between training and development
sets, with 249 and 222 article sections respec-
tively, was fairly even. If we had used such a split,
we would have had an unfeasibly small amount
of data to train from during development, and
possible unexpected effects when we sharply in-
creased the amount of training data for running
over the held-out test set. We instead used our
own data set split during development, pooling
the provided training and development sets, and
randomly selecting six PMC articles (PMC IDs
2626671, 2674207, 3062687, 3148254, 3333881
and 3359311) for the development set, with the
remainder available for training. We respected ar-
ticle boundaries in the new split to avoid training
and testing on sentences taken from different sec-
tions of the same article. Results over the devel-
opment set reported in this section are over this
data split. We will refer to our training subset as
GE13tr, and to the testing subset as GE13dev.
For our runs over the official test of this chal-
lenge, we merged all the manually annotated data
from 2013 to be used as training. We also per-
formed some experiments with adding the exam-
ples from the 2011 GE task to our training data.
3.2 Event Extraction
For our first experiment, we evaluated the contri-
bution of the automatically annotated data over us-
ing GE13tr data only. We performed a set of ex-
periments to explore the parameters described in
Section 2.2 over two sources of extra examples:
EVEX and TEES.
Using EVEX data in training resulted in clear
improvements in performance when only manu-
ally annotated data was consulted for optimisa-
tion. The increase was mainly due to the better
recall, with small variations in precision over the
baseline for the majority of experiments. Our best
run over the GE13dev data followed this setting:
rank events according to trigger scores, include all
top-30000 events (without considering the types of
the events), and use only manually annotated data
for the optimisation step. Other settings also per-
formed well, as we will see below.
For TEES, we selected noisy examples from
MEDLINE and PMC to be used as additional
39
System Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+TEES 59.27 29.89 39.74
+TEES +EVEX (top5k) 46.93 30.78 37.18
+TEES +EVEX (top20k) 56.32 31.90 40.73
+TEES +EVEX (top30k) 55.34 32.48 40.93
+TEES +EVEX (pt1k) 58.54 30.96 40.50
+TEES +EVEX (trx4) 57.83 31.23 40.56
Table 2: Impact of adding extra training data to the
ASM method. top5k,20k,30k: using the top 5,000,
20,000, and 30,000 events. pt1k: using the top
1,000 events per event-type. trx4: following the
training bias of events, with a multiplying factor
of four. For TEES we always use the top 10,000
events. Evaluated over GE13dev.
training data. Initial results showed that when us-
ing only MEDLINE annotated data in the train-
ing step, the performance decreased compared to
not using any additional data. This might have
been due to differences between the EVEX pre-
processed data that we used and what TEES was
expecting, so the MEDLINE set was not consid-
ered for further experimentation. Using PMC ar-
ticles annotated with TEES in the training step se-
lected by the evidence score of TEES shows an in-
crease of recall while slightly decreasing the pre-
cision, which was expected. We selected the top
10000 events from the PMC set based on the evi-
dence score as additional training data.
Table 2 summarises the results of combin-
ing different settings of EVEX with TEES. We
achieve a considerable boost in recall, at the cost
of precision for most configurations. The only set-
ting where there is a slight drop in F-score is the
experiment with only 5000 events from EVEX; in
the remaining runs we are able to alleviate the drop
in precision, and improve the F-score. Consider-
ing the addition of top-events according to their
type, the increment in recall is slightly lower, but
these runs are able to reach similar F-score to the
best ones, using less training data. Results with
TEES might be slightly overoptimistic since the
PMC annotation is based on a TEES model trained
on the 2013 GE data and our configurations are
evaluated on a subset of this data.
For our next experiment, we tested the contribu-
tion of adding the dataset from the 2011 GE task
to the training dataset. We use this data both in
the training and optimisation steps. The results are
Train Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
Table 3: Adding GE11 data to the training and op-
timisation steps. Evaluated over GE13dev.
Parser Train Prec. Rec. F-sc.
clearnlp
GE13 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
CJM
GE13 60.96 33.11 42.91
+GE11 64.11 38.93 48.44
Table 4: Performance depending on the applied
parsing pipeline (clearnlp for this work against
the CJM pipeline of Liu et al (2013b)) over
GE13dev. For each run, the available data was
used both in training and optimisation.
given in Table 3, where we can observe a boost in
recall at the cost of precision. Overall, the im-
proved F-score suggests that this dataset would
make a useful contribution to the system.
We also compared our system to that of Liu
et al (2013b), where the primary difference
(although not the only difference, as noted in
?2.1.2) is the use of clearnlp instead of the CJM
(Charniak-Johnson/McClosky) pipeline. It is thus
somewhat surprising to see in Table 4 that the
CJM pipeline outperforms our clearnlp pipeline
by 5.5?8% in F-score, depending on the train-
ing data. For the smaller GE13-only training set,
the gap is smaller, and the precision figures are
in fact comparable. However, the recall is uni-
formly lower, suggesting that the rules learned
from clearnlp parses are for some reason less gen-
erally applicable. Another interesting difference
is that our clearnlp pipeline gets a smaller benefit
from the addition of the GE11 training data. We
consider possible reasons for this in ?4.1.
Table 5 contains the evaluation of different ex-
periments on the official test data. We tested the
baseline system using the training and develop-
ment data from 2011 and 2013 GE tasks and the
addition of TEES and EVEX data. The additional
data improves the recall slightly compared to not
using it, while, as expected, it decreases the pre-
cision. Table 5 also shows the results for our of-
ficial submission (+TEES+EVEX sub), which due
to time constraints was a combination of the opti-
mised rules of different data splits and has a lower
40
Train Prec. Rec. F-sc.
GE11, GE13 65.71 32.57 43.55
+TEES+EVEX 63.67 33.50 43.91
+TEES+EVEX * 50.68 36.99 42.77
Table 5: Test set results, always optimised over
gold data only. * denotes the official submission.
performance compared to the other results.
3.3 Modification Detection
We show results for selected modification detec-
tion experiments in Table 6. In all cases we used
all of the available gold training data from the
GE11 and GE13 datasets. To assess the impact of
modification cues, we show results using the basic
set as well as with the addition of the data-derived
cues. It has often been noted (MacKinlay et al,
2012; Cohen et al, 2011) that modification detec-
tion accuracy is strongly dependent on the quality
of the upstream event annotation, so we provide an
oracle evaluation, using gold-standard event anno-
tations rather than automatic output.
The performance over the automatically-
annotated runs is respectable, given that the recall
is fundamentally limited by the recall of the input
event annotations, which is only around 30% for
the configurations shown. With the oracle event
annotations, the results improve substantially,
with considerable gains in precision, and recall
increasing by a factor of 4?6. This boost in recall
in particular is more than we would naively expect
from the roughly threefold increase in recall over
the events. It seems that many of the modification
rules we learned were even more effective over
events which our pipeline was unable to detect.
The modification rules were learned from oracle
event data, but this does not fully explain the
discrepancy. Regardless, our algorithm for mod-
ification detection showed excellent performance
over the oracle annotations. Over the 2009 version
of the BioNLP shared task data, MacKinlay et al
(2012) report F-scores of 54.6% for NEGATION
and 51.7% for SPECULATION. These are not
directly comparable with those in Table 6, but
running our newer algorithm over the same 2009
data gives F-scores of 84.2% for NEGATION and
69.1% for SPECULATION.
For the official run, which conflates event
extraction and modification detection accuracy,
our system was ranked third for NEGATION and
SPECULATION out of the three competing teams,
although the other teams had event extraction F-
scores of roughly 8% higher than our system. For
SPECULATION, our system had the highest preci-
sion of 34.15%, while the F-score of 20.22% was
close to the best result of 23.92%. Our NEGA-
TION detection was less competitive, with an F-
score of 20.94% ? roughly 6% lower than the other
teams. We cannot extrapolate directly from the or-
acle evaluation in Table 6, but it seems to indicate
that an increase in event extraction accuracy would
have flow-on benefits in modification detection.
4 Discussion
4.1 Detrimental Effects of Parser Choice
The biggest surprise here was that clearnlp, a
more accurate dependency parser for the biomed-
ical domain, as evaluated on the CRAFT tree-
bank, gave a substantially lower event extrac-
tion F-score than the CJM parser. To determine
whether preprocessing caused the differences, we
replaced the existing modules (sentence-splitting
from JSBD and tokenisation/POS-tagging from
clearnlp) with the BioC-derived versions from the
CJM pipeline, but this yielded only an insignifi-
cant decrease in accuracy.
Over the same training data, the optimised rules
from CJM have an average of 2.6 nodes per sub-
graph path, compared to 3.9 nodes per path using
clearnlp. A longer path is less likely to match
than a shorter path, so this may help to explain
the lower generalisability of the clearnlp-derived
rules. While it is possible for a longer subgraph
to match just as generally, if the test sentences
are parsed consistently, in general there are more
nodes and edges which can fail to match due to mi-
nor surface variations. One way to mitigate this is
to raise the ASM distance thresholds to compen-
sate for this; preliminary experiments suggest it
would provide a small (? 1%) boost in F-score but
this would not close the gap between the parsers.
Both parsers produce outputs with Stanford
Dependency labels (de Marneffe and Manning,
2008), so we might naively expect similar graph
topology and subgraph pattern lengths. However,
the CJM pipeline produces graphs in the ?CCpro-
cessed? SD format, which are simpler and denser.
If a node N has a link to a node O with a conjunc-
tion link to another node P (from e.g. and), an ex-
tra link with the same label is added directly from
N to P in the CCprocessed format. This means
41
NEGATION SPECULATION
Eval Events (F-sc) Cues P / R / F P / R / F
Dev
GE13+TEES+EVEX (40.93) Basic 32.69 / 13.71 / 19.32 37.04 / 14.49 / 20.83
GE13+TEES+EVEX (40.93) B + Data 32.69 / 12.88 / 18.48 39.71 / 17.20 / 24.00
Oracle (100.0) B + Data 82.48 / 71.07 / 76.35 78.79 / 67.71 / 72.83
Test
GE11+GE13 (43.55) B + Data 39.53 / 13.99 / 20.66 50.00 / 13.85 / 21.69
GE11+GE13+TEES+EVEX * (42.77) B + Data 32.76 / 15.38 / 20.94 34.15 / 14.36 / 20.22
Table 6: Results for SPECULATION and NEGATION using automatically-annotated events (showing the
F-score of the configuration), as well as using oracle event annotations from the gold standard, over our
development set and the official test set. Rules are learned from GE13+GE11 gold data (excluding any
test data). Cues for learning rules are either the basic manually-specified set (34 SPEC/21 NEG) or the
augmented set with data-driven additions (47 SPEC/26 NEG). * denotes the official submission.
there are more direct links in the graph, match-
ing the semantics more closely. The shortest path
fromN to P is now direct, instead of viaO, which
could enable the CJM pipeline to produce more
general rules.
To evaluate how much this detrimentally af-
fects the clearnlp pipeline, as a post hoc in-
vestigation, we implemented a conversion mod-
ule. Using Stanford Dependency parser code,
we replicated the CCprocessed conversion on the
clearnlp graphs, reducing the average subgraph
pattern length to 2.8, and slightly improving ac-
curacy. Over our development set, compared to
the results in Table 3 it gave a 0.7% absolute F-
score boost over using GE13 training-data only,
and 1.1% over using GE11 and GE13 training data
(in both cases improving recall). Over the test
set, the improvement was greater, with a P/R/F
of 35.66/64.99/46.05, a 2.5% increase in F-score
compared to the results in Table 5 and only 2.9%
less than the official Liu et al (2012) submission.
Clearly some of the inter-parser discrepancies
are due to surface features and post-processing,
and as noted above, we can also achieve small im-
provements by relaxing ASM thresholds, so some
problems may be caused by the default parameters
being suboptimal for the parser. However, the ac-
curacy is still lower where we would expect it to
be higher, and this remaining discrepancy is diffi-
cult to explain without performing a detailed error
analysis, which we leave for future work.
4.2 Effect of additional data
Our initial intuition that using additional noisy
training data during the training of the system
would improve the performance is supported by
the results in Table 2. Table 3 shows that us-
ing a larger set of manually annotated data based
on 2011 GE task data also improves performance.
However, these tables also indicate that adding
manually annotated data produces an increase in
performance comparable to adding the noisy data,
despite its smaller size, and when using this man-
ually annotated set together with the noisy data,
the improvement resulting from the noisy data is
smaller (Table 5). Noisy data was only used dur-
ing training, which limits its effectiveness?any
rule extracted from automatically acquired anno-
tations that are not seen during optimisation of the
rule set will have a lower weight. On the other
hand, we found that using noisy data for optimi-
sation seemed to decrease performance. Together,
these results suggest that studying strategies, pos-
sibly self-training, for selection of events from the
noisy data to be used during rule set optimisation
in the ASM method are warranted.
5 Conclusion
Using additional training data, whether manually
annotated or noisy, improves the performance of
our baseline event extraction system. The gains
that we achieved by adding training data, however,
were outweighed by a loss of performance due to
our parser substitution, with longer dependency
subgraphs limiting rule generalisability the most
likely explanation. Our experiments demonstrate
that while a given parser might be ?better? in one
evaluation context, that advantage may not trans-
late to improved performance in a downstream
task that depends strongly on the parser output.
We presented an extension of the subgraph match-
ing methodology to extract modification events
which, when based on a good core event extrac-
tion system, shows very promising results.
42
Acknowledgments
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program. This research was
supported in part by the Intramural Research Pro-
gram of the NIH, NLM.
References
Ethem Alpaydin. 2004. Introduction to Machine
Learning. MIT Press.
Jari Bjo?rne and T. Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 183?
191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2011. Ex-
tracting contextualized complex biological events
with rich graph-based features sets. Computational
Intelligence, 27(4):541?557.
Jari Bjorne, Filip Ginter, and Tapio Salakoski. 2012.
University of turku in the bionlp?11 shared task.
BMC Bioinformatics, 13(Suppl 11):S4.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Jinho D. Choi and Martha Palmer. 2011. Getting the
most out of transition-based dependency parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 687?692, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
K.B. Cohen, K. Verspoor, H.L. Johnson, C. Roeder,
P.V. Ogren, W.A. Baumgartner, E. White, H. Tip-
ney, and L. Hunter. 2011. High-precision biological
event extraction: Effects of system and data. Com-
putational Intelligence, 27(4):681701, November.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In CrossParser ?08: Coling 2008: Pro-
ceedings of the workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pages 1?8, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
J.D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-
jii. 2009. Overview of bionlp?09 shared task on
event extraction. Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL 2009
Workshop, pages 1?9.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The genia event and protein coreference tasks of
the bionlp shared task 2011. BMC Bioinformatics,
13(Suppl 11):S1.
H. Liu, R. Komandur, and K. Verspoor. 2011. From
graphs to events: A subgraph matching approach for
information eextraction from biomedical text. ACL
HLT 2011, page 164.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLoS ONE, 8(4):e60954, 04.
Haibin Liu, Karin Verspoor, Don Comeau, Andrew
MacKinlay, and W. John Wilbur. 2013b. General-
izing an approximate subgraph matching-based sys-
tem to extract events in molecular biology and can-
cer genetics. In Proceedings of the 2013 BioNLP
Workshop Companion Volume for the Shared Task.
Andrew MacKinlay, David Martinez, and Timo-
thy Baldwin. 2012. Detecting modification of
biomedical events using a deep parsing approach.
BMC Medical Informatics and Decision Making,
12(Suppl 1):S4.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the Association for Computational Linguistics (ACL
2008, short papers).
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
conference of the North American chapter of the
ACL, pages 152?159.
Paul Rayson and Roger Garside. 2000. Comparing
corpora using frequency profiling. In The Workshop
on Comparing Corpora, pages 1?6, Hong Kong,
China, October. Association for Computational Lin-
guistics.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on con-
ditional random fields. In Proceedings of the 10th
Conference of the Pacific Association for Compu-
tational Linguistics, pages 49?57, Melbourne, Aus-
tralia.
S. Van Landeghem, F. Ginter, Y. Van de Peer, and
T. Salakoski. 2011. EVEX: A pubmed-scale re-
source for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37.
43
S. Van Landeghem, K. Hakala, S. Ro?nnqvist,
T. Salakoski, Y. Van de Peer, and F. Ginter. 2012.
Exploring biomolecular literature with EVEX: Con-
necting genes through events, homology and indirect
associations. Advances in Bioinformatics, Special
issue Literature-Mining Solutions for Life Science
Research:ID 582765.
Karin Verspoor, K. Bretonnel Cohen, Arrick Lan-
franchi, Colin Warner, Helen L. Johnson, Christophe
Roeder, Jinho D. Choi, Christopher Funk, Yuriy
Malenkiy, Miriam Eckert, Nianwen Xue, William
A. Baumgartner Jr., Michael Bada, Martha Palmer, ,
and Lawrence E. Hunter. 2012. A corpus of full-text
journal articles is a robust evaluation tool for reveal-
ing differences in performance of biomedical natural
language processing tools. BMC Bioinformatics.
44
