Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 448?457, Prague, June 2007. c?2007 Association for Computational Linguistics
Enhancing Single-document Summarization by Combining RankNet and
Third-party Sources
Krysta M. Svore
Microsoft Research
1 Microsoft Way
Redmond, WA 98052
ksvore@microsoft.com
Lucy Vanderwende
Microsoft Research
1 Microsoft Way
Redmond, WA 98052
Christopher J.C. Burges
Microsoft Research
1 Microsoft Way
Redmond, WA 98052
Abstract
We present a new approach to automatic
summarization based on neural nets, called
NetSum. We extract a set of features from
each sentence that helps identify its impor-
tance in the document. We apply novel
features based on news search query logs
and Wikipedia entities. Using the RankNet
learning algorithm, we train a pair-based
sentence ranker to score every sentence in
the document and identify the most impor-
tant sentences. We apply our system to
documents gathered from CNN.com, where
each document includes highlights and an
article. Our system significantly outper-
forms the standard baseline in the ROUGE-1
measure on over 70% of our document set.
1 Introduction
Automatic summarization was first studied almost
50 years ago by Luhn (Luhn, 1958) and has contin-
ued to be a steady subject of research. Automatic
summarization refers to the creation of a shortened
version of a document or cluster of documents by
a machine, see (Mani, 2001) for details. The sum-
mary can be an abstraction or extraction. In an ab-
stract summary, content from the original document
may be paraphrased or generated, whereas in an ex-
tract summary, the content is preserved in its original
form, i.e., sentences. Both summary types can in-
volve sentence compression, but abstracts tend to be
more condensed. In this paper, we focus on produc-
ing fully automated single-document extract sum-
maries of newswire articles.
To create an extract, most automatic systems use
linguistic and/or statistical methods to identify key
words, phrases, and concepts in a sentence or across
single or multiple documents. Each sentence is then
assigned a score indicating the strength of presence
of key words, phrases, and so on. Sentence scoring
methods utilize both purely statistical and purely se-
mantic features, for example as in (Vanderwende et
al., 2006; Nenkova et al, 2006; Yih et al, 2007).
Recently, machine learning techniques have been
successfully applied to summarization. The meth-
ods include binary classifiers (Kupiec et al, 1995),
Markov models (Conroy et al, 2004), Bayesian
methods (Daume? III and Marcu, 2005; Aone et al,
1998), and heuristic methods to determine feature
weights (Schiffman, 2002; Lin and Hovy, 2002).
Graph-based methods have also been employed
(Erkan and Radev, 2004a; Erkan and Radev, 2004b;
Mihalcea, 2005; Mihalcea and Tarau, 2005; Mihal-
cea and Radev, 2006).
In 2001?02, the Document Understanding Con-
ference (DUC, 2001), issued the task of creat-
ing a 100-word summary of a single news article.
The best performing systems (Hirao et al, 2002;
Lal and Ruger, 2002) used various learning and
semantic-based methods, although no system could
outperform the baseline with statistical significance
(Nenkova, 2005). After 2002, the single-document
summarization task was dropped.
In recent years, there has been a decline in stud-
ies on automatic single-document summarization,
in part because the DUC task was dropped, and in
part because the task of single-document extracts
may be counterintuitively more difficult than multi-
448
document summarization (Nenkova, 2005). How-
ever, with the ever-growing internet and increased
information access, we believe single-document
summarization is essential to improve quick ac-
cess to large quantities of information. Recently,
CNN.com (CNN.com, 2007a) added ?Story High-
lights? to many news articles on its site to allow
readers to quickly gather information on stories.
These highlights give a brief overview of the arti-
cle and appear as 3?4 related sentences in the form
of bullet points rather than a summary paragraph,
making them even easier to quickly scan.
Our work is motivated by both the addition of
highlights to an extremely visible and reputable on-
line news source, as well as the inability of past
single-document summarization systems to outper-
form the extremely strong baseline of choosing the
first n sentences of a newswire article as the sum-
mary (Nenkova, 2005). Although some recent sys-
tems indicate an improvement over the baseline (Mi-
halcea, 2005; Mihalcea and Tarau, 2005), statistical
significance has not been shown. We show that by
using a neural network ranking algorithm and third-
party datasets to enhance sentence features, our sys-
tem, NetSum, can outperform the baseline with sta-
tistical significance.
Our paper is organized as follows. Section 2 de-
scribes our two studies: summarization and high-
light extraction. We describe our dataset in detail in
Section 3. Our ranking system and feature vectors
are outlined in Section 4. We present our evaluation
measure in Section 5. Sections 6 and 7 report on our
results on summarization and highlight extraction,
respectively. We conclude in Section 8 and discuss
future work in Section 9.
2 Our Task
In this paper, we focus on single-document summa-
rization of newswire documents. Each document
consists of three highlight sentences and the article
text. Each highlight sentence is human-generated,
but is based on the article. In Section 4 we discuss
the process of matching a highlight to an article sen-
tence. The output of our system consists of purely
extracted sentences, where we do not perform any
sentence compression or sentence generation. We
leave such extensions for future work.
We develop two separate problems based on our
document set. First, can we extract three sentences
that best ?match? the highlights as a whole? In
this task, we concatenate the three sentences pro-
duced by our system into a single summary or block,
and similarly concatenate the three highlight sen-
tences into a single summary or block. We then
compare our system?s block against the highlight
block. Second, can we extract three sentences that
best ?match? the three highlights, such that order-
ing is preserved? In this task, we produce three sen-
tences, where the first sentence is compared against
the first highlight, the second sentence is compared
against the second highlight, and the third sentence
is compared against the third highlight. Credit is
not given for producing three sentences that match
the highlights, but are out of order. The second task
considers ordering and compares sentences on an in-
dividual level, whereas the first task considers the
three chosen sentences as a summary or block and
disregards sentence order. In both tasks, we assume
the title has been seen by the reader and will be listed
above the highlights.
3 Evaluation Corpus
Our data consists of 1365 news documents gathered
from CNN.com (CNN.com, 2007a). Each document
was extracted by hand, where a maximum of 50
documents per day were collected. The documents
were hand-collected on consecutive days during the
month of February.
Each document includes the title, timestamp,
story highlights, and article text. The timestamp
on articles ranges from December 2006 to Febru-
ary 2007, since articles remain posted on CNN.com
for up to several months. The story highlights are
human-generated from the article text. The number
of story highlights is between 3?4. Since all articles
include at least 3 story highlights, we consider only
the task of extracting three highlights from each ar-
ticle.
4 Description of Our System
Our goal is to extract three sentences from a single
news document that best match various characteris-
tics of the three document highlights. One way to
identify the best sentences is to rank the sentences
449
TIMESTAMP: 1:59 p.m. EST, January 31, 2007
TITLE: Nigeria reports first human death from bird flu
HIGHLIGHT 1: Government boosts surveillance after woman dies
HIGHLIGHT 2: Egypt, Djibouti also have reported bird flu in humans
HIGHLIGHT 3: H5N1 bird flu virus has killed 164 worldwide since 2003
ARTICLE: 1. Health officials reported Nigeria?s first cases of bird flu in humans on Wednesday,
saying one woman had died and a family member had been infected but was responding to
treatment. 2. The victim, a 22-year old woman in Lagos, died January 17, Information Minister
Frank Nweke said in a statement. 3. He added that the government was boosting surveillance
across Africa?s most-populous nation after the infections in Lagos, Nigeria?s biggest city. 4.
The World Health Organization had no immediate confirmation. 5. Nigerian health officials
earlier said 14 human samples were being tested. 6. Nweke made no mention of those cases on
Wednesday. 7. An outbreak of H5N1 bird flu hit Nigeria last year, but no human infections had
been reported until Wednesday. 8. Until the Nigerian report, Egypt and Djibouti were the only
African countries that had confirmed infections among people. 9. Eleven people have died in
Egypt. 10. The bird flu virus remains hard for humans to catch, but health experts fear H5N1
may mutate into a form that could spread easily among humans and possibly kill millions in
a flu pandemic. 11. Amid a new H5N1 outbreak reported in recent weeks in Nigeria?s north,
hundreds of miles from Lagos, health workers have begun a cull of poultry. 12. Bird flu is
generally not harmful to humans, but the H5N1 virus has claimed at least 164 lives worldwide
since it began ravaging Asian poultry in late 2003, according to the WHO. 13. The H5N1 strain
had been confirmed in 15 of Nigeria?s 36 states. 14. By September, when the last known case
of the virus was found in poultry in a farm near Nigeria?s biggest city of Lagos, 915,650 birds
had been slaughtered nationwide by government veterinary teams under a plan in which the
owners were promised compensation. 15. However, many Nigerian farmers have yet to receive
compensation in the north of the country, and health officials fear that chicken deaths may be
covered up by owners reluctant to slaughter their animals. 16. Since bird flu cases were first
discovered in Nigeria last year, Cameroon, Djibouti, Niger, Ivory Coast, Sudan and Burkina
Faso have also reported the H5N1 strain of bird flu in birds. 17. There are fears that it has
spread even further than is known in Africa because monitoring is difficult on a poor continent
with weak infrastructure. 18. With sub-Saharan Africa bearing the brunt of the AIDS epidemic,
there is concern that millions of people with suppressed immune systems will be particularly
vulnerable, especially in rural areas with little access to health facilities. 19. Many people keep
chickens for food, even in densely populated urban areas.
Figure 1: Example document containing highlights
and article text. Sentences are numbered by their
position. Article is from (CNN.com, 2007b).
using a machine learning approach, for example as
in (Hirao et al, 2002). A train set is labeled such
that the labels identify the best sentences. Then a
set of features is extracted from each sentence in the
train and test sets, and the train set is used to train
the system. The system is then evaluated on the test
set. The system learns from the train set the distri-
bution of features for the best sentences and outputs
a ranked list of sentences for each document. In this
paper, we rank sentences using a neural network al-
gorithm called RankNet (Burges et al, 2005).
4.1 RankNet
From the labels and features for each sentence, we
train a model that, when run on a test set of sen-
tences, can infer the proper ranking of sentences
in a document based on information gathered dur-
ing training about sentence characteristics. To ac-
complish the ranking, we use RankNet (Burges et
al., 2005), a ranking algorithm based on neural net-
works.
RankNet is a pair-based neural network algorithm
used to rank a set of inputs, in this case, the set
of sentences in a given document. The system is
trained on pairs of sentences (Si, Sj), such that Si
should be ranked higher or equal to Sj . Pairs are
generated between sentences in a single document,
not across documents. Each pair is determined from
the input labels. Since our sentences are labeled us-
ing ROUGE (see Section 4.3), if the ROUGE score
of Si is greater than the ROUGE score of Sj , then
(Si, Sj) is one input pair. The cost function for
RankNet is the probabilistic cross-entropy cost func-
tion. Training is performed using a modified version
of the back propagation algorithm for two layer nets
(Le Cun et al, 1998), which is based on optimiz-
ing the cost function by gradient descent. A simi-
lar method of training on sentence pairs in the con-
text of multi-document summarization was recently
shown in (Toutanova et al, 2007).
Our system, NetSum, is a two-layer neural net
trained using RankNet. To speed up the performance
of RankNet, we implement RankNet in the frame-
work of LambdaRank (Burges et al, 2006). For de-
tails, see (Burges et al, 2006; Burges et al, 2005).
We experiment with between 5 and 15 hidden nodes
and with an error rate between 10?2 and 10?7.
We implement 4 versions of NetSum. The first
450
version, NetSum(b), is trained for our first sum-
marization problem (b indicates block). The pairs
are generated using the maximum ROUGE scores
l1 (see Section 4.3). The other three rankers are
trained to identify the sentence in the document
that best matches highlight n. We train one ranker,
NetSum(n), for each highlight n, for n = 1, 2, 3,
resulting in three rankers. NetSum(n) is trained us-
ing pairs generated from the l1,n ROUGE scores be-
tween sentence Si and highlight Hn (see Section
4.3).
4.2 Matching Extracted to Generated
Sentences
In this section, we describe how to determine which
sentence in the document best matches a given high-
light. Choosing three sentences most similar to the
three highlights is very challenging since the high-
lights include content that has been gathered across
sentences and even paragraphs, and furthermore in-
clude vocabulary that may not be present in the
text. Jing showed, for 300 news articles, that 19%
of human-generated summary sentences contain no
matching article sentence (Jing, 2002). In addition,
only 42% of the summary sentences match the con-
tent of a single article sentence, where there are still
semantic and syntactic transformations between the
summary sentence and article sentence.. Since each
highlight is human generated and does not exactly
match any one sentence in the document, we must
develop a method to identify how closely related a
highlight is to a sentence. We use the ROUGE (Lin,
2004b) measure to score the similarity between an
article sentence and a highlight sentence. We antic-
ipate low ROUGE scores for both the baseline and
NetSum due to the difficulty of finding a single sen-
tence to match a highlight.
4.3 ROUGE
Recall-Oriented Understudy for Gisting Evaluation
(Lin, 2004b), known as ROUGE, measures the qual-
ity of a model-generated summary or sentence by
comparing it to a ?gold-standard?, typically human-
generated, summary or sentence. It has been shown
that ROUGE is very effective for measuring both
single-document summaries and single-document
headlines (Lin, 2004a).
ROUGE-N is a N -gram recall between a model-
generated summary and a reference summary. We
use ROUGE-N , for N = 1, for labeling and evalua-
tion of our model-generated highlights.1 ROUGE-
1 and ROUGE-2 have been shown to be statisti-
cally similar to human evaluations and can be used
with a single reference summary (Lin, 2004a). We
have only one reference summary, the set of human-
generated highlights, per document. In our work,
the reference summary can be a single highlight sen-
tence or the highlights as a block. We calculate
ROUGE-N as
?
gramj?R?Si Count(gramj)
?
gramj?R Count(gramj)
, (1)
where R is the reference summary, Si is the model-
generated summary, and N is the length of the N -
gram gramj .2 The numerator cannot excede the
number of N -grams (non-unique) in R.
We label each sentence Si by its ROUGE-1 score.
For the first problem of matching the highlights
as a block, we label each Si by l1, the maximum
ROUGE-1 score between Si and each highlight Hn,
for n = 1, 2, 3, given by l1 = maxn(R(Si,Hn)).
For the second problem of matching three sen-
tences to the three highlights individually, we label
each sentence Si by l1,n, the ROUGE-1 score be-
tween Si and Hn, given by l1,n = R(Si,Hn). The
ranker for highlight n, NetSum(n), is passed sam-
ples labeled using l1,n.
4.4 Features
RankNet takes as input a set of samples, where each
sample contains a label and feature vector. The la-
bels were previously described in Section 4.3. In this
section, we describe each feature in detail and moti-
vate in part why each feature is chosen. We generate
10 features for each sentence Si in each document,
listed in Table 1. Each feature is chosen to identify
characteristics of an article sentence that may match
those of a highlight sentence. Some of the features
such as position and N -gram frequencies are com-
monly used for scoring. Sentence scoring based on
1We use an implementation of ROUGE that does not per-
form stemming or stopword removal.
2ROUGE is typically used when the length of the reference
summary is equal to length of the model-generated summary.
Our reference summary and model-generated summary are dif-
ferent lengths, so there is a slight bias toward longer sentences.
451
Symbol Feature Name
F (Si) Is First Sentence
Pos(Si) Sentence Position
SB(Si) SumBasic Score
SBb(Si) SumBasic Bigram Score
Sim(Si) Title Similarity Score
NT (Si) Average News Query Term Score
NT+(Si) News Query Term Sum Score
NTr(Si) Relative News Query Term Score
WE(Si) Average Wikipedia Entity Score
WE+(Si) Wikipedia Entity Sum Score
Table 1: Features used in our model.
sentence position, terms common with the title, ap-
pearance of keyword terms, and other cue phrases
is known as the Edmundsonian Paradigm (Edmund-
son, 1969; Alfonesca and Rodriguez, 2003; Mani,
2001). We use variations on these features as well
as a novel set of features based on third-party data.
Typically, news articles are written such that the
first sentence summarizes the article. Thus, we in-
clude a binary feature F (Si) that equals 1 if Si is
the first sentence of the document: F (Si) = ?i,1,
where ? is the Kronecker delta function. This fea-
ture is used only for NetSum(b) and NetSum(1).
We include sentence position since we found in
empirical studies that the sentence to best match
highlight H1 is on average 10% down the article, the
sentence to best match H2 is on average 20% down
the article, and the sentence to best match H3 is 31%
down the article.3 We calculate the position of Si in
document D as
Pos(Si) =
i
? , (2)
where i = {1, . . . , ?} is the sentence number and ?
is the number of sentences in D.
We include the SumBasic score (Nenkova et al,
2006) of a sentence to estimate the importance of a
sentence based on word frequency. We calculate the
SumBasic score of Si in document D as
SB(Si) =
?
w?Si p(w)
|Si|
, (3)
3Though this is not always the case, as the sentence to match
H2 precedes that to match H1 in 22.03% of documents, and the
sentence to match H3 precedes that to match H2 in 29.32% of
and precedes that to match H1 in 28.81% of documents.
where p(w) is the probability of word w and |Si| is
the number of words in sentence Si. We calculate
p(w) as p(w) = Count(w)|D| , where Count(w) is the
number of times word w appears in document D and
|D| is the number of words in document D. Note
that the score of a sentence is the average probability
of a word in the sentence.
We also include the SumBasic score over bi-
grams, where w in Eq 3 is replaced by bigrams and
we normalize by the number of bigrams in Si.
We compute the similarity of a sentence Si in doc-
ument D with the title T of D as the relative proba-
bility of title terms t ? T in Si as
Sim(Si) =
?
t?Si p(t)
|Si|
, (4)
where p(t) = Count(t)|T | is the number of times term t
appears in T over the number of terms in T .
The remaining features we use are based on third-
party data sources. Previously, third-party sources
such as WordNet (Fellbaum, 1998), the web (Ja-
galamudi et al, 2006), or click-through data (Sun
et al, 2005) have been used as features. We pro-
pose using news query logs and Wikipedia entities
to enhance features. We base several features on
query terms frequently issued to Microsoft?s news
search engine http://search.live.com/news, and enti-
ties4 found in the online open-source encyclopedia
Wikipedia (Wikipedia.org, 2007). If a query term or
Wikipedia entity appears frequently in a CNN docu-
ment, then we assume highlights should include that
term or entity since it is important on both the doc-
ument and global level. Sentences containing query
terms or Wikipedia entities therefore contain impor-
tant content. We confirm the importance of these
third-party features in Section 7.
We collected several hundred of the most fre-
quently queried terms in February 2007 from the
news query logs. We took the daily top 200 terms
for 10 days. Our hypothesis is that a sentence with
a higher number of news query terms should be a
better candidate highlight. We calculate the average
probability of news query terms q in Si as
NT (Si) =
?
q?Si p(q)
|q ? Si|
, (5)
4We define an entity as a title of a Wikipedia page.
452
where p(q) is the probability of a news term q and
|q ? Si| is the number of news terms in Si. p(q) =
Count(q)
|q?D| , where Count(q) is the number of times
term q appears in D and |q ? D| is the number of
news query terms in D.
We also include the sum of news query terms in
Si, given by NT+(Si) =
?
q?Si p(q), and the rela-
tive probability of news query terms in Si, given by
NTr(Si) =
?
q?Si
p(q)
|Si| .
We perform term disambiguation on each doc-
ument using an entity extractor (Cucerzan, 2007).
Terms are disambiguated to a Wikipedia entity
only if they match a surface form in Wikipedia.
Wikipedia surface forms are terms that disambiguate
to a Wikipedia entity and link to a Wikipedia page
with the entity as its title. For example, ?WHO? and
?World Health Org.? both refer to the World Health
Organization, and should disambiguate to the entity
?World Health Organization?. Sentences in CNN
document D that contain Wikipedia entities that fre-
quently appear in CNN document D are considered
important. We calculate the average Wikipedia en-
tity score for Si as
WE(Si) =
?
e?Si p(e)
|e ? Si|
, (6)
where p(e) is the probability of entity e, given by
p(e) = Count(e)|e?D| , where Count(e) is the number of
times entity e appears in CNN document D and |e ?
D| is the total number of entities in CNN document
D.
We also include the sum of Wikipedia entities,
given by WE+(Si) =
?
e?Si p(e).
Note that all features except position features are
a variant of SumBasic over different term sets. All
features are computed over sentences where every
word has been lowercased and punctuation has been
removed after sentence breaking. We examined us-
ing stemming, but found stemming to be ineffective.
5 Evaluation
We evaluate the performance of NetSum using
ROUGE and by comparing against a baseline sys-
tem. For the first summarization task, we compare
against the baseline of choosing the first three sen-
tences as the block summary. For the second high-
lights task, we compare NetSum(n) against the base-
line of choosing sentence n (to match highlight n).
Both tasks are novel in attempting to match high-
lights rather than a human-generated summary.
We consider ROUGE-1 to be the measure of im-
portance and thus train our model on ROUGE-1 (to
optimize ROUGE-1 scores) and likewise evaluate
our system on ROUGE-1. We list ROUGE-2 scores
for completeness, but do not expect them to be sub-
stantially better than the baseline since we did not
directly optimize for ROUGE-2.5
For every document in our corpus, we compare
NetSum?s output with the baseline output by com-
puting ROUGE-1 and ROUGE-2 between the high-
light block and NetSum and between the highlight
block and the block of sentences. Similarly, for each
highlight, we compute ROUGE-1 and ROUGE-2
between highlight n and NetSum(n) and between
highlight n and sentence n, for n = 1, 2, 3. For
each task, we calculate the average ROUGE-1 and
ROUGE-2 scores of NetSum and of the baseline.
We also report the percent of documents where the
ROUGE-1 score of NetSum is equal to or better than
the ROUGE-1 score of the baseline.
We perform all experiments using five-fold cross-
validation on our dataset of 1365 documents. We
divide our corpus into five random sets and train on
three combined sets, validate on one set, and test on
the remaining set. We repeat this procedure for ev-
ery combination of train, validation, and test sets.
Our results are the micro-averaged results on the five
test sets. For all experiments, Table 3 lists the statis-
tical tests performed and the significance of perfor-
mance differences between NetSum and the baseline
at 95% confidence.
6 Results: Summarization
We first find three sentences that, as a block, best
match the three highlights as a block. NetSum(b)
produces a ranked list of sentences for each docu-
ment. We create a block from the top 3 ranked sen-
tences. The baseline is the block of the first 3 sen-
tences of the document. A similar baseline outper-
5NetSum can directly optimize for any measure by training
on it, such as training on ROUGE-2 or on a weighted sum of
ROUGE-1 and ROUGE-2 to optimize both. Thus, ROUGE-2
scores could be further improved. We leave such studies for
future work.
453
System Av. ROUGE-1 Av. ROUGE-2
Baseline 0.4642 ? 0.0084 0.1726 ? 0.0064
NetSum(b) 0.4956 ? 0.0075 0.1775 ? 0.0066
Table 2: Results on summarization task with stan-
dard error at 95% confidence. Bold indicates signif-
icance under paired tests.
ROUGE-1 ROUGE-2
System 1 2 3 1 2 3
NetSum(b) x x x x o o
NetSum(1) x x x o o o
NetSum(2) x x x x o x
NetSum(3) x x x x x x
Table 3: Paired tests for statistical significance
at 95% confidence between baseline and NetSum
performance; 1: McNemar, 2: Paired t-test, 3:
Wilcoxon signed-rank. ?x? indicates pass, ?o? in-
dicates fail. Since our studies are pair-wise, tests
listed here are more accurate than error bars reported
in Tables 2?5.
forms all previous systems for news article summa-
rization (Nenkova, 2005) and has been used in the
DUC workshops (DUC, 2001).
For each block produced by NetSum(b) and the
baseline, we compute the ROUGE-1 and ROUGE-2
scores of the block against the set of highlights as a
block. For 73.26% of documents, NetSum(b) pro-
duces a block with a ROUGE-1 score that is equal
to or better than the baseline score. The two systems
produce blocks of equal ROUGE-1 score for 24.69%
of documents. Under ROUGE-2, NetSum(b) per-
forms equal to or better than the baseline on 73.19%
of documents and equal to the baseline on 40.51%
of documents.
Table 2 shows the average ROUGE-1 and
ROUGE-2 scores obtained with NetSum(b) and the
baseline. NetSum(b) produces a higher quality
block on average for ROUGE-1.
Table 4 lists the sentences in the block produced
by NetSum(b) and the baseline block, for the arti-
cles shown in Figure 1. The NetSum(b) summary
achieves a ROUGE-1 score of 0.52, while the base-
line summary scores only 0.36.
System Sent. # ROUGE-1
Baseline S1, S2, S3 0.36
NetSum(b) S1, S7, S15 0.52
Table 4: Block results for the block produced by
NetSum(b) and the baseline block for the exam-
ple article. ROUGE-1 scores computed against the
highlights as a block are listed.
7 Results: Highlights
Our second task is to extract three sentences from
a document that best match the three highlights in
order. To accomplish this, we train NetSum(n) for
each highlight n = 1, 2, 3. We compare NetSum(n)
with the baseline of picking the nth sentence of the
document. We perform five-fold cross-validation
across our 1365 documents. Our results are reported
for the micro-average of the test results. For each
highlight n produced by both NetSum(n) and the
baseline, we compute the ROUGE-1 and ROUGE-
2 scores against the nth highlight.
We expect that beating the baseline for n = 1 is a
more difficult task than for n = 2 or 3 since the first
sentence of a news article typically acts as a sum-
mary of the article and since we expect the first high-
light to summarize the article. NetSum(1), however,
produces a sentence with a ROUGE-1 score that is
equal to or better than the baseline score for 93.26%
of documents. The two systems produce sentences
of equal ROUGE-1 scores for 82.84% of documents.
Under ROUGE-2, NetSum(1) performs equal to or
better than the baseline on 94.21% of documents.
Table 5 shows the average ROUGE-1 and
ROUGE-2 scores obtained with NetSum(1) and the
baseline. NetSum(1) produces a higher quality sen-
tence on average under ROUGE-1.
The content of highlights 2 and 3 is typically from
later in the document, so we expect the baseline to
not perform as well in these tasks. NetSum(2) out-
performs the baseline since it is able to identify sen-
tences from further down the document as impor-
tant. For 77.73% of documents, NetSum(2) pro-
duces a sentence with a ROUGE-1 score that is equal
to or better than the score for the baseline. The two
systems produce sentences of equal ROUGE-1 score
for 33.92% of documents. Under ROUGE-2, Net-
Sum(2) performs equal to or better than the baseline
454
System Av. ROUGE-1 Av. ROUGE-2
Baseline(1) 0.4343 ? 0.0138 0.1833 ? 0.0095
NetSum(1) 0.4478 ? 0.0133 0.1857 ? 0.0085
Baseline(2) 0.2451 ? 0.0128 0.0814 ? 0.0106
NetSum(2) 0.3036 ? 0.0117 0.0877 ? 0.0107
Baseline(3) 0.1707 ? 0.0103 0.0412 ? 0.0069
NetSum(3) 0.2603 ? 0.0133 0.0615 ? 0.0075
Table 5: Results on ordered highlights task with
standard error at 95% confidence. Bold indicates
significance under paired tests.
System Sent. # ROUGE-1
Baseline S1 0.167
NetSum(1) S1 0.167
Baseline S2 0.111
NetSum(2) S1 0.556
Baseline S3 0.000
NetSum(3) S15 0.400
Table 6: Highlight results for highlight n produced
by NetSum(n) and highlight n produced by the base-
line for the example article. ROUGE-1 scores com-
puted against highlight n are listed.
84.84% of the time. For 81.09% of documents, Net-
Sum(3) produces a sentence with a ROUGE-1 score
that is equal to or better than the score for the base-
line. The two systems produce sentences of equal
ROUGE-1 score for 28.45% of documents. Under
ROUGE-2, NetSum(3) performs equal to or better
than the baseline 89.91% of the time.
Table 5 shows the average ROUGE-1 and
ROUGE-2 scores obtained for NetSum(2), Net-
Sum(3), and the baseline. Both NetSum(2) and Net-
Sum(3) produce a higher quality sentence on aver-
age under both measures.
Table 6 gives highlights produced by NetSum(n)
and the highlights produced by the baseline, for the
article shown in Figure 1. The NetSum(n) highlights
produce ROUGE-1 scores equal to or higher than the
baseline ROUGE-1 scores.
In feature ablation studies, we confirmed that the
inclusion of news-based and Wikipedia-based fea-
tures improves NetSum?s peformance. For example,
we removed all news-based and Wikipedia-based
features in NetSum(3). The resulting performance
moderately declined. Under ROUGE-1, the base-
line produced a better highlight on 22.34% of docu-
ments, versus only 18.91% when using third-party
features. Similarly, NetSum(3) produced a sum-
mary of equal or better ROUGE-1 score on only
77.66% of documents, compared to 81.09% of doc-
uments when using third-party features. In addi-
tion, the average ROUGE-1 score dropped to 0.2182
and the average ROUGE-2 score dropped to 0.0448.
The performance of NetSum with third-party fea-
tures over NetSum without third-party features is
statistically significant at 95% confidence. However,
NetSum still outperforms the baseline without third-
party features, leading us to conclude that RankNet
and simple position and term frequency features
contribute the maximum performance gains, but in-
creased ROUGE-1 and ROUGE-2 scores are a clear
benefit of third-party features.
8 Conclusions
We have presented a novel approach to automatic
single-document summarization based on neural
networks, called NetSum. Our work is the first
to use both neural networks for summarization and
third-party datasets for features, using Wikipedia
and news query logs. We have evaluated our sys-
tem on two novel tasks: 1) producing a block of
highlights and 2) producing three ordered highlight
sentences. Our experiments were run on previously
unstudied data gathered from CNN.com. Our sys-
tem shows remarkable performance over the base-
line of choosing the first n sentences of the docu-
ment, where the performance difference is statisti-
cally significant under ROUGE-1.
9 Future Work
An immediate future direction is to further explore
feature selection. We found third-party features
beneficial to the performance of NetSum and such
sources can be mined further. In addition, feature se-
lection for each NetSum system could be performed
separately since, for example, highlight 1 has differ-
ent characteristics than highlight 2.
In our experiments, ROUGE scores are fairly low
because a highlight rarely matches the content of a
single sentence. To improve NetSum?s performance,
we must consider extracting content across sentence
455
boundaries. Such work requires a system to produce
abstract summaries. We hope to incorporate sen-
tence simplification and sentence splicing and merg-
ing in a future version of NetSum.
Another future direction is the identification of
?hard? and ?easy? inputs. Although we report av-
erage ROUGE scores, such measures can be mis-
leading since some highlights are simple to match
and some are much more difficult. A better system
evaluation measure would incorporate the difficulty
of the input and weight reported results accordingly.
References
E. Alfonesca and P. Rodriguez. 2003. Description of
the uam system for generating very short summaries
at DUC?2003. In DUC 2003: Document Under-
standing Conference, May 31?June 1, 2003, Edmon-
ton, Canada.
C. Aone, M. Okurowski, and J. Gorlinsky. 1998. Train-
able scalable summarization using robust nlp and ma-
chine learning. In Proceedings of the 17th COLING
and 36th ACL.
C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005.
Learning to Rank using Gradient Descent. In Luc De
Raedt and Stefan Wrobel, editors, ICML, pages 89?96.
ACM.
C.J.C. Burges, R. Ragno, and Q. Le. 2006. Learning to
rank with nonsmooth cost functions. In NIPS 2006:
Neural Information Processing Systems, December 4-
7, 2006, Vancouver, CA.
CNN.com. 2007a. Cable news network.
http://www.cnn.com/.
CNN.com. 2007b. Nigeria reports
first human death from bird flu.
http://edition.cnn.com/2007/WORLD/africa/01/31/
nigeria.bird.flu.ap/index.html?eref=edition world.
J. Conroy, J. Schlesinger, J. Goldstein, and D. O?Leary.
2004. Left-brain/right-brain multi-document summa-
rization. In DUC 2004: Document Understanding
Workshop, May 6?7, 2004, Boston, MA, USA.
S. Cucerzan. 2007. Large scale named entity disam-
biguation based on wikipedia data. In EMNLP 2007:
Empirical Methods in Natural Language Processing,
June 28-30, 2007, Prague, Czech Republic.
H. Daume? III and D. Marcu. 2005. Bayesian multi-
document summarization at mse. In Proceedings of
MSE.
DUC. 2001. Document understanding conferences.
http://www-nlpir.nist.gov/projects/duc/index.html.
H.P. Edmundson. 1969. New methods in automatic ex-
tracting. Journal for the Association of Computing
Machinery, 16:159?165.
G. Erkan and D. R. Radev. 2004a. Lexpagerank:
Prestige in multi-document text summarization. In
EMNLP 2004: Empirical Methods in Natural Lan-
guage Processing, 2004, Barcelona, Spain.
G. Erkan and D. R. Radev. 2004b. Lexrank: Graph-
based centrality as salience in text summarization.
Journal of Artificial Intelligence Research (JAIR), 22.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
T. Hirao, Y. Sasaki, H. Isozaki, and E. Maeda. 2002.
Ntt?s text summarization system for DUC?2002. In
DUC 2002: Workshop on Text Summarization, July
11?12, 2002, Philadelphia, PA, USA.
J. Jagalamudi, P. Pingali, and V. Varma. 2006. Query
independent sentence scoring approach to DUC 2006.
In DUC 2006: Document Understanding Conference,
June 8?9, 2006, Brooklyn, NY, USA.
H. Jing. 2002. Using hidden markov modeling to de-
compose human-written summaries. Computational
Linguistics, 4(28):527?543.
J. Kupiec, J. Pererson, and F. Chen. 1995. A trainable
document summarizer. Research and Development in
Information Retrieval, pages 68?73.
P. Lal and S. Ruger. 2002. Extract-based summarization
with simplification. In DUC 2002: Workshop on Text
Summarization, July 11?12, 2002, Philadelphia, PA,
USA.
Y. Le Cun, L. Bottou, G.B. Orr, and K.R. Mu?ller.
1998. Efficient backprop. In Neural Networks, Tricks
of the Trade, Lecture Notes in Computer Science
LNCS 1524. Springer Verlag.
C.Y. Lin and E. Hovy. 2002. Automated multi-document
summarization in neats. In Proceedings of the Human
Language Technology Conference (HLT2002).
C.Y. Lin. 2004a. Looking for a few good metrics: Auto-
matic summarization evaluation ? how many samples
are enough? In Proceedings of the NTCIR Workshop
4, June 2?4, 2004, Tokyo, Japan.
C.Y. Lin. 2004b. Rouge: A package for automatic evalu-
ation of summaries. In WAS 2004: Proceedings of the
Workshop on Text Summarization Branches Out, July
25?26, 2004, Barcelona, Spain.
456
H. Luhn. 1958. The automatic creation of literature ab-
stracts. IBM Journal of Research and Development,
2(2):159?165.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins Pub. Co.
R. Mihalcea and D. R. Radev, editors. 2006. Textgraphs:
Graph-based methods for NLP. New York City, NY.
R. Mihalcea and P. Tarau. 2005. An algorithm for lan-
guage independent single and multiple document sum-
marization. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), October, 2005, Korea.
R. Mihalcea. 2005. Language independent extractive
summarization. In ACL 2005: Proceedings of the 43rd
Annual Meeting of the Association for Computational
Linguistics, June, 2005, Ann Arbor, MI, USA.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A compositional context sensitive multi-document
summarizer: exploring the factors that influence sum-
marization. In E. N. Efthimiadis, S. T. Dumais,
D. Hawking, and K. Ja?rvelin, editors, SIGIR, pages
573?580. ACM.
A. Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document un-
derstanding conference. In Proceedings of the 20th
National Conference on Artificial Intelligence (AAAI
2005), Pittsburgh, PA.
B. Schiffman. 2002. Building a resource for evaluat-
ing the importance of sentences. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC).
J.T. Sun, D. Shen, H.J. Zeng, Q. Yang, Y. Lu, and
Z. Chen. 2005. Web-page summarization using
click-through data. In R. A. Baeza-Yates, N. Ziviani,
G. Marchionini, A. Moffat, and J. Tait, editors, SIGIR.
ACM.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarla-
mudi, H. Suzuki, and L. Vanderwende. 2007. The
pythy summarization system: Microsoft research at
DUC2007. In DUC 2007: Document Understanding
Conference, April 26?27, 2007, Rochester, NY, USA.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Mi-
crosoft research at DUC2006: Task-focused summa-
rization with sentence simplification. In DUC 2006:
Document Understanding Workshop, June 8?9, 2006,
Brooklyn, NY, USA.
Wikipedia.org. 2007. Wikipedia org.
http://www.wikipedia.org.
W.T. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007. Multi-document summarization by maximizing
informative content words. In IJCAI 2007: 20th In-
ternational Joint Conference on Artificial Intelligence,
January, 2007.
457
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 505?513,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Model Adaptation via Model Interpolation and Boosting 
for Web Search Ranking  
Jianfeng Gao*, Qiang Wu*, Chris Burges*, Krysta Svore*, 
Yi Su#, Nazan Khan$, Shalin Shah$, Hongyan Zhou$ 
*Microsoft Research, Redmond, USA  
{jfgao; qiangwu; cburges; ksvore}@microsoft.com 
#Johns Hopkins University, USA 
suy@jhu.edu
  
$Microsoft Bing Search, Redmond, USA 
{nazanka; a-shas; honzhou}@microsoft.com 
Abstract 
This paper explores two classes of model adapta-
tion methods for Web search ranking: Model In-
terpolation and error-driven learning approaches 
based on a boosting algorithm.  The results show 
that model interpolation, though simple, achieves 
the best results on all the open test sets where the 
test data is very different from the training data. 
The tree-based boosting algorithm achieves the 
best performance on most of the closed test sets 
where the test data and the training data are sim-
ilar, but its performance drops significantly on 
the open test sets due to the instability of trees.  
Several methods are explored to improve the 
robustness of the algorithm, with limited success. 
1 Introduction 
We consider the task of ranking Web search 
results, i.e., a set of retrieved Web documents 
(URLs) are ordered by relevance to a query is-
sued by a user.  In this paper we assume that the 
task is performed using a ranking model (also 
called ranker for short) that is learned on labeled 
training data (e.g., human-judged 
query-document pairs).  The ranking model acts 
as a function that maps the feature vector of a 
query-document pair to a real-valued score of 
relevance. 
Recent research shows that such a learned 
ranker is superior to classical retrieval models in 
two aspects (Burges et al, 2005; 2006; Gao et al, 
2005).  First, the ranking model can use arbitrary 
features. Both traditional criteria such as TF-IDF 
and BM25, and non-traditional features such as 
hyperlinks can be incorporated as features in the 
ranker. Second, if large amounts of high-quality 
human-judged query-document pairs were 
available for model training, the ranker could 
achieve significantly better retrieval results than 
the traditional retrieval models that cannot ben-
efit from training data effectively.  However, 
such training data is not always available for 
many search domains, such as non-English 
search markets or person name search. 
One of the most widely used strategies to re-
medy this problem is model adaptation, which 
attempts to adjust the parameters and/or struc-
ture of a model trained on one domain (called the 
background domain), for which large amounts of 
training data are available, to a different domain 
(the adaptation domain), for which only small 
amounts of training data are available.  In Web 
search applications, domains can be defined by 
query types (e.g., person name queries), or lan-
guages, etc. 
In this paper we investigate two classes of 
model adaptation methods for Web search 
ranking: Model Interpolation approaches and 
error-driven learning approaches.  In model 
interpolation approaches, the adaptation data is 
used to derive a domain-specific model (also 
called in-domain model), which is then com-
bined with the background model trained on the 
background data.  This appealingly simple con-
cept provides fertile ground for experimentation, 
depending on the level at which the combination 
is implemented (Bellegarda, 2004).  In er-
ror-driven learning approaches, the background 
model is adjusted so as to minimize the ranking 
errors the model makes on the adaptation data 
(Bacchiani et al, 2004; Gao et al 2006).  This is 
arguably more powerful than model interpola-
tion for two reasons.  First, by defining a proper 
error function, the method can optimize more 
directly the measure used to assess the final 
quality of the Web search system, e.g., Normalized 
Discounted Cumulative Gain (Javelin & Kekalainen, 
2000) in this study.  Second, in this framework, 
the model can be adjusted to be as fine-grained as 
necessary.  In this study we developed a set of 
error-driven learning methods based on a 
boosting algorithm where, in an incremental 
manner, not only each feature weight could be 
505
changed separately, but new features could be 
constructed. 
We focus our experiments on the robustness 
of the adaptation methods. A model is robust if it 
performs reasonably well on unseen test data 
that could be significantly different from training 
data.  Robustness is important in Web search 
applications.  Labeling training data takes time.  
As a result of the dynamic nature of Web, by the 
time the ranker is trained and deployed, the 
training data may be more or less out of date.  
Our results show that the model interpolation is 
much more robust than the boosting-based me-
thods. We then explore several methods to im-
prove the robustness of the methods, including 
regularization, randomization, and using shal-
low trees, with limited success. 
2 Ranking Model and Quality 
Measure in Web Search 
This section reviews briefly a particular example 
of rankers, called LambdaRank (Burges et al, 
2006), which serves as the baseline ranker in our 
study.  
Assume that training data is a set of input/ 
output pairs (x, y). x is a feature vector extracted 
from a query-document pair. We use approx-
imately 400 features, including dynamic ranking 
features such as term frequency and BM25, and 
statistic ranking features such as PageRank.  y is 
a human-judged relevance score, 0 to 4, with 4 as 
the most relevant. 
LambdaRank is a neural net ranker that maps 
a feature vector x to a real value y that indicates 
the relevance of the document given the query 
(relevance score).  For example, a linear Lamb-
daRank simply maps x to y with a learned weight 
vector w such that ? = ? ? ?. (We used nonli-
near LambdaRank in our experiments). Lamb-
daRank is particularly interesting to us due to the 
way w is learned. Typically, w is optimized w.r.t. 
a cost function using numerical methods if the 
cost function is smooth and its gradient w.r.t. w 
can be computed easily.  In order for the ranker 
to achieve the best performance in document 
retrieval, the cost function used in training 
should be the same as, or as close as possible to, 
the measure used to assess the quality of the 
system. In Web search, Normalized Discounted 
Cumulative Gain (NDCG) (Jarvelin and Kekalai-
nen, 2000) is widely used as quality measure. For 
a query,  NDCG is computed  as 
?? = ?? 
2? ?  ? 1
log 1 + ? 
?
?=1
, (1) 
where ?(?) is the relevance level of the j-th doc-
ument, and the normalization constant Ni is 
chosen so that a perfect ordering would result in 
?? = 1.  Here L is the ranking truncation level at 
which NDCG is computed. The ??  are then av-
eraged over a query set. However, NDCG, if it 
were to be used as a cost function, is either flat or 
discontinuous everywhere, and thus presents 
challenges to most optimization approaches that 
require the computation of the gradient of the 
cost function.  
LambdaRank solves the problem by using an 
implicit cost function whose gradients are speci-
fied by rules. These rules are called ?-functions. 
Burges et al (2006) studied several ?-functions 
that were designed with the NDCG cost function 
in mind. They showed that LambdaRank with 
the best ?-function outperforms significantly a 
similar neural net ranker, RankNet (Burges et al, 
2005), whose parameters are optimized using the 
cost function based on cross-entropy. 
The superiority of LambdaRank illustrates the 
key idea based on which we develop the model 
adaptation methods.  We should always adapt 
the ranking models in such a way that the NDCG 
can be optimized as directly as possible. 
3 Model Interpolation 
One of the simplest model interpolation methods 
is to combine an in-domain model with a back-
ground model at the model level via linear in-
terpolation.  In practice we could combine more 
than two in-domain/background models.  Let-
ting Score(q, d) be a ranking model that maps a 
query-document pair to a relevance score, the 
general form of the interpolation model is  
?????(?,?) = ???????? ?,? ,
?
?=1
 (2) 
where the ??s are interpolation weights, opti-
mized on validation data with respect to a pre-
defined objective, which is NDCG in our case.  
As mentioned in Section 2, NDCG is not easy to 
optimize, for which we resort to two solutions, 
both of which achieve similar results in our ex-
periments. 
The first solution is to view the interpolation 
model of Equation (2) as a linear neural net 
ranker where each component  model Scorei(.) is 
defined as a feature function. Then, we can use 
the LambdaRank algorithm described in Section 
2 to find the optimal weights.  
An alternative solution is to view interpola-
tion weight estimation as a multi-dimensional 
optimization problem, with each model as a 
506
dimension. Since NCDG is not differentiable, we 
tried in our experiments the numerical algo-
rithms that do not require the computation of 
gradient. Among the best performers is the 
Powell Search algorithm (Press et al, 1992). It 
first constructs a set of N virtual directions that 
are conjugate (i.e., independent with each other), 
then it uses line search N times, each on one vir-
tual direction, to find the optimum.  Line search 
is a one-dimensional optimization algorithm. 
Our implementation follows the one described in 
Gao et al (2005), which is used to optimize the 
averaged precision.  
The performance of model interpolation de-
pends to a large degree upon the quality and the 
size of adaptation data. First of all, the adaptation 
data has to be ?rich? enough to suitably charac-
terize the new domain.  This can only be 
achieved by collecting more in-domain data.  
Second, once the domain has been characterized, 
the adaptation data has to be ?large? enough to 
have a model reliably trained.  For this, we de-
veloped a method, which attempts to augment 
adaptation data by gathering similar data from 
background data sets. 
The method is based on the k-nearest-neighbor 
(kNN) algorithm, and is inspired by Bishop 
(1995).  We use the small in-domain data set D1 
as a seed, and expand it using the large back-
ground data set D2.  When the relevance labels 
are assigned by humans, it is reasonable to as-
sume that queries with the lowest information 
entropy of labels are the least noisy.  That is, for 
such a query most of the URLs are labeled as 
highly relevant/not relevant documents rather 
than as moderately relevance/not relevant 
documents. 
Due to computational limitations of 
kNN-based algorithms, a small subset of queries 
from D1 which are least noisy are selected. This 
data set is called S1.  For each sample in D2, its 
3-nearest neighbors in S1 are found using a co-
sine-similarity metric.  If the three neighbors are 
within a very small distance from the sample in 
D2, and one of the labels of the nearest neighbors 
matches exactly, the training sample is selected 
and is added to the expanded set E2, in its own 
query.  This way, S1 is used to choose training 
data from D2, which are found to be close in 
some space.  
This process effectively creates several data 
points in close neighborhood of the points in the 
original small data set D1, thus expanding the 
set, by jittering each training sample a little. This 
is equivalent to training with noise (Bishop, 
1995), except that the training samples used are 
actual queries judged by a human. This is found 
to increase the NDCG in our experiments. 
4 Error-Driven Learning 
Our error-drive learning approaches to ranking 
modeling adaptation are based on the Stochastic 
Gradient Boosting algorithm (or the boosting 
algorithm for short) described in Friedman 
(1999). Below, we follow the notations in Fried-
man (2001). 
Let adaptation data (also called training data in 
this section) be a set of input/output pairs {xi, yi}, 
i = 1?N. In error-driven learning approaches, 
model adaptation is performed by adjusting the 
background model into a new in-domain model 
?: ? ? ? that minimizes a loss function L(y, F(x)) 
over all samples in training data  
?? = argmin
?
 ?(?? ,?(??))
?
?=1
. (3) 
We further assume that F(x) takes the form of 
additive expansion as 
? ? =  ??? ?; ??  
?
?=0
, (4) 
where h(x; a) is called basis function, and is 
usually a simple parameterized function of the 
input x, characterized by parameters a. In what 
follows, we drop a, and use h(x) for short.  In 
practice, the form of h has to be restricted to a 
specific function family to allow for a practically 
efficient procedure of model adaptation.  ? is a 
real-valued coefficient. 
Figure 1 is the generic algorithm.  It starts 
with a base model F0, which is a background 
model.  Then for m = 1, 2, ?, M, the algorithm 
takes three steps to adapt the base model so as to 
best fit the adaptation data: (1) compute the re-
sidual of the current base model (line 3), (2) select 
the optimal basis function (line 4) that best fits 
the residual, and (3) update the base model by 
adding the optimal basis function (line 5).  The 
two model adaptation algorithms that will be 
described below follow the same 3-step adapta-
tion procedure. They only differ in the choice of 
h.  In the LambdaBoost algorithm (Section 4.1) h 
1 Set F0(x) be the background ranking model 
2 for m = 1 to M do 
3 ??
? = ?  
?? ? ? ,? ??  
?? ?? 
 
? ? =???1 ? 
, for i = 1? N 
4 (?? ,?? ) = argmin
? ,?
  ??
? ? ??(??) 
2?
?=1
 
5 ??  ? = ???1 ? + ???(?) 
Figure 1. The generic boosting algorithm for model 
adaptation 
507
is defined as a single feature, and in LambdaS-
MART (Section 4.2), h is a regression tree.  
Now, we describe the way residual is com-
puted, the step that is identical in both algo-
rithms. Intuitively, the residual, denoted by y? 
(line 3 in Figure 1), measures the amount of er-
rors (or loss) the base model makes on the train-
ing samples.  If the loss function in Equation (3) is 
differentiable, the residual can be computed 
easily as the negative  
gradient of the loss function.  As discussed in 
Section 2, we want to directly optimize the 
NDCD, whose gradient is approximated via the 
?-function.  Following Burges et al (2006), the 
gradient of a training sample (xi, yi), where xi is a 
feature vector representing the query-document 
pair (qi, di), w.r.t. the current base model is com-
puted by marginalizing the ?-functions of all 
document pairs, (di, dj), of the query, qi, as 
??
? = ?NDCG ?
????
????
,
???
 (5) 
where ?NDCG is the NDCG gained by swapping 
those two documents (after sorting all docu-
ments by their current scores);  ??? ? ?? ? ??  is the 
difference in ranking scores of di and dj given qi; 
and Cij is the cross entropy cost defined as  
??? ? ? ???  = ?? ? ??
+ log(1 + exp(?? ? ?? )). 
(6) 
Thus, we have 
????
????
=
?1
1 + exp ???  
. (7) 
This ?-function essentially uses the cross en-
tropy cost to smooth the change in NDCG ob-
tained by swapping the two documents. A key 
intuition behind the ?-function is the observation 
that NDCG does not treat all pairs equally; for 
example, it costs more to incorrectly order a pair, 
where the irrelevant document is ranked higher 
than a highly relevant document, than it does to 
swap a moderately relevant/not relevant pair. 
4.1 The LambdaBoost Algorithm 
In LambdaBoost, the basis function h is defined 
as a single feature (i.e., an element feature in the 
feature vector x).  The algorithm is summarized 
in Figure 2.  It iteratively adapts a background 
model to training data using the 3-step proce-
dure, as in Figure 1. Step 1 (line 3 in Figure 2) has 
been described.  
Step 2 (line 4 in Figure 2) finds the optimal 
basis function h, as well as its optimal coefficient 
?, that best fits the residual according to the 
least-squares (LS) criterion. Formally, let h and ? 
denote the candidate basis function and its op-
timal coefficient. The LS error on training data 
is ?? ?;? =   ??
? ? ?? ??=0
2
, where ??
?  is com-
puted as Equation (5). The optimal coefficient of 
h is estimated by solving the equation ?   ??
? ???=1
??2/??=0. Then, ? is computed as 
? =
 ??
??(??)
?
?=1
 ?(??)
?
?=1
. (8) 
Finally, given its optimal coefficient ?, the op-
timal LS loss of h is  
?? ?;? = ??
? ? ??
?  
?
?=1
?
  ??
?? ?? 
?
?=1  
2
 ?2(??)
?
?=1
. (9) 
Step 3 (line 5 in Figure 2) updates the base 
model by adding the chosen optimal basis func-
tion with its optimal coefficient.  As shown in 
Step 2, the optimal coefficient of each candidate 
basis function is computed when the basis func-
tion is evaluated.  However, adding the basis 
function using its optimal efficient is prone to 
overfitting. We thus add a shrinkage coefficient 0 
< ? < 1 ? the fraction of the optimal line step 
taken. The update equation is thus rewritten in 
line 5 in Figure 2.   
Notice that if the background model contains 
all the input features in x, then LambdaBoost 
does not add any new features but adjust the 
weights of existing features.  If the background 
model does not contain all of the input features, 
then LambdaBoost can be viewed as a feature 
selection method, similar to Collins (2000), where 
at each iteration the feature that has the largest 
impact on reducing training loss is selected and 
added to the background model. In either case, 
LambdaBoost adapts the background model by 
adding a model whose form is a (weighted) li-
near combination of input features.  The property 
of linearity makes LambdaBoost robust and less 
likely to overfit in Web search applications.  But 
this also limits the adaptation capacity. A simple 
method that allows us to go beyond linear 
adaptation is to define h as nonlinear terms of the 
input features, such as regression trees in 
LambdaSMART. 
4.2 The LambdaSMART Algorithm 
LambdaSMART was originally proposed in Wu 
et al (2008). It is built on MART (Friedman, 2001) 
but uses the ?-function (Burges et a., 2006) to 
1 Set F0(x) to be the background ranking model 
2 for m = 1 to M do 
3 compute residuals according to Equation (5)  
4 select best hm (with its best ?m), according to LS, 
computed by Equations (8) and (9) 
5 ??  ? = ???1 ? + ????(?) 
Figure 2. The LambdaBoost algorithm for model adaptation. 
508
compute gradients. The algorithm is summa-
rized in Figure 3.  Similar to LambdaBoost, it 
takes M rounds, and at each boosting iteration, it 
adapts the background model to training data 
using the 3-step procedure. Step 1 (line 3 in Fig-
ure 3) has been described.  
Step 2 (lines 4 to 6) searches for the optimal 
basis function h to best fit the residual.  Unlike 
LambdaBoost where there are a finite number of 
candidate basis functions, the function space of 
regression trees is infinite. We define h as a re-
gression tree with L terminal nodes.  In line 4, a 
regression tree is built using Mean Square Error 
to determine the best split at any node in the tree.  
The value associated with a leaf (i.e., terminal 
node) of the trained tree is computed first as the 
residual (computed via ?-function) for the train-
ing samples that land at that leaf.  Then, since 
each leaf corresponds to a different mean, a 
one-dimensional Newton-Raphson line step is 
computed for each leaf (lines 5 and 6).  These line 
steps may be simply computed as the derivatives 
of the LambdaRank gradients w.r.t. the model 
scores si.  Formally, the value of the l-th leaf, ?ml, 
is computed as 
??? =
 ??
?
?????
 ???????
, (10) 
where ??
?  is the residual of training sample i, 
computed in Equation (5), and  ??  is the deriva-
tive of ??
? , i.e., ?? = ???
?/??(??). 
In Step 3 (line 7), the regression tree is added 
to the current base model, weighted by the 
shrinkage coefficient 0 < ? < 1.  
Notice that since a regression tree can be 
viewed as a complex feature that combines mul-
tiple input features, LambdaSMART can be used 
as a feature generation method. LambdaSMART 
is arguably more powerful than LambdaBoost in 
that it introduces new complex features and thus 
adjusts not only the parameters but also the 
structure of the background model1. However, 
                                                     
1  Note that in a sense our proposed LambdaBoost 
algorithm is the same as LambdaSMART, but using a 
single feature at each iteration, rather than a tree. In 
particular, they share the trick of using the Lambda 
one problem of trees is their high variance.  
Often a small change in the data can result in a 
very different series of splits.  As a result, 
tree-based ranking models are much less robust 
to noise, as we will show in our experiments.  In 
addition to the use of shrinkage coefficient 0 < ? 
< 1, which is a form of model regularization 
according to Hastie, et al, (2001), we will ex-
plore in Section 5.3 other methods of improving 
the model robustness, including randomization 
and using shallow trees. 
5 Experiments 
5.1 The Data 
We evaluated the ranking model adaptation 
methods on two Web search domains, namely (1) 
a name query domain, which consists of only 
person name queries, and (2) a Korean query 
domain, which consists of queries that users 
submitted to the Korean market.   
For each domain, we used two in-domain 
data sets that contain queries sampled respec-
tively from the query log of a commercial Web 
search engine that were collected in two 
non-overlapping periods of time.  We used the 
more recent one as open test set, and split the 
other into three non-overlapping data sets, 
namely training, validation and closed test sets, 
respectively.  This setting provides a good si-
mulation to the realistic Web search scenario, 
where the rankers in use are usually trained on 
early collected data, and thus helps us investigate 
the robustness of these model adaptation me-
thods. 
The statistics of the data sets used in our per-
son name domain adaptation experiments are 
shown in Table 1. The names query set serves as 
the adaptation domains, and Web-1 as the back-
ground domain. Since Web-1 is used to train a 
background ranker, we did not split it to 
train/valid/test sets. We used 416 input features 
in these experiments.  
For cross-domain adaptation experiments 
from non-Korean to Korean markets, Korean 
data serves as the adaptation domain, and Eng-
lish, Chinese, and Japanese data sets as the 
background domain.  Again, we did not split the 
data sets in the background domain to 
train/valid/test sets.  The statistics of these data 
sets are shown in Table 2. We used 425 input 
features in these experiments. 
                                                                                
gradients to learn NDCG. 
1 Set F0(x) to be the background ranking model 
2 for m = 1 to M do 
3 compute residuals according to Equation (5)  
4 create a  L-terminal node tree, ?? ?  ???  ?=1??  
5 for l = 1 to L do 
6 compute the optimal ?lm according to Equation 
(10), based on approximate Newton step. 
7 ??  ? = ???1 ? + ? ???1(? ? ??? )
?=1??
 
Figure 3. The LambdaSMART algorithm for model adaptation. 
509
In each domain, the in-domain training data is 
used to train in-domain rankers, and the back-
ground data for background rankers. Validation 
data is used to learn the best training parameters 
of the boosting algorithms, i.e., M, the total 
number of boosting iterations, ?, the shrinkage 
coefficient, and L, the number of leaf nodes for 
each regression tree (L=1 in LambdaBoost). 
Model performance is evaluated on the 
closed/open test sets.  
All data sets contain samples labeled on a 
5-level relevance scale, 0 to 4, with 4 as most 
relevant and 0 as irrelevant. The performance of 
rankers is measured through NDCG evaluated 
against closed/open test sets.  We report NDCG 
scores at positions 1, 3 and 10, and the averaged 
NDCG score (Ave-NDCG), the arithmetic mean 
of the NDCG scores at 1 to 10. Significance test 
(i.e., t-test) was also employed. 
5.2 Model Adaptation Results 
This section reports the results on two adapta-
tion experiments.  The first uses a large set of 
Web data, Web-1, as background domain and 
uses the name query data set as adaptation data. 
The results are summarized in Tables 3 and 4.  
We compared the three model adaptation me-
thods against two baselines: (1) the background 
ranker (Row 1 in Tables 3 and 4), a 2-layer 
LambdaRank model with 15 hidden nodes and a 
learning rate of 10-5 trained on Web-1; and (2) the 
In-domain Ranker (Row 2), a 2-layer Lambda-
Rank model with 10 hidden nodes and a learning 
rate of 10-5 trained on Names-1-Train.  We built 
two interpolated rankers.  The 2-way interpo-
lated ranker (Row 3) is a linear combination of 
the two baseline rankers, where the interpolation 
weights were optimized on Names-1-Valid.  To 
build the 3-way interpolated ranker (Row 4), we 
linearly interpolated three rankers.  In addition 
to the two baseline rankers, the third ranker is 
trained on an augmented training data, which 
was created using the kNN method described in 
Section 3.   
In LambdaBoost (Row 5) and LambdaSMART 
(Row 6), we adapted the background ranker to 
name queries by boosting the background ranker 
with Names-1-Train. We trained LambdaBoost 
with the setting M = 500, ? = 0.5, optimized on 
Names-1-Valid. Since the background ranker 
uses all of the 416 input features, in each boosting 
iteration, LambdaBoost in fact selects one exist-
ing feature in the background ranker and adjusts 
its weight. We trained LambdaSMART with M = 
500, L = 20, ? = 0.5, optimized on Names-1-Valid. 
We see that the results on the closed test set 
(Table 3) are quite different from the results on 
the open test set (Table 4).  The in-domain ranker 
outperforms the background ranker on the 
closed test set, but underperforms significantly 
the background ranker on the open test set.  The 
interpretation is that the training set and the 
closed test set are sampled from the same data 
set and are very similar, but the open test set is a 
very different data set, as described in Section 5.1.  
Similarly, on the closed test set, LambdaSMART 
outperforms LambdaBoost with a big margin 
due to its superior adaptation capacity; but on 
the open test set their performance difference is 
much smaller due to the instability of the trees in 
LambdaSMART, as we will investigate in detail 
later.  Interestingly, model interpolation, though 
simple, leads to the two best rankers on the open 
test set. In particular, the 3-way interpolated 
ranker outperforms the two baseline rankers 
Coll. Description  #qry. # url/qry 
Web-1 Background training data 31555 134 
Names-1-Train In-domain training data  
(adaptation data)  
5752 85 
Names-1-Valid In-domain validation data 158 154 
Names-1-Test Closed test data 318 153 
Names-2-Test Open test data 4370 84 
Table 1. Data sets in the names query domain experiments,  
where # qry is number of queries, and # url/qry is number 
of documents per query. 
Coll. Description  # qry. # url/qry 
Web-En Background En training data 6167 198 
Web-Ja Background Ja training data 45012 58 
Web-Cn Background Ch training data 32827 72 
Kokr-1-Train In-domain Ko training data 
(adaptation data)  
3724 64 
Kokr-1-Valid In-domain validation data 334 130 
Kokr-1-Test Korean closed test data 372 126 
Kokr-2-Test Korean open test data 871 171 
Table 2. Data sets in the Korean domain experiments. 
# Models NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Back. 0.4575 0.4952 0.5446 0.5092 
2 In-domain 0.4921 0.5296 0.5774 0.5433 
3 2W-Interp. 0.4745 0.5254 0.5747 0.5391 
4 3W-Interp. 0.4829 0.5333 0.5814 0.5454 
5 ?-Boost 0.4706 0.5011 0.5569 0.5192 
6 ?-SMART 0.5042 0.5449 0.5951 0.5623 
Table 3. Close test results on Names-1-Test. 
# Models NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Back. 0.5472 0.5347 0.5731 0.5510 
2 In-domain 0.5216 0.5266 0.5789 0.5472 
3 2W-Interp. 0.5452 0.5414 0.5891 0.5604 
4 3W-Interp. 0.5474 0.5470 0.5951 0.5661 
5 ?-Boost 0.5269 0.5233 0.5716 0.5428 
6 ?-SMART 0.5200 0.5331 0.5875 0.5538 
Table 4. Open test results on Names-2-Test. 
510
significantly (i.e., p-value < 0.05 according to 
t-test) on both the open and closed test sets. 
The second adaptation experiment involves 
data sets from several languages (Table 2).  
2-layer LambdaRank baseline rankers were first 
built from Korean, English, Japanese, and Chi-
nese training data and tested on Korean test sets 
(Tables 5 and 6).  These baseline rankers then 
serve as in-domain ranker and background 
rankers for model adaptation.  For model inter-
polation (Tables 7 and 8), Rows 1 to 4 are three 
2-way interpolated rankers built by linearly in-
terpolating  
each of the three background rankers with the 
in-domain ranker, respectively.  Row 4 is a 4-way 
interpolated ranker built by interpolating the 
in-domain ranker with the three background 
rankers.  For LambdaBoost (Tables 9 and 10) and 
LambdaSMART (Tables 11 and 12), we used the 
same parameter settings as those in the name 
query experiments, and adapted the three back-
ground rankers, to the Korean training data, 
Kokr-1-Train. 
The results in Tables 7 to 12 confirm what we 
learned in the name query experiments. There 
are three main conclusions. (1) Model interpola-
tion is an effective method of ranking model 
adaptation. E.g., the 4-way interpolated ranker 
outperforms other ranker significantly. (2) 
LambdaSMART is the best performer on the 
closed test set, but its performance drops signif-
icantly on the open test set due to the instability 
of trees. (3) LambdaBoost does not use trees. So 
its modeling capacity is weaker than Lamb-
daSMART (e.g., it always underperforms 
LambdaSMART significantly on the closed test 
sets), but it is more robust due to its linearity (e.g., 
it performs similarly to LambdaSMART on the 
open test set). 
5.3 Robustness of Boosting Algorithms 
This section investigates the robustness issue 
of the boosting algorithms in more detail. We 
compared LambdaSMART with different values 
of L (i.e., the number of leaf nodes), and with and 
without randomization. Our assumptions are (1) 
allowing more leaf nodes would lead to deeper 
trees, and as a result, would make the resulting 
ranking models less robust; and (2) injecting 
randomness into the basis function (i.e. regres-
sion tree) estimation procedure would improve 
the robustness of the trained models (Breiman, 
2001; Friedman, 1999).  In LambdaSMART, the 
randomness can be injected at different levels of 
tree construction.  We found that the most effec-
tive method is to introduce the randomness at 
the node level (in Step 4 in Figure 3). Before each 
node split, a subsample of the training data and a 
subsample of the features are drawn randomly. 
(The sample rate is 0.7). Then, the two randomly 
selected subsamples, instead of the full samples, 
are used to determine the best split.  
 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Back. (En) 0.5371 0.5413 0.5873 0.5616 
2 Back. (Ja) 0.5640 0.5684 0.6027 0.5808 
3 Back. (Cn) 0.4966 0.5105 0.5761 0.5393 
4 In-domain  0.5927 0.5824 0.6291 0.6055 
Table 5. Close test results of baseline rankers, on Kokr-1-Test 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Back. (En) 0.4991 0.5242 0.5397 0.5278 
2 Back. (Ja) 0.5052 0.5092 0.5377 0.5194 
3 Back. (Cn) 0.4779 0.4855 0.5114 0.4942 
4 In-domain  0.5164 0.5295 0.5675 0.5430 
Table 6. Open test results of baseline rankers, on Kokr-2-Test 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Interp. (En) 0.5954 0.5893 0.6335 0.6088 
2 Interp. (Ja) 0.6047 0.5898 0.6339 0.6116 
3 Interp. (Cn) 0.5812 0.5807 0.6268 0.6024 
4 4W-Interp. 0.5878 0.5870 0.6289 0.6054 
Table 7. Close test results of interpolated rankers, on 
Kokr-1-Test. 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Interp. (En) 0.5178 0.5369 0.5768 0.5500 
2 Interp. (Ja) 0.5274 0.5416 0.5788 0.5531 
3 Interp. (Cn) 0.5224 0.5339 0.5766 0.5487 
4 4W-Interp.  0.5278 0.5414 0.5823 0.5549 
Table 8. Open test results of interpolated rankers, on 
Kokr-2-Test. 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 ?-Boost (En) 0.5757 0.5716 0.6197 0.5935 
2 ?-Boost (Ja) 0.5801 0.5807 0.6225 0.5982 
3 ?-Boost (Cn)  0.5731 0.5793 0.6226 0.5972 
Table 9. Close test results of ?-Boost rankers, on Kokr-1-Test. 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 ?-Boost (En) 0.4960 0.5203 0.5486 0.5281 
2 ?-Boost (Ja) 0.5090 0.5167 0.5374 0.5233 
3 ?-Boost (Cn)  0.5177 0.5324 0.5673 0.5439 
Table 10. Open test results of ?-Boost rankers, on Kokr-2-Test. 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 ?-SMART 
(En) 
0.6096 0.6057 0.6454 0.6238 
2 ?- SMART 
(Ja) 
0.6014 0.5966 0.6385 0.6172 
3 ?- SMART 
(Cn)  
0.5955 0.6095 0.6415 0.6209 
Table 11. Close test results of ?-SMART rankers, on 
Kokr-1-Test. 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 ?- SMART 
(En) 
0.5177 0.5297 0.5563 0.5391 
2 ?- SMART 
(Ja) 
0.5205 0.5317 0.5522 0.5368 
3 ?- SMART 
(Cn)  
0.5198 0.5305 0.5644 0.5410 
Table 12. Open test results of ?-SMART rankers, on 
Kokr-2-Test. 
511
We first performed the experiments on name 
queries. The results on the closed and open test sets 
are shown in Figures 4 (a) and 4 (b), respectively. 
The results are consistent with our assumptions. 
There are three main observations.  First, the gray 
bars in Figures 4 (a) and 4 (b) (boosting without 
randomization) show that on the closed test set, as  
expected, NDCG increases with the value of L, but 
the correlation does not hold on the open test set.  
Second, the black bars in these figures (boosting 
with randomization) show that in both closed and 
open test sets, NDCG increases with the value of L.  
Finally, comparing the gray bars with their cor-
responding black bars, we see that randomization 
consistently improves NDCG on the open test set, 
with a larger margin of gain for the boosting algo-
rithms with deeper trees (L > 5). 
These results are very encouraging.  Randomi-
zation seems to work like a charm. Unfortunately, 
it does not work well enough to help the boosting 
algorithm beat model interpolation on the open test 
sets.  Notice that all the LambdaSMART results 
reported in Section 5.2 use randomization with the 
same sampling rate  of 0.7.  We repeated the com-
parison in the cross-domain adaptation experi-
ments.  As shown in Figure 4, results in 4 (c) and 4 
(d) are consistent with those on names queries in 4 
(b). Results in 4 (f) show a visible performance drop 
from LambdaBoost to LambdaSMART with L = 2, 
indicating again the instability of trees. 
6 Conclusions and Future Work 
In this paper, we extend two classes of model 
adaptation methods (i.e., model interpolation and 
error-driven learning), which have been well stu-
died in statistical language modeling for speech 
and natural language applications (e.g., Bacchiani 
et al, 2004; Bellegarda, 2004; Gao et al, 2006), to 
ranking models for Web search applications.  
We have evaluated our methods on two adap-
tation experiments over a wide variety of datasets 
where the in-domain datasets bear different levels 
of similarities to their background datasets.  We 
reach different conclusions from the results of the 
open and close tests, respectively. Our open test 
results show that in the cases where the in-domain 
data is dramatically different from the background 
data, model interpolation is very robust and out-
performs the baseline and the error-driven learning 
methods significantly; whereas our close test re-
sults show that in the cases where the in-domain 
data is similar to the background data, the tree- 
based boosting algorithm (i.e. LambdaSMART) is 
the best performer, and achieves a significant im-
provement over the baselines.  We also show that 
these different conclusions are largely due to the 
instability of the use of trees in the boosting algo-
rithm. We thus explore several methods of im-
proving the robustness of the algorithm, such as 
randomization, regularization, using shallow trees, 
with limited success.  Of course, our experiments, 
 (a)  (b)  
  
(c)  (d)  (e)  
Figure 4. AveNDCG results (y-axis) of LambdaSMART with different values of L (x-axis), where L=1 is LambdaBoost; (a) and (b) are 
the results on closed and open tests using Names-1-Train as adaptation data, respectively;  (d),  (e) and (f) are the results on the 
Korean open test set, using background models trained on Web-En, Web-Ja, and Web-Cn data sets, respectively. 
   
0.49
0.50
0.51
0.52
0.53
0.54
0.55
0.56
0.57
1 2 4 10 20
0.53
0.54
0.54
0.55
0.55
1 2 4 10 20
0.50
0.51
0.52
0.53
0.54
0.55
1 2 4 10 20
0.49
0.50
0.51
0.52
0.53
0.54
1 2 4 10 20
0.51
0.52
0.53
0.54
0.55
1 2 4 10 20
512
described in Section 5.3, only scratch the surface of 
what is possible.  Robustness deserves more inves-
tigation and forms one area of our future work. 
Another family of model adaptation methods 
that we have not studied in this paper is transfer 
learning, which has been well-studied in the ma-
chine learning community (e.g., Caruana, 1997; 
Marx et al, 2008).  We leave it to future work. 
To solve the issue of inadequate training data, in 
addition to model adaptation, researchers have 
also been exploring the use of implicit user feed-
back data (extracted from log files) for ranking 
model training (e.g., Joachims et al, 2005; Radlinski 
et al, 2008).  Although such data is very noisy, it is 
of a much larger amount and is cheaper to obtain 
than human-labeled data.  It will be interesting to 
apply the model adaptation methods described in 
this paper to adapt a ranker which is trained on a 
large amount of automatically extracted data to a 
relatively small amount of human-labeled data. 
Acknowledgments 
This work was done while Yi Su was visiting Mi-
crosoft Research, Redmond. We thank Steven Yao's 
group at Microsoft Bing Search for their help with 
the experiments. 
References 
Bacchiani, M., Roark, B. and Saraclar, M. 2004. 
Language model adaptation with MAP estima-
tion and the Perceptron algorithm. In 
HLT-NAACL, 21-24. 
Bellegarda, J. R. 2004. Statistical language model 
adaptation: review and perspectives. Speech 
Communication, 42: 93-108. 
Breiman, L. 2001. Random forests. Machine Learning, 
45, 5-23. 
Bishop, C.M. 1995. Training with noise is equiva-
lent to Tikhonov regularization. Neural Computa-
tion, 7, 108-116. 
Burges, C. J., Ragno, R., & Le, Q. V. 2006. Learning 
to rank with nonsmooth cost functions. In ICML. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In 
ICML. 
Caruana, R. 1997. Multitask learning. Machine 
Learning, 28(1): 41-70. 
Collins, M. 2000. Discriminative reranking for nat-
ural language parsing. In ICML. 
Donmea, P., Svore, K. and Burges. 2008. On the 
local optimality for NDCG. Microsoft Technical 
Report, MSR-TR-2008-179. 
Friedman, J. 1999. Stochastic gradient boosting. 
Technical report, Dept. Statistics, Stanford.  
Friedman, J. 2001. Greedy function approximation: 
a gradient boosting machine. Annals of Statistics, 
29(5). 
Gao, J., Qin, H., Xia, X. and Nie, J-Y. 2005. Linear 
discriminative models for information retrieval. 
In SIGIR. 
Gao, J., Suzuki, H. and Yuan, W. 2006. An empirical 
study on language model adaptation. ACM Trans 
on Asian Language Processing, 5(3):207-227. 
Hastie, T., Tibshirani, R. and Friedman, J. 2001. The 
elements of statistical learning. Springer-Verlag, 
New York. 
Jarvelin, K. and Kekalainen, J. 2000. IR evaluation 
methods for retrieving highly relevant docu-
ments. In SIGIR. 
Joachims, T., Granka, L., Pan, B., Hembrooke, H. 
and Gay, G. 2005. Accurately interpreting click-
through data as implicit feedback. In SIGIR. 
Marx, Z., Rosenstein, M.T., Dietterich, T.G. and 
Kaelbling, L.P. 2008. Two algorithms for transfer 
learning. To appear in Inductive Transfer: 10 years 
later. 
Press, W. H., S. A. Teukolsky, W. T. Vetterling and 
B. P. Flannery. 1992. Numerical Recipes In C.  
Cambridge Univ. Press. 
Radlinski, F., Kurup, M. and Joachims, T. 2008. 
How does clickthrough data reflect retrieval 
quality? In CIKM. 
Thrun, S. 1996. Is learning the n-th thing any easier 
than learning the first. In NIPS. 
Wu, Q., Burges, C.J.C., Svore, K.M. and Gao, J. 
2008. Ranking, boosting, and model adaptation. 
Technical Report MSR-TR-2008-109, Microsoft 
Research. 
 
513
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 55?60,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Animacy Detection with Voting Models
Joshua L. Moore?
Dept. Computer Science
Cornell University
Ithaca, NY 14853
jlmo@cs.cornell.edu
Christopher J.C. Burges Erin Renshaw Wen-tau Yih
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{cburges, erinren, scottyih}
@microsoft.com
Abstract
Animacy detection is a problem whose solu-
tion has been shown to be beneficial for a
number of syntactic and semantic tasks. We
present a state-of-the-art system for this task
which uses a number of simple classifiers
with heterogeneous data sources in a voting
scheme. We show how this framework can
give us direct insight into the behavior of the
system, allowing us to more easily diagnose
sources of error.
1 Introduction
Animacy detection has proven useful for a va-
riety of syntactic and semantic tasks, such as
anaphora and coreference resolution (Ora?san and
Evans, 2007; Lee et al, 2013), verb argument dis-
ambiguation (Dell?Orletta et al, 2005) and depen-
dency parsing (?vrelid and Nivre, 2007). Existing
approaches for animacy detection typically rely on
two types of information: linguistic databases, and
syntactic cues observed from the corpus. They usu-
ally combine two types of approaches: rule based
systems, and machine learning techniques. In this
paper we explore a slightly different angle: we wish
to design an animacy detector whose decisions are
interpretable and correctable, so that downstream
semantic modeling systems can revisit those deci-
sions as needed. Thus here, we avoid defining
a large number of features and then using a ma-
chine learning method such as boosted trees, since
such methods, although powerful, result in hard-to-
interpret systems. Instead, we explore combining
interpretable voting models using machine learning
? Work performed while visiting Microsoft Research.
only to reweight their votes. We show that such
an approach can indeed result in a high perform-
ing system, with animacy detection accuracies in the
mid 90% range, which compares well with other re-
ported rates. Ensemble methods are well known (see
for example, Dietterich (2000)) but our focus here is
on using them for interpretability while still main-
taining accuracy.
2 Previous Work
2.1 Definitions of Animacy
Previous work uses several different definitions of
animacy. Ora?san and Evans (2007) define animacy
in the service of anaphora resolution: an NP is con-
sidered animate ?if its referent can also be referred
to using one of the pronouns he, she, him, her, his,
hers, himself, herself, or a combination of such pro-
nouns (e.g. his/her )?. Although useful for the task
at hand, this has counterintuitive consequences: for
example, baby may be considered animate or inan-
imate, and ant is considered inanimate (Ibid., Fig-
ure 1). Others have argued that animacy should be
captured by a hierarchy or by categories (Aissen,
2003; Silverstein, 1986). For instance, Zaenen et
al. (2004) propose three levels of animacy (human,
other animate and inanimate), which cover ten cat-
egories of noun phrases, with categories like ORG
(organization), ANIM (animal) and MAC (intelli-
gent machines such as robots) categorised as other
animate. Bowman and Chopra (2012) report results
for animacy defined both this way and with the cat-
egories collapsed to a binary (animate, inanimate)
definition.
55
2.2 Methods for Animacy Detection
Evans and Ora?san (2000) propose a rule-based sys-
tem based on the WordNet taxonomy (Fellbaum,
1998). Each synset is ascribed a binary animacy
label based on its unique beginner. A given noun
is then associated with the fraction of its animate
synsets (where all synsets are taken to be animate
or inanimate) and one minus that fraction, similarly
for a given verb. Animacy is then ascribed by ap-
plying a series of rules imposing thresholds on those
fractions, together with rules (and a gazetteer) to de-
tect names and acronyms, and a rule triggered by the
occurrence of who, or reflexives, in the NP. In later
work, Ora?san and Evans (2007) extend the algorithm
by propagating animacy labels in the WordNet graph
using a chi-squared test, and then apply a k-nearest
neighbor classifier based on four lexical features. In
their work, the only context used was the animacy of
the verb in the NP, for heads of subject NPs (e.g., the
subject of eat is typically animate). ?vrelid (2009)
and Bowman and Chopra (2012) extend this idea by
using dependency relations to generate features for
their classifier, enabled by corpora created by Zae-
nen et al (2004). In another approach, Ji and Lin
(2009) apply a simple ?relative-pronoun? pattern to
the Google n-gram corpus (Brants and Franz, 2006)
to assign animacy (see the List model in Section 5
for details). Although the animacy decision is again
context-independent, such a list provides a strong
baseline and thus benefit applications like anaphora
resolution (Lee et al, 2013).
3 The Task
We adopt a definition of animacy closest to the bi-
nary version in Bowman and Chopra (2012): we
define an entity to be animate if it is alive and has
the ability to move under its own will. We adopt
this simple definition because it fits well with the
common meaning and is therefore less error prone,
both in terms of incorporation into higher level mod-
els, and for labeling (Ora?san and Evans (2007) re-
port that the labeling of animacy tuned for anaphora
proved challenging for the judges). We also ap-
ply the label to single noun tokens where possible:
the only exceptions are compound names (?Sarah
Jones?) which are treated as single units. Thus,
for example, ?puppy food? is treated as two words,
with puppy animate and food inanimate. A more
complete definition would extend this to all noun
phrases, so that puppy food as a unit would be inan-
imate, a notion we plan to revisit in future work.
Note that even this simple definition presents chal-
lenges, so that a binary label must be applied de-
pending on the predominant meaning. In ?A plate
of chicken,? chicken is treated as inanimate since it
refers to food. In ?Caruso (1873-1921) is consid-
ered one of the world?s best opera singers. He...,?
although at the time of writing clearly Caruso was
not alive, the token is still treated as animate here
because the subsequent writing refers to a live per-
son.
4 The Data
We used the MC160 dataset, which is a subset of the
MCTest dataset and which is composed of 160 grade
level reading comprehension stories generated using
crowd sourcing (Richardson et al, 2013). Workers
were asked to write a short story (typically less than
300 words) with a target audience of 5 to 7 year
olds. The available vocabulary was limited to ap-
proximately 8000 words, to model the reading abil-
ity of a first or second grader. We labeled this data
for animacy using the definition given above. The
first 100 of the 160 stories were used as the training
set, and the remaining 60 were used for the test set.
These animacy labels will be made available on the
web site for MCTest (Richardson et al, 2013).
5 The Models
Since one of our key goals is interpretability we
chose to use an ensemble of simple voting models.
Each model is able to vote for the categories Ani-
mal, Person, Inanimate, or to abstain. The distinc-
tion between Animal and Person is only used when
we combine votes, where Animal and Person votes
appear as distinct inputs for the final voting combi-
nation model. Some voters do not distinguish be-
tween Person and Animal, and vote for Animate or
Inanimate. Our models are:
List: The n-gram list method from (Ji and Lin,
2009). Here, the frequencies with which the rela-
tive pronouns who, where, when, and which occur
are considered. Any noun followed most frequently
by who is classified as Animate, and any other noun
56
in the list is classified as Inanimate. This voter ab-
stains when the noun is not present in the list.
Anaphora Design: The WordNet-based approach
of Evans and Ora?san (2000).
WordNet: A simple approach using WordNet.
This voter chooses Animal or Person if the unique
beginner of the first synset of the noun is either of
these, and Inanimate otherwise.
WordSim: This voter uses the contextual vector
space model of Yih and Qazvinian (2012) computed
using Wikipedia and LA Times data. It uses short
lists of hand-chosen signal words for the categories
Animal, Person, and Inanimate to produce a ?re-
sponse? of the word to each category. This response
is equal to the maximum cosine similarity in the vec-
tor space of the query word to any signal word in the
category. The final vote goes to the category with
the highest response.
Name: We used an in-house named entity tagger.
This voter can recognize some inanimate entities
such as cities, but does not distinguish between peo-
ple and animals, and so can only vote Animate, Inan-
imate or Abstain.
Dictionaries: We use three different dictionary
sources (Simple English Wiktionary, Full English
Wiktionary, and the definitions found in Word-
Net) with a recursive dictionary crawling algorithm.
First, we fetch the first definition of the query and
use a dependency tree and simple heuristics to find
the head noun of the definition, ignoring qualifica-
tion NPs like ?piece? or ?member.? If this noun
belongs to a list of per-category signal words, the
voter stops and votes for that category. Otherwise,
the voter recursively runs on the found head noun.
To prevent cycling, if no prediction is made after 10
recursive lookups, the voter abstains.
Transfer: For each story, we first process each
sentence and detect instances of the patterns x
am/is/was/are/were y and y named x. In each of
these cases, we use majority vote of the remaining
voters to predict the animacy of y and transfer
its vote to x, applying this label (as a vote) to all
instances of x in the text.
The WordSim and Dictionaries voters share lists
of signal words, which were chosen early in the ex-
perimental process using the training set. The sig-
nal words for the Animal category were animal and
mammal1. Person contains person and people. Fi-
nally, Inanimate uses thing, object, space, place,
symbol, food, structure, sound, measure, and unit.
We considered two methods for combining vot-
ers: majority voting (where the reliable Name voter
overrides the others if it does not abstain) and a lin-
ear reweighting of votes. In the reweighting method,
a feature vector is formed from the votes. Except
for WordSim, this vector is an indicator vector of
the vote ? either Animal, Person, Animate (if the
voter doesn?t distinguish between animals and peo-
ple), Inanimate, or Abstain.
For Dictionaries, the vector?s non-zero compo-
nent is multiplied by the number of remaining al-
lowed recursive calls that can be performed, plus one
(so that a success on the final lookup gives a 1). For
example, if the third lookup finds a signal word and
chooses Animal, then the component corresponding
to Animal will have a value of 9.
For WordSim, instead of an indicator vector, the
responses to each category are used, or an indica-
tor for abstain if the model does not contain the
word. If the word is in the model, a second vec-
tor is appended containing the ratio of the maximum
response to the second-largest response in the com-
ponent for the maximum response category. These
per-voter feature vectors are concatenated to form a
35 dimensional vector, and a linear SVM is trained
to obtain the weights for combining the votes.
6 Results
We used the POS tagger in MSR SPLAT (Quirk et
al., 2012) to extract nouns from the stories in the
MC160 dataset and used these as labeled examples
for the SVM. This resulted in 5,120 extracted nouns
in the 100 training stories and 3,009 in the 60 test
stories. We use five-fold cross-validation on the
training set to select the SVM parameters. 57.2%
of the training examples were inanimate, as were
58.1% of the test examples.
Table 1 gives the test accuracy of each voter. List
1This was found to work well given typical dictionary defi-
nitions despite the fact that people are also mammals.
57
List Anaphora WNet WSim Dict Name
84.6 77.1 78.8 57.6 74.3 16.0
Table 1: Accuracy of various individual voters on the test
set. Abstentions are counted as errors. Note that Transfer
depends on a secondary source for classification, and is
therefore not listed here.
Majority SVM
N+WN+D+WS+AD+L 87.7 95.0
N+WN+WS 80.1 95.0
N+WN+D+WS+AD+L+T 87.4 95.0
N+WN+D+WS 86.4 94.8
N+WN+WS+AD+L 86.5 94.7
N+WN+D+WS+T 86.8 94.0
N+WN+D 86.1 93.7
N+WN 89.3 93.0
N+D 82.6 93.0
N+AD 87.6 89.4
N+L 85.4 88.9
Table 2: Accuracy of various combinations of voters
among Name (N), Anaphora Design (AD), List (L),
WordNet (WN), WordSim (WS), Dictionary (D), and
Transfer (T) under majority voting and SVM schemes.
Bold indicates a statistically significant difference over
the next lower bolded entry with p < 0.01, for the SVM.
comes out on top when taken alone, but we see in
later results that it is less critical when used with
other voters. Name performs poorly on its own, but
later we will see that it is a very accurate voter which
frequently abstains.
Table 2 gives the test performance of various com-
binations of voters, both under majority vote and
reweighting. Statistical significance was tested us-
ing a paired t-test, and bold indicates a method
was significant over the next lower bold line with
p value p < 0.01. We see a very large gain from
the SVM reweighting: 14.9 points in the case of
Name+WordNet+WordSim.
In Table 3, we show the results of ablation exper-
iments on the voters. We see that the most valuable
sources of information are WordSim and Dictionar-
ies.
Finally, in Table 4, we show a breakdown of
which voters cause the most errors, for the majority
vote system. In this table, we considered only ?fi-
nal errors,? i.e. errors that the entire system makes.
Over all such errors, we counted the number of times
Majority SVM
WordSim 87.6 93.7
SimpleWikt (dict) 87.3 94.1
FullWikt (dict) 86.4 94.3
Dict 87.4 94.5
Name 86.6 94.7
List 86.4 94.8
WordNet (dict) 88.7 94.8
WordNet 87.5 94.9
Anaphora Design 88.6 94.9
Transfer 87.7 95.0
Table 3: Test accuracy when leaving out various voters,
using both majority vote and and reweighting. Bold indi-
cates statistical significance over the next lower bold line
with p < 0.01.
each voter chose incorrectly, giving a count of how
many times each voter contributed to a final error.
We see that the Anaphora Design system has the
largest number of errors on both train and test sets.
After this, WordNet, List, and WordNet (dict) are also
large sources of error. On the other hand, Name and
WordSim have very few errors, indicating high re-
liability. The table also gives the number of criti-
cal errors, where the voter selected the wrong cate-
gory and was a deciding vote (that is, when chang-
ing its vote would have resulted in a correct overall
classification). We see a similar pattern here, with
Anaphora Design causing the most errors and Word-
Sim and Name among the most reliable. We included
Anaphora Design even though it uses a different def-
inition of animacy, to determine if its vote was nev-
ertheless valuable.
Error tables such as these show how voting mod-
els are more interpretable and therefore correctable
compared to more complex learned models. The ta-
bles indicate the largest sources of error and sug-
gest changes that could be made to increase accu-
racy. For example, we could make significant gains
by improving WordNet, WordNet (dictionary), or
List, whereas there is relatively little reason to ad-
just WordSim or Name.
7 Conclusions
We have shown that linear combinations of voting
models can give animacy detection rates in the mid
90% range. This is well above the accuracy found
58
Errors Critical
Train Test Train Test
Anaphora Design 555 266 117 76
WordNet 480 228 50 45
List 435 195 94 45
Transfer 410 237 54 58
WordNet (dict) 385 194 84 65
SimpleWikt (dict) 175 111 39 16
FullWikt (dict) 158 67 1 5
WordSim 107 89 11 19
Name 71 55 27 19
Table 4: Errors column: number of errors on train and
test where a source voted incorrectly, and was thus at
least in part responsible for an error of the overall sys-
tem. Critical column: number of errors on train and test
where a source voted incorrectly, and in addition cast a
deciding vote. Results are for majority vote.
by using the n-gram method of (Ji and Lin, 2009),
which is used as an animacy detection component
in other systems. In this sense the work presented
here improves upon the state of the art, but there are
caveats, since other workers define animacy differ-
ently and so a direct comparison with their work is
not possible. Our method has the added advantage
of interpretability, which we believe will be useful
when using it as a component in a larger system.
Acknowledgments
We wish to thank Andrzej Pastusiak for his help with
the labeling tool.
References
Judith Aissen. 2003. Differential object marking:
Iconicity vs. economy. Natural Language & Linguis-
tic Theory, 21(3):435?483.
Samuel Bowman and Harshit Chopra. 2012. Automatic
animacy classification. In Proceedings of the NAACL-
HLT 2012 Student Research Workshop.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
Felice Dell?Orletta, Alessandro Lenci, Simonetta Monte-
magni, and Vito Pirrelli. 2005. Climbing the path to
grammar: a maximum entropy model of subject/object
learning. In Proceedings of the Workshop on Psy-
chocomputational Models of Human Language Acqui-
sition, PMHLA ?05, pages 72?81, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thomas G. Dietterich. 2000. Ensemble methods in ma-
chine learning. In Multiple Classifier Systems, pages
1?15.
Richard Evans and Constantin Ora?san. 2000. Improv-
ing anaphora resolution by identifying animate entities
in texts. In Proceedings of the Discourse Anaphora
and Reference Resolution Conference (DAARC2000),
pages 154?162.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Heng Ji and Dekang Lin. 2009. Gender and animacy
knowledge discovery from Web-scale n-grams for un-
supervised person mention detection. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation, pages 220?229, Hong
Kong, December. City University of Hong Kong.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4).
Constantin Ora?san and Richard J. Evans. 2007. NP an-
imacy identification for anaphora resolution. Journal
of Artificial Intelligence Research (JAIR), 29:79?103.
Lilja ?vrelid and Joakim Nivre. 2007. When word or-
der and part-of-speech tags are not enough ? Swedish
dependency parsing with rich linguistic features. In
Proceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP),
pages 447?451.
Lilja ?vrelid. 2009. Empirical evaluations of animacy
annotation. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, Colin Cherry, and Lucy Vanderwende. 2012.
MSR SPLAT, a language analysis toolkit. In Proceed-
ings of the Demonstration Session at the Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 21?24, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Matthew Richardson, Chris Burges, and Erin Renshaw.
2013. MCTest: A challenge dataset for the open-
domain machine comprehension of text. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Michael Silverstein. 1986. Hierarchy of features and
ergativity. In P. Muysken and H. van Riemsdijk, ed-
itors, Features and Projections, pages 163?232. Foris
Publications Holland.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
59
models. In Proceedings of NAACL-HLT, pages 616?
620, Montre?al, Canada, June.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M. Catherine O?Connor, and Tom Wasow. 2004. An-
imacy encoding in English: Why and how. In Bon-
nie Webber and Donna K. Byron, editors, ACL 2004
Workshop on Discourse Annotation, pages 118?125,
Barcelona, Spain, July. Association for Computational
Linguistics.
60
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 193?203,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
MCTest: A Challenge Dataset for the Open-Domain  
Machine Comprehension of Text 
 
Matthew Richardson 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
mattri@microsoft.com 
Christopher J.C. Burges 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
cburges@microsoft.com 
Erin Renshaw 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
erinren@microsoft.com 
 
 
 
 
Abstract 
We present MCTest, a freely available set of 
stories and associated questions intended for 
research on the machine comprehension of 
text. Previous work on machine comprehen-
sion (e.g., semantic modeling) has made great 
strides, but primarily focuses either on lim-
ited-domain datasets, or on solving a more re-
stricted goal (e.g., open-domain relation 
extraction). In contrast, MCTest requires ma-
chines to answer multiple-choice reading 
comprehension questions about fictional sto-
ries, directly tackling the high-level goal of 
open-domain machine comprehension. Read-
ing comprehension can test advanced abilities 
such as causal reasoning and understanding 
the world, yet, by being multiple-choice, still 
provide a clear metric. By being fictional, the 
answer typically can be found only in the sto-
ry itself. The stories and questions are also 
carefully limited to those a young child would 
understand, reducing the world knowledge 
that is required for the task. We present the 
scalable crowd-sourcing methods that allow 
us to cheaply construct a dataset of 500 stories 
and 2000 questions. By screening workers 
(with grammar tests) and stories (with grad-
ing), we have ensured that the data is the same 
quality as another set that we manually edited, 
but at one tenth the editing cost. By being 
open-domain, yet carefully restricted, we hope 
MCTest will serve to encourage research and 
provide a clear metric for advancement on the 
machine comprehension of text. 
1 Reading Comprehension 
A major goal for NLP is for machines to be able to 
understand text as well as people. Several research 
disciplines are focused on this problem: for exam-
ple, information extraction, relation extraction, 
semantic role labeling, and recognizing textual en-
tailment. Yet these techniques are necessarily 
evaluated individually, rather than by how much 
they advance us towards the end goal. On the other 
hand, the goal of semantic parsing is the machine 
comprehension of text (MCT), yet its evaluation 
requires adherence to a specific knowledge repre-
sentation, and it is currently unclear what the best 
representation is, for open-domain text. 
We believe that it is useful to directly tackle the 
top-level task of MCT. For this, we need a way to 
measure progress. One common method for evalu-
ating someone?s understanding of text is by giving 
them a multiple-choice reading comprehension 
test. This has the advantage that it is objectively 
gradable (vs. essays) yet may test a range of abili-
ties such as causal or counterfactual reasoning, 
inference among relations, or just basic under-
standing of the world in which the passage is set. 
Therefore, we propose a multiple-choice reading 
comprehension task as a way to evaluate progress 
on MCT. We have built a reading comprehension 
dataset containing 500 fictional stories, with 4 mul-
tiple choice questions per story. It was built using 
methods which can easily scale to at least 5000 
stories, since the stories were created, and the cura-
tion was done, using crowd sourcing almost entire-
ly, at a total of $4.00 per story. We plan to perio-
dically update the dataset to ensure that methods 
are not overfitting to the existing data. The dataset 
is open-domain, yet restricted to concepts and 
words that a 7 year old is expected to understand. 
This task is still beyond the capability of today?s 
computers and algorithms. 
193
By restricting the concept space, we gain the dif-
ficulty of being an open-domain problem, without 
the full complexity of the real world (for example, 
there will be no need for the machine to understand 
politics, technology, or to have any domain specif-
ic expertise). The multiple choice task avoids am-
biguities (such as when the task is to find a 
sentence that best matches a question, as in some 
early reading comprehension tasks: see Section 2), 
and also avoids the need for additional grading, 
such as is needed in some TREC tasks. The stories 
were chosen to be fictional to focus work on find-
ing the answer in the story itself, rather than in 
knowledge repositories such as Wikipedia; the goal 
is to build technology that actually understands 
stories and paragraphs on a deep level (as opposed 
to using information retrieval methods and the re-
dundancy of the web to find the answers). 
We chose to use crowd sourcing, as opposed to, 
for example, contracting teachers or paying for 
existing standardized tests, for three reasons, 
namely: (1) scalability, both for the sizes of da-
tasets we can provide, and also for the ease of reg-
ularly refreshing the data; (2) for the variety in 
story-telling that having many different authors 
brings; and (3) for the free availability that can on-
ly result from providing non-copyrighted data. The 
content is freely available at http://research.micro-
soft.com/mct, and we plan to use that site to track 
published results and provide other resources, such 
as labels of various kinds. 
2 Previous Work  
The research goal of mapping text to meaning rep-
resentations in order to solve particular tasks has a 
long history. DARPA introduced the Airline Trav-
el Information System (ATIS) in the early 90?s: 
there the task was to slot-fill flight-related infor-
mation by modeling the intent of spoken language 
(see Tur et al, 2010, for a review). This data con-
tinues to be a used in the semantic modeling com-
munity (see, for example, Zettlemoyer and Collins, 
2009). The Geoquery database contains 880 geo-
graphical facts about the US and has played a simi-
lar role for written (as opposed to spoken) natural 
language queries against a database (Zelle and 
Mooney, 1996) and it also continues to spur re-
search (see for example Goldwasser et al, 2011), 
as does the similar Jobs database, which provides 
mappings of 640 sentences to a listing of jobs 
(Tang and Mooney, 2001). More recently, Zweig 
and Burges (2012) provided a set of 1040 sentenc-
es that comprise an SAT-style multiple choice sen-
tence completion task.  
The idea of using story-based reading compre-
hension questions to evaluate methods for machine 
reading itself goes back over a decade, when 
Hirschmann et al (1999) showed that a bag of 
words approach, together with some heuristic lin-
guistic modeling, could achieve 40% accuracy for 
the task of picking the sentence that best matches 
the query for ?who / what / when / where / why? 
questions, on a small reading comprehension da-
taset from Remedia. This dataset spurred several 
research efforts, for example using reinforcement 
learning (Grois and Wilkins, 2005), named entity 
resolution (Harabagiu et al, 2003) and mapping 
questions and answers to logical form (Wellner et 
al., 2006). Work on story understanding itself goes 
back much further, to 1972, when Charniak pro-
posed using a background model to answer ques-
tions about children?s stories. Similarly, the TREC 
(and TAC) Question Answering tracks (e.g., Voor-
hees and Tice, 1999) aim to evaluate systems on 
their ability to answer factual questions such as 
?Where is the Taj Mahal?. The QA4MRE task also 
aims to evaluate machine reading systems through 
question answering (e.g., Clark et al, 2012). Earli-
er work has also aimed at controlling the scope by 
limiting the text to children?s stories: Breck et al 
(2001) collected 75 stories from the Canadian 
Broadcasting Corporation?s web site for children, 
and generated 650 questions for them manually, 
where each question was answered by a sentence 
in the text. Leidner et al (2003) both enriched the 
CBC4kids data by adding several layers of annota-
tion (such as semantic and POS tags), and meas-
ured QA performance as a function of question 
difficulty. For a further compendium of resources 
related to the story comprehension task, see  
Mueller (2010). 
The task proposed here differs from the above 
work in several ways. Most importantly, the data 
collection is scalable: if the dataset proves suffi-
ciently useful to others, it would be straightforward 
to gather an order of magnitude more. Even the 
dataset size presented here is an order of magni-
tude larger than the Remedia or the CBC4kids data 
and many times larger than QA4MRE. Second, the 
multiple choice task presents less ambiguity (and is 
consequently easier to collect data for) than the 
194
task of finding the most appropriate sentence, and 
may be automatically evaluated. Further, our sto-
ries are fictional, which means that the information 
to answer the question is contained only in the sto-
ry itself (as opposed to being able to directly lever-
age knowledge repositories such as Wikipedia). 
This design was chosen to focus the task on the 
machine understanding of short passages, rather 
than the ability to match against an existing 
knowledge base. In addition, while in the 
CBC4kids data each answer was a sentence from 
the story, here we required that approximately half 
of the questions require at least two sentences from 
the text to answer; being able to control complexity 
in this way is a further benefit of using multiple 
choice answers. Finally, as explained in Section 1, 
the use of free-form input makes the problem open 
domain (as opposed to the ATIS, Geoquery and 
Jobs data), leading to the hope that solutions to the 
task presented here will be easier to apply to novel, 
unrelated tasks. 
3 Generating the Stories and Questions 
Our aim was to generate a corpus of fictional story 
sets1 that could be scaled with as little expert input 
as possible. Thus, we designed the process to be 
gated by cost, and keeping the costs low was a 
high priority. Crowd-sourcing seemed particularly 
appropriate, given the nature of the task, so we 
opted to use Amazon Mechanical Turk2 (AMT). 
With over 500,000 workers3, it provides the work 
force required to both achieve scalability and, 
equally importantly, to provide diversity in the sto-
ries and types of questions. We restricted our task 
to AMT workers (workers) residing in the United 
States. The average worker is 36 years old, more 
educated than the United States population in gen-
eral (Paolacci et al, 2010), and the majority of 
workers are female. 
3.1 The Story and Questions 
Workers were instructed to write a short (150-300 
words) fictional story, and to write as if for a child 
in grade school. The choice of 150-300 was made 
to keep the task an appropriate size for workers 
while still allowing for complex stories and ques-
tions. The workers were free to write about any 
topic they desired (as long as it was appropriate for 
a young child), and so there is a wide range, in-
cluding vacations, animals, school, cars, eating, 
gardening, fairy tales, spaceships, and cowboys. 
                                                     
1 We use the term ?story set? to denote the fictional story 
together with its multiple choice questions, hypothetical an-
swers, and correct answer labels. 
2 http://www.mturk.com 
3 https://requester.mturk.com/tour 
James the Turtle was always getting in trouble. 
Sometimes he'd reach into the freezer and empty out 
all the food. Other times he'd sled on the deck and get 
a splinter. His aunt Jane tried as hard as she could to 
keep him out of trouble, but he was sneaky and got 
into lots of trouble behind her back. 
One day, James thought he would go into town and 
see what kind of trouble he could get into. He went to 
the grocery store and pulled all the pudding off the 
shelves and ate two jars. Then he walked to the fast 
food restaurant and ordered 15 bags of fries. He did-
n't pay, and instead headed home. 
His aunt was waiting for him in his room. She told 
James that she loved him, but he would have to start 
acting like a well-behaved turtle. 
After about a month, and after getting into lots of 
trouble, James finally made up his mind to be a better 
turtle. 
 
1) What is the name of the trouble making turtle? 
A) Fries 
B) Pudding 
C) James 
D) Jane 
 
2) What did James pull off of the shelves in the gro-
cery store? 
A) pudding 
B) fries 
C) food 
D) splinters 
 
3) Where did James go after he went to the grocery 
store? 
A) his deck 
B) his freezer 
C) a fast food restaurant 
D) his room 
 
4) What did James do after he ordered the fries? 
A) went to the grocery store 
B) went home without paying 
C) ate them 
D) made up his mind to be a better turtle 
 
Figure 1. Sample Story and Questions (chosen random-
ly from MC500 train set). 
195
Workers were also asked to provide four reading 
comprehension questions pertaining to their story 
and, for each, four multiple-choice answers. Com-
ing up with incorrect alternatives (distractors) is a 
difficult task (see, e.g., Agarwal, 2011) but work-
ers were requested to provide ?reasonable? incor-
rect answers that at least include words from the 
story so that their solution is not trivial. For exam-
ple, for the question ?What is the name of the 
dog??, if only one of the four answers occurs in the 
story, then that answer must be the correct one.  
Finally, workers were asked to design their 
questions and answers such that at least two of the 
four questions required multiple sentences from the 
story to answer them. That is, for those questions it 
should not be possible to find the answer in any 
individual sentence. The motivation for this was to 
ensure that the task could not be fully solved using 
lexical techniques, such as word matching, alone. 
Whilst it is still possible that a sophisticated lexical 
analysis could completely solve the task, requiring 
that answers be constructed from at least two dif-
ferent sentences in the story makes this much less 
likely; our hope is that the solution will instead 
require some inference and some form of limited 
reasoning. This hope rests in part upon the obser-
vation that standardized reading comprehension 
tests, whose goal after all is to test comprehension, 
generally avoid questions that can be answered by 
reading a single sentence. 
3.2 Automatic Validation 
Besides verifying that the story and all of the ques-
tions and answers were provided, we performed 
the following automatic validation before allowing 
the worker to complete the task: 
Limited vocabulary: The lowercase words in the 
story, questions, and answers were stemmed and 
checked against a vocabulary list of approximately 
8000 words that a 7-year old is likely to know 
(Kuperman et al, 2012). Any words not on the list 
were highlighted in red as the worker typed, and 
the task could not be submitted unless all of the 
words satisfied this vocabulary criterion. To allow 
the use of arbitrary proper nouns, capitalized words 
were not checked against the vocabulary list. 
Multiple-sentence questions: As described earli-
er, we required that at least two of the questions 
need multiple sentences to answer. Workers were 
simply asked to mark whether a question needs one 
or multiple sentences and we required that at least 
two are marked as multiple.  
3.3 The Workers 
Workers were required to reside in the United 
States and to have completed 100 HITs with an 
over 95% approval rate4. The median worker took 
22 minutes to complete the task. We paid workers 
$2.50 per story set and allowed each to do a maxi-
mum of 8 tasks (5 in MC500). We did not experi-
ment with paying less, but this rate amounts to 
$6.82/hour, which is approximately the rate paid 
by other writing tasks on AMT at the time, though 
is also significantly higher than the median wage 
of $1.38 found in 2010 (Horton and Chilton, 
2010). Workers could optionally leave feedback on 
the task, which was overwhelmingly positive ? the 
most frequent non-stopword in the comments was 
?fun? and the most frequent phrase was ?thank 
you?. The only negative comments (in <1% of 
submissions) were when the worker felt that a par-
ticular word should have been on the allowed vo-
cabulary list. Given the positive feedback, it may 
be possible to pay less if we collect more data in 
the future. We did not enforce story length con-
straints, but some workers interpreted our sugges-
tion that the story be 150-300 words as a hard 
constraint, and some asked to be able to write a 
longer story.  
The MCTest corpus contains two sets of stories, 
named MC160 and MC500, and containing 160 
and 500 stories respectively. MC160 was gathered 
first, then some improvements were made before 
gathering MC500. We give details on the differ-
ences between these two sets below. 
3.4 MC160: Manually Curated for Quality 
In addition to the details described above, MC160 
workers were given a target elementary grade 
school level (1-4) and a sample story matching that 
grade level5. The intent was to produce a set of 
stories and questions that varied in difficulty so 
that research work can progress grade-by-grade if 
needed. However, we found little difference be-
tween grades in the corpus.. 
After gathering the stories, we manually curated 
the MC160 corpus by reading each story set and 
                                                     
4 The latter two are the default AMT requirements. 
5 From http://www.englishforeveryone.org/. 
 
196
correcting errors. The most common mistakes were 
grammatical, though occasionally questions and/or 
answers needed to be fixed. 66% of the stories 
have at least one correction. We provide both the 
curated and original corpuses in order to allow re-
search on reading comprehension in the presence 
of grammar, spelling, and other mistakes. 
3.5 MC500: Adding a Grammar Test 
Though the construction of MC160 was successful, 
it requires a costly curation process which will not 
scale to larger data sets (although the curation was 
useful, both for improving the design of MC500, 
and for assessing the effectiveness of automated 
curation techniques). To more fully automate the 
process, we added two more stages: (1) A grammar 
test that automatically pre-screens workers for 
writing ability, and (2) a second Mechanical Turk 
task whereby new workers take the reading com-
prehension tests and rate their quality. We will dis-
cuss stage (2) in the next section. 
The grammar test consisted of 20 sentences, half 
of which had one grammatical error (see Figure 2). 
The incorrect sentences were written using com-
mon errors such as you?re vs. your, using ?s to in-
dicate plurality, incorrect use of tense, it?s vs. its, 
less vs. fewer, I vs. me, etc. Workers were required 
to indicate for each sentence whether it was 
grammatically correct or not, and had to pass with 
at least 80% accuracy in order to qualify for the 
task. The 80% threshold was chosen to trade off 
worker quality with the rate at which the tasks 
would be completed; initial experiments using a 
threshold of 90% indicated that collecting 500 sto-
ries would take many weeks instead of days. Note 
that each worker is allowed to write at most 5 
stores, so we required at least 100 workers to pass 
the qualification test. 
To validate the use of the qualification test, we 
gathered 30 stories requiring the test (qual) and 30 
stories without. We selected a random set of 20 
stories (10 from each), hid their origin, and then 
graded the overall quality of the story and ques-
tions from 1-5, meaning do not attempt to fix, bad 
but rescuable, has non-minor problems, has only 
minor problems, and has no problems, respective-
ly. Results are shown in Table 1. The difference is 
statistically significant (p<0.05, using the two-
tailed t-test). The qual stories were also more di-
verse, with fewer of them about animals (the most 
common topic). 
Additional Modifications: Based on our experi-
ence curating MC160, we also made the following 
modifications to the task. In order to eliminate triv-
ially-answerable questions, we required that each 
answer be unique, and that either the correct an-
swer did not appear in the story or, if it did appear, 
that at least two of the incorrect answers also ap-
peared in the story. This is to prevent questions 
that are trivially answered by checking which an-
swer appears in the story. The condition on wheth-
er the correct answer appears is to allow questions 
such as ?How many candies did Susan eat??, 
where the total may never appear in the story, even 
though the information needed to derive it does. 
An answer is considered to appear in the story if at 
least half (rounded down) of its non-stopword 
1. We went to visit the Smith?s at their house. 
2. I altered their suits for them. 
3. You're car is very old. 
4. Jim likes to run, hike, and going kayaking. 
5. He should of come to work on time. 
6. I think its best to wash lots of apples. 
7. Are people who write "ping" thinking of subma-
rines? 
8. Smoke filled the room, making it hard to breathe. 
9. Alert yet alof - that's you. 
10. They wanted they're money back. 
11. Hawks and eagles like to fly high in the sky. 
12. Don't let her wear them down. 
13. The cat particularly liked the greasy plate. 
14. The company is less successful because we have 
less employees. 
15. The hamster belongs to Sam and I. 
16. No one landed on the air strip today. 
17. He was very effected by her tears. 
18. You are a tired piece of toast, metaphorically 
speaking. 
19. Anne plays bass and sings. 
20. Him and me met at the park. 
Figure 2. Grammar test for qualifying workers. 
 Quality 
(1-5) 
About 
animals 
No Grammar Test 3.2 73% 
Grammar Test 4.3  30% 
Table 1. Pre-screening workers using a grammar test 
improves both quality and diversity of stories. Both 
differences are significant using the two-tailed t-test 
(p<0.05 for quality and p<0.01 for animals).  
 
 
197
terms appear in the story (ignoring word endings). 
This check is done automatically and must be satis-
fied before the worker is able to complete the task. 
Workers could also bypass the check if they felt it 
was incorrect, by adding a special term to their 
answer. 
We were also concerned that the sample story 
might bias the workers when writing the story set, 
particularly when designing questions that require 
multiple sentences to answer. So, we removed the 
sample story and grade level from the task. 
Finally, in order to encourage more diversity of 
stories, we added creativity terms, a set of 15 
nouns chosen at random from the allowed vocabu-
lary set. Workers were asked to ?please consider? 
using one or more of the terms in their story, but 
use of the words was strictly optional. On average, 
workers used 3.9 of the creativity terms in their 
stories.  
4 Rating the Stories and Questions 
In this section we discuss the crowd-sourced rating 
of story sets. We wished to ensure story set quality 
despite the fact that MC500 was only minimally 
manually curated (see below). Pre-qualifying 
workers with a grammar test was one step of this 
process. The second step was to have additional 
workers on Mechanical Turk both evaluate each 
story and take its corresponding test. Each story 
was evaluated in this way by 10 workers, each of 
whom provided scores for each of age-
appropriateness (yes/maybe/no), grammaticality 
(few/some/many errors), and story clarity (excel-
lent/reasonable/poor). When answering the four 
reading comprehension questions, workers could 
also mark a question as ?unclear?. Each story set 
was rated by 10 workers who were each paid $0.15 
per set. 
Since we know the purportedly correct answer, 
we can estimate worker quality by measuring what 
fraction of questions that worker got right. Work-
ers with less than 80% accuracy (ignoring those 
questions marked as unclear) were removed from 
the set. This constituted just 4.1% of the raters and 
4.2% of the judgments (see Figure 3). Only one 
rater appeared to be an intentional spammer, an-
swering 1056 questions with only 29% accuracy. 
The others primarily judged only one story. Only 
one worker fell between, answering 336 questions 
with just 75% accuracy. 
For the remaining workers (those who achieved 
at least 80% accuracy), we measured median story 
appropriateness, grammar, and clarity. For each 
category, stories for which less than half of the 
ratings were the best possible (e.g., excellent story 
clarity) were inspected and optionally removed 
from the data set. This required inspecting 40 
(<10%) of the stories, only 2 of which were 
deemed poor enough to be removed (both of which 
had over half of the ratings all the way at the bot-
tom end of the scale, indicating we could potential-
ly have inspected many fewer stories with the same 
results). We also inspected questions for which at 
least 5 workers answered incorrectly, or answered 
?unclear?. In total, 29 questions (<2%) were in-
spected. 5 were fixed by changing the question, 8 
by changing the answers, 2 by changing both, 6 by 
changing the story, and 8 were left unmodified. 
Note that while not fully automated, this process 
of inspecting stories and repairing questions took 
one person one day, so is still scalable to at least an 
order of magnitude more stories. 
5 Dataset Analysis 
In Table 2, we present results demonstrating the 
value of the grammar test and curation process. As 
expected, manually curating MC160 resulted in 
increased grammar quality and percent of ques-
tions answered correctly by raters. The goal of 
MC500 was to find a more scalable method to 
achieve the same quality as the curated MC160. As 
Table 2 shows, the grammar test improved story 
grammar quality from 1.70 to 1.77 (both uncurat-
ed). The rating and one-day curation process in-
 
Figure 3. Just 4.1% of raters had an accuracy below 
80% (constituting 4.2% of the judgments).   
 
198
Set AgeAp Clarity Grammar Correct 
160  1.88 1.63 1.70 95.3 
500 1.92 1.65 1.77 95.3 
500 curated 1.94 1.71 1.79 96.9 
160 curated 1.91 1.67 1.84
?
 97.7 
Table 2. Average age appropriateness, story clarity, 
grammar quality (0-2, with 2 being best), and percent of 
questions answered correctly by raters, for the original 
and curated versions of the data. Bold indicates statisti-
cal significance vs. the original version of the same set, 
using the two-sample t-test with unequal variance. The ? 
indicates the only statistical difference between 500 
curated and 160 curated. 
Baseline Algorithms 
Require: Passage P, set of passage words PW, ith word in 
passage Pi, set of words in question Q, set of words in 
hypothesized answers A1..4, and set of stop words U,  
Define:  ( )  ?  (    )    
Define:   ( )     (  
 
 ( )
). 
 
Algorithm 1 Sliding Window 
for i = 1 to 4 do 
        
 
       
     | |
? {
  (    )          
          
     | |
 
end for 
return        
 
Algorithm 2 Distance Based 
for i = 1 to 4 do 
    (    )    
     ((     )   )    
 if |  |    or |   |    
       
 else 
    
 
| |  
    
         
  (   ),  
where   (   ) is the minimum number of 
words between an occurrence of q and an 
occurrence of a in P, plus one. 
 end if 
end for 
return       
 
Algorithm SW 
Return               
 
Algorithm SW+D 
Return                     
 
Figure 4. The two lexical-based algorithms used for the 
baselines.   
 
creases this to 1.79, whereas a fully manual cura-
tion results in a score of 1.84. Curation also im-
proved the percent of questions answered correctly 
for both MC160 and MC500, but, unlike with 
grammar, there is no significant difference be-
tween the two curated sets. Indeed, the only statis-
tically significant difference between the two is in 
grammar. So, the MC500 grammar test and cura-
tion process is a very scalable method for collect-
ing stories of nearly the quality of the costly 
manual curation of MC160.  
We also computed correlations between these 
measures of quality and various factors such as 
story length and time spent writing the story. On 
MC500, there is a mild correlation between a 
worker?s grammar test score and the judged 
grammar quality of that worker?s story (correlation 
of 0.24). Interestingly, this relation disappeared 
once MC500 was curated, likely due to repairing 
the stories with the worst grammar. On MC160, 
there is a mild correlation between the clarity and 
the number of words in the question and answer 
(0.20 and 0.18). All other correlations were below 
0.15. These factors could be integrated into an es-
timate for age-appropriateness, clarity, and gram-
mar, potentially reducing the need for raters. 
Table 3 provides statistics on each corpus. 
MC160 and MC500 are similar in average number 
of words per story, question, and answer, as well as 
the median writing time. The most commonly used 
nouns in MC500 are: day, friend, time, home, 
house, mother, dog, mom, school, dad, cat, tree, 
and boy. The stories vary widely in theme. The 
first 10 stories of the randomly-ordered MC500 set 
are about: travelling to Miami to visit friends, wak-
ing up and saying hello to pets, a bully on a 
schoolyard, visiting a farm, collecting insects at 
Grandpa?s house, planning a friend?s birthday par-
ty, selecting clothes for a school dance, keeping 
animals from eating your ice cream, animals order-
ing food, and adventures of a boy and his dog. 
Corpus Stories Median 
writing 
time 
Average Words Per: 
Story Question Answer 
MC160 160 26 min 204 8.0 3.4 
MC500 500 20 min 212 7.7 3.4 
Table 3. Corpus statistics for MC160 and MC500.  
 
199
We randomly divided MC160 and MC500 into 
train, development, and test sets of 70, 30, and 60 
stories and 300, 50, and 150 stories, respectively. 
6 Baseline System and Results 
We wrote two baseline systems, both using only 
simple lexical features. The first system used a 
sliding window, matching a bag of words con-
structed from the question and hypothesized an-
swer to the text. Since this ignored long range 
dependencies, we added a second, word-distance 
based algorithm. The distance-based score was 
simply subtracted from the window-based score to 
arrive at the final score (we tried scaling the dis-
tance score before subtraction but this did not im-
prove results on the MC160 train set). The 
algorithms are summarized in Figure 4. A coin flip 
is used to break ties. The use of inverse word 
counts was inspired by TF-IDF. 
Results for MC160 and MC500 are shown in 
Table 4 and Table 5. The MC160 train and devel-
opment sets were used for tuning. The baseline 
algorithm was authored without seeing any portion 
of MC500, so both the MC160 test set and all of 
MC500 were used for testing (although we never-
theless report results on the train/test split). Note 
that adding the distance based algorithm improved 
accuracy by approximately 10% absolute on 
MC160 and approximately 6% on MC500. Over-
all, error rates on MC500 are higher than on 
MC160, which agrees with human performance 
(see Table 2), suggesting that MC500?s questions 
are more difficult. 
7 Recognizing Textual Entailment Results 
We also tried using a ?recognizing textual entail-
ment? (RTE) system to answer MCTest questions. 
The goal of RTE (Dagan et al, 2005) is to deter-
mine whether a given statement can be inferred 
from a particular text. We can cast MCTest as an 
RTE task by converting each question-answer pair 
into a statement, and then selecting the answer 
whose statement has the highest likelihood of be-
ing entailed by the story. For example, in the sam-
ple story given in Figure 1, the second question can 
be converted into four statements (one for each 
answer), and the RTE system should select the 
statement ?James pulled pudding off of the shelves 
in the grocery store? as the most likely one. 
For converting question-answer pairs to state-
ments, we used the rules employed in a web-based 
question answering system (Cucerzan and 
Agichtein, 2005). For RTE, we used BIUTEE 
(Stern and Dagan, 2011), which performs better 
than the median system in the past four RTE com-
petitions. We ran BIUTEE both in its default con-
figuration, as well as with its optional additional 
data sources (FrameNet, ReVerb, DIRT, and others 
as found on the BIUTEE home page). The default 
configuration performed better so we present its 
results here. The results in Table 6 show that the 
RTE method performed worse than the baseline. 
MC160 Train and Dev:  
400 Q?s 
Test:  
240 Q?s 
SW SW+D SW SW+D 
Single 59.46 68.11 64.29 75.89 
Multi 59.53 67.44 48.44 57.81 
All 59.50 67.75 55.83 66.25 
Table 4. Percent correct for the multiple choice ques-
tions for MC160. SW: sliding window algorithm. 
SW+D: combined results with sliding window and 
distance based algorithms. Single/Multi: questions 
marked by worker as requiring a single/multiple sen-
tence(s) to answer. All differences between SW and 
SW+D are significant (p<0.01 using the two-tailed 
paired t-test). 
 
MC500 
 
Train and Dev: 
1400 Q?s 
Test:  
600 Q?s 
All 
SW SW+D SW SW+D SW+D 
Single 55.13 61.77 51.10 57.35 60.44 
Multi 49.80 55.28 51.83 56.10 55.53 
All 52.21 58.21 51.50 56.67 57.75 
Table 5. Percent correct for the multiple choice ques-
tions for MC500, notation as above. All differences 
between SW and SW+D are significant (p<0.01, test-
ed as above). 
 
 MC160 Test MC500 Test 
Baseline (SW+D) 66.25 56.67 
RTE 59.79
?
 53.52 
Combined 67.60 60.83
?
 
Table 6. Percent correct for MC160 and MC500 test 
sets. The ? indicates statistical significance vs. baseline 
(p<0.01 using the two-tailed paired t-test). MC160 
combined vs. baseline has p-value 0.063. 
200
We also combined the baseline and RTE system 
by training BIUTEE on the train set and using the 
development set to optimize a linear combination 
of BIUTEE with the baseline; the combined sys-
tem outperforms either component system on 
MC500. 
It is possible that with some tuning, an RTE sys-
tem will outperform our baseline system.  Never-
theless, these RTE results, and the performance of 
the baseline system, both suggest that the reading 
comprehension task described here will not be triv-
ially solved by off-the-shelf techniques. 
8 Making Data and Results an Ongoing 
Resource 
Our goal in constructing this data is to encourage 
research and innovation in the machine compre-
hension of text. Thus, we have made both MC160 
and MC500 freely available for download at 
http://research.microsoft.com/mct. To our knowl-
edge, these are the largest copyright-free reading 
comprehension data sets publicly available. To 
further encourage research on these data, we will 
be continually updating the webpage with the best-
known published results to date, along with point-
ers to those publications. 
One of the difficulties in making progress on a 
particular task is implementing previous work in 
order to apply improvements to it. To mitigate this 
difficulty, we are encouraging researchers who use 
the data to (optionally) provide per-answer scores 
from their system. Doing so has three benefits: (a) 
a new system can be measured in the context of the 
errors made by the previous systems, allowing 
each research effort to incrementally add useful 
functionality without needing to also re-implement 
the current state-of-the-art; (b) it allows system 
performance to be measured using paired statistical 
testing, which will substantially increase the ability 
to determine whether small improvements are sig-
nificant; and (c) it enables researchers to perform 
error analysis on any of the existing systems, sim-
plifying the process of identifying and tackling 
common sources of error. We will also periodically 
ensemble the known systems using standard ma-
chine learning techniques and make those results 
available as well (unless the existing state-of-the-
art already does such ensembling). 
The released data contains the stories and ques-
tions, as well as the results from workers who rated 
the stories and took the tests. The latter may be 
used, for example, to measure machine perfor-
mance vs. human performance on a per-question 
basis (i.e., does your algorithm make similar mis-
takes to humans?), or vs. the judged clarity of each 
story. The ratings, as well as whether a question 
needs multiple sentences to answer, should typical-
ly only be used in evaluation, since such infor-
mation is not generally available for most text. We 
will also provide an anonymized author id for each 
story, which could allow additional research such 
as using other works by the same author when un-
derstanding a story, or research on authorship at-
tribution (e.g., Stamatatos, 2009). 
9 Future Work 
We plan to use this dataset to evaluate approaches 
for machine comprehension, but are making it 
available now so that others may do the same. If 
MCTest is used we will collect more story sets and 
will continue to refine the collection process. One 
interesting research direction is ensuring that the 
questions are difficult enough to challenge state-of-
the-art techniques as they develop. One idea for 
this is to apply existing techniques automatically 
during story set creation to see whether a question 
is too easily answered by a machine. By requiring 
authors to create difficult questions, each data set 
will be made more and more difficult (but still an-
swerable by humans) as the state-of-the-art meth-
ods advance. We will also experiment with timing 
the raters as they answer questions to see if we can 
find those that are too easy for people to answer. 
Removing such questions may increase the diffi-
culty for machines as well. Additionally, any di-
vergence between how easily a person answers a 
question vs. how easily a machine does may point 
toward new techniques for improving machine 
comprehension; we plan to conduct research in this 
direction as well as make any such data available 
for others. 
10 Conclusion 
We present the MCTest dataset in the hope that it 
will help spur research into the machine compre-
hension of text. The metric (the accuracy on the 
question sets) is clearly defined, and on that metric, 
lexical baseline algorithms only attain approxi-
mately 58% correct on test data (the MC500 set) as 
201
opposed to the 100% correct that the majority of 
crowd-sourced judges attain. A key component of 
MCTest is the scalable design: we have shown that 
data whose quality approaches that of expertly cu-
rated data can be generated using crowd sourcing 
coupled with expert correction of worker-identified 
errors. Should MCTest prove useful to the com-
munity, we will continue to gather data, both to 
increase the corpus size, and to keep the test sets 
fresh. The data is available at http://research.micro-
soft.com/mct and any submitted results will be 
posted there too. Because submissions will be re-
quested to include the score for each test item, re-
searchers will easily be able to compare their 
systems with those of others, and investigation of 
ensembles comprised of components from several 
different teams will be straightforward. MCTest 
also contains supplementary material that re-
searchers may find useful, such as worker accura-
cies on a grammar test and crowd-sourced 
measures of the quality of their stories. 
Acknowledgments 
We would like to thank Silviu Cucerzan and Lucy 
Vanderwende for their help with converting ques-
tions to statements and other useful discussions.  
References  
M. Agarwal and P. Mannem. 2011. Automatic Gap-fill 
Question Generation from Text Books. In Proceed-
ings of the Sixth Workshop on Innovative Use of NLP 
for Building Educational Applications, 56?64. 
E. Breck, M. Light, G.S.Mann, E. Riloff, B. Brown, P. 
Anand, M. Rooth M. Thelen. 2001. Looking under 
the hood: Tools for diagnosing your question answer-
ing engine. In Proceedings of the workshop on Open-
domain question answering, 12, 1-8. 
E. Charniak. 1972. Toward a Model of Children?s Story 
Comprehension. Technical Report, 266, MIT Artifi-
cial Intelligence Laboratory, Cambridge, MA. 
P. Clark, P. Harrison, and X. Yao. An Entailment-Based 
Approach to the QA4MRE Challenge. 2012. In Pro-
ceedings of the Conference and Labs of the Evalua-
tion Forum (CLEF) 2012. 
S. Cucerzan and E. Agichtein. 2005. Factoid Question 
Answering over Unstructured and Structured Content 
on the Web. In Proceedings of the Fourteenth Text 
Retrieval Conference (TREC). 
I. Dagan, O. Glickman, and B. Magnini. 2006. The 
PASCAL Recognising Textual Entailment Chal-
lenge. In J. Qui?onero-Candela, I. Dagan, B. Magni-
ni, F. d'Alch?-Buc (Eds.), Machine Learning 
Challenges. Lecture Notes in Computer Science, Vol. 
3944, pp. 177-190, Springer. 
D. Goldwasser, R. Reichart, J. Clarke, D. Roth. 2011. 
Confidence Driven Unsupervised Semantic Parsing. 
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, 1486-1495. 
E. Grois and D.C. Wilkins. 2005. Learning Strategies 
for Story Comprehension: A Reinforcement Learning 
Approach. In Proceedings of the Twenty Second In-
ternational Conference on Machine Learning, 257-
264. 
S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003. 
Open-Domain Textual Question Answering Tech-
niques. Natural Language Engineering, 9(3):1-38. 
Cambridge University Press, Cambridge, UK. 
L. Hirschman, M. Light, E. Breck, and J.D. Burger. 
1999. Deep Read: A Reading Comprehension Sys-
tem. In Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics (ACL), 
325-332. 
J. Horton and L. Chilton. 2010. The labor economics of 
paid crowdsourcing. In Proceedings of the 11th ACM 
Conference on Electronic Commerce, 209-218. 
V. Kuperman, H. Stadthagen-Gonzalez, M. Brysbaert. 
2012. Age-of-acquisition ratings for 30,000 English 
words. Behavior Research Methods, 44(4):978-990. 
J.L. Leidner, T. Dalmas, B. Webber, J. Bos, C. Grover. 
2003. Automatic Multi-Layer Corpus Annotation for 
Evaluating Question Answering Methods: 
CBC4Kids. In Proceedings of the 3rd International 
Workshop on Linguistically Interpreted Corpora. 
E.T. Mueller. 2010. Story Understanding Resources. 
http://xenia.media.mit.edu/~mueller/storyund/storyre
s.html. 
G. Paolacci, J. Chandler, and P. Iperirotis. 2010. Run-
ning experiments on Amazon Mechanical Turk. 
Judgment and Decision Making. 5(5):411-419. 
E. Stamatatos. 2009. A survey of modern authorship 
attribution methods. J. Am. Soc. Inf. Sci., 60:538?
556. 
A. Stern and I. Dagan. 2011. A Confidence Model for 
Syntactically-Motivated Entailment Proofs. In Pro-
ceedings of Recent Advances in Natural Language 
Processing (RANLP). 
L.R. Tang and R.J. Mooney. 2001. Using Multiple 
Clause Constructors in Inductive Logic Programming 
for Semantic Parsing. In Proceedings of the 12th Eu-
ropean Conference on Machine Learning (ECML), 
466-477. 
G. Tur, D. Hakkani-Tur, and L.Heck. 2010. What is left 
to be understood in ATIS? Spoken Language Tech-
nology Workshop, 19-24. 
E.M. Voorhees and D.M. Tice. 1999. The TREC-8 
Question Answering Track Evaluation. In Proceed-
ings of the Eighth Text Retrieval Conference (TREC-
8). 
202
B. Wellner, L. Ferro, W. Greiff, and L. Hirschman. 
2005. Reading comprehension tests for computer-
based understand evaluation. Natural Language En-
gineering, 12(4):305-334. Cambridge University 
Press, Cambridge, UK. 
J.M. Zelle and R.J. Mooney. 1996. Learning to Parse 
Database Queries using Inductive Logic Program-
ming. In Proceedings of the Thirteenth National 
Conference on Artificial Intelligence (AAAI), 1050-
1055. 
L.S. Zettlemoyer and M. Collins. 2009. Learning Con-
text-Dependent Mappings from Sentences to Logical 
Form. In Proceedings of the 47th Annual Meeting of 
the Association for Computation Linguistics (ACL), 
976-984. 
G. Zweig and C.J.C. Burges. 2012. A Challenge Set for 
Advancing Language Modeling. In Proceedings of 
the Workshop on the Future of Language Modeling 
for HLT, NAACL-HLT. 
 
 
203
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 601?610,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Computational Approaches to Sentence Completion
Geoffrey Zweig, John C. Platt
Christopher Meek
Christopher J.C. Burges
Microsoft Research
Redmond, WA 98052
Ainur Yessenalina
Cornell University
Computer Science Dept.
Ithaca, NY 14853
Qiang Liu
Univ. of California, Irvine
Info. & Comp. Sci.
Irvine, California 92697
Abstract
This paper studies the problem of sentence-
level semantic coherence by answering SAT-
style sentence completion questions. These
questions test the ability of algorithms to dis-
tinguish sense from nonsense based on a vari-
ety of sentence-level phenomena. We tackle
the problem with two approaches: methods
that use local lexical information, such as the
n-grams of a classical language model; and
methods that evaluate global coherence, such
as latent semantic analysis. We evaluate these
methods on a suite of practice SAT questions,
and on a recently released sentence comple-
tion task based on data taken from five Conan
Doyle novels. We find that by fusing local
and global information, we can exceed 50%
on this task (chance baseline is 20%), and we
suggest some avenues for further research.
1 Introduction
In recent years, standardized examinations have
proved a fertile source of evaluation data for lan-
guage processing tasks. They are valuable for many
reasons: they represent facets of language under-
standing recognized as important by educational ex-
perts; they are organized in various formats designed
to evaluate specific capabilities; they are yardsticks
by which society measures educational progress;
and they affect a large number of people.
Previous researchers have taken advantage of this
material to test both narrow and general language
processing capabilities. Among the narrower tasks,
the identification of synonyms and antonyms has
been studied by (Landauer and Dumais, 1997; Mo-
hammed et al, 2008; Mohammed et al, 2011; Tur-
ney et al, 2003; Turney, 2008), who used ques-
tions from the Test of English as a Foreign Lan-
guage (TOEFL), Graduate Record Exams (GRE)
and English as a Second Language (ESL) exams.
Tasks requiring broader competencies include logic
puzzles and reading comprehension. Logic puzzles
drawn from the Law School Administration Test
(LSAT) and the GRE were studied in (Lev et al,
2004), which combined an extensive array of tech-
niques to solve the problems. The DeepRead sys-
tem (Hirschman et al, 1999) initiated a long line of
research into reading comprehension based on test
prep material (Charniak et al, 2000; Riloff and The-
len, 2000; Wang et al, 2000; Ng et al, 2000).
In this paper, we study a new class of problems
intermediate in difficulty between the extremes of
synonym detection and general question answer-
ing - the sentence completion questions found on
the Scholastic Aptitude Test (SAT). These questions
present a sentence with one or two blanks that need
to be filled in. Five possible words (or short phrases)
are given as options for each blank. All possible an-
swers except one result in a nonsense sentence. Two
examples are shown in Figure 1.
The questions are highly constrained in the sense
that all the information necessary is present in the
sentence itself without any other context. Neverthe-
less, they vary widely in difficulty. The first of these
examples is relatively simple: the second half of the
sentence is a clear description of the type of behavior
characterized by the desired adjective. The second
example is more sophisticated; one must infer from
601
1. One of the characters in Milton Murayama?s
novel is considered because he deliber-
ately defies an oppressive hierarchical society.
(A) rebellious (B) impulsive (C) artistic (D)
industrious (E) tyrannical
2. Whether substances are medicines or poisons
often depends on dosage, for substances that are
in small doses can be in large.
(A) useless .. effective
(B) mild .. benign
(C) curative .. toxic
(D) harmful .. fatal
(E) beneficial .. miraculous
Figure 1: Sample sentence completion questions
(Educational-Testing-Service, 2011).
the contrast between medicine and poison that the
correct answer involves a contrast, either useless vs.
effective or curative vs. toxic. Moreover, the first, in-
correct, possibility is perfectly acceptable in the con-
text of the second clause alone; only irrelevance to
the contrast between medicine and poison eliminates
it. In general, the questions require a combination of
semantic and world knowledge as well as occasional
logical reasoning. We study the sentence comple-
tion task because we believe it is complex enough to
pose a significant challenge, yet structured enough
that progress may be possible.
As a first step, we have approached the prob-
lem from two points-of-view: first by exploiting lo-
cal sentence structure, and secondly by measuring
a novel form of global sentence coherence based
on latent semantic analysis. To investigate the use-
fulness of local information, we evaluated n-gram
language model scores, from both a conventional
model with Good-Turing smoothing, and with a re-
cently proposed maximum-entropy class-based n-
gram model (Chen, 2009a; Chen, 2009b). Also
in the language modeling vein, but with potentially
global context, we evaluate the use of a recurrent
neural network language model. In all the language
modeling approaches, a model is used to compute a
sentence probability with each of the potential com-
pletions. To measure global coherence, we propose
a novel method based on latent semantic analysis
(LSA). We find that the LSA based method performs
best, and that both local and global information can
be combined to exceed 50% accuracy. We report re-
sults on a set of questions taken from a collection
of SAT practice exams (Princeton-Review, 2010),
and further validate the methods with the recently
proposed MSR Sentence Completion Challenge set
(Zweig and Burges, 2011).
Our paper thus makes the following contributions:
First, we present the first published results on the
SAT sentence completion task. Secondly, we eval-
uate the effectiveness of both local n-gram informa-
tion, and global coherence in the form of a novel
LSA-based metric. Finally, we illustrate that the lo-
cal and global information can be effectively fused.
The remainder of this paper is organized as fol-
lows. In Section 2 we discuss related work. Section
3 describes the language modeling methods we have
evaluated. Section 4 outlines the LSA-based meth-
ods. Section 5 presents our experimental results. We
conclude with a discussion in Section 6.
2 Related Work
The past work which is most similar to ours is de-
rived from the lexical substitution track of SemEval-
2007 (McCarthy and Navigli, 2007). In this task,
the challenge is to find a replacement for a word or
phrase removed from a sentence. In contrast to our
SAT-inspired task, the original answer is indicated.
For example, one might be asked to find alternates
for match in ?After the match, replace any remain-
ing fluid deficit to prevent problems of chronic de-
hydration throughout the tournament.? Two consis-
tently high-performing systems for this task are the
KU (Yuret, 2007) and UNT (Hassan et al, 2007)
systems. These operate in two phases: first they find
a set of potential replacement words, and then they
rank them. The KU system uses just an N-gram lan-
guage model to do this ranking. The UNT system
uses a large variety of information sources, and a
language model score receives the highest weight.
N-gram statistics were also very effective in (Giu-
liano et al, 2007). That paper also explores the use
of Latent Semantic Analysis to measure the degree
of similarity between a potential replacement and its
context, but the results are poorer than others. Since
the original word provides a strong hint as to the pos-
602
sible meanings of the replacements, we hypothesize
that N-gram statistics are largely able to resolve the
remaining ambiguities. The SAT sentence comple-
tion sentences do not have this property and thus are
more challenging.
Related to, but predating the Semeval lexical sub-
stitution task are the ESL synonym questions pro-
posed by Turney (2001), and subsequently consid-
ered by numerous research groups including Terra
and Clarke (2003) and Pado and Lapata (2007).
These questions are similar to the SemEval task, but
in addition to the original word and the sentence
context, the list of options is provided. Jarmasz and
Szpakowicz (2003) used a sophisticated thesaurus-
based method and achieved state-of-the art perfor-
mance, which is 82%.
Other work on standardized tests includes the syn-
onym and antonym tasks mentioned in Section 1,
and more recent work on a SAT analogy task in-
troduced by (Turney et al, 2003) and extensively
used by other researchers (Veale, 2004; Turney and
Littman, 2005; D. et al, 2009).
3 Sentence Completion via Language
Modeling
Perhaps the most straightforward approach to solv-
ing the sentence completion task is to form the com-
plete sentence with each option in turn, and to eval-
uate its likelihood under a language model. As
discussed in Section 2, this was found be be very
effective in the ranking phase of several SemEval
systems. In this section, we describe the suite of
state-of-the-art language modeling techniques for
which we will present results. We begin with n-
gram models; first a classical n-gram backoff model
(Chen and Goodman, 1999), and then a recently pro-
posed class-based maximum-entropy n-gram model
(Chen, 2009a; Chen, 2009b). N-gram models have
the obvious disadvantage of using a very limited
context in predicting word probabilities. There-
fore we evaluate the recurrent neural net model of
(Mikolov et al, 2010; Mikolov et al, 2011b). This
model has produced record-breaking perplexity re-
sults in several tasks (Mikolov et al, 2011a), and has
the potential to encode sentence-span information in
the network hidden-layer activations. We have also
evaluated the use of parse scores, using an off-the-
shelf stochastic context free grammar parser. How-
ever, the grammatical structure of the alternatives is
often identical. With scores differing only in the fi-
nal non-terminal/terminal rewrites, this did little bet-
ter than chance. The use of other syntactically de-
rived features, for example based on a dependency
parse, are likely to be more effective, but we leave
this for future work.
3.1 Backoff N-gram Language Model
Our baseline model is a Good-Turing smoothed
model trained with the CMU language modeling
toolkit (Clarkson and Rosenfeld, 1997). For the SAT
task, we used a trigram language model trained on
1.1B words of newspaper data, described in Section
5.1. All bigrams occurring at least twice were re-
tained in the model, along with all trigrams occur-
ring at least three times. The vocabulary consisted
of all words occurring at least 100 times in the data,
along with every word in the development or test
sets. This resulted in a 124k word vocabulary and
59M n-grams. For the Conan Doyle data, which we
henceforth refer to as the Holmes data (see Section
5.1), the smaller amount of training data allowed us
to use 4-grams and a vocabulary cutoff of 3. This re-
sulted in 26M n-grams and a 126k word vocabulary.
3.2 Maximum Entropy Class-Based N-gram
Language Model
Word-class information provides a level of abstrac-
tion which is not available in a word-level lan-
guage model; therefore we evaluated a state-of-the-
art class based language model. Model M (Chen,
2009a; Chen, 2009b) is a recently proposed class
based exponential n-gram language model which
has shown improvements across a variety of tasks
(Chen, 2009b; Chen et al, 2009; Emami et al,
2010). The key ideas are the modeling of word n-
gram probabilities with a maximum entropy model,
and the use of word-class information in the defini-
tion of the features. In particular, each word w is
assigned deterministically to a class c, allowing the
n-gram probabilities to be estimated as the product
of class and word parts
P (wi|wi?n+1 . . . wi?2wi?1) =
P (ci|ci?n+1 . . . ci?2ci?1, wi?n+1 . . . wi?2wi?1)
P (wi|wi?n+1 . . . wi?2wi?1, ci).
603
Both components are themselves maximum entropy
n-gram models in which the probability of a word
or class label l given history h is determined by
1
Z exp(
?
k fk(h, l)). The features fk(h, l) used are
the presence of various patterns in the concatena-
tion of hl, for example whether a particular suffix
is present in hl.
3.3 Recurrent Neural Net Language Model
Many of the questions involve long-range depen-
dencies between words. While n-gram models have
no ability to explicitly maintain long-span context,
the recently proposed recurrent neural-net model of
(Mikolov et al, 2010) does. Related approaches
have been proposed by (Sutskever et al, 2011;
Socher et al, 2011). In this model, a set of neu-
ral net activations s(t) is maintained and updated at
each sentence position t. These activations encapsu-
late the sentence history up to the tth word in a real-
valued vector which typically has several hundred
dimensions. The word at position t is represented as
a binary vector w(t) whose length is the vocabulary
size, and with a ?1? in a position uniquely associated
with the word, and ?0? elsewhere. w(t) and s(t) are
concatenated to predict an output distribution over
words, y(t). Updating is done with two weight ma-
trices u and v and nonlinear functions f() and g()
(Mikolov et al, 2011b):
x(t) = [w(t)T s(t ? 1)T ]T
sj(t) = f(
?
i
xi(t)uji)
yk(t) = g(
?
j
sj(t)vkj)
with f() being a sigmoid and g() a softmax:
f(x) =
1
1 + exp(?z)
, g(zm) =
exp(zm)
?
k exp(zk)
The output y(t) is a probability distribution over
words, and the parameters u and v are trained with
back-propagation to minimize the Kullback-Leibler
(KL) divergence between the predicted and observed
distributions. Because of the recurrent connections,
this model is similar to a nonlinear infinite impulse
response (IIR) filter, and has the potential to model
long span dependencies. Theoretical considerations
(Bengio et al, 1994) indicate that for many prob-
lems, this may not be possible, but in practice it is
an empirical question.
4 Sentence Completion via Latent
Semantic Analysis
Latent Semantic Analysis (LSA) (Deerwester et al,
1990) is a widely used method for representing
words and documents in a low dimensional vector
space. The method is based on applying singular
value decomposition (SVD) to a matrix W repre-
senting the occurrence of words in documents. SVD
results in an approximation of W by the product
of three matrices, one in which each word is rep-
resented as a low-dimensional vector, one in which
each document is represented as a low dimensional
vector, and a diagonal scaling matrix. The simi-
larity between two words can then be quantified as
the cosine-similarity between their respective scaled
vectors, and document similarity can be measured
likewise. It has been used in numerous tasks, rang-
ing from information retrieval (Deerwester et al,
1990) to speech recognition (Bellegarda, 2000; Coc-
caro and Jurafsky, 1998).
To perform LSA, one proceeds as follows. The
input is a collection of n documents which are ex-
pressed in terms of words from a vocabulary of size
m. These documents may be actual documents such
as newspaper articles, or simply as in our case no-
tional documents such as sentences. Next, a m x n
matrix W is formed. At its simplest, the ijth entry
contains the number of times word i has occurred in
document j - its term frequency or TF value. More
conventionally, the entry is weighted by some no-
tion of the importance of word i, for example the
negative logarithm of the fraction of documents that
contain it, resulting in a TF-IDF weighting (Salton
et al, 1975). Finally, to obtain a subspace represen-
tation of dimension d, W is decomposed as
W ? USV T
where U is m x d, V T is d x n, and S is a d x d diag-
onal matrix. In applications, d << n and d << m;
for example one might have a 50, 000 word vocab-
ulary and 1, 000, 000 documents and use a 300 di-
mensional subspace representation.
An important property of SVD is that the rows
of US - which represents the words - behave sim-
ilarly to the original rows of W , in the sense that
the cosine similarity between two rows in US ap-
proximates the cosine similarity between the corre-
604
sponding rows in W . Cosine similarity is defined as
sim(x,y) = x?y?x??y? .
4.1 Total Word Similarity
Perhaps the simplest way of doing sentence comple-
tion with LSA is to compute the total similarity of a
potential answer a with the rest of the words in the
sentence S, and to choose the most related option.
We define the total similarity as:
totsim(a,S) =
?
w?S
sim(a,w)
When the completion requires two words, total sim-
ilarity is the sum of the contributions for both words.
This is our baseline method for using LSA, and one
of the best methods we have found.
4.2 Sentence Reconstruction
Recall that LSA approximates a weighted word-
document matrix W as the product of low rank
matrices U and V along with a scaling matrix S:
W ? USV T . Using singular value decomposition,
this is done so as to minimize the mean square re-
construction error
?
ij Q
2
ij whereQ = W?USV
T .
From the basic definition of LSA, each column ofW
(representing a document) is represented as
Wj = USV Tj , (1)
that is, as a linear combination of the set of basis
functions formed by the columns of US, with the
combination weights specified in V Tj . When a new
document is presented, it is also possible to repre-
sent it in terms of the same basis vectors. Moreover,
we may take the reconstruction error induced by this
representation to be a measure of how consistent the
new document is with the original set of documents
used to determine U S and V (Bellegarda, 2000).
It remains to represent a new document in terms
of the LSA bases. This is done as follows (Deer-
wester et al, 1990; Bellegarda, 2000), again with
the objective of minimizing the reconstruction error.
First, note that since U is column-orthonormal, (1)
implies that
Vj = W Tj US
?1 (2)
Thus, if we notionally index a new document by p,
we proceed by forming a new column (document)
vector Wp using the standard term-weighting, and
then find its LSA-space representation Vp using (2).
We can evaluate the reconstruction quality by insert-
ing the result in (1). The reconstruction error is then
||(UUT ? I)Wp||2
Note that if all the dimensions are retained, the re-
construction error is zero; in the case that only the
highest singular vectors are used, however, it is not.
Due to the fact that the sentences vary in length we
choose the number of retained singular vectors as a
fraction f of the sentence length. If the answer has
n words we use the top nf components. In practice,
a f of 1.2 was selected on the basis of development
set results.
4.3 A LSA N-gram Language Model
In the context of speech recognition, LSA has been
combined with classical n-gram language models
in (Coccaro and Jurafsky, 1998; Bellegarda, 2000).
The crux of this idea is to interpolate an n-gram lan-
guage model probability with one based on LSA,
with the intuition that the standard n-gram model
will do a good job predicting function words, and
the LSA model will do a good job on words pre-
dicted by their long-span context. This logic makes
sense for the sentence completion task as well, mo-
tivating us to evaluate it.
To do this, we adopt the procedure of (Coccaro
and Jurafsky, 1998), using linear interpolation be-
tween the n-gram and LSA probabilities:
p(w|history) =
?png(w|history) + (1 ? ?)plsa(w|history)
The probability of a word given its history is com-
puted by the LSA model in the following way. Let h
be the sum of all the LSA word vectors in the his-
tory. Let m be the smallest cosine similarity be-
tween h and any word in the vocabulary V : m =
minw?V sim(h,w). The probability of a word w in
the context of history h is given by
Plsa(w|h) =
sim(h,w) ? m
?
q?V (sim(h, q) ? m)
Since similarity can be negative, subtracting the
minimum (m) ensures that all the estimated prob-
abilities are between 0 and 1.
605
4.4 Improving Efficiency and Expressiveness
Given the basic framework described above, a num-
ber of enhancements are possible. In terms of ef-
ficiency, recall that it is necessary to perform SVD
on a term-document matrix. The data we used was
grouped into paragraph ?documents,? of which there
were over 27 million, with 2.6 million unique words.
While the resulting matrix is highly sparse, it is nev-
ertheless impractical to perform SVD. We overcome
this difficulty in two ways. First, we restrict the set
of documents used to those which are ?relevant? to
a given test set. This is done by requiring that a doc-
ument contain at least one of the potential answer-
words. Secondly, we restrict the vocabulary to the
set of words present in the test set. For the sentence-
reconstruction method of Section 4.2, we have found
it convenient to do data selection per-sentence.
To enhance the expressive power of LSA, the term
vocabulary can be expanded from unigrams to bi-
grams or trigrams of words, thus adding information
about word ordering. This was also used in the re-
construction technique.
5 Experimental Results
5.1 Data Resources
We present results with two datasets. The first is
taken from 11 Practice Tests for the SAT & PSAT
2011 Edition (Princeton-Review, 2010). This book
contains eleven practice tests, and we used all the
sentence completion questions in the first five tests
as a development set, and all the questions in the last
six tests as the test set. This resulted in sets with 95
and 108 questions respectively. Additionally, we re-
port results on the recently released MSR Sentence
Completion Challenge (Zweig and Burges, 2011).
This consists of a set of 1, 040 sentence completion
questions based on sentences occurring in five Co-
nan Doyle Sherlock Holmes novels, and is identical
in format to the SAT questions. Due to the source of
this data, we refer to it as the Holmes data.
To train models, we have experimented with a
variety of data sources. Since there is no publi-
cally available collection of SAT questions suitable
to training, our methods have all relied on unsu-
pervised data. Early on, we ran a set of experi-
ments to determine the relevance of different types
of data. Thinking that data from an encyclopedia
Data Dev % Correct Test % Correct
Encarta 26 33
Wikipedia 32 31
LA Times 39 42
Table 1: Effectiveness of different types of training data.
might be useful, we evaluated an electronic version
of the 2003 Encarta encyclopedia, which has ap-
proximately 29M words. Along similar lines, we
used a collection of Wikipedia articles consisting of
709M words. This data is the entire Wikipedia as of
January 2011, broken down into sentences, with fil-
tering to remove sentences consisting of URLs and
Wiki author comments. Finally, we used a com-
mercial newspaper dataset consisting of all the Los
Angeles Times data from 1985 to 2002, containing
about 1.1B words. These data sources were evalu-
ated using the baseline n-gram LM approach of Sec-
tion 3.1. Initial experiments indicated that that the
Los Angeles Times data is best suited to this task
(see Table 1), and our SAT experiments use this
source. For the MSR Sentence Completion data,
we obtained the training data specified in (Zweig
and Burges, 2011), consisting of approximately 500
19th-century novels available from Project Guten-
berg, and comprising 48M words.
5.2 Human Performance
To provide human benchmark performance, we
asked six native speaking high school students and
five graduate students to answer the questions on the
development set. The high-schoolers attained 87%
accuracy and the graduate students 95%. Zweig and
Burges (2011) cite a human performance of 91%
on the Holmes data. Statistics from a large cross-
section of the population are not available. As a fur-
ther point of comparison, we note that chance per-
formance is 20%.
5.3 Language Modeling Results
Table 2 summarizes our language modeling results
on the SAT data. With the exception of the base-
line backoff n-gram model, these techniques were
too computationally expensive to utilize the full Los
Angeles Times corpus. Instead, as with LSA, a ?rel-
evant? corpus was selected of the sentences which
contain at least one answer option from either the
606
Method Data (Dev / Test) Dev Test
3-gram GT 1.1B / 1.1B 39% 42%
Model M 193M / 236M 35 41
RNN 36M / 44M 37 42
LSA-LM 293M / 358 M 48 44
Table 2: Performance of language modeling methods on
SAT questions.
Method Dev ppl Dev Test ppl Test
3-gram GT 195 36% 190 44%
Model M 178 36 175 42
RNN 147 37 144 42
Table 3: Performance of language modeling methods us-
ing identical training data and vocabularies.
development or test set. Separate subsets were made
for development and test data. This data was further
sub-sampled to obtain the training set sizes indicated
in the second column. For the LSA-LM, an interpo-
lation weight of 0.1 was used for the LSA score, de-
termined through optimization on the development
set. We see from this table that the language models
perform similarly and achieve just above 40% on the
test set.
To make a more controlled comparison that nor-
malizes for the amount of training data, we have
trained Model M, and the Good-Turing model on
the same data subset as the RNN, and with the same
vocabulary. In Table 3, we present perplexity re-
sults on a held-out set of dev/test-relevant Los Ange-
les Times data, and performance on the actual SAT
questions. Two things are notable. First, the re-
current neural net has dramatically lower perplexity
than the other methods. This is consistent with re-
sults in (Mikolov et al, 2011a). Secondly, despite
the differences in perplexity, the methods show little
difference on SAT performance. Because Model M
was not better, only uses n-gram context, and was
used in the construction of the Holmes data (Zweig
and Burges, 2011), we do not consider it further.
5.4 LSA Results
Table 4 presents results for the methods of Sections
4.1 and 4.2. Of all the methods in isolation, the sim-
ple approach of Section 4.1 - to use the total cosine
similarity between a potential answer and the other
words in the sentence - has performed best. The ap-
Method Dev Test
Total Word Similarity 46% 46%
Reconstruction Error 53 41
Table 4: SAT performance of LSA based methods.
Method Test
3-input LSA 46%
LSA + Good-Turing LM 53
LSA + Good-Turing LM + RNN 52
Table 5: SAT test set accuracy with combined methods.
proach of using reconstruction error performed very
well on the development set, but unremarkably on
the test set.
5.5 Combination Results
A well-known trick for obtaining best results from
a machine learning system is to combine a set of
diverse methods into a single ensemble (Dietterich,
2000). We use ensembles to get the highest accuracy
on both of our data sets.
We use a simple linear combination of the out-
puts of the other models discussed in this paper. For
the LSA model, the linear combination has three in-
puts: the total word similarity, the cosine similarity
between the sum of the answer word vectors and the
sum of the rest of sentence?s word vectors, and the
number of out-of-vocabulary terms in the answer.
Each additional language model beyond LSA con-
tributes an additional input: the probability of the
sentence under that language model.
We train the parameters of the linear combination
on the SAT development set. The training minimizes
a loss function of pairs of answers: one correct and
one incorrect fill-in from the same question. We use
the RankNet loss function (Burges et al, 2005):
min
~w
f(~w ? (~x ? ~y)) + ?||~w||2
where ~x are the input features for the incorrect an-
swer, ~y are the features for the correct answer, ~w
are the weights for the combination, and f(z) =
log(1 + exp(z)). We tune the regularizer via 5-
fold cross validation, and minimize the loss using
L-BFGS (Nocedal and Wright, 2006). The results
on the SAT test set for combining various models
are shown in Table 5.
607
5.6 Holmes Data Results
To measure the robustness of our approaches, we
have applied them to the MSR Sentence Completion
set (Zweig and Burges, 2011), termed the Holmes
data. In Table 6, we present the results on this set,
along with the comparable SAT results. Note that
the latter are derived from models trained with the
Los Angeles Times data, while the Holmes results
are derived from models trained with 19th-century
novels. We see from this table that the results are
similar across the two tasks. The best performing
single model is LSA total word similarity.
For the Holmes data, combining the models out-
performs any single model. We train the linear com-
bination function via 5-fold cross-validation: the
model is trained five times, each time on 3/5 of the
data, the regularization tuned on 1/5 of the data, and
tested on 1/5. The test results are pooled across all
5 folds and are shown in Table 6. In this case, the
best combination is to blend LSA, the Good-Turing
language model, and the recurrent neural network.
6 Discussion
To verify that the differences in accuracy between
the different algorithms are not statistical flukes, we
perform a statistical significance test on the out-
puts of each algorithm. We use McNemar?s test,
which is a matched test between two classifiers (Di-
etterich, 1998). We use the False Discovery Rate
method (Benjamini and Hochberg, 1995) to control
the false positive rate caused by multiple tests. If
we allow 2% of our tests to yield incorrectly false
results, then for the SAT data, the combination of
the Good-Turing smoothed language model with an
LSA-based global similarity model (52% accuracy)
is better that the baseline alone (42% accuracy).
Secondly, for the Holmes data, we can state that
LSA total similarity beats the recurrent neural net-
work, which in turn is better than the baseline n-
gram model. The combination of all three is sig-
nificantly better than any of the individual models.
To better understand the system performance and
gain insight into ways of improving it, we have ex-
amined the system?s errors. Encouragingly, one-
third of the errors involve single-word questions
which test the dictionary definition of a word. This
is done either by stating the definition, or provid-
Method SAT Holmes
Chance 20% 20%
GT N-gram LM 42 39
RNN 42 45
LSA Total Similarity 46 49
Reconstruction Error 41 41
LSA-LM 44 42
Combination 53 52
Human 87 to 95 91
Table 6: Performance of methods on the MSR Sentence
Completion Challenge, contrasted with SAT test set.
ing a stereotypical use of the word. An example of
the first case is: ?Great artists are often prophetic
(visual): they perceive what we cannot and antici-
pate the future long before we do.? (The system?s
incorrect answer is in parentheses.) An example
of the second is: ?One cannot help but be moved
by Theresa?s heartrending (therapeutic) struggle to
overcome a devastating and debilitating accident.?
At the other end of the difficulty spectrum are
questions involving world knowledge and/or logical
implications. An example requiring both is, ?Many
fear that the ratification (withdrawal) of more le-
nient tobacco advertising could be detrimental to
public health.? About 40% of the errors require this
sort of general knowledge to resolve. Based on our
analysis, we believe that future research could prof-
itably exploit the structured information present in
a dictionary. However, the ability to identify and
manipulate logical relationships and embed world
knowledge in a manner amenable to logical manip-
ulation may be necessary for a full solution. It is
an interesting research question if this could be done
implicitly with a machine learning technique, for ex-
ample recurrent or recursive neural networks.
7 Conclusion
In this paper we have investigated methods for
answering sentence-completion questions. These
questions are intriguing because they probe the abil-
ity to distinguish semantically coherent sentences
from incoherent ones, and yet involve no more con-
text than the single sentence. We find that both local
n-gram information and an LSA-based global coher-
ence model do significantly better than chance, and
that they can be effectively combined.
608
References
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88(8).
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neural
Networks, 5(2):157 ?166.
Y. Benjamini and Y. Hochberg. 1995. Controlling the
fase discovery rate: a practical and powerful approach
to multiple testing. J. Royal Statistical Society B,
53(1):289?300.
C. Burges, T. Shaked., E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. 2005. Learning to
rank using gradient descent. In Proc. ICML, pages 89?
96.
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo
Braz, Benjamin Garrett, Margaret Kosmala, Tomer
Moscovich, Lixin Pang, Changhee Pyo, Ye Sun,
Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa
Zorn. 2000. Reading comprehension programs in
a statistical-language-processing class. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ?00, pages 1?5. Asso-
ciation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1999. An empirical
study of smoothing techniques for language modeling.
Computer Speech and Language, 13(4):359?393.
S. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya, and
A. Sethy. 2009. Scaling shrinkage-based language
models. In ASRU.
S. Chen. 2009a. Performance prediction for exponential
language models. In NAACL-HLT.
S. Chen. 2009b. Shrinking exponential language models.
In NAACL-HLT.
P.R. Clarkson and R. Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge
Toolkit. In Proceedings ESCA Eurospeech,
http://www.speech.cs.cmu.edu/SLM/toolkit.html.
N. Coccaro and D. Jurafsky. 1998. Towards better in-
tegration of semantic predictors in statistical language
modeling. In Proceedings, ICSLP.
Bollegala D., Matsuo Y., and Ishizuka M. 2009. Measur-
ing the similarity between implicit semantic relations
from the web. InWorldWideWeb Conference (WWW).
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
T.G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
T.G. Dietterich. 2000. Ensemble methods in machine
learning. In International Workshop on Multiple Clas-
sifier Systems, pages 1?15. Springer-Verlag.
Educational-Testing-Service. 2011.
https://satonlinecourse.collegeboard.com/sr/digital assets/
assessment/pdf/0833a611-0a43-10c2-0148-
cc8c0087fb06-f.pdf.
A. Emami, S. Chen, A. Ittycheriah, H. Soltau, and
B. Zhao. 2010. Decoding with shrinkage-based lan-
guage models. In Interspeech.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ?07, pages 145?148, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. Unt: Subfinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations, SemEval ?07,
pages 410?413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lynette Hirschman, Mark Light, Eric Breck, and John D.
Burger. 1999. Deep read: A reading comprehension
system. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), pages 211?
240.
Iddo Lev, Bill MacCartney, Christopher D. Manning, and
Roger Levy. 2004. Solving logic puzzles: from ro-
bust processing to precise semantics. In Proceedings
of the 2nd Workshop on Text Meaning and Interpreta-
tion, pages 9?16. Association for Computational Lin-
guistics.
Jarmasz M. and Szpakowicz S. 2003. Roget?s thesaurus
and semantic similarity. In Recent Advances in Natu-
ral Language Processing (RANLP).
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48?53.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
609
Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas
Burget, and Jan Cernocky. 2011a. Empirical evalua-
tion and combination of advanced language modeling
techniques. In Proceedings of Interspeech 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2011. Measuring degrees of seman-
tic opposition. Technical report, National Research
Council Canada.
Hwee Tou Ng, Leong Hwee Teo, and Jennifer Lai Pheng
Kwan. 2000. A machine learning approach to answer-
ing questions for reading comprehension tests. In Pro-
ceedings of the 2000 Joint SIGDAT conference on Em-
pirical methods in natural language processing and
very large corpora: held in conjunction with the 38th
Annual Meeting of the Association for Computational
Linguistics - Volume 13, EMNLP ?00, pages 124?132.
J. Nocedal and S. Wright. 2006. Numerical Optimiza-
tion. Springer-Verlag.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33 (2), pages 161?199.
Princeton-Review. 2010. 11 Practice Tests for the SAT
& PSAT, 2011 Edition. The Princeton Review.
Ellen Riloff and Michael Thelen. 2000. A rule-based
question answering system for reading comprehension
tests. In Proceedings of the 2000 ANLP/NAACL Work-
shop on Reading comprehension tests as evaluation for
computer-based language understanding sytems - Vol-
ume 6, ANLP/NAACL-ReadingComp ?00, pages 13?
19.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In Proceedings of the 2011 International Con-
ference on Machine Learning (ICML-2011).
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural networks.
In Proceedings of the 2011 International Conference
on Machine Learning (ICML-2011).
E. Terra and C. Clarke. 2003. Frequency estimates for
statistical word similarity measures. In Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Recent Advances in Natural Lan-
guage Processing (RANLP).
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In European Confer-
ence on Machine Learning (ECML).
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
T. Veale. 2004. Wordnet sits the sat: A knowledge-based
approach to lexical analogy. In European Conference
on Artificial Intelligence (ECAI).
W. Wang, J. Auer, R. Parasuraman, I. Zubarev,
D. Brandyberry, and M. P. Harper. 2000. A ques-
tion answering system developed as a project in a
natural language processing course. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ?00, pages 28?35.
Deniz Yuret. 2007. Ku: word sense disambiguation
by substitution. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 207?213, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Geoffrey Zweig and Christopher J.C. Burges. 2011. The
Microsoft Research sentence completion challenge.
Technical Report MSR-TR-2011-129, Microsoft.
610
