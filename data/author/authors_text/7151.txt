Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 141?144,
New York, June 2006. c?2006 Association for Computational Linguistics
Word Domain Disambiguation via Word Sense Disambiguation 
 
Antonio Sanfilippo, Stephen Tratz, Michelle Gregory 
Pacific Northwest National Laboratory 
Richland, WA 99352 
{Antonio.Sanfilippo, Stephen.Tratz, Michelle.Gregory}@pnl.gov 
 
 
  
 
Abstract 
Word subject domains have been 
widely used to improve the perform-
ance of word sense disambiguation al-
gorithms. However, comparatively little 
effort has been devoted so far to the 
disambiguation of word subject do-
mains. The few existing approaches 
have focused on the development of al-
gorithms specific to word domain dis-
ambiguation. In this paper we explore 
an alternative approach where word 
domain disambiguation is achieved via 
word sense disambiguation. Our study 
shows that this approach yields very 
strong results, suggesting that word 
domain disambiguation can be ad-
dressed in terms of word sense disam-
biguation with no need for special 
purpose algorithms.  
1 Introduction 
Word subject domains have been ubiquitously 
used in dictionaries to help human readers pin-
point the specific sense of a word by specifying 
technical usage, e.g. see ?subject field codes? in 
Procter (1978). In computational linguistics, 
word subject domains have been widely used to 
improve the performance of machine translation 
systems. For example, in a review of commonly 
used features in automated translation, Mowatt 
(1999) reports that most of the machine transla-
tion systems surveyed made use of word subject 
domains. Word subject domains have also been 
used in information systems. For example, San-
filippo (1998) describes a summarization system 
where subject domains provide users with useful 
conceptual parameters to tailor summary re-
quests to a user?s interest.  
Successful usage of word domains in applica-
tions such as machine translation and summari-
zation is strongly dependent on the ability to 
assign the appropriate subject domain to a word 
in its context. Such an assignment requires a 
process of Word Domain Disambiguation 
(WDD) because the same word can often be as-
signed different subject domains out of context 
(e.g. the word partner can potentially be re-
lated to FINANCE or MARRIAGE).  
Interestingly enough, word subject domains 
have been widely used to improve the perform-
ance of Word Sense Disambiguation (WSD) 
algorithms (Wilks and Stevenson 1998, Magnini 
et al 2001; Gliozzo et al 2004). However, com-
paratively little effort has been devoted so far to 
the word domain disambiguation itself. The 
most notable exceptions are the work of Magnini 
and Strapparava (2000) and Suarez & Palomar 
(2002). Both studies propose algorithms specific 
to the WDD task and have focused on the dis-
ambiguation of noun domains.  
In this paper we explore an alternative ap-
proach where word domain disambiguation is 
achieved via word sense disambiguation. More-
over, we extend the treatment of WDD to verbs 
and adjectives. Initial results show that this ap-
proach yield very strong results, suggesting that 
WDD can be addressed in terms of word sense 
disambiguation with no need of special purpose 
algorithms.  
141
  
Figure 1: Senses and domains for the word bank in WordNet Domains, with number of occurrences in SemCor, 
adapted from Magnini et al (2002). 
2 WDD via WSD 
Our approach relies on the use of WordNet Do-
mains (Bagnini and Cavagli? 2000) and can be 
outlined in the following two steps:  
1. use a WordNet-based WSD algorithm to 
assign a sense to each word in the input 
text, e.g. doctor  doctor#n#1 
2. use WordNet Domains to map disam-
biguated words into the subject domain 
associated with the word, e.g. doc-
tor#n#1doctor#n#1#MEDICINE. 
2.1 WordNet Domains 
WordNet Domains is an extension of WordNet 
(http://wordnet.princeton.edu/) where synonym 
sets have been annotated with one or more sub-
ject domain labels, as shown in Figure 1. Subject 
domains provide an interesting and useful classi-
fication which cuts across part of speech and 
WordNet sub-hierarchies. For example, doc-
tor#n#1 and operate#n#1 both have sub-
ject domain MEDICINE, and SPORT includes both 
athlete#n#1 with top hypernym life-
form#n#1 and sport#n#1 with  top hy-
pernym act#n#2.  
2.2 Word Sense Disambiguation 
To assign a sense to each word in the input text, 
we used the WSD algorithm presented in San-
filippo et al (2006). This WSD algorithm is 
based on a supervised classification approach 
that uses SemCor1 as training corpus. The algo-
rithm employs the OpenNLP MaxEnt imple-
mentation of the maximum entropy 
classification algorithm (Berger et al 1996) to 
develop word sense recognition signatures for 
each lemma which predicts the most likely sense 
for the lemma according to the context in which 
the lemma occurs. 
Following Dang & Palmer (2005) and Ko-
homban & Lee (2005), Sanfilippo et al (2006) 
use contextual, syntactic and semantic informa-
tion to inform our verb class disambiguation 
system.  
? Contextual information includes the verb 
under analysis plus three tokens found on 
each side of the verb, within sentence 
boundaries. Tokens included word as well 
as punctuation. 
? Syntactic information includes grammatical 
dependencies (e.g. subject, object) and mor-
pho-syntactic features such as part of 
speech, case, number and tense.  
? Semantic information includes named entity 
types (e.g. person, location, organization) 
and hypernyms. 
We chose this WSD algorithm as it provides 
some of the best published results to date, as the 
comparison with top performing WSD systems 
in Senseval3 presented in Table 1 shows---see 
http://www.senseval.org and Snyder & Palmer 
(2004) for terms of reference on Senseval3. 
                                                          
1
 http://www.cs.unt.edu/~rada/downloads.html. 
142
System Precision Fraction of 
Recall 
Sanfilippo et al  2006 61% 22% 
GAMBL 59.0% 21.3% 
SenseLearner 56.1% 20.2% 
Baseline 52.9% 19.1% 
Table 1: Results for verb sense disambiguation on 
Senseval3 data, adapted from Sanfilippo et al (2006). 
3 Evaluation 
To evaluate our WDD approach, we used both 
the SemCor and Senseval3 data sets. Both cor-
pora were stripped of their sense annotations and 
processed with an extension of the WSD algo-
rithm of Sanfilippo et al (2006) to assign a 
WordNet sense to each noun, verb and adjective. 
The extension consisted in extending the train-
ing data set so as to include a selection of 
WordNet examples (full sentences containing a 
main verb) and the Open Mind Word Expert 
corpus (Chklovski and Mihalcea 2002).  
The original hand-coded word sense annota-
tions of the SemCor and Senseval3 corpora and 
the word sense annotations assigned by the 
WSD algorithm used in this study were mapped 
into subject domain annotations using WordNet 
Domains, as described in the opening paragraph 
of section 2 above. The version of the SemCor 
and Senseval3 corpora where subject domain 
annotations were generated from hand-coded 
word senses served as gold standard.  A baseline 
for both corpora was obtained by assigning to 
each lemma the subject domain corresponding to 
sense 1 of the lemma.  
WDD results of a tenfold cross-validation for 
the SemCor data set are given in Table 2. Accu-
racy is high across nouns, verbs and adjectives.2 
To verify the statistical significance of these re-
sults against the baseline, we used a standard 
proportions comparison test (see Fleiss 1981, p. 
30). According to this test, the accuracy of our 
system is significantly better than the baseline.  
The high accuracy of our WDD algorithm is 
corroborated by the results for the Senseval3 
data set in Table 3. Such corroboration is impor-
tant as the Senseval3 corpus was not part of the 
data set used to train the WSD algorithm which 
provided the basis for subject domain assign-
                                                          
2
 We have not worked on adverbs yet, but we expect com-
parable results. 
ment. The standard comparison test for the Sen-
seval3 is not as conclusive as with SemCor. This 
is probably due to the comparatively smaller size 
of the Senseval3 corpus. 
 
 Nouns Verbs Adj.s Overall 
Accuracy 0.874 0.933 0.942 0.912 
Baseline 0.848 0.927 0.932 0.897 
p-value 4.6e-54 1.4e-07 5.5e-08 1.4e-58 
Table 2: SemCor WDD results. 
 
 Nouns Verbs Adj.s Overall 
Accuracy 0.797 0.908 0.888 0.848 
Baseline 0.783 0.893 0.862 0.829 
p-value 0.227 0.169 0.151 0.048 
Table 3: Senseval3 WDD results. 
4 Comparison with Previous WDD 
Work 
Our WDD algorithm compares favorably with 
the approach explored in Bagnini and Strap-
parava (2000), who report 0.82 p/r in the WDD 
tasks for a subset of nouns in SemCor.  
Suarez and Palomar (2002) report WDD re-
sults of 78.7% accuracy for nouns against a 
baseline of 68.7% accuracy for the same data 
set. As in the present study, Suarez and Palomar 
derive the baseline by assigning to each lemma 
the subject domain corresponding to sense 1 of 
the lemma. Unfortunately, a meaningful com-
parison with Suarez and Palomar (2002) is not 
possible as they use a different data set, the DSO 
corpus.3 We are currently working on repeating 
our study with the DSO corpus and will include 
the results of this evaluation in the final version 
of the paper to achieve commensurability with 
the results reported by Suarez and Palomar. 
5 Conclusions and Further Work 
Current approaches to WDD have assumed that 
special purpose algorithms are needed to model 
the WDD task. We have shown that very com-
petitive and perhaps unrivaled results (pending 
on evaluation of our WDD algorithm with the 
DSO corpus) can be obtained using WSD as the 
basis for subject domain assignment. This im-
provement in WDD performance can be used to 
                                                          
3
 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?cata 
logId=LDC97T12.  
143
obtain further gains in WSD accuracy, following 
Wilks and Stevenson (1998), Magnini et al 
(2001) and Gliozzo et al (2004). A more accu-
rate WSD model will in turn yield yet better 
WDD results, as demonstrated in this paper. 
Consequently, further improvements in accuracy 
for both WSD and WDD can be expected 
through a bootstrapping cycle where WDD re-
sults are fed as input to the WSD process, and 
the resulting improved WSD model is then used 
to achieve better WDD results. We intend to 
explore this possibility in future extensions of 
this work. 
Acknowledgements 
We would like to thank Paul Whitney for help 
with the evaluation of the results presented in 
Section 3.  
References  
Berger, A., S. Della Pietra and V. Della Pietra (1996) 
A Maximum Entropy Approach to Natural Lan-
guage Processing. Computational Linguistics, vol-
ume 22, number 1, pages 39-71. 
Chklovski, T. and R. Mihalcea (2002) Building a 
Sense Tagged Corpus with Open Mind Word Ex-
pert. Proceedings of the ACL 2002 Workshop on 
"Word Sense Disambiguation: Recent Successes 
and Future Directions, Philadelphia, July 2002, pp. 
116-122. 
Dang, H. T. and M. Palmer (2005) The Role of Se-
mantic Roles in Disambiguating Verb Senses. In 
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Ar-
bor MI, June 26-28, 2005.  
Fleiss, J. L. (1981) Statistical Methods for Rates and 
Proportions. 2nd edition. New York: John Wiley 
& Sons. 
Gliozzo, A., C. Strapparava, I. Dagan (2004) Unsu-
pervised and Supervised Exploitation of Semantic 
Domains in Lexical Disambiguation. Computer 
Speech and Language,18(3), Pages 275-299. 
Kohomban, U. and  W. Lee (2005) Learning seman-
tic classes for word sense disambiguation. In Pro-
ceedings of the 43rd Annual meeting of the 
Association for Computational Linguistics, Ann 
Arbor, MI.   
Magnini, B., Cavagli?, G. (2000) Integrating Subject 
Field Codes into WordNet. Proceedings of LREC-
2000, Second International Conference on Lan-
guage Resources and Evaluation, Athens, Greece, 
31 MAY- 2 JUNE 2000, pp. 1413-1418. 
Magnini, B., Strapparava C. (2000) Experiments in 
Word Domain Disambiguation for Parallel Texts. 
Proceedings of the ACL Workshop on Word 
Senses and Multilinguality, Hong-Kong, October 
7, 2000, pp. 27-33 
Magnini, B., C. Strapparava, G. Pezzulo and A. 
Gliozzo (2001) Using Domain Information for 
Word Sense Disambiguation. In Proceeding of 
SENSEVAL-2: Second International Workshop on 
Evaluating Word Sense Disambiguation Systems, 
pp. 111-114, 5-6 July 2001, Toulouse, France. 
Magnini, B., C. Strapparava, G. Pezzulo and A. 
Gliozzo (2002) The Role of Domain Information 
in Word Sense Disambiguation. Natural Language 
Engineering, 8(4):359?373. 
Mowatt, D. (1999) Types of Semantic Information 
Necessary in a Machine Translation Lexicon. Con-
f?rence TALN, Carg?se, pp. 12-17. 
Procter, Paul (Ed.) (1978) Longman Dictionary o 
Contemporary English. Longman Group Ltd., Es-
sex, UK. 
Sanfilippo, A. (1998) Ranking Text Units According 
to Textual Saliency, Connectivity and Topic Apt-
ness. COLING-ACL 1998: 1157-1163. 
Sanfilippo, A., S. Tratz, M. Gregory, A.Chappell, P. 
Whitney, C. Posse, P. Paulson, B. Baddeley, R. 
Hohimer, A. White. (2006) Automating Ontologi-
cal Annotation with WordNet. Proceedings of the 
3rd Global WordNet Conference, Jeju Island, 
South Korea, Jan 19-26 2006.  
Snyder, B.  and M. Palmer. 2004. The English all-
words task. SENSEVAL-3: Third International 
Workshop on the Evaluation of Systems for the 
Semantic Analysis of Text. Barcelona, Spain.  
Su?rez, A., Palomar, M. (2002) Word sense vs. word 
domain disambiguation: a maximum entropy ap-
proach. In Sojka P., Kopecek I., Pala K., eds.: 
Text, Speech and Dialogue (TSD 2002). Volume 
2448 of Lecture Notes in Artificial Intelligence, 
Springer, (2002) 131?138. 
Wilks, Y. and Stevenson, M. (1998) Word sense dis-
ambiguation using optimised combinations of 
knowledge sources. Proceedings of the 17th inter-
national conference on Computational Linguistics, 
pp. 1398?1402. 
144
Proceedings of NAACL HLT 2007, Companion Volume, pages 169?172,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A High Accuracy Method for Semi-supervised Information Extraction 
 
 
Stephen Tratz Antonio Sanfilippo 
Pacific Northwest National Laboratory Pacific Northwest National Laboratory 
Richland, WA 99352 Richland, WA 99352 
stephen.tratz@pnl.gov antonio.sanfilippo@pnl.gov 
 
 
 
 
Abstract 
Customization to specific domains of dis-
course and/or user requirements is one of 
the greatest challenges for today?s Infor-
mation Extraction (IE) systems. While 
demonstrably effective, both rule-based 
and supervised machine learning ap-
proaches to IE customization pose too 
high a burden on the user. Semi-
supervised learning approaches may in 
principle offer a more resource effective 
solution but are still insufficiently 
accurate to grant realistic application. We 
demonstrate that this limitation can be 
overcome by integrating fully-supervised 
learning techniques within a semi-
supervised IE approach, without 
increasing resource requirements. 
1 Introduction 
Customization to specific discourse domains 
and/or user requirements is one of the greatest 
challenges for today?s Information Extraction (IE) 
systems. While demonstrably effective, both rule-
based and supervised machine learning approaches 
to IE customization require a substantial 
development effort. For example, Aone and 
Ramos-Santacruz (2000) present a rule-based IE 
system which handles 100 types of relations and 
events. Building such a system requires the manual 
construction of numerous extraction patterns 
supported by customized ontologies. Soderland 
(1999) uses supervised learning to induce a set of 
rules from hand-tagged training examples. While 
Sonderland suggests that the human effort can be 
reduced by interleaving learning and manual 
annotation activities, the creation of training data 
remains an onerous task. 
To reduce the knowledge engineering burden on 
the user in constructing and porting an IE system, 
unsupervised learning has been utilized, e.g. Riloff 
(1996), Yangarber et al (2000), and Sekine (2006). 
Banko et al (2007) present a self-supervised 
system that aims to avoid the manual IE 
customization problem by extracting all possible 
relations of interest from text. Stevenson and 
Greenwood (2005) propose a weakly supervised 
approach to sentence filtering that uses semantic 
similarity and bootstrapping to acquire IE patterns.  
Stevenson?s and Greenwood?s approach provides 
some of the best available results in weakly 
supervised IE to date, with 0.58 F-measure. While 
very good, an F-measure of 0.58 does not provide 
sufficient reliability to grant use in a production 
system.  
In this paper, we show that it is possible to 
provide a significant improvement over 
Stevenson?s and Greenwood?s results, without 
increasing resource requirements, by integrating 
fully-supervised learning techniques within a 
weakly supervised IE approach. 
1.1 Learning Algorithm 
Our method is modeled on the approach developed 
by Stevenson and Greenwood (2005) but uses a 
different technique for ranking candidate patterns. 
Stevenson?s and Greenwood?s algorithm takes as 
data inputs a small set of initial seed patterns and a 
corpus of documents, and uses any of several 
semantic similarity measures (Resnik, 1995; Jiang 
and Conrath, 1997; Patwardhan et al, 2003) to 
iteratively identify patterns in the document corpus 
169
that bear a strong resemblance to the seed patterns. 
After each iteration, the top-ranking candidate 
patterns are added to the seed patterns and 
removed from the corpus. Our approach differs 
from that of Stevenson and Greenwood in that we 
use a supervised classifier to rank candidate 
patterns.  This grants our system greater robustness 
and flexibility because the weight of classification 
features can be automatically determined within a 
supervised classification approach. 
In building supervised classifiers to rank 
candidate patterns at each iteration, we use both 
positive and negative training examples. Instead of 
creating manually annotated training examples, we 
follow an active learning approach where training 
examples are automatically chosen by ranking 
candidate patterns in terms of cosine similarity 
with the seed patterns. More specifically, we  
select patterns that have the lowest similarity with 
seed patterns to be the negative training examples. 
We hypothesized that these negative examples 
would contain many of the uninformative features 
occurring throughout the corpus and that using 
these examples would enable the classifier to 
determine that these features would not be useful.  
The pattern learning approach we propose 
includes the following steps. 
1. An unannotated corpus is required as input. 
For each sentence, a set of features is 
extracted. This information becomes Scand, the 
set of all candidate patterns. 
2. The user defines a set of seed patterns, Sseed. 
These patterns contain features expected to be 
found in a relevant sentence.  
3. The cosine measure is used to determine the 
distance between the patterns in Sseed and Scand. 
The patterns in Scand are then ordered by their 
lowest distance to a member of Sseed. 
4. The ? highest ranked patterns in Scand are 
added to Spos, the set of positive training 
examples. 
5. Sseed and Sacc are added to Spos. Sneg, the set of 
negative training examples is constructed from 
?+iteration*? of the lowest ranked patterns in 
Scand. Then, a maximum entropy classifier is 
built using Spos and Sneg as training data.  
6. The classifier is used to score each pattern in 
Scand. Scand is then sorted by these scores. 
7. The top ? patterns in Scand are added to Sacc and 
removed from Scand. 
8. If a suitable stopping point has been reached, 
the process ends. Otherwise, Spos and Sneg are 
emptied and the process continues at step 6. 
We set ? to 5, ? to 20, ? to 15, ? to 5, and used the 
following linguistic processing tools: (1) the 
OpenNLP library (opennlp.sourceforge.net) for 
sentence splitting and named-entity recognition,  
and (2) Connexor for syntactic parsing 
(Tapanainen and J?rvinen, 1997). For the 
classifier, we used the OpenNLP MaxEnt 
implementation (maxent.sourceforge.net) of the 
maximum entropy classification algorithm (Berger 
et al 1996).  We used the MUC-6 data set as the 
testing ground for our proposed approach. 
1.2 Description of Features Used 
Stevenson and Greenwood (2005) use subject-
verb-object triples for their features. We use a 
richer feature set. Our system can easily 
accommodate more features because we let the 
maximum entropy classifier determine the weight 
for the features. Stevenson?s and Greenwood?s 
approach determines weights using semantic 
similarity and would require significant changes to 
take into account various other features, especially 
those for which a WordNet (Fellbaum, 1998) 
similarity score is not available. 
We use single tokens, token combinations, and 
semantic information to inform our IE pattern 
extraction system. Lexical items marked by the 
named-entity recognition system as PERSON or 
ORGANIZATION are replaced with ?person? and 
?organization?, respectively. Number tokens are 
replaced with ?numeric?. Single Token Features 
include: 
? All words in the sentence and all hypernyms of 
the first sense of the word with attached part-
of-speech 
? All words in the sentence with attached 
dependency 
? The verb base of each nominalization and the 
verb?s first sense hypernyms are included. 
Token Combinations include: 
? All bigrams from the sentence 
? All subject-object pairs 
? All parent-child pairs from the parse tree 
170
? A specially marked copy of the parent-child 
pairs where the main verb is the parent. 
We also added semantic features indicating if a 
PERSON or ORGANIZATION was detected 
within the sentence boundaries. Table1 provides an 
example where a simple sentence is mapped into 
the set of features we have just described. 
 
Alan G. Spoon, 42, will succeed 
Mr. Graham as president of the 
company. 
 
 
Single Token Features 
With attached dependencies: 
attr:person, subj:person, mod:numeric, v-ch:will, 
main:succeed, obj:person, copred:as, pcomp:president, 
mod:of, det:the, pcomp:company 
With part-of-speech tags: 
n:person, v:succeed, v:will, dt:the, n:company, 
n:institution, n:social_group, n:group, n:organization, 
n:person, n:president, n:executive, n:corporat-
e_executive, n:administrator, n:head, n:leader, n:orga-
nism, n:living_thing, n:object, n:entity, num:numeric, 
abbr:person, prp:as, prp:of, v:control, v:declare, 
v:decree, v:express, v:ordain, v:preside, v:state 
Token Combinations 
Bigrams: 
person+comma, comma+numeric, numeric+comma, 
comma+will, will+succeed, succeed+person, person+as, 
as+president, president+of, of+the, the+company 
Subject Object Pairs: 
sop:person+person 
Parent-Child Pairs: 
pc:person+person, pc:person+numeric, pc:will+person, 
pc:succeed+will, pc:succeed+person, pc:succeed+as, 
pc:as+president, pc:president+of, pc:of+company, 
pc:company+the 
Main Verb Parent-Child Pairs: 
mvpc:succeed+person, mvpc:succeed+will, mv-
pc:succeed+as 
Semantic Features 
hasOrganization, hasPerson 
Table 1: Feature representation of a simple sentence. 
 
The seeds we used are adapted from the seed 
patterns employed by Stevenson and Greenwood. 
As shown in Table 2, only a subset of the features 
described above is used in the seed patterns. 
2 Evaluation 
We used the document collection which was 
initially developed for the Sixth Message 
Understanding Conference (MUC-6) as ground 
truth data set to evaluate our approach. The MUC-
6 corpus (www.ldc.upenn.edu) is composed of 100 
Wall Street Journal documents written during 1993 
and 1994.  Our task was to detect sentences which 
included management succession patterns, such as 
those shown in Table 2.  
 
1: subj:organization, main:appoint, obj:person, hasPers-
on, hasOrganization 
2: subj:organization, main:elect, obj:person, hasOrgani-
zation, hasPerson 
3: subj:organization, main:promote, obj:person, hasOrg-
anization, hasPerson 
4: subj:organization, main:name, obj:person, hasOrgani-
zation, hasPerson 
5: subj:person, main:resign, hasPerson 
6: subj:person, main:depart, hasPerson 
7: subj:person, main:quit, hasPerson 
Table 2: Feature representation of seed patterns. 
 
     The version of the MUC-6 corpus produced by 
Soderland (1999) provided us with a specification 
of succession patterns at the sentence level, but as 
shown in Table 3 did not include the source text.  
We reconstructed the original text by  
automatically aligning the succession patterns in 
the sentence structures in Soderland?s version of 
the MUC-6 corpus with the sentences in the 
original MUC-6 corpus. This alignment produced a 
set of 1581 sentences, of which 134 contained 
succession patterns.  
 
@S[ 
  {SUBJ  @CN[ FOX ]CN } 
  {VB  NAMED @NAM } 
  {OBJ  @PN[ LUCILLE S. SALHANY ]PN , @PS[ 
CHAIRMAN ]PS OF @CN[ FOX INC. ]CN 'S 
TELEVISION PRODUCTION ARM , } 
  {REL_V  TO SUCCEED @SUCCEED HIM . } 
]@S 9301060123-5 
@@TAGS Succession {PersonIn @PN[ LUCILLE S. 
SALHANY ]PN}+ {Post @PS[ CHAIRMAN ]PS}+ 
{Org @CN[ FOX INC. ]CN}_  @@COVERED_BY 
@@ENDTAGS 
Table 3: Data sample from Soderland test set. 
 
As shown in Figure 1, our best score of 0.688 F-
measure was obtained on the 36th iteration; at the 
end of this iteration, our algorithm selected 180 
sentences including 108 of the sentences that 
contained succession patterns. This is a significant 
improvement over the 0.58 F-measure score 
171
reported by Stevenson and Greenwood (2005) for 
the same task. The use of a supervised 
classification approach to the ranking of candidate 
patterns with a richer feature set were the two 
determinant factors in achieving such 
improvement. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 10 20 30 40 50 60 70 80
Iteration
F-
m
ea
su
re
 
Figure 1: Evaluation results with MUC-6 data. 
3 Conclusions 
Our results show a substantial improvement over 
previous efforts in weakly supervised IE methods, 
suggesting that weakly supervised methods can be 
made to rival rule-based or fully supervised 
approaches both in resource effectiveness and 
accuracy. We plan to verify the strength of our 
approach evaluating against other ground truth data 
sets. We also plan to detail how the various 
features in our classification model contribute to 
ranking of candidate patterns.  An additional area 
of envisioned improvement regards the use of a 
random sub selection of negative candidate 
patterns as training samples to counteract the 
presence of sentence fragments among low-
ranking candidate patterns. Finally, we intend to 
evaluate the benefit of having a human in the loop 
in the first few iterations to filter out patterns 
chosen by the system. 
References  
C. Aone and M. Ramos-Santacruz. 2000. REES: A 
Large-Scale Relation and Event Extraction System, 
pages 76-83, In Proceedings of the 6th Applied 
Natural Language Processing Conference (ANLP 
2000), Seattle. 
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open Information Extraction 
from the Web. In Proceedings of the 20th 
International Joint Conference on Artificial 
Intelligence (IJCAI 2007). Hyderabad, India. 
A. Berger, S. Della Pietra and V. Della Pietra (1996) A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, volume 22, 
number 1, pages 39-71. 
C. Fellbaum, editor. 1998. WordNet: An Electronic 
Lexical Database and some of its Applications. MIT 
Press, Cambridge, MA. 
J. Jiang and D. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. In 
Proceedings of the 10th International Conference on 
Research in Computational Linguistics, Taiwan. 
S. Patwardhan, S. Banerjee, and T. Pederson. 2003. 
Using measures of semantic relatedness for word 
sense disambiguation. In Proceedings of the 4th 
International Conferences on Intelligent Text 
Processing and Computational Linguistics, pages 
241-257, Mexico City. 
P. Resnik. 1995. Using Information Content to evaluate 
Semantic Similarity in a Taxonomy. In Proceedings 
of the 14th International Joint Conference on 
Artificial Intelligence (IJCAI-95), pages 448-452, 
Montreal, Canada. 
E. Riloff and R. Jones. 1999. Learning Dictionaries for 
Information Extraction by Multi-level Bootstrapping.  
In Proceedings of the 16th National Conference on 
Artificial Intelligence. Orlando, Florida. 
S. Sekine. 2006. On-Demand Information Extraction. In 
Proceedings of the COLING/ACL 2006 Main 
Conference Poster Sessions. Sydney, Australia. 
S. Soderland. 1999. Learning Information Extraction 
Rules for Semi-structured and free text. Machine 
Learning, 31(1-3):233-272. 
M. Stevenson and M. A. Greenwood. 2005. A Semantic 
Approach to IE Pattern Induction. In Proceedings of 
the 43rd Annual Meeting of the ACL (ACL 05), Ann 
Arbor, Michigan.    
P. Tapanainen and Timo J?rvinen. 1997. A non-
projective dependency parser. In Proceedings of the 
5th Conference on Applied Natural Language 
Processing, pages 64?71, Washington D.C. 
Association for Computational Linguistics. 
R. Yangarber, R. Grishman, P. Tapanainen, and S. 
Huttunen. 2000. Automatic acquisition of domain 
knowledge for information extraction. In 
Proceedings of the 18th International Conference of 
Computational Linguistics (COLING 2002), Taipei. 
172
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 25?32,
New York City, June 2006. c?2006 Association for Computational Linguistics
 
Integrating Ontological Knowledge and Textual Evidence in Estimating 
Gene and Gene Product Similarity 
 
Antonio Sanfilippo, Christian Posse, Banu Gopalan, Stephen Tratz, Michelle Gregory 
Pacific Northwest National Laboratory 
Richland, WA 99352 
{Antonio.Sanfilippo, Christian.Posse, Banu.Gopalan, Stephen.Tratz, 
Michelle.Gregory}@pnl.gov  
 
 
  
 
Abstract 
With the rising influence of the Gene On-
tology, new approaches have emerged 
where the similarity between genes or 
gene products is obtained by comparing 
Gene Ontology code annotations associ-
ated with them. So far, these approaches 
have solely relied on the knowledge en-
coded in the Gene Ontology and the gene 
annotations associated with the Gene On-
tology database. The goal of this paper is 
to demonstrate that improvements to these 
approaches can be obtained by integrating 
textual evidence extracted from relevant 
biomedical literature. 
1 Introduction 
The establishment of similarity between genes and 
gene products through homology searches has be-
come an important discovery procedure that biolo-
gists use to infer structural and functional 
properties of genes and gene products?see Chang 
et al (2001) and references therein. With the rising 
influence of the Gene Ontology1 (GO), new ap-
proaches have emerged where the similarity be-
tween genes or gene products is obtained by 
comparing GO code annotations associated with 
them. The Gene Ontology provides three orthogo-
nal networks of functional genomic concepts struc-
                                                          
1 http://www.geneontology.org. 
tured in terms of semantic relationships such as 
inheritance and meronymy, which encode biologi-
cal process (BP), molecular function (MF) and cel-
lular component (CC) properties of genes and gene 
products. GO code annotations explicitly relate 
genes and gene products in terms of participation 
in the same/similar biological processes, presence 
in the same/similar cellular components and ex-
pression of the same/similar molecular functions. 
Therefore, the use of GO code annotations in es-
tablishing gene and gene product similarity pro-
vides significant added functionality to methods 
such as BLAST (Altschul et al 1997) and FASTA 
(Pearson and Lipman 1988) where gene and gene 
product similarity is calculated using string-based 
heuristics to select maximal segment pair align-
ments across gene and gene product sequences to 
approximate the Smith-Waterman algorithm 
(Smith and Waterman 1981). 
Three main GO-based approaches have emerged 
so far to compute gene and gene product similarity. 
One approach assesses GO code similarity in terms 
of shared hierarchical relations within each gene 
ontology (BP, MF, or CC) (Lord et al 2002, 2003; 
Couto et al 2003; Azuaje et al 2005).  For exam-
ple, the relative semantic closeness of two biologi-
cal processes would be determined by the 
informational specificity of the most immediate 
parent that the two biological processes share in 
the BP ontology. The second approach establishes 
GO code similarity by leveraging associative rela-
tions across the three gene ontologies (Bodenreider 
et al 2005). Such associative relations make pre-
dictions such as which cellular component is most 
likely to be the location of a given biological proc-
25
ess and which molecular function is most likely to 
be involved in a given biological process. The third 
approach computes GO code similarity by combin-
ing hierarchical and associative relations (Posse et 
al. 2006). 
Several studies within the last few years 
(Andrade et al 1997, Andrade 1999, MacCallum et 
al. 2000, Chang at al. 2001) have shown that the 
inclusion of evidence from relevant scientific lit-
erature improves homology search. It is therefore 
highly plausible that literature evidence can also 
help improve GO-based approaches to gene and 
gene product similarity. Sanfilippo et al (2004) 
propose a method for integrating literature evi-
dence within an early version of the GO-based 
similarity algorithm presented in Posse et al 
(2006). However, no effort has been made so far in 
evaluating the potential contribution of textual evi-
dence extracted from relevant biomedical literature 
for GO-based approaches to the computation of 
gene and gene product similarity. The goal of this 
paper is to address this gap with specific reference 
to the assessment of protein similarity. 
2 Background 
GO-based similarity methods that focus on meas-
uring intra-ontological relations have adopted the 
information theoretic treatment of semantic simi-
larity developed in Natural Language Process-
ing?see Budanitsky (1999) for an extensive 
survey. An example of such a treatment is given by 
Resnik (1995), who defines semantic similarity 
between two concept nodes c1 c2 in a graph as the 
information content of the least common su-
perordinate (lcs) of c1 and c2, as shown in (1). The 
information content of a concept node c, IC(c), is 
computed as -log p(c) where p(c) indicates the 
probability of encountering instances of c in a spe-
cific corpus. 
(1)     
)),c p(lcs(c
)),c IC(lcs(c) ,csim(c
21log
2121
?=
==
Jiang and Conrath (1997) provide a refinement of 
Resnik?s measure by factoring in the distance from 
each concept to the least common superordinate, as 
shown in (2).2
                                                          
2 Jiang and Conrath (1997) actually define the distance be-
tween two concepts nodes c1 c2, e.g.     
 )), c IC(lcs(c ) -  IC(c)  IC(c) , cdist(c 2122121 ?+=
(2)
)),cIC(lcs(c) -IC(c)IC(c ) ,csim(c 21221
121 ?+=  
Lin (1998) provides a slight variant of Jiang?s and 
Conrath?s measure, as indicated in (3).  
(3) 
)  IC(c) IC(c
)), c IC(lcs(c 
 ) ,csim(c
21
212
21 +
?=  
The information theoretic approach is very well 
suited to assess GO code similarity since each gene 
subontology is formalized as a directed acyclic 
graph. In addition, the GO database3 includes nu-
merous curated GO annotations which can be used 
to calculate the information content of each GO 
code with high reliability. Evaluations of this 
methodology have yielded promising results. For 
example, Lord et al (2002, 2003) demonstrate that 
there is strong correlation between GO-based simi-
larity judgments for human proteins and similarity 
judgments obtained through BLAST searches for 
the same proteins. Azuaje et al (2005) show that 
there is a strong connection between the degree of 
GO-based similarity and the expression correlation 
of gene products. 
As Bodenreider et al (2005) remark, the main 
problem with the information theoretic approach to 
GO code similarity is that it does not take into ac-
count associative relations across the gene ontolo-
gies. For example, the two GO codes 0050909 
(sensory perception of taste) and 0008527 (taste 
receptor activity) belong to different gene ontolo-
gies (BP and MF), but they are undeniably very 
closely related. The information theoretic approach 
would simply miss associations of this kind as it is 
not designed to capture inter-ontological relations.  
Bodenreider et al (2005) propose to recover as-
sociative relations across the gene ontologies using 
a variety of statistical techniques which estimate 
the similarity of two GO codes inter-ontologically 
in terms of the distribution of the gene product an-
notations associated with the two GO codes in the 
GO database. One such technique is an adaptation 
of the vector space model frequently used in In-
formation Retrieval (Salton et al 1975), where 
                                                                                           
For ease of exposition, we have converted Jiang?s and Con-
rath?s semantic distance measure to semantic similarity by 
taking its inverse, following Pedersen et al (2005). 
3 http://www.godatabase.org/dev/database.  
26
each GO code is represented as a vector of gene-
based features weighted according to their distribu-
tion in the GO annotation database, and the simi-
larity between two GO codes is computed as the 
cosine of the vectors for the two codes. 
The ability to measure associative relations 
across the gene ontologies can significantly aug-
ment the functionality of the information theoretic 
approach so as to provide a more comprehensive 
assessment of gene and gene product similarity. 
However, in spite of their complementarities, the 
two GO code similarity measures are not easily 
integrated. This is because the two measures are 
obtained through different methods, express dis-
tinct senses of similarity (i.e. intra- and inter-
ontological) and are thus incomparable.  
Posse et al (2006) develop a GO-based similar-
ity algorithm?XOA, short for Cross-Ontological 
Analytics?capable of combining intra- and inter-
ontological relations by ?translating? each associa-
tive relation across the gene ontologies into a hier-
archical relation within a single ontology. More 
precisely, let c1 denote a GO code in the gene on-
tology O1 and c2 a GO code in the gene ontology 
O2. The XOA similarity between c1 and c2 is de-
fined as shown in (4), where4
? cos(ci,cj) denotes the cosine associative meas-
ure proposed by Bodenreider et al (2005) 
? sim(ci,cj) denotes any of the three intra-
ontological semantic similarities described 
above, see (1)-(3) 
? maxci in Oj {f(ci)} denotes the maximum of the 
function f() over all GO codes ci in the gene 
ontology Oj.  
The major innovation of the XOA approach is to 
allow the comparison of two nodes c1, c2 across 
distinct ontologies O1, O2 by mapping c1 into its 
closest node c4 in O2 and c2 into its closest node 
c3 in O1. The inter-ontological semantic similarity 
between c1 and c2 can be then estimated from the 
intra-ontological semantic similarities between c1-
                                                          
4 If c1 and c2 are in the same ontology, i.e. O1=O2, then 
xoa(c1,c2) is still computed as in (4). In most cases, the 
maximum in (4) would be obtained with c3 = c2 and c4 = c1 
so that  XOA(c1,c2) would simply be computed as sim(c1,c2). 
However, there are situations where there exists a GO code c3 
(c4) in the same ontology which 
? is highly associated with c1 (c2),  
?  is semantically close to c2 (c1), and  
?  leads to a value for  sim(c1,c3) x cos(c2,c3)  ((sim(c2,c4) 
x cos(c1,c4)) that is higher than sim(c1,c2). 
c3 and c2-c4, using multiplication with the associa-
tive relations between c2-c3 and c1-c4 as a score 
enrichment device. 
 
(4)  
??
?
?
??
?
?
?
??
?
?
??
?
?
?
?
?
??
??
?
??
??
?
??
??
?
??
??
?
=
), c(c
), c(c
), c(c
), c(c
Oinc
Oinc
,
), c(c
41cos
42sim
32cos
31sim
XOA
24
13
max
max
max21  
 
Posse et al (2006) show that the XOA similarity 
measure provides substantial advantages. For ex-
ample, a comparative evaluation of protein similar-
ity, following the benchmark study of Lord et al 
(2002, 2003), reveals that XOA provides the basis 
for a better correlation with protein sequence simi-
larities as measured by BLAST bit score than any 
intra-ontological semantic similarity measure. The 
XOA similarity between genes/gene products de-
rives from the XOA similarity between GO codes. 
Let GP1 and GP2 be two genes/gene products. Let 
c11,c12,?, c1n denote the set of GO codes associ-
ated with GP1 and c21, c22,?., c2m the set of GO 
codes associated with GP2. The XOA similarity 
between GP1 and GP2 is defined as in (5), where 
i=1,?,n and j=1,?,m.  
 
(5) XOA(GP1,GP2) = max {XOA(c1i, c2j)} 
 
The results of the study by Posse et al (2006) are 
shown in Table 1. Note that the correlation be-
tween protein similarities based on intra-
ontological similarity measures and BLAST bit 
scores in Table 1 is given for each choice of gene 
ontology (MF, BP, CC). This is because intra-
ontological similarity methods only take into ac-
count GO codes that are in the same ontology and 
can therefore only assess protein similarity from a 
single ontology viewpoint. By contrast, the XOA-
based protein similarity measure makes use of GO 
codes that can belong to any of the three gene on-
tologies and needs not be broken down by single 
ontologies, although the contribution of each gene 
ontology or even single GO codes can still be 
fleshed out, if so desired. 
Is it possible to improve on these XOA results 
by factoring in textual evidence? We will address 
this question in the remaining part of the paper. 
27
 
Semantic Similarity 
Measures 
Resnik Lin Jiang &  
Conrath 
Intra-ontological    
Molecular Function 0.307 0.301 0.296 
Biological Process 0.195 0.202 0.203 
Cellular Component 0.229 0.234 0.233 
XOA 0.405 0.393 0.368 
Table 1: Spearman rank order correlation coeffi-
cients between BLAST bit score and semantic 
similarities, calculated using a set of 255,502 pro-
tein pairs?adapted from Posse et al (2006). 
3 Textual Evidence Selection 
Our first step in integrating textual evidence into 
the XOA algorithm is to select salient information 
from biomedical literature germane to the problem. 
Several approaches can be used to carry out this 
prerequisite. For example, one possibility is to col-
lect documents relevant to the task at hand, e.g. 
through PubMed queries, and use feature weight-
ing and selection techniques from the Information 
Retrieval literature?e.g. tf*idf (Buckley 1985) and 
Information Gain (e.g. Yang and Pedersen 
1997)?to distill the most relevant information. An-
other possibility is to use Information Extraction 
algorithms tailored to the biomedical domain such 
as Medstract (http://www.medstract.org, Puste-
jovsky et al 2002) to extract entity-relationship 
structures of relevance. Yet another possibility is to 
use specialized tools such as GoPubMed (Doms 
and Schroeder 2005) where traditional keyword-
based capabilities are coupled with term extraction 
and ontological annotation techniques.  
In our study, we opted for the latter solution, us-
ing generic Information Retrieval techniques to 
normalize and weigh the textual evidence ex-
tracted. The main advantage of this choice is that 
tools such as GoPubMed provide very high quality 
term extraction at no cost. Less appealing is the 
fact that the textual evidence provided is GO-based 
and therefore does not offer information which is 
orthogonal to the gene ontology. It is reasonable to 
expect better results than those reported in this pa-
per if more GO-independent textual evidence were 
brought to bear. We are currently working on using 
Medstract as a source of additional textual evi-
dence. 
GoPubMed is a web server which allows users 
to explore PubMed search results using the Gene 
Ontology for categorization and navigation pur-
poses (available at http://www.gopubmed.org). As 
shown in Figure 1 below, the system offers the 
following functionality: 
? It provides an overview of PubMed search re-
sults by categorizing abstracts according to the 
Gene Ontology 
? It verifies its classification by providing an 
accuracy percentage for each 
? It shows definitions of Gene Ontology terms 
? It allows users to navigate PubMed search re-
sults by GO categories 
? It automatically shows GO terms related to the 
original query for each result  
? It shows query terms (e.g. ?Rab5? in the mid-
dle windowpane of Figure 1) 
? It automatically extracts terms from search 
results which map to GO categories (e.g. high-
lighted terms other than ?Rab5?  in the middle 
windowpane of Figure 1). 
In integrating textual evidence with the XOA al-
gorithm, we utilized the last functionality (auto-
matic extraction of terms) as an Information 
Extraction capability. Details about the term ex-
traction algorithm used in GoPubMed are given in 
Delfs et al (2004). In short, the GoPubMed term 
extraction algorithm uses word alignment strate-
gies in combination with stemming to match word 
sequences from PubMed abstracts with GO terms. 
In doing so, partial and discontinuous matches are 
allowed. Partial and discontinuous matches are 
weighted according to closeness of fit. This is indi-
cated by the accuracy percentages associated with 
GO in Figure 1 (right side). In this study we did 
not make use of these accuracy percentages, but 
plan to do so in the future. 
28
 
Figure 1: GoPubMed sample query for the ?rab5? protein. The abstracts shown are automatically proposed by the 
system after the user issues the protein query and then selects the GO term ?late endosome? (bottom left) as the 
discriminating parameter. 
  
Our data set consists of 2360 human protein 
pairs containing 1783 distinct human proteins. This 
data set was obtained as a 1% random sample of 
the human proteins used in the benchmark study of 
Posse et al (2006)?see Table 1.5 For each of the 
1783 human proteins, we made a GoPubMed query 
and retrieved up to 100 abstracts. We then col-
lected all the terms extracted by GoPubMed for 
each protein across the abstracts retrieved. Table 2 
provides an example of the output of this process. 
 
nutrient, uptake, carbohydrate, metabolism, affect-
ing, cathepsin, activity, protein, lipid, growth, rate, 
habitually, signal, transduction, fat, protein, cad-
herin, chromosomal, responses, exogenous, lactat-
ing, exchanges, affects, mammary, gland, ?. 
Table 2: Sample output of the GoPubMed term extrac-
tion process for the Cadherin-related tumor suppressor 
protein. 
                                                          
5 We chose such a small sample to facilitate the collection of 
evidence from GoPubMed, which is not yet fully automated. 
Our XOA approach is very scalable, and we do not anticipate 
any problem running the full protein data set of 255,502 pairs, 
once we fully automate the GoPubMed extraction process. 
4 Integrating Textual Evidence in XOA 
Using the output of the GoPubMed term extraction 
process, we created vector-based signatures for 
each of the 1783 proteins, where  
? features are obtained by stemming the terms 
provided by GoPubMed  
? the value for each feature is derived as the 
tf*idf  for the feature. 
We then calculated the similarity between each of 
the 2360 protein pairs as the cosine value of the 
two vector-based signatures associated with the 
protein pair. 
We tried two different strategies to augment the 
XOA score for protein similarity using the protein 
similarity values obtained as the cosine of the 
GoPubMed term-based signatures. The first strat-
egy adopts a fusion approach in which the two 
similarity measures are first normalized to be 
commensurable and then combined to provide an 
interpretable integrated model. A simple normali-
zation is obtained by observing that the Resnik?s 
information content measure is commensurable to 
29
the log of the text based cosine (LC). This leads us 
to the fusion model shown in (5) for XOA, based 
on Resnik?s semantic similarity measure (XOAR). 
(5)      Fusion(Resnik) = XOAR + LC 
We then observe that the XOA measures based on 
Resnik, Lin (XOAL) and Jiang & Conrath (XOAJC) 
are highly correlated (correlations exceed 0.95 on 
the large benchmarking dataset discussed in sec-
tion 2, see Table 1). This suggests the fusion model 
shown in (6), where the averages of the XOA 
scores are computed from the benchmarking data 
set. 
(6)      Fusion(Lin) =  
                XOAL + LC*Ave( XOAL)/Ave(XOAR) 
          Fusion(Jiang & Conrath) =  
               XOAJC + LC*Ave(XOAJC)/Ave(XOAR) 
The second strategy consists in building a predic-
tion model for BLAST bit score (BBS) using the 
XOA score and the log-cosine LC as predictors 
without the constraint of remaining interpretable. 
As in the previous strategy, a different model was 
sought for each of the three XOA variants. In each 
case, we restrict ourselves to cubic polynomial re-
gression models as such models are quite efficient 
at capturing complex nonlinear relationships be-
tween target and predictors (e.g. Weisberg 2005). 
More precisely, for each of the semantic similarity 
measures, we fit the regression model to BBS 
shown in (7), where the subscript x denotes either 
R, L or JC, and the coefficients a to h are found by 
maximizing the Spearman rank order correlations 
between BBS and the regression model. This 
maximization is automatically carried out by using 
a random walk optimization approach (Romeijn 
1992). The coefficients used in this study for each 
semantic similarity measure are shown in Table 3. 
 
(7)    a*XOAx + b*XOAx2 + c*XOAx +  d*LC 
        + e*LC2 + f*LC3 +  g*XOAx*LC 
5 Evaluation 
Table 4 summarizes the results for both strategies, 
comparing Spearman rank correlations between 
BBS and the models from the fusion and regres-
sion approaches with Spearman rank correlations 
between BBS and XOA alone. Note that the latter 
correlations are lower than the one reported in Ta-
ble 2 due to the small size of our sample (1% of the 
original data set, as pointed out above). P-values 
associated with the changes in the correlation val-
ues are also reported, enclosed in parentheses.  
 
 Resnik Lin Jiang & Conrath 
a -10684.43 2.83453e-05 0.2025174 
b 1.786986 -31318.0 -1.93974 
c 503.3746 45388.66 0.08461453 
d -3.952441 208.5917 4.939535e-06 
e 0.0034074 1.55518e-04 0.0033902 
f 1.4036e-05 9.972911e-05 -0.000838812 
g 713.769 -1.10477e-06 2.461781 
Table 3: Coefficients of the regression model maximiz-
ing Spearman rank correlation between BBS and the 
regression model for each of the three semantic similar-
ity measures. 
 
 XOA Fusion Regression 
Resnik 0.295 0.325 (>0.20) 0.388 (0.0008) 
Lin 0.274 0.301 (>0.20) 0.372 (0.0005) 
Jiang & 
Conrath 0.273 0.285 (>0.20) 0.348 (0.008) 
Table 4: Spearman rank order correlation coefficients 
between BLAST bit score BBS and XOA, BBS and the 
fusion model, and BBS and the regression model. P-
values for the differences between the augmented mod-
els and XOA alone are given in parentheses. 
 
An important finding from Table 4 is that inte-
grating text-based evidence in the semantic simi-
larity measures systematically improves the 
relationships between BLAST and XOA. Not sur-
prisingly, the fusion models yield smaller im-
provements. However, these improvements in the 
order of 3% for the Resnik and Lin variants are 
very encouraging, even though they are not statis-
tically significant. The regression models, on the 
other hand, provide larger and statistically signifi-
cant improvements, reinforcing our hypothesis that 
textual evidence complements the GO-based simi-
larity measures. We expect that a more sophisti-
cated NLP treatment of textual evidence will yield 
significant improvements even for the more inter-
pretable fusion models.   
Conclusions and Further Work 
Our early results show that literature evidence pro-
vides a significant contribution, even using very 
simple Information Extraction and integration 
methods such as those described in this paper. The 
employment of more sophisticated Information 
30
Extraction tools and integration techniques is 
therefore likely to bring higher gains.  
Further work using GoPubMed involves factor-
ing in the accuracy percentage which related ex-
tracted terms to their induced GO categories and 
capturing complex phrases (e.g. signal transduc-
tion, fat protein). We also intend to compare the 
advantages provided by the GoPubMed term ex-
traction process with Information Extraction tools 
created for the biomedical domain such as Med-
stract (Pustejovsky et al 2002), and develop a 
methodology for integrating a variety of Informa-
tion Extraction processes into XOA. 
References  
Altschul, S.F., T. L. Madden, A. A. Schaffer, J. Zhang, 
Z. Anang, W. Miller and D.J. Lipman (1997) Gapped 
BLAST and PSI-BLST: a new generation of protein 
database search programs.  Nucl. Acids Res. 25:3389-
3402. 
Andrade, M.A. (1999) Position-specific annotation of 
protein function based on multiple homologs. ISMB 
28-33. 
Andrade, M.A. and A. Valencia (1997) Automatic an-
notation for biological sequences by extraction of 
keywords from MEDLINE abstracts. Development 
of a prototype system. ISMB 25-32. 
Azuaje F., H. Wang and O. Bodenreider (2005) Ontol-
ogy-driven similarity approaches to supporting gene 
functional assessment. In Proceedings of the 
ISMB'2005 SIG meeting on Bio-ontologies 2005, 
pages 9-10.  
Bodenreider, O., M. Aubry and A. Burgun (2005) Non-
lexical approaches to identifying associative relations 
in the Gene Ontology. In Proceedings of Pacific 
Symposium on Biocomputing, pages 104-115. 
Buckley, C. (1985) Implementation of the SMART in-
formation retrieval system. Technical Report 85-686, 
Cornell University. 
Budanitsky, A. (1999) Lexical semantic relatedness and 
its application in natural language processing. Tech-
nical report CSRG-390, Department of Computer 
Science, University of Toronto. 
Chang, J.T., S. Raychaudhuri, and R.B. Altman (2001) 
Including biological literature improves homology 
search. In Proc. Pacific Symposium on Biocomput-
ing, pages 374?383. 
Couto, F. M., M. J. Silva and P. Coutinho (2003) Im-
plementation of a functional semantic similarity 
measure between gene-products. Technical Report, 
Department of Informatics, University of Lisbon, 
http://www.di.fc.ul.pt/tech-reports/03-29.pdf.  
Delfs, R., A. Doms, A. Kozlenkov, and M. Schroeder. 
(2004) GoPubMed: ontology based literature search 
applied to Gene Ontology and PubMed. In Proc. of 
German Bioinformatics Conference, Bielefeld, Ger-
many. LNBI Springer. 
Doms, A. and M. Schroeder (2005) GoPubMed: Explor-
ing PubMed with the GeneOntology. Nucleic Acids 
Research. 33: W783-W786; doi:10.1093/nar/gki470. 
Jiang J. and D. Conrath (1997) Semantic similarity 
based on corpus statistics and lexical taxonomy. In 
Proceedings of International Conference on Re-
search in Computational Linguistics, Taiwan. 
Romeijn, E.H. (1992) Global Optimization by Random 
Walk Sampling Methods. Tinbergen Institute Re-
search Series, Volume 32. Thesis Publishers, Am-
sterdam. 
Lin, D. (1998) An information-theoretic definition of 
similarity. In Proceedings of the 15th International 
Conference on Machine Learning, Madison, WI. 
Lord P.W., R.D. Stevens, A. Brass, and C.A. Goble 
(2002) Investigating semantic similarity measures 
across the Gene Ontology: the relationship between 
sequence and annotation. Bioinformatics 
19(10):1275-1283. 
Lord P.W., R.D. Stevens, A. Brass, and C.A. Goble 
(2003) Semantic similarity measures as tools for ex-
ploring the Gene Ontology. In Proceedings of Pacific 
Symposium on Biocomputing, pages 601-612. 
MacCallum, R. M., L. A. Kelley and Sternberg, M. J. 
(2000) SAWTED: structure assignment with text de-
scription--enhanced detection of remote homologues 
with automated SWISS-PROT annotation compari-
sons. Bioinformatics 16, 125-9. 
Pearson, W. R. and D. J. Lipman (1988) Improved tools 
for biological sequence analysis. In Proceedings of 
the National Academy of Sciences 85:2444-2448.  
Pedersen, T., S. Banerjee and S. Patwardhan (2005) 
Maximizing Semantic Relatedness to Perform Word 
Sense Disambiguation. University of Minnesota Su-
percomputing Institute Research Report UMSI 
2005/25, March. Available at http://www.msi.umn. 
edu/general/Reports/rptfiles/2005-25.pdf.  
Posse, C., A. Sanfilippo, B. Gopalan, R. Riensche, N. 
Beagley, and B. Baddeley (2006) Cross-Ontological 
Analytics: Combining associative and hierarchical re-
lations in the Gene Ontologies to assess gene product 
similarity. To appear in Proceedings of International 
31
Workshop on Bioinformatics Research and Applica-
tions. Reading, U.K. 
Pustejovsky, J., J. Casta?o, R. Saur?, A. Rumshisky, J. 
Zhang, W. Luo (2002) Medstract: Creating large-
scale information servers for biomedical libraries. 
ACL 2002 Workshop on Natural Language Process-
ing in the Biomedical Domain. Philadelphia, PA. 
Resnik, P. (1995) Using information content to evaluate 
semantic similarity. In Proceedings of the 14th Inter-
national Joint Conference on Artificial Intelligence, 
pages 448?453, Montreal. 
Sanfilippo A., C. Posse and B. Gopalan (2004) Aligning 
the Gene Ontologies. In Proceedings of the Stan-
dards and Ontologies for Functional Genomics Con-
ference 2, Philadelphia, PA, http://www.sofg.org/ 
meetings/sofg2004/Sanfilippo.ppt.  
Salton, G., A. Wong and C. S. Yang (1975) A Vector 
space model for automatic indexing, CACM 
18(11):613-620. 
Smith, T. and M. S. Waterman (1981) Identification of 
common molecular subsequences. J. Mol. Biol. 
147:195-197. 
Weisberg, S. (2005) Applied linear regression. Wiley, 
New York. 
Yang, Y. and J.O. Pedersen (1997) A comparative 
Study on feature selection in text categorization. In 
Proceedings of the 14th International Conference on 
Machine Learning (ICML), pages 412-420, Nash-
ville.  
 
32
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 264?267,
Prague, June 2007. c?2007 Association for Computational Linguistics
PNNL: A Supervised Maximum Entropy Approach to Word Sense 
Disambiguation
Stephen Tratz, Antonio Sanfilippo, Michelle Gregory, Alan Chappell, Christian 
Posse, Paul Whitney
Pacific Northwest National Laboratory
902 Battelle Blvd, PO Box 999
Richland, WA 99352, USA
{stephen.tratz, antonio.sanfilippo, michelle, alan.chap-
pell, christian.posse, paul.whitney}@pnl.gov
Abstract
In  this  paper,  we  described  the  PNNL 
Word Sense Disambiguation system as ap-
plied  to  the  English  all-word  task  in  Se-
mEval 2007. We use a supervised learning 
approach,  employing  a  large  number  of 
features and using Information Gain for di-
mension  reduction.  The  rich  feature  set 
combined with a Maximum Entropy classi-
fier  produces results  that  are significantly 
better than baseline and are the highest F-
score  for  the  fined-grained  English  all-
words subtask of SemEval.
1 Introduction
Accurate  word  sense  disambiguation  (WSD)  can 
support  many  natural  language  processing  and 
knowledge management  tasks.  The  main goal  of 
the  PNNL  WSD system  is  to  support  Semantic 
Web applications, such as semantic-driven search 
and  navigation,  through  a  reliable  mapping  of 
words  in  naturally  occurring  text  to  ontological 
classes.  As described  in  Sanfilippo et  al.  (2006), 
this goal is achieved by defining a WordNet-based 
(Fellbaum,  1998)  ontology that  offers  a manage-
able set of concept classes, provides an extensive 
characterization of concept class in terms of lexical 
instances, and integrates an automated class recog-
nition algorithm. We found that the same features 
that are useful for predicting word classes are also 
useful in distinguishing individual word senses. 
Our main objective in this paper is to predict in-
dividual word senses using a large combination of 
features  including contextual,  semantic,  and syn-
tactic information. In our earlier paper (Sanfilippo 
et al, 2006), we reported that the PNNL WSD sys-
tem exceeded the performance of the best perform-
ers  for  verbs  in  the  SENSEVAL-3  English  all-
words task dataset. SemEval 2007 is our first op-
portunity  to  enter  a  word  sense  disambiguation 
competition.
2 Approach
While many unsupervised word sense disambigua-
tion systems have been created, supervised systems 
have generally produced superior  results  (Snyder 
and Palmer, 2004; Mihalcea et al, 2004). Our sys-
tem is based on a supervised WSD approach that 
uses  a  Maximum  Entropy  classifier  to  predict 
WordNet senses.
We  use  SemCor1,  OMWE 1.0  (Chklovski  and 
Mihalcea, 2002), and example sentences in Word-
Net  as  the  training  corpus.  We  utilize  the 
OpenNLP MaxEnt  implementation2 of  the  maxi-
mum  entropy  classification  algorithm  (Berger  et 
al.,  1996)  to  train  classification  models  for  each 
lemma and part-of-speech combination in the train-
ing  corpus.  These  models  are  used  to  predict 
WordNet  senses  for  words found in natural  text. 
For  lemma  and  part-of-speech  combinations  that 
are not present in the training corpus, the PNNL 
WSD system defaults to the most frequent Word-
Net sense.
2.1 Features
We use a rich set of features to predict individual 
word senses.  A large number of  features are ex-
tracted for each word sense instance in the training 
data.  Following  Dang & Palmer  (2005)  and Ko-
homban & Lee (2005), we use contextual, syntac-
tic and semantic  information to inform our word 
1
 http://www.cs.unt.edu/~rada/downloads.html. 
2
 http://maxent.sourceforge.net/.
264
sense disambiguation system. However,  there are 
significant  differences  between the specific  types 
of contextual,  syntactic  and semantic information 
we use in our system and those proposed by Dang 
& Palmer  (2005)  and Kohomban  & Lee (2005). 
More specifically,  we employ novel  features  and 
feature combinations, as described below. 
? Contextual information. The contextual infor-
mation we use includes the word under analy-
sis plus the three tokens found on each side of 
the word, within sentence boundaries. Tokens 
include both words and punctuation.
? Syntactic information. We include grammatical 
dependencies  (e.g.  subject,  object)  and  mor-
pho-syntactic  features such as part of speech, 
case, number and tense. We use the Connexor 
parser3 (Tapanainen and J?rvinen, 1997) to ex-
tract lemma information, parts of speech, syn-
tactic  dependencies,  tense,  case,  and  number 
information.  A sample output  of  a  Connexor 
parse is given in Table 1. Features are extract-
ed  for  all  tokens  that  are  related  through no 
more than 3 levels of dependency to the word 
to be disambiguated. 
? Semantic  information.  The semantic  informa-
tion  we  incorporate  includes  named  entity 
types (e.g. PERSON, LOCATION, ORGANI-
ZATION) and hypernyms. We use OpenNLP4 
and  LingPipe5 to  identify  named  entities,  re-
placing the strings identified as named entities 
(e.g., Joe Smith) with the corresponding entity 
type  (PERSON).  We also  substitute  personal 
pronouns  that  unambiguously  denote  people 
with the entity type PERSON. Numbers in the 
text  are  replaced  with  type  label  NUMBER. 
Hypernyms  are  retrieved  from WordNet  and 
added to the feature set for all noun tokens se-
lected by the contextual and syntactic rules. In 
contrast to Dang & Palmer (2005), we only in-
clude  the  hypernyms  of  the  most  frequent 
sense,  and  we  include  the  entire  hypernym 
chain (e.g. motor, machine, device, instrumen-
tality, artifact, object, whole, entity).
To address feature extraction processes specific 
to  noun and verbs,  we add the  following  condi-
tions.
3
 http://www.connexor.com/.
4
 http://opennlp.sourceforge.nt/.
5
 http://www.alias-i.com/lingpipe/.
? Syntactic  information  for  verbs.  If  the  verb 
does not have a subject, the subject of the clos-
est ancestor verb in the syntax tree is used in-
stead.
? Syntactic information for nouns. The first verb 
ancestor in the syntax tree is also used to gen-
erate features. 
? Semantic information for nouns. A feature in-
dicating whether a token is capitalized for each 
of the tokens used to generate features.
A sample of the resulting feature vectors that are 
used by the PNNL word sense disambiguation sys-
tem is presented in Table 2.
ID Word Lemma Grammatical 
Dependen-
cies
Morphosyntactic 
Features
1
2
3
4
5
6
the
engine
throbbe
d
into
life
.
the
engine
throb
into
life
.
det:>2
subj:>3
main:>0
goa:>3
pcomp:>4
@DN> %>N DET
@SUBJ %NH N NOM SG
@+FMAINV %VA V PAST
@ADVL %EH PREP
@<P %NH N NOM SG
Table 1. Connexor sample output for the sentence 
?The engine throbbed into life?.
the pre:2:the, pre:2:pos:DET, det:the, det:pos:DET, 
hassubj:det:
engine pre:1:instrumentality, pre:1:object, pre:1:artifact,
 pre:1:device, pre:1:engine, pre:1:motor, pre:1:whole, 
pre:1:entity, pre:1:machine, pre:1:pos:N, 
pre:1:case:NOM, 
pre:1:num:SG,subj:instrumentality,subj:object, subj:arti-
fact, subj:device, subj:engine, subj:motor, subj:whole, 
subj:entity, subj:machine, subj:pos:N, hassubj:, 
subj:case:NOM, subj:num:SG,
throbbed haspre:1:,haspre:2:,haspost:1:, haspost:2:, haspost:3:,
self:throb, self:pos:V, main:,throbbed, self:tense:PAST
into post:1:into, post:1:pos:PREP, goa:into, goa:pos:PREP, 
life post:2:life, post:2:state, post:2:being, post:2:pos:N, 
post:2:case:NOM, post:2:num:SG, hasgoa:, pcomp:life, 
pcomp:state, pcomp:being, pcomp:pos:N, 
hasgoa:pcomp:, goa:pcomp:case:NOM, 
goa:pcomp:num:SG
. post:3:.
Table  2. Feature  vector  for  throbbed in the sen-
tence ?The engine throbbed into life?.
As the example in Table 2 indicates, the combi-
nation of contextual, syntactic, and semantic infor-
mation types results in a large number of features. 
Inspection  of  the  training data  reveals  that  some 
features may be more important than others in es-
tablishing word sense assignment for each choice 
of word lemma. We use a feature selection proce-
265
dure to reduce the full set of features to the feature 
subset that is most relevant to word sense assign-
ment for each lemma. This practice improves the 
efficiency of our word sense disambiguation algo-
rithm. The feature selection procedure we adopted 
consists of scoring each potential feature according 
to  a  particular  feature  selection  metric,  and  then 
taking the best k features.
We choose Information Gain as our feature se-
lection metric. Information Gain measures the de-
crease in entropy when the feature is given versus 
when it is absent. Yang and Pederson (1997) report 
that  Information Gain outperformed other feature 
selection  approaches  in  their  multi-class  bench-
marks,  and  Foreman  (2003)  showed  that  it  per-
formed amongst the best for his 2-class problems. 
3 Evaluation
To evaluate our approach and feature set, we ran 
our  model  on  the  SENSEVAL-3  English  all-words 
task test data. Using data provided by the SENSE-
VAL website6, we were able to compare our results 
for  verbs  to  the  top  performers  on  verbs  alone. 
Upali S. Kohomban and Wee Sun Lee provided us 
with  the  results  file  for  the  Simil-Prime  system 
(Kohomban and Lee, 2005). As reported in Sanfil-
ippo et al (2006) and shown in table 3, our results 
for verbs rival those of top performers. We had a 
significant  improvement  (p-value<0.05)  over  the 
baseline of  52.9%, a marginal  improvement  over 
the second best performer (SenseLearner) (Mihal-
cea and Faruque, 2004), and we were as good as 
the top performer (GAMBL) (Decadt et al, 2004).7
System Precision Fraction of 
Recall
Our system 61% 22%
GAMBL 59.0% 21.3%
SenseLearner 56.1% 20.2%
Baseline 52.9% 19.1%
Table 3. Results for verb sense disambiguation on 
SENSEVAL-3 data, adapted from Sanfilippo et al 
(2006).
Since then, we have expanded our evaluation to 
all parts of speech. Table 4 provides the evaluation 
6
 http://www.senseval.org/.
7
 The 2% improvement in precision which our system 
showed as  compared to GAMBL was not statistically 
significant (p=0.21).
of our system as compared  to  the three top per-
formers on the SENSEVAL-3 data and the baseline. 
The baseline of 0.631 F-score8 was computed us-
ing the most frequent WordNet sense. The PNNL 
WSD system performs significantly better than the 
baseline (p-value<0.05) and rivals the top perform-
ers.  The performance of the PNNL WSD system 
relative to the other three systems and the baseline 
remains unchanged when the unknown sense an-
swers  (denoted  by a  ?U?)  are  excluded  from the 
evaluation.
System Precision Recall
PNNL 0.670 0.670
Simil-Prime 0.661 0.663
GAMBL 0.652 0.652
SenseLearner 0.646 0.646
Baseline 0.631 0.631
Table 4. SENSEVAL-3 English all-words.
System Recall Precision 
PNNL 0.669 0.671
GAMBL 0.651 0.651
Simil-Prime 0.644 0.657
SenseLearner 0.642 0.651
Baseline 0.631 0.631
Table 5. SENSEVAL-3 English all-words, No ?U?.
4 Experimental  results  on  SemEval  all-
words subtask
This was our first opportunity to test our model in 
a WSD competition. For this competition, we fo-
cused our efforts  on the fine-grained English all-
words task because our system was set up to per-
form fine-grained WordNet  sense  prediction.  We 
are  pleased that  our  system achieved the  highest 
score for this subtask. Our results for the SemEval 
dataset as compared to baseline are reported in Ta-
ble 6. The PNNL WSD system did not assign the 
unknown sense, ?U?, to any word instances in the 
SemEval dataset.
8
 This baseline is slightly higher than that reported by 
others (Snyder and Palmer 2004).
266
System F-score
PNNL 0.591
Baseline 0.514
p-value <0.01
Table 6. SemEval Results.
5 Discussion
Although these results are promising, there is still 
much work to be done. For example, we need to 
investigate the contribution of each feature to the 
overall performance of the system in terms of pre-
cision and recall. Such a feature sensitivity analysis 
will provide us with a better understanding of how 
the algorithm can be further improved and/or made 
more efficient by leaving out features whose con-
tribution is negligible. 
Another important point to make is that, while 
our system shows the best precision/recall results 
overall,  we  can  only  claim  statistical  relevance 
with  reference  to  the  baseline  and  results  worse 
than  baseline.  The  size  of  the  SemEval  data  set 
(N=465) is too small to establish whether the dif-
ference in precision/recall results with the other top 
systems is statistically significant. 
Acknowledgements
We would like to thank Upali  S. Kohomban and 
Wee Sun Lee for  providing us with their  SENSE-
VAL-3 English all-words task results file for Simil-
Prime. Many thanks also to Patrick Paulson, Bob 
Baddeley, Ryan Hohimer, and Amanda White for 
their  help  in  developing  the  word  class  disam-
biguation system on which the work presented in 
this paper is based.
References
Berger, A., S. Della Pietra and V. Della Pietra (1996) A 
Maximum  Entropy  Approach  to  Natural  Language 
Processing.  Computational  Linguistics,  volume  22, 
number 1, pages 39-71.
Chklovski, T. and R. Mihalcea (2002) Building a sense 
tagged corpus with open mind word expert. In  Pro-
ceedings of the ACL-02 workshop on Word sense dis-
ambiguation: recent successes and future directions.
Dang, H. T. and M. Palmer (2005) The Role of Semant-
ic Roles in Disambiguating Verb Senses. In Proceed-
ings of the 43rd Annual Meeting of the Association  
for Computational Linguistics,  Ann Arbor MI, June 
26-28, 2005. 
Decadt,  B., V. Hoste,  W. Daelemans and A. Van den 
Bosch (2004) GAMBL, genetic  algorithm optimiza-
tion of memory-based WSD. SENSEVAL-3: Third In-
ternational Workshop on the Evaluation of Systems  
for the Semantic Analysis of Text. Barcelona, Spain. 
Fellbaum,  C.,  editor.  (1998)  WordNet:  An  Electronic 
Lexical Database. MIT Press, Cambridge, MA.
Foreman, G. (2003) An Extensive Empirical Study of 
Feature  Selection  Metrics  for  Text  Classification. 
Journal  of  Machine  Learning  Research,  3,  pages 
1289-1305. 
Kohomban, U. and  W. Lee (2005) Learning semantic 
classes  for  word sense disambiguation.  In Proceed-
ings of the 43rd Annual meeting of the Association for  
Computational Linguistics, Ann Arbor, MI.
Mihalcea,  R.,  T.  Chklovski,  and  A.  Kilgarriff  (2004) 
The  SENSEVAL-3  English   Lexical  Sample  Task, 
SENSEVAL-3: Third International Workshop on the  
Evaluation of  Systems for the Semantic Analysis of  
Text. Barcelonna, Span.
Mihalcea,  R.  and  E.  Faruque   (2004)  SenseLearner: 
Minimally supervised word sense disambiguation for 
all words in open text.  SENSEVAL-3: Third Interna-
tional Workshop on the Evaluation of Systems for the 
Semantic Analysis of Text. Barcelona, Spain.
Sanfilippo, A.,  S.  Tratz,  M. Gregory, A.  Chappell,  P. 
Whitney, C. Posse, P. Paulson, B. Baddeley, R. Hohi-
mer,  A.  White  (2006)  Automating  Ontological  An-
notation with WordNet. Proceedings to the Third In-
ternational WordNet Conference, Jan 22-26, Jeju Is-
land, Korea.
Snyder,  B.   and  M.  Palmer.  2004.  The  English  All-
Words  Task.  SENSEVAL-3:  Third  International 
Workshop on the Evaluation of  Systems for the Se-
mantic Analysis of Text. Barcelona, Spain. 
Tapanainen, P. and Timo J?rvinen (1997) A nonproject-
ive  dependency  parser.  In  Proceedings  of  the  5th 
Conference on Applied Natural Language Processing, 
pages 64?71, Washington D.C. Association for Com-
putational Linguistics.
Yang,  Y.  and  J.  O.  Pedersen  (1997)  A  Comparative 
Study on Feature Selection in Text Categorization. In 
Proceedings of the 14th International Conference on  
Machine Learning (ICML), pages 412-420, 1997.
267
